{
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_kh_Symmetry_Understanding_of_3D_Shapes_via_Chirality_Disentanglement_ICCV_2025_paper.html": {
    "title": "kh: Symmetry Understanding of 3D Shapes via Chirality Disentanglement",
    "volume": "main",
    "abstract": "Chirality information (i.e. information that allows distinguishing left from right) is ubiquitous for various data modes in computer vision, including images, videos, point clouds, and meshes. While chirality has been extensively studied in the image domain, its exploration in shape analysis (such as point clouds and meshes) remains underdeveloped. Although many shape vertex descriptors have shown appealing properties (e.g. robustness to rigid-body transformations), they are often not able to disambiguate between left and right symmetric parts. Considering the ubiquity of chirality information in different shape analysis problems and the lack of chirality-aware features within current shape descriptors, developing a chirality feature extractor becomes necessary and urgent. Based on the recent Diff3F framework, we propose an unsupervised chirality feature extraction pipeline to decorate shape vertices with chirality-aware information, extracted from 2D foundation models. We evaluated the extracted chirality features through quantitative and qualitative experiments across diverse datasets. Results from downstream tasks including left-right disentanglement, shape matching, and part segmentation demonstrate their effectiveness and practical utility. Project page: https://wei-kang-wang.github.io/chirality/",
    "checked": false,
    "id": "f69623a7ac98283cba1f11fa0728054b4f99ce4a",
    "semantic_title": "symmetry understanding of 3d shapes via chirality disentanglement",
    "citation_count": 0,
    "authors": [
      "Weikang Wang",
      "Tobias Weißberg",
      "Nafie El Amrani",
      "Florian Bernard"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_Efficient_Adaptation_of_Pre-trained_Vision_Transformer_underpinned_by_Approximately_Orthogonal_ICCV_2025_paper.html": {
    "title": "Efficient Adaptation of Pre-trained Vision Transformer underpinned by Approximately Orthogonal Fine-Tuning Strategy",
    "volume": "main",
    "abstract": "A prevalent approach in Parameter-Efficient Fine-Tuning (PEFT) of pre-trained Vision Transformers (ViT) involves freezing the majority of the backbone parameters and solely learning low-rank adaptation weight matrices to accommodate downstream tasks. These low-rank matrices are commonly derived through the multiplication structure of down-projection and up-projection matrices, exemplified by methods such as LoRA and Adapter. In this study, we observe an approximate orthogonality among any two row or column vectors within any weight matrix of the backbone parameters; however, this property is absent in the vectors of the down/up-projection matrices. Approximate orthogonality implies a reduction in the upper bound of the model's generalization error, signifying that the model possesses enhanced generalization capability. If the fine-tuned down/up-projection matrices were to exhibit this same property as the pre-trained backbone matrices, could the generalization capability of fine-tuned ViTs be further augmented? To address this question, we propose an Approximately Orthogonal Fine-Tuning (AOFT) strategy for representing the low-rank weight matrices. This strategy employs a single learnable vector to generate a set of approximately orthogonal vectors, which form the down/up-projection matrices, thereby aligning the properties of these matrices with those of the backbone. Extensive experimental results demonstrate that our method achieves competitive performance across a range of downstream image classification tasks, confirming the efficacy of the enhanced generalization capability embedded in the down/up-projection matrices. Our code is available at link",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiting Yang",
      "Hao Luo",
      "Yuan Sun",
      "Qingsen Yan",
      "Haokui Zhang",
      "Wei Dong",
      "Guoqing Wang",
      "Peng Wang",
      "Yang Yang",
      "Hengtao Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ding_MM-IFEngine_Towards_Multimodal_Instruction_Following_ICCV_2025_paper.html": {
    "title": "MM-IFEngine: Towards Multimodal Instruction Following",
    "volume": "main",
    "abstract": "The Instruction Following (IF) ability measures how well Multi-modal Large Language Models (MLLMs) understand exactly what users are telling them and doing it right.Existing multimodal instruction following training data is scarce, the benchmarks are simple with atomic instructions, and the evaluation strategies are imprecise for tasks demanding exact output constraints.To address this, we present MM-IFEngine, an effective pipeline to generate high-quality image-instruction pairs.Our MM-IFEngine pipeline yields large-scale, diverse, and high-quality training data MM-IFInstruct-23k, which is suitable for Supervised Fine-Tuning (SFT) and extended as MM-IFDPO-23k for Direct Preference Optimization (DPO).We further introduce MM-IFEval, a challenging and diverse multi-modal instruction-following benchmark that includes (1) both textual constraints for output responses and visual constraints tied to the input images, and (2) a comprehensive evaluation pipeline incorporating rule-based assessment and LLM-as-a-Judge evaluation.We conduct SFT and DPO experiments and demonstrate that fine-tuning MLLMs on MM-IFInstruct-23k and MM-IFDPO-23k achieve notable gains on various IF benchmarks, such as MM-IFEval (+11.8%), MIA (+7.7%), and IFEval (+10.5%)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengyuan Ding",
      "Shenxi Wu",
      "Xiangyu Zhao",
      "Yuhang Zang",
      "Haodong Duan",
      "Xiaoyi Dong",
      "Pan Zhang",
      "Yuhang Cao",
      "Dahua Lin",
      "Jiaqi Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_Who_is_a_Better_Talker_Subjective_and_Objective_Quality_Assessment_ICCV_2025_paper.html": {
    "title": "Who is a Better Talker: Subjective and Objective Quality Assessment for AI-Generated Talking Heads",
    "volume": "main",
    "abstract": "Speech-driven methods for portraits are figuratively known as \"Talkers\" because of their capability to synthesize speaking mouth shapes and facial movements. Especially with the rapid development of the Text-to-Image (T2I) models, AI-Generated Talking Heads (AGTHs) have gradually become an emerging digital human media. However, challenges persist regarding the quality of these talkers and AGTHs they generate, and comprehensive studies addressing these issues remain limited. To address this gap, this paper presents the largest AGTH quality assessment dataset THQA-10K to date, which selects 12 prominent T2I models and 14 advanced talkers to generate AGTHs for 14 prompts. After excluding instances where AGTH generation is unsuccessful, the THQA-10K dataset contains 10,457 AGTHs, which provides rich material for AGTH quality assessment. Then, volunteers are recruited to subjectively rate the AGTHs and give the corresponding distortion categories. In our analysis for subjective experimental results, we evaluate the performance of talkers in terms of generalizability and quality, and also expose the distortions of existing AGTHs. Finally, an objective quality assessment method based on the first frame, Y-T slice and tone-lip consistency is proposed. Experimental results show that this method can achieve state-of-the-art (SOTA) performance in AGTH quality assessment. The work is released at https://github.com/zyj-2000/Talker",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingjie Zhou",
      "Jiezhang Cao",
      "Zicheng Zhang",
      "Farong Wen",
      "Yanwei Jiang",
      "Jun Jia",
      "Xiaohong Liu",
      "Xiongkuo Min",
      "Guangtao Zhai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_LayerAnimate_Layer-level_Control_for_Animation_ICCV_2025_paper.html": {
    "title": "LayerAnimate: Layer-level Control for Animation",
    "volume": "main",
    "abstract": "Traditional animation production decomposes visual elements into discrete layers to enable independent processing for sketching, refining, coloring, and in-betweening. Existing anime generation video methods typically treat animation as a distinct data domain different from real-world videos, lacking fine-grained control at the layer level. To bridge this gap, we introduce LayerAnimate, a novel video diffusion framework with layer-aware architecture that empowers the manipulation of layers through layer-level controls. The development of a layer-aware framework faces a significant data scarcity challenge due to the commercial sensitivity of professional animation assets. To address the limitation, we propose a data curation pipeline featuring Automated Element Segmentation and Motion-based Hierarchical Merging. Through quantitative and qualitative comparisons and user study, we demonstrate that LayerAnimate outperforms current methods in terms of animation quality, control precision, and usability, making it an effective tool for both professional animators and amateur enthusiasts. This framework opens up new possibilities for layer-level animation applications and creative flexibility. Our code is available at https://layeranimate.github.io",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxue Yang",
      "Lue Fan",
      "Zuzeng Lin",
      "Feng Wang",
      "Zhaoxiang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Towards_a_Unified_Copernicus_Foundation_Model_for_Earth_Vision_ICCV_2025_paper.html": {
    "title": "Towards a Unified Copernicus Foundation Model for Earth Vision",
    "volume": "main",
    "abstract": "Advances in Earth observation (EO) foundation models have unlocked the potential of big satellite data to learn generic representations from space, benefiting a wide range of downstream applications crucial to our planet. However, most existing efforts remain limited to fixed spectral sensors, focus solely on the Earth's surface, and overlook valuable metadata beyond imagery. In this work, we take a step towards next-generation EO foundation models with three key components: 1) Copernicus-Pretrain, a massive-scale pretraining dataset that integrates 18.7M aligned images from all major Copernicus Sentinel missions, spanning from the Earth's surface to its atmosphere; 2) Copernicus-FM, a unified foundation model capable of processing any spectral or non-spectral sensor modality using extended dynamic hypernetworks and flexible metadata encoding; and 3) Copernicus-Bench, a systematic evaluation benchmark with 15 hierarchical downstream tasks ranging from preprocessing to specialized applications for each Sentinel mission. Our dataset, model, and benchmark greatly improve the scalability, versatility, and multimodal adaptability of EO foundation models, while also creating new opportunities to connect EO, weather, and climate research. Codes at https://github.com/zhu-xlab/Copernicus-FM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Wang",
      "Zhitong Xiong",
      "Chenying Liu",
      "Adam J. Stewart",
      "Thomas Dujardin",
      "Nikolaos Ioannis Bountos",
      "Angelos Zavras",
      "Franziska Gerken",
      "Ioannis Papoutsis",
      "Laura Leal-Taixé",
      "Xiao Xiang Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ghosh_ROADWork_A_Dataset_and_Benchmark_for_Learning_to_Recognize_Observe_ICCV_2025_paper.html": {
    "title": "ROADWork: A Dataset and Benchmark for Learning to Recognize, Observe, Analyze and Drive Through Work Zones",
    "volume": "main",
    "abstract": "Perceiving and autonomously navigating through work zones is a challenging and under-explored problem. Open datasets for this long-tailed scenario are scarce. We propose the ROADWork dataset to learn to recognize, observe, analyze, and drive through work zones. State-of-the-art foundation models fail when applied to work zones. Fine-tuning models on our dataset significantly improves perception and navigation in work zones. With ROADWork, we discover new work zone images with higher precision (+32.5%) at a much higher rate (12.8x) around the world. Open-vocabulary methods fail too, whereas fine-tuned detectors improve performance (+32.2 AP).Vision-Language Models (VLMs) struggle to describe work zones, but fine-tuning substantially improves performance (+36.7 SPICE). Beyond fine-tuning, we show the value of simple techniques. Video label propagation provides additional gains (+2.6 AP) for instance segmentation. While reading work zone signs, composing a detector and text spotter via crop-scaling improves performance (+14.2% 1-NED). Composing work zone detections to provide context further reduces hallucinations (+3.9 SPICE) in VLMs. We predict navigational goals and compute drivable paths from work zone videos. Incorporating road work semantics ensures 53.6% goals have angular error (AE) < 0.5 (+9.9%) and 75.3% pathways have AE < 0.5 (+8.1%)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anurag Ghosh",
      "Shen Zheng",
      "Robert Tamburo",
      "Khiem Vuong",
      "Juan Alvarez-Padilla",
      "Hailiang Zhu",
      "Michael Cardei",
      "Nicholas Dunn",
      "Christoph Mertz",
      "Srinivasa G. Narasimhan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Luo_Gradient_Decomposition_and_Alignment_for_Incremental_Object_Detection_ICCV_2025_paper.html": {
    "title": "Gradient Decomposition and Alignment for Incremental Object Detection",
    "volume": "main",
    "abstract": "Incremental object detection (IOD) is crucial for enabling AI systems to continuously learn new object classes over time while retaining knowledge of previously learned categories, allowing model to adapt to dynamic environments without forgetting prior information.Existing IOD methods primarily employ knowledge distillation to mitigate catastrophic forgetting, yet these approaches overlook class overlap issues, often resulting in suboptimal performance. In this paper, we propose a novel framework for IOD that leverages a decoupled gradient alignment technique on top of the specially proposed pseudo-labeling strategy. Our method employs a Gaussian Mixture Model to accurately estimate pseudo-labels of previously learned objects in current training images, effectively functioning as a knowledge-replay mechanism. This strategy reinforces prior knowledge retention and prevents the misclassification of unannotated foreground objects from earlier classes as background. Furthermore, we introduce an adaptive gradient decomposition and alignment method to maintain model stability while facilitating positive knowledge transfer. By aligning gradients from both old and new classes, our approach preserves previously learned knowledge while enhancing plasticity for new tasks. Extensive experiments on two IOD benchmarks demonstrate the effectiveness of the proposed method, achieving superior performances to state-of-the-art methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenlong Luo",
      "Shizhou Zhang",
      "De Cheng",
      "Yinghui Xing",
      "Guoqiang Liang",
      "Peng Wang",
      "Yanning Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Mao_One_Polyp_Identifies_All_One-Shot_Polyp_Segmentation_with_SAM_via_ICCV_2025_paper.html": {
    "title": "One Polyp Identifies All: One-Shot Polyp Segmentation with SAM via Cascaded Priors and Iterative Prompt Evolution",
    "volume": "main",
    "abstract": "Polyp segmentation is vital for early colorectal cancer detection, yet traditional fully supervised methods struggle with morphological variability and domain shifts, requiring frequent retraining. Additionally, reliance on large-scale annotations is a major bottleneck due to the time-consuming and error-prone nature of polyp boundary labeling. Recently, vision foundation models like Segment Anything Model (SAM) have demonstrated strong generalizability and fine-grained boundary detection with sparse prompts, effectively addressing key polyp segmentation challenges. However, SAM's prompt-dependent nature limits automation in medical applications, since manually inputting prompts for each image is labor-intensive and time-consuming. We propose OP-SAM, a One-shot Polyp segmentation framework based on SAM that automatically generates prompts from a single annotated image, ensuring accurate and generalizable segmentation without additional annotation burdens. Our method introduces Correlation-based Prior Generation (CPG) for semantic label transfer and Scale-cascaded Prior Fusion (SPF) to adapt to polyp size variations as well as filter out noisy transfers. Instead of dumping all prompts at once, we devise Euclidean Prompt Evolution (EPE) for iterative prompt refinement, progressively enhancing segmentation quality. Extensive evaluations across five datasets validate OP-SAM's effectiveness. Notably, on Kvasir, it achieves 76.93% IoU, surpassing the state-of-the-art by 11.44%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyu Mao",
      "Xiaohan Xing",
      "Fei Meng",
      "Jianbang Liu",
      "Fan Bai",
      "Qiang Nie",
      "Max Meng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Asaad_Gradient_Extrapolation_for_Debiased_Representation_Learning_ICCV_2025_paper.html": {
    "title": "Gradient Extrapolation for Debiased Representation Learning",
    "volume": "main",
    "abstract": "Machine learning classification models trained with empirical risk minimization (ERM) often inadvertently rely on spurious correlations. When absent in the test data, these unintended associations between non-target attributes and target labels lead to poor generalization. This paper addresses this problem from a model optimization perspective and proposes a novel method, Gradient Extrapolation for Debiased Representation Learning (GERNE), designed to learn debiased representations in both known and unknown attribute training cases. GERNE uses two distinct batches with different amounts of spurious correlations and defines the target gradient as a linear extrapolation of the gradients computed from each batch's loss. Our analysis shows that when the extrapolated gradient points toward the batch gradient with fewer spurious correlations, it effectively guides training toward learning a debiased model. GERNE serves as a general framework for debiasing, encompassing ERM and Resampling methods as special cases. We derive the theoretical upper and lower bounds of the extrapolation factor employed by GERNE. By tuning this factor, GERNE can adapt to maximize either Group-Balanced Accuracy (GBA) or Worst-Group Accuracy (WGA). We validate GERNE on five vision and one NLP benchmarks, demonstrating competitive and often superior performance compared to state-of-the-art baselines. The project page is available at: https://gerne-debias.github.io/",
    "checked": true,
    "id": "7e03b86975ccac852cdae81b5a273080f0fb1cad",
    "semantic_title": "gradient extrapolation for debiased representation learning",
    "citation_count": 0,
    "authors": [
      "Ihab Asaad",
      "Maha Shadaydeh",
      "Joachim Denzler"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_From_Gaze_to_Movement_Predicting_Visual_Attention_for_Autonomous_Driving_ICCV_2025_paper.html": {
    "title": "From Gaze to Movement: Predicting Visual Attention for Autonomous Driving Human-Machine Interaction based on Programmatic Imitation Learning",
    "volume": "main",
    "abstract": "Human-machine interaction technology requires not only the distribution of human visual attention but also the prediction of the gaze point trajectory. We introduce PILOT, a programmatic imitation learning approach that predicts a driver's eye movements based on a set of rule-based conditions. These conditions--derived from driving operations and traffic flow characteristics--define how gaze shifts occur. They are initially identified through incremental synthesis, a heuristic search method, and then refined via L-BFGS, a numerical optimization technique. These human-readable rules enable us to understand drivers' eye movement patterns and make efficient and explainable predictions. We also propose DATAD, a dataset that covers 12 types of autonomous driving takeover scenarios, collected from 60 participants and comprising approximately 600,000 frames of gaze point data. Compared to existing eye-tracking datasets, DATAD includes additional driving metrics and surrounding traffic flow characteristics, providing richer contextual information for modeling gaze behavior. Experimental evaluations of PILOT on DATAD demonstrate superior accuracy and faster prediction speeds compared to four baseline models. Specifically, PILOT reduces the MSE of predicted trajectories by 38.59% to 88.02% and improves the accuracy of gaze object predictions by 6.90% to 55.06%. Moreover, PILOT achieves these gains with approximately 30% lower prediction time, offering both more accurate and more efficient eye movement prediction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yexin Huang",
      "Yongbin Lin",
      "Lishengsa Yue",
      "Zhihong Yao",
      "Jie Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_Less-to-More_Generalization_Unlocking_More_Controllability_by_In-Context_Generation_ICCV_2025_paper.html": {
    "title": "Less-to-More Generalization: Unlocking More Controllability by In-Context Generation",
    "volume": "main",
    "abstract": "Although subject-driven generation has been extensively explored in image generation due to its wide applications, it still has challenges in data scalability and subject expansibility. For the first challenge, moving from curating single-subject datasets to multiple-subject ones and scaling them is particularly difficult. For the second, most recent methods center on single-subject generation, making it hard to apply when dealing with multi-subject scenarios. In this study, we propose a highly-consistent data synthesis pipeline to tackle these challenges. This pipeline harnesses the intrinsic in-context generation capabilities of diffusion transformers and generates high-consistent multi-subject paired data. Additionally, we introduce UNO, a multi-subject driven customization architecture based on a diffusion transformer. UNO incorporates a progressive cross-modal alignment training paradigm that progresses from simpler single-subject conditioning to more complex multi-subject conditioning. Along with this, a universal rotary position embedding (UnoPE) adjusts the position indices. Extensive experiments show that our method can achieve high consistency while ensuring controllability in both single-subject and multi-subject driven generation. Code and model: https://github.com/bytedance/UNO",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaojin Wu",
      "Mengqi Huang",
      "Wenxu Wu",
      "Yufeng Cheng",
      "Fei Ding",
      "Qian He"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hernandez_Improving_Large_Vision_and_Language_Models_by_Learning_from_a_ICCV_2025_paper.html": {
    "title": "Improving Large Vision and Language Models by Learning from a Panel of Peers",
    "volume": "main",
    "abstract": "Traditional alignment methods for Large Vision and Language Models (LVLMs) primarily rely on human-curated preference data. Human-generated preference data is costly; machine-generated preference data is limited in quality; and self-supervised preference data often introduces hallucinations. To overcome these limitations, we propose a novel Panel-of-Peers learning framework inspired by collaborative learning among humans. This approach leverages a panel of LVLMs, each evaluating and learning from their collective outputs through an iterative self-improvement process. By simulating a peer review system, our models generate, assess, and refine outputs in response to a curated set of prompts, mimicking a classroom learning environment. We demonstrate that this methodology enhances model performance without requiring extensive human-labeled datasets. Our experiments show significant improvement across multiple benchmarks, demonstrating the potential of peer evaluations as a scalable alternative to self-supervised alignment. Notably, we show that Panel-of-Peers increases the average score on fifteen benchmarks from 48% to 57%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jefferson Hernandez",
      "Jing Shi",
      "Simon Jenni",
      "Vicente Ordonez",
      "Kushal Kafle"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yi_Federated_Representation_Angle_Learning_ICCV_2025_paper.html": {
    "title": "Federated Representation Angle Learning",
    "volume": "main",
    "abstract": "Model-heterogeneous federated learning (MHFL) is a challenging FL paradigm designed to allow FL clients to train structurally heterogeneous models under the coordination of an FL server. Existing MHFL methods face significant limitations when it comes to transferring global knowledge to clients as a result of sharing only partial homogeneous model parameters or calculating distance loss, leading to inferior model generalization. To bridge this gap, we propose a novel model-heterogeneous Federated learning method with Representation Angle Learning (FedRAL). It consists of three innovative designs: (1) We first introduce representation angle learning into MHFL. Specifically, we embed a homogeneous square matrix into the local heterogeneous model of each client, which learns the angle information of local representations. These homogeneous representation angle square matrices are aggregated on the server to fuse representation angle knowledge shared by clients for enhancing the generalization of local representations. (2) As different clients might have heterogeneous system resources, we propose an adaptive diagonal sparsification strategy to reduce the numbers of the parameters of representation angle square matrices uploaded to the server, to improve FL communication efficiency. (3) To enable the effective fusion of sparsified homogeneous local representation angle square matrices, we design an element-wise weighted aggregation approach. Experiments on 4 benchmark datasets under 2 types of non-IID divisions over 6 state-of-the-art baselines demonstrate that FedRAL achieves the best performance. It improves test accuracy, communication efficiency and computational efficiency by up to 5.03%, 12.43x and 6.49x, respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liping Yi",
      "Han Yu",
      "Gang Wang",
      "Xiaoguang Liu",
      "Xiaoxiao Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zheng_Why_LVLMs_Are_More_Prone_to_Hallucinations_in_Longer_Responses_ICCV_2025_paper.html": {
    "title": "Why LVLMs Are More Prone to Hallucinations in Longer Responses: The Role of Context",
    "volume": "main",
    "abstract": "Large Vision-Language Models (LVLMs) have made significant progress in recent years but are also prone to hallucination issues. They exhibit more hallucinations in longer, free-form responses, often attributed to accumulated uncertainties. In this paper, we ask: Does increased hallucination result solely from length-induced errors, or is there a deeper underlying mechanism? After a series of preliminary experiments and findings, we suggest that the risk of hallucinations is not caused by length itself but by the increased reliance on context for coherence and completeness in longer responses. Building on these insights, we propose a novel \"induce-detect-suppress\" framework that actively induces hallucinations through deliberately designed contexts, leverages induced instances for early detection of high-risk cases, and ultimately suppresses potential object-level hallucinations during actual decoding. Our approach achieves consistent, significant improvements across all benchmarks, demonstrating its efficacy. The strong detection and improved hallucination mitigation not only validate our framework but, more importantly, re-validate our hypothesis on context. Rather than solely pursuing performance gains, this study aims to provide new insights and serves as a first step toward a deeper exploration of hallucinations in LVLMs' longer responses",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ge Zheng",
      "Jiaye Qian",
      "Jiajin Tang",
      "Sibei Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Das_Training-Free_Personalization_via_Retrieval_and_Reasoning_on_Fingerprints_ICCV_2025_paper.html": {
    "title": "Training-Free Personalization via Retrieval and Reasoning on Fingerprints",
    "volume": "main",
    "abstract": "Vision Language Models (VLMs) have lead to major improvements in multimodal reasoning, yet they still struggle to understand user-specific concepts. Existing personalization methods address this limitation butheavily rely on training procedures, that can be either costly or unpleasant to individual users.We depart from existing work, and for the first time explore the training-free setting in the context of personalization. We propose a novel method, Retrieval and Reasoning for Personalization (R2P), leveraging internal knowledge of VLMs. First, we leverage VLMs to extract the concept fingerprint, i.e., key attributes uniquely defining the concept within its semantic class. When a query arrives, the most similar fingerprints are retrieved and scored via chain of thought reasoning. To reduce the risk of hallucinations, the scores are validated through cross-modal verification at the attribute level:in case of a discrepancy between the scores, R2P refines the concept association viapairwise multimodal matching, where the retrieved fingerprints and their images aredirectly compared with the query.We validate R2P on two publicly available benchmarks and a newly introduced dataset, Personal Concepts with Visual Ambiguity (PerVA), for concept identification highlighting challenges in visual ambiguity. R2P consistently outperforms state-of-the-art approaches on various downstream tasks across all benchmarks",
    "checked": true,
    "id": "211770b01a075af91e2c5d29f9481db3eb9037a5",
    "semantic_title": "training-free personalization via retrieval and reasoning on fingerprints",
    "citation_count": 1,
    "authors": [
      "Deepayan Das",
      "Davide Talon",
      "Yiming Wang",
      "Massimiliano Mancini",
      "Elisa Ricci"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chang_How_Far_are_AI-generated_Videos_from_Simulating_the_3D_Visual_ICCV_2025_paper.html": {
    "title": "How Far are AI-generated Videos from Simulating the 3D Visual World: A Learned 3D Evaluation Approach",
    "volume": "main",
    "abstract": "Recent advancements in video diffusion models enable the generation of photorealistic videos with impressive 3D consistency and temporal coherence. However, the extent to which these AI-generated videos simulate the 3D visual world remains underexplored. In this paper, we introduce Learned 3D Evaluation (L3DE), an objective, quantifiable, and interpretable method for assessing AI-generated videos' ability to simulate the real world in terms of 3D visual qualities and consistencies, without requiring manually labeled defects or quality annotations. Instead of relying on 3D reconstruction, which is prone to failure with in-the-wild videos, L3DE employs a 3D convolutional network, trained on monocular 3D cues of motion, depth, and appearance, to distinguish real from synthetic videos. Confidence scores from L3DE quantify the gap between real and synthetic videos in terms of 3D visual coherence, while a gradient-based visualization pinpoints unrealistic regions, improving interpretability. We validate L3DE through extensive experiments, demonstrating strong alignment with 3D reconstruction quality and human judgments. Our evaluations on leading generative models (e.g., Sora, MiniMax, and Kling) reveal persistent simulation gaps and subtle inconsistencies. Beyond generative video assessment, L3DE extends to broader applications: benchmarking video generation models, serving as a deepfake detector, and enhancing video synthesis by inpainting flagged inconsistencies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chirui Chang",
      "Jiahui Liu",
      "Zhengzhe Liu",
      "Xiaoyang Lyu",
      "Yi-Hua Huang",
      "Xin Tao",
      "Pengfei Wan",
      "Di Zhang",
      "Xiaojuan Qi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_Rethinking_Detecting_Salient_and_Camouflaged_Objects_in_Unconstrained_Scenes_ICCV_2025_paper.html": {
    "title": "Rethinking Detecting Salient and Camouflaged Objects in Unconstrained Scenes",
    "volume": "main",
    "abstract": "While the human visual system employs distinct mechanisms to perceive salient and camouflaged objects, existing models struggle to disentangle these tasks. Specifically, salient object detection (SOD) models frequently misclassify camouflaged objects as salient, while camouflaged object detection (COD) models conversely misinterpret salient objects as camouflaged. We hypothesize that this can be attributed to two factors: (i) the specific annotation paradigm of current SOD and COD datasets, and (ii) the lack of explicit aspect relationship modeling in current models. Prevalent SOD/COD datasets enforce a mutual exclusivity constraint, assuming scenes contain either salient or camouflaged objects, which poorly aligns with the real world. Furthermore, current SOD/COD methods are primarily designed for these highly constrained datasets and lack explicit modeling of the relationship between salient and camouflaged objects. In this paper, to promote the development of unconstrained salient and camouflaged object detection, we construct a large-scale dataset, USC12K, which features comprehensive labels and four different scenes that cover all possible logical existence scenarios of both salient and camouflaged objects. To explicitly model the relationship between salient and camouflaged objects, we propose a model called USCNet, which introduces two distinct prompt query mechanisms for modeling inter-sample and intra-sample aspect relationships. Additionally, to assess the model's ability to distinguish between salient and camouflaged objects, we design an evaluation metric called CSCS. The proposed method achieves state-of-the-art performance across all scenes in various metrics. Code and dataset are available at https://github.com/ssecv/USCNet",
    "checked": true,
    "id": "0d4d50f2f51e26df5d25aeb70ae51ecc7cc0b82f",
    "semantic_title": "rethinking detecting salient and camouflaged objects in unconstrained scenes",
    "citation_count": 0,
    "authors": [
      "Zhangjun Zhou",
      "Yiping Li",
      "Chunlin Zhong",
      "Jianuo Huang",
      "Jialun Pei",
      "Hua Li",
      "He Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_OccluGaussian_Occlusion-Aware_Gaussian_Splatting_for_Large_Scene_Reconstruction_and_Rendering_ICCV_2025_paper.html": {
    "title": "OccluGaussian: Occlusion-Aware Gaussian Splatting for Large Scene Reconstruction and Rendering",
    "volume": "main",
    "abstract": "In large-scale scene reconstruction using 3D Gaussian splatting, it is common to partition the scene into multiple smaller regions and reconstruct them individually. However, existing division methods are occlusion-agnostic, meaning that each region may contain areas with severe occlusions. As a result, the cameras within those regions are less correlated, leading to a low average contribution to the overall reconstruction. In this paper, we propose an occlusion-aware scene division strategy that clusters training cameras based on their positions and co-visibilities to acquire multiple regions. Cameras in such regions exhibit stronger correlations and a higher average contribution, facilitating high-quality scene reconstruction. We further propose a region-based rendering technique to accelerate large scene rendering, which culls Gaussians invisible to the region where the viewpoint is located. Such a technique significantly speeds up the rendering without compromising quality. Extensive experiments on multiple large scenes show that our method achieves superior reconstruction results with faster rendering speeds compared to existing state-of-the-art approaches. Project page: https://occlugaussian.github.io",
    "checked": true,
    "id": "26db039e11bc4a67f6ad81407f45411be4b56408",
    "semantic_title": "occlugaussian: occlusion-aware gaussian splatting for large scene reconstruction and rendering",
    "citation_count": 2,
    "authors": [
      "Shiyong Liu",
      "Xiao Tang",
      "Zhihao Li",
      "Yingfan He",
      "Chongjie Ye",
      "Jianzhuang Liu",
      "Binxiao Huang",
      "Shunbo Zhou",
      "Xiaofei Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ma_VisionMath_Vision-Form_Mathematical_Problem-Solving_ICCV_2025_paper.html": {
    "title": "VisionMath: Vision-Form Mathematical Problem-Solving",
    "volume": "main",
    "abstract": "Mathematical problems in real-world scenarios are often presented in a purely vision-form, where textual problem statement and accompanying math figures, e.g., geometry figures and functional graphs, are integrated into a single image. This vision-form problem-solving task requires precise comprehension and reasoning on both textual and graphical elements in the images, posing significant challenge to current Multimodal Large Language Models (MLLMs), which process text and math figures in isolation. In this work, we propose VisionMath, the first exploration for vision-form mathematical problem-solving model, which employs a three-stage progressive multimodal reasoning alignment strategy to systematically enhance task-specific capabilities. Building upon a LLM proficient in unimodal mathematical reasoning, VisionMath first establishes foundational OCR capabilities through capturing rendered mathematical problem images. Subsequently, the model develops comprehensive understanding of figure structures and properties via learning from figure descriptions and mathematical educational videos. Finally, the model's reasoning capacity is activated using carefully constructed visual-form problem-solving datasets VisionMath-IT with chain-of-thought annotations. For comprehensive evaluation, we construct multilingual benchmarks covering diverse problem types, including geometry, algebra, function problems in both English and Chinese. Our model weights, data and code will be made available at https://github.com/mengqiDyangge/VisionMath",
    "checked": false,
    "id": "0696511000c091b4eecabc3691714a3c83685e0e",
    "semantic_title": "mathglm-vision: solving mathematical problems with multi-modal large language model",
    "citation_count": 14,
    "authors": [
      "Zongyang Ma",
      "Yuxin Chen",
      "Ziqi Zhang",
      "Zhongang Qi",
      "Chunfeng Yuan",
      "Shaojie Zhu",
      "Chengxiang Zhuo",
      "Bing Li",
      "Ye Liu",
      "Zang Li",
      "Ying Shan",
      "Weiming Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shou_Unsupervised_RGB-D_Point_Cloud_Registration_for_Scenes_with_Low_Overlap_ICCV_2025_paper.html": {
    "title": "Unsupervised RGB-D Point Cloud Registration for Scenes with Low Overlap and Photometric Inconsistency",
    "volume": "main",
    "abstract": "Point cloud registration is a fundamental task in 3D vision, playing a crucial role in various fields. With the rapid advancement of RGB-D sensors, unsupervised point cloud registration methods based on RGB-D sequences have demonstrated excellent performance. However, existing methods struggle in scenes with low overlap and photometric inconsistency. Low overlap results in numerous correspondence outliers, while photometric inconsistency hinders the model's ability to extract discriminative features. To address these challenges, we first propose the Overlapping Constraint for Inliers Detection (OCID) module, which filters and optimizes the initial correspondence set using an overlappping constraint. This module robustly selects reliable correspondences within the overlapping region while maintaining a balance between accuracy and efficiency. Additionally, we introduce a novel scene representation, 3DGS, which integrates both geometric and texture information, making it particularly well-suited for RGB-D registration tasks. Building on this, we propose the Gaussian Rendering for Photometric Adaptation (GRPA) module, which refines the geometric transformation and enhances the model's adaptability to scenes with inconsistent photometric information. Extensive experiments on ScanNet and ScanNet1500 demonstrate that our method achieves state-of-the-art performance",
    "checked": false,
    "id": "5dd8c8993aa5500078d8115ee7b1970552fd0009",
    "semantic_title": "f2m-reg: unsupervised rgb-d point cloud registration with frame-to-model optimization",
    "citation_count": 1,
    "authors": [
      "Yejun Shou",
      "Haocheng Wang",
      "Lingfeng Shen",
      "Qian Zheng",
      "Gang Pan",
      "Yanlong Cao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_CWNet_Causal_Wavelet_Network_for_Low-Light_Image_Enhancement_ICCV_2025_paper.html": {
    "title": "CWNet: Causal Wavelet Network for Low-Light Image Enhancement",
    "volume": "main",
    "abstract": "Traditional Low-Light Image Enhancement (LLIE) methods primarily focus on uniform brightness adjustment, often neglecting instance-level semantic information and the inherent characteristics of different features. To address these limitations, we propose CWNet (Causal Wavelet Network), a novel architecture that leverages wavelet transforms for causal reasoning. Specifically, our approach comprises two key components: 1) Inspired by the concept of intervention in causality, we adopt a causal reasoning perspective to reveal the underlying causal relationships in low-light enhancement. From a global perspective, we employ a metric learning strategy to ensure causal embeddings adhere to causal principles, separating them from non-causal confounding factors while focusing on the invariance of causal factors. At the local level, we introduce an instance-level CLIP semantic loss to precisely maintain causal factor consistency. 2) Based on our causal analysis, we present a wavelet transform-based backbone network that effectively optimizes the recovery of frequency information, ensuring precise enhancement tailored to the specific attributes of wavelet transforms. Extensive experiments demonstrate that CWNet significantly outperforms current state-of-the-art methods across multiple datasets, showcasing its robust performance across diverse scenes. Code is available at https://github.com/bywlzts/CWNet-Causal-Wavelet-Network",
    "checked": true,
    "id": "bdbd61ba48c2e2f1515fb798e31b1f40d4a6ba4c",
    "semantic_title": "cwnet: causal wavelet network for low-light image enhancement",
    "citation_count": 0,
    "authors": [
      "Tongshun Zhang",
      "Pingping Liu",
      "Yubing Lu",
      "Mengen Cai",
      "Zijian Zhang",
      "Zhe Zhang",
      "Qiuzhan Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cheng_Demeter_A_Parametric_Model_of_Crop_Plant_Morphology_from_the_ICCV_2025_paper.html": {
    "title": "Demeter: A Parametric Model of Crop Plant Morphology from the Real World",
    "volume": "main",
    "abstract": "Learning 3D parametric shape models of objects has gained popularity in vision and graphics and has showed broad utility in 3D reconstruction, generation, understanding, and simulation. While powerful models exist for humans and animals, equally expressive approaches for modeling plants are lacking. In this work, we present Demeter, a data-driven parametric model that encodes key factors of a plant morphology, including topology, shape, articulation, and deformation into a compact learned representation. Unlike previous parametric models, Demeter handles varying shape topology across various species and models three sources of shape variation: articulation, subcomponent shape variation, and non-rigid deformation. To advance crop plant modeling, we collected a large-scale, ground-truthed dataset from a soybean farm as a testbed. Experiments show that Demeter effectively synthesizes shapes, reconstructs structures, and simulates biophysical processes. Code and data will be open-sourced",
    "checked": true,
    "id": "0e72f83b0fdedee78b38e8c7154a5878910a722b",
    "semantic_title": "demeter: a parametric model of crop plant morphology from the real world",
    "citation_count": 0,
    "authors": [
      "Tianhang Cheng",
      "Albert J. Zhai",
      "Evan Z. Chen",
      "Rui Zhou",
      "Yawen Deng",
      "Zitong Li",
      "Kejie Zhao",
      "Janice Shiu",
      "Qianyu Zhao",
      "Yide Xu",
      "Xinlei Wang",
      "Yuan Shen",
      "Sheng Wang",
      "Lisa Ainsworth",
      "Kaiyu Guan",
      "Shenlong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_VideoLLaMB_Long_Streaming_Video_Understanding_with_Recurrent_Memory_Bridges_ICCV_2025_paper.html": {
    "title": "VideoLLaMB: Long Streaming Video Understanding with Recurrent Memory Bridges",
    "volume": "main",
    "abstract": "Recent advancements in large-scale video-language models have shown significant potential for real-time planning and detailed interactions. However, their high computational demands and the scarcity of annotated datasets limit their practicality for academic researchers. In this work, we introduce VideoLLaMB, a novel and efficient framework for long video understanding that leverages recurrent memory bridges and temporal memory tokens to enable seamless encoding of entire video sequences with preserved semantic continuity. Central to our approach is a SceneTiling algorithm that segments videos into coherent semantic units, facilitating robust understanding across tasks without requiring additional training. VideoLLaMB achieves state-of-the-art performance, surpassing existing models by 4.2 points on four VideoQA benchmarks and by 2.06 points on egocentric planning tasks. Notably, it maintains strong performance under extreme video length scaling (up to 8x) and excels at fine-grained frame retrieval on our proposed Needle in a Video Haystack (NIAVH) benchmark. With linear GPU memory scaling, VideoLLaMB processes up to 320 frames using a single Nvidia A100 GPU, despite being trained on only 16 frames--offering an unprecedented balance of accuracy, scalability, and cost-effectiveness. This makes it highly accessible and practical for the academic community",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Wang",
      "Yiqi Song",
      "Cihang Xie",
      "Yang Liu",
      "Zilong Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xu_Automated_Red_Teaming_for_Text-to-Image_Models_through_Feedback-Guided_Prompt_Iteration_ICCV_2025_paper.html": {
    "title": "Automated Red Teaming for Text-to-Image Models through Feedback-Guided Prompt Iteration with Vision-Language Models",
    "volume": "main",
    "abstract": "Text-to-image models have achieved remarkable progress in generating high-quality images from textual prompts, yet their potential for misuse like generating unsafe content remains a critical concern. Existing safety mechanisms, such as filtering and fine-tuning, remain insufficient in preventing vulnerabilities exposed by adversarial prompts. To systematically evaluate these weaknesses, we propose an automated red-teaming framework, Feedback-Guided Prompt Iteration (FGPI), which utilizes a Vision-Language Model (VLM) as the red-teaming agent following a feedback-guide-rewrite paradigm for iterative prompt optimization. The red-teaming VLM analyzes prompt-image pairs based on evaluation results, provides feedback and modification strategies to enhance adversarial effectiveness while preserving safety constraints, and iteratively improves prompts. To enable this functionality, we construct a multi-turn conversational VQA dataset with over 6,000 instances, covering seven attack types and facilitating the fine-tuning of the red-teaming VLM. Extensive experiments demonstrate the effectiveness of our approach, achieving over 90% attack success rate within five iterations while maintaining prompt stealthiness and safety. The experiments also validate the adaptability, diversity, transferability, and explainability of FGPI. The source code and dataset are available at https://github.com/Weiww-Xu/FGPI",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Xu",
      "Kangjie Chen",
      "Jiawei Qiu",
      "Yuyang Zhang",
      "Run Wang",
      "Jin Mao",
      "Tianwei Zhang",
      "Lina Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_CoA-VLA_Improving_Vision-Language-Action_Models_via_Visual-Text_Chain-of-Affordance_ICCV_2025_paper.html": {
    "title": "CoA-VLA: Improving Vision-Language-Action Models via Visual-Text Chain-of-Affordance",
    "volume": "main",
    "abstract": "Robot foundation models, particularly Vision-Language-Action (VLA) models, have garnered significant attention for their ability to enhance robot policy learning, greatly improving robot's generalization and robustness. OpenAI's recent model, O1, showcased impressive capabilities in solving complex problems by utilizing extensive reasoning chains. This prompts an important question: can robot models achieve better performance in multi-task, complex environments by reviewing prior observations and then providing task-specific reasoning to guide action prediction?In this paper, we introduce Chain-of-Affordance (CoA-VLA), a novel approach to scaling robot models by incorporating reasoning in the format of sequential robot affordances to facilitate task completion. Specifically, we prompt the model to consider the following four types of affordances before taking action: (1) object affordance-- what object to manipulate and where it is; (2) grasp affordance -- the specific object part to grasp; (3) spatial affordance -- the optimal space to place the object; and (4) movement affordance-- the collision-free path for movement. We further transform each affordance into two prompting formats: visual affordance and textual affordance. We introduce a novel vision-language co-injection module that integrates this knowledge into the policy network. This allows the robot to leverage essential contextual information during action inference, resulting in improved precision and robustness. Our experiments demonstrate that CoA-VLA outperforms state-of-the-art robot foundation models, including OpenVLA and Octo, on a variety of tasks. Furthermore, CoA-VLA exhibits strong generalization capabilities, including recognizing unseen object poses, identifying free space, and avoiding obstacles in novel environments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinming Li",
      "Yichen Zhu",
      "Zhibin Tang",
      "Junjie Wen",
      "Minjie Zhu",
      "Xiaoyu Liu",
      "Chengmeng Li",
      "Ran Cheng",
      "Yaxin Peng",
      "Yan Peng",
      "Feifei Feng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Peirone_HiERO_Understanding_the_Hierarchy_of_Human_Behavior_Enhances_Reasoning_on_ICCV_2025_paper.html": {
    "title": "HiERO: Understanding the Hierarchy of Human Behavior Enhances Reasoning on Egocentric Videos",
    "volume": "main",
    "abstract": "Human activities are particularly complex and variable, and this makes challenging for deep learning models to reason about them. However, we note that such variability does have an underlying structure, composed of a hierarchy of patterns of related actions. We argue that such structure can emerge naturally from unscripted videos of human activities, and can be leveraged to better reason about their content. We present HiERO, a weakly-supervised method to enrich video segments features with the corresponding hierarchical activity threads. By aligning video clips with their narrated descriptions, HiERO infers contextual, semantic and temporal reasoning with an hierarchical architecture. We prove the potential of our enriched features with multiple video-text alignment benchmarks (EgoMCQ, EgoNLQ) with minimal additional training, and in zero-shot for procedure learning tasks (EgoProceL and Ego4D Goal-Step). Notably, HiERO achieves state-of-the-art performance in all the benchmarks, and for procedure learning tasks it outperforms fully-supervised methods by a large margin (+12.5% F1 on EgoProceL) in zero shot. Our results prove the relevance of using knowledge of the hierarchy of human activities for multiple reasoning tasks in egocentric vision",
    "checked": true,
    "id": "b9fc84be2669b4886e6af6a17c84ab52cb6ecf8e",
    "semantic_title": "hiero: understanding the hierarchy of human behavior enhances reasoning on egocentric videos",
    "citation_count": 0,
    "authors": [
      "Simone Alberto Peirone",
      "Francesca Pistilli",
      "Giuseppe Averta"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Teng_FVGen_Accelerating_Novel-View_Synthesis_with_Adversarial_Video_Diffusion_Distillation_ICCV_2025_paper.html": {
    "title": "FVGen: Accelerating Novel-View Synthesis with Adversarial Video Diffusion Distillation",
    "volume": "main",
    "abstract": "Recent progress in 3D reconstruction has enabled realistic 3D models from dense image captures, yet challenges persist with sparse views, often leading to artifacts in unseen areas. Recent works leverage Video Diffusion Models (VDMs) to generate dense observations, filling the gaps when only sparse views are available for 3D reconstruction tasks. A significant limitation of these methods is their slow sampling speed when using VDMs. In this paper, we present FVGen, a novel framework that addresses this challenge by enabling fast novel view synthesis using VDMs in as few as 4 sampling steps. We propose a novel video diffusion model distillation method that distills a multi-step denoising teacher model into a few-step denoising student model using Generative Adversarial Networks (GANs) and softened reverse KL-divergence minimization. Extensive experiments on real-world datasets show that, compared to prior works, our framework generates the same number of novel views with similar (or even better) visual quality while reducing sampling time by more than 90%. FVGen significantly improves time efficiency for downstream reconstruction tasks, particularly when working with sparse input views (more than 2) where pre-trained VDMs need to be run multiple times to achieve better spatial coverage. Our code will be released upon acceptance of the paper",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenbin Teng",
      "Gonglin Chen",
      "Haiwei Chen",
      "Yajie Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Esmaeilzehi_ZFusion_Efficient_Deep_Compositional_Zero-shot_Learning_for_Blind_Image_Super-Resolution_ICCV_2025_paper.html": {
    "title": "ZFusion: Efficient Deep Compositional Zero-shot Learning for Blind Image Super-Resolution with Generative Diffusion Prior",
    "volume": "main",
    "abstract": "Deep blind image super resolution (Blind SR) schemes strive to provide high performances under various image degradation processes. Despite the significant advancement in the area of Blind SR, the performances of these methods still may not be as high as one would desire in the case of real-world degradation operations. In this paper, we develop a novel diffusion-based Blind SR method, which, by leveraging compositional zero-shot learning, is able to provide superior performances for both synthetic and real-world unknown degradation processes. Specifically, we first extract both synthetic and real-world degradation embeddings from the input visual signal in a compositional zero-shot fashion. Next, we have efficiently embedded such degradation embeddings in the architecture of our diffusion-based scheme for guiding the diffusion feature generation process. The results of extensive experiments have demonstrated the effectiveness of the proposed Blind SR method over the state-of-the-art algorithms. Our source code and pre-trained models will be publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alireza Esmaeilzehi",
      "Hossein Zaredar",
      "Yapeng Tian",
      "Laleh Seyyed-Kalantari"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Maity_Doodle_Your_Keypoints_Sketch-Based_Few-Shot_Keypoint_Detection_ICCV_2025_paper.html": {
    "title": "Doodle Your Keypoints: Sketch-Based Few-Shot Keypoint Detection",
    "volume": "main",
    "abstract": "Keypoint detection, integral to modern machine perception, faces challenges in few-shot learning, particularly when source data from the same distribution as the query is unavailable. This gap is addressed by leveraging sketches, a popular form of human expression, providing a source-free alternative. However, challenges arise in mastering cross-modal embeddings and handling user-specific sketch styles. Our proposed framework overcomes these hurdles with a prototypical setup, combined with a grid-based locator and prototypical domain adaptation. We also demonstrate success in few-shot convergence across novel keypoints and classes through extensive experiments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Subhajit Maity",
      "Ayan Kumar Bhunia",
      "Subhadeep Koley",
      "Pinaki Nath Chowdhury",
      "Aneeshan Sain",
      "Yi-Zhe Song"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Open-Vocabulary_Octree-Graph_for_3D_Scene_Understanding_ICCV_2025_paper.html": {
    "title": "Open-Vocabulary Octree-Graph for 3D Scene Understanding",
    "volume": "main",
    "abstract": "Open-vocabulary 3D scene understanding is indispensable for embodied agents. Recent works leverage pretrained vision-language models (VLMs) for object segmentation and project them to point clouds to build 3D maps. Despite progress, a point cloud is a set of unordered coordinates that requires substantial storage space and can not directly convey occupancy information or spatial relation, making existing methods inefficient for downstream tasks, e.g., path planning and complex text-based object retrieval. To address these issues, we propose Octree-Graph, a novel scene representation for open-vocabulary 3D scene understanding. Specifically, a Chronological Group-wise Segment Merging (CGSM) strategy and an Instance Feature Aggregation (IFA) algorithm are first designed to get 3D instances and corresponding semantic features. Subsequently, an adaptive-octree structure is developed that stores semantics and depicts the occupancy of an object adjustably according to its shape. Finally, the Octree-Graph is constructed where each adaptive-octree acts as a graph node, and edges describe the spatial relations among nodes. Extensive experiments on various tasks are conducted on several widely-used datasets, demonstrating the versatility and effectiveness of our method. Code is available at https://github.com/yifeisu/OV-Octree-Graph",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhigang Wang",
      "Yifei Su",
      "Chenhui Li",
      "Dong Wang",
      "Yan Huang",
      "Xuelong Li",
      "Bin Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xu_FlexGen_Flexible_Multi-View_Generation_from_Text_and_Image_Inputs_ICCV_2025_paper.html": {
    "title": "FlexGen: Flexible Multi-View Generation from Text and Image Inputs",
    "volume": "main",
    "abstract": "In this work, we introduce FlexGen, a flexible framework designed to generate controllable and consistent multi-view images, conditioned on a single-view image, or a text prompt, or both. FlexGen tackles the challenges of controllable multi-view synthesis through additional conditioning on 3D-aware text annotations. We utilize the strong reasoning capabilities of GPT-4V to generate 3D-aware text annotations. By analyzing four orthogonal views of an object arranged as tiled multi-view images, GPT-4V can produce text annotations that include 3D-aware information with spatial relationship. By integrating the control signal with proposed adaptive dual-control module, our model can generate multi-view images that correspond to the specified text. FlexGen supports multiple controllable capabilities, allowing users to modify text prompts to generate reasonable and corresponding unseen parts. Additionally, users can influence attributes such as appearance and material properties, including metallic and roughness. Extensive experiments demonstrate that our approach offers enhanced multiple controllability, marking a significant advancement over existing multi-view diffusion models. This work has substantial implications for fields requiring rapid and flexible 3D content creation, including game development, animation, and virtual reality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinli Xu",
      "Wenhang Ge",
      "Jiantao Lin",
      "Jiawei Feng",
      "Lie Xu",
      "Hanfeng Zhao",
      "Shunsi Zhang",
      "Ying-Cong Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_SummDiff_Generative_Modeling_of_Video_Summarization_with_Diffusion_ICCV_2025_paper.html": {
    "title": "SummDiff: Generative Modeling of Video Summarization with Diffusion",
    "volume": "main",
    "abstract": "Video summarization is a task of shortening a video by choosing a subset of frames while preserving its essential moments. Despite the innate subjectivity of the task, previous works have deterministically regressed to an averaged frame score over multiple raters, ignoring the inherent subjectivity of what constitutes a \"good\" summary. We propose a novel problem formulation by framing video summarization as a conditional generation task, allowing a model to learn the distribution of good summaries and to generate multiple plausible summaries that better reflect varying human perspectives. Adopting diffusion models for the first time in video summarization, our proposed method, SummDiff, dynamically adapts to visual contexts and generates multiple candidate summaries conditioned on the input video. Extensive experiments demonstrate that SummDiff not only achieves the state-of-the-art performance on various benchmarks but also produces summaries that closely align with individual annotator preferences. Moreover, we provide a deeper insight with novel metrics from an analysis of the knapsack, which is an important last step of generating summaries but has been overlooked in evaluation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kwanseok Kim",
      "Jaehoon Hahm",
      "Sumin Kim",
      "Jinhwan Sul",
      "Byunghak Kim",
      "Joonseok Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_FlowDPS__Flow-Driven_Posterior_Sampling_for_Inverse_Problems_ICCV_2025_paper.html": {
    "title": "FlowDPS : Flow-Driven Posterior Sampling for Inverse Problems",
    "volume": "main",
    "abstract": "Flow matching is a recent state-of-the-art framework for generative modeling based on ordinary differential equations (ODEs). While closely related to diffusion models, it provides a more general perspective on generative modeling.Although inverse problem solving has been extensively explored using diffusion models, it has not been rigorously examined within the broader context of flow models. Therefore, here we extend the diffusion inverse solvers (DIS)-- which perform posterior sampling by combining a denoising diffusion prior with an likelihood gradient--into the flow framework. Specifically, by driving the flow-version of Tweedie's formula, we decompose the flow ODE into two components: one for clean image estimation and the other for noise estimation.By integrating the likelihood gradient and stochastic noise into each component, respectively, we demonstrate that posterior sampling for inverse problem solving can be effectively achieved using flows. Our proposed solver, Flow-Driven Posterior Sampling (FlowDPS), can also be seamlessly integrated into a latent flow model with a transformer architecture. Across four linear inverse problems, we confirm that FlowDPS outperforms state-of-the-art alternatives, all without requiring additional training",
    "checked": false,
    "id": "067e74328cf6327bd6937d6fcf13ad21ef683897",
    "semantic_title": "flowdps: flow-driven posterior sampling for inverse problems",
    "citation_count": 6,
    "authors": [
      "Jeongsol Kim",
      "Bryan Sangwoo Kim",
      "Jong Chul Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tran_Head2Body_Body_Pose_Generation_from_Multi-sensory_Head-mounted_Inputs_ICCV_2025_paper.html": {
    "title": "Head2Body: Body Pose Generation from Multi-sensory Head-mounted Inputs",
    "volume": "main",
    "abstract": "Generating body pose from head-mounted, egocentric inputs is essential for immersive VR/AR and assistive technologies, as it supports more natural interactions. However, the task is challenging due to limited visibility of body parts in first-person views and the sparseness of sensory data, with only a single device placed on the head. To address these challenges, we introduce Head2Body, a novel framework for body pose estimation that effectively combines IMU and visual data. First, we introduce a pre-trained IMU encoder, trained on over 1,700 hours of head-IMU data from wearable eyeglasses, to better capture detailed temporal motion cues given limited labeled egocentric pose data. For visual processing, we leverage large vision-language models (LVLMs) to segment body parts that appear sporadically in video frames to improve visual feature extraction. To better guide the pose generation process with sparse signals from only head-mounted devices, we incorporates a Vector Quantized Variational Autoencoder (VQ-VAE) to represent poses as discrete tokens, which capture high-frequency motion patterns and provide a more structured representation of body pose. Our experiments demonstrate the effectiveness of the proposed approach, yielding 6-13% gains over state-of-the-art baselines on four datasets: AMASS, KinPoly, GIMO, and EgoExo4D. By capturing subtle temporal dynamics and leveraging complementary sensory data, our approach advances accurate egocentric body pose estimation and sets a new benchmark for multi-modal, first-person motion tracking",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minh Tran",
      "Hongda Mao",
      "Qingshuang Chen",
      "Yelin Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tang_Closed-Loop_Transfer_for_Weakly-supervised_Affordance_Grounding_ICCV_2025_paper.html": {
    "title": "Closed-Loop Transfer for Weakly-supervised Affordance Grounding",
    "volume": "main",
    "abstract": "Humans can perform previously unexperienced interactions with novel objects simply by observing others engage with them. Weakly-supervised affordance grounding mimics this process by learning to locate object regions that enable actions on egocentric images, using exocentric interaction images with image-level annotations. However, extracting affordance knowledge solely from exocentric images and transferring it one-way to egocentric images limits the applicability of previous works in complex interaction scenarios. Instead, this study introduces LoopTrans, a novel closed-loop framework that not only transfers knowledge from exocentric to egocentric but also transfers back to enhance exocentric knowledge extraction. Within LoopTrans, several innovative mechanisms are introduced, including unified cross-modal localization and denoising knowledge distillation, to bridge domain gaps between object-centered egocentric and interaction-centered exocentric images while enhancing knowledge transfer. Experiments show that LoopTrans achieves consistent improvements across all metrics on image and video benchmarks, even handling challenging scenarios where object interaction regions are fully occluded by the human body",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiajin Tang",
      "Zhengxuan Wei",
      "Ge Zheng",
      "Sibei Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tan_OminiControl_Minimal_and_Universal_Control_for_Diffusion_Transformer_ICCV_2025_paper.html": {
    "title": "OminiControl: Minimal and Universal Control for Diffusion Transformer",
    "volume": "main",
    "abstract": "We present OminiControl, a novel approach that rethinks how image conditions are integrated into Diffusion Transformer (DiT) architectures. Current image conditioning methods either introduce substantial parameter overhead or handle only specific control tasks effectively, limiting their practical versatility. OminiControl addresses these limitations through three key innovations: (1) a minimal architectural design that leverages the DiT's own VAE encoder and transformer blocks, requiring just 0.1% additional parameters; (2) a unified sequence processing strategy that combines condition tokens with image tokens for flexible token interactions; and (3) a dynamic position encoding mechanism that adapts to both spatially-aligned and non-aligned control tasks. Our extensive experiments show that this streamlined approach not only matches but surpasses the performance of specialized methods across multiple conditioning tasks. To overcome data limitations in subject-driven generation, we also introduce Subjects200K, a large-scale dataset of identity-consistent image pairs synthesized using DiT models themselves. This work demonstrates that effective image control can be achieved without architectural complexity, opening new possibilities for efficient and versatile image generation systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenxiong Tan",
      "Songhua Liu",
      "Xingyi Yang",
      "Qiaochu Xue",
      "Xinchao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yu_Zeroth-Order_Fine-Tuning_of_LLMs_in_Random_Subspaces_ICCV_2025_paper.html": {
    "title": "Zeroth-Order Fine-Tuning of LLMs in Random Subspaces",
    "volume": "main",
    "abstract": "Fine-tuning Large Language Models (LLMs) has proven effective for a variety of downstream tasks. However, as LLMs grow in size, the memory demands for backpropagation become increasingly prohibitive. Zeroth-order (ZO) optimization methods offer a memory-efficient alternative by using forward passes to estimate gradients, but the variance of gradient estimates typically scales linearly with the model's parameter dimension--a significant issue for LLMs. In this paper, we propose the random Subspace Zeroth-order (SubZero) optimization to address the challenges posed by LLMs' high dimensionality. We introduce a low-rank perturbation tailored for LLMs that significantly reduces memory consumption while improving performance. Additionally, we prove that our gradient estimation closely approximates the backpropagation gradient, exhibits lower variance than traditional ZO methods, and ensures convergence when combined with SGD. Experimental results show that SubZero enhances fine-tuning performance and achieves faster convergence compared to standard ZO approaches like MeZO across various language modeling tasks. Code is available at https://github.com/zimingyy/SubZero",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziming Yu",
      "Pan Zhou",
      "Sike Wang",
      "Jia Li",
      "Mi Tian",
      "Hua Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Rakib_G2D_Boosting_Multimodal_Learning_with_Gradient-Guided_Distillation_ICCV_2025_paper.html": {
    "title": "G2D: Boosting Multimodal Learning with Gradient-Guided Distillation",
    "volume": "main",
    "abstract": "Multimodal learning aims to leverage information from diverse data modalities to achieve more comprehensive performance. However, conventional multimodal models often suffer from modality imbalance, where one or a few modalities dominate model optimization, leading to suboptimal feature representation and underutilization of weak modalities. To address this challenge, we introduce Gradient-Guided Distillation (G^ 2 D), a knowledge distillation framework that optimizes the multimodal model with a custom-built loss function that fuses both unimodal and multimodal objectives. G^ 2 D further incorporates a dynamic sequential modality prioritization (SMP) technique in the learning process to ensure each modality leads the learning process, avoiding the pitfall of stronger modalities overshadowing weaker ones. We validate G^ 2 D on multiple real-world datasets and show that G^ 2 D amplifies the significance of weak modalities while training and outperforms state-of-the-art methods in classification and regression tasks. Our code is available \\href https://github.com/rAIson-Lab/G2D here",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammed Rakib",
      "Arunkumar Bagavathi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_AIComposer_Any_Style_and_Content_Image_Composition_via_Feature_Integration_ICCV_2025_paper.html": {
    "title": "AIComposer: Any Style and Content Image Composition via Feature Integration",
    "volume": "main",
    "abstract": "Image composition has advanced significantly with large-scale pre-trained T2I diffusion models. Despite progress in same-domain composition, cross-domain composition remains under-explored. The main challenges are the stochastic nature of diffusion models and the style gap between input images, leading to failures and artifacts. Additionally, heavy reliance on text prompts limits practical applications. This paper presents the first cross-domain image composition method that does not require text prompts, allowing natural stylization and seamless compositions. Our method is efficient and robust, preserving the diffusion prior, as it involves minor steps for backward inversion and forward denoising without training the diffuser. Our method also uses a simple multilayer perceptron network to integrate CLIP features from foreground and background, manipulating diffusion with a local cross-attention strategy. It effectively preserves foreground content while enabling stable stylization without a pre-stylization network. Finally, we create a benchmark dataset with diverse contents and styles for fair evaluation, addressing the lack of testing datasets for cross-domain image composition. Our method outperforms state-of-the-art techniques in both qualitative and quantitative evaluations, significantly improving the LPIPS score by 30.5% and the CSD metric by 18.1%. We believe our method will advance future research and applications. Code and benchmark at https://github.com/sherlhw/AIComposer",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haowen Li",
      "Zhenfeng Fan",
      "Zhang Wen",
      "Zhengzhou Zhu",
      "Yunjin Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Do_PAN-Crafter_Learning_Modality-Consistent_Alignment_for_PAN-Sharpening_ICCV_2025_paper.html": {
    "title": "PAN-Crafter: Learning Modality-Consistent Alignment for PAN-Sharpening",
    "volume": "main",
    "abstract": "PAN-sharpening aims to fuse high-resolution panchromatic (PAN) images with low-resolution multi-spectral (MS) images to generate high-resolution multi-spectral (HRMS) outputs. However, cross-modality misalignment---caused by sensor placement, acquisition timing, and resolution disparity---induces a fundamental challenge. Conventional deep learning methods assume perfect pixel-wise alignment and rely on per-pixel reconstruction losses, leading to spectral distortion, double edges, and blurring when misalignment is present. To address this, we propose PAN-Crafter, a modality-consistent alignment framework that explicitly mitigates the misalignment gap between PAN and MS modalities. At its core, Modality-Adaptive Reconstruction (MARs) enables a single network to jointly reconstruct HRMS and PAN images, leveraging PAN's high-frequency details as auxiliary self-supervision. Additionally, we introduce Cross-Modality Alignment-Aware Attention (CM3A), a novel mechanism that bidirectionally aligns MS texture to PAN structure and vice versa, enabling adaptive feature refinement across modalities. Extensive experiments on multiple benchmark datasets demonstrate that our PAN-Crafter outperforms the most recent state-of-the-art method in all metrics, even with 50.11xfaster inference time and 0.63xthe memory size. Furthermore, it demonstrates strong generalization performance on unseen satellite datasets, showing its robustness across different conditions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeonghyeok Do",
      "Sungpyo Kim",
      "Geunhyuk Youk",
      "Jaehyup Lee",
      "Munchurl Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Nam_M2SFormer_Multi-Spectral_and_Multi-Scale_Attention_with_Edge-Aware_Difficulty_Guidance_for_ICCV_2025_paper.html": {
    "title": "M2SFormer: Multi-Spectral and Multi-Scale Attention with Edge-Aware Difficulty Guidance for Image Forgery Localization",
    "volume": "main",
    "abstract": "Image editing techniques have rapidly advanced, facilitating both innovative use cases and malicious manipulation of digital images. Deep learning-based methods have recently achieved high accuracy in pixel-level forgery localization, yet they frequently struggle with computational overhead and limited representation power, particularly for subtle or complex tampering. In this paper, we propose M2SFormer, a novel Transformer encoder-based framework designed to overcome these challenges. Unlike approaches that process spatial and frequency cues separately, M2SFormer unifies multi-frequency and multi-scale attentions in the skip connection, harnessing global context to better capture diverse forgery artifacts. Additionally, our framework addresses the loss of fine detail during upsampling by utilizing a global prior map--a curvature metric indicating the difficulty of forgery localization--which then guides a difficulty-guided attention module to preserve subtle manipulations more effectively. Extensive experiments on multiple benchmark datasets demonstrate that M2SFormer outperforms existing state-of-the-art models, offering superior generalization in detecting and localizing forgeries across unseen domains. Our M2SFormer code is available in Github Link",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ju-Hyeon Nam",
      "Dong-Hyun Moon",
      "Sang-Chul Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lu_Pinco_Position-induced_Consistent_Adapter_for_Diffusion_Transformer_in_Foreground-conditioned_Inpainting_ICCV_2025_paper.html": {
    "title": "Pinco: Position-induced Consistent Adapter for Diffusion Transformer in Foreground-conditioned Inpainting",
    "volume": "main",
    "abstract": "Foreground-conditioned inpainting aims to seamlessly fill the background region of an image by utilizing the provided foreground subject and a text description. While existing T2I-based image inpainting methods can be applied to this task, they suffer from issues of subject shape expansion, distortion, or impaired ability to align with the text description, resulting in inconsistencies between the visual elements and the text description. To address these challenges, we propose Pinco, a plug-and-play foreground-conditioned inpainting adapter that generates high-quality backgrounds with good text alignment while effectively preserving the shape of the foreground subject. Firstly, we design a Self-Consistent Adapter that integrates the foreground subject features into the layout-related self-attention layer, which helps to alleviate conflicts between the text and subject features by ensuring that the model can effectively consider the foreground subject's characteristics while processing the overall image layout. Secondly, we design a Decoupled Image Feature Extraction method that employs distinct architectures to extract semantic and spatial features separately, significantly improving subject feature extraction and ensuring high-quality preservation of the subject's shape. Thirdly, to ensure precise utilization of the extracted features and to focus attention on the subject region, we introduce a Shared Positional Embedding Anchor, greatly improving the model's understanding of subject features and boosting training efficiency. Extensive experiments demonstrate that our method achieves superior performance and efficiency in foreground-conditioned inpainting",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangben Lu",
      "Yuzhen Du",
      "Yizhe Tang",
      "Zhimin Sun",
      "Ran Yi",
      "Yifan Qi",
      "Tianyi Wang",
      "Lizhuang Ma",
      "Fangyuan Zou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhao_ReconDreamer_Harmonizing_Generative_and_Reconstructive_Models_for_Driving_Scene_Representation_ICCV_2025_paper.html": {
    "title": "ReconDreamer++: Harmonizing Generative and Reconstructive Models for Driving Scene Representation",
    "volume": "main",
    "abstract": "Combining reconstruction models with generative models has emerged as a promising paradigm for closed-loop simulation in autonomous driving. For example, ReconDreamer has demonstrated remarkable success in rendering large-scale maneuvers. However, a significant gap remains between the generated data and real-world sensor observations, particularly in terms of fidelity for structured elements, such as the ground surface. To address these challenges, we propose ReconDreamer++, an enhanced framework that significantly improves the overall rendering quality by mitigating the domain gap and refining the representation of the ground surface.Specifically, ReconDreamer++ introduces the Novel Trajectory Deformable Network (NTDNet), which leverages learnable spatial deformation mechanisms to bridge the domain gap between synthesized novel views and original sensor observations. Moreover, for structured elements such as the ground surface, we preserve geometric prior knowledge in 3D Gaussians, andthe optimization process focuses on refining appearance attributes while preserving the underlying geometric structure. Experimental evaluations conducted on multiple datasets (Waymo, nuScenes, PandaSet, and EUVS) confirm the superior performance of ReconDreamer++. Specifically, on Waymo, ReconDreamer++ achieves performance comparable to Street Gaussians for the original trajectory while significantly outperforming ReconDreamer on novel trajectories. In particular, it achieves substantial improvements, including a 6.1% increase in NTA-IoU, a 23. 0% improvement in FID, and a remarkable 4.5% gain in the ground surface metric NTL-IoU, highlighting its effectiveness in accurately reconstructing structured elements such as the road surface",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guosheng Zhao",
      "Xiaofeng Wang",
      "Chaojun Ni",
      "Zheng Zhu",
      "Wenkang Qin",
      "Guan Huang",
      "Xingang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/He_SyncDiff_Synchronized_Motion_Diffusion_for_Multi-Body_Human-Object_Interaction_Synthesis_ICCV_2025_paper.html": {
    "title": "SyncDiff: Synchronized Motion Diffusion for Multi-Body Human-Object Interaction Synthesis",
    "volume": "main",
    "abstract": "Synthesizing realistic human-object interaction motions is a critical problem in VR/AR and human animation. Unlike the commonly studied scenarios involving a single human or hand interacting with one object, we address a more generic multi-body setting with arbitrary numbers of humans, hands, and objects. The high correlations and mutual influences among bodies leads to two major challenges, for which we propose solutions. First, to satisfy the high demands for synchronization of different body motions, we mathematically derive a new set of alignment scores during the training process, and use maximum likelihood sampling on a dynamic graphical model for explicit synchronization during inference. Second, the high-frequency interactions between objects are often overshadowed by the large-scale low-frequency movements. To address this, we introduce frequency decomposition and explicitly represent high-frequency components in the frequency domain. Extensive experiments across five datasets with various multi-body configurations demonstrate the superiority of SyncDiff over existing state-of-the-art motion synthesis methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenkun He",
      "Yun Liu",
      "Ruitao Liu",
      "Li Yi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kravets_Rethinking_Few_Shot_CLIP_Benchmarks_A_Critical_Analysis_in_the_ICCV_2025_paper.html": {
    "title": "Rethinking Few Shot CLIP Benchmarks: A Critical Analysis in the Inductive Setting",
    "volume": "main",
    "abstract": "CLIP is a foundational model with transferable classification performance in the few-shot setting. Several methods have shown improved performance of CLIP using few-shot examples. However, so far all these techniques have been benchmarked using standard few-shot datasets. We argue that this mode of evaluation does not provide a true indication of the inductive generalization ability using few-shot examples. As most datasets have been seen by the CLIP model, the resultant setting can be termed as partially transductive. To solve this, we propose a pipeline that uses an unlearning technique to obtain true inductive baselines. In this new inductive setting, methods show a significant drop in performance (-55% on average among 13 baselines with multiple datasets). We validate the unlearning technique using oracle baselines. An improved few-shot classification technique is proposed that consistently obtains state-of-the-art performance over 13 other recent baseline methods on a comprehensive analysis with 5880 experiments - varying the datasets, differing number of few-shot examples, unlearning setting, and with different seeds. Thus, we identify the issue with the evaluation of CLIP-based few-shot classification, provide a solution using unlearning, propose new benchmarks, and provide an improved method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexey Kravets",
      "Da Chen",
      "Vinay P. Namboodiri"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Mind_the_Gap_Aligning_Vision_Foundation_Models_to_Image_Feature_ICCV_2025_paper.html": {
    "title": "Mind the Gap: Aligning Vision Foundation Models to Image Feature Matching",
    "volume": "main",
    "abstract": "Leveraging the vision foundation models has emerged as a mainstream paradigm that improves the performance of image feature matching. However, previous works have ignored the misalignment when introducing the foundation models into feature matching. The misalignment arises from the discrepancy between the foundation models focusing on single-image understanding and the cross-image understanding requirement of feature matching. Specifically, 1) the embeddings derived from commonly used foundation models exhibit discrepancies with the optimal embeddings required for feature matching; 2) lacking an effective mechanism to leverage the single-image understanding ability into cross-image understanding. A significant consequence of the misalignment is they struggle when addressing multi-instance feature matching problems. To address this, we introduce a simple but effective framework, called IMD (Image feature Matching with a pre-trained Diffusion model) with two parts: 1) Unlike the dominant solutions employing contrastive-learning based foundation models that emphasize global semantics, we integrate the generative-based diffusion models to effectively capture instance-level details. 2) We leverage the prompt mechanism in generative model as a natural tunnel, propose a novel cross-image interaction prompting module to facilitate bidirectional information interaction between image pairs. To more accurately measure the misalignment, we propose a new benchmark called IMIM, which focuses on multi-instance scenarios. Our proposed IMD establishes a new state-of-the-art in commonly evaluated benchmarks, and the superior improvement 12% in IMIM indicates our method efficiently mitigates the misalignment",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhan Liu",
      "Jingwen Fu",
      "Yang Wu",
      "Kangyi Wu",
      "Pengna Li",
      "Jiayi Wu",
      "Sanping Zhou",
      "Jingmin Xin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_CoStoDet-DDPM_Collaborative_Training_of_Stochastic_and_Deterministic_Models_Improves_Surgical_ICCV_2025_paper.html": {
    "title": "CoStoDet-DDPM: Collaborative Training of Stochastic and Deterministic Models Improves Surgical Workflow Anticipation and Recognition",
    "volume": "main",
    "abstract": "Anticipating and recognizing surgical workflows are critical for intelligent surgical assistance systems. However, existing methods rely on deterministic decision-making, struggling to generalize across the large anatomical and procedural variations inherent in real-world surgeries. In this paper, we introduce an innovative framework that incorporates stochastic modeling through a denoising diffusion probabilistic model (DDPM) into conventional deterministic learning for surgical workflow analysis. At the heart of our approach is a collaborative co-training paradigm: the DDPM branch captures procedural uncertainties to enrich feature representations, while the task branch focuses on predicting surgical phases and instrument usage. Theoretically, we demonstrate that this mutual refinement mechanism benefits both branches: the DDPM reduces prediction errors in uncertain scenarios, and the task branch directs the DDPM toward clinically meaningful representations. Notably, the DDPM branch is discarded during inference, enabling real-time predictions without sacrificing accuracy. Experiments on the Cholec80 dataset show that for the anticipation task, our method achieves a 16% reduction in eMAE compared to state-of-the-art approaches, and for phase recognition, it improves the Jaccard score by 1.0%. Additionally, on the AutoLaparo dataset, our method achieves a 1.5% improvement in the Jaccard score for phase recognition, while also exhibiting robust generalization to patient-specific variations. Our code and weight will be available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaixiang Yang",
      "Xin Li",
      "Qiang Li",
      "Zhiwei Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jiao_GSOT3D_Towards_Generic_3D_Single_Object_Tracking_in_the_Wild_ICCV_2025_paper.html": {
    "title": "GSOT3D: Towards Generic 3D Single Object Tracking in the Wild",
    "volume": "main",
    "abstract": "In this paper, we present a novel benchmark, GSOT3D, that aims at facilitating development of generic 3D single object tracking (SOT) in the wild. Specifically, GSOT3D offers 620 sequences with 123K frames, and covers a wide selection of 54 object categories. Each sequence is offered with multiple modalities, including the point cloud (PC), RGB image, and depth. This allows GSOT3D to support various 3D tracking tasks, such as single-modal 3D SOT on PC and multi-modal 3D SOT on RGB-PC or RGB-D, and thus greatly broadens research directions for 3D object tracking. To provide high-quality per-frame 3D annotations, all sequences are labeled manually with multiple rounds of meticulous inspection and refinement. To our best knowledge, GSOT3D is the largest benchmark dedicated to various generic 3D object tracking tasks. To understand how existing 3D trackers perform and to provide comparisons for future research on GSOT3D, we assess eight representative point cloud-based tracking models. Our evaluation results exhibit that these models heavily degrade on GSOT3D, and more efforts are required for robust and generic 3D object tracking. Besides, to encourage future research, we present a simple yet effective generic 3D tracker, named PROT3D, that localizes the target object via a progressive spatial-temporal network and outperforms all current solutions by a large margin. By releasing GSOT3D, we expect to advance further 3D tracking in future research and applications. Our benchmark and model as well as the evaluation toolkit and results are publicly available at https://github.com/ailovejinx/GSOT3D",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Jiao",
      "Yunhao Li",
      "Junhua Ding",
      "Qing Yang",
      "Song Fu",
      "Heng Fan",
      "Libo Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_UnZipLoRA_Separating_Content_and_Style_from_a_Single_Image_ICCV_2025_paper.html": {
    "title": "UnZipLoRA: Separating Content and Style from a Single Image",
    "volume": "main",
    "abstract": "This paper introduces UnZipLoRA, a method for decomposing an image into its constituent subject and style, represented as two distinct LoRAs (Low-Rank Adaptations). Unlike existing personalization techniques that focus on either subject or style in isolation, or require separate training sets for each, UnZipLoRA disentangles these elements from a single image by training both the LoRAs simultaneously. UnZipLoRA ensures that the resulting LoRAs are compatible, i.e., they can be seamlessly combined using direct addition. UnZipLoRA enables independent manipulation and recontextualization of subject and style, including generating variations of each, applying the extracted style to new subjects, and recombining them to reconstruct the original image or create novel variations. To address the challenge of subject and style entanglement, UnZipLoRA employs a novel prompt separation technique, as well as column and block separation strategies to accurately preserve the characteristics of subject and style, and ensure compatibility between the learned LoRAs. Evaluation with human studies and quantitative metrics demonstrates UnZipLoRA's effectiveness compared to other state-of-the-art methods, including DreamBooth-LoRA, Inspiration Tree, and B-LoRA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chang Liu",
      "Viraj Shah",
      "Aiyu Cui",
      "Svetlana Lazebnik"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tan_What_You_Have_is_What_You_Track_Adaptive_and_Robust_ICCV_2025_paper.html": {
    "title": "What You Have is What You Track: Adaptive and Robust Multimodal Tracking",
    "volume": "main",
    "abstract": "Multimodal data is known to be helpful for visual tracking by improving robustness to appearance variations. However, sensor synchronization challenges often compromise data availability, particularly in video settings where shortages can be temporal. Despite its importance, this area remains underexplored. In this paper, we present the first comprehensive study on tracker performance with temporally incomplete multimodal data. Unsurprisingly, under such a circumstance, existing trackers exhibit significant performance degradation, as their rigid architectures lack the adaptability needed to effectively handle missing modalities.To address these limitations, we propose a flexible framework for robust multimodal tracking. We venture that a tracker should dynamically activate computational units based on missing data rates. This is achieved through a novel Heterogeneous Mixture-of-Experts fusion mechanism with adaptive complexity, coupled with a video-level masking strategy that ensures both temporal consistency and spatial completeness -- critical for effective video tracking. Surprisingly, our model not only adapts to varying missing rates but also adjusts to scene complexity. Extensive experiments show that our model achieves SOTA performance across 9 benchmarks, excelling in both conventional complete and missing modality settings. The code and benchmark will be made publicly available",
    "checked": true,
    "id": "714fa2992d2dee25baedaa8292579ae2dfbe5c90",
    "semantic_title": "what you have is what you track: adaptive and robust multimodal tracking",
    "citation_count": 0,
    "authors": [
      "Yuedong Tan",
      "Jiawei Shao",
      "Eduard Zamfir",
      "Ruanjun Li",
      "Zhaochong An",
      "Chao Ma",
      "Danda Paudel",
      "Luc Van Gool",
      "Radu Timofte",
      "Zongwei Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/He_RareCLIP_Rarity-aware_Online_Zero-shot_Industrial_Anomaly_Detection_ICCV_2025_paper.html": {
    "title": "RareCLIP: Rarity-aware Online Zero-shot Industrial Anomaly Detection",
    "volume": "main",
    "abstract": "Large vision-language models such as CLIP have made significant strides in zero-shot anomaly detection through prompt engineering. However, most existing methods typically process each test image individually, ignoring the practical rarity of abnormal patches in real-world scenarios. Although some batch-based approaches exploit the rarity by processing multiple samples concurrently, they generally introduce unacceptable latency for real-time applications. To mitigate these limitations, we propose RareCLIP, a novel online zero-shot anomaly detection framework that enables sequential image processing in real-time without requiring prior knowledge of the target domain. RareCLIP capitalizes on the zero-shot capabilities of CLIP and integrates a dynamic test-time rarity estimation mechanism. A key innovation of our framework is the introduction of a prototype patch feature memory bank, which aggregates representative features from historical observations and continuously updates their corresponding rarity measures. For each incoming image patch, RareCLIP computes a rarity score by aggregating the rarity measures of its nearest neighbors within the memory bank. Moreover, we introduce a prototype sampling strategy based on dissimilarity to enhance computational efficiency, as well as a similarity calibration strategy to enhance the robustness of rarity estimation. Extensive experiments demonstrate that RareCLIP attains state-of-the-art performance with 98.2% image-level AUROC on MVTec AD and 94.4% on VisA, while achieving a latency of 59.4 ms. Code is available at https://github.com/hjf02/RareCLIP",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianfang He",
      "Min Cao",
      "Silong Peng",
      "Qiong Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Bi_AdaDCP_Learning_an_Adapter_with_Discrete_Cosine_Prior_for_Clear-to-Adverse_ICCV_2025_paper.html": {
    "title": "AdaDCP: Learning an Adapter with Discrete Cosine Prior for Clear-to-Adverse Domain Generalization",
    "volume": "main",
    "abstract": "Vision Foundation Model (VFM) provides an inherent generalization ability to unseen domains for downstream tasks. However, fine-tuning VFM to parse various adverse scenes (e.g., fog, snow, night) is particularly challenging, as these samples are difficult to collect. Using easy-to-acquire clear scenes as the source domain is a feasible solution, but a huge domain gap exists between clear and adverse scenes due to their dramatically different appearances. To address this challenge, this paper proposes AdaDCP, a VFM adapter with discrete cosine prior. The innovation originates from the observation that, the frequency components from a VFM exhibit either variant or invariant properties on adverse weather conditions after discrete cosine transform. Technically, the weather-invariant property learning preceives most of the scene content that is invariant to the adverse condition. The weather-variant property learning, in contrast, perceives the weather-specific information from different types of adverse conditions. Finally, the weather-invariant property alignment implicitly enforces the weather-variant components to incorporate the weather-invariant information, therefore mitigating the clear-to-adverse domain gap. Experiments conducted on eight unseen adverse scene segmentation datasets show its state-of-the-art performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Bi",
      "Yixian Shen",
      "Jingjun Yi",
      "Gui-Song Xia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_HERMES_A_Unified_Self-Driving_World_Model_for_Simultaneous_3D_Scene_ICCV_2025_paper.html": {
    "title": "HERMES: A Unified Self-Driving World Model for Simultaneous 3D Scene Understanding and Generation",
    "volume": "main",
    "abstract": "Driving World Models (DWMs) have become essential for autonomous driving by enabling future scene prediction. However, existing DWMs are limited to scene generation and fail to incorporate scene understanding, which involves interpreting and reasoning about the driving environment. In this paper, we present a unified Driving World Model named HERMES. We seamlessly integrate 3D scene understanding and future scene evolution (generation) through a unified framework in driving scenarios. Specifically, HERMES leverages a Bird's-Eye View (BEV) representation to consolidate multi-view spatial information while preserving geometric relationships and interactions. We also introduce world queries, which incorporate world knowledge into BEV features via causal attention in the Large Language Model, enabling contextual enrichment for understanding and generation tasks. We conduct comprehensive studies on nuScenes and OmniDrive-nuScenes datasets to validate the effectiveness of our method. HERMES achieves state-of-the-art performance, reducing generation error by 32.4% and improving understanding metrics such as CIDEr by 8.0%. The model and code will be publicly released at https://github.com/LMD0311/HERMES",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Zhou",
      "Dingkang Liang",
      "Sifan Tu",
      "Xiwu Chen",
      "Yikang Ding",
      "Dingyuan Zhang",
      "Feiyang Tan",
      "Hengshuang Zhao",
      "Xiang Bai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Deng_ArgMatch_Adaptive_Refinement_Gathering_for_Efficient_Dense_Matching_ICCV_2025_paper.html": {
    "title": "ArgMatch: Adaptive Refinement Gathering for Efficient Dense Matching",
    "volume": "main",
    "abstract": "Establishing dense correspondences is crucial yet computationally demanding in multi-view tasks. Although coarse-to-fine schemes mitigate computational costs, their efficiency remains limited by the substantial demands of heavy feature extractors and global matchers. In this paper, we propose Adaptive Refinement Gathering, a refinement pipeline that reduces reliance on these costly components without sacrificing accuracy. The pipeline consists of (i) a content-aware offset estimator that leverages content information for lightweight correlation volume encoding and decoding; (ii) a locally consistent match rectifier robust to large global initial errors; (iii) a locally consistent upsampler that yields fewer artifacts around depth-discontinuous edges. Additionally, we introduce an adaptive gating strategy that, in conjunction with local consistency, dynamically modulates the contribution of different components and pixels. This enables adaptive gradient backpropagation and allows the network to fully exploit its capacity. Compared to the state-of-the-art, our lightweight network, termed ArgMatch, achieves competitive performance in serval tasks, while significantly reducing the computational cost. Codes are available in https://github.com/ACuOoOoO/argmatch",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxin Deng",
      "Kaining  Zhang",
      "Linfeng  Tang",
      "Jiaqi Yang",
      "Jiayi Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hu_Enhancing_Image_Restoration_Transformer_via_Adaptive_Translation_Equivariance_ICCV_2025_paper.html": {
    "title": "Enhancing Image Restoration Transformer via Adaptive Translation Equivariance",
    "volume": "main",
    "abstract": "Translation equivariance is a fundamental inductive bias in image restoration, ensuring that translated inputs produce translated outputs. Attention mechanisms in modern restoration transformers undermine this property, adversely impacting both training convergence and generalization. To alleviate this issue, we propose two key strategies for incorporating translation equivariance: slide indexing and component stacking. Slide indexing maintains operator responses at fixed positions, with sliding window attention being a notable example, while component stacking enables the arrangement of translation-equivariant operators in parallel or sequentially, thereby building complex architectures while preserving translation equivariance. However, these strategies still create a dilemma in model design between the high computational cost of self-attention and the fixed receptive field associated with sliding window attention. To address this, we develop an adaptive sliding indexing mechanism to efficiently select key-value pairs for each query, which are then concatenated in parallel with globally aggregated key-value pairs. The designed network, called the Translation Equivariance Adaptive Transformer (TEAFormer), is assessed across a variety of image restoration tasks. The results highlight its superiority in terms of effectiveness, training convergence, and generalization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "JiaKui Hu",
      "Zhengjian Yao",
      "Lujia Jin",
      "Hangzhou He",
      "Yanye Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Free4D_Tuning-free_4D_Scene_Generation_with_Spatial-Temporal_Consistency_ICCV_2025_paper.html": {
    "title": "Free4D: Tuning-free 4D Scene Generation with Spatial-Temporal Consistency",
    "volume": "main",
    "abstract": "We present Free4D, a novel tuning-free framework for 4D scene generation from a single image. Existing methods either focus on object-level generation, making scene-level generation infeasible, or rely on large-scale multi-view video datasets for expensive training, with limited generalization ability due to the scarcity of 4D scene data. In contrast, our key insight is to distill pre-trained foundation models for consistent 4D scene representation, which offers promising advantages such as efficiency and generalizability. 1) To achieve this, we first animate the input image using image-to-video diffusion models followed by 4D geometric structure initialization. 2) To turn this coarse structure into spatial-temporal consistent multi-view videos, we design an adaptive guidance mechanism with a point-guided denoising strategy for spatial consistency and a novel latent replacement strategy for temporal coherence. 3) To lift these generated observations into consistent 4D representation, we propose a modulation-based refinement to mitigate inconsistencies while fully leveraging the generated information. The resulting 4D representation enables real-time, controllable rendering, marking a significant advancement in single-image-based 4D scene generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianqi Liu",
      "Zihao Huang",
      "Zhaoxi Chen",
      "Guangcong Wang",
      "Shoukang Hu",
      "Liao Shen",
      "Huiqiang Sun",
      "Zhiguo Cao",
      "Wei Li",
      "Ziwei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Niewiadomski_Generative_Zoo_ICCV_2025_paper.html": {
    "title": "Generative Zoo",
    "volume": "main",
    "abstract": "The model-based estimation of 3D animal pose and shape from images enables computational modeling of animal behavior. Training models for this purpose requires large amounts of labeled image data with precise pose and shape annotations. However, capturing such data requires the use of multi-view or marker-based motion-capture systems, which are impractical to adapt to wild animals in situ and impossible to scale across a comprehensive set of animal species. Some have attempted to address the challenge of procuring training data by pseudo-labeling individual real-world images through manual 2D annotation, followed by 3D-parameter optimization to those labels. While this approach may produce silhouette-aligned samples, the obtained pose and shape parameters are often implausible due to the ill-posed nature of the monocular fitting problem. Sidestepping real-world ambiguity, others have designed complex synthetic-data-generation pipelines leveraging game engines and collections of artist-designed 3D assets. Such engines yield perfect ground-truth labels but are often lacking in visual realism and require considerable manual effort to adapt to new species or environments. We propose an alternative approach to synthetic-data generation: rendering with a conditional image-generation model. We introduce a pipeline that samples a diverse set of poses and shapes for a variety of mammalian quadrupeds and generates realistic images with corresponding ground-truth pose and shape parameters. To demonstrate the scalability of our approach, we introduce GenZoo, a synthetic dataset containing one million images of distinct subjects. We train a 3D pose and shape regressor on GenZoo, which achieves state-of-the-art performance on a real-world multi-species 3D animal pose and shape estimation benchmark, despite being trained solely on synthetic data. We release our data and pipeline at https://genzoo.is.tue.mpg.de",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tomasz Niewiadomski",
      "Anastasios Yiannakidis",
      "Hanz Cuevas-Velasquez",
      "Soubhik Sanyal",
      "Michael J. Black",
      "Silvia Zuffi",
      "Peter Kulits"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tong_Any-SSR_How_Recursive_Least_Squares_Works_in_Continual_Learning_of_ICCV_2025_paper.html": {
    "title": "Any-SSR: How Recursive Least Squares Works in Continual Learning of Large Language Model",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) possess encompassing capabilities that can process diverse language-related tasks. However, finetuning on LLMs will diminish this general skills and continual finetuning will further cause severe degradation on accumulated knowledge. Recently, Continual Learning (CL) in Large Language Models (LLMs) arises which aims to continually adapt the LLMs to new tasks while maintaining previously learned knowledge and inheriting general skills. Existing techniques either leverage previous data to replay, leading to extra computational costs, or utilize a single parameter-efficient module to learn the downstream task, constraining new knowledge absorption with interference between different tasks. Toward these issues, this paper proposes Analytic Subspace Routing(Any-SSR) to address these challenges. For each task, we isolate the learning within a subspace of deep layers' features via low-rank adaptation, eliminating knowledge interference between different tasks. Additionally, we propose an analytic routing mechanism to properly utilize knowledge learned in different subspaces. Our approach employs Recursive Least Squares to train a multi-task router model, allowing the router to dynamically adapt to incoming data without requiring access to historical data. Also, the router effectively assigns the current task to an appropriate subspace and has a non-forgetting property of previously learned tasks with a solid theoretical guarantee. Experimental results demonstrate that our method achieves near-perfect retention of prior knowledge while seamlessly integrating new information, effectively overcoming the core limitations of existing methods. Our code is available at https://github.com/ZHUANGHP/Any-SSR",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Tong",
      "Kang Pan",
      "Xiao Zhang",
      "Erli Meng",
      "Run He",
      "Yawen Cui",
      "Nuoyan Guo",
      "Huiping Zhuang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Instruction-Oriented_Preference_Alignment_for_Enhancing_Multi-Modal_Comprehension_Capability_of_MLLMs_ICCV_2025_paper.html": {
    "title": "Instruction-Oriented Preference Alignment for Enhancing Multi-Modal Comprehension Capability of MLLMs",
    "volume": "main",
    "abstract": "Preference alignment has emerged as an effective strategy to enhance the performance of Multimodal Large Language Models (MLLMs) following supervised fine-tuning. While existing preference alignment methods predominantly target hallucination factors, they overlook the factors essential for multi-modal comprehension capabilities, often narrowing their improvements on hallucination mitigation. To bridge this gap, we propose Instruction-oriented Preference Alignment (IPA), a scalable framework designed to automatically construct alignment preferences grounded in instruction fulfillment efficacy. Our method involves an automated preference construction coupled with a dedicated verification process that identifies instruction-oriented factors, avoiding significant variability in response representations. Additionally, IPA incorporates a progressive preference collection pipeline, further recalling challenging samples through model self-evolution and reference-guided refinement. Experiments conducted on Qwen2VL-7B demonstrate IPA's effectiveness across multiple benchmarks, including hallucination evaluation, visual question answering, and text understanding tasks, highlighting its capability to enhance general comprehension",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zitian Wang",
      "Yue Liao",
      "Kang Rong",
      "Fengyun Rao",
      "Yibo Yang",
      "Si Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_RapVerse_Coherent_Vocals_and_Whole-Body_Motion_Generation_from_Text_ICCV_2025_paper.html": {
    "title": "RapVerse: Coherent Vocals and Whole-Body Motion Generation from Text",
    "volume": "main",
    "abstract": "In this work, we introduce a challenging task for simultaneously generating 3D holistic body motions and singing vocals directly from textual lyrics inputs, advancing beyond existing works that typically address these two modalities in isolation. To facilitate this, we first collect the RapVerse dataset, a large dataset containing synchronous rapping vocals, lyrics, and high-quality 3D holistic body meshes. With the RapVerse dataset, we investigate the extent to which scaling autoregressive multimodal transformers across language, audio, and motion can enhance the coherent and realistic generation of vocals and whole-body human motions. For modality unification, a vector-quantized variational autoencoder is employed to encode whole-body motion sequences into discrete motion tokens, while a vocal-to-unit model is leveraged to obtain quantized audio tokens preserving content, prosodic information and singer identity. By jointly performing transformer modeling on these three modalities in a unified way, our framework ensures a seamless and realistic blend of vocals and human motions. Extensive experiments demonstrate that our unified generation framework not only produces coherent and realistic singing vocals alongside human motions directly from textual inputs, but also rivals the performance of specialized single-modality generation systems, establishing new benchmarks for joint vocal-motion generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaben Chen",
      "Xin Yan",
      "Yihang Chen",
      "Siyuan Cen",
      "Zixin Wang",
      "Qinwei Ma",
      "Haoyu Zhen",
      "Kaizhi Qian",
      "Lie Lu",
      "Chuang Gan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_MoFRR_Mixture_of_Diffusion_Models_for_Face_Retouching_Restoration_ICCV_2025_paper.html": {
    "title": "MoFRR: Mixture of Diffusion Models for Face Retouching Restoration",
    "volume": "main",
    "abstract": "The widespread use of face retouching on social media platforms raises concerns about the authenticity of face images. While existing methods focus on detecting face retouching, how to accurately recover the original faces from the retouched ones has yet to be answered. This paper introduces Face Retouching Restoration (FRR), a novel computer vision task aimed at restoring original faces from their retouched counterparts. FRR differs from traditional image restoration tasks by addressing the complex retouching operations with various types and degrees, which focuses more on the restoration of the low-frequency information of the faces. To tackle this challenge, we propose MoFRR, Mixture of Diffusion Models for FRR. Inspired by DeepSeek's expert isolation strategy, the MoFRR uses sparse activation of specialized experts handling distinct retouching types and the engagement of a shared expert dealing with universal retouching traces. Each specialized expert follows a dual-branch structure with a DDIM-based low-frequency branch guided by an Iterative Distortion Evaluation Module (IDEM) and a Cross-Attention-based High-Frequency branch (HFCAM) for detail refinement. Extensive experiments on a newly constructed face retouching dataset, RetouchingFFHQ++, demonstrate the effectiveness of MoFRR for FRR",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxin Liu",
      "Qichao Ying",
      "Zhenxing Qian",
      "Sheng Li",
      "Runqi Zhang",
      "Jian Liu",
      "Xinpeng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Park_SFUOD_Source-Free_Unknown_Object_Detection_ICCV_2025_paper.html": {
    "title": "SFUOD: Source-Free Unknown Object Detection",
    "volume": "main",
    "abstract": "Source-free object detection adapts a detector pre-trained on a source domain to an unlabeled target domain without requiring access to labeled source data. While this setting is practical as it eliminates the need for the source dataset during domain adaptation, it operates under the restrictive assumption that only pre-defined objects from the source domain exist in the target domain. This closed-set setting prevents the detector from detecting undefined objects. To ease this assumption, we propose Source-Free Unknown Object Detection (SFUOD), a novel scenario which enables the detector to not only recognize known objects but also detect undefined objects as unknown objects. To this end, we propose CollaPAUL (Collaborative tuning and Principal Axis-based Unknown Labeling), a novel framework for SFUOD. Collaborative tuning enhances knowledge adaptation by integrating target-dependent knowledge from the auxiliary encoder with source-dependent knowledge from the pre-trained detector through a cross-domain attention mechanism. Additionally, principal axis-based unknown labeling assigns pseudo-labels to unknown objects by estimating objectness via principal axes projection and confidence scores from model predictions. The proposed CollaPAUL achieves state-of-the-art performances on SFUOD benchmarks, and extensive experiments validate its effectiveness. The code will be released after the review",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keon-Hee Park",
      "Seun-An Choe",
      "Gyeong-Moon Park"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Patel_UniEgoMotion_A_Unified_Model_for_Egocentric_Motion_Reconstruction_Forecasting_and_ICCV_2025_paper.html": {
    "title": "UniEgoMotion: A Unified Model for Egocentric Motion Reconstruction, Forecasting, and Generation",
    "volume": "main",
    "abstract": "Egocentric human motion generation and forecasting with scene-context is crucial for enhancing AR/VR experiences, improving human-robot interaction, advancing assistive technologies, and enabling adaptive healthcare solutions by accurately predicting and simulating movement from a first-person perspective. However, existing methods primarily focus on third-person motion synthesis with structured 3D scene contexts, limiting their effectiveness in real-world egocentric settings where limited field of view, frequent occlusions, and dynamic cameras hinder scene perception. To bridge this gap, we introduce Egocentric Motion Generation and Egocentric Motion Forecasting, two novel tasks that utilize first-person images for scene-aware motion synthesis without relying on explicit 3D scene. We propose UniEgoMotion, a unified conditional motion diffusion model with a novel head-centric motion representation tailored for egocentric devices. UniEgoMotion's simple yet effective design supports egocentric motion reconstruction, forecasting, and generation from first-person visual inputs in a unified framework. Unlike previous works that overlook scene semantics, our model effectively extracts image-based scene context to infer plausible 3D motion. To facilitate training, we introduce EE4D-Motion, a large-scale dataset derived from EgoExo4D, augmented with pseudo-ground-truth 3D motion annotations. UniEgoMotion achieves state-of-the-art performance in egocentric motion reconstruction and is the first to generate motion from a single egocentric image. Extensive evaluations demonstrate the effectiveness of our unified framework, setting a new benchmark for egocentric motion modeling and unlocking new possibilities for egocentric applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaitanya Patel",
      "Hiroki Nakamura",
      "Yuta Kyuragi",
      "Kazuki Kozuka",
      "Juan Carlos Niebles",
      "Ehsan Adeli"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yin_ToolVQA_A_Dataset_for_Multi-step_Reasoning_VQA_with_External_Tools_ICCV_2025_paper.html": {
    "title": "ToolVQA: A Dataset for Multi-step Reasoning VQA with External Tools",
    "volume": "main",
    "abstract": "Integrating external tools into Large Foundation Models (LFMs) has emerged as a promising approach to enhance their problem-solving capabilities. While existing studies have demonstrated strong performance in tool-augmented Visual Question Answering (VQA), recent benchmarks re- veal significant gaps in real-world tool-use proficiency, particularly in functionally diverse multimodal settings re- quiring multi-step reasoning. In this work, we intro- duce ToolVQA, a large-scale multimodal dataset compris- ing 23K samples, designed to bridge this gap. Unlike pre- vious datasets that rely on synthetic scenarios and sim- plified queries, ToolVQA features real-world visual con- texts and challenging implicit multi-step reasoning tasks, better aligning with real user interactions. To construct this dataset, we propose ToolEngine, a novel data genera- tion pipeline that employs image-guided Depth-First Search (DFS) with a Longest Common Subsequence (LCS)-based example matching mechanism to simulate human-like tool- use reasoning. ToolVQA encompasses 10 multimodal tools across 7 diverse domains, with an average inference length of 2.78 reasoning steps per sample. The LLaVA-7B model fine-tuned on ToolVQA not only achieves impressive per- formance on the ToolVQA test set, but also surpasses the large closed-source model GPT-3.5-turbo on five out-of- distribution (OOD) datasets, showing strong generalizabil- ity in real-world tool-use scenarios. Code is available at https://github.com/Fugtemypt123/ToolVQA-release",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaofeng Yin",
      "Ting Lei",
      "Yang Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Brousseau_Spherical_Epipolar_Rectification_for_Deep_Two-View_Absolute_Depth_Estimation_ICCV_2025_paper.html": {
    "title": "Spherical Epipolar Rectification for Deep Two-View Absolute Depth Estimation",
    "volume": "main",
    "abstract": "Absolute depth estimation from single camera sequence of images is a relevant task given that mobile machines increasingly rely on vision to navigate. Deep learning for stereo matching has been demonstrated to improve performance for stereo rectified depth estimation but these methods require straightforward left-right camera setups. This work proposes to introduce deep stereo matching to two views of a monocular image sequence obtained from a camera in motion in a static scene. This paper introduces a novel and principled spherical epipolar rectification model, which handles all camera motions. This rectification model is differentiable and allows self-supervised deep stereo matching algorithms to compute disparity and recover depth, given known camera pose. This paper also introduces a spherical crop operation which limits rectified image size and allows for competitive absolute depth estimation performance. This results in a spherical rectification model that is demonstrated to provide metric depth and compete favorably with current state-of-the-art monocular depth estimators. The code is available at https://gitlab.com/labv3d/spherical-stereo.git",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pierre-André Brousseau",
      "Sébastien Roy"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xia_ScenePainter_Semantically_Consistent_Perpetual_3D_Scene_Generation_with_Concept_Relation_ICCV_2025_paper.html": {
    "title": "ScenePainter: Semantically Consistent Perpetual 3D Scene Generation with Concept Relation Alignment",
    "volume": "main",
    "abstract": "Perpetual 3D scene generation aims to produce long-range and coherent 3D view sequences, which is applicable for long-term video synthesis and 3D scene reconstruction. Existing methods follow a \"navigate-and-imagine\" fashion and rely on outpainting for successive view expansion. However, the generated view sequences suffer from semantic drift issue derived from the accumulated deviation of the outpainting module. To tackle this challenge, we propose ScenePainter, a new framework for semantically consistent 3D scene generation, which aligns the outpainter's scene-specific prior with the comprehension of the current scene. To be specific, we introduce a hierarchical graph structure dubbed SceneConceptGraph to construct relations among multi-level scene concepts, which directs the outpainter for consistent novel views and can be dynamically refined to enhance diversity. Extensive experiments demonstrate that our framework overcomes the semantic drift issue and generates more consistent and immersive 3D view sequences",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chong Xia",
      "Shengjun Zhang",
      "Fangfu Liu",
      "Chang Liu",
      "Khodchaphun Hirunyaratsameewong",
      "Yueqi Duan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ye_ESCNetEdge-Semantic_Collaborative_Network_for_Camouflaged_Object_Detection_ICCV_2025_paper.html": {
    "title": "ESCNet:Edge-Semantic Collaborative Network for Camouflaged Object Detection",
    "volume": "main",
    "abstract": "Camouflaged object detection (COD) faces unique challenges where target boundaries are intrinsically ambiguous due to their textural similarity to backgrounds. Existing methods relying on single-modality features often produce fragmented predictions due to insufficient boundary constraints.To address this, we propose ESCNet with dynamically coupled edge-texture perception. Our framework introduces three core innovations that work in concert:1) Adaptive Edge-Texture Perceptor (AETP), which creates an edge prediction behaviour where edge and texture information are mutually reinforcing based on the multi-scale features of the image integrated with the global semantic context of the Transformer;2) Dual-Stream Feature Augmentor (DSFA), which dynamically adjusts the kernel sampling position according to the local texture complexity and edge orientation, thus accurately enhancing the feature information at fractal boundaries and amorphous texture locations;3) Multi-Feature Modulation Module (MFMM), which establishes incremental fine-grained improvements for feature calibration and model prediction through enhanced characterisation of edge perception and hierarchical integration of multiple textures. This interconnected system forms a feedback loop where enhanced representations of edge perception enhance model texture prediction and vice versa. Our ESCNet demonstrates significant performance advantages on all three authoritative datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheng Ye",
      "Xin Chen",
      "Yan Zhang",
      "Xianming Lin",
      "Liujuan Cao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jin_PixelStitch_Structure-Preserving_Pixel-Wise_Bidirectional_Warps_for_Unsupervised_Image_Stitching_ICCV_2025_paper.html": {
    "title": "PixelStitch: Structure-Preserving Pixel-Wise Bidirectional Warps for Unsupervised Image Stitching",
    "volume": "main",
    "abstract": "We propose PixelStitch, a pixel-wise bidirectional warp that learns to stitch images as well as preserve structure in an unsupervised paradigm. To produce natural stitched images, we first determine the middle plane through homography decomposition and globally project the original images toward the desired plane. Compared with unidirectional homography transformation, it evenly spreads projective distortion across two views and decreases the proportion of invalid pixels. Then, the bidirectional optical flow fields are established to carry out residual pixel-wise deformation with projection-weighted natural coefficients, encouraging pixel motions to be as unnoticeable as possible in non-overlapping regions while smoothly transitioning into overlapping areas. Crucially, this flexible deformation enables PixelStitch to align large-parallax images and preserve the structural integrity of non-overlapping contents. To obtain high-quality stitched images in the absence of labels, a comprehensive unsupervised objective function is proposed to simultaneously encourage content alignment, structure preservation, and bidirectional consistency. Finally, extensive experiments are conducted to show our superiority to existing state-of-the-art (SoTA) methods in the quantitative metric, qualitative appearance, and generalization ability. The code will be available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hengzhe Jin",
      "Lang Nie",
      "Chunyu Lin",
      "Xiaomei Feng",
      "Yao Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_SANA-Sprint_One-Step_Diffusion_with_Continuous-Time_Consistency_Distillation_ICCV_2025_paper.html": {
    "title": "SANA-Sprint: One-Step Diffusion with Continuous-Time Consistency Distillation",
    "volume": "main",
    "abstract": "This paper presents SANA-Sprint, an efficient diffusion model for ultra-fast text-to-image (T2I) generation. SANA-Sprint is built on a pre-trained foundation model and augmented with hybrid distillation, dramatically reducing inference steps from 20 to 1-4.We introduce three key innovations: (1) We propose a training-free approach that transforms a pre-trained flow-matching model for continuous-time consistency distillation (sCM), eliminating costly training from scratch and achieving high training efficiency. Our hybrid distillation strategy combines sCM with latent adversarial distillation (LADD): sCM ensures alignment with the teacher model, while LADD enhances single-step generation fidelity. (2) SANA-Sprint is a unified step-adaptive model that achieves high-quality generation in 1-4 steps, eliminating step-specific training and improving efficiency. (3) We integrate ControlNet with SANA-Sprint for real-time interactive image generation, enabling instant visual feedback for user interaction. SANA-Sprint establishes a new Pareto frontier in speed-quality tradeoffs, achieving state-of-the-art performance with 7.59 FID and 0.74 GenEval in just 1 step -- outperforming FLUX-schnell (7.94 FID / 0.71 GenEval) while being 10x faster (0.1s vs 1.1s on H100). It also achieves 0.1s (T2I) and 0.25s (ControlNet) latency for 1024x1024 images on H100, and 0.31s (T2I) on an RTX 4090, showcasing its exceptional efficiency and potential for AI-powered consumer applications (AIPC). Code and pre-trained models will be open-sourced",
    "checked": true,
    "id": "ed3ae99e77c7daa8a8ff0655ab61649a4aa8aa56",
    "semantic_title": "sana-sprint: one-step diffusion with continuous-time consistency distillation",
    "citation_count": 22,
    "authors": [
      "Junsong Chen",
      "Shuchen Xue",
      "Yuyang Zhao",
      "Jincheng Yu",
      "Sayak Paul",
      "Junyu Chen",
      "Han Cai",
      "Song Han",
      "Enze Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yin_Information-Bottleneck_Driven_Binary_Neural_Network_for_Change_Detection_ICCV_2025_paper.html": {
    "title": "Information-Bottleneck Driven Binary Neural Network for Change Detection",
    "volume": "main",
    "abstract": "In this paper, we propose Binarized Change Detection (BiCD), the first binary neural network (BNN) designed specifically for change detection. Conventional network binarization approaches, which directly quantize both weights and activations in change detection models, severely limit the network's ability to represent input data and distinguish between changed and unchanged regions. This results in significantly lower detection accuracy compared to real-valued networks. To overcome these challenges, BiCD enhances both the representational power and feature separability of BNNs, improving detection performance. Specifically, we introduce an auxiliary objective based on the Information Bottleneck (IB) principle, guiding the encoder to retain essential input information while promoting better feature discrimination. Since directly computing mutual information under the IB principle is intractable, we design a compact, learnable auxiliary module as an approximation target, leading to a simple yet effective optimization strategy that minimizes both reconstruction loss and standard change detection loss.Extensive experiments on street-view and remote sensing datasets demonstrate that BiCD establishes a new benchmark for BNN-based change detection, achieving state-of-the-art performance in this domain",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaijie Yin",
      "Zhiyuan Zhang",
      "Shu Kong",
      "Tian Gao",
      "Cheng-Zhong Xu",
      "Hui Kong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Amara_Erasing_More_Than_Intended_How_Concept_Erasure_Degrades_the_Generation_ICCV_2025_paper.html": {
    "title": "Erasing More Than Intended? How Concept Erasure Degrades the Generation of Non-Target Concepts",
    "volume": "main",
    "abstract": "Concept erasure techniques have recently gained significant attention for their potential to remove unwanted concepts from text-to-image models. While these methods often demonstrate promising results in controlled settings, their robustness in real-world applications and suitability for deployment remain uncertain. In this work, we (1) identify a critical gap in evaluating sanitized models, particularly in assessing their performance across diverse concept dimensions, and (2) systematically analyze the failure modes of text-to-image models post-erasure. We focus on the unintended consequences of concept removal on non-target concepts across different levels of interconnected relationships including visually similar, binomial, and semantically related concepts. To address this, we introduce EraseBench, a comprehensive benchmark for evaluating post-erasure performance. EraseBench includes over 100 curated concepts, targeted evaluation prompts, and a robust set of metrics to assess both effectiveness and side effects of erasure. Our findings reveal a phenomenon of concept entanglement, where erasure leads to unintended suppression of non-target concepts, causing spillover degradation that manifests as distortions and a decline in generation quality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ibtihel Amara",
      "Ahmed Imtiaz Humayun",
      "Ivana Kajic",
      "Zarana Parekh",
      "Natalie Harris",
      "Sarah Young",
      "Chirag Nagpal",
      "Najoung Kim",
      "Junfeng He",
      "Cristina Nader Vasconcelos",
      "Deepak Ramachandran",
      "Golnoosh Farnadi",
      "Katherine Heller",
      "Mohammad Havaei",
      "Negar Rostamzadeh"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sastry_Global_and_Local_Entailment_Learning_for_Natural_World_Imagery_ICCV_2025_paper.html": {
    "title": "Global and Local Entailment Learning for Natural World Imagery",
    "volume": "main",
    "abstract": "Learning the hierarchical structure of data in vision-language models is a significant challenge. Previous works have attempted to address this challenge by employing entailment learning. However, these approaches fail to model the transitive nature of entailment explicitly, which establishes the relationship between order and semantics within a representation space. In this work, we introduce Radial Cross-Modal Embeddings (RCME), a framework that enables the explicit modeling of transitivity-enforced entailment. Our proposed framework optimizes for the partial order of concepts within vision-language models. By leveraging our framework, we develop a hierarchical vision-language foundation model capable of representing the hierarchy in the Tree of Life. Our experiments on hierarchical species classification and hierarchical retrieval tasks demonstrate the enhanced performance of our models compared to the existing state-of-the-art models. Our code and models are open-sourced at https://vishu26.github.io/RCME",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Srikumar Sastry",
      "Aayush Dhakal",
      "Eric Xing",
      "Subash Khanal",
      "Nathan Jacobs"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Ross3D_Reconstructive_Visual_Instruction_Tuning_with_3D-Awareness_ICCV_2025_paper.html": {
    "title": "Ross3D: Reconstructive Visual Instruction Tuning with 3D-Awareness",
    "volume": "main",
    "abstract": "The rapid development of Large Multimodal Models (LMMs) for 2D images and videos has spurred efforts to adapt these models for interpreting 3D scenes. However, the absence of large-scale 3D vision-language datasets has posed a significant obstacle. To address this issue, typical approaches focus on injecting 3D awareness into 2D LMMs by designing 3D input-level scene representations. This work provides a new perspective. We introduce reconstructive visual instruction tuning with 3D-awareness (ROSS3D), which integrates 3D-aware visual supervision into the training procedure. Specifically, it incorporates cross-view and global-view reconstruction. The former requires reconstructing masked views by aggregating overlapping information from other views. The latter aims to aggregate information from all available views to recover Bird's-Eye-View images, contributing to a comprehensive overview of the entire scene. Empirically, ROSS3D achieves state-of-the-art performance across various 3D scene understanding benchmarks. More importantly, our semi-supervised experiments demonstrate significant potential in leveraging large amounts of unlabeled 3D vision-only data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haochen Wang",
      "Yucheng Zhao",
      "Tiancai Wang",
      "Haoqiang Fan",
      "Xiangyu Zhang",
      "Zhaoxiang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/You_LVFace_Progressive_Cluster_Optimization_for_Large_Vision_Models_in_Face_ICCV_2025_paper.html": {
    "title": "LVFace: Progressive Cluster Optimization for Large Vision Models in Face Recognition",
    "volume": "main",
    "abstract": "Vision Transformers (ViTs) have revolutionized large-scale visual modeling, yet remain underexplored in face recognition (FR) where CNNs still dominate. We identify a critical bottleneck: CNN-inspired training paradigms fail to unlock ViT's potential, leading to suboptimal performance and convergence instability.To address this challenge, we propose LVFace, a ViT-based FR model that integrates Progressive Cluster Optimization (PCO) to achieve superior results. Specifically, PCO sequentially applies negative class sub-sampling (NCS) for robust and fast feature alignment from random initialization, feature expectation penalties for centroid stabilization, performing cluster boundary refinement through full-batch training without NCS constraints. LVFace establishes a new state-of-the-art face recognition baseline, surpassing leading approaches such as UniFace and TopoFR across multiple benchmarks. Extensive experiments demonstrate that LVFace delivers consistent performance gains, while exhibiting scalability to large-scale datasets and compatibility with mainstream VLMs and LLMs. Notably, LVFace secured 1st place in the ICCV 2021 Masked Face Recognition (MFR)-Ongoing Challenge (March 2025), proving its efficacy in real-world scenarios. Project is available at https://github.com/bytedance/LVFace",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinghan You",
      "Shanglin Li",
      "Yuanrui Sun",
      "Jiangchuan Wei",
      "Mingyu Guo",
      "Chao Feng",
      "Jiao Ran"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xie_Dataset_Ownership_Verification_for_Pre-trained_Masked_Models_ICCV_2025_paper.html": {
    "title": "Dataset Ownership Verification for Pre-trained Masked Models",
    "volume": "main",
    "abstract": "High-quality open-source datasets have emerged as a pivotal catalyst driving the swift advancement of deep learning, while facing the looming threat of potential exploitation. Protecting these datasets is of paramount importance for the interests of their owners. The verification of dataset ownership has evolved into a crucial approach in this domain; however, existing verification techniques are predominantly tailored to supervised models and contrastive pre-trained models, rendering them ill-suited for direct application to the increasingly prevalent masked models. In this work, we introduce the inaugural methodology addressing this critical, yet unresolved challenge, termed Dataset Ownership Verification for Masked Modeling (DOV4MM). The central objective is to ascertain whether a suspicious black-box model has been pre-trained on a particular unlabeled dataset, thereby assisting dataset owners in safeguarding their rights. DOV4MM is grounded in our empirical observation that when a model is pre-trained on the target dataset, the difficulty of reconstructing masked information within the embedding space exhibits a marked contrast to models not pre-trained on that dataset. We validated the efficacy of DOV4MM through ten masked image models on ImageNet-1K and four masked language models on WikiText-103. The results demonstrate that DOV4MM rejects the null hypothesis, with a p-value considerably below 0.05, surpassing all prior approaches. Code is available at https://github.com/xieyc99/DOV4MM",
    "checked": true,
    "id": "1258472153ce0f2b7280aae582a5ceb7c06ccf18",
    "semantic_title": "dataset ownership verification for pre-trained masked models",
    "citation_count": 0,
    "authors": [
      "Yuechen Xie",
      "Jie Song",
      "Yicheng Shan",
      "Xiaoyan Zhang",
      "Yuanyu Wan",
      "Shengxuming Zhang",
      "Jiarui Duan",
      "Mingli Song"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kong_VLR-Driver_Large_Vision-Language-Reasoning_Models_for_Embodied_Autonomous_Driving_ICCV_2025_paper.html": {
    "title": "VLR-Driver: Large Vision-Language-Reasoning Models for Embodied Autonomous Driving",
    "volume": "main",
    "abstract": "The rise of embodied intelligence and multi-modal large language models has led to exciting advancements in the field of autonomous driving, establishing it as a prominent research focus in both academia and industry. However, when confronted with intricate and ambiguous traffic scenarios, the lack of logical reasoning and cognitive decision-making capabilities remains the primary challenge impeding the realization of embodied autonomous driving. Although Vision Language Models (VLMs) have enhanced the deep semantic understanding of autonomous driving systems, they exhibit notable limitations in decision explainability when handling rare and long-tail traffic scenarios. In this paper, we propose VLR-Driver, a novel multi-modal Vision-Language-Reasoning (VLR) framework based on Chain of Thought (CoT) for embodied autonomous driving. The framework employs a spatiotemporal CoT reasoning approach to recursively analyze potential safety risks and driving intentions of other agents, thereby delivering an efficient and transparent decision-making process. Furthermore, we construct a multi-modal reasoning-decision dataset to support the advancement of hierarchical reasoning of VLMs in autonomous driving. Closed-loop experiments conducted in CARLA demonstrate that the VLR-Driver significantly outperforms state-of-the-art end-to-end methods. Notably, key metrics such as driving score improved by 17.5%, while the success rate improved by 22.2%, offering a more transparent, reliable, and secure solution for autonomous driving systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fanjie Kong",
      "Yitong Li",
      "Weihuang Chen",
      "Chen Min",
      "Yizhe Li",
      "Zhiqiang Gao",
      "Haoyang Li",
      "Zhongyu Guo",
      "Hongbin Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lyu_ResGS_Residual_Densification_of_3D_Gaussian_for_Efficient_Detail_Recovery_ICCV_2025_paper.html": {
    "title": "ResGS: Residual Densification of 3D Gaussian for Efficient Detail Recovery",
    "volume": "main",
    "abstract": "Recently, 3D Gaussian Splatting (3D-GS) has prevailed in novel view synthesis, achieving high fidelity and efficiency. However, it often struggles to capture rich details and complete geometry. Our analysis reveals that the 3D-GS densification operation lacks adaptiveness and faces a dilemma between geometry coverage and detail recovery. To address this, we introduce a novel densification operation, residual split, which adds a downscaled Gaussian as a residual. Our approach is capable of adaptively retrieving details and complementing missing geometry. To further support this method, we propose a pipeline named ResGS. Specifically, we integrate a Gaussian image pyramid for progressive supervision and implement a selection scheme that prioritizes the densification of coarse Gaussians over time. Extensive experiments demonstrate that our method achieves SOTA rendering quality. Consistent performance improvements can be achieved by applying our residual split on various 3D-GS variants, underscoring its versatility and potential for broader application in 3D-GS-based applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanzhe Lyu",
      "Kai Cheng",
      "Xin Kang",
      "Xuejin Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yu_Language_Driven_Occupancy_Prediction_ICCV_2025_paper.html": {
    "title": "Language Driven Occupancy Prediction",
    "volume": "main",
    "abstract": "We introduce LOcc, an effective and generalizable framework for open-vocabulary occupancy (OVO) prediction. Previous approaches typically supervise the networks through coarse voxel-to-text correspondences via image features as intermediates or noisy and sparse correspondences from voxel-based model-view projections. To alleviate the inaccurate supervision, we propose a semantic transitive labeling pipeline to generate dense and fine-grained 3D language occupancy ground truth. Our pipeline presents a feasible way to dig into the valuable semantic information of images, transferring text labels from images to LiDAR point clouds and ultimately to voxels, to establish precise voxel-to-text correspondences. By replacing the original prediction head of supervised occupancy models with a geometry head for binary occupancy states and a language head for language features, LOcc effectively uses the generated language ground truth to guide the learning of 3D language volume. Through extensive experiments, we demonstrate that our transitive semantic labeling pipeline can produce more accurate pseudo-labeled ground truth, diminishing labor-intensive human annotations. Additionally, we validate LOcc across various architectures, where all models consistently outperform state-of-the-art zero-shot occupancy prediction approaches on the Occ3D-nuScenes dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhu Yu",
      "Bowen Pang",
      "Lizhe Liu",
      "Runmin Zhang",
      "Qiang Li",
      "Si-Yuan Cao",
      "Maochun Luo",
      "Mingxia Chen",
      "Sheng Yang",
      "Hui-Liang Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Schmidt_Prior2Former_-_Evidential_Modeling_of_Mask_Transformers_for_Assumption-Free_Open-World_ICCV_2025_paper.html": {
    "title": "Prior2Former - Evidential Modeling of Mask Transformers for Assumption-Free Open-World Panoptic Segmentation",
    "volume": "main",
    "abstract": "In panoptic segmentation, individual instances must be separated within semantic classes. As state-of-the-art methods rely on a pre-defined set of classes, they struggle with novel categories and out-of-distribution (OOD) data. This is particularly problematic in safety-critical applications, such as autonomous driving, where reliability in unseen scenarios is essential. We address the gap between outstanding benchmark performance and reliability by proposing Prior2Former (P2F), the first approach for segmentation vision transformers rooted in evidential learning. P2F extends the mask vision transformer architecture by incorporating a Beta prior for computing model uncertainty in pixel-wise binary mask assignments. This design enables high-quality uncertainty estimation that effectively detects novel and OOD objects, enabling state-of-the-art anomaly instance segmentation and open-world panoptic segmentation. Unlike most segmentation models addressing unknown classes, P2F operates without access to OOD data samples or contrastive training on void (i.e., unlabeled) classes, making it highly applicable in real-world scenarios where such prior information is unavailable. Additionally, P2F can be flexibly applied to anomaly instance and panoptic segmentation. Through comprehensive experiments on the Cityscapes, COCO, SegmentMeIfYouCan, and OoDIS datasets, P2F demonstrates state-of-the-art performance. Especially in OoDIS, P2F ranks first in its category",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Schmidt",
      "Julius Koerner",
      "Dominik Fuchsgruber",
      "Stefano Gasperini",
      "Federico Tombari",
      "Stephan Günnemann"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_E-NeMF_Event-based_Neural_Motion_Field_for_Novel_Space-time_View_Synthesis_ICCV_2025_paper.html": {
    "title": "E-NeMF: Event-based Neural Motion Field for Novel Space-time View Synthesis of Dynamic Scenes",
    "volume": "main",
    "abstract": "Synthesizing novel space-time views from a monocular video is a highly ill-posed problem, and its effectiveness relies on accurately reconstructing motion and appearance of the dynamic scene.Frame-based methods for novel space-time view synthesis in dynamic scenes rely on simplistic motion assumptions due to the absence of inter-frame cues, which makes them fall in complex motion. Event camera captures inter-frame cues with high temporal resolution, which makes it hold the promising potential to handle high speed and complex motion. However, it is still difficult due to the event noise and sparsity. To mitigate the impact caused by event noise and sparsity, we propose E-NeMF, which alleviates the impact of event noise with Parametric Motion Representation and mitigates the event sparsity with Flow Prediction Module. Experiments on multiple real-world datasets demonstrate our superior performance in handling high-speed and complex motion",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Liu",
      "Zehao Chen",
      "Haojie Yan",
      "De Ma",
      "Huajin Tang",
      "Qian Zheng",
      "Gang Pan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Event-based_Tiny_Object_Detection_A_Benchmark_Dataset_and_Baseline_ICCV_2025_paper.html": {
    "title": "Event-based Tiny Object Detection: A Benchmark Dataset and Baseline",
    "volume": "main",
    "abstract": "Small object detection (SOD) in anti-UAV task is a challenging problem due to the small size of UAVs and complex backgrounds. Traditional frame-based cameras struggle to detect small objects in complex environments due to their low frame rates, limited dynamic range, and data redundancy. Event cameras, with microsecond temporal resolution and high dynamic range, provide a more effective solution for SOD. However, existing event-based object detection datasets are limited in scale, feature large targets size, and lack diverse backgrounds, making them unsuitable for SOD benchmarks. In this paper, we introduce a Event-based Small object detection (EVSOD) dataset (namely EV-UAV), the first large-scale, highly diverse benchmark for anti-UAV tasks. It includes 147 sequences with over 2.3 million event-level annotations, featuring extremely small targets (averaging 6.8 x 5.4 pixels) and diverse scenarios such as urban clutter and extreme lighting conditions. Furthermore, based on the observation that small moving targets form continuous curves in spatiotemporal event point clouds, we propose Event based Sparse Segmentation Network (EV-SpSegNet), a novel baseline for event segmentation in point cloud space, along with a Spatiotemporal Correlation (STC) loss that leverages motion continuity to guide the network in retaining target events. Extensive experiments on the EV-UAV dataset demonstrate the superiority of our method and provide a benchmark for future research in EVSOD",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nuo Chen",
      "Chao Xiao",
      "Yimian Dai",
      "Shiman He",
      "Miao Li",
      "Wei An"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xiao_Optimal_Transport_for_Brain-Image_Alignment_Unveiling_Redundancy_and_Synergy_in_ICCV_2025_paper.html": {
    "title": "Optimal Transport for Brain-Image Alignment: Unveiling Redundancy and Synergy in Neural Information Processing",
    "volume": "main",
    "abstract": "The design of artificial neural networks (ANNs) is inspired by the structure of the human brain, and in turn, ANNs offer a potential means to interpret and understand brain signals. Existing methods primarily align brain signals with stimulus signals using Mean Squared Error (MSE), which focuses only on local point-wise alignment and ignores global matching, leading to coarse interpretations and inaccuracies in brain signal decoding. In this paper, we address these issues through optimal transport (OT) and theoretically demonstrate why OT provides a more effective alignment strategy than MSE. Specifically, we construct a transport plan between brain voxel embeddings and image embeddings, enabling more precise matching. By controlling the amount of transport, we mitigate the influence of redundant information. We apply our alignment model directly to the Brain Captioning task by feeding brain signals into a large language model (LLM) instead of images. Our approach achieves state-of-the-art performance across ten evaluation metrics, surpassing the previous best method by an average of 6.11% in singlesubject training and 3.81% in cross-subject training. Additionally, we have uncovered several insightful conclusions that align with existing brain research. We unveil the redundancy and synergy of brain information processing through region masking and data dimensionality reduction visualization experiments. We believe our approach paves the way for a more precise understanding of brain signals in the future. The code is available at https://github.com/NKUShaw/OT-Alignment4brain-to-image",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Xiao",
      "Wang Lu",
      "Jie  Ji",
      "Ruimeng Ye",
      "Gen Li",
      "Xiaolong Ma",
      "Bo Hui"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lu_Adversarial_Distribution_Matching_for_Diffusion_Distillation_Towards_Efficient_Image_and_ICCV_2025_paper.html": {
    "title": "Adversarial Distribution Matching for Diffusion Distillation Towards Efficient Image and Video Synthesis",
    "volume": "main",
    "abstract": "Distribution Matching Distillation (DMD) is a promising score distillation technique that compresses pre-trained teacher diffusion models into efficient one-step or multi-step student generators.Nevertheless, its reliance on the reverse Kullback-Leibler (KL) divergence minimization potentially induces mode collapse (or mode-seeking) in certain applications.To circumvent this inherent drawback, we propose Adversarial Distribution Matching (ADM), a novel framework that leverages diffusion-based discriminators to align the latent predictions between real and fake score estimators for score distillation in an adversarial manner.In the context of extremely challenging one-step distillation, we further improve the pre-trained generator by adversarial distillation with hybrid discriminators in both latent and pixel spaces.Different from the mean squared error used in DMD2 pre-training, our method incorporates the distributional loss on ODE pairs collected from the teacher model, and thus providing a better initialization for score distillation fine-tuning in the next stage.By combining the adversarial distillation pre-training with ADM fine-tuning into a unified pipeline termed DMDX, our proposed method achieves superior one-step performance on SDXL compared to DMD2 while consuming less GPU time.Additional experiments that apply multi-step ADM distillation on SD3-Medium, SD3.5-Large, and CogVideoX set a new benchmark towards efficient image and video synthesis",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanzuo Lu",
      "Yuxi Ren",
      "Xin Xia",
      "Shanchuan Lin",
      "Xing Wang",
      "Xuefeng Xiao",
      "Andy J. Ma",
      "Xiaohua Xie",
      "Jian-Huang Lai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Majumder_Switch-a-View_View_Selection_Learned_from_Unlabeled_In-the-wild_Videos_ICCV_2025_paper.html": {
    "title": "Switch-a-View: View Selection Learned from Unlabeled In-the-wild Videos",
    "volume": "main",
    "abstract": "We introduce Switch-a-View, a model that learns to automatically select the viewpoint to display at each timepoint when creating a how-to video. The key insight of our approach is how to train such a model from unlabeled--but human-edited--video samples. We pose a pretext task that pseudo-labels segments in the training videos for their primary viewpoint (egocentric or exocentric), and then discovers the patterns between the visual and spoken content in a how-to video on the one hand and its view-switch moments on the other hand. Armed with this predictor, our model can be applied to new multi-view videos to orchestrate which viewpoint should be displayed when. We demonstrate our idea on a variety of real-world videos from HowTo100M and Ego-Exo4D, and rigorously validate its advantages",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sagnik Majumder",
      "Tushar Nagarajan",
      "Ziad Al-Halah",
      "Kristen Grauman"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_E-SAM_Training-Free_Segment_Every_Entity_Model_ICCV_2025_paper.html": {
    "title": "E-SAM: Training-Free Segment Every Entity Model",
    "volume": "main",
    "abstract": "Entity Segmentation (ES) aims at identifying and segmenting distinct entities within an image without the need for predefined class labels. This characteristic makes ES well-suited to open-world applications with adaptation to diverse and dynamically changing environments, where new and previously unseen entities may appear frequently. Existing ES methods either require large annotated datasets or high training costs, limiting their scalability and adaptability. Recently, the Segment Anything Model (SAM), especially in its Automatic Mask Generation (AMG) mode, has shown potential for holistic image segmentation. However, it struggles with over-segmentation and under-segmentation, making it less effective for ES. In this paper, we introduce E-SAM, a novel training-free framework that exhibits exceptional ES capability. Specifically, we first propose Multi-level Mask Generation (MMG) that hierarchically processes SAM's AMG outputs to generate reliable object-level masks while preserving fine details at other levels. Entity-level Mask Refinement (EMR) then refines these object-level masks into accurate entity-level masks. That is, it separates overlapping masks to address the redundancy issues inherent in SAM's outputs and merges similar masks by evaluating entity-level consistency. Lastly, Under-Segmentation Refinement (USR) addresses under-segmentation by generating additional high-confidence masks fused with EMR outputs to produce the final ES map. These three modules are seamlessly optimized to achieve the best ES without additional training overhead. Extensive experiments demonstrate that E-SAM achieves state-of-the-art performance compared to prior ES methods, demonstrating a significant improvement by +30.1 on benchmark metrics",
    "checked": true,
    "id": "9e248e6e62252147fe70cffc18ec1793617cba97",
    "semantic_title": "e-sam: training-free segment every entity model",
    "citation_count": 1,
    "authors": [
      "Weiming Zhang",
      "Dingwen Xiao",
      "Lei Chen",
      "Lin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_ViewSRD_3D_Visual_Grounding_via_Structured_Multi-View_Decomposition_ICCV_2025_paper.html": {
    "title": "ViewSRD: 3D Visual Grounding via Structured Multi-View Decomposition",
    "volume": "main",
    "abstract": "3D visual grounding aims to identify and localize objects in a 3D space based on textual descriptions. However, existing methods struggle with disentangling targets from anchors in complex multi-anchor queries and resolving inconsistencies in spatial descriptions caused by perspective variations.To tackle these challenges, we propose ViewSRD, a framework that formulates 3D visual grounding as a structured multi-view decomposition process. First, the Simple Relation Decoupling (SRD) module restructures complex multi-anchor queries into a set of targeted single-anchor statements, generating a structured set of perspective-aware descriptions that clarify positional relationships. These decomposed representations serve as the foundation for the Multi-view Textual-Scene Interaction (Multi-TSI) module, which integrates textual and scene features across multiple viewpoints using shared, Cross-modal Consistent View Tokens (CCVTs) to preserve spatial correlations. Finally, a Textual-Scene Reasoning module synthesizes multi-view predictions into a unified and robust 3D visual grounding.Experiments on 3D visual grounding datasets show that ViewSRD significantly outperforms state-of-the-art methods, particularly in complex queries requiring precise spatial differentiation.Code is available at https://github.com/visualjason/ViewSRD",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ronggang Huang",
      "Haoxin Yang",
      "Yan Cai",
      "Xuemiao Xu",
      "Huaidong Zhang",
      "Shengfeng He"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/He_UniPortrait_A_Unified_Framework_for_Identity-Preserving_Single-_and_Multi-Human_Image_ICCV_2025_paper.html": {
    "title": "UniPortrait: A Unified Framework for Identity-Preserving Single- and Multi-Human Image Personalization",
    "volume": "main",
    "abstract": "This paper presents UniPortrait, an innovative human image personalization framework that unifies single- and multi-ID customization with high face fidelity, extensive facial editability, free-form input description, and diverse layout generation. UniPortrait consists of only two plug-and-play modules: an ID embedding module and an ID routing module. The ID embedding module extracts versatile editable facial features with a decoupling strategy for each ID and embeds them into the context space of diffusion models. The ID routing module then combines and distributes these embeddings adaptively to their respective regions within the synthesized image, achieving the customization of single and multiple IDs. With a carefully designed two-stage training scheme, UniPortrait achieves superior performance in both single- and multi-ID customization. Quantitative and qualitative experiments demonstrate the advantages of our method over existing approaches as well as its good scalability, e.g., the universal compatibility with existing generative control tools",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junjie He",
      "Yifeng Geng",
      "Liefeng Bo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_FDPT_Federated_Discrete_Prompt_Tuning_for_Black-Box_Visual-Language_Models_ICCV_2025_paper.html": {
    "title": "FDPT: Federated Discrete Prompt Tuning for Black-Box Visual-Language Models",
    "volume": "main",
    "abstract": "General-purpose Vision-Language Models (VLMs) have driven major advancements in multimodal AI. Fine-tuning these models with task-specific data enhances adaptability to various downstream tasks but suffers from privacy risks. While potential solutions like federated learning can address user data privacy concerns, model protection is also essential. Other methods that rely on black-box VLM APIs usually require the access of prediction logits, leaving them open to inversion attacks. Moreover, addressing the challenges of tuning complexity and data transmission efficiency in federated VLM scenarios is also crucial. To address these challenges, we propose FDPT--a federated discrete prompt tuning method utilizing black-box VLMs. During client optimization stage, FDPT employs an agent-driven framework leveraging large language models (LLMs) with enhanced reasoning capacities to systematically optimize discrete prompt representations, and also utilizes feedback mechanisms and chain of thought to enhance prediction accuracy. Importantly, it performs optimization by relying not on the predicted logic vectors output by LLMs but on textual results, avoiding reverse attack risks. During global aggregation stage, We mimic human electoral activities by employing evolutionary computation methods underpinned by semantic similarity computation to implement enhanced zero-order optimization for acquiring representative global tokens, thereby achieving knowledge aggregation. FDPT significantly outperforms nine state-of-the-art methods in image classification and visual question-answering, reducing communication overhead while generating highly transferable optimized prompts. Additionally, it exhibits improved robustness to data heterogeneity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqi Wu",
      "Simin Chen",
      "Jing Tang",
      "Yuzhe Yang",
      "Yiming Chen",
      "Lixu Wang",
      "Song Lin",
      "Zehua Wang",
      "Wei Chen",
      "Zijian Tian"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_Exploiting_Diffusion_Prior_for_Task-driven_Image_Restoration_ICCV_2025_paper.html": {
    "title": "Exploiting Diffusion Prior for Task-driven Image Restoration",
    "volume": "main",
    "abstract": "Task-driven image restoration (TDIR) has recently emerged to address performance drops in high-level vision tasks caused by low-quality (LQ) inputs. Previous TDIR methods struggle to handle practical scenarios in which images are degraded by multiple complex factors, leaving minimal clues for restoration. This motivates us to leverage the diffusion prior, one of the most powerful natural image priors. However, while the diffusion prior can help generate visually plausible results, using it to restore task-relevant details remains challenging, even when combined with recent TDIR methods. To address this, we propose EDTR, which effectively harnesses the power of diffusion prior to restore task-relevant details. Specifically, we propose directly leveraging useful clues from LQ images in the diffusion process by generating from pixel-error-based pre-restored LQ images with mild noise added. Moreover, we employ a small number of denoising steps to prevent the generation of redundant details that dilute crucial task-related information. We demonstrate that our method effectively utilizes diffusion prior for TDIR, significantly enhancing task performance and visual quality across diverse tasks with multiple complex degradations",
    "checked": true,
    "id": "61730a01cf93e6cd2f6ac60f803e08eb7df3cc06",
    "semantic_title": "exploiting diffusion prior for task-driven image restoration",
    "citation_count": 0,
    "authors": [
      "Jaeha Kim",
      "Junghun Oh",
      "Kyoung Mu Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kuroki_CE-FAM_Concept-Based_Explanation_via_Fusion_of_Activation_Maps_ICCV_2025_paper.html": {
    "title": "CE-FAM: Concept-Based Explanation via Fusion of Activation Maps",
    "volume": "main",
    "abstract": "Although saliency maps can highlight important regions to explain the reasoning behind image classification in artificial intelligence (AI), the meaning of these regions is left to the user's interpretation. In contrast, concept-based explanations decompose AI predictions into human-understandable concepts, clarifying their contributions. However, few methods can simultaneously reveal what concepts an image classifier learns, which regions are associated with them, and how they contribute to predictions.We propose a novel concept-based explanation method, Concept-based Explanation via Fusion of Activation Maps (CE-FAM). It employs a branched network that shares activation maps with an image classifier and learns to mimic the embeddings of a Vision and Language Model (VLM). The branch network predicts concepts in an image, and their corresponding regions are represented by a weighted sum of activation maps, with weights given by the gradients of the concept prediction scores. Their contributions are quantified based on their impact on the image classification score. Our method provides a general framework for identifying the concept regions and their contributions while leveraging VLM knowledge to handle arbitrary concepts without requiring an annotated dataset. Furthermore, we introduce a novel evaluation metric to assess the accuracy of the concept regions. Our qualitative and quantitative evaluations demonstrate our method outperforms existing approaches and excels in zero-shot inference for unseen concepts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michihiro Kuroki",
      "Toshihiko Yamasaki"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Dual-level_Prototype_Learning_for_Composite_Degraded_Image_Restoration_ICCV_2025_paper.html": {
    "title": "Dual-level Prototype Learning for Composite Degraded Image Restoration",
    "volume": "main",
    "abstract": "Images captured under severe weather conditions often suffer from complex, composite degradations, varying in intensity. In this paper, we introduce a novel method, Dual-Level Prototype Learning (DPL), to tackle the challenging task of composite degraded image restoration. Unlike previous methods that rely on fixed embeddings to characterize degradation types, DPL maintains a number of degradation-level prototypes to represent the specific degradation scenes dynamically. Furthermore, considering the diverse factors influencing each degradation type, factor-level prototypes are incorporated to capture variations in individual degradation factors. Image features are matched with both degradation-level and factor-level prototypes, producing detailed scene embeddings that enhance the network's understanding of composite degradations. These scene embeddings are then processed through Dual Scene Embedding Transformer Blocks to guide the restoration process. To further refine the prototype distribution, we propose a Prototype Scatter Learning Loss, which enables prototypes within the same degradation to learn more information and push prototypes between different degradations to be separate. Additionally, we introduce a new dataset named Variable Composite Degradation (VCD) dataset which contains images with different intensities of each type of composite degradation to validate the efficacy of our method. Extensive experiments demonstrate that DPL significantly outperforms existing methods in restoring images with composite degradations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongze Wang",
      "Haitao Zhao",
      "Lujian Yao",
      "Jingchao Peng",
      "Kaijie Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Fang_Forensic-MoE_Exploring_Comprehensive_Synthetic_Image_Detection_Traces_with_Mixture_of_ICCV_2025_paper.html": {
    "title": "Forensic-MoE: Exploring Comprehensive Synthetic Image Detection Traces with Mixture of Experts",
    "volume": "main",
    "abstract": "Recently, synthetic images have evolved incredibly realistic with the development of generative techniques. To avoid the spread of misinformation and identify synthetic content, research on synthetic image detection becomes urgent. Unfortunately, limited to the singular forensic perspective, existing methods struggle to explore sufficient traces encountered with diverse synthetic techniques. In response to this, we argue that different synthetic images encompass a variety of forensic traces, and utilizing multiple experts to explore traces from diverse perspectives will be beneficial. Accordingly, a novel detector with the Mixture of multiple forensic Experts is proposed, named Forensic-MoE. To integrate multiple experts and enhance the knowledge interaction, Forensic-MoE follows an adapter-backbone architecture. Specifically, multiple adapters trained on different synthetic images serve as the trace exploration experts, and they are uniformly integrated into a pretrained backbone model to learn the detection prior and encourage the expert interaction. By guiding multiple experts to align with each other and collaborate, Forensic-MoE can integrate comprehensive and discriminative detection traces from multiple perspectives. Moreover, for the discrimination improvement of each expert, a multi-stage structure is proposed for efficient trace perception, and a patch decentralization strategy is applied to encourage the model's attention on every local region. Code will be available in https://github.com/fangmq77/Forensic-MoE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingqi Fang",
      "Ziguang Li",
      "Lingyun Yu",
      "Quanwei Yang",
      "Hongtao Xie",
      "Yongdong Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jeong_Robust_Adverse_Weather_Removal_via_Spectral-based_Spatial_Grouping_ICCV_2025_paper.html": {
    "title": "Robust Adverse Weather Removal via Spectral-based Spatial Grouping",
    "volume": "main",
    "abstract": "Adverse weather conditions cause diverse and complex degradation patterns, driving the development of All-in-One (AiO) models. However, recent AiO solutions still struggle to capture diverse degradations, since global filtering methods like direct operations on the frequency domain fail to handle highly variable and localized distortions. To address these issue, we propose Spectral-based Spatial Grouping Transformer (SSGformer), a novel approach that leverages spectral decomposition and group-wise attention for multi-weather image restoration. SSGformer decomposes images into high-frequency edge features using conventional edge detection and low-frequency information via Singular Value Decomposition. We utilize multi-head linear attention to effectively model the relationship between these features. The fused features are integrated with the input to generate a grouping-mask that clusters regions based on the spatial similarity and image texture. To fully leverage this mask, we introduce a group-wise attention mechanism, enabling robust adverse weather removal and ensuring consistent performance across diverse weather conditions. We also propose a Spatial Grouping Transformer Block that uses both channel attention and spatial attention, effectively balancing feature-wise relationships and spatial dependencies. Extensive experiments show the superiority of our approach, validating its effectiveness in handling the varied and intricate adverse weather degradations",
    "checked": true,
    "id": "c00f0d02a1661879cac8d33b00f1bdfc88849097",
    "semantic_title": "robust adverse weather removal via spectral-based spatial grouping",
    "citation_count": 0,
    "authors": [
      "Yuhwan Jeong",
      "Yunseo Yang",
      "Youngho Yoon",
      "Kuk-Jin Yoon"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_PLAN_Proactive_Low-Rank_Allocation_for_Continual_Learning_ICCV_2025_paper.html": {
    "title": "PLAN: Proactive Low-Rank Allocation for Continual Learning",
    "volume": "main",
    "abstract": "Continual learning (CL) requires models to continuously adapt to new tasks without forgetting past knowledge. In this work, we propose \\underline P roactive \\underline L ow-rank \\underline A llocatio\\underline N (PLAN), a framework that extends Low-Rank Adaptation (LoRA) to enable efficient and interference-aware fine-tuning of large pre-trained models in CL settings. PLAN proactively manages the allocation of task-specific subspaces by introducing orthogonal basis vectors for each task and optimizing them through a perturbation-based strategy that minimizes conflicts with previously learned parameters. Furthermore, PLAN incorporates a novel selection mechanism that identifies and assigns basis vectors with minimal sensitivity to interference, reducing the risk of degrading past knowledge while maintaining efficient adaptation to new tasks. Empirical results on standard CL benchmarks demonstrate that PLAN consistently outperforms existing methods, establishing a new state-of-the-art for continual learning with foundation models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiequn Wang",
      "Zhan Zhuang",
      "Yu Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wan_EMoTive_Event-guided_Trajectory_Modeling_for_3D_Motion_Estimation_ICCV_2025_paper.html": {
    "title": "EMoTive: Event-guided Trajectory Modeling for 3D Motion Estimation",
    "volume": "main",
    "abstract": "Visual 3D motion estimation aims to infer the motion of 2D pixels in 3D space based on visual cues. The key challenge arises from depth variation induced spatio-temporal motion inconsistencies, disrupting the assumptions of local spatial or temporal motion smoothness in previous motion estimation frameworks. In contrast, event cameras offer new possibilities for 3D motion estimation through continuous adaptive pixel-level responses to scene changes. This paper presents EMoTive, a novel event-based framework that models spatio-temporal trajectories via event-guided non-uniform parametric curves, effectively characterizing locally heterogeneous spatio-temporal motion. Specifically, we first introduce Event Kymograph - an event projection method that leverages a continuous temporal projection kernel and decouples spatial observations to encode fine-grained temporal evolution explicitly. For motion representation, we introduce a density-aware adaptation mechanism to fuse spatial and temporal features under event guidance, coupled with a non-uniform rational curve parameterization framework to adaptively model heterogeneous trajectories. The final 3D motion estimation is achieved through multi-temporal sampling of parametric trajectories, yielding optical flow and depth motion fields. To facilitate evaluation, we introduce CarlaEvent3D, a multi-dynamic synthetic dataset for comprehensive validation. Extensive experiments on both this dataset and a real-world benchmark demonstrate the effectiveness of the proposed method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zengyu Wan",
      "Wei Zhai",
      "Yang Cao",
      "Zhengjun Zha"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_RobuSTereo_Robust_Zero-Shot_Stereo_Matching_under_Adverse_Weather_ICCV_2025_paper.html": {
    "title": "RobuSTereo: Robust Zero-Shot Stereo Matching under Adverse Weather",
    "volume": "main",
    "abstract": "Learning-based stereo matching models struggle in adverse weather conditions due to the scarcity of corresponding training data and the challenges in extracting discriminative features from degraded images. These limitations significantly hinder zero-shot generalization to out-of-distribution weather conditions. In this paper, we propose **RobuSTereo** a novel framework that enhances the zero-shot generalization of stereo matching models under adverse weather by addressing both data scarcity and feature extraction challenges. First, we introduce a diffusion-based simulation pipeline with a stereo consistency module, which generates high-quality stereo data tailored for adverse conditions. By training stereo matching models on our synthetic datasets, we reduce the domain gap between clean and degraded images, significantly improving the models' robustness to unseen weather conditions. The stereo consistency module ensures structural alignment across synthesized image pairs, preserving geometric integrity and enhancing depth estimation accuracy. Second, we design a robust feature encoder that combines a specialized ConvNet with a denoising transformer to extract stable and reliable features from degraded images. The ConvNet captures fine-grained local structures, while the denoising transformer refines global representations, effectively mitigating the impact of noise, low visibility, and weather-induced distortions. This enables more accurate disparity estimation even under challenging visual conditions. Extensive experiments demonstrate that **RobuSTereo** significantly improves the robustness and generalization of stereo matching models across diverse adverse weather scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuran Wang",
      "Yingping Liang",
      "Yutao Hu",
      "Ying Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_SemGes_Semantics-aware_Co-Speech_Gesture_Generation_using_Semantic_Coherence_and_Relevance_ICCV_2025_paper.html": {
    "title": "SemGes: Semantics-aware Co-Speech Gesture Generation using Semantic Coherence and Relevance Learning",
    "volume": "main",
    "abstract": "Creating a virtual avatar with semantically coherent gestures that are aligned with speech is a challenging task. Existing gesture generation research mainly focused on generating rhythmic beat gestures, neglecting the semantic context of the gestures. In this paper, we propose a novel approach for semantic grounding in co-speech gesture generation that integrates semantic information at both fine-grained and global levels. Our approach starts with learning the motion prior through a vector-quantized variational autoencoder. Built on this model, a second-stage module is applied to automatically generate gestures from speech, text-based semantics and speaker identity that ensures consistency between the semantic relevance of generated gestures and co-occurring speech semantics through semantic coherence and relevance modules. Experimental results demonstrate that our approach enhances the realism and coherence of semantic gestures. Extensive experiments and user studies show that our method outperforms state-of-the-art approaches across two benchmarks in co-speech gesture generation in both objective and subjective metrics. The qualitative results of our model can be viewed at https://semgesture.github.io. Our code, dataset and pre-trained models will be shared upon acceptance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lanmiao Liu",
      "Esam Ghaleb",
      "Asli Ozyurek",
      "Zerrin Yumak"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_From_Sharp_to_Blur_Unsupervised_Domain_Adaptation_for_2D_Human_ICCV_2025_paper.html": {
    "title": "From Sharp to Blur: Unsupervised Domain Adaptation for 2D Human Pose Estimation Under Extreme Motion Blur Using Event Cameras",
    "volume": "main",
    "abstract": "Human pose estimation is critical for applications such as rehabilitation, sports analytics, and AR/VR systems. However, rapid motion and low-light conditions often introduce motion blur, significantly degrading pose estimation due to the domain gap between sharp and blurred images. Most datasets assume stable conditions, making models trained on sharp images struggle in blurred environments. To address this, we introduce a novel domain adaptation approach that leverages event cameras, which capture high temporal resolution motion data and are inherently robust to motion blur. Using event-based augmentation, we generate motion-aware blurred images, effectively bridging the domain gap between sharp and blurred domains without requiring paired annotations. Additionally, we develop a student-teacher framework that iteratively refines pseudo-labels, leveraging mutual uncertainty masking to eliminate incorrect labels and enable more effective learning. Experimental results demonstrate that our approach outperforms conventional domain-adaptive human pose estimation methods, achieving robust pose estimation under motion blur without requiring annotations in the target domain. Our findings highlight the potential of event cameras as a scalable and effective solution for domain adaptation in real-world motion blur environments. Our project codes are available at https://github.com/kmax2001/EvSharp2Blur",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youngho Kim",
      "Hoonhee Cho",
      "Kuk-Jin Yoon"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_HyPiDecoder_Hybrid_Pixel_Decoder_for_Efficient_Segmentation_and_Detection_ICCV_2025_paper.html": {
    "title": "HyPiDecoder: Hybrid Pixel Decoder for Efficient Segmentation and Detection",
    "volume": "main",
    "abstract": "Recently, Mask2Former has achieved significant success as a universal image segmentation framework, with its Multi-Scale Deformable Attention (MSDeformAttn) Pixel Decoder becoming a widely adopted component in current segmentation models. However, the inefficiency of MSDeformAttn has become a performance bottleneck for segmenters. To address this, we propose the Hyper Pixel Decoder (HyPiDecoder), an improved Pixel Decoder design that replaces parts of the MSDeformAttn layers with convolution-based FPN layers, introducing explicit locality information and significantly boosting inference speed. Experimental results show that HyPiDecoder can be applied to both universal segmentation models and unified segmentation and detection models, achieving improvements in both speed and accuracy across object detection, semantic, instance, and panoptic segmentation tasks. The Mask DINO model integrated with HyPiDecoder achieves a new SOTA of 58.8 PQ on COCO panoptic segmentation with SwinL-scale backbone and no extra training data, with a 127% increase in inference speed compared to the original model. Code is available at \\href https://github.com/HyPiDecoder/HybridPixelDecoder https://github.com/HyPiDecoder/HybridPixelDecoder",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fengzhe Zhou",
      "Humphrey Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Parast_DDB_Diffusion_Driven_Balancing_to_Address_Spurious_Correlations_ICCV_2025_paper.html": {
    "title": "DDB: Diffusion Driven Balancing to Address Spurious Correlations",
    "volume": "main",
    "abstract": "Deep neural networks trained with Empirical Risk Minimization (ERM) perform well when both training and test data come from the same domain, but they often fail to generalize to out-of-distribution samples. In image classification, these models may rely on spurious correlations that often exist between labels and irrelevant features of images, making predictions unreliable when those features do not exist. We propose a Diffusion Driven Balancing (DDB) technique to generate training samples with text-to-image diffusion models for addressing the spurious correlation problem. First, we compute the best describing token for the visual features pertaining to the causal components of samples by a textual inversion mechanism. Then, leveraging a language segmentation method and a diffusion model, we generate new samples by combining the causal component with the elements from other classes. We also meticulously prune the generated samples based on the prediction probabilities and attribution scores of the ERM model to ensure their correct composition for our objective. Finally, we retrain the ERM model on our augmented dataset. This process reduces the model's reliance on spurious correlations by learning from carefully crafted samples in which this correlation does not exist. Our experiments show that across different benchmarks, our technique achieves better worst-group accuracy than the existing state-of-the-art methods. Our code is available at https://github.com/ArianYp/DDB",
    "checked": true,
    "id": "758fbcdfbf93a63f25321b68c58fd10937bc1738",
    "semantic_title": "ddb: diffusion driven balancing to address spurious correlations",
    "citation_count": 1,
    "authors": [
      "Aryan Yazdan Parast",
      "Basim Azam",
      "Naveed Akhtar"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jin_Feature_Purification_Matters_Suppressing_Outlier_Propagation_for_Training-Free_Open-Vocabulary_Semantic_ICCV_2025_paper.html": {
    "title": "Feature Purification Matters: Suppressing Outlier Propagation for Training-Free Open-Vocabulary Semantic Segmentation",
    "volume": "main",
    "abstract": "Training-free open-vocabulary semantic segmentation has advanced with vision-language models like CLIP, which exhibit strong zero-shot abilities. However, CLIP's attention mechanism often wrongly emphasises specific image tokens, namely outliers, which results in irrelevant over-activation. Existing approaches struggle with these outliers that arise in intermediate layers and propagate through the model, ultimately degrading spatial perception. In this paper, we propose a Self-adaptive Feature Purifier framework (SFP) to suppress propagated outliers and enhance semantic representations for open-vocabulary semantic segmentation. Specifically, based on an in-depth analysis of attention responses between image and class tokens, we design a self-adaptive outlier mitigator to detect and mitigate outliers at each layer for propagated feature purification. In addition, we introduce a semantic-aware attention enhancer to augment attention intensity in semantically relevant regions, which strengthens the purified feature to focus on objects. Further, we introduce a hierarchical attention integrator to aggregate multi-layer attention maps to refine spatially coherent feature representations for final segmentation. Our proposed SFP enables robust outlier suppression and object-centric feature representation, leading to a more precise segmentation. Extensive experiments show that our method achieves state-of-the-art performance and surpasses existing methods by an average of 4.6% mIoU on eight segmentation benchmarks. The code will be released",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuo Jin",
      "Siyue Yu",
      "Bingfeng Zhang",
      "Mingjie Sun",
      "Yi Dong",
      "Jimin Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_Motal_Unsupervised_3D_Object_Detection_by_Modality_and_Task-specific_Knowledge_ICCV_2025_paper.html": {
    "title": "Motal: Unsupervised 3D Object Detection by Modality and Task-specific Knowledge Transfer",
    "volume": "main",
    "abstract": "The performance of unsupervised 3D object classification and bounding box regression relies heavily on the quality of initial pseudo-labels. Traditionally, the labels of classification and regression are represented by a single set of candidate boxes generated by motion or geometry heuristics. However, due to the similarity of many objects to the background in shape or lack of motion, the labels often fail to achieve high accuracy in two tasks simultaneously. Using these labels to directly train the network results in decreased detection performance. To address this challenge, we introduce Motal that performs unsupervised 3D object detection by Modality and task-specific knowledge transfer. Motal decouples the pseudo-labels into two sets of candidates, from which Motal discovers classification knowledge by motion and image appearance prior, and discovers box regression knowledge by geometry prior, respectively. Motal finally transfers all knowledge to a single student network by a TMT (Task-specific Masked Training) scheme, attaining high performance in both classification and regression. Motal can greatly enhance various unsupervised methods by about 2x mAP. For example, on the WOD test set, Motal improves the state-of-the-art CPD by 21.56% mAP L1 (from 20.54% to 42.10%) and 19.90% mAP L2 (from 18.18% to 38.08%). These achievements highlight the significance of our method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hai Wu",
      "Hongwei Lin",
      "Xusheng Guo",
      "Xin Li",
      "Mingming Wang",
      "Cheng Wang",
      "Chenglu Wen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cao_MMAIF_Multi-task_and_Multi-degradation_All-in-One_for_Image_Fusion_with_Language_ICCV_2025_paper.html": {
    "title": "MMAIF: Multi-task and Multi-degradation All-in-One for Image Fusion with Language Guidance",
    "volume": "main",
    "abstract": "Image fusion, a fundamental low-level vision task, aims to integrate multiple image sequences into a single output while preserving as much information as possible from the input. However, existing methods face several significant limitations: 1) requiring task- or dataset-specific models; 2) neglecting real-world image degradations (e.g., noise), which causes failure when processing degraded inputs; 3) operating in pixel space, where attention mechanisms are computationally expensive; and 4) lacking user interaction capabilities.To address these challenges, we propose a unified framework for multi-task, multi-degradation, and language-guided image fusion. Our framework includes two key components: 1) a practical degradation pipeline that simulates real-world image degradations and generates interactive prompts to guide the model; 2) an all-in-one Diffusion Transformer (DiT) operating in latent space, which fuses a clean image conditioned on both the degraded inputs and the generated prompts. Furthermore, we introduce principled modifications to the original DiT architecture to better suit the fusion task. Based on this framework, we develop two versions of the model: Regression-based and Flow Matching-based variants.Extensive qualitative and quantitative experiments demonstrate that our approach effectively addresses the aforementioned limitations and outperforms previous restoration+fusion and all-in-one pipelines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihan Cao",
      "Yu Zhong",
      "Ziqi Wang",
      "Liang-Jian Deng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Pittner_SparseLaneSTP_Leveraging_Spatio-Temporal_Priors_with_Sparse_Transformers_for_3D_Lane_ICCV_2025_paper.html": {
    "title": "SparseLaneSTP: Leveraging Spatio-Temporal Priors with Sparse Transformers for 3D Lane Detection",
    "volume": "main",
    "abstract": "3D lane detection has emerged as a critical challenge in autonomous driving, encompassing identification and localization of lane markings and the 3D road surface. Conventional 3D methods detect lanes from dense birds-eye-viewed (BEV) features, though erroneous transformations often result in a poor feature representation misaligned with the true 3D road surface. While recent sparse lane detectors have surpassed dense BEV approaches, they completely disregard valuable lane-specific priors. Furthermore, existing methods fail to utilize historic lane observations, which yield the potential to resolve ambiguities in situations of poor visibility. To address these challenges, we present SparseLaneSTP, a novel method that integrates both geometric properties of the lane structure and temporal information into a sparse lane transformer. It introduces a new lane-specific spatio-temporal attention mechanism, a continuous lane representation tailored for sparse architectures as well as temporal regularization. Identifying weaknesses of existing 3D lane datasets, we also introduce a precise and consistent 3D lane dataset using a simple yet effective auto-labeling strategy. Our experimental section proves the benefits of our contributions and demonstrates state-of-the-art performance across all detection and error metrics on existing 3D lane detection benchmarks as well as on our novel dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maximilian Pittner",
      "Joel Janai",
      "Mario Faigle",
      "Alexandru Paul Condurache"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Collorone_MonSTeR_a_Unified_Model_for_Motion_Scene_Text_Retrieval_ICCV_2025_paper.html": {
    "title": "MonSTeR: a Unified Model for Motion, Scene, Text Retrieval",
    "volume": "main",
    "abstract": "Intention drives human movement in complex environments, but such movement can only happen if the surrounding context supports it. Despite the intuitive nature of this mechanism, existing research has not yet provided tools to evaluate the alignment between skeletal movement (motion), intention (text), and the surrounding context (scene). In this work, we introduce MonSTeR, the first MOtioN-Scene-TExt Retrieval model. Inspired by the modeling of higher-order relations, MonSTeR constructs a unified latent space by leveraging unimodal and cross-modal representations. This allows MonSTeR to capture the intricate dependencies between modalities, enabling flexible but robust retrieval across various tasks. Our results show that MonSTeR outperforms trimodal models that rely solely on unimodal representations. Furthermore, we validate the alignment of our retrieval scores with human preferences through a dedicated user study. We demonstrate the versatility of MonSTeR's latent space on zero-shot in-Scene Object Placement and Motion Captioning. Code and pre-trained models are available at github.com/colloroneluca/MonSTeR",
    "checked": true,
    "id": "5e6b42a817aa0748497b83b7daf36254f6a311bd",
    "semantic_title": "monster: a unified model for motion, scene, text retrieval",
    "citation_count": 0,
    "authors": [
      "Luca Collorone",
      "Matteo Gioia",
      "Massimiliano Pappa",
      "Paolo Leoni",
      "Giovanni Ficarra",
      "Or Litany",
      "Indro Spinelli",
      "Fabio Galasso"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Semantic_Causality-Aware_Vision-Based_3D_Occupancy_Prediction_ICCV_2025_paper.html": {
    "title": "Semantic Causality-Aware Vision-Based 3D Occupancy Prediction",
    "volume": "main",
    "abstract": "Vision-based 3D semantic occupancy prediction is a critical task in 3D vision that integrates volumetric 3D reconstruction with semantic understanding. Existing methods, however, often rely on modular pipelines. These modules are typically optimized independently or use pre-configured inputs, leading to cascading errors. In this paper, we address this limitation by designing a novel causal loss that enables holistic, end-to-end supervision of the modular 2D-to-3D transformation pipeline. Grounded in the principle of 2D-to-3D semantic causality, this loss regulates the gradient flow from 3D voxel representations back to the 2D features. Consequently, it renders the entire pipeline differentiable, unifying the learning process and making previously non-trainable components fully learnable. Building on this principle, we propose the Semantic Causality-Aware 2D-to-3D Transformation, which comprises three components guided by our causal loss: Channel-Grouped Lifting for adaptive semantic mapping, Learnable Camera Offsets for enhanced robustness against camera perturbations, and Normalized Convolution for effective feature propagation. Extensive experiments demonstrate that our method achieves state-of-the-art performance on the Occ3D benchmark, demonstrating significant robustness to camera perturbations and improved 2D-to-3D semantic consistency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dubing Chen",
      "Huan Zheng",
      "Yucheng Zhou",
      "Xianfei Li",
      "Wenlong Liao",
      "Tao He",
      "Pai Peng",
      "Jianbing Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhong_OmniSAM_Omnidirectional_Segment_Anything_Model_for_UDA_in_Panoramic_Semantic_ICCV_2025_paper.html": {
    "title": "OmniSAM: Omnidirectional Segment Anything Model for UDA in Panoramic Semantic Segmentation",
    "volume": "main",
    "abstract": "Segment Anything Model 2 (SAM2) has emerged as a strong base model in various pinhole imaging segmentation tasks. However, when applying it to 360^\\circ domain, the significant field-of-view (FoV) gap between pinhole (70^\\circ x70^\\circ) and panoramic images (180^\\circ x360^\\circ) poses unique challenges. Two major concerns for this application includes 1) inevitable distortion and object deformation brought by the large FoV disparity between domains; 2) the lack of pixel-level semantic understanding that the original SAM2 cannot provide. To address these issues, we propose a novel OmniSAM framework, which makes the first attempt to apply SAM2 for panoramic semantic segmentation. Specifically, to bridge the first gap, OmniSAM first divides the panorama into sequences of patches. These patches are then treated as image sequences in similar manners as in video segmentation tasks. We then leverage the SAM2's memory mechanism to extract cross-patch correspondences that embeds the cross-FoV dependencies, improving feature continuity and the prediction consistency along mask boundaries. For the second gap, OmniSAM fine-tunes the pretrained image encoder and reutilize the mask decoder for semantic prediction. An FoV-based prototypical adaptation module with dynamic pseudo label update mechanism is also introduced to facilitate the alignment of memory and backbone features, thereby improving model generalization ability across different sizes of source models. Extensive experimental results demonstrate that our method outperforms the state-of-the-art methods by large margins, e.g., 79.06% (10.22%\\uparrow) on SPin8-to-SPan8, 62.46% (6.58%\\uparrow) on CS13-to-DP13",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ding Zhong",
      "Xu Zheng",
      "Chenfei Liao",
      "Yuanhuiyi Lyu",
      "Jialei Chen",
      "Shengyang Wu",
      "Linfeng Zhang",
      "Xuming Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lim_Event-Driven_Storytelling_with_Multiple_Lifelike_Humans_in_a_3D_Scene_ICCV_2025_paper.html": {
    "title": "Event-Driven Storytelling with Multiple Lifelike Humans in a 3D Scene",
    "volume": "main",
    "abstract": "In this work, we propose a framework that creates a lively virtual dynamic scene with contextual motions of multiple humans. Generating multi-human contextual motion requires holistic reasoning over dynamic relationships among human-human and human-scene interactions. We adapt the power of a large language model (LLM) to digest the contextual complexity within textual input and convert the task into tangible subproblems such that we can generate multi-agent behavior beyond the scale that was not considered before. Specifically, our event generator formulates the temporal progression of a dynamic scene into a sequence of small events. Each event calls for a well-defined motion involving relevant characters and objects. Next, we synthesize the motions of characters at positions sampled based on spatial guidance. We employ a high-level module to deliver scalable yet comprehensive context, translating events into relative descriptions that enable the retrieval of precise coordinates. As the first to address this problem at scale and with diversity, we offer a benchmark to assess diverse aspects of contextual reasoning. Benchmark results and user studies show that our framework effectively captures scene context with high scalability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Donggeun Lim",
      "Jinseok Bae",
      "Inwoo Hwang",
      "Seungmin Lee",
      "Hwanhee Lee",
      "Young Min Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_Probabilistic_Inertial_Poser_ProbIP_Uncertainty-aware_Human_Motion_Modeling_from_Sparse_ICCV_2025_paper.html": {
    "title": "Probabilistic Inertial Poser (ProbIP): Uncertainty-aware Human Motion Modeling from Sparse Inertial Sensors",
    "volume": "main",
    "abstract": "Wearable Inertial Measurement Units (IMUs) allow non-intrusive motion tracking, but limited sensor placements can introduce uncertainty in capturing detailed full-body movements. Existing methods mitigate this issue by selecting more physically plausible motion patterns but do not directly address inherent uncertainties in the data. We introduce the Probabilistic Inertial Poser (ProbIP), a novel probabilistic model that transforms sparse IMU data into human motion predictions without physical constraints. ProbIP utilizes RU-Mamba blocks to predict a matrix Fisher distribution over rotations, effectively estimating both rotation matrices and associated uncertainties. To refine motion distribution through layers, our Progressive Distribution Narrowing (PDN) technique enables stable learning across a diverse range of motions. Experimental results demonstrate that ProbIP achieves state-of-the-art performance on multiple public datasets with six and fewer IMU sensors. Our contributions include the development of ProbIP with RU-Mamba blocks for probabilistic motion estimation, applying Progressive Distribution Narrowing (PDN) for uncertainty reduction, and evidence of superior results with six and reduced sensor configurations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Min Kim",
      "Younho Jeon",
      "Sungho Jo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zou_Bi-Level_Optimization_for_Self-Supervised_AI-Generated_Face_Detection_ICCV_2025_paper.html": {
    "title": "Bi-Level Optimization for Self-Supervised AI-Generated Face Detection",
    "volume": "main",
    "abstract": "AI-generated face detectors trained via supervised learning typically rely on synthesized images from specific generators, limiting their generalization to emerging generative techniques. To overcome this limitation, we introduce a self-supervised method based on bi-level optimization. In the inner loop, we pretrain a vision encoder only on photographic face images using a set of linearly weighted pretext tasks: classification of categorical exchangeable image file format (EXIF) tags, ranking of ordinal EXIF tags, and detection of artificial face manipulations. The outer loop then optimizes the relative weights of these pretext tasks to enhance the coarse-grained detection of manipulated faces, serving as a proxy task for identifying AI-generated faces. In doing so, it aligns self-supervised learning more closely with the ultimate goal of AI-generated face detection. Once pretrained, the encoder remains fixed, and AI-generated faces are detected either as anomalies under a Gaussian mixture model fitted to photographic face features or by a lightweight two-layer perceptron serving as a binary classifier. Extensive experiments demonstrate that our detectors significantly outperform existing approaches in both one-class and binary classification settings, exhibiting strong generalization to unseen generators",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mian Zou",
      "Nan Zhong",
      "Baosheng Yu",
      "Yibing Zhan",
      "Kede Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Fan_RIOcc_Efficient_Cross-Modal_Fusion_Transformer_with_Collaborative_Feature_Refinement_for_ICCV_2025_paper.html": {
    "title": "RIOcc: Efficient Cross-Modal Fusion Transformer with Collaborative Feature Refinement for 3D Semantic Occupancy Prediction",
    "volume": "main",
    "abstract": "The multi-modal 3D semantic occupancy task provides a comprehensive understanding of the scene and has received considerable attention in the field of autonomous driving. However, existing methods mainly focus on processing large-scale voxels, which bring high computational costs and degrade details. Additionally, they struggle to accurately capture occluded targets and distant information. In this paper, we propose a novel LiDAR-Camera 3D semantic occupancy prediction framework called RIOcc, with collaborative feature refinement and multi-scale cross-modal fusion transformer. Specifically, RIOcc encodes multi-modal data into a unified Bird's Eye View (BEV) space, which reduces computational complexity and enhances the efficiency of feature alignment. Then, multi-scale feature processing substantially expands the receptive fields. Meanwhile, in the LiDAR branch, we design the Dual-branch Pooling (DBP) to adaptively enhance geometric features across both the Channel and Grid dimensions. In the camera branch, the Wavelet and Semantic Encoders are developed to extract high-level semantic features with abundant edge and structural information. Finally, to facilitate effective cross-modal complementarity, we develop the Deformable Dual-Attention (DDA) module. Extensive experiments demonstrate that RIOcc achieves state-of-the-art performance, with 54.2 mIoU and 25.9 mIoU on the Occ3D-nuScenes and nuScenes-Occupancy datasets, respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baojie Fan",
      "Xiaotian Li",
      "Yuhan Zhou",
      "Yuyu Jiang",
      "Jiandong Tian",
      "Huijie Fan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_Golden_Noise_for_Diffusion_Models_A_Learning_Framework_ICCV_2025_paper.html": {
    "title": "Golden Noise for Diffusion Models: A Learning Framework",
    "volume": "main",
    "abstract": "Text-to-image diffusion model is a popular paradigm that synthesizes personalized images by providing a text prompt and a random Gaussian noise. While people observe that some noises are \"golden noises\" that can achieve better text-image alignment and higher human preference than others, we still lack a machine learning framework to obtain those golden noises. To learn golden noises for diffusion sampling, we mainly make three contributions in this paper. First, we identify a new concept termed the noise prompt, which aims at turning a random Gaussian noise into a golden noise by adding a small desirable perturbation derived from the text prompt. Following the concept, we first formulate the noise prompt learning framework that systematically learns \"prompted\" golden noise associated with a text prompt for diffusion models. Second, we design a noise prompt data collection pipeline and collect a large-scale noise prompt dataset (NPD) that contains 100k pairs of random noises and golden noises with the associated text prompts. With the prepared NPD as the training dataset, we trained a small noise prompt network (NPNet) that can directly learn to transform a random noise into a golden noise. The learned golden noise perturbation can be considered as a kind of prompt for noise, as it is rich in semantic information and tailored to the given text prompt. Third, our extensive experiments demonstrate the impressive effectiveness and generalization of NPNet on improving the quality of synthesized images across various diffusion models, including SDXL, DreamShaper-xl-v2-turbo, and Hunyuan-DiT. Moreover, NPNet is a small and efficient controller that acts as a plug-and-play module with very limited additional inference and computational costs, as it just provides a golden noise instead of a random noise without accessing the original pipeline",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zikai Zhou",
      "Shitong Shao",
      "Lichen Bai",
      "Shufei Zhang",
      "Zhiqiang Xu",
      "Bo Han",
      "Zeke Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_CharaConsist_Fine-Grained_Consistent_Character_Generation_ICCV_2025_paper.html": {
    "title": "CharaConsist: Fine-Grained Consistent Character Generation",
    "volume": "main",
    "abstract": "In text-to-image generation, producing a series of consistent contents that preserve the same identity is highly valuable for real-world applications. Although a few works have explored training-free methods to enhance the consistency of generated subjects, we observe that they suffer from the following problems.First, they fail to maintain consistent background details, which limits their applicability. Furthermore, when the foreground character undergoes large motion variations, inconsistencies in identity and clothing details become evident. To address these problems, we propose CharaConsist, which employs point-tracking attention and adaptive token merge along with decoupled control of the foreground and background.CharaConsist enables fine-grained consistency for both foreground and background, supporting the generation of one character in continuous shots within a fixed scene or in discrete shots across different scenes.Moreover, CharaConsist is the first consistent generation method tailored for text-to-image DiT model. Its ability to maintain fine-grained consistency, combined with the larger capacity of latest base model, enables it to produce high-quality visual outputs, broadening its applicability to a wider range of real-world scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengyu Wang",
      "Henghui Ding",
      "Jianing Peng",
      "Yao Zhao",
      "Yunpeng Chen",
      "Yunchao Wei"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_RetinexMCNet_A_Memory_Controller_Dominated_Network_for_Low-Light_Video_Enhancement_ICCV_2025_paper.html": {
    "title": "RetinexMCNet: A Memory Controller Dominated Network for Low-Light Video Enhancement Based on Retinex",
    "volume": "main",
    "abstract": "Low-light video enhancement (LLVE) aims to restore videos degraded by insufficient illumination.While existing methods have demonstrated their effectiveness, they often face challenges with intra-frame noise, overexposure, and inter-frame inconsistency since they fail to exploit the temporal continuity across frames.Inspired by the progressive video understanding mechanism of human, we propose a novel end-to-end two-stage memory controller (MC) dominated network (RetinexMCNet). Specifically, we first define the overall optimization objective for Retinex-based LLVE, and accordingly design our framework.In stage one, aided by a dual-perspective Lightness-Texture Stability (LTS) loss, we perform per-frame enhancement without the MC, which uses a channel-aware Illumination Adjustment Module (IAM) and an illumination-guided Reflectance Denoising Module (RDM) based on Retinex theory to mitigate intra-frame noise and overexposure.In stage two, we activate the MC to simulate human temporal memory and integrate it with high-quality single frames for global consistency.Extensive qualitative and quantitative experiments on common low-light sRGB datasets demonstrate our method significantly outperforms state-of-the-art approaches. Code is available at xxx/xxx/xxx",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meiao Wang",
      "Xuejing Kang",
      "Yaxi Lu",
      "Jie Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Qiao_Learnable_Fractional_Reaction-Diffusion_Dynamics_for_Under-Display_ToF_Imaging_and_Beyond_ICCV_2025_paper.html": {
    "title": "Learnable Fractional Reaction-Diffusion Dynamics for Under-Display ToF Imaging and Beyond",
    "volume": "main",
    "abstract": "Under-display ToF imaging aims to achieve accurate depth sensing through a ToF camera placed beneath a screen panel. However, transparent OLED (TOLED) layers introduce severe degradations--such as signal attenuation, multi-path interference (MPI), and temporal noise--that significantly compromise depth quality. To alleviate this drawback, we propose Learnable Fractional Reaction-Diffusion Dynamics (LFRD^2), a hybrid framework that combines the expressive power of neural networks with the interpretability of physical modeling. Specifically, we implement a time-fractional reaction-diffusion module that enables iterative depth refinement with dynamically generated differential orders, capturing long-term dependencies. In addition, we introduce an efficient continuous convolution operator via coefficient prediction and repeated differentiation to further improve restoration quality. Experiments on four benchmark datasets demonstrate the effectiveness of our approach. The code is publicly available at https://github.com/wudiqx106/LFRD2",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Qiao",
      "Matteo Poggi",
      "Xing Wei",
      "Pengchao Deng",
      "Yanhui Zhou",
      "Stefano Mattoccia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yu_Anomaly_Detection_of_Integrated_Circuits_Package_Substrates_Using_the_Large_ICCV_2025_paper.html": {
    "title": "Anomaly Detection of Integrated Circuits Package Substrates Using the Large Vision Model SAIC: Dataset Construction, Methodology, and Application",
    "volume": "main",
    "abstract": "Anomaly detection plays a crucial role in the industrial sector, especially in ensuring the quality of integrated circuits (IC), which are critical for product reliability and performance. With increasing demands for higher quality standards, anomaly detection during the IC manufacturing process has become a significant research focus. However, the progress of IC anomaly detection is hampered by the scarcity of defective samples and the shortage of well-defined annotations. To address this challenge, this paper focuses on the research in the field of IC, especially on ceramic package substrates (CPS). We construct a systematic automated optical inspection (AOI) equipment, and based on this, collected large-scale CPS 2D images to build a novel anomaly detection dataset (CPS2D-AD), which offers copious samples with precise annotations, including category, mask, and bounding box. To the best of our knowledge, CPS2D-AD is the largest dataset in the field of IC. Meanwhile, we conduct an extensive benchmark of CPS2D-AD, intending to supplement existing research by providing a baseline for the detection and localization of anomalies in high-resolution data of ceramic package substrates. In addition, we have developed a novel large vision model, Segment Any Integrated Circuits (SAIC), by embedding-based distillation mechanism based on CPS2D-AD datasets. Our CPS2D-AD is the first open-source anomaly detection dataset about ceramic package substrates, which can be accessed at https://github.com/Bingyang0410/CPS2D-AD",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruiyun Yu",
      "Bingyang Guo",
      "Haoyuan Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cao_Refer_to_Any_Segmentation_Mask_Group_With_Vision-Language_Prompts_ICCV_2025_paper.html": {
    "title": "Refer to Any Segmentation Mask Group With Vision-Language Prompts",
    "volume": "main",
    "abstract": "Recent image segmentation models have advanced to segment images into high-quality masks for visual entities, and yet they cannot provide comprehensive semantic understanding for complex queries based on both language and vision. This limitation reduces their effectiveness in applications that require user-friendly interactions driven by vision-language prompts. To bridge this gap, we introduce a novel task of omnimodal referring expression segmentation (ORES). In this task, a model produces a group of masks based on arbitrary prompts specified by text only or text plus reference visual entities. To address this new challenge, we propose a novel framework to \"Refer to Any Segmentation Mask Group\" (RAS), which augments segmentation models with complex multimodal interactions and comprehension via a mask-centric large multimodal model. For training and benchmarking ORES models, we create datasets MaskGroups-2M and MaskGroups-HQ to include diverse mask groups specified by text and reference entities. Through extensive evaluation, we demonstrate superior performance of RAS on our new ORES task, as well as classic referring expression segmentation (RES) and generalized referring expression segmentation (GRES) tasks. Project page: https://Ref2Any.github.io",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengcao Cao",
      "Zijun Wei",
      "Jason Kuen",
      "Kangning Liu",
      "Lingzhi Zhang",
      "Jiuxiang Gu",
      "HyunJoon Jung",
      "Liang-Yan Gui",
      "Yu-Xiong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Guo_Integrating_Visual_Interpretation_and_Linguistic_Reasoning_for_Geometric_Problem_Solving_ICCV_2025_paper.html": {
    "title": "Integrating Visual Interpretation and Linguistic Reasoning for Geometric Problem Solving",
    "volume": "main",
    "abstract": "Current large vision-language models (LVLMs) typically employ a connector module to link visual features with text embeddings of large language models (LLMs) and use end-to-end training to achieve multi-modal understanding in a unified process. Well alignment needs high-quality pre-training data and a carefully designed training process. Current LVLMs face challenges when addressing complex vision-language reasoning tasks, with their reasoning capabilities notably lagging behind those of LLMs. This paper proposes a paradigm shift: instead of training end-to-end vision-language reasoning models, we advocate for developing a decoupled reasoning framework based on existing visual interpretation specialists and text-based reasoning LLMs. Our approach leverages (1) a dedicated vision-language model to transform the visual content of images into textual descriptions and (2) an LLM to perform reasoning according to the visual-derived text and the original question. This method presents a cost-efficient solution for multi-modal model development by optimizing existing models to work collaboratively, avoiding end-to-end development of vision-language models from scratch. By transforming images into language model-compatible text representations, it facilitates future low-cost and flexible upgrades to upcoming powerful LLMs. We introduce an outcome-rewarded joint-tuning strategy to optimize the cooperation between the visual interpretation and linguistic reasoning model. Evaluation results on vision-language benchmarks demonstrate that the decoupled reasoning framework outperforms recent LVLMs. Our approach yields particularly significant performance gains on visually intensive geometric mathematics problems. The code is available: https://github.com/guozix/DVLR",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixian Guo",
      "Ming Liu",
      "Qilong Wang",
      "Zhilong Ji",
      "Jinfeng Bai",
      "Lei Zhang",
      "Wangmeng Zuo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/He_Dual-Rate_Dynamic_Teacher_for_Source-Free_Domain_Adaptive_Object_Detection_ICCV_2025_paper.html": {
    "title": "Dual-Rate Dynamic Teacher for Source-Free Domain Adaptive Object Detection",
    "volume": "main",
    "abstract": "Source-Free Domain Adaptive Object Detection transfers knowledge from a labeled source domain to an unlabeled target domain while preserving data privacy by restricting access to source data during adaptation. Existing approaches predominantly leverage the Mean Teacher framework for self-training in the target domain. The exponential moving average (EMA) mechanism in the Mean Teacher stabilizes the training by averaging the student weights over training steps. However, in domain adaptation, its inherent lag in responding to emerging knowledge can hinder the rapid adaptation of the student to target-domain shifts. To address this challenge, Dual-rate Dynamic Teacher (DDT) with Asynchronous EMA (AEMA) is proposed, which implements group-wise parameter updates. In contrast to traditional EMA, which simultaneously updates all parameters, AEMA dynamically decomposes teacher parameters into two functional groups based on their contributions to capture the domain shift. By applying a distinct smoothing coefficient to two groups, AEMA simultaneously enables fast adaptation and historical knowledge retention. Comprehensive experiments carried out on three widely used traffic benchmarks have demonstrated that the proposed DDT achieves superior performance, outperforming SOTA methods by a clear margin. The codes are available at https://github.com/qih96/DDT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi He",
      "Xiao Wu",
      "Jun-Yan He",
      "Shuai Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_How_Do_Multimodal_Large_Language_Models_Handle_Complex_Multimodal_Reasoning_ICCV_2025_paper.html": {
    "title": "How Do Multimodal Large Language Models Handle Complex Multimodal Reasoning? Placing Them in An Extensible Escape Game",
    "volume": "main",
    "abstract": "The rapid advancing of Multimodal Large Language Models (MLLMs) has spurred interest in complex multimodal reasoning tasks in the real-world and virtual environment, which require coordinating multiple abilities, including visual perception, visual reasoning, spatial awareness, and target deduction. However, existing evaluations primarily assess the final task completion, often degrading assessments to isolated abilities such as visual grounding and visual question answering. Less attention is given to comprehensively and quantitatively analyzing reasoning process in multimodal environments, which is crucial for understanding model behaviors and underlying reasoning mechanisms beyond merely task success. To address this, we introduce MM-Escape, an extensible benchmark for investigating multimodal reasoning, inspired by real-world escape games. MM-Escape emphasizes intermediate model behaviors alongside final task completion. To achieve this, we develop EscapeCraft, a customizable and open environment that enables models to engage in free-form exploration for assessing multimodal reasoning. Extensive experiments show that MLLMs, regardless of scale, can successfully complete the simplest room escape tasks, with some exhibiting human-like exploration strategies. Yet, performance dramatically drops as task difficulty increases. Moreover, we observe that models severely suffer from accidental success, and that performance bottlenecks vary across models, revealing distinct failure modes and limitations in their multimodal reasoning abilities, such as repetitive trajectories without adaptive exploration, getting stuck in corners due to poor visual spatial awareness, and ineffective use of acquired props, such as the key. We hope our work sheds light on new challenges in multimodal reasoning, and uncovers potential improvements in MLLMs capabilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyue Wang",
      "Yurui Dong",
      "Fuwen Luo",
      "Minyuan Ruan",
      "Zhili Cheng",
      "Chi Chen",
      "Peng Li",
      "Yang Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_UST-SSM_Unified_Spatio-Temporal_State_Space_Models_for_Point_Cloud_Video_ICCV_2025_paper.html": {
    "title": "UST-SSM: Unified Spatio-Temporal State Space Models for Point Cloud Video Modeling",
    "volume": "main",
    "abstract": "Point cloud videos capture dynamic 3D motion while reducing the effects of lighting and viewpoint variations, making them highly effective for recognizing subtle and continuous human actions. Although Selective State Space Models (SSMs) have shown good performance in sequence modeling with linear complexity, the spatio-temporal disorder of point cloud videos hinders their unidirectional modeling when directly unfolding the point cloud video into a 1D sequence through temporally sequential scanning. To address this challenge, we propose the Unified Spatio-Temporal State Space Model (UST-SSM), which extends the latest advancements in SSMs to point cloud videos. Specifically, we introduce Spatial-Temporal Selection Scanning (STSS), which reorganizes unordered points into semantic-aware sequences through prompt-guided clustering, thereby enabling the effective utilization of points that are spatially and temporally distant yet similar within the sequence. For missing 4D geometric and motion details, Spatio-Temporal Structure Aggregation (STSA) aggregates spatio-temporal features and compensates. To improve temporal interaction within the sampled sequence, Temporal Interaction Sampling (TIS) enhances fine-grained temporal dependencies through non-anchor frame utilization and expanded receptive fields. Experimental results on the MSR-Action3D, NTU RGB+D, and Synthia 4D datasets validate the effectiveness of our method. Our code is available at https://github.com/wangzy01/UST-SSM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peiming Li",
      "Ziyi Wang",
      "Yulin Yuan",
      "Hong Liu",
      "Xiangming Meng",
      "Junsong Yuan",
      "Mengyuan Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/He_Recover_Biological_Structure_from_Sparse-View_Diffraction_Images_with_Neural_Volumetric_ICCV_2025_paper.html": {
    "title": "Recover Biological Structure from Sparse-View Diffraction Images with Neural Volumetric Prior",
    "volume": "main",
    "abstract": "Volumetric reconstruction of label-free living cells from non-destructive optical microscopic images reveals cellular metabolism in native environments. However, current optical tomography techniques require hundreds of 2D images to reconstruct a 3D volume, hindering them from intravital imaging of biological samples undergoing rapid dynamics. This poses a challenge of reconstructing the entire volume of semi-transparent biological samples from sparse views due to the restricted viewing angles of microscopes and the limited number of measurements. In this work, we develop Neural Volumetric Prior (NVP) for high-fidelity volumetric reconstruction of semi-transparent biological samples from sparse-view microscopic images. NVP integrates explicit and implicit neural representations and incorporates the physical prior of diffractive optics. We validate NVP on both simulated data and experimentally captured microscopic images. Compared to previous methods, NVP significantly reduces the required number of images by nearly 50-fold and processing time by 3-fold while maintaining state-of-the-art performance.NVP is the first technique to enable volumetric reconstruction of label-free biological samples from sparse-view microscopic images, paving the way for real-time 3D imaging of dynamically changing biological samples",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Renzhi He",
      "Haowen Zhou",
      "Yubei Chen",
      "Yi Xue"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Dai_PropVG_End-to-End_Proposal-Driven_Visual_Grounding_with_Multi-Granularity_Discrimination_ICCV_2025_paper.html": {
    "title": "PropVG: End-to-End Proposal-Driven Visual Grounding with Multi-Granularity Discrimination",
    "volume": "main",
    "abstract": "Recent advances in visual grounding have largely shifted away from traditional proposal-based two-stage frameworks due to their inefficiency and high computational complexity, favoring end-to-end direct reference paradigms. However, these methods rely exclusively on the referred target for supervision, overlooking the potential benefits of prominent prospective targets. Moreover, existing approaches often fail to incorporate multi-granularity discrimination, which is crucial for robust object identification in complex scenarios. To address these limitations, we propose PropVG, an end-to-end proposal-based framework that, to the best of our knowledge, is the first to seamlessly integrate foreground object proposal generation with referential object comprehension without requiring additional detectors. Furthermore, we introduce a Contrastive-based Refer Scoring (CRS) module, which employs contrastive learning at both sentence and word levels to enhance the model's capability in understanding and distinguishing referred objects. Additionally, we design a Multi-granularity Target Discrimination (MTD) module that fuses object- and semantic-level information to improve the recognition of absent targets. Extensive experiments on gRefCOCO (GREC/GRES), Ref-ZOM, R-RefCOCO/+/g, and RefCOCO/+/g (REC/RES) benchmarks demonstrate the effectiveness of PropVG",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ming Dai",
      "Wenxuan Cheng",
      "Jiedong Zhuang",
      "Jiang-jiang Liu",
      "Hongshen Zhao",
      "Zhenhua Feng",
      "Wankou Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yuan_StyleSRN_Scene_Text_Image_Super-Resolution_with_Text_Style_Embedding_ICCV_2025_paper.html": {
    "title": "StyleSRN: Scene Text Image Super-Resolution with Text Style Embedding",
    "volume": "main",
    "abstract": "Scene text image super-resolution (STISR) focuses on enhancing the clarity and readability of low-resolution text images. Existing methods often rely on text probability distribution priors derived from text recognizers to guide the super-resolution process. While effective in capturing general structural information of text, these priors lack the ability to preserve specific text style details, such as font, stereoscopic effect and spatial transformation, leading to a loss of visual quality and stylistic consistency in the super-resolved images. To address these limitations, we propose a Style embedding-based scene text image Super-Resolution Network (StyleSRN), which introduces a text style embedding mechanism to preserve and enhance text style features during the super-resolution process. The proposed architecture includes a Style Enhancement Block for capturing multi-scale cross-channel dependencies, and a Style Content Fusion Block that effectively integrates text content with style information, ensuring that the structure and style of the restored text are not distorted. Furthermore, we introduce a Text Style Loss based on the Gram matrix to supervise the reconstruction process at the style level, thereby maintaining the stylistic consistency of the restored text images. Extensive experiments on the TextZoom dataset and five scene text recognition benchmarks demonstrate the superiority of our method. The code will be released in the future",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengrong Yuan",
      "Runmin Wang",
      "Ke Hao",
      "Xuqi Ma",
      "Changxin Gao",
      "Li Liu",
      "Nong Sang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sheng_SpatialSplat_Efficient_Semantic_3D_from_Sparse_Unposed_Images_ICCV_2025_paper.html": {
    "title": "SpatialSplat: Efficient Semantic 3D from Sparse Unposed Images",
    "volume": "main",
    "abstract": "A major breakthrough in 3D reconstruction is the feedforward paradigm to generate pixel-wise 3D points or Gaussian primitives from sparse, unposed images. To further incorporate semantics while avoiding the significant memory and storage costs of high-dimensional semantic features, existing methods extend this paradigm by associating each primitive with a compressed semantic feature vector.However, these methods have two major limitations: (a) the naively compressed feature compromises expressiveness, affecting the model's ability to capture fine-grained semantics, and (b) the pixel-wise primitive prediction introduces redundancy in overlapping areas, causing unnecessary memory overhead. To this end, we introduce SpatialSplat, a feedforward framework that produces redundancy-aware Gaussians and capitalizes on a dual-field semantic representation. Particularly, with the insight that primitives within the same instance exhibit high semantic consistency, we decompose the semantic representation into a coarse feature field that encodes uncompressed semantics with minimal primitives, and a fine-grained yet low-dimensional feature field that captures detailed inter-instance relationships. Moreover, we propose a selective Gaussian mechanism, which retains only essential Gaussians in the scene, effectively eliminating redundant primitives. Our proposed Spatialsplat learns accurate semantic information and detailed instances prior with more compact 3D Gaussians, making semantic 3D reconstruction more applicable. We conduct extensive experiments to evaluate our method, demonstrating a remarkable 60% reduction in scene representation parameters while achieving superior performance over state-of-the-art methods. The code will be made available for future investigation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Sheng",
      "Jiajun Deng",
      "Xinran Zhang",
      "Yu Zhang",
      "Bei Hua",
      "Yanyong Zhang",
      "Jianmin Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Learning_Visual_Proxy_for_Compositional_Zero-Shot_Learning_ICCV_2025_paper.html": {
    "title": "Learning Visual Proxy for Compositional Zero-Shot Learning",
    "volume": "main",
    "abstract": "Compositional Zero-Shot Learning (CZSL) aims to recognize novel attribute-object compositions by leveraging knowledge from seen compositions. Existing methods typically align textual prototypes with visual features using Vision-Language Models (VLMs), but they face two key limitations: (1) modality gaps hinder the ability to distinguish semantically similar attribute-object pairs, and (2) textual prototypes alone lack the fine-grained visual cues needed for accurate recognition. To address these challenges, we propose Visual Proxy Learning, a method that reduces modality gaps and enhances compositional generalization by initializing visual proxies for attributes, objects, and their compositions from text representations and optimizing the visual space to better capture fine-grained visual cues. To further strengthen cross-modal understanding, we introduce Cross-Modal Joint Learning (CMJL), which enforces consistency between text-image embeddings and fine-grained visual representations. This dual strategy improves generalization to unseen compositions and enhances the discrimination of similar pairs. Extensive experiments demonstrate that our method achieves state-of-the-art performance in closed-world settings and competitive results in open-world scenarios across four CZSL benchmarks, validating its effectiveness in improving compositional generalization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiyu Zhang",
      "Cheng Yan",
      "Yang Liu",
      "Chenchen Jing",
      "Lei Zhou",
      "Wenjun Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ramos_Processing_and_acquisition_traces_in_visual_encoders_What_does_CLIP_ICCV_2025_paper.html": {
    "title": "Processing and acquisition traces in visual encoders: What does CLIP know about your camera?",
    "volume": "main",
    "abstract": "Prior work has analyzed the robustness of visual encoders to image transformations and corruptions, particularly in cases where such alterations are not seen during training. When this occurs, they introduce a form of distribution shift at test time, often leading to performance degradation. The primary focus has been on severe corruptions that, when applied aggressively, distort useful signals necessary for accurate semantic predictions. We take a different perspective by analyzing parameters of the image acquisition process and transformations that may be subtle or even imperceptible to the human eye. We find that such parameters are systematically encoded in the learned visual representations and can be easily recovered. More strikingly, their presence can have a profound impact, either positively or negatively, on semantic predictions. This effect depends on whether there is a strong correlation or anti-correlation between semantic labels and these acquisition-based or processing-based labels. Our code and data are available at: https://github.com/ryan-caesar-ramos/visual-encoder-traces",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryan Ramos",
      "Vladan Stojnić",
      "Giorgos Kordopatis-Zilos",
      "Yuta Nakashima",
      "Giorgos Tolias",
      "Noa Garcia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zheng_Holistic_Tokenizer_for_Autoregressive_Image_Generation_ICCV_2025_paper.html": {
    "title": "Holistic Tokenizer for Autoregressive Image Generation",
    "volume": "main",
    "abstract": "Vanilla autoregressive image generation models generate visual tokens step-by-step, limiting their ability to capture holistic relationships among token sequences. Moreover, because most visual tokenizers map local image patches into latent tokens, global information is limited. To address this, we introduce Hita, a novel image tokenizer for autoregressive (AR) image generation. It introduces a holistic-to-local tokenization scheme with learnable holistic queries and local patch tokens. Hita incorporates two key strategies to better align with the AR generation process: 1) arranging a sequential structure with holistic tokens at the beginning, followed by patch-level tokens, and using causal attention to maintain awareness of previous tokens; and 2) adopting a lightweight fusion module before feeding the de-quantized tokens into the decoder to control information flow and prioritize holistic tokens. Extensive experiments show that Hita accelerates the training speed of AR generators and outperforms those trained with vanilla tokenizers, achieving 2.59 FID and 281.9 IS on the ImageNet benchmark. Detailed analysis of the holistic representation highlights its ability to capture global image properties, such as textures, materials, and shapes. Additionally, Hita also demonstrates effectiveness in zero-shot style transfer and image in-painting. The code is available at https://github.com/CVMI-Lab/Hita",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anlin Zheng",
      "Haochen Wang",
      "Yucheng Zhao",
      "Weipeng Deng",
      "Tiancai Wang",
      "Xiangyu Zhang",
      "Xiaojuan Qi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhu_ConsistentCity_Semantic_Flow-guided_Occupancy_DiT_for_Temporally_Consistent_Driving_Scene_ICCV_2025_paper.html": {
    "title": "ConsistentCity: Semantic Flow-guided Occupancy DiT for Temporally Consistent Driving Scene Synthesis",
    "volume": "main",
    "abstract": "Scene synthesis plays a crucial role in autonomous driving by addressing data scarcity and close-loop validation. Current approaches struggle to maintain temporal consistency in synthesized videos while preserving fine-grained details. We introduce ConsistentCity, a two-stage framework with a novel Semantic Flow-guided Diffusion Transformers (SF-DiT) that convert sequential BEV semantic maps into temporally consistent driving videos. Operating in a pretrained occupancy VQ-VAE latent space, our SF-DiT generates temporally consistent 3D occupancy, which provides guidance for controlled image and video diffusion for scene synthesis. To address the temporal consistency, SF-DiT enhances standard DiT blocks with temporal semantic modeling through two designs: (1) A Semantic Flow Estimation module capturing scene motions (flow, uncertainty, and classification) from sequential BEV semantic maps, and (2) A Semantic Flow-Modulated Cross-Attention module that dynamically adapts attention based on semantic flow patterns. This integration of semantic flow modeling in DiT enables consistent scene evolution understanding. Evaluations of image and video synthesis on nuScenes dataset demonstrate state-of-the-art performance with FID 8.3 and FVD 73.6, and superior temporal occupancy generation results on nuCraft and OpenOccupancy benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjin Zhu",
      "Xiaogang Wang",
      "Hongsheng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_G2PDiffusion_Cross-Species_Genotype-to-Phenotype_Prediction_via_Evolutionary_Diffusion_ICCV_2025_paper.html": {
    "title": "G2PDiffusion: Cross-Species Genotype-to-Phenotype Prediction via Evolutionary Diffusion",
    "volume": "main",
    "abstract": "Understanding how genes influence phenotype across species is a fundamental challenge in genetic engineering, which will facilitate advances in various fields such as crop breeding, conservation biology, and personalized medicine. However, current phenotype prediction models are limited to individual species and expensive phenotype labeling process, making the genotype-to-phenotype prediction a highly domain-dependent and data-scarce problem. To this end, we suggest taking images as morphological proxies, facilitating cross-species generalization through large-scale multimodal pretraining. We propose the first genotype-to-phenotype diffusion model (G2PDiffusion) that generates morphological images from DNA considering two critical evolutionary signals, i.e., multiple sequence alignments (MSA) and environmental contexts. The model contains three novel components: 1) a MSA retrieval engine that identifies conserved and co-evolutionary patterns; 2) an environment-aware MSA conditional encoder that effectively models complex genotype-environment interactions; and 3) an adaptive phenomic alignment module to improve genotype-phenotype consistency. Extensive experiments show that integrating evolutionary signals with environmental context enriches the model's understanding of phenotype variability across species, thereby offering a valuable and promising exploration into advanced AI-assisted genomic analysis",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengdi Liu",
      "Zhangyang Gao",
      "Hong Chang",
      "Stan Z. Li",
      "Shiguang Shan",
      "Xilin Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ghezloo_PathFinder_A_Multi-Modal_Multi-Agent_System_for_Medical_Diagnostic_Decision-Making_Applied_ICCV_2025_paper.html": {
    "title": "PathFinder: A Multi-Modal Multi-Agent System for Medical Diagnostic Decision-Making Applied to Histopathology",
    "volume": "main",
    "abstract": "Diagnosing diseases through histopathology whole slide images (WSIs) is fundamental in modern pathology but is challenged by the gigapixel scale and complexity of WSIs. Trained histopathologists overcome this challenge by navigating the WSI, looking for relevant patches, taking notes, and compiling them to produce a final holistic diagnostic. Traditional AI approaches, such as multiple instance learning and transformer-based models, fail short of such a holistic, iterative, multi-scale diagnostic procedure, limiting their adoption in the real-world. We introduce PathFinder, a multi-modal, multi-agent framework that emulates the decision-making process of expert pathologists. PathFinder integrates four AI agents, the Triage Agent, Navigation Agent, Description Agent, and Diagnosis Agent, that collaboratively navigate WSIs, gather evidence, and provide comprehensive diagnoses with natural language explanations. The Triage Agent classifies the WSI as benign or risky; if risky, the Navigation and Description Agents iteratively focus on significant regions, generating importance maps and descriptive insights of sampled patches. Finally, the Diagnosis Agent synthesizes the findings to determine the patient's diagnostic classification. Our Experiments show that PathFinder outperforms state-of-the-art methods in skin melanoma diagnosis by 8% while offering inherent explainability through natural language descriptions of diagnostically relevant patches. Qualitative analysis by pathologists shows that the Description Agent's outputs are of high quality and comparable to GPT-4o. PathFinder is also the first AI-based system to surpass the average performance of pathologists in this challenging melanoma classification task by 9%, setting a new record for efficient, accurate, and interpretable AI-assisted diagnostics in pathology. Our Data, code and models will be made available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fatemeh Ghezloo",
      "Mehmet Saygin Seyfioglu",
      "Rustin Soraki",
      "Wisdom O. Ikezogwo",
      "Beibin Li",
      "Tejoram Vivekanandan",
      "Joann G. Elmore",
      "Ranjay Krishna",
      "Linda Shapiro"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_VLABench_A_Large-Scale_Benchmark_for_Language-Conditioned_Robotics_Manipulation_with_Long-Horizon_ICCV_2025_paper.html": {
    "title": "VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks",
    "volume": "main",
    "abstract": "General-purposed embodied agents are designed to understand the users' natural instructions or intentions and act precisely to complete universal tasks. Recently, methods based on foundation models especially Vision-Language-Action models (VLAs) have shown a substantial potential to solve language-conditioned manipulation (LCM) tasks well. However, existing benchmarks do not adequately meet the needs of VLAs and relative algorithms. To better define such general-purpose tasks in the context of LLMs and advance the research in VLAs, we present VLABench, an open-source benchmark for evaluating universal LCM task learning. VLABench provides 100 carefully designed categories of tasks, with strong randomization in each category of task and a total of 2000+ objects. VLABench stands out from previous benchmarks in four key aspects: 1) tasks requiring world knowledge and common sense transfer, 2) natural language instructions with implicit human intentions rather than templates, 3) long-horizon tasks demanding multi-step reasoning, and 4) evaluation of both action policies and language model capabilities. The benchmark assesses multiple competencies including understanding of mesh&texture, spatial relationship, semantic instruction, physical laws, knowledge transfer and reasoning, etc. To support the downstream finetuning, we provide high-quality training data collected via an automated framework incorporating heuristic skills and prior information. The experimental results indicate that both the current state-of-the-art pretrained VLAs and the workflow based on VLMs face challenges in our tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiduo Zhang",
      "Zhe Xu",
      "Peiju Liu",
      "Xiaopeng Yu",
      "Yuan Li",
      "Qinghui Gao",
      "Zhaoye Fei",
      "Zhangyue Yin",
      "Zuxuan Wu",
      "Yu-Gang Jiang",
      "Xipeng Qiu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Fang_MolParser_End-to-end_Visual_Recognition_of_Molecule_Structures_in_the_Wild_ICCV_2025_paper.html": {
    "title": "MolParser: End-to-end Visual Recognition of Molecule Structures in the Wild",
    "volume": "main",
    "abstract": "In recent decades, chemistry publications and patents have increased rapidly. A significant portion of key information is embedded in molecular structure figures, complicating large-scale literature searches and limiting the application of large language models in fields such as biology, chemistry, and pharmaceuticals. The automatic extraction of precise chemical structures is of critical importance. However, the presence of numerous Markush structures in real-world documents, along with variations in molecular image quality, drawing styles, and noise, significantly limits the performance of existing optical chemical structure recognition (OCSR) methods. We present MolParser, a novel end-to-end OCSR method that efficiently and accurately recognizes chemical structures from real-world documents, including difficult Markush structure. We use a extended SMILES encoding rule to annotate our training dataset. Under this rule, we build MolParser-7M, the largest annotated molecular image dataset to our knowledge. While utilizing a large amount of synthetic data, we employed active learning methods to incorporate substantial in-the-wild data, specifically samples cropped from real patents and scientific literature, into the training process. We trained an end-to-end molecular image captioning model, MolParser, using a curriculum learning approach. MolParser significantly outperforms classical and learning-based methods across most scenarios, with potential for broader downstream applications. The dataset is publicly available in huggingface",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xi Fang",
      "Jiankun Wang",
      "Xiaochen Cai",
      "Shangqian Chen",
      "Shuwen Yang",
      "Haoyi Tao",
      "Nan Wang",
      "Lin Yao",
      "Linfeng Zhang",
      "Guolin Ke"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_UPRE_Zero-Shot_Domain_Adaptation_for_Object_Detection_via_Unified_Prompt_ICCV_2025_paper.html": {
    "title": "UPRE: Zero-Shot Domain Adaptation for Object Detection via Unified Prompt and Representation Enhancement",
    "volume": "main",
    "abstract": "Zero-shot domain adaptation (ZSDA) presents substantial challenges due to the lack of images in the target domain. Previous approaches leverage Vision-Language Models (VLMs) to tackle this challenge, exploiting their zero-shot learning capabilities. However, these methods primarily address domain distribution shifts and overlook the misalignment between the detection task and VLMs, which rely on manually crafted prompts. To overcome these limitations, we propose the unified prompt and representation enhancement (UPRE) framework, which jointly optimizes both textual prompts and visual representations. Specifically, our approach introduces a multi-view domain prompt that combines linguistic domain priors with detection-specific knowledge, and a visual representation enhancement module that produces domain style variations. Furthermore, we introduce multi-level enhancement strategies, including relative domain distance and positive-negative separation, which align multi-modal representations at the image level and capture diverse visual representations at the instance level, respectively. Extensive experiments conducted on nine benchmark datasets demonstrate the superior performance of our framework in ZSDA detection scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Zhang",
      "Fei Wei",
      "Yong Wang",
      "Wenda Zhao",
      "Feiyi Li",
      "Xiangxiang Chu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yu_Heavy_Labels_Out_Dataset_Distillation_with_Label_Space_Lightening_ICCV_2025_paper.html": {
    "title": "Heavy Labels Out! Dataset Distillation with Label Space Lightening",
    "volume": "main",
    "abstract": "Dataset distillation or condensation aims to condense a large-scale training dataset into a much smaller synthetic one such that the training performance of distilled and original sets on neural networks are similar. Although the number of training samples can be reduced substantially, current state-of-the-art methods heavily rely on enormous soft labels to achieve satisfactory performance. As a result, the required storage can be comparable even to original datasets, especially for large-scale ones. To solve this problem, instead of storing these heavy labels, we propose a novel label-lightening framework termed HeLlO aiming at effective image-to-label projectors, with which synthetic labels can be directly generated online from synthetic images. Specifically, to construct such projectors, we leverage prior knowledge in open-source foundation models, e.g., CLIP, and introduce a LoRA-like fine-tuning strategy to mitigate the gap between pre-trained and target distributions, so that original models for soft-label generation can be distilled into a group of low-rank matrices. Moreover, an effective image optimization method is proposed to further mitigate the potential error between the original and distilled label generators. Extensive experiments show that our method significantly reduces the storage cost to merely 0.001% compared to full soft-label storage methods while achieving comparable performance to state-of-the-art dataset distillation methods on large-scale datasets. Our codes are available at https://github.com/Lexie-YU/HeLlO",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruonan Yu",
      "Songhua Liu",
      "Zigeng Chen",
      "Jingwen Ye",
      "Xinchao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cao_VideoMiner_Iteratively_Grounding_Key_Frames_of_Hour-Long_Videos_via_Tree-based_ICCV_2025_paper.html": {
    "title": "VideoMiner: Iteratively Grounding Key Frames of Hour-Long Videos via Tree-based Group Relative Policy Optimization",
    "volume": "main",
    "abstract": "Understanding hour-long videos with multi-modal large language models (MM-LLMs) enriches the landscape of human-centered AI applications. However, for end-to-end video understanding with LLMs, uniformly sampling video frames results in LLMs being overwhelmed by a vast amount of irrelevant information as video length increases. Existing hierarchical key frame extraction methods improve the accuracy of video understanding but still face two critical challenges. 1) How can the interference of extensive redundant information in long videos be mitigated? 2) How can a model dynamically adapt to complex hierarchical structures while accurately identifying key frames? To address these issues, we propose VideoMiner, which iteratively segments, captions, and clusters long videos, forming a hierarchical tree structure. The proposed VideoMiner progresses from long videos to events to frames while preserving temporal coherence, effectively addressing the first challenge. To precisely locate key frames, we introduce T-GRPO, a tree-based group relative policy optimization in reinforcement learning method that guides the exploration of the VideoMiner. The proposed T-GRPO is specifically designed for tree structures, integrating spatiotemporal information at the event level while being guided by the question, thus solving the second challenge. We achieve superior performance in all long-video understanding tasks and uncover several interesting insights. Our proposed T-GRPO surprisingly incentivizes the model to spontaneously generate a reasoning chain. Additionally, the designed tree growth auxin dynamically adjusts the expansion depth, obtaining accuracy and efficiency gains. The code is publicly available at https://github.com/caoxinye/VideoMiner",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinye Cao",
      "Hongcan Guo",
      "Jiawen Qian",
      "Guoshun Nan",
      "Chao Wang",
      "Yuqi Pan",
      "Tianhao Hou",
      "Xiaojuan Wang",
      "Yutong Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Dong_Hierarchical_Visual_Prompt_Learning_for_Continual_Video_Instance_Segmentation_ICCV_2025_paper.html": {
    "title": "Hierarchical Visual Prompt Learning for Continual Video Instance Segmentation",
    "volume": "main",
    "abstract": "Video instance segmentation (VIS) has gained significant attention for its capability in tracking and segmenting object instances across video frames. However, most of the existing VIS approaches unrealistically assume that the categories of object instances remain fixed over time. Moreover, they experience catastrophic forgetting of old classes when required to continuously learn object instances belonging to new categories. To resolve these challenges, we develop a novel Hierarchical Visual Prompt Learning (HVPL) model that overcomes catastrophic forgetting of previous categories from both frame-level and video-level perspectives. Specifically, to mitigate forgetting at the frame level, we devise a task-specific frame prompt and an orthogonal gradient correction (OGC) module. The OGC module helps the frame prompt encode task-specific global instance information for new classes in each individual frame by projecting its gradients onto the orthogonal feature space of old classes. Furthermore, to address forgetting at the video level, we design a task-specific video prompt and a video context decoder. This decoder first embeds structural inter-class relationships across frames into the frame prompt features, and then propagates task-specific global video contexts from the frame prompt features to the video prompt. Through rigorous comparisons, our HVPL model proves to be more effective than baseline approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahua Dong",
      "Hui Yin",
      "Wenqi Liang",
      "Hanbin Zhao",
      "Henghui Ding",
      "Nicu Sebe",
      "Salman Khan",
      "Fahad Shahbaz Khan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hou_FROSS_Faster-Than-Real-Time_Online_3D_Semantic_Scene_Graph_Generation_from_RGB-D_ICCV_2025_paper.html": {
    "title": "FROSS: Faster-Than-Real-Time Online 3D Semantic Scene Graph Generation from RGB-D Images",
    "volume": "main",
    "abstract": "The ability to abstract complex 3D environments into simplified and structured representations is crucial across various domains. 3D semantic scene graphs (SSGs) achieve this by representing objects as nodes and their interrelationships as edges, facilitating high-level scene understanding. Existing methods for 3D SSG generation, however, face significant challenges, including high computational demands and non-incremental processing that hinder their suitability for real-time open-world applications. To address this issue, we propose FROSS (Faster-than-Real-Time Online 3D Semantic Scene Graph Generation), an innovative approach for online and faster-than-real-time 3D SSG generation that leverages the direct lifting of 2D scene graphs to 3D space and represents objects as 3D Gaussian distributions. This framework eliminates the dependency on precise and computationally-intensive point cloud processing. Furthermore, we extend the Replica dataset with inter-object relationship annotations, creating the ReplicaSSG dataset for comprehensive evaluation of FROSS. The experimental results from evaluations on ReplicaSSG and 3DSSG datasets show that FROSS can achieve superior performance while operating significantly faster than prior 3D SSG generation methods. Our implementation and dataset are publicly available at https://github.com/Howardkhh/FROSS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao-Yu Hou",
      "Chun-Yi Lee",
      "Motoharu Sonogashira",
      "Yasutomo Kawanishi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ha_HVPUNet_Hybrid-Voxel_Point-cloud_Upsampling_Network_ICCV_2025_paper.html": {
    "title": "HVPUNet: Hybrid-Voxel Point-cloud Upsampling Network",
    "volume": "main",
    "abstract": "Point-cloud upsampling aims to generate dense point sets from sparse or incomplete 3D data. Most existing work uses a point-to-point framework. While this method achieves high geometric precision, it is slow because of irregular memory accesses to process unstructured point data. Alternatively, voxel-based methods offer computational efficiency by using regular grids, but struggle to preserve precise point locations due to discretization. To resolve this efficiency-precision trade-off, we introduce Hybrid Voxels, a representation that combines both voxel occupancy and a continuous point offset. We then present the Hybrid-Voxel Point-cloud Upsampling Network (HVPUNet), an efficient framework built upon this representation. HVPUNet integrates two key modules: (1) Shape Completion to restore missing geometry by filling empty voxels, and (2) Super-Resolution to enhance spatial resolution and capture finer surface details. We also use progressive refinement, operational voxel expansion, and implicit geometric learning. Experimental results demonstrate that HVPUNet can upsample point clouds at significantly lower computational cost than the state-of-the-art, but with comparable model accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juhyung Ha",
      "Vibhas Kumar Vats",
      "Soon-heung Jung",
      "Alimoor Reza",
      "David J. Crandall"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhu_MorphoGen_Efficient_Unconditional_Generation_of_Long-Range_Projection_Neuronal_Morphology_via_ICCV_2025_paper.html": {
    "title": "MorphoGen: Efficient Unconditional Generation of Long-Range Projection Neuronal Morphology via a Global-to-Local Framework",
    "volume": "main",
    "abstract": "Capturing the spatial patterns of neurons and generating high-fidelity morphological data remain critical challenges in developing biologically realistic large-scale brain network models. Existing methods fail to reconcile anatomical complexity with diversity and computational scalability. We propose MorphoGen, a hierarchical framework integrating global structure prediction through denoising diffusion probabilistic models (DDPMs) with local neurites optimization. The pipeline initiates with DDPM-generated coarse-grained neuronal point clouds, followed by skeletonization and growth-guided linking to derive plausible tree-like structures, and culminates in natural neural fibers refinement via a pragmatic smoothing network. Comprehensive evaluations across three distinct long-range projection neuron datasets demonstrate that the proposed method improves 1-Nearest Neighbor Accuracy by approximately 12% on average compared to state-of-the-art baseline, reduces average training time by around 55%, and aligns the distributions of several morphometrics with real data. This work establishes a novel global-to-local paradigm for neuronal morphology generation, offering a more direct and efficient approach compared to current branch-sequential modeling methods. Code is available at https://github.com/Brainsmatics/MorphoGen",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianfang Zhu",
      "Hongyang Zhou",
      "Anan Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tian_REDUCIO_Generating_1K_Video_within_16_Seconds_using_Extremely_Compressed_ICCV_2025_paper.html": {
    "title": "REDUCIO! Generating 1K Video within 16 Seconds using Extremely Compressed Motion Latents",
    "volume": "main",
    "abstract": "Commercial video generation models have exhibited realistic, high-fidelity results but are still restricted to limited access.One crucial obstacle for large-scale applications is the expensive training and inference cost.In this paper, we argue that videos contain significantly more redundant information than images, allowing them to be encoded with very few motion latents.Towards this goal, we design an image-conditioned VAE that projects videos into extremely compressed latent space and decode them based on content images. This magic Reducio charm enables 64xreduction of latents compared to a common 2D VAE, without sacrificing the quality.Building upon Reducio-VAE, we can train diffusion models for high-resolution video generation efficiently. Specifically, we adopt a two-stage generation paradigm, first generating a condition image via text-to-image generation, followed by text-image-to-video generation with the proposed Reducio-DiT. Extensive experiments show that our model achieves strong performance in evaluation.More importantly, our method significantly boosts the training and inference efficiency of video LDMs. Reducio-DiT is trained in just 3.2K A100 GPU hours in total and can generate a 16-frame 1024x1024 video clip within 15.5 seconds on a single A100 GPU. Code is available at https://github.com/microsoft/Reducio-VAE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Tian",
      "Qi Dai",
      "Jianmin Bao",
      "Kai Qiu",
      "Yifan Yang",
      "Chong Luo",
      "Zuxuan Wu",
      "Yu-Gang Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_EgoAgent_A_Joint_Predictive_Agent_Model_in_Egocentric_Worlds_ICCV_2025_paper.html": {
    "title": "EgoAgent: A Joint Predictive Agent Model in Egocentric Worlds",
    "volume": "main",
    "abstract": "Learning an agent model that behaves like humans--capable of jointly perceiving the environment, predicting the future, and taking actions from a first-person perspective--is a fundamental challenge in computer vision. Existing methods typically train separate models for these abilities, which fail to capture their intrinsic relationships and prevent them from learning from each other. Inspired by how humans learn through the perception-action loop, we propose EgoAgent, a unified agent model that simultaneously learns to represent, predict, and act within a single transformer. EgoAgent explicitly models the causal and temporal dependencies among these abilities by formulating the task as an interleaved sequence of states and actions. It further introduces a joint embedding-action-prediction architecture with temporally asymmetric predictor and observer branches, enabling synergistic optimization across all three capabilities. Comprehensive evaluations of EgoAgent on representative tasks such as image classification, egocentric future state prediction, and 3D human motion prediction demonstrate the superiority of our method. The code and trained models will be publicly available at https://github.com/zju3dv/EgoAgent",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lu Chen",
      "Yizhou Wang",
      "Shixiang Tang",
      "Qianhong Ma",
      "Tong He",
      "Wanli Ouyang",
      "Xiaowei Zhou",
      "Hujun Bao",
      "Sida Peng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shao_Growing_a_Twig_to_Accelerate_Large_Vision-Language_Models_ICCV_2025_paper.html": {
    "title": "Growing a Twig to Accelerate Large Vision-Language Models",
    "volume": "main",
    "abstract": "Large vision-language models (VLMs) have demonstrated remarkable capabilities in open-world multimodal understanding, yet their high computational overheads pose great challenges for practical deployment. Some recent works have proposed methods to accelerate VLMs by pruning redundant visual tokens guided by the attention maps of VLM's early layers. Despite the success of these token pruning methods, they still suffer from two major shortcomings: (i) considerable accuracy drop due to insensitive attention signals in early layers, and (ii) limited speedup when generating long responses (e.g., 30 tokens). To address the limitations above, we present TwigVLM---a simple and general architecture by \"growing\" a lightweight twig upon an early layer of the base VLM. Compared with most existing VLM acceleration methods purely based on visual token pruning, our TwigVLM not only achieves better accuracy retention by employing a twig-guided token pruning (TTP) strategy, but also yields higher generation speed by utilizing a self-speculative decoding (SSD) strategy. Taking LLaVA-1.5-7B as the base VLM, experimental results show that TwigVLM preserves 96% of the original performance after pruning 88.9% of visual tokens and achieves 154% speedup in generating long responses, delivering significantly better performance in terms of both accuracy and speed over the state-of-the-art VLM acceleration methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenwei Shao",
      "Mingyang Wang",
      "Zhou Yu",
      "Wenwen Pan",
      "Yan Yang",
      "Tao Wei",
      "Hongyuan Zhang",
      "Ning Mao",
      "Wei Chen",
      "Jun Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Is_Less_More_Exploring_Token_Condensation_as_Training-free_Test-time_Adaptation_ICCV_2025_paper.html": {
    "title": "Is Less More? Exploring Token Condensation as Training-free Test-time Adaptation",
    "volume": "main",
    "abstract": "Contrastive Language-Image Pretraining (CLIP) excels at learning generalizable image representations but often falls short in zero-shot inference on certain downstream datasets. Test-time adaptation (TTA) mitigates this issue by adjusting components like normalization layers or context prompts, yet it typically requires large batch sizes and extensive augmentations, leading to high computational costs. This raises a key question: Can VLMs' performance drop in specific test cases be mitigated through efficient, training-free approaches? To explore the solution, we investigate token condensation (TC) techniques, originally designed to enhance vision transformer efficiency by refining token usage during inference. We observe that informative tokens improve visual-text alignment in VLMs like CLIP on unseen datasets. However, existing TC methods often fail to maintain in-distribution performance when reducing tokens, prompting us to ask: How can we transform TC into an effective \"free-lunch\" adaptation strategy for VLMs? To address this, we propose Token Condensation as Adaptation (TCA), a training-free adaptation method that takes a step beyond standard TC. Rather than passively discarding tokens, TCA condenses token representation by introducing reservoir-based domain anchor tokens for information-preserving token reduction and logit correction. TCA achieves up to a 21.4% performance improvement over the strongest baseline on cross-dataset benchmark and the CIFAR-100-Corrupted dataset while reducing GFLOPs by 12.2% to 48.9%, with minimal hyperparameter dependency on both CLIP and SigLIP series. Code is available at https://github.com/Jo-wang/TCA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixin Wang",
      "Dong Gong",
      "Sen Wang",
      "Zi Huang",
      "Yadan Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yu_Structure-Guided_Diffusion_Models_for_High-Fidelity_Portrait_Shadow_Removal_ICCV_2025_paper.html": {
    "title": "Structure-Guided Diffusion Models for High-Fidelity Portrait Shadow Removal",
    "volume": "main",
    "abstract": "We present a diffusion-based portrait shadow removal approach that can robustly produce high-fidelity results. Unlike previous methods, we cast shadow removal as diffusion-based inpainting. To this end, we first train a shadow-independent structure extraction network on a real-world portrait dataset with various synthetic lighting conditions, which allows to generate a shadow-independent structure map including facial details while excluding the unwanted shadow boundaries. The structure map is then used as condition to train a structure-guided inpainting diffusion model for removing shadows in a generative manner. Finally, to restore the fine-scale details (e.g., eyelashes, moles and spots) that may not be captured by the structure map, we take the gradients inside the shadow regions as guidance and train a detail restoration diffusion model to refine the shadow removal result. Extensive experiments on the benchmark datasets show that our method clearly outperforms existing methods, and is effective to avoid previously common issues such as facial identity tampering, shadow residual, color distortion, structure blurring, and loss of details. Our code is available at https://github.com/wanchang-yu/Structure-Guided-Diffusion-for-Portrait-Shadow-Removal",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wanchang Yu",
      "Qing Zhang",
      "Rongjia Zheng",
      "Wei-Shi Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Boutaj_Controllable_Latent_Space_Augmentation_for_Digital_Pathology_ICCV_2025_paper.html": {
    "title": "Controllable Latent Space Augmentation for Digital Pathology",
    "volume": "main",
    "abstract": "Whole slide image (WSI) analysis in digital pathology presents unique challenges due to the gigapixel resolution of WSIs and the scarcity of dense supervision signals. While Multiple Instance Learning (MIL) is a natural fit for slide-level tasks, training robust models requires large and diverse datasets. Even though image augmentation techniques could be utilized to increase data variability and reduce overfitting, implementing them effectively is not a trivial task. Traditional patch-level augmentation is prohibitively expensive due to the large number of patches extracted from each WSI, and existing feature-level augmentation methods lack control over transformation semantics. We introduce HistAug, a fast and efficient generative model for controllable augmentations in the latent space for digital pathology. By conditioning on explicit patch-level transformations (e.g., hue, erosion), HistAug generates realistic augmented embeddings while preserving initial semantic information. Our method allows the processing of a large number of patches in a single forward pass efficiently, while at the same time consistently improving MIL model performance. Experiments across multiple slide-level tasks and diverse organs show that HistAug outperforms existing methods, particularly in low-data regimes. Ablation studies confirm the benefits of learned transformations over noise-based perturbations and highlight the importance of uniform WSI-wise augmentation",
    "checked": true,
    "id": "5708d6236180bc91f8a067a800df8fd0ac35b6ac",
    "semantic_title": "controllable latent space augmentation for digital pathology",
    "citation_count": 0,
    "authors": [
      "Sofiène Boutaj",
      "Marin Scalbert",
      "Pierre Marza",
      "Florent Couzinie-Devy",
      "Maria Vakalopoulou",
      "Stergios Christodoulidis"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lin_Global_Motion_Corresponder_for_3D_Point-Based_Scene_Interpolation_under_Large_ICCV_2025_paper.html": {
    "title": "Global Motion Corresponder for 3D Point-Based Scene Interpolation under Large Motion",
    "volume": "main",
    "abstract": "Existing dynamic scene interpolation methods typically assume that the motion between consecutive timesteps is small enough so that displacements can be locally approximated by linear models. In practice, even slight deviations from this small-motion assumption can cause conventional techniques to fail. In this paper, we introduce Global Motion Corresponder (GMC), a novel approach that robustly handles large motion and achieves smooth transitions. GMC learns unary potential fields that predict SE(3) mappings into a shared canonical space, balancing correspondence, spatial and semantic smoothness, and local rigidity. We demonstrate that our method significantly outperforms existing baselines on 3D scene interpolation when the two states undergo large global motions. Furthermore, our method enables extrapolation capabilities where other baseline methods cannot",
    "checked": true,
    "id": "e315541265c3b4f3bfa825a71d1b8730905fb3de",
    "semantic_title": "global motion corresponder for 3d point-based scene interpolation under large motion",
    "citation_count": 0,
    "authors": [
      "Junru Lin",
      "Chirag Vashist",
      "Mikaela Angelina Uy",
      "Colton Stearns",
      "Xuan Luo",
      "Leonidas Guibas",
      "Ke Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Park_WINS_Winograd_Structured_Pruning_for_Fast_Winograd_Convolution_ICCV_2025_paper.html": {
    "title": "WINS: Winograd Structured Pruning for Fast Winograd Convolution",
    "volume": "main",
    "abstract": "Recent GPUs leverage Winograd convolution and structured pruning to significantly accelerate inference. First, Winograd convolution is theoretically 2.25x faster than standard convolution. Second, structured pruning reduces inference time without additional overhead as the pruning ratio increases. However, applying conventional structured pruning alongside Winograd convolution is inefficient. Existing structured pruning methods, which do not account for how GPUs process Winograd convolution, require large pruning unit sizes, leading to significant information loss. In this paper, we propose Winograd Structured Pruning (WINS), the first approach to employ optimized structured pruning for Winograd convolution. WINS is designed based on an in-depth analysis of Winograd convolution's computational characteristics on GPUs. Additionally, we introduce two variants, WINS-B and WINS-AB, which further enhance performance. Experimental results show that WINS-AB achieves up to 2.8x practical speedup in baseline inference on GPUs while preserving the accuracy of ResNet-18 on ImageNet",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheonjun Park",
      "Hyun Jae Oh",
      "Mincheol Park",
      "Hyunchan Moon",
      "Minsik Kim",
      "Suhyun Kim",
      "Myung Kuk Yoon",
      "Won Woo Ro"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_Revelio_Interpreting_and_leveraging_semantic_information_in_diffusion_models_ICCV_2025_paper.html": {
    "title": "Revelio: Interpreting and leveraging semantic information in diffusion models",
    "volume": "main",
    "abstract": "We study how rich visual semantic information is represented within various layers and denoising timesteps of different diffusion architectures. We uncover monosemantic interpretable features by leveraging k-sparse autoencoders (k-SAE). We substantiate our mechanistic interpretations via transfer learning using light-weight classifiers on off-the-shelf diffusion models' features. On 4 datasets, we demonstrate the effectiveness of diffusion features for representation learning. We provide an in-depth analysis of how different diffusion architectures, pre-training datasets, and language model conditioning impacts visual representation granularity, inductive biases, and transfer learning capabilities. Our work is a critical step towards deepening interpretability of black-box diffusion models. Code and visualizations available at: https://github.com/revelio-diffusion/revelio",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dahye Kim",
      "Xavier Thomas",
      "Deepti Ghadiyaram"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_UniRes_Universal_Image_Restoration_for_Complex_Degradations_ICCV_2025_paper.html": {
    "title": "UniRes: Universal Image Restoration for Complex Degradations",
    "volume": "main",
    "abstract": "Real-world image restoration is hampered by diverse degradations stemming from varying capture conditions, capture devices and post-processing pipelines. Existing works make improvements through simulating those degradations and leveraging image generative priors, however generalization to in-the-wild data remains an unresolved problem. In this paper, we focus on complex degradations, i.e., arbitrary mixtures of multiple types of known degradations, which is frequently seen in the wild. A simple yet flexible diffusion-based framework, named UniRes, is proposed to address such degradations in an end-to-end manner. It combines several specialized models during the diffusion sampling steps, hence transferring the knowledge from several well-isolated restoration tasks to the restoration of complex in-the-wild degradations. This only requires well-isolated training data for several degradation types. The framework is flexible as extensions can be added through a unified formulation, and the fidelity-quality trade-off can be adjusted through a new paradigm. Our proposed method is evaluated on both complex-degradation and single-degradation image restoration datasets. Extensive qualitative and quantitative experimental results show consistent performance gain especially for images with complex degradations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mo Zhou",
      "Keren Ye",
      "Mauricio Delbracio",
      "Peyman Milanfar",
      "Vishal M. Patel",
      "Hossein Talebi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Blanc_RayGaussX_Accelerating_Gaussian-Based_Ray_Marching_for_Real-Time_and_High-Quality_Novel_ICCV_2025_paper.html": {
    "title": "RayGaussX: Accelerating Gaussian-Based Ray Marching for Real-Time and High-Quality Novel View Synthesis",
    "volume": "main",
    "abstract": "RayGauss has recently achieved state-of-the-art results on synthetic and indoor scenes, representing radiance and density fields with irregularly distributed elliptical basis functions rendered via volume ray casting using a Bounding Volume Hierarchy (BVH). However, its computational cost prevents real-time rendering on real-world scenes. Our approach, RayGaussX, builds on RayGauss by introducing key contributions that significantly accelerate both training and inference. Specifically, we incorporate volumetric rendering acceleration strategies such as empty-space skipping and adaptive sampling, enhance ray coherence, and introduce scale regularization to reduce false-positive intersections. Additionally, we propose a new densification criterion that improves density distribution in distant regions, leading to enhanced graphical quality on larger scenes. As a result, RayGaussX achieves 5x to 12x faster training and 50x to 80x higher rendering speeds (FPS) on real-world datasets while improving visual quality by up to +0.56 dB in PSNR. The code will soon be publicly available on GitHub",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hugo Blanc",
      "Jean-Emmanuel Deschaud",
      "Alexis Paljic"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhong_CoopTrack_Exploring_End-to-End_Learning_for_Efficient_Cooperative_Sequential_Perception_ICCV_2025_paper.html": {
    "title": "CoopTrack: Exploring End-to-End Learning for Efficient Cooperative Sequential Perception",
    "volume": "main",
    "abstract": "Cooperative perception aims to address the inherent limitations of single-vehicle autonomous driving systems through information exchange among multiple agents. Previous research has primarily focused on single-frame perception tasks. However, the more challenging cooperative sequential perception tasks, such as cooperative 3D multi-object tracking, have not been thoroughly investigated. Therefore, we propose CoopTrack, a fully instance-level end-to-end framework for cooperative tracking, featuring learnable instance association, which fundamentally differs from existing approaches. CoopTrack transmits sparse instance-level features that significantly enhance perception capabilities while maintaining low transmission costs. Furthermore, the framework comprises two key components: Multi-Dimensional Feature Extraction, and Cross-Agent Association and Aggregation, which collectively enable comprehensive instance representation with semantic and motion features, and adaptive cross-agent association and fusion based on a feature graph. Experiments on both the V2X-Seq and Griffin datasets demonstrate that CoopTrack achieves excellent performance. Specifically, it attains state-of-the-art results on V2X-Seq, with 39.0% mAP and 32.8% AMOTA. The project is available at https://github.com/zhongjiaru/CoopTrack",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaru Zhong",
      "Jiahao Wang",
      "Jiahui Xu",
      "Xiaofan Li",
      "Zaiqing Nie",
      "Haibao Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Durvasula_ContraGS_Codebook-Condensed_and_Trainable_Gaussian_Splatting_for_Fast_Memory-Efficient_Reconstruction_ICCV_2025_paper.html": {
    "title": "ContraGS: Codebook-Condensed and Trainable Gaussian Splatting for Fast, Memory-Efficient Reconstruction",
    "volume": "main",
    "abstract": "3D Gaussian Splatting (3DGS) is a state-of-art technique to model real-world scenes with high quality and real-time rendering.Typically, a higher quality representation can be achieved by using a large number of 3D Gaussians. However, using large 3D Gaussian counts significantly increases the GPU device memory for storing model parameters. A large model thus requires powerful GPUs with high memory capacities for training and has slower training/rendering latencies due to the inefficiencies of memory access and data movement. In this work, we introduce ContraGS, a method to enable training directly on compressed 3DGS representations without reducing the Gaussian Counts, and thus with a little loss in model quality. ContraGS leverages codebooks to compactly store a set of Gaussian parameter vectors throughout the training process, thereby significantly reducing memory consumption. While codebooks have been demonstrated to be highly effective at compressing fully trained 3DGS models, directly training using codebook representations is an unsolved challenge. ContraGS solves the problem of learning non-differentiable parameters in codebook-compressed representations by posing parameter estimation as a Bayesian inference problem. To this end, ContraGS provides a framework that effectively uses MCMC sampling to sample over a posterior distribution of these compressed representations. With ContraGS, we demonstrate that ContraGS significantly reduces the peak memory during training (on average 3.49X) and accelerated training and rendering 1.36Xand 1.88X on average, respectively), while retraining close to state-of-art quality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sankeerth Durvasula",
      "Sharanshangar Muhunthan",
      "Zain Moustafa",
      "Richard Chen",
      "Ruofan Liang",
      "Yushi Guan",
      "Nilesh Ahuja",
      "Nilesh Jain",
      "Selvakumar Panneer",
      "Nandita Vijaykumar"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_NATRA_Noise-Agnostic_Framework_for_Trajectory_Prediction_with_Noisy_Observations_ICCV_2025_paper.html": {
    "title": "NATRA: Noise-Agnostic Framework for Trajectory Prediction with Noisy Observations",
    "volume": "main",
    "abstract": "Trajectory prediction aims to forecast an agent's future trajectories based on its historical observed trajectories, which is a critical task for various applications such as autonomous driving, robotics, and surveillance systems. Most existing trajectory prediction methods assume that the observed trajectories collected for forecasting are clean. However, in real-world scenarios, noise is inevitably introduced into the observations, resulting in the collapse of the existing approaches. Therefore, it is essential to perform robust trajectory prediction based on noisy observations, which is a more practical scenario. In this paper, we propose **NATRA**, a **N**oise-**A**gnostic framework capable of tackling the problem of **TRA**jectory prediction with arbitrary types of noisy observations. Specifically, we put forward a mutual information-based mechanism to denoise the original noisy observations. It optimizes the produced trajectories to exhibit a pattern that closely resembles the clean trajectory pattern while deviating from the noisy one.Considering that the trajectory structure may be destroyed through the only optimization of mutual information, we introduce an additional reconstruction loss to preserve the structure information of the produced observed trajectories. Moreover, we further propose a ranking loss to further enhance the performance. Because NATRA does not rely on any specific module tailored to particular noise distributions, it can handle arbitrary types of noise in principle. Additionally, our proposed NATRA can be easily integrated into existing trajectory prediction models. Extensive experiments on both synthetic and real-world noisy datasets demonstrate the effectiveness of our method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rongqing Li",
      "Changsheng Li",
      "Ruilin Lv",
      "Yuhang Li",
      "Yang Gao",
      "Xiaolu Zhang",
      "Jun Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Devulapally_Your_Text_Encoder_Can_Be_An_Object-Level_Watermarking_Controller_ICCV_2025_paper.html": {
    "title": "Your Text Encoder Can Be An Object-Level Watermarking Controller",
    "volume": "main",
    "abstract": "Invisible watermarking of AI-generated images can help with copyright protection, enabling detection and identification of AI-generated media. In this work, we present a novel approach to watermark images of T2I Latent Diffusion Models (LDMs). By only fine-tuning text token embeddings \\mathcal W _*, we enable watermarking in selected objects or parts of the image, offering greater flexibility compared to traditional full-image watermarking. Our method leverages the text encoder's compatibility across various LDMs, allowing plug-and-play integration for different LDMs. Moreover, introducing the watermark early in the encoding stage improves robustness to adversarial perturbations in later stages of the pipeline. Our approach achieves 99% bit accuracy (48 bits) with a 10^5 xreduction in model parameters, enabling efficient watermarking",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Naresh Kumar Devulapally",
      "Mingzhen Huang",
      "Vishal Asnani",
      "Shruti Agarwal",
      "Siwei Lyu",
      "Vishnu Suresh Lokhande"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Spatial-Temporal_Aware_Visuomotor_Diffusion_Policy_Learning_ICCV_2025_paper.html": {
    "title": "Spatial-Temporal Aware Visuomotor Diffusion Policy Learning",
    "volume": "main",
    "abstract": "Visual imitation learning is effective for robots to learn versatile tasks. However, many existing methods rely on behavior cloning with supervised historical trajectories, limiting their 3D spatial and 4D spatiotemporal awareness. Consequently, these methods struggle to capture the 3D structures and 4D spatiotemporal relationships necessary for real-world deployment. In this work, we propose 4D Diffusion Policy (DP4), a novel visual imitation learning method that incorporates spatiotemporal awareness into diffusion-based policies. Unlike traditional approaches that rely on trajectory cloning, DP4 leverages a dynamic Gaussian world model to guide the learning of 3D spatial and 4D spatiotemporal perceptions from interactive environments. Our method constructs the current 3D scene from a single-view RGB-D observation and predicts the future 3D scene, optimizing trajectory generation by explicitly modeling both spatial and temporal dependencies. Extensive experiments across 17 simulation tasks with 173 variants and 3 real-world robotic tasks demonstrate that the 4D Diffusion Policy (DP4) outperforms baseline methods, improving the average simulation task success rate by 16.4% (Adroit), 14% (DexArt), and 6.45% (RLBench), and the average real-world robotic task success rate by 8.6%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenyang Liu",
      "Yikai Wang",
      "Kuanning Wang",
      "Longfei Liang",
      "Xiangyang Xue",
      "Yanwei Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ramanathan_ModalTune_Fine-Tuning_Slide-Level_Foundation_Models_with_Multi-Modal_Information_for_Multi-task_ICCV_2025_paper.html": {
    "title": "ModalTune: Fine-Tuning Slide-Level Foundation Models with Multi-Modal Information for Multi-task Learning in Digital Pathology",
    "volume": "main",
    "abstract": "Prediction tasks in digital pathology are challenging due to the massive size of whole-slide images (WSIs) and the weak nature of training signals. Advances in computing, data availability, and self-supervised learning (SSL) have paved the way for slide-level foundation models (SLFMs) that can improve prediction tasks in low-data regimes. However, current methods under-utilize shared information between tasks and modalities. To overcome this challenge, we propose ModalTune, a novel fine-tuning framework which introduces the Modal Adapter to integrate new modalities without modifying SLFM weights. Additionally, we use large-language models (LLMs) to encode labels as text, capturing semantic relationships across multiple tasks and cancer types in a single training recipe. ModalTune achieves state-of-the-art (SOTA) results against both uni-modal and multi-modal models across four cancer types, jointly improving survival and cancer subtype prediction while remaining competitive in pan-cancer settings. Additionally, we show ModalTune is generalizable to two out-of-distribution (OOD) datasets. To our knowledge, this is the first unified fine-tuning framework for multi-modal, multi-task, and pan-cancer modeling in digital pathology",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vishwesh Ramanathan",
      "Tony Xu",
      "Pushpak Pati",
      "Faruk Ahmed",
      "Maged Goubran",
      "Anne L. Martel"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shi_Harnessing_Vision_Foundation_Models_for_High-Performance_Training-Free_Open_Vocabulary_Segmentation_ICCV_2025_paper.html": {
    "title": "Harnessing Vision Foundation Models for High-Performance, Training-Free Open Vocabulary Segmentation",
    "volume": "main",
    "abstract": "While CLIP has advanced open-vocabulary predictions, its performance on semantic segmentation remains suboptimal. This shortfall primarily stems from its spatial-invariant semantic features and constrained resolution. While previous adaptations addressed spatial invariance semantic by modifying the self-attention in CLIP's image encoder, the issue of limited resolution remains unexplored. Different from previous segment-then-splice methods that segment sub-images via a sliding window and splice the results, we introduce a splice-then-segment paradigm that incorporates Segment-Anything Model (SAM) to tackle the resolution issue since SAM excels at extracting fine-grained semantic correlations from high-resolution images. Specifically, we introduce Trident, a training-free framework that first splices features extracted by CLIP and DINO from sub-images, then leverages SAM's encoder to create a correlation matrix for global aggregation, enabling a broadened receptive field. Besides, we propose a refinement strategy for CLIP's coarse segmentation outputs by transforming them into prompts for SAM. Trident achieves a significant improvement in the mIoU across eight popular benchmarks compared with the current SOTA. Furthermore, it can also be utilized to generate visual prompts that enhance the performance of Large Vision-Language Models (LVLMs). Code is available at https://github.com/YuHengsss/Trident",
    "checked": true,
    "id": "9dadb571b26ea7bd387b9273b34263fd2c2e981b",
    "semantic_title": "harnessing vision foundation models for high-performance, training-free open vocabulary segmentation",
    "citation_count": 5,
    "authors": [
      "Yuheng Shi",
      "Minjing Dong",
      "Chang Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tan_FixTalk_Taming_Identity_Leakage_for_High-Quality_Talking_Head_Generation_in_ICCV_2025_paper.html": {
    "title": "FixTalk: Taming Identity Leakage for High-Quality Talking Head Generation in Extreme Cases",
    "volume": "main",
    "abstract": "Talking head generation is gaining significant importance across various domains, with a growing demand for high-quality rendering. However, existing methods often suffer from identity leakage (IL) and rendering artifacts (RA), particularly in extreme cases. Through an in-depth analysis of previous approaches, we identify two key insights: (1) IL arises from identity information embedded within motion features, and (2) this identity information can be leveraged to address RA. Building on these findings, this paper introduces FixTalk, a novel framework designed to simultaneously resolve both issues for high-quality talking head generation. Firstly, we propose an Enhanced Motion Indicator (EMI) to effectively decouple identity information from motion features, mitigating the impact of IL on generated talking heads. To address RA, we introduce an Enhanced Detail Indicator (EDI), which utilizes the leaked identity information to supplement missing details, thus fixing the artifacts. Extensive experiments demonstrate that FixTalk effectively mitigates IL and RA, achieving superior performance compared to state-of-the-art methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuai Tan",
      "Bill Gong",
      "Bin Ji",
      "Ye Pan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/An_MinCD-PnP_Learning_2D-3D_Correspondences_with_Approximate_Blind_PnP_ICCV_2025_paper.html": {
    "title": "MinCD-PnP: Learning 2D-3D Correspondences with Approximate Blind PnP",
    "volume": "main",
    "abstract": "Image-to-point-cloud (I2P) registration is a fundamental problem in computer vision, focusing on establishing 2D-3D correspondences between an image and a point cloud. Recently, the differentiable perspective-n-point (PnP) has been widely used to supervise I2P registration networks by enforcing projective constraints on 2D-3D correspondences. However, differentiable PnP is highly sensitive to noise and outliers in the predicted correspondences, which hinders the effectiveness of correspondence learning. Inspired by the robustness of blind PnP to noise and outliers in correspondences, we propose an approximate blind PnP-based correspondence learning approach. To mitigate the high computational cost of blind PnP, we reformulate it as a more tractable problem: minimizing the Chamfer distance between learned 2D and 3D keypoints, referred to as MinCD-PnP. To effectively solve MinCD-PnP, we introduce a lightweight multi-task learning module, MinCD-Net, which can be easily integrated into the existing I2P registration architectures. Extensive experiments on 7-Scenes, RGBD-V2, ScanNet, self-collected, and KITTI datasets demonstrate that MinCD-Net outperforms state-of-the-art methods and achieves higher inlier ratio and registration recall in both cross-scene and cross-dataset settings. The source code: https://github.com/anpei96/mincd-pnp-demo",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pei An",
      "Jiaqi Yang",
      "Muyao Peng",
      "You Yang",
      "Qiong Liu",
      "Xiaolin Wu",
      "Liangliang Nan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Attention_to_Trajectory_Trajectory-Aware_Open-Vocabulary_Tracking_ICCV_2025_paper.html": {
    "title": "Attention to Trajectory: Trajectory-Aware Open-Vocabulary Tracking",
    "volume": "main",
    "abstract": "Open-Vocabulary Multi-Object Tracking (OV-MOT) aims to enable approaches to track objects without being limited to a predefined set of categories. Current OV-MOT methods typically rely primarily on instance-level detection and association, often overlooking trajectory information that is unique and essential for object tracking tasks. Utilizing trajectory information can enhance association stability and classification accuracy, especially in cases of occlusion and category ambiguity, thereby improving adaptability to novel classes. Thus motivated, in this paper we propose TRACT, an open-vocabulary tracker that leverages trajectory information to improve both object association and classification in OV-MOT. Specifically, we introduce a Trajectory Consistency Reinforcement (TCR) strategy, that benefits tracking performance by improving target identity and category consistency. In addition, we present TraCLIP, a plug-and-play trajectory classification module. It integrates Trajectory Feature Aggregation (TFA) and Trajectory Semantic Enrichment (TSE) strategies to fully leverage trajectory information from visual and language perspectives for enhancing the classification results. Extensive experiments on OV-TAO show that our TRACT significantly improves tracking performance, highlighting trajectory information as a valuable asset for OV-MOT. We will release TRACT at https://github.com/Nathan-Li123/TRACT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunhao Li",
      "Yifan Jiao",
      "Dan Meng",
      "Heng Fan",
      "Libo Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Che_Hallucinatory_Image_Tokens_A_Training-free_EAZY_Approach_to_Detecting_and_ICCV_2025_paper.html": {
    "title": "Hallucinatory Image Tokens: A Training-free EAZY Approach to Detecting and Mitigating Object Hallucinations in LVLMs",
    "volume": "main",
    "abstract": "Despite their remarkable potential, Large Vision-Language Models (LVLMs) still face challenges with object hallucination, a problem where their generated outputs mistakenly incorporate objects that do not actually exist. Although most works focus on addressing this issue within the language-model backbone, our work shifts the focus to the image input source, investigating how specific image tokens contribute to hallucinations. Our analysis reveals that a small subset of image tokens with high attention scores are the main drivers of object hallucination. By removing these hallucinatory image tokens (only 1.5% of all image tokens), the issue can be effectively mitigated. This finding holds consistently across different models. Building on this insight, we introduce \\eazy, a novel, training-free method that automatically identifies and Eliminates hAllucinations by Zeroing out hallucinator Y image tokens. We utilize EAZY for unsupervised object hallucination detection, achieving a 15% improvement compared to previous methods. Additionally, EAZY demonstrates remarkable effectiveness in mitigating hallucinations while preserving model utility and seamlessly adapting to various LVLM architectures",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liwei Che",
      "Tony Qingze Liu",
      "Jing Jia",
      "Weiyi Qin",
      "Ruixiang Tang",
      "Vladimir Pavlovic"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_CVPT_Cross_Visual_Prompt_Tuning_ICCV_2025_paper.html": {
    "title": "CVPT: Cross Visual Prompt Tuning",
    "volume": "main",
    "abstract": "Parameter-Efficient Fine-Tuning (PEFT) has emerged to mitigate the computational demands of large-scale models. Within computer vision, adapter-based PEFT methods are often favored over prompt-based approaches like Visual Prompt Tuning (VPT) due to the latter's performance and efficiency limitations. Our analysis reveals that VPT's shortcomings stem from its prompt deployment strategy, which can distort the model's inherent self-attention mechanism. To address this, we propose Cross Visual Prompt Tuning (CVPT). CVPT introduces a cross-attention module to directly model interactions between prompts and image tokens. This design decouples the prompts from the input sequence, preserving the original self-attention integrity while enabling efficient feature integration. Furthermore, we employ a weight-sharing mechanism for cross-attention initialization, which enhances representative capability without a large parameter overhead. Extensive experiments across 25 datasets show that CVPT significantly outperforms VPT. For instance, on the VTAB-1K benchmark, CVPT achieves over 4% higher average accuracy, rivaling leading adapter-based methods in both performance and efficiency. Our work confirms that prompt-based methods can achieve exceptional results in visual fine-tuning. The code is available at https://github.com/Lingyun0419/CVPT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingyun Huang",
      "Jianxu Mao",
      "Junfei Yi",
      "Ziming Tao",
      "Yaonan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_On_the_Robustness_Tradeoff_in_Fine-Tuning_ICCV_2025_paper.html": {
    "title": "On the Robustness Tradeoff in Fine-Tuning",
    "volume": "main",
    "abstract": "Fine-tuning has become the standard practice for adapting pre-trained models to downstream tasks. However, the impact on model robustness is not well understood. In this work, we characterize the robustness-accuracy trade-off in fine-tuning. We evaluate the robustness and accuracy of fine-tuned models over 6 benchmark datasets and 7 different fine-tuning strategies. We observe a consistent trade-off between adversarial robustness and accuracy. Peripheral updates such as BitFit are more effective for simple tasks---over 75% above the average measured by the area under the Pareto frontiers on CIFAR-10 and CIFAR-100. In contrast, fine-tuning information-heavy layers, such as attention layers via Compacter, achieves a better Pareto frontier on more complex tasks---57.5% and 34.6% above the average on Caltech-256 and CUB-200, respectively. Lastly, we observe that the robustness of fine-tuning against out-of-distribution data closely tracks accuracy. These insights emphasize the need for robustness-aware fine-tuning to ensure reliable real-world deployments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kunyang Li",
      "Jean-Charles Noirot Ferrand",
      "Ryan Sheatsley",
      "Blaine Hoak",
      "Yohan Beugin",
      "Eric Pauley",
      "Patrick McDaniel"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ma_Lay2Story_Extending_Diffusion_Transformers_for_Layout-Togglable_Story_Generation_ICCV_2025_paper.html": {
    "title": "Lay2Story: Extending Diffusion Transformers for Layout-Togglable Story Generation",
    "volume": "main",
    "abstract": "Storytelling tasks involving generating consistent subjects have gained significant attention recently. However, existing methods, whether training-free or training-based, continue to face challenges in maintaining subject consistency due to the lack of fine-grained guidance and inter-frame interaction. Additionally, the scarcity of high-quality data in this field makes it difficult to precisely control storytelling tasks, including the subject's position, appearance, clothing, expression, and posture, thereby hindering further advancements. In this paper, we demonstrate that layout conditions, such as the subject's position and detailed attributes, effectively facilitate fine-grained interactions between frames. This not only strengthens the consistency of the generated frame sequence but also allows for precise control over the subject's position, appearance, and other key details. Building on this, we introduce an advanced storytelling task: Layout-Toggable Storytelling, which enables precise subject control by incorporating layout conditions. To address the lack of high-quality datasets with layout annotations for this task, we develop Lay2Story-1M, which contains over 1 million 720p and higher-resolution images, processed from approximately 11,300 hours of cartoon videos. Building on Lay2Story-1M, we create Lay2Story-Bench, a benchmark with 3,000 prompts designed to evaluate the performance of different methods on this task. Furthermore, we propose Lay2Story, a robust framework based on the Diffusion Transformers (DiTs) architecture for Layout-Togglable Storytelling tasks. Through both qualitative and quantitative experiments, we find that our method outperforms the previous state-of-the-art (SOTA) techniques, achieving the best results in terms of consistency, semantic correlation, and aesthetic quality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ao Ma",
      "Jiasong Feng",
      "Ke Cao",
      "Jing Wang",
      "Yun Wang",
      "Quanwei Zhang",
      "Zhanjie Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Fang_Unfolding-Associative_Encoder-Decoder_Network_with_Progressive_Alignment_for_Pansharpening_ICCV_2025_paper.html": {
    "title": "Unfolding-Associative Encoder-Decoder Network with Progressive Alignment for Pansharpening",
    "volume": "main",
    "abstract": "Deep Unfolding Networks (DUNs) have emerged as a powerful framework for pansharpening due to their interpretable fusion strategies. However, existing DUNs are limited by their serial iterative architectures, which hinder cross-stage and cross-modal feature interactions at different abstraction levels. This limitation results in insufficient integration of multi-level multimodal features and compromised reconstruction accuracy. To address these challenges, we propose the Unfolding-Associative Encoder-Decoder Network (UED-Net), an innovative framework that iteratively extracts multi-level cross-modal degradation encodings and recursively refines features for cross-stage adaptive aggregation decoding through lightweight processes. Specifically, we first introduce the spatial-spectral encoding module, which progressively and interpretably perceives the hierarchical degradation encoding features of both space and spectrum. Moreover, we develop the unfolding-associative attention module to capture pixel-level attention across stages, thereby leveraging the causal relationships of multi-level features for aggregation during decoding. Meanwhile, we implement a progressive alignment mechanism, which coordinates both feature distribution and alignment of spatial and spectral modalities between iterative stages to facilitate adaptive fusion. These modules enable UED-Net to achieve efficient pansharpening by aggregating multi-level features. Extensive qualitative and quantitative experiments confirm the superiority of UED-Net",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shijie Fang",
      "Hongping Gan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhao_DeepMesh_Auto-Regressive_Artist-mesh_Creation_with_Reinforcement_Learning_ICCV_2025_paper.html": {
    "title": "DeepMesh: Auto-Regressive Artist-mesh Creation with Reinforcement Learning",
    "volume": "main",
    "abstract": "Triangle meshes play a crucial role in 3D applications for efficient manipulation and rendering. While auto-regressive methods generate structured meshes by predicting discrete vertex tokens, they are often constrained by limited face counts and mesh incompleteness. To address these challenges, we propose DeepMesh, a framework that optimizes mesh generation through two key innovations: (1) an efficient pre-training strategy incorporating a novel tokenization algorithm, along with improvements in data curation and processing, and (2) the introduction of Reinforcement Learning (RL) into 3D mesh generation to achieve human preference alignment via Direct Preference Optimization (DPO). We design a scoring standard that combines human evaluation with 3D metrics to collect preference pairs for DPO, ensuring both visual appeal and geometric accuracy. Conditioned on point clouds and images, DeepMesh generates meshes with intricate details and precise topology, outperforming state-of-the-art methods in both precision and quality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruowen Zhao",
      "Junliang Ye",
      "Zhengyi Wang",
      "Guangce Liu",
      "Yiwen Chen",
      "Yikai Wang",
      "Jun Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_VisRL_Intention-Driven_Visual_Perception_via_Reinforced_Reasoning_ICCV_2025_paper.html": {
    "title": "VisRL: Intention-Driven Visual Perception via Reinforced Reasoning",
    "volume": "main",
    "abstract": "Visual understanding is inherently intention-driven--humans selectively focus on different regions of a scene based on their goals. Recent advances in large multimodal models (LMMs) enable flexible expression of such intentions through natural language, allowing queries to guide visual reasoning processes. Frameworks like Visual Chain-of-Thought have demonstrated the benefit of incorporating explicit reasoning steps, where the model predicts a focus region before answering a query. However, existing approaches rely heavily on supervised training with annotated intermediate bounding boxes, which severely limits scalability due to the combinatorial explosion of intention-region pairs. To overcome this limitation, we propose VisRL, the first framework that applies reinforcement learning (RL) to the problem of intention-driven visual perception. VisRL optimizes the entire visual reasoning process using only reward signals. By treating intermediate focus selection as an internal decision optimized through trial-and-error, our method eliminates the need for costly region annotations while aligning more closely with how humans learn to perceive the world. Extensive experiments across multiple benchmarks show that VisRL consistently outperforms strong baselines, demonstrating both its effectiveness and its strong generalization across different LMMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhangquan Chen",
      "Xufang Luo",
      "Dongsheng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ota_PINO_Person-Interaction_Noise_Optimization_for_Long-Duration_and_Customizable_Motion_Generation_ICCV_2025_paper.html": {
    "title": "PINO: Person-Interaction Noise Optimization for Long-Duration and Customizable Motion Generation of Arbitrary-Sized Groups",
    "volume": "main",
    "abstract": "Generating realistic group interactions involving multiple characters remains challenging due to increasing complexity as group size expands. While existing conditional diffusion models incrementally generate motions by conditioning on previously generated characters, they rely on single shared prompts, limiting nuanced control and leading to overly simplified interactions. In this paper, we introduce Person-Interaction Noise Optimization (PINO), a novel, training-free framework designed for generating realistic and customizable interactions among groups of arbitrary size. PINO decomposes complex group interactions into semantically relevant pairwise interactions, and leverages pretrained two-person interaction diffusion models to incrementally compose group interactions. To ensure physical plausibility and avoid common artifacts such as overlapping or penetration between characters, PINO employs physics-based penalties during noise optimization. This approach allows precise user control over character orientation, speed, and spatial relationships without additional training. Comprehensive evaluations demonstrate that PINO generates visually realistic, physically coherent, and adaptable multi-person interactions suitable for diverse animation, gaming, and robotics applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sakuya Ota",
      "Qing Yu",
      "Kent Fujiwara",
      "Satoshi Ikehata",
      "Ikuro Sato"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gao_MagicDrive-V2_High-Resolution_Long_Video_Generation_for_Autonomous_Driving_with_Adaptive_ICCV_2025_paper.html": {
    "title": "MagicDrive-V2: High-Resolution Long Video Generation for Autonomous Driving with Adaptive Control",
    "volume": "main",
    "abstract": "The rapid advancement of diffusion models has greatly improved video synthesis, especially in controllable video generation, which is vital for applications like autonomous driving. Although DiT with 3D VAE has become a standard framework for video generation, it introduces challenges in controllable driving video generation, especially for frame-wise geometric control, rendering existing methods ineffective. To address these issues, we propose MagicDrive-V2, a novel approach that integrates the MVDiT block and spatial-temporal conditional encoding to enable multi-view video generation and precise geometric control. Additionally, we introduce an efficient method for obtaining contextual descriptions for videos to support diverse textual control, along with a progressive training strategy using mixed video data to enhance training efficiency and generalizability. Consequently, MagicDrive-V2 enables multi-view driving video synthesis with 3.3x resolution and 4x frame count (compared to current SOTA), rich contextual control, and geometric controls. Extensive experiments demonstrate MagicDrive-V2's ability, unlocking broader applications in autonomous driving. Project page: https://flymin.github.io/magicdrive-v2/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruiyuan Gao",
      "Kai Chen",
      "Bo Xiao",
      "Lanqing Hong",
      "Zhenguo Li",
      "Qiang Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hong_Communication-Efficient_Multi-Vehicle_Collaborative_Semantic_Segmentation_via_Sparse_3D_Gaussian_Sharing_ICCV_2025_paper.html": {
    "title": "Communication-Efficient Multi-Vehicle Collaborative Semantic Segmentation via Sparse 3D Gaussian Sharing",
    "volume": "main",
    "abstract": "Collaborative perception is considered a promising approach to address the inherent limitations of single-vehicle systems by sharing data among vehicles, thereby enhancing performance in perception tasks such as bird's-eye view (BEV) semantic segmentation. However, existing methods share the entire dense, scene-level BEV feature, which contains significant redundancy and lacks height information, ultimately leading to unavoidable bandwidth waste and performance degradation. To address these challenges, we present GSCOOP, the first collaborative semantic segmentation framework that leverages sparse, object-centric 3D Gaussians to fundamentally overcome communication bottlenecks. By representing scenes with compact Gaussians that preserve complete spatial information, GSCOOP achieves both high perception accuracy and communication efficiency. To further optimize transmission, we introduce the Priority-Based Gaussian Selection (PGS) module to adaptively select critical Gaussians and a Semantic Gaussian Compression (SGC) module to compress Gaussian attributes with minimal overhead. Extensive experiments on OPV2V and V2X-Seq demonstrate that GSCOOP achieves state-of-the-art performance, even with more than 500X lower communication volume",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyu Hong",
      "Xiaobo Zhou",
      "Wenkai Hu",
      "Qi Xie",
      "Zhihui Ke",
      "Tie Qiu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Du_Category-Specific_Selective_Feature_Enhancement_for_Long-Tailed_Multi-Label_Image_Classification_ICCV_2025_paper.html": {
    "title": "Category-Specific Selective Feature Enhancement for Long-Tailed Multi-Label Image Classification",
    "volume": "main",
    "abstract": "Since real-world multi-label data often exhibit significant label imbalance, long-tailed multi-label image classification has emerged as a prominent research area in computer vision. Traditionally, it is considered that deep neural networks' classifiers are vulnerable to long-tailed distributions, whereas the feature extraction backbone remains relatively robust. However, our analysis from the feature learning perspective reveals that the backbone struggles to maintain high sensitivity to sample-scarce categories but retains the ability to localize specific areas effectively. Based on this observation, we propose a new model for long-tailed multi-label image classification named category-specific selective feature enhancement (CSSFE). First, it utilizes the retained localization capability of the backbone to capture label-dependent class activation maps. Then, a progressive attention enhancement mechanism, updating from head to medium to tail categories, is introduced to address the low-confidence issue in medium and tail categories. Finally, visual features are extracted according to the optimized class activation maps and combined with semantic information to perform the classification task. Extensive experiments on two benchmark datasets highlight our findings' generalizability and the proposed CSSFE's superior performance",
    "checked": false,
    "id": "efd7646c527fd45d1187f6c1989bf997cf0374c9",
    "semantic_title": "category-prompt refined feature learning for long-tailed multi-label image classification",
    "citation_count": 3,
    "authors": [
      "Ruiqi Du",
      "Xu Tang",
      "Xiangrong Zhang",
      "Jingjing Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Pujol-Perich_Sparse-Dense_Side-Tuner_for_efficient_Video_Temporal_Grounding_ICCV_2025_paper.html": {
    "title": "Sparse-Dense Side-Tuner for efficient Video Temporal Grounding",
    "volume": "main",
    "abstract": "Video Temporal Grounding (VTG) involves Moment Retrieval (MR) and Highlight Detection (HD) based on textual queries. For this, most methods rely solely on final-layer features of frozen large pre-trained backbones, limiting their adaptability to new domains. While full fine-tuning is often impractical, parameter-efficient fine-tuning --and particularly side-tuning (ST)-- has emerged as an effective alternative. However, prior ST approaches this problem from a frame-level refinement perspective, overlooking the inherent sparse nature of MR. To address this, we propose the Sparse-Dense Side-Tuner (SDST), the first anchor-free ST architecture for VTG. We also introduce the Reference-based Deformable Self-Attention, a novel mechanism that enhances the context modeling of the deformable attention --a key limitation of existing anchor-free methods. Additionally, we present the first effective integration of InternVideo2 backbone into an ST framework, showing its profound implications in performance. Overall, our method significantly improves existing ST methods, achieving highly competitive or SOTA results on QVHighlights, TACoS, and Charades-STA, while reducing up to a 73% the parameter count w.r.t. the existing SOTA methods. The code is publicly accessible at https://github.com/davidpujol/SDST",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Pujol-Perich",
      "Sergio Escalera",
      "Albert Clapés"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jiang_Corvid_Improving_Multimodal_Large_Language_Models_Towards_Chain-of-Thought_Reasoning_ICCV_2025_paper.html": {
    "title": "Corvid: Improving Multimodal Large Language Models Towards Chain-of-Thought Reasoning",
    "volume": "main",
    "abstract": "Recent advancements in multimodal large language models (MLLMs) have demonstrated exceptional performance in multimodal perception and understanding. However, leading open-source MLLMs exhibit significant limitations in complex and structured reasoning, particularly in tasks requiring deep reasoning for decision-making and problem-solving. In this work, we present Corvid, an MLLM with enhanced chain-of-thought (CoT) reasoning capabilities. Architecturally, Corvid incorporates a hybrid vision encoder for informative visual representation and a meticulously designed connector (GateMixer) to facilitate cross-modal alignment. To enhance Corvid's CoT reasoning capabilities, we introduce MCoT-Instruct-287K, a high-quality multimodal CoT instruction-following dataset, refined and standardized from diverse public reasoning sources. Leveraging this dataset, we fine-tune Corvid with a two-stage CoT-formatted training approach to progressively enhance its step-by-step reasoning abilities. Furthermore, we propose an effective inference-time scaling strategy that enables Corvid to mitigate over-reasoning and under-reasoning through self-verification. Extensive experiments demonstrate that Corvid outperforms existing o1-like MLLMs and state-of-the-art MLLMs with similar parameter scales, with notable strengths in mathematical reasoning and science problem-solving. Project page: https://mm-vl.github.io/corvid",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingjing Jiang",
      "Chao Ma",
      "Xurui Song",
      "Hanwang Zhang",
      "Jun Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_CompCap_Improving_Multimodal_Large_Language_Models_with_Composite_Captions_ICCV_2025_paper.html": {
    "title": "CompCap: Improving Multimodal Large Language Models with Composite Captions",
    "volume": "main",
    "abstract": "How well can Multimodal Large Language Models (MLLMs) understand composite images? Composite images (CIs) are synthetic visuals created by merging multiple visual elements, such as charts, posters, or screenshots, rather than being captured directly by a camera. While CIs are prevalent in real-world applications, recent MLLM developments have primarily focused on interpreting natural images (NIs). Our research reveals that current MLLMs face significant challenges in accurately understanding CIs, often struggling to extract information or perform complex reasoning based on these images. We find that existing training data for CIs are mostly formatted for question-answer tasks (e.g., in datasets like ChartQA and ScienceQA), while high-quality image-caption datasets, critical for robust vision-language alignment, are only available for NIs. To bridge this gap, we introduce Composite Captions (CompCap), a flexible framework that leverages Large Language Models (LLMs) and automation tools to synthesize CIs with accurate and detailed captions. Using CompCap, we curate CompCap-118K, a dataset containing 118K image-caption pairs across six CI types. We validate the effectiveness of CompCap-118K by supervised fine-tuning MLLMs of three sizes: xGen-MM-inst.-4B and LLaVA-NeXT-Vicuna-7B/13B. Empirical results show that CompCap-118K significantly enhances MLLMs' understanding of CIs, yielding average gains of 1.7%, 2.0%, and 2.9% across eleven benchmarks, respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaohui Chen",
      "Satya Narayan Shukla",
      "Mahmoud Azab",
      "Aashu Singh",
      "Qifan Wang",
      "David Yang",
      "ShengYun Peng",
      "Hanchao Yu",
      "Shen Yan",
      "Xuewen Zhang",
      "Baosheng He"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_COSMO_Combination_of_Selective_Memorization_for_Low-cost_Vision-and-Language_Navigation_ICCV_2025_paper.html": {
    "title": "COSMO: Combination of Selective Memorization for Low-cost Vision-and-Language Navigation",
    "volume": "main",
    "abstract": "Vision-and-Language Navigation (VLN) tasks have gained prominence within artificial intelligence research due to their potential application in fields like home assistants. Many contemporary VLN approaches, while based on transformer architectures, have increasingly incorporated additional components such as external knowledge bases or map information to enhance performance. These additions, while boosting performance, also lead to larger models and increased computational costs. In this paper, to achieve both high performance and low computational costs, we propose a novel architecture with the **co**mbination of **s**elective **m**em**o**rization (COSMO), which integrates state-space modules (SSMs) and transformer modules. However, direct application of SSMs in VLN results in significant performance degradation. Therefore, we propose two VLN-customized selective state space modules: the Round Selective Scan (RSS) and the Cross-modal Selective State Space Module (CS3). RSS facilitates comprehensive inter-modal interactions within a single scan, while the CS3 module adapts the selective state space module into a dual-stream architecture, thereby enhancing the acquisition of cross-modal interactions. Experimental validations on three mainstream VLN benchmarks, REVERIE, R2R, and R2R-CE, not only demonstrate competitive navigation performance of our model but also show a significant reduction in computational costs. Code is available at \\href https://github.com/siqiZ805/VLN-COSMO.git VLN-COSMO",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siqi Zhang",
      "Yanyuan Qiao",
      "Qunbo Wang",
      "Zike Yan",
      "Qi Wu",
      "Zhihua Wei",
      "Jing Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_STI-Bench_Are_MLLMs_Ready_for_Precise_Spatial-Temporal_World_Understanding_ICCV_2025_paper.html": {
    "title": "STI-Bench: Are MLLMs Ready for Precise Spatial-Temporal World Understanding?",
    "volume": "main",
    "abstract": "The use of Multimodal Large Language Models (MLLMs) as an end-to-end solution for Embodied AI and Autonomous Driving has become a prevailing trend. While MLLMs have been extensively studied for visual semantic understanding tasks, their ability to perform precise and quantitative spatial-temporal understanding in real-world applications remains largely unexamined, leading to uncertain prospects. To address this gap, we introduce ST-Bench, a benchmark designed to evaluate MLLMs' spatial-temporal understanding through challenging tasks such as estimating and predicting the appearance, pose, displacement, and motion of objects. Our benchmark encompasses a wide range of robot and vehicle operations across desktop, indoor, and outdoor scenarios. The extensive experiments reveals that the state-of-the-art MLLMs still struggle in real-world spatial-temporal understanding, especially in tasks requiring precise distance estimation and motion analysis",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yun Li",
      "Yiming Zhang",
      "Tao Lin",
      "Xiangrui Liu",
      "Wenxiao Cai",
      "Zheng Liu",
      "Bo Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Describe_Adapt_and_Combine_Empowering_CLIP_Encoders_for_Open-set_3D_ICCV_2025_paper.html": {
    "title": "Describe, Adapt and Combine: Empowering CLIP Encoders for Open-set 3D Object Retrieval",
    "volume": "main",
    "abstract": "Open-set 3D object retrieval (3DOR) is an emerging task aiming to retrieve 3D objects of unseen categories beyond the training set. Existing methods typically utilize all modalities (i.e., voxels, point clouds, multi-view images) and train specific backbones before fusion. However, they still struggle to produce generalized representations due to insufficient 3D training data. Being contrastively pre-trained on web-scale image-text pairs, CLIP inherently produces generalized representations for a wide range of downstream tasks. Building upon it, we present a simple yet effective framework named Describe, Adapt and Combine (DAC) by taking only multi-view images for open-set 3DOR. DAC innovatively synergizes a CLIP model with a multi-modal large language model (MLLM) to learn generalized 3D representations, where the MLLM is used for dual purposes. First, it describes the seen category information to align with CLIP's training objective for adaptation during training. Second, it provides external hints about unknown objects complementary to visual cues during inference. To improve the synergy, we introduce an Additive-Bias Low-Rank adaptation (AB-LoRA), which alleviates overfitting and further enhances the generalization to unseen categories. With only multi-view images, DAC significantly surpasses prior arts by an average of +10.01% mAP on four open-set 3DOR datasets. Moreover, its generalization is also validated on image-based and cross-dataset setups. Code is available at https://github.com/wangzhichuan123/DAC",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhichuan Wang",
      "Yang Zhou",
      "Zhe Liu",
      "Rui Yu",
      "Song Bai",
      "Yulong Wang",
      "Xinwei He",
      "Xiang Bai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Galliena_Embodied_Image_Captioning_Self-supervised_Learning_Agents_for_Spatially_Coherent_Image_ICCV_2025_paper.html": {
    "title": "Embodied Image Captioning: Self-supervised Learning Agents for Spatially Coherent Image Descriptions",
    "volume": "main",
    "abstract": "We present a self-supervised method to improve an agent's abilities in describing arbitrary objects while actively exploring a generic environment. This is a challenging problem, as current models struggle to obtain coherent image captions due to different camera viewpoints and clutter. We propose a three-phase framework to fine-tune existing captioning models that enhances caption accuracy and consistency across views via a consensus mechanism. First, an agent explores the environment, collecting noisy image-caption pairs. Then, a consistent pseudo-caption for each object instance is distilled via consensus using a large language model. Finally, these pseudo-captions are used to fine-tune an off-the-shelf captioning model, with the addition of contrastive learning. We analyse the performance of the combination of captioning models, exploration policies, pseudo-labeling methods, and fine-tuning strategies, on our manually labeled test set. Results show that a policy can be trained to mine samples with higher disagreement compared to classical baselines. Our pseudo-captioning method, in combination with all policies, has a higher semantic similarity compared to other existing methods, and fine-tuning improves caption accuracy and consistency by a significant margin. Code and test set annotations available at https://hsp-iit.github.io/embodied-captioning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tommaso Galliena",
      "Tommaso Apicella",
      "Stefano Rosa",
      "Pietro Morerio",
      "Alessio Del Bue",
      "Lorenzo Natale"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Mallis_CAD-Assistant_Tool-Augmented_VLLMs_as_Generic_CAD_Task_Solvers_ICCV_2025_paper.html": {
    "title": "CAD-Assistant: Tool-Augmented VLLMs as Generic CAD Task Solvers",
    "volume": "main",
    "abstract": "We propose CAD-Assistant, a general-purpose CAD agent for AI-assisted design. Our approach is based on a powerful Vision and Large Language Model (VLLM) as a planner and a tool-augmentation paradigm using CAD-specific tools. CAD-Assistant addresses multimodal user queries by generating actions that are iteratively executed on a Python interpreter equipped with the FreeCAD software, accessed via its Python API. Our framework is able to assess the impact of generated CAD commands on geometry and adapts subsequent actions based on the evolving state of the CAD design. We consider a wide range of CAD-specific tools including a sketch image parameterizer, rendering modules, a 2D cross-section generator, and other specialized routines. CAD-Assistant is evaluated on multiple CAD benchmarks, where it outperforms VLLM baselines and supervised task-specific methods. Beyond existing benchmarks, we qualitatively demonstrate the potential of tool-augmented VLLMs as general-purpose CAD solvers across diverse workflows",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dimitrios Mallis",
      "Ahmet Serda Karadeniz",
      "Sebastian Cavada",
      "Danila Rukhovich",
      "Niki Foteinopoulou",
      "Kseniya Cherenkova",
      "Anis Kacem",
      "Djamila Aouada"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xu_Fast_Image_Super-Resolution_via_Consistency_Rectified_Flow_ICCV_2025_paper.html": {
    "title": "Fast Image Super-Resolution via Consistency Rectified Flow",
    "volume": "main",
    "abstract": "Diffusion models (DMs) have demonstrated remarkable success in real-world image super-resolution (SR), yet their reliance on time-consuming multi-step sampling largely hinders their practical applications. While recent efforts have introduced few- or single-step solutions, existing methods either inefficiently model the process from noisy input or fail to fully exploit iterative generative priors, compromising the fidelity and quality of the reconstructed images. To address this issue, we propose FlowSR, a novel approach that reformulates the SR problem as a rectified flow from low-resolution (LR) to high-resolution (HR) images. Our method leverages an improved consistency learning strategy to enable high-quality SR in a single step. Specifically, we refine the original consistency distillation process by incorporating HR regularization, ensuring that the learned SR flow not only enforces self-consistency but also converges precisely to the ground-truth HR target. Furthermore, we introduce a fast-slow scheduling strategy, where adjacent timesteps for consistency learning are sampled from two distinct schedulers: a fast scheduler with fewer timesteps to improve efficiency, and a slow scheduler with more timesteps to capture fine-grained texture details. Extensive experiments demonstrate that FlowSR achieves outstanding performance in both efficiency and image quality",
    "checked": false,
    "id": "a502b89e179220407e33e9098db0a033664de9d9",
    "semantic_title": "nami: efficient image generation via progressive rectified flow transformers",
    "citation_count": 1,
    "authors": [
      "Jiaqi Xu",
      "Wenbo Li",
      "Haoze Sun",
      "Fan Li",
      "Zhixin Wang",
      "Long Peng",
      "Jingjing Ren",
      "Haoran Yang",
      "Xiaowei Hu",
      "Renjing Pei",
      "Pheng-Ann Heng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yi_Adapt_Foundational_Segmentation_Models_with_Heterogeneous_Searching_Space_ICCV_2025_paper.html": {
    "title": "Adapt Foundational Segmentation Models with Heterogeneous Searching Space",
    "volume": "main",
    "abstract": "Foundation Segmentation Models (FSMs) show suboptimal performance on unconventional image domains like camouflage objects. Fine-tuning is often impractical due to data preparation challenges, time limits, and optimization issues. To boost segmentation performance while keeping zero-shot features, one approach is pre-augmenting images for the segmentation model. However, existing image augmentations mainly depend on rule-based methods, restricting augmentation effectiveness. Though learning-based methods can diversify augmentation, rule-based ones are degree-describable (e.g., slight/intense brightening), while learning-based methods usually predict non-degree-describable ground truths (e.g., depth estimation), creating a heterogeneous search space when combined. To this end, we propose an \"Augmenting-to-Adapt\" paradigm, replacing traditional rule-based augmentation with an optimal heterogeneous augmentation policy to enhance segmentation. Our method uses 32 augmentation techniques (22 rule-based, 10 learning-based) to ease parameter misalignment, forming a robust, multi-discrete heterogeneous search space.To apply the optimal policy in real-world scenarios, we distill the augmentation process to speed up the preprocess. Extensive evaluations across diverse datasets and domains show our method significantly improves model adaptation with a domain-specific augmentation strategy. We will release our code to support further research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li Yi",
      "Jie Hu",
      "Songan Zhang",
      "Guannan Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Adversarial_Exploitation_of_Data_Diversity_Improves_Visual_Localization_ICCV_2025_paper.html": {
    "title": "Adversarial Exploitation of Data Diversity Improves Visual Localization",
    "volume": "main",
    "abstract": "Visual localization, which estimates a camera's pose within a known scene, is a fundamental capability for autonomous systems. While absolute pose regression (APR) methods have shown promise for efficient inference, they often struggle with generalization. Recent approaches attempt to address this through data augmentation with varied viewpoints, yet they overlook a critical factor: appearance diversity.In this work, we identify appearance variation as the key to robust localization. Specifically, we first lift real 2D images into 3D Gaussian Splats with varying appearance and deblurring capabilities, enabling the synthesis of diverse training data that varies not just in poses but also in environmental conditions such as lighting and weather. To fully unleash the potential of the appearance-diverse data, we build a two-branch joint training pipeline with an adversarial discriminator to bridge the syn-to-real gap.Extensive experiments demonstrate that our approach significantly outperforms state-of-the-art methods, reducing translation and rotation errors by 50% and 22% on indoor datasets, and 37% and 42% on outdoor datasets. Most notably, our method shows remarkable robustness in dynamic driving scenarios under varying weather conditions and in day-to-night scenarios, where previous APR methods fail",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sihang Li",
      "Siqi Tan",
      "Bowen Chang",
      "Jing Zhang",
      "Chen Feng",
      "Yiming Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Learning_Implicit_Features_with_Flow-Infused_Transformations_for_Realistic_Virtual_Try-On_ICCV_2025_paper.html": {
    "title": "Learning Implicit Features with Flow-Infused Transformations for Realistic Virtual Try-On",
    "volume": "main",
    "abstract": "Diffusion-based virtual try-on aims to synthesize a realistic image that seamlessly integrating the specific garment into a target model. The primary challenge lies in effectively guiding the warping process of the latent diffusion model. However, previous methods either lack direct guidance or explicitly warp the garment image, which highly depends on the performance of the warping module. In this paper, we propose FIA-VTON, which leverages the implicit flow feature as guidance by adopting a Flow Infused Attention module on virtual try-on. The dense warp flow map is projected as indirect guidance to enhance the feature map warping in the generation process implicitly, which is less sensitive to the warping estimation accuracy than an explicit warp of the garment image. To further enhance implicit warp guidance, we incorporate high-level spatial attention to complement the dense warp. Experimental results on the VTON-HD and DressCode dataset significantly outperform state-of-the-art methods, demonstrating that FIA-VTON is effective and robust for virtual try-on",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Delong Zhang",
      "Qiwei Huang",
      "Yang Sun",
      "Yuanliu Liu",
      "Wei-Shi Zheng",
      "Pengfei Xiong",
      "Wei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yu_BoxDreamer_Dreaming_Box_Corners_for_Generalizable_Object_Pose_Estimation_ICCV_2025_paper.html": {
    "title": "BoxDreamer: Dreaming Box Corners for Generalizable Object Pose Estimation",
    "volume": "main",
    "abstract": "This paper presents a generalizable RGB-based approach for object pose estimation, specifically designed to address challenges in sparse-view settings. While existing methods can estimate the poses of unseen objects, their generalization ability remains limited in scenarios involving occlusions and sparse reference views, restricting their real-world applicability. To overcome these limitations, we introduce corner points of the object bounding box as an intermediate representation of the object pose. The 3D object corners can be reliably recovered from sparse input views, while the 2D corner points in the target view are estimated through a novel reference-based point synthesizer, which works well even in scenarios involving occlusions. As object semantic points, object corners naturally establish 2D-3D correspondences for object pose estimation with a PnP algorithm. Extensive experiments on the YCB-Video and Occluded-LINEMOD datasets show that our approach outperforms state-of-the-art methods, highlighting the effectiveness of the proposed representation and significantly enhancing the generalization capabilities of object pose estimation, which is crucial for real-world applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanhong Yu",
      "Xingyi He",
      "Chen Zhao",
      "Junhao Yu",
      "Jiaqi Yang",
      "Ruizhen Hu",
      "Yujun Shen",
      "Xing Zhu",
      "Xiaowei Zhou",
      "Sida Peng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yan_LawDIS_Language-Window-based_Controllable_Dichotomous_Image_Segmentation_ICCV_2025_paper.html": {
    "title": "LawDIS: Language-Window-based Controllable Dichotomous Image Segmentation",
    "volume": "main",
    "abstract": "We present LawDIS, a language-window-based controllable dichotomous image segmentation (DIS) framework that produces high-quality object masks. Our framework recasts DIS as an image-conditioned mask generation task within a latent diffusion model, enabling seamless integration of user controls. LawDIS is enhanced with macro-to-micro control modes. Specifically, in macro mode, we introduce a language-controlled segmentation strategy (LS) to generate an initial mask based on user-provided language prompts. In micro mode, a window-controlled refinement strategy (WR) allows flexible refinement of user-defined regions (i.e., size-adjustable windows) within the initial mask. Coordinated by a mode switcher, these modes can operate independently or jointly, making the framework well-suited for high-accuracy, personalised applications. Extensive experiments on the DIS5K benchmark reveal that our LawDIS significantly outperforms 11 cutting-edge methods across all metrics. Notably, compared to the second-best model MVANet, we achieve weighted F-measure gains of 4.6% with both the LS and WR strategies and 3.6% gains with only the LS strategy on DIS-TE. Codes will be made available at https://github.com/XinyuYanTJU/LawDIS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyu Yan",
      "Meijun Sun",
      "Ge-Peng Ji",
      "Fahad Shahbaz Khan",
      "Salman Khan",
      "Deng-Ping Fan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zemskova_3DGraphLLM_Combining_Semantic_Graphs_and_Large_Language_Models_for_3D_ICCV_2025_paper.html": {
    "title": "3DGraphLLM: Combining Semantic Graphs and Large Language Models for 3D Scene Understanding",
    "volume": "main",
    "abstract": "A 3D scene graph represents a compact scene model by capturing both the objects present and the semantic relationships between them, making it a promising structure for robotic applications. To effectively interact with users, an embodied intelligent agent should be able to answer a wide range of natural language queries about the surrounding 3D environment. Large Language Models (LLMs) are beneficial solutions for user-robot interaction due to their natural language understanding and reasoning abilities. Recent methods for learning scene representations have shown that adapting these representations to the 3D world can significantly improve the quality of LLM responses. However, existing methods typically rely only on geometric information, such as object coordinates, and overlook the rich semantic relationships between objects. In this work, we propose 3DGraphLLM, a method for constructing a learnable representation of a 3D scene graph that explicitly incorporates semantic relationships. This representation is used as input to LLMs for performing 3D vision-language tasks. In our experiments on popular ScanRefer, Multi3DRefer, ScanQA, Sqa3D, and Scan2cap datasets, we demonstrate that our approach outperforms baselines that do not leverage semantic relationships between objects. The code is publicly available at https://github.com/CognitiveAISystems/3DGraphLLM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tatiana Zemskova",
      "Dmitry Yudin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_FineMotion_A_Dataset_and_Benchmark_with_both_Spatial_and_Temporal_ICCV_2025_paper.html": {
    "title": "FineMotion: A Dataset and Benchmark with both Spatial and Temporal Annotation for Fine-grained Motion Generation and Editing",
    "volume": "main",
    "abstract": "Generating realistic human motions from textual descriptions has undergone significant advancements. However, existing methods often overlook specific body part movements and their timing. In this paper, we address this issue by enriching the textual description with more details. Specifically, we propose the FineMotion dataset, which contains over 442,000 human motion snippets -- short segments of human motion sequences -- and their corresponding detailed descriptions of human body part movements. Additionally, the dataset includes about 95k detailed paragraphs describing the movements of human body parts in entire motion sequences. Experimental results demonstrate the significance of our dataset on the text-driven fine-grained human motion generation task, especially with a remarkable +15.3% improvement in Top-3 accuracy for the MDM model. Notably, we further support a zero-shot pipeline of fine-grained motion editing, which focuses on detailed editing in both spatial and temporal dimensions via text. Dataset and code available at: CVI-SZU/FineMotion",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bizhu Wu",
      "Jinheng Xie",
      "Meidan Ding",
      "Zhe Kong",
      "Jianfeng Ren",
      "Ruibin Bai",
      "Rong Qu",
      "Linlin Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xu_MC-Bench_A_Benchmark_for_Multi-Context_Visual_Grounding_in_the_Era_ICCV_2025_paper.html": {
    "title": "MC-Bench: A Benchmark for Multi-Context Visual Grounding in the Era of MLLMs",
    "volume": "main",
    "abstract": "While multimodal large language models (MLLMs) have demonstrated extraordinary vision-language understanding capabilities, their abilities to solve instance-level visual-language problems beyond a single image warrant further exploration. To assess these unproven abilities of MLLMs, this paper proposes a new visual grounding task called multi-context visual grounding, which aims to localize instances of interest across multiple images based on open-ended text prompts. In order to facilitate this research, we construct a new dataset MC-Bench that features 2K high-quality and manually annotated samples. Each sample consists of an instance-level labeled image pair and a corresponding text prompt that indicates the target instances in the images. These text prompts are highly open-ended and follow three distinct styles, covering 20 practical skills. We benchmark over 20 state-of-the-art MLLMs and foundation models with potential multi-context visual grounding capabilities, along with our developed simple yet effective agentic baseline and a finetuned baseline by multi-context instruction tuning. Our evaluation reveals a non-trivial performance gap between existing MLLMs and humans, along with some insightful observations that suggest potential future directions. We hope that MC-Bench and our empirical findings encourage the research community to further advance the untapped potentials of MLLMs in instance-level tasks, particularly in multi-image contexts. Project page: https://xuyunqiu.github.io/MC-Bench",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunqiu Xu",
      "Linchao Zhu",
      "Yi Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ni_What_Makes_for_Text_to_360-degree_Panorama_Generation_with_Stable_ICCV_2025_paper.html": {
    "title": "What Makes for Text to 360-degree Panorama Generation with Stable Diffusion?",
    "volume": "main",
    "abstract": "Recent prosperity of text-to-image diffusion models, e.g. Stable Diffusion, has stimulated research to adapt them to 360-degree panorama generation. Prior work has demonstrated the feasibility of using conventional low-rank adaptation techniques on pre-trained diffusion models to generate panoramic images. However, the substantial domain gap between perspective and panoramic images raises questions about the underlying mechanisms enabling this empirical success. We hypothesize and examine that the trainable counterparts exhibit distinct behaviors when fine-tuned on panoramic data, and such an adaptation conceals some intrinsic mechanism to leverage the prior knowledge within the pre-trained diffusion models. Our analysis reveals the following: 1) the query and key matrices in the attention modules are responsible for common information that can be shared between the panoramic and perspective domains, thus are less relevant to panorama generation; and 2) the value and output weight matrices specialize in adapting pre-trained knowledge to the panoramic domain, playing a more critical role during fine-tuning for panorama generation. We empirically verify these insights by introducing a simple framework called UniPano, with the objective of establishing an elegant baseline for future research. UniPano not only outperforms existing methods but also significantly reduces memory usage and training time compared to prior dual-branch approaches, making it scalable for end-to-end panorama generation with higher resolution. The code is available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinhong Ni",
      "Chang-Bin Zhang",
      "Qiang Zhang",
      "Jing Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Han_CHORDS_Diffusion_Sampling_Accelerator_with_Multi-core_Hierarchical_ODE_Solvers_ICCV_2025_paper.html": {
    "title": "CHORDS: Diffusion Sampling Accelerator with Multi-core Hierarchical ODE Solvers",
    "volume": "main",
    "abstract": "Diffusion-based generative models have become dominant generators of high-fidelity images and videos but remain limited by their computationally expensive inference procedures. Existing acceleration techniques either require extensive model retraining or compromise significantly on sample quality. This paper explores a general, training-free, and model-agnostic acceleration strategy via multi-core parallelism. Our framework views multi-core diffusion sampling as an ODE solver pipeline, where slower yet accurate solvers progressively rectify faster solvers through a theoretically justified inter-core communication mechanism. This motivates our multi-core training-free diffusion sampling accelerator, CHORDS, which is compatible with various diffusion samplers, model architectures, and modalities. Through extensive experiments, CHORDS significantly accelerates sampling across diverse large-scale image and video diffusion models, yielding up to 2.1x speedup with four cores, improving by 50% over baselines, and 2.9x speedup with eight cores, all without quality degradation. This advancement enables CHORDS to establish a solid foundation for real-time, high-fidelity diffusion generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqi Han",
      "Haotian Ye",
      "Puheng Li",
      "Minkai Xu",
      "James Zou",
      "Stefano Ermon"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Faure_HERMES_temporal-coHERent_long-forM_understanding_with_Episodes_and_Semantics_ICCV_2025_paper.html": {
    "title": "HERMES: temporal-coHERent long-forM understanding with Episodes and Semantics",
    "volume": "main",
    "abstract": "Long-form video understanding presents unique challenges that extend beyond traditional short-video analysis approaches, particularly in capturing long-range dependencies, processing redundant information efficiently, and extracting high-level semantic concepts. To address these challenges, we propose a novel approach that more accurately reflects human cognition. This paper introduces HERMES: temporal-coHERent long-forM understanding with Episodes and Semantics, featuring two versatile modules that can enhance existing video-language models or operate as a standalone system. Our Episodic COmpressor (ECO) efficiently aggregates representations from micro to semi-macro levels, reducing computational overhead while preserving temporal dependencies. Our Semantics ReTRiever (SeTR) enriches these representations with semantic information by focusing on broader context, dramatically reducing feature dimensionality while preserving relevant macro-level information. We demonstrate that these modules can be seamlessly integrated into existing SOTA models, consistently improving their performance while reducing inference latency by up to 43% and memory usage by 46%. As a standalone system, HERMES achieves state-of-the-art performance across multiple long-video understanding benchmarks in both zero-shot and fully-supervised settings. Our project page and code can be found at https://joslefaure.github.io/assets/html/hermes.html",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gueter Josmy Faure",
      "Jia-Fong Yeh",
      "Min-Hung Chen",
      "Hung-Ting Su",
      "Shang-Hong Lai",
      "Winston H. Hsu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zheng_CoralSRT_Revisiting_Coral_Reef_Semantic_Segmentation_by_Feature_Rectification_via_ICCV_2025_paper.html": {
    "title": "CoralSRT: Revisiting Coral Reef Semantic Segmentation by Feature Rectification via Self-supervised Guidance",
    "volume": "main",
    "abstract": "We investigate coral reef semantic segmentation, in which coral reefs are governed by multifaceted factors, like genes, environmental changes, and internal interactions. Unlike segmenting structural units/instances, which are predictable and follow a set pattern, also referred to as commonsense or prior, segmenting coral reefs involves modeling self-repeated, asymmetric, and amorphous distribution of elements, e.g., corals can grow in almost any shape and appearance. We revisited existing segmentation approaches and found that both computer vision and coral reef communities failed to incorporate the intrinsic properties of corals into model design. In this work, we propose a simple formulation for coral reef semantic segmentation: we regard the segment as the basis to model both within-segment and cross-segment affinities. We propose CoralSRT, a feature rectification module via self-supervised guidance, to reduce the stochasticity of coral features extracted by powerful foundation models (FMs), as demonstrated in Fig. 1. We incorporate intrinsic properties of corals to strengthen within-segment affinity by guiding the features within generated segments to align with the centrality. We investigate features from FMs that were optimized by various pretext tasks on significantly large-scale unlabeled or labeled data, which already contain rich information for modeling both within-segment and cross-segment affinities, enabling the adaptation of FMs for coral segmentation. CoralSRT can rectify features from FMs to more efficient features for label propagation and lead to further significant semantic segmentation performance gains, all without requiring additional human supervision, retraining/finetuning FMs or even domain-specific data. These advantages help reduce human effort and the need for domain expertise in data collection and labeling. Our method is easy to implement, and also task- and model-agnostic. CoralSRT bridges the self-supervised pre-training and supervised training in the feature space, also offering insights for segmenting elements/stuffs (e.g., grass, plants, cells, and biofoulings)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziqiang Zheng",
      "Yuk-Kwan Wong",
      "Binh-Son Hua",
      "Jianbo Shi",
      "Sai-Kit Yeung"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhu_VGMamba_Attribute-to-Location_Clue_Reasoning_for_Quantity-Agnostic_3D_Visual_Grounding_ICCV_2025_paper.html": {
    "title": "VGMamba: Attribute-to-Location Clue Reasoning for Quantity-Agnostic 3D Visual Grounding",
    "volume": "main",
    "abstract": "As an important direction of embodied intelligence, 3D Visual Grounding has attracted much attention, aiming to identify 3D objects matching the given language description. Most existing methods often follow a two-stage process, i.e., first detecting proposal objects and identifying the right objects based on the relevance to the given query. However, when the query is complex, it is difficult to leverage an abstract language representation to lock the corresponding objects accurately, affecting the grounding performance. In general, given a specific object, humans usually follow two clues to finish the corresponding grounding, i.e., attribute and location clues. To this end, we explore a new mechanism, attribute-to-location clue reasoning, to conduct accurate grounding. Particularly, we propose a VGMamba network that consists of an SVD-based attribute mamba, location mamba, and multi-modal fusion mamba. Taking a 3D point cloud scene and language query as the input, we first exploit SVD to make a decomposition of the extracted features. Then, a slidingwindow operation is conducted to capture attribute characteristics. Next, a location mamba is presented to obtain the corresponding location information. Finally, by means of multi-modal mamba fusion, the model could effectively localize the object that matches the given query. In the experiment, our method is verified on four datasets. Extensive experimental results demonstrate the superiority of our method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihang Zhu",
      "Jinhao Zhang",
      "Yuxuan Wang",
      "Aming Wu",
      "Cheng Deng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shi_Decouple_and_Track_Benchmarking_and_Improving_Video_Diffusion_Transformers_For_ICCV_2025_paper.html": {
    "title": "Decouple and Track: Benchmarking and Improving Video Diffusion Transformers For Motion Transfer",
    "volume": "main",
    "abstract": "The motion transfer task aims to transfer motion from a source video to newly generated videos, requiring the model to decouple motion from appearance. Previous diffusion-based methods primarily rely on separate spatial and temporal attention mechanisms within the 3D U-Net. In contrast, state-of-the-art video Diffusion Transformers (DiT) models use 3D full attention, which does not explicitly separate temporal and spatial information. Thus, the interaction between spatial and temporal dimensions makes decoupling motion and appearance more challenging for DiT models. In this paper, we propose DeT, a method that adapts DiT models to improve motion transfer ability. Our approach introduces a simple yet effective temporal kernel to smooth DiT features along the temporal dimension, facilitating the decoupling of foreground motion from background appearance. Meanwhile, the temporal kernel effectively captures temporal variations in DiT features, which are closely related to motion. Moreover, we introduce explicit supervision along dense trajectories in the latent feature space to further enhance motion consistency. Additionally, we present MTBench, a general and challenging benchmark for motion transfer. We also introduce a hybrid motion fidelity metric that considers both the global and local motion similarity. Therefore, our work provides a more comprehensive evaluation than previous works. Extensive experiments on MTBench demonstrate that DeT achieves the best trade-off between motion fidelity and edit fidelity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingyu Shi",
      "Jianzong Wu",
      "Jinbin Bai",
      "Jiangning Zhang",
      "Lu Qi",
      "Yunhai Tong",
      "Xiangtai Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Pinyoanuntapong_MaskControl_Spatio-Temporal_Control_for_Masked_Motion_Synthesis_ICCV_2025_paper.html": {
    "title": "MaskControl: Spatio-Temporal Control for Masked Motion Synthesis",
    "volume": "main",
    "abstract": "Recent advances in motion diffusion models have enabled spatially controllable text-to-motion generation. However, these models struggle to achieve high-precision control while maintaining high-quality motion generation. To address these challenges, we propose MaskControl, the first approach to introduce controllability to the generative masked motion model. Our approach introduces two key innovations. First, Logits Regularizer implicitly perturbs logits at training time to align the distribution of motion tokens with the controlled joint positions, while regularizing the categorical token prediction to ensure high-fidelity generation. Second, Logit Optimization explicitly optimizes the predicted logits during inference time, directly reshaping the token distribution that forces the generated motion to accurately align with the controlled joint positions. Moreover, we introduce Differentiable Expectation Sampling (DES) to combat the non-differential distribution sampling process encountered by logits regularizer and optimization. Extensive experiments demonstrate that MaskControl outperforms state-of-the-art methods, achieving superior motion quality (FID decreases by 77%) and higher control precision (average error 0.91 vs. 1.08). Additionally, MaskControl enables diverse applications, including any-joint-any-frame control, body-part timeline control, and zero-shot objective control. Video visualization can be found at https://anonymous-ai-agent.github.io/CAM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ekkasit Pinyoanuntapong",
      "Muhammad Saleem",
      "Korrawe Karunratanakul",
      "Pu Wang",
      "Hongfei Xue",
      "Chen Chen",
      "Chuan Guo",
      "Junli Cao",
      "Jian Ren",
      "Sergey Tulyakov"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xiang_Evidential_Knowledge_Distillation_ICCV_2025_paper.html": {
    "title": "Evidential Knowledge Distillation",
    "volume": "main",
    "abstract": "Existing logit-based knowledge distillation methods typically employ singularly deterministic categorical distributions, which eliminates the inherent uncertainty in network predictions and thereby limiting the effective transfer of knowledge. To address this limitation, we introduce distribution-based probabilistic modeling as a more comprehensive representation of network knowledge. Specifically, we regard the categorical distribution as a random variable and leverage deep neural networks to predict its distribution, representing it as an evidential second-order distribution. Based on the second-oder modeling, we propose Evidential Knowledge Distillation (EKD) which distills both the expectation of the teacher distribution and the distribution itself into the student. The expectation captures the macroscopic characteristics of the distribution, while the distribution itself conveys microscopic information about the classification boundaries. Additionally, we theoretically show that EKD's distillation objective provides an upper bound on the student's expected risk when treating the teacher's predictions as ground truth. Extensive experiments on several standard benchmarks across various teacher-student network pairs highlight the effectiveness and superior performance of EKD. Our code is available at https://github.com/lyxiang-casia/EKD",
    "checked": false,
    "id": "d72b9032a04c85650e96cd097b9ec360c5542594",
    "semantic_title": "frequency attention for knowledge distillation",
    "citation_count": 27,
    "authors": [
      "Liangyu Xiang",
      "Junyu Gao",
      "Changsheng Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Dai_Straighten_Viscous_Rectified_Flow_via_Noise_Optimization_ICCV_2025_paper.html": {
    "title": "Straighten Viscous Rectified Flow via Noise Optimization",
    "volume": "main",
    "abstract": "The Reflow operation aims to straighten the inference trajectories of the rectified flow during training by constructing deterministic couplings between noises and images, thereby improving the quality of generated images in single-step or few-step generation. However, we identify critical limitations in Reflow, particularly its inability to rapidly generate high-quality images due to a distribution gap between images in its constructed deterministic couplings and real images. To address these shortcomings, we propose a novel alternative called Straighten Viscous Rectified Flow via Noise Optimization (VRFNO), which is a joint training framework integrating an encoder and a neural velocity field. VRFNO introduces two key innovations: (1) a historical velocity term that enhances trajectory distinction, enabling the model to more accurately predict the velocity of the current trajectory, and (2) the noise optimization through reparameterization to form optimized couplings with real images which are then utilized for training, effectively mitigating errors caused by Reflow's limitations. Comprehensive experiments on synthetic data and real datasets with varying resolutions show that VRFNO significantly mitigates the limitations of Reflow, achieving state-of-the-art performance in both one-step and few-step generation tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jimin Dai",
      "Jiexi Yan",
      "Jian Yang",
      "Lei Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tastan_A_Framework_for_Double-Blind_Federated_Adaptation_of_Foundation_Models_ICCV_2025_paper.html": {
    "title": "A Framework for Double-Blind Federated Adaptation of Foundation Models",
    "volume": "main",
    "abstract": "Foundation models (FMs) excel in zero-shot tasks but benefit from task-specific adaptation. However, privacy concerns prevent data sharing among multiple data owners, and proprietary restrictions prevent the learning service provider (LSP) from sharing the FM. In this work, we propose BlindFed, a framework enabling collaborative FM adaptation while protecting both parties: data owners do not access the FM or each other's data, and the LSP does not see sensitive task data. BlindFed relies on fully homomorphic encryption (FHE) and consists of three key innovations: (i) FHE-friendly architectural modifications via polynomial approximations and low-rank adapters, (ii) a two-stage split learning approach combining offline knowledge distillation and online encrypted inference for adapter training without backpropagation through the FM, and (iii) a privacy-boosting scheme using sample permutations and stochastic block sampling to mitigate model extraction attacks. Empirical results on four image classification datasets demonstrate the practical feasibility of the BlindFed framework, albeit at a high communication cost and large computational complexity for the LSP. The code can be found at https://github.com/tnurbek/blindfed",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nurbek Tastan",
      "Karthik Nandakumar"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Omni-scene_Perception-oriented_Point_Cloud_Geometry_Enhancement_for_Coordinate_Quantization_ICCV_2025_paper.html": {
    "title": "Omni-scene Perception-oriented Point Cloud Geometry Enhancement for Coordinate Quantization",
    "volume": "main",
    "abstract": "Information quantization has been widely adopted in multimedia content, such as images, videos, and point clouds. The goal of information quantization is to achieve efficient storage and transmission by reducing data precision or redundancy. However, the information distortion caused by quantization will lead to the degradation of signal fidelity and the performance of downstream tasks. This paper focuses on the geometry quantization distortion of point clouds and proposes a unified learning-based quality enhancement framework for omni-scene point clouds. Based on the characteristics of geometry quantization distortion, we analyze and find that existing upsampling methods are not competitive in dealing with point reduction and geometry displacement simultaneously caused by coordinate quantization. Therefore, we design a general rooting-growing-pruning paradigm to efficiently perceive the geometry feature of quantized point clouds and improve the quality significantly. In addition, a novel loss constraint term related to the quantization step parameter is proposed to further improve quality and accelerate model convergence. To the best of our knowledge, this is the first unified quality enhancement framework for object and scene point clouds with coordinate quantization. Extensive experiments verify the superiority of the proposed method on multi-scale point clouds with different levels of quantization distortion, including object (ModelNet40, 8iVFB) and scene (KITTI). In particular, the enhanced point clouds improve the performance of downstream analysis tasks, including classification and 3D object detection",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wang Liu",
      "Wei Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lu_B-VLLM_A_Vision_Large_Language_Model_with_Balanced_Spatio-Temporal_Tokens_ICCV_2025_paper.html": {
    "title": "B-VLLM: A Vision Large Language Model with Balanced Spatio-Temporal Tokens",
    "volume": "main",
    "abstract": "Recently, Vision Large Language Models (VLLMs) with integrated vision encoders have shown promising performance in vision understanding. They encode visual content into sequences of visual tokens, enabling joint processing of visual and textual data. However, understanding videos, especially long videos, remains a challenge as the rapid growth of visual tokens during video encoding risks exceeding VLLMs' context window length and significantly escalates computational cost. To restrict the number of visual tokens, existing VLLMs either: (1) uniformly downsample videos into a fixed number of frames or (2) reducing the number of visual tokens encoded from each frame. We argue that the former neglects temporal dynamics in videos, while the latter fails to preserve spatial details within individual frame. In this work, we propose Balanced-VLLM (B-VLLM), a novel VLLM framework designed to model task relevant spatio-temporal cues, while restricting the number of visual tokens within the VLLM's context window length. Central to our framework is a text-conditioned adaptive frame selection module that dynamically identifies task-relevant frames, which are further de-duplicated with a temporal frame token merging strategy.The visual tokens of these frames then undergo spatial token sampling and an optional spatial token merging strategy for granular control against the token budget. Experiments demonstrate the effectiveness of B-VLLM in balancing the number of frames and visual tokens, moreover, our proposed method introduce 10% performance gain on MVBench. Our code will be publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuqiang Lu",
      "Zhenfei Yin",
      "Mengwei He",
      "Zhihui Wang",
      "Zicheng Liu",
      "Zhiyong Wang",
      "Kun Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Potamias_ImHead_A_Large-scale_Implicit_Morphable_Model_for_Localized_Head_Modeling_ICCV_2025_paper.html": {
    "title": "ImHead: A Large-scale Implicit Morphable Model for Localized Head Modeling",
    "volume": "main",
    "abstract": "Over the last years, 3D morphable models (3DMMs) have emerged as a state-of-the-art methodology for modeling and generating expressive 3D avatars. However, given their reliance on a strict topology, along with their linear nature, they struggle to represent complex full-head shapes. Following the advent of deep implicit functions, we propose imHead, a novel implicit 3DMM that not only models expressive 3D head avatars but also facilitates localized editing of the facial features. Previous methods directly divided the latent space into local components accompanied by an identity encoding to capture the global shape variations, leading to expensive latent sizes. In contrast, we retain a single compact identity space and introduce an intermediate region-specific latent representation to enable local edits. To train imHead, we curate a large-scale dataset of 4K distinct identities, making a step-towards large scale 3D head modeling. Under a series of experiments we demonstrate the expressive power of the proposed model to represent diverse identities and expressions outperforming previous approaches. Additionally, the proposed approach provides an interpretable solution for 3D face manipulation, allowing the user to make localized edits",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rolandos Alexandros Potamias",
      "Stathis Galanakis",
      "Jiankang Deng",
      "Athanasios Papaioannou",
      "Stefanos Zafeiriou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Rezaeian_SL2A-INR_Single-Layer_Learnable_Activation_for_Implicit_Neural_Representation_ICCV_2025_paper.html": {
    "title": "SL2A-INR: Single-Layer Learnable Activation for Implicit Neural Representation",
    "volume": "main",
    "abstract": "Implicit Neural Representation (INR), leveraging a neural network to transform coordinate input into corresponding attributes, has recently driven significant advances in several vision-related domains. However, the performance of INR is heavily influenced by the choice of the nonlinear activation function used in its multilayer perceptron (MLP) architecture. To date, multiple nonlinearities have been investigated, but current INRs still face limitations in capturing high-frequency components and diverse signal types. We show that these challenges can be alleviated by introducing a novel approach in INR architecture. Specifically, we propose SL^ 2 A-INR, a hybrid network that combines a single-layer learnable activation function with an MLP that uses traditional ReLU activations. Our method performs superior across diverse tasks, including image representation, 3D shape reconstruction, and novel view synthesis. Through comprehensive experiments, SL^ 2 A-INR sets new benchmarks in accuracy, quality, and robustness for INR",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Reza Rezaeian",
      "Moein Heidari",
      "Reza Azad",
      "Dorit Merhof",
      "Hamid Soltanian-Zadeh",
      "Ilker Hacihaliloglu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Authentic_4D_Driving_Simulation_with_a_Video_Generation_Model_ICCV_2025_paper.html": {
    "title": "Authentic 4D Driving Simulation with a Video Generation Model",
    "volume": "main",
    "abstract": "Simulating driving environments in 4D is crucial for developing accurate and immersive autonomous driving systems. Despite progress in generating driving scenes, challenges in transforming views and modeling the dynamics of space and time remain. To tackle these issues, we propose a fresh methodology that reconstructs real-world driving environments and utilizes a generative network to enable 4D simulation. This approach builds continuous 4D point cloud scenes by leveraging surround-view data from autonomous vehicles. By separating the spatial and temporal elements, it creates smooth keyframe sequences. Furthermore, video generation techniques are employed to produce lifelike 4D simulation videos from any given perspective. To extend the range of possible viewpoints, we incorporate training using decomposed camera poses, which allows for enhanced modeling of distant scenes. Additionally, we merge camera trajectory data to synchronize 3D points across consecutive frames, fostering a richer understanding of the evolving scene. With training across multiple scene levels, our method is capable of simulating scenes from any viewpoint and offers deep insight into the evolution of scenes over time in a consistent spatial-temporal framework. In comparison with current methods, this approach excels in maintaining consistency across views, background coherence, and overall accuracy, significantly contributing to the development of more realistic autonomous driving simulations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lening Wang",
      "Wenzhao Zheng",
      "Dalong Du",
      "Yunpeng Zhang",
      "Yilong Ren",
      "Han Jiang",
      "Zhiyong Cui",
      "Haiyang Yu",
      "Jie Zhou",
      "Shanghang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_Partial_Forward_Blocking_A_Novel_Data_Pruning_Paradigm_for_Lossless_ICCV_2025_paper.html": {
    "title": "Partial Forward Blocking: A Novel Data Pruning Paradigm for Lossless Training Acceleration",
    "volume": "main",
    "abstract": "The ever-growing size of training datasets enhances the generalization capability of machine learning models but also incurs exorbitant computational costs. Existing data pruning approaches aim to accelerate training by removing those less important samples. However, they often rely on gradients or proxy models, leading to prohibitive additional costs of gradient back-propagation and proxy model training. In this paper, we propose Partial Forward Blocking (PFB), a novel framework for lossless training acceleration. The efficiency of PFB stems from its unique adaptive pruning pipeline: sample importance is assessed based on features extracted from the shallow layers of the target model. Less important samples are then pruned, allowing only the retained ones to proceed with the subsequent forward pass and loss back-propagation. This mechanism significantly reduces the computational overhead of deep-layer forward passes and back-propagation for pruned samples, while also eliminating the need for auxiliary backward computations and proxy model training. Moreover, PFB introduces probability density as an indicator of sample importance. Combined with an adaptive distribution estimation module, our method dynamically prioritizes relatively rare samples, aligning with the constantly evolving training state. Extensive experiments demonstrate the significant superiority of PFB in performance and speed. On ImageNet, PFB achieves a 0.5% accuracy improvement and 33% training time reduction with 40% data pruned",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongyue Wu",
      "Zilin Guo",
      "Jialong Zuo",
      "Nong Sang",
      "Changxin Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_FedPall_Prototype-based_Adversarial_and_Collaborative_Learning_for_Federated_Learning_with_ICCV_2025_paper.html": {
    "title": "FedPall: Prototype-based Adversarial and Collaborative Learning for Federated Learning with Feature Drift",
    "volume": "main",
    "abstract": "Federated learning (FL) enables collaborative training of a global model in the centralized server with data from multiple parties while preserving privacy. However, data heterogeneity can significantly degrade the performance of the global model when each party uses datasets from different sources to train a local model, thereby affecting personalized local models. Among various cases of data heterogeneity, feature drift, feature space difference among parties, is prevalent in real-life data but remains largely unexplored. Feature drift can distract feature extraction learning in clients and thus lead to poor feature extraction and classification performance. To tackle the problem of feature drift in FL, we propose FedPall, an FL framework that utilizes prototype-based adversarial learning to unify feature spaces and collaborative learning to reinforce class information within the features. Moreover, FedPall leverages mixed features generated from global prototypes and local features to enhance the global classifier with classification-relevant information from a global perspective. Evaluation results on three representative feature-drifted datasets demonstrate FedPall's consistently superior performance in classification with feature-drifted data in the FL scenario",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yong Zhang",
      "Feng Liang",
      "Guanghu Yuan",
      "Min Yang",
      "Chengming Li",
      "Xiping Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yu_From_Easy_to_Hard_Progressive_Active_Learning_Framework_for_Infrared_ICCV_2025_paper.html": {
    "title": "From Easy to Hard: Progressive Active Learning Framework for Infrared Small Target Detection with Single Point Supervision",
    "volume": "main",
    "abstract": "Recently, single-frame infrared small target (SIRST) detection with single point supervision has drawn wide-spread attention. However, the latest label evolution with single point supervision (LESPS) framework suffers from instability, excessive label evolution, and difficulty in exerting embedded network performance. Inspired by organisms gradually adapting to their environment and continuously accumulating knowledge, we construct an innovative Progressive Active Learning (PAL) framework, which drives the existing SIRST detection networks progressively and actively recognizes and learns harder samples. Specifically, to avoid the early low-performance model leading to the wrong selection of hard samples, we propose a model pre-start concept, which focuses on automatically selecting a portion of easy samples and helping the model have basic task-specific learning capabilities. Meanwhile, we propose a refined dual-update strategy, which can promote reasonable learning of harder samples and continuous refinement of pseudo-labels. In addition, to alleviate the risk of excessive label evolution, a decay factor is reasonably introduced, which helps to achieve a dynamic balance between the expansion and contraction of target annotations. Extensive experiments show that existing SIRST detection networks equipped with our PAL framework have achieved state-of-the-art (SOTA) results on multiple public datasets. Furthermore, our PAL framework can build an efficient and stable bridge between full supervision and single point supervision tasks. Our code is available at https://github.com/YuChuang1205/PAL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuang Yu",
      "Jinmiao Zhao",
      "Yunpeng Liu",
      "Sicheng Zhao",
      "Yimian Dai",
      "Xiangyu Yue"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Asynchronous_Event_Error-Minimizing_Noise_for_Safeguarding_Event_Dataset_ICCV_2025_paper.html": {
    "title": "Asynchronous Event Error-Minimizing Noise for Safeguarding Event Dataset",
    "volume": "main",
    "abstract": "With more event datasets being released online, safeguarding the event dataset against unauthorized usage has become a serious concern for data owners. Unlearnable Examples are proposed to prevent the unauthorized exploitation of image datasets. However, it's unclear how to create unlearnable asynchronous event streams to prevent event misuse. In this work, we propose the first unlearnable event stream generation method to prevent unauthorized training from event datasets. A new form of asynchronous event error-minimizing noise is proposed to perturb event streams, tricking the unauthorized model into learning embedded noise instead of realistic features. To be compatible with the sparse event, a projection strategy is presented to sparsify the noise to render our unlearnable event streams (UEvs). Extensive experiments demonstrate that our method effectively protects event data from unauthorized exploitation, while preserving their utility for legitimate use. We hope our UEvs contribute to the advancement of secure and trustworthy event dataset sharing. Code is available at: https://github.com/rfww/uevs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruofei Wang",
      "Peiqi Duan",
      "Boxin Shi",
      "Renjie Wan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Peng_A_Constrained_Optimization_Approach_for_Gaussian_Splatting_from_Coarsely-posed_Images_ICCV_2025_paper.html": {
    "title": "A Constrained Optimization Approach for Gaussian Splatting from Coarsely-posed Images and Noisy Lidar Point Clouds",
    "volume": "main",
    "abstract": "3D Gaussian Splatting (3DGS) is a powerful reconstruction technique; however, it requires initialization from accurate camera poses and high-fidelity point clouds. Typically, the initialization is taken from Structure-from-Motion (SfM) algorithms; however, SfM is time-consuming and restricts the application of 3DGS in real-world scenarios and large-scale scene reconstruction. We introduce a constrained optimization method for simultaneous camera pose estimation and 3D reconstruction that does not require SfM support. Core to our approach is decomposing a camera pose into a sequence of camera-to-(device-)center and (device-)center-to-world optimizations. To facilitate, we propose two optimization constraints conditioned to the sensitivity of each parameter group and restricts each parameter's search space. In addition, as we learn the scene geometry directly from the noisy point clouds, we propose geometric constraints to improve the reconstruction quality. Experiments demonstrate that the proposed method significantly outperforms the existing (multi-modal) 3DGS baseline and methods supplemented by COLMAP on both our collected dataset and two public benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jizong Peng",
      "Tze Ho Elden Tse",
      "Kai Xu",
      "Wenchao Gao",
      "Angela Yao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_BabyVLM_Data-Efficient_Pretraining_of_VLMs_Inspired_by_Infant_Learning_ICCV_2025_paper.html": {
    "title": "BabyVLM: Data-Efficient Pretraining of VLMs Inspired by Infant Learning",
    "volume": "main",
    "abstract": "Human infants rapidly develop visual reasoning skills from minimal input, suggesting that developmentally inspired pretraining could significantly enhance the efficiency of vision-language models (VLMs). Although recent efforts have leveraged infant-inspired datasets like SAYCam, existing evaluation benchmarks remain misaligned--they are either too simplistic, narrowly scoped, or tailored for large-scale pretrained models. Additionally, training exclusively on SAYCam overlooks the broader, diverse input from which infants naturally learn. To address these limitations, we propose BabyVLM, a novel framework comprising diverse in-domain evaluation benchmarks and a synthetic training dataset created via child-directed transformations of existing datasets. We demonstrate that VLMs trained with our synthetic dataset achieve superior performance on BabyVLM tasks compared to models trained solely on SAYCam or general-purpose data of the SAYCam size. BabyVLM thus provides a robust, developmentally aligned evaluation tool and illustrates how compact models trained on carefully curated data can generalize effectively, opening pathways toward data-efficient vision-language learning paradigms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengao Wang",
      "Arjun Chandra",
      "Aoming Liu",
      "Venkatesh Saligrama",
      "Boqing Gong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kuo_Efficient_Concertormer_for_Image_Deblurring_and_Beyond_ICCV_2025_paper.html": {
    "title": "Efficient Concertormer for Image Deblurring and Beyond",
    "volume": "main",
    "abstract": "The Transformer architecture has excelled in NLP and vision tasks, but its self-attention complexity grows quadratically with image size, making high-resolution tasks computationally expensive. We introduce Concertormer, featuring Concerto Self-Attention (CSA) for image deblurring. CSA splits self-attention into global and local components while retaining partial information in additional dimensions, achieving linear complexity. A Cross-Dimensional Communication module enhances expressiveness by linearly combining attention maps. Additionally, our gated-dconv MLP merges the two-staged Transformer design into a single stage. Extensive evaluations show our method performs favorably against state-of-the-art works in deblurring, deraining, and JPEG artifact removal",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pin-Hung Kuo",
      "Jinshan Pan",
      "Shao-Yi Chien",
      "Ming-Hsuan Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gao_Curve-Aware_Gaussian_Splatting_for_3D_Parametric_Curve_Reconstruction_ICCV_2025_paper.html": {
    "title": "Curve-Aware Gaussian Splatting for 3D Parametric Curve Reconstruction",
    "volume": "main",
    "abstract": "This paper presents an end-to-end framework for reconstructing 3D parametric curves directly from multi-view edge maps. Contrasting with existing two-stage methods that follow a sequential \"edge point cloud reconstruction and parametric curve fitting\" pipeline, our one-stage approach optimizes 3D parametric curves directly from 2D edge maps, eliminating error accumulation caused by the inherent optimization gap between disconnected stages. However, parametric curves inherently lack suitability for rendering-based multi-view optimization, necessitating a complementary representation that preserves their geometric properties while enabling differentiable rendering. We propose a novel bi-directional coupling mechanism between parametric curves and edge-oriented Gaussian components. This tight correspondence formulates a curve-aware Gaussian representation, CurveGaussian, that enables differentiable rendering of 3D curves, allowing direct optimization guided by multi-view evidence. Furthermore, we introduce a dynamically adaptive topology optimization framework during training to refine curve structures through linearization, merging, splitting, and pruning operations. Comprehensive evaluations on the ABC dataset and real-world benchmarks demonstrate our one-stage method's superiority over two-stage alternatives, particularly in producing cleaner and more robust reconstructions. Additionally, by directly optimizing parametric curves, our method significantly reduces the parameter count during training, achieving both higher efficiency and superior performance compared to existing approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhirui Gao",
      "Renjiao Yi",
      "Yaqiao Dai",
      "Xuening Zhu",
      "Wei Chen",
      "Chenyang Zhu",
      "Kai Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tang_Sim-DETR_Unlock_DETR_for_Temporal_Sentence_Grounding_ICCV_2025_paper.html": {
    "title": "Sim-DETR: Unlock DETR for Temporal Sentence Grounding",
    "volume": "main",
    "abstract": "Temporal sentence grounding aims to identify exact moments in a video that correspond to a given textual query, typically addressed with detection transformer (DETR) solutions. However, we find that typical strategies designed to enhance DETR do not improve, and may even degrade, its performance in this task. We systematically analyze and identify the root causes of this abnormal behavior: (1) conflicts between queries from similar target moments and (2) internal query conflicts due to the tension between global semantics and local localization. Building on these insights, we propose a simple yet powerful baseline, Sim-DETR, which extends the standard DETR with two minor modifications in the decoder layers: (1) constraining self-attention between queries based on their semantic and positional overlap and (2) adding query-to-frame alignment to bridge the global and local contexts. Experiments demonstrate that Sim-DETR unlocks the full potential of DETR for temporal sentence grounding, offering a strong baseline for future research",
    "checked": true,
    "id": "b7d6bc0c1ff8a959f106556afad6dca2592275ce",
    "semantic_title": "sim-detr: unlock detr for temporal sentence grounding",
    "citation_count": 1,
    "authors": [
      "Jiajin Tang",
      "Zhengxuan Wei",
      "Yuchen Zhu",
      "Cheng Shi",
      "Guanbin Li",
      "Liang Lin",
      "Sibei Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hu_DynamicID_Zero-Shot_Multi-ID_Image_Personalization_with_Flexible_Facial_Editability_ICCV_2025_paper.html": {
    "title": "DynamicID: Zero-Shot Multi-ID Image Personalization with Flexible Facial Editability",
    "volume": "main",
    "abstract": "Recent advances in text-to-image generation have driven interest in generating personalized human images that depict specific identities from reference images. Although existing methods achieve high-fidelity identity preservation, they are generally limited to single-ID scenarios and offer insufficient facial editability. We present DynamicID, a tuning-free framework that inherently facilitates both single-ID and multi-ID personalized generation with high fidelity and flexible facial editability. Our key innovations include: 1) Semantic-Activated Attention (SAA), which employs query-level activation gating to minimize disruption to the base model when injecting ID features and achieve multi-ID personalization without requiring multi-ID samples during training. 2) Identity-Motion Reconfigurator (IMR), which applies feature-space manipulation to effectively disentangle and reconfigure facial motion and identity features, supporting flexible facial editing. 3) a task-decoupled training paradigm that reduces data dependency, together with VariFace-10k, a curated dataset of 10k unique individuals, each represented by 35 distinct facial images. Experimental results demonstrate that DynamicID outperforms state-of-the-art methods in identity fidelity, facial editability, and multi-ID personalization capability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xirui Hu",
      "Jiahao Wang",
      "Hao Chen",
      "Weizhan Zhang",
      "Benqi Wang",
      "Yikun Li",
      "Haishun Nan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_Wavelet_Policy_Lifting_Scheme_for_Policy_Learning_in_Long-Horizon_Tasks_ICCV_2025_paper.html": {
    "title": "Wavelet Policy: Lifting Scheme for Policy Learning in Long-Horizon Tasks",
    "volume": "main",
    "abstract": "Policy learning focuses on devising strategies for agents in embodied artificial intelligence systems to perform optimal actions based on their perceived states. One of the key challenges in policy learning involves handling complex, long-horizon tasks that require managing extensive sequences of actions and observations with multiple modes. Wavelet analysis offers significant advantages in signal processing, notably in decomposing signals at multiple scales to capture both global trends and fine-grained details. In this work, we introduce a novel wavelet policy learning framework that utilizes wavelet transformations to enhance policy learning. Our approach leverages learnable multi-scale wavelet decomposition to facilitate detailed observation analysis and robust action planning over extended sequences. We detail the design and implementation of our wavelet policy, which incorporates lifting schemes for effective multi-resolution analysis and action generation. This framework is evaluated across multiple complex scenarios, including robotic manipulation, self-driving, and multi-robot collaboration, demonstrating the effectiveness of our method in improving the precision and reliability of the learned policy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Huang",
      "Shuaihang Yuan",
      "Geeta Chandra Raju Bethala",
      "Congcong Wen",
      "Anthony Tzes",
      "Yi Fang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_METEOR_Multi-Encoder_Collaborative_Token_Pruning_for_Efficient_Vision_Language_Models_ICCV_2025_paper.html": {
    "title": "METEOR: Multi-Encoder Collaborative Token Pruning for Efficient Vision Language Models",
    "volume": "main",
    "abstract": "Vision encoders serve as the cornerstone of multimodal understanding. Single-encoder architectures like CLIP exhibit inherent constraints in generalizing across diverse multimodal tasks, while recent multi-encoder fusion methods introduce prohibitive computational overhead to achieve superior performance using complementary visual representations from multiple vision encoders. To address this, we propose a progressive pruning framework, namely Multi-Encoder collaboraTivE tOken pRuning (METEOR), that eliminates redundant visual tokens across the encoding, fusion, and decoding stages for multi-encoder MLLMs. For multi-vision encoding, we discard redundant tokens within each encoder via a rank guided collaborative token assignment strategy. Subsequently, for multi-vision fusion, we combine the visual features from different encoders while reducing cross-encoder redundancy with cooperative pruning. Finally, we propose an adaptive token pruning method in the LLM decoding stage to further discard irrelevant tokens based on the text prompts with dynamically adjusting pruning ratios for specific task demands. To our best knowledge, this is the first successful attempt that achieves an efficient multi-encoder based vision language model with multi-stage pruning strategies. Extensive experiments on 11 benchmarks demonstrate the effectiveness of our proposed approach. Compared with EAGLE, a typical multi-encoder MLLMs, METEOR reduces 76% visual tokens with only 0.3% performance drop in average. The code is available at https://github.com/YuchenLiu98/METEOR",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchen Liu",
      "Yaoming Wang",
      "Bowen Shi",
      "Xiaopeng Zhang",
      "Wenrui Dai",
      "Chenglin Li",
      "Hongkai Xiong",
      "Qi Tian"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chi_Contact-Aware_Amodal_Completion_for_Human-Object_Interaction_via_Multi-Regional_Inpainting_ICCV_2025_paper.html": {
    "title": "Contact-Aware Amodal Completion for Human-Object Interaction via Multi-Regional Inpainting",
    "volume": "main",
    "abstract": "Amodal completion, the task of inferring the complete appearance of objects despite partial occlusions, is crucial for understanding complex human-object interactions (HOI) in computer vision and robotics. Existing methods, including pre-trained diffusion models, often struggle to generate plausible completions in dynamic scenarios due to their limited understanding of HOI. To address this challenge, we propose a novel approach that leverages physical prior knowledge alongside a specialized multi-regional inpainting technique tailored for HOI. By incorporating physical constraints derived from human topology and contact information, we define two distinct regions: the primary region, where occluded object parts are most likely to reside, and the secondary region, where occlusions are less probable. Our multi-regional inpainting method employs customized denoising strategies across these regions within a diffusion model, thereby enhancing the accuracy and realism of generated completions in both shape and visual detail. Experimental results demonstrate that our approach substantially outperforms existing methods in HOI scenarios, advancing machine perception toward a more human-like understanding of dynamic environments. Furthermore, we show that our pipeline remains robust even without ground-truth contact annotations, broadening its applicability to tasks such as 3D reconstruction and novel view/pose synthesis",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seunggeun Chi",
      "Enna Sachdeva",
      "Pin-Hao Huang",
      "Kwonjoon Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xie_SeqGrowGraph_Learning_Lane_Topology_as_a_Chain_of_Graph_Expansions_ICCV_2025_paper.html": {
    "title": "SeqGrowGraph: Learning Lane Topology as a Chain of Graph Expansions",
    "volume": "main",
    "abstract": "Accurate lane topology is essential for autonomous driving, yet traditional methods struggle to model the complex, non-linear structures--such as loops and bidirectional lanes--prevalent in real-world road structure. We present SeqGrowGraph, a novel framework that learns lane topology as a chain of graph expansions, inspired by human map-drawing processes. Representing the lane graph as a directed graph G=(V,E), with intersections (V) and centerlines (E), SeqGrowGraph incrementally constructs this graph by introducing one vertex at a time. At each step, an adjacency matrix (A) expands from nn to (n+1)(n+1) to encode connectivity, while a geometric matrix (M) captures centerline shapes as quadratic Bezier curves. The graph is serialized into sequences, enabling a transformer model to autoregressively predict the chain of expansions, guided by a depth-first search ordering. Evaluated on nuScenes and Argoverse 2 datasets, SeqGrowGraph achieves state-of-the-art performance",
    "checked": true,
    "id": "fda3ff7597d8e3efbd5186effb07a9c9bbbfa2be",
    "semantic_title": "seqgrowgraph: learning lane topology as a chain of graph expansions",
    "citation_count": 3,
    "authors": [
      "Mengwei Xie",
      "Shuang Zeng",
      "Xinyuan Chang",
      "Xinran Liu",
      "Zheng Pan",
      "Mu Xu",
      "Xing Wei"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shi_Ultra-Precision_6DoF_Pose_Estimation_Using_2-D_Interpolated_Discrete_Fourier_Transform_ICCV_2025_paper.html": {
    "title": "Ultra-Precision 6DoF Pose Estimation Using 2-D Interpolated Discrete Fourier Transform",
    "volume": "main",
    "abstract": "Ultra-precision estimation of 6DoF pose is essential in applications such as semiconductor manufacturing and nanoscale manipulation. Conventional vision-based techniques are often hampered by sensitivity to defocus and limited estimation accuracy. In this paper, we propose a novel two-dimensional interpolated Discrete Fourier Transform (2D-IpDFT) method for robust 6DoF pose estimation using periodic patterns. We further develop a mathematical framework that links image parameters--phase and frequency--to 6DoF pose, which is applicable to both orthographic and quasi-orthographic imaging systems. Extensive experiments on a low-cost setup, featuring an industrial camera and an etched checkerboard pattern, demonstrate translation estimation accuracy at the nanometer level and rotation estimation accuracy at the microradian level",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guowei Shi",
      "Zian Mao",
      "Peisen Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ren_Neural_Compression_for_3D_Geometry_Sets_ICCV_2025_paper.html": {
    "title": "Neural Compression for 3D Geometry Sets",
    "volume": "main",
    "abstract": "We present NeCGS, the first neural compression paradigm, which can compress a geometry set encompassing thousands of detailed and diverse 3D mesh models by up to 900 times with high accuracy and preservation of detailed geometric structures. Specifically, we first propose TSDF-Def, a new implicit representation that is capable of accurately representing irregular 3D mesh models with various structures into regular 4D tensors of uniform and compact size, where 3D surfaces can be extracted through the deformable marching cubes. Then we construct a quantization-aware auto-decoder network architecture to regress these 4D tensors to explore the local geometric similarity within each shape and across different shapes for redundancy removal, resulting in more compact representations, including an embedded feature of a smaller size associated with each 3D model and a network parameter shared by all models. We finally encode the resulting features and network parameters into bitstreams through entropy coding. Besides, our NeCGS can handle the dynamic scenario well, where new 3D models are constantly added to a compressed set. Extensive experiments and ablation studies demonstrate the significant advantages of our NeCGS over state-of-the-art methods both quantitatively and qualitatively. The source code is publicly available at https://github.com/rsy6318/NeCGS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyu Ren",
      "Junhui Hou",
      "Weiyao Lin",
      "Wenping Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Luan_Lifting_the_Structural_Morphing_for_Wide-Angle_Images_Rectification_Unified_Content_ICCV_2025_paper.html": {
    "title": "Lifting the Structural Morphing for Wide-Angle Images Rectification: Unified Content and Boundary Modeling",
    "volume": "main",
    "abstract": "The mainstream approach for correcting distortions in wide-angle images typically involves a cascading process of rectification followed by rectangling. These tasks address distorted image content and irregular boundaries separately, using two distinct pipelines. However, this independent optimization prevents the two stages from benefiting each other. It also increases susceptibility to error accumulation and misaligned optimization, ultimately degrading the quality of the rectified image and the performance of downstream vision tasks. In this work, we observe and verify that transformations based on motion representations ((e.g.,, Thin-Plate Spline) exhibit structural continuity in both rectification and rectangling tasks. This continuity enables us to establish their relationships through the perspective of structural morphing, allowing for an optimal solution within a single end-to-end framework. To this end, we propose ConBo-Net, a unified Content and Boundary modeling approach for one-stage wide-angle image correction. Our method jointly addresses distortion rectification and boundary rectangling in an end-to-end manner. To further enhance the model's structural recovery capability, we incorporate physical priors based on the wide-angle camera model during training and introduce an ordinal geometric loss to enforce curvature monotonicity. Extensive experiments demonstrate that ConBo-Net outperforms state-of-the-art two-stage solutions. The code and dataset are available at https://github.com/lwttttt/ConBo-Net",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenting Luan",
      "Siqi Lu",
      "Yongbin Zheng",
      "Wanying Xu",
      "Lang Nie",
      "Zongtan Zhou",
      "Kang Liao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yuan_ASGS_Single-Domain_Generalizable_Open-Set_Object_Detection_via_Adaptive_Subgraph_Searching_ICCV_2025_paper.html": {
    "title": "ASGS: Single-Domain Generalizable Open-Set Object Detection via Adaptive Subgraph Searching",
    "volume": "main",
    "abstract": "Albeit existing Single-Domain Generalized Object Detection (Single-DGOD) methods enable models to generalize to unseen domains, most assume that the training and testing data share the same label space. In real-world scenarios, unseen domains often introduce previously unknown objects, a challenge that has been largely overlooked. In this paper, we tackle the practical problem of Single-domain Generalizable Open-Set Object Detection (SG-OSOD), which addresses both unseen domains and unknown classes. We identify two key challenges: (1) detecting unknown classes with only known-class data, and (2) learning robust features to mitigate domain shift. To address these challenges, we propose the framework termed \\texttt ASGS , which leverages adaptive subgraph structures to enhance the understanding of unknown scenes and classes. \\texttt ASGS consists of Subgraph-wise Unknown-class Learning (SUL) and Class-wise Embedding Compaction (CEC). SUL employs non-parametric methods to detect unknown samples and performs Adaptive Subgraph Searching (ASS) for high-order structural feature extraction, enabling domain-robust unknown class learning. Moreover, the CEC module enhances class discrimination robustness through contrastive learning, which results in more compact class clusters in unknown scenarios. Experimental results demonstrate the effectiveness of the proposed \\texttt ASGS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Yuan",
      "Luyao Tang",
      "Yixin Chen",
      "Chaoqi Chen",
      "Yue Huang",
      "Xinghao Ding"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xu_Dual-Temporal_Exemplar_Representation_Network_for_Video_Semantic_Segmentation_ICCV_2025_paper.html": {
    "title": "Dual-Temporal Exemplar Representation Network for Video Semantic Segmentation",
    "volume": "main",
    "abstract": "Video semantic segmentation aims to assign a class label for each pixel in every video frame. Existing methods predominantly follow the reference-target interaction paradigm, focusing on extracting local temporal contexts while neglecting the integration of global temporal information. Moreover, complex dynamics and varying lighting conditions introduce inter-frame intra-class discrepancies in feature representations, leading to unstable predictions. In this paper, we propose a novel framework, the Dual-Temporal Exemplar Representation Network (DTERN), which utilizes the strong representational capability of cluster centers, i.e., exemplars, to effectively model both local and global temporal information. DTERN consists of two core modules: 1) the Local Temporal Exemplar Module (LTEM), which constructs local exemplars to capture local temporal contexts, ensuring stable and reliable predictions. 2) the Global Temporal Exemplar Module (GTEM), which introduces learnable global exemplars to dynamically model global temporal information, thereby improving the effective consistency of segmentation. Furthermore, we observe that the existing Video Consistency (VC) metric fails to evaluate segmentation accuracy and lacks sensitivity to small-object segmentation. To this end, we propose Video Effective Consistency (VEC) to comprehensively evaluate temporal consistency and segmentation effectiveness. Experiments on VSPW and Cityscape demonstrate that DTERN outperforms state-of-the-art methods. The code is available at https://github.com/zlxilo/DTERN",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaolong Xu",
      "Lei Zhang",
      "Jiayi Li",
      "Lituan Wang",
      "Yifan Guan",
      "Yu Yan",
      "Leyi Zhang",
      "Hao Song"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_UniConvNet_Expanding_Effective_Receptive_Field_while_Maintaining_Asymptotically_Gaussian_Distribution_ICCV_2025_paper.html": {
    "title": "UniConvNet: Expanding Effective Receptive Field while Maintaining Asymptotically Gaussian Distribution for ConvNets of Any Scale",
    "volume": "main",
    "abstract": "Convolutional neural networks (ConvNets) with large effective receptive field (ERF), still in their early stages, have demonstrated promising effectiveness while constrained by high parameters and FLOPs costs and disrupted asymptotically Gaussian distribution (AGD) of ERF. This paper proposes an alternative paradigm: rather than merely employing extremely large ERF, it is more effective and effcient to expand the ERF while maintaining AGD of ERF by proper combination of smaller kernels, such as 7x 7 , 9x 9 , 11x 11 . This paper introduces a Three-layer Receptive Field Aggregator and designs a Layer Operator as the fundamental operator from the perspective of receptive field. The ERF can be expanded to the level of existing large-kernel ConvNets through the stack of proposed modules while maintaining AGD of ERF. Using these designs, we propose a universal ConvNet, termed UniConvNet. Extensive experiments on ImageNet-1K, COCO2017, and ADE20K demonstrate that UniConvNet outperforms state-of-the-art CNNs and ViTs across various vision recognition tasks for both lightweight and large-scale models with comparable throughput. Surprisingly, UniConvNet-T achieves 84.2% ImageNet top-1 accuracy with 30M parameters and 5.1G FLOPs. UniConvNet-XL also shows competitive scalability to big data and large models, acquiring 88.4% top-1 accuracy on ImageNet",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhao Wang",
      "Wei Xi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yao_Denoising_Token_Prediction_in_Masked_Autoregressive_Models_ICCV_2025_paper.html": {
    "title": "Denoising Token Prediction in Masked Autoregressive Models",
    "volume": "main",
    "abstract": "Autoregressive models are just at a tipping point where they could really take off for visual generation. In this paper, we propose to model token prediction using diffusion procedure particularly in masked autoregressive models for image generation. We look into the problem from two critical perspectives: progressively refining the unmasked tokens prediction via a denoising head with the autoregressive model, and representing masked tokens probability distribution by capitalizing on the interdependency across masked and unmasked tokens through a diffusion head. Our proposal harbors an innate agency that remains advantageous in the speed of sequence prediction, and strongly favors high capability in generating quality samples by leveraging the principles of denoising diffusion process. Extensive experiments on both class-conditional and text-to-image tasks demonstrate its superiority, achieving the state-of-the-art FID score of 1.47 and 5.27 on ImageNet and MSCOCO datasets, respectively. More remarkably, our approach leads to 45% speedup in the inference time of image generation against the diffusion models such as DiT-XL/2",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ting Yao",
      "Yehao Li",
      "Yingwei Pan",
      "Zhaofan Qiu",
      "Tao Mei"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sun_Uncertainty-Aware_Gradient_Stabilization_for_Small_Object_Detection_ICCV_2025_paper.html": {
    "title": "Uncertainty-Aware Gradient Stabilization for Small Object Detection",
    "volume": "main",
    "abstract": "Despite advances in generic object detection, there remains a performance gap in detecting small objects compared to normal-scale objects. We reveal that conventional object localization methods suffer from gradient instability in small objects due to sharper loss curvature, leading to a convergence challenge. To address the issue, we propose Uncertainty-Aware Gradient Stabilization (UGS), a framework that reformulates object localization as a classification task to stabilize gradients. UGS quantizes continuous labels into interval non-uniform discrete representations. Under a classification-based objective, the localization branch generates bounded and confidence-driven gradients, mitigating instability. Furthermore, UGS integrates an uncertainty minimization (UM) loss that reduces prediction variance and an uncertainty-guided refinement (UR) module that identifies and refines high-uncertainty regions via perturbations. Evaluated on four benchmarks, UGS consistently improves anchor-based, anchor-free, and leading small object detectors. Notably, UGS enhances DINO-5scale by 2.6 AP on VisDrone, surpassing prior state-of-the-art performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huixin Sun",
      "Yanjing Li",
      "Linlin Yang",
      "Xianbin Cao",
      "Baochang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Govindarajan_Radiant_Foam_Real-Time_Differentiable_Ray_Tracing_ICCV_2025_paper.html": {
    "title": "Radiant Foam: Real-Time Differentiable Ray Tracing",
    "volume": "main",
    "abstract": "Research on differentiable scene representations is consistently moving towards more efficient, real-time models. Recently, this has led to the popularization of splatting methods, which eschew the traditional ray-based rendering of radiance fields in favor of rasterization. This has yielded a significant improvement in rendering speeds due to the efficiency of rasterization algorithms and hardware, but has come at a cost: the approximations that make rasterization efficient also make implementation of light transport phenomena like reflection and refraction much more difficult. We propose a novel scene representation which avoids these approximations, but keeps the efficiency and reconstruction quality of splatting by leveraging a decades-old efficient volumetric mesh ray tracing algorithm which has been largely overlooked in recent computer vision research. The resulting model, which we name Radiant Foam, achieves rendering speed and quality comparable to Gaussian Splatting, without the constraints of rasterization. Unlike ray traced Gaussian models that use hardware ray tracing acceleration, our method requires no special hardware or APIs beyond the standard features of a programmable GPU",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shrisudhan Govindarajan",
      "Daniel Rebain",
      "Kwang Moo Yi",
      "Andrea Tagliasacchi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Attention_to_the_Burstiness_in_Visual_Prompt_Tuning_ICCV_2025_paper.html": {
    "title": "Attention to the Burstiness in Visual Prompt Tuning!",
    "volume": "main",
    "abstract": "Visual Prompt Tuning (VPT) is a parameter-efficient finetuning technique that adapts a pre-trained vision Transformer (ViT) by learning a small set of parameters in the input space, known as prompts. In VPT, we uncover \"burstiness\" in the values arising from the interaction of image patch embeddings, and the key and query projectors within Transformer's self-attention module. Interestingly, the values of patch embeddings and the key and query projectors exhibit Laplacian and hyper-Laplacian distribution, respectively. Intuitively, these non-Gaussian distributions pose challenges for learning prompts. To address this, we propose whitening these data, de-correlating them and equalizing their variance towards more Gaussian before learning prompts. We derive the whitening matrix over random image patch embeddings and ViT's key and query projectors, and multiply it with the prompt to be learned in a bilinear manner.Surprisingly, this method significantly accelerates prompt tuning and boosts accuracy, e.g., >25 points on the CUB dataset; interestingly, it learns \"bursty prompts\".As bilinear models are known to introduce burstiness, we present a compact method by learning two small sets of parameters whose multiplication yields the final prompts. We call the proposed methods Bilinear Prompt Tuning (BPT). Extensive experiments demonstrate that BPT methods not only outperform various VPT methods across multiple benchmark datasets but also reduce parameter count and computation overhead",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuzhu Wang",
      "Manni Duan",
      "Shu Kong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_BadVideo_Stealthy_Backdoor_Attack_against_Text-to-Video_Generation_ICCV_2025_paper.html": {
    "title": "BadVideo: Stealthy Backdoor Attack against Text-to-Video Generation",
    "volume": "main",
    "abstract": "Text-to-video (T2V) generative models have rapidly advanced and found widespread applications across fields like entertainment, education, and marketing. However, the adversarial vulnerabilities of these models remain rarely explored. We observe that in T2V generation tasks, the generated videos often contain substantial redundant information not explicitly specified in the text prompts, such as environmental elements, secondary objects, and additional details, providing opportunities for malicious attackers to embed hidden harmful content. Exploiting this inherent redundancy, we introduce BadVideo, the first backdoor attack framework tailored for T2V generation. Our attack focuses on designing target adversarial outputs through two key strategies: (1) Spatio-Temporal Composition, which combines different spatiotemporal features to encode malicious information; (2) Dynamic Element Transformation, which introduces transformations in redundant elements over time to convey malicious information. Based on these strategies, the attacker's malicious target seamlessly integrates with the user's textual instructions, providing high stealthiness. Moreover, by exploiting the temporal dimension of videos, our attack successfully evades traditional content moderation systems that primarily analyze spatial information within individual frames. Extensive experiments demonstrate that BadVideo achieves high attack success rates while preserving original semantics and maintaining excellent performance on clean inputs. Overall, our work reveals the adversarial vulnerability of T2V models, calling attention to potential risks and misuse. Our project page is at https://wrt2000.github.io/BadVideo2025/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruotong Wang",
      "Mingli Zhu",
      "Jiarong Ou",
      "Rui Chen",
      "Xin Tao",
      "Pengfei Wan",
      "Baoyuan Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Qin_Cassic_Towards_Content-Adaptive_State-Space_Models_for_Learned_Image_Compression_ICCV_2025_paper.html": {
    "title": "Cassic: Towards Content-Adaptive State-Space Models for Learned Image Compression",
    "volume": "main",
    "abstract": "Learned image compression (LIC) demonstrates superior rate-distortion (RD) performance compared to traditional methods. Recent method MambaVC attempts to introduce Mamba, a variant of state space models, into this field aim to establish a new paradigm beyond convolutional neural networks and transformers. However, this approach relies on predefined four-directional scanning, which prioritizes spatial proximity over content and semantic relationships, resulting in suboptimal redundancy elimination. Additionally, it focuses solely on nonlinear transformations, neglecting entropy model improvements crucial for accurate probability estimation in entropy coding. To address these limitations, we propose a novel framework based on content-adaptive visual state space model, Cassic, through dual innovation.First, we design a content-adaptive selective scan based on weighted activation maps and bit allocation maps, subsequently developing a content-adaptive visual state space block. Second, we present a mamba-based channel-wise auto-regressive entropy model to fully leverage inter-slice bit allocation consistency for enhanced probability estimation. Extensive experimental results demonstrate that our method achieves state-of-the-art performance across three datasets while maintaining faster processing speeds than existing MambaVC approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiyu Qin",
      "Jinpeng Wang",
      "Yimin Zhou",
      "Bin Chen",
      "Tianci Luo",
      "Baoyi An",
      "Tao Dai",
      "Shu-Tao Xia",
      "Yaowei Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_AIGI-Holmes_Towards_Explainable_and_Generalizable_AI-Generated_Image_Detection_via_Multimodal_ICCV_2025_paper.html": {
    "title": "AIGI-Holmes: Towards Explainable and Generalizable AI-Generated Image Detection via Multimodal Large Language Models",
    "volume": "main",
    "abstract": "The rapid development of AI-generated content (AIGC) technology has led to the misuse of highly realistic AI-generated images (AIGI) in spreading misinformation, posing a threat to public information security. Although existing AIGI detection techniques are generally effective, they face two issues: 1) a lack of human-verifiable explanations, and 2) a lack of generalization in the latest generation technology. To address these issues, we introduce a large-scale and comprehensive dataset, Holmes-Set, which includes the Holmes-SFTSet, an instruction-tuning dataset with explanations on whether images are AI-generated, and the Holmes-DPOSet, a human-aligned preference dataset. Our work introduces an efficient data annotation method called the Multi-Expert Jury, enhancing data generation through structured MLLM explanations and quality control via cross-model evaluation, expert defect filtering, and human preference modification. In addition, we propose Holmes Pipeline, a meticulously designed three-stage training framework comprising visual expert pre-training, supervised fine-tuning, and direct preference optimization. Holmes Pipeline adapts multimodal large language models (MLLMs) for AIGI detection while generating human-verifiable and human-aligned explanations, ultimately yielding our model AIGI-Holmes. During the inference stage, we introduce a collaborative decoding strategy that integrates the model perception of the visual expert with the semantic reasoning of MLLMs, further enhancing the generalization capabilities. Extensive experiments on three benchmarks validate the effectiveness of our AIGI-Holmes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyin Zhou",
      "Yunpeng Luo",
      "Yuanchen Wu",
      "Ke Sun",
      "Jiayi Ji",
      "Ke Yan",
      "Shouhong Ding",
      "Xiaoshuai Sun",
      "Yunsheng Wu",
      "Rongrong Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ouyang_TokensGen_Harnessing_Condensed_Tokens_for_Long_Video_Generation_ICCV_2025_paper.html": {
    "title": "TokensGen: Harnessing Condensed Tokens for Long Video Generation",
    "volume": "main",
    "abstract": "Generating consistent long videos is a complex challenge: while diffusion-based generative models generate visually impressive short clips, extending them to longer durations often leads to memory bottlenecks and long-term inconsistency. In this paper, we propose TokensGen, a novel two-stage framework that leverages condensed tokens to address these issues. Our method decomposes long video generation into three core tasks: (1) inner-clip semantic control, (2) long-term consistency control, and (3) inter-clip smooth transition. First, we train To2V (Token-to-Video), a short video diffusion model guided by text and video tokens, with a Video Tokenizer that condenses short clips into semantically rich tokens. Second, we introduce T2To (Text-to-Token), a video token diffusion transformer that generates all tokens at once, ensuring global consistency across clips. Finally, during inference, an adaptive FIFO-Diffusion strategy seamlessly connects adjacent clips, reducing boundary artifacts and enhancing smooth transitions.Experimental results demonstrate that our approach significantly enhances long-term temporal and content coherence without incurring prohibitive computational overhead. By leveraging condensed tokens and pre-trained short video models, our method provides a scalable, modular solution for long video generation, opening new possibilities for storytelling, cinematic production, and immersive simulations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenqi Ouyang",
      "Zeqi Xiao",
      "Danni Yang",
      "Yifan Zhou",
      "Shuai Yang",
      "Lei Yang",
      "Jianlou Si",
      "Xingang Pan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shen_Cross-View_Isolated_Sign_Language_Recognition_via_View_Synthesis_and_Feature_ICCV_2025_paper.html": {
    "title": "Cross-View Isolated Sign Language Recognition via View Synthesis and Feature Disentanglement",
    "volume": "main",
    "abstract": "Cross-view isolated sign language recognition (CV-ISLR) addresses the challenge of identifying isolated signs from viewpoints unseen during training, a problem aggravated by the scarcity of multi-view data in existing benchmarks. To bridge this gap, we introduce a novel two-stage framework comprising View Synthesis and Contrastive Multi-task View-Semantics Recognition. In the View Synthesis stage, we simulate unseen viewpoints by extracting 3D keypoints from the front-view training dataset and synthesizing common-view 2D skeleton sequences with virtual camera rotation, which enriches view diversity without the cost of multi-camera setups. However, direct training on these synthetic samples leads to limited improvement, as viewpoint-specific and semantics-specific features remain entangled. To overcome this drawback, we present a Contrastive Multi-task View-Semantics Recognition (CMVSR) module that disentangles viewpoint-dependent features from sign semantics. In this way, CMVSR obtains view-invariant representations of the sign video, leading to robust recognition performance against various camera viewpoints. We evaluate our approach on the MM-WLAuslan dataset, the first benchmark for CV-ISLR, and on our extended protocol MTV-Test that includes additional multi-view data captured in the wild. Experimental results demonstrate that our method not only improves the accuracy of front-view skeleton-based isolated sign language recognition, but also exhibits superior generalization to novel viewpoints",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Shen",
      "Xinyu Wang",
      "Lei Shen",
      "Kaihao Zhang",
      "Xin Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_R1-Onevision_Advancing_Generalized_Multimodal_Reasoning_through_Cross-Modal_Formalization_ICCV_2025_paper.html": {
    "title": "R1-Onevision: Advancing Generalized Multimodal Reasoning through Cross-Modal Formalization",
    "volume": "main",
    "abstract": "Large Language Models have demonstrated remarkable reasoning capability in complex textual tasks. However, multimodal reasoning, which requires integrating visual and textual information, remains a significant challenge. Existing visual-language models often struggle to effectively analyze and reason visual content, resulting in suboptimal performance on complex reasoning tasks. Moreover, the absence of comprehensive benchmarks hinders the accurate assessment of multimodal reasoning capabilities. In this paper, we introduce R1-Onevision, a multimodal reasoning model designed to bridge the gap between visual perception and deep reasoning. To achieve this, we propose a cross-modal reasoning pipeline that transforms images into formal textual representations, enabling precise language-based reasoning. Leveraging this pipeline, we construct the R1-Onevision dataset which provides detailed, step-by-step multimodal reasoning annotations across diverse domains. We further develop the R1-Onevision model through supervised fine-tuning and reinforcement learning to cultivate advanced reasoning and robust generalization abilities. To comprehensively evaluate multimodal reasoning performance across different grades, we introduce R1-Onevision-Bench, a benchmark aligned with human educational stages, covering exams from junior high school to university and beyond. Experimental results show that R1-Onevision achieves state-of-the-art performance, outperforming models such as GPT-4o and Qwen2.5-VL on multiple challenging multimodal reasoning benchmarks. Code, dataset and benchmark are available at https://github.com/Fancy-MLLM/R1-Onevision",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Yang",
      "Xiaoxuan He",
      "Hongkun Pan",
      "Xiyan Jiang",
      "Yan Deng",
      "Xingtao Yang",
      "Haoyu Lu",
      "Dacheng Yin",
      "Fengyun Rao",
      "Minfeng Zhu",
      "Bo Zhang",
      "Wei Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhao_HIS-GPT_Towards_3D_Human-In-Scene_Multimodal_Understanding_ICCV_2025_paper.html": {
    "title": "HIS-GPT: Towards 3D Human-In-Scene Multimodal Understanding",
    "volume": "main",
    "abstract": "We propose a new task to benchmark human-in-scene understanding for embodied agents: Human-In-Scene Question Answering (HIS-QA). Given a human motion within a 3D scene, HIS-QA requires the agent to comprehend human states and behaviors, reason about its surrounding environment, and answer human-related questions within the scene. To support this new task, we present HIS-Bench, a multimodal benchmark that systematically evaluates HIS understanding across a broad spectrum, from basic perception to commonsense reasoning and planning. Our evaluation of various vision-language models on HIS-Bench reveals significant limitations in their ability to handle HIS-QA tasks. To this end, we propose HIS-GPT, the first foundation model for HIS understanding. HIS-GPT integrates 3D scene context and human motion dynamics into large language models while incorporating specialized mechanisms to capture human-scene interactions. Extensive experiments demonstrate that HIS-GPT sets a new state-of-the-art on HIS-QA tasks. We hope this work inspires future research of human behavior analysis in 3D scenes, advancing embodied AI and world models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahe Zhao",
      "Ruibing Hou",
      "Zejie Tian",
      "Hong Chang",
      "Shiguang Shan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lee_Learnable_Logit_Adjustment_for_Imbalanced_Semi-Supervised_Learning_under_Class_Distribution_ICCV_2025_paper.html": {
    "title": "Learnable Logit Adjustment for Imbalanced Semi-Supervised Learning under Class Distribution Mismatch",
    "volume": "main",
    "abstract": "In class-imbalanced learning (CIL), post-hoc logit adjustment (LA) effectively mitigates class imbalance by adjusting biased logits according to label frequencies. Given the success of LA in CIL, recent class-imbalanced semi-supervised learning (CISSL) algorithms incorporated LA, leading to improved performance when labeled and unlabeled datasets share the same class distribution. However, a common real-world scenario involves the unknown class distribution of the unlabeled set, which may mismatch that of the labeled set. In this case, LA may result in an inappropriate degree of logit adjustments, potentially degrading classification performance due to its inability to incorporate the unknown class distribution of the unlabeled set. To address this problem, we propose a novel CISSL algorithm named learnable logit adjustment (LLA). Unlike the original LA, LLA learns the appropriate degree of logit adjustment by minimizing the class-averaged loss computed for both the labeled and unlabeled sets. Based on the learned degree, LLA refines the biased pseudo-labels of base semi-supervised learning algorithms and adjusts the biased class predictions on the test set by adjusting the logits. Experimental results on benchmark datasets demonstrate that LLA achieves state-of-the-art performance in CISSL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyuck Lee",
      "Taemin Park",
      "Heeyoung Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Subjective_Camera_1.0_Bridging_Human_Cognition_and_Visual_Reconstruction_through_ICCV_2025_paper.html": {
    "title": "Subjective Camera 1.0: Bridging Human Cognition and Visual Reconstruction through Sequence-Aware Sketch-Guided Diffusion",
    "volume": "main",
    "abstract": "We introduce the concept of a subjective camera to reconstruct meaningful moments that physical cameras fail to capture. We propose Subjective Camera 1.0, a framework for reconstructing real-world scenes from readily accessible subjective readouts, i.e., textual descriptions and progressively drawn rough sketches. Built on optimization-based alignment of diffusion models, our approach avoids large-scale paired training data and mitigates generalization issues. To address the challenge of integrating multiple abstract concepts in real-world scenarios, we design a Sequence-Aware Sketch-Guided Diffusion framework with three loss terms for concept-wise sequential optimization, following the natural order of subjective readouts. Experiments on two datasets demonstrate that our method achieves state-of-the-art performance in image quality as well as spatial and semantic alignment with target scenes. User studies with 40 participants further confirm that our approach is consistently preferred. Our project page is at: subjective-camera.github.io",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyang Chen",
      "Dongfang Sun",
      "Caoyuan Ma",
      "Shiqin Wang",
      "Kewei Zhang",
      "Zheng Wang",
      "Zhixiang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Learning_Robust_Image_Watermarking_with_Lossless_Cover_Recovery_ICCV_2025_paper.html": {
    "title": "Learning Robust Image Watermarking with Lossless Cover Recovery",
    "volume": "main",
    "abstract": "Watermarking as a traceable authentication technology has been widely applied in image copyright protection. However, most existing watermarking methods embed watermarks by adding irremovable perturbations to the cover image, causing permanent distortion. To address this issue, we propose a novel watermarking approach termed Cover-Recoverable Watermark (CRMark). CRMark can losslessly recover the cover image and watermark in lossless channels and enables robust watermark extraction in lossy channels. CRMark leverages an integer Invertible Watermarking Network (iIWN) to achieve a lossless invertible mapping between the cover-image-watermark pair and the stego image. During the training phase, CRMark employs an encoder-noise-layer-decoder architecture to enhance its robustness against distortions. In the inference phase, CRMark first maps the cover-image-watermark pair into an overflowed stego image and a latent variable. Subsequently, the overflowed pixels and the latent variable are losslessly compressed into an auxiliary bitstream, which is then embedded into the clipped stego image using reversible data hiding. During extraction, in lossy channels, the noised stego image can directly undergo inverse mapping via iIWN to extract the watermark. In lossless channels, the latent variable and overflowed stego image are first recovered using reversible data hiding, followed by watermark extraction through iIWN. Extensive experimental results demonstrate that CRMark can be perfectly recovered in lossless channels while remaining robust to common distortions. Code is available at https://github.com/chenoly/CRMark",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiale Chen",
      "Wei Wang",
      "Chongyang Shi",
      "Li Dong",
      "Xiping Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Saratchandran_Enhancing_Transformers_Through_Conditioned_Embedded_Tokens_ICCV_2025_paper.html": {
    "title": "Enhancing Transformers Through Conditioned Embedded Tokens",
    "volume": "main",
    "abstract": "Transformers have transformed modern machine learning, driving breakthroughs in computer vision, natural language processing, and robotics. At the core of their success lies the attention mechanism, which enables the modeling of global dependencies among input tokens. However, we reveal that the attention block in transformers suffers from inherent ill-conditioning, which hampers gradient-based optimization and leads to inefficient training. To address this, we develop a theoretical framework that establishes a direct relationship between the conditioning of the attention block and that of the embedded tokenized data. Building on this insight, we introduce conditioned embedded tokens, a method that systematically modifies the embedded tokens to improve the conditioning of the attention mechanism. Our analysis demonstrates that this approach significantly mitigates ill-conditioning, leading to more stable and efficient training. We validate our methodology across various transformer architectures, achieving consistent improvements in image classification, object detection, instance segmentation, and natural language processing, highlighting its broad applicability and effectiveness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hemanth Saratchandran",
      "Simon  Lucey"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_Drawing_Developmental_Trajectory_from_Cortical_Surface_Reconstruction_ICCV_2025_paper.html": {
    "title": "Drawing Developmental Trajectory from Cortical Surface Reconstruction",
    "volume": "main",
    "abstract": "Diffeomorphic-based cortical surface reconstruction typically involves a series of deformation processes to extract the cerebral cortex from brain magnetic resonance images (MRI). While most methods are designed for adult brains using Neural Ordinary Differential Equations (NODE) with fixed step sizes, the neonatal brain, which exhibits dramatic changes in cortical folding patterns early in life, requires a more adaptive approach. To address this, we develop a dual-task framework to directly characterize the brain development trajectory through processes of cortical surface reconstruction. For white matter (inner surfaces), we employ an Age-Conditioned ODE with adaptive step sizes. It is initially trained on a limited set of longitudinal paired data to establish a coarse trajectory, which is then refined through sample training of single-point data and knowledge distillation. For the pial surfaces (outer surfaces), we position the midthickness surfaces as intermediates and employ a cycle-consistent semi-supervised training strategy to depict a coherent brain development trajectory between the inner and outer surfaces. Our approach is the first to achieve precise developmental prediction directly on triangular meshes. Furthermore, by enhancing interpretability at each stage of the deformation process, this approach improves the applicability of diffeomorphic-based methods. The proposed method has demonstrated state-of-the-art performance in modeling developmental trajectories and cortical surface reconstruction within the developing Human Connectome Project dataset (dHCP)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenxuan Wu",
      "Ruowen Qu",
      "Zhongliang Liu",
      "Zhuoyan Dai",
      "Dongzi Shi",
      "Sijin Yu",
      "Tong Xiong",
      "Shiping Liu",
      "Xiangmin Xu",
      "Xiaofen Xing",
      "Xin Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_CryoFastAR_Fast_Cryo-EM_Ab_initio_Reconstruction_Made_Easy_ICCV_2025_paper.html": {
    "title": "CryoFastAR: Fast Cryo-EM Ab initio Reconstruction Made Easy",
    "volume": "main",
    "abstract": "Pose estimation from unordered images is fundamental for 3D reconstruction, robotics, and scientific imaging. Recent geometric foundation models, such as DUSt3R, enable end-to-end dense 3D reconstruction but remain underexplored in scientific imaging fields like cryo-electron microscopy (cryo-EM) for near-atomic protein reconstruction. In cryo-EM, pose estimation and 3D reconstruction from unordered particle images still depend on time-consuming iterative optimization, primarily due to challenges such as low signal-to-noise ratios (SNR) and distortions from the contrast transfer function (CTF). We introduce CryoFastAR, the first geometric foundation model that can directly predict poses from Cryo-EM noisy images for Fast ab initio Reconstruction. By integrating multi-view features and training on large-scale simulated cryo-EM data with realistic noise and CTF modulations, CryoFastAR enhances pose estimation accuracy and generalization. To enhance training stability, we propose a progressive training strategy that first allows the model to extract essential features under simpler conditions before gradually increasing difficulty to improve robustness. Experiments show that CryoFastAR achieves comparable quality while significantly accelerating inference over traditional iterative approaches on both synthetic and real datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiakai Zhang",
      "Shouchen Zhou",
      "Haizhao Dai",
      "Xinhang Liu",
      "Peihao Wang",
      "Zhiwen Fan",
      "Yuan Pei",
      "Jingyi Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hollein_3DGS-LM_Faster_Gaussian-Splatting_Optimization_with_Levenberg-Marquardt_ICCV_2025_paper.html": {
    "title": "3DGS-LM: Faster Gaussian-Splatting Optimization with Levenberg-Marquardt",
    "volume": "main",
    "abstract": "We present 3DGS-LM, a new method that accelerates the reconstruction of 3D Gaussian Splatting (3DGS) by replacing its ADAM optimizer with a tailored Levenberg-Marquardt (LM). Existing methods reduce the optimization time by decreasing the number of Gaussians or by improving the implementation of the differentiable rasterizer. However, they still rely on the ADAM optimizer to fit Gaussian parameters of a scene in thousands of iterations, which can take up to an hour. To this end, we change the optimizer to LM that runs in conjunction with the 3DGS differentiable rasterizer. For efficient GPU parallelization, we propose a caching data structure for intermediate gradients that allows us to efficiently calculate Jacobian-vector products in custom CUDA kernels. In every LM iteration, we calculate update directions from multiple image subsets using these kernels and combine them in a weighted mean. Overall, our method is 20% faster than the original 3DGS while obtaining the same reconstruction quality. Our optimization is also agnostic to other methods that accelerate 3DGS, thus enabling even faster speedups compared to vanilla 3DGS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lukas Höllein",
      "Aljaž Božič",
      "Michael Zollhöfer",
      "Matthias Nießner"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ren_GauUpdate_New_Object_Insertion_in_3D_Gaussian_Fields_with_Consistent_ICCV_2025_paper.html": {
    "title": "GauUpdate: New Object Insertion in 3D Gaussian Fields with Consistent Global Illumination",
    "volume": "main",
    "abstract": "3D Gaussian Splatting (3DGS) is a prevailing technique to reconstruct large-scale 3D scenes from multiview images for novel view synthesis, like a room, a block, and even a city. Such large-scale scenes are not static with changes constantly happening in these scenes, like a new building being built or a new decoration being set up. To keep the reconstructed 3D Gaussian fields up-to-date, a naive way is to reconstruct the whole scene after changing, which is extremely costly and inefficient. In this paper, we propose a new method called GauUpdate that allows partially updating an old 3D Gaussian field with new objects from a new 3D Gaussian field. However, simply inserting the new objects leads to inconsistent appearances because the old and new Gaussian fields may have different lighting environments from each other. GauUpdate addresses this problem by applying inverse rendering techniques in the 3DGS to recover both the materials and environmental lights. Based on the materials and lighting, we relight the new objects in the old 3D Gaussian field for consistent global illumination. For an accurate estimation of the materials and lighting, we put additional constraints on the materials and lighting conditions, that these two fields share the same materials but different environment lights, to improve their qualities. We conduct experiments on both synthetic scenes and real-world scenes to evaluate GauUpdate, which demonstrate that GauUpdate achieves realistic object insertion in 3D Gaussian fields with consistent appearances",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengwei Ren",
      "Fan Zhang",
      "Liangchao Xu",
      "Liang Pan",
      "Ziwei Liu",
      "Wenping Wang",
      "Xiao-Ping Zhang",
      "Yuan Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hu_OphCLIP_Hierarchical_Retrieval-Augmented_Learning_for_Ophthalmic_Surgical_Video-Language_Pretraining_ICCV_2025_paper.html": {
    "title": "OphCLIP: Hierarchical Retrieval-Augmented Learning for Ophthalmic Surgical Video-Language Pretraining",
    "volume": "main",
    "abstract": "Vision-language pretraining (VLP) enables open-world generalization beyond predefined labels, a critical capability in surgery due to the diversity of procedures, instruments, and patient anatomies. However, applying VLP to ophthalmic surgery presents unique challenges, including limited vision-language data, intricate procedural workflows, and the need for hierarchical understanding, ranging from fine-grained surgical actions to global clinical reasoning. To address these, we introduce OphVL, a large-scale, hierarchically structured dataset containing over 375K video-text pairs, making it 15x larger than existing surgical VLP datasets. OphVL captures a diverse range of ophthalmic surgical attributes, including surgical phases, operations, actions, instruments, medications, disease causes, surgical objectives, and postoperative care recommendations. By aligning short clips with detailed narratives and full-length videos with structured titles, OphVL provides both fine-grained surgical details and high-level procedural context. Building on OphVL, we propose OphCLIP, a hierarchical retrieval-augmented VLP framework. OphCLIP leverages silent surgical videos as a knowledge base, retrieving semantically relevant content to enhance narrated procedure learning. This enables OphCLIP to integrate explicit linguistic supervision with implicit visual knowledge, improving ophthalmic workflow modeling. Evaluations across 11 benchmark datasets for surgical phase recognition and multi-instrument identification demonstrate OphCLIP's robust generalization and superior performance, establishing it as a foundation model for ophthalmic surgery",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ming Hu",
      "Kun Yuan",
      "Yaling Shen",
      "Feilong Tang",
      "Xiaohao Xu",
      "Lin Zhou",
      "Wei Li",
      "Ying Chen",
      "Zhongxing Xu",
      "Zelin Peng",
      "Siyuan Yan",
      "Vinkle Srivastav",
      "Diping Song",
      "Tianbin Li",
      "Danli Shi",
      "Jin Ye",
      "Nicolas Padoy",
      "Nassir Navab",
      "Junjun He",
      "Zongyuan Ge"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_Hints_of_Prompt_Enhancing_Visual_Representation_for_Multimodal_LLMs_in_ICCV_2025_paper.html": {
    "title": "Hints of Prompt: Enhancing Visual Representation for Multimodal LLMs in Autonomous Driving",
    "volume": "main",
    "abstract": "In light of the dynamic nature of autonomous driving environments and stringent safety requirements, general MLLMs combined with CLIP alone often struggle to accurately represent driving-specific scenarios, particularly in complex interactions and long-tail cases. To address this, we propose the Hints of Prompt (HoP) framework, which introduces three key enhancements: Affinity hint to emphasize instance-level structure by strengthening token-wise connections, Semantic hint to incorporate high-level information relevant to driving-specific cases, such as complex interactions among vehicles and traffic signs, and Question hint to align visual features with the query context, focusing on question-relevant regions. These hints are fused through a Hint Fusion module, enriching visual representations by capturing driving-related representations with limited domain data, ensuring faster adaptation to driving scenarios. Extensive experiments confirm the effectiveness of the HoP framework, showing that it significantly outperforms previous state-of-the-art methods in all key metrics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Zhou",
      "Zhanning Gao",
      "Zhili Chen",
      "Maosheng Ye",
      "Qifeng Chen",
      "Tongyi Cao",
      "Honggang Qi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jung_IM360_Large-scale_Indoor_Mapping_with_360_Cameras_ICCV_2025_paper.html": {
    "title": "IM360: Large-scale Indoor Mapping with 360 Cameras",
    "volume": "main",
    "abstract": "We present a novel 3D mapping pipeline for large-scale indoor environments. To address the significant challenges in large-scale indoor scenes, such as prevalent occlusions and textureless regions, we propose IM360, a novel approach that leverages the wide field of view of omnidirectional images and integrates the spherical camera model into the Structure-from-Motion (SfM) pipeline. Our SfM utilizes dense matching features specifically designed for 360 images, demonstrating superior capability in image registration. Furthermore, with the aid of mesh-based neural rendering techniques, we introduce a texture optimization method that refines texture maps and accurately captures view-dependent properties by combining diffuse and specular components. We evaluate our pipeline on large-scale indoor scenes, demonstrating its effectiveness in real-world scenarios. In practice, IM360 demonstrates superior performance, achieving a 3.5 PSNR increase in textured mesh reconstruction. We attain state-of-the-art performance in terms of camera localization and registration on Matterport3D and Stanford2D3D",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongki Jung",
      "Jaehoon Choi",
      "Yonghan Lee",
      "Dinesh Manocha"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cheng_RegGS_Unposed_Sparse_Views_Gaussian_Splatting_with_3DGS_Registration_ICCV_2025_paper.html": {
    "title": "RegGS: Unposed Sparse Views Gaussian Splatting with 3DGS Registration",
    "volume": "main",
    "abstract": "3D Gaussian Splatting (3DGS) has demonstrated its potential in reconstructing scenes from unposed images. However, optimization-based 3DGS methods struggle with sparse views due to limited prior knowledge. Meanwhile, feed-forward Gaussian approaches are constrained by input formats, making it challenging to incorporate more input views. To address these challenges, we propose RegGS, a 3D Gaussian registration-based framework for reconstructing unposed sparse views. RegGS aligns local 3D Gaussians generated by a feed-forward network into a globally consistent 3D Gaussian representation. Technically, we implement an entropy-regularized Sinkhorn algorithm to efficiently solve the optimal transport Mixture 2-Wasserstein MW_2 distance, which serves as an alignment metric for Gaussian mixture models (GMMs) in Sim(3) space. Furthermore, we design a joint 3DGS registration module that integrates the MW_2 distance, photometric consistency, and depth geometry. This enables a coarse-to-fine registration process while accurately estimating camera poses and aligning the scene. Experiments on the RE10K and ACID datasets demonstrate that RegGS effectively registers local Gaussians with high fidelity, achieving precise pose estimation and high-quality novel view synthesis",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chong Cheng",
      "Yu Hu",
      "Sicheng Yu",
      "Beizhen Zhao",
      "Zijian Wang",
      "Hao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_SU-RGS_Relightable_3D_Gaussian_Splatting_from_Sparse_Views_under_Unconstrained_ICCV_2025_paper.html": {
    "title": "SU-RGS: Relightable 3D Gaussian Splatting from Sparse Views under Unconstrained Illuminations",
    "volume": "main",
    "abstract": "The latest advancements in scene relighting have been predominantly driven by inverse rendering with 3D Gaussian Splatting (3DGS). However, existing methods remain overly reliant on densely sampled images under static illumination conditions, which is prohibitively expensive and even impractical in real-world scenarios. In this paper, we propose a novel learning from Sparse views under Unconstrained illuminations Relightable 3D Gaussian Splatting (dubbed SU-RGS), to address this challenge by jointly optimizing 3DGS representations, surface materials, and environment illuminations (i.e., unknown and various lighting conditions in training) using only sparse input views. Firstly, SU-RGS presents a varying appearance rendering strategy, enabling each 3D Gaussian can perform inconsistent color under various lightings. Next, SU-RGS establishes the multi-view semantic consistency by constructing hierarchical semantics pseudo-labels across inter-views, to compensate for extra supervisions and facilitate sparse inverse rendering for confronting unconstrained illuminations. Additionally, we introduce an adaptive transient object perception component that integrates the scene geometry and semantics in a fine-grained manner, to quantify and eliminate the uncertainty of the foreground. Extensive experiments on both synthetic and real-world challenging datasets demonstrate the effectiveness of SU-RGS, achieving the state-of-the-art performance for scene inverse rendering by learning 3DGS from only sparse views under unconstrained illuminations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Zhang",
      "Chi Huang",
      "Qian Zhang",
      "Nan Li",
      "Wei Feng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Choi_Representation_Shift_Unifying_Token_Compression_with_FlashAttention_ICCV_2025_paper.html": {
    "title": "Representation Shift: Unifying Token Compression with FlashAttention",
    "volume": "main",
    "abstract": "Transformers have demonstrated remarkable success across vision, language, and video. Yet, increasing task complexity has led to larger models and more tokens, raising the quadratic cost of self-attention and the overhead of GPU memory access. To reduce the computation cost of self-attention, prior work has proposed token compression techniques that drop redundant or less informative tokens. Meanwhile, fused attention kernels such as FlashAttention have been developed to alleviate memory overhead by avoiding attention map construction and its associated I/O to HBM. This, however, makes it incompatible with most training-free token compression methods, which rely on attention maps to determine token importance. Here, we propose Representation Shift, a training-free, model-agnostic metric that measures the degree of change in each token's representation. This seamlessly integrates token compression with FlashAttention, without attention maps or retraining. Our method further generalizes beyond Transformers to CNNs and state space models. Extensive experiments show that Representation Shift enables effective token compression compatible with FlashAttention, yielding significant speedups of up to 5.5x and 4.4x in video-text retrieval and video QA, respectively. Code is available at https://github.com/mlvlab/Representation-Shift",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joonmyung Choi",
      "Sanghyeok Lee",
      "Byungoh Ko",
      "Eunseo Kim",
      "Jihyung Kil",
      "Hyunwoo J. Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liang_DGTalker_Disentangled_Generative_Latent_Space_Learning_for_Audio-Driven_Gaussian_Talking_ICCV_2025_paper.html": {
    "title": "DGTalker: Disentangled Generative Latent Space Learning for Audio-Driven Gaussian Talking Heads",
    "volume": "main",
    "abstract": "In this work, we investigate the generation of high-fidelity, audio-driven 3D Gaussian talking heads from monocular videos. We present DGTalker, an innovative framework designed for real-time, high-fidelity, and 3D-aware talking head synthesis. By leveraging Gaussian generative priors and treating the task as a latent space navigation problem, our method effectively alleviates the lack of 3D information and the low-quality detail reconstruction caused by the absence of structure priors in monocular videos, which is a longstanding challenge in existing 3DGS-based approaches. To ensure precise lip synchronization and nuanced expression control, we propose a disentangled latent space navigation method that independently models lip motion and talking expressions. Additionally, we introduce an effective masked cross-view supervision strategy to enable robust learning within the disentangled framework. We conduct extensive experiments and demonstrate that DGTalker surpasses current state-of-the-art methods in visual quality, motion accuracy, and controllability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoxi Liang",
      "Yanbo Fan",
      "Qiya Yang",
      "Xuan Wang",
      "Wei Gao",
      "Ge Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_RALoc_Enhancing_Outdoor_LiDAR_Localization_via_Rotation_Awareness_ICCV_2025_paper.html": {
    "title": "RALoc: Enhancing Outdoor LiDAR Localization via Rotation Awareness",
    "volume": "main",
    "abstract": "LiDAR localization is a fundamental task in autonomous driving and robotics. Scene Coordinate Regression (SCR) exhibits leading pose accuracy, achieving impressive results in learning-based localization. We observe that the real-world LiDAR scans captured from different viewpoints usually result in the catastrophic collapse of SCR. However, existing LiDAR localization methods have largely overlooked the issue of rotation sensitivity in SCR. In this paper, we present RALoc, an outdoor LiDAR localization method with rotation awareness to achieve accurate localization. The key to our approach is to design a Point Cloud Canonicalization module, which leverages a powerful equivariant key feature aggregation to transform the input LiDAR scan towards a consistent orientation, effectively eliminating the adverse effects of rotation. This proposed module has promising scalability and can be seamlessly integrated with the existing LiDAR localization network. Moreover, we propose the Bidirectional LiDAR Localization (BiLiLo) dataset as a benchmark to evaluate the performance of various methods in large outdoor scenes with significant rotation changes. Extensive experiments show that RALoc significantly improves localization performance in scenarios with large rotation changes, and also achieves competitive performance in the Oxford Radar RobotCar dataset. Our code and dataset will be released upon acceptance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuyang Yang",
      "Wen Li",
      "Sheng Ao",
      "Qingshan Xu",
      "Shangshu Yu",
      "Yu Guo",
      "Yin Zhou",
      "Siqi Shen",
      "Cheng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Becker_EDiT_Efficient_Diffusion_Transformers_with_Linear_Compressed_Attention_ICCV_2025_paper.html": {
    "title": "EDiT: Efficient Diffusion Transformers with Linear Compressed Attention",
    "volume": "main",
    "abstract": "Diffusion Transformers (DiTs) have emerged as a leading architecture for text-to-image synthesis, producing high-quality and photorealistic images. However, the quadratic scaling properties of the attention in DiTs hinder image generation with higher resolution or devices with limited resources. This work introduces an efficient diffusion transformer (EDiT) to alleviate these efficiency bottlenecks in conventional DiTs and Multimodal DiTs (MM-DiTs). First, we present a novel linear compressed attention method that uses a multi-layer convolutional network to modulate queries with local information while keys and values are spatially aggregated. Second, we formulate a hybrid attention scheme for multi-modal inputs that combines linear attention for image-to-image interactions and standard scaled dot-product attention for interactions involving prompts. Merging these two approaches leads to an expressive, linear-time Multimodal Efficient Diffusion Transformer (MM-EDiT). We demonstrate the effectiveness of the EDiT and MM-EDiT architectures by integrating them into PixArt-Sigma (conventional DiT) and Stable Diffusion 3.5-Medium (MM-DiT), achieving up to 2.2x speedup with comparable image quality after distillation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Philipp Becker",
      "Abhinav Mehrotra",
      "Ruchika Chavhan",
      "Malcolm Chadwick",
      "Luca Morreale",
      "Mehdi Noroozi",
      "Alberto Gil C. P. Ramos",
      "Sourav Bhattacharya"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ma_GenHancer_Imperfect_Generative_Models_are_Secretly_Strong_Vision-Centric_Enhancers_ICCV_2025_paper.html": {
    "title": "GenHancer: Imperfect Generative Models are Secretly Strong Vision-Centric Enhancers",
    "volume": "main",
    "abstract": "The synergy between generative and discriminative models receives growing attention. While discriminative Contrastive Language-Image Pre-Training (CLIP) excels in high-level semantics, it struggles with perceiving fine-grained visual details. Generally, to enhance representations, generative models take CLIP's visual features as conditions for reconstruction. However, the underlying principle remains underexplored. In this work, we empirically found that visually perfect generations are not always optimal for representation enhancement. The essence lies in effectively extracting fine-grained knowledge from generative models while mitigating irrelevant information. To explore critical factors, we delve into three aspects: (1) Conditioning mechanisms: We found that even a small number of local tokens can drastically reduce the difficulty of reconstruction, leading to collapsed training. We thus conclude that utilizing only global visual tokens as conditions is the most effective strategy. (2) Denoising configurations: We observed that end-to-end training introduces extraneous information. To address this, we propose a two-stage training strategy to prioritize learning useful visual knowledge. Additionally, we demonstrate that lightweight denoisers can yield remarkable improvements. (3) Generation paradigms: We explore both continuous and discrete denoisers with desirable outcomes, validating the versatility of our method. Through our in-depth explorations, we have finally arrived at an effective method, namely GenHancer, which consistently outperforms prior arts on the MMVP-VLM benchmark, e.g., 6.0% on OpenAICLIP. The enhanced CLIP can be further plugged into multimodal large language models for better vision-centric performance. All the models and codes are made publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shijie Ma",
      "Yuying Ge",
      "Teng Wang",
      "Yuxin Guo",
      "Yixiao Ge",
      "Ying Shan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhu_LLaVA-3D_A_Simple_yet_Effective_Pathway_to_Empowering_LMMs_with_ICCV_2025_paper.html": {
    "title": "LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with 3D Capabilities",
    "volume": "main",
    "abstract": "Recent advancements in Large Multimodal Models (LMMs) have greatly enhanced their proficiency in 2D visual understanding tasks, enabling them to effectively process and understand images and videos. However, the development of LMMs with 3D scene understanding capabilities has been hindered by the lack of large-scale 3D vision-language datasets and powerful 3D encoders. In this paper, we introduce a simple yet effective framework called LLaVA-3D. Leveraging the strong 2D visual understanding priors from LLaVA, our LLaVA-3D efficiently adapts LLaVA for 3D scene understanding without compromising 2D understanding capabilities. To achieve this, we utilize the 3D position embeddings to enhance the 2D CLIP Patches with 3D spatial context information and construct 3D patches. By integrating the 3D position embeddings into 2D LMMs and employing joint 2D and 3D vision-language instruction tuning, we establish a unified architecture for both 2D visual understanding and 3D scene understanding. In contrast to previous 3D LMMs, LLaVA-3D supports decoding accurate 3D spatial perception outputs, e.g., 3D bounding boxes, directly from these 3D patches, without relying on the time-consuming off-the-shelf 3D segmentors. Experimental results show that LLaVA-3D converges 3.5x faster than existing 3D LMMs when trained on 3D vision-language datasets. Moreover, LLaVA-3D not only achieves state-of-the-art performance across various 3D tasks but also maintains comparable 2D visual understanding and vision-language conversation capabilities with LLaVA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenming Zhu",
      "Tai Wang",
      "Wenwei Zhang",
      "Jiangmiao Pang",
      "Xihui Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Balauca_Understanding_Museum_Exhibits_using_Vision-Language_Reasoning_ICCV_2025_paper.html": {
    "title": "Understanding Museum Exhibits using Vision-Language Reasoning",
    "volume": "main",
    "abstract": "Museums serve as repositories of cultural heritage and historical artifacts from diverse epochs, civilizations, and regions, preserving well-documented collections that encapsulate vast knowledge, which, when systematically structured into large-scale datasets, can train specialized models. Visitors engage with exhibits through curiosity and questions, making expert domain-specific models essential for interactive query resolution and gaining historical insights. Understanding exhibits from images requires analyzing visual features and linking them to historical knowledge to derive meaningful correlations. We facilitate such reasoning by (a) collecting and curating a large-scale dataset of 65M images and 200M question-answer pairs for exhibits from all around the world; (b) training large vision-language models (VLMs) on the collected dataset; (c) benchmarking their ability on five visual question answering tasks, specifically designed to reflect real-world inquiries and challenges observed in museum settings.The complete dataset is labeled by museum experts, ensuring the quality and the practical significance of the labels. We train two VLMs from different categories: BLIP with vision-language aligned embeddings, but lacking the expressive power of large language models, and the LLaVA model, a powerful instruction-tuned LLM enriched with vision-language reasoning capabilities. Through extensive experiments, we find that while both model types effectively answer visually grounded questions, large vision-language models excel in queries requiring deeper historical context and reasoning. We further demonstrate the necessity of fine-tuning models on large-scale domain-specific datasets by showing that our fine-tuned models significantly outperform current SOTA VLMs in answering questions related to specific attributes, highlighting their limitations in handling complex, nuanced queries. Our dataset, benchmarks, and source code will be made publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ada-Astrid Balauca",
      "Sanjana Garai",
      "Stefan Balauca",
      "Rasesh Udayakumar Shetty",
      "Naitik Agrawal",
      "Dhwanil Subhashbhai  Shah",
      "Yuqian Fu",
      "Xi Wang",
      "Kristina Toutanova",
      "Danda Pani Paudel",
      "Luc Van Gool"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Nagrani_MINERVA_Evaluating_Complex_Video_Reasoning_ICCV_2025_paper.html": {
    "title": "MINERVA: Evaluating Complex Video Reasoning",
    "volume": "main",
    "abstract": "Multimodal LLMs are turning their focus to video benchmarks, however most video benchmarks only provide outcome supervision, with no intermediate or interpretable reasoning steps. This makes it challenging to assess if models are truly able to combine perceptual and temporal information to reason about videos, or simply get the correct answer by chance or by exploiting linguistic biases. To remedy this, we provide a new video reasoning dataset called MINERVA for modern multimodal models. Each question in the dataset comes with 5 answer choices, as well as detailed, hand-crafted reasoning traces. Our dataset is multimodal, diverse in terms of video domain and length, and consists of complex multi-step questions. Extensive benchmarking shows that our dataset provides a challenge for frontier open-source and proprietary models. We perform fine-grained error analysis to identify common failure modes across various models, and create a taxonomy of reasoning errors. We use this to explore both human and LLM-as-a-judge methods for scoring video reasoning traces, and find that failure modes are primarily related to temporal localization, followed by visual perception errors, as opposed to logical or completeness errors. The dataset, along with questions, answer candidates and reasoning traces is publicly available under https://github.com/google-deepmind/neptune?tab=readme-ov-file#minerva https://github.com/google-deepmind/neptune?tab=readme-ov-file\\#minerva",
    "checked": true,
    "id": "34d350d400c9e4865e35e650356f2ae426f143ae",
    "semantic_title": "minerva: evaluating complex video reasoning",
    "citation_count": 5,
    "authors": [
      "Arsha Nagrani",
      "Sachit  Menon",
      "Ahmet Iscen",
      "Shyamal Buch",
      "Ramin Mehran",
      "Nilpa Jha",
      "Anja Hauth",
      "Yukun Zhu",
      "Carl  Vondrick",
      "Mikhail Sirotenko",
      "Cordelia Schmid",
      "Tobias Weyand"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/DeAlcala_Active_Membership_Inference_Test_aMINT_Enhancing_Model_Auditability_with_Multi-Task_ICCV_2025_paper.html": {
    "title": "Active Membership Inference Test (aMINT): Enhancing Model Auditability with Multi-Task Learning",
    "volume": "main",
    "abstract": "Active Membership Inference Test (aMINT) is a method designed to detect whether given data were used during the training of machine learning models. In Active MINT, we propose a novel multitask learning process that involves training simultaneously two models: the original or Audited Model, and a secondary model, referred to as the MINT Model, responsible for identifying the data used for training the Audited Model. This novel multi-task learning approach has been designed to incorporate the auditability of the model as an optimization objective during the training process of neural networks. The proposed approach incorporates intermediate activation maps as inputs to the MINT layers, which are trained to enhance the detection of training data. We present results using a wide range of neural networks, from lighter architectures such as MobileNet to more complex ones such as Vision Transformers, evaluated in 5 public benchmarks. Our proposed Active MINT achieves over 80% accuracy in detecting if given data was used for training, significantly outperforming previous approaches in the literature. Our aMINT and related methodological developments contribute to increasing transparency in AI models, facilitating stronger safeguards in AI deployments to achieve proper security, privacy, and copyright protection (Code available in https://github.com/DanieldeAlcala/Membership-Inference-Test.git)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel DeAlcala",
      "Aythami Morales",
      "Julian Fierrez",
      "Gonzalo Mancera",
      "Ruben Tolosana",
      "Javier Ortega-Garcia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Fang_One_Perturbation_is_Enough_On_Generating_Universal_Adversarial_Perturbations_against_ICCV_2025_paper.html": {
    "title": "One Perturbation is Enough: On Generating Universal Adversarial Perturbations against Vision-Language Pre-training Models",
    "volume": "main",
    "abstract": "Vision-Language Pre-training (VLP) models have exhibited unprecedented capability in many applications by taking full advantage of the learned multimodal alignment. However, previous studies have shown they are vulnerable to maliciously crafted adversarial samples. Despite recent success, these attacks are generally instance-specific and require generating perturbations for each input sample. In this paper, we reveal that VLP models are also susceptible to the instance-agnostic universal adversarial perturbation (UAP). Specifically, we design a novel Contrastive-training Perturbation Generator with Cross-modal conditions (C-PGC). In light that the pivotal multimodal alignment in VLP models is achieved via contrastive learning, we devise to turn this powerful weapon against VLP models themselves. I.e., we employ a malicious version of contrastive learning to train the proposed generator using our carefully crafted positive and negative image-text pairs. Once training is complete, the generator is able to produce universal perturbations that can essentially destroy the established alignment relationship in VLP models. Besides, C-PGC fully utilizes the characteristics of Vision-and-Language (V+L) scenarios by incorporating both unimodal and cross-modal information as effective guidance. Extensive experiments show that C-PGC successfully forces adversarial samples to move away from their original area in the VLP model's feature space, thus fundamentally enhancing attack performance across various victim models and V+L tasks",
    "checked": true,
    "id": "b9d975e94eaef636c969e464402c22226eabc90c",
    "semantic_title": "one perturbation is enough: on generating universal adversarial perturbations against vision-language pre-training models",
    "citation_count": 21,
    "authors": [
      "Hao Fang",
      "Jiawei Kong",
      "Wenbo Yu",
      "Bin Chen",
      "Jiawei Li",
      "Hao Wu",
      "Shu-Tao Xia",
      "Ke Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_When_Lighting_Deceives_Exposing_Vision-Language_Models_Illumination_Vulnerability_Through_Illumination_ICCV_2025_paper.html": {
    "title": "When Lighting Deceives: Exposing Vision-Language Models' Illumination Vulnerability Through Illumination Transformation Attack",
    "volume": "main",
    "abstract": "Vision-Language Models (VLMs) have achieved remarkable success in various tasks, yet their robustness to real-world illumination variations remains largely unexplored. To bridge this gap, we propose Illumination Transformation Attack (ITA), the first framework to systematically assess VLMs' robustness against illumination changes. However, there still exist two key challenges: (1) how to model global illumination with fine-grained control to achieve diverse lighting conditions and (2) how to ensure adversarial effectiveness while maintaining naturalness. To address the first challenge, we innovatively decompose global illumination into multiple parameterized point light sources based on the illumination rendering equation. This design enables us to model more diverse lighting variations that previous methods could not capture. Then, by integrating these parameterized lighting variations with physics-based lighting reconstruction techniques, we could precisely render such light interactions in the original scenes, finally meeting the goal of fine-grained lighting control. For the second challenge, by controlling illumination through the lighting reconstrution model's latent space rather than direct pixel manipulation, we inherently preserve physical lighting priors. Furthermore, to prevent potential reconstruction artifacts, we design additional perceptual constraints for maintaining visual consistency with original images and diversity constraints for avoiding light source convergence. Extensive experiments demonstrate that our ITA could significantly reduce the performance of advanced VLMs, e.g., LLaVA-1.6, while possessing competitive naturalness, exposing VLMS' critical illuminiation vulnerabilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanqing Liu",
      "Shouwei Ruan",
      "Yao Huang",
      "Shiji Zhao",
      "Xingxing Wei"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Exploring_View_Consistency_for_Scene-Adaptive_Low-Light_Light_Field_Image_Enhancement_ICCV_2025_paper.html": {
    "title": "Exploring View Consistency for Scene-Adaptive Low-Light Light Field Image Enhancement",
    "volume": "main",
    "abstract": "Light Field (LF) images captured under low illumination conditions typically exhibit low quality. Recent learning-based methods for low-light LF enhancement are generally tailored to specific illumination inputs, limiting their performance in real-world scenes. Moreover, how to maintain the inherent view-consistency in the enhanced images also remain as a difficult problem. In this paper, we propose to explore the view consistency for scene-adaptive low-light LF enhancement. We first analyze the view consistency for LF illumination maps and design a self-supervised view-consistent loss to keep the consistency between the illumination maps of different views in LFs. To enhance the model's perception of illumination, we combine both global and local information to estimate the illumination map, which is easily plugged into other models. Subsequently, we use the illumination maps to light up the low-light LF images and restore the corruption to produce the final enhanced image. Extensive experiments demonstrate that our View-Consistenct Network (VCNet) outperforms state-of-the-art methods on real-world low-light LF datasets in both fixed lighting conditions and dynamic lighting conditions. Our proposed illumination adjustment is also demonstrated that can comprehensively improve the performance of existing methods in terms of both image quality and view consistency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuo Zhang",
      "Chen Gao",
      "Youfang Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hong_DIA_The_Adversarial_Exposure_of_Deterministic_Inversion_in_Diffusion_Models_ICCV_2025_paper.html": {
    "title": "DIA: The Adversarial Exposure of Deterministic Inversion in Diffusion Models",
    "volume": "main",
    "abstract": "Diffusion models have shown to be strong representation learners, showcasing state-of-the-art performance across multiple domains. Aside from accelerated sampling, DDIM also enables the inversion of real images back to their latent codes. A direct inheriting application of this inversion operation is real image editing, where the inversion yields latent trajectories to be utilized during the synthesis of the edited image. Unfortunately, this practical tool has enabled malicious users to freely synthesize misinformative or deepfake contents with greater ease, which promotes the spread of unethical and abusive, as well as privacy-, and copyright-infringing contents. While defensive algorithms such as AdvDM and Photoguard have been shown to disrupt the diffusion process on these images, the misalignment between their objectives and the iterative denoising trajectory at test time results in weak disruptive performance. In this work, we present the DDIM Inversion Attack (DIA) that attacks the integrated DDIM trajectory path. Our results support the effective disruption, surpassing previous defensive methods across various editing methods. We believe that our frameworks and results can provide practical defense methods against the malicious use of AI for both the industry and the research community. Our code is available here: https://anonymous.4open.science/r/DIA-13419/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seunghoo Hong",
      "Geonho Son",
      "Juhun Lee",
      "Simon S. Woo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Fan_Rectifying_Magnitude_Neglect_in_Linear_Attention_ICCV_2025_paper.html": {
    "title": "Rectifying Magnitude Neglect in Linear Attention",
    "volume": "main",
    "abstract": "As the core operator of Transformers, Softmax Attention exhibits excellent global modeling capabilities. However, its quadratic complexity limits its applicability to vision tasks. In contrast, Linear Attention shares a similar formulation with Softmax Attention while achieving linear complexity, enabling efficient global information modeling. Nevertheless, Linear Attention suffers from a significant performance degradation compared to standard Softmax Attention. In this paper, we analyze the underlying causes of this issue based on the formulation of Linear Attention. We find that, unlike Softmax Attention, Linear Attention entirely disregards the magnitude information of the Query(Q or \\phi(Q)). The absence of magnitude information prevents the attention score distribution from dynamically adapting as the Query scales. As a result, despite its structural similarity to Softmax Attention, Linear Attention exhibits a significantly different attention score distribution. Based on this observation, we propose **Magnitude-Aware Linear Attention** (MALA), which modifies the computation of Linear Attention to fully incorporate the Query's magnitude. This adjustment allows MALA to generate an attention score distribution that closely resembles Softmax Attention while exhibiting a more well-balanced structure. As a result, MALA surpasses Softmax Attention in performance while maintaining only linear complexity. We build Magnitude-Aware Vision Transformer (MAViT) based on MALA, achieving **84.7%** accuracy on ImageNet-1K with only **27M** parameters and **4.6G** flops, without using any additional data or labels. It also exhibits excellent inference efficiency. This result highlights the strong potential of MALA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qihang Fan",
      "Huaibo Huang",
      "Yuang Ai",
      "Ran He"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_GEMeX_A_Large-Scale_Groundable_and_Explainable_Medical_VQA_Benchmark_for_ICCV_2025_paper.html": {
    "title": "GEMeX: A Large-Scale, Groundable, and Explainable Medical VQA Benchmark for Chest X-ray Diagnosis",
    "volume": "main",
    "abstract": "Medical Visual Question Answering (Med-VQA) combines computer vision and natural language processing to automatically answer clinical inquiries about medical images. However, current Med-VQA datasets exhibit two significant limitations: (1) they often lack visual and textual explanations for answers, hindering comprehension for patients and junior doctors; (2) they typically offer a narrow range of question formats, inadequately reflecting the diverse requirements in practical scenarios. These limitations pose significant challenges to the development of a reliable and user-friendly Med-VQA system. To address these challenges, we introduce a large-scale, Groundable, and Explainable Medical VQA benchmark for chest X-ray diagnosis (GEMeX), featuring several innovative components: (1) a multi-modal explainability mechanism that offers detailed visual and textual explanations for each question-answer pair, thereby enhancing answer comprehensibility; (2) four question types--open-ended, closed-ended, single-choice, and multiple-choice--to better reflect practical needs. With 151,025 images and 1,605,575 questions, GEMeX is the currently largest chest X-ray VQA dataset. Evaluation of 12 representative large vision language models (LVLMs) on GEMeX reveals suboptimal performance, underscoring the dataset's complexity. Meanwhile, we propose a strong model by fine-tuning an existing LVLM on the GEMeX training set. The substantial performance improvement showcases the dataset's effectiveness. The benchmark is available at www.med-vqa.com/GEMeX",
    "checked": true,
    "id": "fe542f138a84185b0060c26dc1988827a0f3498d",
    "semantic_title": "gemex: a large-scale, groundable, and explainable medical vqa benchmark for chest x-ray diagnosis",
    "citation_count": 4,
    "authors": [
      "Bo Liu",
      "Ke Zou",
      "Li-Ming Zhan",
      "Zexin Lu",
      "Xiaoyu Dong",
      "Yidi Chen",
      "Chengqiang Xie",
      "Jiannong Cao",
      "Xiao-Ming Wu",
      "Huazhu Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kichler_Learning_to_See_Inside_Opaque_Liquid_Containers_using_Speckle_Vibrometry_ICCV_2025_paper.html": {
    "title": "Learning to See Inside Opaque Liquid Containers using Speckle Vibrometry",
    "volume": "main",
    "abstract": "Computer vision seeks to infer a wide range of information about objects and events. However, vision systems based on conventional imaging are limited to extracting information only from the visible surfaces of scene objects. For instance, a vision system can detect and identify a Coke can in the scene, but it cannot determine whether the can is full or empty. In this paper, we aim to expand the scope of computer vision to include the novel task of inferring the hidden liquid levels of opaque containers by sensing the tiny vibrations on their surfaces. Our method provides a first-of-a-kind way to inspect the fill level of multiple sealed containers remotely, at once, without needing physical manipulation and manual weighing. First, we propose a novel speckle-based vibration sensing system for simultaneously capturing scene vibrations on a 2D grid of points. We use our system to efficiently and remotely capture a dataset of vibration responses for a variety of everyday liquid containers. Then, we develop a transformer-based approach for analyzing the captured vibrations and classifying the container type and its hidden liquid level at the time of measurement. Our architecture is invariant to the vibration source, yielding correct liquid level estimates for controlled and ambient scene sound sources. Moreover, our model generalizes to unseen container instances within known classes (e.g., training on five Coke cans of a six-pack, testing on a sixth) and fluid levels. We demonstrate our method by recovering liquid levels from various everyday containers",
    "checked": true,
    "id": "1e3bde1f5148587d26e0206bb312c817c17de720",
    "semantic_title": "learning to see inside opaque liquid containers using speckle vibrometry",
    "citation_count": 0,
    "authors": [
      "Matan Kichler",
      "Shai Bagon",
      "Mark Sheinin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Celen_HouseTour_A_Virtual_Real_Estate_AIgent_ICCV_2025_paper.html": {
    "title": "HouseTour: A Virtual Real Estate A(I)gent",
    "volume": "main",
    "abstract": "We introduce HouseTour, a method for spatially-aware 3D camera trajectory and natural language summary generation from a collection of images depicting an existing 3D space. Unlike existing vision-language models (VLMs), which struggle with geometric reasoning, our approach generates smooth video trajectories via a diffusion process constrained by known camera poses and integrates this information into the VLM for 3D-grounded descriptions. We synthesize the final video using 3D Gaussian splatting to render novel views along the trajectory. To support this task, we present the HouseTour dataset, which includes over 1,200 house-tour videos with camera poses, 3D reconstructions, and real estate descriptions. Experiments demonstrate that incorporating 3D camera trajectories into the text generation process improves performance over methods handling each task independently. We evaluate both individual and end-to-end performance, introducing a new joint metric. Our work enables automated, professional-quality video creation for real estate and touristic applications without requiring specialized expertise or equipment",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ata Çelen",
      "Marc Pollefeys",
      "Daniel Barath",
      "Iro Armeni"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cui_Debiased_Teacher_for_Day-to-Night_Domain_Adaptive_Object_Detection_ICCV_2025_paper.html": {
    "title": "Debiased Teacher for Day-to-Night Domain Adaptive Object Detection",
    "volume": "main",
    "abstract": "Day-to-Night Domain Adaptive Object Detection (DN-DAOD) is a significant challenge due to the low visibility and signal-to-noise ratio at night. Although recent self-training approaches achieve promising results, they fail to address three critical biases: distribution bias, training bias, and confirmation bias. Therefore, we propose a Debiased Teacher to address the above biases from three aspects: domain transforming, representation compensating, and pseudo label calibrating. Concretely, the day-to-night domain transforming module (DNDT) leverages physical priors to model some key day-night domain differences, thus transforming daytime images into night-like images. Then, the cross-domain representation compensating module (CDRC) selectively mixes objects from nighttime and night-like images to compensate for the model's general representation of nighttime objects. Further, to correct confirmation bias caused by learning from inaccurate pseudo labels, the pseudo label confirmation calibrating module (ConCal) is designed to obtain accurate pseudo labels for better nighttime knowledge learning. Experimental results on three benchmarks demonstrate that our method outperforms current SOTA methods by a large margin",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiming Cui",
      "Liang Li",
      "Haibing Yin",
      "Yuhan Gao",
      "Yaoqi Sun",
      "Chenggang Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Frequency-Aligned_Knowledge_Distillation_for_Lightweight_Spatiotemporal_Forecasting_ICCV_2025_paper.html": {
    "title": "Frequency-Aligned Knowledge Distillation for Lightweight Spatiotemporal Forecasting",
    "volume": "main",
    "abstract": "Spatiotemporal forecasting tasks, such as traffic flow, combustion dynamics, and weather forecasting, often require complex models that suffer from low training efficiency and high memory consumption. This paper proposes a lightweight framework, Spectral Decoupled Knowledge Distillation, which transfers the multi-scale spatiotemporal representations from a complex teacher model to a more efficient lightweight student network. The teacher model follows an encoder-latent evolution-decoder architecture, where its latent evolution module decouples high-frequency details (e.g., instant traffic fluctuations) and low-frequency trends (e.g. long-term weather evolution) using convolution (local high-frequency extractor) and Transformer (global low-frequency modeler). However, the multi-layer convolution and deconvolution structures result in slow training and high memory usage. To address these issues, we propose a frequency-aligned knowledge distillation strategy, which extracts multi-scale spectral features from the teacher's latent space, including high and low frequency components, to guide the lightweight student model (e.g., ResNet, U-Net) in capturing both local fine-grained variations and global evolution patterns. Experiments show that the student model achieves over 95% of the teacher's forecasting accuracy while using only 20%-30% of its memory, with training speed improved by more than 50%. Our theoretical analysis reveals that the frequency-domain decoupling enables the student model to capture long-range dependencies without the need for complex structures. The frequency-aligned distillation mechanism further mitigates the inherent bias of lightweight models in cross-scale spatiotemporal dynamics modeling. This framework offers an effective and general solution for high-accuracy spatiotemporal forecasting in resource-constrained scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuqi Li",
      "Chuanguang Yang",
      "Hansheng Zeng",
      "Zeyu Dong",
      "Zhulin An",
      "Yongjun Xu",
      "Yingli Tian",
      "Hao Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Pathak_Colors_See_Colors_Ignore_Clothes_Changing_ReID_with_Color_Disentanglement_ICCV_2025_paper.html": {
    "title": "Colors See Colors Ignore: Clothes Changing ReID with Color Disentanglement",
    "volume": "main",
    "abstract": "Clothes-Changing Re-Identification (CC-ReID) aims to recognize individuals across different locations and times, irrespective of clothing. Existing methods often rely on additional models or annotations to learn robust, clothing-invariant features, making them resource-intensive. In contrast, we explore the use of color --specifically foreground and background colors--as a lightweight, annotation-free proxy for mitigating appearance bias in ReID models. We propose Colors See, Colors Ignore (CSCI), an RGB-only method that leverages color information directly from raw images or video frames. CSCI efficiently captures color-related appearance bias ('Color See') while disentangling it from identity-relevant ReID features ('Color Ignore'). To achieve this, we introduce S2A self-attention, a novel self-attention to prevent information leak between color and identity cues within the feature space. Our analysis shows a strong correspondence between learned color embeddings and clothing attributes, validating color as an effective proxy when explicit clothing labels are unavailable. We demonstrate the effectiveness of CSCI on both image and video ReID with extensive experiments on four CC-ReID datasets. We improve the baseline by Top-1 2.9% on LTCC and 5.0% on PRCC for image-based ReID, and 1.0% on CCVID and 2.5% on MeVID for video-based ReID without relying on additional supervision. Our results highlight the potential of color as a cost-effective solution for addressing appearance bias in CC-ReID",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Priyank Pathak",
      "Yogesh S. Rawat"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Image_as_an_IMU_Estimating_Camera_Motion_from_a_Single_ICCV_2025_paper.html": {
    "title": "Image as an IMU: Estimating Camera Motion from a Single Motion-Blurred Image",
    "volume": "main",
    "abstract": "In many robotics and VR/AR applications, fast camera motions lead to a high level of motion blur, causing existing camera pose estimation methods to fail. In this work, we propose a novel framework that leverages motion blur as a rich cue for motion estimation rather than treating it as an unwanted artifact. Our approach works by predicting a dense motion flow field and a monocular depth map directly from a single motion-blurred image. We then recover the instantaneous camera velocity by solving a linear least squares problem under the small motion assumption. In essence, our method produces an IMU-like measurement that robustly captures fast and aggressive camera movements. To train our model, we construct a large-scale dataset with realistic synthetic motion blur derived from ScanNet++v2 and further refine our model by training end-to-end on real data using our fully differentiable pipeline. Extensive evaluations on real-world benchmarks demonstrate that our method achieves state-of-the-art angular and translational velocity estimates, outperforming current methods like MASt3R and COLMAP",
    "checked": true,
    "id": "52019d755480d70ceb54139340741d9ced873578",
    "semantic_title": "image as an imu: estimating camera motion from a single motion-blurred image",
    "citation_count": 2,
    "authors": [
      "Jerred Chen",
      "Ronald Clark"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_AnyI2V_Animating_Any_Conditional_Image_with_Motion_Control_ICCV_2025_paper.html": {
    "title": "AnyI2V: Animating Any Conditional Image with Motion Control",
    "volume": "main",
    "abstract": "Recent advancements in video generation, particularly in diffusion models, have driven notable progress in text-to-video (T2V) and image-to-video (I2V) synthesis. However, challenges remain in effectively integrating dynamic motion signals and flexible spatial constraints. Existing T2V methods typically rely on text prompts, which inherently lack precise control over the spatial layout of generated content. In contrast, I2V methods are limited by their dependence on real images, which restricts the editability of the synthesized content. Although some methods incorporate ControlNet to introduce image-based conditioning, they often lack explicit motion control and require computationally expensive training. To address these limitations, we propose AnyI2V, a training-free framework that animates any conditional images with user-defined motion trajectories. AnyI2V supports a broader range of modalities as the conditional image, including data types such as meshes and point clouds that are not supported by ControlNet, enabling more flexible and versatile video generation. Additionally, it supports mixed conditional inputs and enables style transfer and editing via LoRA and text prompts. Extensive experiments demonstrate that the proposed AnyI2V achieves superior performance and provides a new perspective in spatial- and motion-controlled video generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziye Li",
      "Hao Luo",
      "Xincheng Shuai",
      "Henghui Ding"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_GENMO_A_GENeralist_Model_for_Human_MOtion_ICCV_2025_paper.html": {
    "title": "GENMO: A GENeralist Model for Human MOtion",
    "volume": "main",
    "abstract": "Human motion modeling traditionally separates motion generation and estimation into distinct tasks with specialized models. Motion generation models focus on creating diverse, realistic motions from inputs like text, audio, or keyframes, while motion estimation models aim to reconstruct accurate motion trajectories from observations like videos. Despite sharing underlying representations of temporal dynamics and kinematics, this separation limits knowledge transfer between tasks and requires maintaining separate models. We present GENMO, a unified Generalist Model for Human Motion that bridges motion estimation and generation in a single framework. Our key insight is to reformulate motion estimation as constrained motion generation, where the output motion must precisely satisfy observed conditioning signals. Leveraging the synergy between regression and diffusion, GENMO achieves accurate global motion estimation while enabling diverse motion generation. We also introduce an estimation-guided training objective that exploits in-the-wild videos with 2D annotations and text descriptions to enhance generative diversity. Furthermore, our novel architecture handles variable-length motions and mixed multimodal conditions (text, audio, video) at different time intervals, offering flexible control. This unified approach creates synergistic benefits: generative priors improve estimated motions under challenging conditions like occlusions, while diverse video data enhances generation capabilities. Extensive experiments demonstrate GENMO's effectiveness as a generalist framework that successfully handles multiple human motion tasks within a single model",
    "checked": true,
    "id": "03f55635c685c1cf1cae19e176a15bebb9041439",
    "semantic_title": "genmo: a generalist model for human motion",
    "citation_count": 2,
    "authors": [
      "Jiefeng Li",
      "Jinkun Cao",
      "Haotian Zhang",
      "Davis Rempe",
      "Jan Kautz",
      "Umar Iqbal",
      "Ye Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lu_M-Net_MRI_Brain_Tumor_Sequential_Segmentation_Network_via_Mesh-Cast_ICCV_2025_paper.html": {
    "title": "M-Net: MRI Brain Tumor Sequential Segmentation Network via Mesh-Cast",
    "volume": "main",
    "abstract": "MRI tumor segmentation remains a critical challenge in medical imaging, where volumetric analysis faces unique computational demands due to the complexity of 3D data. The spatially sequential arrangement of adjacent MRI slices provides valuable information that enhances segmentation continuity and accuracy, yet this characteristic remains underutilized in many existing models. The spatial correlations between adjacent MRI slices can be regarded as \"temporal-like\" data, similar to frame sequences in video segmentation tasks. To bridge this gap, we propose M-Net, a flexible framework specifically designed for sequential image segmentation. M-Net introduces the novel Mesh-Cast mechanism, which seamlessly integrates arbitrary sequential models into the processing of both channel and temporal information, thereby systematically capturing the inherent \"temporal-like\" spatial correlations between MRI slices and ensuring consistent segmentation across sequences. Additionally, we define an MRI sequential input pattern and design a Two-Phase Sequential (TPS) training strategy, which first focuses on learning common patterns across sequences before refining slice-specific feature extraction. This approach leverages temporal modeling techniques to preserve volumetric contextual information while avoiding the high computational cost of full 3D convolutions, thereby enhancing the generalizability and robustness of M-Net in sequential segmentation tasks. Experiments on the BraTS2019 and BraTS2023 datasets demonstrate that M-Net outperforms existing methods across all key metrics, establishing itself as a robust solution for temporally-aware MRI tumor segmentation",
    "checked": true,
    "id": "1b9bb55dede2cdb7132b7251b08d3c592b09a5bc",
    "semantic_title": "m-net: mri brain tumor sequential segmentation network via mesh-cast",
    "citation_count": 1,
    "authors": [
      "Jiacheng Lu",
      "Hui Ding",
      "Shiyu Zhang",
      "Guoping Huo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Weakly_Supervised_Visible-Infrared_Person_Re-Identification_via_Heterogeneous_Expert_Collaborative_Consistency_ICCV_2025_paper.html": {
    "title": "Weakly Supervised Visible-Infrared Person Re-Identification via Heterogeneous Expert Collaborative Consistency Learning",
    "volume": "main",
    "abstract": "To reduce the reliance of visible-infrared person re-identification (ReID) models on labeled cross-modal samples, this paper explores a weakly supervised cross-modal person ReID method that uses only single-modal sample identity labels, addressing scenarios where cross-modal identity labels are unavailable. To mitigate the impact of missing cross-modal labels on model performance, we propose a heterogeneous expert collaborative consistency learning framework, designed to establish robust cross-modal identity correspondences in a weakly supervised manner. This framework leverages labeled data from each modality to independently train dedicated classification experts. To associate cross-modal samples, these classification experts act as heterogeneous predictors, predicting the identities of samples from the other modality. To improve prediction accuracy, we design a cross-modal relationship fusion mechanism that effectively integrates predictions from different experts. Under the implicit supervision provided by cross-modal identity correspondences, collaborative and consistent learning among the experts is encouraged, significantly enhancing the model's ability to extract modality-invariant features and improve cross-modal identity recognition. Experimental results on two challenging datasets validate the effectiveness of the proposed method. Code is available at https://github.com/KongLingqi2333/WSL-VIReID",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yafei Zhang",
      "Lingqi Kong",
      "Huafeng Li",
      "Jie Wen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ji_Prompt-A-Video_Prompt_Your_Video_Diffusion_Model_via_Preference-Aligned_LLM_ICCV_2025_paper.html": {
    "title": "Prompt-A-Video: Prompt Your Video Diffusion Model via Preference-Aligned LLM",
    "volume": "main",
    "abstract": "Text-to-video models have made remarkable advancements through optimization on high-quality text-video pairs, where the textual prompts play a pivotal role in determining quality of output videos. However, achieving the desired output often entails multiple revisions and iterative inference to refine user-provided prompts. Current automatic methods for refining prompts encounter challenges such as Modality-Inconsistency, Cost-Discrepancy, and Model-Unaware when applied to text-to-video diffusion models. To address these problem, we introduce an LLM-based prompt adaptation framework, termed as Prompt-A-Video, which excels in crafting Video-Centric, Labor-Free and Preference-Aligned prompts tailored to specific video diffusion model. Our approach involves a meticulously crafted two-stage optimization and alignment system. Initially, we conduct a reward-guided prompt evolution pipeline to automatically create optimal prompts pool and leverage them for supervised fine-tuning (SFT) of the LLM. Then multi-dimensional rewards are employed to generate pairwise data for the SFT model, followed by the direct preference optimization (DPO) algorithm to further facilitate preference alignment. Through extensive experimentation and comparative analyses, we validate the effectiveness of Prompt-A-Video across diverse generation models, highlighting its potential to push the boundaries of video generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yatai Ji",
      "Jiacheng Zhang",
      "Jie Wu",
      "Shilong Zhang",
      "Shoufa Chen",
      "Chongjian Ge",
      "Peize Sun",
      "Weifeng Chen",
      "Wenqi Shao",
      "Xuefeng Xiao",
      "Weilin Huang",
      "Ping Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chihaoui_Diffusion_Image_Prior_ICCV_2025_paper.html": {
    "title": "Diffusion Image Prior",
    "volume": "main",
    "abstract": "Zero-shot image restoration (IR) methods based on pretrained diffusion models have recently achieved significant success. These methods typically require at least a parametric form of the degradation model. However, in real-world scenarios, the degradation may be too complex to define explicitly without relying on crude approximations. To handle this general case, we introduce the DIffusion Image Prior (DIIP). We take inspiration from the Deep Image Prior (DIP), since it can be used to remove artifacts without the need for an explicit degradation model. However, in contrast to DIP, we find that pretrained diffusion models offer a much stronger prior, despite being trained without knowledge from corrupted data. We show that, the optimization process in DIIP first reconstructs a clean version of the image before eventually overfitting to the degraded input, but it does so for a broader range of degradations than DIP. In light of this result, we propose a blind image restoration (IR) method based on early stopping, which does not require prior knowledge of the degradation model. We validate DIIP on various degradation-blind IR tasks, including JPEG artifact removal, waterdrop removal, denoising and super-resolution with state-of-the-art results",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hamadi Chihaoui",
      "Paolo Favaro"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Constructing_Ophthalmic_MLLM_for_Positioning-diagnosis_Collaboration_Through_Clinical_Cognitive_Chain_ICCV_2025_paper.html": {
    "title": "Constructing Ophthalmic MLLM for Positioning-diagnosis Collaboration Through Clinical Cognitive Chain Reasoning",
    "volume": "main",
    "abstract": "Multimodal large language models (MLLMs) demonstrate significant potential in the field of medical diagnosis. However, they face critical challenges in specialized domains such as ophthalmology, particularly the fragmentation of annotation granularity and inconsistencies in clinical reasoning logic, which hinder precise cross-modal understanding. This paper introduces FundusExpert, an ophthalmology-specific MLLM with integrated positioning-diagnosis reasoning capabilities, along with FundusGen, a dataset constructed through the intelligent Fundus-Engine system. Fundus-Engine automates localization and leverages MLLM-based semantic expansion to integrate global disease classification, local object detection, and fine-grained feature analysis within a single fundus image. Additionally, by constructing a clinically aligned cognitive chain, it guides the model to generate interpretable reasoning paths. FundusExpert, fine-tuned with instruction data from FundusGen, achieves the best performance in ophthalmic question-answering tasks, surpassing the average accuracy of the 40B MedRegA by 26.6%. It also excels in zero-shot report generation tasks, achieving a clinical consistency of 77.0%, significantly outperforming GPT-4o's 47.6%. Furthermore, we reveal a scaling law between data quality and model capability (L \\propto N^ 0.068 ), demonstrating that the cognitive alignment annotations in FundusGen enhance data utilization efficiency. By integrating region-level localization with diagnostic reasoning chains, our work develops a scalable, clinically-aligned MLLM and explores a pathway toward bridging the visual-language gap in specific MLLMs. Our project can be found at https://github.com/MeteorElf/FundusExpert",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyao Liu",
      "Diping Song"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kwon_MemDistill_Distilling_LiDAR_Knowledge_into_Memory_for_Camera-Only_3D_Object_ICCV_2025_paper.html": {
    "title": "MemDistill: Distilling LiDAR Knowledge into Memory for Camera-Only 3D Object Detection",
    "volume": "main",
    "abstract": "Camera-based 3D object detection has gained attention for its cost-effectiveness, but it in general lags behind LiDAR-based approaches due to its lack of explicit 3D spatial cues. To take the best of both camera- and LiDAR-based detectors, we propose MemDistill, a novel cross-modal knowledge distillation framework for 3D object detection.MemDistill transfers rich 3D knowledge from a LiDAR-based teacher model to a camera-based student model through a dedicated memory unit and a scene-dependent memory retrieval module.To be specific, our framework distills the teacher's 3D knowledge, optimizes the memory to store that knowledge compactly, and learns the retriever that searches the memory to produce 3D features relevant to the input scene, compensating for the missing LiDAR modality.Experiments on the nuScenes dataset demonstrate that MemDistill significantly improves performance of its camera-only baseline, achieving the state of the art in camera-based 3D object detection",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Donghyeon Kwon",
      "Youngseok Yoon",
      "Hyeongseok Son",
      "Suha Kwak"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_OpenRSD_Towards_Open-prompts_for_Object_Detection_in_Remote_Sensing_Images_ICCV_2025_paper.html": {
    "title": "OpenRSD: Towards Open-prompts for Object Detection in Remote Sensing Images",
    "volume": "main",
    "abstract": "Remote sensing object detection has made significant progress, but most studies still focus on closed-set detection, limiting generalization across diverse datasets. Open-vocabulary object detection (OVD) provides a solution by leveraging multimodal associations between text prompts and visual features. However, existing OVD methods for remote sensing (RS) images are constrained by small-scale datasets and fail to address the unique challenges of remote sensing interpretation, include oriented object detection and the need for both high precision and real-time performance in diverse scenarios. To tackle these challenges, we propose OpenRSD, a universal open-prompt RS object detection framework. OpenRSD supports multimodal prompts and integrates multi-task detection heads to balance accuracy and real-time requirements. Additionally, we design a multi-stage training pipeline to enhance the generalization of model. Evaluated on seven public datasets, OpenRSD demonstrates superior performance in oriented and horizontal bounding box detection, with real-time inference capabilities suitable for large-scale RS image analysis. Compared to YOLO-World, OpenRSD exhibits an 8.7% higher average precision and achieves an inference speed of 20.8 FPS. Codes and models will be released",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyue Huang",
      "Yongchao Feng",
      "Ziqi Liu",
      "Shuai Yang",
      "Qingjie Liu",
      "Yunhong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Carr_Privacy-centric_Deep_Motion_Retargeting_for_Anonymization_of_Skeleton-Based_Motion_Visualization_ICCV_2025_paper.html": {
    "title": "Privacy-centric Deep Motion Retargeting for Anonymization of Skeleton-Based Motion Visualization",
    "volume": "main",
    "abstract": "Capturing and visualizing motion using skeleton-based techniques is a key aspect of computer vision, particularly in virtual reality (VR) settings. Its popularity has surged, driven by the simplicity of obtaining skeleton data and the growing appetite for virtual interaction. Although this skeleton data appears to be non-identifiable, it can be exploited to derive personally identifiable information (PII), posing a risk of inadvertent privacy breaches. In this paper, we explore the application of motion retargeting and its ability to mitigate privacy leakages. Motion retargeting can effectively transfer the motion from an initial user onto a dummy skeleton with the purpose of hiding PII. We propose a Privacy-centric Deep Motion Retargeting model (PMR), which mitigates the PII through adversarial learning. In our evaluation, our proposed model achieves motion retargeting performance on par with the current state-of-the-art models. More importantly, it effectively prevents the attackers from identifying the initial user",
    "checked": false,
    "id": "34debdf56f915e905afaac828ebeb76be8084449",
    "semantic_title": "linkage attack on skeleton-based motion visualization",
    "citation_count": 6,
    "authors": [
      "Thomas Carr",
      "Depeng Xu",
      "Shuhan Yuan",
      "Aidong Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chou_FlashDepth_Real-time_Streaming_Video_Depth_Estimation_at_2K_Resolution_ICCV_2025_paper.html": {
    "title": "FlashDepth: Real-time Streaming Video Depth Estimation at 2K Resolution",
    "volume": "main",
    "abstract": "A versatile video depth estimation model should be consistent and accurate across frames, produce high-resolution depth maps, and support real-time streaming. We propose a method, FlashDepth, that satisfies all three requirements, performing depth estimation for a 2044x1148 streaming video at 24 FPS. We show that, with careful modifications to pretrained single-image depth models, these capabilities are enabled with relatively little data and training. We validate our approach across multiple unseen datasets against state-of-the-art depth models, and find that our method outperforms them in terms of boundary sharpness and speed by a significant margin, while maintaining competitive accuracy. We hope our model will enable various applications that require high-resolution depth, such as visual effects editing, and online decision-making, such as robotics. We release all code and model weights at https://github.com/Eyeline-Research/FlashDepth",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gene Chou",
      "Wenqi Xian",
      "Guandao Yang",
      "Mohamed Abdelfattah",
      "Bharath Hariharan",
      "Noah Snavely",
      "Ning Yu",
      "Paul Debevec"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Towards_Performance_Consistency_in_Multi-Level_Model_Collaboration_ICCV_2025_paper.html": {
    "title": "Towards Performance Consistency in Multi-Level Model Collaboration",
    "volume": "main",
    "abstract": "Parameter-level model merging is an emerging paradigm in multi-task learning with significant promise. Previous research has explored its connections with prediction-level model ensembling--commonly viewed as the upper bound for merging--to reveal the potential of achieving performance consistency between the two. However, this observation relies on certain preconditions, such as being limited to two models, using ViT-based models, and all models are fine-tuned from the same pre-trained checkpoint. To further understand the intrinsic connections between these two paradigms, this paper explores an interesting possibility: If these restrictions are removed, can performance consistency still be achieved between merging and ensembling? To answer this question, we first theoretically establish a performance correlation between merging and ensembling. We find that even when previous restrictions are not met, there is still a way for model merging to attain a near-identical and superior performance similar to that of ensembling. To verify whether our findings are practical, we introduce a validation framework termed \\underline Neu ral \\underline Lig and (NeuLig). The learning process of NeuLig is meticulously designed with a specialized loss function supported by theoretical foundations. Experimental results demonstrate the robust resilience of NeuLig in terms of both model scale and the number of collaborating models. For instance, for the case involving 5 CLIP-ViT-B/32 models, parameter-level merging achieves the same performance as prediction-level ensembling (merging: 95.44% vs. ensembling: 95.46%)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Li",
      "Runpeng Yu",
      "Xinchao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_Polarimetric_Neural_Field_via_Unified_Complex-Valued_Wave_Representation_ICCV_2025_paper.html": {
    "title": "Polarimetric Neural Field via Unified Complex-Valued Wave Representation",
    "volume": "main",
    "abstract": "Polarization has found applications in various computer vision tasks by providing additional physical cues. However, due to the limitations of current imaging systems, polarimetric parameters are typically stored in discrete form, which is non-differentiable and limits their applicability in polarization-based vision. While current neural field methods have shown promise for continuous signal reconstruction, they struggle to model the intrinsic physical interdependencies among polarimetric parameters. In this work, we propose a physics-grounded representation scheme to represent polarimetric parameters as a unified complex-valued wave. Tailored to this scheme, we propose a tuning-free fitting strategy along with a lightweight complex-valued neural network, enabling property-preserved reconstruction. Experimental results show that our method achieves state-of-the-art performance and facilitates smooth polarized image rendering and flexible resolution adjustments",
    "checked": false,
    "id": "b1b0814c53b2e13b5b211e4eb7cfbf632395dadb",
    "semantic_title": "nerf-or: neural radiance fields for operating room scene reconstruction from sparse-view rgb-d videos",
    "citation_count": 3,
    "authors": [
      "Chu Zhou",
      "Yixin Yang",
      "Junda Liao",
      "Heng Guo",
      "Boxin Shi",
      "Imari Sato"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ge_CLIP-Adapted_Region-to-Text_Learning_for_Generative_Open-Vocabulary_Semantic_Segmentation_ICCV_2025_paper.html": {
    "title": "CLIP-Adapted Region-to-Text Learning for Generative Open-Vocabulary Semantic Segmentation",
    "volume": "main",
    "abstract": "In recent years, Open-Vocabulary Semantic Segmentation (OVSS) has been largely advanced. However, existing methods mostly rely on a pre-trained vision-language model (e.g., CLIP) and require a predefined set of classes to guide the semantic segmentation process during the inference. This not only narrows the application scenario but also constrains comprehension within a finite vocabulary. To overcome this, we reformulate OVSS as a text generation task and propose the CLIP-adapted Region-to-Text Network (CRTNet) that achieves vocabulary-free OVSS by generating category names and descriptions upon segmentation masks. The training process consists of two steps to ensure an accurate and detailed interpretation of the masked regions: (i) the initial step adapts CLIP visual features to mask-level proposal features using binarized masks extracted by a trained mask extractor, and (ii) the subsequent step involves selecting and aggregating these features to become text-aware by integrating CLIP text embeddings, effectively aligning visual data with corresponding linguistic data to facilitate region-to-text learning. Furthermore, we introduce a series of parsing and filtering techniques to integrate multiple sources of training data to improve the generalization ability of our model. Experiments demonstrate that our model not only excels in OVSS but also exhibits scalability and can be adapted to various foundation models (e.g., SAM) without being retrained",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiannan Ge",
      "Lingxi Xie",
      "Hongtao Xie",
      "Pandeng Li",
      "Sun-Ao Liu",
      "Xiaopeng Zhang",
      "Qi Tian",
      "Yongdong Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_Where_What_Why_Towards_Explainable_Driver_Attention_Prediction_ICCV_2025_paper.html": {
    "title": "Where, What, Why: Towards Explainable Driver Attention Prediction",
    "volume": "main",
    "abstract": "Modeling task-driven attention in driving is a fundamental challenge for both autonomous vehicles and cognitive science. Existing methods primarily predict where drivers look by generating spatial heatmaps, but fail to capture the cognitive motivations behind attention allocation in specific contexts, which limits deeper understanding of attention mechanisms. To bridge this gap, we introduce Explainable Driver Attention Prediction, a novel task paradigm that jointly predicts spatial attention regions (where), parses attended semantics (what), and provides cognitive reasoning for attention allocation (why). To support this, we present W3DA, the first large-scale explainable driver attention dataset. It enriches existing benchmarks with detailed semantic and causal annotations across diverse driving scenarios, including normal conditions, safety-critical situations, and traffic accidents. We further propose LLada, a Large Language model-driven framework for driver attention prediction, which unifies pixel modeling, semantic parsing, and cognitive reasoning within an end-to-end architecture. Extensive experiments demonstrate the effectiveness of LLada, exhibiting robust generalization across datasets and driving conditions. This work serves as a key step toward a deeper understanding of driver attention mechanisms, with significant implications for autonomous driving, intelligent driver training, and human-computer interaction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchen Zhou",
      "Jiayu Tang",
      "Xiaoyan Xiao",
      "Yueyao Lin",
      "Linkai Liu",
      "Zipeng Guo",
      "Hao Fei",
      "Xiaobo Xia",
      "Chao Gou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_MeshAnything_V2_Artist-Created_Mesh_Generation_with_Adjacent_Mesh_Tokenization_ICCV_2025_paper.html": {
    "title": "MeshAnything V2: Artist-Created Mesh Generation with Adjacent Mesh Tokenization",
    "volume": "main",
    "abstract": "Meshes are the de facto 3D representation in the industry but are labor-intensive to produce. Recently, a line of research has focused on autoregressively generating meshes. This approach processes meshes into a sequence composed of vertices and then generates them vertex by vertex, similar to how a language model generates text. These methods have achieved some success but still struggle to generate complex meshes. One primary reason for this limitation is their inefficient tokenization methods. To address this issue, we introduce MeshAnything V2, an advanced mesh generation model designed to create Artist-Created Meshes that align precisely with specified shapes. A key innovation behind MeshAnything V2 is our novel Adjacent Mesh Tokenization (AMT) method. Unlike traditional approaches that represent each face using three vertices, AMT optimizes this by employing a single vertex wherever feasible, effectively reducing the token sequence length by about half on average. This not only streamlines the tokenization process but also results in more compact and well-structured sequences, enhancing the efficiency of mesh generation. With these improvements, MeshAnything V2 effectively doubles the face limit compared to previous models, delivering superior performance without increasing computational costs. Our extensive experiments across various mesh tokenization methods demonstrate that AMT is pivotal in achieving optimal results in both efficiency and performance",
    "checked": true,
    "id": "946c1deee03873d807847dbcb4ab449b0bcffc35",
    "semantic_title": "meshanything v2: artist-created mesh generation with adjacent mesh tokenization",
    "citation_count": 52,
    "authors": [
      "Yiwen Chen",
      "Yikai Wang",
      "Yihao Luo",
      "Zhengyi Wang",
      "Zilong Chen",
      "Jun Zhu",
      "Chi Zhang",
      "Guosheng Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sun_Visual_Intention_Grounding_for_Egocentric_Assistants_ICCV_2025_paper.html": {
    "title": "Visual Intention Grounding for Egocentric Assistants",
    "volume": "main",
    "abstract": "Visual grounding associates textual descriptions with objects in an image. Conventional methods target third-person image inputs and named object queries. In applications such as AI assistants, the perspective shifts -- inputs are egocentric, and objects may be referred to implicitly through needs and intentions. To bridge this gap, we introduce EgoIntention, the first dataset for egocentric visual intention grounding. EgoIntention challenges multimodal LLMs to 1) understand and ignore unintended contextual objects and 2) reason about uncommon object functionalities. Benchmark results show that current models misidentify context objects and lack affordance understanding in egocentric views. We also propose Reason-to-Ground (RoG) instruction tuning; it enables hybrid training with normal descriptions and egocentric intentions with a chained intention reasoning and object grounding mechanism. RoG significantly outperforms naive finetuning and hybrid training on EgoIntention, while maintaining or slightly improving naive description grounding. This advancement enables unified visual grounding for egocentric and exocentric visual inputs while handling explicit object queries and implicit human intentions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengzhan Sun",
      "Junbin Xiao",
      "Tze Ho Elden Tse",
      "Yicong Li",
      "Arjun Akula",
      "Angela Yao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Khayatan_Analyzing_Finetuning_Representation_Shift_for_Multimodal_LLMs_Steering_ICCV_2025_paper.html": {
    "title": "Analyzing Finetuning Representation Shift for Multimodal LLMs Steering",
    "volume": "main",
    "abstract": "Multimodal LLMs (MLLMs) have reached remarkable levels of proficiency in understanding multimodal inputs. However, understanding and interpreting the behavior of such complex models is a challenging task, not to mention the dynamic shifts that may occur during fine-tuning, or due to covariate shift between datasets. In this work, we apply concept-level analysis towards MLLM understanding. More specifically, we propose to map hidden states to interpretable visual and textual concepts. This enables us to more efficiently compare certain semantic dynamics, such as the shift from an original and fine-tuned model, revealing concept alteration and potential biases that may occur during fine-tuning. We also demonstrate the use of shift vectors to capture these concepts changes. These shift vectors allow us to recover fine-tuned concepts by applying simple, computationally inexpensive additive concept shifts in the original model. Finally, our findings also have direct applications for MLLM steering, which can be used for model debiasing as well as enforcing safety in MLLM output. All in all, we propose a novel, training-free, ready-to-use framework for MLLM behavior interpretability and control. Our implementation is publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pegah Khayatan",
      "Mustafa Shukor",
      "Jayneel Parekh",
      "Arnaud Dapogny",
      "Matthieu Cord"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lei_TeethGenerator_A_two-stage_framework_for_paired_pre-_and_post-orthodontic_3D_ICCV_2025_paper.html": {
    "title": "TeethGenerator: A two-stage framework for paired pre- and post-orthodontic 3D dental data generation",
    "volume": "main",
    "abstract": "Digital orthodontics represents a prominent and critical application of computer vision technology in the medical field. So far, the labor-intensive process of collecting clinical data, particularly in acquiring paired 3D orthodontic teeth models, constitutes a crucial bottleneck for developing tooth arrangement neural networks. Although numerous general 3D shape generation methods have been proposed, most of them focus on single-object generation and are insufficient for generating anatomically structured teeth models, each comprising 24-32 segmented teeth. In this paper, we propose TeethGenerator, a novel two-stage framework designed to synthesize paired 3D teeth models pre- and post-orthodontic, aiming to facilitate the training of downstream tooth arrangement networks. Specifically, our approach consists of two key modules: (1) a teeth shape generation module that leverages a diffusion model to learn the distribution of morphological characteristics of teeth, enabling the generation of diverse post-orthodontic teeth models; and (2) a teeth style generation module that synthesizes corresponding pre-orthodontic teeth models by incorporating desired styles as conditional inputs. Extensive qualitative and quantitative experiments demonstrate that our synthetic dataset aligns closely with the distribution of real orthodontic data, and promotes tooth alignment performance significantly when combined with real data for training. The code and dataset are available at https://github.com/lcshhh/teeth generator",
    "checked": true,
    "id": "1547838746dbc2ca38245e7b0829693e621fc555",
    "semantic_title": "teethgenerator: a two-stage framework for paired pre- and post-orthodontic 3d dental data generation",
    "citation_count": 1,
    "authors": [
      "Changsong Lei",
      "Yaqian Liang",
      "Shaofeng Wang",
      "Jiajia Dai",
      "Yong-Jin Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Enhancing_Zero-shot_Object_Counting_via_Text-guided_Local_Ranking_and_Number-evoked_ICCV_2025_paper.html": {
    "title": "Enhancing Zero-shot Object Counting via Text-guided Local Ranking and Number-evoked Global Attention",
    "volume": "main",
    "abstract": "Text-guided zero-shot object counting leverages vision-language models (VLMs) to count objects of an arbitrary class given by a text prompt. Existing approaches for this challenging task only utilize local patch-level features to fuse with text feature, ignoring the important influence of the global image-level feature. In this paper, we propose a universal strategy that can exploit both local patch-level features and global image-level feature simultaneously. Specifically, to improve the localization ability of VLMs, we propose Text-guided Local Ranking. Depending on the prior knowledge that foreground patches have higher similarity with the text prompt, a new local-text rank loss is designed to increase the differences between the similarity scores of foreground and background patches which push foreground and background patches apart. To enhance the counting ability of VLMs, Number-evoked Global Attention is introduced to first align global image-level feature with multiple number-conditioned text prompts. Then, the one with the highest similarity is selected to compute cross-attention with the global image-level feature. Through extensive experiments on widely used datasets and methods, the proposed approach has demonstrated superior advancements in performance, generalization, and scalability. Furthermore, to better evaluate text-guided zero-shot object counting methods, we propose a dataset named ZSC-8K, which is larger and more challenging, to establish a new benchmark. Codes and ZSC-8K dataset will be available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiwei Zhang",
      "Qi Zhou",
      "Wei Ke"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ko_Bidirectional_Likelihood_Estimation_with_Multi-Modal_Large_Language_Models_for_Text-Video_ICCV_2025_paper.html": {
    "title": "Bidirectional Likelihood Estimation with Multi-Modal Large Language Models for Text-Video Retrieval",
    "volume": "main",
    "abstract": "Text-Video Retrieval aims to find the most relevant text (or video) candidate given a video (or text) query from large-scale online databases. Recent work leverages multi-modal large language models (MLLMs) to improve retrieval, especially for long or complex query-candidate pairs. However, we observe that the naive application of MLLMs, i.e., retrieval based on candidate likelihood, introduces candidate prior bias, favoring candidates with inherently higher priors over those more relevant to the query. To this end, we propose a novel retrieval framework, Bidirectional Likelihood Estimation with MLLM (BLiM), which leverages both query and candidate likelihoods by training the model to generate text from a given video as well as video features from a given text. Furthermore, we introduce Candidate Prior Normalization (CPN), a simple yet effective training-free score calibration module designed to mitigate candidate prior bias in candidate likelihood. On four Text-Video Retrieval benchmarks, our BLiM equipped with CPN outperforms previous state-of-the-art models by 6.4 R@1 on average, effectively alleviating candidate prior bias and emphasizing query-candidate relevance. Our in-depth analysis across various multi-modal tasks beyond retrieval highlights the broad applicability of CPN which enhances visual understanding by reducing reliance on textual priors. Code is available at \\href https://github.com/mlvlab/BLiM https://github.com/mlvlab/BLiM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dohwan Ko",
      "Ji Soo Lee",
      "Minhyuk Choi",
      "Zihang Meng",
      "Hyunwoo J. Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ma_TinyViM_Frequency_Decoupling_for_Tiny_Hybrid_Vision_Mamba_ICCV_2025_paper.html": {
    "title": "TinyViM: Frequency Decoupling for Tiny Hybrid Vision Mamba",
    "volume": "main",
    "abstract": "Mamba has shown great potential for computer vision due to its linear complexity in modeling the global context with respect to the input length. However, existing lightweight Mamba-based backbones cannot demonstrate performance that matches Convolution or Transformer-based methods. By observing, we find that simply modifying the scanning path in the image domain is not conducive to fully exploiting the potential of vision Mamba. In this paper, we first perform comprehensive spectral and quantitative analyses, and verify that the Mamba block mainly models low-frequency information under Convolution-Mamba hybrid architecture. Based on the analyses, we introduce a novel Laplace mixer to decouple the features in terms of frequency and input only the low-frequency components into the Mamba block. In addition, considering the redundancy of the features and the different requirements for high-frequency details and low-frequency global information at different stages, we introduce a frequency ramp inception, i.e., gradually reduce the input dimensions of the high-frequency branches, so as to efficiently trade-off the high-frequency and low-frequency components at different layers. By integrating mobile-friendly convolution and efficient Laplace mixer, we build a series of tiny hybrid vision Mamba called TinyViM. The proposed TinyViM achieves impressive performance on several downstream tasks including image classification, semantic segmentation, object detection and instance segmentation. In particular, TinyViM outperforms Convolution, Transformer and Mamba-based models with similar scales, and the throughput is about 2-3 times higher than that of other Mamba-based models. The good balance between the efficiency and performance of TinyViM shows that a properly designed and optimized vision mamba can achieve high performance with a small model size. Code is available at https://github.com/xwmaxwma/TinyViM",
    "checked": true,
    "id": "cbd49d121760b0932d9e8e88822bc4c3ef7eb4b6",
    "semantic_title": "tinyvim: frequency decoupling for tiny hybrid vision mamba",
    "citation_count": 9,
    "authors": [
      "Xiaowen Ma",
      "Zhenliang Ni",
      "Xinghao Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Harnessing_Massive_Satellite_Imagery_with_Efficient_Masked_Image_Modeling_ICCV_2025_paper.html": {
    "title": "Harnessing Massive Satellite Imagery with Efficient Masked Image Modeling",
    "volume": "main",
    "abstract": "Masked Image Modeling (MIM) has become an essential method for building foundational visual models in remote sensing (RS). However, the limitations in size and diversity of existing RS datasets restrict the ability of MIM methods to learn generalizable representations. Additionally, conventional MIM techniques, which require reconstructing all tokens, introduce unnecessary computational overhead. To address these issues, we present a new pre-training pipeline for RS models, featuring the creation of a large-scale RS dataset and an efficient MIM approach. We curated a high-quality dataset named **OpticalRS-13M** by collecting publicly available RS datasets and processing them through exclusion, slicing, and deduplication. OpticalRS-13M comprises 13 million optical images covering various RS tasks, such as object detection and pixel segmentation. To enhance efficiency, we propose SelectiveMAE, a pre-training method that dynamically encodes and reconstructs semantically rich patch tokens, thereby reducing the inefficiencies of traditional MIM models caused by redundant background pixels in RS images. Extensive experiments show that OpticalRS-13M significantly improves classification, detection, and segmentation performance, while SelectiveMAE increases training efficiency over 2xtimes. This highlights the effectiveness and scalability of our pipeline in developing RS foundational models",
    "checked": true,
    "id": "4f6ed189ee0b06d5196cdd3ea31629c5829dee0d",
    "semantic_title": "harnessing massive satellite imagery with efficient masked image modeling",
    "citation_count": 7,
    "authors": [
      "Fengxiang Wang",
      "Hongzhen Wang",
      "Di Wang",
      "Zonghao Guo",
      "Zhenyu Zhong",
      "Long Lan",
      "Wenjing Yang",
      "Jing Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zeng_Visual-Oriented_Fine-Grained_Knowledge_Editing_for_MultiModal_Large_Language_Models_ICCV_2025_paper.html": {
    "title": "Visual-Oriented Fine-Grained Knowledge Editing for MultiModal Large Language Models",
    "volume": "main",
    "abstract": "Existing knowledge editing works for MultiModal Large Language Models primarily focus on text-oriented, coarse-grained scenarios, where modifying textual content alone is sufficient. As a result, they fail to capture the unique challenges of multimodal editing, particularly when visual information is central to knowledge representation. In this paper, we introduce a visual-oriented, fine-grained multimodal knowledge editing task that targets precise modifications in images containing multiple interacting entities. To support this, we propose the Fine-Grained Visual Knowledge Editing (FGVEdit) benchmark, designed to evaluate the accuracy and effectiveness of multimodal editing at a granular level. To address this challenge, we present the Multimodal Scope Classifier-based Knowledge Editor (MSCKE), a new framework that leverages a multimodal scope classifier to integrate both textual and visual information. By accurately identifying and updating knowledge localized within images, MSCKE ensures precise editing while preserving unrelated content. Extensive experiments on the FGVEdit benchmark highlight the complexity of this new task and demonstrate that existing methods struggle with fine-grained multimodal editing. Our results highlight MSCKE as a scalable and promising framework for advancing multimodal knowledge editing. Code is available at https://github.com/zeng-zhen/FGVEdit",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Zeng",
      "Leijiang Gu",
      "Xun Yang",
      "Zhangling Duan",
      "Zenglin Shi",
      "Meng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_Bridging_Domain_Generalization_to_Multimodal_Domain_Generalization_via_Unified_Representations_ICCV_2025_paper.html": {
    "title": "Bridging Domain Generalization to Multimodal Domain Generalization via Unified Representations",
    "volume": "main",
    "abstract": "Domain Generalization (DG) aims to enhance model robustness in unseen or distributionally shifted target domains through training exclusively on source domains. Although existing DG techniques, such as data manipulation, learning strategies, and representation learning, have shown significant progress, they predominantly address single-modal data. With the emergence of numerous multi-modal datasets and increasing demand for multi-modal tasks, a key challenge in Multi-modal Domain Generalization (MMDG) has emerged: enabling models trained on multi-modal sources to generalize to unseen target distributions within the same modality set. Due to the inherent differences between modalities, directly transferring methods from single-modal DG to MMDG typically yields sub-optimal results. These methods often exhibit randomness during generalization due to the invisibility of target domains and fail to consider inter-modal consistency. Applying these methods independently to each modality in the MMDG setting before combining them can lead to divergent generalization directions across different modalities, resulting in degraded generalization capabilities. To address these challenges, we propose a novel approach that leverages Unified Representations to map different paired modalities together, effectively adapting DG methods to MMDG by enabling synchronized multi-modal improvements within the unified space. Additionally, we introduce a supervised disentanglement framework that separates modal-general and modal-specific information, further enhancing the alignment of unified representations. Extensive experiments on benchmark datasets, including EPIC-Kitchens and Human-Animal-Cartoon, demonstrate the effectiveness and superiority of our method in enhancing multi-modal domain generalization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hai Huang",
      "Yan Xia",
      "Sashuai Zhou",
      "Hanting Wang",
      "Shulei Wang",
      "Zhou Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Schwarz_A_Recipe_for_Generating_3D_Worlds_from_a_Single_Image_ICCV_2025_paper.html": {
    "title": "A Recipe for Generating 3D Worlds from a Single Image",
    "volume": "main",
    "abstract": "We introduce a recipe for generating immersive 3D worlds from a single image by framing the task as an in-context learning problem for 2D inpainting models. This approach requires minimal training and uses existing generative models. Our process involves two steps: generating coherent panoramas using a pre-trained diffusion model and lifting these into 3D with a metric depth estimator. We then fill unobserved regions by conditioning the inpainting model on rendered point clouds, requiring minimal fine-tuning. Tested on both synthetic and real images, our method produces high-quality 3D environments suitable for VR display. By explicitly modeling the 3D structure of the generated environment from the start, our approach consistently outperforms state-of-the-art, video synthesis-based methods along multiple quantitative image quality metrics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Katja Schwarz",
      "Denis Rozumny",
      "Samuel Rota Bulò",
      "Lorenzo Porzi",
      "Peter Kontschieder"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xie_MVGBench_a_Comprehensive_Benchmark_for_Multi-view_Generation_Models_ICCV_2025_paper.html": {
    "title": "MVGBench: a Comprehensive Benchmark for Multi-view Generation Models",
    "volume": "main",
    "abstract": "We propose MVGBench, a comprehensive benchmark for multi-view image generation models (MVGs) that evaluates 3D consistency in geometry and texture, image quality, and semantics (using vision language models). Recently, MVGs have been the main driving force in 3D object creation. However, existing metrics compare generated images against ground truth target views, which is not suitable for generative tasks where multiple solutions exist while differing from ground truth. Furthermore, different MVGs are trained on different view angles, synthetic data and specific lightings robustness to these factors and generalization to real data are rarely evaluated thoroughly. Without a rigorous evaluation protocol, it is also unclear what design choices contribute to the progress of MVGs. MVGBench evaluates three different aspects: best setup performance, generalization to real data and robustness. Instead of comparing against ground truth, we introduce a novel 3D self-consistency metric which compares 3D reconstructions from disjoint generated multi-views. We systematically compare 12 existing MVGs on 4 different curated real and synthetic datasets. With our analysis, we identify important limitations of existing methods specially in terms of robustness and generalization, and we find the most critical design choices. Using the discovered best practices, we propose ViFiGen, a method that outperforms all evaluated MVGs on 3D consistency. Our benchmark suite and pretrained models are released",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xianghui Xie",
      "Jan Eric Lessen",
      "Gerard Pons-Moll"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Du_Fewer_Denoising_Steps_or_Cheaper_Per-Step_Inference_Towards_Compute-Optimal_Diffusion_ICCV_2025_paper.html": {
    "title": "Fewer Denoising Steps or Cheaper Per-Step Inference: Towards Compute-Optimal Diffusion Model Deployment",
    "volume": "main",
    "abstract": "Diffusion models have shown remarkable success across generative tasks, yet their high computational demands challenge deployment on resource-limited platforms. This paper investigates a critical question for compute-optimal diffusion model deployment: Under a post-training setting without fine-tuning, is it more effective to reduce the number of denoising steps or to use a cheaper per-step inference? Intuitively, reducing the number of denoising steps increases the variability of the distributions across steps, making the model more sensitive to compression. In contrast, keeping more denoising steps makes the differences smaller, preserving redundancy, and making post-training compression more feasible. To systematically examine this, we propose PostDiff, a training-free framework for accelerating pre-trained diffusion models by reducing redundancy at both the input level and module level in a post-training manner. At the input level, we propose a mixed-resolution denoising scheme based on the insight that reducing generation resolution in early denoising steps can enhance low-frequency components and improve final generation fidelity. At the module level, we employ a hybrid module caching strategy to reuse computations across denoising steps. Extensive experiments and ablation studies demonstrate that (1) PostDiff can significantly improve the fidelity-efficiency trade-off of state-of-the-art diffusion models, and (2) to boost efficiency while maintaining decent generation fidelity, reducing per-step inference cost is often more effective than reducing the number of denoising steps. Our code is available at https://github.com/GATECH-EIC/PostDiff",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenbang Du",
      "Yonggan Fu",
      "Lifu Wang",
      "Jiayi Qian",
      "Xiao Luo",
      "Yingyan Celine Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shi_Competitive_Distillation_A_Simple_Learning_Strategy_for_Improving_Visual_Classification_ICCV_2025_paper.html": {
    "title": "Competitive Distillation: A Simple Learning Strategy for Improving Visual Classification",
    "volume": "main",
    "abstract": "Deep Neural Networks (DNNs) have significantly advanced the field of computer vision. To improve DNN training process, knowledge distillation methods demonstrate their effectiveness in accelerating network training by introducing a fixed learning direction from the teacher network to student networks. In this context, several distillation-based optimization strategies are proposed, e.g., deep mutual learning and self-distillation, as an attempt to achieve generic training performance enhancement through the cooperative training of multiple networks. However, such strategies achieve limited improvements due to the poor understanding of the impact of learning directions among networks across different iterations. In this paper, we propose a novel competitive distillation strategy that allows each network in a group to potentially act as a teacher based on its performance, enhancing the overall learning performance. Competitive distillation organizes a group of networks to perform a shared task and engage in competition, where competitive optimization is proposed to improve the parameter updating process. We further introduce stochastic perturbation in competitive distillation, aiming to motivate networks to induce mutations to achieve better visual representations and global optimum. The experimental results show that competitive distillation achieves promising performance in diverse tasks and datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daqian Shi",
      "Xiaolei Diao",
      "Xu Chen",
      "Cédric M John"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Do_DynFaceRestore_Balancing_Fidelity_and_Quality_in_Diffusion-Guided_Blind_Face_Restoration_ICCV_2025_paper.html": {
    "title": "DynFaceRestore: Balancing Fidelity and Quality in Diffusion-Guided Blind Face Restoration with Dynamic Blur-Level Mapping and Guidance",
    "volume": "main",
    "abstract": "Blind Face Restoration aims to recover high-fidelity, detail-rich facial images from unknown degraded inputs, presenting significant challenges in preserving both identity and detail. Pre-trained diffusion models have been increasingly used as image priors to generate fine details. Still, existing methods often use fixed diffusion sampling timesteps and a global guidance scale, assuming uniform degradation. This limitation and potentially imperfect degradation kernel estimation frequently lead to under- or over-diffusion, resulting in an imbalance between fidelity and quality. We propose DynFaceRestore, a novel blind face restoration approach that learns to map any blindly degraded input to Gaussian blurry images. By leveraging these blurry images and their respective Gaussian kernels, we dynamically select the starting timesteps for each blurry image and apply closed-form guidance during the diffusion sampling process to maintain fidelity. Additionally, we introduce a dynamic guidance scaling adjuster that modulates the guidance strength across local regions, enhancing detail generation in complex areas while preserving structural fidelity in contours. This strategy effectively balances the trade-off between fidelity and quality. DynFaceRestore achieves state-of-the-art performance in both quantitative and qualitative evaluations, demonstrating robustness and effectiveness in blind face restoration",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huu-Phu Do",
      "Yu-Wei Chen",
      "Yi-Cheng Liao",
      "Chi-Wei Hsiao",
      "Han-Yang Wang",
      "Wei-Chen Chiu",
      "Ching-Chun Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sonmezer_LoRAverse_A_Submodular_Framework_to_Retrieve_Diverse_Adapters_for_Diffusion_ICCV_2025_paper.html": {
    "title": "LoRAverse: A Submodular Framework to Retrieve Diverse Adapters for Diffusion Models",
    "volume": "main",
    "abstract": "Low-rank Adaptation (LoRA) models have revolutionized the personalization of pre-trained diffusion models by enabling fine-tuning through low-rank, factorized weight matrices specifically optimized for attention layers. These models facilitate the generation of highly customized content across a variety of objects, individuals, and artistic styles without the need for extensive retraining. Despite the availability of over 100K LoRA adapters on platforms like Civit.ai, users often face challenges in navigating, selecting, and effectively utilizing the most suitable adapters due to their sheer volume, diversity, and lack of structured organization. This paper addresses the problem of selecting the most relevant and diverse LoRA models from this vast database by framing the task as a combinatorial optimization problem and proposing a novel submodular framework. Our quantitative and qualitative experiments demonstrate that our method generates diverse outputs across a wide range of domains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mert Sonmezer",
      "Matthew Zheng",
      "Pinar Yanardag"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Harley_AllTracker_Efficient_Dense_Point_Tracking_at_High_Resolution_ICCV_2025_paper.html": {
    "title": "AllTracker: Efficient Dense Point Tracking at High Resolution",
    "volume": "main",
    "abstract": "We introduce AllTracker: a model that estimates long-range point tracks by way of estimating the flow field between a query frame and every other frame of a video. Unlike existing point tracking methods, our approach delivers high-resolution and dense (all-pixel) correspondence fields, which can be visualized as flow maps. Unlike existing optical flow methods, our approach corresponds one frame to hundreds of subsequent frames, rather than just the next frame. We develop a new architecture for this task, blending techniques from existing work in optical flow and point tracking: the model performs iterative inference on low-resolution grids of correspondence estimates, propagating information spatially via 2D convolution layers, and propagating information temporally via pixel-aligned attention layers. The model is fast and parameter-efficient (16 million parameters), and delivers state-of-the-art point tracking accuracy at high resolution (i.e., tracking 768x1024 pixels, on a 40G GPU). A benefit of our design is that we can train jointly on optical flow datasets and point tracking datasets, and we find that doing so is crucial for top performance. We provide an extensive ablation study on our architecture details and training recipe, making it clear which details matter most. Our code and model weights are available: https://alltracker.github.io",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adam W. Harley",
      "Yang You",
      "Xinglong Sun",
      "Yang Zheng",
      "Nikhil Raghuraman",
      "Yunqi Gu",
      "Sheldon Liang",
      "Wen-Hsuan Chu",
      "Achal Dave",
      "Suya You",
      "Rares Ambrus",
      "Katerina Fragkiadaki",
      "Leonidas Guibas"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Feng_Gaussian-based_World_Model_Gaussian_Priors_for_Voxel-Based_Occupancy_Prediction_and_ICCV_2025_paper.html": {
    "title": "Gaussian-based World Model: Gaussian Priors for Voxel-Based Occupancy Prediction and Future Motion Prediction",
    "volume": "main",
    "abstract": "In autonomous driving, accurately predicting occupancy and motion is crucial for safe navigation within dynamic environments. However, existing methods often suffer from difficulties in handling complex scenes and uncertainty arising from sensor data. To address these issues, we propose a new Gaussian-based World Model (GWM), seamlessly integrating raw multi-modal sensor inputs. In 1st stage, Gaussian representation learner utilizes self-supervised pretraining to learn robust Gaussian representation. Gaussian representation integrates semantic and geometric information and establishes a robust probabilistic understanding of the environment. In 2nd stage, GWM seamlessly integrates learning, simulation, and planning into a unified framework, empowering the uncertainty-aware simulator & planner to jointly forecast future scene evolutions and vehicle trajectories. Simulator generates future scene predictions by modeling both static and dynamic elements, while planner calculates optimal paths to minimize collision risks, thus enhancing navigation safety. Overall, GWM employs a sensor-to-planning world model that directly processes raw sensor data, setting it apart from previous methods. Experiments show that GWM outperforms state-of-the-art approaches by 1.46% in semantic comprehension and 0.07m in motion prediction. Moreover, we provide an in-depth analysis of Gaussian representations under complex scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tuo Feng",
      "Wenguan Wang",
      "Yi Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_CombatVLA_An_Efficient_Vision-Language-Action_Model_for_Combat_Tasks_in_3D_ICCV_2025_paper.html": {
    "title": "CombatVLA: An Efficient Vision-Language-Action Model for Combat Tasks in 3D Action Role-Playing Games",
    "volume": "main",
    "abstract": "Recent advances in Vision-Language-Action models (VLAs) have expanded the capabilities of embodied intelligence. However, significant challenges remain in real-time decision-making in complex 3D environments, which demand second-level responses, high-resolution perception, and tactical reasoning under dynamic conditions. To advance the field, we introduce CombatVLA, an efficient VLA model optimized for combat tasks in 3D action role-playing games(ARPGs). Specifically, our CombatVLA is a 3B model trained on video-action pairs collected by an action tracker, where the data is formatted as action-of-thought (AoT) sequences. Thereafter, CombatVLA seamlessly integrates into an action execution framework, allowing efficient inference through our truncated AoT strategy. Experimental results demonstrate that CombatVLA not only outperforms all existing models on the combat understanding benchmark but also achieves a 50-fold acceleration in game combat. Moreover, it has a higher task success rate than human players. We will release all resources, including the action tracker, dataset, model weights, training code, and action execution framework implementation at https://combatvla.github.io/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peng Chen",
      "Pi Bu",
      "Yingyao Wang",
      "Xinyi Wang",
      "Ziming Wang",
      "Jie Guo",
      "Yingxiu Zhao",
      "Qi Zhu",
      "Jun Song",
      "Siran Yang",
      "Jiamang Wang",
      "Bo Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ying_SketchSplat_3D_Edge_Reconstruction_via_Differentiable_Multi-view_Sketch_Splatting_ICCV_2025_paper.html": {
    "title": "SketchSplat: 3D Edge Reconstruction via Differentiable Multi-view Sketch Splatting",
    "volume": "main",
    "abstract": "Edges are one of the most basic parametric primitives to describe structural information in 3D. In this paper, we study parametric 3D edge reconstruction from calibrated multi-view images. Previous methods usually reconstruct a 3D edge point set from multi-view 2D edge images, and then fit 3D edges to the point set. However, noise in the point set may cause gaps among fitted edges, and the recovered edges may not align with input multi-view images since the edge fitting depends only on the reconstructed 3D point set. To mitigate these problems, we propose SketchSplat, a method to reconstruct accurate, complete, and compact 3D edges via differentiable multi-view sketch splatting. We represent 3D edges as sketches, which are parametric lines and curves defined by attributes including control points, scales, and opacity. During reconstruction, we iteratively sample Gaussian points from a set of sketches and rasterize the Gaussians onto 2D edge images. Then the gradient of the image loss can be back-propagated to optimize the sketch attributes. Our method bridges 2D edge images and 3D edges in a differentiable manner, which ensures that 3D edges align well with 2D images and leads to accurate and complete results. We also propose a series of adaptive topological operations to reduce redundant edges and apply them along with the sketch optimization, yielding a more compact reconstruction. Finally, we contribute an accurate 2D edge detector that improves the performance of both ours and existing methods. Experiments show that our method achieves state-of-the-art accuracy, completeness, and compactness on a benchmark CAD dataset",
    "checked": true,
    "id": "dc89f5252f6dab5200f508a5f0aa02be9c3c4dcc",
    "semantic_title": "sketchsplat: 3d edge reconstruction via differentiable multi-view sketch splatting",
    "citation_count": 0,
    "authors": [
      "Haiyang Ying",
      "Matthias Zwicker"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cheng_Social_Debiasing_for_Fair_Multi-modal_LLMs_ICCV_2025_paper.html": {
    "title": "Social Debiasing for Fair Multi-modal LLMs",
    "volume": "main",
    "abstract": "Multi-modal Large Language Models (MLLMs) have dramatically advanced the research field and delivered powerful vision-language understanding capabilities. However, these models often inherit deep-rooted social biases from their training data, leading to uncomfortable responses with respect to attributes such as race and gender. This paper addresses the issue of social biases in MLLMs by i) introducing a comprehensive counterfactual dataset with multiple social concepts (CMSC), which complements existing datasets by providing 18 diverse and balanced social concepts; and ii) proposing a counter-stereotype debiasing (CSD) strategy that mitigates social biases in MLLMs by leveraging the opposites of prevalent stereotypes. CSD incorporates both a novel bias-aware data sampling method and a loss rescaling method, enabling the model to effectively reduce biases. We conduct extensive experiments with four prevalent MLLM architectures. The results demonstrate the advantage of the CMSC dataset and the edge of CSD strategy in reducing social biases compared to existing competing methods, without compromising the overall performance on general multi-modal reasoning benchmarks",
    "checked": true,
    "id": "d80be05a11da0a725845cd027cf0ce116172014d",
    "semantic_title": "social debiasing for fair multi-modal llms",
    "citation_count": 3,
    "authors": [
      "Harry Cheng",
      "Yangyang Guo",
      "Qingpei Guo",
      "Ming Yang",
      "Tian Gan",
      "Weili Guan",
      "Liqiang Nie"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_DOGR_Towards_Versatile_Visual_Document_Grounding_and_Referring_ICCV_2025_paper.html": {
    "title": "DOGR: Towards Versatile Visual Document Grounding and Referring",
    "volume": "main",
    "abstract": "With recent advances in Multimodal Large Language Models (MLLMs), grounding and referring capabilities have gained increasing attention for achieving detailed understanding and flexible user interaction. However, these capabilities still remain underdeveloped in visual document understanding due to the scarcity of fine-grained datasets and comprehensive benchmarks. To fill this gap, we propose the **DO**cument **G**rounding and **R**eferring data engine (**DOGR-Engine**), which generates two types of high-quality fine-grained document data: (1) multi-granular parsing data to improve text localization and recognition, and (2) instruction-tuning data to activate MLLMs' grounding and referring capabilities in dialogue and reasoning. Using the DOGR-Engine, we construct **DOGR-Bench**, a benchmark covering seven grounding and referring tasks across three document types (chart, poster, and PDF document), offering a comprehensive evaluation of fine-grained document understanding. Leveraging the generated data, we further develop **DOGR**, a strong baseline model that excels in text localization and recognition, while precisely grounds and refers to key textual information during conversation and reasoning, thereby advancing document understanding to a finer granularity and enable flexible interaction paradigms. Our code, data, and model are open-sourced at https://github.com/zyinan99/DOGR",
    "checked": true,
    "id": "f1c14c09c20431663e93ef5f11b3abbb96431ea9",
    "semantic_title": "dogr: towards versatile visual document grounding and referring",
    "citation_count": 0,
    "authors": [
      "Yinan Zhou",
      "Yuxin Chen",
      "Haokun Lin",
      "Yichen Wu",
      "Shuyu Yang",
      "Zhongang Qi",
      "Chen Ma",
      "Li Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Benchmarking_and_Learning_Multi-Dimensional_Quality_Evaluator_for_Text-to-3D_Generation_ICCV_2025_paper.html": {
    "title": "Benchmarking and Learning Multi-Dimensional Quality Evaluator for Text-to-3D Generation",
    "volume": "main",
    "abstract": "Text-to-3D generation has achieved remarkable progress in recent years, yet evaluating these methods remains challenging for two reasons: i) existing benchmarks lack fine-grained evaluation on different prompt categories and evaluation dimensions; ii) previous evaluation metrics only focus on a single aspect (e.g., text-3D alignment) and fail to perform multi-dimensional quality assessment. To address these problems, we first propose a comprehensive benchmark named MATE-3D. The benchmark contains eight well-designed prompt categories that cover single and multiple object generation, resulting in 1,280 generated textured meshes. We have conducted a large-scale subjective experiment from four different evaluation dimensions and collected 107,520 annotations, followed by detailed analyses of the results. Based on MATE-3D, we propose a novel quality evaluator named HyperScore. Utilizing hypernetwork to generate specified mapping functions for each evaluation dimension, our metric can effectively perform multi-dimensional quality assessment. HyperScore presents superior performance over existing metrics on MATE-3D, making it a promising metric for assessing and improving text-to-3D generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujie Zhang",
      "Bingyang Cui",
      "Qi Yang",
      "Zhu Li",
      "Yiling Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hua_Sat2City_3D_City_Generation_from_A_Single_Satellite_Image_with_ICCV_2025_paper.html": {
    "title": "Sat2City: 3D City Generation from A Single Satellite Image with Cascaded Latent Diffusion",
    "volume": "main",
    "abstract": "Recent advancements in generative models have enabled 3D urban scene generation from satellite imagery, unlocking promising applications in gaming, digital twins, and beyond. However, most existing methods rely heavily on neural rendering techniques, which hinder their ability to produce detailed 3D structures on a broader scale, largely due to the inherent structural ambiguity derived from relatively limited 2D observations. To address this challenge, we propose Sat2City, a novel framework that synergizes the representational capacity of sparse voxel grids with latent diffusion models, tailored specifically for our novel 3D city dataset. Our approach is enabled by three key components: (1) A cascaded latent diffusion framework that progressively recovers 3D city structures from satellite imagery, (2) a Re-Hash operation at its Variational Autoencoder (VAE) bottleneck to compute multi-scale feature grids for stable appearance optimization, and (3) an inverse sampling strategy enabling implicit supervision for smooth appearance transitioning. To overcome the challenge of collecting real-world city-scale 3D models with high-quality geometry and appearance, we introduce a dataset of synthesized large-scale 3D cities paired with satellite-view height maps. Validated on this dataset, our framework generates detailed 3D structures from a single satellite image, achieving superior fidelity compared to existing city generation models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tongyan Hua",
      "Lutao Jiang",
      "Ying-Cong Chen",
      "Wufan Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ye_Hi3DGen_High-fidelity_3D_Geometry_Generation_from_Images_via_Normal_Bridging_ICCV_2025_paper.html": {
    "title": "Hi3DGen: High-fidelity 3D Geometry Generation from Images via Normal Bridging",
    "volume": "main",
    "abstract": "With the growing demand for high-fidelity 3D models from 2D images, existing methods still face significant challenges in accurately reproducing fine-grained geometric details due to limitations in domain gaps and inherent ambiguities in RGB images. To address these issues, we propose Hi3DGen, a novel framework for generating highfidelity 3D geometry from images via normal bridging. Hi3DGen consists of three key components: 1. an imageto-normal estimator that decouples the low-high frequency image pattern with noise injection and dual-stream training to achieve generalizable, stable, and sharp estimation; 2. a normal-to-geometry learning approach that uses normalregularized latent diffusion learning to enhance 3D geometry generation fidelity; and 3. a 3D data synthesis pipeline that constructs a high-quality dataset to support training. Extensive experiments demonstrate the effectiveness and superiority of our framework in generating rich geometric details, outperforming state-of-the-art methods in terms of fidelity. Our work provides a new direction for high-fidelity 3D geometry generation from images by leveraging normal maps as an intermediate representation",
    "checked": true,
    "id": "5ed22a760b347fb5f6e46bc1bcdbb0e931fbf922",
    "semantic_title": "hi3dgen: high-fidelity 3d geometry generation from images via normal bridging",
    "citation_count": 20,
    "authors": [
      "Chongjie  Ye",
      "Yushuang Wu",
      "Ziteng  Lu",
      "Jiahao Chang",
      "Xiaoyang Guo",
      "Jiaqing Zhou",
      "Hao Zhao",
      "Xiaoguang Han"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_SC-Captioner_Improving_Image_Captioning_with_Self-Correction_by_Reinforcement_Learning_ICCV_2025_paper.html": {
    "title": "SC-Captioner: Improving Image Captioning with Self-Correction by Reinforcement Learning",
    "volume": "main",
    "abstract": "We propose SC-Captioner, a reinforcement learning framework that enables the self-correcting capability of image caption models. Our crucial technique lies in the design of the reward function to incentivize accurate caption corrections. Specifically, the predicted and reference captions are decomposed into object, attribute, and relation sets using scene-graph parsing algorithms. We calculate the set difference between sets of initial and self-corrected captions to identify added and removed elements. These elements are matched against the reference sets to calculate correctness bonuses for accurate refinements and mistake punishments for wrong additions and removals, thereby forming the final reward. For image caption quality assessment, we propose a set of metrics refined from CAPTURE that alleviate its incomplete precision evaluation and inefficient relation matching problems. Furthermore, we collect a fine-grained annotated image caption dataset, RefinedCaps, consisting of 6.5K diverse images from COCO dataset. Experiments show that applying SC-Captioner on large visual-language models can generate better image captions across various scenarios, significantly outperforming the direct preference optimization training strategy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lin Zhang",
      "Xianfang Zeng",
      "Kangcong Li",
      "Gang Yu",
      "Tao Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Pang_DreamDance_Animating_Human_Images_by_Enriching_3D_Geometry_Cues_from_ICCV_2025_paper.html": {
    "title": "DreamDance: Animating Human Images by Enriching 3D Geometry Cues from 2D Poses",
    "volume": "main",
    "abstract": "In this work, we present DreamDance, a novel method for animating human images using only skeleton pose sequences as conditional inputs. Existing approaches struggle with generating coherent, high-quality content in an efficient and user-friendly manner. Concretely, baseline methods relying on only 2D pose guidance lack the cues of 3D information like depth and normal maps, leading to suboptimal results. Other works introduce extra representations to provide additional 3D information but inevitably involve a cumbersome and time-intensive process. To address these limitations, DreamDance enriches 3D geometry cues from 2D poses by introducing an efficient diffusion model, enabling high-quality human image animation with various guidance. Our key insight is that human images naturally exhibit multiple levels of correlation, progressing from coarse skeleton poses to fine-grained geometry cues, and further from these geometry cues to explicit appearance details. Capturing such correlations could enrich the guidance signals, facilitating intra-frame coherency and inter-frame consistency. Specifically, we construct the TikTok-Dance5K dataset, comprising 5K high-quality dance videos with detailed frame annotations, including human pose, depth, and normal maps. Next, we introduce a Mutually Aligned Geometry Diffusion Model to generate fine-grained depth and normal maps for enriched guidance. Finally, a Cross-domain Controller incorporates multi-level guidance to animate human images effectively with a video diffusion model. Extensive experiments demonstrate that our method achieves state-of-the-art performance in animating human images compared to baseline methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yatian Pang",
      "Bin Zhu",
      "Bin Lin",
      "Mingzhe Zheng",
      "Francis E. H. Tay",
      "Ser-Nam Lim",
      "Harry Yang",
      "Li Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Huh_MBTI_Masked_Blending_Transformers_with_Implicit_Positional_Encoding_for_Frame-rate_ICCV_2025_paper.html": {
    "title": "MBTI: Masked Blending Transformers with Implicit Positional Encoding for Frame-rate Agnostic Motion Estimation",
    "volume": "main",
    "abstract": "Human motion estimation models typically assume a fixed number of input frames, making them sensitive to variations in frame rate and leading to inconsistent motion predictions across different temporal resolutions. This limitation arises because input frame rates inherently determine the temporal granularity of motion capture, causing discrepancies when models trained on a specific frame rate encounter different sampling frequencies. To address this challenge, we propose MBTI (Masked Blending Transformers with Implicit Positional Encoding), a frame rate-agnostic human motion estimation framework designed to maintain temporal consistency across varying input frame rates. Our approach leverages a masked autoencoder (MAE) architecture with masked token blending, which aligns input tokens with a predefined high-reference frame rate, ensuring a standardized temporal representation. Additionally, we introduce implicit positional encoding, which encodes absolute time information using neural implicit functions, enabling more natural motion reconstruction beyond discrete sequence indexing. By reconstructing motion at a high reference frame rate and optional downsampling, MBTI ensures both frame rate generalization and temporal consistency. To comprehensively evaluate MBTI, we introduce EMDB-FPS, an augmented benchmark designed to assess motion estimation robustness across multiple frame rates in both local and global motion estimation tasks. To further assess MBTI's robustness, we introduce the Motion Consistency across Frame rates (MCF), a novel metric to quantify the deviation of motion predictions across different input frame rates. Our results demonstrate that MBTI outperforms state-of-the-art methods in both motion accuracy and temporal consistency, achieving the most stable and consistent motion predictions across varying frame rates",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jungwoo Huh",
      "Yeseung Park",
      "Seongjean Kim",
      "Jungsu Kim",
      "Sanghoon Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_MixRI_Mixing_Features_of_Reference_Images_for_Novel_Object_Pose_ICCV_2025_paper.html": {
    "title": "MixRI: Mixing Features of Reference Images for Novel Object Pose Estimation",
    "volume": "main",
    "abstract": "We present MixRI, a lightweight network that solves the CAD-based novel object pose estimation problem in RGB images. It can be instantly applied to a novel object at test time without finetuning. We design our network to meet the demands of real-world applications, emphasizing reduced memory requirements and fast inference time. Unlike existing works that utilize many reference images and have large network parameters, we directly match points based on the multi-view information between the query and reference images with a lightweight network. Thanks to our reference image fusion strategy, we significantly decrease the number of reference images, thus decreasing the time needed to process these images and the memory required to store them. Furthermore, with our lightweight network, our method requires less inference time. Though with fewer reference images, experiments on seven core datasets in the BOP challenge show that our method achieves comparable results with other methods that require more reference images and larger network parameters",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinhang Liu",
      "Jiawei Shi",
      "Zheng Dang",
      "Yuchao Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liang_p-AVAS_Can_Physics-Integrated_Audio-Visual_Modeling_Boost_Neural_Acoustic_Synthesis_ICCV_2025_paper.html": {
    "title": "p-AVAS: Can Physics-Integrated Audio-Visual Modeling Boost Neural Acoustic Synthesis?",
    "volume": "main",
    "abstract": "The Audio-Visual Acoustic Synthesis (AVAS) task aims to model realistic audio propagation behavior within a specific visual scene. Prior works often rely on sparse image representations to guide acoustic synthesis. However, we argue that this approach is insufficient to capture the intricate physical properties of the environment and may struggle with generalization across diverse scenes. In this work, we review the limitations of existing pipelines and address the research question: Can we leverage physical audio-visual associations to enhance neural acoustic synthesis? We introduce Physics-Integrated Audio-Visual Acoustic Synthesis (PI-AVAS or \\pi-AVAS), a novel framework designed with two key objectives. i) Generalization: We develop a vision-guided audio simulation framework that leverages physics-based sound propagation. By explicitly modeling vision-grounded geometry and sound rays, our approach achieves robust performance across diverse visual environments. ii) Realism: While simulation-based approaches offer generalizability, they often compromise on realism. To mitigate this, we incorporate a second stage for data-centric refinement, where we propose a flow matching-based audio refinement model to narrow the gap between simulation and real-world audio-visual scenes. Extensive experiments demonstrate the effectiveness and robustness of our method. We achieve state-of-the-art performance on the RWAVS-Gen, RWAVS, and RAF datasets. Additionally, we show that our approach can be seamlessly integrated with existing methods to significantly improve their performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Susan Liang",
      "Chao Huang",
      "Yunlong Tang",
      "Zeliang Zhang",
      "Chenliang Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Learning_Precise_Affordances_from_Egocentric_Videos_for_Robotic_Manipulation_ICCV_2025_paper.html": {
    "title": "Learning Precise Affordances from Egocentric Videos for Robotic Manipulation",
    "volume": "main",
    "abstract": "Affordance, defined as the potential actions that an object offers, is crucial for embodied AI agents. For example, such knowledge directs an agent to grasp a knife by the handle for cutting or by the blade for safe handover. While existing approaches have made notable progress, affordance research still faces three key challenges: data scarcity, poor generalization, and real-world deployment. Specifically, there is a lack of large-scale affordance datasets with precise segmentation maps, existing models struggle to generalize across different domains or novel object and affordance classes, and little work demonstrates deployability in real-world scenarios. In this work, we address these issues by proposing a complete affordance learning system that (1) takes in egocentric videos and outputs precise affordance annotations without human labeling, (2) leverages geometric information and vision foundation models to improve generalization, and (3) introduces a framework that facilitates affordance-oriented robotic manipulation such as tool grasping and robot-to-human tool handover. Experimental results show that our model surpasses the state-of-the-art by 13.8% in mIoU, and the framework achieves 77.1% successful grasping among 179 trials, including evaluations on seen, unseen classes, and cluttered scenes. Project page: https://reagan1311.github.io/affgrasp",
    "checked": true,
    "id": "67c6a599a8e353722a38c136b6662daed21004f0",
    "semantic_title": "learning precise affordances from egocentric videos for robotic manipulation",
    "citation_count": 13,
    "authors": [
      "Gen Li",
      "Nikolaos Tsagkas",
      "Jifei Song",
      "Ruaridh Mon-Williams",
      "Sethu Vijayakumar",
      "Kun Shao",
      "Laura Sevilla-Lara"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_VAFlow_Video-to-Audio_Generation_with_Cross-Modality_Flow_Matching_ICCV_2025_paper.html": {
    "title": "VAFlow: Video-to-Audio Generation with Cross-Modality Flow Matching",
    "volume": "main",
    "abstract": "Video-to-audio (V2A) generation aims to synthesize temporally aligned, realistic sounds for silent videos, a critical capability for immersive multimedia applications. Current V2A methods, predominantly based on diffusion or flow models, rely on suboptimal noise-to-audio paradigms that entangle cross-modal mappings with stochastic priors, resulting in inefficient training and convoluted transport paths. We propose VAFlow, a novel flow-based framework that directly models the video-to-audio transformation, eliminating reliance on noise priors. To address modality discrepancies, we employ an alignment variational autoencoder that compresses heterogeneous video features into audio-aligned latent spaces while preserving spatiotemporal semantics. By retaining cross-attention mechanisms between video features and flow blocks, our architecture enables classifier-free guidance within video source-driven generation. Without external data or complex training tricks, VAFlow achieves state-of-the-art performance on VGGSound benchmark, surpassing even text-augmented models in audio fidelity, diversity, and distribution alignment. This work establishes a new paradigm for V2A generation with a direct and effective video-to-audio transformation via flow matching",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xihua Wang",
      "Xin Cheng",
      "Yuyue Wang",
      "Ruihua Song",
      "Yunfeng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Guan_HDR_Image_Generation_via_Gain_Map_Decomposed_Diffusion_ICCV_2025_paper.html": {
    "title": "HDR Image Generation via Gain Map Decomposed Diffusion",
    "volume": "main",
    "abstract": "While diffusion models have demonstrated significant success in standard dynamic range (SDR) image synthesis, generating high dynamic range (HDR) images with higher luminance and broader color gamuts remains challenging. This arises primarily from two factors: (1) The incompatibility between pretrained SDR image auto-encoders and the high-bit-depth HDR images; (2) The lack of large-scale HDR image datasets for effective learning and supervision. In this paper, we propose a novel framework for HDR image generation with two key innovations: (1) Decomposed HDR Image Generation: We leverage a double-layer HDR image format to decompose the HDR image into two low-bit-depth components: an SDR image with a corresponding Gain Map (GM).This format is inherently compatible with pretrained SDR auto-encoders, motivating the decomposition of HDR image generation into SDR image and GM prediction. (2) Unsupervised Data Construction: We develop an automated pipeline to construct \"Text-SDR-GM\" triplets from large-scale text-image datasets by brightness-aware compression and gamut-constrained reduction, enabling unsupervised learning of GMs without ground-truth data. Building upon these innovations, we adapt the Stable Diffusion model to jointly predict GMs and SDR images, enabling high-quality decomposed HDR image generation. Experiments show that our framework excels in HDR image generation and SDR-to-HDRTV up-conversion, generalizing well across diverse scenes and conditions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanshen Guan",
      "Ruikang Xu",
      "Yinuo Liao",
      "Mingde Yao",
      "Lizhi Wang",
      "Zhiwei Xiong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Pierson_DiffuMatch_Category-Agnostic_Spectral_Diffusion_Priors_for_Robust_Non-rigid_Shape_Matching_ICCV_2025_paper.html": {
    "title": "DiffuMatch: Category-Agnostic Spectral Diffusion Priors for Robust Non-rigid Shape Matching",
    "volume": "main",
    "abstract": "Deep functional maps have recently emerged as a powerful tool for solving non-rigid shape correspondence tasks. Methods that use this approach combine the power and flexibility of the functional map framework, with data-driven learning for improved accuracy and generality. However, most existing methods in this area restrict the learning aspect only to the feature functions and still rely on axiomatic modeling for formulating the training loss or for functional map regularization inside the networks. This limits both the accuracy and the applicability of the resulting approaches only to scenarios where assumptions of the axiomatic models hold. In this work, we show, for the first time, that both in-network regularization and functional map training can be replaced with data-driven methods. For this, we first train a generative model of functional maps in the spectral domain using score-based generative modeling, built from a large collection of high-quality maps. We then exploit the resulting model to promote the structural properties of ground truth functional maps on new shape collections. Remarkably, we demonstrate that the learned models are category-agnostic, and can fully replace commonly used strategies such as enforcing Laplacian commutativity or orthogonality of functional maps. Our key technical contribution is a novel distillation strategy from diffusion models in the spectral domain. Experiments demonstrate that our learned regularization leads to better results than axiomatic approaches for zero-shot non-rigid shape matching. Our code is available at: https://github.com/daidedou/diffumatch/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emery Pierson",
      "Lei Li",
      "Angela Dai",
      "Maks Ovsjanikov"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Dong_MoGA_3D_Generative_Avatar_Prior_for_Monocular_Gaussian_Avatar_Reconstruction_ICCV_2025_paper.html": {
    "title": "MoGA: 3D Generative Avatar Prior for Monocular Gaussian Avatar Reconstruction",
    "volume": "main",
    "abstract": "We present MoGA, a novel method to reconstruct high-fidelity 3D Gaussian avatars from a single-view image. The main challenge lies in inferring unseen appearance and geometric details while ensuring 3D consistency and realism. Most previous methods rely on 2D diffusion models to synthesize unseen views; however, these generated views are sparse and inconsistent, resulting in unrealistic 3D artifacts and blurred appearance. To address these limitations, we leverage a generative avatar model, that can generate diverse 3D avatars by sampling deformed Gaussians from a learned prior distribution. Due to limited 3D training data, such a 3D model alone cannot capture all image details of unseen identities. Consequently, we integrate it as a prior, ensuring 3D consistency by projecting input images into its latent space and enforcing additional 3D appearance and geometric constraints. Our novel approach formulates Gaussian avatar creation as model inversion by fitting the generative avatar to synthetic views from 2D diffusion models. The generative avatar provides an initialization for model fitting, enforces 3D regularization, and helps in refining pose. Experiments show that our method surpasses state-of-the-art techniques and generalizes well to real-world scenarios. Our Gaussian avatars are also inherently animatable. For code, see https:// zj-dong.github.io/ MoGA/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijian Dong",
      "Longteng Duan",
      "Jie Song",
      "Michael J. Black",
      "Andreas Geiger"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kumar_Aligning_Moments_in_Time_using_Video_Queries_ICCV_2025_paper.html": {
    "title": "Aligning Moments in Time using Video Queries",
    "volume": "main",
    "abstract": "Video-to-video moment retrieval (Vid2VidMR) is the task of localizing unseen events or moments in a target video using a query video. This task poses several challenges, such as the need for semantic frame-level alignment and modeling complex dependencies between query and target videos. To tackle this challenging problem, we introduce MATR (Moment Alignment TRansformer), a transformer-based model designed to capture semantic context as well as the temporal details necessary for precise moment localization. MATR conditions target video representations on query video features using dual-stage sequence alignment that encodes the required correlations and dependencies. These representations are then used to guide foreground/background classification and boundary prediction heads, enabling the model to accurately identify moments in the target video that semantically match with the query video. Additionally, to provide a strong task-specific initialization for MATR, we propose a self-supervised pre-training technique that involves training the model to localize random clips within videos. Extensive experiments demonstrate that MATR achieves notable performance improvements of 13.1% in R@1 and 8.1% in mIoU on an absolute scale compared to state-of-the-art methods on the popular ActivityNet-VRL dataset. Additionally, on our newly proposed dataset, SportsMoments, MATR shows a 14.7% gain in R@1 and a 14.4% gain in mIoU on an absolute scale over strong baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yogesh Kumar",
      "Uday Agarwal",
      "Manish Gupta",
      "Anand Mishra"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_StreamGS_Online_Generalizable_Gaussian_Splatting_Reconstruction_for_Unposed_Image_Streams_ICCV_2025_paper.html": {
    "title": "StreamGS: Online Generalizable Gaussian Splatting Reconstruction for Unposed Image Streams",
    "volume": "main",
    "abstract": "The advent of 3D Gaussian Splatting (3DGS) has advanced 3D scene reconstruction and novel view synthesis. With the growing interest of interactive applications that need immediate feedback, online 3DGS reconstruction in real-time is in high demand. However, none of existing methods yet meet the demand due to three main challenges: the absence of predetermined camera parameters, the need for generalizable 3DGS optimization, and the necessity of reducing redundancy. We propose StreamGS, an online generalizable 3DGS reconstruction method for unposed image streams, which progressively transform image streams to 3D Gaussian streams by predicting and aggregating per-frame Gaussians. Our method overcomes the limitation of the initial point reconstruction in tackling out-of-domain (OOD) issues by introducing a content adaptive refinement. The refinement enhances cross-frame consistency by establishing reliable pixel correspondences between adjacent frames. Such correspondences further aid in merging redundant Gaussians through cross-frame feature aggregation. The density of Gaussians is thereby reduced, empowering online reconstruction by significantly lowering computational and memory costs. Extensive experiments on diverse datasets have demonstrated that StreamGS achieves quality on par with optimization-based approaches but does so 150 times faster, and exhibits superior generalizability in handling OOD scenes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Li",
      "Jinglu Wang",
      "Lei Chu",
      "Xiao Li",
      "Shiu-Hong Kao",
      "Ying-Cong Chen",
      "Yan Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yahn_Adversarial_Attention_Perturbations_for_Large_Object_Detection_Transformers_ICCV_2025_paper.html": {
    "title": "Adversarial Attention Perturbations for Large Object Detection Transformers",
    "volume": "main",
    "abstract": "Adversarial perturbations are useful tools for exposing vulnerabilities in neural networks. Existing adversarial perturbation methods for object detection are either limited to attacking CNN-based detectors or weak against transformer-based detectors. This paper presents an Attention-Focused Offensive Gradient (AFOG) attack against object detection transformers. By design, AFOG is neural-architecture agnostic and effective for attacking both large transformer-based object detectors and conventional CNN-based detectors with a unified adversarial attention framework. This paper makes three original contributions. First, AFOG utilizes a learnable attention mechanism that focuses perturbations on vulnerable image regions in multi-box detection tasks, increasing performance over non-attention baselines by up to 30.6%. Second, AFOG's attack loss is formulated by integrating two types of feature loss through learnable attention updates with iterative injection of adversarial perturbations. Finally, AFOG is an efficient and stealthy adversarial perturbation method. It probes the weak spots of detection transformers by adding strategically generated and visually imperceptible perturbations which can cause well-trained object detection models to fail. Extensive experiments conducted with twelve large detection transformers on COCO demonstrate the efficacy of AFOG. Our empirical results also show that AFOG outperforms existing attacks on transformer-based and CNN-based object detectors by up to 83% with superior speed and imperceptibility. Code is available at: https://github.com/zacharyyahn/AFOG",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zachary Yahn",
      "Selim Furkan Tekin",
      "Fatih Ilhan",
      "Sihao Hu",
      "Tiansheng Huang",
      "Yichang Xu",
      "Margaret Loper",
      "Ling Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Mu_Meta-Learning_Dynamic_Center_Distance_Hard_Sample_Mining_for_Learning_with_ICCV_2025_paper.html": {
    "title": "Meta-Learning Dynamic Center Distance: Hard Sample Mining for Learning with Noisy Labels",
    "volume": "main",
    "abstract": "The sample selection approach is a widely adopted strategy for learning with noisy labels, where examples with lower losses are effectively treated as clean during training. However, this clean set often becomes dominated by easy examples, limiting the model's meaningful exposure to more challenging cases and reducing its expressive power. To overcome this limitation, we introduce a novel metric called Dynamic Center Distance (DCD), which can quantify sample difficulty and provide information that critically complements loss values. Unlike approaches that rely on predictions, DCD is computed in feature space as the distance between sample features and a dynamically updated center, established through a proposed meta-learning framework. Building on preliminary semi-supervised training that captures fundamental data patterns, we incorporate DCD to further refine the classification loss, down-weighting well-classified examples and strategically focusing training on a sparse set of hard instances. This strategy prevents easy examples from dominating the classifier, leading to more robust learning. Extensive experiments across multiple benchmark datasets, including synthetic and real-world noise settings, as well as natural and medical images, consistently demonstrate the effectiveness of our method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenyu Mu",
      "Yijun Qu",
      "Jiexi Yan",
      "Erkun Yang",
      "Cheng Deng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_Ensemble_Foreground_Management_for_Unsupervised_Object_Discovery_ICCV_2025_paper.html": {
    "title": "Ensemble Foreground Management for Unsupervised Object Discovery",
    "volume": "main",
    "abstract": "Unsupervised object discovery (UOD) aims to detect and segment objects in 2D images without handcrafted annotations. Recent progress in self-supervised representation learning has led to some success in UOD algorithms. However, the absence of ground truth provides existing UOD methods with two challenges: 1) determining if a discovered region is foreground or background, and 2) knowing how many objects remain undiscovered. To address these two problems, previous solutions rely on foreground priors to distinguish if the discovered region is foreground, and conduct one or fixed iterations of discovery. However, the existing foreground priors are heuristic and not always robust, and a fixed number of discoveries leads to under or over-segmentation, since the number of objects in images varies. This paper introduces UnionCut, a robust and well-grounded foreground prior based on min-cut and ensemble methods that detects the union of foreground areas of an image, allowing UOD algorithms to identify foreground objects and stop discovery once the majority of the foreground union in the image is segmented. In addition, we propose UnionSeg, a distilled transformer of UnionCut that outputs the foreground union more efficiently and accurately. Our experiments show that by combining with UnionCut or UnionSeg, previous state-of-the-art UOD methods witness an increase in the performance of single object discovery, saliency detection and self-supervised instance segmentation on various benchmarks. The code is available at https://github.com/YFaris/UnionCut",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziling Wu",
      "Armaghan Moemeni",
      "Praminda Caleb-Solly"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_RESCUE_Crowd_Evacuation_Simulation_via_Controlling_SDM-United_Characters_ICCV_2025_paper.html": {
    "title": "RESCUE: Crowd Evacuation Simulation via Controlling SDM-United Characters",
    "volume": "main",
    "abstract": "Crowd evacuation simulation is critical for enhancing public safety, and demanded for realistic virtual environments. Current mainstream evacuation models overlook the complex human behaviors that occur during evacuation, such as pedestrian collisions, interpersonal interactions, and variations in behavior influenced by terrain types or individual body shapes. This results in the failure to accurately simulate the escape of people in the real world. In this paper, aligned with the sensory-decision-motor (SDM) flow of the human brain, we propose a real-time 3D crowd evacuation simulation framework that integrates a 3D-adaptive SFM (Social Force Model) Decision Mechanism and a Personalized Gait Control Motor. This framework allows multiple agents to move in parallel and is suitable for various scenarios, with dynamic crowd awareness. Additionally, we introduce Part-level Force Visualization to assist in evacuation analysis. Experimental results demonstrate that our framework supports dynamic trajectory planning and personalized behavior for each agent throughout the evacuation process, and is compatible with uneven terrain. Visually, our method generates evacuation results that are more realistic and plausible, providing enhanced insights for crowd simulation. The code is available at http://cic.tju.edu.cn/faculty/likun/projects/RESCUE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaolin Liu",
      "Tianyi Zhou",
      "Hongbo Kang",
      "Jian Ma",
      "Ziwen Wang",
      "Jing Huang",
      "Wenguo Weng",
      "Yu-Kun Lai",
      "Kun Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Skiers_Joint_Diffusion_Models_in_Continual_Learning_ICCV_2025_paper.html": {
    "title": "Joint Diffusion Models in Continual Learning",
    "volume": "main",
    "abstract": "In this work, we introduce JDCL - a new method for continual learning with generative rehearsal based on joint diffusion models. Neural networks suffer from catastrophic forgetting defined as abrupt loss in the model's performance when retrained with additional data coming from a different distribution. Generative-replay-based continual learning methods try to mitigate this issue by retraining a model with a combination of new and rehearsal data sampled from a generative model. In this work, we propose to extend this idea by combining a continually trained classifier with a diffusion-based generative model into a single - jointly optimized neural network. We show that such shared parametrization, combined with the knowledge distillation technique allows for stable adaptation to new tasks without catastrophic forgetting. We evaluate our approach on several benchmarks, where it outperforms recent state-of-the-art generative replay techniques. Additionally, we extend our method to the semi-supervised continual learning setup, where it outperforms competing buffer-based replay techniques, and evaluate, in a self-supervised manner, the quality of trained representations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paweł Skierś",
      "Kamil Deja"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Pipoli_MissRAG_Addressing_the_Missing_Modality_Challenge_in_Multimodal_Large_Language_ICCV_2025_paper.html": {
    "title": "MissRAG: Addressing the Missing Modality Challenge in Multimodal Large Language Models",
    "volume": "main",
    "abstract": "Recently, Multimodal Large Language Models (MLLMs) have emerged as a leading framework for enhancing the ability of Large Language Models (LLMs) to interpret non-linguistic modalities. Despite their impressive capabilities, the robustness of MLLMs under conditions where one or more modalities are missing remains largely unexplored. In this paper, we investigate the extent to which MLLMs can maintain performance when faced with missing modality inputs. Moreover, we propose a novel framework to mitigate the aforementioned issue called retrieval-augmented generation for missing modalities (MissRAG). It consists of a novel multimodal RAG technique alongside a tailored prompt engineering strategy designed to enhance model robustness by mitigating the impact of absent modalities while preventing the burden of additional instruction tuning. To demonstrate the effectiveness of our techniques, we conduct comprehensive evaluations across five diverse datasets, covering tasks such as audio-visual question answering, audio-visual captioning, and multimodal sentiment analysis. Our source code is available at https://github.com/aimagelab/MissRAG",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vittorio Pipoli",
      "Alessia Saporita",
      "Federico Bolelli",
      "Marcella Cornia",
      "Lorenzo Baraldi",
      "Costantino Grana",
      "Rita Cucchiara",
      "Elisa Ficarra"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zuo_Signs_as_Tokens_A_Retrieval-Enhanced_Multilingual_Sign_Language_Generator_ICCV_2025_paper.html": {
    "title": "Signs as Tokens: A Retrieval-Enhanced Multilingual Sign Language Generator",
    "volume": "main",
    "abstract": "Sign language is a visual language that encompasses all linguistic features of natural languages and serves as the primary communication method for the deaf and hard-of-hearing communities. Although many studies have successfully adapted pretrained language models (LMs) for sign language translation (sign-to-text), the reverse task--sign language generation (text-to-sign)--remains largely unexplored. In this work, we introduce a multilingual sign language model, Signs as Tokens (SOKE), which can generate 3D sign avatars autoregressively from text inputs using a pretrained LM. To align sign language with the LM, we leverage a decoupled tokenizer that discretizes continuous signs into token sequences representing various body parts. During decoding, unlike existing approaches that flatten all part-wise tokens into a single sequence and predict one token at a time, we propose a multi-head decoding method capable of predicting multiple tokens simultaneously. This approach improves inference efficiency while maintaining effective information fusion across different body parts. To further ease the generation process, we propose a retrieval-enhanced SLG approach, which incorporates external sign dictionaries to provide accurate word-level signs as auxiliary conditions, significantly improving the precision of generated signs. Extensive qualitative and quantitative evaluations demonstrate the effectiveness of SOKE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ronglai Zuo",
      "Rolandos Alexandros Potamias",
      "Evangelos Ververas",
      "Jiankang Deng",
      "Stefanos Zafeiriou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jeong_Test-Time_Prompt_Tuning_for_Zero-Shot_Depth_Completion_ICCV_2025_paper.html": {
    "title": "Test-Time Prompt Tuning for Zero-Shot Depth Completion",
    "volume": "main",
    "abstract": "Zero-shot depth completion with metric scales poses significant challenges, primarily due to performance limitations such as domain specificity and sensor characteristics. One recent emerging solution is to integrate monocular depth foundation models into depth completion frameworks, yet these efforts still face issues with suboptimal performance and often require further adaptation to the target task. Surprisingly, we find that a simple test-time training, which fine-tunes monocular depth foundation models on sparse depth measurements from sensors just as it is, yields reasonable results. However, this test-time training obviously incurs high computational costs and introduces biases towards specific conditions, making it impractical for real-world scenarios. In this paper, we introduce a new approach toward parameter-efficient zero-shot depth completion. Our key idea of this work is to leverage visual prompt tuning, achieving sensor-specific depth scale adaptation without forgetting foundational knowledge. Experimental results on diverse datasets demonstrate that our approach outperforms relevant state-of-the-art methods, showing superior generalization and efficiency. Our source code is available in the supplementary materials",
    "checked": false,
    "id": "b4f2975251ed774985fd61753c6c71b049e58e4b",
    "semantic_title": "dts-tpt: dual temporal-sync test-time prompt tuning for zero-shot activity recognition",
    "citation_count": 5,
    "authors": [
      "Chanhwi Jeong",
      "Inhwan Bae",
      "Jin-Hwi Park",
      "Hae-Gon Jeon"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lu_GUIOdyssey_A_Comprehensive_Dataset_for_Cross-App_GUI_Navigation_on_Mobile_ICCV_2025_paper.html": {
    "title": "GUIOdyssey: A Comprehensive Dataset for Cross-App GUI Navigation on Mobile Devices",
    "volume": "main",
    "abstract": "Autonomous Graphical User Interface (GUI) navigation agents can enhance user experience in communication, entertainment, and productivity by streamlining workflows and reducing manual intervention. However, prior GUI agents often trained with datasets comprising tasks that can be completed within a single app, leading to poor performance in cross-app navigation. To address this problem, we present GUIOdyssey, a comprehensive dataset for cross-app mobile GUI navigation. GUIOdyssey comprises 8,334 episodes with an average of 15.3 steps per episode, covering 6 mobile devices, 212 distinct apps, and 1,357 app combinations. Each step is enriched with detailed semantic reasoning annotations, which aid the model in building cognitive processes and enhancing its reasoning abilities for complex cross-app tasks. Building on GUIOdyssey, we develop OdysseyAgent, an exploratory multimodal agent for long-step cross-app navigation equipped with a history resampler module that efficiently attends to historical screenshot tokens, balancing performance and inference speed. Extensive experiments conducted in both in-domain and out-of-domain scenarios validate the effectiveness of our approach. Moreover, we demonstrate that historial information involving actions, screenshots and context in our dataset can significantly enhances OdysseyAgent's performance on complex cross-app tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quanfeng Lu",
      "Wenqi Shao",
      "Zitao Liu",
      "Lingxiao Du",
      "Fanqing Meng",
      "Boxuan Li",
      "Botong Chen",
      "Siyuan Huang",
      "Kaipeng Zhang",
      "Ping Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ren_TOTP_Transferable_Online_Pedestrian_Trajectory_Prediction_with_Temporal-Adaptive_Mamba_Latent_ICCV_2025_paper.html": {
    "title": "TOTP: Transferable Online Pedestrian Trajectory Prediction with Temporal-Adaptive Mamba Latent Diffusion",
    "volume": "main",
    "abstract": "Pedestrian trajectory prediction is crucial for many intelligent tasks. While existing methods predict future trajectories from fixed-frame historical observations, they are limited by the observational perspective and the need for extensive historical information, resulting in prediction delays and inflexible generalization in real-time systems. In this paper, we propose a novel task called Transferable Online Pedestrian Trajectory Prediction (TOTP), which synchronously predicts future trajectories with variable observations and enables effective task transfer under different observation constraints. To advance TOTP modeling, we propose a Temporal-Adaptive Mamba Latent Diffusion (TAMLD) model. It utilizes the Social-Implicit Mamba Synthesizer to extract motion states with social interaction and refine temporal representations through Temporal-Aware Distillation. A Trend-Conditional Mamba Decomposer generates the motion latent distribution of the future motion trends and predicts future motion trajectories through sampling decomposition. We utilize Motion-Latent Mamba Diffusion to reconstruct the latent space disturbed by imbalanced temporal noise. Our method achieves state-of-the-art results on multiple datasets and tasks, showcasing temporal adaptability and generalization ability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyang Ren",
      "Ping Wei",
      "Shangqi Deng",
      "Haowen Tang",
      "Jiapeng Li",
      "Huan Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Berton_AstroLoc_Robust_Space_to_Ground_Image_Localizer_ICCV_2025_paper.html": {
    "title": "AstroLoc: Robust Space to Ground Image Localizer",
    "volume": "main",
    "abstract": "Thousands of photos of Earth are taken every day by astronauts from the International Space Station. Localizing these photos, which has been performed manually for decades, has recently been approached through image retrieval solutions: given an astronaut photo, the goal is to find its most similar match among a large database of geo-tagged satellite images, in a task called Astronaut Photography Localization (APL). Yet, existing APL approaches are trained only using satellite images, without taking advantage of the millions of open-source astronaut photos. In this work we present the first APL pipeline capable of leveraging astronaut photos for training. We first produce full localization information for 300,000 manually weakly-labeled astronaut photos through an automated pipeline, and then use these images to train a model, called AstroLoc. AstroLoc learns a robust representation of Earth's surface features through two objective functions: pairing astronaut photos with their matching satellite counterparts in a pairwise loss, and a second loss on clusters of satellite imagery weighted by their relevance to astronaut photography through unsupervised mining. AstroLoc achieves a staggering 35% average improvement in recall@1 over previous SOTA, reaching a recall@100 consistently over 99% for existing datasets. Moreover, without fine-tuning, AstroLoc provides excellent results for related tasks like the lost-in-space satellite problem and historical space imagery localization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gabriele Berton",
      "Alex Stoken",
      "Carlo Masone"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_Dual_Recursive_Feedback_on_Generation_and_Appearance_Latents_for_Pose-Robust_ICCV_2025_paper.html": {
    "title": "Dual Recursive Feedback on Generation and Appearance Latents for Pose-Robust Text-to-Image Diffusion",
    "volume": "main",
    "abstract": "Recent advancements in controllable text-to-image (T2I) diffusion models, such as Ctrl-X and FreeControl, have demonstrated robust spatial and appearance control without requiring auxiliary module training. However, these models often struggle to accurately preserve spatial structures and fail to capture fine-grained conditions related to object poses and scene layouts. To address these challenges, we propose a training-free Dual Recursive Feedback (DRF) system that properly reflects control conditions in controllable T2I models. The proposed DRF consists of appearance feedback and generation feedback that recursively refines the intermediate latents to better reflect the given appearance information and the user's intent.This dual-update mechanism guides latent representations toward reliable manifolds, effectively integrating structural and appearance attributes. Our approach enables fine-grained generation even between class-invariant structure-appearance fusion, such as transferring human motion onto a tiger's form. Extensive experiments demonstrate the efficacy of our method in producing high-quality, semantically coherent, and structurally consistent image generations. Our source code is available at https://github.com/jwonkm/DRF",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiwon Kim",
      "Pureum Kim",
      "SeonHwa Kim",
      "Soobin Park",
      "Eunju Cha",
      "Kyong Hwan Jin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Peng_Chimera_Improving_Generalist_Model_with_Domain-Specific_Experts_ICCV_2025_paper.html": {
    "title": "Chimera: Improving Generalist Model with Domain-Specific Experts",
    "volume": "main",
    "abstract": "Large Multi-modal Models (LMMs), trained on web-scale datasets predominantly composed of natural images, have demonstrated remarkable performance on general tasks. However, these models often exhibit limited specialized capabilities for domain-specific tasks that require extensive domain prior knowledge. An intuitive solution is to post-train LMMs on a specific domain, but often suffers from the labor-intensive annotating process and the inaccessibility of private training data. Directly integrating expert models tailored for those tasks is also challenging due to representational gaps and imbalanced optimization. To address these challenges, we introduce Chimera, a scalable and low-cost multi-modal pipeline designed to boost the ability of existing LMMs with domain-specific experts. Specifically, we design a progressive training strategy to integrate features from expert models into the input of a generalist LMM. To address the imbalanced optimization caused by the well-aligned general visual encoder, we introduce a novel Generalist-Specialist Collaboration Masking (GSCM) mechanism. This results in a versatile model that excels across the chart, table, math, and document domains, achieving state-of-the-art performance on multi-modal reasoning and visual content extraction tasks, both of which are challenging tasks for assessing existing LMMs. We will release model weights, along with the data used for training and evaluation, to facilitate future research on LMMs",
    "checked": true,
    "id": "ec277a5684d52f2e5f5b571c1b6b17552da59708",
    "semantic_title": "chimera: improving generalist model with domain-specific experts",
    "citation_count": 9,
    "authors": [
      "Tianshuo Peng",
      "Mingsheng Li",
      "Jiakang Yuan",
      "Hongbin Zhou",
      "Renqiu Xia",
      "Renrui Zhang",
      "Lei Bai",
      "Song Mao",
      "Bin Wang",
      "Aojun Zhou",
      "Botian Shi",
      "Tao Chen",
      "Bo Zhang",
      "Xiangyu Yue"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_EgoM2P_Egocentric_Multimodal_Multitask_Pretraining_ICCV_2025_paper.html": {
    "title": "EgoM2P: Egocentric Multimodal Multitask Pretraining",
    "volume": "main",
    "abstract": "Understanding multimodal signals in egocentric vision, such as RGB video, depth, camera poses, and gaze, is essential for applications in augmented reality, robotics, and human-computer interaction, enabling systems to better interpret the camera wearer's actions, intentions, and surrounding environment. However, building large-scale egocentric multimodal and multitask models presents unique challenges. Egocentric data are inherently heterogeneous, with large variations in modality coverage across devices and settings. Generating pseudo-labels for missing modalities, such as gaze or head-mounted camera trajectories, is often infeasible, making standard supervised learning approaches difficult to scale. Furthermore, dynamic camera motion and the complex temporal and spatial structure of first-person video pose additional challenges for the direct application of existing multimodal foundation models. To address these challenges, we introduce a set of efficient temporal tokenizers and propose EgoM2P, a masked modeling framework that learns from temporally-aware multimodal tokens to train a large, general-purpose model for egocentric 4D understanding. This unified design supports multitasking across diverse egocentric perception and synthesis tasks, including gaze prediction, egocentric camera tracking, and monocular depth estimation from egocentric video, and also serves as a generative model for conditional egocentric video synthesis. Across these tasks, EgoM2P matches or outperforms specialist models while being an order of magnitude faster. We will fully open-source EgoM2P to support the community and advance egocentric vision research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gen Li",
      "Yutong Chen",
      "Yiqian Wu",
      "Kaifeng Zhao",
      "Marc Pollefeys",
      "Siyu Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Conti_On_Large_Multimodal_Models_as_Open-World_Image_Classifiers_ICCV_2025_paper.html": {
    "title": "On Large Multimodal Models as Open-World Image Classifiers",
    "volume": "main",
    "abstract": "Traditional image classification requires a predefined list of semantic categories. In contrast, Large Multimodal Models (LMMs) can sidestep this requirement by classifying images directly using natural language (e.g., answering the prompt \"What is the main object in the image?\"). Despite this remarkable capability, most existing studies on LMM classification performance are surprisingly limited in scope, often assuming a closed-world setting with a predefined set of categories. In this work, we address this gap by thoroughly evaluating LMM classification performance in a truly open-world setting. We first formalize the task and introduce an evaluation protocol, defining various metrics to assess the alignment between predicted and ground truth classes. We then evaluate 13 models across 10 benchmarks, encompassing prototypical, non-prototypical, fine-grained, and very fine-grained classes, demonstrating the challenges LMMs face in this task. Further analyses based on the proposed metrics reveal the types of errors LMMs make, highlighting challenges related to granularity and fine-grained capabilities, showing how tailored prompting and reasoning can alleviate them",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alessandro Conti",
      "Massimiliano Mancini",
      "Enrico Fini",
      "Yiming Wang",
      "Paolo Rota",
      "Elisa Ricci"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_Adaptive_Hyper-Graph_Convolution_Network_for_Skeleton-based_Human_Action_Recognition_with_ICCV_2025_paper.html": {
    "title": "Adaptive Hyper-Graph Convolution Network for Skeleton-based Human Action Recognition with Virtual Connections",
    "volume": "main",
    "abstract": "The shared topology of human skeletons motivated the recent investigation of graph convolutional network (GCN) solutions for action recognition.However, most of the existing GCNs rely on the binary connection of two neighboring vertices (joints) formed by an edge (bone), overlooking the potential of constructing multi-vertex convolution structures.Although some studies have attempted to utilize hyper-graphs to represent the topology, they rely on a fixed construction strategy, which limits their adaptivity in uncovering the intricate latent relationships within the action.In this paper, we address this oversight and explore the merits of an adaptive hyper-graph convolutional network (Hyper-GCN) to achieve the aggregation of rich semantic information conveyed by skeleton vertices.In particular, our Hyper-GCN adaptively optimises the hyper-graphs during training, revealing the action-driven multi-vertex relations. Besides, virtual connections are often designed to support efficient feature aggregation, implicitly extending the spectrum of dependencies within the skeleton.By injecting virtual connections into hyper-graphs, the semantic clues of diverse action categories can be highlighted. The results of experiments conducted on the NTU-60, NTU-120, and NW-UCLA datasets demonstrate the merits of our Hyper-GCN, compared to the state-of-the-art methods. The code is available at https://github.com/6UOOON9/Hyper-GCN",
    "checked": true,
    "id": "cf634693873d13118793faef97aa791360db8f83",
    "semantic_title": "adaptive hyper-graph convolution network for skeleton-based human action recognition with virtual connections",
    "citation_count": 0,
    "authors": [
      "Youwei Zhou",
      "Tianyang Xu",
      "Cong Wu",
      "Xiaojun Wu",
      "Josef Kittler"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shiba_Simultaneous_Motion_And_Noise_Estimation_with_Event_Cameras_ICCV_2025_paper.html": {
    "title": "Simultaneous Motion And Noise Estimation with Event Cameras",
    "volume": "main",
    "abstract": "Event cameras are emerging vision sensors whose noise is challenging to characterize. Existing denoising methods for event cameras are often designed in isolation and thus consider other tasks, such as motion estimation, separately (i.e., sequentially after denoising). However, motion is an intrinsic part of event data, since scene edges cannot be sensed without motion. We propose, to the best of our knowledge, the first method that simultaneously estimates motion in its various forms (e.g., ego-motion, optical flow) and noise. The method is flexible, as it allows replacing the one-step motion estimation of the widely-used Contrast Maximization framework with any other motion estimator, such as deep neural networks. The experiments show that the proposed method achieves state-of-the-art results on the E-MLB denoising benchmark and competitive results on the DND21 benchmark, while demonstrating effectiveness across motion estimation and intensity reconstruction tasks. Our approach advances event-data denoising theory and expands practical denoising use-cases via open-source code. Project page: https://github.com/tub-rip/ESMD",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shintaro Shiba",
      "Yoshimitsu Aoki",
      "Guillermo Gallego"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Grader_Supercharging_Floorplan_Localization_with_Semantic_Rays_ICCV_2025_paper.html": {
    "title": "Supercharging Floorplan Localization with Semantic Rays",
    "volume": "main",
    "abstract": "Floorplans provide a compact representation of the building's structure, revealing not only layout information but also detailed semantics such as the locations of windows and doors. However, contemporary floorplan localization techniques mostly focus on matching depth-based structural cues, ignoring the rich semantics communicated within floorplans. In this work, we introduce a semantic-aware localization framework that jointly estimates depth and semantic rays, consolidating over both for predicting a structural-semantic probability volume. Our probability volume is constructed in a coarse-to-fine manner: We first sample a small set of rays to obtain an initial low-resolution probability volume. We then refine these probabilities by performing a denser sampling only in high-probability regions and process the refined values for predicting a 2D location and orientation angle. We conduct an evaluation on two standard floorplan localization benchmarks. Our experiments demonstrate that our approach substantially outperforms state-of-the-art methods, achieving significant improvements in recall metrics compared to prior works. Moreover, we show that our framework can easily incorporate additional metadata such as room labels, enabling additional gains in both accuracy and efficiency",
    "checked": true,
    "id": "f8e491d0c8d2f2838641deed63e4c36bcf08d119",
    "semantic_title": "supercharging floorplan localization with semantic rays",
    "citation_count": 1,
    "authors": [
      "Yuval Grader",
      "Hadar Averbuch-Elor"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sun_MDP3_A_Training-free_Approach_for_List-wise_Frame_Selection_in_Video-LLMs_ICCV_2025_paper.html": {
    "title": "MDP3: A Training-free Approach for List-wise Frame Selection in Video-LLMs",
    "volume": "main",
    "abstract": "Video large language models (Video-LLMs) have made significant progress in understanding videos. However, processing multiple frames leads to lengthy visual token sequences, presenting challenges such as the limited context length cannot accommodate the entire video, and the inclusion of irrelevant frames hinders visual perception. Hence, effective frame selection is crucial. This paper emphasizes that frame selection should follow three key principles: query relevance, list-wise diversity, and sequentiality. Existing methods, such as uniform frame sampling and query-frame matching, do not capture all of these principles. Thus, we propose Markov decision determinantal point process with dynamic programming (MDP^3) for frame selection, a training-free and model-agnostic method that can be seamlessly integrated into existing Video-LLMs. Our method first estimates frame similarities conditioned on the query using a conditional Gaussian kernel within the reproducing kernel Hilbert space (RKHS). We then apply the determinantal point process (DPP) to the similarity matrix to capture both query relevance and list-wise diversity. To incorporate sequentiality, we segment the video and apply DPP within each segment, conditioned on the preceding segment selection, modeled as a Markov decision process (MDP) for allocating selection sizes across segments. Theoretically, MDP^3 provides a (1-1/e)-approximate solution to the NP-hard list-wise frame selection problem with pseudo-polynomial time complexity, demonstrating its efficiency. Empirically, MDP significantly outperforms existing methods, verifying its effectiveness and robustness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hui Sun",
      "Shiyin Lu",
      "Huanyu Wang",
      "Qing-Guo Chen",
      "Zhao Xu",
      "Weihua Luo",
      "Kaifu Zhang",
      "Ming Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hong_General_Compression_Framework_for_Efficient_Transformer_Object_Tracking_ICCV_2025_paper.html": {
    "title": "General Compression Framework for Efficient Transformer Object Tracking",
    "volume": "main",
    "abstract": "Previous works have attempted to improve tracking efficiency through lightweight architecture design or knowledge distillation from teacher models to compact student trackers. However, these solutions often sacrifice accuracy for speed to a great extent, and also have the problems of complex training process and structural limitations. Thus, we propose a general model compression framework for efficient transformer object tracking, named CompressTracker, to reduce model size while preserving tracking accuracy. Our approach features a novel stage division strategy that segments the transformer layers of the teacher model into distinct stages to break the limitation of model structure. Additionally, we also design a unique replacement training technique that randomly substitutes specific stages in the student model with those from the teacher model, as opposed to training the student model in isolation. Replacement training enhances the student model's ability to replicate the teacher model's behavior and simplifies the training process. To further forcing student model to emulate teacher model, we incorporate prediction guidance and stage-wise feature mimicking to provide additional supervision during the teacher model's compression process. CompressTracker is structurally agnostic, making it compatible with any transformer architecture. We conduct a series of experiment to verify the effectiveness and generalizability of our CompressTracker. Our CompressTracker-SUTrack, compressed from SUTrack, retains about 99% performance on LaSOT (72.2% AUC) while achieves 2.42x speed up. Code is available at https://github.com/LingyiHongfd/CompressTracker",
    "checked": true,
    "id": "e3d128c4a43fe2d5a9c8e8657cd5926c53197ba8",
    "semantic_title": "general compression framework for efficient transformer object tracking",
    "citation_count": 1,
    "authors": [
      "Lingyi Hong",
      "Jinglun Li",
      "Xinyu Zhou",
      "Shilin Yan",
      "Pinxue Guo",
      "Kaixun Jiang",
      "Zhaoyu Chen",
      "Shuyong Gao",
      "Runze Li",
      "Xingdong Sheng",
      "Wei Zhang",
      "Hong Lu",
      "Wenqiang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_Toward_Long-Tailed_Online_Anomaly_Detection_through_Class-Agnostic_Concepts_ICCV_2025_paper.html": {
    "title": "Toward Long-Tailed Online Anomaly Detection through Class-Agnostic Concepts",
    "volume": "main",
    "abstract": "Anomaly detection (AD) identifies the defect regions of a given image. Recent works have studied AD, focusing on learning AD without abnormal images, with long-tailed distributed training data, and using a unified model for all classes. In addition, online AD learning has also been explored. In this work, we expand in both directions to a realistic setting by considering the new novel task of long-tailed online AD (LTOAD). We first identified that the offline state-of-the-art LTAD methods cannot be directly applied to the online setting. Specifically, LTAD is class-aware, requiring class labels that are not available in the online setting. To address this challenge, we propose a class-agnostic framework for LTAD and then adapt it to our online learning setting. Our method outperforms the SOTA baselines in most offline LTAD settings, including both the industrial manufacturing and the medical domain. In particular, we observe +4.63% image-AUROC on MVTec even compared to methods that have access to class labels and the number of classes. In the most challenging long-tailed online setting, we achieve +0.53% image-AUROC compared to baselines",
    "checked": true,
    "id": "74c3f5e4389f9f8cd65ea4549c7ecd40dcf5daf7",
    "semantic_title": "toward long-tailed online anomaly detection through class-agnostic concepts",
    "citation_count": 0,
    "authors": [
      "Chiao-An Yang",
      "Kuan-Chuan Peng",
      "Raymond A. Yeh"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lu_CIARD_Cyclic_Iterative_Adversarial_Robustness_Distillation_ICCV_2025_paper.html": {
    "title": "CIARD: Cyclic Iterative Adversarial Robustness Distillation",
    "volume": "main",
    "abstract": "Adversarial robustness distillation (ARD) aims to transfer both performance and robustness from teacher model to lightweight student model, enabling resilient performance on resource-constrained scenarios. Though existing ARD approaches enhance student model's robustness, the inevitable by-product leads to the degraded performance on clean examples. We summarize the causes of this problem inherent in existing methods with dual-teacher framework as: 1 The divergent optimization objectives of dual-teacher models, i.e., the clean and robust teachers, impede effective knowledge transfer to the student model, and 2 The iteratively generated adversarial examples during training lead to performance deterioration of the robust teacher model. To address these challenges, we propose a novel Cyclic Iterative ARD (CIARD) method with two key innovations: 1 A multi-teacher framework with contrastive push-loss alignment to resolve conflicts in dual-teacher optimization objectives, and 2 Continuous adversarial retraining to maintain dynamic teacher robustness against performance degradation from the varying adversarial examples. Extensive experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet demonstrate that CIARD achieves remarkable performance with an average 3.53% improvement in adversarial defense rates across various attack scenarios and a 5.87% increase in clean sample accuracy, establishing a new benchmark for balancing model robustness and generalization. Our code is available at https://github.com/CIARD2025/CIARD",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liming Lu",
      "Shuchao Pang",
      "Xu Zheng",
      "Xiang Gu",
      "Anan Du",
      "Yunhuai Liu",
      "Yongbin Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Di_DiffTell_A_High-Quality_Dataset_for_Describing_Image_Manipulation_Changes_ICCV_2025_paper.html": {
    "title": "DiffTell: A High-Quality Dataset for Describing Image Manipulation Changes",
    "volume": "main",
    "abstract": "The image difference captioning (IDC) task is to describe the distinctions between two images. However, existing datasets do not offer comprehensive coverage across all image-difference categories. In this work, we introduce a high-quality dataset, DiffTell with various types of image manipulations, including global image alterations, object-level changes, and text manipulations. The data quality is controlled by careful human filtering. Additionally, to scale up the data collection without prohibitive human labor costs, we explore the possibility of automatically filtering for quality control. We demonstrate that both traditional methods and recent multimodal large language models (MLLMs) exhibit performance improvements on the IDC task after training on the DiffTell dataset. Through extensive ablation studies, we provide a detailed analysis of the performance gains attributed to DiffTell. Experiments show DiffTell significantly enhances the availability of resources for IDC research, offering a more comprehensive foundation and benchmark for future investigations",
    "checked": false,
    "id": "8a83a00a8fff0dd4d9878023fdc0f5b0121cbe01",
    "semantic_title": "generate and analyze three‐dimensional dendritic spine morphology datasets with spinetool software",
    "citation_count": 1,
    "authors": [
      "Zonglin Di",
      "Jing Shi",
      "Yifei Fan",
      "Hao Tan",
      "Alexander Black",
      "John Collomosse",
      "Yang Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_A_Plug-and-Play_Physical_Motion_Restoration_Approach_for_In-the-Wild_High-Difficulty_Motions_ICCV_2025_paper.html": {
    "title": "A Plug-and-Play Physical Motion Restoration Approach for In-the-Wild High-Difficulty Motions",
    "volume": "main",
    "abstract": "Extracting physically plausible 3D human motion from videos is a critical task. Although existing simulation-based motion imitation methods can enhance the physical quality of daily motions estimated from monocular video capture, extending this capability to high-difficulty motions remains an open challenge. This can be attributed to some flawed motion clips in video-based motion capture results and the inherent complexity in modeling high-difficulty motions. Therefore, sensing the advantage of segmentation in localizing human body, we introduce a mask-based motion correction module (MCM) that leverages motion context and video mask to repair flawed motions; and propose a physics-based motion transfer module (PTM), which employs a prior injected pretrain and adapt approach for motion imitation, improving physical plausibility with the ability to handle in-the-wild and challenging motions. Our approach is designed as a plug-and-play module to physically refine the video motion capture, which also excels in motion generation tasks. Finally, we collected a challenging in-the-wild test set to establish a benchmark, and our method has demonstrated effectiveness on both the new benchmark and existing public datasets. Our project page is : https://physicalmotionrestoration.github.io/",
    "checked": true,
    "id": "b8f2b63d350c88549db85b660f3e26cc63661ee1",
    "semantic_title": "a plug-and-play physical motion restoration approach for in-the-wild high-difficulty motions",
    "citation_count": 0,
    "authors": [
      "Youliang Zhang",
      "Ronghui Li",
      "Yachao Zhang",
      "Liang Pan",
      "Jingbo Wang",
      "Yebin Liu",
      "Xiu Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xu_Local_Dense_Logit_Relations_for_Enhanced_Knowledge_Distillation_ICCV_2025_paper.html": {
    "title": "Local Dense Logit Relations for Enhanced Knowledge Distillation",
    "volume": "main",
    "abstract": "State-of-the-art logit distillation methods exhibit versatility, simplicity, and efficiency.Despite the advances, existing studies have yet to delve thoroughly into fine-grained relationships within logit knowledge.In this paper, we propose Local Dense Relational Logit Distillation (LDRLD), a novel method that captures inter-class relationships through recursively decoupling and recombining logit information, thereby providing more detailed and clearer insights for student learning.To further optimize the performance, we introduce an Adaptive Decay Weight (ADW) strategy, which can dynamically adjust the weights for critical category pairs using Inverse Rank Weighting (IRW) and Exponential Rank Decay (ERD).Specifically, IRW assigns weights inversely proportional to the rank differences between pairs, while ERD adaptively controls weight decay based on total ranking scores of category pairs. Furthermore, after the recursive decoupling, we distill the remaining non-target knowledge to ensure knowledge completeness and enhance performance. Ultimately, our method improves the student's performance by transferring fine-grained knowledge and emphasizing the most critical relationships.Extensive experiments on datasets such as CIFAR-100, ImageNet-1K, and Tiny-ImageNet demonstrate that our method compares favorably with state-of-the-art logit-based knowledge distillation approaches. The code will be made publicly available",
    "checked": true,
    "id": "bae290f73b0020a43d2258c2f93582926eebd582",
    "semantic_title": "local dense logit relations for enhanced knowledge distillation",
    "citation_count": 1,
    "authors": [
      "Liuchi Xu",
      "Kang Liu",
      "Jinshuai Liu",
      "Lu Wang",
      "Lisheng Xu",
      "Jun Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Less_is_More_Empowering_GUI_Agent_with_Context-Aware_Simplification_ICCV_2025_paper.html": {
    "title": "Less is More: Empowering GUI Agent with Context-Aware Simplification",
    "volume": "main",
    "abstract": "The research focus of GUI agents is shifting from text-dependent to pure-vision-based approaches, which, though promising, prioritize comprehensive pre-training data collection while neglecting contextual modeling challenges. We probe the characteristics of element and history contextual modeling in GUI agents and summarize: **1) the high-density and loose-relation of element context** highlight the existence of many unrelated elements and their negative influence; **2) the high redundancy of history context** reveals the inefficient history modeling in current GUI agents. In this work, we propose a context-aware simplification framework for building an efficient and effective GUI Agent, termed **SimpAgent**. To mitigate potential interference from numerous unrelated elements, we introduce a **masking-based element pruning** method that circumvents the intractable relation modeling through an efficient masking mechanism. To reduce the redundancy in historical information, we devise a **consistency-guided history compression** module, which enhances implicit LLM-based compression through innovative explicit guidance, achieving an optimal balance between performance and efficiency. With the above components, SimpAgent reduces 27% FLOPs and achieves superior GUI navigation performances. Comprehensive navigation experiments across diverse web and mobile environments demonstrate the effectiveness and potential of our agent",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gongwei Chen",
      "Xurui Zhou",
      "Rui Shao",
      "Yibo Lyu",
      "Kaiwen Zhou",
      "Shuai Wang",
      "Wentao Li",
      "Yinchuan Li",
      "Zhongang Qi",
      "Liqiang Nie"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xia_TrafficLoc_Localizing_Traffic_Surveillance_Cameras_in_3D_Scenes_ICCV_2025_paper.html": {
    "title": "TrafficLoc: Localizing Traffic Surveillance Cameras in 3D Scenes",
    "volume": "main",
    "abstract": "We tackle the problem of localizing traffic cameras within a 3D reference map and propose a novel image-to-point cloud registration (I2P) method, TrafficLoc, in a coarse-to-fine matching fashion. To overcome the lack of large-scale real-world intersection datasets, we first introduce Carla Intersection, a new simulated dataset with 75 urban and rural intersections in Carla. We find that current I2P methods struggle with cross-modal matching under large viewpoint differences, especially at traffic intersections. TrafficLoc thus employs a novel Geometry-guided Attention Loss (GAL) to focus only on the corresponding geometric regions under different viewpoints during 2D-3D feature fusion. To address feature inconsistency in paired image patch-point groups, we further propose Inter-intra Contrastive Learning (ICL) to enhance separating 2D patch / 3D group features within each intra-modality and introduce Dense Training Alignment (DTA) with soft-argmax for improving position regression. Extensive experiments show our TrafficLoc greatly improves the performance over the SOTA I2P methods (up to 86%) on Carla Intersection and generalizes well to real-world data. TrafficLoc also achieves new SOTA performance on KITTI and NuScenes datasets, demonstrating the superiority across both in-vehicle and traffic cameras. Our project page is publicly available at https://tum-luk.github.io/projects/trafficloc/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Xia",
      "Yunxiang Lu",
      "Rui Song",
      "Oussema Dhaouadi",
      "João F. Henriques",
      "Daniel Cremers"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Detect_Anything_3D_in_the_Wild_ICCV_2025_paper.html": {
    "title": "Detect Anything 3D in the Wild",
    "volume": "main",
    "abstract": "Despite the success of deep learning in close-set 3D object detection, existing approaches struggle with zero-shot generalization to novel objects and camera configurations. We introduce DetAny3D, a promptable 3D detection foundation model capable of detecting any novel object under arbitrary camera configurations using only monocular inputs. Training a foundation model for 3D detection is fundamentally constrained by the limited availability of annotated 3D data, which motivates DetAny3D to leverage the rich prior knowledge embedded in extensively pre-trained 2D foundation models to compensate for this scarcity. To effectively transfer 2D knowledge to 3D, DetAny3D incorporates two core modules: the 2D Aggregator, which aligns features from different 2D foundation models, and the 3D Interpreter with Zero-Embedding Mapping, which stabilizes early training in 2D-to-3D knowledge transfer. Experimental results validate the strong generalization of our DetAny3D, which not only achieves state-of-the-art performance on unseen categories and novel camera configurations, but also surpasses most competitors on in-domain data. DetAny3D sheds light on the potential of the 3D foundation model for diverse applications in real-world scenarios, e.g., rare object detection in autonomous driving, and demonstrates promise for further exploration of 3D-centric tasks in open-world settings. More visualization results can be found at our code repository",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanxue Zhang",
      "Haoran Jiang",
      "Qingsong Yao",
      "Yanan Sun",
      "Renrui Zhang",
      "Hao Zhao",
      "Hongyang Li",
      "Hongzi Zhu",
      "Zetong Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Failure_Cases_Are_Better_Learned_But_Boundary_Says_Sorry_Facilitating_ICCV_2025_paper.html": {
    "title": "Failure Cases Are Better Learned But Boundary Says Sorry: Facilitating Smooth Perception Change for Accuracy-Robustness Trade-Off in Adversarial Training",
    "volume": "main",
    "abstract": "Adversarial Training (AT) is one of the most effective methods to train robust Deep Neural Networks (DNNs). However, AT creates an inherent trade-off between clean accuracy and adversarial robustness, which is commonly attributed to the more complicated decision boundary caused by the insufficient learning of hard adversarial samples. In this work, we reveal a counterintuitive fact for the first time: **From the perspective of perception consistency, hard adversarial samples that can still attack the robust model after AT are already learned better than those successfully defended**. Thus, different from previous views, we argue that it is rather the over-sufficient learning of hard adversarial samples that degrades the decision boundary and contributes to the trade-off problem. Specifically, the excessive pursuit of perception consistency would force the model to view the perturbations as noise and ignore the information within them, which should have been utilized to induce a smoother perception transition towards the decision boundary to support its establishment to an appropriate location. In response, we define a new AT objective named **Robust Perception**, encouraging the model perception to change smoothly with input perturbations, based on which we propose a novel **R**obust **P**erception **A**dversarial **T**raining (**RPAT**) method, effectively mitigating the current accuracy-robustness trade-off. Experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet with ResNet-18, PreActResNet-18, and WideResNet-34-10 demonstrate the effectiveness of our method beyond four common baselines and 12 state-of-the-art (SOTA) works. The code is available at https://github.com/FlaAI/RPAT",
    "checked": true,
    "id": "c768d0a2a94b6f1c3937bccf6f402d98e17f17b2",
    "semantic_title": "failure cases are better learned but boundary says sorry: facilitating smooth perception change for accuracy-robustness trade-off in adversarial training",
    "citation_count": 0,
    "authors": [
      "Yanyun Wang",
      "Li Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cheng_CA-I2P_Channel-Adaptive_Registration_Network_with_Global_Optimal_Selection_ICCV_2025_paper.html": {
    "title": "CA-I2P: Channel-Adaptive Registration Network with Global Optimal Selection",
    "volume": "main",
    "abstract": "Detection-free methods typically follow a coarse-to-fine pipeline, extracting image and point cloud features for patch-level matching and refining dense pixel-to-point correspondences. However, differences in feature channel attention between images and point clouds may lead to degraded matching results, ultimately impairing registration accuracy.Furthermore, similar structures in the scene could lead to redundant correspondences in cross-modal matching.To address these issues, we propose Channel Adaptive Adjustment Module (CAA) and Global Optimal Selection Module (GOS). CAA enhances intra-modal features and suppresses cross-modal sensitivity, while GOS replaces local selection with global optimization. Experiments on RGB-D Scenes V2 and 7-Scenes demonstrate the superiority of our method, achieving state-of-the-art performance in image-to-point cloud registration",
    "checked": true,
    "id": "12ca2066e63372153dac110e5d8564f05ea5ffa1",
    "semantic_title": "ca-i2p: channel-adaptive registration network with global optimal selection",
    "citation_count": 1,
    "authors": [
      "Zhixin Cheng",
      "Jiacheng Deng",
      "Xinjun Li",
      "Xiaotian Yin",
      "Bohao Liao",
      "Baoqun Yin",
      "Wenfei Yang",
      "Tianzhu Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Exploiting_Vision_Language_Model_for_Training-Free_3D_Point_Cloud_OOD_ICCV_2025_paper.html": {
    "title": "Exploiting Vision Language Model for Training-Free 3D Point Cloud OOD Detection via Graph Score Propagation",
    "volume": "main",
    "abstract": "Out-of-distribution (OOD) detection in 3D point cloud data remains a challenge, particularly in applications where safe and robust perception is critical. While existing OOD detection methods have shown progress for 2D image data, extending these to 3D environments involves unique obstacles. This paper introduces a training-free framework that leverages Vision-Language Models (VLMs) for effective OOD detection in 3D point clouds. By constructing a graph based on class prototypes and testing data, we exploit the data manifold structure to enhancing the effectiveness of VLMs for 3D OOD detection. We propose a novel Graph Score Propagation (GSP) method that incorporates prompt clustering and self-training negative prompting to improve OOD scoring with VLM. Our method is also adaptable to few-shot scenarios, providing options for practical applications. We demonstrate that GSP consistently outperforms state-of-the-art methods across synthetic and real-world datasets for 3D point cloud OOD detection",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tiankai Chen",
      "Yushu Li",
      "Adam Goodge",
      "Fei Teng",
      "Xulei Yang",
      "Tianrui Li",
      "Xun Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xu_Monocular_Facial_Appearance_Capture_in_the_Wild_ICCV_2025_paper.html": {
    "title": "Monocular Facial Appearance Capture in the Wild",
    "volume": "main",
    "abstract": "We present a new method for reconstructing the appearance properties of human faces from a lightweight capture procedure in an unconstrained environment. Our method recovers the surface geometry, diffuse albedo, specular intensity and specular roughness from a monocular video containing a simple head rotation in-the-wild. Notably, we make no simplifying assumptions on the environment lighting, and we explicitly take visibility and occlusions into account. As a result, our method can produce facial appearance maps that approach the fidelity of studio-based multi-view captures, but with a far easier and cheaper procedure",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingyan Xu",
      "Kate Gadola",
      "Prashanth Chandran",
      "Sebastian Weiss",
      "Markus Gross",
      "Gaspard Zoss",
      "Derek Bradley"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Gaussian_Variation_Field_Diffusion_for_High-fidelity_Video-to-4D_Synthesis_ICCV_2025_paper.html": {
    "title": "Gaussian Variation Field Diffusion for High-fidelity Video-to-4D Synthesis",
    "volume": "main",
    "abstract": "In this paper, we present a novel framework for video-to-4D generation that creates high-quality dynamic 3D content from single video inputs. Direct 4D diffusion modeling is extremely challenging due to costly data construction and the high-dimensional nature of jointly representing 3D shape, appearance, and motion. We address these challenges by introducing a Direct 4DMesh-to-GS Variation Field VAE that directly encodes canonical Gaussian Splats (GS) and their temporal variations from 3D animation data without per-instance fitting, and compresses high-dimensional animations into a compact latent space. Building upon this efficient representation, we train a Gaussian Variation Field diffusion model with temporal-aware Diffusion Transformer conditioned on input videos and canonical GS. Trained on carefully-curated animatable 3D objects from the Objaverse dataset, our model demonstrates superior generation quality compared to existing methods. It also exhibits remarkable generalization to in-the-wild video inputs despite being trained exclusively on synthetic data, paving the way for generating high-quality animated 3D content. Project page: https://gvfdiffusion.github.io/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowen Zhang",
      "Sicheng Xu",
      "Chuxin Wang",
      "Jiaolong Yang",
      "Feng Zhao",
      "Dong Chen",
      "Baining Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gao_Can3Tok_Canonical_3D_Tokenization_and_Latent_Modeling_of_Scene-Level_3D_ICCV_2025_paper.html": {
    "title": "Can3Tok: Canonical 3D Tokenization and Latent Modeling of Scene-Level 3D Gaussians",
    "volume": "main",
    "abstract": "3D generation has made significant progress, however, it still largely remains at the object-level. Feedforward 3D scene-level generation has been rarely explored due to the lack of models capable of scaling-up latent representation learning on 3D scene-level data. Unlike object-level generative models, which are trained on well-labeled 3D data in a bounded canonical space, scene-level generations with 3D scenes represented by 3D Gaussian Splatting (3DGS) are unbounded and exhibit scale inconsistency across different scenes, making unified latent representation learning for generative purposes extremely challenging. In this paper, we introduce Can3Tok, the first 3D scene-level variational autoencoder (VAE) capable of encoding a large number of Gaussian primitives into a low-dimensional latent embedding, which effectively captures both semantic and spatial information of the inputs. Beyond model design, we propose a general pipeline for 3D scene data processing to address scale inconsistency issue. We validate our method on the recent scene-level 3D dataset DL3DV-10K, where we found that only Can3Tok successfully generalizes to novel 3D scenes, while compared methods fail to converge on even a few hundred scene inputs during training and exhibit zero generalization ability during inference. Finally, we demonstrate image-to-3DGS and text-to-3DGS generation as our applications to demonstrate it's ability to faciliate downstream generation tasks. Code will be released",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quankai Gao",
      "Iliyan Georgiev",
      "Tuanfeng Y. Wang",
      "Krishna Kumar Singh",
      "Ulrich Neumann",
      "Jae Shin Yoon"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Dang_EmotiCrafter_Text-to-Emotional-Image_Generation_based_on_Valence-Arousal_Model_ICCV_2025_paper.html": {
    "title": "EmotiCrafter: Text-to-Emotional-Image Generation based on Valence-Arousal Model",
    "volume": "main",
    "abstract": "Recent research shows that emotions can enhance users' cognition and influence information communication. While research on visual emotion analysis is extensive, limited work has been done on helping users generate emotionally rich image content. Existing work on emotional image generation relies on discrete emotion categories, making it challenging to capture complex and subtle emotional nuances accurately. Additionally, these methods struggle to control the specific content of generated images based on text prompts. In this paper, we introduce the task of continuous emotional image content generation (C-EICG) and present EmotiCrafter, a general emotional image generation model that generates images based on free text prompts and Valence-Arousal (V-A) values. It leverages a novel emotion-embedding mapping network to fuse V-A values into textual features, enabling the capture of emotions in alignment with intended input prompts. A novel loss function is also proposed to enhance emotion expression. The experimental results show that our method effectively generates images representing specific emotions with the desired content and outperforms existing techniques",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengqi Dang",
      "Yi He",
      "Long Ling",
      "Ziqing Qian",
      "Nanxuan Zhao",
      "Nan Cao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_HFD-Teacher_High-Frequency_Depth_Distillation_from_Depth_Foundation_Models_for_Enhanced_ICCV_2025_paper.html": {
    "title": "HFD-Teacher: High-Frequency Depth Distillation from Depth Foundation Models for Enhanced Depth Completion",
    "volume": "main",
    "abstract": "Depth completion, the task of reconstructing dense depth maps from sparse depth and RGB images, plays a critical role in 3D scene understanding. However, existing methods often struggle to recover high-frequency details, such as regions with fine structures or weak signals, since depth sensors may fail to capture accurate depth maps in those regions, leading to imperfect supervision ground truth. To overcome this limitation, it is essential to introduce an alternative training source for the models. Emerging depth foundation models excel at producing high-frequency details from RGB images, yet their depth maps suffer from inconsistent scaling. Therefore, we propose a novel teacher-student framework that enhances depth completion by distilling high-frequency knowledge from depth foundation models across multiple scales. Our approach introduces two key innovations: Adaptive Local Wavelet Decomposition, which dynamically adjusts wavelet decomposition level based on local complexity for efficient feature extraction, and Topological Constraints, which apply persistent homology to enforce structural coherence and suppress spurious depth edges. Experiment results demonstrate that our method outperforms state-of-the-art methods, preserving high-frequency details and overall depth fidelity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyuan Yang",
      "Anqi Cheng",
      "Haiyue Zhu",
      "Tianjiao Li",
      "Pey Yuen Tao",
      "Kezhi Mao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lin_EAMamba_Efficient_All-Around_Vision_State_Space_Model_for_Image_Restoration_ICCV_2025_paper.html": {
    "title": "EAMamba: Efficient All-Around Vision State Space Model for Image Restoration",
    "volume": "main",
    "abstract": "Image restoration is a key task in low-level computer vision that aims to reconstruct high-quality images from degraded inputs. The emergence of Vision Mamba, which draws inspiration from the advanced state space model Mamba, marks a significant advancement in this field. Vision Mamba demonstrates excellence in modeling long-range dependencies with linear complexity, a crucial advantage for image restoration tasks. Despite its strengths, Vision Mamba encounters challenges in low-level vision tasks, including computational complexity that scales with the number of scanning sequences and local pixel forgetting. To address these limitations, this study introduces Efficient All-Around Mamba (EAMamba), an enhanced framework that incorporates a Multi-Head Selective Scan Module (MHSSM) with an all-around scanning mechanism. MHSSM efficiently aggregates multiple scanning sequences, which avoids increases in computational complexity and parameter count. The all-around scanning strategy implements multiple patterns to capture holistic information and resolves the local pixel forgetting issue. Our experimental evaluations validate these innovations across several restoration tasks, including super resolution, denoising, deblurring, and dehazing. The results validate that EAMamba achieves a significant 31-89% reduction in FLOPs while maintaining favorable performance compared to existing low-level Vision Mamba methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu-Cheng Lin",
      "Yu-Syuan Xu",
      "Hao-Wei Chen",
      "Hsien-Kai Kuo",
      "Chun-Yi Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_AHCPTQ_Accurate_and_Hardware-Compatible_Post-Training_Quantization_for_Segment_Anything_Model_ICCV_2025_paper.html": {
    "title": "AHCPTQ: Accurate and Hardware-Compatible Post-Training Quantization for Segment Anything Model",
    "volume": "main",
    "abstract": "The Segment Anything Model (SAM) has demonstrated strong versatility across various visual tasks. However, its large storage requirements and high computational cost pose challenges for practical deployment. Post-training quantization (PTQ) has emerged as an effective strategy for efficient deployment, but we identify two key challenges in SAM that hinder the effectiveness of existing PTQ methods: the heavy-tailed and skewed distribution of post-GELU activations, and significant inter-channel variation in linear projection activations. To address these challenges, we propose AHCPTQ, an accurate and hardware-efficient PTQ method for SAM. AHCPTQ introduces hardware-compatible Hybrid Log-Uniform Quantization (HLUQ) to manage post-GELU activations, employing log2 quantization for dense small values and uniform quantization for sparse large values to enhance quantization resolution. Additionally, AHCPTQ incorporates Channel-Aware Grouping (CAG) to mitigate inter-channel variation by progressively clustering activation channels with similar distributions, enabling them to share quantization parameters and improving hardware efficiency. The combination of HLUQ and CAG not only enhances quantization effectiveness but also ensures compatibility with efficient hardware execution. For instance, under the W4A4 configuration on the SAM-L model, AHCPTQ achieves 36.6% mAP on instance segmentation with the DINO detector, while achieving a 7.89x speedup and 8.64x energy efficiency over its floating-point counterpart in FPGA implementation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenlun Zhang",
      "Yunshan Zhong",
      "Shimpei Ando",
      "Kentaro Yoshioka"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ni_WonderTurbo_Generating_Interactive_3D_World_in_0.72_Seconds_ICCV_2025_paper.html": {
    "title": "WonderTurbo: Generating Interactive 3D World in 0.72 Seconds",
    "volume": "main",
    "abstract": "Interactive 3D generation is gaining momentum and capturing extensive attention for its potential to create immersive virtual experiences. However, a critical challenge in current 3D generation technologies lies in achieving real-time interactivity. To address this issue, we introduce WonderTurbo, the first real-time interactive 3D scene generation framework capable of generating novel perspectives of 3D scenes within 0.72 seconds. Specifically, WonderTurbo accelerates both geometric and appearance modeling in 3D scene generation. In terms of geometry, we propose StepSplat, an innovative method that constructs efficient 3D geometric representations through dynamic updates, each taking only 0.26 seconds. Additionally, we design QuickDepth, a lightweight depth completion module that provides consistent depth input for StepSplat, further enhancing geometric accuracy. For appearance modeling, we develop FastPaint, a 2-steps diffusion model tailored for instant inpainting, which focuses on maintaining spatial appearance consistency. Experimental results demonstrate that WonderTurbo achieves a remarkable 15xspeedup compared to baseline methods, while preserving excellent spatial consistency and delivering high-quality output",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaojun Ni",
      "Xiaofeng Wang",
      "Zheng Zhu",
      "Weijie Wang",
      "Haoyun Li",
      "Guosheng Zhao",
      "Jie Li",
      "Wenkang Qin",
      "Guan Huang",
      "Wenjun Mei"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Learning_Hierarchical_Line_Buffer_for_Image_Processing_ICCV_2025_paper.html": {
    "title": "Learning Hierarchical Line Buffer for Image Processing",
    "volume": "main",
    "abstract": "In recent years, neural networks have achieved significant progress in offline image processing. However, in online scenarios, particularly in on-chip implementations, memory usage emerges as a critical bottleneck due to the limited memory resources of integrated image processors. In this study, we focus on reducing the memory footprint of neural networks for on-chip image processing by optimizing network design for efficient memory utilization. Specifically, we consider a typical scenario in which images outputted from an image sensor are processed sequentially using line buffers in a line-by-line manner. This setting necessitates the modeling of both intra-line and inter-line correlations--capturing dependencies among pixels within a single line group and across different line groups, respectively.To model intra-line correlations, we propose a progressive feature enhancement strategy, where line pixels are processed with expanding strip convolutions in multiple stages.For inter-line correlation modeling, we introduce a hierarchical line buffer formulation, where features extracted from previous lines are incrementally reused and compressed across multiple hierarchical levels.Comprehensive experiments on various image processing tasks, including RAW denoising, Gaussian denoising, and super-resolution, demonstrate that the proposed method achieves a superior trade-off between performance and memory efficiency than previous solutions, e.g., up to 1dB PSNR gain in RAW denoising at one-fifth of peak memory usage",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiacheng Li",
      "Feiran Li",
      "Daisuke Iso"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chowdhury_Looking_in_the_Mirror_A_Faithful_Counterfactual_Explanation_Method_for_ICCV_2025_paper.html": {
    "title": "Looking in the Mirror: A Faithful Counterfactual Explanation Method for Interpreting Deep Image Classification Models",
    "volume": "main",
    "abstract": "Counterfactual explanations (CFE) for deep image classifiers aim to reveal how minimal input changes lead to different model decisions, providing critical insights for model interpretation and improvement. However, existing CFE methods often rely on additional image encoders and generative models to create plausible images, neglecting the classifier's own feature space and decision boundaries. As such, they do not explain the intrinsic feature space and decision boundaries learned by the classifier. To address this limitation, we propose Mirror-CFE, a novel method that generates faithful counterfactual explanations by operating directly in the classifier's feature space, treating decision boundaries as mirrors that \"reflect\" feature representations in the mirror. Mirror-CFE learns a mapping function from feature space to image space while preserving distance relationships, enabling smooth transitions between source images and their counterfactuals. Through extensive experiments on four image datasets, we demonstrate that Mirror-CFE achieves superior performance in validity while maintaining input resemblance compared to state-of-the-art explanation methods. Finally, mirror-CFE provides interpretable visualization of the classifier's decision process by generating step-wise transitions that reveal how features evolve as classification confidence changes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Townim Chowdhury",
      "Vu Minh Hieu Phan",
      "Kewen Liao",
      "Nanyu Dong",
      "Minh-Son To",
      "Anton van den Hengel",
      "Johan W. Verjans",
      "Zhibin Liao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Park_Inference-Time_Diffusion_Model_Distillation_ICCV_2025_paper.html": {
    "title": "Inference-Time Diffusion Model Distillation",
    "volume": "main",
    "abstract": "Diffusion distillation models effectively accelerate reverse sampling by compressing the process into fewer steps. However, these models still exhibit a performance gap compared to their pre-trained diffusion model counterparts, exacerbated by distribution shifts and accumulated errors during multi-step sampling. To address this, we introduce Distillation++, a novel inference-time distillation framework that reduces this gap by incorporating teacher-guided refinement during sampling. Inspired by recent advances in conditional sampling, our approach recasts student model sampling as a proximal optimization problem with a score distillation sampling loss (SDS). To this end, we integrate distillation optimization during reverse sampling, which can be viewed as teacher guidance that drives student sampling trajectory towards the clean manifold using pre-trained diffusion models. Thus, Distillation++ improves the denoising process in real-time without additional source data or fine-tuning. Distillation++ demonstrates substantial improvements over state-of-the-art distillation baselines, particularly in early sampling stages, positioning itself as a robust guided sampling process crafted for diffusion distillation models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Geon Yeong Park",
      "Sang Wan Lee",
      "Jong Chul Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_S3E_Self-Supervised_State_Estimation_for_Radar-Inertial_System_ICCV_2025_paper.html": {
    "title": "S3E: Self-Supervised State Estimation for Radar-Inertial System",
    "volume": "main",
    "abstract": "Millimeter-wave radar for state estimation is gaining significant attention for its affordability and reliability in harsh conditions. Existing localization solutions typically rely on post-processed radar point clouds as landmark points. Nonetheless, the inherent sparsity of radar point clouds, ghost points from multi-path effects, and limited angle resolution in single-chirp radar severely degrade state estimation performance. To address these issues, we propose S^3E, a Self-Supervised State Estimator that employs more richly informative radar signal spectra to bypass sparse points and fuses complementary inertial information to achieve accurate localization. S^3E fully explores the association between exteroceptive radar and proprioceptive inertial sensor to achieve complementary benefits. To deal with limited angle resolution, we introduce a novel cross-fusion technique that enhances spatial structure information by exploiting subtle rotational shift correlations across heterogeneous data. The experimental results demonstrate our method achieves robust and accurate performance without relying on localization ground truth supervision. To the best of our knowledge, this is the first attempt to achieve state estimation by fusing radar spectra and inertial data in a complementary self-supervised manner",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengpeng Wang",
      "Yulong Xie",
      "Qing Liao",
      "Wei Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Rethinking_Key-frame-based_Micro-expression_Recognition_A_Robust_and_Accurate_Framework_Against_ICCV_2025_paper.html": {
    "title": "Rethinking Key-frame-based Micro-expression Recognition: A Robust and Accurate Framework Against Key-frame Errors",
    "volume": "main",
    "abstract": "Micro-expression recognition (MER) is a highly challenging task in affective computing. With the reduced-sized micro-expression (ME) input that contains key information based on key-frame indexes, key-frame-based methods have significantly improved the performance of MER. However, most of these methods focus on improving the performance with relatively accurate key-frame indexes, while ignoring the difficulty of obtaining accurate key-frame indexes and the objective existence of key-frame index errors, which impedes them from moving towards practical applications. In this paper, we propose CausalNet, a novel framework to achieve robust MER facing key-frame index errors while maintaining accurate recognition. To enhance robustness, CausalNet takes the representation of the entire ME sequence as the input. To address the information redundancy brought by the complete ME range input and maintain accurate recognition, first, the Causal Motion Position Learning Module (CMPLM) is proposed to help the model locate the muscle movement areas related to Action Units (AUs), thereby reducing the attention to other redundant areas. Second, the Causal Attention Block (CAB) is proposed to deeply learn the causal relationships between the muscle contraction and relaxation movements in MEs. Empirical experiments have demonstrated that on popular ME benchmarks, the CausalNet has achieved robust MER under different levels of key-frame index noise. Meanwhile, it has surpassed state-of-the-art (SOTA) methods on several standard MER benchmarks when using the provided annotated key-frames. Code is available at https://github.com/tony19980810/CausalNet",
    "checked": true,
    "id": "e2651adb399bb15d8d7f5d86c2e58066983e2d7d",
    "semantic_title": "rethinking key-frame-based micro-expression recognition: a robust and accurate framework against key-frame errors",
    "citation_count": 0,
    "authors": [
      "Zheyuan Zhang",
      "Weihao Tang",
      "Hong Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jang_Target_Bias_Is_All_You_Need_Zero-Shot_Debiasing_of_Vision-Language_ICCV_2025_paper.html": {
    "title": "Target Bias Is All You Need: Zero-Shot Debiasing of Vision-Language Models with Bias Corpus",
    "volume": "main",
    "abstract": "Vision-Language Models (VLMs) like CLIP have shown remarkable zero-shot performance by aligning different modalities in the embedding space, enabling diverse applications from image editing to visual question answering (VQA). However, these models often inherit biases from their training data, resulting in performance disparities across specific subpopulations. Traditional debiasing methods for VLMs primarily focus on specific downstream tasks using labeled datasets, which we argue is insufficient given the broad applicability of VLMs. Specifically, these methods struggle with generalizability, transferability, and feasibility due to overfitting, limited task applicability, and regulatory constraints on the use of sensitive data, making them less practical in real-world scenarios. To address these challenges, we propose a novel task-agnostic method for learning debiased image embeddings in VLMs. Our approach does not require expensive annotated datasets or curated prompts for downstream tasks, while still preserving the inherent zero-shot capabilities of these models. Instead, we leverage easily accessible information: 1) a bias text corpus generated by a large language model, and 2) a generic unsupervised vision dataset. Our method disentangles the image embedding into bias and neutral components by applying centered kernel alignment (CKA) regularization to the text-vision representational similarity, using the bias text corpus over the generic vision dataset. Experimental results validate the effectiveness of our approach across multiple tasks, offering a practical and versatile solution to debiasing VLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taeuk Jang",
      "Hoin Jung",
      "Xiaoqian Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yu_Mastering_Collaborative_Multi-modal_Data_Selection_A_Focus_on_Informativeness_Uniqueness_ICCV_2025_paper.html": {
    "title": "Mastering Collaborative Multi-modal Data Selection: A Focus on Informativeness, Uniqueness, and Representativeness",
    "volume": "main",
    "abstract": "Instruction tuning fine-tunes pre-trained Multi-modal Large Language Models (MLLMs) to handle real-world tasks. However, the rapid expansion of visual instruction datasets introduces data redundancy, leading to excessive computational costs. We propose a collaborative framework, DataTailor, which leverages three key principles--informativeness, uniqueness, and representativeness--for effective data selection. We argue that a valuable sample should be informative of the task, non-redundant, and represent the sample distribution (i.e., not an outlier). We further propose practical ways to score against each principle, which automatically adapts to a given dataset without tedious hyperparameter tuning. Comprehensive experiments on various benchmarks demonstrate that DataTailor achieves 101.3% of the performance of full-data fine-tuning with only 15% of the data, significantly reducing computational costs while maintaining superior results. This exemplifies the \"Less is More\" philosophy in MLLM development. The code and data is available in this URL",
    "checked": true,
    "id": "4925492709034cfb614e89c15174669efc54d169",
    "semantic_title": "mastering collaborative multi-modal data selection: a focus on informativeness, uniqueness, and representativeness",
    "citation_count": 3,
    "authors": [
      "Qifan Yu",
      "Zhebei Shen",
      "Zhongqi Yue",
      "Yang Wu",
      "Bosheng Qin",
      "Wenqiao Zhang",
      "Yunfei Li",
      "Juncheng Li",
      "Siliang Tang",
      "Yueting Zhuang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_WIR3D_Visually-Informed_and_Geometry-Aware_3D_Shape_Abstraction_ICCV_2025_paper.html": {
    "title": "WIR3D: Visually-Informed and Geometry-Aware 3D Shape Abstraction",
    "volume": "main",
    "abstract": "In this work we present WIR3D, a technique for abstracting 3D shapes through a sparse set of visually meaningful curves in 3D. We optimize the parameters of Bezier curves such that they faithfully represent both the geometry and salient visual features (e.g. texture) of the shape from arbitrary viewpoints. We leverage the intermediate activations of a pre-trained foundation model (CLIP) to guide our optimization process. We divide our optimization into two phases: one for capturing the coarse geometry of the shape, and the other for representing fine-grained features. Our second phase supervision is spatially guided by a novel localized keypoint loss. This spatial guidance enables user control over abstracted features. We ensure fidelity to the original surface through a neural SDF loss, which allows the curves to be used as intuitive deformation handles. We successfully apply our method for shape abstraction over a broad dataset of shapes with varying complexity, geometric structure, and texture, and demonstrate downstream applications for feature control and shape deformation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Richard Liu",
      "Daniel Fu",
      "Noah Tan",
      "Itai Lang",
      "Rana Hanocka"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tan_XTrack_Multimodal_Training_Boosts_RGB-X_Video_Object_Trackers_ICCV_2025_paper.html": {
    "title": "XTrack: Multimodal Training Boosts RGB-X Video Object Trackers",
    "volume": "main",
    "abstract": "Multimodal sensing has proven valuable for visual tracking, as different sensor types offer unique strengths in handling one specific challenging scene where object appearance varies. While a generalist model capable of leveraging all modalities would be ideal, development is hindered by data sparsity, typically in practice, only one modality is available at a time. Therefore, it is crucial to ensure and achieve that knowledge gained from multimodal sensing -- such as identifying relevant features and regions -- is effectively shared, even when certain modalities are unavailable at inference. We venture with a simple assumption: similar samples across different modalities have more knowledge to share than otherwise. To implement this, we employ a classifier with weak loss tasked with distinguishing between modalities. More specifically, if the classifier \"fails\" to accurately identify the modality of the given sample, this signals an opportunity for cross-modal knowledge sharing. Intuitively, knowledge transfer is facilitated whenever a sample from one modality is sufficiently close and aligned with another. Technically, we achieve this by routing samples from one modality to the expert of the others, within a mixture-of-experts framework designed for multimodal video object tracking. During the inference, the expert of the respective modality is chosen, which we show to benefit from the multimodal knowledge available during training, thanks to the proposed method. Through the exhaustive experiments that use only paired RGB-E, RGB-D, and RGB-T during training, we showcase the benefit of the proposed method for RGB-X tracker during inference, with an average +3% precision improvement over the current SOTA. The source code is publicly available at https://github.com/supertyd/XTrack",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuedong Tan",
      "Zongwei Wu",
      "Yuqian Fu",
      "Zhuyun Zhou",
      "Guolei Sun",
      "Eduard Zamfir",
      "Chao Ma",
      "Danda Paudel",
      "Luc Van Gool",
      "Radu Timofte"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yin_FaceCraft4D_Animated_3D_Facial_Avatar_Generation_from_a_Single_Image_ICCV_2025_paper.html": {
    "title": "FaceCraft4D: Animated 3D Facial Avatar Generation from a Single Image",
    "volume": "main",
    "abstract": "We present a novel framework for generating high-quality, animatable 4D avatar from a single image. While recent advances have shown promising results in 4D avatar creation, existing methods either require extensive multiview data or struggle with geometry accuracy and identity consistency. To address these limitations, we propose a comprehensive system that leverages geometry, image, and video priors to create full-view, animatable avatars. Our approach first obtains initial coarse geometry through 3D-GAN inversion. Then, it enhances multiview textures using depth-guided warping signals for cross-view consistency with the help of the image diffusion model. To handle expression animation, we incorporate a video prior with synchronized driving signals across viewpoints. We further introduce a Consistent-Inconsistent training to effectively handle data inconsistencies during 4D reconstruction. Experimental results demonstrate that our method achieves superior quality compared to the prior art, while maintaining consistency across different viewpoints and expressions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fei Yin",
      "Mallikarjun B R",
      "Chun-Han Yao",
      "Rafal K. Mantiuk",
      "Varun Jampani"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_GenFlow3D_Generative_Scene_Flow_Estimation_and_Prediction_on_Point_Cloud_ICCV_2025_paper.html": {
    "title": "GenFlow3D: Generative Scene Flow Estimation and Prediction on Point Cloud Sequences",
    "volume": "main",
    "abstract": "Scene flow provides the fundamental information of the scene dynamics. Existing scene flow estimation methods typically rely on the correlation between only a consecutive point cloud pair, which makes them limited to the instantaneous state of the scene and face challenges in real-world scenarios with factors like occlusion, noise, and diverse motion of background and foreground. In this paper, we study the joint sequential scene flow estimation and future scene flow prediction on point cloud sequences. The expanded sequential input introduces long-term and high-order motion information. We propose GenFlow3D, a recurrent neural network model which integrates diffusion in the decoder to better incorporate the two tasks and enhance the ability to extract general motion patterns. A transformer-based denoising network is adopted to help capture useful information. Depending on the input point clouds, discriminative condition signals are generated to guide the diffusion decoder to switch among different modes specific for scene flow estimation and prediction in a multi-scale manner. GenFlow3D is evaluated on the real-world datasets nuScenes and Argoverse 2, and demonstrates superior performance compared with the existing methods. Our code is available at https://github.com/ustc-hlli/GenFlow3D",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanlin Li",
      "Wenming Weng",
      "Yueyi Zhang",
      "Zhiwei Xiong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zou_Attention_to_Neural_Plagiarism_Diffusion_Models_Can_Plagiarize_Your_Copyrighted_ICCV_2025_paper.html": {
    "title": "Attention to Neural Plagiarism: Diffusion Models Can Plagiarize Your Copyrighted Images!",
    "volume": "main",
    "abstract": "In this paper, we highlight a critical threat posed by emerging neural models--data plagiarism. We demonstrate how modern neural models (e.g., diffusion models) can effortlessly replicate copyrighted images, even when protected by advanced watermarking techniques. To expose the vulnerability in copyright protection and facilitate future research, we propose a general approach regarding neural plagiarism that can either forge replicas of copyrighted data or introduce copyright ambiguity. Our method, based on \"anchors and shims\", employs inverse latents as anchors and finds shim perturbations that can gradually deviate the anchor latents, thereby evading watermark or copyright detection. By applying perturbation to the cross-attention mechanism at different timesteps, our approach induces varying degrees of semantic modifications in copyrighted images, making it to bypass protections ranging from visible trademarks, signatures to invisible watermarks. Notably, our method is a purely gradient-based search that requires no additional training or fine-tuning. Empirical experiments on MS-COCO and real-world copyrighted images show that diffusion models can replicate copyrighted images, underscoring the urgent need for countermeasures against neural plagiarism. Source code is available at: https://github.com/zzzucf/Neural-Plagiarism",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihang Zou",
      "Boqing Gong",
      "Liqiang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_PBCAT_Patch-Based_Composite_Adversarial_Training_against_Physically_Realizable_Attacks_on_ICCV_2025_paper.html": {
    "title": "PBCAT: Patch-Based Composite Adversarial Training against Physically Realizable Attacks on Object Detection",
    "volume": "main",
    "abstract": "Object detection plays a crucial role in many security-sensitive applications, such as autonomous driving and video surveillance. However, several recent studies have shown that object detectors can be easily fooled by physically realizable attacks, e.g., adversarial patches and recent adversarial textures, which pose realistic and urgent threats. Adversarial Training (AT) has been recognized as the most effective defense against adversarial attacks. While AT has been extensively studied in the l_infinity-bounded attack settings on classification models, AT against physically realizable attacks on object detectors has received limited exploration. Early attempts are only performed to defend against adversarial patches, leaving AT against a wider range of physically realizable attacks under-explored. In this work, we consider defending against various physically realizable attacks with a unified AT method. We propose PBCAT, a novel Patch-Based Composite Adversarial Training strategy. PBCAT optimizes the model by incorporating the combination of small-area gradient-guided adversarial patches and imperceptible global adversarial perturbations covering the entire image. With these designs, PBCAT has the potential to defend against not only adversarial patches but also unseen physically realizable attacks such as adversarial textures. Extensive experiments in multiple settings demonstrated that PBCAT significantly improved robustness against various physically realizable attacks over state-of-the-art defense methods. Notably, it improved the detection accuracy by 29.7% over previous defense methods under one recent adversarial texture attack",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Li",
      "Yiming Zhu",
      "Yifan Huang",
      "Wei Zhang",
      "Yingzhe He",
      "Jie Shi",
      "Xiaolin Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_TRCE_Towards_Reliable_Malicious_Concept_Erasure_in_Text-to-Image_Diffusion_Models_ICCV_2025_paper.html": {
    "title": "TRCE: Towards Reliable Malicious Concept Erasure in Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "Recent advances in text-to-image diffusion models enable photorealistic image generation, but they also risk producing malicious content, such as NSFW images. To mitigate risk, concept erasure methods are studied to facilitate the model to unlearn specific concepts. However, current studies struggle to fully erase malicious concepts implicitly embedded in prompts (e.g., metaphorical expressions or adversarial prompts) while preserving the model's normal generation capability. To address this challenge, our study proposes TRCE, using a two-stage concept erasure strategy to achieve an effective trade-off between reliable erasure and knowledge preservation. Firstly, TRCE starts by erasing the malicious semantics implicitly embedded in textual prompts. By identifying a critical mapping objective(i.e., the [EoT] embedding), we optimize the cross-attention layers to map malicious prompts to contextually similar prompts but with safe concepts. This step prevents the model from being overly influenced by malicious semantics during the denoising process. Following this, considering the deterministic properties of the sampling trajectory of the diffusion model, TRCE further steers the early denoising prediction toward the safe direction and away from the unsafe one through contrastive learning, thus further avoiding the generation of malicious content. Finally, we conduct comprehensive evaluations of TRCE on multiple malicious concept erasure benchmarks, and the results demonstrate its effectiveness in erasing malicious concepts while better preserving the model's original generation ability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruidong Chen",
      "Honglin Guo",
      "Lanjun Wang",
      "Chenyu Zhang",
      "Weizhi Nie",
      "An-An Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhu_PossLoss_A_Reliable_and_Sensitive_Facial_Landmark_Detection_Loss_Function_ICCV_2025_paper.html": {
    "title": "PossLoss: A Reliable and Sensitive Facial Landmark Detection Loss Function",
    "volume": "main",
    "abstract": "ate and locate the peak of the heatmap, while adaptively balancing the influence of landmarks and background pixels through self-weighting, addressing the extreme imbalance between landmarks and non-landmarks. More advanced is that our PossLoss is sample-sensitive, which can distinguish easy and hard landmarks and adaptively make the model focused more on hard landmarks. Moreover, it addresses the difficulty of accurately evaluating heatmap distribution, especially in addressing small errors due to peak mismatches. We analyzed and evaluated our PossLoss on three challenging facial landmark detection tasks. The experimental results show that our PossLoss significantly improves the performance of landmark detection and outperforms the state-of-the-art methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qikui Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xu_ChartPoint_Guiding_MLLMs_with_Grounding_Reflection_for_Chart_Reasoning_ICCV_2025_paper.html": {
    "title": "ChartPoint: Guiding MLLMs with Grounding Reflection for Chart Reasoning",
    "volume": "main",
    "abstract": "Multimodal Large Language Models (MLLMs) have emerged as powerful tools for chart comprehension. However, they heavily rely on extracted content via OCR, which leads to numerical hallucinations when chart textual annotations are sparse. While existing methods focus on scaling instructions, they fail to address the fundamental challenge, i.e., reasoning with visual perception. In this paper, we identify a critical observation: MLLMs exhibit weak grounding in chart elements and proportional relationships, as evidenced by their inability to localize key positions to match their reasoning. To bridge this gap, we propose PointCoT, which integrates reflective interaction into chain-of-thought reasoning in charts. By prompting MLLMs to generate bounding boxes and re-render charts based on location annotations, we establish connections between textual reasoning steps and visual grounding regions. We further introduce an automated pipeline to construct ChartPoint-SFT-62k, a dataset featuring 19.2K high-quality chart samples with step-by-step CoT, bounding box, and re-rendered visualizations. Leveraging this data, we develop two instruction-tuned models, ChartPoint_Q2 and ChartPoint_Q2.5, which outperform state-of-the-art across several chart benchmarks, e.g., +5.04% on ChartBench",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengzhuo Xu",
      "SiNan Du",
      "Yiyan Qi",
      "Siwen Lu",
      "Chengjin Xu",
      "Chun Yuan",
      "Jian Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ke_StealthAttack_Robust_3D_Gaussian_Splatting_Poisoning_via_Density-Guided_Illusions_ICCV_2025_paper.html": {
    "title": "StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided Illusions",
    "volume": "main",
    "abstract": "3D scene representation methods like Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have significantly advanced novel view synthesis. As these methods become prevalent, addressing their vulnerabilities becomes critical. We analyze 3DGS robustness against image-level poisoning attacks and propose a novel density-guided poisoning method. Our method strategically injects Gaussian points into low-density regions identified via Kernel Density Estimation (KDE), embedding viewpoint-dependent illusory objects clearly visible from poisoned views while minimally affecting innocent views. Additionally, we introduce an adaptive noise strategy to disrupt multi-view consistency, further enhancing attack effectiveness. We propose a KDE-based evaluation protocol to assess attack difficulty systematically, enabling objective benchmarking for future research. Extensive experiments demonstrate our method's superior performance compared to state-of-the-art techniques",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo-Hsu Ke",
      "You-Zhe Xie",
      "Yu-Lun Liu",
      "Wei-Chen Chiu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_LightCity_An_Urban_Dataset_for_Outdoor_Inverse_Rendering_and_Reconstruction_ICCV_2025_paper.html": {
    "title": "LightCity: An Urban Dataset for Outdoor Inverse Rendering and Reconstruction under Multi-illumination Conditions",
    "volume": "main",
    "abstract": "We propose an outdoor scene dataset and propose a series of benchmarks based on it.Inverse rendering in urban scenes is pivotal for applications like autonomous driving and digital twins, yet it faces significant challenges due to complex illumination conditions, including multi-illumination and indirect light and shadow effects.However, these challenges' effects on intrinsic decomposition and 3D reconstruction are not explored due to the lack of appropriate datasets. In this paper, we present LightCity, a novel high-quality synthetic urban dataset featuring diverse illumination conditions with realistic indirect light and shadow effects.LightCity encompasses over 300 sky maps with highly controllable illumination, varying scales with both street-level and aerial perspectives over 50K images, and rich properties such as depth, normal, and material components, light and indirect light, etc.Besides, we leverage LightCity to benchmark three fundamental tasks in the urban environments and conduct a comprehensive analysis of these benchmarks, laying a robust foundation for advancing related research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingjing Wang",
      "Qirui Hu",
      "Chong Bao",
      "Yuke Zhu",
      "Hujun Bao",
      "Zhaopeng Cui",
      "Guofeng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kondylatos_On_the_Generalization_of_Representation_Uncertainty_in_Earth_Observation_ICCV_2025_paper.html": {
    "title": "On the Generalization of Representation Uncertainty in Earth Observation",
    "volume": "main",
    "abstract": "Recent advances in Computer Vision have introduced the concept of pretrained representation uncertainty, enabling zero-shot uncertainty estimation. This holds significant potential for Earth Observation (EO), where trustworthiness is critical, yet the complexity of EO data poses challenges to uncertainty-aware methods. In this work, we investigate the generalization of representation uncertainty in EO, considering the domain's unique semantic characteristics. We pretrain uncertainties on large EO datasets and propose an evaluation framework to assess their zero-shot performance in multi-label classification and segmentation EO tasks. Our findings reveal that, unlike uncertainties pretrained on natural images, EO-pretraining exhibits strong generalization across unseen EO domains, geographic locations, and target granularities, while maintaining sensitivity to variations in ground sampling distance. We demonstrate the practical utility of pretrained uncertainties showcasing their alignment with task-specific uncertainties in downstream tasks, their sensitivity to real-world EO image noise, and their ability to generate spatial uncertainty estimates out-of-the-box. Initiating the discussion on representation uncertainty in EO, our study provides insights into its strengths and limitations, paving the way for future research in the field. Code and weights are available at: https://github.com/Orion-AI-Lab/EOUncertaintyGeneralization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Spyros Kondylatos",
      "Nikolaos Ioannis Bountos",
      "Dimitrios Michail",
      "Xiao Xiang Zhu",
      "Gustau Camps-Valls",
      "Ioannis Papoutsis"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ouyang_Region-aware_Anchoring_Mechanism_for_Efficient_Referring_Visual_Grounding_ICCV_2025_paper.html": {
    "title": "Region-aware Anchoring Mechanism for Efficient Referring Visual Grounding",
    "volume": "main",
    "abstract": "Referring Visual Grounding (RVG) tasks revolve around utilizing vision-language interactions to incorporate object information from language expressions, thereby enabling targeted object detection or segmentation within images. Transformer-based methods have enabled effective interaction through attention mechanisms, achieving notable performance in RVG tasks. However, existing strategies for RVG, which involve direct interaction between visual and linguistic features, face three key challenges: (i) tendency to focus on a single target, (ii) insufficient control over linguistic noise, and (iii) high computational cost. To address these challenges, we propose a Region-aware Anchoring Mechanism (RaAM) that mediates vision-language interactions. In RaAM, region-aware anchors engage in alternating interactions with vision and language modalities, acting as indicators for object presence across different regions within the image. RaAM (i) directs attention to multiple target regions for better localization, (ii) reduces cross-modal redundancy by using anchors as buffers, and (iii) lowers time complexity. In addition, we design region and pixel level loss functions to enhance object presence assessment and edge precision. We evaluate our RaAM-RVG on four benchmark datasets and integrate RaAM into various models by replacing their interaction design. Results show that RaAM outperforms state-of-the-art methods with lower computational cost",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuyi Ouyang",
      "Ziwei Niu",
      "Hongyi Wang",
      "Yen-Wei Chen",
      "Lanfen Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Han_Mind_the_Cost_of_Scaffold_Benign_Clients_May_Even_Become_ICCV_2025_paper.html": {
    "title": "Mind the Cost of Scaffold! Benign Clients May Even Become Accomplices of Backdoor Attack",
    "volume": "main",
    "abstract": "By using a control variate to calibrate the local gradient of each client, Scaffold has been widely known as a powerful solution to mitigate the impact of data heterogeneity in Federated Learning. Although Scaffold achieves significant performance improvements, we show that this superiority is at the cost of increased security vulnerabilities. Specifically, this paper presents BadSFL, the first backdoor attack targeting Scaffold, which turns benign clients into accomplices to amplify the attack effect. The core idea of BadSFL is to uniquely tamper with the control variate to subtly steer benign clients' local gradient updates towards the attacker's poisoned direction, effectively turning them into unwitting accomplices, significantly enhancing the backdoor persistence. Additionally, BadSFL leverages a GAN-enhanced poisoning strategy to enrich the attacker's dataset, maintaining high accuracy on both benign and backdoored samples while remaining stealthy. Extensive experiments demonstrate that BadSFL achieves superior attack durability, maintaining effectiveness for over 60 global rounds--lasting up to three times longer than existing baselines even after ceasing malicious model injections",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingshuo Han",
      "Xuanye Zhang",
      "Xiang Lan",
      "Haozhao Wang",
      "Shengmin Xu",
      "Shen Ren",
      "Jason Zeng",
      "Ming Wu",
      "Michael Heinrich",
      "Tianwei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_MV-Adapter_Multi-View_Consistent_Image_Generation_Made_Easy_ICCV_2025_paper.html": {
    "title": "MV-Adapter: Multi-View Consistent Image Generation Made Easy",
    "volume": "main",
    "abstract": "Existing multi-view image generation methods often make invasive modifications to pre-trained text-to-image (T2I) models and require full fine-tuning, leading to high computational costs and degradation in image quality due to scarce high-quality 3D data. This paper introduces MV-Adapter, an efficient and versatile adapter that enhances T2I models and their derivatives without altering the original network structure or feature space. To efficiently model the 3D geometric knowledge within the adapter, we introduce innovative designs that include duplicated self-attention layers and parallel attention architecture, enabling the adapter to inherit the powerful priors of the pre-trained models to model the novel 3D knowledge. Moreover, we present a unified condition encoder that seamlessly integrates camera parameters and geometric information, facilitating applications such as text- and image-based 3D generation and texturing. MV-Adapter achieves multi-view generation at 768 resolution on Stable Diffusion XL (SDXL), and demonstrates adaptability and versatility. It can also be extended to arbitrary view generation, enabling broader applications. We demonstrate that MV-Adapter sets a new quality standard for multi-view image generation, and opens up new possibilities due to its efficiency, adaptability and versatility",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zehuan Huang",
      "Yuan-Chen Guo",
      "Haoran Wang",
      "Ran Yi",
      "Lizhuang Ma",
      "Yan-Pei Cao",
      "Lu Sheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lan_Hybrid-Tower_Fine-grained_Pseudo-query_Interaction_and_Generation_for_Text-to-Video_Retrieval_ICCV_2025_paper.html": {
    "title": "Hybrid-Tower: Fine-grained Pseudo-query Interaction and Generation for Text-to-Video Retrieval",
    "volume": "main",
    "abstract": "The Text-to-Video Retrieval (T2VR) task aims to retrieve unlabeled videos by textual queries with the same semantic meanings. Recent CLIP-based approaches have explored two frameworks: Two-Tower versus Single-Tower framework, yet the former suffers from low effectiveness, while the latter suffers from low efficiency. In this study, we explore a new Hybrid-Tower framework that can hybridize the advantages of the Two-Tower and Single-Tower framework, achieving high effectiveness and efficiency simultaneously. We propose a novel hybrid method, Fine-grained Pseudo-query Interaction and Generation for T2VR, i.e. \\name , which includes a new pseudo-query generator designed to generate a pseudo-query for each video. This enables the video feature and the textual features of pseudo-query to interact in a fine-grained manner, similar to the Single-Tower approaches to hold high effectiveness, even before the real textual query is received. Simultaneously, our method introduces no additional storage or computational overhead compared to the Two-Tower framework during the inference stage, thus maintaining high efficiency. Extensive experiments on five commonly used text-video retrieval benchmarks demonstrate that our method achieves a significant improvement over the baseline, with an increase of 1.6% ~ 3.9% in R@1. Furthermore, our method matches the efficiency of Two-Tower models while achieving near state-of-the-art performance, highlighting the advantages of the Hybrid-Tower framework",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bangxiang Lan",
      "Ruobing Xie",
      "Ruixiang Zhao",
      "Xingwu Sun",
      "Zhanhui Kang",
      "Gang Yang",
      "Xirong Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Park_ATLAS_Decoupling_Skeletal_and_Shape_Parameters_for_Expressive_Parametric_Human_ICCV_2025_paper.html": {
    "title": "ATLAS: Decoupling Skeletal and Shape Parameters for Expressive Parametric Human Modeling",
    "volume": "main",
    "abstract": "Parametric body models offer expressive 3D representation of humans across a wide range of poses, shapes, and facial expressions, typically derived by learning a basis over registered 3D meshes. However, existing human mesh modeling approaches struggle to capture detailed variations across diverse body poses and shapes, largely due to limited training data diversity and restrictive modeling assumptions. Moreover, the common paradigm first optimizes the external body surface using a linear basis, then regresses internal skeletal joints from surface vertices. This approach introduces problematic dependencies between internal skeleton and outer soft tissue, limiting direct control over body height and bone lengths. To address these issues, we present ATLAS, a high-fidelity body model learned from 600k high-resolution scans captured using 240 synchronized cameras. Unlike previous methods, we explicitly decouple the shape and skeleton bases by grounding our mesh representation in the human skeleton. This decoupling enables enhanced shape expressivity, fine-grained customization of body attributes, and keypoint fitting independent of external soft-tissue characteristics. ATLAS outperforms existing methods by fitting unseen subjects in diverse poses more accurately, and quantitative evaluations show that our non-linear pose correctives more effectively capture complex poses compared to linear models",
    "checked": true,
    "id": "bf17cfffe51480b78b1be18098187eb1052d3233",
    "semantic_title": "atlas: decoupling skeletal and shape parameters for expressive parametric human modeling",
    "citation_count": 0,
    "authors": [
      "Jinhyung Park",
      "Javier Romero",
      "Shunsuke Saito",
      "Fabian Prada",
      "Takaaki Shiratori",
      "Yichen Xu",
      "Federica Bogo",
      "Shoou-I Yu",
      "Kris Kitani",
      "Rawal Khirodkar"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ba_Enhancing_Reward_Models_for_High-quality_Image_Generation_Beyond_Text-Image_Alignment_ICCV_2025_paper.html": {
    "title": "Enhancing Reward Models for High-quality Image Generation: Beyond Text-Image Alignment",
    "volume": "main",
    "abstract": "Contemporary image generation systems have achieved high fidelity and superior aesthetic quality beyond basic text-image alignment. However, existing evaluation frameworks have failed to evolve in parallel. This study reveals that human preference reward models fine-tuned based on CLIP and BLIP architectures have inherent flaws: they inappropriately assign low scores to images with rich details and high aesthetic value, creating a significant discrepancy with actual human aesthetic preferences. To address this issue, we design a novel evaluation score, ICT (Image-Contained-Text) score, that achieves and surpasses the objectives of text-image alignment by assessing the degree to which images represent textual content. Building upon this foundation, we further train a HP (High-Preference) score model using solely the image modality, aiming to enhance image quality in aspects such as aesthetics and detail refinement while maintaining achieved text-image alignment.Experiments demonstrate that the proposed evaluation model improves scoring accuracy by over 10% compared to existing methods, and achieves significant results in optimizing state-of-the-art text-to-image models. This research provides theoretical foundation and empirical support for the evolution of image generation technology toward better alignment with higher-order human aesthetic preferences",
    "checked": true,
    "id": "976d741869a7067f5df4102b8b68eab3ec396453",
    "semantic_title": "enhancing reward models for high-quality image generation: beyond text-image alignment",
    "citation_count": 2,
    "authors": [
      "Ying Ba",
      "Tianyu Zhang",
      "Yalong Bai",
      "Wenyi Mo",
      "Tao Liang",
      "Bing Su",
      "Ji-Rong Wen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tsai_PHATNet_A_Physics-guided_Haze_Transfer_Network_for_Domain-adaptive_Real-world_Image_ICCV_2025_paper.html": {
    "title": "PHATNet: A Physics-guided Haze Transfer Network for Domain-adaptive Real-world Image Dehazing",
    "volume": "main",
    "abstract": "Image dehazing aims to remove unwanted hazy artifacts in images. Although previous research has collected paired real-world hazy and haze-free images to improve dehazing models' performance in real-world scenarios, these models often experience significant performance drops when handling unseen real-world hazy images due to limited training data. This issue motivates us to develop a flexible domain adaptation method to enhance dehazing performance during testing. Observing that predicting haze patterns is generally easier than recovering clean content, we propose the Physics-guided Haze Transfer Network (PHATNet) which transfers haze patterns from unseen target domains to source-domain haze-free images, creating domain-specific fine-tuning sets to update dehazing models for effective domain adaptation. Additionally, we introduce a Haze-Transfer-Consistency loss and a Content-Leakage Loss to enhance PHATNet's disentanglement ability. Experimental results demonstrate that PHATNet significantly boosts state-of-the-art dehazing models on benchmark real-world image dehazing datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fu-Jen Tsai",
      "Yan-Tsung Peng",
      "Yen-Yu Lin",
      "Chia-Wen Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xiong_Diagnosing_Pretrained_Models_for_Out-of-distribution_Detection_ICCV_2025_paper.html": {
    "title": "Diagnosing Pretrained Models for Out-of-distribution Detection",
    "volume": "main",
    "abstract": "This work questions a common assumption of OOD detection, that models with higher in-distribution (ID) accuracy tend to have better OOD performance. Recent findings show this assumption doesn't always hold. A direct observation is that the later version of torchvision models improves ID accuracy but suffers from a significant drop in OOD performance. We systematically diagnose torchvision training recipes and explain this effect by analyzing the maximal logits of ID and OOD samples. We then propose post-hoc and training-time solutions to mitigate the OOD decrease by fixing problematic augmentations in torchvision recipes. Both solutions enhance OOD detection and maintain strong ID performance",
    "checked": false,
    "id": "e4dd8e6f58e0c2af4d978b386a67f91721eb021d",
    "semantic_title": "greenhyperspectra: a multi-source hyperspectral dataset for global vegetation trait prediction",
    "citation_count": 0,
    "authors": [
      "Haipeng Xiong",
      "Kai Xu",
      "Angela Yao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_LD-RPS_Zero-Shot_Unified_Image_Restoration_via_Latent_Diffusion_Recurrent_Posterior_ICCV_2025_paper.html": {
    "title": "LD-RPS: Zero-Shot Unified Image Restoration via Latent Diffusion Recurrent Posterior Sampling",
    "volume": "main",
    "abstract": "Unified image restoration is a significantly challenging task in low-level vision. Existing methods either make tailored designs for specific tasks, limiting their generalizability across various types of degradation, or rely on training with paired datasets, thereby suffering from closed-set constraints. To address these issues, we propose a novel, dataset-free, and unified approach through recurrent posterior sampling utilizing a pretrained latent diffusion model. Our method incorporates the multimodal understanding model to provide sematic priors for the generative model under a task-blind condition. Furthermore, it utilizes a lightweight module to align the degraded input with the generated preference of the diffusion model, and employs recurrent refinement for posterior sampling. Extensive experiments demonstrate that our method outperforms state-of-the-art methods, validating its effectiveness and robustness. Our code and data will be available at https://github.com/AMAP-ML/LD-RPS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huaqiu Li",
      "Yong Wang",
      "Tongwen Huang",
      "Hailang Huang",
      "Haoqian Wang",
      "Xiangxiang Chu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Outlier-Aware_Post-Training_Quantization_for_Image_Super-Resolution_ICCV_2025_paper.html": {
    "title": "Outlier-Aware Post-Training Quantization for Image Super-Resolution",
    "volume": "main",
    "abstract": "Quantization techniques, including quantization-aware training (QAT) and post-training quantization (PTQ), have become essential for inference acceleration of image super-resolution (SR) networks. Compared to QAT, PTQ has garnered significant attention as it eliminates the need for ground truth and model retraining. However, existing PTQ methods for SR often fail to achieve satisfactory performance as they overlook the impact of outliers in activation. Our empirical analysis reveals that these prevalent activation outliers are strongly correlated with image color information, and directly removing them leads to significant performance degradation. Motivated by this, we propose a dual-region quantization strategy that partitions activations into an outlier region and a dense region, applying uniform quantization to each region independently to better balance bit-width allocation. Furthermore, we observe that different network layers exhibit varying sensitivities to quantization, leading to different levels of performance degradation. To address this, we introduce sensitivity-aware finetuning that encourages the model to focus more on highly sensitive layers, further enhancing quantization performance. Extensive experiments demonstrate that our method outperforms existing PTQ approaches across various SR networks and datasets, while achieving performance comparable to QAT methods in most scenarios with at least a 75x speedup",
    "checked": false,
    "id": "478f633edb3a05103ab3efdfb851d9beee7107e9",
    "semantic_title": "toward accurate post-training quantization for image super resolution",
    "citation_count": 21,
    "authors": [
      "Hailing Wang",
      "Jianglin Lu",
      "Yitian Zhang",
      "Yun Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhao_Equipping_Vision_Foundation_Model_with_Mixture_of_Experts_for_Out-of-Distribution_ICCV_2025_paper.html": {
    "title": "Equipping Vision Foundation Model with Mixture of Experts for Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "Pre-trained vision foundation models have transformed many computer vision tasks. Despite their strong ability to learn discriminative and generalizable features crucial for out-of-distribution (OOD) detection, their impact on this task remains underexplored. Motivated by this gap, we systematically investigate representative vision foundation models for OOD detection. Our findings reveal that a pre-trained DINOv2 model, even without fine-tuning on in-domain (ID) data, naturally provides a highly discriminative feature space for OOD detection, achieving performance comparable to existing state-of-the-art methods without requiring complex designs. Beyond this, we explore how fine-tuning foundation models on in-domain (ID) data can enhance OOD detection. However, we observe that the performance of vision foundation models remains unsatisfactory in scenarios with a large semantic space. This is due to the increased complexity of decision boundaries as the number of categories grows, which complicates the optimization process. To mitigate this, we propose the Mixture of Feature Experts (MoFE) module, which partitions features into subspaces, effectively capturing complex data distributions and refining decision boundaries. Further, we introduce a Dynamic-b Mixup strategy, which samples interpolation weights from a dynamic beta distribution. This adapts to varying levels of learning difficulty across categories, improving feature learning for more challenging categories. Extensive experiments demonstrate the effectiveness of our approach, significantly outperforming baseline methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shizhen  Zhao",
      "Jiahui Liu",
      "Xin Wen",
      "Haoru Tan",
      "Xiaojuan Qi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cheng_The_Curse_of_Conditions_Analyzing_and_Improving_Optimal_Transport_for_ICCV_2025_paper.html": {
    "title": "The Curse of Conditions: Analyzing and Improving Optimal Transport for Conditional Flow-Based Generation",
    "volume": "main",
    "abstract": "Minibatch optimal transport coupling straightens paths in unconditional flow matching. This leads to computationally less demanding inference as fewer integration steps and less complex numerical solvers can be employed when numerically solving an ordinary differential equation at test time. However, in the conditional setting, minibatch optimal transport falls short. This is because the default optimal transport mapping disregards conditions, resulting in a conditionally skewed prior distribution during training. In contrast, at test time, we have no access to the skewed prior, and instead sample from the full, unbiased prior distribution. This gap between training and testing leads to a subpar performance. To bridge this gap, we propose conditional optimal transport (C2OT) that adds a conditional weighting term in the cost matrix when computing the optimal transport assignment. Experiments demonstrate that this simple fix works with both discrete and continuous conditions in 8gaussians-to-moons, CIFAR-10, ImageNet-32x32, and ImageNet-256x256. Our method performs better overall compared to the existing baselines across different function evaluation budgets. Code is available at hkchengrex.github.io/C2OT",
    "checked": true,
    "id": "3439bc980f3628de3fe59535e7f9ebea350862b9",
    "semantic_title": "the curse of conditions: analyzing and improving optimal transport for conditional flow-based generation",
    "citation_count": 4,
    "authors": [
      "Ho Kei Cheng",
      "Alexander Schwing"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shen_BlinkTrack_Feature_Tracking_over_80_FPS_via_Events_and_Images_ICCV_2025_paper.html": {
    "title": "BlinkTrack: Feature Tracking over 80 FPS via Events and Images",
    "volume": "main",
    "abstract": "Event cameras, known for their high temporal resolution and ability to capture asynchronous changes, have gained significant attention for their potential in feature tracking, especially in challenging conditions. However, event cameras lack the fine-grained texture information that conventional cameras provide, leading to error accumulation in tracking. To address this, we propose a novel framework, BlinkTrack, which integrates event data with grayscale images for high-frequency feature tracking. Our method extends the traditional Kalman filter into a learning-based framework, utilizing differentiable Kalman filters in both event and image branches. This approach improves single-modality tracking and effectively solves the data association and fusion from asynchronous event and image data. We also introduce new synthetic and augmented datasets to better evaluate our model. Experimental results indicate that BlinkTrack significantly outperforms existing methods, exceeding 80 FPS with multi-modality data and 100 FPS with preprocessed event data. Codes and dataset are available at https://github.com/ColieShen/BlinkTrack",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yichen Shen",
      "Yijin Li",
      "Shuo Chen",
      "Guanglin Li",
      "Zhaoyang Huang",
      "Hujun Bao",
      "Zhaopeng Cui",
      "Guofeng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Bae_Less_is_More_Improving_Motion_Diffusion_Models_with_Sparse_Keyframes_ICCV_2025_paper.html": {
    "title": "Less is More: Improving Motion Diffusion Models with Sparse Keyframes",
    "volume": "main",
    "abstract": "Recent advances in motion diffusion models have led to remarkable progress in diverse motion generation tasks, including text-to-motion synthesis.However, existing approaches represent motions as dense frame sequences, requiring the model to process redundant or less informative frames.The processing of dense animation frames imposes significant training complexity, especially when learning intricate distributions of large motion datasets even with modern neural architectures. This severely limits the performance of generative motion models for downstream tasks.Inspired by professional animators who mainly focus on sparse keyframes, we propose a novel diffusion framework explicitly designed around sparse and geometrically meaningful keyframes.Our method reduces computation by masking non-keyframes and efficiently interpolating missing frames. We dynamically refine the keyframe mask during inference to prioritize informative frames in later diffusion steps.Extensive experiments show that our approach consistently outperforms state-of-the-art methods in text alignment and motion realism, while also effectively maintaining high performance at significantly fewer diffusion steps.We further validate the robustness of our framework by using it as a generative prior and adapting it to different downstream tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinseok Bae",
      "Inwoo Hwang",
      "Young-Yoon Lee",
      "Ziyu Guo",
      "Joseph Liu",
      "Yizhak Ben-Shabat",
      "Young Min Kim",
      "Mubbasir Kapadia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lee_CAVIS_Context-Aware_Video_Instance_Segmentation_ICCV_2025_paper.html": {
    "title": "CAVIS: Context-Aware Video Instance Segmentation",
    "volume": "main",
    "abstract": "In this paper, we introduce the Context-Aware Video Instance Segmentation (CAVIS), a novel framework designed to enhance instance association by integrating contextual information adjacent to each object. To efficiently extract and leverage this information, we propose the Context-Aware Instance Tracker (CAIT), which merges contextual data surrounding the instances with the core instance features to improve tracking accuracy. Additionally, we design the Prototypical Cross-frame Contrastive (PCC) loss, which ensures consistency in object-level features across frames, thereby significantly enhancing instance matching accuracy. CAVIS demonstrates superior performance over state-of-the-art methods on all benchmark datasets in video instance segmentation (VIS) and video panoptic segmentation (VPS). Notably, our method excels on the OVIS dataset, which is known for its particularly challenging videos",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seunghun Lee",
      "Jiwan Seo",
      "Kiljoon Han",
      "Minwoo Choi",
      "Sunghoon Im"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cheng_LeanVAE_An_Ultra-Efficient_Reconstruction_VAE_for_Video_Diffusion_Models_ICCV_2025_paper.html": {
    "title": "LeanVAE: An Ultra-Efficient Reconstruction VAE for Video Diffusion Models",
    "volume": "main",
    "abstract": "Recent advances in Latent Video Diffusion Models (LVDMs) have revolutionized video generation by leveraging Video Variational Autoencoders (Video VAEs) to compress intricate video data into a compact latent space. However, as LVDM training scales, the computational overhead of Video VAEs becomes a critical bottleneck, particularly for encoding high-resolution videos. To address this, we propose LeanVAE, a novel and ultra-efficient Video VAE framework that introduce two key innovations: (1) a lightweight architecture based on a Neighborhood-Aware Feedforward (NAF) module and non-overlapping patch operations, drastically reducing computational cost, and (2) the integration of wavelet transforms and compressed sensing techniques to enhance reconstruction quality. Extensive experiments validate LeanVAE's superiority in video reconstruction and generation, particularly in enhancing efficiency over existing Video VAEs. Our model offers up to 50x fewer FLOPs and 44x faster inference speed while maintaining competitive reconstruction quality, providing insights for scalable, efficient video generation. Our models and code are available at https://github.com/westlake-repl/LeanVAE",
    "checked": true,
    "id": "d4233613baadd347dbb8d8532890bd997572a46d",
    "semantic_title": "leanvae: an ultra-efficient reconstruction vae for video diffusion models",
    "citation_count": 2,
    "authors": [
      "Yu Cheng",
      "Fajie Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Monocular_Semantic_Scene_Completion_via_Masked_Recurrent_Networks_ICCV_2025_paper.html": {
    "title": "Monocular Semantic Scene Completion via Masked Recurrent Networks",
    "volume": "main",
    "abstract": "Monocular Semantic Scene Completion (MSSC) aims to predict the voxel-wise occupancy and semantic category from a single-view RGB image. Existing methods adopt a single-stage framework that aims to simultaneously achieve visible region segmentation and occluded region hallucination, while also being affected by inaccurate depth estimation. Such methods often achieve suboptimal performance, especially in complex scenes. We propose a novel two-stage framework that decomposes MSSC into coarse MSSC followed by the Masked Recurrent Network. Specifically, we propose the Masked Sparse Gated Recurrent Unit (MS-GRU) which concentrates on the occupied regions by the proposed mask updating mechanism, and a sparse GRU design is proposed to reduce the computation cost. Additionally, we propose the distance attention projection to reduce projection errors by assigning different attention scores according to the distance to the observed surface. Experimental results demonstrate that our proposed unified framework, MonoMRN, effectively supports both indoor and outdoor scenes and achieves state-of-the-art performance on the NYUv2 and SemanticKITTI datasets. Furthermore, we conduct robustness analysis under various disturbances, highlighting the role of the Masked Recurrent Network in enhancing the model's resilience to such challenges. The source code is publicly available at: https://github.com/alanWXZ/MonoMRN",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuzhi Wang",
      "Xinran Wu",
      "Song Wang",
      "Lingdong Kong",
      "Ziping Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yan_TurboReg_TurboClique_for_Robust_and_Efficient_Point_Cloud_Registration_ICCV_2025_paper.html": {
    "title": "TurboReg: TurboClique for Robust and Efficient Point Cloud Registration",
    "volume": "main",
    "abstract": "Robust estimation is essential in correspondence-based Point Cloud Registration (PCR). Existing methods using maximal clique search in compatibility graphs achieve high recall but suffer from exponential time complexity, limiting their use in time-sensitive applications. To address this challenge, we propose a fast and robust estimator, TurboReg, built upon a novel lightweight clique, TurboClique, and a highly parallelizable Pivot-Guided Search (PGS) algorithm. First, we define the TurboClique as a 3-clique within a highly-constrained compatibility graph. The lightweight nature of the 3-clique allows for efficient parallel searching, and the highly-constrained compatibility graph ensures robust spatial consistency for stable transformation estimation. Next, PGS selects matching pairs with high SC^2 scores as pivots, effectively guiding the search toward TurboCliques with higher inlier ratios. Moreover, the PGS algorithm has linear time complexity and is significantly more efficient than the maximal clique search with exponential time complexity. Extensive experiments show that TurboReg achieves state-of-the-art performance across multiple real-world datasets, with substantial speed improvements. For example, on the 3DMatch+FCGF dataset, TurboReg (1K) operates 208.22xfaster than 3DMAC while also achieving higher recall. Our code is accessible at \\href https://github.com/Laka-3DV/TurboReg \\texttt TurboReg",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaocheng Yan",
      "Pengcheng Shi",
      "Zhenjun Zhao",
      "Kaixin Wang",
      "Kuang Cao",
      "Ji Wu",
      "Jiayuan Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_AR-VRM_Imitating_Human_Motions_for_Visual_Robot_Manipulation_with_Analogical_ICCV_2025_paper.html": {
    "title": "AR-VRM: Imitating Human Motions for Visual Robot Manipulation with Analogical Reasoning",
    "volume": "main",
    "abstract": "Visual Robot Manipulation (VRM) aims to enable a robot to follow natural language instructions based on robot states and visual observations, and therefore requires costly multi- modal data. To compensate for the deficiency of robot data, existing approaches have employed vision-language pre- training with large-scale data. However, they either utilize web data that differs from robotic tasks, or train the model in an implicit way (e.g., predicting future frames at the pixel level), thus showing limited generalization ability under in- sufficient robot data. In this paper, we propose to learn from large-scale human action video datasets in an explicit way (i.e., imitating human actions from hand keypoints), introduc- ing Visual Robot Manipulation with Analogical Reasoning (AR-VRM). To acquire action knowledge explicitly from hu- man action videos, we propose a keypoint Vision-Language Model (VLM) pretraining scheme, enabling the VLM to learn human action knowledge and directly predict human hand keypoints. During fine-tuning on robot data, to facilitate the robotic arm in imitating the action patterns of human motions, we first retrieve human action videos that perform similar manipulation tasks and have similar historical obser- vations, and then learn the Analogical Reasoning (AR) map between human hand keypoints and robot components. Tak- ing advantage of focusing on action keypoints instead of irrel- evant visual cues, our method achieves leading performance on the CALVIN benchmark and real-world experiments. In few-shot scenarios, our AR-VRM outperforms previous meth- ods by large margins, underscoring the effectiveness of explicitly imitating human actions under data scarcity. Code available at https://github.com/idejie/ar",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dejie Yang",
      "Zijing Zhao",
      "Yang Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Herbreteau_Self-Calibrated_Variance-Stabilizing_Transformations_for_Real-World_Image_Denoising_ICCV_2025_paper.html": {
    "title": "Self-Calibrated Variance-Stabilizing Transformations for Real-World Image Denoising",
    "volume": "main",
    "abstract": "Supervised deep learning has become the method of choice for image denoising. It involves the training of neural networks on large datasets composed of pairs of noisy and clean images. However, the necessity of training data that are specific to the targeted application constrains the widespread use of denoising networks. Recently, several approaches have been developed to overcome this difficulty by whether artificially generating realistic clean/noisy image pairs, or training exclusively on noisy images. In this paper, we show that, contrary to popular belief, denoising networks specialized in the removal of Gaussian noise can be efficiently leveraged in favor of real-world image denoising, even without additional training. For this to happen, an appropriate variance-stabilizing transform (VST) has to be applied beforehand. We propose an algorithm termed Noise2VST for the learning of such a model-free VST. Our approach requires only the input noisy image and an off-the-shelf Gaussian denoiser. We demonstrate through extensive experiments the efficiency and superiority of Noise2VST in comparison to existing methods trained in the absence of specific clean/noisy pairs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sébastien Herbreteau",
      "Michael Unser"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_HccePoseBF_Predicting_Front__Back_Surfaces_to_Construct_Ultra-Dense_2D-3D_ICCV_2025_paper.html": {
    "title": "HccePose(BF): Predicting Front & Back Surfaces to Construct Ultra-Dense 2D-3D Correspondences for Pose Estimation",
    "volume": "main",
    "abstract": "In pose estimation for seen objects, a prevalent pipeline involves using neural networks to predict dense 3D coordinates of the object surface on 2D images, which are then used to establish dense 2D-3D correspondences. However, current methods primarily focus on more efficient encoding techniques to improve the precision of predicted 3D coordinates on the object's front surface, overlooking the potential benefits of incorporating the back surface and interior of the object. To better utilize the full surface and interior of the object, this study predicts 3D coordinates of both the object's front and back surfaces and densely samples 3D coordinates between them. This process creates ultra-dense 2D-3D correspondences, effectively enhancing pose estimation accuracy based on the Perspective-n-Point (PnP) algorithm. Additionally, we propose Hierarchical Continuous Coordinate Encoding (HCCE) to provide a more accurate and efficient representation of front and back surface coordinates. Experimental results show that, compared to existing state-of-the-art (SOTA) methods on the BOP website, the proposed approach outperforms across seven classic BOP core datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yulin Wang",
      "Mengting Hu",
      "Hongli Li",
      "Chen Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ma_Find_Any_Part_in_3D_ICCV_2025_paper.html": {
    "title": "Find Any Part in 3D",
    "volume": "main",
    "abstract": "Why don't we have foundation models in 3D yet? A key limitation is data scarcity. For 3D object part segmentation, existing datasets are small in size and lack diversity. We show that it is possible to break this data barrier by building a data engine powered by 2D foundation models. Our data engine automatically annotates any number of object parts: 1755x more unique part types than existing datasets combined. By training on our annotated data with a simple contrastive objective, we obtain an open-world model that generalizes to any part in any object based on any text query. Even when evaluated zero-shot, we outperform existing methods on the datasets they train on. We achieve 260% improvement in mIoU and boost speed by 6x to 300x. Our scaling analysis confirms that this generalization stems from the data scale, which underscores the impact of our data engine. Finally, to advance general-category open-world 3D part segmentation, we release a benchmark covering a wide range of objects and parts. Project website: https://ziqi-ma.github.io/find3dsite/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziqi Ma",
      "Yisong Yue",
      "Georgia Gkioxari"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tsai_CompleteMe_Reference-based_Human_Image_Completion_ICCV_2025_paper.html": {
    "title": "CompleteMe: Reference-based Human Image Completion",
    "volume": "main",
    "abstract": "Recent methods for human image completion can reconstruct plausible body shapes but often fail to preserve unique details, such as specific clothing patterns or distinctive accessories, without explicit reference images. Even state-of-the-art reference-based inpainting approaches struggle to accurately capture and integrate fine-grained details from reference images. To address this limitation, we propose CompleteMe, a novel reference-based human image completion framework. CompleteMe employs a dual U-Net architecture combined with a Region-focused Attention (RFA) Block, which explicitly guides the model's attention toward relevant regions in reference images. This approach effectively captures fine details and ensures accurate semantic correspondence, significantly improving the fidelity and consistency of completed images. Additionally, we introduce a challenging benchmark specifically designed for evaluating reference-based human image completion tasks. Extensive experiments demonstrate that our proposed method achieves superior visual quality and semantic consistency compared to existing techniques",
    "checked": true,
    "id": "44e962b12a9dfe676ea77ca599068314f2f96501",
    "semantic_title": "completeme: reference-based human image completion",
    "citation_count": 0,
    "authors": [
      "Yu-Ju Tsai",
      "Brian Price",
      "Qing Liu",
      "Luis Figueroa",
      "Daniil Pakhomov",
      "Zhihong Ding",
      "Scott Cohen",
      "Ming-Hsuan Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gu_MMOne_Representing_Multiple_Modalities_in_One_Scene_ICCV_2025_paper.html": {
    "title": "MMOne: Representing Multiple Modalities in One Scene",
    "volume": "main",
    "abstract": "Humans perceive the world through multimodal cues to understand and interact with the environment. Learning a scene representation for multiple modalities enhances comprehension of the physical world. However, modality conflicts, arising from inherent distinctions among different modalities, present two critical challenges: property disparity and granularity disparity. To address these challenges, we propose a general framework, MMOne, to represent multiple modalities in one scene, which can be readily extended to additional modalities. Specifically, a modality modeling module with a novel modality indicator is proposed to capture the unique properties of each modality. Additionally, we design a multimodal decomposition mechanism to separate multi-modal Gaussians into single-modal Gaussians based on modality differences. We address the essential distinctions among modalities by disentangling multimodal information into shared and modality-specific components, resulting in a more compact and efficient multimodal scene representation. Extensive experiments demonstrate that our method consistently enhances the representation capability for each modality and is scalable to additional modalities. The code is available at https://github.com/Neal2020GitHub/MMOne",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhifeng Gu",
      "Bing Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_VA-MoE_Variables-Adaptive_Mixture_of_Experts_for_Incremental_Weather_Forecasting_ICCV_2025_paper.html": {
    "title": "VA-MoE: Variables-Adaptive Mixture of Experts for Incremental Weather Forecasting",
    "volume": "main",
    "abstract": "This paper presents Variables-Adaptive Mixture of Experts (VA-MoE), a novel framework for incremental weather forecasting that dynamically adapts to evolving spatiotemporal patterns in real-time data. Traditional weather prediction models often struggle with exorbitant computational expenditure and the need to continuously update forecasts as new observations arrive. VA-MoE addresses these challenges by leveraging a hybrid architecture of experts, where each expert specializes in capturing distinct sub-patterns of atmospheric variables (e.g., temperature, humidity, wind speed). Moreover, the proposed method employs a variable-adaptive gating mechanism to dynamically select and combine relevant experts based on the input context, enabling efficient knowledge distillation and parameter sharing. This design significantly reduces computational overhead while maintaining high forecast accuracy. Experiments on real-world ERA5 dataset demonstrate that VA-MoE performs comparable against state-of-the-art models in both short-term (e.g., 1-3 days) and long-term (e.g., 5 days) forecasting tasks, with only about 25% of trainable parameters and 50% of the initial training data. Code: https://github.com/chenhao-zju/VAMoE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Chen",
      "Han Tao",
      "Guo Song",
      "Jie Zhang",
      "Yonghan Dong",
      "Yunlong Yu",
      "Lei Bai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Heidinger_2HandedAfforder_Learning_Precise_Actionable_Bimanual_Affordances_from_Human_Videos_ICCV_2025_paper.html": {
    "title": "2HandedAfforder: Learning Precise Actionable Bimanual Affordances from Human Videos",
    "volume": "main",
    "abstract": "When interacting with objects, humans effectively reason about which regions of objects are viable for an intended action, i.e., the affordance regions of the object. They can also account for subtle differences in object regions based on the task to be performed and whether one or two hands need to be used. However, current vision-based affordance prediction methods often reduce the problem to naive object part segmentation. In this work, we propose a framework for extracting affordance data from human activity video datasets. Our extracted 2HANDS dataset contains precise object affordance region segmentations and affordance class-labels as narrations of the activity performed. The data also accounts for bimanual actions, i.e., two hands co-ordinating and interacting with one or more objects. We present a VLM-based affordance prediction model, 2HandedAfforder, trained on the dataset and demonstrate superior performance over baselines in affordance region segmentation for various activities. Finally, we show that our predicted affordance regions are actionable, i.e., can be used by an agent performing a task, through demonstration in robotic manipulation scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marvin Heidinger",
      "Snehal Jauhri",
      "Vignesh Prasad",
      "Georgia Chalvatzaki"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_GenDoP_Auto-regressive_Camera_Trajectory_Generation_as_a_Director_of_Photography_ICCV_2025_paper.html": {
    "title": "GenDoP: Auto-regressive Camera Trajectory Generation as a Director of Photography",
    "volume": "main",
    "abstract": "Camera trajectory design plays a crucial role in video production, serving as a fundamental tool for conveying directorial intent and enhancing visual storytelling. In cinematography, Directors of Photography meticulously craft camera movements to achieve expressive and intentional framing. However, existing methods for camera trajectory generation remain limited: Traditional approaches rely on geometric optimization or handcrafted procedural systems, while recent learning-based methods often inherit structural biases or lack textual alignment, constraining creative synthesis. In this work, we introduce an auto-regressive model inspired by the expertise of Directors of Photography to generate artistic and expressive camera trajectories. We first introduce DataDoP, a large-scale multi-modal dataset containing 29K real-world shots with free-moving camera trajectories, depth maps, and detailed captions in specific movements, interaction with the scene, and directorial intent. Thanks to the comprehensive and diverse database, we further train an auto-regressive, decoder-only Transformer for high-quality, context-aware camera movement generation based on text guidance and RGBD inputs, named GenDoP. Extensive experiments demonstrate that compared to existing methods, GenDoP offers better controllability, finer-grained trajectory adjustments, and higher motion stability. We believe our approach establishes a new standard for learning-based cinematography, paving the way for future advancements in camera control and filmmaking. Our project website: https://kszpxxzmc.github.io/GenDoP",
    "checked": true,
    "id": "b61c9cb435e4bcd31cf01d0962d20c20a7dd9908",
    "semantic_title": "gendop: auto-regressive camera trajectory generation as a director of photography",
    "citation_count": 2,
    "authors": [
      "Mengchen Zhang",
      "Tong Wu",
      "Jing Tan",
      "Ziwei Liu",
      "Gordon Wetzstein",
      "Dahua Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Dai_Latent_Swap_Joint_Diffusion_for_2D_Long-Form_Latent_Generation_ICCV_2025_paper.html": {
    "title": "Latent Swap Joint Diffusion for 2D Long-Form Latent Generation",
    "volume": "main",
    "abstract": "This paper introduces Swap Forward (SaFa), a modality-agnostic and efficient method to generate seamless and coherent long spectrum and panorama using a latent swap joint diffusion process across multi-views. We first investigate spectrum aliasing problem in spectrum-based audio generation caused by existing joint diffusion methods. Through a comparative analysis of the VAE latent representation of spectra and RGB images, we identify that the failure arises from excessive suppression of high-frequency components due to the step-wise averaging operator. To address this issue, we propose Self-Loop Latent Swap, a frame-level bidirectional swap operator, applied to the overlapping region of adjacent views. Leveraging step-wise differentiated trajectories, this swap operator avoids spectrum distortion and adaptively enhances high-frequency components. Furthermore, to improve global cross-view consistency in non-overlapping regions, we introduce Reference-Guided Latent Swap, a unidirectional latent swap operator that provides a centralized reference trajectory to synchronize subview diffusions. By refining swap timing and intervals, we canachieve a balance between cross-view similarity and diversity in a feed-forward manner. Quantitative and qualitative experiments demonstrate that SaFa significantly outperforms existing joint diffusion methods and even training-based methods in audio generation using both U-Net and DiT models. It also adapts well to panorama generation, achieving comparable performance with a 2 xto 20 xspeedup. The project website is available at https://swapforward.github.io",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yusheng Dai",
      "Chenxi Wang",
      "Chang Li",
      "Chen Wang",
      "Kewei Li",
      "Jun Du",
      "Lei Sun",
      "Jianqing Gao",
      "Ruoyu Wang",
      "Jiefeng Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Urakawa_Neural_Inverse_Rendering_for_High-Accuracy_3D_Measurement_of_Moving_Objects_ICCV_2025_paper.html": {
    "title": "Neural Inverse Rendering for High-Accuracy 3D Measurement of Moving Objects with Fewer Phase-Shifting Patterns",
    "volume": "main",
    "abstract": "Among structured-light methods, the phase-shifting approach enables high-resolution and high-accuracy measurements using a minimum of three patterns. However, its performance is significantly affected when dynamic and complex-shaped objects are measured, as motion artifacts and phase inconsistencies can degrade accuracy. In this study, we propose an enhanced phase-shifting method that incorporates neural inverse rendering to enable the 3D measurement of moving objects. To effectively capture object motion, we introduce a displacement field into the rendering model, which accurately represents positional changes and mitigates motion-induced distortions. Additionally, to achieve high-precision reconstruction with fewer phase-shifting patterns, we design a multiview-rendering framework that utilizes multiple cameras in conjunction with a single projector. Comparisons with state-of-the-art methods and various ablation studies demonstrated that our method accurately reconstructs the shapes of moving objects, even with a small number of patterns, using only simple, well-known phase-shifting patterns",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuki Urakawa",
      "Yoshihiro Watanabe"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kondoh_Embodied_Navigation_with_Auxiliary_Task_of_Action_Description_Prediction_ICCV_2025_paper.html": {
    "title": "Embodied Navigation with Auxiliary Task of Action Description Prediction",
    "volume": "main",
    "abstract": "The field of multimodal robot navigation in indoor environments has garnered significant attention in recent years. However, as tasks and methods become more advanced, the action decision systems tend to become more complex and operate as black-boxes. For a reliable system, the ability to explain or describe its decisions is crucial; however, there tends to be a trade-off in that explainable systems cannot outperform non-explainable systems in terms of performance. In this paper, we propose incorporating the task of describing actions in language into the reinforcement learning of navigation as an auxiliary task. Existing studies have found it difficult to incorporate describing actions into reinforcement learning due to the absence of ground-truth data. We address this issue by leveraging knowledge distillation from pre-trained description generation models, such as vision-language models. We comprehensively evaluate our approach across various navigation tasks, demonstrating that it can describe actions while attaining high navigation performance. Furthermore, it achieves state-of-the-art performance in the particularly challenging multimodal navigation task of semantic audio-visual navigation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haru Kondoh",
      "Asako Kanezaki"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Taming_the_Untamed_Graph-Based_Knowledge_Retrieval_and_Reasoning_for_MLLMs_ICCV_2025_paper.html": {
    "title": "Taming the Untamed: Graph-Based Knowledge Retrieval and Reasoning for MLLMs to Conquer the Unknown",
    "volume": "main",
    "abstract": "The real value of knowledge lies not just in its accumulation, but in its potential to be harnessed effectively to conquer the unknown. Although recent multimodal large language models (MLLMs) exhibit impressing multimodal capabilities, they often fail in rarely encountered domain-specific tasks due to limited relevant knowledge. To explore this, we adopt visual game cognition as a testbed and select \"Monster Hunter: World\" as the target to construct a multimodal knowledge graph (MH-MMKG), which incorporates multi-modalities and intricate entity relations. We also design a series of challenging queries based on MH-MMKG to evaluate the models' ability for complex knowledge retrieval and reasoning. Furthermore, we propose a multi-agent retriever that enables a model to autonomously search relevant knowledge without additional training. Experimental results show that our approach significantly enhances the performance of MLLMs, providing a new perspective on multimodal knowledge-augmented reasoning and laying a solid foundation for future research",
    "checked": true,
    "id": "2a948ac8d1d922336bbb4eb18303e5b0be0c9d6c",
    "semantic_title": "taming the untamed: graph-based knowledge retrieval and reasoning for mllms to conquer the unknown",
    "citation_count": 0,
    "authors": [
      "Bowen Wang",
      "Zhouqiang Jiang",
      "Yasuaki Susumu",
      "Shotaro Miwa",
      "Tianwei Chen",
      "Yuta Nakashima"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Velez_From_Image_to_Video_An_Empirical_Study_of_Diffusion_Representations_ICCV_2025_paper.html": {
    "title": "From Image to Video: An Empirical Study of Diffusion Representations",
    "volume": "main",
    "abstract": "Diffusion models have revolutionized generative modeling, enabling unprecedented realism in image and video synthesis.This success has sparked interest in leveraging their representations for visual understanding tasks. While recent works have explored this potential for image generation, the visual understanding capabilities of video diffusion models remain largely uncharted. To address this gap, we analyze the performance of latent image and video diffusion representations on various downstream tasks including image classification, action recognition, depth estimation, and tracking. For the most informative comparison, we utilize the same model architecture, WALT, across image and video generation objectives. Our results show that video generation pre-training consistently outperforms its image counterpart, though we find a striking range in the extent of this superiority. We further analyze features extracted from different layers and with varying noise levels, as well as the effect of model size and training budget on representation and generation quality. This work marks the first direct comparison of video and image diffusion objectives for visual understanding, offering insights into the role of temporal information in representation learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pedro Vélez",
      "Luisa F. Polanía",
      "Yi Yang",
      "Chuhan Zhang",
      "Rishabh Kabra",
      "Anurag Arnab",
      "Mehdi S. M. Sajjadi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liang_Diffusion_Curriculum_Synthetic-to-Real_Data_Curriculum_via_Image-Guided_Diffusion_ICCV_2025_paper.html": {
    "title": "Diffusion Curriculum: Synthetic-to-Real Data Curriculum via Image-Guided Diffusion",
    "volume": "main",
    "abstract": "Low-quality or scarce data has posed significant challenges for training deep neural networks in practice. While classical data augmentation cannot contribute very different new data, diffusion models opens up a new door to build self-evolving AI by generating high-quality and diverse synthetic data through text-guided prompts. However, text-only guidance cannot control synthetic images' proximity to the original images, resulting in out-of-distribution data detrimental to the model performance. To overcome the limitation, we study image guidance to achieve a spectrum of interpolations between synthetic and real images. With stronger image guidance, the generated images are similar to the training data but hard to learn. While with weaker image guidance, the synthetic images will be easier for model but contribute to a larger distribution gap with the original data. The generated full spectrum of data enables us to build a novel \"Diffusion CurricuLum (DisCL)\". DisCL adjusts the image guidance level of image synthesis for each training stage: It identifies and focuses on hard samples for the model and assesses the most effective guidance level of synthetic images to improve hard data learning. We apply DisCL to two challenging tasks: long-tail (LT) classification and learning from low-quality data. It focuses on lower-guidance images of high-quality to learn prototypical features as a warm-up of learning higher-guidance images that might be weak on diversity or quality. Extensive experiments showcase a gain of 2.7% and 2.1% in OOD and ID macro-accuracy when applying DisCL to iWildCam dataset. On ImageNet-LT, DisCL improves the base model's tail-class accuracy from 4.4% to 23.64% and leads to a 4.02% improvement in all-class accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yijun Liang",
      "Shweta Bhardwaj",
      "Tianyi Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Dynamic-VLM_Simple_Dynamic_Visual_Token_Compression_for_VideoLLM_ICCV_2025_paper.html": {
    "title": "Dynamic-VLM: Simple Dynamic Visual Token Compression for VideoLLM",
    "volume": "main",
    "abstract": "The application of Large Vision-Language Models (LVLMs) for analyzing images and videos is an exciting and rapidly evolving field. In recent years, we've seen significant growth in high-quality image-text datasets for fine-tuning image understanding, but there is still a lack of comparable datasets for videos. Additionally, many VideoLLMs are extensions of single-image VLMs, which may not efficiently handle the complexities of longer videos. In this study, we introduce a large-scale synthetic dataset created from proprietary models, using carefully designed prompts to tackle a wide range of questions. We also explore a dynamic visual token compression architecture that strikes a balance between computational efficiency and performance. Our proposed Dynamic-VLM achieves state-of-the-art results across various video tasks and shows impressive generalization, setting new baselines in multi-image understanding. Notably, Dynamic-VLM delivers an absolute improvement of 2.7% over LLaVA-OneVision on VideoMME and 10.7% on MuirBench",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han  Wang",
      "Yuxiang Nie",
      "Yongjie Ye",
      "Yanjie Wang",
      "Shuai Li",
      "Haiyang Yu",
      "Jinghui Lu",
      "Can Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tian_CCL-LGS_Contrastive_Codebook_Learning_for_3D_Language_Gaussian_Splatting_ICCV_2025_paper.html": {
    "title": "CCL-LGS: Contrastive Codebook Learning for 3D Language Gaussian Splatting",
    "volume": "main",
    "abstract": "Recent advances in 3D reconstruction techniques and vision-language models have fueled significant progress in 3D semantic understanding, a capability critical to robotics, autonomous driving, and virtual/augmented reality. However, methods that rely on 2D priors are prone to a critical challenge: cross-view semantic inconsistencies induced by occlusion, image blur, and view-dependent variations. These inconsistencies, when propagated via projection supervision, deteriorate the quality of 3D Gaussian semantic fields and introduce artifacts in the rendered outputs. To mitigate this limitation, we propose CCL-LGS, a novel framework that enforces view-consistent semantic supervision by integrating multi-view semantic cues. Specifically, our approach first employs a zero-shot tracker to align a set of SAM-generated 2D masks and reliably identify their corresponding categories. Next, we utilize CLIP to extract robust semantic encodings across views. Finally, our Contrastive Codebook Learning (CCL) module distills discriminative semantic features by enforcing intra-class compactness and inter-class distinctiveness. In contrast to previous methods that directly apply CLIP to imperfect masks, our framework explicitly resolves semantic conflicts while preserving category discriminability. Extensive experiments demonstrate that CCL-LGS outperforms previous state-of-the-art methods. Our project page is available at https://epsilontl.github.io/CCL-LGS/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Tian",
      "Xiaomin Li",
      "Liqian Ma",
      "Hao Yin",
      "Zirui Zheng",
      "Hefei Huang",
      "Taiqing Li",
      "Huchuan Lu",
      "Xu Jia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_TRACE_Learning_3D_Gaussian_Physical_Dynamics_from_Multi-view_Videos_ICCV_2025_paper.html": {
    "title": "TRACE: Learning 3D Gaussian Physical Dynamics from Multi-view Videos",
    "volume": "main",
    "abstract": "In this paper, we aim to model 3D scene geometry, appearance, and physical information just from dynamic multi-view videos in the absence of any human labels. By leveraging physics-informed losses as soft constraints or integrating simple physics models into neural networks, existing works often fail to learn complex motion physics, or doing so requires additional labels such as object types or masks. In this paper, we propose a new framework named **TRACE** to model the motion physics of complex dynamic 3D scenes. The key novelty of our approach is that, by formulating each 3D point as a rigid particle with size and orientation in space, we choose to directly learn a translation rotation dynamics system for each particle, explicitly estimating a complete set of physical parameters to govern the particle's motion over time. Extensive experiments on three existing dynamic datasets and one newly created challenging synthetic datasets demonstrate the extraordinary performance of our method over baselines in the task of future frame extrapolation. A nice property of our framework is that multiple objects or parts can be easily segmented just by clustering the learned physical parameters. Our datasets and code will be released at https://github.com/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinxi Li",
      "Ziyang Song",
      "Bo Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sun_Two_Losses_One_Goal_Balancing_Conflict_Gradients_for_Semi-supervised_Semantic_ICCV_2025_paper.html": {
    "title": "Two Losses, One Goal: Balancing Conflict Gradients for Semi-supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "Semi-supervised semantic segmentation has attracted considerable attention as it alleviates the need for extensive pixel-level annotations. However, existing methods often overlook the potential optimization conflict between supervised and unsupervised learning objectives, leading to suboptimal performance. In this paper, we identify this under-explored issue and propose a novel Pareto Optimization Strategy (POS) to tackle it. POS aims to find a descent gradient direction that benefits both learning objectives, thereby facilitating model training. By dynamically assigning weights to the gradients at each iteration based on the model's learning status, POS effectively reconciles the intrinsic tension between the two objectives. Furthermore, we analyze POS from the perspective of gradient descent in random batch sampling and propose the Magnitude Enhancement Operation (MEO) to further unleash its potential by considering both direction and magnitude during gradient integration. Extensive experiments on challenging benchmarks demonstrate that integrating POS into existing semi-supervised segmentation methods yields consistent improvements across different data splits and architectures (CNN, Transformer), showcasing its effectiveness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Sun",
      "Huayu Mai",
      "Wangkai Li",
      "Yujia Chen",
      "Yuan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_FreeCus_Free_Lunch_Subject-driven_Customization_in_Diffusion_Transformers_ICCV_2025_paper.html": {
    "title": "FreeCus: Free Lunch Subject-driven Customization in Diffusion Transformers",
    "volume": "main",
    "abstract": "In light of recent breakthroughs in text-to-image (T2I) generation, particularly with diffusion transformers (DiT), subject-driven technologies are increasingly being employed for high-fidelity customized production that preserves subject identity from reference inputs, enabling thrilling design workflows and engaging entertainment. Existing alternatives typically require either per-subject optimization via trainable text embeddings or training specialized encoders for subject feature extraction on large-scale datasets. Such dependencies on training procedures fundamentally constrain their practical applications. More importantly, current methodologies fail to fully leverage the inherent zero-shot potential of modern diffusion transformers (e.g., the Flux series) for authentic subject-driven synthesis. To bridge this gap, we propose FreeCus, a genuinely training-free framework that activates DiT's capabilities through three key innovations: 1) We introduce a pivotal attention sharing mechanism that captures the subject's layout integrity while preserving crucial editing flexibility. 2) Through a straightforward analysis of DiT's dynamic shifting, we propose an upgraded variant that significantly improves fine-grained feature extraction. 3) We further integrate advanced Multimodal Large Language Models (MLLMs) to enrich cross-modal semantic representations. Extensive experiments reflect that our method successfully unlocks DiT's zero-shot ability for consistent subject synthesis across diverse contexts, achieving state-of-the-art or comparable results compared to approaches that require additional training. Notably, our framework demonstrates seamless compatibility with existing inpainting pipelines and control modules, facilitating more compelling experiences. Our code is available at: https://github.com/Monalissaa/FreeCus",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanbing Zhang",
      "Zhe Wang",
      "Qin Zhou",
      "Mengping Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kumar_Trokens_Semantic-Aware_Relational_Trajectory_Tokens_for_Few-Shot_Action_Recognition_ICCV_2025_paper.html": {
    "title": "Trokens: Semantic-Aware Relational Trajectory Tokens for Few-Shot Action Recognition",
    "volume": "main",
    "abstract": "Video understanding requires effective modeling of both motion and appearance information, particularly for few-shot action recognition. While recent advances in point tracking have been shown to improve few-shot action recognition, two fundamental challenges persist: selecting informative points to track and effectively modeling their motion patterns. We present Trokens, a novel approach that transforms trajectory points into semantic-aware relational tokens for action recognition. First, we introduce a semantic-aware sampling strategy to adaptively distribute tracking points based on object scale and semantic relevance. Second, we develop a motion modeling framework that captures both intra-trajectory dynamics through the Histogram of Oriented Displacements (HoD) and inter-trajectory relationships to model complex action patterns. Our approach effectively combines these trajectory tokens with semantic features to enhance appearance features with motion information, achieving state-of-the-art performance across six diverse few-shot action recognition benchmarks: Something-Something-V2 (both full and small splits), Kinetics, UCF101, HMDB51, and FineGym",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pulkit Kumar",
      "Shuaiyi Huang",
      "Matthew Walmer",
      "Sai Saketh Rambhatla",
      "Abhinav Shrivastava"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xiao_MotionStreamer_Streaming_Motion_Generation_via_Diffusion-based_Autoregressive_Model_in_Causal_ICCV_2025_paper.html": {
    "title": "MotionStreamer: Streaming Motion Generation via Diffusion-based Autoregressive Model in Causal Latent Space",
    "volume": "main",
    "abstract": "This paper addresses the challenge of text-conditioned streaming motion generation, which requires us to predict the next-step human pose based on variable-length historical motions and incoming texts. Existing methods struggle to achieve streaming motion generation, e.g., diffusion models are constrained by pre-defined motion lengths, while GPT-based methods suffer from delayed response and error accumulation problem due to discretized non-causal tokenization. To solve these problems, we propose MotionStreamer, a novel framework that incorporates a continuous causal latent space into a probabilistic autoregressive model. The continuous latents mitigate information loss caused by discretization and effectively reduce error accumulation during long-term autoregressive generation. In addition, by establishing temporal causal dependencies between current and historical motion latents, our model fully utilizes the available information to achieve accurate online motion decoding. Experiments show that our method outperforms existing approaches while offering more applications, including multi-round generation, long-term generation, and dynamic motion composition. Project Page: https://zju3dv.github.io/MotionStreamer/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lixing Xiao",
      "Shunlin Lu",
      "Huaijin Pi",
      "Ke Fan",
      "Liang Pan",
      "Yueer Zhou",
      "Ziyong Feng",
      "Xiaowei Zhou",
      "Sida Peng",
      "Jingbo Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hu_RS-vHeat_Heat_Conduction_Guided_Efficient_Remote_Sensing_Foundation_Model_ICCV_2025_paper.html": {
    "title": "RS-vHeat: Heat Conduction Guided Efficient Remote Sensing Foundation Model",
    "volume": "main",
    "abstract": "Remote sensing foundation models largely break away from the traditional paradigm of designing task-specific models, offering greater scalability across multiple tasks. However, they face challenges such as low computational efficiency and limited interpretability, especially when dealing with large-scale remote sensing images. To overcome these, we draw inspiration from heat conduction, a physical process modeling local heat diffusion. Building on this idea, we are the first to explore the potential of using the parallel computing model of heat conduction to simulate the local region correlations in high-resolution remote sensing images, and introduce RS-vHeat, an efficient multi-modal remote sensing foundation model. Specifically, RS-vHeat 1) applies the Heat Conduction Operator (HCO) with a complexity of O(N^ 1.5 ) and a global receptive field, reducing computational overhead while capturing remote sensing object structure information to guide heat diffusion; 2) learns the frequency distribution representations of various scenes through a self-supervised strategy based on frequency domain hierarchical masking and multi-domain reconstruction; 3) significantly improves efficiency and performance over state-of-the-art techniques across 4 tasks and 10 datasets. Compared to attention-based remote sensing foundation models, we reduce memory usage by 84%, FLOPs by 24% and improves throughput by 2.7 times. The code will be made publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huiyang Hu",
      "Peijin Wang",
      "Hanbo Bi",
      "Boyuan Tong",
      "Zhaozhi Wang",
      "Wenhui Diao",
      "Hao Chang",
      "Yingchao Feng",
      "Ziqi Zhang",
      "Yaowei Wang",
      "Qixiang Ye",
      "Kun Fu",
      "Xian Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhu_EA-Vit_Efficient_Adaptation_for_Elastic_Vision_Transformer_ICCV_2025_paper.html": {
    "title": "EA-Vit: Efficient Adaptation for Elastic Vision Transformer",
    "volume": "main",
    "abstract": "Vision Transformers (ViTs) have emerged as a foundational model in computer vision, excelling in generalization and adaptation to downstream tasks. However, deploying ViTs to support diverse resource constraints typically requires retraining multiple, size-specific ViTs, which is both time-consuming and energy-intensive. To address this issue, we propose an efficient ViT adaptation framework that enables a single adaptation process to generate multiple models of varying sizes for deployment on platforms with various resource constraints. Our approach comprises two stages. In the first stage, we enhance a pre-trained ViT with a nested elastic architecture that enables structural flexibility across MLP expansion ratio, number of attention heads, embedding dimension, and network depth. To preserve pre-trained knowledge and ensure stable adaptation, we adopt a curriculum-based training strategy that progressively increases elasticity. In the second stage, we design a lightweight router to select submodels according to computational budgets and downstream task demands. Initialized with Pareto-optimal configurations derived via a customized NSGA-II algorithm, the router is then jointly optimized with the backbone. Extensive experiments on multiple benchmarks demonstrate the effectiveness and versatility of EA-ViT. The code is available at https://github.com/zcxcf/EA-ViT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Zhu",
      "Wangbo Zhao",
      "Huiwen Zhang",
      "Yuhao Zhou",
      "Weidong Tang",
      "Shuo Wang",
      "Zhihang Yuan",
      "Yuzhang Shang",
      "Xiaojiang Peng",
      "Kai Wang",
      "Dawei Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhu_Aether_Geometric-Aware_Unified_World_Modeling_ICCV_2025_paper.html": {
    "title": "Aether: Geometric-Aware Unified World Modeling",
    "volume": "main",
    "abstract": "The integration of geometric reconstruction and generative modeling remains a critical challenge in developing AI systems capable of human-like spatial reasoning. This paper proposes Aether, a unified framework that enables geometry-aware reasoning in world models by jointly optimizing three core capabilities: (1) 4D dynamic reconstruction, (2) action-conditioned video prediction, and (3) goal-conditioned visual planning. Through task-interleaved feature learning, Aether achieves synergistic knowledge sharing across reconstruction, prediction, and planning objectives. Building upon video generation models, our framework demonstrates zero-shot synthetic-to-real generalization despite never observing real-world data during training. Furthermore, our approach achieves zero-shot generalization in both action following and reconstruction tasks, thanks to its intrinsic geometric modeling. Notably, even without real-world data, its reconstruction performance is comparable with or even better than that of domain-specific models. Additionally, Aether employs camera trajectories as geometry-informed action spaces, enabling effective action-conditioned prediction and visual planning. We hope our work inspires the community to explore new frontiers in physically-reasonable world modeling and its applications",
    "checked": true,
    "id": "249804a0941fc402d2bd130c02f094dddebead00",
    "semantic_title": "aether: geometric-aware unified world modeling",
    "citation_count": 31,
    "authors": [
      "Haoyi Zhu",
      "Yifan Wang",
      "Jianjun Zhou",
      "Wenzheng Chang",
      "Yang Zhou",
      "Zizun Li",
      "Junyi Chen",
      "Chunhua Shen",
      "Jiangmiao Pang",
      "Tong He"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_ArtEditor_Learning_Customized_Instructional_Image_Editor_from_Few-Shot_Examples_ICCV_2025_paper.html": {
    "title": "ArtEditor: Learning Customized Instructional Image Editor from Few-Shot Examples",
    "volume": "main",
    "abstract": "We introduce ArtEditor, a novel framework for instruction-based image editing that learns unique editing styles from few-shot examples. While image editing has seen significant advancements, customized instructional editing remains underexplored. Existing methods often rely on complex, multi-stage pipelines that are difficult to adapt to specific styles. Additionally, this domain lacks a standardized benchmark, making it challenging to evaluate progress. To address these issues, we propose ArtEditor, a two-stage training framework. In the first stage, we train ArtEditor-Base, a general-purpose image editing model, on large-scale datasets to build a strong foundational capability. In the second stage, we fine-tune this model using ArtEditor-LoRA, a lightweight adaptation module, on a small dataset of before-and-after image pairs. This approach enables the model to efficiently learn distinct editing styles and techniques with minimal data. To enhance the performance of a pre-trained Diffusion Transformer (DiT) model, we introduce two key innovations: position encoding cloning and a noise-free conditioning paradigm. These techniques ensure stable and coherent edits, even when adapting to new styles. To support research in this area, we contribute the DoodleArt dataset, the first benchmark specifically designed for customized image editing. DoodleArt features six high-quality artistic styles created by professional artists and designers, providing a valuable resource for evaluating and advancing future work. Extensive experiments demonstrate that ArtEditor achieves superior performance and robustness in customized image editing. Our framework opens new possibilities for artistic creation, offering artists intuitive and flexible tools to bring their visions to life",
    "checked": false,
    "id": "4cd9389c227b771ec40cf09d9e7657391098f285",
    "semantic_title": "an opinion on chatgpt in health care—written by humans only",
    "citation_count": 45,
    "authors": [
      "Shijie Huang",
      "Yiren Song",
      "Yuxuan Zhang",
      "Hailong Guo",
      "Xueyin Wang",
      "Jiaming Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_LiON-LoRA_Rethinking_LoRA_Fusion_to_Unify_Controllable_Spatial_and_Temporal_ICCV_2025_paper.html": {
    "title": "LiON-LoRA: Rethinking LoRA Fusion to Unify Controllable Spatial and Temporal Generation for Video Diffusion",
    "volume": "main",
    "abstract": "Video Diffusion Models (VDMs) have demonstrated remarkable capabilities in synthesizing realistic videos by learning from large-scale data. Although vanilla Low-Rank Adaptation (LoRA) can learn specific spatial or temporal movement to driven VDMs with constrained data, achieving precise control over both camera trajectories and object motion remains challenging due to the unstable fusion and non-linear scalability. To address these issues, we propose LiON-LoRA, a novel framework that rethinks LoRA fusion through three core principles: Linear scalability, Orthogonality, and Norm consistency. First, we analyze the orthogonality of LoRA features in shallow VDM layers, enabling decoupled low-level controllability. Second, norm consistency is enforced across layers to stabilize fusion during complex camera motion combinations. Third, a controllable token is integrated into the diffusion transformer (DiT) to linearly adjust motion amplitudes for both cameras and objects with a modified self-attention mechanism to ensure decoupled control. Additionally, we extend LiON-LoRA to temporal generation by leveraging static-camera videos, unifying spatial and temporal controllability. Experiments demonstrate that LiON-LoRA outperforms state-of-the-art methods in trajectory control accuracy and motion strength adjustment, achieving superior generalization with minimal training data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yisu Zhang",
      "Chenjie Cao",
      "Chaohui Yu",
      "Jianke Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_VLIPP_Towards_Physically_Plausible_Video_Generation_with_Vision_and_Language_ICCV_2025_paper.html": {
    "title": "VLIPP: Towards Physically Plausible Video Generation with Vision and Language Informed Physical Prior",
    "volume": "main",
    "abstract": "Video diffusion models (VDMs) have advanced significantly in recent years, enabling the generation of highly realistic videos and drawing the attention of the community in their potential as world simulators. However, despite their capabilities, VDMs often fail to produce physically plausible videos due to an inherent lack of understanding of physics, resulting in incorrect dynamics and event sequences. To address this limitation, we propose a novel two-stage image-to-video generation framework that explicitly incorporates physics with vision and language informed physical prior. In the first stage, we employ a Vision Language Model (VLM) as a coarse-grained motion planner, integrating chain-of-thought and physics-aware reasoning to predict a rough motion trajectories/changes that approximate real-world physical dynamics while ensuring the inter-frame consistency. In the second stage, we use the predicted motion trajectories/changes to guide the video generation of a VDM. As the predicted motion trajectories/changes are rough, noise is added during inference to provide freedom to the VDM in generating motion with more fine details. Extensive experimental results demonstrate that our framework can produce physically plausible motion, and comparative evaluations highlight the notable superiority of our approach over existing methods. More video results and code are available on our Project Page: https://madaoer.github.io/projects/physically_plausible_video_generation/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xindi Yang",
      "Baolu Li",
      "Yiming Zhang",
      "Zhenfei Yin",
      "Lei Bai",
      "Liqian Ma",
      "Zhiyong Wang",
      "Jianfei Cai",
      "Tien-Tsin Wong",
      "Huchuan Lu",
      "Xu Jia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xu_SAM4D_Segment_Anything_in_Camera_and_LiDAR_Streams_ICCV_2025_paper.html": {
    "title": "SAM4D: Segment Anything in Camera and LiDAR Streams",
    "volume": "main",
    "abstract": "We present SAM4D, a multi-modal and temporal foundation model designed for promptable segmentation across camera and LiDAR streams. Unified Multi-modal Positional Encoding (UMPE) is introduced to align camera and LiDAR features in a shared 3D space, enabling seamless cross-modal prompting and interaction. Additionally, we propose Motion-aware Cross-modal Memory Attention (MCMA), which leverages ego-motion compensation to enhance temporal consistency and long-horizon feature retrieval, ensuring robust segmentation across dynamically changing autonomous driving scenes. To avoid annotation bottlenecks, we develop a multi-modal automated data engine that synergizes VFM-driven video masklets, spatiotemporal 4D reconstruction, and cross-modal masklet fusion. This framework generates camera-LiDAR aligned pseudo-labels at a speed orders of magnitude faster than human annotation while preserving VFM-derived semantic fidelity in point cloud representations. We conduct extensive experiments on the constructed Waymo-4DSeg, which demonstrate the powerful cross-modal segmentation ability and great potential in data annotation of proposed SAM4D",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianyun Xu",
      "Song Wang",
      "Ziqian Ni",
      "Chunyong Hu",
      "Sheng Yang",
      "Jianke Zhu",
      "Qiang Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wei_DreamRelation_Relation-Centric_Video_Customization_ICCV_2025_paper.html": {
    "title": "DreamRelation: Relation-Centric Video Customization",
    "volume": "main",
    "abstract": "Relational video customization refers to the creation of personalized videos that depict user-specified relations between two subjects, a crucial task for comprehending real-world visual content. While existing methods can personalize subject appearances and motions, they still struggle with complex relational video customization, where precise relational modeling and high generalization across subject categories are essential. The primary challenge arises from the intricate spatial arrangements, layout variations, and nuanced temporal dynamics inherent in relations; consequently, current models tend to overemphasize irrelevant visual details rather than capturing meaningful interactions. To address these challenges, we propose DreamRelation, a novel approach that personalizes relations through a small set of exemplar videos, leveraging two key components: Relational Decoupling Learning and Relational Dynamics Enhancement. First, in Relational Decoupling Learning, we disentangle relations from subject appearances using relation LoRA triplet and hybrid mask training strategy, ensuring better generalization across diverse relationships. Furthermore, we determine the optimal design of relation LoRA triplet by analyzing the distinct roles of the query, key, and value features within MM-DiT's attention mechanism, making DreamRelation the first relational video generation framework with explainable components. Second, in Relational Dynamics Enhancement, we introduce space-time relational contrastive loss, which prioritizes relational dynamics while minimizing the reliance on detailed subject appearances. Extensive experiments demonstrate that DreamRelation outperforms state-of-the-art methods in relational video customization. Project page at: https://dreamrelation.github.io/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujie Wei",
      "Shiwei Zhang",
      "Hangjie Yuan",
      "Biao Gong",
      "Longxiang Tang",
      "Xiang Wang",
      "Haonan Qiu",
      "Hengjia Li",
      "Shuai Tan",
      "Yingya Zhang",
      "Hongming Shan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_DropletVideo_A_Dataset_and_Approach_to_Explore_Integral_Spatio-Temporal_Consistent_ICCV_2025_paper.html": {
    "title": "DropletVideo: A Dataset and Approach to Explore Integral Spatio-Temporal Consistent Video Generation",
    "volume": "main",
    "abstract": "Spatio-temporal consistency is a critical topic in video generation. A qualified generated video segment must ensure plot plausibility and coherence while maintaining visual consistency of objects and scenes across varying viewpoints. Prior research, especially in open-source projects, primarily focuses on either temporal or spatial consistency, or their basic combination, such as appending a camera-movement description after a prompt without constraining its outcomes. However, camera movement may introduce new objects to the scene or eliminate existing ones, thereby affecting the preceding narrative. Especially in videos with numerous camera movements, the interplay between multiple plots becomes increasingly complex. This paper introduces and examines integral spatio-temporal consistency, considering the synergy between plot progression and camera techniques, and the long-term impact of prior content on subsequent generation. Our research encompasses dataset construction through to model development. Initially, we constructed DropletVideo-10M, which comprises 10 million videos that feature dynamic camera motion and object actions. With an average length of 206 words, the captions offer detailed accounts of camera movements. Following this, we developed the DropletVideo model, which excels in preserving spatio-temporal coherence during video generation. The work has been open-sourced: https://dropletx.github.io/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runze Zhang",
      "Guoguang Du",
      "Xiaochuan Li",
      "Qi Jia",
      "Liang Jin",
      "Lu Liu",
      "Jingjing Wang",
      "Cong Xu",
      "Zhenhua Guo",
      "Yaqian Zhao",
      "Xiaoli Gong",
      "Rengang Li",
      "Baoyu Fan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Rahmanzadehgervi_TAB_Transformer_Attention_Bottlenecks_enable_User_Intervention_and_Debugging_in_ICCV_2025_paper.html": {
    "title": "TAB: Transformer Attention Bottlenecks enable User Intervention and Debugging in Vision-Language Models",
    "volume": "main",
    "abstract": "Multi-head self-attention (MHSA) is a key component of Transformers, a widely popular architecture in both language and vision. Multiple heads intuitively enable different parallel processes over the same input. Yet, they also obscure the attribution of each input patch to the output of a model. We propose a novel 1-head Transformer Attention Bottleneck (TAB) layer, inserted after the traditional MHSA architecture, to serve as an attention bottleneck for interpretability and intervention. Unlike standard self-attention, TAB constrains the total attention over all patches to \\in [0, 1]. That is, when the total attention is 0, no visual information is propagated further into the network, and the vision-language model (VLM) would default to a generic, image-independent response. To demonstrate the advantages of TAB, we train VLMs with TAB to perform image-difference captioning. Over three datasets, our models perform similarly to baseline VLMs in captioning but the bottleneck is superior in localizing changes and in identifying when no changes occur. TAB is the first architecture to enable users to debug by editing attention, which often produces expected outputs by VLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pooyan Rahmanzadehgervi",
      "Hung Huy Nguyen",
      "Rosanne Liu",
      "Long Mai",
      "Anh Totti Nguyen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Towards_Stabilized_and_Efficient_Diffusion_Transformers_through_Long-Skip-Connections_with_Spectral_ICCV_2025_paper.html": {
    "title": "Towards Stabilized and Efficient Diffusion Transformers through Long-Skip-Connections with Spectral Constraints",
    "volume": "main",
    "abstract": "Diffusion Transformers (DiT) have emerged as a powerful architecture for image and video generation, offering superior quality and scalability. However, their practical application suffers from inherent dynamic feature instability, leading to error amplification during cached inference. Through systematic analysis, we identify the absence of long-range feature preservation mechanisms as the root cause of unstable feature propagation and perturbation sensitivity. To this end, we propose Skip-DiT, a novel DiT variant enhanced with Long-Skip-Connections (LSCs) - the key efficiency component in U-Nets. Theoretical spectral norm and visualization analysis demonstrate how LSCs stabilize feature dynamics. Skip-DiT architecture and its stabilized dynamic feature enable an efficient statical caching mechanism that reuses deep features across timesteps while updating shallow components. Extensive experiments across image and video generation tasks demonstrate that Skip-DiT achieves: (1) **4.4x** training acceleration and faster convergence, (2) **1.5-2x** inference acceleration without quality loss and high fidelity to original output, outperforming existing DiT caching methods across various quantitative metrics. Our findings establish long-skip connections as critical architectural components for training stable and efficient diffusion transformers. Codes are provided in the anonymous URL https://anonymous.4open.science/r/Skip-DiT-72B7/",
    "checked": true,
    "id": "60dd6e831a6ce11f1fa69f859171a4ec65bf0420",
    "semantic_title": "towards stabilized and efficient diffusion transformers through long-skip-connections with spectral constraints",
    "citation_count": 3,
    "authors": [
      "Guanjie Chen",
      "Xinyu Zhao",
      "Yucheng Zhou",
      "Xiaoye Qu",
      "Tianlong Chen",
      "Yu Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Qiu_Feature_Extraction_and_Representation_of_Pre-training_Point_Cloud_Based_on_ICCV_2025_paper.html": {
    "title": "Feature Extraction and Representation of Pre-training Point Cloud Based on Diffusion Models",
    "volume": "main",
    "abstract": "The pretrain-finetune paradigm of pre-training a model on large amounts of image and text data and then fine-tuning the model for a specific task has led to significant progress in many 2D image and natural language processing tasks.Similarly, the use of pre-training methods in point cloud data can also enhance the working performance and generalization ability of the model.Therefore, in this paper, we propose a pre-training framework based on a diffusion model called PreDifPoint. It is able to accomplish the pre-training of the model's backbone network through a diffusion process of gradual denoising. We aggregate the potential features extracted from the backbone network, input them as conditions into the subsequent diffusion model, and direct the point-to-point mapping relationship of the noisy point clouds at neighboring time steps, so as to generate high-quality point clouds and at the same time better perform various downstream tasks of the point clouds.We also introduce a bi-directional covariate attention (DXCA-Attention) mechanism for capturing complex feature interactions, fusing local and global features, and improving the detail recovery of point clouds.In addition, we propose a density-adaptive sampling strategy, which can help the model dynamically adjust the sampling strategy between different time steps, and guide the model to pay more attention to the denser regions in the point cloud, thus improving the effectiveness of the model in point cloud recovery.Our PreDifPoint framework achieves more competitive results on various real-world datasets. Specifically, PreDifPoint achieves an overall accuracy of 87.96%, which is 0.35% higher than PointDif, on the classification task on PB-T50-395RS, a variant of ScanObjectNN dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chang Qiu",
      "Feipeng Da",
      "Zilei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ki_FLOAT_Generative_Motion_Latent_Flow_Matching_for_Audio-driven_Talking_Portrait_ICCV_2025_paper.html": {
    "title": "FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking Portrait",
    "volume": "main",
    "abstract": "With the rapid advancement of diffusion-based generative models, portrait image animation has achieved remarkable results. However, it still faces challenges in temporally consistent video generation and fast sampling due to its iterative sampling nature. This paper presents FLOAT, an audio-driven talking portrait video generation method based on flow matching generative model. Instead of a pixel-based latent space, we take advantage of a learned orthogonal motion latent space, enabling efficient generation and editing of temporally consistent motion. To achieve this, we introduce a transformer-based vector field predictor with an effective frame-wise conditioning mechanism. Additionally, our method supports speech-driven emotion enhancement, enabling a natural incorporation of expressive motions. Extensive experiments demonstrate that our method outperforms state-of-the-art audio-driven talking portrait methods in terms of visual quality, motion fidelity, and efficiency",
    "checked": true,
    "id": "bdf3375410d55056521b64b190eddfbcbf45c9d7",
    "semantic_title": "float: generative motion latent flow matching for audio-driven talking portrait",
    "citation_count": 10,
    "authors": [
      "Taekyung Ki",
      "Dongchan Min",
      "Gyeongsu Chae"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhao_Membership_Inference_Attacks_with_False_Discovery_Rate_Control_ICCV_2025_paper.html": {
    "title": "Membership Inference Attacks with False Discovery Rate Control",
    "volume": "main",
    "abstract": "Recent studies have shown that deep learning models are vulnerable to membership inference attacks (MIAs), which aim to infer whether a data record was used to train a target model or not. To analyze and study these vulnerabilities, various MIA methods have been proposed. Despite the significance and popularity of MIAs, existing works on MIAs are limited in providing guarantees on the false discovery rate (FDR), which refers to the expected proportion of false discoveries among the identified positive discoveries. However, it is very challenging to ensure the false discovery rate guarantees, because the underlying distribution is usually unknown, and the estimated non-member probabilities often exhibit interdependence. To tackle the above challenges, in this paper, we design a novel membership inference attack method, which can provide the guarantees on the false discovery rate. Additionally, we show that our method can also provide the marginal probability guarantee on labeling true non-member data as member data. Notably, our method can work as a wrapper that can be seamlessly integrated with existing MIA methods in a post-hoc manner, while also providing the FDR control. We perform the theoretical analysis for our method. Extensive experiments in various settings (e.g., the black-box setting and the lifelong learning setting) are also conducted to verify the desirable performance of our method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenxu Zhao",
      "Wei Qian",
      "Aobo Chen",
      "Mengdi Huai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chhablani_EmbodiedSplat_Personalized_Real-to-Sim-to-Real_Navigation_with_Gaussian_Splats_from_a_Mobile_ICCV_2025_paper.html": {
    "title": "EmbodiedSplat: Personalized Real-to-Sim-to-Real Navigation with Gaussian Splats from a Mobile Device",
    "volume": "main",
    "abstract": "The field of Embodied AI predominantly relies on simulation for training and evaluation, often using either fully synthetic environments that lack photorealism or high-fidelity real-world reconstructions captured with expensive hardware. As a result, sim-to-real transfer remains a major challenge. In this paper, we introduce EmbodiedSplat, a novel approach that personalizes policy training by efficiently capturing the deployment environment and fine-tuning policies within the reconstructed scenes. Our method leverages 3D Gaussian Splatting (GS) and the Habitat-Sim simulator to bridge the gap between realistic scene capture and effective training environments. Using iPhone-captured deployment scenes, we reconstruct meshes via GS, enabling training in settings that closely approximate real-world conditions. We conduct a comprehensive analysis of training strategies, pre-training datasets, and mesh reconstruction techniques, evaluating their impact on sim-to-real predictivity in real-world scenarios. Experimental results demonstrate that agents fine-tuned with EmbodiedSplat outperform both zero-shot baselines pre-trained on large-scale real-world datasets (HM3D) and synthetically generated datasets (HSSD), achieving absolute success rate improvements of 20% and 40% on real-world Image Navigation task. Moreover, our approach yields a high sim-vs-real correlation (0.87-0.97) for the reconstructed meshes, underscoring its effectiveness in adapting policies to diverse environments with minimal effort. Project page: https://gchhablani.github.io/embodied-splat",
    "checked": true,
    "id": "b1d45b781ff70af8c44384c22d8002a07a5ac696",
    "semantic_title": "embodiedsplat: personalized real-to-sim-to-real navigation with gaussian splats from a mobile device",
    "citation_count": 0,
    "authors": [
      "Gunjan Chhablani",
      "Xiaomeng Ye",
      "Muhammad Zubair Irshad",
      "Zsolt Kira"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Delic_Sequential_keypoint_density_estimator_an_overlooked_baseline_of_skeleton-based_video_ICCV_2025_paper.html": {
    "title": "Sequential keypoint density estimator: an overlooked baseline of skeleton-based video anomaly detection",
    "volume": "main",
    "abstract": "Detecting anomalous human behaviouris an important visual taskin safety-critical applicationssuch as healthcare monitoring,workplace safety,or public surveillance.In these contexts,abnormalities are often reflectedwith unusual human poses.Thus, we propose SeeKer,a method for detecting anomaliesin sequences of human skeletons.Our method formulates the skeleton sequence densitythrough autoregressive factorization at the keypoint level.The corresponding conditional distributionsrepresent probable keypoint locations given prior skeletal motion.We formulate the joint distribution of the considered skeletonas causal prediction of conditional Gaussiansacross its constituent keypoints.A skeleton is flagged as anomalous if its keypoint locations surprise our model(i.e. receive a low density).In practice, our anomaly score is a weighted sum of per-keypoint log-conditionals,where the weights account for the confidence of the underlying keypoint detector.Despite its conceptual simplicity,SeeKer surpasses all previous methodson the UBnormal and MSAD-HR datasetswhile delivering competitive performanceon the ShanghaiTech dataset",
    "checked": true,
    "id": "9242e6ba7a9ddd07996f4b69072fb07577e3a341",
    "semantic_title": "sequential keypoint density estimator: an overlooked baseline of skeleton-based video anomaly detection",
    "citation_count": 1,
    "authors": [
      "Anja Delić",
      "Matej Grcic",
      "Siniša Šegvić"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Engstler_SynCity_Training-Free_Generation_of_3D_Worlds_ICCV_2025_paper.html": {
    "title": "SynCity: Training-Free Generation of 3D Worlds",
    "volume": "main",
    "abstract": "We propose SynCity, a method for generating explorable 3D worlds from textual descriptions. Our approach leverages pre-trained textual, image, and 3D generators without requiring fine-tuning or inference-time optimization. While most 3D generators are object-centric and unable to create large-scale worlds, we demonstrate how 2D and 3D generators can be combined to produce ever-expanding scenes. The world is generated tile by tile, with each new tile created within its context and seamlessly integrated into the scene. SynCity enables fine-grained control over the appearance and layout of the generated worlds, which are both detailed and diverse",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paul Engstler",
      "Aleksandar Shtedritski",
      "Iro Laina",
      "Christian Rupprecht",
      "Andrea Vedaldi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wong_Resonance_Learning_to_Predict_Social-Aware_Pedestrian_Trajectories_as_Co-Vibrations_ICCV_2025_paper.html": {
    "title": "Resonance: Learning to Predict Social-Aware Pedestrian Trajectories as Co-Vibrations",
    "volume": "main",
    "abstract": "Learning to forecast trajectories of intelligent agents has caught much more attention recently. However, it remains a challenge to accurately account for agents' intentions and social behaviors when forecasting, and in particular, to simulate the unique randomness within each of those components in an explainable and decoupled way. Inspired by vibration systems and their resonance properties, we propose the Resonance (short for Re) model to encode and forecast pedestrian trajectories in the form of \"co-vibrations\". It decomposes trajectory modifications and randomness into multiple vibration portions to simulate agents' reactions to each single cause, and forecasts trajectories as the superposition of these independent vibrations separately. Also, benefiting from such vibrations and their spectral properties, representations of social interactions can be learned by emulating the resonance phenomena, further enhancing its explainability. Experiments on multiple datasets have verified its usefulness both quantitatively and qualitatively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Conghao Wong",
      "Ziqian Zou",
      "Beihao Xia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Adversarial_Training_for_Probabilistic_Robustness_ICCV_2025_paper.html": {
    "title": "Adversarial Training for Probabilistic Robustness",
    "volume": "main",
    "abstract": "Deep learning (DL) has shown transformative potential across industries, yet its sensitivity to adversarial examples (AEs) limits its reliability and broader deployment. Research on DL robustness has developed various techniques, with adversarial training (AT) established as a leading approach to counter AEs. Traditional AT focuses on worst-case robustness (WCR), but recent work has introduced probabilistic robustness (PR), which evaluates the proportion of AEs within a local perturbation range, providing an overall assessment of the model's local robustness and acknowledging residual risks that are more practical to manage. However, existing AT methods are fundamentally designed to improve WCR, and no dedicated methods currently target PR. To bridge this gap, we reformulate a new min-max optimization as the theoretical foundation for AT focused on PR, and introduce an AT-PR training scheme with effective numerical algorithms to solve the new optimization problem. Our experiments, based on 38 DL models trained on common datasets and architectures, demonstrate that AT-PR achieves higher improvements in PR than AT-WCR methods and shows more consistent effectiveness across varying local inputs, with a smaller trade-off in model generalization. Open-source tools and all experiments are publicly accessible",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Zhang",
      "Yuhang Chen",
      "Zhen Chen",
      "Wenjie Ruan",
      "Xiaowei Huang",
      "Siddartha Khastgir",
      "Xingyu Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_MUNBa_Machine_Unlearning_via_Nash_Bargaining_ICCV_2025_paper.html": {
    "title": "MUNBa: Machine Unlearning via Nash Bargaining",
    "volume": "main",
    "abstract": "Machine Unlearning (MU) aims to selectively erase harmful behaviors from models while retaining the overall utility of the model. As a multi-task learning problem, MU involves balancing objectives related to forgetting specific concepts/data and preserving general performance. A naive integration of these forgetting and preserving objectives can lead to gradient conflicts and dominance, impeding MU algorithms from reaching optimal solutions.To address the gradient conflict and dominance issue, we reformulate MU as a two-player cooperative game, where the two players, namely, the forgetting player and the preservation player, contribute via their gradient proposals to maximize their overall gain and balance their contributions.To this end, inspired by the Nash bargaining theory, we derive a closed-form solution to guide the model toward the Pareto stationary point.Our formulation of MU guarantees an equilibrium solution, where any deviation from the final state would lead to a reduction in the overall objectives for both players, ensuring optimality in each objective.We evaluate our algorithm's effectiveness on a diverse set of tasks across image classification and image generation.Extensive experiments with ResNet, vision-language model CLIP, and text-to-image diffusion models demonstrate that our method outperforms state-of-the-art MU algorithms, achieving a better trade-off between forgetting and preserving.Our results also highlight improvements in forgetting precision, preservation of generalization, and robustness against adversarial attacks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Wu",
      "Mehrtash Harandi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xue_DLF_Extreme_Image_Compression_with_Dual-generative_Latent_Fusion_ICCV_2025_paper.html": {
    "title": "DLF: Extreme Image Compression with Dual-generative Latent Fusion",
    "volume": "main",
    "abstract": "Recent studies in extreme image compression have achieved remarkable performance by compressing the tokens from generative tokenizers. However, these methods often prioritize clustering common semantics within the dataset, while overlooking the diverse details of individual objects. Consequently, this results in suboptimal reconstruction fidelity, especially at low bitrates. To address this issue, we introduce a Dual-generative Latent Fusion (DLF) paradigm. DLF decomposes the latent into semantic and detail elements, compressing them through two distinct branches. The semantic branch clusters high-level information into compact tokens, while the detail branch encodes perceptually critical details to enhance the overall fidelity. Additionally, we propose a cross-branch interactive design to reduce redundancy between the two branches, thereby minimizing the overall bit cost. Experimental results demonstrate the impressive reconstruction quality of DLF even below 0.01 bits per pixel (bpp). On the CLIC2020 test set, our method achieves bitrate savings of up to 27.93% on LPIPS and 53.55% on DISTS compared to MS-ILLM. Furthermore, DLF surpasses recent diffusion-based codecs in visual fidelity while maintaining a comparable level of generative realism. Project: https://dlfcodec.github.io/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Naifu Xue",
      "Zhaoyang Jia",
      "Jiahao Li",
      "Bin Li",
      "Yuan Zhang",
      "Yan Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_PanoLlama_Generating_Endless_and_Coherent_Panoramas_with_Next-Token-Prediction_LLMs_ICCV_2025_paper.html": {
    "title": "PanoLlama: Generating Endless and Coherent Panoramas with Next-Token-Prediction LLMs",
    "volume": "main",
    "abstract": "Panoramic Image Generation (PIG) aims to create coherent images of arbitrary lengths. Most existing methods fall in the joint diffusion paradigm, but their complex and heuristic crop connection designs often limit their ability to achieve multilevel coherence. By deconstructing this challenge into its core components, we find it naturally aligns with next-token prediction, leading us to adopt an autoregressive (AR) paradigm for PIG modeling. However, existing visual AR (VAR) models are limited to fixed-size generation, lacking the capability to produce panoramic images. In this paper, we propose PanoLlama, a novel framework that achieves endless and coherent panorama generation with the autoregressive paradigm. Our approach develops a training-free strategy that utilizes token redirection to overcome the size limitations of existing VAR models, enabling next-crop prediction in both horizontal and vertical directions. This refreshes the PIG pipeline while achieving SOTA performance in coherence (47.50%), fidelity(28.16%), and aesthetics (15%). Additionally, PanoLlama supports applications other PIG methods cannot achieve, including mask-free layout control, multi-scale and multi-guidance synthesis. To facilitate standardized evaluation, we also establish a dataset with 1,000 prompts spanning 100+ themes, providing a new testing benchmark for PIG research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Teng Zhou",
      "Xiaoyu Zhang",
      "Yongchuan Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Koneputugodage_Leaps_and_Bounds_An_Improved_Point_Cloud_Winding_Number_Formulation_ICCV_2025_paper.html": {
    "title": "Leaps and Bounds: An Improved Point Cloud Winding Number Formulation for Fast Normal Estimation and Surface Reconstruction",
    "volume": "main",
    "abstract": "Recent methods for point cloud surface normal estimation predominantly use the generalized winding number field induced by the normals. Optimizing the field towards satisfying desired properties, such as the input points being on the surface defined by the field, provides a principled way to obtain globally consistent surface normals. However, we show that the existing winding number formulation for point clouds is a poor approximation near the input surface points, diverging as the query point approaches a surface point. This is problematic for methods that rely on the accuracy and stability of this approximation, requiring heuristics to compensate. Instead, we derive a more accurate approximation that is properly bounded and converges to the correct value. We then examine two distinct approaches that optimize for globally consistent normals using point cloud winding numbers. We show how the original unbounded formulation influences key design choices in both methods and demonstrate that substituting our formulation yields substantive improvements with respect to normal estimation and surface reconstruction accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chamin Hewa Koneputugodage",
      "Dylan Campbell",
      "Stephen Gould"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_VQ-SGen_A_Vector_Quantized_Stroke_Representation_for_Creative_Sketch_Generation_ICCV_2025_paper.html": {
    "title": "VQ-SGen: A Vector Quantized Stroke Representation for Creative Sketch Generation",
    "volume": "main",
    "abstract": "This paper presents VQ-SGen, a novel algorithm for high-quality creative sketch generation. Recent approaches have framed the task as pixel-based generation either as a whole or part-by-part, neglecting the intrinsic and contextual relationships among individual strokes, such as the shape and spatial positioning of both proximal and distant strokes. To overcome these limitations, we propose treating each stroke within a sketch as an entity and introducing a vector-quantized (VQ) stroke representation for fine-grained sketch generation. Our method follows a two-stage framework - in stage one, we decouple each stroke's shape and location information to ensure the VQ representation prioritizes stroke shape learning. In stage two, we feed the precise and compact representation into an auto-decoding Transformer to incorporate stroke semantics, positions, and shapes into the generation process. By utilizing tokenized stroke representation, our approach generates strokes with high fidelity and facilitates novel applications, such as text or class label conditioned generation and sketch completion. Comprehensive experiments demonstrate our method surpasses existing state-of-the-art techniques on the CreativeSketch dataset, underscoring its effectiveness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawei Wang",
      "Zhiming Cui",
      "Changjian Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hu_Cross-Category_Subjectivity_Generalization_for_Style-Adaptive_Sketch_Re-ID_ICCV_2025_paper.html": {
    "title": "Cross-Category Subjectivity Generalization for Style-Adaptive Sketch Re-ID",
    "volume": "main",
    "abstract": "Sketch-based person re-identification (re-ID) enables pedestrian retrieval using sketches. While recent methods have improved modality alignment between sketches and RGB images, the challenge of subjective style variation, where sketches exhibit diverse and unpredictable appearances, remains largely unresolved.A natural solution is to train on a diverse range of pedestrian sketches, but the high cost of large-scale pedestrian sketch collection makes this impractical.In contrast, sketches of general categories (e.g., animals, objects) exhibit diverse style variations and are accessible at a low cost, making them an intuitive and scalable alternative for enhancing style generalization in sketch re-ID.To this end, we propose Adaptive Incremental Prompt-tuning (AIP), the first approach that explores cross-category subjective style generalization for sketch re-ID. Specifically, AIP incorporates a multi-stage prompt-tuning strategy that learns a broad but shareable spectrum of sketch styles from non-pedestrian data. In addition, an input-sensitive prompt generator enables the model to adapt dynamically to unseen sketch styles.Extensive experimental results demonstrate that the performance gain is not merely attributed to the inclusion of additional data but rather to the effectiveness of AIP in leveraging non-pedestrian data for subjective style generalization. Our method outperforms existing works by a significant margin, establishing new state-of-the-art results",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zechao Hu",
      "Zhengwei Yang",
      "Hao Li",
      "Zheng Wang",
      "Yixiong Zou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Mihajlovic_VolumetricSMPL_A_Neural_Volumetric_Body_Model_for_Efficient_Interactions_Contacts_ICCV_2025_paper.html": {
    "title": "VolumetricSMPL: A Neural Volumetric Body Model for Efficient Interactions, Contacts, and Collisions",
    "volume": "main",
    "abstract": "Parametric human body models play a crucial role in computer graphics and vision, enabling applications ranging from human motion analysis to understanding human-environment interactions. Traditionally, these models use surface meshes, which pose challenges in efficiently handling interactions with other geometric entities, such as objects and scenes, typically represented as meshes or point clouds. To address this limitation, recent research has explored volumetric neural implicit body models. However, existing works are either insufficiently robust for complex human articulations or impose high computational and memory costs, limiting their widespread use. To this end, we introduce VolumetricSMPL, a neural volumetric body model that leverages Neural Blend Weights (NBW) to generate compact, yet efficient MLP decoders. Unlike prior approaches that rely on large MLPs, NBW dynamically blends a small set of learned weight matrices using predicted shape- and pose-dependent coefficients, significantly improving computational efficiency while preserving expressiveness. VolumetricSMPL outperforms prior volumetric occupancy model COAP with 10x faster inference, 6x lower GPU memory usage, enhanced accuracy, and a Signed Distance Function (SDF) for efficient and differentiable contact modeling. We demonstrate VolumetricSMPL's strengths across four challenging tasks: (1) reconstructing human-object interactions from in-the-wild images, (2) recovering human meshes in 3D scenes from egocentric views, (3) scene-constrained motion synthesis, and (4) resolving self-intersections. Our results highlight its broad applicability and significant performance and efficiency gains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marko Mihajlovic",
      "Siwei Zhang",
      "Gen Li",
      "Kaifeng Zhao",
      "Lea Muller",
      "Siyu Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Spatial-Temporal_Forgery_Trace_based_Forgery_Image_Identification_ICCV_2025_paper.html": {
    "title": "Spatial-Temporal Forgery Trace based Forgery Image Identification",
    "volume": "main",
    "abstract": "The rapid development of AIGC technology has enabled highly realistic forged images to deceive human perception, posing serious risks across many areas. Current deepfake image detection methods primarily identify forgeries by extracting handcrafted features, deep features, and frequency-domain features. While these features contain forgery traces, they also include a substantial amount of the image's semantic information, which interferes with the precision and generalization of forgery detection models. To tackle these challenges, this paper introduces a novel forgery image identification method based on the Spatial-Temporal Forgery Trace (STFT). Motivated by the fact that forgery images are more easily fitted to a specific distribution than real images, the STFT method approaches the issue from a forged image distribution modeling perspective, employing generative diffusion models to meticulously capture the temporal distribution of images. It further models the relationship between temporal feature variations and spatially corresponding temporal features, treating them as temporal and spatial forgery traces. Moreover, STFT incorporates frequency-domain features as weighting factors to accelerate the localization of spatio-temporal forgery traces. Experiments demonstrate that by integrating spatial, temporal, and frequency perspectives within the latent space, STFT effectively captures subtle spatio-temporal forgery traces, exhibiting strong robustness and generalizability. It outperforms state-of-the-art methods on major benchmark datasets in the field. The source code for STFT is available at \\href https://anonymous.4open.science/r/STFT-B552/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yilin Wang",
      "Zunlei Feng",
      "Jiachi Wang",
      "Hengrui Lou",
      "Binjia Zhou",
      "Jie Lei",
      "Mingli Song",
      "Yijun Bei"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wei_Passing_the_Driving_Knowledge_Test_ICCV_2025_paper.html": {
    "title": "Passing the Driving Knowledge Test",
    "volume": "main",
    "abstract": "If a Large Language Model (LLM) were to take a driving knowledge test today, would it pass? Beyond standard spatial and visual question-answering (QA) tasks on current autonomous driving benchmarks, driving knowledge tests require a complete understanding of all traffic rules, signage, and right-of-way principles. To pass this test, human drivers must discern various edge cases that rarely appear in real-world datasets. In this work, we present DriveQA, an extensive open-source text and vision-based benchmark that exhaustively covers traffic regulations and scenarios. Through our experiments using DriveQA, we show that (1) state-of-the-art LLMs and Multimodal LLMs (MLLMs) perform well on basic traffic rules but exhibit significant weaknesses in numerical reasoning and complex right-of-way scenarios, traffic sign variations, and spatial layouts, (2) fine-tuning on DriveQA improves accuracy across multiple categories, particularly in regulatory sign recognition and intersection decision-making, (3) controlled variations in DriveQA-V provide insights into model sensitivity to environmental factors such as lighting, perspective, distance, and weather conditions, and (4) pretraining on DriveQA enhances downstream driving task performance, leading to improved results on real-world datasets such as nuScenes and BDD, while also demonstrating that models can internalize text and synthetic traffic knowledge to generalize effectively across downstream QA tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maolin Wei",
      "Wanzhou Liu",
      "Eshed Ohn-Bar"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_OneGT_One-Shot_Geometry-Texture_Neural_Rendering_for_Head_Avatars_ICCV_2025_paper.html": {
    "title": "OneGT: One-Shot Geometry-Texture Neural Rendering for Head Avatars",
    "volume": "main",
    "abstract": "Existing solutions for creating high-fidelity digital head avatars encounter various obstacles. Traditional rendering tools offer realistic results, while heavily requiring expert skills. Neural rendering methods are more efficient but often compromise between the generated fidelity and flexibility. We present OneGT that adheres to the frameworks of the rendering tools, while restructuring individual stages of the rendering pipeline through neural networks. OneGT maintains high systemic interpretability, inheriting the superior performances of neural rendering approaches. Specifically, OneGT contains a skeleton-anchoring stage and a texture-rendering stage, in which well-designed Transformers learn the geometric transformations and the proposed reference-perceptible DiT renders the textures respectively. Our framework learns geometric consistency from the innovatively introduced synthetic data, thus achieving superior performance while requiring only 10%-30% of the real-world data typically used by competitive methods. Experimental results demonstrate that OneGT achieves high fidelity in producing portrait avatars, meanwhile maintaining the flexibility of editing",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinshu Chen",
      "Bingchuan Li",
      "Fan Zhang",
      "Songtao Zhao",
      "Qian He"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Joint_Asymmetric_Loss_for_Learning_with_Noisy_Labels_ICCV_2025_paper.html": {
    "title": "Joint Asymmetric Loss for Learning with Noisy Labels",
    "volume": "main",
    "abstract": "Learning with noisy labels is a crucial task for training accurate deep neural networks. To mitigate label noise, prior studies have proposed various robust loss functions, particularly symmetric losses. Nevertheless, symmetric losses usually suffer from the underfitting issue due to the overly strict constraint. To address this problem, the Active Passive Loss (APL) jointly optimizes an active and a passive loss to mutually enhance the overall fitting ability. Within APL, symmetric losses have been successfully extended, yielding advanced robust loss functions. Despite these advancements, emerging theoretical analyses indicate that asymmetric losses, a new class of robust loss functions, possess superior properties compared to symmetric losses. However, existing asymmetric losses are not compatible with advanced optimization frameworks such as APL, limiting their potential and applicability. Motivated by this theoretical gap and the prospect of asymmetric losses, we extend the asymmetric loss to the more complex passive loss scenario and propose the Asymetric Mean Square Error (AMSE), a novel asymmetric loss. We rigorously establish the necessary and sufficient condition under which AMSE satisfies the asymmetric condition. By substituting the traditional symmetric passive loss in APL with our proposed AMSE, we introduce a novel robust loss framework termed Joint Asymmetric Loss (JAL). Extensive experiments demonstrate the effectiveness of our method in mitigating label noise. Code available at: https://github.com/cswjl/joint-asymmetric-loss",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jialiang Wang",
      "Xianming Liu",
      "Xiong Zhou",
      "Gangfeng Hu",
      "Deming Zhai",
      "Junjun Jiang",
      "Xiangyang Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Feng_A_Unified_Framework_for_Industrial_Cel-Animation_Colorization_with_Temporal-Structural_Awareness_ICCV_2025_paper.html": {
    "title": "A Unified Framework for Industrial Cel-Animation Colorization with Temporal-Structural Awareness",
    "volume": "main",
    "abstract": "Line drawing colorization is a critical step in the cel-animation industry, where artists use a paint bucket tool to apply RGB values to segments based on a character's color design sheet. Current automated methods predominantly focus on consecutive frame colorization, using a single adjacent frame as a reference. These approaches often face two major challenges: inaccurate segment colorization due to significant deformations between the target and reference frames, and incomplete information in a single frame that prevents finding suitable reference segments, leading to poor color accuracy. To address these challenges, we propose a novel colorization framework that integrates both temporal and structural information. Using multiple reference keyframes, our method effectively captures temporal information across frames, enhancing the accuracy of colorization for transitional frames. In addition, we leverage structural information through a matching-based approach that ensures precise segment alignment across frames. This combination of temporal awareness through multi-frame references and structural alignment improves colorization robustness, even in scenarios with large motion and deformations. Our method outperforms existing techniques, demonstrating superior colorization accuracy and consistency in industrial cel-animation workflows",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyi Feng",
      "Tao Huang",
      "Peng Wang",
      "Zizhou Huang",
      "Zhang Haihang",
      "Yuntao Zou",
      "Dagang Li",
      "Kaifeng Zou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ventura_Uncalibrated_Structure_from_Motion_on_a_Sphere_ICCV_2025_paper.html": {
    "title": "Uncalibrated Structure from Motion on a Sphere",
    "volume": "main",
    "abstract": "Spherical motion is a special case of camera motion where the camera moves on the imaginary surface of a sphere with the optical axis normal to the surface. Common sources of spherical motion are a person capturing a stereo panorama with a phone held in an outstretched hand, or a hemi-spherical camera rig used for multi-view scene capture. However, traditional structure-from-motion pipelines tend to fail on spherical camera motion sequences, especially when the camera is facing outward. Building upon prior work addressing the calibrated case, we explore uncalibrated reconstruction from spherical motion, assuming a fixed but unknown focal length parameter. We show that, although two-view spherical motion is always a critical case, self-calibration is possible from three or more views. Through analysis of the relationship between focal length and spherical relative pose, we devise a global structure-from-motion approach for uncalibrated reconstruction. We demonstrate the effectiveness of our approach on real-world captures in various settings, even when the camera motion deviates from perfect spherical motion. Code and data for our method are available at https://github.com/jonathanventura/spherical-sfm",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonathan Ventura",
      "Viktor Larsson",
      "Fredrik Kahl"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ji_CARL_Causality-guided_Architecture_Representation_Learning_for_an_Interpretable_Performance_Predictor_ICCV_2025_paper.html": {
    "title": "CARL: Causality-guided Architecture Representation Learning for an Interpretable Performance Predictor",
    "volume": "main",
    "abstract": "Performance predictors have emerged as a promising method to accelerate the evaluation stage of neural architecture search (NAS). These predictors estimate the performance of unseen architectures by learning from the correlation between a small set of trained architectures and their performance. However, most existing predictors ignore the inherent distribution shift between limited training samples and diverse test samples. Hence, they tend to learn spurious correlations as shortcuts to predictions, leading to poor generalization. To address this, we propose a Causality-guided Architecture Representation Learning (CARL) method aiming to separate critical (causal) and redundant (non-causal) features of architectures for generalizable architecture performance prediction. Specifically, we employ a substructure extractor to split the input architecture into critical and redundant substructures in the latent space. Then, we generate multiple interventional samples by pairing critical representations with diverse redundant representations to prioritize critical features. Extensive experiments on five NAS search spaces demonstrate the state-of-the-art accuracy and superior interpretability of CARL. For instance, CARL achieves 97.67% top-1 accuracy on CIFAR-10 using DARTS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Ji",
      "Yuqi Feng",
      "Jiahao Fan",
      "Yanan Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wald_An_OpenMind_for_3D_Medical_Vision_Self-supervised_Learning_ICCV_2025_paper.html": {
    "title": "An OpenMind for 3D Medical Vision Self-supervised Learning",
    "volume": "main",
    "abstract": "The field of self-supervised learning (SSL) for 3D medical images lacks consistency and standardization. While many methods have been developed, it is impossible to identify the current state-of-the-art, due to i) varying and small pretraining datasets, ii) varying architectures, and iii) being evaluated on differing downstream datasets. In this paper, we bring clarity to this field and lay the foundation for further method advancements through three key contributions: We a) publish the largest publicly available pre-training dataset comprising 114k 3D brain MRI volumes, enabling all practitioners to pre-train on a large-scale dataset. We b) benchmark existing 3D self-supervised learning methods on this dataset for a state-of-the-art CNN and Transformer architecture, clarifying the state of 3D SSL pre-training. Among many findings, we show that pre-trained methods can exceed a strong from-scratch nnU-Net ResEnc-L baseline. Lastly, we c) publish the code of our pre-training and fine-tuning frameworks and provide the pre-trained models created during the benchmarking process to facilitate rapid adoption and reproduction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tassilo Wald",
      "Constantin Ulrich",
      "Jonathan Suprijadi",
      "Sebastian Ziegler",
      "Michal Nohel",
      "Robin Peretzke",
      "Gregor Kohler",
      "Klaus  Maier-Hein"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Dong_Online_Dense_Point_Tracking_with_Streaming_Memory_ICCV_2025_paper.html": {
    "title": "Online Dense Point Tracking with Streaming Memory",
    "volume": "main",
    "abstract": "Dense point tracking is a challenging task requiring the continuous tracking of every point in the initial frame throughout a substantial portion of a video, even in the presence of occlusions. Traditional methods use optical flow models to directly estimate long-range motion, but they often suffer from appearance drifting without considering temporal consistency. Recent point tracking algorithms usually depend on sliding windows for indirect information propagation from the first frame to the current one, which is slow and less effective for long-range tracking. To account for temporal consistency and enable efficient information propagation, we present a lightweight and fast model with Streaming memory for dense POint Tracking and online video processing. The SPOT framework features three core components: a customized memory reading module for feature enhancement, a sensory memory for short-term motion dynamics modeling, and a visibility-guided splatting module for accurate information propagation. This combination enables SPOT to perform dense point tracking with state-of-the-art accuracy on the CVO benchmark, as well as comparable or superior performance to offline models on sparse tracking benchmarks such as TAP-Vid and RoboTAP. Notably, SPOT with 10x smaller parameter numbers operates at least 2x faster than previous state-of-the-art models while maintaining the best performance on CVO. We will release the models and codes at: https://dqiaole.github.io/SPOT/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiaole Dong",
      "Yanwei Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_MCID_Multi-aspect_Copyright_Infringement_Detection_for_Generated_Images_ICCV_2025_paper.html": {
    "title": "MCID: Multi-aspect Copyright Infringement Detection for Generated Images",
    "volume": "main",
    "abstract": "With the rapid advancement of generative models, we can now create highly realistic images. This represents a significant technical breakthrough but also introduces new challenges for copyright protection. Previous methods for detecting copyright infringement in AI-generated images mainly depend on global similarity. However, real-world infringement often occurs only on certain attributes rather than being a global infringement. To address these challenges, we propose a novel Multi-aspect Copyright Infringement Detection (MCID) task, which encompasses various types of infringement, including content, style, structure, and intellectual property infringement. We further develop the Hybrid Infringement Detection Model (HIDM) to address the MCID task. By combining feature-based methods with VLMs, it enables the detection of various infringement types and provides interpretable results. To ensure the MCID task meets actual legal requirements, we construct a Large-Scale Copyright Dataset (LSCD) with clear author copyright ownership. Based on LSCD, we provide a benchmark annotated by legal experts for performance evaluation. Experimental results show that HIDM effectively detects various types of image copyright infringement and offers a more interpretable and superior solution compared to previous methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuanwei Huang",
      "Zexi Jia",
      "Hongyan Fei",
      "Yeshuang Zhu",
      "Zhiqiang Yuan",
      "Ying Deng",
      "Jiapei Zhang",
      "Xiaoyue Duan",
      "Jinchao Zhang",
      "Jie Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Guo_Any2AnyTryon_Leveraging_Adaptive_Position_Embeddings_for_Versatile_Virtual_Clothing_Tasks_ICCV_2025_paper.html": {
    "title": "Any2AnyTryon: Leveraging Adaptive Position Embeddings for Versatile Virtual Clothing Tasks",
    "volume": "main",
    "abstract": "Image-based virtual try-on (VTON) aims to generate a virtual try-on result by transferring an input garment onto a target person's image. However, the scarcity of paired garment-model data makes it challenging for existing meth ods to achieve high generalization and quality in VTON.Also, it limits the ability to generate mask-free try-ons. To tackle the data scarcity problem, approaches such as Stable Garment and MMTryon use a synthetic data strategy, effectively increasing the amount of paired data on the model side. However, existing methods are typically limited to performing specific try-on tasks and lack user-friendliness.To enhance the generalization and controllability of VTON generation, we propose Any2AnyTryon, which can generate try-on results based on different textual instructions and model garment images to meet various needs,eliminating the reliance on masks, poses, or other conditions. Specifically, we first construct the virtual try-on dataset LAION-Garment, the largest known open-source garment try-on dataset. Then, we introduce adaptive position embedding, which enables the model to generate satisfactory outfitted model images or garment images based on input images of different sizes and categories, significantly enhancing the generalization and controllability of VTON generation. In our experiments, we demonstrate the effectiveness of our Any2AnyTryon and compare it with existing methods. The results show that Any2AnyTryon enables flexible, controllable, and high-quality image-based virtual try-on generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hailong Guo",
      "Bohan Zeng",
      "Yiren Song",
      "Wentao Zhang",
      "Jiaming Liu",
      "Chuang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shehzadi_STEP-DETR_Advancing_DETR-based_Semi-Supervised_Object_Detection_with_Super_Teacher_and_ICCV_2025_paper.html": {
    "title": "STEP-DETR: Advancing DETR-based Semi-Supervised Object Detection with Super Teacher and Pseudo-Label Guided Text Queries",
    "volume": "main",
    "abstract": "This paper addresses key limitations in current Semi-Supervised Object Detection (SSOD) frameworks, focusing on issues related to pseudo-label quality, confidence bias, and inefficient query generation. Traditional methods, including CNN-based and DETR-based architectures, often face challenges such as noisy pseudo-labels, overfitting to common object categories, and consequently face difficulty detecting rare objects. Specifically, recent DETR-based SSOD approaches struggle with the one-to-many assignment strategy, which produces noisy pseudo-labels and overlapping predictions, resulting in suboptimal performance. To address these challenges, we propose STEP-DETR, a transformer-based SSOD framework. STEP-DETR introduces Super Teacher to generate higher-quality pseudo-labels and improve the student's learning process. Furthermore, STEP-DETR proposes Pseudo-Label Text Queries, which incorporate text embeddings from Super Teacher, balancing the student's confidence across common and rare categories, thereby mitigating confidence bias and enhancing generalization. Moreover, Denoising Text Guided Object Queries synthesizes query-label pairs for foreground and background using contrastive learning, enabling the model to better distinguish objects from background noise. To further boost performance and training efficiency, a Query Refinement Module is incorporated to filter out redundant denoising queries. On MS-COCO and Pascal VOC benchmarks, STEP-DETR outperforms state-of-the-art methods, demonstrating its effectiveness in improving semi-supervised object detection. Notably, with just 10% labeled data, it achieves 45.4 mAP, surpassing the baseline Semi-DETR by 1.9 mAP",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tahira Shehzadi",
      "Khurram Azeem Hashmi",
      "Shalini Sarode",
      "Didier Stricker",
      "Muhammad Zeshan Afzal"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Han_DISTA-Net_Dynamic_Closely-Spaced_Infrared_Small_Target_Unmixing_ICCV_2025_paper.html": {
    "title": "DISTA-Net: Dynamic Closely-Spaced Infrared Small Target Unmixing",
    "volume": "main",
    "abstract": "Resolving closely-spaced small targets in dense clusters presents a significant challenge in infrared imaging, as the overlapping signals hinder precise determination of their quantity, sub-pixel positions, and radiation intensities. While deep learning has advanced the field of infrared small target detection, its application to closely-spaced infrared small targets has not yet been explored. This gap exists primarily due to the complexity of separating superimposed characteristics and the lack of an open-source infrastructure. In this work, we propose the Dynamic Iterative Shrinkage Thresholding Network (DISTA-Net), which reconceptualizes traditional sparse reconstruction within a dynamic framework. DISTA-Net adaptively generates convolution weights and thresholding parameters to tailor the reconstruction process in real time. To the best of our knowledge, DISTA-Net is the first deep learning model designed specifically for the unmixing of closely-spaced infrared small targets, achieving superior sub-pixel detection accuracy. Moreover, we have established the first open-source ecosystem to foster further research in this field. This ecosystem comprises three key components: (1) CSIST-100K, a publicly available benchmark dataset; (2) CSO-mAP, a custom evaluation metric for sub-pixel detection; and (3) GrokCSO, an open-source toolkit featuring DISTA-Net and other state-of-the-art models, available at https://github.com/GrokCV/GrokCSO",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengdong Han",
      "Shangdong Yang",
      "Yuxuan Li",
      "Xin Zhang",
      "Xiang Li",
      "Jian Yang",
      "Ming-Ming Cheng",
      "Yimian Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gupta_MDD_A_Dataset_for_Text-and-Music_Conditioned_Duet_Dance_Generation_ICCV_2025_paper.html": {
    "title": "MDD: A Dataset for Text-and-Music Conditioned Duet Dance Generation",
    "volume": "main",
    "abstract": "We introduce Multimodal DuetDance (MDD), a diverse multimodal benchmark dataset designed for text-controlled and music-conditioned 3D duet dance motion generation. Our dataset comprises 620 minutes of high-quality motion capture data performed by professional dancers, synchronized with music, and detailed with over 10K fine-grained natural language descriptions. The annotations capture a rich movement vocabulary, detailing spatial relationships, body movements, and rhythm, making MDD the first dataset to seamlessly integrate human motions, music, and text for duet dance generation. We introduce two novel tasks supported by our dataset: (1) Text-to-Duet, where given music and a textual prompt, both the leader and follower dance motion are generated (2) Text-to-Dance Accompaniment, where given music, textual prompt, and the leader's motion, the follower's motion is generated in a cohesive, text-aligned manner. We include baseline evaluations on both tasks to support future research. Please refer to the project website for the latest updates",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prerit Gupta",
      "Jason Alexander Fotso-Puepi",
      "Zhengyuan Li",
      "Jay Mehta",
      "Aniket Bera"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Met2Net_A_Decoupled_Two-Stage_Spatio-Temporal_Forecasting_Model_for_Complex_Meteorological_ICCV_2025_paper.html": {
    "title": "Met2Net: A Decoupled Two-Stage Spatio-Temporal Forecasting Model for Complex Meteorological Systems",
    "volume": "main",
    "abstract": "The increasing frequency of extreme weather events due to global climate change urges accurate weather prediction. Recently, great advances are made by the end-to-end methods, thanks to deep learning techniques, but they face limitations of representation inconsistency in multivariable integration and struggle to effectively capture the dependency between variables, which is required in complex weather systems. Treating different variables as distinct modalities and applying a two-stage training approach from multimodal models can partially alleviate this issue, but due to the inconformity in training tasks between the two stages, the results are often suboptimal. To address these challenges, we propose an implicit two-stage training method, configuring separate encoders and decoders for each variable. In detailed, in the first stage, the Translator is frozen while the Encoders and Decoders learn a shared latent space, in the second stage, the Encoders and Decoders are frozen, and the Translator captures inter-variable interactions for prediction. Besides, by introducing a self-attention mechanism for multivariable fusion in the latent space, the performance achieves further improvements. Empirically, extensive experiments shows state-of-the-art performance of our method. In specific, it reduces the MSE for near-surface air temperature and relative humidity predictions by 28.82% and 23.39%, respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaohan Li",
      "Hao Yang",
      "Min Chen",
      "Xiaolin Qin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sun_SVG-Head_Hybrid_Surface-Volumetric_Gaussians_for_High-Fidelity_Head_Reconstruction_and_Real-Time_ICCV_2025_paper.html": {
    "title": "SVG-Head: Hybrid Surface-Volumetric Gaussians for High-Fidelity Head Reconstruction and Real-Time Editing",
    "volume": "main",
    "abstract": "Creating high-fidelity and editable head avatars is a pivotal challenge in computer vision and graphics, boosting many AR/VR applications. While recent advancements have achieved photorealistic renderings and plausible animation, head editing, especially real-time appearance editing, remains challenging due to the implicit representation and entangled modeling of the geometry and global appearance. To address this, we propose Surface-Volumetric Gaussian Head Avatar (SVG-Head), a novel hybrid representation that explicitly models the geometry with 3D Gaussians bound on a FLAME mesh and leverages disentangled texture images to capture the global appearance. Technically, it contains two types of Gaussians, in which surface Gaussians explicitly model the appearance of head avatars using learnable texture images, facilitating real-time texture editing, while volumetric Gaussians enhance the reconstruction quality of non-Lambertian regions (e.g., lips and hair). To model the correspondence between 3D world and texture space, we provide a mesh-aware Gaussian UV mapping method, which leverages UV coordinates given by the FLAME mesh to obtain sharp texture images and real-time rendering speed. A hierarchical optimization strategy is further designed to pursue the optimal performance in both reconstruction quality and editing flexibility. Experiments on the NeRSemble dataset show that SVG-Head not only generates high-fidelity rendering results, but also is the first method to obtain explicit texture images for Gaussian head avatars and support real-time appearance editing",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heyi Sun",
      "Cong Wang",
      "Tian-Xing Xu",
      "Jingwei Huang",
      "Di Kang",
      "Chunchao Guo",
      "Song-Hai Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zou_Dynamic_Dictionary_Learning_for_Remote_Sensing_Image_Segmentation_ICCV_2025_paper.html": {
    "title": "Dynamic Dictionary Learning for Remote Sensing Image Segmentation",
    "volume": "main",
    "abstract": "Remote sensing image segmentation faces persistent challenges in distinguishing morphologically similar categories and adapting to diverse scene variations. While existing methods rely on implicit representation learning paradigms, they often fail to dynamically adjust semantic embeddings according to contextual cues, leading to suboptimal performance in fine-grained scenarios such as cloud thickness differentiation. This work introduces a dynamic dictionary learning framework that explicitly models class ID embeddings through iterative refinement. The core contribution lies in a novel dictionary construction mechanism, where class-aware semantic embeddings are progressively updated via multi-stage alternating cross-attention querying between image features and dictionary embeddings. This process enables adaptive representation learning tailored to input-specific characteristics, effectively resolving ambiguities in intra-class heterogeneity and inter-class homogeneity. To further enhance discriminability, a contrastive constraint is applied to the dictionary space, ensuring compact intra-class distributions while maximizing inter-class separability. Extensive experiments across both coarse- and fine-grained datasets demonstrate consistent improvements over state-of-the-art methods, particularly in two online test benchmarks (LoveDA and UAVid). Code is available at https://github.com/XavierJiezou/D2LS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuechao Zou",
      "Yue Li",
      "Shun Zhang",
      "Kai Li",
      "Shiying Wang",
      "Pin Tao",
      "Junliang Xing",
      "Congyan Lang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Video_Motion_Graphs_ICCV_2025_paper.html": {
    "title": "Video Motion Graphs",
    "volume": "main",
    "abstract": "We present Video Motion Graphs, a system designed to generate realistic human motion videos. Using a reference video and conditional signals such as music or motion tags, the system synthesizes new videos by first retrieving video clips with gestures matching the conditions and then generating interpolation frames to seamlessly connect clip boundaries. The core of our approach is HMInterp, a robust Video Frame Interpolation (VFI) model that enables seamless interpolation of discontinuous frames, even for complex motion scenarios like dancing. HMInterp i) employs a dual-branch interpolation approach, combining a Motion Diffusion Model for human skeleton motion interpolation with a diffusion-based video frame interpolation model for final frame generation. ii) adopts condition progressive training to effectively leverage identity strong and weak conditions, such as images and pose. These designs ensure both high video texture quality and accurate motion trajectory. Our Video Motion Graphs outperforms existing generative- and retrieval-based methods for human motion video generation. Our codes and pretrained models are public available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiyang Liu",
      "Zhan Xu",
      "Fa-Ting Hong",
      "Hsin-Ping Huang",
      "Yi Zhou",
      "Yang Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Marouf_Ask_and_Remember_A_Questions-Only_Replay_Strategy_for_Continual_Visual_ICCV_2025_paper.html": {
    "title": "Ask and Remember: A Questions-Only Replay Strategy for Continual Visual Question Answering",
    "volume": "main",
    "abstract": "Continual Learning in Visual Question Answering (VQACL) requires models to acquire new visual-linguistic skills (plasticity) while preserving previously learned knowledge (stability). The inherent multimodality of VQACL exacerbates this challenge, as models must balance stability across visual and textual domains while adapting to novel objects and reasoning tasks. Existing methods, primarily designed for unimodal settings, often fall short in addressing this dual requirement. In this work, we present QUestion-only replay with Attention Distillation (QUAD), a novel approach for VQACL that leverages only past task questions for regularization. By eliminating the need to store visual data, QUAD not only reduces memory overhead, but also alleviates privacy concerns. Our method introduces a Question-only Replay mechanism that selectively reuses prior task questions to counteract overfitting to the answer space of the current task, addressing the problem out of answer set. Complementing this, we propose Attention Consistency Distillation to enforce both intra-modal and inter-modal attention consistency across tasks, preserving essential visual-linguistic associations. Extensive experiments on VQAv2 and NExT-QA demonstrate that QUAD significantly outperforms state-of-the-art methods, achieving robust performance in continual VQA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Imad Eddine Marouf",
      "Enzo Tartaglione",
      "Stéphane Lathuilière",
      "Joost Van De Weijer"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Su_Dense_Policy_Bidirectional_Autoregressive_Learning_of_Actions_ICCV_2025_paper.html": {
    "title": "Dense Policy: Bidirectional Autoregressive Learning of Actions",
    "volume": "main",
    "abstract": "Mainstream visuomotor policies predominantly rely on generative models for holistic action prediction, while current autoregressive policies, predicting the next token or chunk, have shown suboptimal results. This motivates a search for more effective learning methods to unleash the potential of autoregressive policies for robotic manipulation. This paper introduces a bidirectionally expanded learning approach, termed Dense Policy, to establish a new paradigm for autoregressive policies in action prediction. It employs a lightweight encoder-only architecture to iteratively unfold the action sequence from an initial single frame into the target sequence in a coarse-to-fine manner with logarithmic-time inference. Extensive experiments validate that our dense policy has superior autoregressive learning capabilities and can surpass existing holistic generative policies. Our policy, example data, and training code will be publicly available upon publication",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Su",
      "Xinyu Zhan",
      "Hongjie Fang",
      "Han Xue",
      "Hao-Shu Fang",
      "Yong-Lu Li",
      "Cewu Lu",
      "Lixin Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Conditional_Visual_Autoregressive_Modeling_for_Pathological_Image_Restoration_ICCV_2025_paper.html": {
    "title": "Conditional Visual Autoregressive Modeling for Pathological Image Restoration",
    "volume": "main",
    "abstract": "Pathological image has been recognized as the gold standard for cancer diagnosis for more than a century. However, some internal regions of pathological images may inevitably exhibit various degradation issues, including low resolution, image blurring, and image noising, which will affect disease diagnosis, staging, and risk stratification.Existing pathological image restoration methods were mainly based on generative adversarial networks (GANs) to improve image quality, which are limited by the inherent instability and loss of structural details, often resulting in artifacts in the restored images.Large scale of whole slide images (WSIs) also makes it hard for efficient processing and restoration. To address these limitations, we propose a conditional visual autoregressive model (CVARPath) for next-scale token prediction, guided by the degraded tokens from the current scale. We introduce a novel framework that employs quantified encoders specifically designed for pathological image generation, which learns consistent sparse vocabulary tokens through self-supervised contrastive learning. Furthermore, our method efficiently compresses image patches into compact degraded sparse tokens at smaller scales and reconstructs high-quality large-scale whole slide images (WSIs). This is achieved using only an 8x8 vocabulary index for 256x256 images while maintaining minimal reconstruction loss. Experimental results demonstrate that our approach significantly enhances image quality, achieving an approximately 30% improvement in mean Frechet inception distance (FID) compared to popular conditional GANs and diffusion models across various degradation scenarios in pathological images",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyi Liu",
      "Zhe Xu",
      "Jiabo Ma",
      "Wenqiang Li",
      "Ruixuan Wang",
      "Bo Du",
      "Hao Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_BlueNeg_A_35mm_Negative_Film_Dataset_for_Restoring_Channel-Heterogeneous_Deterioration_ICCV_2025_paper.html": {
    "title": "BlueNeg: A 35mm Negative Film Dataset for Restoring Channel-Heterogeneous Deterioration",
    "volume": "main",
    "abstract": "While digitally acquired photographs have been dominating since around 2000, there remains a huge amount of legacy photographs being acquired by optical cameras and are stored in the form of film negatives. In this paper, we address the unique challenge of channel-heterogeneous deterioration in film negatives and introduce BlueNeg, the first high-quality 35mm negative film dataset specifically designed for restoration in this context. Our work aims to spotlight this underexplored area of image restoration on channel-heterogeneous deterioration. A key challenge in evaluating restoration performance is the absence of ground truth, as many negatives are already contaminated. To overcome this, we utilize printed photographs from the same negatives as proxies for quantitative assessment. These prints, while affected by spatially invariant color fading, are less affected from channel-heterogeneous degradation. We propose a reverse-printing process to estimate pseudo-ground truth from the prints and introduce a dedicated evaluation protocol. Our empirical analysis shows that existing restoration methods struggle with this dataset, highlighting the need for specialized techniques. We hope that BlueNeg and our evaluation framework will inspire further research in legacy photograph restoration, contributing to the digital preservation of historical moments for archival use. Our dataset is publicly available at https://huggingface.co/datasets/ttgroup/blueneg-release and https://blueneg.github.io",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanyuan Liu",
      "Chengze Li",
      "Minshan Xie",
      "Zhenni Wang",
      "Jiawen Liang",
      "Chi-Sing Leung",
      "Tien-Tsin Wong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Braun_egoPPG_Heart_Rate_Estimation_from_Eye-Tracking_Cameras_in_Egocentric_Systems_ICCV_2025_paper.html": {
    "title": "egoPPG: Heart Rate Estimation from Eye-Tracking Cameras in Egocentric Systems to Benefit Downstream Vision Tasks",
    "volume": "main",
    "abstract": "Egocentric vision systems aim to understand the spatial surroundings and the wearer's behavior inside it, including motions, activities, and interactions. We argue that egocentric systems must additionally detect physiological states to capture a person's attention and situational responses, which are critical for context-aware behavior modeling. In this paper, we propose egoPPG, a novel vision task for egocentric systems to recover a person's cardiac activity to aid downstream vision tasks. We introduce PulseFormer, a method to extract heart rate as a key indicator of physiological state from the eye tracking cameras on unmodified egocentric vision systems. PulseFormer continuously estimates the photoplethysmogram (PPG) from areas around the eyes and fuses motion cues from the headset's inertial measurement unit to track HR values. We demonstrate egoPPG's downstream benefit for a key task on EgoExo4D, an existing egocentric dataset for which we find PulseFormer's estimates of HR to improve proficiency estimation by 14%. To train and validate PulseFormer, we collected a dataset of 13+ hours of eye tracking videos from Project Aria and contact-based PPG signals as well as an electrocardiogram (ECG) for ground-truth HR values. Similar to EgoExo4D, 25 participants performed diverse everyday activities such as office work, cooking, dancing, and exercising, which induced significant natural motion and HR variation (44-164 bpm). Our model robustly estimates HR (MAE=7.67 bpm) and captures patterns (r=0.85). Our results show how egocentric systems may unify environmental and physiological tracking to better understand users and that egoPPG as a complementary task provides meaningful augmentations for existing datasets and tasks. We release our code, dataset, and HR augmentations for EgoExo4D to inspire research on physiology-aware egocentric tasks",
    "checked": true,
    "id": "4786969c5dbfc4d20e81a92ea0e72b49e8fa550b",
    "semantic_title": "egoppg: heart rate estimation from eye-tracking cameras in egocentric systems to benefit downstream vision tasks",
    "citation_count": 4,
    "authors": [
      "Björn Braun",
      "Rayan Armani",
      "Manuel Meier",
      "Max Moebus",
      "Christian Holz"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhu_Beyond_Pixel_Uncertainty_Bounding_the_OoD_Objects_in_Road_Scenes_ICCV_2025_paper.html": {
    "title": "Beyond Pixel Uncertainty: Bounding the OoD Objects in Road Scenes",
    "volume": "main",
    "abstract": "Recognizing out-of-distribution (OoD) objects on roads is crucial for safe driving. Most existing methods rely on segmentation models' uncertainty as anomaly scores, often resulting in false positives - especially at ambiguous regions like boundaries, where segmentation models inherently exhibit high uncertainty. Additionally, it is challenging to define a suitable threshold to generate anomaly masks, especially with the inconsistencies in predictions across consecutive frames. We propose DetSeg, a novel paradigm that helps incorporate object-level understanding. DetSeg first detects all objects in the open world and then suppresses in-distribution (ID) bounding boxes, leaving only OoD proposals. These proposals can either help previous methods eliminate false positives (DetSeg-\\mathcal R ), or generate binary anomaly masks without complex threshold search when combined with a box-prompted segmentation module (DetSeg-\\mathcal S ). Additionally, we introduce vanishing point guided Hungarian matching (VPHM) to smooth the prediction results within a video clip, mitigating abrupt variations of predictions between consecutive frames. Comprehensive experiments on various benchmarks demonstrate that DetSeg significantly improves performance, reducing the FPR\\it _ 95 of previous methods by up to 37.45%, offering a more robust and practical solution. Code: https://github.com/huachao0124/DetSeg-official",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huachao Zhu",
      "Zelong Liu",
      "Zhichao Sun",
      "Yuda Zou",
      "Gui-Song Xia",
      "Yongchao Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Canham_Gain-MLP_Improving_HDR_Gain_Map_Encoding_via_a_Lightweight_MLP_ICCV_2025_paper.html": {
    "title": "Gain-MLP: Improving HDR Gain Map Encoding via a Lightweight MLP",
    "volume": "main",
    "abstract": "While most images shared on the web and social media platforms are encoded in standard dynamic range (SDR), many displays now can accommodate high dynamic range (HDR) content. Additionally, modern cameras can capture images in an HDR format but convert them to SDR to ensure maximum compatibility with existing workflows and legacy displays. To support both SDR and HDR, new encoding formats are emerging that store additional metadata in SDR images in the form of a gain map. When applied to the SDR image, the gain map recovers the HDR version of the image as needed. These gain maps, however, are typically down-sampled and encoded using standard image compression, such as JPEG and HEIC, which can result in unwanted artifacts. In this paper, we propose to use a lightweight multi-layer perceptron (MLP) network to encode the gain map. The MLP is optimized using the SDR image information as input and provides superior performance in terms of HDR reconstruction. Moreover, the MLP-based approach uses a fixed memory footprint (10 KB) and requires no additional adjustments to accommodate different image sizes or encoding parameters. We conduct extensive experiments on various MLP based HDR embedding strategies and demonstrate that our approach outperforms the current state-of-the-art",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Trevor D. Canham",
      "SaiKiran Tedla",
      "Michael J. Murdoch",
      "Michael S. Brown"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/He_MaterialMVP_Illumination-Invariant_Material_Generation_via_Multi-view_PBR_Diffusion_ICCV_2025_paper.html": {
    "title": "MaterialMVP: Illumination-Invariant Material Generation via Multi-view PBR Diffusion",
    "volume": "main",
    "abstract": "Physically-based rendering (PBR) has become a cornerstone in modern computer graphics, enabling realistic material representation and lighting interactions in 3D scenes. In this paper, we present MaterialMVP, a novel end-to-end model for generating PBR textures from 3D meshes and image prompts, addressing key challenges in multi-view material synthesis. Our approach leverages Reference Attention to extract and encode informative latent from the input reference images, enabling intuitive and controllable texture generation. We also introduce a Consistency-Regularized Training strategy to enforce stability across varying viewpoints and illumination conditions, ensuring illumination-invariant and geometrically consistent results. Additionally, we propose Dual-Channel Material Generation, which separately optimizes albedo and metallic-roughness (MR) textures while maintaining precise spatial alignment with the input images through Multi-Channel Aligned Attention. Learnable material embeddings are further integrated to capture the distinct properties of albedo and MR. Experimental results demonstrate that our model generates PBR textures with realistic behavior across diverse lighting scenarios, outperforming existing methods in both consistency and quality for scalable 3D asset creation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zebin He",
      "Mingxin Yang",
      "Shuhui Yang",
      "Yixuan Tang",
      "Tao Wang",
      "Kaihao Zhang",
      "Guanying Chen",
      "Yuhong Liu",
      "Jie Jiang",
      "Chunchao Guo",
      "Wenhan Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jiang_Learning_to_See_in_the_Extremely_Dark_ICCV_2025_paper.html": {
    "title": "Learning to See in the Extremely Dark",
    "volume": "main",
    "abstract": "Learning-based methods have made promising advances in low-light RAW image enhancement, while their capability to extremely dark scenes where the environmental illuminance drops as low as 0.0001 lux remains to be explored due to the lack of corresponding datasets. To this end, we propose a paired-to-paired data synthesis pipeline capable of generating well-calibrated extremely low-light RAW images at three precise illuminance ranges of 0.01-0.1 lux, 0.001-0.01 lux, and 0.0001-0.001 lux, together with high-quality sRGB references to comprise a large-scale paired dataset named See-in-the-Extremely-Dark (SIED) to benchmark low-light RAW image enhancement approaches. Furthermore, we propose a diffusion-based framework that leverages the generative ability and intrinsic denoising property of diffusion models to restore visually pleasing results from extremely low-SNR RAW inputs, in which an Adaptive Illumination Correction Module (AICM) and a color consistency loss are introduced to ensure accurate exposure correction and color restoration. Extensive experiments on the proposed SIED and publicly available benchmarks demonstrate the effectiveness of our method. The code and dataset are available at https://github.com/JianghaiSCU/SIED",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hai Jiang",
      "Binhao Guan",
      "Zhen Liu",
      "Xiaohong Liu",
      "Jian Yu",
      "Zheng Liu",
      "Songchen Han",
      "Shuaicheng Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Guo_StyleMotif_Multi-Modal_Motion_Stylization_using_Style-Content_Cross_Fusion_ICCV_2025_paper.html": {
    "title": "StyleMotif: Multi-Modal Motion Stylization using Style-Content Cross Fusion",
    "volume": "main",
    "abstract": "We present StyleMotif, a novel Stylized Motion Latent Diffusion model, generating motion conditioned on both content and style from multiple modalities. Unlike existing approaches that either focus on generating diverse motion content or transferring style from sequences, StyleMotif seamlessly synthesizes motion across a wide range of content while incorporating stylistic cues from multi-modal inputs, including motion, text, image, video, and audio. To achieve this, we introduce a style-content cross fusion mechanism and align a style encoder with a pre-trained multi-modal model, ensuring that the generated motion accurately captures the reference style while preserving realism. Extensive experiments demonstrate that our framework surpasses existing methods in stylized motion generation and exhibits emergent capabilities for multi-modal motion stylization, enabling more nuanced motion synthesis. Project Page: https://stylemotif.github.io",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyu Guo",
      "Young Yoon Lee",
      "Joseph Liu",
      "Yizhak Ben-Shabat",
      "Victor Zordan",
      "Mubbasir Kapadia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Alam_SpecGuard_Spectral_Projection-based_Advanced_Invisible_Watermarking_ICCV_2025_paper.html": {
    "title": "SpecGuard: Spectral Projection-based Advanced Invisible Watermarking",
    "volume": "main",
    "abstract": "Watermarking embeds imperceptible patterns into images for authenticity verification. However, existing methods often lack robustness against various transformations primarily including distortions, image regeneration, and adversarial perturbation, creating real-world challenges. In this work, we introduce SpecGuard, a novel watermarking approach for robust and invisible image watermarking. Unlike prior approaches, we embed the message inside hidden convolution layers by converting from the spatial domain to the frequency domain using spectral projection of a higher frequency band that is decomposed by wavelet projection. Spectral projection employs Fast Fourier Transform approximation to transform spatial data into the frequency domain efficiently. In the encoding phase, a strength factor enhances resilience against diverse attacks, including adversarial, geometric, and regeneration-based distortions, ensuring the preservation of copyrighted information. Meanwhile, the decoder leverages Parseval's theorem to effectively learn and extract the watermark pattern, enabling accurate retrieval under challenging transformations. We evaluate the proposed SpecGuard based on the embedded watermark's invisibility, capacity, and robustness. Comprehensive experiments demonstrate the proposed SpecGuard outperforms the state-of-the-art models. To ensure reproducibility, the full code is released on \\href https://github.com/inzamamulDU/SpecGuard_ICCV_2025 GitHub",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Inzamamul Alam",
      "Md Tanvir Islam",
      "Simon S. Woo",
      "Khan Muhammad"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shukor_Scaling_Laws_for_Native_Multimodal_Models_ICCV_2025_paper.html": {
    "title": "Scaling Laws for Native Multimodal Models",
    "volume": "main",
    "abstract": "Building general-purpose models that can effectively perceive the world through multimodal signals has been a long-standing goal. Current approaches involve integrating separately pre-trained components, such as connecting vision encoders to LLMs and continuing multimodal training. While such approaches exhibit remarkable sample efficiency, it remains an open question whether such late-fusion architectures are inherently superior. In this work, we revisit the architectural design of native multimodal models (NMMs)--those trained from the ground up on all modalities--and conduct an extensive scaling laws study, spanning 457 trained models with different architectures and training mixtures. Our investigation reveals no inherent advantage to late-fusion architectures over early-fusion ones, which do not rely on image encoders or tokenizers. On the contrary, early fusion exhibits stronger performance at lower parameter counts, is more efficient to train, and is easier to deploy. Motivated by the strong performance of the early-fusion architectures, we show that incorporating Mixture of Experts (MoEs) allows models to learn modality-specific weights, significantly benefiting performance",
    "checked": true,
    "id": "0f7c74d0d990126bd96d8881dcca65286183ae35",
    "semantic_title": "scaling laws for native multimodal models",
    "citation_count": 20,
    "authors": [
      "Mustafa Shukor",
      "Enrico Fini",
      "Victor Guilherme Turrisi da Costa",
      "Matthieu Cord",
      "Joshua Susskind",
      "Alaaeldin El-Nouby"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Bridging_Diffusion_Models_and_3D_Representations_A_3D_Consistent_Super-Resolution_ICCV_2025_paper.html": {
    "title": "Bridging Diffusion Models and 3D Representations: A 3D Consistent Super-Resolution Framework",
    "volume": "main",
    "abstract": "We propose 3D Super Resolution (3DSR), a novel 3D Gaussian-splatting-based super-resolution framework that leverages off-the-shelf diffusion-based 2D super-resolution models. 3DSR encourages 3D consistency across views via the use of an explicit 3D Gaussian-splatting-based scene representation. This makes the proposed 3DSR different from prior work, such as image upsampling or the use of video super-resolution, which either don't consider 3D consistency or aim to incorporate 3D consistency implicitly. Notably, our method enhances visual quality without additional fine-tuning, ensuring spatial coherence within the reconstructed scene. We evaluate 3DSR on MipNeRF360 and LLFF data, demonstrating that it produces high-resolution results that are visually compelling, while maintaining structural consistency in 3D reconstructions. Code will be released",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi-Ting Chen",
      "Ting-Hsuan Liao",
      "Pengsheng Guo",
      "Alexander  Schwing",
      "Jia-Bin Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_MagicID_Hybrid_Preference_Optimization_for_ID-Consistent_and_Dynamic-Preserved_Video_Customization_ICCV_2025_paper.html": {
    "title": "MagicID: Hybrid Preference Optimization for ID-Consistent and Dynamic-Preserved Video Customization",
    "volume": "main",
    "abstract": "Video identity customization seeks to produce high-fidelity videos that maintain consistent identity and exhibit significant dynamics based on users' reference images. However, existing approaches face two key challenges: identity degradation over extended video length and reduced dynamics during training, primarily due to their reliance on traditional self-reconstruction training with static images. To address these issues, we introduce MagicID, a novel framework designed to directly promote the generation of identity-consistent and dynamically rich videos tailored to user preferences. Specifically, we propose constructing pairwise preference video data with explicit identity and dynamic rewards for preference learning, instead of sticking to the traditional self-reconstruction. To address the constraints of customized preference data, we introduce a hybrid sampling strategy. This approach first prioritizes identity preservation by leveraging static videos derived from reference images, then enhances dynamic motion quality in the generated videos using a Frontier-based sampling method. By utilizing these hybrid preference pairs, we optimize the model to align with the reward differences between pairs of customized preferences. Extensive experiments show that MagicID successfully achieves consistent identity and natural dynamics, surpassing existing methods across various metrics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hengjia Li",
      "Lifan Jiang",
      "Xi Xiao",
      "Tianyang Wang",
      "Hongwei Yi",
      "Boxi Wu",
      "Deng Cai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Pan_LookOut_Real-World_Humanoid_Egocentric_Navigation_ICCV_2025_paper.html": {
    "title": "LookOut: Real-World Humanoid Egocentric Navigation",
    "volume": "main",
    "abstract": "The ability to predict collision-free future trajectories from egocentric observations is crucial in applications such as humanoid robotics, VR / AR, and assistive navigation. In this work, we introduce the challenging problem of predicting a sequence of future 6D head poses from an egocentric video. In particular, we predict both head translations and rotations to learn the active information-gathering behavior expressed through head-turning events. To solve this task, we propose a framework that reasons over temporally aggregated 3D latent features, which models the geometric and semantic constraints for both the static and dynamic parts of the environment. Motivated by the lack of training data in this space, we further contribute a data collection pipeline using the Project Aria glasses, and present a dataset collected through this approach. Our dataset, dubbed Aria Navigation Dataset (AND), consists of 4 hours of recording of users navigating in real-world scenarios. It includes diverse situations and navigation behaviors, providing a valuable resource for learning real-world egocentric navigation policies. Extensive experiments show that our model learns human-like navigation behaviors such as waiting / slowing down, rerouting, and looking around for traffic while generalizing to unseen environments. Check out our project webpage at https://sites.google.com/stanford.edu/lookout",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boxiao Pan",
      "Adam W. Harley",
      "Francis Engelmann",
      "C. Karen Liu",
      "Leonidas J. Guibas"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Niedermayr_Lightweight_Gradient-Aware_Upscaling_of_3D_Gaussian_Splatting_Images_ICCV_2025_paper.html": {
    "title": "Lightweight Gradient-Aware Upscaling of 3D Gaussian Splatting Images",
    "volume": "main",
    "abstract": "We introduce an image upscaling technique tailored for 3D Gaussian Splatting (3DGS) on lightweight GPUs. Compared to 3DGS, it achieves significantly higher rendering speeds and reduces artifacts commonly observed in 3DGS reconstructions. Our technique upscales low-resolution 3DGS renderings with a marginal increase in cost by directly leveraging the analytical image gradients of Gaussians for gradient-based bicubic spline interpolation.The technique is agnostic to the specific 3DGS implementation, achieving novel view synthesis at rates 3x-4x higher than the baseline implementation.Through extensive experiments on multiple datasets, we showcase the performance improvements and high reconstruction fidelity attainable with gradient-aware upscaling of 3DGS images.We further demonstrate the integration of gradient-aware upscaling into the gradient-based optimization of a 3DGS model and analyze its effects on reconstruction quality and performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simon Niedermayr",
      "Christoph Neuhauser",
      "Rüdiger Westermann"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Guo_Motion-2-to-3_Leveraging_2D_Motion_Data_for_3D_Motion_Generations_ICCV_2025_paper.html": {
    "title": "Motion-2-to-3: Leveraging 2D Motion Data for 3D Motion Generations",
    "volume": "main",
    "abstract": "Text-driven human motion synthesis has showcased its potential for revolutionizing motion design in the movie and game industry.Existing methods often rely on 3D motion capture data, which requires special setups, resulting in high costs for data acquisition, ultimately limiting the diversity and scope of human motion. In contrast, 2D human videos offer a vast and accessible source of motion data, covering a wider range of styles and activities.In this paper, we explore the use of 2D human motion extracted from videos as an alternative data source to improve text-driven 3D motion generation.Our approach introduces a novel framework that disentangles local joint motion from global movements, enabling efficient learning of local motion priors from 2D data.We first train a single-view 2D local motion generator on a large dataset of text-2D motion pairs.Then we fine-tune the generator with 3D data, transforming it into a multi-view generator that predicts view-consistent local joint motion and root dynamics.Evaluations on the well-acknowledged datasets and novel text prompts demonstrate that our method can efficiently utilizes 2D data, supporting a wider range of realistic 3D human motion generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruoxi Guo",
      "Huaijin Pi",
      "Zehong Shen",
      "Qing Shuai",
      "Zechen Hu",
      "Zhumei Wang",
      "Yajiao Dong",
      "Ruizhen Hu",
      "Taku Komura",
      "Sida Peng",
      "Xiaowei Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zou_Balancing_Conservatism_and_Aggressiveness_Prototype-Affinity_Hybrid_Network_for_Few-Shot_Segmentation_ICCV_2025_paper.html": {
    "title": "Balancing Conservatism and Aggressiveness: Prototype-Affinity Hybrid Network for Few-Shot Segmentation",
    "volume": "main",
    "abstract": "This paper studies the few-shot segmentation (FSS) task, which aims to segment objects belonging to unseen categories in a query image by learning a model on a small number of well-annotated support samples. Our analysis of two mainstream FSS paradigms reveals that the predictions made by prototype learning methods are usually conservative, while those of affinity learning methods tend to be more aggressive. This observation motivates us to balance the conservative and aggressive information captured by these two types of FSS frameworks so as to improve the segmentation performance. To achieve this, we propose a Prototype-Affinity Hybrid Network (PAHNet), which introduces a Prototype-guided Feature Enhancement (PFE) module and an Attention Score Calibration (ASC) module in each attention block of an affinity learning model (called affinity learner). These two modules utilize the predictions generated by a pre-trained prototype learning model (called prototype predictor) to enhance the foreground information in support and query image representations and suppress the mismatched foreground-background (FG-BG) relationships between them, respectively. In this way, the aggressiveness of the affinity learner can be effectively mitigated, thereby eventually increasing the segmentation accuracy of our PAHNet method. Experimental results show that PAHNet outperforms most recently proposed methods across 1-shot and 5-shot settings on both PASCAL-5^i and COCO-20^i datasets, suggesting its effectiveness. The code is available at: https://github.com/tianyu-zou/PAHNet",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyu Zou",
      "Shengwu Xiong",
      "Ruilin Yao",
      "Yi Rong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hu_Large_Scene_Generation_with_Cube-Absorb_Discrete_Diffusion_ICCV_2025_paper.html": {
    "title": "Large Scene Generation with Cube-Absorb Discrete Diffusion",
    "volume": "main",
    "abstract": "Generating realistic 3D outdoor scenes is essential for applications in autonomous driving, virtual reality, environmental science, and urban development. Traditional 3D generation approaches using single-layer diffusion methods can produce detailed scenes for individual objects but struggle with high-resolution, large-scale outdoor environments due to scalability limitations. Recent hierarchical diffusion models tackle this by progressively scaling up low-resolution scenes. However, they often sample fine details from pure noise rather than from the coarse scene, which limits the efficiency. We propose a novel cube-absorb discrete diffusion (CADD) model, which employs low-resolution scenes as the base state in the diffusion process to generate fine details, eliminating the need to sample entirely from noise. Moreover, we introduce the Sparse Cube Diffusion Transformer (SCDT), a transformer-based model with a sparse cube attention operator, optimized for generating large-scale sparse voxel scenes. Our method demonstrates state-of-the-art performance on the CarlaSC and KITTI360 datasets, supported by qualitative visualizations and extensive ablation studies that highlight the impact of the CADD process and sparse cube attention operator on high-resolution 3D scene generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qianjiang Hu",
      "Wei Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lei_Advancing_Text-to-3D_Generation_with_Linearized_Lookahead_Variational_Score_Distillation_ICCV_2025_paper.html": {
    "title": "Advancing Text-to-3D Generation with Linearized Lookahead Variational Score Distillation",
    "volume": "main",
    "abstract": "Text-to-3D generation based on score distillation of pre-trained 2D diffusion models has gained increasing interest, with variational score distillation (VSD) as a remarkable example. VSD proves that vanilla score distillation can be improved by introducing an extra score-based model, which characterizes the distribution of images rendered from 3D models, to correct the distillation gradient. Despite the theoretical foundations, VSD, in practice, is likely to suffer from slow and sometimes ill-posed convergence.In this paper, we perform an in-depth investigation of the interplay between the introduced score model and the 3D model, and find that there exists a mismatching problem between LoRA and 3D distributions in practical implementation. We can simply adjust their optimization order to improve the generation quality. By doing so, the score model looks ahead to the current 3D state and hence yields more reasonable corrections. Nevertheless, naive lookahead VSD may suffer from unstable training in practice due to the potential over-fitting. To address this, we propose to use a linearized variant of the model for score distillation, giving rise to the Linearized Lookahead Variational Score Distillation (L^2-VSD). L^2-VSD can be realized efficiently with forward-mode autodiff functionalities of existing deep learning libraries. Extensive experiments validate the efficacy of L^2-VSD, revealing its clear superiority over prior score distillation-based methods. We also show that our method can be seamlessly incorporated into any other VSD-based text-to-3D framework",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Lei",
      "Bingde Liu",
      "Qingsong Xie",
      "Haonan Lu",
      "Zhijie Deng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Luo_MS3D_High-Quality_3D_Generation_via_Multi-Scale_Representation_Modeling_ICCV_2025_paper.html": {
    "title": "MS3D: High-Quality 3D Generation via Multi-Scale Representation Modeling",
    "volume": "main",
    "abstract": "High-quality textured mesh reconstruction from sparse-view images remains a fundamental challenge in computer graphics and computer vision. Traditional large reconstruction models operate in a single-scale manner, forcing the models to simultaneously capture global structure and local details, often resulting in compromised reconstructed shapes. In this work, we propose MS3D, a novel multi-scale 3D reconstruction framework. At its core, our method introduces a hierarchical structured latent representation for multi-scale modeling, coupled with a multi-scale feature extraction and integration mechanism. This enables progressive reconstruction, effectively decomposing the complex task of detailed geometry reconstruction into a sequence of easier steps. This coarse-to-fine approach effectively captures multi-frequency details, learns complex geometric patterns, and generalizes well across diverse objects while preserving fine-grained details. Extensive experiments demonstrate MS3D outperforms state-of-the-art methods and is broadly applicable to both image- and text-to-3D generation. The entire pipeline reconstructs high-quality textured meshes in under five seconds",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guan Luo",
      "Jianfeng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Memory-Efficient_4-bit_Preconditioned_Stochastic_Optimization_ICCV_2025_paper.html": {
    "title": "Memory-Efficient 4-bit Preconditioned Stochastic Optimization",
    "volume": "main",
    "abstract": "Preconditioned stochastic optimization algorithms, exemplified by Shampoo, outperform first-order optimizers by offering theoretical convergence benefits and practical gains in large-scale neural network training. However, they incur substantial memory overhead due to the storage demands of non-diagonal preconditioning matrices. To address this, we introduce 4-bit quantization for Shampoo's preconditioners. We introduce two key methods: First, we apply Cholesky decomposition followed by quantization of the Cholesky factors, reducing memory usage by leveraging their lower triangular structure while better preserving spectral properties to minimize information loss. To our knowledge, this is the first quantization approach applied to Cholesky factors of preconditioners. Second, we incorporate error feedback in the quantization process, efficiently storing Cholesky factor and error state in the lower and upper triangular parts of the same matrix. Through extensive experiments, we demonstrate that combining Cholesky quantization with error feedback enhances memory efficiency and algorithm performance in large-scale deep-learning tasks. Theoretically, we also provide convergence proofs for quantized Shampoo under both smooth and non-smooth stochastic optimization settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyang Li",
      "Kuangyu Ding",
      "Kim-Chuan Toh",
      "Pan Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Madhavan_On_the_Recovery_of_Cameras_from_Fundamental_Matrices_ICCV_2025_paper.html": {
    "title": "On the Recovery of Cameras from Fundamental Matrices",
    "volume": "main",
    "abstract": "The viewing graph is a compact tool to encode the geometry of multiple views: nodes represent uncalibrated cameras and edges represent fundamental matrices (when available). Most research focuses on theoretical analyses, exploring for which viewing graphs it is possible (in principle) to retrieve cameras from fundamental matrices, in the sense that the problem admits a unique solution for noiseless data. However, the practical task of recovering cameras from noisy fundamental matrices is still open, as available methods are limited to special graphs (such as those covered by triplets). In this paper, we develop the first method that can deal with the recovery of cameras from noisy fundamental matrices in a general viewing graph. Experimental results demonstrate the promise of the proposed approach on a variety of synthetic and real scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rakshith Madhavan",
      "Federica Arrigoni"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_Not_All_Frame_Features_Are_Equal_Video-to-4D_Generation_via_Decoupling_ICCV_2025_paper.html": {
    "title": "Not All Frame Features Are Equal: Video-to-4D Generation via Decoupling Dynamic-Static Features",
    "volume": "main",
    "abstract": "Recently, the generation of dynamic 3D objects from a video has shown impressive results. Existing methods directly optimize Gaussians using whole information in frames. However, when dynamic regions are interwoven with static regions within frames, particularly if the static regions account for a large proportion, existing methods often overlook information in dynamic regions and are prone to overfitting on static regions. This leads to producing results with blurry textures. We consider that decoupling dynamic-static features to enhance dynamic representations can alleviate this issue. Thus, we propose a dynamic-static feature decoupling module (DSFD). Along temporal axes, it regards the regions of current frame features that possess significant differences relative to reference frame features as dynamic features. Conversely, the remaining parts are the static features. Then, we acquire decoupled features driven by dynamic features and current frame features. Moreover, to further enhance the dynamic representation of decoupled features from different viewpoints and ensure accurate motion prediction, we design a temporal-spatial similarity fusion module (TSSF). Along spatial axes, it adaptively selects similar information of dynamic regions. Hinging on the above, we construct a novel approach, DS4D. Experimental results verify our method achieves state-of-the-art (SOTA) results in video-to-4D. In addition, the experiments on a real-world scenario dataset demonstrate its effectiveness on the 4D scene. Project page: https://github.com/LiyingCV/DS4D",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liying Yang",
      "Chen Liu",
      "Zhenwei Zhu",
      "Ajian Liu",
      "Hui Ma",
      "Jian Nong",
      "Yanyan Liang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_FedDifRC_Unlocking_the_Potential_of_Text-to-Image_Diffusion_Models_in_Heterogeneous_ICCV_2025_paper.html": {
    "title": "FedDifRC: Unlocking the Potential of Text-to-Image Diffusion Models in Heterogeneous Federated Learning",
    "volume": "main",
    "abstract": "Federated learning aims at training models collaboratively across participants while protecting privacy. However, one major challenge for this paradigm is the data heterogeneity issue, where biased data preferences across multiple clients, harming the model's convergence and performance. In this paper, we first introduce powerful diffusion models into the federated learning paradigm and show that diffusion representations are effective steers during federated training. To explore the possibility of using diffusion representations in handling data heterogeneity, we propose a novel diffusion-inspired Federated paradigm with Diffusion Representation Collaboration, termed FedDifRC, leveraging meaningful guidance of diffusion models to mitigate data heterogeneity. The key idea is to construct text-driven diffusion contrasting and noise-driven diffusion regularization, aiming to provide abundant class-related semantic information and consistent convergence signals. On the one hand, we exploit the conditional feedback from the diffusion model for different text prompts to build a text-driven contrastive learning strategy. On the other hand, we introduce a noise-driven consistency regularization to align local instances with diffusion denoising representations, constraining the optimization region in the feature space. In addition, FedDifRC can be extended to a self-supervised scheme without relying on any labeled data. We also provide a theoretical analysis for FedDifRC to ensure convergence under non-convex objectives. The experiments on different scenarios validate the effectiveness of FedDifRC and the efficiency of crucial components",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huan Wang",
      "Haoran Li",
      "Huaming Chen",
      "Jun Yan",
      "Jiahua Shi",
      "Jun Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_Vivid4D_Improving_4D_Reconstruction_from_Monocular_Video_by_Video_Inpainting_ICCV_2025_paper.html": {
    "title": "Vivid4D: Improving 4D Reconstruction from Monocular Video by Video Inpainting",
    "volume": "main",
    "abstract": "Reconstructing 4D dynamic scenes from casually captured monocular videos is valuable but highly challenging, as each timestamp is observed from a single viewpoint. We introduce Vivid4D, a novel approach that enhances 4D monocular video synthesis by augmenting observation views -- synthesizing multi-view videos from a monocular input. Unlike existing methods that either solely leverage geometric priors for supervision or use generative priors while overlooking geometry, we integrate both. This reformulates view augmentation as a video inpainting task, where observed views are warped into new viewpoints based on monocular depth priors. To achieve this, we train a video inpainting model on unposed web videos with synthetically generated masks that mimic warping occlusions, ensuring spatially and temporally consistent completion of missing regions. To further mitigate inaccuracies in monocular depth priors, we introduce an iterative view augmentation strategy and a robust reconstruction loss. Experiments demonstrate that our method effectively improves monocular 4D scene reconstruction and completion",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxin Huang",
      "Sheng Miao",
      "Bangbang Yang",
      "Yuewen Ma",
      "Yiyi Liao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Endo_Feather_the_Throttle_Revisiting_Visual_Token_Pruning_for_Vision-Language_Model_ICCV_2025_paper.html": {
    "title": "Feather the Throttle: Revisiting Visual Token Pruning for Vision-Language Model Acceleration",
    "volume": "main",
    "abstract": "Recent works on accelerating Vision-Language Models achieve strong performance across a variety of vision-language tasks despite highly compressing visual information. In this work, we examine the popular acceleration approach of early pruning of visual tokens inside the language model. Surprisingly, we find that while strong performance is maintained across many tasks, it exhibits drastically different behavior for a subset of vision-centric tasks such as localization. Upon further investigation, we uncover a core issue with the acceleration approach where most tokens towards the top of the image are pruned away. Yet, on many benchmarks aiming to evaluate vision-centric capabilities, strong performance persists with the flawed pruning strategy, highlighting these benchmarks' limited ability to assess fine-grained visual capabilities. Based on these findings, we propose FEATHER (Fast and Effective Acceleration wiTH Ensemble cRiteria), a straightforward approach that resolves the discovered early-layer pruning issue and further enhances the preservation of relevant tokens via multistage pruning with early uniform sampling to ensure broad image coverage. With comparable computational savings, we find that FEATHER achieves more than 5x performance improvement on the vision-centric localization benchmarks compared to the original acceleration approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mark Endo",
      "Xiaohan Wang",
      "Serena Yeung-Levy"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ding_RePoseD_Efficient_Relative_Pose_Estimation_With_Known_Depth_Information_ICCV_2025_paper.html": {
    "title": "RePoseD: Efficient Relative Pose Estimation With Known Depth Information",
    "volume": "main",
    "abstract": "Recent advances in monocular depth estimation methods (MDE) and their improved accuracy open new possibilities for their applications. In this paper, we investigate how monocular depth estimates can be used for relative pose estimation. In particular, we are interested in answering the question whether using MDEs improves results over traditional point-based methods. We propose a novel framework for estimating the relative pose of two cameras from point correspondences with associated monocular depths. Since depth predictions are typically defined up to an unknown scale or even both unknown scale and shift parameters, our solvers jointly estimate the scale or both the scale and shift parameters along with the relative pose. We derive efficient solvers considering different types of depths for three camera configurations: (1) two calibrated cameras, (2) two cameras with an unknown shared focal length, and (3) two cameras with unknown different focal lengths. Our new solvers outperform state-of-the-art depth-aware solvers in terms of speed and accuracy. In extensive real experiments on multiple datasets and with various MDEs, we discuss which depth-aware solvers are preferable in which situation. The code is available at https://github.com/kocurvik/mdrp",
    "checked": true,
    "id": "7bc0d55d012ac72270ad8d04c4c0d29f3e6f8a43",
    "semantic_title": "reposed: efficient relative pose estimation with known depth information",
    "citation_count": 1,
    "authors": [
      "Yaqing Ding",
      "Viktor Kocur",
      "Václav Vávra",
      "Zuzana Berger Haladová",
      "Jian Yang",
      "Torsten Sattler",
      "Zuzana Kukelova"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_MEGA_Memory-Efficient_4D_Gaussian_Splatting_for_Dynamic_Scenes_ICCV_2025_paper.html": {
    "title": "MEGA: Memory-Efficient 4D Gaussian Splatting for Dynamic Scenes",
    "volume": "main",
    "abstract": "4D Gaussian Splatting (4DGS) has recently emerged as a promising technique for capturing complex dynamic 3D scenes with high fidelity. It utilizes a 4D Gaussian representation and a GPU-friendly rasterizer, enabling rapid rendering speeds. Despite its advantages, 4DGS faces significant challenges, notably the requirement of millions of 4D Gaussians, each with extensive associated attributes, leading to substantial memory and storage cost. This paper introduces a memory-efficient framework for 4DGS. We streamline the color attribute by decomposing it into a per-Gaussian direct color component with only 3 parameters and a shared lightweight alternating current color predictor. This approach eliminates the need for spherical harmonics coefficients, which typically involve up to 144 parameters in classic 4DGS, thereby creating a memory-efficient 4D Gaussian representation. Furthermore, we introduce an entropy-constrained Gaussian deformation technique that uses a deformation field to expand the action range of each Gaussian and integrates an opacity-based entropy loss to limit the number of Gaussians, thus forcing our model to use as few Gaussians as possible to fit a dynamic scene well. With simple half-precision storage and zip compression, our framework achieves a storage reduction by approximately 190xand 125xon the Technicolor and Neural 3D Video datasets, respectively, compared to the original 4DGS. Meanwhile, it maintains comparable rendering speeds and scene representation quality, setting a new standard in the field. Code is available at https://github.com/Xinjie-Q/MEGA",
    "checked": true,
    "id": "33e98e1ae30f1d88723cc271d3286c6e2f31a27b",
    "semantic_title": "mega: memory-efficient 4d gaussian splatting for dynamic scenes",
    "citation_count": 11,
    "authors": [
      "Xinjie Zhang",
      "Zhening Liu",
      "Yifan Zhang",
      "Xingtong Ge",
      "Dailan He",
      "Tongda Xu",
      "Yan Wang",
      "Zehong Lin",
      "Shuicheng Yan",
      "Jun Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Qian_Beyond_Label_Semantics_Language-Guided_Action_Anatomy_for_Few-shot_Action_Recognition_ICCV_2025_paper.html": {
    "title": "Beyond Label Semantics: Language-Guided Action Anatomy for Few-shot Action Recognition",
    "volume": "main",
    "abstract": "Few-shot action recognition (FSAR) aims to classify human actions in videos with only a small number of labeled samples per category. The scarcity of training data has driven recent efforts to incorporate additional modalities, particularly text. However, the subtle variations in human posture, motion dynamics, and the object interactions that occur during different phases, are critical inherent knowledge of actions that cannot be fully exploited by action labels alone. In this work, we propose Language-Guided Action Anatomy (LGA), a novel framework that goes beyond label semantics by leveraging Large Language Models (LLMs) to dissect the essential representational characteristics hidden beneath action labels. Guided by the prior knowledge encoded in LLM, LGA effectively captures rich spatiotemporal cues in few-shot scenarios. Specifically, for text, we prompt an off-the-shelf LLM to anatomize labels into sequences of atomic action descriptions, focusing on the three core elements of action (subject, motion, object). For videos, a Visual Anatomy Module segments actions into atomic video phases to capture the sequential structure of actions. A fine-grained fusion strategy then integrates textual and visual features at the atomic level, resulting in more generalizable prototypes. Finally, we introduce a Multimodal Matching mechanism, comprising both video-video and video-text matching, to ensure robust few-shot classification. Experimental results demonstrate that LGA achieves state-of-the-art performance across multiple FSAR benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zefeng Qian",
      "Xincheng Yao",
      "Yifei Huang",
      "Chongyang Zhang",
      "Jiangyong Ying",
      "Hong Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/de_La_Gorce_VoluMe_-_Authentic_3D_Video_Calls_from_Live_Gaussian_Splat_ICCV_2025_paper.html": {
    "title": "VoluMe - Authentic 3D Video Calls from Live Gaussian Splat Prediction",
    "volume": "main",
    "abstract": "Virtual 3D meetings offer the potential to enhance copresence, increase engagement and thus improve effectiveness of remote meetings compared to standard 2D video calls. However, representing people in 3D meetings remains a challenge; existing solutions achieve high quality by using complex hardware, making use of fixed appearance via enrolment, or by inverting a pre-trained generative model. These approaches lead to constraints that are unwelcome and ill-fitting for videoconferencing applications.We present the first method to predict 3D Gaussian reconstructions in real time from a single 2D webcam feed, where the 3D representation is not only live and realistic, but also authentic to the input video. By conditioning the 3D representation on each video frame independently, our reconstruction faithfully recreates the input video from the captured viewpoint (a property we call authenticity), while generalizing realistically to novel viewpoints. Additionally, we introduce a stability loss to obtain reconstructions that are temporally stable on video sequences.We show that our method delivers state-of-the-art accuracy in visual quality and stability metrics compared to existing methods, and demonstrate our approach in live one-to-one 3D meetings using only a standard 2D camera and display. This demonstrates that our approach can allow anyone to communicate volumetrically, via a method for 3D videoconferencing that is not only highly accessible, but also realistic and authentic",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Martin de La Gorce",
      "Charlie Hewitt",
      "Tibor Takács",
      "Robert Gerdisch",
      "Zafiirah Hosenie",
      "Givi Meishvili",
      "Marek Kowalski",
      "Thomas J. Cashman",
      "Antonio Criminisi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zheng_Bridging_3D_Anomaly_Localization_and_Repair_via_High-Quality_Continuous_Geometric_ICCV_2025_paper.html": {
    "title": "Bridging 3D Anomaly Localization and Repair via High-Quality Continuous Geometric Representation",
    "volume": "main",
    "abstract": "3D point cloud anomaly detection is essential for robust vision systems but is challenged by pose variations and complex geometric anomalies. Existing patch-based methods often suffer from geometric fidelity issues due to discrete voxelization or projection-based representations, limiting fine-grained anomaly localization.We introduce Pose-Aware Signed Distance Field (PASDF), a novel framework that integrates 3D anomaly detection and repair by learning a continuous, pose-invariant shape representation. PASDF leverages a Pose Alignment Module for canonicalization and a SDF Network to dynamically incorporate pose, enabling implicit learning of high-fidelity anomaly repair templates from the continuous SDF. This facilitates precise pixel-level anomaly localization through an Anomaly-Aware Scoring Module.Crucially, the continuous 3D representation in PASDF extends beyond detection, facilitating in-situ anomaly repair. Experiments on Real3D-AD and Anomaly-ShapeNet demonstrate state-of-the-art performance, achieving high object-level AUROC scores of 80.2% and 90.0%, respectively. These results highlight the effectiveness of continuous geometric representations in advancing 3D anomaly detection and facilitating practical anomaly region repair. Our code will be released to drive further research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bozhong Zheng",
      "Jinye Gan",
      "Xiaohao Xu",
      "Xintao Chen",
      "Wenqiao Li",
      "Xiaonan Huang",
      "Na Ni",
      "Yingna Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_MonoMobility_Zero-Shot_3D_Mobility_Analysis_from_Monocular_Videos_ICCV_2025_paper.html": {
    "title": "MonoMobility: Zero-Shot 3D Mobility Analysis from Monocular Videos",
    "volume": "main",
    "abstract": "Accurately analyzing the motion parts and their motion attributes in dynamic environments is crucial for advancing key areas such as embodied intelligence. Addressing the limitations of existing methods that rely on dense multi-view images or detailed part-level annotations, we propose an innovative framework that can analyze 3D mobility from monocular videos in a zero-shot manner. This framework can precisely parse motion parts and motion attributes only using a monocular video, completely eliminating the need for annotated training data. Specifically, our method first constructs the scene geometry and roughly analyzes the motion parts and their initial motion attributes combining depth estimation, optical flow analysis and point cloud registration method, then employs 2D Gaussian splatting for scene representation. Building on this, we introduce an end-to-end dynamic scene optimization algorithm specifically designed for articulated objects, refining the initial analysis results to ensure the system can handle 'rotation', 'translation', and even complex movements ('rotation+translation'), demonstrating high flexibility and versatility. To validate the robustness and wide applicability of our method, we created a comprehensive dataset comprising both simulated and real-world scenarios. Experimental results show that our framework can effectively analyze articulated object motions in an annotation-free manner, showcasing its significant potential in future embodied intelligence applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyi Zhou",
      "Yulan Guo",
      "Xiaogang Wang",
      "Kai Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ren_Prior-aware_Dynamic_Temporal_Modeling_Framework_for_Sequential_3D_Hand_Pose_ICCV_2025_paper.html": {
    "title": "Prior-aware Dynamic Temporal Modeling Framework for Sequential 3D Hand Pose Estimation",
    "volume": "main",
    "abstract": "3D hand pose estimation plays a critical role in various human-computer interaction tasks. Single-frame 3D hand pose estimation methods have poor temporal smoothness and are easily affected by self-occlusion, which severely impacts their practical applicability. Traditional joint-based sequential pose estimation methods primarily focus on the human body and struggle to handle the complex hand structure, high degrees of freedom in hand motion, and rapidly changing hand motion trends. To address these challenges, we propose a prior-aware dynamic temporal modeling framework for sequential 3D hand pose estimation. We introduce a flexible memory mechanism to model hand prior information, which alleviates the scale and depth ambiguity in single-frame hand pose estimation. Additionally, we propose a dynamic temporal convolution module that adjusts the receptive field size and feature aggregation weights based on the motion information at each moment, effectively capturing rapid motion trends. By decoupling dynamic temporal modeling at the joint and hand levels, our method captures both subtle short-term variations and long-term motion trends, significantly improving the smoothness and accuracy of hand pose estimation. Experiments on four public datasets demonstrate that our method achieves the state-of-the-art results in terms of hand pose estimation accuracy and temporal smoothness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengfei Ren",
      "Jingyu Wang",
      "Haifeng Sun",
      "Qi Qi",
      "Xingyu Liu",
      "Menghao Zhang",
      "Lei Zhang",
      "Jing Wang",
      "Jianxin Liao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Generate_Refine_and_Encode_Leveraging_Synthesized_Novel_Samples_for_On-the-Fly_ICCV_2025_paper.html": {
    "title": "Generate, Refine, and Encode: Leveraging Synthesized Novel Samples for On-the-Fly Fine-Grained Category Discovery",
    "volume": "main",
    "abstract": "In this paper, we investigate a practical yet challenging task: On-the-fly Category Discovery (OCD). This task focuses on the online identification of newly arriving stream data that may belong to both known and unknown categories, utilizing the category knowledge from only labeled data. Existing OCD methods are devoted to fully mining transferable knowledge from only labeled data. However, the transferability learned by these methods is limited because the knowledge contained in known categories is often insufficient, especially when few annotated data/categories are available in fine-grained recognition. To mitigate this limitation, we propose a diffusion-based OCD framework, dubbed DiffGRE, which integrates Generation, Refinement, and Encoding in a multi-stage fashion. Specifically, we first design an attribute-composition generation method based on cross-image interpolation in the diffusion latent space to synthesize novel samples. Then, we propose a diversity-driven refinement approach to select the synthesized images that differ from known categories for subsequent OCD model training. Finally, we leverage a semi-supervised leader encoding to inject additional category knowledge contained in synthesized data into the OCD models, which can benefit the discovery of both known and unknown categories during the on-the-fly inference process. Extensive experiments demonstrate the superiority of our DiffGRE over previous methods on six fine-grained datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Liu",
      "Nan Pu",
      "Haiyang Zheng",
      "Wenjing Li",
      "Nicu Sebe",
      "Zhun Zhong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Visual-RFT_Visual_Reinforcement_Fine-Tuning_ICCV_2025_paper.html": {
    "title": "Visual-RFT: Visual Reinforcement Fine-Tuning",
    "volume": "main",
    "abstract": "Reinforcement Fine-Tuning (RFT) in Large Reasoning Models like OpenAI o1 learns from feedback on its answers, which is especially useful in applications when fine-tuning data is scarce.Recent open-source work like DeepSeek-R1 demonstrates that reinforcement learning with verifiable reward is possibly one key direction in reproducing o1.While the R1-style model has demonstrated success in language models, its application in multi-modal domains remains under-explored.This work introduces Visual Reinforcement Fine-Tuning (Visual-RFT), which further extends the application areas of RFT on visual tasks.Specifically, Visual-RFT first uses Large Vision-Language Models (LVLMs) to generate multiple responses containing reasoning tokens and final answers for each input, and then uses our proposed visual perception verifiable reward functions to update the model via the policy optimization algorithm such as Group Relative Policy Optimization (GRPO).We design different verifiable reward functions for different perception tasks, such as the Intersection over Union (IoU) reward for object detection.Experimental results on fine-grained image classification, few-shot object detection, reasoning grounding, as well as open-vocabulary object detection benchmarks show the competitive performance and advanced generalization ability of Visual-RFT compared with Supervised Fine-tuning (SFT).For example, Visual-RFT improves accuracy by 24.3% over the baseline in one-shot fine-grained image classification with around 100 samples.In few-shot object detection, Visual-RFT also exceeds the baseline by 21.0 on COCO's 4-shot setting and 15.4 on LVIS.Our Visual-RFT represents a paradigm shift in fine-tuning LVLMs, offering a data-efficient, reward-driven approach that enhances reasoning and adaptability for domain-specific tasks",
    "checked": true,
    "id": "2464e287e6f92738fd5627819597172e8674e36b",
    "semantic_title": "visual-rft: visual reinforcement fine-tuning",
    "citation_count": 261,
    "authors": [
      "Ziyu Liu",
      "Zeyi Sun",
      "Yuhang Zang",
      "Xiaoyi Dong",
      "Yuhang Cao",
      "Haodong Duan",
      "Dahua Lin",
      "Jiaqi Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Pei_Foresight_in_Motion_Reinforcing_Trajectory_Prediction_with_Reward_Heuristics_ICCV_2025_paper.html": {
    "title": "Foresight in Motion: Reinforcing Trajectory Prediction with Reward Heuristics",
    "volume": "main",
    "abstract": "Motion forecasting for on-road traffic agents presents both a significant challenge and a critical necessity for ensuring safety in autonomous driving systems. In contrast to most existing data-driven approaches that directly predict future trajectories, we rethink this task from a planning perspective, advocating a \"First Reasoning, Then Forecasting\" strategy that explicitly incorporates behavior intentions as spatial guidance for trajectory prediction. To achieve this, we introduce an interpretable, reward-driven intention reasoner grounded in a novel query-centric Inverse Reinforcement Learning (IRL) scheme. Our method first encodes traffic agents and scene elements into a unified vectorized representation, then aggregates contextual features through a query-centric paradigm. This enables the derivation of a reward distribution, a compact yet informative representation of the target agent's behavior within the given scene context via IRL. Guided by this reward heuristic, we perform policy rollouts to reason about multiple plausible intentions, providing valuable priors for subsequent trajectory generation. Finally, we develop a hierarchical DETR-like decoder integrated with bidirectional selective state space models to produce accurate future trajectories along with their associated probabilities. Extensive experiments on the large-scale Argoverse and nuScenes motion forecasting datasets demonstrate that our approach significantly enhances trajectory prediction confidence, achieving highly competitive performance relative to state-of-the-art methods",
    "checked": true,
    "id": "c49c5b00580d4a08ead9da2630ab11aa84fafa61",
    "semantic_title": "foresight in motion: reinforcing trajectory prediction with reward heuristics",
    "citation_count": 1,
    "authors": [
      "Muleilan Pei",
      "Shaoshuai Shi",
      "Xuesong Chen",
      "Xu Liu",
      "Shaojie Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Makabe_Spectral_Sensitivity_Estimation_with_an_Uncalibrated_Diffraction_Grating_ICCV_2025_paper.html": {
    "title": "Spectral Sensitivity Estimation with an Uncalibrated Diffraction Grating",
    "volume": "main",
    "abstract": "This paper introduces a practical and accurate calibration method for camera spectral sensitivity using a diffraction grating. Accurate calibration of camera spectral sensitivity is crucial for various computer vision tasks, including color correction, illumination estimation, and material analysis. Unlike existing approaches that require specialized narrow-band filters or reference targets with known spectral reflectances, our method only requires an uncalibrated diffraction grating sheet, readily available off-the-shelf. By capturing images of the direct illumination and its diffracted pattern through the grating sheet, our method estimates both the camera spectral sensitivity and the diffraction grating parameters in a closed-form manner. Experiments on synthetic and real-world data demonstrate that our method outperforms conventional reference target-based methods, underscoring its effectiveness and practicality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lilika Makabe",
      "Hiroaki Santo",
      "Fumio Okura",
      "Michael S. Brown",
      "Yasuyuki Matsushita"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Mirzaei_DISTIL_Data-Free_Inversion_of_Suspicious_Trojan_Inputs_via_Latent_Diffusion_ICCV_2025_paper.html": {
    "title": "DISTIL: Data-Free Inversion of Suspicious Trojan Inputs via Latent Diffusion",
    "volume": "main",
    "abstract": "Deep neural networks have demonstrated remarkable success across numerous tasks, yet they remain vulnerable to Trojan (backdoor) attacks, raising serious concerns about their safety in real-world mission-critical applications. A common countermeasure is trigger inversion -- reconstructing malicious \"shortcut\" patterns (triggers) inserted by an adversary during training. Current trigger-inversion methods typically search the full pixel space under specific assumptions but offer no assurances that the estimated trigger is more than an adversarial perturbation that flips the model output. Here, we propose a data-free, zero-shot trigger-inversion strategy that restricts the search space while avoiding strong assumptions on trigger appearance. Specifically, we incorporate a diffusion-based generator guided by the target classifier; through iterative generation, we produce candidate triggers that align with the internal representations the model relies on for malicious behavior. Empirical evaluations, both quantitative and qualitative, show that our approach reconstructs triggers that effectively distinguish clean versus Trojaned models. DISTIL surpasses alternative methods by high margins, achieving up to 7.1% higher accuracy on the BackdoorBench dataset and a 9.4% improvement on trojaned object detection model scanning, offering a promising new direction for reliable backdoor defense without reliance on extensive data or strong prior assumptions about triggers. The code is available at https://github.com/AdaptiveMotorControlLab/DISTIL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hossein Mirzaei",
      "Zeinab Taghavi",
      "Sepehr Rezaee",
      "Masoud  Hadi",
      "Moein Madadi",
      "Mackenzie W. Mathis"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sun_Moment_Quantization_for_Video_Temporal_Grounding_ICCV_2025_paper.html": {
    "title": "Moment Quantization for Video Temporal Grounding",
    "volume": "main",
    "abstract": "Video temporal grounding is a critical video understanding task, which aims to localize moments relevant to a language description. The challenge of this task lies in distinguishing relevant and irrelevant moments. Previous methods focused on learning continuous features exhibit weak differentiation between foreground and background features. In this paper, we propose a novel Moment-Quantization based Video Temporal Grounding method (MQVTG), which quantizes the input video into various discrete vectors to enhance the discrimination between relevant and irrelevant moments. Specifically, MQVTG maintains a learnable moment codebook, where each video moment matches a codeword. Considering the visual diversity, i.e., various visual expressions for the same moment, MQVTG treats moment-codeword matching as a clustering process without using discrete vectors, avoiding the loss of useful information from direct hard quantization. Additionally, we employ effective prior-initialization and joint-projection strategies to enhance the maintained moment codebook. With its simple implementation, the proposed method can be integrated into existing temporal grounding models as a plug-and-play component. Extensive experiments on six popular benchmarks demonstrate the effectiveness and generalizability of MQVTG, significantly outperforming state-of-the-art methods. Further qualitative analysis shows that our method effectively groups relevant features and separates irrelevant ones, aligning with our goal of enhancing discrimination. Code is available at https://github.com/TensorsSun/MQVTG",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaolong Sun",
      "Le Wang",
      "Sanping Zhou",
      "Liushuai Shi",
      "Kun Xia",
      "Mengnan Liu",
      "Yabing Wang",
      "Gang Hua"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_ADIEE_Automatic_Dataset_Creation_and_Scorer_for_Instruction-Guided_Image_Editing_ICCV_2025_paper.html": {
    "title": "ADIEE: Automatic Dataset Creation and Scorer for Instruction-Guided Image Editing Evaluation",
    "volume": "main",
    "abstract": "Recent advances in instruction-guided image editing underscore the need for effective automated evaluation. While Vision-Language Models (VLMs) have been explored as judges, open-source models struggle with alignment, and proprietary models lack transparency and cost efficiency. Additionally, no public training datasets exist to fine-tune open-source VLMs, only small benchmarks with diverse evaluation schemes. To address this, we introduce ADIEE, an automated dataset creation approach which is then used to train a scoring model for instruction-guided image editing evaluation. We generate a large-scale dataset with over 100K samples and use it to fine-tune a LLaVA-NeXT-8B model modified to decode a numeric score from a custom token. The resulting scorer outperforms all open-source VLMs and Gemini-Pro 1.5 across all benchmarks, achieving a 0.0696 (+17.24%) gain in score correlation with human ratings on AURORA-Bench, and improving pair-wise comparison accuracy by 4.03% (+7.21%) on GenAI-Bench and 4.75% (+9.35%) on AURORA-Bench, respectively, compared to the state-of-the-art. The scorer can act as a reward model, enabling automated best edit selection and model fine-tuning. Notably, the proposed scorer can boost MagicBrush model's average evaluation score on ImagenHub from 5.90 to 6.43 (+8.98%). Our code and models are available at https://github.com/SherryXTChen/ADIEE.git",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sherry X. Chen",
      "Yi Wei",
      "Luowei Zhou",
      "Suren Kumar"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yu_Dynamic_Reconstruction_of_Hand-Object_Interaction_with_Distributed_Force-aware_Contact_Representation_ICCV_2025_paper.html": {
    "title": "Dynamic Reconstruction of Hand-Object Interaction with Distributed Force-aware Contact Representation",
    "volume": "main",
    "abstract": "We present ViTaM-D, a novel visual-tactile framework for reconstructing dynamic hand-object interaction with distributed tactile sensing to enhance contact modeling. Existing methods, relying solely on visual inputs, often fail to capture occluded interactions and object deformation. To address this, we introduce DF-Field, a distributed force-aware contact representation leveraging kinetic and potential energy in hand-object interactions. ViTaM-D first reconstructs interactions using a visual network with contact constraint, then refines contact details through force-aware optimization, improving object deformation modeling. To evaluate deformable object reconstruction, we introduce the HOT dataset, featuring 600 hand-object interaction sequences in a high-precision simulation environment. Experiments on DexYCB and HOT datasets show that ViTaM-D outperforms state-of-the-art methods in reconstruction accuracy for both rigid and deformable objects. DF-Field also proves more effective in refining hand poses and enhancing contact modeling than previous refinement methods. The code, models, and datasets are available at https://sites.google.com/view/vitam-d/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenjun Yu",
      "Wenqiang Xu",
      "Pengfei Xie",
      "Yutong Li",
      "Brian W. Anthony",
      "Zhuorui Zhang",
      "Cewu Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Nguyen_CSD-VAR_Content-Style_Decomposition_in_Visual_Autoregressive_Models_ICCV_2025_paper.html": {
    "title": "CSD-VAR: Content-Style Decomposition in Visual Autoregressive Models",
    "volume": "main",
    "abstract": "Disentangling content and style from a single image, known as content-style decomposition (CSD), enables recontextualization of extracted content and stylization of extracted styles, offering greater creative flexibility in visual synthesis. While recent personalization methods have explored the decomposition of explicit content style, they remain tailored for diffusion models. Meanwhile, Visual Autoregressive Modeling (VAR) has emerged as a promising alternative with a next-scale prediction paradigm, achieving performance comparable to that of diffusion models. In this paper, we explore VAR as a generative framework for CSD, leveraging its scale-wise generation process for improved disentanglement. To this end, we propose CSD-VAR, a novel method that introduces three key innovations: (1) a scale-aware alternating optimization strategy that aligns content and style representation with their respective scales to enhance separation, (2) an SVD-based rectification method to mitigate content leakage into style representations, and (3) an Augmented Key-Value (K-V) memory enhancing content identity preservation. To benchmark this task, we introduce CSD-100, a dataset specifically designed for content-style decomposition, featuring diverse subjects rendered in various artistic styles. Experiments demonstrate that CSD-VAR outperforms prior approaches, achieving superior content preservation and stylization fidelity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quang-Binh Nguyen",
      "Minh Luu",
      "Quang Nguyen",
      "Anh Tran",
      "Khoi Nguyen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Han_MSQ_Memory-Efficient_Bit_Sparsification_Quantization_ICCV_2025_paper.html": {
    "title": "MSQ: Memory-Efficient Bit Sparsification Quantization",
    "volume": "main",
    "abstract": "As deep neural networks (DNNs) see increased deployment on mobile and edge devices, optimizing model efficiency has become crucial. Mixed-precision quantization is widely favored, as it offers a superior balance between efficiency and accuracy compared to uniform quantization. However, finding the optimal precision for each layer is challenging. Recent studies utilizing bit-level sparsity have shown promise, yet they often introduce substantial training complexity and high GPU memory requirements. In this paper, we propose Memory-Efficient Bit Sparsification Quantization (MSQ), a novel approach that addresses these limitations. MSQ applies a round-clamp quantizer to enable differentiable computation of the least significant bits (LSBs) from model weights. It further employs regularization to induce sparsity in these LSBs, enabling effective precision reduction without explicit bit-level parameter splitting. Additionally, MSQ incorporates Hessian information, allowing the simultaneous pruning of multiple LSBs to further enhance training efficiency. Experimental results show that MSQ achieves up to 8.00x reduction in trainable parameters and up to 86% reduction in training time compared to previous bit-level quantization, while maintaining competitive accuracy and compression rates. This makes it a practical solution for training efficient DNNs on resource-constrained devices",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seokho Han",
      "Seoyeon Yoon",
      "Jinhee Kim",
      "Dongwei Wang",
      "Kang Eun Jeon",
      "Huanrui Yang",
      "Jong Hwan Ko"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Junhao_Disrupting_Model_Merging_A_Parameter-Level_Defense_Without_Sacrificing_Accuracy_ICCV_2025_paper.html": {
    "title": "Disrupting Model Merging: A Parameter-Level Defense Without Sacrificing Accuracy",
    "volume": "main",
    "abstract": "Model merging is a technique that combines multiple finetuned models into a single model without additional training, allowing a free-rider to cheaply inherit specialized capabilities. This study investigates methodologies to suppress unwanted model merging by free-riders. Existing methods such as model watermarking or fingerprinting can only detect merging in hindsight. In contrast, we propose a first proactive defense against model merging. Specifically, our defense method modifies the model parameters so that the model is disrupted if the model is merged with any other model, while its functionality is kept unchanged if not merged with others. Our approach consists of two modules, rearranging MLP parameters and scaling attention heads, which push the model out of the shared basin in parameter space, causing the merging performance with other models to degrade significantly. We conduct extensive experiments on image classification, image generation, and text classification to demonstrate that our defense severely disrupts merging while retaining the functionality of the post-protect model. Our code is available on Github: https://github.com/ISCT-W/PaRaMS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Junhao",
      "Yu Zhe",
      "Jun Sakuma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shi_VSSD_Vision_Mamba_with_Non-Causal_State_Space_Duality_ICCV_2025_paper.html": {
    "title": "VSSD: Vision Mamba with Non-Causal State Space Duality",
    "volume": "main",
    "abstract": "Vision transformers have significantly advanced the field of computer vision, offering robust modeling capabilities and global receptive field. However, their high computational demands limit their applicability in processing long sequences. To tackle this issue, State Space Models (SSMs) have gained prominence in vision tasks as they offer linear computational complexity. Recently, State Space Duality (SSD), an improved variant of SSMs, was introduced in Mamba2 to enhance model performance and efficiency. However, the inherent causal nature of SSD/SSMs restricts their applications in non-causal vision tasks. To address this limitation, we introduce Visual State Space Duality (VSSD) model, which has a non-causal format of SSD. Specifically, we propose to discard the magnitude of interactions between the hidden state and tokens while preserving their relative weights, which relieves the dependencies of token contribution on previous tokens. Together with the involvement of multi-scan strategies, we show that the scanning results can be integrated to achieve non-causality, which not only improves the performance of SSD in vision tasks but also enhances its efficiency. We conduct extensive experiments on various benchmarks including image classification, detection, and segmentation, where VSSD surpasses existing state-of-the-art SSM-based models. Code is available at https://github.com/YuHengsss/VSSD",
    "checked": true,
    "id": "0da8568dc1b3dfc781c51881c082a83f731bc89f",
    "semantic_title": "vssd: vision mamba with non-causal state space duality",
    "citation_count": 11,
    "authors": [
      "Yuheng Shi",
      "Mingjia Li",
      "Minjing Dong",
      "Chang Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gao_Epipolar_Consistent_Attention_Aggregation_Network_for_Unsupervised_Light_Field_Disparity_ICCV_2025_paper.html": {
    "title": "Epipolar Consistent Attention Aggregation Network for Unsupervised Light Field Disparity Estimation",
    "volume": "main",
    "abstract": "Disparity estimation is an essential step in processing and analyzing Light Field (LF) images. Recent methods construct the cost volume to exploit the correspondence of the LFs over the preset maximum disparity, limiting them to process the large parallax scenes. Different from constructing cost volume, the self-attention mechanism calculates the parallax attention between epipolar lines to find the matching points. However, for LFs that have different views, the related disparity scales are different in parallax attention since the baselines with the central view are different. Moreover, if the matching information is occluded in one view, the disparity information can be explored through other views. Therefore, mapping these attentions to the same scale and selecting effective matching information are key points for disparity estimation from parallax attention. In this paper, we explore parallax attention for LF and design an unsupervised method, named Epipolar Consistent Attention Aggregation Network (ECAAN). We first introduce an epipolar consistent scale unification block by considering the consistency relationships to standardize disparity scales of the parallax attention maps. Based on the intra-properties and inter-relationships of parallax attention, we further propose a consistent occlusion-free aggregation block to integrate the information from the occlusion-free areas. Besides, we design an improved photometric loss to constrain the model. ECAAN achieves state-of-the-art performance in LF depth estimation. Notably, ECAAN attains a mean square error (MSE) of 0.2 on large-disparity LF datasets, achieving a 68% error reduction compared to the second-best method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Gao",
      "Shuo Zhang",
      "Youfang Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cai_LLaVA-KD_A_Framework_of_Distilling_Multimodal_Large_Language_Models_ICCV_2025_paper.html": {
    "title": "LLaVA-KD: A Framework of Distilling Multimodal Large Language Models",
    "volume": "main",
    "abstract": "The success of Large Language Models (LLMs) has inspired the development of Multimodal Large Language Models (MLLMs) for unified understanding of vision and language. However, the increasing model size and computational complexity of large-scale MLLMs (l-MLLMs) limit their use in resource-constrained scenarios. Although small-scale MLLMs (s-MLLMs) are designed to reduce computational costs, they typically suffer from performance degradation.To mitigate this limitation, we propose a novel LLaVA-KD framework to transfer knowledge from l-MLLMs to s-MLLMs. Specifically, we introduce Multimodal Distillation (MDist) to transfer teacher model's robust representations across both visual and linguistic modalities, and Relation Distillation (RDist) to transfer teacher model's ability to capture visual token relationships.Additionally, we propose a three-stage training scheme to fully exploit the potential of the proposed distillation strategy: 1) Distilled Pre-Training to strengthen the alignment between visual-linguistic representations in s-MLLMs, 2) Supervised Fine-Tuning to equip the s-MLLMs with multimodal understanding capacity, and 3) Distilled Fine-Tuning to refine s-MLLM's knowledge.Our approach significantly improves s-MLLMs performance without altering the model architecture. Extensive experiments and ablation studies validate the effectiveness of each proposed component. Code will be available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Cai",
      "Jiangning Zhang",
      "Haoyang He",
      "Xinwei He",
      "Ao Tong",
      "Zhenye Gan",
      "Chengjie Wang",
      "Zhucun Xue",
      "Yong Liu",
      "Xiang Bai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jiao_CLIP-GS_Unifying_Vision-Language_Representation_with_3D_Gaussian_Splatting_ICCV_2025_paper.html": {
    "title": "CLIP-GS: Unifying Vision-Language Representation with 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "Recent works in 3D representation learning and multimodal pre-training have made remarkable progress. However, typically multimodal 3D models are only capable of handling point clouds. Compared to the emerging 3D representation technique, 3D Gaussian Splatting (3DGS), the spatially sparse point cloud cannot depict the texture information of 3D objects, resulting in inferior reconstruction capabilities. This limitation constrains the potential of point cloud-based 3D multimodal representation learning. In this paper, we present CLIP-GS, a novel multimodal representation learning framework grounded in 3DGS. We introduce the GS Tokenizer to generate serialized gaussian tokens, which are then processed through a series of transformer layers pre-initialized with weights from point cloud models, resulting in the 3DGS embeddings. CLIP-GS leverages contrastive loss between 3DGS and the visual-text embeddings of CLIP, and we introduce an image voting loss to guide the directionality and convergence of gradient optimization. Furthermore, we develop an efficient way to generate triplets of 3DGS, images, and text, facilitating CLIP-GS in learning unified multimodal representations. Leveraging the well-aligned multimodal representations, CLIP-GS demonstrates versatility and outperforms point cloud-based models on various 3D tasks, including multimodal retrieval, zero-shot, and few-shot classification",
    "checked": true,
    "id": "93d37c4f0ca27f88045601f18afdc585c38c46ed",
    "semantic_title": "clip-gs: unifying vision-language representation with 3d gaussian splatting",
    "citation_count": 2,
    "authors": [
      "Siyu Jiao",
      "Haoye Dong",
      "Yuyang Yin",
      "Zequn Jie",
      "Yinlong Qian",
      "Yao Zhao",
      "Humphrey Shi",
      "Yunchao Wei"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Nguyen_Region-Level_Data_Attribution_for_Text-to-Image_Generative_Models_ICCV_2025_paper.html": {
    "title": "Region-Level Data Attribution for Text-to-Image Generative Models",
    "volume": "main",
    "abstract": "Data attribution in text-to-image generative models is a crucial yet underexplored problem, particularly at the regional level, where identifying the most influential training regions for generated content can enhance transparency, copyright protection, and error diagnosis. Existing data attribution methods either operate at the whole-image level or lack scalability for large-scale generative models. In this work, we propose a novel framework for region-level data attribution. At its core is the Attribution Region (AR) detector, which localizes influential regions in training images used by the text-to-image generative model. To support this research, we construct a large-scale synthetic dataset with ground-truth region-level attributions, enabling both training and evaluation of our method. Empirical results show that our method outperforms existing attribution techniques in accurately tracing generated content back to training data. Additionally, we demonstrate practical applications, including identifying artifacts in generated images and suggesting improved replacements for generated content. Our dataset and framework will be released to advance further research in region-level data attribution for generative models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Trong Bang Nguyen",
      "Phi Le Nguyen",
      "Simon Lucey",
      "Minh Hoai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zholus_TAPNext_Tracking_Any_Point_TAP_as_Next_Token_Prediction_ICCV_2025_paper.html": {
    "title": "TAPNext: Tracking Any Point (TAP) as Next Token Prediction",
    "volume": "main",
    "abstract": "Tracking Any Point (TAP) in a video is a challenging computer vision problem with many demonstrated applications in robotics, video editing, and 3D reconstruction. Existing methods for TAP rely heavily on complex tracking-specific inductive biases and heuristics, limiting their generality and potential for scaling. To address these challenges, we present TAPNext, a new approach that casts TAP as sequential masked token decoding. Our model is causal, tracks in a purely online fashion, and removes tracking-specific inductive biases. This enables TAPNext to run with minimal latency, and removes the temporal windowing required by many existing state of art trackers. Despite its simplicity, TAPNext achieves a new state-of-the-art tracking performance among both online and offline trackers. Finally, we present evidence that many widely used tracking heuristics emerge naturally in TAPNext through end-to-end training",
    "checked": true,
    "id": "c490787518185bf04a2f76dd9d7fa4b35bbfd5ff",
    "semantic_title": "tapnext: tracking any point (tap) as next token prediction",
    "citation_count": 8,
    "authors": [
      "Artem Zholus",
      "Carl Doersch",
      "Yi Yang",
      "Skanda Koppula",
      "Viorica Patraucean",
      "Xu Owen He",
      "Ignacio Rocco",
      "Mehdi S. M. Sajjadi",
      "Sarath Chandar",
      "Ross Goroshin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xu_Learnable_Feature_Patches_and_Vectors_for_Boosting_Low-light_Image_Enhancement_ICCV_2025_paper.html": {
    "title": "Learnable Feature Patches and Vectors for Boosting Low-light Image Enhancement without External Knowledge",
    "volume": "main",
    "abstract": "A major challenge in Low-Light Image Enhancement (LLIE) is its ill-posed nature: low-light images often lack sufficient information to align with normal-light ones (e.g., not all training data can be fully fitted to the ground truth). Numerous studies have attempted to bridge the gap between low- and normal-light data by introducing effective additional information, which is called \"references\" in this paper. However, existing methods overlook the valuable references hidden within the training dataset itself. In this work, we propose a novel LLIE strategy that simultaneously learns image-specific features by neural networks while formulating effective common features from the training data as the reference. These common features are correlated with the samples that are not fully fitted by the LLIE network itself, and they are represented as a set of Learnable Feature Patches and Vectors (LFPVs) in the hidden feature space. LFPVs are updated through two mechanisms: the sample-updater, which extracts useful features from training samples to refine LFPVs, and the mutual-updater, which propagates information across LFPVs to mutually update them. LFPVs can be adaptively aligned with image-specific features via our designed query-and-fusion procedure, boosting the LLIE performance. Our proposed method can be integrated into any LLIE framework, improving both enhancement quality and downstream task performance. Extensive experiments on various benchmarks demonstrate the effectiveness of our approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaogang Xu",
      "Jiafei Wu",
      "Qingsen Yan",
      "Jiequan Cui",
      "Richang Hong",
      "Bei Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Bahng_Cycle_Consistency_as_Reward_Learning_Image-Text_Alignment_without_Human_Preferences_ICCV_2025_paper.html": {
    "title": "Cycle Consistency as Reward: Learning Image-Text Alignment without Human Preferences",
    "volume": "main",
    "abstract": "Learning alignment between language and vision is a fundamental challenge, especially as multimodal data becomes increasingly detailed and complex. Existing methods often rely on collecting human or AI preferences, which can be costly and time-intensive. We propose an alternative approach that leverages cycle consistency as a supervisory signal. Given an image and generated text, we map the text back to image space using a text-to-image model and compute the similarity between the original image and its reconstruction. Analogously, for text-to-image generation, we measure the textual similarity between an input caption and its reconstruction through the cycle. We use the cycle consistency score to rank candidates and construct a preference dataset of 866K comparison pairs. The reward model trained on our dataset, CycleReward, outperforms state-of-the-art alignment metrics on detailed captioning, with superior inference-time scalability when used as a verifier for Best-of-N sampling, while maintaining speed and differentiability. Furthermore, performing DPO and Diffusion DPO using our dataset enhances performance across a wide range of vision-language tasks and text-to-image generation. Our dataset, model, and code are publicly released at https://cyclereward.github.io/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyojin Bahng",
      "Caroline Chan",
      "Fredo Durand",
      "Phillip Isola"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Bao_Vid-Group_Temporal_Video_Grounding_Pretraining_from_Unlabeled_Videos_in_the_ICCV_2025_paper.html": {
    "title": "Vid-Group: Temporal Video Grounding Pretraining from Unlabeled Videos in the Wild",
    "volume": "main",
    "abstract": "Given a natural language query, temporal video grounding aims to localize the described temporal moment in an untrimmed video. A major challenge of this task is its heavy dependence on labor-intensive annotations for training. Unlike existing works that directly train models on manually curated data, we propose a novel paradigm to reduce annotation costs: pretraining the model on unlabeled, real-world videos. To support this, we introduce Temporal Video Grounding Pretraining (Vid-Group), a large-scale dataset collected in a scalable manner with minimal human intervention, consisting of over 50K videos captured in the wild and 200K pseudo annotations. Direct pretraining on these imperfect pseudo annotations, however, presents significant challenges, including mismatched sentence-video pairs and imprecise temporal boundaries. To address these issues, we propose the ReCorrect algorithm, which comprises two main phases: semantics-guided refinement and memory-consensus correction. The semantics-guided refinement enhances the pseudo labels by leveraging semantic similarity with video frames to clean out unpaired data and make initial adjustments to temporal boundaries. In the following memory-consensus correction phase, a memory bank tracks the model predictions, progressively correcting the temporal boundaries based on consensus within the memory. Comprehensive experiments demonstrate ReCorrect's strong generalization abilities across multiple downstream settings. The code, dataset, and pretrained models are available at https://github.com/baopj/Vid-Group",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peijun Bao",
      "Chenqi Kong",
      "Siyuan Yang",
      "Zihao Shao",
      "Xinghao Jiang",
      "Boon Poh Ng",
      "Meng Hwa Er",
      "Alex Kot"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xu_BANet_Bilateral_Aggregation_Network_for_Mobile_Stereo_Matching_ICCV_2025_paper.html": {
    "title": "BANet: Bilateral Aggregation Network for Mobile Stereo Matching",
    "volume": "main",
    "abstract": "State-of-the-art stereo matching methods typically use costly 3D convolutions to aggregate a full cost volume, but their computational demands make mobile deployment challenging. Directly applying 2D convolutions for cost aggregation often results in edge blurring, detail loss, and mismatches in textureless regions. Some complex operations, like deformable convolutions and iterative warping, can partially alleviate this issue; however, they are not mobile-friendly, limiting their deployment on mobile devices. In this paper, we present a novel bilateral aggregation network (BANet) for mobile stereo matching that produces high-quality results with sharp edges and fine details using only 2D convolutions. Specifically, we first separate the full cost volume into detailed and smooth volumes using a spatial attention map, then perform detailed and smooth aggregations accordingly, ultimately fusing both to obtain the final disparity map. Experimental results demonstrate that our BANet-2D significantly outperforms other mobile-friendly methods, achieving 35.3% higher accuracy on the KITTI 2015 leaderboard than MobileStereoNet-2D, with faster runtime on mobile devices",
    "checked": true,
    "id": "5ea65ec07804030490bb0fe096f8b22d57cb6b6b",
    "semantic_title": "banet: bilateral aggregation network for mobile stereo matching",
    "citation_count": 3,
    "authors": [
      "Gangwei Xu",
      "Jiaxin Liu",
      "Xianqi Wang",
      "Junda Cheng",
      "Yong Deng",
      "Jinliang Zang",
      "Yurui Chen",
      "Xin Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jin_Diffuman4D_4D_Consistent_Human_View_Synthesis_from_Sparse-View_Videos_with_ICCV_2025_paper.html": {
    "title": "Diffuman4D: 4D Consistent Human View Synthesis from Sparse-View Videos with Spatio-Temporal Diffusion Models",
    "volume": "main",
    "abstract": "This paper addresses the challenge of high-fidelity view synthesis of humans with sparse-view videos as input. Previous methods solve the issue of insufficient observation by leveraging 4D diffusion models to generate videos at novel viewpoints. However, the generated videos from these models often lack spatio-temporal consistency, thus degrading view synthesis quality. In this paper, we propose a novel sliding iterative denoising process to enhance the spatio-temporal consistency of the 4D diffusion model. Specifically, we define a latent grid in which each latent encodes the image, camera pose, and human pose for a certain viewpoint and timestamp, then alternately denoise the latent grid along spatial and temporal dimensions with a sliding window, and finally decode the videos at target viewpoints from the corresponding denoised latents. Through the iterative sliding, information flows sufficiently across the latent grid, allowing the diffusion model to obtain a large receptive field and thus enhance the 4D consistency of the output, while making the GPU memory consumption affordable. The experiments on the DNA-Rendering and ActorsHQ datasets demonstrate that our method is able to synthesize high-quality and consistent novel-view videos and significantly outperforms the existing approaches. See our project page for interactive demos and video results: https://diffuman4d.github.io/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yudong Jin",
      "Sida Peng",
      "Xuan Wang",
      "Tao Xie",
      "Zhen Xu",
      "Yifan Yang",
      "Yujun Shen",
      "Hujun Bao",
      "Xiaowei Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sun_AnnofreeOD_Detecting_All_Classes_at_Low_Frame_Rates_Without_Human_ICCV_2025_paper.html": {
    "title": "AnnofreeOD: Detecting All Classes at Low Frame Rates Without Human Annotations",
    "volume": "main",
    "abstract": "Manual annotation of 3D bounding boxes in large-scale 3D scenes is expensive and time-consuming. This motivates the exploration of annotation-free 3D object detection using unlabeled point cloud data. Existing unsupervised 3D detection frameworks predominantly identify moving objects via scene flow, which has significant limitations: (1) limited detection classes (<=3), (2) difficulty in detecting stationary objects, and (3) reliance on high frame rates. To address these limitations, we propose AnnofreeOD, a novel Annotation-free Object Detection framework based on 2D-to-3D knowledge distillation. First, we explore an effective strategy to generate high-quality pseudo boxes using single-frame 2D knowledge. Second, we observe the noise from the previous step and introduce Noise-Resistant Regression (NRR) based on Box Augmentation (BA). AnnofreeOD achieves state-of-the-art performance across multiple experiments. On the nuScenes dataset, we established the first annotation-free 10-class object detection baseline, achieving 40% of fully supervised performance. Furthermore, in 3-class and class-agnostic object detection tasks, our approach surpasses prior state-of-the-art methods by +9.3% mAP (+12.2% NDS) and +6.0% AP (+4.2% NDS), significantly improving precision",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boyi Sun",
      "Yuhang Liu",
      "Houxin He",
      "Yonglin Tian",
      "Fei-Yue Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhong_AIM_Adaptive_Inference_of_Multi-Modal_LLMs_via_Token_Merging_and_ICCV_2025_paper.html": {
    "title": "AIM: Adaptive Inference of Multi-Modal LLMs via Token Merging and Pruning",
    "volume": "main",
    "abstract": "Large language models (LLMs) have enabled the creation of multi-modal LLMs that exhibit strong comprehension of visual data such as images and videos. However, these models usually rely on extensive visual tokens from visual encoders, leading to high computational demands, which limits their applicability in resource-constrained environments and for long-context tasks. In this work, we propose a training-free adaptive inference method for multi-modal LLMs that can accommodate a broad range of efficiency requirements with a minimum performance drop. Our method consists of a) iterative token merging based on embedding similarity before LLMs, and b) progressive token pruning within LLM layers based on multi-modal importance. With a minimalist design, our method can be applied to both video and image LLMs. Extensive experiments on diverse video and image benchmarks demonstrate that our method substantially reduces computation load (e.g., a 7-fold reduction in FLOPs) while preserving the performance of video and image LLMs. Further, at a similar computational cost, our method outperforms the state-of-the-art methods in long video understanding (e.g., +4.6 on MLVU). Additionally, our in-depth analysis provides insights into token redundancy and LLM layer behaviors, offering guidance for future research in designing efficient multi-modal LLMs. Our code is available at https://github.com/LaVi-Lab/AIM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiwu Zhong",
      "Zhuoming Liu",
      "Yin Li",
      "Liwei Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zust_PanSt3R_Multi-view_Consistent_Panoptic_Segmentation_ICCV_2025_paper.html": {
    "title": "PanSt3R: Multi-view Consistent Panoptic Segmentation",
    "volume": "main",
    "abstract": "Panoptic segmentation in 3D is a fundamental problem in scene understanding. Existing approaches typically rely on costly test-time optimizations (often based on NeRF) to consolidate 2D predictions of off-the-shelf panoptic segmentation methods into 3D. Instead, in this work, we propose a unified and integrated approach PanSt3R, which eliminates the need for test-time optimization by jointly predicting 3D geometry and multi-view-consistent panoptic segmentation in a single forward pass. Our approach harnesses the 3D representations of MUSt3R, a recent scalable multi-view version of DUSt3R, and 2D representations of DINOv2, then performs joint multi-view panoptic prediction via a mask transformer architecture. We additionally revisit the standard post-processing mask merging procedure and introduce a more principled approach for multi-view segmentation. We also introduce a simple method for generating novel-view predictions based on the predictions of PanSt3R and vanilla 3DGS. Overall, the proposed PanSt3R is conceptually simple yet fast and scalable, and achieves state-of-the-art performance on several benchmarks, while being orders of magnitude faster",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lojze Zust",
      "Yohann Cabon",
      "Juliette Marrie",
      "Leonid Antsfeld",
      "Boris Chidlovskii",
      "Jerome Revaud",
      "Gabriela Csurka"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tao_G2SF_Geometry-Guided_Score_Fusion_for_Multimodal_Industrial_Anomaly_Detection_ICCV_2025_paper.html": {
    "title": "G2SF: Geometry-Guided Score Fusion for Multimodal Industrial Anomaly Detection",
    "volume": "main",
    "abstract": "Industrial quality inspection plays a critical role in modern manufacturing by identifying defective products during production. While single-modality approaches using either 3D point clouds or 2D RGB images suffer from information incompleteness, multimodal anomaly detection offers promise through the complementary fusion of crossmodal data. However, existing methods face challenges in effectively integrating unimodal results and improving discriminative power. To address these limitations, we first reinterpret memory bank-based anomaly scores in single modalities as isotropic Euclidean distances in local feature spaces. Dynamically evolving from Euclidean metrics, we propose a novel \\underline G eometry-\\underline G uided \\underline S core \\underline F usion (G^ 2 SF) framework that progressively learns an anisotropic local distance metric as a unified score for the fusion task. Through a geometric encoding operator, a novel Local Scale Prediction Network (LSPN) is proposed to predict direction-aware scaling factors that characterize first-order local feature distributions, thereby enhancing discrimination between normal and anomalous patterns. Additionally, we develop specialized loss functions and score aggregation strategy from geometric priors to ensure both metric generalization and efficacy. Comprehensive evaluations on the MVTec-3D AD and Eyecandies datasets demonstrate the state-of-the-art detection performance of our method, and detailed ablation analysis validates each component's contribution. Our code is available at https://github.com/ctaoaa/G2SF",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengyu Tao",
      "Xuanming Cao",
      "Juan Du"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_SA-Occ_Satellite-Assisted_3D_Occupancy_Prediction_in_Real_World_ICCV_2025_paper.html": {
    "title": "SA-Occ: Satellite-Assisted 3D Occupancy Prediction in Real World",
    "volume": "main",
    "abstract": "Existing vision-based 3D occupancy prediction methods are inherently limited in accuracy due to their exclusive reliance on street-view imagery, neglecting the potential benefits of incorporating satellite views. We propose SA-Occ, the first Satellite-Assisted 3D occupancy prediction model, which leverages GPS & IMU to integrate historical yet readily available satellite imagery into real-time applications, effectively mitigating limitations of ego-vehicle perceptions, involving occlusions and degraded performance in distant regions. To address the core challenges of cross-view perception, we propose: 1) Dynamic-Decoupling Fusion, which resolves inconsistencies in dynamic regions caused by the temporal asynchrony between satellite and street views; 2) 3D-Proj Guidance, a module that enhances 3D feature extraction from inherently 2D satellite imagery; and 3) Uniform Sampling Alignment, which aligns the sampling density between street and satellite views. Evaluated on Occ3D-nuScenes, SA-Occ achieves state-of-the-art performance, especially among single-frame methods, with a 39.05% mIoU (a 6.97% improvement), while incurring only 6.93 ms of additional latency per frame",
    "checked": true,
    "id": "c018b4ebda63add3fbdfe8fcd7b00b264a4db330",
    "semantic_title": "sa-occ: satellite-assisted 3d occupancy prediction in real world",
    "citation_count": 1,
    "authors": [
      "Chen Chen",
      "Zhirui Wang",
      "Taowei Sheng",
      "Yi Jiang",
      "Yundu Li",
      "Peirui Cheng",
      "Luning Zhang",
      "Kaiqiang Chen",
      "Yanfeng Hu",
      "Xue Yang",
      "Xian Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hwang_Generic_Event_Boundary_Detection_via_Denoising_Diffusion_ICCV_2025_paper.html": {
    "title": "Generic Event Boundary Detection via Denoising Diffusion",
    "volume": "main",
    "abstract": "Generic event boundary detection (GEBD) aims to identify natural boundaries in a video, segmenting it into distinct and meaningful chunks. Despite the inherent subjectivity of event boundaries, previous methods have focused on deterministic predictions, overlooking the diversity of plausible solutions. In this paper, we introduce a novel diffusion-based boundary detection model, dubbed DiffGEBD, that tackles the problem of GEBD from a generative perspective. The proposed model encodes relevant changes across adjacent frames via temporal self-similarity and then iteratively decodes random noise into plausible event boundaries being conditioned on the encoded features. Classifier-free guidance allows the degree of diversity to be controlled in denoising diffusion. In addition, we introduce a new evaluation metric to assess the quality of predictions considering both diversity and fidelity. Experiments show that our method achieves strong performance on two standard benchmarks, Kinetics-GEBD and TAPOS, generating diverse and plausible event boundaries",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaejun Hwang",
      "Dayoung Gong",
      "Manjin Kim",
      "Minsu Cho"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Guo_ImageGem_In-the-wild_Generative_Image_Interaction_Dataset_for_Generative_Model_Personalization_ICCV_2025_paper.html": {
    "title": "ImageGem: In-the-wild Generative Image Interaction Dataset for Generative Model Personalization",
    "volume": "main",
    "abstract": "We introduce ImageGem, a dataset for studying generative models that understand fine-grained individual preferences. We posit that a key challenge hindering the development of such a generative model is the lack of in-the-wild and fine-grained user preference annotations. Our dataset features real-world interaction data from 57K users, who collectively have built 242K customized LoRAs, written 3M text prompts, and created 5M generated images. With user preference annotations from our dataset, we were able to train better preference alignment models. In addition, leveraging individual user preference, we investigated the performance of retrieval models and a vision-language model on personalized image retrieval and generative model recommendation. Finally, we propose an end-to-end framework for editing customized diffusion models in a latent weight space to align with individual user preferences. Our results demonstrate that the ImageGem dataset enables, for the first time, a new paradigm for generative model personalization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanhe Guo",
      "Linxi Xie",
      "Zhuoran Chen",
      "Kangrui Yu",
      "Ryan Po",
      "Guandao Yang",
      "Gordon Wetzstein",
      "Hongyi Wen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zha_Implicit_Counterfactual_Learning_for_Audio-Visual_Segmentation_ICCV_2025_paper.html": {
    "title": "Implicit Counterfactual Learning for Audio-Visual Segmentation",
    "volume": "main",
    "abstract": "Audio-visual segmentation (AVS) aims to segment objects in videos based on audio cues. Existing AVS methods are primarily designed to enhance interaction efficiency but pay limited attention to modality representation discrepancies and imbalances. To overcome this, we propose the implicit counterfactual framework (ICF) to achieve unbiased cross-modal understanding. Due to the lack of semantics, heterogeneous representations may lead to erroneous matches, especially in complex scenes with ambiguous visual content or interference from multiple audio sources. We introduce the multi-granularity implicit text (MIT) involving video-, segment- and frame-level as the bridge to establish the modality-shared space, reducing modality gaps and providing prior guidance. Visual content carries more information and typically dominates, thereby marginalizing audio features in the decision-making. To mitigate knowledge preference, we propose the semantic counterfactual (SC) to learn orthogonal representations in the latent space, generating diverse counterfactual samples, thus avoiding biases introduced by complex functional designs and explicit modifications of text structures or attributes. We further formulate the collaborative distribution-aware contrastive learning (CDCL), incorporating factual-counterfactual and inter-modality contrasts to align representations, promoting cohesion and decoupling. Extensive experiments on three public datasets validate that the proposed method achieves state-of-the-art performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingfeng Zha",
      "Tianyu Li",
      "Guoqing Wang",
      "Peng Wang",
      "Yangyang Wu",
      "Yang Yang",
      "Heng Tao Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tong_Context_Guided_Transformer_Entropy_Modeling_for_Video_Compression_ICCV_2025_paper.html": {
    "title": "Context Guided Transformer Entropy Modeling for Video Compression",
    "volume": "main",
    "abstract": "Conditional entropy models effectively leverage spatio-temporal contexts to reduce video redundancy. However, incorporating temporal context often introduces additional model complexity and increases computational cost. In parallel, many existing spatial context models lack explicit modeling the ordering of spatial dependencies, which may limit the availability of relevant context during decoding. To address these issues, we propose the Context Guided Transformer (CGT) entropy model, which estimates probability mass functions of the current frame conditioned on resampled temporal context and dependency-weighted spatial context. A temporal context resampler learns predefined latent queries to extract critical temporal information using transformer encoders, reducing downstream computational overhead. Meanwhile, a teacher-student network is designed as dependency-weighted spatial context assigner to explicitly model the dependency of spatial context order. The teacher generates an attention map to represent token importance and an entropy map to reflect prediction certainty from randomly masked inputs, guiding the student to select the weighted top-k tokens with the highest spatial dependency. During inference, only the student is used to predict undecoded tokens based on high-dependency context. Experimental results demonstrate that our CGT model reduces entropy modeling time by approximately 65% and achieves an 11% BD-Rate reduction compared to the previous state-of-the-art conditional entropy model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junlong Tong",
      "Wei Zhang",
      "Yaohui Jin",
      "Xiaoyu Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kulikov_FlowEdit_Inversion-Free_Text-Based_Editing_Using_Pre-Trained_Flow_Models_ICCV_2025_paper.html": {
    "title": "FlowEdit: Inversion-Free Text-Based Editing Using Pre-Trained Flow Models",
    "volume": "main",
    "abstract": "Editing real images using a pre-trained text-to-image (T2I) diffusion/flow model often involves inverting the image into its corresponding noise map. However, inversion by itself is typically insufficient for obtaining satisfactory results, and therefore many methods additionally intervene in the sampling process. Such methods achieve improved results but are not seamlessly transferable between model architectures. Here, we introduce FlowEdit, a text-based editing method for pre-trained T2I flow models, which is inversion-free, optimization-free and model agnostic. Our method constructs an ODE that directly maps between the source and target distributions (corresponding to the source and target text prompts) and achieves a lower transport cost than the inversion approach. This leads to state-of-the-art results, as we illustrate with Stable Diffusion 3 and FLUX",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vladimir Kulikov",
      "Matan Kleiner",
      "Inbar Huberman-Spiegelglas",
      "Tomer Michaeli"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_CMT_A_Cascade_MAR_with_Topology_Predictor_for_Multimodal_Conditional_ICCV_2025_paper.html": {
    "title": "CMT: A Cascade MAR with Topology Predictor for Multimodal Conditional CAD Generation",
    "volume": "main",
    "abstract": "While accurate and user-friendly Computer-Aided Design (CAD) is crucial for industrial design and manufacturing, existing methods still struggle to achieve this due to their over-simplified representations or architectures incapable of supporting multimodal design requirements. In this paper, we attempt to tackle this problem from both methods and datasets aspects. First, we propose a cascade MAR with topology predictor (CMT), the first multimodal framework for CAD generation based on Boundary Representation (B-Rep). Specifically, the cascade MAR can effectively capture the \"edge-counters-surface\" priors that are essential in B-Reps, while the topology predictor directly estimates topology in B-Reps from the compact tokens in MAR. Second, to facilitate large-scale training, we develop a large-scale multimodal CAD dataset, mmABC, which includes over 1.3 million B-Rep models with multimodal annotations, including point clouds, text descriptions, and multi-view images. Extensive experiments show the superior of CMT in both conditional and unconditional CAD generation tasks. For example, we improve Coverage and Valid ratio by +10.68% and +10.3%, respectively, compared to state-of-the-art methods on ABC in unconditional generation. CMT also improves +4.01 Chamfer on image conditioned CAD generation on mmABC",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianyu Wu",
      "Yizhou Wang",
      "Xiangyu Yue",
      "Xinzhu Ma",
      "Jinyang Guo",
      "Dongzhan Zhou",
      "Wanli Ouyang",
      "Shixiang Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_CLIPSym_Delving_into_Symmetry_Detection_with_CLIP_ICCV_2025_paper.html": {
    "title": "CLIPSym: Delving into Symmetry Detection with CLIP",
    "volume": "main",
    "abstract": "Symmetry is one of the most fundamental geometric cues in computer vision, and detecting it has been an ongoing challenge. With the recent advances in vision-language models, i.e., CLIP, we investigate whether a pre-trained CLIP model can aid symmetry detection by leveraging the additional symmetry cues found in the natural image descriptions. We propose CLIPSym, which leverages CLIP's image and language encoders and a rotation-equivariant decoder based on a hybrid of Transformer and G-Convolution to detect rotation and reflection symmetries. To fully utilize CLIP's language encoder, we have developed a novel prompting technique called Semantic-Aware Prompt Grouping (SAPG), which aggregates a diverse set of frequent object-based prompts to better integrate the semantic cues for symmetry detection. Empirically, we show that CLIPSym outperforms the current state-of-the-art on three standard symmetry detection datasets (DENDI, SDRW, and LDRS). Finally, we conduct detailed ablations verifying the benefits of CLIP's pre-training, the proposed equivariant decoder, and the SAPG technique",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tinghan Yang",
      "Md Ashiqur Rahman",
      "Raymond A. Yeh"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_MRGen_Segmentation_Data_Engine_For_Underrepresented_MRI_Modalities_ICCV_2025_paper.html": {
    "title": "MRGen: Segmentation Data Engine For Underrepresented MRI Modalities",
    "volume": "main",
    "abstract": "Training medical image segmentation models for rare yet clinically important imaging modalities is challenging due to the scarcity of annotated data, and manual mask annotations can be costly and labor-intensive to acquire. This paper investigates leveraging generative models to synthesize data, for training segmentation models for underrepresented modalities, particularly on annotation-scarce MRI. Concretely, our contributions are threefold: (i) we introduce MRGen-DB, a large-scale radiology image-text dataset comprising extensive samples with rich metadata, including modality labels, attributes, regions, and organs information, with a subset featuring pixel-wise mask annotations; (ii) we present MRGen, a diffusion-based data engine for controllable medical image synthesis, conditioned on text prompts and segmentation masks. MRGen can generate realistic images for diverse MRI modalities lacking mask annotations, facilitating segmentation training in low-source domains; (iii) extensive experiments across multiple modalities demonstrate that MRGen significantly improves segmentation performance on unannotated modalities by providing high-quality synthetic data. We believe that our method bridges a critical gap in medical image analysis, extending segmentation capabilities to scenarios that are challenging to acquire manual annotations. The codes, models, and data will be publicly available at: https://haoningwu3639.github.io/MRGen/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoning Wu",
      "Ziheng Zhao",
      "Ya Zhang",
      "Yanfeng Wang",
      "Weidi Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jia_PrimHOI_Compositional_Human-Object_Interaction_via_Reusable_Primitives_ICCV_2025_paper.html": {
    "title": "PrimHOI: Compositional Human-Object Interaction via Reusable Primitives",
    "volume": "main",
    "abstract": "Synthesizing realistic Human-Object Interaction (HOI) motions is essential for creating believable digital characters and intelligent robots. Existing approaches rely on data-intensive learning models that struggle with the compositional structure of daily HOI motions, particularly for complex multi-object manipulation tasks. The exponential growth of possible interaction scenarios makes comprehensive data collection prohibitively expensive. The fundamental challenge is synthesizing unseen, complex HOI sequences without extensive task-specific training data. Here we show that PrimHOI generates complex HOI motions through spatial and temporal composition of generalizable interaction primitives defined by relative geometry. Our approach demonstrates that repetitive local contact patterns -- grasping, clamping, and supporting -- serve as reusable building blocks for diverse interaction sequences. Unlike previous data-driven methods requiring end-to-end training for each task variant, PrimHOI achieves zero-shot transfer to unseen scenarios through hierarchical primitive planning. Experimental validation demonstrates substantial improvements in adaptability, diversity, and motion quality compared to existing approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Jia",
      "Tengyu Liu",
      "Mingtao Pei",
      "Yixin Zhu",
      "Siyuan Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Nautilus_Locality-aware_Autoencoder_for_Scalable_Mesh_Generation_ICCV_2025_paper.html": {
    "title": "Nautilus: Locality-aware Autoencoder for Scalable Mesh Generation",
    "volume": "main",
    "abstract": "Triangle meshes are fundamental to 3D applications. Current automatic mesh generation methods typically rely on intermediate representations that lack the continuous surface quality inherent to meshes. Converting these representations into meshes produces dense, suboptimal outputs. Although recent autoregressive approaches demonstrate promise in directly modeling mesh vertices and faces, they are constrained by the limitation in face count, scalability, and structural fidelity.To address these challenges, we propose Nautilus, a locality-aware autoencoder for artist-like mesh generation that leverages the local properties of manifold meshes to achieve structural fidelity and efficient representation. Our approach introduces a novel tokenization algorithm that preserves face proximity relationships and compresses sequence length through locally shared vertices and edges, enabling the generation of meshes with an unprecedented scale of up to 5,000 faces. Furthermore, we develop a Dual-stream Point Conditioner that captures fine-grained geometry, ensuring global consistency and local structural fidelity. Our experiments demonstrate that Nautilus significantly outperforms existing methods in generation quality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Wang",
      "Xuanyu Yi",
      "Haohan Weng",
      "Qingshan Xu",
      "Xiaokang Wei",
      "Xianghui Yang",
      "Chunchao Guo",
      "Long Chen",
      "Hanwang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kung_What_Changed_and_What_Could_Have_Changed_State-Change_Counterfactuals_for_ICCV_2025_paper.html": {
    "title": "What Changed and What Could Have Changed? State-Change Counterfactuals for Procedure-Aware Video Representation Learning",
    "volume": "main",
    "abstract": "Understanding a procedural activity requires modeling both how action steps transform the scene, and how evolving scene transformations can influence the sequence of action steps, even those that are accidental or erroneous. Existing work has studied procedure-aware video representations by modeling the temporal order of actions, but has not explicitly learned the state changes (scene transformations). In this work, we study procedure-aware video representation learning by incorporating state-change descriptions generated by Large Language Models (LLMs) as supervision signals for video encoders. Moreover, we generate state-change counterfactuals that simulate hypothesized failure outcomes, allowing models to learn by imagining unseen \"What if\" scenarios. This counterfactual reasoning facilitates the model's ability to understand the cause and effect of each step in an activity. We conduct extensive experiments on procedure-aware tasks, including temporal action segmentation, error detection, action phase classification, frame retrieval, multi-instance retrieval, and action recognition. Our results demonstrate the effectiveness of the proposed state-change descriptions and their counterfactuals, and achieve significant improvements on multiple tasks. Code is available at https://github.com/HCIS-Lab/counterfactual-video-pretrain",
    "checked": true,
    "id": "bbc8d785637a78c776a9738e80ed3346b78d3702",
    "semantic_title": "what changed and what could have changed? state-change counterfactuals for procedure-aware video representation learning",
    "citation_count": 2,
    "authors": [
      "Chi-Hsi Kung",
      "Frangil Ramirez",
      "Juhyung Ha",
      "Yi-Ting Chen",
      "David Crandall",
      "Yi-Hsuan Tsai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xu_Accelerate_3D_Object_Detection_Models_via_Zero-Shot_Attention_Key_Pruning_ICCV_2025_paper.html": {
    "title": "Accelerate 3D Object Detection Models via Zero-Shot Attention Key Pruning",
    "volume": "main",
    "abstract": "Query-based methods with dense features have demonstrated remarkable success in 3D object detection tasks. However, the computational demands of these models, particularly with large image sizes and multiple transformer layers, pose significant challenges for efficient running on edge devices. Existing pruning and distillation methods either need retraining or are designed for ViT models, which are hard to migrate to 3D detectors. To address this issue, we propose a zero-shot runtime pruning method for transformer decoders in 3D object detection models. The method, termed tgGBC (trim keys gradually Guided By Classification scores), systematically trims keys in transformer modules based on their importance. We expand the classification score to multiply it with the attention map to get the importance score of each key and then prune certain keys after each transformer layer according to their importance scores. Our method achieves a 1.99x speedup in the transformer decoder of the latest ToC3D model, with only a minimal performance loss of less than 1%. Interestingly, for certain models, our method even enhances their performance. Moreover, we deploy 3D detectors with tgGBC on an edge device, further validating the effectiveness of our method. The code can be found at https://github.com/iseri27/tg_gbc",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lizhen Xu",
      "Xiuxiu Bai",
      "Xiaojun Jia",
      "Jianwu Fang",
      "Shanmin Pang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lei_Adaptive_Learning_of_High-Value_Regions_for_Semi-Supervised_Medical_Image_Segmentation_ICCV_2025_paper.html": {
    "title": "Adaptive Learning of High-Value Regions for Semi-Supervised Medical Image Segmentation",
    "volume": "main",
    "abstract": "Existing semi-supervised learning methods typically mitigate the impact of unreliable predictions by suppressing low-confidence regions. However, these methods fail to explore which regions hold higher learning value and how to design adaptive learning strategies for these regions. To address these issues, we propose a novel adaptive learning of high-value regions (ALHVR) framework. By exploiting the diversity of predictions from dual-branch networks, the prediction regions are classified into three groups: reliable stable region, reliable unstable region, and unreliable stable region. For high-value regions (reliable unstable region and unreliable stable region), different training strategies are designed. Specifically, for reliable unstable region, we propose a confidence-guided cross-prototype consistency learning (CG-CPCL) module, which enforces prototype consistency constraints in the feature space. By leveraging confidence information, the high-confidence predictions from one network selectively supervise the low-confidence predictions from the other, thus helping the model learn inter-class discrimination more stably. Additionally, for unreliable stable region, we design a dynamic teacher competition teaching (DTCT) module, which dynamically selects the most reliable pixels as teachers by evaluating the unperturbed predictions from both networks. These selected pixels are then used to supervise perturbed predictions, thereby enhancing the model's learning capability in unreliable region. Experimental results show that our method outperforms state-of-the-art approaches on three public datasets. Code is available at https://github.com/ziziyao/ALHVR",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Lei",
      "Ziyao Yang",
      "Xingwu Wang",
      "Yi Wang",
      "Xuan Wang",
      "Feiman Sun",
      "Asoke K. Nandi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_X-Dancer_Expressive_Music_to_Human_Dance_Video_Generation_ICCV_2025_paper.html": {
    "title": "X-Dancer: Expressive Music to Human Dance Video Generation",
    "volume": "main",
    "abstract": "We present X-Dancer, a novel zero-shot music-driven image animation pipeline that creates diverse and long-range lifelike human dance videos from a single static image. As its core, we introduce a unified transformer-diffusion framework, featuring an autoregressive transformer model that synthesize extended and music-synchronized token sequences for 2D body, head and hands poses, which then guide a diffusion model to produce coherent and realistic dance video frames. Unlike traditional methods that primarily generate human motion in 3D, X-Dancer addresses data limitations and enhances scalability by modeling a wide spectrum of 2D dance motions, capturing their nuanced alignment with musical beats through readily available monocular videos. To achieve this, we first build a spatially compositional token representation from 2D human pose labels associated with keypoint confidences, encoding both large articulated body movements (e.g., upper and lower body) and fine-grained motions (e.g., head and hands). We then design a music-to-motion transformer model that autoregressively generates music-aligned dance pose token sequences, incorporating global attention to both musical style and prior motion context. Finally we leverage a diffusion backbone to animate the reference image with these synthesized pose tokens through AdaIN, forming a fully differentiable end-to-end framework. Experimental results demonstrate that X-Dancer is able to produce both diverse and characterized dance videos, substantially outperforming state-of-the-art methods in term of diversity, expressiveness and realism. See our project page for more results: https://zeyuan-chen.com/X-Dancer/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyuan Chen",
      "Hongyi Xu",
      "Guoxian Song",
      "You Xie",
      "Chenxu Zhang",
      "Xin Chen",
      "Chao Wang",
      "Di Chang",
      "Linjie Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_IDF_Iterative_Dynamic_Filtering_Networks_for_Generalizable_Image_Denoising_ICCV_2025_paper.html": {
    "title": "IDF: Iterative Dynamic Filtering Networks for Generalizable Image Denoising",
    "volume": "main",
    "abstract": "Image denoising is a fundamental challenge in computer vision, with applications in photography and medical imaging. While deep learning-based methods have shown remarkable success, their reliance on specific noise distributions limits generalization to unseen noise types and levels. Existing approaches attempt to address this with extensive training data and high computational resources but they still suffer from overfitting. To address these issues, we conduct image denoising by utilizing dynamically generated kernels via efficient operations. This approach helps prevent overfitting and improves resilience to unseen noise. Specifically, our method leverages a Feature Extraction Module for robust noise-invariant features, Global Statistics and Local Correlation Modules to capture comprehensive noise characteristics and structural correlations. The Kernel Prediction Module then employs these cues to produce pixel-wise varying kernels adapted to local structures, which are then applied iteratively for denoising. This ensures both efficiency and superior restoration quality. Despite being trained on single-level Gaussian noise, our compact model ( 0.04 M) excels across diverse noise types and levels, demonstrating the promise of iterative dynamic filtering for practical image denoising",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongjin Kim",
      "Jaekyun Ko",
      "Muhammad Kashif Ali",
      "Tae Hyun Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Erdogan_LayerLock_Non-collapsing_Representation_Learning_with_Progressive_Freezing_ICCV_2025_paper.html": {
    "title": "LayerLock: Non-collapsing Representation Learning with Progressive Freezing",
    "volume": "main",
    "abstract": "We introduce LayerLock, a simple yet effective approach for self-supervised visual representation learning, that gradually transitions throughout training from predicting shallow features to deeper ones through progressive layer freezing. First, we make the observation that during training of video masked-autoencoding (MAE) models, ViT layers converge in the order of their depth: shallower layers converge early, deeper layers converge late. We then show that this observation can be exploited to accelerate standard MAE by progressively freezing the model according to an explicit schedule, throughout training. Furthermore, this same schedule can be used in a simple and scalable approach to latent prediction that does not suffer from \"representation collapse\". We apply our proposed approach, LayerLock, to both pixel and latent prediction approaches with large models of up to 4B parameters and show improvements on both semantic (action classification) and low level (depth estimation) vision tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Goker Erdogan",
      "Nikhil Parthasarathy",
      "Catalin Ionescu",
      "Drew A. Hudson",
      "Alexander Lerchner",
      "Andrew Zisserman",
      "Mehdi S. M. Sajjadi",
      "Joao Carreira"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wei_Augmenting_Moment_Retrieval_Zero-Dependency_Two-Stage_Learning_ICCV_2025_paper.html": {
    "title": "Augmenting Moment Retrieval: Zero-Dependency Two-Stage Learning",
    "volume": "main",
    "abstract": "Existing Moment Retrieval methods face three critical bottlenecks: (1) data scarcity forces models into shallow keyword-feature associations; (2) boundary ambiguity in transition regions between adjacent events; (3) insufficient discrimination of fine-grained semantics (e.g., distinguishing \"kicking\" vs. \"throwing\" a ball). In this paper, we propose a zero-external-dependency Augmented Moment Retrieval framework, AMR, designed to overcome local optima caused by insufficient data annotations and the lack of robust boundary and semantic discrimination capabilities. AMR is built upon two key insights: (1) it resolves ambiguous boundary information and semantic confusion in existing annotations without additional data (avoiding costly manual labeling), and (2) it preserves boundary and semantic discriminative capabilities enhanced by training while generalizing to real-world scenarios, significantly improving performance. Furthermore, we propose a two-stage training framework with cold-start and distillation adaptation. The cold-start stage employs curriculum learning on augmented data to build foundational boundary/semantic awareness. The distillation stage introduces dual query sets: Original Queries maintain DETR-based localization using frozen Base Queries from the cold-start model, while Active Queries dynamically adapt to real-data distributions. A cross-stage distillation loss enforces consistency between Original and Base Queries, preventing knowledge forgetting while enabling real-world generalization. Experiments on multiple benchmarks show that AMR achieves improved performance over prior state-of-the-art approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengxuan Wei",
      "Jiajin Tang",
      "Sibei Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yan_Derm1M_A_Million-scale_Vision-Language_Dataset_Aligned_with_Clinical_Ontology_Knowledge_ICCV_2025_paper.html": {
    "title": "Derm1M: A Million-scale Vision-Language Dataset Aligned with Clinical Ontology Knowledge for Dermatology",
    "volume": "main",
    "abstract": "The emergence of vision-language models has transformed medical AI, enabling unprecedented advances in diagnostic capability and clinical applications. However, progress in dermatology has lagged behind other medical domains due to the lack of standard image-text pairs. Existing dermatological datasets are limited in both scale and depth, offering only single-label annotations across a narrow range of diseases instead of rich textual descriptions, and lacking the crucial clinical context needed for real-world applications. To address these limitations, we present Derm1M, the first large-scale vision-language dataset for dermatology, comprising 1,029,761 image-text pairs. Built from diverse educational resources and structured around a standard ontology collaboratively developed by experts, Derm1M provides comprehensive coverage for over 390 skin conditions across four hierarchical levels and 130 clinical concepts with rich contextual information such as medical history, symptoms, and skin tone. To demonstrate Derm1M's potential in advancing both AI research and clinical application, we pretrained a series of CLIP-like models, collectively called DermLIP, on this dataset. The DermLIP family significantly outperforms state-of-the-art foundation models on eight diverse datasets across multiple tasks, including zero-shot skin disease classification, clinical and artifacts concept identification, few-shot/full-shot learning, and cross-modal retrieval. Our dataset and code are available at https://github.com/SiyuanYan1/Derm1M",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyuan Yan",
      "Ming Hu",
      "Yiwen Jiang",
      "Xieji Li",
      "Hao Fei",
      "Philipp Tschandl",
      "Harald Kittler",
      "Zongyuan Ge"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xiang_Expressive_Talking_Human_from_Single-Image_with_Imperfect_Priors_ICCV_2025_paper.html": {
    "title": "Expressive Talking Human from Single-Image with Imperfect Priors",
    "volume": "main",
    "abstract": "Building realistic and animatable avatars still requires minutes of multi-view or monocular self-rotating videos, and most methods lack precise control over gestures and expressions. To push this boundary, we address the challenge of constructing a whole-body talking avatar from a single image. We propose a novel pipeline that tackles two critical issues: 1) complex dynamic modeling and 2) generalization to novel gestures and expressions. To achieve seamless generalization, we leverage recent pose-guided image-to-video diffusion models to generate imperfect video frames as pseudo-labels. To overcome the dynamic modeling challenge posed by inconsistent and noisy pseudo-frames, we introduce a tightly coupled 3DGS-mesh hybrid avatar representation and apply several key regularizations to mitigate inconsistencies caused by imperfect labels. Extensive experiments on diverse subjects demonstrate that our method enables the creation of a photorealistic, precisely animatable, and expressive whole-body talking avatar from just a single image",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Xiang",
      "Yudong Guo",
      "Leipeng Hu",
      "Boyang Guo",
      "Yancheng Yuan",
      "Juyong Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_DADM_Dual_Alignment_of_Domain_and_Modality_for_Face_Anti-spoofing_ICCV_2025_paper.html": {
    "title": "DADM: Dual Alignment of Domain and Modality for Face Anti-spoofing",
    "volume": "main",
    "abstract": "With the availability of diverse sensor modalities (i.e., RGB, Depth, Infrared) and the success of multi-modal learning, multi-modal face anti-spoofing (FAS) has emerged as a prominent research focus. The intuition behind it is that leveraging multiple modalities can uncover more intrinsic spoofing traces. However, this approach presents more risk of misalignment. We identify two main types of misalignment: (1) Intra-domain modality misalignment, where the importance of each modality varies across different attacks. For instance, certain modalities (e.g., Depth) may be non-defensive against specific attacks (e.g., 3D mask), indicating that each modality has unique strengths and weaknesses in countering particular attacks. Consequently, simple fusion strategies may fall short. (2) Inter-domain modality misalignment, where the introduction of additional modalities exacerbates domain shifts, potentially overshadowing the benefits of complementary fusion. To tackle (1), we propose an alignment module between modalities based on mutual information, which adaptively enhances favorable modalities while suppressing unfavorable ones. To address (2), we employ a dual alignment optimization method that aligns both sub-domain hyperplanes and modality angle margins, thereby mitigating domain gaps. Our method, dubbed Dual Alignment of Domain and Modality (DADM), achieves state-of-the-art performance in extensive experiments across four challenging protocols, demonstrating its robustness in multi-modal domain generalization scenarios. Our code is available at \\href https://github.com/yjyddq/DADM here",
    "checked": true,
    "id": "a52470bf3760f524f4ba12548ffbd8ec3fd7b8fc",
    "semantic_title": "dadm: dual alignment of domain and modality for face anti-spoofing",
    "citation_count": 0,
    "authors": [
      "Jingyi Yang",
      "Xun Lin",
      "Zitong Yu",
      "Liepiao Zhang",
      "Xin Liu",
      "Hui Li",
      "Xiaochen Yuan",
      "Xiaochun Cao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kumar_IntroStyle_Training-Free_Introspective_Style_Attribution_using_Diffusion_Features_ICCV_2025_paper.html": {
    "title": "IntroStyle: Training-Free Introspective Style Attribution using Diffusion Features",
    "volume": "main",
    "abstract": "Text-to-image (T2I) models have recently gained widespread adoption. This has spurred concerns about safeguarding intellectual property rights and an increasing demand for mechanisms that prevent the generation of specific artistic styles. Existing methods for style extraction typically necessitate the collection of custom datasets and the training of specialized models. This, however, is resource-intensive, time-consuming, and often impractical for real-time applications. We present a novel, training-free framework to solve the style attribution problem, using the features produced by a diffusion model alone, without any external modules or retraining. This is denoted as Introspective Style attribution(IntroStyle) and is shown to have superior performance to state-of-the-art models for style attribution. We also introduce a synthetic dataset of Artistic Style Split (ArtSplit) to isolate artistic style and evaluate fine-grained style attribution performance. Our experimental results on WikiArt and DomainNet datasets show that \\ours is robust to the dynamic nature of artistic styles, outperforming existing methods by a wide margin",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anand Kumar",
      "Jiteng Mu",
      "Nuno Vasconcelos"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Learning_Deblurring_Texture_Prior_from_Unpaired_Data_with_Diffusion_Model_ICCV_2025_paper.html": {
    "title": "Learning Deblurring Texture Prior from Unpaired Data with Diffusion Model",
    "volume": "main",
    "abstract": "Since acquiring large amounts of realistic blurry-sharp image pairs is difficult and expensive, learning blind image deblurring from unpaired data is a more practical and promising solution. Unfortunately, most existing approaches only use adversarial learning to bridge the gap from blurry domains to sharp domains, ignoring the complex and unpredictable nature of real-world blurry patterns. In this paper, we propose a novel diffusion model (DM)-based framework, dubbed TP-Diff, for image deblurring by learning spatially varying texture prior from unpaired sharp data. In particular, TP-Diff performs DM to generate the prior knowledge used to recover the texture of blurry images. To implement it, we propose a Texture Prior Encoder (TPE) that introduces a memory mechanism to encode the texture prior and thereby provide supervision for the DM training. To fully exploit the generated texture priors, we further present the Texture Transfer Transformer layer (TTformer), in which a novel Filter-Modulated Multi-head Self-Attention (FM-MSA) efficiently removes spatially varying blurring through adaptive filtering. In addition, a wavelet-based adversarial loss is used to preserve high-frequency texture details. Extensive evaluations demonstrate that TP-Diff provides a promising unsupervised deblurring solution and outperforms SOTA methods in six widely-used benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengxu Liu",
      "Lu Qi",
      "Jinshan Pan",
      "Xueming Qian",
      "Ming-Hsuan Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Oasis_One_Image_is_All_You_Need_for_Multimodal_Instruction_ICCV_2025_paper.html": {
    "title": "Oasis: One Image is All You Need for Multimodal Instruction Data Synthesis",
    "volume": "main",
    "abstract": "The success of multi-modal large language models (MLLMs) has been largely attributed to the large-scale training data. However, the training data of many MLLMs is unavailable due to privacy concerns. The expensive and labor-intensive process of collecting multi-modal data further exacerbates the problem. Is it possible to synthesize multi-modal training data automatically without compromising diversity and quality? In this paper, we propose a new method, Oasis, to synthesize high-quality multi-modal data with only images. Oasis breaks through traditional methods by prompting only images to the MLLMs, thus extending the data diversity by a large margin. Our method features a delicate quality control method which ensures the data quality. We collected over 500k data and conducted incremental experiments on LLaVA-NeXT. Extensive experiments demonstrate that our method can significantly improve the performance of MLLMs. The image-based synthesis also allows us to focus on the specific-domain ability of MLLMs. Code and data will be publicly available",
    "checked": true,
    "id": "f1ebeadf2cfac33d126589d74f39041198a00be2",
    "semantic_title": "oasis: one image is all you need for multimodal instruction data synthesis",
    "citation_count": 4,
    "authors": [
      "Letian Zhang",
      "Quan Cui",
      "Bingchen Zhao",
      "Cheng Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chang_Learning_Neural_Scene_Representation_from_iToF_Imaging_ICCV_2025_paper.html": {
    "title": "Learning Neural Scene Representation from iToF Imaging",
    "volume": "main",
    "abstract": "Indirect Time-of-Flight (iToF) cameras are popular for 3D perception because they are cost-effective and easy to deploy. They emit modulated infrared signals to illuminate the scene and process the received signals to generate amplitude and phase images. The depth is calculated from the phase using the modulation frequency. However, the obtained depth often suffers from noise caused by multi-path interference, low signal-to-noise ratio (SNR), and depth wrapping. Building on recent advancements in neural scene representations, which have shown great potential in 3D modeling from multi-view RGB images, we propose leveraging this approach to reconstruct 3D representations from noisy iToF data. Our method utilizes the multi-view consistency of amplitude and phase maps, fusing information from all input views to generate an accurate scene representation. Considering the impact of infrared illumination, we propose a new rendering scheme for amplitude maps based on signed distance function (SDF) and introduce a neural lighting function to model the appearance variations caused by active illumination. We also incorporate a phase-guided sampling strategy and a wrapping-aware phase-to-depth loss to utilize raw phase information and mitigate depth wrapping. Additionally, we add a noise-weight loss to prevent excessive smoothing information across noisy multi-view measurements. Experiments conducted on synthetic and real-world datasets demonstrate that the proposed method outperforms state-of-the-art techniques",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenjie Chang",
      "Hanzhi Chang",
      "Yueyi Zhang",
      "Wenfei Yang",
      "Tianzhu Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xu_GeometryCrafter_Consistent_Geometry_Estimation_for_Open-world_Videos_with_Diffusion_Priors_ICCV_2025_paper.html": {
    "title": "GeometryCrafter: Consistent Geometry Estimation for Open-world Videos with Diffusion Priors",
    "volume": "main",
    "abstract": "Despite remarkable advancements in video depth estimation, existing methods fall short in geometric fidelity due to their affine-invariant predictions, restricting their applicability in reconstruction and other metrically grounded downstream tasks. We propose a novel point map Variational Autoencoder (VAE) for encoding and decoding unbounded point maps. Notably, its latent space is agnostic to video latent distributions of video diffusion models, allowing us to leverage generation priors to model the distribution of point map sequences conditioned on the input videos. Thus, we can recover high-fidelity point map sequences with temporal coherence from open-world videos, facilitating accurate 3D/4D reconstruction, camera parameter estimation, and other depth-based applications. Extensive evaluations on diverse datasets demonstrate that our method achieves state-of-the-art 3D accuracy, temporal consistency, and generalization capability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tian-Xing Xu",
      "Xiangjun Gao",
      "Wenbo Hu",
      "Xiaoyu Li",
      "Song-Hai Zhang",
      "Ying Shan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ha_Multi-modal_Multi-platform_Person_Re-Identification_Benchmark_and_Method_ICCV_2025_paper.html": {
    "title": "Multi-modal Multi-platform Person Re-Identification: Benchmark and Method",
    "volume": "main",
    "abstract": "Conventional person re-identification (ReID) research is often limited to single-modality sensor data from static cameras, which fails to address the complexities of real-world scenarios where multi-modal signals are increasingly prevalent. For instance, consider an urban ReID system integrating stationary RGB cameras, nighttime infrared sensors, and UAVs equipped with dynamic tracking capabilities. Such systems face significant challenges due to variations in camera perspectives, lighting conditions, and sensor modalities, hindering effective person ReID.To address these challenges, we introduce the MP-ReID benchmark, a novel dataset designed specifically for multi-modality and multi-platform ReID. This benchmark uniquely compiles data from 1,930 identities across diverse modalities, including RGB, infrared, and thermal imaging, captured by both UAVs and ground-based cameras in indoor and outdoor environments.Building on this benchmark, we introduce Uni-Prompt ReID, a framework with specific-designed prompts, tailored for cross-modality and cross-platform scenarios. Our method consistently outperforms state-of-the-art approaches, establishing a robust foundation for future research in complex and dynamic ReID environments. Additionally, our dataset will be made publicly available to support further advancements",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruiyang Ha",
      "Songyi Jiang",
      "Bin Li",
      "Bikang Pan",
      "Yihang Zhu",
      "Junjie Zhang",
      "Xiatian Zhu",
      "Shaogang Gong",
      "Jingya Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Aghazadeh_CAP_Evaluation_of_Persuasive_and_Creative_Image_Generation_ICCV_2025_paper.html": {
    "title": "CAP: Evaluation of Persuasive and Creative Image Generation",
    "volume": "main",
    "abstract": "We address the task of advertisement image generation and introduce three evaluation metrics to assess Creativity, prompt Alignment, and Persuasiveness (CAP) in generated advertisement images. Despite recent advancements in Text-to-Image (T2I) methods and their performance in generating high-quality images for explicit descriptions, evaluating these models remains challenging. Existing evaluation methods focus largely on assessing alignment with explicit, detailed descriptions, but evaluating alignment with visually implicit prompts remains an open problem. Additionally, creativity and persuasiveness are essential qualities that enhance the effectiveness of advertisement images, yet are seldom measured. To address this, we propose three novel metrics for evaluating the creativity, alignment, and persuasiveness of generated images. We show that current T2I models struggle with creativity, persuasiveness, and alignment when the input text is implicit messages. We further introduce a simple yet effective approach to enhance T2I models' capabilities in producing images that are better aligned, more creative, and more persuasive",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aysan Aghazadeh",
      "Adriana Kovashka"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xiao_SpatialTrackerV2_Advancing_3D_Point_Tracking_with_Explicit_Camera_Motion_ICCV_2025_paper.html": {
    "title": "SpatialTrackerV2: Advancing 3D Point Tracking with Explicit Camera Motion",
    "volume": "main",
    "abstract": "We present SpatialTrackerV2, a feed-forward 3D point tracking method for monocular videos. Going beyond modular pipelines built on off-the-shelf components for 3D tracking, our approach unifies the intrinsic connections between point tracking, monocular depth, and camera pose estimation into a high-performing and feedforward 3D point tracker. It decomposes world-space 3D motion into scene geometry, camera ego-motion, and pixel-wise object motion, with a fully differentiable and end-to-end architecture, allowing scalable training across a wide range of datasets, including synthetic sequences, posed RGB-D videos, and unlabeled in-the-wild footage. By learning geometry and motion jointly from such heterogeneous data, SpatialTrackerV2 outperforms existing 3D tracking methods by 30%, and matches the accuracy of leading dynamic 3D reconstruction approaches while running 50x faster",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxi Xiao",
      "Jianyuan Wang",
      "Nan Xue",
      "Nikita Karaev",
      "Yuri Makarov",
      "Bingyi Kang",
      "Xing Zhu",
      "Hujun Bao",
      "Yujun Shen",
      "Xiaowei Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lu_monoVLN_Bridging_the_Observation_Gap_between_Monocular_and_Panoramic_Vision_ICCV_2025_paper.html": {
    "title": "monoVLN: Bridging the Observation Gap between Monocular and Panoramic Vision and Language Navigation",
    "volume": "main",
    "abstract": "Vision and Language Navigation(VLN) requires agents to navigate 3D environments by following natural language instructions. While existing methods predominantly assume access to panoramic observations, many practical robotics are equipped with monocular RGBD cameras, creating a significant configuration disparity. In this work, we address this critical gap by developing a novel 3DGS-based framework for monocular VLN agents, focusing on the intrinsic information incompleteness challenge. Our approach incorporates two key innovations: (1) implicit partial completion module for inferring representations of missing regions in incompletely rendered panoramic feature maps, and (2) an uncertainty-aware active perception strategy that enables the agent to actively acquire visual observation when uncertain about its decision. Extensive experiments on R2R-CE and RxR-CE datasets demonstrate that our monoVLN outperforms all existing monocular methods, significantly improve 8% success rate on R2R-CE compared to previous monocular methods. We also validate our monoVLN in real-world environments, providing a practical solution for real-world VLN",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Renjie Lu",
      "Yu Zhou",
      "Hao Cheng",
      "Jingke Meng",
      "Wei-Shi Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Park_Know_No_Better_A_Data-Driven_Approach_for_Enhancing_Negation_Awareness_ICCV_2025_paper.html": {
    "title": "Know \"No\" Better: A Data-Driven Approach for Enhancing Negation Awareness in CLIP",
    "volume": "main",
    "abstract": "While CLIP has significantly advanced multimodal understanding by bridging vision and language, the inability to grasp negation -- such as failing to differentiate concepts like \"parking\" from \"no parking\" -- poses substantial challenges.By analyzing the data used in the public CLIP model's pre-training, we posit this limitation stems from a lack of negation-inclusive data.To address this, we introduce data generation pipelines that employ a large language model (LLM) and a multimodal LLM to produce negation-inclusive captions.Fine-tuning CLIP with data generated from our pipelines, we develop NegationCLIP, which enhances negation awareness while preserving the generality.Moreover, to enable a comprehensive evaluation of negation understanding, we propose NegRefCOCOg--a benchmark tailored to test VLMs' ability to interpret negation across diverse expressions and positions within a sentence.Experiments on various CLIP architectures validate the effectiveness of our data generation pipelines in enhancing CLIP's ability to perceive negation accurately.Additionally, NegationCLIP's enhanced negation awareness has practical applications across various multimodal tasks, demonstrated by performance gains in text-to-image generation and referring image segmentation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junsung Park",
      "Jungbeom Lee",
      "Jongyoon Song",
      "Sangwon Yu",
      "Dahuin Jung",
      "Sungroh Yoon"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Neurons_Emulating_the_Human_Visual_Cortex_Improves_Fidelity_and_Interpretability_ICCV_2025_paper.html": {
    "title": "Neurons: Emulating the Human Visual Cortex Improves Fidelity and Interpretability in fMRI-to-Video Reconstruction",
    "volume": "main",
    "abstract": "Decoding visual stimuli from neural activity is essential for understanding the human brain. While fMRI methods have successfully reconstructed static images, fMRI-tovideo reconstruction faces challenges due to the need for capturing spatiotemporal dynamics like motion and scene transitions. Recent approaches have improved semantic and perceptual alignment but struggle to integrate coarse fMRI data with detailed visual features. Inspired by the hierarchical organization of the visual system, we propose NEURONS, a novel framework that decouples learning into four correlated sub-tasks: key object segmentation, concept recognition, scene description, and blurry video reconstruction. This approach simulates the visual cortex's functional specialization, allowing the model to capture diverse video content. In the inference stage, NEURONS generates robust conditioning signals for a pre-trained textto-video diffusion model to reconstruct the videos. Extensive experiments demonstrate that NEURONS outperforms state-of-the-art baselines, achieving solid improvements in video consistency (26.6%) and semantic-level accuracy (19.1%). Notably, NEURONS shows a strong functional correlation with the visual cortex, highlighting its potential for brain-computer interfaces and clinical applications. Code and model weights are available at: https://github.com/xmed-lab/NEURONS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haonan Wang",
      "Qixiang Zhang",
      "Lehan Wang",
      "Xuanqi Huang",
      "Xiaomeng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Maillard_LACONIC_A_3D_Layout_Adapter_for_Controllable_Image_Creation_ICCV_2025_paper.html": {
    "title": "LACONIC: A 3D Layout Adapter for Controllable Image Creation",
    "volume": "main",
    "abstract": "Existing generative approaches for guided image synthesis of multi-object scenes typically rely on 2D controls in the image or text space. As a result, these methods struggle to maintain and respect consistent three-dimensional geometric structure, underlying the scene. In this paper, we propose a novel conditioning approach, training method and adapter network that can be plugged into pretrained text-to-image diffusion models. Our approach provides a way to endow such models with 3D-awareness, while leveraging their rich prior knowledge. Our method supports camera control, conditioning on explicit 3D geometries and, for the first time, accounts for the entire context of a scene, i.e., both on and off-screen items, to synthesize plausible and semantically rich images. Despite its multi-modal nature, our model is lightweight, requires a reasonable number of data for supervised learning and shows remarkable generalization power. We also introduce methods for intuitive and consistent image editing and restyling, e.g., by positioning, rotating or resizing individual objects in a scene. Our method integrates well within various image creation workflows and enables a richer set of applications compared to previous approaches",
    "checked": true,
    "id": "f646f8db3db66ac45585771bcf8e92e0ac127547",
    "semantic_title": "laconic: a 3d layout adapter for controllable image creation",
    "citation_count": 0,
    "authors": [
      "Léopold Maillard",
      "Tom Durand",
      "Adrien Ramanana Rahary",
      "Maks Ovsjanikov"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhuo_InfiniDreamer_Arbitrarily_Long_Human_Motion_Generation_via_Segment_Score_Distillation_ICCV_2025_paper.html": {
    "title": "InfiniDreamer: Arbitrarily Long Human Motion Generation via Segment Score Distillation",
    "volume": "main",
    "abstract": "We present InfiniDreamer, a novel framework for generating human motions of arbitrary length. Existing methods typically produce only short sequences, limited by the scarcity of long-range motion data. To address this, InfiniDreamer first generates short sub-motions for each textual description, then coarsely assembles them into a long sequence using randomly initialized transition segments. To refine this coarse motion, we introduce Segment Score Distillation (SSD)---an optimization-based approach that leverages a pre-trained motion diffusion model trained solely on short clips. SSD iteratively refines overlapping short segments sampled from the full sequence, progressively aligning them with the pre-trained short motion prior. This procedure ensures local fidelity within each segment and global consistency across segments. Extensive experiments demonstrate that InfiniDreamer produces coherent, diverse, and context-aware long-range motions without requiring additional long-sequence training",
    "checked": true,
    "id": "65bd52dd2cc99de8e82cab5375e8b2eadb832356",
    "semantic_title": "infinidreamer: arbitrarily long human motion generation via segment score distillation",
    "citation_count": 3,
    "authors": [
      "Wenjie Zhuo",
      "Fan Ma",
      "Hehe Fan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tong_Robust_Machine_Unlearning_for_Quantized_Neural_Networks_via_Adaptive_Gradient_ICCV_2025_paper.html": {
    "title": "Robust Machine Unlearning for Quantized Neural Networks via Adaptive Gradient Reweighting with Similar Labels",
    "volume": "main",
    "abstract": "Model quantization enables efficient deployment of deep neural networks on edge devices through low-bit parameter representation, yet raises critical challenges for implementing machine unlearning (MU) under data privacy regulations. Existing MU methods designed for full-precision models fail to address two fundamental limitations in quantized networks: 1) Noise amplification from label mismatch during data processing, and 2) Gradient imbalance between forgotten and retained data during training. These issues are exacerbated by quantized models' constrained parameter space and discrete optimization. We propose Q-MUL, the first dedicated unlearning framework for quantized models. Our method introduces two key innovations: 1) Similar Labels assignment replaces random labels with semantically consistent alternatives to minimize noise injection, and 2) Adaptive Gradient Reweighting dynamically aligns parameter update contributions from forgotten and retained data. Through systematic analysis of quantized model vulnerabilities, we establish theoretical foundations for these mechanisms. Extensive evaluations on benchmark datasets demonstrate Q-MUL's superiority over existing approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujia Tong",
      "Yuze Wang",
      "Jingling Yuan",
      "Chuang Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jiang_FairGen_Enhancing_Fairness_in_Text-to-Image_Diffusion_Models_via_Self-Discovering_Latent_ICCV_2025_paper.html": {
    "title": "FairGen: Enhancing Fairness in Text-to-Image Diffusion Models via Self-Discovering Latent Directions",
    "volume": "main",
    "abstract": "While Diffusion Models (DM) exhibit remarkable performance across various image generative tasks, they nonetheless reflect the inherent bias presented in the training set.As DMs are now widely used in real-world applications, these biases could perpetuate a distorted worldview and hinder opportunities for minority groups. Existing methods on debiasing DMs usually requires model re-training with a human-crafted reference dataset or additional classifiers, which suffer from two major limitations: (1) collecting reference datasets causes expensive annotation cost; (2) the debiasing performance is heavily constrained by the quality of the reference dataset or the additional classifier. To address the above limitations, we propose FairGen, a plug-and-play method that learns attribute latent directions in a self-discovering manner, thus eliminating the reliance on such reference dataset. Specifically, FairGen consists of two parts: a set of attribute adapters and a distribution indicator. Each adapter in the set aims to learn an attribute latent direction, and is optimized via noise composition through a self-discovering process.Then, the distribution indicator is multiplied by the set of adapters to guide the generation process towards the prescribed distribution. Our method enables debiasing multiple attributes in DMs simultaneously, while remaining lightweight and easily integrable with other DMs, eliminating the need for re-training. Extensive experiments on debiasing gender, racial, and their intersectional biases show that our method outperforms previous SOTA by a large margin",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yilei Jiang",
      "Wei-Hong Li",
      "Yiyuan Zhang",
      "Minghong Cai",
      "Xiangyu Yue"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Mondal_Gaze-Language_Alignment_for_Zero-Shot_Prediction_of_Visual_Search_Targets_from_ICCV_2025_paper.html": {
    "title": "Gaze-Language Alignment for Zero-Shot Prediction of Visual Search Targets from Human Gaze Scanpaths",
    "volume": "main",
    "abstract": "Decoding human intent from eye gaze during a visual search task has become an increasingly important capability within augmented and virtual reality systems. However, gaze target prediction models used within such systems are constrained by the predefined target categories found within available gaze data, limiting their generalizability to novel categories and their usefulness within real-world, interactive systems. In this work, we present the Gaze-Language Alignment Model (GLAM), a vision-language model that can generalize gaze target predictions to novel categories of search targets lacking gaze annotation. To do so, GLAM uses a novel gaze encoder to encode foveal and peripheral information of a gaze scanpath. The resultant gaze embeddings are aligned with language embeddings of large language model-generated search descriptions for associated target categories using a novel contrastive learning strategy called Gaze-Language Alignment Decomposition (GLAD). When used to train GLAM in a zero-shot setup, GLAD surpassed naive contrastive learning strategies by nearly one-third in target prediction accuracy, even outperforming a fully supervised baseline. Moreover, in a fully supervised setup, GLAM outperformed previous methods in target prediction accuracy, regardless of the training strategy used",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sounak Mondal",
      "Naveen Sendhilnathan",
      "Ting Zhang",
      "Yue Liu",
      "Michael Proulx",
      "Michael Louis Iuzzolino",
      "Chuan Qin",
      "Tanya R. Jonker"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Qu_ReCoT_Reflective_Self-Correction_Training_for_Mitigating_Confirmation_Bias_in_Large_ICCV_2025_paper.html": {
    "title": "ReCoT: Reflective Self-Correction Training for Mitigating Confirmation Bias in Large Vision-Language Models",
    "volume": "main",
    "abstract": "Recent advancements in Large Vision-Language Models (LVLMs) have greatly improved their ability to understand both visual and text information. However, a common problem in LVLMs is confirmation bias, where models tend to repeat previous assumptions and follow earlier viewpoints instead of reflecting and correcting themselves. This problem is more common in smaller-scale LVLMs, as they are usually fine-tuned with training data that is mostly positive, focusing on generating coherent dialogue. To address this issue, we introduce ReCoT, a method designed to mitigate confirmation bias in smaller-scale LVLMs through Reflective Self-Correction Training.The method follows a two-stage SFT-DPO paradigm: the first SFT stage aims to cultivate the model's reflective correction abilities, while the DPO stage focuses on enhancing the consistency between answers and reflections. Specifically, we construct dialogue-based reflective samples, which serve as adversarial samples during SFT. In this process, the model is initially presented with a potentially incorrect answer, followed by a reflection and correction phase to generate the final answer. To enhance answer-reflection consistency, we propose the consistency direct preference optimization. To comprehensively evaluate the effectiveness of our ReCoT, we introduce a set of novel metrics to measure the accuracy of the reflection and correction process. Extensive experiments show that ReCoT enables LVLM to engage in robust self-reflection and error correction and reduce confirmation bias",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengxue Qu",
      "Yibo Hu",
      "Kunyang Han",
      "Yunchao Wei",
      "Yao Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Bridging_Continuous_and_Discrete_Tokens_for_Autoregressive_Visual_Generation_ICCV_2025_paper.html": {
    "title": "Bridging Continuous and Discrete Tokens for Autoregressive Visual Generation",
    "volume": "main",
    "abstract": "Autoregressive visual generation models typically rely on tokenizers to compress images into tokens that can be predicted sequentially. A fundamental dilemma exists in token representation: discrete tokens enable straightforward modeling with standard cross-entropy loss, but suffer from information loss and tokenizer training instability; continuous tokens better preserve visual details, but require complex distribution modeling, complicating the generation pipeline. In this paper, we propose TokenBridge, which bridges this gap by maintaining the strong representation capacity of continuous tokens while preserving the modeling simplicity of discrete tokens. To achieve this, we decouple discretization from the tokenizer training process through post-training quantization that directly obtains discrete tokens from continuous representations. Specifically, we introduce a dimension-wise quantization strategy that independently discretizes each feature dimension, paired with a lightweight autoregressive prediction mechanism that efficiently model the resulting large token space. Extensive experiments show that our approach achieves reconstruction and generation quality on par with continuous methods while using standard categorical prediction. This work demonstrates that bridging discrete and continuous paradigms can effectively harness the strengths of both approaches, providing a promising direction for high-quality visual generation with simple autoregressive modeling. Project page: https://yuqingwang1029.github.io/TokenBridge",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuqing Wang",
      "Zhijie Lin",
      "Yao Teng",
      "Yuanzhi Zhu",
      "Shuhuai Ren",
      "Jiashi Feng",
      "Xihui Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Trade-offs_in_Image_Generation_How_Do_Different_Dimensions_Interact_ICCV_2025_paper.html": {
    "title": "Trade-offs in Image Generation: How Do Different Dimensions Interact?",
    "volume": "main",
    "abstract": "Model performance in text-to-image (T2I) and image-to-image (I2I) generation often depends on multiple aspects, including quality, alignment, diversity, and robustness. However, models' complex trade-offs among these dimensions have been rarely explored due to (1) the lack of datasets that allow fine-grained quantification of these trade-offs, and (2) using a single metric for multiple dimensions. To address this gap, we introduce TRIG-Bench (Trade-offs in Image Generation), which spans 10 dimensions (Realism, Originality, Aesthetics, Content, Relation, Style, Knowledge, Ambiguity, Toxicity and Bias), contains over 40,200 samples, and covers 132 Pairwise Dimensional Subsets. Furthermore, we develop TRIGScore,a VLM-as-judge metric that automatically adapts to various dimensions. Based on this, we evaluate 14 cutting-edge models across T2I and I2I tasks. In addition, we propose the Relation Recognition System and generate the Dimension Trade-off Map (DTM), which visualizes model-specific capability trade-offs. Our experiments demonstrate that DTM consistently provides a comprehensive understanding of the trade-offs between dimensions for each type of generation models. Notably, after fine-tuning on DTM, the model's dimension-specific impact is mitigated, and overall performance is enhanced. Code is available at: https://github.com/fesvhtr/TRIG",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sicheng Zhang",
      "Binzhu Xie",
      "Zhonghao Yan",
      "Yuli Zhang",
      "Donghao Zhou",
      "Xiaofei Chen",
      "Shi Qiu",
      "Jiaqi Liu",
      "Guoyang Xie",
      "Zhichao Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Du_From_Easy_to_Hard_The_MIR_Benchmark_for_Progressive_Interleaved_ICCV_2025_paper.html": {
    "title": "From Easy to Hard: The MIR Benchmark for Progressive Interleaved Multi-Image Reasoning",
    "volume": "main",
    "abstract": "Multi-image Interleaved Reasoning aims to improve Multimodal Large Language Models' (MLLMs) ability to jointly comprehend and reason across multiple images and their associated textual contexts, introducing unique challenges beyond single-image or non-interleaved multi-image tasks.While current multi-image benchmarks overlook interleaved textual contexts and neglect distinct relationships between individual images and their associated texts, enabling models to reason over multi-image interleaved data may significantly enhance their comprehension of complex scenes and better capture cross-modal correlations.To bridge this gap, we introduce a novel benchmark MIR, requiring joint reasoning over multiple images accompanied by interleaved textual contexts to accurately associate image regions with corresponding texts and logically connect information across images.To enhance MLLMs' ability to comprehend multi-image interleaved data, we introduce reasoning steps for each instance within the benchmark and propose a stage-wise curriculum learning strategy. This strategy follows an \"easy to hard\" approach, progressively guiding models from simple to complex scenarios, thereby enhancing their ability to handle challenging tasks.Extensive experiments benchmarking multiple MLLMs demonstrate that our method significantly enhances models' reasoning performance on MIR and other established benchmarks, highlighting the challenges current MLLMs face with multi-image interleaved reasoning.We believe that MIR will encourage further research into multi-image interleaved reasoning, facilitating advancements in MLLMs' capability to handle complex inter-modal tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hang Du",
      "Jiayang Zhang",
      "Guoshun Nan",
      "Wendi Deng",
      "Zhenyan Chen",
      "Chenyang Zhang",
      "Wang Xiao",
      "Shan Huang",
      "Yuqi Pan",
      "Tao Qi",
      "Sicong Leng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Rethinking_Layered_Graphic_Design_Generation_with_a_Top-Down_Approach_ICCV_2025_paper.html": {
    "title": "Rethinking Layered Graphic Design Generation with a Top-Down Approach",
    "volume": "main",
    "abstract": "Graphic design is crucial for conveying ideas and messages. Designers usually organize their work into objects, backgrounds, and vectorized text layers to simplify editing. However, this workflow demands considerable expertise. With the rise of GenAI methods, an endless supply of high-quality graphic designs in pixel format has become more accessible, though these designs often lack editability. Despite this, non-layered designs still inspire human designers, influencing their choices in layouts and text styles, ultimately guiding the creation of layered designs. Motivated by this observation, we propose Accordion, a graphic design generation framework taking the first attempt to convert AI-generated designs into editable layered designs, meanwhile refining nonsensical AI-generated text with meaningful alternatives guided by user prompts. It is built around a vision language model (VLM) playing distinct roles in three curated stages: (1) reference creation, (2) design planning, and (3) layer generation. For each stage, we design prompts to guide the VLM in executing different tasks. Distinct from existing bottom-up methods (e.g., COLE and Open-COLE) that gradually generate elements to create layered designs, our approach works in a top-down manner by using the visually harmonious reference image as global guidance to decompose each layer. Additionally, it leverages multiple vision experts such as SAM and element removal models to facilitate the creation of graphic layers. We train our method using the in-house graphic design dataset Design39K, augmented with AI-generated design images coupled with refined ground truth created by a customized inpainting model. Experimental results and user studies by designers show that Accordion generates favorable results on the DesignIntention benchmark, including tasks such as text-to-template, adding text to background, and text de-rendering, and also excels in creating design variations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingye Chen",
      "Zhaowen Wang",
      "Nanxuan Zhao",
      "Li Zhang",
      "Difan Liu",
      "Jimei Yang",
      "Qifeng Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_SegAnyPET_Universal_Promptable_Segmentation_from_Positron_Emission_Tomography_Images_ICCV_2025_paper.html": {
    "title": "SegAnyPET: Universal Promptable Segmentation from Positron Emission Tomography Images",
    "volume": "main",
    "abstract": "Positron Emission Tomography (PET) is a powerful molecular imaging tool that plays a crucial role in modern medical diagnostics by visualizing radio-tracer distribution to reveal physiological processes. Accurate organ segmentation from PET images is essential for comprehensive multi-systemic analysis of interactions between different organs and pathologies.Existing segmentation methods are limited by insufficient annotation data and varying levels of annotation, resulting in weak generalization ability and difficulty in clinical application. Recent developments in segmentation foundation models have shown superior versatility across diverse segmentation tasks. Despite the efforts of medical adaptations, these works primarily focus on structural medical images with detailed physiological structural information and exhibit limited generalization performance on molecular PET imaging. In this paper, we collect and construct PETS-5k, the largest PET segmentation dataset to date, comprising 5,731 three-dimensional whole-body PET images and encompassing over 1.3M 2D images. Based on the established dataset, we develop SegAnyPET, a modality-specific 3D foundation model for universal promptable segmentation from PET images. To issue the challenge of discrepant annotation quality, we adopt a cross prompting confident learning (CPCL) strategy with an uncertainty-guided self-rectification process to robustly learn segmentation from high-quality labeled data and low-quality noisy labeled data for promptable segmentation. Experimental results demonstrate that SegAnyPET can segment seen and unseen target organs using only one or a few prompt points, outperforming state-of-the-art foundation models and task-specific fully supervised models with higher accuracy and strong generalization ability for universal segmentation. As the first foundation model for PET images, we believe that SegAnyPET will advance the applications to various downstream tasks for molecular imaging",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yichi Zhang",
      "Le Xue",
      "Wenbo Zhang",
      "Lanlan Li",
      "Yuchen Liu",
      "Chen Jiang",
      "Yuan Cheng",
      "Yuan Qi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_StolenLoRA_Exploring_LoRA_Extraction_Attacks_via_Synthetic_Data_ICCV_2025_paper.html": {
    "title": "StolenLoRA: Exploring LoRA Extraction Attacks via Synthetic Data",
    "volume": "main",
    "abstract": "Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA have transformed vision model adaptation, enabling the rapid deployment of customized models. However, the compactness of LoRA adaptations introduces new safety concerns, particularly their vulnerability to model extraction attacks. This paper introduces a new focus of model extraction attacks named LoRA extraction that extracts LoRA-adaptive models based on a public pre-trained model. We then propose a novel extraction method called StolenLoRA which trains a substitute model to extract the functionality of a LoRA-adapted model using synthetic data. StolenLoRA leverages a Large Language Model to craft effective prompts for data generation, and it incorporates a Disagreement-based Semi-supervised Learning (DSL) strategy to maximize information gain from limited queries.Our experiments demonstrate the effectiveness of StolenLoRA, achieving up to a 96.60% attack success rate with only 10k queries, even in cross-backbone scenarios where the attacker and victim models utilize different pre-trained backbones. These findings reveal the specific vulnerability of LoRA-adapted models to this type of extraction and underscore the urgent need for robust defense mechanisms tailored to PEFT methods.We also explore a preliminary defense strategy based on diversified LoRA deployments, highlighting its potential to mitigate such attacks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixu Wang",
      "Yan Teng",
      "Yingchun Wang",
      "Xingjun Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yin_The_Best_of_Both_Worlds_Integrating_Language_Models_and_Diffusion_ICCV_2025_paper.html": {
    "title": "The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation",
    "volume": "main",
    "abstract": "Recent advancements in text-to-video (T2V) generation have been driven by two competing paradigms: autoregressive language models and diffusion models. However, each paradigm has intrinsic limitations: language models struggle with visual quality and error accumulation, while diffusion models lack semantic understanding and causal modeling. In this work, we propose LanDiff, a hybrid framework that synergizes the strengths of both paradigms through coarse-to-fine generation. Our architecture introduces three key innovations: (1) a semantic tokenizer that compresses 3D visual features into compact 1D discrete representations through efficient semantic compression, achieving a ~14,000xcompression ratio; (2) a language model that generates semantic tokens with high-level semantic relationships; (3) a streaming diffusion model that refines coarse semantics into high-fidelity videos. Experiments show that LanDiff, a 5B model, achieves a score of 85.43 on the VBench T2V benchmark, surpassing the state-of-the-art open-source models Hunyuan Video (13B) and other commercial models such as Sora, Kling, and Hailuo. Furthermore, our model also achieves state-of-the-art performance in long video generation, surpassing other open-source models in this field. Our demo can be viewed at https://landiff.github.io/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aoxiong Yin",
      "Xu Tan",
      "Kai Shen",
      "Yichong Leng",
      "Xinyu Zhou",
      "Juncheng Li",
      "Siliang Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yao_SV4D_2.0_Enhancing_Spatio-Temporal_Consistency_in_Multi-View_Video_Diffusion_for_ICCV_2025_paper.html": {
    "title": "SV4D 2.0: Enhancing Spatio-Temporal Consistency in Multi-View Video Diffusion for High-Quality 4D Generation",
    "volume": "main",
    "abstract": "We present Stable Video 4D 2.0 (SV4D 2.0), a multi-view video diffusion model for dynamic 3D asset generation. Compared to its predecessor SV4D, SV4D 2.0 is more robust to occlusions and large motion, generalizes better to real-world videos, and produces higher-quality outputs in terms of detail sharpness and spatio-temporal consistency. We achieve this by introducing key improvements in multiple aspects: 1) network architecture: eliminating the dependency of reference multi-views and designing blending mechanism for 3D and frame attention, 2) data: enhancing quality and quantity of training data, 3) training strategy: adopting progressive 3D-4D training for better generalization, and 4) 4D optimization: handling 3D inconsistency and large motion via 2-stage refinement and progressive frame sampling. Extensive experiments demonstrate significant performance gain by SV4D 2.0 both visually and quantitatively, achieving better detail (-14% LPIPS) and 4D consistency (-44% FV4D) in novel-view video synthesis and 4D optimization (-12% LPIPS and -24% FV4D) compared to SV4D",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chun-Han Yao",
      "Yiming Xie",
      "Vikram Voleti",
      "Huaizu Jiang",
      "Varun Jampani"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Guo_Long_Context_Tuning_for_Video_Generation_ICCV_2025_paper.html": {
    "title": "Long Context Tuning for Video Generation",
    "volume": "main",
    "abstract": "Recent advances in video generation can produce realistic, minute-long single-shot videos with scalable diffusion transformers. However, real-world narrative videos require multi-shot scenes with visual and dynamic consistency across shots. In this work, we introduce Long Context Tuning (LCT), a training paradigm that expands the context window of pre-trained single-shot video diffusion models to learn scene-level consistency directly from data. Our method expands full attention mechanisms from individual shots to encompass all shots within a scene, incorporating interleaved 3D position embedding and an asynchronous noise strategy, enabling both joint and auto-regressive shot generation without additional parameters. Models with bidirectional attention after LCT can further be fine-tuned with context-causal attention, facilitating auto-regressive generation with efficient KV-cache. Experiments demonstrate single-shot models after LCT can produce coherent multi-shot scenes and exhibit emerging capabilities, including compositional generation and interactive shot extension, paving the way for more practical visual content creation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuwei Guo",
      "Ceyuan Yang",
      "Ziyan Yang",
      "Zhibei Ma",
      "Zhijie Lin",
      "Zhenheng Yang",
      "Dahua Lin",
      "Lu Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Fu_TCFG_Truncated_Classifier-Free_Guidance_for_Efficient_and_Scalable_Text-to-Image_Acceleration_ICCV_2025_paper.html": {
    "title": "TCFG: Truncated Classifier-Free Guidance for Efficient and Scalable Text-to-Image Acceleration",
    "volume": "main",
    "abstract": "Diffusion models have achieved remarkable success in image and video generation due to their powerful generative capabilities. However, they suffer from slow inference speed and high computational costs. Existing acceleration methods for diffusion models may compromise model performance and struggle to generalize across diverse diffusion model architectures and downstream tasks. To address these issues, we propose a model-agnostic and highly scalable acceleration strategy for text-controlled image generation. Specifically, we dynamically modulate the text guidance coefficience and truncate redundant text-related computations during the denoising process. Experimental results demonstrate that our approach achieves significant model acceleration while preserving precise text-image alignment, showcasing the potential for a wide range of diffusion models and downstream applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaomeng Fu",
      "Jia Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lan_Removing_Out-of-Focus_Reflective_Flares_via_Color_Alignment_ICCV_2025_paper.html": {
    "title": "Removing Out-of-Focus Reflective Flares via Color Alignment",
    "volume": "main",
    "abstract": "Reflective flares are common artifacts in photography that degrade image quality, introducing in-focus flares, which appear as bright, regular spot patterns, and out-of-focus flares, which are diffuse and semi-transparent, obscuring the underlying scene. While previous methods have achieved some success in removing in-focus flares, they struggle with the diffuse nature of out-of-focus flares. The lack of an out-of-focus flare dataset has further hindered the development of effective flare removal models. In this work, we construct a large-scale out-of-focus flare dataset generated with Blender. We propose to use a color alignment approach on diffusion models to address the challenges of out-of-focus reflective flare removal. Rather than fully reconstructing flare-affected regions, our method adjusts the color distribution to reduce artifact visibility while preserving image content. Specifically, we apply a differentiable histogram loss on the diffusion model, which is derived from the Earth Mover's Distance (EMD), to effectively align color distributions. The proposed approach, trained exclusively on synthetic data, outperforms existing methods on both synthetic and real-world data, demonstrating improved performance in flare removal",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fengbo Lan",
      "Chang Wen Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_ILLUME_Illuminating_Your_LLMs_to_See_Draw_and_Self-Enhance_ICCV_2025_paper.html": {
    "title": "ILLUME: Illuminating Your LLMs to See, Draw, and Self-Enhance",
    "volume": "main",
    "abstract": "In this paper, we introduce ILLUME, a unified multimodal large language model (MLLM) that seamlessly integrates multimodal understanding and generation capabilities within a single large language model through a unified next-token prediction formulation.To address the large dataset size typically required for image-text alignment, we propose to enhance data efficiency through the design of a vision tokenizer that incorporates semantic information and a progressive multi-stage training procedure. This approach reduces the dataset size to just 15M for pretraining -- over four times fewer than what is typically needed -- while achieving competitive or even superior performance with existing unified MLLMs, such as Janus. Additionally, to promote synergistic enhancement between understanding and generation capabilities, which is under-explored in previous works, we introduce a novel self-enhancing multimodal alignment scheme. This scheme supervises the MLLM to self-assess the consistency between text descriptions and self-generated images, facilitating the model to interpret images more accurately and avoid unrealistic and incorrect predictions caused by misalignment in image generation. Based on our extensive experiments, our proposed ILLUME stands out and competes with state-of-the-art unified MLLMs and specialized models across various benchmarks for multimodal understanding, generation, and editing",
    "checked": true,
    "id": "045237d01ace21094707c6510c9a43d8b1fbc65c",
    "semantic_title": "illume: illuminating your llms to see, draw, and self-enhance",
    "citation_count": 36,
    "authors": [
      "Chunwei Wang",
      "Guansong Lu",
      "Junwei Yang",
      "Runhui Huang",
      "Jianhua Han",
      "Lu Hou",
      "Wei Zhang",
      "Hang Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_No_Pose_at_All_Self-Supervised_Pose-Free_3D_Gaussian_Splatting_from_ICCV_2025_paper.html": {
    "title": "No Pose at All: Self-Supervised Pose-Free 3D Gaussian Splatting from Sparse Views",
    "volume": "main",
    "abstract": "We introduce SPFSplat, an efficient framework for 3D Gaussian splatting from sparse multi-view images, requiring no ground-truth poses during training or inference. It employs a shared feature extraction backbone, enabling simultaneous prediction of 3D Gaussian primitives and camera poses in a canonical space from unposed inputs within a single feed-forward step. Alongside the rendering loss based on estimated novel-view poses, a reprojection loss is integrated to enforce the learning of pixel-aligned Gaussian primitives for enhanced geometric constraints. This pose-free training paradigm and efficient one-step feed-forward design make SPFSplat well-suited for practical applications. Remarkably, despite the absence of pose supervision, SPFSplat achieves state-of-the-art performance in novel view synthesis even under significant viewpoint changes and limited image overlap. It also surpasses recent methods trained with geometry priors in relative pose estimation. Code and trained models are available on our project page: https://ranrhuang.github.io/spfsplat/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ranran Huang",
      "Krystian Mikolajczyk"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gokay_Skeleton_Motion_Words_for_Unsupervised_Skeleton-Based_Temporal_Action_Segmentation_ICCV_2025_paper.html": {
    "title": "Skeleton Motion Words for Unsupervised Skeleton-Based Temporal Action Segmentation",
    "volume": "main",
    "abstract": "Current state-of-the-art methods for skeleton-based temporal action segmentation are predominantly supervised and require annotated data, which is expensive to collect. In contrast, existing unsupervised temporal action segmentation methods have focused primarily on video data, while skeleton sequences remain underexplored, despite their relevance to real-world applications, robustness, and privacy-preserving nature. In this paper, we propose a novel approach for unsupervised skeleton-based temporal action segmentation. Our method utilizes a sequence-to-sequence temporal autoencoder that keeps the information of the different joints disentangled in the embedding space. Latent skeleton sequences are then divided into non-overlapping patches and quantized to obtain distinctive skeleton motion words, driving the discovery of semantically meaningful action clusters. We thoroughly evaluate the proposed approach on three widely used skeleton-based datasets, namely HuGaDB, LARa, and BABEL. The results demonstrate that our model outperforms the current state-of-the-art unsupervised temporal action segmentation methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Uzay Gökay",
      "Federico Spurio",
      "Dominik R. Bach",
      "Juergen Gall"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_Towards_Robust_Defense_against_Customization_via_Protective_Perturbation_Resistant_to_ICCV_2025_paper.html": {
    "title": "Towards Robust Defense against Customization via Protective Perturbation Resistant to Diffusion-based Purification",
    "volume": "main",
    "abstract": "Diffusion models like Stable Diffusion have become prominent in visual synthesis tasks due to their powerful customization capabilities, which also introduce significant security risks, including deepfakes and copyright infringement. In response, a class of methods known as protective perturbation emerged, which mitigates image misuse by injecting imperceptible adversarial noise. However, purification can remove protective perturbations, thereby exposing images again to the risk of malicious forgery. In this work, we formalize the anti-purification task, highlighting challenges that hinder existing approaches, and propose a simple diagnostic protective perturbation named AntiPure. AntiPure exposes vulnerabilities of purification within the \"purification-customization\" workflow, owing to two guidance mechanisms: 1) Patch-wise Frequency Guidance, which reduces the model's influence over high-frequency components in the purified image, and 2) Erroneous Timestep Guidance, which disrupts the model's denoising strategy across different timesteps. With additional guidance, AntiPure embeds imperceptible perturbations that persist under representative purification settings, achieving effective post-customization distortion. Experiments show that, as a stress test for purification, AntiPure achieves minimal perceptual discrepancy and maximal distortion, outperforming other protective perturbation methods within the purification-customization workflow",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenkui Yang",
      "Jie Cao",
      "Junxian Duan",
      "Ran He"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_EmbodiedOcc_Embodied_3D_Occupancy_Prediction_for_Vision-based_Online_Scene_Understanding_ICCV_2025_paper.html": {
    "title": "EmbodiedOcc: Embodied 3D Occupancy Prediction for Vision-based Online Scene Understanding",
    "volume": "main",
    "abstract": "3D occupancy prediction provides a comprehensive description of the surrounding scenes and has become an essential task for 3D perception. Most existing methods focus on offline perception from one or a few views and cannot be applied to embodied agents that demand to gradually perceive the scene through progressive embodied exploration. In this paper, we formulate an embodied 3D occupancy prediction task to target this practical scenario and propose a Gaussian-based EmbodiedOcc framework to accomplish it. We initialize the global scene with uniform 3D semantic Gaussians and progressively update local regions observed by the embodied agent. For each update, we extract semantic and structural features from the observed image and efficiently incorporate them via deformable cross-attention to refine the regional Gaussians. Finally, we employ Gaussian-to-voxel splatting to obtain the global 3D occupancy from the updated 3D Gaussians. Our EmbodiedOcc assumes an unknown (i.e., uniformly distributed) environment and maintains an explicit global memory of it with 3D Gaussians. It gradually gains knowledge through the local refinement of regional Gaussians, which is consistent with how humans understand new scenes through embodied exploration. We reorganize an EmbodiedOcc-ScanNet benchmark based on local annotations to facilitate the evaluation of the embodied 3D occupancy prediction task. Our EmbodiedOcc outperforms existing methods by a large margin and accomplishes the embodied occupancy prediction with high accuracy and efficiency. Code: https://github.com/YkiWu/EmbodiedOcc",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuqi Wu",
      "Wenzhao Zheng",
      "Sicheng Zuo",
      "Yuanhui Huang",
      "Jie Zhou",
      "Jiwen Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sarridis_MAVias_Mitigate_any_Visual_Bias_ICCV_2025_paper.html": {
    "title": "MAVias: Mitigate any Visual Bias",
    "volume": "main",
    "abstract": "Mitigating biases in computer vision models is an essential step towards trustworthy artificial intelligence systems. Existing bias mitigation methods are limited to predefined biases, preventing their use in visual datasets where multiple, possibly unknown biases exist. To address this limitation, we introduce MAVias, an open-set bias mitigation approach that leverages foundation models to discover spurious associations between visual attributes and target classes. MAVias first captures a wide variety of visual features in natural language via a foundation image tagging model, and then leverages a large language model to select visual features that define the target class, resulting in a set of language-coded potential visual biases. It then translates these biases into vision-language embeddings and introduces an in-processing bias mitigation approach to prevent the model from encoding information related to them. Experiments on diverse datasets, including CelebA, Waterbirds, ImageNet, and UrbanCars, show that MAVias effectively detects and mitigates a wide range of biases in visual recognition tasks, outperforming current state-of-the-art",
    "checked": true,
    "id": "d24fc4b1535a53a8e41e5b282c63825949987b64",
    "semantic_title": "mavias: mitigate any visual bias",
    "citation_count": 2,
    "authors": [
      "Ioannis Sarridis",
      "Christos Koutlis",
      "Symeon Papadopoulos",
      "Christos Diou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Slyman_Calibrating_MLLM-as-a-judge_via_Multimodal_Bayesian_Prompt_Ensembles_ICCV_2025_paper.html": {
    "title": "Calibrating MLLM-as-a-judge via Multimodal Bayesian Prompt Ensembles",
    "volume": "main",
    "abstract": "Multimodal large language models (MLLMs) are increasingly used to evaluate text-to-image (TTI) generation systems, providing automated judgments based on visual and textual context. However, these \"judge\" models often suffer from biases, overconfidence, and inconsistent performance across diverse image domains. While prompt ensembling has shown promise for mitigating these issues in unimodal, text-only settings, our experiments reveal that standard ensembling methods fail to generalize effectively for TTI tasks. To address these limitations, we propose a new multimodal-aware method called **M**ultimodal **M**ixture-of-**B**ayesian Prompt Ensembles (MMB). Our approach uses a Bayesian prompt ensemble approach augmented by image clustering, allowing the judge to dynamically assign prompt weights based on the visual characteristics of each sample. We show that MMB improves accuracy in pairwise preference judgments and greatly enhances calibration, making it easier to gauge the judge's true uncertainty. In evaluations on two TTI benchmarks, HPSv2 and MJBench, MMB outperforms existing baselines in alignment with human annotations and calibration across varied image content. Our findings highlight the importance of multimodal-specific strategies for judge calibration and suggest a promising path forward for reliable large-scale TTI evaluation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eric Slyman",
      "Mehrab Tanjim",
      "Kushal Kafle",
      "Stefan Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ma_SCFlow_Implicitly_Learning_Style_and_Content_Disentanglement_with_Flow_Models_ICCV_2025_paper.html": {
    "title": "SCFlow: Implicitly Learning Style and Content Disentanglement with Flow Models",
    "volume": "main",
    "abstract": "Explicitly disentangling style and content in vision models remains challenging due to their semantic overlap and the subjectivity of human perception. Existing methods propose separation through generative or discriminative objectives, but they still face the inherent ambiguity of disentangling intertwined concepts. Instead, we ask: Can we bypass explicit disentanglement by learning to merge style and content invertibly, allowing separation to emerge naturally? We propose SCFlow, a flow-matching framework that learns bidirectional mappings between entangled and disentangled representations. Our approach is built upon three key insights: 1) Training solely to merge style and content, a well-defined task, enables invertible disentanglement without explicit supervision; 2) flow matching bridges on arbitrary distributions, avoiding the restrictive Gaussian priors of diffusion models and normalizing flows; and 3) a synthetic dataset of 510,000 samples was curated to simulate disentanglement through systematic style-content pairing. Beyond controllable generation tasks, we demonstrate that SCFlow generalizes to ImageNet-1k and WikiArt in zero-shot settings and achieves competitive performance, highlighting that disentanglement naturally emerges from the invertible merging process. Code and dataset: https://github.com/CompVis/SCFlow",
    "checked": true,
    "id": "5fe57cff0db057d948d830bff3ede6a1286bca24",
    "semantic_title": "scflow: implicitly learning style and content disentanglement with flow models",
    "citation_count": 0,
    "authors": [
      "Pingchuan Ma",
      "Xiaopei Yang",
      "Yusong Li",
      "Ming Gui",
      "Felix Krause",
      "Johannes Schusterbauer",
      "Björn Ommer"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Srivastava_Lay-Your-Scene_Natural_Scene_Layout_Generation_with_Diffusion_Transformers_ICCV_2025_paper.html": {
    "title": "Lay-Your-Scene: Natural Scene Layout Generation with Diffusion Transformers",
    "volume": "main",
    "abstract": "We present Lay-Your-Scene (shorthand LayouSyn), a novel text-to-layout generation pipeline for natural scenes. Prior scene layout generation methods are either closed-vocabulary or use proprietary large language models for open-vocabulary generation, limiting their modeling capabilities and broader applicability in controllable image generation. In this work, we propose to use lightweight open-source language models to obtain scene elements from text prompts and a novel aspect-aware diffusion Transformer architecture trained in an open-vocabulary manner for conditional layout generation. Extensive experiments demonstrate that LayouSyn outperforms existing methods and achieves state-of-the-art performance on challenging spatial and numerical reasoning benchmarks. Additionally, we present two applications of LayouSyn: First, we show that coarse initialization from large language models can be seamlessly combined with our method to achieve better results. Second, we present a pipeline for adding objects to images, demonstrating the potential of LayouSyn in image editing applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Divyansh Srivastava",
      "Xiang Zhang",
      "He Wen",
      "Chenru Wen",
      "Zhuowen Tu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Guo_DiST-4D_Disentangled_Spatiotemporal_Diffusion_with_Metric_Depth_for_4D_Driving_ICCV_2025_paper.html": {
    "title": "DiST-4D: Disentangled Spatiotemporal Diffusion with Metric Depth for 4D Driving Scene Generation",
    "volume": "main",
    "abstract": "Current generative models struggle to synthesize dynamic 4D driving scenes that simultaneously support temporal extrapolation and spatial novel view synthesis (NVS) without per-scene optimization. A key challenge lies in finding an efficient and generalizable geometric representation that seamlessly connects temporal and spatial synthesis. To address this, we propose DiST-4D, the first disentangled spatiotemporal diffusion framework for 4D driving scene generation, which leverages metric depth as the core geometric representation. DiST-4D decomposes the problem into two diffusion processes: DiST-T, which predicts future metric depth and multi-view RGB sequences directly from past observations, and DiST-S, which enables spatial NVS by training only on existing viewpoints while enforcing cycle consistency. This cycle consistency mechanism introduces a forward-backward rendering constraint, reducing the generalization gap between observed and unseen viewpoints. Metric depth is essential for both accurate reliable forecasting and accurate spatial NVS, as it provides a view-consistent geometric representation that generalizes well to unseen perspectives. Experiments demonstrate that DiST-4D achieves state-of-the-art performance in both temporal prediction and NVS tasks, while also delivering competitive performance in planning-related evaluations. The Project is available at https://royalmelon0505.github.io/DiST-4D",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiazhe Guo",
      "Yikang Ding",
      "Xiwu Chen",
      "Shuo Chen",
      "Bohan Li",
      "Yingshuang Zou",
      "Xiaoyang Lyu",
      "Feiyang Tan",
      "Xiaojuan Qi",
      "Zhiheng Li",
      "Hao Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Enhancing_Prompt_Generation_with_Adaptive_Refinement_for_Camouflaged_Object_Detection_ICCV_2025_paper.html": {
    "title": "Enhancing Prompt Generation with Adaptive Refinement for Camouflaged Object Detection",
    "volume": "main",
    "abstract": "Foundation models, such as Segment Anything Model (SAM), have exhibited remarkable performance in conventional segmentation tasks, primarily due to their training on large-scale datasets. Nonetheless, challenges remain in specific downstream tasks, such as Camouflaged Object Detection (COD). Existing research primarily aims to enhance performance by integrating additional multimodal information derived from other foundation models. However, directly leveraging the information generated by these models may introduce additional biases due to domain shifts. To address this issue, we propose an Adaptive Refinement Module (ARM), which efficiently processes multimodal information and simultaneously refining the mask prompt. Furthermore, we construct an auxiliary embedding that effectively exploits the intermediate information generated during ARM, providing SAM with richer feature representations. Experimental results indicate that our proposed architecture surpasses most state-of-the-art (SOTA) models in the COD task, particularly excelling in structured target segmentation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuehan Chen",
      "Guangyu Ren",
      "Tianhong Dai",
      "Tania Stathaki",
      "Hengyan Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sun_Generalizable_Non-Line-of-Sight_Imaging_with_Learnable_Physical_Priors_ICCV_2025_paper.html": {
    "title": "Generalizable Non-Line-of-Sight Imaging with Learnable Physical Priors",
    "volume": "main",
    "abstract": "Non-line-of-sight (NLOS) imaging, recovering the hidden volume from indirect reflections, has attracted increasing attention due to its potential applications. Despite promising results, existing NLOS reconstruction approaches are constrained by the reliance on empirical physical priors, e.g., single fixed path compensation. Moreover, these approaches still possess limited generalization ability, particularly when dealing with scenes at a low signal-to-noise ratio (SNR). To overcome the above problems, we introduce a novel learning-based approach, comprising two key designs: Learnable Path Compensation (LPC) and Adaptive Phasor Field (APF). The LPC applies tailored path compensation coefficients to adapt to different objects in the scene, effectively reducing light wave attenuation, especially in distant regions. Meanwhile, the APF learns the precise Gaussian window of the illumination function for the phasor field, dynamically selecting the relevant spectrum band of the transient measurement. Experimental validations demonstrate that our proposed approach, only trained on synthetic data, exhibits the capability to seamlessly generalize across various real-world datasets captured by different imaging systems and characterized by low SNRs. Code is available at https://github.com/ssd1051/NLOS-LPP",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shida Sun",
      "Yue Li",
      "Yueyi Zhang",
      "Zhiwei Xiong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Stoica_Contrastive_Flow_Matching_ICCV_2025_paper.html": {
    "title": "Contrastive Flow Matching",
    "volume": "main",
    "abstract": "Unconditional flow matching trains diffusion models to transport samples from a source distribution to a target distribution by enforcing that the flows between sample pairs are unique. However, in conditional settings (e.g., class-conditioned models), this uniqueness is no longer guaranteed--flows from different conditions may overlap, leading to more ambiguous generations. We introduce Contrastive Flow Matching, an extension to the flow matching objective that explicitly enforces uniqueness across all conditional flows, enhancing condition separation. Our approach adds a contrastive objective that maximizes dissimilarities between predicted flows from arbitrary sample pairs. We validate Contrastive Flow Matching by conducting extensive experiments across varying model architectures on both class-conditioned (ImageNet-1k) and text-to-image (CC3M) benchmarks. Notably, we find that training models with Contrastive Flow Matching (1) improves training speed by a factor of up to 9x, (2) requires up to 5x fewer de-noising steps and (3) lowers FID by up to 8.9 compared to training the same models with flow matching. We release our code at: https://github.com/gstoica27/DeltaFM.git",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "George Stoica",
      "Vivek Ramanujan",
      "Xiang Fan",
      "Ali Farhadi",
      "Ranjay Krishna",
      "Judy Hoffman"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Multi-scenario_Overlapping_Text_Segmentation_with_Depth_Awareness_ICCV_2025_paper.html": {
    "title": "Multi-scenario Overlapping Text Segmentation with Depth Awareness",
    "volume": "main",
    "abstract": "Overlapping text poses significant challenges for text-related perception tasks, particularly in open scenes characterized by diverse fonts and visual effects. While existing research has primarily addressed the overlapping problem in documents, its applicability to other scenes remains limited. To bridge this gap, we propose a new task of multi-scenario overlapping text segmentation and introduce a corresponding real dataset in both English and Chinese, spanning various contexts such as printed text, bills, artistic designs, and house numbers. To further enhance the generalization of overlapping text segmentation models, we propose a hierarchical training data synthesis strategy that simulates diverse overlapping patterns across different scenarios. Furthermore, we found that depth maps can provide clear relative position relationships in three-dimensional space, assisting the model in capturing complex overlapping relationships between text instances. Building on this insight, we present a depth-guided decoder that seamlessly integrates image and depth features to capture overlapping interactions. Our proposed model achieves a 5.3% improvement in text mIoU and a 6.4% improvement in overall mIoU compared to existing SOTA methods on our benchmark and SignaTR6k datasets, respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Liu",
      "Xudong Xie",
      "Yuliang Liu",
      "Xiang Bai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_SemiVisBooster_Boosting_Semi-Supervised_Learning_for_Fine-Grained_Classification_through_Pseudo-Label_Semantic_ICCV_2025_paper.html": {
    "title": "SemiVisBooster: Boosting Semi-Supervised Learning for Fine-Grained Classification through Pseudo-Label Semantic Guidance",
    "volume": "main",
    "abstract": "Deep learning models rely on large-scale labeled datasets, but collecting such data is expensive and time-consuming. Semi-supervised learning (SSL) mitigates this issue by learning from a small set of labeled samples along with a large pool of unlabeled data. However, existing SSL methods struggle with fine-grained classification when dealing with visually similar classes, as they rely solely on visual features and ignore the semantic information within label names. This paper introduces SemiVisBooster, an SSL enhancement approach that utilizes semantic information from label names to guide visual feature learning, addressing the challenges of fine-grained classification. By aligning text embeddings from label names with visual features, our method helps the model capture subtle visual distinctions that purely visual representations may overlook. To enhance robustness, we propose two key components: (1) text embedding de-similarity (TEDS) to reduce confusion caused by similar text embeddings across different class names, and (2) class-aware visual-text alignment loss to accurately define positive and negative pairs during visual-text alignment. Our method achieves state-of-the-art performance on the latest SSL benchmarks. Additionally, on the challenging Food-101 dataset, which contains many visually similar classes and uses only 404 labeled images, our approach improves performance by approximately 13.6% over the second-best method. Code is available at \\href https://github.com/wenjinzhang/SemiVisBooster SemiVisBooster",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenjin Zhang",
      "Xinyu Li",
      "Chenyang Gao",
      "Ivan Marsic"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Backdooring_Self-Supervised_Contrastive_Learning_by_Noisy_Alignment_ICCV_2025_paper.html": {
    "title": "Backdooring Self-Supervised Contrastive Learning by Noisy Alignment",
    "volume": "main",
    "abstract": "Self-supervised contrastive learning (CL) effectively learns transferable representations from unlabeled data containing images or image-text pairs but suffers vulnerability to data poisoning backdoor attacks (DPCLs). An adversary can inject poisoned images into pretraining datasets, causing compromised CL encoders to exhibit targeted misbehavior in downstream tasks. Existing DPCLs, however, achieve limited efficacy due to their dependence on fragile implicit co-occurrence between backdoor and target object and inadequate suppression of discriminative features in backdoored images. We propose Noisy Alignment (NA), a DPCL method that explicitly suppresses noise components in poisoned images. Inspired by powerful training-controllable CL attacks, we identify and extract the critical objective of noisy alignment, adapting it effectively into data-poisoning scenarios. Our method implements noisy alignment by strategically manipulating contrastive learning's random cropping mechanism, formulating this process as an image layout optimization problem with theoretically derived optimal parameters. The resulting method is simple yet effective, achieving state-of-the-art performance compared to existing DPCLs, while maintaining clean-data accuracy. Furthermore, Noisy Alignment demonstrates robustness against common backdoor defenses. Codes can be found at https://github.com/jsrdcht/Noisy-Alignment",
    "checked": true,
    "id": "be10565d97e9fa41a551fcadaf4ae2ab9fba82ce",
    "semantic_title": "backdooring self-supervised contrastive learning by noisy alignment",
    "citation_count": 0,
    "authors": [
      "Tuo Chen",
      "Jie Gui",
      "Minjing Dong",
      "Ju Jia",
      "Lanting Fang",
      "Jian Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_VisualCloze_A_Universal_Image_Generation_Framework_via_Visual_In-Context_Learning_ICCV_2025_paper.html": {
    "title": "VisualCloze: A Universal Image Generation Framework via Visual In-Context Learning",
    "volume": "main",
    "abstract": "Recent advances in diffusion models have significantly advanced image generation; however, existing models remain task-specific, limiting their efficiency and generalizability. While universal models attempt to address these limitations, they face critical challenges, including generalizable instruction design, appropriate task distributions, and unified architectural design. In this work, we propose VisualCloze, a universal image generation framework, to tackle these challenges. Unlike existing methods that rely on language-based task descriptions, leading to task ambiguity and weak generalization, we integrate visual in-context learning, allowing models to identify tasks from demonstrations. Meanwhile, the inherent sparsity of visual task distributions hampers the learning of transferable knowledge across tasks. To this end, we introduce Graph200K, a graph-structured dataset that establishes various interrelated tasks, enhancing task density and knowledge transfer. Furthermore, we uncover an intrinsic alignment between image infilling and in-context learning, enabling us to leverage the strong generative priors of pre-trained infilling models without modifying their architectures. Experiments demonstrate that VisualCloze achieves strong performance across more than 100 in-domain tasks while generalizing to unseen tasks in few-shot and zero-shot settings. Our codes and dataset are available at https://visualcloze.github.io/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhong-Yu Li",
      "Ruoyi Du",
      "Juncheng Yan",
      "Le Zhuo",
      "Zhen Li",
      "Peng Gao",
      "Zhanyu Ma",
      "Ming-Ming Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_MagicHOI_Leveraging_3D_Priors_for_Accurate_Hand-object_Reconstruction_from_Short_ICCV_2025_paper.html": {
    "title": "MagicHOI: Leveraging 3D Priors for Accurate Hand-object Reconstruction from Short Monocular Video Clips",
    "volume": "main",
    "abstract": "Most RGB-based hand-object reconstruction methods rely on object templates, while template-free methods typically assume full object visibility. This assumption often breaks in real-world settings, where fixed camera viewpoints and static grips leave parts of the object unobserved, resulting in implausible reconstructions. To overcome this, we present MagicHOI, a method for reconstructing hands and objects from short monocular interaction videos, even under limited viewpoint variation. Our key insight is that, despite the scarcity of paired 3D hand-object data, large-scale novel view synthesis diffusion models offer rich object supervision. This supervision serves as a prior to regularize unseen object regions during hand interactions. Leveraging this insight, we integrate a novel view synthesis model into our hand-object reconstruction framework. We further align hand to object by incorporating visible contact constraints. Our results demonstrate that MagicHOI significantly outperforms existing state-of-the-art hand-object reconstruction methods. We also show that novel view synthesis diffusion priors effectively regularize unseen object regions, enhancing 3D hand-object reconstruction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shibo Wang",
      "Haonan He",
      "Maria Parelli",
      "Christoph Gebhardt",
      "Zicong Fan",
      "Jie Song"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ma_ReMP-AD_Retrieval-enhanced_Multi-modal_Prompt_Fusion_for_Few-Shot_Industrial_Visual_Anomaly_ICCV_2025_paper.html": {
    "title": "ReMP-AD: Retrieval-enhanced Multi-modal Prompt Fusion for Few-Shot Industrial Visual Anomaly Detection",
    "volume": "main",
    "abstract": "Industrial visual inspection is crucial for detecting defects in manufactured products, but it traditionally relies on human operators, leading to inefficiencies. Industrial Visual Anomaly Detection (IVAD) has emerged as a promising solution, with methods such as zero-shot, few-shot, and reconstruction-based techniques. However, zero-shot methods struggle with subtle anomalies, and reconstruction-based methods fail to capture fine-grained details. Few-shot methods, which use limited samples and prompts, offer a more efficient approach. Despite their promise, challenges remain in managing intra-class variation among references and in effectively extracting more representative anomaly features.This paper presents Retrieval-enhanced Multi-modal Prompt Fusion Anomaly Detection (ReMP-AD), a framework that introduces Intra-Class Token Retrieval (ICTR) to reduce noise in the memory bank and Vision-Language Prior Fusion (VLPF) to guide the encoder in capturing more distinctive and relevant features of anomalies. Experiments on the VisA and MVTec-AD datasets demonstrate that ReMP-AD outperforms existing methods, achieving 97.8%/94.1% performance in 4-shot anomaly segmentation and classification. Our approach also shows strong results on the PCB-Bank dataset, highlighting its effectiveness in few-shot industrial anomaly detection. Code is available at https://github.com/cshcma/ReMP-AD.git",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongchi Ma",
      "Guanglei Yang",
      "Debin Zhao",
      "Yanli Ji",
      "Wangmeng Zuo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Trial-Oriented_Visual_Rearrangement_ICCV_2025_paper.html": {
    "title": "Trial-Oriented Visual Rearrangement",
    "volume": "main",
    "abstract": "Towards visual room rearrangement for embodied agents, this paper tackles the intricate challenge of restoring a disarrayed scene configuration to its intended goal state. The task necessitates a range of sophisticated capabilities, including efficient spatial navigation, precise and accurate object interaction, sensitive scene change detection, and meticulous restoration techniques. The inherent complexity of this endeavor stems from the diverse nature of potential object changes, encompassing movements within the space, alterations in appearance, and changes in existence--where objects may be introduced or removed from the scene. Previous methods, either end-to-end reinforcement learning or modular approaches, struggle with handling these changes in a unified manner due to the heterogeneous nature of the inference spaces. To address this, this paper proposes a Trial-Oriented Visual Rearrangement (TOR) framework, which leverages the principles of stronger embodiment to prune the joint reasoning space and identify a smaller shared space for processing various object changes. TOR maintains a differential point cloud representation to capture environmental changes and uses two core mechanisms, assessment and refinement, to iteratively restore the scene to the goal state. Experimental results demonstrate the effectiveness of TOR in restoring both object movement and appearance changes and show its generalization to complex multi-room environments",
    "checked": false,
    "id": "dd31ab689bbae4dc4c4e5f9d71fbed4e509a8eac",
    "semantic_title": "visual imitation learning of task-oriented object grasping and rearrangement",
    "citation_count": 8,
    "authors": [
      "Yuyi Liu",
      "Xinhang Song",
      "Tianliang Qi",
      "Shuqiang Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gupta_TOGA_Temporally_Grounded_Open-Ended_Video_QA_with_Weak_Supervision_ICCV_2025_paper.html": {
    "title": "TOGA: Temporally Grounded Open-Ended Video QA with Weak Supervision",
    "volume": "main",
    "abstract": "We address the problem of video question answering (video QA) with temporal grounding in a weakly supervised setup, without any temporal annotations. Given a video and a question, we generate an open-ended answer grounded with the start and end time. For this task, we propose TOGA: a vision-language model for Temporally Grounded Open-Ended Video QA with Weak Supervision. We instruct-tune TOGA to jointly generate the answer and the temporal grounding. We operate in a weakly supervised setup where the temporal grounding annotations are not available.We generate pseudo labels for temporal grounding and ensure the validity of these labels by imposing a consistency constraint between the question of a grounding response and the response generated by a question referring to the same temporal segment. We notice that jointly generating the answers with the grounding improves performance on question answering as well as grounding.We evaluate TOGA on grounded QA and open-ended QA tasks. For grounded QA, we consider the NExT-GQA benchmark which is designed to evaluate weakly supervised grounded open-ended question answering.For open-ended QA, we consider the MSVD-QA and ActivityNet-QA benchmarks. We achieve state-of-the-art performance for both tasks on these benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ayush Gupta",
      "Anirban Roy",
      "Rama Chellappa",
      "Nathaniel D. Bastian",
      "Alvaro Velasquez",
      "Susmit Jha"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Multi-Object_Sketch_Animation_by_Scene_Decomposition_and_Motion_Planning_ICCV_2025_paper.html": {
    "title": "Multi-Object Sketch Animation by Scene Decomposition and Motion Planning",
    "volume": "main",
    "abstract": "Sketch animation, which brings static sketches to life by generating dynamic video sequences, has found widespread applications in GIF design, cartoon production, and daily entertainment. While current methods for sketch animation perform well in single-object sketch animation, they struggle in multi-object scenarios. By analyzing their failures, we identify two major challenges of transitioning from single-object to multi-object sketch animation: object-aware motion modeling and complex motion optimization. For multi-object sketch animation, we propose MoSketch based on iterative optimization through Score Distillation Sampling (SDS) and thus animating a multi-object sketch in a training-data free manner. To tackle the two challenges in a divide-and-conquer strategy, MoSketch has four novel modules, i.e., LLM-based scene decomposition, LLM-based motion planning, multi-grained motion refinement, and compositional SDS. Extensive qualitative and quantitative experiments demonstrate the superiority of our method over existing sketch animation approaches. MoSketch takes a pioneering step towards multi-object sketch animation, opening new avenues for future research and applications",
    "checked": true,
    "id": "90b3bf5e2f9daa0652a3fe4ce537123ea287d4e8",
    "semantic_title": "multi-object sketch animation by scene decomposition and motion planning",
    "citation_count": 2,
    "authors": [
      "Jingyu Liu",
      "Zijie Xin",
      "Yuhan Fu",
      "Ruixiang Zhao",
      "Bangxiang Lan",
      "Xirong Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xie_MUSE-VL_Modeling_Unified_VLM_through_Semantic_Discrete_Encoding_ICCV_2025_paper.html": {
    "title": "MUSE-VL: Modeling Unified VLM through Semantic Discrete Encoding",
    "volume": "main",
    "abstract": "We introduce MUSE-VL, a Unified Vision-Language Model through Semantic discrete Encoding for multimodal understanding and generation. Recently, the research community has begun exploring unified models for visual generation and understanding. However, existing vision tokenizers (e.g., VQGAN) only consider low-level information, which makes it difficult to align with language tokens. This results in high training complexity and necessitates a large amount of training data to achieve optimal performance.Additionally, their performance is still far from dedicated understanding models. This paper proposes Semantic Discrete Encoding (SDE), which effectively aligns the information of visual tokens and language tokens by adding semantic constraints to the visual tokenizer. This greatly reduces the amount of training data and improves the performance of the unified model. With the same LLM size, our method improved the understanding performance by 4.8% compared to the previous SOTA Emu3 and surpassed the dedicated understanding model LLaVA-NeXT 34B by 3.7%. For visual generation, our model achieves a FID score of 7.73 on MJHQ-30k, surpassing the existing unified models",
    "checked": true,
    "id": "6d2a2e27bd6d4b72ff92f5dcf5ca46ed2baf68b0",
    "semantic_title": "muse-vl: modeling unified vlm through semantic discrete encoding",
    "citation_count": 19,
    "authors": [
      "Rongchang Xie",
      "Chen Du",
      "Ping Song",
      "Chang Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Patsch_MistSense_Versatile_Online_Detection_of_Procedural_and_Execution_Mistakes_ICCV_2025_paper.html": {
    "title": "MistSense: Versatile Online Detection of Procedural and Execution Mistakes",
    "volume": "main",
    "abstract": "Online mistake detection is crucial across various domains, ranging from industrial automation to educational applications, as mistakes can be corrected by the human operator after their detection due to the continuous inference on a video stream. While prior research mainly addresses procedural errors that often relate to temporal and ordering information, identifying a broader range of error types is essential for real-world implementation. In this work, we present MistSense, an approach for online mistake identification that includes this versatility by considering both procedural errors, which involve incorrect action sequences, and execution errors, such as motor inaccuracies or improper equipment use. Our method integrates RGB and hand pose features to capture fine-grained contextual cues in order to detect a mistake. By jointly modeling spatial and sequential aspects of human actions, our framework enables robust and adaptive error detection in dynamic environments. Once a mistake has been detected, we leverage a large language model (LLM) which provides an error explanation that gives the user further insights into why an action has been identified as a mistake. The evaluation on common mistake detection benchmarks shows the effectiveness of our approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Constantin Patsch",
      "Yuankai Wu",
      "Marsil Zakour",
      "Driton Salihu",
      "Eckehard Steinbach"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ying_FusionPhys_A_Flexible_Framework_for_Fusing_Complementary_Sensing_Modalities_in_ICCV_2025_paper.html": {
    "title": "FusionPhys: A Flexible Framework for Fusing Complementary Sensing Modalities in Remote Physiological Measurement",
    "volume": "main",
    "abstract": "Remote physiological measurement using visible light cameras has emerged as a powerful tool for non-contact health monitoring, yet its reliability degrades under challenging conditions such as low-light environments or diverse skin tones. These limitations have motivated the exploration of alternative sensing modalities, such as near-infrared sensors and radar systems, which offer complementary physiological information through distinct sensing principles. While these modalities provide valuable information, existing methods fail to holistically integrate these heterogeneous data. Our key insight is that while visible light, near-infrared, and radar operate on distinct physical principles, they all capture temporally dynamic physiological signatures that can be represented as time-varying signals reflecting underlying physiological processes. Based on this insight, we propose FusionPhys, a novel framework that implements an adaptive integration mechanism to refine physiological information across complementary modalities. We further introduce a sub-modality decomposition technique that extends fusion principles to single-modality videos. Extensive experiments across five datasets demonstrate that FusionPhys achieves superior performance in diverse sensing configurations, representing a significant advancement toward more reliable and versatile remote physiological measurement systems. The code is available at: https://github.com/chh-ying/fusionphys",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenhang Ying",
      "Huiyu Yang",
      "Jieyi Ge",
      "Zhaodong Sun",
      "Xu Cheng",
      "Kui Ren",
      "Xiaobai Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_Task_Vector_Quantization_for_Memory-Efficient_Model_Merging_ICCV_2025_paper.html": {
    "title": "Task Vector Quantization for Memory-Efficient Model Merging",
    "volume": "main",
    "abstract": "Model merging enables efficient multi-task models by combining task-specific fine-tuned checkpoints. However, storing multiple task-specific checkpoints requires significant memory, limiting scalability and restricting model merging to larger models and diverse tasks. In this paper, we propose quantizing task vectors (i.e., the difference between pre-trained and fine-tuned checkpoints) instead of quantizing fine-tuned checkpoints. We observe that task vectors exhibit a narrow weight range, enabling low-precision quantization (<= 4 bit) within existing task vector merging frameworks. To further mitigate quantization errors within ultra-low bit precision (e.g., 2 bit), we introduce Residual Task Vector Quantization, which decomposes the task vector into a base vector and offset component. We allocate bits based on quantization sensitivity, ensuring precision while minimizing error within a memory budget. Experiments on image classification and dense prediction show our method maintains or improves model merging performance while using only 8% of the memory required for full-precision checkpoints. Our code is available at https://aim-skku.github.io/TVQ/",
    "checked": true,
    "id": "e0d47f0b715f50bb775e4dfd275e1602e7ff6038",
    "semantic_title": "task vector quantization for memory-efficient model merging",
    "citation_count": 2,
    "authors": [
      "Youngeun Kim",
      "Seunghwan Lee",
      "Aecheon Jung",
      "Bogon Ryu",
      "Sungeun Hong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Dirichlet-Constrained_Variational_Codebook_Learning_for_Temporally_Coherent_Video_Face_Restoration_ICCV_2025_paper.html": {
    "title": "Dirichlet-Constrained Variational Codebook Learning for Temporally Coherent Video Face Restoration",
    "volume": "main",
    "abstract": "Video face restoration faces a critical challenge in maintaining temporal consistency while recovering fine facial details from degraded inputs. This paper presents a novel approach that extends Vector-Quantized Variational Autoencoders (VQ-VAEs), pretrained on static high-quality portraits, into a video restoration framework through variational latent space modeling. Our key innovation lies in reformulating discrete codebook representations as Dirichlet-distributed continuous variables, enabling probabilistic transitions between facial features across frames. A spatio-temporal Transformer architecture jointly models inter-frame dependencies and predicts latent distributions, while a Laplacian-constrained reconstruction loss combined with perceptual (LPIPS) regularization enhances both pixel accuracy and visual quality. Comprehensive evaluations on blind face restoration, video inpainting, and facial colorization tasks demonstrate state-of-the-art performance. This work establishes an effective paradigm for adapting intensive image priors, pretrained on high-quality images, to video restoration while addressing the critical challenge of flicker artifacts. The source code has been open-sourced and is available at https://github.com/fudan-generative-vision/DicFace",
    "checked": false,
    "id": "e7e575917a59cff8fb94307c5f085904a7ce2490",
    "semantic_title": "dicface: dirichlet-constrained variational codebook learning for temporally coherent video face restoration",
    "citation_count": 0,
    "authors": [
      "Baoyou Chen",
      "Ce Liu",
      "Weihao Yuan",
      "Zilong Dong",
      "Siyu Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lv_Dual-Expert_Consistency_Model_for_Efficient_and_High-Quality_Video_Generation_ICCV_2025_paper.html": {
    "title": "Dual-Expert Consistency Model for Efficient and High-Quality Video Generation",
    "volume": "main",
    "abstract": "Diffusion Models have achieved remarkable results in video synthesis but require iterative denoising steps, leading to substantial computational overhead. Consistency Models have made significant progress in accelerating diffusion models. However, directly applying them to video diffusion models often results in severe degradation of temporal consistency and appearance details. In this paper, by analyzing the training dynamics of Consistency Models, we identify a key conflicting learning dynamics during the distillation process: there is a significant discrepancy in the optimization gradients and loss contributions across different timesteps. This discrepancy prevents the distilled student model from achieving an optimal state, leading to compromised temporal consistency and degraded appearance details.To address this issue, we propose a parameter-efficient Dual-Expert Consistency Model (DCM), where a semantic expert focuses on learning semantic layout and motion, while a detail expert specializes in fine detail refinement. Furthermore, we introduce Temporal Coherence Loss to improve motion consistency for the semantic expert and apply GAN and Feature Matching Loss to enhance the synthesis quality of the detail expert. Our approach achieves state-of-the-art visual quality with significantly reduced sampling steps, demonstrating the effectiveness of expert specialization in video diffusion model distillation. Our code and models will be made publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengyao Lv",
      "Chenyang Si",
      "Tianlin Pan",
      "Zhaoxi Chen",
      "Kwan-Yee K. Wong",
      "Yu Qiao",
      "Ziwei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jin_GeoFormer_Geometry_Point_Encoder_for_3D_Object_Detection_with_Graph-based_ICCV_2025_paper.html": {
    "title": "GeoFormer: Geometry Point Encoder for 3D Object Detection with Graph-based Transformer",
    "volume": "main",
    "abstract": "Lidar-based 3D detection is one of the most popular research fields in autonomous driving. 3D detectors typically detect specific targets in a scene according to the pattern formed by the spatial distribution of point clouds. However, existing voxel-based methods usually adopt MLP and global pooling (e.g., PointNet, CenterPoint) as voxel feature encoder, which makes it less effective to extract detailed spatial structure information from raw points, leading to information loss and inferior performance. In this paper, we propose a novel graph-based transformer to encode voxel features by condensing the full and detailed point's geometry, termed as GeoFormer. We first represent points within a voxel as a graph, based on relative distances to capture its spatial geometry. Then, We introduce a geometry-guided transformer architecture to encode voxel features, where the adjacent geometric clues are used to re-weight point feature similarities, enabling more effective extraction of geometric relationships between point pairs at varying distances. We highlight that GeoFormer is a plug-and-play module which can be seamlessly integrated to enhance the performance of existing voxel-based detectors. Extensive experiments conducted on three popular outdoor datasets demonstrate that our GeoFormer achieves the start-of-the-art performance on both effectiveness and robustness comparisons",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Jin",
      "Haisheng Su",
      "Cong Ma",
      "Kai Liu",
      "Wei Wu",
      "Fei Hui",
      "Junchi Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ge_Unraveling_the_Effects_of_Synthetic_Data_on_End-to-End_Autonomous_Driving_ICCV_2025_paper.html": {
    "title": "Unraveling the Effects of Synthetic Data on End-to-End Autonomous Driving",
    "volume": "main",
    "abstract": "End-to-end (E2E) autonomous driving (AD) models require diverse, high-quality data to perform well across various driving scenarios. However, collecting large-scale real-world data is expensive and time-consuming, making high-fidelity synthetic data essential for enhancing data diversity and model robustness. Existing driving simulators for synthetic data generation have significant limitations: game-engine-based simulators struggle to produce realistic sensor data, while NeRF-based and diffusion-based methods face efficiency challenges. Additionally, recent simulators designed for closed-loop evaluation provide limited interaction with other vehicles, failing to simulate complex real-world traffic dynamics. To address these issues, we introduce SceneCrafter, a realistic, interactive, and efficient AD simulator based on 3D Gaussian Splatting (3DGS). SceneCrafter not only efficiently generates realistic driving logs across diverse traffic scenarios but also enables robust closed-loop evaluation of end-to-end models. Experimental results demonstrate that SceneCrafter serves as both a reliable evaluation platform and a efficient data generator that significantly improves end-to-end model generalization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junhao Ge",
      "Zuhong Liu",
      "Longteng Fan",
      "Yifan Jiang",
      "Jiaqi Su",
      "Yiming Li",
      "Zhejun Zhang",
      "Siheng Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zheng_Dense2MoE_Restructuring_Diffusion_Transformer_to_MoE_for_Efficient_Text-to-Image_Generation_ICCV_2025_paper.html": {
    "title": "Dense2MoE: Restructuring Diffusion Transformer to MoE for Efficient Text-to-Image Generation",
    "volume": "main",
    "abstract": "Diffusion Transformer (DiT) has demonstrated remarkable performance in text-to-image generation; however, its large parameter size results in substantial inference overhead. Existing parameter compression methods primarily focus on pruning, but aggressive pruning often leads to severe performance degradation due to reduced model capacity. To address this limitation, we pioneer the transformation of a dense DiT into a Mixture of Experts (MoE) for structured sparsification, reducing the number of activated parameters while preserving model capacity. Specifically, we replace the Feed-Forward Networks (FFNs) in DiT Blocks with MoE layers, reducing the number of activated parameters in the FFNs by 62.5%.Furthermore, we propose the Mixture of Blocks (MoB) to selectively activate DiT blocks, thereby further enhancing sparsity.To ensure an effective dense-to-MoE conversion, we design a multi-step distillation pipeline, incorporating Taylor metric-based expert initialization, knowledge distillation with load balancing, and group feature loss for MoB optimization. We transform large diffusion transformers (e.g., FLUX.1 [dev]) into an MoE structure, reducing activated parameters by 60% while maintaining original performance and surpassing pruning-based approaches in extensive experiments. Overall, Dense2MoE establishes a new paradigm for efficient text-to-image generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youwei Zheng",
      "Yuxi Ren",
      "Xin Xia",
      "Xuefeng Xiao",
      "Xiaohua Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_KinMo_Kinematic-aware_Human_Motion_Understanding_and_Generation_ICCV_2025_paper.html": {
    "title": "KinMo: Kinematic-aware Human Motion Understanding and Generation",
    "volume": "main",
    "abstract": "Current human motion synthesis frameworks rely on global action descriptions, creating a modality gap that limits both motion understanding and generation capabilities. A single coarse description, such as \"run\", fails to capture essential details like variations in speed, limb positioning, and kinematic dynamics, leading to significant ambiguities between text and motion modalities. To address this challenge, we introduce KinMo, a unified framework built on a hierarchical describable motion representation that extends beyond global action by incorporating kinematic group movements and their interactions.We design an automated annotation pipeline to generate high-quality, fine-grained descriptions for this decomposition, resulting in the KinMo dataset. To leverage these structured descriptions, we propose Hierarchical Text-Motion Alignment, improving spatial understanding by integrating additional motion details. Furthermore, we introduce a coarse-to-fine generation procedure to demonstrate how enhanced spatial understanding benefits motion synthesis. Experimental results show that KinMo significantly improves motion understanding, demonstrated by enhanced text-motion retrieval performance and enabling more fine-grained motion generation and editing capabilities. Project Page: https://andypinxinliu.github.io/KinMo",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengfei Zhang",
      "Pinxin Liu",
      "Pablo Garrido",
      "Hyeongwoo Kim",
      "Bindita Chaudhuri"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_NeuFrameQ_Neural_Frame_Fields_for_Scalable_and_Generalizable_Anisotropic_Quadrangulation_ICCV_2025_paper.html": {
    "title": "NeuFrameQ: Neural Frame Fields for Scalable and Generalizable Anisotropic Quadrangulation",
    "volume": "main",
    "abstract": "Quad meshes play a crucial role in computer graphics applications, yet automatically generating high-quality quad meshes remains challenging. Traditional quadrangulation approaches rely on local geometric features and manual constraints, often producing suboptimal mesh layouts that fail to capture global shape semantics. We introduce NeuFrameQ, a novel learning-based framework for scalable and generalizable mesh quadrangulation via frame field prediction. We first create a large-scale dataset of high-quality quad meshes with various shapes to serve as priors of domain knowledge. Empowered by this dataset, we employ a connectivity-agnostic learning approach that operates on point clouds with normals, enabling robust processing of complex geometries. By decomposing frame field prediction into direction regression and magnitude estimation tasks, we effectively handle the ill-posed nature in frame field estimation. We also employ the polyvector representation and attention mechanism in both tasks to handle the inherent ambiguities in frame field representation. Extensive experiments demonstrate that NeuFrameQ produces high-quality quad meshes with superior semantic alignment, also for geometries derived from neural fields. Our method significantly advances the state of the art in automatic quad mesh generation, bridging the gap between neural content creation and production-ready geometric assets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ying-Tian Liu",
      "Jiajun Li",
      "Yu-Tao Liu",
      "Xin Yu",
      "Yuan-Chen Guo",
      "Yan-Pei Cao",
      "Ding Liang",
      "Ariel Shamir",
      "Song-Hai Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Keyframe-oriented_Vision_Token_Pruning_Enhancing_Efficiency_of_Large_Vision_Language_ICCV_2025_paper.html": {
    "title": "Keyframe-oriented Vision Token Pruning: Enhancing Efficiency of Large Vision Language Models on Long-Form Video Processing",
    "volume": "main",
    "abstract": "Vision language models (VLMs) demonstrate strong capabilities in jointly processing visual and textual data. However, they often incur substantial computational overhead due to redundant visual information, particularly in long-form video scenarios. Existing approaches predominantly focus on either vision token pruning, which may overlook spatio-temporal dependencies, or keyframe selection, which identifies informative frames but discards others, thus disrupting contextual continuity. In this work, we propose KVTP (Keyframe-oriented Vision Token Pruning), a novel framework that overcomes the drawbacks of token pruning and keyframe selection. By adaptively assigning pruning rates based on frame relevance to the query, KVTP effectively retains essential contextual information while significantly reducing redundant computation. To thoroughly evaluate the long-form video understanding capacities of VLMs, we curated and reorganized subsets from VideoMME, EgoSchema, and NextQA into a unified benchmark named SparseKV-QA that highlights real-world scenarios with sparse but crucial events. Our experiments with VLMs of various scales show that KVTP can reduce token usage by 80% without compromising spatiotemporal and contextual consistency, significantly cutting computation while maintaining the performance. These results demonstrate our approach's effectiveness in efficient long-video processing, facilitating more scalable VLM deployment",
    "checked": true,
    "id": "f19a8fd4dd063597c78ad362614236ea20dbb6f2",
    "semantic_title": "keyframe-oriented vision token pruning: enhancing efficiency of large vision language models on long-form video processing",
    "citation_count": 4,
    "authors": [
      "Yudong Liu",
      "Jingwei Sun",
      "Yueqian Lin",
      "Jianyi Zhang",
      "Jingyang Zhang",
      "Ming Yin",
      "Qinsi Wang",
      "Hai Li",
      "Yiran Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lin_LongSplat_Robust_Unposed_3D_Gaussian_Splatting_for_Casual_Long_Videos_ICCV_2025_paper.html": {
    "title": "LongSplat: Robust Unposed 3D Gaussian Splatting for Casual Long Videos",
    "volume": "main",
    "abstract": "LongSplat addresses critical challenges in novel view synthesis (NVS) from casually captured long videos characterized by irregular camera motion, unknown camera poses, and expansive scenes. Current methods often suffer from pose drift, inaccurate geometry initialization, and severe memory limitations. To address these issues, we introduce LongSplat, a robust unposed 3D Gaussian Splatting framework featuring: (1) Incremental Joint Optimization that concurrently optimizes camera poses and 3D Gaussians to avoid local minima and ensure global consistency; (2) a robust Pose Estimation Module leveraging learned 3D priors; and (3) an efficient Octree Anchor Formation mechanism that converts dense point clouds into anchors based on spatial density. Extensive experiments on challenging benchmarks demonstrate that LongSplat achieves state-of-the-art results, substantially improving rendering quality, pose accuracy, and computational efficiency compared to prior approaches. Project page: https://linjohnss.github.io/longsplat/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chin-Yang Lin",
      "Cheng Sun",
      "Fu-En Yang",
      "Min-Hung Chen",
      "Yen-Yu Lin",
      "Yu-Lun Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zeng_AVAM_a_Universal_Training-free_Adaptive_Visual_Anchoring_Embedded_into_Multimodal_ICCV_2025_paper.html": {
    "title": "AVAM: a Universal Training-free Adaptive Visual Anchoring Embedded into Multimodal Large Language Model for Multi-image Question Answering",
    "volume": "main",
    "abstract": "The advancement of Multimodal Large Language Models (MLLMs) has driven significant progress in Visual Question Answering (VQA), evolving from Single to Multi Image VQA (MVQA). However, the increased number of images in MVQA inevitably introduces substantial visual redundancy that is irrelevant to question answering, negatively impacting both accuracy and efficiency. To address this issue, existing methods lack flexibility in controlling the number of compressed visual tokens and tend to produce discrete visual fragments, which hinder MLLMs' ability to comprehend images holistically. In this paper, we propose a straightforward yet universal Adaptive Visual Anchoring strategy, which can be seamlessly integrated into existing MLLMs, offering significant accuracy improvements through adaptive compression. Meanwhile, to balance the results derived from both global and compressed visual input, we further introduce a novel collaborative decoding mechanism, enabling optimal performance. Extensive experiments validate the effectiveness of our method, demonstrating consistent performance improvements across various MLLMs. The code will be publicly available",
    "checked": false,
    "id": "85043e449d84f80fd158dc4024582ae26efa1933",
    "semantic_title": "avam: universal training-free adaptive visual anchoring embedded into multimodal large language model for multi-image question answering",
    "citation_count": 0,
    "authors": [
      "Kang Zeng",
      "Guojin Zhong",
      "Jintao Cheng",
      "Jin Yuan",
      "Zhiyong Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_WikiAutoGen_Towards_Multi-Modal_Wikipedia-Style_Article_Generation_ICCV_2025_paper.html": {
    "title": "WikiAutoGen: Towards Multi-Modal Wikipedia-Style Article Generation",
    "volume": "main",
    "abstract": "Knowledge discovery and collection are intelligence-intensive tasks that traditionally require significant human effort to ensure high-quality outputs. Recent research has explored multi-agent frameworks for automating Wikipedia-style article generation by retrieving and synthesizing information from the internet. However, these methods primarily focus on text-only generation, overlooking the importance of multimodal content in enhancing informativeness and engagement. In this work, we introduce WikiAutoGen, a novel system for automated multimodal Wikipedia-style article generation. Unlike prior approaches, WikiAutoGen retrieves and integrates relevant images alongside text, enriching both the depth and visual appeal of generated content. To further improve factual accuracy and comprehensiveness, we propose a multi-perspective self-reflection mechanism, which critically assesses retrieved content from diverse viewpoints to enhance reliability, breadth, and coherence, etc. Additionally, we introduce WikiSeek, a benchmark comprising Wikipedia articles with topics paired with both textual and image-based representations, designed to evaluate multimodal knowledge generation on more challenging topics. Experimental results show that WikiAutoGen outperforms previous methods by 8%-29% on our WikiSeek benchmark, producing more accurate, coherent, and visually enriched Wikipedia-style articles. We show some of our generated examples in https://anonymous.4open.science/r/WikiAutoGen-C3C4",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongyu Yang",
      "Jun Chen",
      "Dannong Xu",
      "Junjie Fei",
      "Xiaoqian Shen",
      "Liangbing Zhao",
      "Chun-Mei Feng",
      "Mohamed Elhoseiny"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zheng_GMMamba_Group_Masking_Mamba_for_Whole_Slide_Image_Classification_ICCV_2025_paper.html": {
    "title": "GMMamba: Group Masking Mamba for Whole Slide Image Classification",
    "volume": "main",
    "abstract": "Recent advances in selective state space models (Mamba) have shown great promise in whole slide image (WSI) classification. Despite this, WSIs contain explicit local redundancy (similar patches) and irrelevant regions (uninformative instances), posing significant challenges for Mamba-based multi-instance learning (MIL) methods in capturing global representations. Furthermore, bag-level approaches struggle to extract critical features from all instances, while group-level methods fail to adequately account for tumor dispersion and intrinsic correlations across groups, leading to suboptimal global representations. To address these issues, we propose group masking Mamba (GMMamba), a novel framework that combines two elaborate modules: (1) intra-group masking Mamba (IMM) for selective instance exploration within groups, and (2) cross-group super-feature sampling (CSS) to ameliorate long-range relation learning. Specifically, IMM adaptively predicts sparse masks to filter out features with low attention scores (i.e., uninformative patterns) during bidirectional Mamba modeling, facilitating the removal of instance redundancies for compact local representation. For improved bag prediction, the CSS module further aggregates sparse group representations into discriminative features, effectively grasping comprehensive dependencies among dispersed and sparse tumor regions inherent in large-scale WSIs. Extensive experiments on four datasets demonstrate that GMMamba outperforms the state-of-the-art ACMIL by 2.2% and 6.4% in accuracy on the TCGA-BRCA and TCGA-ESCA datasets, respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tingting Zheng",
      "Hongxun Yao",
      "Kui Jiang",
      "Yi Xiao",
      "Sicheng Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zheng_S3R-GS_Streamlining_the_Pipeline_for_Large-Scale_Street_Scene_Reconstruction_ICCV_2025_paper.html": {
    "title": "S3R-GS: Streamlining the Pipeline for Large-Scale Street Scene Reconstruction",
    "volume": "main",
    "abstract": "Recently, 3D Gaussian Splatting (3DGS) has reshaped the field of photorealistic 3D reconstruction, achieving impressive rendering quality and speed. However, when applied to large-scale street scenes, existing methods suffer from rapidly escalating per-viewpoint reconstruction costs as scene size increases, leading to significant computational overhead.After revisiting the conventional pipeline, we identify three key factors accounting for this issue: unnecessary local-to-global transformations, excessive 3D-to-2D projections, and inefficient rendering of distant content. To address these challenges, we propose S3R-GS, a 3DGS framework that Streamlines the pipeline for large-scale Street Scene Reconstruction, effectively mitigating these limitations. Moreover, most existing street 3DGS methods rely on ground-truth 3D bounding boxes to separate dynamic and static components, but 3D bounding boxes are difficult to obtain, limiting real-world applicability. To address this, we propose an alternative solution with 2D boxes, which are easier to annotate or can be predicted by off-the-shelf vision foundation models. Such designs together make S3R-GS readily adapt to large, in-the-wild scenarios.Extensive experiments demonstrate that S3R-GS enhances rendering quality and significantly accelerates reconstruction. Remarkably, when applied to videos from the challenging Argoverse2 dataset, it achieves state-of-the-art PSNR and SSIM, reducing reconstruction time to below 50%--and even 20%--of competing methods. Code is available at https://github.com/Tom-zgt/S3R-GS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangting Zheng",
      "Jiajun Deng",
      "Xiaomeng Chu",
      "Yu Yuan",
      "Houqiang Li",
      "Yanyong Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_LIRA_Reasoning_Reconstruction_via_Multimodal_Large_Language_Models_ICCV_2025_paper.html": {
    "title": "LIRA: Reasoning Reconstruction via Multimodal Large Language Models",
    "volume": "main",
    "abstract": "Existing language instruction-guided online 3D reconstruction systems mainly rely on explicit instructions or queryable maps, showing inadequate capability to handle implicit and complex instructions. In this paper, we first introduce a reasoning reconstruction task. This task inputs an implicit instruction involving complex reasoning and an RGB-D sequence, and outputs incremental 3D reconstruction of instances that conform to the instruction. To handle this task, we propose LIRA: Language Instructed Reconstruction Assistant. It leverages a multimodal large language model to actively reason about the implicit instruction and obtain instruction-relevant 2D candidate instances and their attributes. Then, candidate instances are back-projected into the incrementally reconstructed 3D geometric map, followed by instance fusion and target instance inference. In LIRA, to achieve higher instance fusion quality, we propose TIFF, a Text-enhanced Instance Fusion module operating within Fragment bounding volume, which is learning-based and fuses multiple keyframes simultaneously. Since the evaluation system for this task is not well established, we propose a benchmark ReasonRecon comprising the largest collection of scene-instruction data samples involving implicit reasoning. Experiments demonstrate that LIRA outperforms existing methods in the reasoning reconstruction task and is capable of running in real time. Code and benchmark are available at https://github.com/zhen6618/LIRA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Zhou",
      "Tong Wang",
      "Yunkai Ma",
      "Xiao Tan",
      "Fengshui Jing"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Saha_Generate_Transduct_Adapt_Iterative_Transduction_with_VLMs_ICCV_2025_paper.html": {
    "title": "Generate, Transduct, Adapt: Iterative Transduction with VLMs",
    "volume": "main",
    "abstract": "Transductive zero-shot learning with vision-language models leverages image-image similarities within the dataset to achieve better classification accuracy compared to the inductive setting. However, there is little work that explores the structure of the language space in this context. We propose GTA-CLIP, a novel technique that incorporates supervision from language models for joint transduction in language and vision spaces. Our approach is iterative and consists of three steps: (i) incrementally exploring the attribute space by querying language models, (ii) an attribute-augmented transductive inference procedure, and (iii) fine-tuning the language and vision encoders based on inferred labels within the dataset. Through experiments with CLIP encoders, we demonstrate that GTA-CLIP, yields an average performance improvement of 9.5% and 4.0% across 12 datasets and 3 encoders, over CLIP and transductive CLIP respectively in the zero-shot setting. We also observe similar improvements in a few-shot setting. We present ablation studies that demonstrate the value of each step and visualize how the vision and language spaces evolve over iterations driven by the transductive learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oindrila Saha",
      "Logan Lawrence",
      "Grant Van Horn",
      "Subhransu Maji"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xu_Beyond_One_Shot_Beyond_One_Perspective_Cross-View_and_Long-Horizon_Distillation_ICCV_2025_paper.html": {
    "title": "Beyond One Shot, Beyond One Perspective: Cross-View and Long-Horizon Distillation for Better LiDAR Representations",
    "volume": "main",
    "abstract": "LiDAR representation learning aims to extract rich structural and semantic information from large-scale, readily available datasets, reducing reliance on costly human annotations. However, existing LiDAR representation strategies often overlook the inherent spatiotemporal cues in LiDAR sequences, limiting their effectiveness. In this work, we propose LiMA, a novel long-term image-to-LiDAR Memory Aggregation framework that explicitly captures longer range temporal correlations to enhance LiDAR representation learning. LiMA comprises three key components: 1) a Cross-View Aggregation module that aligns and fuses overlapping regions across neighboring camera views, constructing a more unified and redundancy-free memory bank; 2) a Long-Term Feature Propagation mechanism that efficiently aligns and integrates multi-frame image features, reinforcing temporal coherence during LiDAR representation learning; and 3) a Cross-Sequence Memory Alignment strategy that enforces consistency across driving sequences, improving generalization to unseen environments. LiMA maintains high pretraining efficiency and incurs no additional computational overhead during downstream tasks. Extensive experiments on mainstream LiDAR-based perception benchmarks demonstrate that LiMA significantly improves both LiDAR semantic segmentation and 3D object detection. We hope this work inspires more effective pretraining paradigms for autonomous driving. The code has been made publicly accessible for future research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Xu",
      "Lingdong Kong",
      "Song Wang",
      "Chuanwei Zhou",
      "Qingshan Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Fan_Test-Time_Retrieval-Augmented_Adaptation_for_Vision-Language_Models_ICCV_2025_paper.html": {
    "title": "Test-Time Retrieval-Augmented Adaptation for Vision-Language Models",
    "volume": "main",
    "abstract": "Vision-language models (VLMs) have shown promise in test-time adaptation tasks due to their remarkable capabilities in understanding and reasoning about visual content through natural language descriptions. However, training VLMs typically demands substantial computational resources, and they often struggle to adapt efficiently to new domains or tasks. Additionally, dynamically estimating the test distribution from streaming data at test time remains a significant challenge. In this work, we propose a novel test-time retrieval-augmented adaptation (TT-RAA) method that enables VLMs to maintain high performance across diverse visual recognition tasks without the need for task-specific training or large computational overhead. During inference, TT-RAA employs a streaming mixture of Gaussian database (SMGD) to continuously estimate test distributions, requiring minimal storage. Then, TT-RAA retrieves the most relevant information from the SMGD, enhancing the original VLM outputs. A key limitation of CLIP-based VLMs is their inter-modal vision-language optimization, which does not optimize vision-space similarity, leading to larger intra-modal variance. To address this, we propose a multimodal retrieval augmentation module that transforms the SMGD into a unified multimodal space, enabling retrieval that aligns both vision and language modalities. Extensive experiments across both cross-domain and out-of-distribution benchmarks comprising fourteen datasets demonstrate TT-RAA's superior performance compared to state-of-the-art methods. Ablation studies and hyperparameter analyses further validate the effectiveness of the proposed modules. The source code of our work is available at https://github.com/xinqi-fan/TT-RAA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinqi Fan",
      "Xueli Chen",
      "Luoxiao Yang",
      "Chuin Hong Yap",
      "Rizwan Qureshi",
      "Qi Dou",
      "Moi Hoon Yap",
      "Mubarak Shah"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jiang_Causal_Disentanglement_and_Cross-Modal_Alignment_for_Enhanced_Few-Shot_Learning_ICCV_2025_paper.html": {
    "title": "Causal Disentanglement and Cross-Modal Alignment for Enhanced Few-Shot Learning",
    "volume": "main",
    "abstract": "Few-shot learning (FSL) often requires effective adaptation of models using limited labeled data. However, most existing FSL methods rely on entangled representations, requiring the model to implicitly recover the unmixing process to obtain disentangled representations using only limited supervision, which hinders effective adaptation. Recent theoretical studies show that multimodal contrastive learning methods, such as CLIP, can disentangle latent representations up to linear transformations. In light of this, we propose the Causal CLIP Adapter (CCA), a novel framework that explicitly disentangles visual features extracted from CLIP using unsupervised Independent Component Analysis (ICA). This removes the need to learn the unmixing process from the labeled data, thereby reducing the number of trainable parameters and mitigating overfitting. Taking a step further, while ICA can obtain visual disentangled representations, it may also disrupt CLIP's intra- and inter-modal alignment. To counteract this, CCA further leverages CLIP's inherent cross-modal alignment by enhancing it in two ways: unidirectionally, through fine-tuning a CLIP-based text classifier, and bidirectionally, via a cross-attention mechanism that enriches visual and textual representations through mutual interaction. Both unimodal and cross-modal classification outputs can be effectively combined linearly to improve classification accuracy. Extensive experiments on 11 benchmark datasets demonstrate that our method consistently outperforms state-of-the-art approaches in terms of few-shot performance and robustness to distributional shifts, while maintaining computational efficiency. Code will be available at https://github.com/tianjiao-j/CCA",
    "checked": true,
    "id": "9b490ca9b9fd2d22891cfd7adf9f36a3d65331ac",
    "semantic_title": "causal disentanglement and cross-modal alignment for enhanced few-shot learning",
    "citation_count": 0,
    "authors": [
      "Tianjiao Jiang",
      "Zhen Zhang",
      "Yuhang Liu",
      "Javen Qinfeng Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wen_Seeing_and_Seeing_Through_the_Glass_Real_and_Synthetic_Data_ICCV_2025_paper.html": {
    "title": "Seeing and Seeing Through the Glass: Real and Synthetic Data for Multi-Layer Depth Estimation",
    "volume": "main",
    "abstract": "Transparent objects are common in daily life, and understanding their multi-layer depth information--perceiving both the transparent surface and the objects behind it--is crucial for real-world applications that interact with transparent materials.In this paper, we introduce LayeredDepth, the first dataset with multi-layer depth annotations, including a real-world benchmark and a synthetic data generator, to support the task of multi-layer depth estimation. Our real-world benchmark consists of 1,500 images from diverse scenes, and evaluating state-of-the-art depth estimation methods on it reveals that they struggle with transparent objects. The synthetic data generator is fully procedural and capable of providing training data for this task with an unlimited variety of objects and scene compositions. Using this generator, we create a synthetic dataset with 15,300 images. Baseline models training solely on this synthetic dataset produce good cross-domain multi-layer depth estimation. Fine-tuning state-of-the-art single-layer depth models on it substantially improves their performance on transparent objects, with quadruplet accuracy on our benchmark increased from 55.59% to 75.16%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyu Wen",
      "Yiming Zuo",
      "Venkat Subramanian",
      "Patrick Chen",
      "Jia Deng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zeng_GaussianUpdate_Continual_3D_Gaussian_Splatting_Update_for_Changing_Environments_ICCV_2025_paper.html": {
    "title": "GaussianUpdate: Continual 3D Gaussian Splatting Update for Changing Environments",
    "volume": "main",
    "abstract": "Novel view synthesis with neural models has advanced rapidly in recent years, yet adapting these models to scene changes remains an open problem. Existing methods are either labor-intensive, requiring extensive model retraining, or fail to capture detailed types of changes over time. In this paper, we present GaussianUpdate, a novel approach that combines 3D Gaussian representation with continual learning to address these challenges. Our method effectively updates the Gaussian radiance fields with current data while preserving information from past scenes. Unlike existing methods, GaussianUpdate explicitly models different types of changes through a novel multi-stage update strategy. Additionally, we introduce a visibility-aware continual learning approach with generative replay, enabling self-aware updating without the need to store images. The experiments on the benchmark dataset demonstrate our method achieves superior and real-time rendering with the capability of visualizing changes over different times",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lin Zeng",
      "Boming Zhao",
      "Jiarui Hu",
      "Xujie Shen",
      "Ziqiang Dang",
      "Hujun Bao",
      "Zhaopeng Cui"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yao_NavMorph_A_Self-Evolving_World_Model_for_Vision-and-Language_Navigation_in_Continuous_ICCV_2025_paper.html": {
    "title": "NavMorph: A Self-Evolving World Model for Vision-and-Language Navigation in Continuous Environments",
    "volume": "main",
    "abstract": "Vision-and-Language Navigation in Continuous Environments (VLN-CE) requires agents to execute sequential navigation actions in complex environments guided by natural language instructions. Current approaches often struggle with generalizing to novel environments and adapting to ongoing changes during navigation. Inspired by human cognition, we present NavMorph, a self-evolving world model framework that enhances environmental understanding and decision-making in VLN-CE tasks. NavMorph employs compact latent representations to model environmental dynamics, equipping agents with foresight for adaptive planning and policy refinement. By integrating a novel Contextual Evolution Memory, NavMorph leverages scene-contextual information to support effective navigation while maintaining online adaptability. Extensive experiments demonstrate that our method achieves notable performance improvements on popular VLN-CE benchmarks. Code is available in the Supplementary Material",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan Yao",
      "Junyu Gao",
      "Changsheng Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Costanzino_SiM3D_Single-instance_Multiview_Multimodal_and_Multisetup_3D_Anomaly_Detection_Benchmark_ICCV_2025_paper.html": {
    "title": "SiM3D: Single-instance Multiview Multimodal and Multisetup 3D Anomaly Detection Benchmark",
    "volume": "main",
    "abstract": "We propose SiM3D, the first benchmark considering the integration of multiview and multimodal information for comprehensive 3D anomaly detection and segmentation (ADS) where the task is to produce a voxel-based Anomaly Volume. Moreover, SiM3D focuses on a scenario of high interest in manufacturing: single-instance anomaly detection, where only one object, either real or synthetic, is available for training. In this respect, SiM3D stands out as the first ADS benchmark that addresses the challenge of generalizing from synthetic training data to real test data. SiM3D includes a novel multimodal multiview dataset acquired using top-tier industrial sensors and robots. The dataset features multiview high-resolution images (12 \\tt Mpx ) and point clouds (~7M points) for 333 instances of eight types of objects, alongside a CAD model for each type. We also provide manually annotated 3D segmentation GTs for anomalous test samples. To establish reference baselines for the proposed multiview 3D ADS task, we adapt prominent singleview methods and assess their performance using novel metrics that operate on Anomaly Volumes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alex Costanzino",
      "Pierluigi Zama Ramirez",
      "Luigi Lella",
      "Matteo Ragaglia",
      "Alessandro Oliva",
      "Giuseppe Lisanti",
      "Luigi Di Stefano"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Distilling_Diffusion_Models_to_Efficient_3D_LiDAR_Scene_Completion_ICCV_2025_paper.html": {
    "title": "Distilling Diffusion Models to Efficient 3D LiDAR Scene Completion",
    "volume": "main",
    "abstract": "Diffusion models have been applied to 3D LiDAR scene completion due to their strong training stability and high completion quality. However, the slow sampling speed limits the practical application of diffusion-based scene completion models since autonomous vehicles require an efficient perception of surrounding environments. This paper proposes a novel distillation method tailored for 3D Li- DAR scene completion models, dubbed ScoreLiDAR, which achieves efficient yet high-quality scene completion. Score- LiDAR enables the distilled model to sample in significantly fewer steps after distillation. To improve completion quality, we also introduce a novel Structural Loss, which encourages the distilled model to capture the geometric structure of the 3D LiDAR scene. The loss contains a scene-wise term constraining the holistic structure and a point-wise term constraining the key landmark points and their relative configuration. Extensive experiments demonstrate that ScoreLiDAR significantly accelerates the completion time from 30.55 to 5.37 seconds per frame (>5x) on SemanticKITTI and achieves superior performance compared to state-of-the-art 3D LiDAR scene completion models. Our model and code are publicly available on https://github.com/happyw1nd/ScoreLiDAR",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengyuan Zhang",
      "An Zhao",
      "Ling Yang",
      "Zejian Li",
      "Chenye Meng",
      "Haoran Xu",
      "Tianrun Chen",
      "AnYang Wei",
      "Perry Pengyun Gu",
      "Lingyun Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gao_Self-supervised_Learning_of_Hybrid_Part-aware_3D_Representations_of_2D_Gaussians_ICCV_2025_paper.html": {
    "title": "Self-supervised Learning of Hybrid Part-aware 3D Representations of 2D Gaussians and Superquadrics",
    "volume": "main",
    "abstract": "Low-level 3D representations, such as point clouds, meshes, NeRFs and 3D Gaussians, are commonly used for modeling 3D objects and scenes. However, cognitive studies indicate that human perception operates at higher levels and interprets 3D environments by decomposing them into meaningful structural parts, rather than low-level elements like points or voxels. Structured geometric decomposition enhances scene interpretability and facilitates downstream tasks requiring component-level manipulation. In this work, we introduce PartGS, a self-supervised part-aware reconstruction framework that integrates 2D Gaussians and superquadrics to parse objects and scenes into an interpretable decomposition, leveraging multi-view image inputs to uncover 3D structural information. Our method jointly optimizes superquadric meshes and Gaussians by coupling their parameters within a hybrid representation. On one hand, superquadrics enable the representation of a wide range of shape primitives, facilitating flexible and meaningful decompositions. On the other hand, 2D Gaussians capture detailed texture and geometric details, ensuring high-fidelity appearance and geometry reconstruction. Operating in a self-supervised manner, our approach demonstrates superior performance compared to state-of-the-art methods across extensive experiments on the DTU, ShapeNet, and real-world datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhirui Gao",
      "Renjiao Yi",
      "Yuhang Huang",
      "Wei Chen",
      "Chenyang Zhu",
      "Kai Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jeon_Teaching_AI_the_Anatomy_Behind_the_Scan_Addressing_Anatomical_Flaws_ICCV_2025_paper.html": {
    "title": "Teaching AI the Anatomy Behind the Scan: Addressing Anatomical Flaws in Medical Image Segmentation with Learnable Prior",
    "volume": "main",
    "abstract": "Imposing key anatomical features, such as the number of organs, their shapes and relative positions, is crucial for building a robust multi-organ segmentation model. Current attempts to incorporate anatomical features include broadening the effective receptive field (ERF) size with data-intensive modules, or introducing anatomical constraints that scales poorly to multi-organ segmentation. We introduce a novel architecture called the Anatomy-Informed Cascaded Segmentation Network (AIC-Net). AIC-Net incorporates a learnable input termed \"Anatomical Prior\", which can be adapted to patient-specific anatomy using a differentiable spatial deformation. The deformed prior later guides decoder layers towards more anatomy-informed predictions. We repeat this process at a local patch level to enhance the representation of intricate objects, resulting in a cascaded network structure. AIC-Net is a general method that enhances any existing segmentation models to be more anatomy-aware. We have validated the performance of AIC-Net, with various backbones, on three multi-organ segmentation tasks: abdominal organs, vertebrae, and ribs. For each respective task, our benchmarks demonstrate improved dice score and Hausdorff distance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Young Seok Jeon",
      "Hongfei Yang",
      "Huazhu Fu",
      "Mengling Feng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Song_OCK_Unsupervised_Dynamic_Video_Prediction_with_Object-Centric_Kinematics_ICCV_2025_paper.html": {
    "title": "OCK: Unsupervised Dynamic Video Prediction with Object-Centric Kinematics",
    "volume": "main",
    "abstract": "Human perception involves decomposing complex multi-object scenes into time-static object appearance (i.e., size, shape, color) and time-varying object motion (i.e., position, velocity, acceleration). For machines to achieve human-like intelligence in real-world interactions, understanding these physical properties of objects is essential, forming the foundation for dynamic video prediction. While recent advancements in object-centric transformers have demonstrated potential in video prediction, they primarily focus on object appearance, often overlooking motion dynamics, which is crucial for modeling dynamic interactions and maintaining temporal consistency in complex environments. To address these limitations, we propose OCK, a dynamic video prediction model leveraging object-centric kinematics and object slots. We introduce a novel component named Object Kinematics that comprises explicit object motions, serving as an additional attribute beyond conventional appearance features to model dynamic scenes. The Object Kinematics are integrated into various OCK mechanisms, enabling spatiotemporal prediction of complex object interactions over long video sequences. Our model demonstrates superior performance in handling complex scenes with intricate object attributes and motions, highlighting its potential for applicability in vision-related dynamics learning tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yeon-Ji Song",
      "Jaein Kim",
      "Suhyung Choi",
      "Jin-Hwa Kim",
      "Byoung-Tak Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Moon_Holistic_Unlearning_Benchmark_A_Multi-Faceted_Evaluation_for_Text-to-Image_Diffusion_Model_ICCV_2025_paper.html": {
    "title": "Holistic Unlearning Benchmark: A Multi-Faceted Evaluation for Text-to-Image Diffusion Model Unlearning",
    "volume": "main",
    "abstract": "As text-to-image diffusion models gain widespread commercial applications, there are increasing concerns about unethical or harmful use, including the unauthorized generation of copyrighted or sensitive content. Concept unlearning has emerged as a promising solution to these challenges by removing undesired and harmful information from the pre-trained model. However, the previous evaluations primarily focus on whether target concepts are removed while preserving image quality, neglecting the broader impacts such as unintended side effects. In this work, we propose Holistic Unlearning Benchmark (HUB), a comprehensive framework for evaluating unlearning methods across six key dimensions: faithfulness, alignment, pinpoint-ness, multilingual robustness, attack robustness, and efficiency. Our benchmark covers 33 target concepts, including 16,000 prompts per concept, spanning four categories: Celebrity, Style, Intellectual Property, and NSFW. Our investigation reveals that no single method excels across all evaluation criteria. By releasing our evaluation code and dataset, we hope to inspire further research in this area, leading to more reliable and effective unlearning methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saemi Moon",
      "Minjong Lee",
      "Sangdon Park",
      "Dongwoo Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shen_AutoComPose_Automatic_Generation_of_Pose_Transition_Descriptions_for_Composed_Pose_ICCV_2025_paper.html": {
    "title": "AutoComPose: Automatic Generation of Pose Transition Descriptions for Composed Pose Retrieval Using Multimodal LLMs",
    "volume": "main",
    "abstract": "Composed pose retrieval (CPR) enables users to search for human poses by specifying a reference pose and a transition description, but progress in this field is hindered by the scarcity and inconsistency of annotated pose transitions. Existing CPR datasets rely on costly human annotations or heuristic-based rule generation, both of which limit scalability and diversity. In this work, we introduce AutoComPose, the first framework that leverages multimodal large language models (MLLMs) to automatically generate rich and structured pose transition descriptions. Our method enhances annotation quality by structuring transitions into fine-grained body part movements and introducing swapped/mirrored variations, while a cyclic consistency constraint ensures logical coherence between forward and reverse transitions. To advance CPR research, we construct and release two dedicated benchmarks, AIST-CPR and PoseFixCPR, supplementing prior datasets with enhanced attributes. Extensive experiments demonstrate that training retrieval models with AutoComPose yields superior performance over human-annotated and heuristic-based methods, significantly reducing annotation costs while improving retrieval quality. Our work pioneers the automatic annotation of pose transitions, establishing a scalable foundation for future CPR research",
    "checked": true,
    "id": "7ed08e2711678cc2b39c5d103e7d640a45a38028",
    "semantic_title": "autocompose: automatic generation of pose transition descriptions for composed pose retrieval using multimodal llms",
    "citation_count": 0,
    "authors": [
      "Yi-Ting Shen",
      "Sungmin Eum",
      "Doheon Lee",
      "Rohit Shete",
      "Chiao-Yi Wang",
      "Heesung Kwon",
      "Shuvra S. Bhattacharyya"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Duan_WorldScore_A_Unified_Evaluation_Benchmark_for_World_Generation_ICCV_2025_paper.html": {
    "title": "WorldScore: A Unified Evaluation Benchmark for World Generation",
    "volume": "main",
    "abstract": "We introduce the WorldScore benchmark, the first unified benchmark for world generation. We decompose world generation into a sequence of next-scene generation tasks with explicit camera trajectory-based layout specifications, enabling unified evaluation of diverse approaches from 3D and 4D scene generation to video generation models. The WorldScore benchmark encompasses a curated dataset of 3,000 test examples that span diverse worlds: static and dynamic, indoor and outdoor, photorealistic and stylized. The WorldScore metric evaluates generated worlds through three key aspects: controllability, quality, and dynamics. Through extensive evaluation of 20 representative models, including both open-source and closed-source ones, we reveal key insights and challenges for each category of models. Our dataset, evaluation code, and leaderboard can be found at https://haoyi-duan.github.io/WorldScore/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyi Duan",
      "Hong-Xing Yu",
      "Sirui Chen",
      "Li Fei-Fei",
      "Jiajun Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jiang_PhysTwin_Physics-Informed_Reconstruction_and_Simulation_of_Deformable_Objects_from_Videos_ICCV_2025_paper.html": {
    "title": "PhysTwin: Physics-Informed Reconstruction and Simulation of Deformable Objects from Videos",
    "volume": "main",
    "abstract": "Creating a physical digital twin of a real-world object has immense potential in robotics, content creation, and XR. In this paper, we present PhysTwin, a novel framework that uses sparse videos of dynamic objects in interaction to produce a photo- and physically realistic, real-time interactive virtual replica.Our approach centers on two key components: (1) a physics-informed representation that combines spring-mass models for realistic physical simulation, generative shape models for geometry, and Gaussian splats for rendering, and (2) a novel multi-stage optimization-based inverse modeling framework that reconstructs complete geometry, infers dense physical properties, and replicates realistic appearance from videos. Our method integrates an inverse physics framework with visual perception cues, enabling high-fidelity reconstruction even from partial, occluded, and limited viewpoints.PhysTwin supports modeling various deformable objects, including ropes, stuffed animals, cloth, and delivery packages. Experiments show that PhysTwin outperforms competing methods in reconstruction, rendering, future prediction, and simulation under novel interactions. We further demonstrate its applications in interactive real-time simulation and model-based robotic motion planning. Project Page: https://jianghanxiao.github.io/phystwin-web/",
    "checked": true,
    "id": "b05d2376027fb08d43e67b5dd88a07614791e5a5",
    "semantic_title": "phystwin: physics-informed reconstruction and simulation of deformable objects from videos",
    "citation_count": 18,
    "authors": [
      "Hanxiao Jiang",
      "Hao-Yu Hsu",
      "Kaifeng Zhang",
      "Hsin-Ni Yu",
      "Shenlong Wang",
      "Yunzhu Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Physical_Degradation_Model-Guided_Interferometric_Hyperspectral_Reconstruction_with_Unfolding_Transformer_ICCV_2025_paper.html": {
    "title": "Physical Degradation Model-Guided Interferometric Hyperspectral Reconstruction with Unfolding Transformer",
    "volume": "main",
    "abstract": "Interferometric Hyperspectral Imaging (IHI) is a critical technique for large-scale remote sensing tasks due to its advantages in flux and spectral resolution. However, IHI is susceptible to complex errors arising from imaging steps, and its quality is limited by existing signal processing-based reconstruction algorithms. Two key challenges hinder performance enhancement: 1) the lack of training datasets. 2) the difficulty in eliminating IHI-specific degradation components through learning-based methods. To address these challenges, we propose a novel IHI reconstruction pipeline. First, based on imaging physics and radiometric calibration data, we establish a simplified yet accurate IHI degradation model and a parameter estimation method. This model enables the synthesis of realistic IHI training datasets from hyperspectral images (HSIs), bridging the gap between IHI reconstruction and deep learning. Second, we design the Interferometric Hyperspectral Reconstruction Unfolding Transformer (IHRUT), which achieves effective spectral correction and detail restoration through a stripe-pattern enhancement mechanism and a spatial-spectral transformer architecture. Experimental results demonstrate the superior performance and generalization capability of our method. The code and are available at https://github.com/bit1120203554/IHRUT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuansheng Li",
      "Yunhao Zou",
      "Linwei Chen",
      "Ying Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yu_EvaGaussians_Event_Stream_Assisted_Gaussian_Splatting_from_Blurry_Images_ICCV_2025_paper.html": {
    "title": "EvaGaussians: Event Stream Assisted Gaussian Splatting from Blurry Images",
    "volume": "main",
    "abstract": "3D Gaussian Splatting (3D-GS) has demonstrated exceptional capabilities in synthesizing novel views of 3D scenes. However, its training is heavily reliant on high-quality images and precise camera poses. Meeting these criteria can be challenging in non-ideal real-world conditions, where motion-blurred images frequently occur due to high-speed camera movements.To address these challenges, we introduce Event Stream Assisted Gaussian Splatting (EvaGaussians), a novel approach that harnesses event streams captured by event cameras to facilitate the learning of high-quality 3D-GS from motion-blurred images. Capitalizing on the high temporal resolution and dynamic range offered by event streams, we seamlessly integrate them into the initialization and optimization of 3D-GS, thereby enhancing the acquisition of high-fidelity novel views with intricate texture details. We also contribute two novel datasets comprising RGB frames, event streams, and corresponding camera parameters, featuring a wide variety of scenes and various camera motions. The comparison results reveal that our approach not only excels in generating high-fidelity novel views, but also offers improved training and inference efficiency. Video and code are available at the project page",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wangbo Yu",
      "Chaoran Feng",
      "Jianing Li",
      "Jiye Tang",
      "Jiashu Yang",
      "Zhenyu Tang",
      "Meng Cao",
      "Xu Jia",
      "Yuchao Yang",
      "Li Yuan",
      "Yonghong Tian"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hong_RainbowPrompt_Diversity-Enhanced_Prompt-Evolving_for_Continual_Learning_ICCV_2025_paper.html": {
    "title": "RainbowPrompt: Diversity-Enhanced Prompt-Evolving for Continual Learning",
    "volume": "main",
    "abstract": "Prompt-based continual learning provides a rehearsal-free solution by tuning small sets of parameters while keeping pre-trained models frozen. To meet the complex demands of sequential tasks, it is crucial to integrate task-specific knowledge within prompts effectively. However, existing works rely on either fixed learned prompts (i.e., prompts whose representations remain unchanged during new task learning) or on prompts generated from an entangled task-shared space, limiting the representational diversity of the integrated prompt. To address this issue, we propose a novel prompt-evolving mechanism to adaptively aggregate base prompts (i.e., task-specific prompts) into a unified prompt while ensuring diversity. By transforming and aligning base prompts, both previously learned and newly introduced, our approach continuously evolves accumulated knowledge to facilitate learning new tasks. We further introduce a learnable probabilistic gate that adaptively determines which layers to activate during the evolution process. We validate our method on image classification and video action recognition tasks in class-incremental learning, achieving average gains of 9.07% and 7.40% over existing methods across all scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kiseong Hong",
      "Gyeong-hyeon Kim",
      "Eunwoo  Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lin_OmniHuman-1_Rethinking_the_Scaling-Up_of_One-Stage_Conditioned_Human_Animation_Models_ICCV_2025_paper.html": {
    "title": "OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models",
    "volume": "main",
    "abstract": "End-to-end human animation, such as audio-driven talking human generation, has undergone notable advancements in the recent few years. However, existing methods still struggle to scale up as large general video generation models, limiting their potential in real applications. In this paper, we propose OmniHuman, a Diffusion Transformer-based framework that scales up data by mixing motion-related conditions into the training phase. To this end, we introduce two training principles for these mixed conditions, along with the corresponding model architecture and inference strategy. These designs enable OmniHuman to fully leverage data-driven motion generation, ultimately achieving highly realistic human video generation. More importantly, OmniHuman supports various portrait contents (face close-up, portrait, half-body, full-body), supports both talking and singing, handles human-object interactions and challenging body poses, and accommodates different image styles. Compared to existing end-to-end audio-driven methods, OmniHuman not only produces more realistic videos, but also offers greater flexibility in inputs. It also supports multiple driving modalities (audio-driven, video-driven and combined driving signals)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gaojie Lin",
      "Jianwen Jiang",
      "Jiaqi Yang",
      "Zerong Zheng",
      "Chao Liang",
      "Yuan Zhang",
      "Jingtuo Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wen_Principal_Components_Enable_A_New_Language_of_Images_ICCV_2025_paper.html": {
    "title": "Principal Components\" Enable A New Language of Images",
    "volume": "main",
    "abstract": "We introduce a novel visual tokenization framework that embeds a provable PCA-like structure into the latent token space. While existing visual tokenizers primarily optimize for reconstruction fidelity, they often neglect the structural properties of the latent space--a critical factor for both interpretability and downstream tasks. Our method generates a 1D causal token sequence for images, where each successive token contributes non-overlapping information with mathematically guaranteed decreasing explained variance, analogous to principal component analysis. This structural constraint ensures the tokenizer extracts the most salient visual features first, with each subsequent token adding diminishing yet complementary information. Additionally, we identified and resolved a semantic-spectrum coupling effect that causes the unwanted entanglement of high-level semantic content and low-level spectral details in the tokens by leveraging a diffusion decoder. Experiments demonstrate that our approach achieves state-of-the-art reconstruction performance and enables better interpretability to align with the human vision system. Moreover, autoregressive models trained on our token sequences achieve performance comparable to current state-of-the-art methods while requiring fewer tokens for training and inference",
    "checked": true,
    "id": "a5c1a7d76a957ab750e0eb708966d18a7385baa1",
    "semantic_title": "principal components\" enable a new language of images",
    "citation_count": 3,
    "authors": [
      "Xin Wen",
      "Bingchen Zhao",
      "Ismail Elezi",
      "Jiankang Deng",
      "Xiaojuan Qi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Park_Fair_Generation_without_Unfair_Distortions_Debiasing_Text-to-Image_Generation_with_Entanglement-Free_ICCV_2025_paper.html": {
    "title": "Fair Generation without Unfair Distortions: Debiasing Text-to-Image Generation with Entanglement-Free Attention",
    "volume": "main",
    "abstract": "Recent advancements in diffusion-based text-to-image (T2I) models have enabled the generation of high-quality and photorealistic images from text. However, they often exhibit societal biases related to gender, race, and socioeconomic status, thereby potentially reinforcing harmful stereotypes and shaping public perception in unintended ways. While existing bias mitigation methods demonstrate effectiveness, they often encounter attribute entanglement, where adjustments to attributes relevant to the bias (i.e., target attributes) unintentionally alter attributes unassociated with the bias (i.e., non-target attributes), causing undesirable distribution shifts. To address this challenge, we introduce Entanglement-Free Attention (EFA), a method that accurately incorporates target attributes (e.g., White, Black, and Asian) while preserving non-target attributes (e.g., background) during bias mitigation. At inference time, EFA randomly samples a target attribute with equal probability and adjusts the cross-attention in selected layers to incorporate the sampled attribute, achieving a fair distribution of target attributes. Extensive experiments demonstrate that EFA outperforms existing methods in mitigating bias while preserving non-target attributes, thereby maintaining the original model's output distribution and generative capacity",
    "checked": true,
    "id": "cf8a3506f41264a4f41ae2bcdff660d4f74020d4",
    "semantic_title": "fair generation without unfair distortions: debiasing text-to-image generation with entanglement-free attention",
    "citation_count": 3,
    "authors": [
      "Jeonghoon Park",
      "Juyoung Lee",
      "Chaeyeon Chung",
      "Jaeseong Lee",
      "Jaegul Choo",
      "Jindong Gu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liao_GM-MoE_Low-Light_Enhancement_with_Gated-Mechanism_Mixture-of-Experts_ICCV_2025_paper.html": {
    "title": "GM-MoE: Low-Light Enhancement with Gated-Mechanism Mixture-of-Experts",
    "volume": "main",
    "abstract": "Low-light enhancement has wide applications in autonomous driving, 3D reconstruction, remote sensing, surveillance, and so on, which can significantly improve information utilization. However, most existing methods lack generalization and are limited to specific tasks such as image recovery. To address these issues, we propose \\textbf Gated-Mechanism Mixture-of-Experts (GM-MoE), the first framework to introduce a mixture-of-experts network for low-light image enhancement. GM-MoE comprises a dynamic gated weight conditioning network and three sub-expert networks, each specializing in a distinct enhancement task. Combining a self-designed gated mechanism that dynamically adjusts the weights of the sub-expert networks for different data domains. Additionally, we integrate local and global feature fusion within sub-expert networks to enhance image quality by capturing multi-scale features. Experimental results demonstrate that the GM-MoE achieves superior generalization compared to over 20 existing approaches, reaching state-of-the-art performance on PSNR on 5 benchmarks and SSIM on 4 benchmarks, respectively. Code is available at: https://github.com/Sameenok/gm-moe-lowlight-enhancement.git",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minwen Liao",
      "Haobo Dong",
      "Xinyi Wang",
      "Kurban Ubul",
      "Yihua Shao",
      "Ziyang Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Beyond_Low-Rank_Tuning_Model_Prior-Guided_Rank_Allocation_for_Effective_Transfer_ICCV_2025_paper.html": {
    "title": "Beyond Low-Rank Tuning: Model Prior-Guided Rank Allocation for Effective Transfer in Low-Data and Large-Gap Regimes",
    "volume": "main",
    "abstract": "Low-Rank Adaptation (LoRA) has proven effective in reducing computational costs while maintaining performance comparable to fully fine-tuned foundation models across various tasks. However, its fixed low-rank structure restricts its adaptability in scenarios with substantial domain gaps, where higher ranks are often required to capture domain-specific complexities. Current adaptive LoRA methods attempt to overcome this limitation by dynamically expanding or selectively allocating ranks, but these approaches frequently depend on computationally intensive techniques such as iterative pruning, rank searches, or additional regularization. To address these challenges, we introduce Stable Rank-Guided Low-Rank Adaptation (SR-LoRA), a novel framework that utilizes the stable rank of pre-trained weight matrices as a natural prior for layer-wise rank allocation. By leveraging the stable rank, which reflects the intrinsic dimensionality of the weights, SR-LoRA enables a principled and efficient redistribution of ranks across layers, enhancing adaptability without incurring additional search costs. Empirical evaluations on few-shot tasks with significant domain gaps show that SR-LoRA consistently outperforms recent adaptive LoRA variants, achieving a superior trade-off between performance and efficiency. Our code is available at https://anonymous.4open.science/r/SR-LoRA-A18F",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuyan Zhang",
      "Kefan Wang",
      "Yun Gu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lee_Reference-based_Super-Resolution_via_Image-based_Retrieval-Augmented_Generation_Diffusion_ICCV_2025_paper.html": {
    "title": "Reference-based Super-Resolution via Image-based Retrieval-Augmented Generation Diffusion",
    "volume": "main",
    "abstract": "Most existing diffusion models have primarily utilized reference images for image-to-image translation rather than for super-resolution (SR). In SR-specific tasks, diffusion methods rely solely on low-resolution (LR) inputs, limiting their ability to leverage reference information. Prior reference-based diffusion SR methods have shown that incorporating appropriate references can significantly enhance reconstruction quality; however, identifying suitable references in real-world scenarios remains a critical challenge. Recently, Retrieval-Augmented Generation (RAG) has emerged as an effective framework that integrates retrieval-based and generation-based information from databases to enhance the accuracy and relevance of responses. Inspired by RAG, we propose an image-based RAG framework (iRAG) for realistic super-resolution, which employs a trainable hashing function to retrieve either real-world or generated references given an LR query. Retrieved patches are passed to a restoration module that generates high-fidelity super-resolved features, and a hallucination filtering mechanism is used to refine generated references from pre-trained diffusion models. Experimental results demonstrate that our approach not only resolves practical difficulties in reference selection but also delivers superior performance over existing diffusion and non-diffusion RefSR methods. Code is available at https://github.com/ByeonghunLee12/iRAG",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Byeonghun Lee",
      "Hyunmin Cho",
      "Hong Gyu Choi",
      "Soo Min Kang",
      "Iljun Ahn",
      "Kyong Hwan Jin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wei_Backdoor_Mitigation_by_Distance-Driven_Detoxification_ICCV_2025_paper.html": {
    "title": "Backdoor Mitigation by Distance-Driven Detoxification",
    "volume": "main",
    "abstract": "Backdoor attacks undermine the integrity of machine learning models by allowing attackers to manipulate predictions using poisoned training data. Such attacks lead to targeted misclassification when specific triggers are present, while the model behaves normally under other conditions. This paper considers a post-training backdoor defense task, aiming to detoxify the backdoors in pre-trained models. We begin by analyzing the underlying issues of vanilla fine-tuning and observe that it is often trapped in regions with low loss for both clean and poisoned samples. Motivated by such observations, we propose Distance-Driven Detoxification (D3), an innovative approach that reformulates backdoor defense as a constrained optimization problem. Specifically, D3 promotes the model's departure from the vicinity of its initial weights, effectively reducing the influence of backdoors. Extensive experiments on state-of-the-art (SOTA) backdoor attacks across various model architectures and datasets demonstrate that D3 not only matches but often surpasses the performance of existing SOTA post-training defense techniques",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaokui Wei",
      "Jiayin Liu",
      "Hongyuan Zha"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_FakeRadar_Probing_Forgery_Outliers_to_Detect_Unknown_Deepfake_Videos_ICCV_2025_paper.html": {
    "title": "FakeRadar: Probing Forgery Outliers to Detect Unknown Deepfake Videos",
    "volume": "main",
    "abstract": "In this paper, we propose FakeRadar, a novel deepfake video detection framework designed to address the challenges of cross-domain generalization in real-world scenarios. Existing detection methods typically rely on manipulation-specific cues, performing well on known forgery types but exhibiting severe limitations against emerging manipulation techniques. This poor generalization stems from their inability to adapt effectively to unseen forgery patterns. To overcome this, we leverage large-scale pretrained models (e.g. CLIP) to proactively probe the feature space, explicitly highlighting distributional gaps between real videos, known forgeries, and unseen manipulations. Specifically, FakeRadar introduces Forgery Outlier Probing, which employs dynamic subcluster modeling and cluster-conditional outlier generation to synthesize outlier samples near boundaries of estimated subclusters, simulating novel forgery artifacts beyond known manipulation types. Additionally, we design Outlier-Guided Tri-Training, which optimizes the detector to distinguish real, fake, and outlier samples using proposed outlier-driven contrastive learning and outlier-conditioned cross-entropy losses. Experiments show that FakeRadar outperforms existing methods across various benchmark datasets for deepfake video detection, particularly in cross-domain evaluations, by handling the variety of emerging manipulation techniques",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaolun Li",
      "Jichang Li",
      "Yinqi Cai",
      "Junye Chen",
      "Xiaonan Luo",
      "Guanbin Li",
      "Rushi Lan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Korkmaz_CuMPerLay_Learning_Cubical_Multiparameter_Persistence_Vectorizations_ICCV_2025_paper.html": {
    "title": "CuMPerLay: Learning Cubical Multiparameter Persistence Vectorizations",
    "volume": "main",
    "abstract": "We present CuMPerLay, a novel differentiable vectorization layer that enables the integration of Cubical Multiparameter Persistence (CMP) into deep learning pipelines. While CMP presents a natural and powerful way to topologically work with images, its use is hindered by the complexity of multifiltration structures as well as the vectorization of CMP. In face of these challenges, we introduce a new algorithm for vectorizing MP homologies of cubical complexes. Our CuMPerLay decomposes the CMP into a combination of individual, learnable single-parameter persistence, where the bifiltration functions are jointly learned. Thanks to the differentiability, its robust topological feature vectors can be seamlessly used within state-of-the-art architectures such as Swin Transformers. We establish theoretical guarantees for the stability of our vectorization under generalized Wasserstein metrics. Our experiments on benchmark medical imaging and computer vision datasets show the benefit CuMPerLay on classification and segmentation performance, particularly in limited-data scenarios. Overall, CuMPerLay offers a promising direction for integrating global structural information into deep networks for structured image analysis",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Caner Korkmaz",
      "Brighton Nuwagira",
      "Baris Coskunuzer",
      "Tolga Birdal"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tan_Towards_Privacy-preserved_Pre-training_of_Remote_Sensing_Foundation_Models_with_Federated_ICCV_2025_paper.html": {
    "title": "Towards Privacy-preserved Pre-training of Remote Sensing Foundation Models with Federated Mutual-guidance Learning",
    "volume": "main",
    "abstract": "Traditional Remote Sensing Foundation Models (RSFMs) are pre-trained with a data-centralized paradigm, through self-supervision on large-scale curated remote sensing data. For each institution, however, pre-training RSFMs with limited data in a standalone manner may lead to suboptimal performance, while aggregating remote sensing data from multiple institutions for centralized pre-training raises privacy concerns. Seeking for collaboration is a promising solution to resolve this dilemma, where multiple institutions can collaboratively train RSFMs without sharing private data. In this paper, we propose a novel privacy-preserved pre-training framework (FedSense), which enables multiple institutions to collaboratively train RSFMs without sharing private data. However, it is a non-trivial task hindered by a vicious cycle, which results from model drift by remote sensing data heterogeneity and high communication overhead. To break this vicious cycle, we introduce federated mutual-guidance learning. Specifically, we propose a Server-to-Clients Guidance (SCG) mechanism to guide clients' updates towards global-flatness optimal solutions. Additionally, we propose a Clients-to-Server Guidance (CSG) mechanism to inject local knowledge into the server by low-bit communication. Extensive experiments on four downstream tasks demonstrate the effectiveness of our FedSense in both full-precision and communication-reduced scenarios, showcasing remarkable communication efficiency and performance gains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jieyi Tan",
      "Chengwei Zhang",
      "Bo Dang",
      "Yansheng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Fang_SynTag_Enhancing_the_Geometric_Robustness_of_Inversion-based_Generative_Image_Watermarking_ICCV_2025_paper.html": {
    "title": "SynTag: Enhancing the Geometric Robustness of Inversion-based Generative Image Watermarking",
    "volume": "main",
    "abstract": "Robustness is significant for generative image watermarking, typically achieved by injecting distortion-invariant watermark features. The leading paradigm, i.e., inversion-based framework, excels against non-geometric distortions but struggles with geometric ones. To address this, we propose SynTag, a synchronization tag injection-based method that enhances geometric robustness in inversion-based schemes. Due to the complexity of geometric distortions, finding universally geometric-invariant features is challenging, and it is not clear whether such an invariant representation exists. Therefore, instead of seeking invariant representations, we embed a sensitive template feature alongside the watermarking features. This template evolves with geometric distortions, allowing us to reconstruct the distortion trajectory for correction before extraction. Focusing on latent diffusion models, we fine-tune the VAE decoder to inject the invisible SynTag feature, pairing it with a prediction network for extraction and correction. Additionally, we introduce a dither compensation mechanism to further improve correction accuracy. SynTag is highly compatible with existing inversion-based methods. Extensive experiments demonstrate a significant boost in geometric distortion robustness while maintaining resilience against non-geometric distortions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Fang",
      "Kejiang Chen",
      "Zehua Ma",
      "Jiajun  Deng",
      "Yicong Li",
      "Weiming Zhang",
      "Ee-Chien Chang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sifferman_Recovering_Parametric_Scenes_from_Very_Few_Time-of-Flight_Pixels_ICCV_2025_paper.html": {
    "title": "Recovering Parametric Scenes from Very Few Time-of-Flight Pixels",
    "volume": "main",
    "abstract": "We aim to recover the geometry of 3D parametric scenes using very few depth measurements from low-cost, commercially available time-of-flight sensors. These sensors offer very low spatial resolution (i.e., a single pixel), but image a wide field-of-view per pixel and capture detailed time-of-flight data in the form of time-resolved photon counts. This time-of-flight data encodes rich scene information and thus enables recovery of simple scenes from sparse measurements. We investigate the feasibility of using a distributed set of few measurements (e.g., as few as 15 pixels) to recover the geometry of simple parametric scenes with a strong prior, such as estimating the 6D pose of a known object. To achieve this, we design a method that utilizes both feed-forward prediction to infer scene parameters, and differentiable rendering within an analysis-by-synthesis framework to refine the scene parameter estimate. We develop hardware prototypes and demonstrate that our method effectively recovers object pose given an untextured 3D model in both simulations and controlled real-world captures, and show promising initial results for other parametric scenes. We additionally conduct experiments to explore the limits and capabilities of our imaging solution",
    "checked": true,
    "id": "d9a94228f843f8a7122e40b4bff9d10eb5a85b50",
    "semantic_title": "recovering parametric scenes from very few time-of-flight pixels",
    "citation_count": 0,
    "authors": [
      "Carter Sifferman",
      "Yiquan Li",
      "Yiming Li",
      "Fangzhou Mu",
      "Michael Gleicher",
      "Mohit Gupta",
      "Yin Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Teotia_DIMCIM_A_Quantitative_Evaluation_Framework_for_Default-mode_Diversity_and_Generalization_ICCV_2025_paper.html": {
    "title": "DIMCIM: A Quantitative Evaluation Framework for Default-mode Diversity and Generalization in Text-to-Image Generative Models",
    "volume": "main",
    "abstract": "Recent advances in text-to-image (T2I) models have achieved impressive quality and consistency. However, this has come at the cost of representation diversity. While automatic evaluation methods exist for benchmarking model diversity, they either require reference image datasets or lack specificity about the kind of diversity measured, limiting their adaptability and interpretability. To address this gap, we introduce the Does-it/Can-it framework, DIM-CIM, a reference-free measurement of default-mode diversity (\"Does\" the model generate images with expected attributes?) and generalization capacity (\"Can\" the model generate diverse attributes for a particular concept?). We construct the COCO-DIMCIM benchmark, which is seeded with COCO concepts and captions and augmented by a large language model. With COCO-DIMCIM, we find that widely-used models improve in generalization at the cost of default-mode diversity when scaling from 1.5B to 8.1B parameters. DIMCIM also identifies fine-grained failure cases, such as attributes that are generated with generic prompts but are rarely generated when explicitly requested. Finally, we use DIMCIM to evaluate the training data of a T2I model and observe a correlation of 0.85 between diversity in training images and default-mode diversity. Our work provides a flexible and interpretable framework for assessing T2I model diversity and generalization, enabling a more comprehensive understanding of model performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Revant Teotia",
      "Candace Ross",
      "Karen Ullrich",
      "Sumit Chopra",
      "Adriana Romero-Soriano",
      "Melissa Hall",
      "Matthew Muckley"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Geng_SAUCE_Selective_Concept_Unlearning_in_Vision-Language_Models_with_Sparse_Autoencoders_ICCV_2025_paper.html": {
    "title": "SAUCE: Selective Concept Unlearning in Vision-Language Models with Sparse Autoencoders",
    "volume": "main",
    "abstract": "Unlearning methods for vision-language models (VLMs) have primarily adapted techniques from large language models (LLMs), relying on weight updates that demand extensive annotated forget sets. Moreover, these methods perform unlearning at a coarse granularity, often leading to excessive forgetting and reduced model utility. To address this issue, we introduce SAUCE, a novel method that leverages sparse autoencoders (SAEs) for fine-grained and selective concept unlearning in VLMs. Briefly, SAUCE first trains SAEs to capture high-dimensional, semantically rich sparse features. It then identifies the features most relevant to the target concept for unlearning. During inference, it selectively modifies these features to suppress specific concepts while preserving unrelated information. We evaluate SAUCE on two distinct VLMs, LLaVA-v1.5-7B and LLaMA-3.2-11B-Vision-Instruct, across two types of tasks: concrete concept unlearning (objects and sports scenes) and abstract concept unlearning (emotions, colors, and materials), encompassing a total of 60 concepts. Extensive experiments demonstrate that SAUCE outperforms state-of-the-art methods by 18.04% in unlearning quality while maintaining comparable model utility. Furthermore, we investigate SAUCE's robustness against widely used adversarial attacks, its transferability across models, and its scalability in handling multiple simultaneous unlearning requests. Our findings establish SAUCE as an effective and scalable solution for selective concept unlearning in VLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahui Geng",
      "Qing Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cai_ForCenNet_Foreground-Centric_Network_for_Document_Image_Rectification_ICCV_2025_paper.html": {
    "title": "ForCenNet: Foreground-Centric Network for Document Image Rectification",
    "volume": "main",
    "abstract": "Document image rectification aims to eliminate geometric deformation in photographed documents to facilitate text recognition. However, existing methods often neglect the significance of foreground elements, which provide essential geometric references and layout information for document image correction. In this paper, we introduce Foreground-Centric Network(ForCenNet) to eliminate geometric distortions in document images. Specifically, we initially propose a foreground-centric label generation method, which extracts detailed foreground elements from an undistorted image. Then we introduce a foreground-centric mask mechanism to enhance the distinction between readable and background regions. Furthermore, we design a curvature consistency loss to leverage the detailed foreground labels to help the model understand the distorted geometric distribution. Extensive experiments demonstrate that ForCenNet achieves new state-of-the-art on four real-world benchmarks, such as DocUNet, DIR300, WarpDoc, and DocReal. Quantitative analysis shows that the proposed method effectively undistorts layout elements, such as text lines and table borders. The resources for further comparison are provided at https://github.com/caipeng328/ForCenNet",
    "checked": true,
    "id": "ef7a0311cda467d9f0c5ede521b10b87af289e8d",
    "semantic_title": "forcennet: foreground-centric network for document image rectification",
    "citation_count": 0,
    "authors": [
      "Peng Cai",
      "Qiang Li",
      "Kaicheng Yang",
      "Dong Guo",
      "Jia Li",
      "Nan Zhou",
      "Xiang An",
      "Ninghua Yang",
      "Jiankang Deng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Adversarial_Data_Augmentation_for_Single_Domain_Generalization_via_Lyapunov_Exponent-Guided_ICCV_2025_paper.html": {
    "title": "Adversarial Data Augmentation for Single Domain Generalization via Lyapunov Exponent-Guided Optimization",
    "volume": "main",
    "abstract": "Single Domain Generalization (SDG) aims to develop models capable of generalizing to unseen target domains using only one source domain, a task complicated by substantial domain shifts and limited data diversity. Existing SDG approaches primarily rely on data augmentation techniques, which struggle to effectively adapt training dynamics to accommodate large domain shifts. To address this, we propose LEAwareSGD, a novel Lyapunov Exponent (LE)-guided optimization approach inspired by dynamical systems theory. By leveraging LE measurements to modulate the learning rate, LEAwareSGD encourages model training near the edge of chaos, a critical state that optimally balances stability and adaptability. This dynamic adjustment allows the model to explore a wider parameter space and capture more generalizable features, ultimately enhancing the model's generalization capability. Extensive experiments on PACS, OfficeHome, and DomainNet demonstrate that LEAwareSGD yields substantial generalization gains, achieving up to 9.47% improvement on PACS in low-data regimes. These results underscore the effectiveness of training near the edge of chaos for enhancing model generalization capability in SDG tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zuyu Zhang",
      "Ning Chen",
      "Yongshan Liu",
      "Qinghua Zhang",
      "Xu Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ji_Always_Skip_Attention_ICCV_2025_paper.html": {
    "title": "Always Skip Attention",
    "volume": "main",
    "abstract": "We highlight a curious empirical result within modern Vision Transformers (ViTs). Specifically, self-attention catastrophically fails to train unless it is used in conjunction with a skip connection. This is in contrast to other elements of a ViT that continue to exhibit good performance (albeit suboptimal) when skip connections are removed. Further, we show that this critical dependence on skip connections is a relatively new phenomenon, with previous deep architectures (e.g., CNNs) exhibiting good performance in their absence. In this paper, we theoretically characterize that the self-attention mechanism is fundamentally ill-conditioned and is, therefore, uniquely dependent on skip connections for regularization. Additionally, we propose Token Graying (TG), a simple yet effective complement (to skip connections) that further improves the condition of in- put tokens. We validate our approach in both supervised and self-supervised training methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiping Ji",
      "Hemanth Saratchandran",
      "Peyman Moghadam",
      "Simon Lucey"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yeshwanth_ExCap3D_Expressive_3D_Scene_Understanding_via_Object_Captioning_with_Varying_ICCV_2025_paper.html": {
    "title": "ExCap3D: Expressive 3D Scene Understanding via Object Captioning with Varying Detail",
    "volume": "main",
    "abstract": "Generating text descriptions of objects in 3D indoor scenes is an important building block of embodied understanding. Existing methods do this by describing objects at a single level of detail, which often does not capture fine-grained details such as varying textures, materials, and shapes of the parts of objects.We propose the task of expressive 3D captioning: given an input 3D scene, describe objects at multiple levels of detail: a high-level object description, and a low-level description of the properties of its parts.To produce such captions, we present ExCap3D, an expressive 3D captioning model which takes as input a 3D scan, and for each detected object in the scan, generates a fine-grained collective description of the parts of the object, along with an object-level description conditioned on the part-level description.We design ExCap3D to encourage semantic consistency between the generated text descriptions, as well as textual similarity in the latent space, to further increase the quality of the generated captions.To enable this task, we generated the ExCap3D Dataset by leveraging a visual-language model (VLM) for multi-view captioning. ExCap3D Dataset contains captions on the ScanNet++ dataset with varying levels of detail,comprising 190k text descriptions of 34k 3D objects in 947 indoor scenes.Our experiments show that the object- and part-level of detail captions generated by ExCap3D are of higher quality than those produced by state-of-the-art methods, with a Cider score improvement of 17% and 126% for object- and part-level details respectively. Our code, dataset and models will be made publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chandan Yeshwanth",
      "Dávid Rozenberszki",
      "Angela Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhuang_CMAD_Correlation-Aware_and_Modalities-Aware_Distillation_for_Multimodal_Sentiment_Analysis_with_ICCV_2025_paper.html": {
    "title": "CMAD: Correlation-Aware and Modalities-Aware Distillation for Multimodal Sentiment Analysis with Missing Modalities",
    "volume": "main",
    "abstract": "Multimodal Sentiment Analysis (MSA) enhances emotion recognition by integrating information from multiple modalities. However, multimodal learning with missing modalities suffers from representation inconsistency and optimization instability, leading to suboptimal performance. In this paper, we introduce Correlation-Aware and Modalities-Aware Distillation (CMAD), a unified framework designed for MSA under varying missing-modality conditions. Specifically, CMAD comprises two key components: (1) Correlation-Aware Feature Distillation (CAFD), which enforces multi-level representation alignment by preserving both feature similarities and high-order correlation structures between teacher and student models, and (2) Modality-Aware Regularization (MAR) employs an adaptive weighting strategy guided by modality difficulty, enabling a curriculum learning paradigm to stabilize the training process. Extensive evaluations on five datasets show that CMAD consistently outperforms existing methods, achieving average performance improvements of 1.0% on MOSEI, 4.4% on IEMOCAP, 1.9% on MUStARD, 0.5% on UR-FUNNY and 1.9% on CHERMA. Code is available at: https://github.com/YetZzzzzz/CMAD",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Zhuang",
      "Minhao Liu",
      "Wei Bai",
      "Yanru Zhang",
      "Xiaoyue Zhang",
      "Jiawen Deng",
      "Fuji Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ci_Describe_Dont_Dictate_Semantic_Image_Editing_with_Natural_Language_Intent_ICCV_2025_paper.html": {
    "title": "Describe, Don't Dictate: Semantic Image Editing with Natural Language Intent",
    "volume": "main",
    "abstract": "Despite the progress in text-to-image generation, semantic image editing remains a challenge. Inversion-based algorithms unavoidably introduce reconstruction errors, while instruction-based models mainly suffer from limited dataset quality and scale. To address these problems, we propose a descriptive-prompt-based editing framework, named DescriptiveEdit. The core idea is to re-frame `instruction-based image editing' as `reference-image-based text-to-image generation', which preserves the generative power of well-trained Text-to-Image models without architectural modifications or inversion. Specifically, taking the reference image and a prompt as input, we introduce a Cross-Attentive UNet, which newly adds attention bridges to inject reference image features into the prompt-to-edit-image generation process. Owing to its text-to-image nature, DescriptiveEdit overcomes limitations in instruction dataset quality, integrates seamlessly with ControlNet, IP-Adapter, and other extensions, and is more scalable. Experiments on the Emu Edit benchmark show it improves editing accuracy and consistency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "En Ci",
      "Shanyan Guan",
      "Yanhao Ge",
      "Yilin Zhang",
      "Wei Li",
      "Zhenyu Zhang",
      "Jian Yang",
      "Ying Tai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_LiT_Delving_into_a_Simple_Linear_Diffusion_Transformer_for_Image_ICCV_2025_paper.html": {
    "title": "LiT: Delving into a Simple Linear Diffusion Transformer for Image Generation",
    "volume": "main",
    "abstract": "In this paper, we investigate how to convert a pre-trained Diffusion Transformer (DiT) into a linear DiT, as its simplicity, parallelism, and efficiency for image generation. Through detailed exploration, we offer a suite of ready-to-use solutions, ranging from linear attention design to optimization strategies. Our core contributions include 5 practical guidelines: 1) Applying depth-wise convolution within simple linear attention is sufficient for image generation. 2) Using fewer heads in linear attention provides a free-lunch performance boost without increasing latency. 3) Inheriting weights from a fully converged, pre-trained DiT. 4) Loading all parameters except those related to linear attention. 5) Hybrid knowledge distillation: using a pre-trained teacher DiT to help the training of the student linear DiT, supervising not only the predicted noise but also the variance of the reverse diffusion process. These guidelines lead to our proposed Linear Diffusion Transformer (LiT), which serves as a safe and efficient alternative baseline for DiT with pure linear attention. In class-conditional 256x256 and 512x512 ImageNet generation, LiT can be quickly adapted from DiT using only 20% and 33% of DiT's training steps, respectively, while achieving comparable performance. LiT also rivals methods based on Mamba or Gated Linear Attention. Moreover, the same guidelines generalize to text-to-image generation: LiT can be swiftly converted from PixArt-S to generate high-quality images, maintaining comparable GenEval scores",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahao Wang",
      "Ning Kang",
      "Lewei Yao",
      "Mengzhao Chen",
      "Chengyue Wu",
      "Songyang Zhang",
      "Shuchen Xue",
      "Yong Liu",
      "Taiqiang Wu",
      "Xihui Liu",
      "Kaipeng Zhang",
      "Shifeng Zhang",
      "Wenqi Shao",
      "Zhenguo Li",
      "Ping Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Bousselham_LeGrad_An_Explainability_Method_for_Vision_Transformers_via_Feature_Formation_ICCV_2025_paper.html": {
    "title": "LeGrad: An Explainability Method for Vision Transformers via Feature Formation Sensitivity",
    "volume": "main",
    "abstract": "Vision Transformers (ViTs) have become a standard architecture in computer vision. However, because of their modeling of long-range dependencies through self-attention mechanisms, the explainability of these models remains a challenge. To address this, we propose LeGrad, an explainability method specifically designed for ViTs. LeGrad computes the gradient with respect to the attention maps of single ViT layers, considering the gradient itself as the explainability signal. We aggregate the signal over all layers, combining the activations of the last as well as intermediate tokens to produce the merged explainability map. This makes LeGrad a conceptually simple and an easy-to-implement method to enhance the transparency of ViTs. We evaluate LeGrad in various setups, including segmentation, perturbation, and open-vocabulary settings, showcasing its improved spatial fidelity and its versatility compared to other SotA explainability methods. A demo and the code is available at https://walidbousselham.com/LeGrad",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Walid Bousselham",
      "Angie Boggust",
      "Sofian Chaybouti",
      "Hendrik Strobelt",
      "Hilde Kuehne"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Boeder_GaussianFlowOcc_Sparse_and_Weakly_Supervised_Occupancy_Estimation_using_Gaussian_Splatting_ICCV_2025_paper.html": {
    "title": "GaussianFlowOcc: Sparse and Weakly Supervised Occupancy Estimation using Gaussian Splatting and Temporal Flow",
    "volume": "main",
    "abstract": "Occupancy estimation has become a prominent task in 3D computer vision, particularly within the autonomous driving community. In this paper, we present a novel approach to occupancy estimation, termed GaussianFlowOcc, which is inspired by Gaussian Splatting and replaces traditional dense voxel grids with a sparse 3D Gaussian representation. Our efficient model architecture based on a Gaussian Transformer significantly reduces computational and memory requirements by eliminating the need for expensive 3D convolutions used with inefficient voxel-based representations that predominantly represent empty 3D spaces. GaussianFlowOcc effectively captures scene dynamics by estimating temporal flow for each Gaussian during the overall network training process, offering a straightforward solution to a complex problem that is often neglected by existing methods. Moreover, GaussianFlowOcc is designed for scalability, as it employs weak supervision and does not require costly dense 3D voxel annotations based on additional data (e.g., LiDAR). Through extensive experimentation, we demonstrate that GaussianFlowOcc significantly outperforms all previous methods for weakly supervised occupancy estimation on the nuScenes dataset while featuring an inference speed that is 50 times faster than current SOTA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simon Boeder",
      "Fabian Gigengack",
      "Benjamin Risse"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_DreamLayer_Simultaneous_Multi-Layer_Generation_via_Diffusion_Model_ICCV_2025_paper.html": {
    "title": "DreamLayer: Simultaneous Multi-Layer Generation via Diffusion Model",
    "volume": "main",
    "abstract": "Text-driven image generation using diffusion models has recently gained significant attention. To enable more flexible image manipulation and editing, recent research has expanded from single image generation to transparent layer generation and multi-layer compositions. However, existing approaches often fail to provide a thorough exploration of multi-layer structures, leading to inconsistent inter-layer interactions, such as occlusion relationships, spatial layout, and shadowing. In this paper, we introduce DreamLayer, a novel framework that enables coherent text-driven generation of multiple image layers, by explicitly modeling the relationship between transparent foreground and background layers. DreamLayer incorporates three key components, i.e., Context-Aware Cross-Attention (CACA) for global-local information exchange, Layer-Shared Self-Attention (LSSA) for establishing robust inter-layer connections, and Information Retained Harmonization (IRH) for refining fusion details at the latent level.By leveraging a coherent full-image context, DreamLayer builds inter-layer connections through attention mechanisms and applies a harmonization step to achieve seamless layer fusion. To facilitate research in multi-layer generation, we construct a high-quality, diverse multi-layer dataset including 400k samples. Extensive experiments and user studies demonstrate that DreamLayer generates more coherent and well-aligned layers, with broad applicability, including latent-space image editing and image-to-layer decomposition",
    "checked": false,
    "id": "b1a19bc58eeddf0e22e6fd1121a6a9cb1a892c4f",
    "semantic_title": "dreamlayer: simultaneous multi-layer generation via diffusion mode",
    "citation_count": 1,
    "authors": [
      "Junjia Huang",
      "Pengxiang Yan",
      "Jinhang Cai",
      "Jiyang Liu",
      "Zhao Wang",
      "Yitong Wang",
      "Xinglong Wu",
      "Guanbin Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shimomura_OD-RASE_Ontology-Driven_Risk_Assessment_and_Safety_Enhancement_for_Autonomous_Driving_ICCV_2025_paper.html": {
    "title": "OD-RASE: Ontology-Driven Risk Assessment and Safety Enhancement for Autonomous Driving",
    "volume": "main",
    "abstract": "Although autonomous driving systems demonstrate high perception performance, they still face limitations when handling rare situations or complex road structures. Since existing road infrastructures are designed for human drivers, safety improvements are typically introduced only after accidents occur. This reactive approach poses a significant challenge for autonomous systems, which require proactive risk mitigation. To address this issue, we propose OD-RASE, a framework for enhancing the safety of autonomous driving systems by detecting road structures that cause traffic accidents and connecting these findings to infrastructure development. First, we formalize an ontology based on specialized domain knowledge of road traffic systems. In parallel, we generate infrastructure improvement proposals using a large-scale visual language model (LVLM) and use ontology-driven data filtering to enhance their reliability. This process automatically annotates improvement proposals on pre-accident road images, leading to the construction of a new dataset. Furthermore, we introduce the Baseline approach (OD-RASE model), which leverages LVLM and a diffusion model to produce both infrastructure improvement proposals and generated images of the improved road environment. Our experiments demonstrate that ontology-driven data filtering enables highly accurate prediction of accident-causing road structures and the corresponding improvement plans. We believe that this work contributes to the overall safety of traffic environments and marks an important step toward the broader adoption of autonomous driving systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kota Shimomura",
      "Masaki Nambata",
      "Atsuya Ishikawa",
      "Ryota Mimura",
      "Koki Inoue",
      "Takayoshi Yamashita",
      "Takayuki Kawabuchi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zheng_FREE-Merging_Fourier_Transform_for_Efficient_Model_Merging_ICCV_2025_paper.html": {
    "title": "FREE-Merging: Fourier Transform for Efficient Model Merging",
    "volume": "main",
    "abstract": "With the rapid growth of deep learning, there is an increasing availability of open-source models for various tasks. However, single fine-tuned models often fall short of meeting the diverse needs of users. Model merging has thus emerged as an efficient method to integrate the capabilities of existing models into a unified model. Nevertheless, existing model merging methods face challenging trade-offs between performance and deployment costs, primarily due to task interference. For the first time, we reveal that task interference is evident in the frequency domain of model parameters, yet current efforts only focus on spatial domain solutions, which are largely ineffective in addressing frequency domain interference. To mitigate the impact of frequency domain interference, we propose **FR-Merging**, an innovative method that effectively filters harmful frequency domain interference on the backbone with minimal computational overhead. Since performance loss is inevitable with cost-free methods, we propose a lightweight task-specific expert module that dynamically compensates for information loss during merging. This proposed framework, **FREE-Merging** (FR-Merging with experts), strikes a balanced trade-off between training cost, inference latency, storage requirements, and performance. We demonstrate the effectiveness of both FR-Merging and FREE-Merging on multiple tasks across CV, NLP, and Multi-Modal domains and show that they can be flexibly adapted to specific needs",
    "checked": true,
    "id": "db426acfb1f36c71337b2b617a6c95379be5fb26",
    "semantic_title": "free-merging: fourier transform for efficient model merging",
    "citation_count": 2,
    "authors": [
      "Shenghe Zheng",
      "Hongzhi Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_ForgeLens_Data-Efficient_Forgery_Focus_for_Generalizable_Forgery_Image_Detection_ICCV_2025_paper.html": {
    "title": "ForgeLens: Data-Efficient Forgery Focus for Generalizable Forgery Image Detection",
    "volume": "main",
    "abstract": "The rise of generative models has raised concerns about image authenticity online, highlighting the urgent need for a detector that is (1) highly generalizable, capable of handling unseen forgery techniques, and (2) data-efficient, achieving optimal performance with minimal training data, enabling it to counter newly emerging forgery techniques effectively. To achieve this, we propose ForgeLens, a data-efficient, feature-guided framework that incorporates two lightweight designs to enable a frozen network to focus on forgery-specific features. First, we introduce the Weight-Shared Guidance Module (WSGM), which guides the extraction of forgery-specific features during training. Second, a forgery-aware feature integrator, FAFormer, is used to effectively integrate forgery information across multi-stage features. ForgeLens addresses a key limitation of previous frozen network-based methods, where general-purpose features extracted from large datasets often contain excessive forgery-irrelevant information. As a result, it achieves strong generalization and reaches optimal performance with minimal training data. Experimental results on 19 generative models, including both GANs and diffusion models, demonstrate improvements of 13.61% in Avg.Acc and 8.69% in Avg.AP over the base model. Notably, ForgeLens outperforms existing forgery detection methods, achieving state-of-the-art performance with just 1% of the training data",
    "checked": true,
    "id": "5c4d47f7d7dfca649c7dc4fc8518cf72f1bc4acb",
    "semantic_title": "forgelens: data-efficient forgery focus for generalizable forgery image detection",
    "citation_count": 1,
    "authors": [
      "Yingjian Chen",
      "Lei Zhang",
      "Yakun Niu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lu_Serialization_based_Point_Cloud_Oversegmentation_ICCV_2025_paper.html": {
    "title": "Serialization based Point Cloud Oversegmentation",
    "volume": "main",
    "abstract": "Point cloud oversegmentation, as a fundamental preprocessing step for 3D understanding, is a challenging task due to its spatial proximity and semantic similarity requirements. Most existing works struggle to efficiently group semantically consistent points into superpoints while maintaining spatial proximity. In this paper, we propose a novel serialization based point cloud oversegmentation method, which leverages serialization to avoid complex spatial queries, directly accessing neighboring points through sequence locality for similarity matching and superpoint clustering. Specifically, we first serialize point clouds into a Hilbert curve and spatially-continuously partition them into initial segments. Then, to guarantee the internal semantic consistency of superpoints, we design an adaptive update algorithm that clusters superpoints by matching feature similarities between neighboring segments and refines segment features via Cross-Attention. Experiments on largescale indoor and outdoor datasets demonstrate state-of-the-art performance in point cloud oversegmentation. Moreover, it is also adaptable to semantic segmentation and achieves promising performance. The code is available at https://github.com/CHL-glitch/SPCNet",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenghui Lu",
      "Jianlong Kwan",
      "Dilong Li",
      "Ziyi Chen",
      "Haiyan Guan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yu_Latent_Expression_Generation_for_Referring_Image_Segmentation_and_Grounding_ICCV_2025_paper.html": {
    "title": "Latent Expression Generation for Referring Image Segmentation and Grounding",
    "volume": "main",
    "abstract": "Visual grounding tasks, such as referring image segmentation (RIS) and referring expression comprehension (REC), aim to localize a target object based on a given textual description. The target object in an image can be described in multiple ways, reflecting diverse attributes such as color, position, and more. However, most existing methods rely on a single textual input, which captures only a fraction of the rich information available in the visual domain. This mismatch between rich visual details and sparse textual cues can lead to the misidentification of similar objects. To address this, we propose a novel visual grounding framework that leverages multiple latent expressions generated from a single textual input by incorporating complementary visual details absent from the original description. Specifically, we introduce subject distributor and visual concept injector modules to embed both shared-subject and distinct-attributes concepts into the latent representations, thereby capturing unique and target-specific visual cues. We also propose a positive-margin contrastive learning strategy to align all latent expressions with the original text while preserving subtle variations. Experimental results show that our method not only outperforms state-of-the-art RIS and REC approaches on multiple benchmarks but also achieves outstanding performance on the generalized referring expression segmentation (GRES) benchmark",
    "checked": true,
    "id": "c3c924de8cb14f733d965a1eecef5227dd0b5c51",
    "semantic_title": "latent expression generation for referring image segmentation and grounding",
    "citation_count": 0,
    "authors": [
      "Seonghoon Yu",
      "Joonbeom Hong",
      "Joonseok Lee",
      "Jeany Son"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Dong_NoiseController_Towards_Consistent_Multi-view_Video_Generation_via_Noise_Decomposition_and_ICCV_2025_paper.html": {
    "title": "NoiseController: Towards Consistent Multi-view Video Generation via Noise Decomposition and Collaboration",
    "volume": "main",
    "abstract": "High-quality video generation is crucial for many fields, including the film industry and autonomous driving. However, generating videos with spatiotemporal consistencies remains challenging. Current methods typically utilize attention mechanisms or modify noise to achieve consistent videos, neglecting global spatiotemporal information that could help ensure spatial and temporal consistency during video generation. In this paper, we propose the ***NoiseController***, consisting of **Multi-Level Noise Decomposition**, **Multi-Frame Noise Collaboration**, and **Joint Denoising**, to enhance spatiotemporal consistencies in video generation. In multi-level noise decomposition, we first decompose initial noises into scene-level foreground/background noises, capturing distinct motion properties to model multi-view foreground/background variations. Furthermore, each scene-level noise is further decomposed into individual-level shared and residual components. The shared noise preserves consistency, while the residual component maintains diversity. In multi-frame noise collaboration, we introduce an inter-view spatiotemporal collaboration matrix and an intra-view impact collaboration matrix, which captures mutual cross-view effects and historical cross-frame impacts to enhance video quality. The joint denoising contains two parallel denoising U-Nets to remove each scene-level noise, mutually enhancing video generation. We evaluate our ***NoiseController*** on public datasets focusing on video generation and downstream tasks, demonstrating its state-of-the-art performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haotian Dong",
      "Xin Wang",
      "Di Lin",
      "Yipeng Wu",
      "Qin Chen",
      "Ruonan Liu",
      "Kairui Yang",
      "Ping Li",
      "Qing Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lv_Rethinking_Cross-Modal_Interaction_in_Multimodal_Diffusion_Transformers_ICCV_2025_paper.html": {
    "title": "Rethinking Cross-Modal Interaction in Multimodal Diffusion Transformers",
    "volume": "main",
    "abstract": "Multimodal Diffusion Transformers (MM-DiTs) have achieved remarkable progress in text-driven visual generation. However, even state-of-the-art MM-DiT models like FLUX struggle with achieving precise alignment between text prompts and generated content. We identify two key issues in the attention mechanism of MM-DiT, namely 1) the suppression of cross-modal attention due to token imbalance between visual and textual modalities and 2) the lack of timestep-aware attention weighting, which hinder the alignment. To address these issues, we propose Temperature-Adjusted Cross-modal Attention (TACA), a parameter-efficient method that dynamically rebalances multimodal interactions through temperature scaling and timestep-dependent adjustment. When combined with LoRA fine-tuning, TACA significantly enhances text-image alignment on the T2I-CompBench benchmark with minimal computational overhead. We tested TACA on state-of-the-art models like FLUX and SD3.5, demonstrating its ability to improve text-image alignment in terms of object appearance, attribute binding, and spatial relationships. Our findings highlight the importance of balancing cross-modal attention in improving semantic fidelity in text-to-image diffusion models. Our codes are publicly available at \\href https://github.com/Vchitect/TACA https://github.com/Vchitect/TACA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengyao Lv",
      "Tianlin Pan",
      "Chenyang Si",
      "Zhaoxi Chen",
      "Wangmeng Zuo",
      "Ziwei Liu",
      "Kwan-Yee K. Wong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_MOERL_When_Mixture-of-Experts_Meet_Reinforcement_Learning_for_Adverse_Weather_Image_ICCV_2025_paper.html": {
    "title": "MOERL: When Mixture-of-Experts Meet Reinforcement Learning for Adverse Weather Image Restoration",
    "volume": "main",
    "abstract": "Adverse weather conditions, such as rain, snow, and haze, introduce complex degradations that present substantial challenges for effective image restoration. Existing all-in-one models often rely on fixed network structures, limiting their ability to adapt to the varying characteristics of different weather conditions. Moreover, these models typically lack the iterative refinement process that human experts use for progressive image restoration. In this work, we propose MOERL, a Mixture-of-Experts (MoE) model optimized with reinforcement learning (RL) to enhance image restoration across diverse weather conditions. Our method incorporates two core types of experts, i.e., channel-wise modulation and spatial modulation experts, to address task-specific degradation characteristics while minimizing task interference. In addition, inspired by human expertise, we frame the optimization process as a sequential, progressive problem, allowing the network to refine its parameters progressively and adapt to specific weather conditions. Extensive experiments demonstrate the efficacy and superiority of our proposed method. The code and pre-trained models will be available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Wang",
      "Peiwen Xia",
      "Bo Li",
      "Peng-Tao Jiang",
      "Zhe Kong",
      "Kaihao Zhang",
      "Tong Lu",
      "Wenhan Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Nagata_DACoN_DINO_for_Anime_Paint_Bucket_Colorization_with_Any_Number_ICCV_2025_paper.html": {
    "title": "DACoN: DINO for Anime Paint Bucket Colorization with Any Number of Reference Images",
    "volume": "main",
    "abstract": "Automatic colorization of line drawings has been widely studied to reduce the labor cost of hand-drawn anime production. Deep learning approaches, including image/video generation and feature-based correspondence, have improved accuracy but struggle with occlusions, pose variations, and viewpoint changes. To address these challenges, we propose DACoN, a framework that leverages foundation models to capture part-level semantics, even in line drawings. Our method fuses low-resolution semantic features from foundation models with high-resolution spatial features from CNNs for fine-grained yet robust feature extraction.In contrast to previous methods that rely on the Multiplex Transformer and support only one or two reference images, DACoN removes this constraint, allowing any number of references. Quantitative and qualitative evaluations demonstrate the benefits of using multiple reference images, achieving superior colorization performance. Our code and model are available at https://github.com/kzmngt/DACoN",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kazuma Nagata",
      "Naoshi Kaneko"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Du_Beyond_Single_Images_Retrieval_Self-Augmented_Unsupervised_Camouflaged_Object_Detection_ICCV_2025_paper.html": {
    "title": "Beyond Single Images: Retrieval Self-Augmented Unsupervised Camouflaged Object Detection",
    "volume": "main",
    "abstract": "At the core of Camouflaged Object Detection (COD) lies segmenting objects from their highly similar surroundings. Previous efforts navigate this challenge primarily through image-level modeling or annotation-based optimization. Despite advancing considerably, this commonplace practice hardly taps valuable dataset-level contextual information or relies on laborious annotations. In this paper, we propose RISE, a RetrIeval SElf-augmented paradigm that exploits the entire training dataset to generate pseudo-labels for single images, which could be used to train COD models. RISE begins by constructing prototype libraries for environments and camouflaged objects using training images (without ground truth), followed by K-Nearest Neighbor (KNN) retrieval to generate pseudo-masks for each image based on these libraries. It is important to recognize that using only training images without annotations exerts a pronounced challenge in crafting high-quality prototype libraries. In this light, we introduce a Clustering-then-Retrieval (CR) strategy, where coarse masks are first generated through clustering, facilitating subsequent histogram-based image filtering and cross-category retrieval to produce high-confidence prototypes. In the KNN retrieval stage, to alleviate the effect of artifacts in feature maps, we propose Multi-View KNN Retrieval (MVKR), which integrates retrieval results from diverse views to produce more robust and precise pseudo-masks. Extensive experiments demonstrate that RISE significantly outperforms state-of-the-art unsupervised and prompt-based methods. Code is available at https://github.com/xiaohainku/RISE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ji Du",
      "Xin Wang",
      "Fangwei Hao",
      "Mingyang Yu",
      "Chunyuan Chen",
      "Jiesheng Wu",
      "Bin Wang",
      "Jing Xu",
      "Ping Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Multi-identity_Human_Image_Animation_with_Structural_Video_Diffusion_ICCV_2025_paper.html": {
    "title": "Multi-identity Human Image Animation with Structural Video Diffusion",
    "volume": "main",
    "abstract": "Generating human videos from a single image while ensuring high visual quality and precise control is a challenging task, especially in complex scenarios involving multiple individuals and interactions with objects. Existing methods, while effective for single-human cases, often fail to handle the intricacies of multi-identity interactions because they struggle to associate the correct pairs of human appearance and pose condition and model the distribution of 3D-aware dynamics. To address these limitations, we present Structural Video Diffusion, a novel framework designed for generating realistic multi-human videos. Our approach introduces two core innovations: identity-specific embeddings to maintain consistent appearances across individuals and a structural learning mechanism that incorporates depth and surface-normal cues to model human-object interactions. Additionally, we expand existing human video dataset with 25K new videos featuring diverse multi-human and object interaction scenarios, providing a robust foundation for training. Experimental results demonstrate that Structural Video Diffusion achieves superior performance in generating lifelike, coherent videos for multiple subjects with dynamic and rich interactions, advancing the state of human-centric video generation. Code is available at https://github.com/zhenzhiwang/Multi-HumanVid",
    "checked": true,
    "id": "5588e943742454b82444f2b800034cc11f8cb1ae",
    "semantic_title": "multi-identity human image animation with structural video diffusion",
    "citation_count": 2,
    "authors": [
      "Zhenzhi Wang",
      "Yixuan Li",
      "Yanhong Zeng",
      "Yuwei Guo",
      "Dahua Lin",
      "Tianfan Xue",
      "Bo Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Fu_UniVG_A_Generalist_Diffusion_Model_for_Unified_Image_Generation_and_ICCV_2025_paper.html": {
    "title": "UniVG: A Generalist Diffusion Model for Unified Image Generation and Editing",
    "volume": "main",
    "abstract": "Text-to-Image (T2I) diffusion models have shown impressive results in generating visually compelling images following user prompts. Building on this, various methods further fine-tune the pre-trained T2I model for specific tasks. However, this requires separate model architectures, training designs, and multiple parameter sets to handle different tasks. In this paper, we introduce UniVG, a generalist diffusion model capable of supporting a diverse range of image generation tasks with a single set of weights. UniVG treats multi-modal inputs as unified conditions to enable various downstream applications, ranging from T2I generation, inpainting, instruction-based editing, identity-preserving generation, and layout-guided generation, to depth estimation and referring segmentation. Through comprehensive empirical studies on data mixing and multi-task training, we provide detailed insights into the training processes and decisions that inform our final designs. For example, we show that T2I generation and other tasks, such as instruction-based editing, can coexist without performance trade-offs, while auxiliary tasks like depth estimation and referring segmentation enhance image editing. Notably, our model can even outperform some task-specific models on their respective benchmarks, marking a significant step towards a unified image generation model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tsu-Jui Fu",
      "Yusu Qian",
      "Chen Chen",
      "Wenze Hu",
      "Zhe Gan",
      "Yinfei Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Bueno-Benito_CLOT_Closed_Loop_Optimal_Transport_for_Unsupervised_Action_Segmentation_ICCV_2025_paper.html": {
    "title": "CLOT: Closed Loop Optimal Transport for Unsupervised Action Segmentation",
    "volume": "main",
    "abstract": "Unsupervised action segmentation has recently pushed its limits with ASOT, an optimal transport (OT)-based method that simultaneously learns action representations and performs clustering using pseudo-labels. Unlike other OT-based approaches, ASOT makes no assumptions about action ordering and can decode a temporally consistent segmentation from a noisy cost matrix between video frames and action labels. However, the resulting segmentation lacks segment-level supervision, limiting the effectiveness of feedback between frames and action representations. To address this limitation, we propose Closed Loop Optimal Transport (CLOT), a novel OT-based framework with a multi-level cyclic feature learning mechanism. Leveraging its encoder-decoder architecture, CLOT learns pseudo-labels alongside frame and segment embeddings by solving two separate OT problems. It then refines both frame embeddings and pseudo-labels through cross-attention between the learned frame and segment embeddings, by integrating a third OT problem. Experimental results on four benchmark datasets demonstrate the benefits of cyclical learning for unsupervised action segmentation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elena Bueno-Benito",
      "Mariella Dimiccoli"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ye_GeoSplatting_Towards_Geometry_Guided_Gaussian_Splatting_for_Physically-based_Inverse_Rendering_ICCV_2025_paper.html": {
    "title": "GeoSplatting: Towards Geometry Guided Gaussian Splatting for Physically-based Inverse Rendering",
    "volume": "main",
    "abstract": "Recent 3D Gaussian Splatting (3DGS) representations have demonstrated remarkable performance in novel view synthesis; further, material-lighting disentanglement on 3DGS warrants relighting capabilities and its adaptability to broader applications. While the general approach to the latter operation lies in integrating differentiable physically-based rendering (PBR) techniques to jointly recover BRDF materials and environment lighting, achieving a precise disentanglement remains an inherently difficult task due to the challenge of accurately modeling light transport. Existing approaches typically approximate Gaussian points' normals, which constitute an implicit geometric constraint. However, they usually suffer from inaccuracies in normal estimation that subsequently degrade light transport, resulting in noisy material decomposition and flawed relighting results. To address this, we propose GeoSplatting, a novel approach that augments 3DGS with explicit geometry guidance for precise light transport modeling. By differentiably constructing a surface-grounded 3DGS from an optimizable mesh, our approach leverages well-defined mesh normals and the opaque mesh surface, and additionally facilitates the use of mesh-based ray tracing techniques for efficient, occlusion-aware light transport calculations. This enhancement ensures precise material decomposition while preserving the efficiency and high-quality rendering capabilities of 3DGS. Comprehensive evaluations across diverse datasets demonstrate the effectiveness of GeoSplatting, highlighting its superior efficiency and state-of-the-art inverse rendering performance",
    "checked": true,
    "id": "f901a2d3ce9bafaa9685692e8a632036daca2efb",
    "semantic_title": "geosplatting: towards geometry guided gaussian splatting for physically-based inverse rendering",
    "citation_count": 5,
    "authors": [
      "Kai Ye",
      "Chong Gao",
      "Guanbin Li",
      "Wenzheng Chen",
      "Baoquan Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cheng_Temporal-aware_Query_Routing_for_Real-time_Video_Instance_Segmentation_ICCV_2025_paper.html": {
    "title": "Temporal-aware Query Routing for Real-time Video Instance Segmentation",
    "volume": "main",
    "abstract": "With the rise of applications such as embodied intelligence, developing high real-time online video instance segmentation (VIS) has become increasingly important. However, through time profiling of the components in advanced online VIS architecture (i.e., transformer-based architecture), we find that the transformer decoder significantly hampers the inference speed. Further analysis of the similarities between the outputs from adjacent frames at each transformer decoder layer reveals significant redundant computations within the transformer decoder. To address this issue, we introduce Temporal-Aware query Routing (TAR) mechanism. We embed it before each transformer decoder layer. By fusing the optimal queries from the previous frame, the queries output by the preceding decoder layer, and their differential information, TAR predicts a binary classification score and then uses an argmax operation to determine whether the current layer should be skipped. Experimental results demonstrate that integrating TAR into the baselines achieves significant efficiency gains (24.7 - 34.6 FPS for MinVIS, 22.4 - 32.8 FPS for DVIS++) while also improving performance (e.g., on YoutubeVIS 2019, 47.4 - 48.4 AP for MinVIS, 55.5 - 55.7 AP for DVIS++). Furthermore, our analysis of the TAR mechanism shows that the number of skipped layers increases as the differences between adjacent video frames decrease, which suggests that our method effectively utilizes inter-frame differences to reduce redundant computations in the transformer decoder",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zesen Cheng",
      "Kehan Li",
      "Yian Zhao",
      "Hang Zhang",
      "Chang Liu",
      "Jie Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhu_Accelerating_Diffusion_Sampling_via_Exploiting_Local_Transition_Coherence_ICCV_2025_paper.html": {
    "title": "Accelerating Diffusion Sampling via Exploiting Local Transition Coherence",
    "volume": "main",
    "abstract": "Text-based diffusion models have made significant breakthroughs in generating high-quality images and videos from textual descriptions. However, the lengthy sampling time of the denoising process remains a significant bottleneck in practical applications. Previous methods either ignore the statistical relationships between adjacent steps or rely on attention or feature similarity between them, which often only works with specific network structures. To address this issue, we discover a new statistical relationship in the transition operator between adjacent steps, focusing on the relationship of the outputs from the network. This relationship does not impose any requirements on the network structure. Based on this observation, we propose a novel training-free acceleration method called LTC-Accel, which uses the identified relationship to estimate the current transition operator based on adjacent steps. Due to no specific assumptions regarding the network structure, LTC-Accel is applicable to almost all diffusion-based methods and orthogonal to almost all existing acceleration techniques, making it easy to combine with them. Experimental results demonstrate that LTC-Accel significantly speeds up sampling in text-to-image and text-to-video synthesis while maintaining competitive sample quality. Specifically, LTC-Accel achieves a speedup of 1.67 times in Stable Diffusion v2 and a speedup of 1.55 times in video generation models. When combined with distillation models, LTC-Accel achieves a remarkable 10 times speedup in video generation, allowing real-time generation of more than 16 FPS. Our code (include colab version) is available on https://zhushangwen.github.io/LTC-accel.io/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shangwen Zhu",
      "Han Zhang",
      "Zhantao Yang",
      "Qianyu Peng",
      "Zhao Pu",
      "Huangji Wang",
      "Fan Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Baraldi_What_Changed_Detecting_and_Evaluating_Instruction-Guided_Image_Edits_with_Multimodal_ICCV_2025_paper.html": {
    "title": "What Changed? Detecting and Evaluating Instruction-Guided Image Edits with Multimodal Large Language Models",
    "volume": "main",
    "abstract": "Instruction-based image editing models offer increased personalization opportunities in generative tasks. However, properly evaluating their results is challenging, and most of the existing metrics lag in terms of alignment with human judgment and explainability. To tackle these issues, we introduce DICE (DIfference Coherence Estimator), a model designed to detect localized differences between the original and the edited image and to assess their relevance to the given modification request. DICE consists of two key components: a difference detector and a coherence estimator, both built on an autoregressive Multimodal Large Language Model (MLLM) and trained using a strategy that leverages self-supervision, distillation from inpainting networks, and full supervision. Through extensive experiments, we evaluate each stage of our pipeline, comparing different MLLMs within the proposed framework. We demonstrate that DICE effectively identifies coherent edits, effectively evaluating images generated by different editing models with a strong correlation with human judgment. We publicly release our source code, models, and data at https://aimagelab.github.io/DICE",
    "checked": true,
    "id": "5ddeb50d71e63b7e1d690441dce1577ba8df3030",
    "semantic_title": "what changed? detecting and evaluating instruction-guided image edits with multimodal large language models",
    "citation_count": 2,
    "authors": [
      "Lorenzo Baraldi",
      "Davide Bucciarelli",
      "Federico Betti",
      "Marcella Cornia",
      "Lorenzo Baraldi",
      "Nicu Sebe",
      "Rita Cucchiara"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Park_Saliency-Aware_Quantized_Imitation_Learning_for_Efficient_Robotic_Control_ICCV_2025_paper.html": {
    "title": "Saliency-Aware Quantized Imitation Learning for Efficient Robotic Control",
    "volume": "main",
    "abstract": "Deep neural network (DNN)-based policy models, such as vision-language-action (VLA) models, excel at automating complex decision-making from multi-modal inputs. However, scaling these models greatly increases computational overhead, complicating deployment in resource-constrained settings like robot manipulation and autonomous driving. To address this, we propose Saliency-Aware Quantized Imitation Learning (\\method), which combines quantization-aware training with a selective loss-weighting strategy for mission-critical states. By identifying these states via saliency scores and emphasizing them in the training loss, \\method preserves decision fidelity under low-bit precision. We validate \\method's generalization capability across extensive simulation benchmarks with environment variations, real-world tasks, and cross-domain tasks (self-driving, physics simulation), consistently recovering full-precision performance. Notably, a 4-bit weight-quantized VLA model for robotic manipulation achieves up to 2.5xspeedup and 2.5xenergy savings on an edge GPU with minimal accuracy loss. These results underline \\method's potential for efficiently deploying large IL-based policy models on resource-limited devices",
    "checked": true,
    "id": "727994e50fd985b875707641f4ce43097eaba474",
    "semantic_title": "saliency-aware quantized imitation learning for efficient robotic control",
    "citation_count": 0,
    "authors": [
      "Seongmin Park",
      "Hyungmin Kim",
      "Sangwoo Kim",
      "Wonseok Jeon",
      "Juyoung Yang",
      "Byeongwook Jeon",
      "Yoonseon Oh",
      "Jungwook Choi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ye_Where_am_I_Cross-View_Geo-localization_with_Natural_Language_Descriptions_ICCV_2025_paper.html": {
    "title": "Where am I? Cross-View Geo-localization with Natural Language Descriptions",
    "volume": "main",
    "abstract": "Cross-view geo-localization identifies the locations of street-view images by matching them with geo-tagged satellite images or OSM. However, most existing studies focus on image-to-image retrieval, with fewer addressing text-guided retrieval, a task vital for applications like pedestrian navigation and emergency response.In this work, we introduce a novel task for cross-view geo-localization with natural language descriptions, which aims to retrieve corresponding satellite images or OSM database based on scene text descriptions. To support this task, we construct the CVG-Text dataset by collecting cross-view data from multiple cities and employing a scene text generation approach that leverages the annotation capabilities of Large Multimodal Models to produce high-quality scene text descriptions with localization details. Additionally, we propose a novel text-based retrieval localization method, CrossText2Loc, which improves recall by 10% and demonstrates excellent long-text retrieval capabilities. In terms of explainability, it not only provides similarity scores but also offers retrieval reasons. More information can be found at https://github.com/yejy53/CVG-Text",
    "checked": true,
    "id": "2a5a06c69421da0c446a3e3c8d9e26301c01e87d",
    "semantic_title": "where am i? cross-view geo-localization with natural language descriptions",
    "citation_count": 10,
    "authors": [
      "Junyan Ye",
      "Honglin Lin",
      "Leyan Ou",
      "Dairong Chen",
      "Zihao Wang",
      "Qi Zhu",
      "Conghui He",
      "Weijia Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Deng_Boost_3D_Reconstruction_using_Diffusion-based_Monocular_Camera_Calibration_ICCV_2025_paper.html": {
    "title": "Boost 3D Reconstruction using Diffusion-based Monocular Camera Calibration",
    "volume": "main",
    "abstract": "In this paper, we present DM-Calib, a diffusion-based approach for estimating pinhole camera intrinsic parameters from a single input image. Monocular camera calibration is essential for many 3D vision tasks. However, most existing methods depend on handcrafted assumptions or are constrained by limited training data, resulting in poor generalization across diverse real-world images. Recent advancements in stable diffusion models, trained on massive data, have shown the ability to generate high-quality images with varied characteristics. Emerging evidence indicates that these models implicitly capture the relationship between camera focal length and image content. Building on this insight, we explore how to leverage the powerful priors of diffusion models for monocular pinhole camera calibration. Specifically, we introduce a new image-based representation, termed Camera Image, which losslessly encodes the numerical camera intrinsics and integrates seamlessly with the diffusion framework. Using this representation, we reformulate the problem of estimating camera intrinsics as the generation of a dense Camera Image conditioned on an input image. By fine-tuning a stable diffusion model to generate a Camera Image from a single RGB input, we can extract camera intrinsics via a RANSAC operation. We further demonstrate that our monocular calibration method enhances performance across various 3D tasks, including zero-shot metric depth estimation, 3D metrology, pose estimation and sparse-view reconstruction. Extensive experiments on multiple public datasets show that our approach significantly outperforms baselines and provides broad benefits to 3D vision tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyuan Deng",
      "Wei Yin",
      "Xiaoyang Guo",
      "Qian Zhang",
      "Xiaotao Hu",
      "Weiqiang Ren",
      "Xiao-Xiao Long",
      "Ping Tan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wei_GTR_Guided_Thought_Reinforcement_Prevents_Thought_Collapse_in_RL-based_VLM_ICCV_2025_paper.html": {
    "title": "GTR: Guided Thought Reinforcement Prevents Thought Collapse in RL-based VLM Agent Training",
    "volume": "main",
    "abstract": "Reinforcement learning with verifiable outcome rewards (RLVR) has effectively scaled up chain-of-thought (CoT) reasoning in large language models (LLMs). Yet, its efficacy in training vision-language model (VLM) agents for goal-directed action reasoning in visual environments is less established. This work investigates this problem through extensive experiments on complex card games, such as 24 points, and embodied tasks from ALFWorld. We find that when rewards are based solely on action outcomes, RL fails to incentivize CoT reasoning in VLMs, instead leading to a phenomenon we termed thought collapse, characterized by a rapid loss of diversity in the agent's thoughts, state-irrelevant and incomplete reasoning, and subsequent invalid actions, resulting in negative rewards. To counteract thought collapse, we highlight the necessity of process guidance and propose an automated corrector that evaluates and refines the agent's reasoning at each RL step. This simple and scalable GTR (Guided Thought Reinforcement) framework trains reasoning and action simultaneously without the need for dense, per-step human labeling. Our experiments demonstrate that GTR significantly enhances the performance and generalization of the LLaVA-7B model across various visual environments, achieving 3-5 times higher task success rates compared to SoTA models with notably smaller model sizes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tong Wei",
      "Yijun Yang",
      "Junliang Xing",
      "Yuanchun Shi",
      "Zongqing Lu",
      "Deheng Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Miao_Multi-view_Gaze_Target_Estimation_ICCV_2025_paper.html": {
    "title": "Multi-view Gaze Target Estimation",
    "volume": "main",
    "abstract": "This paper presents a method that utilizes multiple camera views for the gaze target estimation (GTE) task. The approach integrates information from different camera views to improve accuracy and expand applicability, addressing limitations in existing single-view methods that face challenges such as face occlusion, target ambiguity, and out-of-view targets. Our method processes a pair of camera views as input, incorporating a Head Information Aggregation (HIA) module for leveraging head information from both views for more accurate gaze estimation, an Uncertainty-based Gaze Selection (UGS) for identifying the most reliable gaze output, and an Epipolar-based Scene Attention (ESA) module for cross-view background information sharing. This approach significantly outperforms single-view baselines, especially when the second camera provides a clear view of the person's face. Additionally, our method can estimate the gaze target in the first view using the image of the person in the second view only, a capability not possessed by single-view GTE methods. Furthermore, the paper introduces a multi-view dataset for developing and evaluating multi-view GTE methods. Data and code are available",
    "checked": true,
    "id": "a846afc40cf436f2970f04b71fdc1aea166432d2",
    "semantic_title": "multi-view gaze target estimation",
    "citation_count": 0,
    "authors": [
      "Qiaomu Miao",
      "Vivek Raju Golani",
      "Jingyi Xu",
      "Progga Paromita Dutta",
      "Minh Hoai",
      "Dimitris Samaras"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Semantic_Discrepancy-aware_Detector_for_Image_Forgery_Identification_ICCV_2025_paper.html": {
    "title": "Semantic Discrepancy-aware Detector for Image Forgery Identification",
    "volume": "main",
    "abstract": "With the rapid advancement of image generation techniques, robust forgery detection has become increasingly imperative to ensure the trustworthiness of digital media. Recent research indicates that the learned semantic concepts of pre-trained models are critical for identifying fake images. However, the misalignment between the forgery and semantic concept spaces hinders the model's forgery detection performance. To address this problem, we propose a novel Semantic Discrepancy-aware Detector (SDD) that leverages reconstruction learning to align the two spaces at a fine-grained visual level. By exploiting the conceptual knowledge embedded in the pre-trained vision-language model, we specifically design a semantic token sampling module to mitigate the space shifts caused by features irrelevant to both forgery traces and semantic concepts. A concept-level forgery discrepancy learning module, built upon a visual reconstruction paradigm, is proposed to strengthen the interaction between visual semantic concepts and forgery traces, effectively capturing discrepancies under the concepts' guidance. Finally, the low-level forgery feature enhancemer integrates the learned concept-level forgery discrepancies to minimize redundant forgery information. Experiments conducted on two standard image forgery datasets demonstrate the efficacy of the proposed SDD, which achieves superior results compared to existing methods. The code is available at https://github.com/wzy1111111/SSD",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziye Wang",
      "Minghang Yu",
      "Chunyan Xu",
      "Zhen Cui"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_CapeLLM_Support-Free_Category-Agnostic_Pose_Estimation_with_Multimodal_Large_Language_Models_ICCV_2025_paper.html": {
    "title": "CapeLLM: Support-Free Category-Agnostic Pose Estimation with Multimodal Large Language Models",
    "volume": "main",
    "abstract": "Category-agnostic pose estimation (CAPE) has traditionally relied on support images with annotated keypoints, a process that is often cumbersome and may fail to fully capture the necessary correspondences across diverse object categories. Recent efforts have explored the use of text queries, leveraging their enhanced stability and generalization capabilities. However, existing approaches often remain constrained by their reliance on support queries, their failure to fully utilize the rich priors embedded in pre-trained large language models, and the limitations imposed by their parametric distribution assumptions. To address these challenges, we introduce CapeLLM, the first multimodal large language model (MLLM) designed for CAPE. Our method only employs query image and detailed text descriptions as an input to estimate category-agnostic keypoints. Our method encompasses effective training strategies and carefully designed instructions for applying the MLLM to CAPE. Moreover, we propose an inference mechanism that further enhances the reasoning process for unseen keypoints. while flexibly modeling their underlying spatial distribution and uncertainty, allowing for adaptive refinement based on contextual cues. We conducted extensive experiments to apply the MLLM to CAPE effectively, focusing not only on the model architecture and prompt design but also on ensuring robustness across input variations. Our approach sets a new state-of-the-art on the MP-100 benchmark in the 1-shot and even 5-shot setting, marking a significant advancement in the field of category-agnostic pose estimation. Code is available https://github.com/Junhojuno/CapeLLM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junho Kim",
      "Hyungjin Chung",
      "Byung-Hoon Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lu_VisHall3D_Monocular_Semantic_Scene_Completion_from_Reconstructing_the_Visible_Regions_ICCV_2025_paper.html": {
    "title": "VisHall3D: Monocular Semantic Scene Completion from Reconstructing the Visible Regions to Hallucinating the Invisible Regions",
    "volume": "main",
    "abstract": "This paper introduces VisHall3D, a novel two-stage framework for monocular semantic scene completion that aims to address the issues of feature entanglement and geometric inconsistency prevalent in existing methods. VisHall3D decomposes the scene completion task into two stages: reconstructing the visible regions (vision) and inferring the invisible regions (hallucination). In the first stage, VisFrontierNet, a visibility-aware projection module, is introduced to accurately trace the visual frontier while preserving fine-grained details. In the second stage, OcclusionMAE, a hallucination network, is employed to generate plausible geometries for the invisible regions using a noise injection mechanism. By decoupling scene completion into these two distinct stages, VisHall3D effectively mitigates feature entanglement and geometric inconsistency, leading to significantly improved reconstruction quality.The effectiveness of VisHall3D is validated through extensive experiments on two challenging benchmarks: SemanticKITTI and SSCBench-KITTI-360. VisHall3D achieves state-of-the-art performance, outperforming previous methods by a significant margin and paves the way for more accurate and reliable scene understanding in autonomous driving and other applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoang Lu",
      "Yuanqi Su",
      "Xiaoning Zhang",
      "Longjun Gao",
      "Yu Xue",
      "Le Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Controllable_3D_Outdoor_Scene_Generation_via_Scene_Graphs_ICCV_2025_paper.html": {
    "title": "Controllable 3D Outdoor Scene Generation via Scene Graphs",
    "volume": "main",
    "abstract": "Three-dimensional scene generation is crucial in computer vision, with applications spanning autonomous driving and gaming. However, current methods offer limited or non-intuitive user control. In this work, we propose a method that uses scene graph as a user-friendly control format to generate outdoor 3D scenes. We develop an interactive system that transforms a sparse scene graph into a dense Bird's Eye View (BEV) Embedding Map, which guides a conditional diffusion model to generate 3D scenes that match the scene graph description. Users can easily create or modify scene graphs to generate large-scale outdoor scenes. We create a large-scale dataset with paired scene graphs and 3D semantic scenes to train the BEV embedding and diffusion models. Experimental results show that our approach consistently produces high-quality 3D urban scenes closely aligned with the input scene graphs. To the best of our knowledge, this is the first approach to generate 3D outdoor scenes conditioned on scene graphs. Code is available at https://github.com/yuhengliu02/control-3d-scene",
    "checked": true,
    "id": "fc46095bc8e19f026193f68dcc5d132c1fac7a6c",
    "semantic_title": "controllable 3d outdoor scene generation via scene graphs",
    "citation_count": 2,
    "authors": [
      "Yuheng Liu",
      "Xinke Li",
      "Yuning Zhang",
      "Lu Qi",
      "Xin Li",
      "Wenping Wang",
      "Chongshou Li",
      "Xueting Li",
      "Ming-Hsuan Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Han_JPEG_Processing_Neural_Operator_for_Backward-Compatible_Coding_ICCV_2025_paper.html": {
    "title": "JPEG Processing Neural Operator for Backward-Compatible Coding",
    "volume": "main",
    "abstract": "Despite significant advances in learning-based lossy compression algorithms, standardizing codecs remains a critical challenge. In this paper, we present the JPEG Processing Neural Operator (JPNeO), a next-generation JPEG algorithm that maintains full backward compatibility with the current JPEG format. Our JPNeO improves chroma component preservation and enhances reconstruction fidelity compared to existing artifact removal methods by incorporating neural operators in both the encoding and decoding stages. JPNeO achieves practical benefits in terms of reduced memory usage and parameter count. We further validate our hypothesis about the existence of a space with high mutual information through empirical evidence. In summary, the JPNeO functions as a high-performance out-of-the-box image compression pipeline without changing source coding's protocol. Our source code is available at https://github.com/WooKyoungHan/JPNeO",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Woo Kyoung Han",
      "Yongjun Lee",
      "Byeonghun Lee",
      "Sang Hyun Park",
      "Sunghoon Im",
      "Kyong Hwan Jin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lu_DPoser-X_Diffusion_Model_as_Robust_3D_Whole-body_Human_Pose_Prior_ICCV_2025_paper.html": {
    "title": "DPoser-X: Diffusion Model as Robust 3D Whole-body Human Pose Prior",
    "volume": "main",
    "abstract": "We present DPoser-X, a diffusion-based prior model for 3D whole-body human poses. Building a versatile and robust full-body human pose prior remains challenging due to the inherent complexity of articulated human poses and the scarcity of high-quality whole-body pose datasets. To address these limitations, we introduce a Diffusion model as body Pose prior (DPoser) and extend it to DPoser-X for expressive whole-body human pose modeling.Our approach unifies various pose-centric tasks as inverse problems, solving them through variational diffusion sampling. To enhance performance on downstream applications, we introduce a novel truncated timestep scheduling method specifically designed for pose data characteristics. We also propose a masked training mechanism that effectively combines whole-body and part-specific datasets, enabling our model to capture interdependencies between body parts while avoiding overfitting to specific actions.Extensive experiments demonstrate DPoser-X's robustness and versatility across multiple benchmarks for body, hand, face, and full-body pose modeling. Our model consistently outperforms state-of-the-art alternatives, establishing a new benchmark for whole-body human pose prior modeling",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junzhe Lu",
      "Jing Lin",
      "Hongkun Dou",
      "Ailing Zeng",
      "Yue Deng",
      "Xian Liu",
      "Zhongang Cai",
      "Lei Yang",
      "Yulun Zhang",
      "Haoqian Wang",
      "Ziwei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhen_Learning_4D_Embodied_World_Models_ICCV_2025_paper.html": {
    "title": "Learning 4D Embodied World Models",
    "volume": "main",
    "abstract": "This paper presents an effective approach for learning novel 4D embodied world models, which predict the dynamic evolution of 3D scenes over time in response to an embodied agent's actions, providing both spatial and temporal consistency. We propose to learn a 4D world model by training on RGB-DN (RGB, Depth, and Normal) videos.This not only surpasses traditional 2D models by incorporating detailed shape, configuration, and temporal changes into their predictions, but also allows us to effectively learn accurate inverse dynamic models for an embodied agent. Specifically, we first extend existing robotic manipulation video datasets with depth and normal information leveraging off-the-shelf models. Next, we fine-tune a video generation model on this annotated dataset, which jointly predicts RGB-DN (RGB, Depth, and Normal) for each frame. We then present an algorithm to directly convert generated RGB, Depth, and Normal videos into a high-quality 4D scene of the world. Our method ensures temporal and spatial coherence in 4D scene predictions from embodied scenarios, enables novel view synthesis for embodied environments, and facilitates policy learning that significantly outperforms those derived from prior video-based world models",
    "checked": false,
    "id": "bdb7f4493f9f4729c56bf28fb982c686d33681bb",
    "semantic_title": "tesseract: learning 4d embodied world models",
    "citation_count": 21,
    "authors": [
      "Haoyu Zhen",
      "Qiao Sun",
      "Hongxin Zhang",
      "Junyan Li",
      "Siyuan Zhou",
      "Yilun Du",
      "Chuang Gan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/He_FlowTok_Flowing_Seamlessly_Across_Text_and_Image_Tokens_ICCV_2025_paper.html": {
    "title": "FlowTok: Flowing Seamlessly Across Text and Image Tokens",
    "volume": "main",
    "abstract": "Bridging different modalities lies at the heart of cross-modality generation. While conventional approaches treat the text modality as a conditioning signal that gradually guides the denoising process from Gaussian noise to the target image modality, we explore a much simpler paradigm--directly evolving between text and image modalities through flow matching. This requires projecting both modalities into a shared latent space, which poses a significant challenge due to their inherently different representations: text is highly semantic and encoded as 1D tokens, whereas images are spatially redundant and represented as 2D latent embeddings. To address this, we introduce FlowTok, a minimal framework that seamlessly flows across text and images by encoding images into a compact 1D token representation. Compared to prior methods, this design reduces the latent space size by 3.3x at an image resolution of 256, eliminating the need for complex conditioning mechanisms or noise scheduling. Moreover, FlowTok naturally extends to image-to-text generation under the same formulation. With its streamlined architecture centered around compact 1D tokens, FlowTok is highly memory-efficient, requires significantly fewer training resources, and achieves much faster sampling speeds--all while delivering performance comparable to state-of-the-art models. Code is available at https://github.com/TACJu/FlowTok",
    "checked": true,
    "id": "c9a95ebd24f3bb96ed352d68194fd0b23f7205a0",
    "semantic_title": "flowtok: flowing seamlessly across text and image tokens",
    "citation_count": 7,
    "authors": [
      "Ju He",
      "Qihang Yu",
      "Qihao Liu",
      "Liang-Chieh Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ayalew_PROGRESSOR_A_Perceptually_Guided_Reward_Estimator_with_Self-Supervised_Online_Refinement_ICCV_2025_paper.html": {
    "title": "PROGRESSOR: A Perceptually Guided Reward Estimator with Self-Supervised Online Refinement",
    "volume": "main",
    "abstract": "We present PROGRESSOR, a novel framework that learns a task-agnostic reward function from videos, enabling policy training through goal-conditioned reinforcement learning (RL) without manual supervision. Underlying this reward is an estimate of the distribution over task progress as a function of the current, initial, and goal observations that is learned in a self-supervised fashion. Crucially, PROGRESSOR refines rewards adversarially during online RL training by pushing back predictions for out-of-distribution observations in order to mitigate distribution shift inherent in non-expert observations. Utilizing this progress prediction as a dense reward together with an adversarial push-back, we show that PROGRESSOR enables robots to learn complex behaviors without any external supervision. Pretrained on large-scale egocentric human video from EPIC-KITCHENS, PROGRESSOR requires no fine-tuning on in-domain task-specific data for generalization to real-robot offline RL under noisy demonstrations, outperforming contemporary methods that provide dense visual reward for robotic learning. Our findings highlight the potential of PROGRESSOR for scalable robotic applications where direct action labels and task-specific rewards are not readily available",
    "checked": true,
    "id": "990cf36607011a3953c99b0c0fe878309cd83c19",
    "semantic_title": "progressor: a perceptually guided reward estimator with self-supervised online refinement",
    "citation_count": 2,
    "authors": [
      "Tewodros W. Ayalew",
      "Xiao Zhang",
      "Kevin Yuanbo Wu",
      "Tianchong Jiang",
      "Michael Maire",
      "Matthew R. Walter"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_UniOcc_A_Unified_Benchmark_for_Occupancy_Forecasting_and_Prediction_in_ICCV_2025_paper.html": {
    "title": "UniOcc: A Unified Benchmark for Occupancy Forecasting and Prediction in Autonomous Driving",
    "volume": "main",
    "abstract": "We introduce UniOcc, a comprehensive, unified benchmark and toolkit for occupancy forecasting (i.e., predicting future occupancies based on historical information) and occupancy prediction (i.e., predicting current-frame occupancy from camera images. UniOcc unifies the data from multiple real-world datasets (i.e., nuScenes, Waymo) and high-fidelity driving simulators (i.e., CARLA, OpenCOOD), providing 2D/3D occupancy labels and annotating innovative per-voxel flows. Unlike existing studies that rely on suboptimal pseudo labels for evaluation, UniOcc incorporates novel evaluation metrics that do not depend on ground-truth labels, enabling robust assessment on additional aspects of occupancy quality. Through extensive experiments on state-of-the-art models, we demonstrate that large-scale, diverse training data and explicit flow information significantly enhance occupancy prediction and forecasting performance. Our data and code are available at https://uniocc.github.io/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuping Wang",
      "Xiangyu Huang",
      "Xiaokang Sun",
      "Mingxuan Yan",
      "Shuo Xing",
      "Zhengzhong Tu",
      "Jiachen Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_AdaHuman_Animatable_Detailed_3D_Human_Generation_with_Compositional_Multiview_Diffusion_ICCV_2025_paper.html": {
    "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion",
    "volume": "main",
    "abstract": "Existing methods for image-to-3D avatar generation struggle to produce highly detailed, animation-ready avatars suitable for real-world applications. We introduce AdaHuman, a novel framework that generates high-fidelity animatable 3D avatars from a single in-the-wild image. AdaHuman incorporates two key innovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes consistent multi-view images in arbitrary poses alongside corresponding 3D Gaussian Splats (3DGS) reconstruction at each diffusion step; (2) A compositional 3DGS refinement module that enhances the details of local body parts through image-to-image refinement and seamlessly integrates them using a novel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These components allow AdaHuman to generate highly realistic standardized A-pose avatars with minimal self-occlusion, enabling rigging and animation with any input motion. Extensive evaluation on public benchmarks and in-the-wild images demonstrates that AdaHuman significantly outperforms state-of-the-art methods in both avatar reconstruction and reposing. Code and models will be publicly available for research purposes",
    "checked": true,
    "id": "df2d65e2dadfc85722e2fd0f34839fd8073a0a47",
    "semantic_title": "adahuman: animatable detailed 3d human generation with compositional multiview diffusion",
    "citation_count": 0,
    "authors": [
      "Yangyi Huang",
      "Ye Yuan",
      "Xueting Li",
      "Jan Kautz",
      "Umar Iqbal"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Bai_ReCamMaster_Camera-Controlled_Generative_Rendering_from_A_Single_Video_ICCV_2025_paper.html": {
    "title": "ReCamMaster: Camera-Controlled Generative Rendering from A Single Video",
    "volume": "main",
    "abstract": "Camera control has been actively studied in text or image conditioned video generation tasks. However, altering camera trajectories of a given video remains under-explored, despite its importance in the field of video creation. It is non-trivial due to the extra constraints of maintaining multiple-frame appearance and dynamic synchronization. To address this, we present ReCamMaster, a camera-controlled generative video re-rendering framework that reproduces the dynamic scene of an input video at novel camera trajectories. The core innovation lies in harnessing the generative capabilities of pre-trained text-to-video models through an elegant yet powerful video conditioning mechanism--an aspect often overlooked in current research. To overcome the scarcity of qualified training data, we construct a comprehensive multi-camera synchronized video dataset using Unreal Engine 5, which is carefully curated to follow real-world filming characteristics, covering diverse scenes and camera movements. It helps the model generalize to in-the-wild videos. Lastly, we further improve the robustness to diverse inputs through a meticulously designed training strategy. Extensive experiments show that our method substantially outperforms existing state-of-the-art approaches and strong baselines. Our method also finds promising applications in video stabilization, super-resolution, and outpainting. Our code and dataset are publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianhong Bai",
      "Menghan Xia",
      "Xiao Fu",
      "Xintao Wang",
      "Lianrui Mu",
      "Jinwen Cao",
      "Zuozhu Liu",
      "Haoji Hu",
      "Xiang Bai",
      "Pengfei Wan",
      "Di Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Saleem_MaskHand_Generative_Masked_Modeling_for_Robust_Hand_Mesh_Reconstruction_in_ICCV_2025_paper.html": {
    "title": "MaskHand: Generative Masked Modeling for Robust Hand Mesh Reconstruction in the Wild",
    "volume": "main",
    "abstract": "Reconstructing a 3D hand mesh from a single RGB image is challenging due to complex articulations, self-occlusions, and depth ambiguities. Traditional discriminative methods, which learn a deterministic mapping from a 2D image to a single 3D mesh, often struggle with the inherent ambiguities in 2D-to-3D mapping. To address this challenge, we propose MaskHand, a novel generative masked model for hand mesh recovery that synthesizes plausible 3D hand meshes by learning and sampling from the probabilistic distribution of the ambiguous 2D-to-3D mapping process. MaskHand consists of two key components: (1) a VQ-MANO, which encodes 3D hand articulations as discrete pose tokens in a latent space, and (2) a Context-Guided Masked Transformer that randomly masks out pose tokens and learns their joint distribution, conditioned on corrupted token sequence, image context, and 2D pose cues. This learned distribution facilitates confidence-guided sampling during inference, producing mesh reconstructions with low uncertainty and high precision. Extensive evaluations on benchmark and real-world datasets demonstrate that MaskHand achieves state-of-the-art accuracy, robustness, and realism in 3D hand mesh reconstruction. Project website: https://m-usamasaleem.github.io/publication/MaskHand/MaskHand.html",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Usama Saleem",
      "Ekkasit Pinyoanuntapong",
      "Mayur Jagdishbhai Patel",
      "Hongfei Xue",
      "Ahmed Helmy",
      "Srijan Das",
      "Pu Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tran_Image-Guided_Shape-from-Template_Using_Mesh_Inextensibility_Constraints_ICCV_2025_paper.html": {
    "title": "Image-Guided Shape-from-Template Using Mesh Inextensibility Constraints",
    "volume": "main",
    "abstract": "Shape-from-Template (SfT) refers to the class of methods that reconstruct the 3D shape of a deforming object from images/videos using a 3D template. Traditional SfT methods require point correspondences between images and the texture of the 3D template in order to reconstruct 3D shapes from images/videos in real time. Their performance severely degrades when encountered with severe occlusions in the images because of the unavailability of correspondences. In contrast, modern SfT methods use a correspondence-free approach by incorporating deep neural networks to reconstruct 3D objects, thus requiring huge amounts of data for supervision. Recent advances use a fully unsupervised or self-supervised approach by combining differentiable physics and graphics to deform 3D template to match input images. In this paper, we propose an unsupervised SfT which uses only image observations: color features, gradients and silhouettes along with a mesh inextensibility constraint to reconstruct at a 400xfaster pace than (best-performing) unsupervised SfT. Moreover, when it comes to generating finer details and severe occlusions, our method outperforms the existing methodologies by a large margin. Code is available at https://github.com/dvttran/nsft",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thuy Tran",
      "Ruochen Chen",
      "Shaifali Parashar"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Dong_Confound_from_All_Sides_Distill_with_Resilience_Multi-Objective_Adversarial_Paths_ICCV_2025_paper.html": {
    "title": "Confound from All Sides, Distill with Resilience: Multi-Objective Adversarial Paths to Zero-Shot Robustness",
    "volume": "main",
    "abstract": "Adversarially robust knowledge distillation transfers the robustness of a large-scale teacher model to a lightweight student while preserving natural performance. However, foundation Vision-Language Models (VLMs) also demand the transfer of zero-shot inference capabilities. We find that standard robust distillation using untargeted adversarial examples fails to transfer out-of-distribution (zero-shot) robustness, as these adversaries primarily push inputs away from their original distribution, exploring a limited portion of the teacher's decision space and missing more diverse failure modes. A natural solution is to generate multiple targeted adversaries that traverse diverse paths across decision boundaries. Thus, these adversaries probe a broader region of the teacher's decision surface. However, naive targeted adversary optimization often converges to local optima within a single category's decision region, limiting the diversity. To address this, we propose a Multi-Objective Optimization (MOO)-based adversarial distillation framework that transfers robustness from large VLMs to lightweight ones by exploiting adversaries with two main objectives: misclassification and category-level adversarial diversity. Theoretically, we show that optimizing for diversity mitigates adversarial collapse into local optima, ensuring adversaries span multiple decision regions and capture the teacher's generalizable robust features. Extensive experiments demonstrate the superiority of our method over state-of-the-art adversarial learning across diverse scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junhao Dong",
      "Jiao Liu",
      "Xinghua Qu",
      "Yew-Soon Ong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Dasgupta_NGD_Neural_Gradient_Based_Deformation_for_Monocular_Garment_Reconstruction_ICCV_2025_paper.html": {
    "title": "NGD: Neural Gradient Based Deformation for Monocular Garment Reconstruction",
    "volume": "main",
    "abstract": "Dynamic garment reconstruction from monocular video is an important yet challenging task due to the complex dynamics and unconstrained nature of the garments. Recent advancements in neural rendering have enabled high-quality geometric reconstruction with image/video supervision. However, implicit representation methods that use volume rendering often provide smooth geometry and fail to model high-frequency details. While template reconstruction methods model explicit geometry, they use vertex displacement for deformation which results in artifacts. Addressing these limitations, we propose NGD, a Neural Gradient-based Deformation method to reconstruct dynamically evolving textured garments from monocular videos. Additionally, we propose a novel adaptive remeshing strategy for modeling dynamically evolving surfaces like wrinkles and pleats of the skirt, leading to high-quality reconstruction. Finally, we learn dynamic texture maps to capture per-frame lighting and shadow effects. We provide extensive qualitative and quantitative evaluations to demonstrate significant improvements over existing SOTA methods and provide high-quality garment reconstructions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soham Dasgupta",
      "Shanthika  Naik",
      "Preet  Savalia",
      "Sujay Kumar  Ingle",
      "Avinash  Sharma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Learned_Image_Compression_with_Hierarchical_Progressive_Context_Modeling_ICCV_2025_paper.html": {
    "title": "Learned Image Compression with Hierarchical Progressive Context Modeling",
    "volume": "main",
    "abstract": "Context modeling is essential in learned image compression for accurately estimating the distribution of latents. While recent advanced methods have expanded context modeling capacity, they still struggle to efficiently exploit long-range dependency and diverse context information across different coding steps. In this paper, we introduce a novel Hierarchical Progressive Context Model (HPCM) for more efficient context information acquisition. Specifically, HPCM employs a hierarchical coding schedule to sequentially model the contextual dependencies among latents at multiple scales, which enables more efficient long-range context modeling. Furthermore, we propose a progressive context fusion mechanism that incorporates contextual information from previous coding steps into the current step, effectively exploiting diverse contextual information. Experimental results demonstrate that our method achieves state-of-the-art rate-distortion performance and strikes a better balance between compression performance and computational complexity. The code is available at https://github.com/lyq133/LIC-HPCM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuqi Li",
      "Haotian Zhang",
      "Li Li",
      "Dong Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_U-ViLAR_Uncertainty-Aware_Visual_Localization_for_Autonomous_Driving_via_Differentiable_Association_ICCV_2025_paper.html": {
    "title": "U-ViLAR: Uncertainty-Aware Visual Localization for Autonomous Driving via Differentiable Association and Registration",
    "volume": "main",
    "abstract": "Accurate localization using visual information is a critical yet challenging task, especially in urban environments where nearby buildings and construction sites significantly degrade GNSS (Global Navigation Satellite System) signal quality. This issue underscores the importance of visual localization techniques in scenarios where GNSS signals are unreliable. This paper proposes U-ViLAR, a novel uncertainty-aware visual localization framework designed to address these challenges while enabling adaptive localization using high-definition (HD) maps or navigation maps. Specifically, our method first extracts features from the input visual data and maps them into Bird's-Eye-View (BEV) space to enhance spatial consistency with the map input. Subsequently, we introduce: a) Perceptual Uncertainty-guided Association, which mitigates errors caused by perception uncertainty, and b) Localization Uncertainty-guided Registration, which reduces errors introduced by localization uncertainty. By effectively balancing the coarse-grained large-scale localization capability of association with the fine-grained precise localization capability of registration, our approach achieves robust and accurate localization. Experimental results demonstrate that our method achieves state-of-the-art performance across multiple localization tasks. Furthermore, our model has undergone rigorous testing on large-scale autonomous driving fleets and has demonstrated stable performance in various challenging urban scenarios",
    "checked": true,
    "id": "6941fb8fbc1e199a90d2550ab3cdf9e234943c86",
    "semantic_title": "u-vilar: uncertainty-aware visual localization for autonomous driving via differentiable association and registration",
    "citation_count": 0,
    "authors": [
      "Xiaofan Li",
      "Zhihao Xu",
      "Chenming Wu",
      "Zhao Yang",
      "Yumeng Zhang",
      "Jiang-Jiang Liu",
      "Haibao Yu",
      "Xiaoqing Ye",
      "Yuan Wang",
      "Shirui Li",
      "Xun Sun",
      "Ji Wan",
      "Jun Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lee_CF3_Compact_and_Fast_3D_Feature_Fields_ICCV_2025_paper.html": {
    "title": "CF3: Compact and Fast 3D Feature Fields",
    "volume": "main",
    "abstract": "3D Gaussian Splatting (3DGS) has begun incorporating rich information from 2D foundation models. However, most approaches rely on a bottom-up optimization process that treats raw 2D features as ground truth, incurring increased computational costs. We propose a top-down pipeline for constructing compact and fast 3D feature fields, namely, CF3. We first perform a fast weighted fusion of multi-view 2D features with pre-trained Gaussians. This approach enables training a per-Gaussian autoencoder directly on the lifted features, instead of training autoencoders in the 2D domain. As a result, the autoencoder better aligns with the inference-time feature distribution, achieving high compression fidelity and storage efficiency. Finally, we introduce an adaptive sparsification method that prunes and merges Gaussians, constructing an efficient representation while maintaining details. Our approach achieves a competitive 3D feature field while using as little as 5% of the Gaussians compared to Feature-3DGS",
    "checked": true,
    "id": "5f4da5cc22a67c51b58da4127b1405ddccd1ab6f",
    "semantic_title": "cf3: compact and fast 3d feature fields",
    "citation_count": 0,
    "authors": [
      "Hyunjoon Lee",
      "Joonkyu Min",
      "Jaesik Park"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ali_Joint_Self-Supervised_Video_Alignment_and_Action_Segmentation_ICCV_2025_paper.html": {
    "title": "Joint Self-Supervised Video Alignment and Action Segmentation",
    "volume": "main",
    "abstract": "We introduce a novel approach for simultaneous self-supervised video alignment and action segmentation based on a unified optimal transport framework. In particular, we first tackle self-supervised video alignment by developing a fused Gromov-Wasserstein optimal transport formulation with a structural prior, which trains efficiently on GPUs and needs only a few iterations for solving the optimal transport problem. Our single-task method achieves the state-of-the-art performance on multiple video alignment benchmarks and outperforms VAVA, which relies on a traditional Kantorovich optimal transport formulation with an optimality prior. Furthermore, we extend our approach by proposing a unified optimal transport framework for joint self-supervised video alignment and action segmentation, which requires training and storing a single model and saves both time and memory consumption as compared to two different single-task models. Extensive evaluations on several video alignment and action segmentation datasets demonstrate that our multi-task method achieves comparable video alignment yet superior action segmentation results over previous methods in video alignment and action segmentation respectively. Finally, to the best of our knowledge, this is the first work to unify video alignment and action segmentation into a single model",
    "checked": true,
    "id": "fe2098df05d4ad2b8654f388fa83e21c3ddd8cc2",
    "semantic_title": "joint self-supervised video alignment and action segmentation",
    "citation_count": 2,
    "authors": [
      "Ali Shah Ali",
      "Syed Ahmed Mahmood",
      "Mubin Saeed",
      "Andrey Konin",
      "M. Zeeshan Zia",
      "Quoc-Huy Tran"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_2.5_Years_in_Class_A_Multimodal_Textbook_for_Vision-Language_Pretraining_ICCV_2025_paper.html": {
    "title": "2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining",
    "volume": "main",
    "abstract": "Compared to image-text pair data, interleaved corpora enable Vision-Language Models (VLMs) to understand the world more naturally like humans. However, such existing datasets are crawled from webpage, facing challenges like low knowledge density, loose image-text relations, and poor logical coherence between images. On the other hand, the internet hosts vast instructional videos (e.g., online geometry courses) that are widely used by humans to learn foundational subjects, yet these valuable resources remain underexplored in VLM training. In this paper, we introduce a high-quality multimodal textbook corpus with richer foundational knowledge for VLM pretraining. It collects over 2.5 years of instructional videos, totaling 22,000 class hours. We first use an LLM-proposed taxonomy to systematically gather instructional videos. Then we progressively extract and refine visual (keyframes), audio (ASR), and textual knowledge (OCR) from the videos, and organize as an image-text interleaved corpus based on temporal order. Compared to its counterparts, our video-centric textbook offers more coherent context, richer knowledge, and better image-text alignment. Experiments demonstrate its superb pretraining performance, particularly in knowledge- and reasoning-intensive tasks like ScienceQA and MathVista. Moreover, VLMs pre-trained on our textbook exhibit outstanding interleaved context awareness, leveraging visual and textual cues in their few-shot context for task solving. Code and dataset are available on https://multimodal-interleaved-textbook.github.io",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenqi Zhang",
      "Hang Zhang",
      "Xin Li",
      "Jiashuo Sun",
      "Yongliang Shen",
      "Weiming Lu",
      "Deli Zhao",
      "Yueting Zhuang",
      "Lidong Bing"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_Learning_3D_Scene_Analogies_with_Neural_Contextual_Scene_Maps_ICCV_2025_paper.html": {
    "title": "Learning 3D Scene Analogies with Neural Contextual Scene Maps",
    "volume": "main",
    "abstract": "Understanding scene contexts is crucial for machines to perform tasks and adapt prior knowledge in unseen or noisy 3D environments. As data-driven learning is intractable to comprehensively encapsulate diverse ranges of layouts and open spaces, we propose teaching machines to identify relational commonalities in 3D spaces. Instead of focusing on point-wise or object-wise representations, we introduce 3D scene analogies, which are smooth maps between 3D scene regions that align spatial relationships. Unlike well-studied single instance-level maps, these scene-level maps smoothly link large scene regions, potentially enabling unique applications in trajectory transfer in AR/VR, long demonstration transfer for imitation learning, and context-aware object rearrangement. To find 3D scene analogies, we propose neural contextual scene maps, which extract descriptor fields summarizing semantic and geometric contexts, and holistically align them in a coarse-to-fine manner for map estimation. This approach reduces reliance on individual feature points, making it robust to input noise or shape variations. Experiments demonstrate the effectiveness of our approach in identifying scene analogies and transferring trajectories or object placements in diverse indoor scenes, indicating its potential for robotics and AR/VR applications. Project page including the code is available through this link: https://82magnolia.github.io/3d_scene_analogies/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junho Kim",
      "Gwangtak Bae",
      "Eun Sun Lee",
      "Young Min Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_Are_They_the_Same_Exploring_Visual_Correspondence_Shortcomings_of_Multimodal_ICCV_2025_paper.html": {
    "title": "Are They the Same? Exploring Visual Correspondence Shortcomings of Multimodal LLMs",
    "volume": "main",
    "abstract": "Recent advancements in multimodal large language models (MLLM) have shown a strong ability in visual perception, reasoning abilities, and vision-language understanding. However, the visual matching ability of MLLMs is rarely studied, despite finding the visual correspondence of objects is essential in computer vision. Our research reveals that the matching capabilities in recent MLLMs still exhibit systematic shortcomings, even with current strong MLLMs models, GPT-4o. In particular, we construct a Multimodal Visual Matching (MMVM) benchmark to fairly benchmark over 30 different MLLMs. The MMVM benchmark is built from 15 open-source datasets and Internet videos with manual annotation. In addition, we have designed an automatic annotation pipeline to generate the MMVM SFT dataset, including 220K visual matching data with reasoning annotation. To our knowledge, this is the first MLLMs dataset and benchmark for the MLLM community. Finally, we present CoLVA, a novel contrastive MLLM with two novel technical designs: fine-grained vision expert with object-level contrastive learning and instruction augmentation strategy. The former learns instance discriminative tokens, while the latter further improves instruction following ability. CoLVA-InternVL2-4B achieves an overall accuracy (OA) of 49.80% on the MMVM benchmark, surpassing GPT-4o and the best open-source MLLM, Qwen2VL-72B, by 7.15% and 11.72% OA, respectively. These results demonstrate the effectiveness of our MMVM SFT dataset and our novel technical designs. Code, benchmark, dataset, and models will be released",
    "checked": true,
    "id": "815d24b05335dce6253be01a61f47538ed654ea8",
    "semantic_title": "are they the same? exploring visual correspondence shortcomings of multimodal llms",
    "citation_count": 9,
    "authors": [
      "Yikang Zhou",
      "Tao Zhang",
      "Shilin Xu",
      "Shihao Chen",
      "Qianyu Zhou",
      "Yunhai Tong",
      "Shunping Ji",
      "Jiangning Zhang",
      "Lu Qi",
      "Xiangtai Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Poggi_FlowSeek_Optical_Flow_Made_Easier_with_Depth_Foundation_Models_and_ICCV_2025_paper.html": {
    "title": "FlowSeek: Optical Flow Made Easier with Depth Foundation Models and Motion Bases",
    "volume": "main",
    "abstract": "We present FlowSeek, a novel framework for optical flow requiring minimal hardware resources for training. FlowSeek marries the latest advances on the design space of optical flow networks with cutting-edge single-image depth foundation models and classical low-dimensional motion parametrization, implementing a compact, yet accurate architecture. FlowSeek is trained on a single consumer-grade GPU, a hardware budget about 8x lower compared to most recent methods, and still achieves superior cross-dataset generalization on Sintel Final and KITTI, with a relative improvement of 10 and 15% over the previous state-of-the-art SEA-RAFT, as well as on Spring and LayeredFlow datasets",
    "checked": true,
    "id": "7ad9140e49383c1af6076f8e2e6a1d407100b23f",
    "semantic_title": "flowseek: optical flow made easier with depth foundation models and motion bases",
    "citation_count": 0,
    "authors": [
      "Matteo Poggi",
      "Fabio Tosi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liang_EventUPS_Uncalibrated_Photometric_Stereo_Using_an_Event_Camera_ICCV_2025_paper.html": {
    "title": "EventUPS: Uncalibrated Photometric Stereo Using an Event Camera",
    "volume": "main",
    "abstract": "We present EventUPS, the first uncalibrated photometric stereo (UPS) method using an event camera--a neuromorphic sensor that asynchronously detects brightness changes with microsecond resolution. Traditional frame-based UPS methods are hindered by high bandwidth demands and limited use in dynamic scenes. These methods require dense image correspondence under varying illumination and are incompatible with the fundamentally different sensing paradigm of event data. Our approach introduces three key innovations: an augmented null space formulation that directly relates each event to joint constraints on surface normals and lighting, naturally handling ambient illumination; a continuous parameterization of time-varying illumination that connects asynchronous events to synchronized lighting estimation; and a lighting fixture with known relative geometry that reduces ambiguity to a convex-concave uncertainty. We validate EventUPS using a custom-built LED lighting system. Experimental results show that our method achieves accuracy surpassing its frame-based counterpart while requiring only 5% of the data bandwidth",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinxiu Liang",
      "Bohan Yu",
      "Siqi Yang",
      "Haotian Zhuang",
      "Jieji Ren",
      "Peiqi Duan",
      "Boxin Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Daroya_WildSAT_Learning_Satellite_Image_Representations_from_Wildlife_Observations_ICCV_2025_paper.html": {
    "title": "WildSAT: Learning Satellite Image Representations from Wildlife Observations",
    "volume": "main",
    "abstract": "Species distributions encode valuable ecological and environmental information, yet their potential for guiding representation learning in remote sensing remains underexplored. We introduce WildSAT, which pairs satellite images with millions of geo-tagged wildlife observations readily-available on citizen science platforms. WildSAT employs a contrastive learning approach that jointly leverages satellite images, species occurrence maps, and textual habitat descriptions to train or fine-tune models. This approach significantly improves performance on diverse satellite image recognition tasks, outperforming both ImageNet-pretrained models and satellite-specific baselines. Additionally, by aligning visual and textual information, WildSAT enables zero-shot retrieval, allowing users to search geographic locations based on textual descriptions. WildSAT surpasses recent cross-modal learning methods, including approaches that align satellite images with ground imagery or wildlife photos, demonstrating the advantages of our approach. Finally, we analyze the impact of key design choices and highlight the broad applicability of WildSAT to remote sensing and biodiversity monitoring",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rangel Daroya",
      "Elijah Cole",
      "Oisin Mac Aodha",
      "Grant Van Horn",
      "Subhransu Maji"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sargent_Flow_to_the_Mode_Mode-Seeking_Diffusion_Autoencoders_for_State-of-the-Art_Image_ICCV_2025_paper.html": {
    "title": "Flow to the Mode: Mode-Seeking Diffusion Autoencoders for State-of-the-Art Image Tokenization",
    "volume": "main",
    "abstract": "Since the advent of popular visual generation frameworks like VQGAN and Latent Diffusion Models, state-of-the-art image generation systems have generally been two-stage systems that first tokenize or compress visual data into a lower-dimensional latent space before learning a generative model. Tokenizer training typically follows a standard recipe in which images are compressed and reconstructed subject to a combination of MSE, perceptual, and adversarial losses. Diffusion autoencoders have been proposed in prior work as a way to learn end-to-end perceptually-oriented image compression, but have not yet shown state-of-the-art performance on the competitive task of ImageNet-1K reconstruction. In this work, we propose FlowMo, a transformer-based diffusion autoencoder. FlowMo achieves a new state-of-the-art for image tokenization at multiple bitrates. We achieve this without using convolutions, adversarial losses, spatially-aligned 2D latent codes, or distilling from other tokenizers. Our key insight is that FlowMo training should be broken into a mode-matching pre-training stage and a mode-seeking post-training stage. We conduct extensive analysis and ablations, and we additionally train generative models atop the FlowMo tokenizer and verify the performance. We will release our code and model checkpoints upon acceptance",
    "checked": true,
    "id": "d35b4684fb042c172d0860ac036efe873a809e5c",
    "semantic_title": "flow to the mode: mode-seeking diffusion autoencoders for state-of-the-art image tokenization",
    "citation_count": 14,
    "authors": [
      "Kyle Sargent",
      "Kyle Hsu",
      "Justin Johnson",
      "Li Fei-Fei",
      "Jiajun Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_UniPhys_Unified_Planner_and_Controller_with_Diffusion_for_Flexible_Physics-Based_ICCV_2025_paper.html": {
    "title": "UniPhys: Unified Planner and Controller with Diffusion for Flexible Physics-Based Character Control",
    "volume": "main",
    "abstract": "Generating natural and physically plausible character motion remains challenging, particularly for long-horizon control with diverse guidance signals. While prior work combines high-level diffusion-based motion planners with low-level physics controllers, these systems suffer from domain gaps that degrade motion quality and require task-specific fine-tuning.To tackle this problem, we introduce UniPhys, a diffusion-based behavior cloning framework that unifies motion planning and control into a single model. UniPhys enables flexible, expressive character motion conditioned on multi-modal inputs such as text, trajectories, and goals. To address accumulated prediction errors over long sequences, UniPhys is trained with the Diffusion Forcing paradigm, learning to denoise noisy motion histories and handle discrepancies introduced by the physics simulator. This design allows UniPhys to robustly generate physically plausible, long-horizon motions. Through guided sampling, UniPhys generalizes to a wide range of control signals, including unseen ones, without requiring task-specific fine-tuning. Experiments show that UniPhys outperforms prior methods in motion naturalness, generalization, and robustness across diverse control tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Wu",
      "Korrawe Karunratanakul",
      "Zhengyi Luo",
      "Siyu Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hu_SPADE_Spatial-Aware_Denoising_Network_for_Open-vocabulary_Panoptic_Scene_Graph_Generation_ICCV_2025_paper.html": {
    "title": "SPADE: Spatial-Aware Denoising Network for Open-vocabulary Panoptic Scene Graph Generation with Long- and Local-range Context Reasoning",
    "volume": "main",
    "abstract": "Panoptic Scene Graph Generation (PSG) integrates instance segmentation with relation understanding to capture pixel-level structural relationships in complex scenes. Although recent approaches leveraging pre-trained vision-language models (VLMs) have significantly improved performance in the open-vocabulary setting, they commonly ignore the inherent limitations of VLMs in spatial relation reasoning, such as difficulty in distinguishing object relative positions, which results in suboptimal relation prediction.Motivated by the denoising diffusion model's inversion process in preserving the spatial structure of input images, we propose SPADE (SPatial-Aware Denoising-nEtwork) framework---a novel approach for open-vocabulary PSG. SPADE consists of two key steps: (1) inversion-guided calibration for the UNet adaption, and (2) spatial-aware context reasoning. In the first step, we calibrate a general pre-trained teacher diffusion model into a PSG-specific denoising network with cross-attention maps derived during inversion through a lightweight LoRA-based fine-tuning strategy. In the second step, we develop a spatial-aware relation graph transformer that captures both local and long-range contextual information, facilitating the generation of high-quality relation queries. Extensive experiments on benchmark PSG and Visual Genome datasets demonstrate that SPADE outperforms state-of-the-art methods in both closed-set and open-set scenarios, particularly excelling in spatial relationship prediction. The code is available at: https://anonymous.4open.science/r/SPADE-105F",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Hu",
      "Ke Qin",
      "Guiduo Duan",
      "Ming Li",
      "Yuan-Fang Li",
      "Tao He"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jo_Few-Shot_Pattern_Detection_via_Template_Matching_and_Regression_ICCV_2025_paper.html": {
    "title": "Few-Shot Pattern Detection via Template Matching and Regression",
    "volume": "main",
    "abstract": "We address the problem of few-shot pattern detection, which aims to detect all instances of a given pattern, typically represented by a few exemplars, from an input image. Although similar problems have been studied in few-shot object counting and detection (FSCD), previous methods and their benchmarks have narrowed patterns of interest to object categories and often fail to localize non-object patterns. In this work, we propose a simple yet effective detector based on template matching and regression, dubbed TMR. While previous FSCD methods typically represent target exemplars as spatially collapsed prototypes and lose structural information, we revisit classic template matching and regression. It effectively preserves and leverages the spatial layout of exemplars through a minimalistic structure with a small number of learnable convolutional or projection layers on top of a frozen backbone. We also introduce a new dataset, dubbed RPINE, which covers a wider range of patterns than existing object-centric datasets. Our method outperforms the state-of-the-art methods on the three benchmarks, RPINE, FSCD-147, and FSCD-LVIS, and demonstrates strong generalization in cross-dataset evaluation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eunchan Jo",
      "Dahyun Kang",
      "Sanghyun Kim",
      "Yunseon Choi",
      "Minsu Cho"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_Humans_as_Checkerboards_Calibrating_Camera_Motion_Scale_for_World-Coordinate_Human_ICCV_2025_paper.html": {
    "title": "Humans as Checkerboards: Calibrating Camera Motion Scale for World-Coordinate Human Mesh Recovery",
    "volume": "main",
    "abstract": "Accurate camera motion estimation is essential for recovering global human motion in world coordinates from RGB video inputs. While SLAM is widely used for estimating camera trajectory and point cloud, monocular SLAM does so only up to an unknown scale factor. Previous works estimate the scale factor through optimization, but this is unreliable and time-consuming. This paper presents an optimization-free scale calibration framework, Human as Checkerboard (HAC). HAC explicitly leverages the human body predicted by human mesh recovery model as a calibration reference. Specifically, it innovatively uses the absolute depth of human-scene contact joints as references to calibrate the corresponding relative scene depth from SLAM. HAC benefits from geometric priors encoded in human mesh recovery models to estimate the SLAM scale and achieves precise global human motion estimation. Simple yet powerful, our method sets a new state-of-the-art performance for global human mesh estimation tasks. It reduces motion errors by 50% over prior local-to-global methods while using 100x less post-SLAM inference time than optimization-based methods. Our code is available at https://martayang.github.io/HAC/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fengyuan Yang",
      "Kerui Gu",
      "Ha Linh Nguyen",
      "Tze Ho Elden Tse",
      "Angela Yao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ye_Leveraging_BEV_Paradigm_for_Ground-to-Aerial_Image_Synthesis_ICCV_2025_paper.html": {
    "title": "Leveraging BEV Paradigm for Ground-to-Aerial Image Synthesis",
    "volume": "main",
    "abstract": "Ground-to-aerial image synthesis focuses on generating realistic aerial images from corresponding ground street view images while maintaining consistent content layout, simulating a top-down view. The significant viewpoint difference leads to domain gaps between views, and dense urban scenes limit the visible range of street views, making this cross-view generation task particularly challenging. In this paper, we introduce SkyDiffusion, a novel cross-view generation method for synthesizing aerial images from street view images, utilizing a diffusion model and the Bird's-Eye View (BEV) paradigm. The Curved-BEV method in SkyDiffusion converts street-view images into a BEV perspective, effectively bridging the domain gap, and employs a \"multi-to-one\" mapping strategy to address occlusion issues in dense urban scenes. Next, SkyDiffusion designed a BEV-guided diffusion model to generate content-consistent and realistic aerial images. Additionally, we introduce a novel dataset, Ground2Aerial-3, designed for diverse ground-to-aerial image synthesis applications, including disaster scene aerial synthesis, low-altitude UAV image synthesis, and historical high-resolution satellite image synthesis tasks. Experimental results demonstrate that SkyDiffusion outperforms state-of-the-art methods on cross-view datasets across natural (CVUSA), suburban (CVACT), urban (VIGOR-Chicago), and various application scenarios (G2A-3), achieving realistic and content-consistent aerial image generation. The code, datasets and more information of this work can be found at https://opendatalab.github.io/skydiffusion/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyan Ye",
      "Jun He",
      "Weijia Li",
      "Zhutao Lv",
      "Yi Lin",
      "Jinhua Yu",
      "Haote Yang",
      "Conghui He"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_Deciphering_Cross-Modal_Alignment_in_Large_Vision-Language_Models_via_Modality_Integration_ICCV_2025_paper.html": {
    "title": "Deciphering Cross-Modal Alignment in Large Vision-Language Models via Modality Integration Rate",
    "volume": "main",
    "abstract": "The early stage of multi-modal pre-training plays a pivotal role in aligning two modalities for Large Vision-Language Models (LVLMs), while evaluating its training quality usually requires the costly supervised fine-tuning (SFT) stage to verify the downstream benchmark scores. Loss, perplexity, and in-context evaluation results are commonly used pre-training metrics for Large Language Models (LLMs), while we observed that these metrics are less indicative when quantifying the pre-trained LVLMs. Due to the lack of proper metrics, the research of LVLMs in the multi-modal fusion stage is hindered greatly, including the training data choice, efficient module design, etc.In this paper, we first present Modality Integration Rate (MIR), an effective, robust, and generalized metric to indicate the multi-modal alignment quality of LVLMs without SFT. This metric evaluates LVLM pre-training from the inter-modal distribution distance perspective, which is 1) Effective to represent the fusion quality and show a positive relation with the benchmark performance after SFT, 2) Robust toward different training/evaluation data, and 3) Generalize across training configurations and architecture choices. Complementing MIR, we further propose learnable Modality Calibration (MoCa), a lightweight module to narrow the modality gap at each language model layer during training. A series of experiments are conducted to explore the effectiveness of MIR and MoCa, demonstrating that MIR is highly indicative about training data selection, training strategy schedule, and model architecture design to get better pre-training results. The code is avaliable at \\href https://github.com/shikiw/Modality-Integration-Rate shikiw/Modality-Integration-Rate",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qidong Huang",
      "Xiaoyi Dong",
      "Pan Zhang",
      "Yuhang Zang",
      "Yuhang Cao",
      "Jiaqi Wang",
      "Weiming Zhang",
      "Nenghai Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_FPEM_Face_Prior_Enhanced_Facial_Attractiveness_Prediction_for_Live_Videos_ICCV_2025_paper.html": {
    "title": "FPEM: Face Prior Enhanced Facial Attractiveness Prediction for Live Videos with Face Retouching",
    "volume": "main",
    "abstract": "Facial attractiveness prediction (FAP) has long been an important computer vision task, which could be widely applied in live videos with facial retouching. However, previous FAP datasets are either small or closed-source. Moreover, the corresponding FAP models exhibit limited generalization and adaptation ability.To overcome these limitations, we introduce the first large-scale FAP dataset LiveBeauty specifically designed for live video scenarios wherein face images may be real-time processed for aesthetics purposes.10,000 face images are collected directly from a live streaming platform, with 200,000 corresponding attractiveness annotations obtained from a well-devised subjective experiment, making LiveBeauty the largest open-access FAP dataset. Based on the built dataset, a novel FAP method named Facial Prior Enhanced Multi-modal model (FPEM) is proposed to measure the attractiveness of facial images.Extensive experiments conducted on both LiveBeauty and other open-source FAP datasets demonstrate that our proposed method achieves state-of-the-art performance. The dataset will be released at https://github.com/Estella-LH/FPEM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hui Li",
      "Xiaoyu Ren",
      "Hongjiu Yu",
      "Ying Chen",
      "Kai Li",
      "L Wang",
      "Xiongkuo Min",
      "Huiyu Duan",
      "Guangtao Zhai",
      "Xu Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wong_MultiverSeg_Scalable_Interactive_Segmentation_of_Biomedical_Imaging_Datasets_with_In-Context_ICCV_2025_paper.html": {
    "title": "MultiverSeg: Scalable Interactive Segmentation of Biomedical Imaging Datasets with In-Context Guidance",
    "volume": "main",
    "abstract": "Medical researchers and clinicians often need to perform novel segmentation tasks on a set of related images. Existing methods for segmenting a new dataset are either interactive, requiring substantial human effort for each image, or require an existing set of previously labeled images. We introduce a system, MultiverSeg, that enables practitioners to rapidly segment an entire new dataset without requiring access to any existing labeled data from that task or domain. Along with the image to segment, the model takes user interactions such as clicks, bounding boxes or scribbles as input, and predicts a segmentation. As the user segments more images, those images and segmentations become additional inputs to the model, providing context. As the context set of labeled images grows, the number of interactions required to segment each new image decreases. We demonstrate that MultiverSeg enables users to interactively segment new datasets efficiently, by amortizing the number of interactions per image to achieve an accurate segmentation. Compared to using a state-of-the-art interactive segmentation method, MultiverSeg reduced the total number of clicks by 36% and scribble steps by 25% to achieve 90% Dice on sets of images from unseen tasks. We release code and model weights at https://multiverseg.csail.mit.edu/",
    "checked": true,
    "id": "b1b19a129089ca190adb12a1a761930448df68fb",
    "semantic_title": "multiverseg: scalable interactive segmentation of biomedical imaging datasets with in-context guidance",
    "citation_count": 1,
    "authors": [
      "Hallee E. Wong",
      "Jose Javier Gonzalez Ortiz",
      "John Guttag",
      "Adrian V. Dalca"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Guan_Is_Meta-Learning_Out_Rethinking_Unsupervised_Few-Shot_Classification_with_Limited_Entropy_ICCV_2025_paper.html": {
    "title": "Is Meta-Learning Out? Rethinking Unsupervised Few-Shot Classification with Limited Entropy",
    "volume": "main",
    "abstract": "Meta-learning is a powerful paradigm for tackling few-shot tasks. However, recent studies indicate that models trained with the whole-class training strategy can achieve comparable performance to those trained with meta-learning in few-shot classification tasks. To demonstrate the value of meta-learning, we establish an entropy-limited supervised setting for fair comparisons. Through both theoretical analysis and experimental validation, we establish that meta-learning has a tighter generalization bound compared to whole-class training. We unravel that meta-learning is more efficient with limited entropy and is more robust to label noise and heterogeneous tasks, making it well-suited for unsupervised tasks. Based on these insights, We propose MINO, a meta-learning framework designed to enhance unsupervised performance. MINO utilizes the adaptive clustering algorithm DBSCAN with a dynamic head for unsupervised task construction and a stability-based meta-scaler for robustness against label noise. Extensive experiments confirm its effectiveness in multiple unsupervised few-shot and zero-shot tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunchuan Guan",
      "Yu Liu",
      "Ke Zhou",
      "Zhiqi Shen",
      "Jenq-Neng Hwang",
      "Serge Belongie",
      "Lei Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Synergistic_Prompting_for_Robust_Visual_Recognition_with_Missing_Modalities_ICCV_2025_paper.html": {
    "title": "Synergistic Prompting for Robust Visual Recognition with Missing Modalities",
    "volume": "main",
    "abstract": "Large-scale multi-modal models have demonstrated remarkable performance across various visual recognition tasks by leveraging extensive paired multi-modal training data. However, in real-world applications, the presence of missing or incomplete modality inputs often leads to significant performance degradation. Recent research has focused on prompt-based strategies to tackle this issue; however, existing methods are hindered by two major limitations: (1) static prompts lack the flexibility to adapt to varying missing-data conditions, and (2) basic prompt-tuning methods struggle to ensure reliable performance when critical modalities are missing. To address these challenges, we propose a novel Synergistic Prompting (SyP) framework for robust visual recognition with missing modalities. The proposed SyP introduces two key innovations: (I) a Dynamic Adapter, which computes adaptive scaling factors to dynamically generate prompts, replacing static parameters for flexible multi-modal adaptation, and (II) a Synergistic Prompting Strategy, which combines static and dynamic prompts to balance information across modalities, ensuring robust reasoning even when key modalities are missing. The proposed SyP achieves significant performance improvements over existing approaches across three widely-used visual recognition datasets, demonstrating robustness under diverse missing rates and conditions. Extensive experiments and ablation studies validate its effectiveness in handling missing modalities, highlighting its superior adaptability and reliability. The source code will be released",
    "checked": true,
    "id": "6c4ac7c2b9feef24a5b8c7873a3cfe6737eed6fa",
    "semantic_title": "synergistic prompting for robust visual recognition with missing modalities",
    "citation_count": 2,
    "authors": [
      "Zhihui Zhang",
      "Luanyuan Dai",
      "Qika Lin",
      "Yunfeng Diao",
      "Guangyin Jin",
      "Yufei Guo",
      "Jing Zhang",
      "Xiaoshuai Hao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Dong_DAMap_Distance-aware_MapNet_for_High_Quality_HD_Map_Construction_ICCV_2025_paper.html": {
    "title": "DAMap: Distance-aware MapNet for High Quality HD Map Construction",
    "volume": "main",
    "abstract": "High-definition (HD) map is an important component to support navigation and planning for autonomous driving vehicles. Predicting map elements with high quality (high classification and localization scores) is crucial to the safety of autonomous driving vehicles. However, current methods perform poorly in high quality predictions due to inherent task misalignment. Two main factors are responsible for misalignment: 1) inappropriate task labels due to one-to-many matching queries sharing the same labels, and 2) sub-optimal task features due to task-shared sampling mechanism. In this paper, we reveal two inherent defects in current methods and develop a novel HD map construction method named DAMap to address these problems. Specifically, DAMap consists of three components: Distance-aware Focal Loss (DAFL), Hybrid Loss Scheme (HLS), and Task Modulated Deformable Attention (TMDA). The DAFL is introduced to assign appropriate classification labels for one-to-many matching samples. The TMDA is proposed to obtain discriminative task-specific features. Furthermore, the HLS is proposed to better utilize the advantages of the DAFL. We perform extensive experiments and consistently achieve performance improvement on the NuScenes and Argoverse2 benchmarks under different metrics, baselines, splits, backbones, and schedules",
    "checked": true,
    "id": "73e3a55798749ada72a57a2758a77b4933479092",
    "semantic_title": "damap: distance-aware mapnet for high quality hd map construction",
    "citation_count": 0,
    "authors": [
      "Jinpeng Dong",
      "Chen Li",
      "Yutong Lin",
      "Jingwen Fu",
      "Sanping Zhou",
      "Nanning Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ruiz_Magic_Insert_Style-Aware_Drag-and-Drop_ICCV_2025_paper.html": {
    "title": "Magic Insert: Style-Aware Drag-and-Drop",
    "volume": "main",
    "abstract": "We present Magic Insert, a method to drag-and-drop subjects from a user-provided image into a target image of a different style in a plausible manner while matching the style of the target image. This work formalizes our version of the problem of style-aware drag-and-drop and proposes to tackle it by decomposing it into two sub-problems: style-aware personalization and realistic object insertion in stylized images. For style-aware personalization, we cast our method as a weight-and-text-embedding finetuning method with inference-time module-targeted style injection. For subject insertion, we propose Bootstrapped Domain Adaption (BDA) to adapt a domain-specific photorealistic object insertion model to the domain of diverse artistic styles. Overall, the method significantly outperforms traditional and state-of-the-art approaches that struggle with quality, subject fidelity and harmonious stylization. Finally, we present a new dataset, SubjectPlop, to facilitate evaluation and future progress in this area",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nataniel Ruiz",
      "Yuanzhen Li",
      "Neal Wadhwa",
      "Yael Pritch",
      "Michael Rubinstein",
      "David E. Jacobs",
      "Shlomi Fruchter"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yin_ProGait_A_Multi-Purpose_Video_Dataset_and_Benchmark_for_Transfemoral_Prosthesis_ICCV_2025_paper.html": {
    "title": "ProGait: A Multi-Purpose Video Dataset and Benchmark for Transfemoral Prosthesis Users",
    "volume": "main",
    "abstract": "Prosthetic legs play a pivotal role in clinical rehabilitation, allowing individuals with lower-limb amputations the ability to regain mobility and improve their quality of life. Gait analysis is fundamental for optimizing prosthesis design and alignment, directly impacting the mobility and life quality of individuals with lower-limb amputations. Vision-based machine learning (ML) methods offer a scalable and non-invasive solution to gait analysis, but face challenges in correctly detecting and analyzing prosthesis, due to their unique appearances and new movement patterns. In this paper, we aim to bridge this gap by introducing a multi-purpose dataset, namely ProGait, to support multiple vision tasks including Video Object Segmentation, 2D Human Pose Estimation, and Gait Analysis (GA). ProGait provides 412 video clips from four above-knee amputees when testing multiple newly-fitted prosthetic legs through walking trials, and depicts the presence, contours, poses, and gait patterns of human subjects with transfemoral prosthetic legs. Alongside the dataset itself, we also present benchmark tasks and fine-tuned baseline models to illustrate the practical application and performance of the ProGait dataset. We compared our baseline models against pre-trained vision models, demonstrating improved generalizability when applying the ProGait dataset for prosthesis-specific tasks. The ProGait dataset is available at https://huggingface.co/datasets/ericyxy98/ProGait, and the source codes of our benchmark tasks are available at https://github.com/pittisl/ProGait",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangyu Yin",
      "Boyuan Yang",
      "Weichen Liu",
      "Qiyao Xue",
      "Abrar Alamri",
      "Goeran Fiedler",
      "Wei Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lou_MuGS_Multi-Baseline_Generalizable_Gaussian_Splatting_Reconstruction_ICCV_2025_paper.html": {
    "title": "MuGS: Multi-Baseline Generalizable Gaussian Splatting Reconstruction",
    "volume": "main",
    "abstract": "We present Multi-Baseline Gaussian Splatting (MuGS), a generalized feed-forward approach for novel view synthesis that effectively handles diverse baseline settings, including sparse input views with both small and large baselines. Specifically, we integrate features from Multi-View Stereo (MVS) and Monocular Depth Estimation (MDE) to enhance feature representations for generalizable reconstruction. Next, We propose a projection-and-sampling mechanism for deep depth fusion, which constructs a fine probability volume to guide the regression of the feature map. Furthermore, We introduce a reference-view loss to improve geometry and optimization efficiency. We leverage 3D Gaussian representations to accelerate training and inference time while enhancing rendering quality. MuGS achieves state-of-the-art performance across multiple baseline settings and diverse scenarios ranging from simple objects (DTU) to complex indoor and outdoor scenes (RealEstate10K). We also demonstrate promising zero-shot performance on the LLFF and Mip-NeRF 360 datasets. Code is available at https://github.com/EuclidLou/MuGS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaopeng Lou",
      "Liao Shen",
      "Tianqi Liu",
      "Jiaqi Li",
      "Zihao Huang",
      "Huiqiang Sun",
      "Zhiguo Cao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liao_HADES_Human_Avatar_with_Dynamic_Explicit_Hair_Strands_ICCV_2025_paper.html": {
    "title": "HADES: Human Avatar with Dynamic Explicit Hair Strands",
    "volume": "main",
    "abstract": "We introduce HADES, the first framework to seamlessly integrate dynamic hair into human avatars. HADES represents hair as strands bound to 3D Gaussians, with roots attached to the scalp. By modeling inertial and velocity-aware motion, HADES is able to simulate realistic hair dynamics that naturally align with body movements. To enhance avatar fidelity, we incorporate multi-scale data and address color inconsistencies across cameras using a lightweight MLP-based correction module, which generates color correction matrices for consistent color tones. Besides, we resolve rendering artifacts, such as hair dilation during zoom-out, through a 2D Mip filter and physically constrained hair radii. Furthermore, a temporal fusion module is introduced to ensure temporal coherence by modeling historical motion states. Experimental results demonstrate that HADES achieves high-fidelity avatars with realistic hair dynamics, outperforming existing state-of-the-art solutions in terms of realism and robustness",
    "checked": false,
    "id": "8719f022f6f0d67bb231d7c5317decf0e675e251",
    "semantic_title": "relightable gaussian codec avatars",
    "citation_count": 100,
    "authors": [
      "Zhanfeng Liao",
      "Hanzhang Tu",
      "Cheng Peng",
      "Hongwen Zhang",
      "Boyao Zhou",
      "Yebin Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Aligning_Effective_Tokens_with_Video_Anomaly_in_Large_Language_Models_ICCV_2025_paper.html": {
    "title": "Aligning Effective Tokens with Video Anomaly in Large Language Models",
    "volume": "main",
    "abstract": "Understanding abnormal events in videos is a vital and challenging task that has garnered significant attention in a wide range of applications. Although current video understanding Multi-modal Large Language Models (MLLMs) are capable of analyzing general videos, they often struggle to handle anomalies due to the spatial and temporal sparsity of abnormal events, where the redundant information always leads to suboptimal outcomes. To address these challenges, exploiting the representation and generalization capabilities of Vison Language Models (VLMs) and Large Language Models (LLMs), we propose VA-GPT, a novel MLLM designed for summarizing and localizing abnormal events in various videos. Our approach efficiently aligns effective tokens between visual encoders and LLMs through two key proposed modules: Spatial Effective Token Selection (SETS) and Temporal Effective Token Generation (TETG). These modules enable our model to effectively capture and analyze both spatial and temporal information associated with abnormal events, resulting in more accurate responses and interactions. Furthermore, we construct an instruction-following dataset specifically for fine-tuning video-anomaly-aware MLLMs, and introduce a cross-domain evaluation benchmark based on XD-Violence dataset. Our proposed method outperforms existing state-of-the-art methods on various benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingxian Chen",
      "Jiahui Liu",
      "Ruidi Fan",
      "Yanwei Li",
      "Chirui Chang",
      "Shizhen Zhao",
      "Wilton W. T. Fok",
      "Xiaojuan Qi",
      "Yik-Chung Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lowens_PseudoMapTrainer_Learning_Online_Mapping_without_HD_Maps_ICCV_2025_paper.html": {
    "title": "PseudoMapTrainer: Learning Online Mapping without HD Maps",
    "volume": "main",
    "abstract": "Online mapping models show remarkable results in predicting vectorized maps from multi-view camera images only. However, all existing approaches still rely on ground-truth high-definition maps during training, which are expensive to obtain and often not geographically diverse enough for reliable generalization. In this work, we propose PseudoMapTrainer, a novel approach to online mapping that uses pseudo-labels generated from unlabeled sensor data. We derive those pseudo-labels by reconstructing the road surface from multi-camera imagery using Gaussian splatting and semantics of a pre-trained 2D segmentation network. In addition, we introduce a mask-aware assignment algorithm and loss function to handle partially masked pseudo-labels, allowing for the first time the training of online mapping models without any ground-truth maps. Furthermore, our pseudo-labels can be effectively used to pre-train an online model in a semi-supervised manner to leverage large-scale unlabeled crowdsourced data. The code is available at github.com/boschresearch/PseudoMapTrainer",
    "checked": true,
    "id": "730ec497bbc5815d7f64328efcbd152cdc4f0b7e",
    "semantic_title": "pseudomaptrainer: learning online mapping without hd maps",
    "citation_count": 1,
    "authors": [
      "Christian Löwens",
      "Thorben Funke",
      "Jingchao Xie",
      "Alexandru Paul Condurache"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhuo_From_Reflection_to_Perfection_Scaling_Inference-Time_Optimization_for_Text-to-Image_Diffusion_ICCV_2025_paper.html": {
    "title": "From Reflection to Perfection: Scaling Inference-Time Optimization for Text-to-Image Diffusion Models via Reflection Tuning",
    "volume": "main",
    "abstract": "Recent text-to-image diffusion models achieve impressive visual quality through extensive scaling of training data and model parameters, yet they often struggle with complex scenes and fine-grained details. Inspired by the self-reflection capabilities emergent in large language models, we propose ReflectionFlow, an inference-time framework enabling diffusion models to iteratively reflect upon and refine their outputs. ReflectionFlow introduces three complementary inference-time scaling axes: (1) noise-level scaling to optimize latent initialization; (2) prompt-level scaling for precise semantic guidance; and most notably, (3) reflection-level scaling, which explicitly provides actionable reflections to iteratively assess and correct previous generations. To facilitate reflection-level scaling, we construct GenRef, a large-scale dataset comprising 1 million triplets, each containing a reflection, a flawed image, and an enhanced image. Leveraging this dataset, we efficiently perform reflection tuning on state-of-the-art diffusion transformer, FLUX.1-dev, by jointly modeling multimodal inputs within a unified framework. Experimental results show that ReflectionFlow significantly outperforms naive noise-level scaling methods, offering a scalable and compute-efficient solution toward higher-quality image synthesis on challenging tasks. All code, checkpoints, and datasets are available at https://diffusion-cot.github.io/reflection2perfection",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Le Zhuo",
      "Liangbing Zhao",
      "Sayak Paul",
      "Yue Liao",
      "Renrui Zhang",
      "Yi Xin",
      "Peng Gao",
      "Mohamed Elhoseiny",
      "Hongsheng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhao_FreeDance_Towards_Harmonic_Free-Number_Group_Dance_Generation_via_a_Unified_ICCV_2025_paper.html": {
    "title": "FreeDance: Towards Harmonic Free-Number Group Dance Generation via a Unified Framework",
    "volume": "main",
    "abstract": "Generating harmonic and diverse human motions from music signals, especially for a group of dancers, is a practical yet challenging task in virtual avatar creation. Existing methods merely model a fixed number of dancers, lacking the flexibility for arbitrary individuals. To fulfill this goal, we propose a novel unified framework, FreeDance. Considering the plausibility of generating arbitrary dancers while preserving the diverse dynamics of multiple individuals, we build the framework upon collaborative masked token modeling in a 2D discrete space. In particular, we devise a Cross-modality Residual Alignment Module (CRAM) to diversify the movement of each individual and intensify its alignment with music. CRAM captures the spatial motion deformation of each dancer using residual learning and integrates it with rhythmic representation to reinforce the intrinsic connection. Moreover, recognizing the requirement of interactive coordination, we design a Temporal Interaction Module (TIM). Benefiting from masked 2D motion tokens, TIM effectively models the temporal correlation between current individuals w.r.t. neighboring dancers as interaction guidance to foster stronger inter-dancer dependencies. Extensive experiments demonstrate that our approach generates harmonic group dance with any number of individuals, outperforming state-of-the-art methods adapting number-fixed counterparts. Code is available at https://github.com/Tsukasane/FreeDance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiwen Zhao",
      "Yang Wang",
      "Liting Wen",
      "Hengyuan Zhang",
      "Xingqun Qi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Luo_Beyond_the_Frame_Generating_360deg_Panoramic_Videos_from_Perspective_Videos_ICCV_2025_paper.html": {
    "title": "Beyond the Frame: Generating 360deg Panoramic Videos from Perspective Videos",
    "volume": "main",
    "abstract": "360deg videos have emerged as a promising medium to represent our dynamic visual world. Compared to the \"tunnel vision\" of standard cameras, their borderless field of view offers a more complete perspective of our surroundings. While existing video models excel at producing standard videos, their ability to generate full panoramic videos remains elusive. In this paper, we investigate the task of video-to-360deg generation: given a perspective video as input, our goal is to generate a full panoramic video that is consistent with the original video. Unlike conventional video generation tasks, the output's field of view is significantly larger, and the model is required to have a deep understanding of both the spatial layout of the scene and the dynamics of objects to maintain spatio-temporal consistency. To address these challenges, we first leverage the abundant 360deg videos available online and develop a high-quality data filtering pipeline to curate pairwise training data. We then carefully design a series of geometry- and motion-aware operations to facilitate the learning process and improve the quality of 360deg video generation. Experimental results demonstrate that our model can generate realistic and coherent 360deg videos from in-the-wild perspective video. In addition, we showcase its potential applications, including video stabilization, camera viewpoint control, and interactive visual question answering",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rundong Luo",
      "Matthew Wallingford",
      "Ali Fahardi",
      "Noah Snavely",
      "Wei-Chiu Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_Multi-turn_Consistent_Image_Editing_ICCV_2025_paper.html": {
    "title": "Multi-turn Consistent Image Editing",
    "volume": "main",
    "abstract": "Many real-world applications, such as interactive photo retouching, artistic content creation, and product design, require flexible and iterative image editing. However, existing image editing methods primarily focus on achieving the desired modifications in a single step, which often struggles with ambiguous user intent, complex transformations, or the need for progressive refinements. As a result, these methods frequently produce inconsistent outcomes or fail to meet user expectations. To address these challenges, we propose a multi-turn image editing framework that enables users to iteratively refine their edits, progressively achieving more satisfactory results. Our approach leverages flow matching for accurate image inversion and a dual-objective Linear Quadratic Regulators (LQR) for stable sampling, effectively mitigating error accumulation. Additionally, by analyzing the layer-wise roles of transformers, we introduce a adaptive attention highlighting method that enhances editability while preserving multi-turn coherence. Extensive experiments demonstrate that our framework significantly improves edit success rates and visual fidelity compared to existing methods. The code is available at: https://zhouzj-dl.github.io/Multi-turn_Consistent_Image_Editing/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijun Zhou",
      "Yingying Deng",
      "Xiangyu He",
      "Weiming Dong",
      "Fan Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lu_HUMOTO_A_4D_Dataset_of_Mocap_Human_Object_Interactions_ICCV_2025_paper.html": {
    "title": "HUMOTO: A 4D Dataset of Mocap Human Object Interactions",
    "volume": "main",
    "abstract": "We present Human Motions with Objects (HUMOTO), a high-fidelity dataset of human-object interactions for motion generation, computer vision, and robotics applications. Featuring 735 sequences (7,875 seconds at 30 fps), HUMOTO captures interactions with 63 precisely modeled objects and 72 articulated parts. Our innovations include a scene-driven LLM scripting pipeline creating complete, purposeful tasks with natural progression, and a mocap-and-camera recording setup to effectively handle occlusions. Spanning diverse activities from cooking to outdoor picnics, HUMOTO preserves both physical accuracy and logical task flow. Professional artists rigorously clean and verify each sequence, minimizing foot sliding and object penetrations. We also provide benchmarks compared to other datasets. HUMOTO's comprehensive full-body motion and simultaneous multi-object interactions address key data-capturing challenges and provide opportunities to advance realistic human-object interaction modeling across research domains with practical applications in animation, robotics, and embodied AI systems. Project Page: https://jiaxin-lu.github.io/humoto/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxin Lu",
      "Chun-Hao Paul Huang",
      "Uttaran Bhattacharya",
      "Qixing Huang",
      "Yi Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Learning_Visual_Hierarchies_in_Hyperbolic_Space_for_Image_Retrieval_ICCV_2025_paper.html": {
    "title": "Learning Visual Hierarchies in Hyperbolic Space for Image Retrieval",
    "volume": "main",
    "abstract": "Structuring latent representations in a hierarchical manner enables models to learn patterns at multiple levels of abstraction. However, most prevalent image understanding models focus on visual similarity, and learning visual hierarchies is relatively unexplored. In this work, for the first time, we introduce a learning paradigm that can encode user-defined multi-level complex visual hierarchies in hyperbolic space without requiring explicit hierarchical labels. As a concrete example, first, we define a part-based image hierarchy using object-level annotations within and across images. Then, we introduce an approach to enforce the hierarchy using contrastive loss with pairwise entailment metrics. Finally, we discuss new evaluation metrics to effectively measure hierarchical image retrieval. Encoding these complex relationships ensures that the learned representations capture semantic and structural information that transcends mere visual similarity. Experiments in part-based image retrieval show significant improvements in hierarchical retrieval tasks, demonstrating the capability of our model in capturing visual hierarchies",
    "checked": true,
    "id": "71593b7bf862e4e86ea4964b43a6f105e369fedc",
    "semantic_title": "learning visual hierarchies in hyperbolic space for image retrieval",
    "citation_count": 0,
    "authors": [
      "Ziwei Wang",
      "Sameera Ramasinghe",
      "Chenchen Xu",
      "Julien Monteil",
      "Loris Bazzani",
      "Thalaiyasingam Ajanthan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_AutoPrompt_Automated_Red-Teaming_of_Text-to-Image_Models_via_LLM-Driven_Adversarial_Prompts_ICCV_2025_paper.html": {
    "title": "AutoPrompt: Automated Red-Teaming of Text-to-Image Models via LLM-Driven Adversarial Prompts",
    "volume": "main",
    "abstract": "Despite rapid advancements in text-to-image (T2I) models, their safety mechanisms are vulnerable to adversarial prompts, which maliciously generate unsafe images. Current red-teaming methods for proactively assessing such vulnerabilities usually require white-box access to T2I models, and rely on inefficient per-prompt optimization, as well as inevitably generate semantically meaningless prompts easily blocked by filters. In this paper, we propose APT (AutoPrompT), a black-box framework that leverages large language models (LLMs) to automatically generate human-readable adversarial suffixes for benign prompts. We first introduce an alternating optimization-finetuning pipeline between adversarial suffix optimization and fine-tuning the LLM utilizing the optimized suffix. Furthermore, we integrate a dual-evasion strategy in the optimization phase, enabling the bypass of both perplexity-based filter and blacklist word filter: (1) we constrain the LLM generating human-readable prompts through an auxiliary LLM perplexity scoring, which starkly contrasts with prior token-level gibberish, and (2) we also introduce banned-token penalties to suppress the explicit generation of banned-tokens in the blacklist. Extensive experiments demonstrate the excellent red-teaming performance of our human-readable, filter-resistant adversarial prompts, as well as superior zero-shot transferability which enables instant adaptation to unseen prompts and exposes critical vulnerabilities even in commercial APIs (e.g., Leonardo.Ai.)",
    "checked": true,
    "id": "5ef58b926d69915934afd9fc8f1c67f0c6d1f519",
    "semantic_title": "autoprompt: automated red-teaming of text-to-image models via llm-driven adversarial prompts",
    "citation_count": 0,
    "authors": [
      "Yufan Liu",
      "Wanqian Zhang",
      "Huashan Chen",
      "Lin Wang",
      "Xiaojun Jia",
      "Zheng Lin",
      "Weiping Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Dong_LLM-assisted_Entropy-based_Adaptive_Distillation_for_Unsupervised_Fine-grained_Visual_Representation_Learning_ICCV_2025_paper.html": {
    "title": "LLM-assisted Entropy-based Adaptive Distillation for Unsupervised Fine-grained Visual Representation Learning",
    "volume": "main",
    "abstract": "Unsupervised Fine-grained Visual Represent Learning (FVRL) aims to learn discriminative features to distinguish subtle differences among visually similar categories without using labeled fine-grained data. Existing works, which typically learn representation from target data, often struggle to capture subtle inter-class variations due to the limited prior fine-grained knowledge. To alleviate it, this paper proposes LLM-assisted Entropy-based Adaptive Distillation (LEAD), a novel unsupervised FVRL framework that selectively distills fine-grained knowledge from a powerful teacher model built upon pre-trained models. Specifically, we first harness the powerful reasoning capabilities of Large Language Models (LLMs) to generate contextual knowledge of fine-grained category-aware descriptions, enriching semantic priors in the teacher model. These descriptions are then used to form a prototype-driven fine-grained classifier, which acts as an assistant to generate rich knowledge with a frozen vision-language model. Besides, to achieve effective knowledge transfer, we further introduce an entropy-based adaptive mechanism, which dynamically adjusts the distillation strength based on the information entropy to identify and prioritize valuable knowledge. Extensive experimental results on three fine-grained datasets demonstrate the effectiveness and efficiency of our proposed LEAD for unsupervised FVRL. Our source code is available at https://anonymous.4open.science/r/EAD-FFAB",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianfeng Dong",
      "Danfeng Luo",
      "Daizong Liu",
      "Jie Sun",
      "Xiaoye Qu",
      "Xun Yang",
      "Dongsheng Liu",
      "Xun Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhao_Jailbreaking_Multimodal_Large_Language_Models_via_Shuffle_Inconsistency_ICCV_2025_paper.html": {
    "title": "Jailbreaking Multimodal Large Language Models via Shuffle Inconsistency",
    "volume": "main",
    "abstract": "Multimodal Large Language Models (MLLMs) have achieved impressive performance and have been put into practical use in commercial applications, but they still have potential safety mechanism vulnerabilities. Jailbreak attacks are red teaming methods that aim to bypass safety mechanisms and discover MLLMs' potential risks. Existing MLLMs' jailbreak methods often bypass the model's safety mechanism through complex optimization methods or carefully designed image and text prompts. Despite achieving some progress, they have a low attack success rate on commercial closed-source MLLMs. Unlike previous research, we empirically find that there exists a Shuffle Inconsistency between MLLMs' comprehension ability and safety ability for the shuffled harmful instruction. That is, from the perspective of comprehension ability, MLLMs can understand the shuffled harmful text-image instructions well. However, they can be easily bypassed by the shuffled harmful instructions from the perspective of safety ability, leading to harmful responses. Then we innovatively propose a text-image jailbreak attack named SI-Attack. Specifically, to fully utilize the Shuffle Inconsistency and overcome the shuffle randomness, we apply a query-based black-box optimization method to select the most harmful shuffled inputs based on the feedback of the toxic judge model. A series of experiments show that SI-Attack can effectively improve the attack's performance on three benchmarks for both open-source and closed-source MLLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiji Zhao",
      "Ranjie Duan",
      "Fengxiang Wang",
      "Chi Chen",
      "Caixin Kang",
      "Shouwei Ruan",
      "Jialing Tao",
      "YueFeng Chen",
      "Hui Xue",
      "Xingxing Wei"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yao_UMDATrack_Unified_Multi-Domain_Adaptive_Tracking_Under_Adverse_Weather_Conditions_ICCV_2025_paper.html": {
    "title": "UMDATrack: Unified Multi-Domain Adaptive Tracking Under Adverse Weather Conditions",
    "volume": "main",
    "abstract": "Visual object tracking has gained promising progress in past decades. Most of the existing approaches focus on learning target representation in well-conditioned daytime data, while for the unconstrained real-world scenarios with adverse weather conditions, e.g. nighttime or foggy environment, the tremendous domain shift leads to significant performance degradation. In this paper, we propose UMDATrack, which is capable of maintaining high-quality target state prediction under various adverse weather conditions within a unified domain adaptation framework. Specifically, we first use a controllable scenario generator to synthesize a small amount of unlabeled videos (less than 2% frames in source daytime datasets) in multiple weather conditions under the guidance of different text prompts. Afterwards, we design a simple yet effective domain-customized adapter (DCA), allowing the target objects' representation to rapidly adapt to various weather conditions without redundant model updating. Furthermore, to enhance the localization consistency between source and target domains, we propose a target-aware confidence alignment module (TCA) following optimal transport theorem. Extensive experiments demonstrate that UMDATrack can surpass existing advanced visual trackers and lead new state-of-the-art performance by a significant margin",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyuan Yao",
      "Rui Zhu",
      "Ziqi Wang",
      "Wenqi Ren",
      "Yanyang Yan",
      "Xiaochun Cao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_Boosting_MLLM_Reasoning_with_Text-Debiased_Hint-GRPO_ICCV_2025_paper.html": {
    "title": "Boosting MLLM Reasoning with Text-Debiased Hint-GRPO",
    "volume": "main",
    "abstract": "MLLM reasoning has drawn widespread research for its excellent problem-solving capability. Current reasoning methods fall into two types: PRM, which supervises the intermediate reasoning steps, and ORM, which supervises the final results. Recently, DeepSeek-R1 has challenged the traditional view that PRM outperforms ORM, which demonstrates strong generalization performance using an ORM method (i.e., GRPO). However, current MLLM's GRPO algorithms still struggle to handle challenging and complex multimodal reasoning tasks (e.g., mathematical reasoning). In this work, we reveal two problems that impede the performance of GRPO on the MLLM: Low data utilization and Text-bias. Low data utilization refers to that GRPO cannot acquire positive rewards to update the MLLM on difficult samples, and text-bias is a phenomenon that the MLLM bypasses image condition and solely relies on text condition for generation after GRPO training. To tackle these problems, this work proposes Hint-GRPO that improves data utilization by adaptively providing hints for samples of varying difficulty, and text-bias calibration that mitigates text-bias by calibrating the token prediction logits with image condition in test-time. Experiment results on three base MLLMs across eleven datasets demonstrate that our proposed methods advance the reasoning capability of original MLLM by a large margin, exhibiting superior performance to existing MLLM reasoning methods. Our code is available at https://github.com/hqhQAQ/Hint-GRPO",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qihan Huang",
      "Weilong Dai",
      "Jinlong Liu",
      "Wanggui He",
      "Hao Jiang",
      "Mingli Song",
      "Jingyuan Chen",
      "Chang Yao",
      "Jie Song"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Qin_Learning_on_the_Go_A_Meta-learning_Object_Navigation_Model_ICCV_2025_paper.html": {
    "title": "Learning on the Go: A Meta-learning Object Navigation Model",
    "volume": "main",
    "abstract": "Object navigation tasks require an agent to locate a target object using visual observations in unseen environments, where unfamiliar layouts and novel object appearances can hinder navigation. Most existing methods lack the adaptability needed to handle these uncertainties, as their navigation models remain fixed during testing. In this paper, we address this challenge by examining object-conditioned trajectory distribution shifts in navigation caused by changes in environmental dynamics. We propose learning a central conditional distribution as a prior that approximates the specific distributions of diverse environments. To retain environment-specific information during navigation, we allow each environment-specific distribution to approximate this central distribution rather than relying on it directly. To implement this, we introduce a meta-learning mechanism that integrates with traditional navigation methods, offering tailored solutions for various types of navigation approaches. Our approach, Learning on the Go (LOG), enables agents to learn on the go, allowing for flexible, adaptive, real-time learning during navigation. Our theoretical analysis highlights the benefits of learning a central distribution for effective generalization across environments, and empirical results confirm the proposed method's effectiveness, demonstrating superior performance compared to existing approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaorong Qin",
      "Xinhang Song",
      "Sixian Zhang",
      "Xinyao Yu",
      "Xinmiao Zhang",
      "Shuqiang Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Abdullahu_Visual_Interestingness_Decoded_How_GPT-4o_Mirrors_Human_Interests_ICCV_2025_paper.html": {
    "title": "Visual Interestingness Decoded: How GPT-4o Mirrors Human Interests",
    "volume": "main",
    "abstract": "Our daily life is highly influenced by what we consume and see. Attracting and holding one's attention -- the definition of (visual) interestingness -- is essential. The rise of Large Multimodal Models (LMMs) trained on large-scale visual and textual data has demonstrated impressive capabilities. We explore these models' potential to understand to what extent the concepts of visual interestingness are captured and examine the alignment between human assessments and GPT-4o's, a leading LMM, predictions through comparative analysis. Our studies reveal partial alignment between humans and GPT-4o. It already captures the concept as best compared to state-of-the-art methods. Hence, this allows for the effective labeling of image pairs according to their (commonly) interestingness, which are used as training data to distill the knowledge into a learning-to-rank model. The insights pave the way for a deeper understanding of human interest",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fitim Abdullahu",
      "Helmut Grabner"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_PriOr-Flow_Enhancing_Primitive_Panoramic_Optical_Flow_with_Orthogonal_View_ICCV_2025_paper.html": {
    "title": "PriOr-Flow: Enhancing Primitive Panoramic Optical Flow with Orthogonal View",
    "volume": "main",
    "abstract": "Panoramic optical flow enables a comprehensive understanding of temporal dynamics across wide fields of view. However, severe distortions caused by sphere-to-plane projections, such as the equirectangular projection (ERP), significantly degrade the performance of conventional perspective-based optical flow methods, especially in polar regions. To address this challenge, we propose PriOr-Flow, a novel dual-branch framework that leverages the low-distortion nature of the orthogonal view to enhance optical flow estimation in these regions. Specifically, we introduce the Dual-Cost Collaborative Lookup (DCCL) operator, which jointly retrieves correlation information from both the primitive and orthogonal cost volumes, effectively mitigating distortion noise during cost volume construction. Furthermore, our Ortho-Driven Distortion Compensation (ODDC) module iteratively refines motion features of the primitive branch, further suppressing polar distortions. Extensive experiments demonstrate that PriOr-Flow is compatible with various perspective-based iterative optical flow methods and consistently achieves state-of-the-art performance on publicly available panoramic optical flow datasets, setting a new benchmark for wide-field motion estimation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Longliang Liu",
      "Miaojie Feng",
      "Junda Cheng",
      "Jijun Xiang",
      "Xuan Zhu",
      "Xin Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Bi_A_Simple_yet_Mighty_Hartley_Diffusion_Versatilist_for_Generalizable_Dense_ICCV_2025_paper.html": {
    "title": "A Simple yet Mighty Hartley Diffusion Versatilist for Generalizable Dense Vision Tasks",
    "volume": "main",
    "abstract": "Diffusion models have demonstrated powerful capability as a versatilist for dense vision tasks, yet the generalization ability to unseen domains remains rarely explored. This paper presents HarDiff, an efficient frequency learning scheme, so as to advance generalizable paradigms for diffusion based dense prediction. It draws inspiration from a fine-grained analysis of Discrete Hartley Transform, where some low-frequency features activate the broader content of an image, while some high-frequency features maintain sufficient details for dense pixels. Consequently, HarDiff consists of two key components. The low-frequency training process extracts structural priors from the source domain, to enhance understanding of task-related content. The high-frequency sampling process utilizes detail-oriented guidance from the unseen target domain, to infer precise dense predictions with target-related details. Extensive empirical evidence shows that HarDiff can be easily plugged into various dense vision tasks, e.g., semantic segmentation, depth estimation and haze removal, yielding improvements over the state-of-the-art methods in twelve public benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Bi",
      "Jingjun Yi",
      "Huimin Huang",
      "Hao Zheng",
      "Haolan Zhan",
      "Wei Ji",
      "Yawen Huang",
      "Yuexiang Li",
      "Yefeng Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhu_Separation_for_Better_Integration_Disentangling_Edge_and_Motion_in_Event-based_ICCV_2025_paper.html": {
    "title": "Separation for Better Integration: Disentangling Edge and Motion in Event-based Deblurring",
    "volume": "main",
    "abstract": "Traditional motion deblurring methods struggle to effectively model motion information within the exposure time. Recently, event cameras have attracted significant research interest for its ability to model motion cues over the exposure duration. However, these methods directly fuse event features with image, overlooking the intrinsic heterogeneity of events. In this paper, we identify that the event modality contains two conflicting types of information: edge features and motion cues. Events accumulated over a short exposure period capture sharp edge details but lose motion information, while those accumulated over a long exposure period blur edge details due to motion. To address this issue, we propose a simple yet effective approach to disentangle these two cues from event features and employ an edge-aware sharpening module along with motion-driven scale-adaptive deblurring module to fully leverage both. Specifically, the first module aids in restoring sharp edges by leveraging the clear edge features provided by events, while the second module leverages motion cues to learn diverse blur kernels, adaptively adjusting the receptive field for optimal deblurring. Extensive experiments on synthetic and real-world datasets validate the effectiveness of our approach and yield a substantial improvement over state-of-the-art single-frame methods and surpasses most multi-frame-based methods. Code will be publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufei Zhu",
      "Hao Chen",
      "Yongjian Deng",
      "Wei You"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Bitrate-Controlled_Diffusion_for_Disentangling_Motion_and_Content_in_Video_ICCV_2025_paper.html": {
    "title": "Bitrate-Controlled Diffusion for Disentangling Motion and Content in Video",
    "volume": "main",
    "abstract": "We propose a novel and general framework to disentangle video data into its dynamic motion and static content components. Our proposed method is a self-supervised pipeline with less assumptions and inductive biases than previous works: it utilizes a transformer-based architecture to jointly generate flexible implicit features for frame-wise motion and clip-wise content, and incorporates a low-bitrate vector quantization as an information bottleneck to promote disentanglement and form a meaningful discrete motion space. The bitrate-controlled latent motion and content are used as conditional inputs to a denoising diffusion model to facilitate self-supervised representation learning. We validate our disentangled representation learning framework on real world talking head videos with motion transfer and auto-regressive motion generation tasks. Furthermore, we also show that our method can generalize to other type of video data, such as pixel sprites of 2D cartoon characters. Our work presents a new perspective on self-supervised learning of disentangled video representations, contributing to the broader field of video analysis and generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Li",
      "Qi Chen",
      "Xiulian Peng",
      "Kai Yu",
      "Xie Chen",
      "Yan Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lyu_LGA-Net_Learning_Local_and_Global_Affinities_for_Sparse_Scribble_based_ICCV_2025_paper.html": {
    "title": "LGA-Net: Learning Local and Global Affinities for Sparse Scribble based Image Colorization",
    "volume": "main",
    "abstract": "Image colorization is a typical ill-posed problem. Among various colorization methods, scribble-based methods have a unique advantage that allows users to accurately resolve ambiguities and modify the colors of any objects to suit their specific tastes. However, due to the time-consuming scribble drawing process, users tend to draw sparse scribbles instead of dense and detailed scribbles, which makes it challenging for existing methods, especially for regions with no immediate scribbles. Facing the above problems, this paper proposes a novel colorization algorithm named Local and Global Affinity Net (LGA-Net) that formulates the scribble-based colorization task as an affinity propagation process at both local and global levels. Instead of predicting color values directly, our neural network learns to predict local and global affinity relationships between pixels for a given grayscale input, describing how colors should be propagated, which are independent of the scribbles. Given reliable affinity relationships, the color propagation process is formulated as a maximum a posteriori problem. Both local and global affinities are represented using a weighted graph and enabled by a graph Laplacian regularizer to ensure accurate color propagation. Extensive experiments demonstrate that LGA-Net produces state-of-the-art colorization results when using sparse scribbles",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongjin Lyu",
      "Bo Li",
      "Paul L. Rosin",
      "Yu-Kun Lai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hwang_Benchmarking_Burst_Super-Resolution_for_Polarization_Images_Noise_Dataset_and_Analysis_ICCV_2025_paper.html": {
    "title": "Benchmarking Burst Super-Resolution for Polarization Images: Noise Dataset and Analysis",
    "volume": "main",
    "abstract": "Snapshot polarization imaging calculates polarization states from linearly polarized subimages. To achieve this, a polarization camera employs a double Bayer-patterned sensor to capture both color and polarization. It demonstrates low light efficiency and low spatial resolution, resulting in increased noise and compromised polarization measurements. Although burst super-resolution effectively reduces noise and enhances spatial resolution, applying it to polarization imaging poses challenges due to the lack of tailored datasets and reliable ground truth noise statistics. To address these issues, we introduce PolarNS and PolarBurstSR, two innovative datasets developed specifically for polarization imaging. PolarNS provides characterization of polarization noise statistics, facilitating thorough analysis, while PolarBurstSR functions as a benchmark for burst superresolution in polarization images. These datasets, collected under various real-world conditions, enable comprehensive evaluation. Additionally, we present a model for analyzing polarization noise to quantify noise propagation, tested on a large dataset captured in a darkroom environment. As part of our application, we compare the latest burst superresolution models, highlighting the advantages of training tailored to polarization compared to RGB-based methods. This work establishes a benchmark for polarization burst super-resolution and offers critical insights into noise propagation, thereby enhancing polarization image reconstruction. Both code and dataset are publicly available on https://github.com/KAIST-VCLAB/polarns",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Inseung Hwang",
      "Kiseok Choi",
      "Hyunho Ha",
      "Min H. Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Byung-Ki_JointDiT_Enhancing_RGB-Depth_Joint_Modeling_with_Diffusion_Transformers_ICCV_2025_paper.html": {
    "title": "JointDiT: Enhancing RGB-Depth Joint Modeling with Diffusion Transformers",
    "volume": "main",
    "abstract": "We present JointDiT, a diffusion transformer that models the joint distribution of RGB and depth. By leveraging the architectural benefit and outstanding image prior of the state-of-the-art diffusion transformer, JointDiT not only generates high-fidelity images but also produces geometrically plausible and accurate depth maps. This solid joint distribution modeling is achieved through two simple yet effective techniques that we propose, i.e., adaptive scheduling weights, which depend on the noise levels of each modality, and the unbalanced timestep sampling strategy. With these techniques, we train our model across all noise levels for each modality, enabling JointDiT to naturally handle various combinatorial generation tasks, including joint generation, depth estimation, and depth-conditioned image generation by simply controlling the timesteps of each branch. JointDiT demonstrates outstanding joint generation performance. Furthermore, it achieves comparable results in depth estimation and depth-conditioned image generation, suggesting that joint distribution modeling can serve as a replaceable alternative to conditional generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kwon Byung-Ki",
      "Qi Dai",
      "Lee Hyoseok",
      "Chong Luo",
      "Tae-Hyun Oh"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_Discretized_Gaussian_Representation_for_Tomographic_Reconstruction_ICCV_2025_paper.html": {
    "title": "Discretized Gaussian Representation for Tomographic Reconstruction",
    "volume": "main",
    "abstract": "Computed Tomography (CT) enables detailed cross-sectional imaging but continues to face challenges in balancing reconstruction quality and computational efficiency. While deep learning-based methods have significantly improved image quality and noise reduction, they typically require large-scale training data and intensive computation. Recent advances in scene reconstruction, such as Neural Radiance Fields and 3D Gaussian Splatting, offer alternative perspectives but are not well-suited for direct volumetric CT reconstruction. In this work, we propose Discretized Gaussian Representation (DGR), a novel framework that reconstructs the 3D volume directly using a set of discretized Gaussian functions in an end-to-end manner. To further enhance efficiency, we introduce Fast Volume Reconstruction, a highly parallelized technique that aggregates Gaussian contributions into the voxel grid with minimal overhead. Extensive experiments on both real-world and synthetic datasets demonstrate that DGR achieves superior reconstruction quality and runtime performance across various CT reconstruction scenarios. Our code is publicly available at https://github.com/wskingdom/DGR",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaokai Wu",
      "Yuxiang Lu",
      "Yapan Guo",
      "Wei Ji",
      "Suizhi Huang",
      "Fengyu Yang",
      "Shalayiding Sirejiding",
      "Qichen He",
      "Jing Tong",
      "Yanbiao Ji",
      "Yue Ding",
      "Hongtao Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_ScoreHOI_Physically_Plausible_Reconstruction_of_Human-Object_Interaction_via_Score-Guided_Diffusion_ICCV_2025_paper.html": {
    "title": "ScoreHOI: Physically Plausible Reconstruction of Human-Object Interaction via Score-Guided Diffusion",
    "volume": "main",
    "abstract": "Joint reconstruction of human-object interaction marks a significant milestone in comprehending the intricate interrelations between humans and their surrounding environment. Nevertheless, previous optimization methods often struggle to achieve physically plausible reconstruction results due to the lack of prior knowledge about human-object interactions. In this paper, we introduce ScoreHOI, an effective diffusion-based optimizer that introduces diffusion priors for the precise recovery of human-object interactions. By harnessing the controllability within score-guided sampling, the diffusion model can reconstruct a conditional distribution of human and object pose given the image observation and object feature. During inference, the ScoreHOI effectively improves the reconstruction results by guiding the denoising process with specific physical constraints. Furthermore, we propose a contact-driven iterative refinement approach to enhance the contact plausibility and improve the reconstruction accuracy. Extensive evaluations on standard benchmarks demonstrate ScoreHOI's superior performance over state-of-the-art methods, highlighting its ability to achieve a precise and robust improvement in joint human-object interaction reconstruction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ao Li",
      "Jinpeng Liu",
      "Yixuan Zhu",
      "Yansong Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Amato_CMB-ML_A_Cosmic_Microwave_Background_Dataset_for_the_Oldest_Possible_ICCV_2025_paper.html": {
    "title": "CMB-ML: A Cosmic Microwave Background Dataset for the Oldest Possible Computer Vision Task",
    "volume": "main",
    "abstract": "The Cosmic Microwave Background (CMB) radiation is a pillar of modern cosmology that gives rise to a better understanding of the fundamental parameters of the universe. While the astrophysics community has developed computational methods to extract this signal from data, these methods have limited scalability, and several groups have proposed the adoption of computer vision based models for CMB signal extraction. However, these diverse models are difficult to compare: the underlying datasets and evaluations are inconsistent and have not been made publicly available. We propose CMB-ML, a dataset and library that integrates dataset creation, model inference, and result evaluation into a pipeline to fill this gap and to make the problem accessible to researchers outside of cosmology. The library and links for data are available at github.com/CMB-ML/cmb-ml",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "James Amato",
      "Yunan Xie",
      "Leonel Medina-Varela",
      "Ammar Aljerwi",
      "Adam McCutcheon",
      "T. Seth Rippentrop",
      "Kristian Gonzalez",
      "Jacques Delabrouille",
      "Mustapha Ishak",
      "Nicholas Ruozzi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shin_Deeply_Supervised_Flow-Based_Generative_Models_ICCV_2025_paper.html": {
    "title": "Deeply Supervised Flow-Based Generative Models",
    "volume": "main",
    "abstract": "Flow-based generative models have charted an impressive path across multiple visual generation tasks by adhering to a simple principle: learning velocity representations of a linear interpolant. However, we observe that training velocity solely from the final layer's output under-utilizes the rich inter-layer representations, potentially impeding model convergence. To address this limitation, we introduce DeepFlow, a novel framework that enhances velocity representation through inter-layer communication. DeepFlow partitions transformer layers into balanced branches with deep supervision and inserts a lightweight Velocity Refiner with Acceleration (VeRA) block between adjacent branches, which aligns the intermediate velocity features within transformer blocks. Powered by the improved deep supervision via the internal velocity alignment, DeepFlow converges 8x faster on ImageNet-256x256 with equivalent performance and further reduces FID by 2.6 while halving training time compared to previous flow-based models without a classifier-free guidance. DeepFlow also outperforms baselines in text-to-image generation tasks, as evidenced by evaluations on MS-COCO and zero-shot GenEval. The code will be made publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Inkyu Shin",
      "Chenglin Yang",
      "Liang-Chieh Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sun_Towards_Efficient_General_Feature_Prediction_in_Masked_Skeleton_Modeling_ICCV_2025_paper.html": {
    "title": "Towards Efficient General Feature Prediction in Masked Skeleton Modeling",
    "volume": "main",
    "abstract": "Recent advances in the masked autoencoder (MAE) paradigm have significantly propelled self-supervised skeleton-based action recognition. However, most existing approaches limit reconstruction targets to raw joint coordinates or their simple variants, resulting in computational redundancy and limited semantic representation. To address this, we propose a novel General Feature Prediction framework (GFP) for efficient mask skeleton modeling. Our key innovation is replacing conventional low-level reconstruction with high-level feature prediction that spans from local motion patterns to global semantic representations. Specifically, we introduce a collaborative learning framework where a lightweight target generation network dynamically produces diversified supervision signals across spatial-temporal hierarchies, avoiding reliance on pre-computed offline features. The framework incorporates constrained optimization to ensure feature diversity while preventing model collapse. Experiments on NTU RGB+D 60, NTU RGB+D 120 and PKU-MMD demonstrate the benefits of our approach: Computational efficiency (with 6.2x faster training than standard masked skeleton modeling methods) and superior representation quality, achieving state-of-the-art performance in various downstream tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengkai Sun",
      "Zefan Zhang",
      "Jianfeng Dong",
      "Zhiyong Cheng",
      "Xiaojun Chang",
      "Meng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sucar_Dynamic_Point_Maps_A_Versatile_Representation_for_Dynamic_3D_Reconstruction_ICCV_2025_paper.html": {
    "title": "Dynamic Point Maps: A Versatile Representation for Dynamic 3D Reconstruction",
    "volume": "main",
    "abstract": "DUSt3R has recently demonstrated that many tasks in multi-view geometry, including estimating camera intrinsics and extrinsics, reconstructing 3D scenes, and establishing image correspondences, can be reduced to predicting a pair of viewpoint-invariant point maps, i.e., pixel-aligned point clouds defined in a common reference frame. While this formulation is elegant and powerful, it is limited to static scenes. To overcome this limitation, we introduce the concept of Dynamic Point Maps (DPM), which extends standard point maps to support 4D tasks such as motion segmentation, scene flow estimation, 3D object tracking, and 2D correspondence. Our key insight is that, when time is introduced, several possible spatial and temporal references can be used to define the point maps. We identify a minimal subset of these combinations that can be regressed by a network to solve the aforementioned tasks. We train a DPM predictor on a mixture of synthetic and real data and evaluate it across diverse benchmarks, including video depth prediction, dynamic point cloud reconstruction, 3D scene flow, and object pose tracking, achieving state-of-the-art performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Edgar Sucar",
      "Zihang Lai",
      "Eldar Insafutdinov",
      "Andrea Vedaldi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Language_Decoupling_with_Fine-grained_Knowledge_Guidance_for_Referring_Multi-object_Tracking_ICCV_2025_paper.html": {
    "title": "Language Decoupling with Fine-grained Knowledge Guidance for Referring Multi-object Tracking",
    "volume": "main",
    "abstract": "Referring Multi-Object Tracking (RMOT) aims to detect and track specific objects based on natural language expressions. Previous methods typically rely on sentence-level vision-language alignment, often failing to exploit fine-grained linguistic cues that are crucial for distinguishing objects with similar characteristics. Notably, these cues play distinct roles at different tracking stages and should be leveraged accordingly to provide more explicit guidance. In this work, we propose DKGTrack, a novel RMOT method that enhances language comprehension for precise object tracking by decoupling language expressions into localized descriptions and motion states. To improve the accuracy of language-guided object identification, we introduce a Static Semantic Enhancement (SSE) module, which enhances region-level vision-language alignment through hierarchical cross-modal feature interaction, providing more discriminative object representations for tracking. Furthermore, we propose a Motion Perception Alignment (MPA) module that explicitly aligns object queries with motion descriptions, enabling accurate object trajectory prediction across frames. Experimental results on multiple RMOT benchmarks demonstrate the effectiveness of our method, which achieves competitive performance in challenging tracking scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangyao Li",
      "Siping Zhuang",
      "Yajun Jian",
      "Yan Yan",
      "Hanzi Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Timestep-Aware_Diffusion_Model_for_Extreme_Image_Rescaling_ICCV_2025_paper.html": {
    "title": "Timestep-Aware Diffusion Model for Extreme Image Rescaling",
    "volume": "main",
    "abstract": "Image rescaling aims to learn the optimal low-resolution (LR) image that can be accurately reconstructed to its original high-resolution (HR) counterpart, providing an efficient image processing and storage method for ultra-high definition media. However, extreme downscaling factors pose significant challenges to the upscaling process due to its highly ill-posed nature, causing existing image rescaling methods to struggle in generating semantically correct structures and perceptual friendly textures. In this work, we propose a novel framework called Timestep-Aware Diffusion Model (TADM) for extreme image rescaling, which performs rescaling operations in the latent space of a pre-trained autoencoder and effectively leverages powerful natural image priors learned by a pre-trained text-to-image diffusion model. Specifically, TADM adopts a pseudo-invertible module to establish the bidirectional mapping between the latent features of the HR image and the target-sized LR image. Then, the rescaled latent features are enhanced by a pre-trained diffusion model to generate more faithful details. Considering the spatially non-uniform degradation caused by the rescaling operation, we propose a novel time-step alignment strategy, which can adaptively allocate the generative capacity of the diffusion model based on the quality of the reconstructed latent features. Extensive experiments demonstrate the superiority of TADM over previous methods in both quantitative and qualitative evaluations. The code will be available at: https://github.com/wwangcece/TADM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ce Wang",
      "Zhenyu Hu",
      "Wanjie Sun",
      "Zhenzhong Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Qian_COVTrack_Continuous_Open-Vocabulary_Tracking_via_Adaptive_Multi-Cue_Fusion_ICCV_2025_paper.html": {
    "title": "COVTrack: Continuous Open-Vocabulary Tracking via Adaptive Multi-Cue Fusion",
    "volume": "main",
    "abstract": "Open-Vocabulary Multi-Object Tracking (OVMOT) aims to detect and track diverse object categories in videos, including both seen (base) and unseen (novel) categories. Current methods rely on appearance features from generated image pairs or utilize the discontinuous annotations of the video dataset (TAO) for training, primarily due to the lack of available continuous annotated video datasets for OVMOT. This limitation affects their effectiveness, since continuous target trajectories are necessary for robust tracker learning. In this work, we propose the C-TAO dataset, which provides a continuous version of TAO, thereby constructing the first continuous annotated training dataset for OVMOT. This addresses the previous limitations in training data availability. Additionally, we introduce COVTrack, a unified framework that effectively integrates motion and semantic features with appearance features, in which the multi-cue feature aggregation strategy dynamically aggregates and balances these features, based on the confidence estimation from both intra-frame and inter-frame contexts. Our proposed framework significantly improves OVMOT performance, establishing COVTrack as a state-of-the-art solution on OVMOT benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zekun Qian",
      "Ruize Han",
      "Zhixiang Wang",
      "Junhui Hou",
      "Wei Feng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhao_Toward_Material-Agnostic_System_Identification_from_Videos_ICCV_2025_paper.html": {
    "title": "Toward Material-Agnostic System Identification from Videos",
    "volume": "main",
    "abstract": "System identification from videos aims to recover object geometry and governing physical laws. Existing methods integrate differentiable rendering with simulation but rely on predefined material priors, limiting their ability to handle unknown ones. We introduce MASIV, the first vision-based framework for material-agnostic system identification. Unlike existing approaches that depend on hand-crafted constitutive laws, MASIV employs learnable neural constitutive models, inferring object dynamics without assuming a scene-specific material prior. However, the absence of full particle state information imposes unique challenges, leading to unstable optimization and physically implausible behaviors. To address this, we introduce dense geometric guidance by reconstructing continuum particle trajectories, providing temporally rich motion constraints beyond sparse visual cues. Comprehensive experiments show that MASIV achieves state-of-the-art performance in geometric accuracy, rendering quality, and generalization ability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yizhou Zhao",
      "Haoyu Chen",
      "Chunjiang Liu",
      "Zhenyang Li",
      "Charles Herrmann",
      "Junhwa Hur",
      "Yinxiao Li",
      "Ming-Hsuan Yang",
      "Bhiksha Raj",
      "Min Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Goto_Referring_Expression_Comprehension_for_Small_Objects_ICCV_2025_paper.html": {
    "title": "Referring Expression Comprehension for Small Objects",
    "volume": "main",
    "abstract": "Referring expression comprehension (REC) aims to localize the target object described by a natural language expression.Recent advances in vision-language learning have led to significant performance improvements in REC tasks.However, localizing extremely small objects remains a considerable challenge despite its importance in real-world applications such as autonomous driving.To address this issue, we introduce a novel dataset and method for REC targeting small objects.First, we present the small object REC (SOREC) dataset, which consists of 100,000 pairs of referring expressions and corresponding bounding boxes for small objects in driving scenarios.Second, we propose the progressive-iterative zooming adapter (PIZA), an adapter module for parameter-efficient fine-tuning that enables models to progressively zoom in and localize small objects.In a series of experiments, we apply PIZA to GroundingDINO and demonstrate a significant improvement in accuracy on the SOREC dataset.Our dataset, codes and pre-trained models are publicly available on the project page",
    "checked": true,
    "id": "58445b04c5c4127adad62e5dd862e6c65bbb2610",
    "semantic_title": "referring expression comprehension for small objects",
    "citation_count": 0,
    "authors": [
      "Kanoko Goto",
      "Takumi Hirose",
      "Mahiro Ukai",
      "Shuhei Kurita",
      "Nakamasa Inoue"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_Robust_3D-Masked_Part-level_Editing_in_3D_Gaussian_Splatting_with_Regularized_ICCV_2025_paper.html": {
    "title": "Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with Regularized Score Distillation Sampling",
    "volume": "main",
    "abstract": "Recent advances in 3D neural representations and instance-level editing models have enabled the efficient creation of high-quality 3D content. However, achieving precise local 3D edits remains challenging, especially for Gaussian Splatting, due to inconsistent multi-view 2D part segmentations and inherently ambiguous nature of Score Distillation Sampling (SDS) loss. To address these limitations, we propose RoMaP, a novel local 3D Gaussian editing framework that enables precise and drastic part-level modifications. First, we introduce a robust 3D mask generation module with our 3D-Geometry Aware Label Prediction (3D-GALP), which uses spherical harmonics (SH) coefficients to model view-dependent label variations and soft-label property, yielding accurate and consistent part segmentations across viewpoints. Second, we propose a regularized SDS loss that combines the standard SDS loss with additional regularizers. In particular, an L1 anchor loss is introduced via our Scheduled Latent Mixing and Part (SLaMP) editing method, which generates high-quality part-edited 2D images and confines modifications only to the target region while preserving contextual coherence. Additional regularizers, such as Gaussian prior removal, further improve flexibility by allowing changes beyond the existing context, and robust 3D masking prevents unintended edits. Experimental results demonstrate that our RoMaP achieves state-of-the-art local 3D editing on both reconstructed and generated Gaussian scenes and objects qualitatively and quantitatively, making it possible for more robust and flexible part-level 3D Gaussian editing. Code is available at https://janeyeon.github.io/romap",
    "checked": true,
    "id": "4aae5071c067e6938d2824f854326589ba7ab9f4",
    "semantic_title": "robust 3d-masked part-level editing in 3d gaussian splatting with regularized score distillation sampling",
    "citation_count": 0,
    "authors": [
      "Hayeon Kim",
      "Ji Ha Jang",
      "Se Young Chun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_InstaDrive_Instance-Aware_Driving_World_Models_for_Realistic_and_Consistent_Video_ICCV_2025_paper.html": {
    "title": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
    "volume": "main",
    "abstract": "Autonomous driving relies on robust models trained on high-quality, large-scale multi-view driving videos for tasks like perception and planning. While world models offer a cost-effective solution for generating realistic driving videos, they struggle to maintain instance-level temporal consistency and spatial geometric fidelity. To address these challenges, we propose **InstaDrive**, a novel framework that enhances driving video realism through two key advancements: (1) Instance Flow Guider module, which extracts and propagates instance features across frames to enforce temporal consistency, preserving instance identity over time. (2) Spatial Geometric Aligner module, which improves spatial reasoning, ensures precise instance positioning, and explicitly models occlusion hierarchies. By incorporating these instance-aware mechanisms, InstaDrive achieves state-of-the-art video generation quality and enhances downstream autonomous driving tasks on the nuScenes dataset. Additionally, we utilize CARLA's autopilot to procedurally and stochastically simulate rare yet safety-critical driving scenarios across diverse maps and regions, enabling rigorous safety evaluation for autonomous systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuoran Yang",
      "Xi Guo",
      "Chenjing Ding",
      "Chiyu Wang",
      "Wei Wu",
      "Yanyong Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_Effective_Training_Data_Synthesis_for_Improving_MLLM_Chart_Understanding_ICCV_2025_paper.html": {
    "title": "Effective Training Data Synthesis for Improving MLLM Chart Understanding",
    "volume": "main",
    "abstract": "Being able to effectively read scientific plots, or chart understanding, is a central part toward building effective agents for science. However, existing multimodal large language models (MLLMs), especially open-source ones, are still falling behind with a typical success rate of 30%-50% on challenging benchmarks. Previous studies on fine-tuning MLLMs with synthetic charts are often restricted by their inadequate similarity to the real charts, which could compromise model training and performance on complex real-world charts. In this study, we show that modularizing chart generation and diversifying visual details improves chart understanding capabilities. In particular, we design a five-step data synthesis pipeline, where we separate data and function creation for single plot generation, condition the generation of later subplots on earlier ones for multi-subplot figures, visually diversify the generated figures, filter out low quality data, and finally generate the question-answer (QA) pairs with GPT-4o. This approach allows us to streamline the generation of fine-tuning datasets and introduce the effective chart dataset (ECD), which contains 10k+ chart images and 300k+ QA pairs, covering 25 topics and featuring 250+ chart type combinations with high visual complexity. We show that ECD consistently improves the performance of various MLLMs on a range of real-world and synthetic test sets. Code, data and models are available at: https://github.com/yuweiyang-anu/ECD",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuwei Yang",
      "Zeyu Zhang",
      "Yunzhong Hou",
      "Zhuowan Li",
      "Gaowen Liu",
      "Ali Payani",
      "Yuan-Sen Ting",
      "Liang Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_DIVE_Taming_DINO_for_Subject-Driven_Video_Editing_ICCV_2025_paper.html": {
    "title": "DIVE: Taming DINO for Subject-Driven Video Editing",
    "volume": "main",
    "abstract": "Building on the success of diffusion models in image generation and editing, video editing has recently gained substantial attention. However, maintaining temporal consistency and motion alignment still remains challenging. To address these issues, this paper proposes DINO-guided Video Editing (DIVE), a framework designed to facilitate subject-driven editing in source videos conditioned on either target text prompts or reference images with specific identities. The core of DIVE lies in leveraging the powerful semantic features extracted from a pretrained DINOv2 model as implicit correspondences to guide the editing process. Specifically, to ensure temporal motion consistency, DIVE employs DINO features to align with the motion trajectory of the source video. For precise subject editing, DIVE incorporates the DINO features of reference images into a pretrained text-to-image model to learn Low-Rank Adaptations (LoRAs), effectively registering the target subject's identity. Extensive experiments on diverse real-world videos demonstrate that our framework can achieve high-quality editing results with robust motion consistency, highlighting the potential of DINO to contribute to video editing",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Huang",
      "Wei Xiong",
      "He Zhang",
      "Chaoqi Chen",
      "Jianzhuang Liu",
      "Mingfu Yan",
      "Shifeng Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liao_Neural_Architecture_Search_Driven_by_Locally_Guided_Diffusion_for_Personalized_ICCV_2025_paper.html": {
    "title": "Neural Architecture Search Driven by Locally Guided Diffusion for Personalized Federated Learning",
    "volume": "main",
    "abstract": "Neural Architecture Search (NAS) has gained significant attention in personalized federated learning (PFL) due to its ability to automatically design tailored models for individual clients. While most existing NAS approaches for PFL perform architecture search on the server side, client-side NAS--where architectures are optimized locally on clients--offers stronger privacy protection by eliminating the need to transmit sensitive model information. However, this paradigm remains underexplored and often suffers from suboptimal average client performance, primarily due to two limitations: (1) Inefficient client-side search strategies caused by data isolation and restricted access to local architectures across clients, and (2) slow supernet convergence arising from server aggregation and local supernet training. To address these challenges, we propose a Personalized Federated Stochastic Differential Equation-based NAS (PerFedSDE-NAS). To achieve effective local search, each client employs a guided diffusion model to generate promising personalized architectures tailored to local data characteristics, while a performance predictor based on radial basis functions is used to select only the most promising subset of architectures for evaluation. To accelerate supernet convergence, each client maintains a supernet with an archive-driven training mechanism, and a novel model aggregation strategy is proposed to further enhance weight convergence during FL rounds. We validate PerFedSDE-NAS across three NAS search spaces, including convolutional neural networks and transformers, demonstrating its broad applicability. Compared to traditional fixed-model and NAS-based PFL approaches, our method achieves the state-of-the-art performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peng Liao",
      "Xilu Wang",
      "Yaochu Jin",
      "Wenli Du",
      "Han Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Global-Aware_Monocular_Semantic_Scene_Completion_with_State_Space_Models_ICCV_2025_paper.html": {
    "title": "Global-Aware Monocular Semantic Scene Completion with State Space Models",
    "volume": "main",
    "abstract": "Monocular Semantic Scene Completion (MonoSSC) reconstructs and interprets 3D environments from a single image, enabling diverse real-world applications. However, existing methods are often constrained by the local receptive field of Convolutional Neural Networks (CNNs), making it challenging to handle the non-uniform distribution of projected points (Fig. \\ref fig:perspective ) and effectively reconstruct missing information caused by the 3D-to-2D projection. In this work, we introduce GA-MonoSSC, a hybrid architecture for MonoSSC that effectively captures global context in both the 2D image domain and 3D space. Specifically, we propose a Dual-Head Multi-Modality Encoder, which leverages a Transformer architecture to capture spatial relationships across all features in the 2D image domain, enabling more comprehensive 2D feature extraction. Additionally, we introduce the Frustum Mamba Decoder, built on the State Space Model (SSM), to efficiently capture long-range dependencies in 3D space. Furthermore, we propose a frustum reordering strategy within the Frustum Mamba Decoder to mitigate feature discontinuities in the reordered voxel sequence, ensuring better alignment with the scan mechanism of the State Space Model (SSM) for improved 3D representation learning. We conduct extensive experiments on the widely used Occ-ScanNet and NYUv2 datasets, demonstrating that our proposed method achieves state-of-the-art performance, validating its effectiveness. The code is available here",
    "checked": true,
    "id": "24e420a8e6cbf623ba15d9ab5146c2731d49a4ad",
    "semantic_title": "global-aware monocular semantic scene completion with state space models",
    "citation_count": 0,
    "authors": [
      "Shijie Li",
      "Zhongyao Cheng",
      "Rong Li",
      "Shuai Li",
      "Juergen Gall",
      "Xun Xu",
      "Xulei Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yuan_ULTHO_Ultra-Lightweight_yet_Efficient_Hyperparameter_Optimization_in_Deep_Reinforcement_Learning_ICCV_2025_paper.html": {
    "title": "ULTHO: Ultra-Lightweight yet Efficient Hyperparameter Optimization in Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "Hyperparameter optimization (HPO) is a billion-dollar problem in machine learning, which significantly impacts the training efficiency and model performance. However, achieving efficient and robust HPO in deep reinforcement learning (RL) is consistently challenging due to its high non-stationarity and computational cost. To tackle this problem, existing approaches attempt to adapt common HPO techniques (e.g., population-based training or Bayesian optimization) to the RL scenario. However, they remain sample-inefficient and computationally expensive, which cannot facilitate a wide range of applications. In this paper, we propose ULTHO, an ultra-lightweight yet powerful framework for fast HPO in deep RL within single runs. Specifically, we formulate the HPO process as a multi-armed bandit with clustered arms (MABC) and link it directly to long-term return optimization. ULTHO also provides a quantified and statistical perspective to filter the HPs efficiently. We test ULTHO on benchmarks including ALE, Procgen, MiniGrid, and PyBullet. Extensive experiments demonstrate that the ULTHO can achieve superior performance with a simple architecture, contributing to the development of advanced and automated RL systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingqi Yuan",
      "Bo Li",
      "Xin Jin",
      "Wenjun Zeng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shi_GReg_Geometry-Aware_Region_Refinement_for_Sign_Language_Video_Generation_ICCV_2025_paper.html": {
    "title": "GReg: Geometry-Aware Region Refinement for Sign Language Video Generation",
    "volume": "main",
    "abstract": "Sign Language Video Generation (SLVG) aims to transform sign language sequences into natural and fluent sign language videos. Existing SLVG methods lack geometric modeling of human anatomical structures, leading to anatomically implausible and temporally inconsistent generation. To address these challenges, we propose a novel SLVG framework: Geometry-Aware Region Refinement (GReg). GReg uses 3D geometric information (such as normal maps and gradient maps) from the SMPL-X model to ensure anatomical and temporal consistency.To fully leverage the 3D geometric priors, we propose two novel methods: 1) Regional Prior Generation, which uses regional expert networks to generate target-structured regions as generation priors; 2) Gradient-Enhanced Refinement, which guides the refinement of detailed structures in key regions using gradient features.Furthermore, we enhance visual realism in key regions through adversarial training on both these regions and their gradient maps.Experimental results demonstrate that GReg achieves state-of-the-art performance with superior structural accuracy and temporal consistency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tongkai Shi",
      "Lianyu Hu",
      "Fanhua Shang",
      "Liqing Gao",
      "Wei Feng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Habib_CarGait_Cross-Attention_based_Re-ranking_for_Gait_recognition_ICCV_2025_paper.html": {
    "title": "CarGait: Cross-Attention based Re-ranking for Gait recognition",
    "volume": "main",
    "abstract": "Gait recognition is a computer vision task that identifies individuals based on their walking patterns. Its performance is commonly evaluated by ranking a gallery of candidates and measuring the identification accuracy at Rank-K. Existing models are typically single-staged, searching for the probe's nearest neighbors in a gallery, using a global feature representation. While these models can excel at retrieving the correct identity within the top-K predictions, they often struggle when hard negatives are among the top shortlist, leading to relatively low performance at the highest ranks (e.g., Rank-1). In this paper, we introduce CarGait, a Re-ranking (re-ordering the top-K list) method for gait recognition, leveraging the fine-grained correlations between pairs of gait sequences, through cross-attention between gait strips. This re-ranking scheme can be adapted to existing single-stage models to enhance their final results. We demonstrate the capabilities of CarGait by extensive experiments on three common gait datasets, Gait3D, GREW, and OU-MVLP, and seven different gait models, showing consistent gains in Rank-1,5 accuracy, while outperforming existing re-ranking approaches, and a strong baseline",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gavriel Habib",
      "Noa Barzilay",
      "Or Shimshi",
      "Rami Ben-Ari",
      "Nir Darshan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Meng_Text2VDM_Text_to_Vector_Displacement_Maps_for_Expressive_and_Interactive_ICCV_2025_paper.html": {
    "title": "Text2VDM: Text to Vector Displacement Maps for Expressive and Interactive 3D Sculpting",
    "volume": "main",
    "abstract": "Professional 3D asset creation often requires diverse sculpting brushes to add surface details and geometric structures.Despite recent progress in 3D generation, producing reusable sculpting brushes compatible with artists' workflows remains an open and challenging problem.These sculpting brushes are typically represented as vector displacement maps (VDMs), which existing models cannot easily generate compared to natural images.This paper presents Text2VDM, a novel framework for text-to-VDM brush generation through the deformation of a dense planar mesh guided by score distillation sampling (SDS).The original SDS loss is designed for generating full objects and struggles with generating desirable sub-object structures from scratch in brush generation.We refer to this issue as semantic coupling, which we address by introducing weighted blending of prompt tokens to SDS, resulting in a more accurate target distribution and semantic guidance.Experiments demonstrate that Text2VDM can generate diverse, high-quality VDM brushes for sculpting surface details and geometric structures.Our generated brushes can be seamlessly integrated into mainstream modeling software, enabling various applications such as mesh stylization and real-time interactive modeling",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hengyu Meng",
      "Duotun Wang",
      "Zhijing Shao",
      "Ligang Liu",
      "Zeyu Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_DiffVSR_Revealing_an_Effective_Recipe_for_Taming_Robust_Video_Super-Resolution_ICCV_2025_paper.html": {
    "title": "DiffVSR: Revealing an Effective Recipe for Taming Robust Video Super-Resolution Against Complex Degradations",
    "volume": "main",
    "abstract": "Diffusion models have demonstrated exceptional capabilities in image restoration, yet their application to video super-resolution (VSR) faces significant challenges in balancing fidelity with temporal consistency. Our evaluation reveals a critical gap: existing approaches consistently fail on severely degraded videos--precisely where diffusion models' generative capabilities are most needed. We identify that existing diffusion-based VSR methods struggle primarily because they face an overwhelming learning burden: simultaneously modeling complex degradation distributions, content representations, and temporal relationships with limited high-quality training data. To address this fundamental challenge, we present DiffVSR, featuring a Progressive Learning Strategy (PLS) that systematically decomposes this learning burden through staged training, enabling superior performance on complex degradations. Our framework additionally incorporates an Interweaved Latent Transition (ILT) technique that maintains competitive temporal consistency without additional training overhead. Experiments demonstrate that our approach excels in scenarios where competing methods struggle, particularly on severely degraded videos. Our work reveals that addressing the learning strategy, rather than focusing solely on architectural complexity, is the critical path toward robust real-world video super-resolution with diffusion models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaohui Li",
      "Yihao Liu",
      "Shuo Cao",
      "Ziyan Chen",
      "Shaobin Zhuang",
      "Xiangyu Chen",
      "Yinan He",
      "Yi Wang",
      "Yu Qiao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lou_3D_Gaussian_Splatting_Driven_Multi-View_Robust_Physical_Adversarial_Camouflage_Generation_ICCV_2025_paper.html": {
    "title": "3D Gaussian Splatting Driven Multi-View Robust Physical Adversarial Camouflage Generation",
    "volume": "main",
    "abstract": "Physical adversarial attack methods expose the vulnerabilities of deep neural networks and pose a significant threat to safety-critical scenarios such as autonomous driving. Camouflage-based physical attack is a more promising approach compared to the patch-based attack, offering stronger adversarial effectiveness in complex physical environments. However, most prior work relies on mesh priors of the target object and virtual environments constructed by simulators, which are time-consuming to obtain and inevitably differ from the real world. Moreover, due to the limitations of the backgrounds in training images, previous methods often fail to produce multi-view robust adversarial camouflage and tend to fall into sub-optimal solutions. Due to these reasons, prior work lacks adversarial effectiveness and robustness across diverse viewpoints and physical environments. We propose a physical attack framework based on 3D Gaussian Splatting (3DGS), named PGA, which provides rapid and precise reconstruction with few images, along with photo-realistic rendering capabilities. Our framework further enhances cross-view robustness and adversarial effectiveness by preventing mutual and self-occlusion among Gaussians and employing a min-max optimization approach that adjusts the imaging background of each viewpoint, helping the algorithm filter out non-robust adversarial features. Extensive experiments validate the effectiveness and superiority of PGA. Our code is available at: https://github.com/TRLou/PGA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianrui Lou",
      "Xiaojun Jia",
      "Siyuan Liang",
      "Jiawei Liang",
      "Ming Zhang",
      "Yanjun Xiao",
      "Xiaochun Cao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lu_GenieBlue_Integrating_both_Linguistic_and_Multimodal_Capabilities_for_Large_Language_ICCV_2025_paper.html": {
    "title": "GenieBlue: Integrating both Linguistic and Multimodal Capabilities for Large Language Models on Mobile Devices",
    "volume": "main",
    "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have enabled their deployment on mobile devices. However, challenges persist in maintaining strong language capabilities and ensuring hardware compatibility, both of which are crucial for user experience and practical deployment efficiency. In our deployment process, we observe that existing MLLMs often face performance degradation on pure language tasks, and the current NPU platforms on smartphones do not support the MoE architecture, which is commonly used to preserve pure language capabilities during multimodal training. To address these issues, we systematically analyze methods to maintain pure language capabilities during the training of MLLMs, focusing on both training data and model architecture aspects. Based on these analyses, we propose GenieBlue, an efficient MLLM structural design that integrates both linguistic and multimodal capabilities for LLMs on mobile devices. GenieBlue freezes the original LLM parameters during MLLM training to maintain pure language capabilities. It acquires multimodal capabilities by duplicating specific transformer blocks for full fine-tuning and integrating lightweight LoRA modules. This approach preserves language capabilities while achieving comparable multimodal performance through extensive training. Deployed on smartphone NPUs, GenieBlue demonstrates efficiency and practicality for applications on mobile devices",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xudong Lu",
      "Yinghao Chen",
      "Renshou Wu",
      "Haohao Gao",
      "Xi Chen",
      "Xue Yang",
      "Xiangyu Zhao",
      "Aojun Zhou",
      "Fangyuan Li",
      "Yafei Wen",
      "Xiaoxin Chen",
      "Shuai Ren",
      "Hongsheng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhu_MambaML_Exploring_State_Space_Models_for_Multi-Label_Image_Classification_ICCV_2025_paper.html": {
    "title": "MambaML: Exploring State Space Models for Multi-Label Image Classification",
    "volume": "main",
    "abstract": "Mamba, a selective state-space model, has recently seen widespread application across various visual tasks due to its exceptional ability to capture long-range dependencies. While promising results have been demonstrated in image classification, its potential in multi-label image classification remains underexplored. To bridge this gap, we propose a novel Mamba-based decoder, which utilizes the intrinsic attention of Mamba to aggregate visual information from image features into label embeddings, yielding label-specific visual representations. Building upon this, a MambaML framework is developed for multi-label image classification, which models the self-correlations of image features and label embeddings with bi-directional Mamba, as well as their cross-correlations with the Mamba-based decoder, allowing visual spatial relationships, label semantic dependencies, and cross-modal associations to be explored in a unified system. In this way, robust label-specific visual representations are acquired, facilitating the training of binary classifiers towards accurate label recognition. Experiments on public benchmarks suggest that our MambaML achieves performance comparable to state-of-the-art methods in multi-label image classification, while requiring fewer parameters and computational overhead",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuelin Zhu",
      "Jian Liu",
      "Jiuxin Cao",
      "Bing Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hong_SuperMat_Physically_Consistent_PBR_Material_Estimation_at_Interactive_Rates_ICCV_2025_paper.html": {
    "title": "SuperMat: Physically Consistent PBR Material Estimation at Interactive Rates",
    "volume": "main",
    "abstract": "Decomposing physically-based materials from images into their constituent properties remains challenging, particularly when maintaining both computational efficiency and physical consistency. While recent diffusion-based approaches have shown promise, they face substantial computational overhead due to multiple denoising steps and separate models for different material properties. We present SuperMat, a single-step framework that achieves high-quality material decomposition with one-step inference. This enables end-to-end training with perceptual and re-render losses while decomposing albedo, metallic, and roughness maps at millisecond-scale speeds. We further extend our framework to 3D objects through a UV refinement network, enabling consistent material estimation across viewpoints while maintaining efficiency. Experiments demonstrate that SuperMat achieves state-of-the-art PBR material decomposition quality while reducing inference time from seconds to milliseconds per image, and completes PBR material estimation for 3D objects in approximately 3 seconds. The project page is at https://hyj542682306.github.io/SuperMat/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yijia Hong",
      "Yuan-Chen Guo",
      "Ran Yi",
      "Yulong Chen",
      "Yan-Pei Cao",
      "Lizhuang Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Deep_Adaptive_Unfolded_Network_via_Spatial_Morphology_Stripping_and_Spectral_ICCV_2025_paper.html": {
    "title": "Deep Adaptive Unfolded Network via Spatial Morphology Stripping and Spectral Filtration for Pan-sharpening",
    "volume": "main",
    "abstract": "In the field of pan-sharpening, existing deep methods are hindered in deepening cross-modal complementarity in the intermediate feature, and lack effective strategies to harness the network entirety for optimal solutions, exhibiting limited feasibility and interpretability due to their black-box designs. Besides, validating pan-sharpening performance in high-level semantic tasks is intractable for the absence of datasets. To tackle these issues, we propose a deep adaptive unfolded network via spatial morphology stripping and spectral filtration for pan-sharpening, which is conceptualized as a linear inverse problem regularized by spatial and spectral priors. Specifically, we incorporate phase-oriented constraints into the spatial prior to facilitate the extraction of modal-invariant spatial morphology by intrinsic decomposition and leverage a physics-driven spectral filtration attention mechanism aligned with the spectral prior to mine the inherent spectral correlation. After transparently unfolding the model into a multi-stage network, an adaptive stage-exiting mechanism is designed to capitalize on fusion diversity by aggregating optimal image patches across candidate stages. To systematically complete the assessment, we construct the first panoptic segmentation dataset as a semantic-level benchmark for pan-sharpening performance validation. Extensive experiments are conducted to verify the merits of our method with state-of-the-arts. Code is available at https://github.com/Baixuzx7/DAPNet",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hebaixu Wang",
      "Jiayi Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_HiMTok_Learning_Hierarchical_Mask_Tokens_for_Image_Segmentation_with_Large_ICCV_2025_paper.html": {
    "title": "HiMTok: Learning Hierarchical Mask Tokens for Image Segmentation with Large Multimodal Model",
    "volume": "main",
    "abstract": "The remarkable performance of large multimodal models (LMMs) has attracted significant interest from the image segmentation community.To align with the next-token-prediction paradigm, current LMM-driven segmentation methods either use object boundary points to represent masks or introduce special segmentation tokens, whose hidden states are decoded by a segmentation model requiring the original image as input.However, these approaches often suffer from inadequate mask representation and complex architectures, limiting the potential of LMMs.In this work, we propose the Hierarchical Mask Tokenizer (HiMTok), which represents segmentation masks with up to 32 tokens and eliminates the need for the original image during mask de-tokenization.HiMTok allows for compact and coarse-to-fine mask representations, aligning well with the LLM next-token-prediction paradigm and facilitating the direct acquisition of segmentation capabilities.We develop a 3-stage training recipe for progressive learning of segmentation and visual capabilities, featuring a hierarchical mask loss for effective coarse-to-fine learning.Additionally, we enable bidirectional information flow, allowing conversion between bounding boxes and mask tokens to fully leverage multi-task training potential.Extensive experiments demonstrate that our method achieves state-of-the-art performance across various segmentation tasks,while also enhancing visual grounding and maintaining overall visual understanding",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Wang",
      "Changxu Cheng",
      "Lingfeng Wang",
      "Senda Chen",
      "Wuyue Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_MixA-Q_Revisiting_Activation_Sparsity_for_Vision_Transformers_from_a_Mixed-Precision_ICCV_2025_paper.html": {
    "title": "MixA-Q: Revisiting Activation Sparsity for Vision Transformers from a Mixed-Precision Quantization Perspective",
    "volume": "main",
    "abstract": "In this paper, we propose MixA-Q, a mixed-precision activation quantization framework that leverages intra-layer activation sparsity (a concept widely explored in activation pruning methods) for efficient inference of quantized window-based vision transformers. For a given uniform-bit quantization configuration, MixA-Q separates the batched window computations within Swin blocks and assigns a lower bit width to the activations of less important windows, improving the trade-off between model performance and efficiency. We introduce a Two-Branch Swin Block that processes activations separately in high- and low-bit precision, enabling seamless integration of our method with most quantization-aware training (QAT) and post-training quantization (PTQ) methods, or with simple modifications. Our experimental evaluations over the COCO dataset demonstrate that MixA-Q achieves a training-free 1.35x computational speedup without accuracy loss in PTQ configuration. With QAT, MixA-Q achieves a lossless 1.25x speedup and a 1.53x speedup with only a 1% mAP drop by incorporating activation pruning. Notably, by reducing the quantization error in important regions, our sparsity-aware quantization adaptation improves the mAP of the quantized W4A4 model (with both weights and activations in 4-bit precision) by 0.7%, reducing quantization degradation by 24%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weitian Wang",
      "Rai Shubham",
      "Cecilia De La Parra",
      "Akash Kumar"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhu_A_Quality-Guided_Mixture_of_Score-Fusion_Experts_Framework_for_Human_Recognition_ICCV_2025_paper.html": {
    "title": "A Quality-Guided Mixture of Score-Fusion Experts Framework for Human Recognition",
    "volume": "main",
    "abstract": "Whole-body biometric recognition is a challenging multi-modal task that integrates various biometric modalities, including face, gait, and body. This integration is essential for overcoming the limitations of unimodal systems. Traditionally, whole-body recognition involves deploying different models to process multiple modalities, achieving the final outcome by score-fusion (e.g., weighted averaging similarity matrices from each model). However, these conventional methods may overlook the variations in score distributions of individual modalities, making it challenging to improve final performance. In this work, we present Quality-guided Mixture of score-fusion Experts (QME), a novel framework designed for improving whole-body biometric recognition performance through a learnable score-fusion strategy using a Mixture of Experts (MoE). We introduce a novel pseudo quality loss for quality estimation with a modality-specific Quality Estimator (QE), and a score triplet loss to improve the metric performance. Extensive experiments on multiple whole-body biometric datasets demonstrate the effectiveness of our proposed approach, achieving state-of-the-art results across various metrics compared to baseline methods. Our method is effective for multi-modal and multi-model, addressing key challenges such as model misalignment in the similarity score domain and variability in data quality. Code is available at the \\href https://github.com/jiezhu23/QME_ICCV25 Project Link",
    "checked": true,
    "id": "c50edfb29f97289ee5e963d63d79d48547de5d47",
    "semantic_title": "a quality-guided mixture of score-fusion experts framework for human recognition",
    "citation_count": 1,
    "authors": [
      "Jie Zhu",
      "Yiyang Su",
      "Minchul Kim",
      "Anil Jain",
      "Xiaoming Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wei_EMD_Explicit_Motion_Modeling_for_High-Quality_Street_Gaussian_Splatting_ICCV_2025_paper.html": {
    "title": "EMD: Explicit Motion Modeling for High-Quality Street Gaussian Splatting",
    "volume": "main",
    "abstract": "Photorealistic reconstruction of street scenes is essential for developing real-world simulators in autonomous driving. While recent methods based on 3D/4D Gaussian Splatting (GS) have demonstrated promising results, they still encounter challenges in complex street scenes due to the unpredictable motion of dynamic objects. Current methods typically decompose street scenes into static and dynamic objects, learning the Gaussians in either a supervised manner (e.g., w/ 3D bounding-box) or a self-supervised manner (e.g., w/o 3D bounding-box). However, these approaches do not effectively model the motions of dynamic objects (e.g., the motion speed of pedestrians is clearly different from that of vehicles), resulting in suboptimal scene decomposition. To address this, we propose Explicit Motion Decomposition (EMD), which models the motions of dynamic objects by introducing learnable motion embeddings to the Gaussians, enhancing the decomposition in street scenes. The proposed plug-and-play EMD module compensates for the lack of motion modeling in self-supervised street Gaussian splatting methods. We also introduce tailored training strategies to extend EMD to supervised approaches. Comprehensive experiments demonstrate the effectiveness of our method, achieving state-of-the-art novel view synthesis performance in self-supervised settings. The code is available at: https://qingpowuwu.github.io/emd",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaobao Wei",
      "Qingpo Wuwu",
      "Zhongyu Zhao",
      "Zhuangzhe Wu",
      "Nan Huang",
      "Ming Lu",
      "Ningning Ma",
      "Shanghang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ying_MOVE_Motion-Guided_Few-Shot_Video_Object_Segmentation_ICCV_2025_paper.html": {
    "title": "MOVE: Motion-Guided Few-Shot Video Object Segmentation",
    "volume": "main",
    "abstract": "This work addresses motion-guided few-shot video object segmentation (FSVOS), which aims to segment dynamic objects in videos based on a few annotated examples with the same motion patterns. Existing FSVOS datasets and methods typically focus on object categories, which are static attributes that ignore the rich temporal dynamics in videos, limiting their application in scenarios requiring motion understanding. To fill this gap, we introduce MOVE, a large-scale dataset specifically designed for motion-guided FSVOS. Based on MOVE, we comprehensively evaluate 6 state-of-the-art methods from 3 different related tasks across 2 experimental settings. Our results reveal that current methods struggle to address motion-guided FSVOS, prompting us to analyze the associated challenges and propose a baseline method, Decoupled Motion Appearance Network (DMA). Experiments demonstrate that our approach achieves superior performance in few shot motion understanding, establishing a solid foundation for future research in this direction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaining Ying",
      "Hengrui Hu",
      "Henghui Ding"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cao_Neural_Multi-View_Self-Calibrated_Photometric_Stereo_without_Photometric_Stereo_Cues_ICCV_2025_paper.html": {
    "title": "Neural Multi-View Self-Calibrated Photometric Stereo without Photometric Stereo Cues",
    "volume": "main",
    "abstract": "We propose a neural inverse rendering approach that jointly reconstructs geometry, spatially varying reflectance, and lighting conditions from multi-view images captured under varying directional lighting. Unlike prior multi-view photometric stereo methods that require light calibration or intermediate cues such as per-view normal maps, our method jointly optimizes all scene parameters from raw images in a single stage. We represent both geometry and reflectance as neural implicit fields and apply shadow-aware volume rendering. A spatial network first predicts the signed distance and a reflectance latent code for each scene point. A reflectance network then estimates reflectance values conditioned on the latent code and angularly encoded surface normal, view, and light directions. The proposed method outperforms state-of-the-art normal-guided approaches in shape and lighting estimation accuracy, generalizes to view-unaligned multi-light images, and handles objects with challenging geometry and reflectance",
    "checked": true,
    "id": "7d748e8d6c1908986d7a3ed701e8c899a9a96144",
    "semantic_title": "neural multi-view self-calibrated photometric stereo without photometric stereo cues",
    "citation_count": 0,
    "authors": [
      "Xu Cao",
      "Takafumi Taketomi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zheng_CO2-Net_A_Physics-Informed_Spatio-Temporal_Model_for_Global_Surface_CO2_Reconstruction_ICCV_2025_paper.html": {
    "title": "CO2-Net: A Physics-Informed Spatio-Temporal Model for Global Surface CO2 Reconstruction",
    "volume": "main",
    "abstract": "Reconstructing atmospheric surface \\text CO _2 is crucial for understanding climate dynamics and informing global mitigation strategies. Traditional inversion models achieve precise global \\text CO _2 reconstruction but rely heavily on uncertain prior estimates of fluxes and emissions. Inspired by recent advances in data-driven weather forecasting, we explore whether data-driven models can reduce reliance on these priors. However, \\text CO _2 reconstruction presents unique challenges, including complex spatio-temporal dynamics, periodic patterns and sparse observations. We propose \\text CO _2-Net, a data-driven model that addresses these challenges without requiring extensive prior data. We formulate \\text CO _2 reconstruction as solving a constrained advection-diffusion equation and derive three key components: physics-informed spatio-temporal factorization for capturing complex transport dynamics, wind-based embeddings for modeling periodic variations and a semi-supervised loss for integrating sparse \\text CO _2 observations with dense meteorological data. \\text CO _2-Net is designed in three sizes---small (S), base (B) and large (L)---to balance performance and efficiency. On CMIP6 reanalysis data, \\text CO _2-Net (S) and (L) reduce RMSE by 11% and 71% , respectively, when compared to the best data-driven baseline. On real observations, \\text CO _2-Net (L) achieves RMSE comparable to inversion models. The ablation study shows that the effectiveness of wind-based embedding and semi-supervised loss stems from their compatibility with our spatio-temporal factorization. Code is available at https://github.com/Leamonz/CORE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Zheng",
      "Yuting Zheng",
      "Hanbo Huang",
      "Chaofan Sun",
      "Enhui Liao",
      "Lin Liu",
      "Yi Han",
      "Hao Zhou",
      "Shiyu Liang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sharma_Multi-Modal_Multi-Task_Unified_Embedding_Model_M3T-UEM_A_Task-Adaptive_Representation_Learning_ICCV_2025_paper.html": {
    "title": "Multi-Modal Multi-Task Unified Embedding Model (M3T-UEM): A Task-Adaptive Representation Learning Framework",
    "volume": "main",
    "abstract": "We present Multi-Modal Multi-Task Unified Embedding Model (M3T-UEM), a framework that advances vision-language matching and retrieval by leveraging a large language model (LLM) backbone. While concurrent LLM-based approaches like VLM2VEC, MM-Embed, NV-Embed, and MM-GEM have demonstrated impressive capabilities in multi-modal and multi-task scenarios, our work introduces novel mechanisms for task-adaptive learning and embedding extraction that further enhance the potential of LLM-based retrieval systems. Our key technical contribution lies in the development of a task-aware contrastive learning framework with an automated Bayesian weighing mechanism. This approach provides a principled way to balance multiple tasks during training, departing from conventional contrastive learning strategies. We further enhance the framework through a multiple-token summarization strategy and an auxiliary language modeling objective, which together significantly improve retrieval performance.Comprehensive experiments on M-BEIR and ICinW benchmarks demonstrate M3T-UEM's effectiveness, showing competitive or superior performance compared to both traditional encoder-based methods and recent LLM-based approaches. Furthermore, we demonstrate particular strengths in handling compositional conceptual changes and multilingual scenarios owing to the incorporation of an LLM backbone where the method drastically outperforms CLIP in zero-shot settings, often by orders of magnitude",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rohan Sharma",
      "Changyou Chen",
      "Feng-Ju Chang",
      "Seongjun Yun",
      "Xiaohu Xie",
      "Rui Meng",
      "Dehong Xu",
      "Alejandro Mottini",
      "Qingjun Cui"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yu_Randomized_Autoregressive_Visual_Generation_ICCV_2025_paper.html": {
    "title": "Randomized Autoregressive Visual Generation",
    "volume": "main",
    "abstract": "This paper presents Randomized AutoRegressive modeling (RAR) for visual generation, which sets a new state-of-the-art performance on the image generation task while maintaining full compatibility with language modeling frameworks. The proposed RAR is simple: during a standard autoregressive training process with a next-token prediction objective, the input sequence-typically ordered in raster form-is randomly permuted into different factorization orders with a probability r, where r starts at 1 and linearly decays to 0 over the course of training. This annealing training strategy enables the model to learn to maximize the expected likelihood over all factorization orders and thus effectively improve the model's capability of modeling bidirectional contexts. Importantly, RAR preserves the integrity of the autoregressive modeling framework, ensuring full compatibility with language modeling while significantly improving performance in image generation. On the ImageNet-256 benchmark, RAR achieves an FID score of 1.48, not only surpassing prior state-of-the-art autoregressive image generators but also outperforming leading diffusion-based and masked transformer-based methods. Code and models are available at https://github.com/bytedance/1d-tokenizer",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qihang Yu",
      "Ju He",
      "Xueqing Deng",
      "Xiaohui Shen",
      "Liang-Chieh Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Towards_More_Diverse_and_Challenging_Pre-training_for_Point_Cloud_Learning_ICCV_2025_paper.html": {
    "title": "Towards More Diverse and Challenging Pre-training for Point Cloud Learning: Self-Supervised Cross Reconstruction with Decoupled Views",
    "volume": "main",
    "abstract": "Point cloud learning, especially in a self-supervised way without manual labels, has gained growing attention in both vision and learning communities due to its potential utility in a wide range of applications. Most existing generative approaches for point cloud self-supervised learning focus on recovering masked points from visible ones within a single view. Recognizing that a two-view pre-training paradigm inherently introduces greater diversity and variance, it may thus enable more challenging and informative pre-training. Inspired by this, we explore the potential of two-view learning in this domain. In this paper, we propose Point-PQAE, a cross-reconstruction generative paradigm that first generates two decoupled point clouds/views and then reconstructs one from the other. To achieve this goal, we develop a crop mechanism for point cloud view generation for the first time and further propose a novel positional encoding to represent the 3D relative position between the two decoupled views. The cross-reconstruction significantly increases the difficulty of pre-training compared to self-reconstruction, which enables our method to surpass previous single-modal self-reconstruction methods in 3D self-supervised learning. Specifically, it outperforms the self-reconstruction baseline (Point-MAE) by 6.5%, 7.0%, and 6.7% in three variants of ScanObjectNN with the Mlp-Linear evaluation protocol. The code is available at https://github.com/aHapBean/Point-PQAE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangdong Zhang",
      "Shaofeng Zhang",
      "Junchi Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cai_Baking_Gaussian_Splatting_into_Diffusion_Denoiser_for_Fast_and_Scalable_ICCV_2025_paper.html": {
    "title": "Baking Gaussian Splatting into Diffusion Denoiser for Fast and Scalable Single-stage Image-to-3D Generation and Reconstruction",
    "volume": "main",
    "abstract": "Existing feedforward image-to-3D methods mainly rely on 2D multi-view diffusion models that cannot guarantee 3D consistency. These methods easily collapse when changing the prompt view direction and mainly handle object-centric cases. In this paper, we propose a novel single-stage 3D diffusion model, DiffusionGS, for object generation and scene reconstruction from a single view. DiffusionGS directly outputs 3D Gaussian point clouds at each timestep to enforce view consistency and allow the model to generate robustly given prompt views of any directions, beyond object-centric inputs. Plus, to improve the capability and generality of DiffusionGS, we scale up 3D training data by developing a scene-object mixed training strategy. Experiments show that DiffusionGS yields improvements of 2.20 dB/23.25 and 1.34 dB/19.16 in PSNR/FID for objects and scenes than the state-of-the-art methods, without using 2D diffusion prior and depth estimator. Plus, our method enjoys over 5x faster speed ( 6s on an A100 GPU). The project page is at https://caiyuanhao1998.github.io/project/DiffusionGS/",
    "checked": false,
    "id": "568516ca7d5feda8f7289bee31e4f2412f4493e7",
    "semantic_title": "baking gaussian splatting into diffusion denoiser for fast and scalable single-stage image-to-3d generation",
    "citation_count": 9,
    "authors": [
      "Yuanhao Cai",
      "He Zhang",
      "Kai Zhang",
      "Yixun Liang",
      "Mengwei Ren",
      "Fujun Luan",
      "Qing Liu",
      "Soo Ye Kim",
      "Jianming Zhang",
      "Zhifei Zhang",
      "Yuqian Zhou",
      "Yulun Zhang",
      "Xiaokang Yang",
      "Zhe Lin",
      "Alan Yuille"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Berisha_Variance-Based_Pruning_for_Accelerating_and_Compressing_Trained_Networks_ICCV_2025_paper.html": {
    "title": "Variance-Based Pruning for Accelerating and Compressing Trained Networks",
    "volume": "main",
    "abstract": "Increasingly expensive training of ever larger models such as Vision Transfomers motivate reusing the vast library of already trained state-of-the-art networks. However, their latency, high computational costs and memory demands pose significant challenges for deployment, especially on resource-constrained hardware. While structured pruning methods can reduce these factors, they often require costly retraining, sometimes for up to hundreds of epochs, or even training from scratch to recover the lost accuracy resulting from the structural modifications. Maintaining the provided performance of trained models after structured pruning and thereby avoiding extensive retraining remains a challenge. To solve this, we introduce Variance-Based Pruning, a simple and structured one-shot pruning technique for efficiently compressing networks, with minimal finetuning. Our approach first gathers activation statistics, which are then used to select neurons for pruning. Simultaneously the mean activations are integrated back into the model to preserve a high degree of performance. On ImageNet-1k recognition tasks, we demonstrate that directly after pruning DeiT-Base retains over 70% of its original performance and requires only 10 epochs of fine-tuning to regain 99% of the original accuracy while simultaneously reducing MACs by 35% and model size by 36%, thus speeding up the model by 1.44 times",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Uranik Berisha",
      "Jens Mehnert",
      "Alexandru Paul Condurache"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yuan_Learning_Normal_Flow_Directly_From_Events_ICCV_2025_paper.html": {
    "title": "Learning Normal Flow Directly From Events",
    "volume": "main",
    "abstract": "Event-based motion field estimation is an important task. However, current optical flow methods face challenges: learning-based approaches, often frame-based and relying on CNNs, lack cross-domain transferability, while model-based methods, though more robust, are less accurate. To address the limitations of optical flow estimation, recent works have focused on normal flow, which can be more reliably measured in regions with limited texture or strong edges. However, existing normal flow estimators are predominantly model-based and suffer from high errors. In this paper, we propose a novel supervised point-based method for normal flow estimation that overcomes the limitations of existing event learning-based approaches. Using a local point cloud encoder, our method directly estimates per-event normal flow from raw events, offering multiple unique advantages: 1) It produces temporally and spatially sharp predictions. 2) It supports more diverse data augmentation, such as random rotation, to improve robustness across various domains. 3) It naturally supports uncertainty quantification via ensemble inference, which benefits downstream tasks. 4) It enables training and inference on undistorted data in normalized camera coordinates, improving transferability across cameras. Extensive experiments demonstrate our method achieves better and more consistent performance than state-of-the-art methods when transferred across different datasets. Leveraging this transferability, we train our model on the union of datasets and release it for public use. Finally, we introduce an egomotion solver based on a maximum-margin problem that uses normal flow and IMU to achieve strong performance in challenging scenarios. Codes are available at https://github.com/dhyuan99/VecKM_flow",
    "checked": false,
    "id": "5e27a37cb8f7582ae3e4fea3ec091e57c45397ba",
    "semantic_title": "learning normal flow directly from event neighborhoods",
    "citation_count": 5,
    "authors": [
      "Dehao Yuan",
      "Levi Burner",
      "Jiayi Wu",
      "Minghui Liu",
      "Jingxi Chen",
      "Yiannis Aloimonos",
      "Cornelia Fermüller"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Phung_Federated_Prompt-Tuning_with_Heterogeneous_and_Incomplete_Multimodal_Client_Data_ICCV_2025_paper.html": {
    "title": "Federated Prompt-Tuning with Heterogeneous and Incomplete Multimodal Client Data",
    "volume": "main",
    "abstract": "This paper introduces a generalized federated prompt-tuning framework for practical scenarios where local datasets are multi-modal and exhibit different distributional patterns of missing features at the input level. The proposed framework bridges the gap between federated learning and multi-modal prompt-tuning which have traditionally focused on either uni-modal or centralized data. A key challenge in this setting arises from the lack of semantic alignment between prompt instructions that encode similar distributional patterns of missing data across different clients. To address this, our framework introduces specialized client-tuning and server-aggregation designs that simultaneously optimize, align, and aggregate prompt-tuning instructions across clients and data modalities. This allows prompt instructions to complement one another and be combined effectively. Extensive evaluations on diverse multimodal benchmark datasets demonstrate that our work consistently outperforms state-of-the-art (SOTA) baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thu Hang Phung",
      "Duong M. Nguyen",
      "Thanh Trung Huynh",
      "Quoc Viet Hung Nguyen",
      "Trong Nghia Hoang",
      "Phi Le Nguyen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shen_Trace3D_Consistent_Segmentation_Lifting_via_Gaussian_Instance_Tracing_ICCV_2025_paper.html": {
    "title": "Trace3D: Consistent Segmentation Lifting via Gaussian Instance Tracing",
    "volume": "main",
    "abstract": "We address the challenge of lifting 2D visual segmentation to 3D in Gaussian Splatting. Existing methods often suffer from inconsistent 2D masks across viewpoints and produce noisy segmentation boundaries as they neglect these semantic cues to refine the learned Gaussians. To overcome this, we introduce Gaussian Instance Tracing (GIT), which augments the standard Gaussian representation with an instance weight matrix across input views. Leveraging the inherent consistency of Gaussians in 3D, we use this matrix to identify and correct 2D segmentation inconsistencies. Furthermore, since each Gaussian ideally corresponds to a single object, we propose a GIT-guided adaptive density control mechanism to split and prune ambiguous Gaussians during training, resulting in sharper and more coherent 2D and 3D segmentation boundaries. Experimental results show that our method extracts clean 3D assets and consistently improves 3D segmentation in both online (e.g., self-prompting) and offline (e.g., contrastive lifting) settings, enabling applications such as hierarchical segmentation, object extraction, and scene editing",
    "checked": true,
    "id": "058ccf936ce574447ea2b25ef3c77dee4f4ebcc9",
    "semantic_title": "trace3d: consistent segmentation lifting via gaussian instance tracing",
    "citation_count": 2,
    "authors": [
      "Hongyu Shen",
      "Junfeng Ni",
      "Yixin Chen",
      "Weishuo Li",
      "Mingtao Pei",
      "Siyuan Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_LMM-Det_Make_Large_Multimodal_Models_Excel_in_Object_Detection_ICCV_2025_paper.html": {
    "title": "LMM-Det: Make Large Multimodal Models Excel in Object Detection",
    "volume": "main",
    "abstract": "Large multimodal models (LMMs) have garnered wide-spread attention and interest within the artificial intelligence research and industrial communities, owing to their remarkable capability in multimodal understanding, reasoning, and in-context learning, among others. While LMMs have demonstrated promising results in tackling multimodal tasks like image captioning, visual question answering, and visual grounding, the object detection capabilities of LMMs exhibit a significant gap compared to specialist detectors. To bridge the gap, we depart from the conventional methods of integrating heavy detectors with LMMs and propose LMM-Det, a simple yet effective approach that leverages a large multimodal model for vanilla object detection without relying on specialized detection modules. Specifically, we conduct a comprehensive exploratory analysis when a large multimodal model meets with object detection, revealing that the recall rate degrades significantly compared with specialist detection models. To mitigate this, we propose to increase the recall rate by introducing data distribution adjustment and inference optimization tailored for object detection. We re-organize the instruction conversations to enhance the object detection capabilities of large multimodal models. We claim that a large multimodal model possesses detection capability without any extra detection modules. Extensive experiments support our claim and show the effectiveness of the versatile LMM-Det. The datasets, models, and codes are available at https://github.com/360CVGroup/LMM-Det",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jincheng Li",
      "Chunyu Xie",
      "Ji Ao",
      "Dawei Leng",
      "Yuhui Yin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sun_Bootstrap3D_Improving_Multi-view_Diffusion_Model_with_Synthetic_Data_ICCV_2025_paper.html": {
    "title": "Bootstrap3D: Improving Multi-view Diffusion Model with Synthetic Data",
    "volume": "main",
    "abstract": "Recent years have witnessed remarkable progress in multi-view diffusion models for 3D content creation. However, there remains a significant gap in image quality and prompt-following ability compared to 2D diffusion models. A critical bottleneck is the scarcity of high-quality 3D data with detailed captions. To address this challenge, we propose Bootstrap3D, a novel framework that automatically generates filtered multi-view images to assist in training multi-view diffusion models. Specifically, we introduce a data generation pipeline that employs (1) 2D and video diffusion models to generate multi-view images based on constructed text prompts, and (2) our fine-tuned 3D-aware MV-LLaVA for filtering data and rewriting inaccurate captions. Leveraging this pipeline, we have generated large scale synthetic multi-view images with dense descriptive captions. Furthermore, we present a Training Timestep Reschedule (TTR) strategy that leverages the denoising process to learn multi-view consistency while maintaining the original 2D diffusion prior. Extensive experiments demonstrate that Bootstrap3D can generate high-quality multi-view images with superior aesthetic quality, image-text alignment, and view consistency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyi Sun",
      "Tong Wu",
      "Pan Zhang",
      "Yuhang Zang",
      "Xiaoyi Dong",
      "Yuanjun Xiong",
      "Dahua Lin",
      "Jiaqi Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Meanti_Unsupervised_Imaging_Inverse_Problems_with_Diffusion_Distribution_Matching_ICCV_2025_paper.html": {
    "title": "Unsupervised Imaging Inverse Problems with Diffusion Distribution Matching",
    "volume": "main",
    "abstract": "Inverse problems provide a fundamental framework for image reconstruction tasks, spanning deblurring, calibration, or low-light enhancement for instance. While widely used, they often assume full knowledge of the forward model -- an unrealistic expectation -- while collecting ground truth and measurement pairs is time-consuming and labor-intensive.Without paired supervision or an invertible forward model, solving inverse problems becomes significantly more challenging and error-prone. To address this, strong priors have traditionally been introduced to regularize the problem, enabling solutions from single images alone.In this work, however, we demonstrate that with minimal assumptions on the forward model and by leveraging small, unpaired clean and degraded datasets, we can achieve good estimates of the true degradation. We employ conditional flow matching to efficiently model the degraded data distribution and explicitly learn the forward model using a tailored distribution-matching loss.Through experiments on uniform and non-uniform deblurring tasks, we show that our method outperforms both single-image blind and unsupervised approaches, narrowing the gap to non-blind methods. We also showcase the effectiveness of our method with a proof of concept for automatic lens calibration-- a real-world application traditionally requiring time-consuming experiments and specialized equipment. In contrast, our approach achieves this with minimal data acquisition effort",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giacomo Meanti",
      "Thomas Ryckeboer",
      "Michael Arbel",
      "Julien Mairal"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Towards_Adversarial_Robustness_via_Debiased_High-Confidence_Logit_Alignment_ICCV_2025_paper.html": {
    "title": "Towards Adversarial Robustness via Debiased High-Confidence Logit Alignment",
    "volume": "main",
    "abstract": "Despite the remarkable progress of deep neural networks (DNNs) in various visual tasks, their vulnerability to adversarial examples raises significant security concerns. Recent adversarial training methods leverage inverse adversarial attacks to generate high-confidence examples, aiming to align adversarial distributions with high-confidence class regions. However, our investigation reveals that under inverse adversarial attacks, high-confidence outputs are influenced by biased feature activations, causing models to rely on background features that lack a causal relationship with the labels. This spurious correlation bias leads to overfitting irrelevant background features during adversarial training, thereby degrading the model's robust performance and generalization capabilities. To address this issue, we propose Debiased High-Confidence Adversarial Training (DHAT), a novel approach that aligns adversarial logits with debiased high-confidence logits and restores proper attention by enhancing foreground logit orthogonality. Extensive experiments demonstrate that DHAT achieves state-of-the-art robustness on both CIFAR and ImageNet-1K benchmarks, while significantly improving generalization by mitigating the feature bias inherent in inverse adversarial training approaches. Code is available at https://github.com/KejiaZhang-Robust/DHAT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kejia Zhang",
      "Juanjuan Weng",
      "Shaozi Li",
      "Zhiming Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Bai_FiffDepth_Feed-forward_Transformation_of_Diffusion-Based_Generators_for_Detailed_Depth_Estimation_ICCV_2025_paper.html": {
    "title": "FiffDepth: Feed-forward Transformation of Diffusion-Based Generators for Detailed Depth Estimation",
    "volume": "main",
    "abstract": "Monocular Depth Estimation (MDE) is a fundamental 3D vision problem with numerous applications such as 3D scene reconstruction, autonomous navigation, and AI content creation. However, robust and generalizable MDE remains challenging due to limited real-world labeled data and distribution gaps between synthetic datasets and real data. Existing methods often struggle on real-world test data with low efficiency, reduced accuracy, and lack of detail. To address these issues, we propose an efficient MDE approach named FiffDepth. The key feature of FiffDepth is its use of diffusion priors. It transforms diffusion-based image generators into a feed-forward architecture for detailed depth estimation. FiffDepth preserves key generative features and integrates the strong generalization capabilities of models like DINOv2. Through benchmark evaluations, we demonstrate that FiffDepth achieves exceptional accuracy, stability, and fine-grained detail, offering significant improvements in MDE performance against state-of-the-art MDE approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunpeng Bai",
      "Qixing Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_Reinforcement_Learning-Guided_Data_Selection_via_Redundancy_Assessment_ICCV_2025_paper.html": {
    "title": "Reinforcement Learning-Guided Data Selection via Redundancy Assessment",
    "volume": "main",
    "abstract": "Modern deep architectures often rely on large-scale datasets, but training on these datasets incurs high computational and storage overhead. Real-world datasets often contain substantial redundancies, prompting the need for more data-efficient training paradigms. Data selection has shown promise to mitigate redundancy by identifying the most representative samples, thereby reducing training costs without compromising performance. Existing methods typically rely on static scoring metrics or pretrained models, overlooking the combined effect of selected samples and their evolving dynamics during training. To address this, we introduce the concept of epsilon-sample cover, which quantifies sample redundancy based on inter-sample relationships, capturing the intrinsic structure of the dataset. Based on this, we reformulate data selection as a reinforcement learning (RL) process, where a lightweight RL agent optimizes the selection policy by leveraging epsilon-sample cover derived from evolving dataset distribution as a reward signal. Extensive experiments across benchmark datasets and diverse architectures demonstrate that our method consistently outperforms existing state-of-the-art baselines. Models trained with our selected datasets show enhanced generalization performance with improved training efficiency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suorong Yang",
      "Peijia Li",
      "Furao Shen",
      "Jian Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Integrating_Biological_Knowledge_for_Robust_Microscopy_Image_Profiling_on_De_ICCV_2025_paper.html": {
    "title": "Integrating Biological Knowledge for Robust Microscopy Image Profiling on De Novo Cell Lines",
    "volume": "main",
    "abstract": "High-throughput screening techniques, such as microscopy imaging of cellular responses to genetic and chemical perturbations, play a crucial role in drug discovery and biomedical research. However, robust perturbation screening for de novo cell lines remains challenging due to the significant morphological and biological heterogeneity across cell lines. To address this, we propose a novel framework that integrates external biological knowledge into existing pretraining strategies to enhance microscopy image profiling models. Our approach explicitly disentangles perturbation-specific and cell line-specific representations using external biological information. Specifically, we construct a knowledge graph leveraging protein interaction data from STRING and Hetionet databases to guide models toward perturbation-specific features during pretraining. Additionally, we incorporate transcriptomic features from single-cell foundation models to capture cell line-specific representations. By learning these disentangled features, our method improves the generalization of imaging models to de novo cell lines. We evaluate our framework on the RxRx database through one-shot fine-tuning on an RxRx1 cell line and few-shot fine-tuning on cell lines from the RxRx19a dataset. Experimental results demonstrate that our method enhances microscopy image profiling for de novo cell lines, highlighting its effectiveness in real-world phenotype-based drug discovery applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayuan Chen",
      "Thai-Hoang Pham",
      "Yuanlong Wang",
      "Ping Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ramachandran_OuroMamba_A_Data-Free_Quantization_Framework_for_Vision_Mamba_ICCV_2025_paper.html": {
    "title": "OuroMamba: A Data-Free Quantization Framework for Vision Mamba",
    "volume": "main",
    "abstract": "We present OuroMamba, the first data-free post-training quantization (DFQ) method for vision Mamba-based models (VMMs). We identify two key challenges in enabling DFQ for VMMs, (1) VMM's recurrent state transitions restricts the capturing of long-range interactions and leads to semantically weak synthetic data, (2) VMM activations exhibit dynamic outlier variations across time-steps, rendering existing static PTQ techniques ineffective. To address these challenges, OuroMamba presents a two-stage framework: (1) OuroMamba-Gen to generate semantically rich and meaningful synthetic data. It applies contrastive learning on patch level VMM features generated through neighborhood interactions in the latent state space, (2) OuroMamba-Quant to employ mixed-precision quantization with lightweight dynamic outlier detection during inference. In specific, we present a thresholding based outlier channel selection strategy for activations that gets updated every time-step. Extensive experiments across vision and generative tasks show that our data-free OuroMamba surpasses existing data-driven PTQ techniques, achieving state-of-the-art performance across diverse quantization settings. Additionally, we implement efficient GPU kernels to achieve practical latency speedup of up to 2.36x. Code and synthetic dataset are available here: https://github.com/georgia-tech-synergy-lab/ICCV-OuroMamba",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akshat Ramachandran",
      "Mingyu Lee",
      "Huan Xu",
      "Souvik Kundu",
      "Tushar Krishna"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Fan_Optical_Model-Driven_Sharpness_Mapping_for_Autofocus_in_Small_Depth-of-Field_and_ICCV_2025_paper.html": {
    "title": "Optical Model-Driven Sharpness Mapping for Autofocus in Small Depth-of-Field and Severe Defocus Scenarios",
    "volume": "main",
    "abstract": "Autofocus (AF) is essential for imaging systems, particularly in industrial applications such as automated optical inspection (AOI), where achieving precise focus is critical. Conventional AF methods rely on peak-searching algorithms that require dense focal sampling, making them inefficient in small depth-of-field (DoF) scenarios. Deep learning (DL)-based AF methods, while effective in general imaging, have a limited working range in small DoF conditions due to defocus uncertainty. In this work, we propose a novel AF framework that integrates an optical model-based sharpness indicator with a deep learning approach to predict sharpness from defocused images. We leverage sharpness estimation as a reliable focus measure and apply an adaptive adjustment algorithm to adjust the focus position based on the sharpness-to-distance mapping. This method effectively addresses defocus uncertainty and enables robust autofocus across a 35x DoF range.Experimental results on an AOI system demonstrate that our approach achieves reliable autofocus even from highly defocused starting points and remains robust across different textures and illumination conditions. Compared to conventional and existing DL-based approaches, our method offers improved precision, efficiency, and adaptability, making it suitable for industrial applications and small DoF scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen-Liang Fan",
      "Mingpei Cao",
      "Chih Chien  Hung",
      "Yuesheng Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Luo_DICE_Staleness-Centric_Optimizations_for_Parallel_Diffusion_MoE_Inference_ICCV_2025_paper.html": {
    "title": "DICE: Staleness-Centric Optimizations for Parallel Diffusion MoE Inference",
    "volume": "main",
    "abstract": "Mixture-of-Experts-based (MoE-based) diffusion models demonstrate remarkable scalability in high-fidelity image generation, yet their reliance on expert parallelism introduces critical communication bottlenecks. State-of-the-art methods alleviate such overhead in parallel diffusion inference through computation-communication overlapping, termed displaced parallelism. However, we identify that these techniques induce severe staleness --- the usage of outdated activations from previous timesteps that significantly degrades quality, especially in expert-parallel scenarios. We tackle this fundamental tension and propose DICE, a staleness-centric optimization framework with a three-fold approach: (1) Interweaved Parallelism introduces staggered pipelines, effectively halving step-level staleness for free; (2) Selective Synchronization operates at layer-level and protects layers vulnerable to staled activations; and (3) Conditional Communication, a token-level, training-free method that dynamically adjusts communication frequency based on token importance. Together, these strategies effectively reduce staleness, achieving 1.26x speedup with minimal quality degradation. Empirical results establish DICE as an effective and scalable solution. Our code is available at https://github.com/Cobalt-27/DICE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiajun Luo",
      "Lizhuo Luo",
      "Jianru Xu",
      "Jiajun Song",
      "Rongwei Lu",
      "Chen Tang",
      "Zhi Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Safeguarding_Vision-Language_Models_Mitigating_Vulnerabilities_to_Gaussian_Noise_in_Perturbation-based_ICCV_2025_paper.html": {
    "title": "Safeguarding Vision-Language Models: Mitigating Vulnerabilities to Gaussian Noise in Perturbation-based Attacks",
    "volume": "main",
    "abstract": "Vision-Language Models (VLMs) extend the capabilities of Large Language Models (LLMs) by incorporating visual information, yet they remain vulnerable to jailbreak attacks, especially when processing noisy or corrupted images. Although existing VLMs adopt security measures during training to mitigate such attacks, vulnerabilities associated with noise-augmented visual inputs are overlooked. In this work, we identify that missing noise-augmented training causes critical security gaps: many VLMs are susceptible to even simple perturbations such as Gaussian noise. To address this challenge, we propose Robust-VLGuard, a multimodal safety dataset with aligned / misaligned image-text pairs, combined with noise-augmented fine-tuning that reduces attack success rates while preserving functionality of VLM. For stronger optimization-based visual perturbation attacks, we propose DiffPure-VLM, leveraging diffusion models to convert adversarial perturbations into Gaussian-like noise, which can be defended by VLMs with noise-augmented safety fine-tuning. Experimental results demonstrate that the distribution-shifting property of diffusion model aligns well with our fine-tuned VLMs, significantly mitigating adversarial perturbations across varying intensities. The dataset and code are available at https://github.com/JarvisUSTC/DiffPure-RobustVLM",
    "checked": true,
    "id": "7632f78bcbd6258d17a02886bd6b03819f3e5af1",
    "semantic_title": "safeguarding vision-language models: mitigating vulnerabilities to gaussian noise in perturbation-based attacks",
    "citation_count": 0,
    "authors": [
      "Jiawei Wang",
      "Yushen Zuo",
      "Yuanjun Chai",
      "Zhendong Liu",
      "Yicheng Fu",
      "Yichun Feng",
      "Kin-Man Lam"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Schoneveld_SHeaP_Self-Supervised_Head_Geometry_Predictor_Learned_via_2D_Gaussians_ICCV_2025_paper.html": {
    "title": "SHeaP: Self-Supervised Head Geometry Predictor Learned via 2D Gaussians",
    "volume": "main",
    "abstract": "Accurate, real-time 3D reconstruction of human heads from monocular images and videos underlies numerous visual applications. As 3D ground truth data is hard to come by at scale, previous methods have sought to learn from abundant 2D videos in a self-supervised manner. Typically, this involves the use of differentiable mesh rendering, which is effective but faces limitations. To improve on this, we propose SHeaP (Self-supervised Head Geometry Predictor Learned via 2D Gaussians).Given a source image, we predict a 3DMM mesh and a set of Gaussians that are rigged to this mesh. We then reanimate this rigged head avatar to match a target frame, and backpropagate photometric losses to both the 3DMM and Gaussian prediction networks. We find that using Gaussians for rendering substantially improves the effectiveness of this self-supervised approach.Training solely on 2D data, our method surpasses existing self-supervised approaches in geometric evaluations on the NoW benchmark for neutral faces and a new benchmark for non-neutral expressions. Our method also produces highly expressive meshes, outperforming state-of-the-art in emotion classification",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liam Schoneveld",
      "Zhe Chen",
      "Davide Davoli",
      "Jiapeng Tang",
      "Saimon Terazawa",
      "Ko Nishino",
      "Matthias Nießner"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xue_Pretrained_Reversible_Generation_as_Unsupervised_Visual_Representation_Learning_ICCV_2025_paper.html": {
    "title": "Pretrained Reversible Generation as Unsupervised Visual Representation Learning",
    "volume": "main",
    "abstract": "Recent generative models based on score matching and flow matching have significantly advanced generation tasks, but their potential in discriminative tasks remains underexplored. Previous approaches, such as generative classifiers, have not fully leveraged the capabilities of these models for discriminative tasks due to their intricate designs. We propose Pretrained Reversible Generation (PRG), which extracts unsupervised representations by reversing the generative process of a pretrained continuous generation model. PRG effectively reuses unsupervised generative models, leveraging their high capacity to serve as robust and generalizable feature extractors for downstream tasks. This framework enables the flexible selection of feature hierarchies tailored to specific downstream tasks. Our method consistently outperforms prior approaches across multiple benchmarks, achieving state-of-the-art performance among generative model based methods, including 78% top-1 accuracy on ImageNet at a resolution of 64x64. Extensive ablation studies, including out-of-distribution evaluations, further validate the effectiveness of our approach. PRG is available at https://opendilab.github.io/PRG",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rongkun Xue",
      "Jinouwen Zhang",
      "Yazhe Niu",
      "Dazhong Shen",
      "Bingqi Ma",
      "Yu Liu",
      "Jing Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhu_CompSlider_Compositional_Slider_for_Disentangled_Multiple-Attribute_Image_Generation_ICCV_2025_paper.html": {
    "title": "CompSlider: Compositional Slider for Disentangled Multiple-Attribute Image Generation",
    "volume": "main",
    "abstract": "In text-to-image (T2I) generation, achieving fine-grained control over attributes - such as age or smile - remains challenging, even with detailed text prompts. Slider-based methods offer a solution for precise control of image attributes.Existing approaches typically train individual adapter for each attribute separately, overlooking the entanglement among multiple attributes. As a result, interference occurs among different attributes, preventing precise control of multiple attributes together. To address this challenge, we aim to disentangle multiple attributes in slider-based generation to enbale more reliable and independent attribute manipulation. Our approach, CompSlider, can generate a conditional prior for the T2I foundation model to control multiple attributes simultaneously. Furthermore, we introduce novel disentanglement and structure losses to compose multiple attribute changes while maintaining structural consistency within the image. Since CompSlider operates in the latent space of the conditional prior and does not require retraining the foundation model, it reduces the computational burden for both training and inference. We evaluate our approach on a variety of image attributes and highlight its generality by extending to video generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixin Zhu",
      "Kevin Duarte",
      "Mamshad Nayeem Rizve",
      "Chengyuan Xu",
      "Ratheesh Kalarot",
      "Junsong Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Byun_MA-CIR_A_Multimodal_Arithmetic_Benchmark_for_Composed_Image_Retrieval_ICCV_2025_paper.html": {
    "title": "MA-CIR: A Multimodal Arithmetic Benchmark for Composed Image Retrieval",
    "volume": "main",
    "abstract": "Composed Image Retrieval (CIR) seeks to retrieve a target image by using a reference image and conditioning text specifying desired modifications. While recent approaches have shown steady performance improvements on existing CIR benchmarks, we argue that it remains unclear whether these gains genuinely reflect an enhanced compositional understanding of both visual and textual information. For example, current benchmarks do not explicitly consider negation cases and offer limited semantic diversity, with insufficient hard negatives to thoroughly evaluate the CIR task. To bridge this gap, we introduce Multimodal Arithmetic Benchmark for CIR (MA-CIR), a challenging CIR benchmark that integrates arithmetic types (negation, replacement, and addition) across seven complex semantic categories (e.g., spatial reasoning, object reasoning, etc). Moreover, carefully constructed hard negatives are incorporated to assess models in a controlled setting. In MA-CIR, we observe that current CIR models struggle with negation (or replacement) arithmetic types and semantic types that require complex reasoning, indicating a potential reliance on object or concept information. To tackle this, we propose leveraging strong text encoders, particularly those based on large language models (LLMs), and fine-tuning them using carefully constructed text triplets that include hard negatives, thereby enhancing their compositional understanding. As a result, MA-CIR achieves a 16% gain on MA-CIR, along with 6% and 9% improvements on CIRR and CIRCO, respectively. Our code and dataset are available in https://github.com/jaeseokbyun/MACIR",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaeseok Byun",
      "Young Kyun Jang",
      "Seokhyeon Jeong",
      "Donghyun Kim",
      "Taesup Moon"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Nan_NAPPure_Adversarial_Purification_for_Robust_Image_Classification_under_Non-Additive_Perturbations_ICCV_2025_paper.html": {
    "title": "NAPPure: Adversarial Purification for Robust Image Classification under Non-Additive Perturbations",
    "volume": "main",
    "abstract": "Adversarial purification has achieved great success in combating adversarial image perturbations, which are usually assumed to be additive. However, non-additive adversarial perturbations such as blur, occlusion, and distortion are also common in the real world. Under such perturbations, existing adversarial purification methods are much less effective since they are designed to fit the additive nature. In this paper, we propose an extended adversarial purification framework named NAPPure, which can further handle non-additive perturbations. Specifically, we first establish the generation process of an adversarial image, and then disentangle the underlying clean image and perturbation parameters through likelihood maximization. Experiments on GTSRB and CIFAR-10 datasets show that NAPPure significantly boosts the robustness of image classification models against non-additive perturbations",
    "checked": true,
    "id": "c462d69d8898571240745a7c3fadaaf6ca332f08",
    "semantic_title": "nappure: adversarial purification for robust image classification under non-additive perturbations",
    "citation_count": 0,
    "authors": [
      "Junjie Nan",
      "Jianing Li",
      "Wei Chen",
      "Mingkun Zhang",
      "Xueqi Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kazakos_Large-scale_Pre-training_for_Grounded_Video_Caption_Generation_ICCV_2025_paper.html": {
    "title": "Large-scale Pre-training for Grounded Video Caption Generation",
    "volume": "main",
    "abstract": "We propose a novel approach for captioning and object grounding in video, where the objects in the caption are grounded in the video via temporally dense bounding boxes. We introduce the following contributions. First, we present a large-scale automatic annotation method that aggregates frame-level captions grounded with bounding boxes into temporally dense and consistent annotations. We apply this approach on the HowTo100M dataset to construct a large-scale pre-training dataset, named HowToGround1M. We also introduce a Grounded Video Caption Generation model, dubbed GROVE, and pre-train the model on HowToGround1M. Second, we introduce iGround-a dataset of 3513 videos with manually annotated captions and dense spatio-temporally grounded bounding boxes. This allows us to measure progress on this challenging problem, as well as to fine-tune our model on this small-scale but high-quality data. Third, we demonstrate that our approach achieves state-of-the-art results on the proposed iGround dataset, as well as on the VidSTG, ActivityNet-Entities, GroundingYouTube, and YouCook-Interactions datasets. Our ablations demonstrate the importance of pre-training on our automatically annotated HowToGround1M dataset followed by fine-tuning on the manually annotated iGround dataset and validate the key technical contributions of our model. The dataset and code are available at https://ekazakos.github.io/grounded_video_caption_generation/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Evangelos Kazakos",
      "Cordelia Schmid",
      "Josef Sivic"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yao_MMReason_An_Open-Ended_Multi-Modal_Multi-Step_Reasoning_Benchmark_for_MLLMs_Toward_ICCV_2025_paper.html": {
    "title": "MMReason: An Open-Ended Multi-Modal Multi-Step Reasoning Benchmark for MLLMs Toward AGI",
    "volume": "main",
    "abstract": "Reasoning plays a crucial role in advancing Multimodal Large Language Models (MLLMs) toward Artificial General Intelligence.However, existing MLLM benchmarks often fall short in precisely and comprehensively evaluating long-chain reasoning abilities from three key aspects: (1) lack of difficulty and diversity, (2) susceptibility to guessability and memorization, (3) inadequate assessment of intermediate reasoning steps.To fill this gap, we introduce **MMReason**, a new benchmark designed to precisely and comprehensively evaluate MLLM long-chain reasoning capability with diverse, open-ended, challenging questions.First, we curate challenging questions requiring multi-step reasoning from various fields (i.e., 6 disciplines) and multiple difficulty levels (i.e., from pre-university to university, and from foundational to competition tiers).Second, these questions are reformulated into an open-ended format and filtered using a multi-model voting technique to eliminate shortcut cases related to guessing and memorization, ensuring robust reasoning evaluations.Third, we annotate the questions with detailed step-by-step solutions, and design a reference-based ternary scoring mechanism to reliably assess intermediate reasoning steps.With MMReason, we benchmark popular leading MLLMs and provide an in-depth analysis of their reasoning capabilities.We hope MMReason will serve as a valuable resource for advancing MLLM reasoning research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huanjin Yao",
      "Jiaxing Huang",
      "Yawen Qiu",
      "Michael K. Chen",
      "Wenzheng Liu",
      "Wei Zhang",
      "Wenjie Zeng",
      "Xikun Zhang",
      "Jingyi Zhang",
      "YuXin Song",
      "Wenhao Wu",
      "Dacheng Tao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_SimMLM_A_Simple_Framework_for_Multi-modal_Learning_with_Missing_Modality_ICCV_2025_paper.html": {
    "title": "SimMLM: A Simple Framework for Multi-modal Learning with Missing Modality",
    "volume": "main",
    "abstract": "In this paper, we propose SimMLM, a simple yet powerful framework for multimodal learning with missing modalities. Unlike existing approaches that rely on sophisticated network architectures or complex data imputation techniques, SimMLM provides a generic and effective solution that can adapt to various missing modality scenarios with improved accuracy and robustness. Specifically, SimMLM consists of a generic Dynamic Mixture of Modality Experts (DMoME) architecture, featuring a dynamic, learnable gating mechanism that automatically adjusts each modality's contribution in both full and partial modality settings. A key innovation of SimMLM is the proposed More vs. Fewer (MoFe) ranking loss, which ensures that task accuracy improves or remains stable as more modalities are made available. This aligns the model with an intuitive principle: removing one or more modalities should not increase accuracy. We validate SimMLM on multimodal medical image segmentation (BraTS 2018) and multimodal classification (UPMC Food-101, avMNIST) tasks, where it consistently surpasses competitive methods, demonstrating superior accuracy, interpretability, robustness, and reliability across both complete and missing modality scenarios at test time",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sijie Li",
      "Chen Chen",
      "Jungong Han"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liang_Dual_Reciprocal_Learning_of_Language-based_Human_Motion_Understanding_and_Generation_ICCV_2025_paper.html": {
    "title": "Dual Reciprocal Learning of Language-based Human Motion Understanding and Generation",
    "volume": "main",
    "abstract": "Language-based human motion understanding focuses on describing human motions using natural language descriptions. Conversely, human motion generation aims to generate human motions from textual inputs. Despite significant progress in both fields, further advancements are hindered by two primary challenges: (i) Both tasks heavily rely on vast amounts of paired motion-language data for model training. However, human labeling is costly, making it increasingly unsustainable as model scales increase. (ii) Existing models often learn the two tasks in parallel. The strong reciprocity between them has not been fully explored. In response, this work proposes Dual Reciprocal Learning (DRL) for language-based human motion understanding and generation. DRL establishes a symmetric learning framework where both tasks collaboratively evolve in a closed-loop, bootstrapping manner, effectively leveraging the reciprocity between them. In DRL, the tasks serve as evaluators for each other, enabling the generation of informative feedback signals even with easily acquired unidirectional motion or language data. Furthermore, to mitigate dataset-specific bias in existing evaluations, we propose a generalized protocol that extends evaluation to a general-domain cross-modal feature space. Experimental results on standard benchmarks demonstrate that DRL achieves remarkable performance boosts over representative baselines in both tasks across evaluation protocols",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Liang",
      "Zhicheng Shi",
      "Wenguan Wang",
      "Yi Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_InteractAvatar_Modeling_Hand-Face_Interaction_in_Photorealistic_Avatars_with_Deformable_Gaussians_ICCV_2025_paper.html": {
    "title": "InteractAvatar: Modeling Hand-Face Interaction in Photorealistic Avatars with Deformable Gaussians",
    "volume": "main",
    "abstract": "With the rising interest from the community in digital avatars coupled with the importance of expressions and gestures in communication, modeling natural avatar behavior remains an important challenge across many industries such as teleconferencing, gaming, and AR/VR. Human hands are the primary tool for interacting with the environment and essential for realistic human behavior modeling, yet existing 3D hand and head avatar models often overlook the crucial aspect of hand-body interactions, such as between hand and face. We present InteractAvatar, the first model to faithfully capture the photorealistic appearance of dynamic hand and non-rigid hand-face interactions. Our novel Dynamic Gaussian Hand model, combining template model and 3D Gaussian Splatting as well as a dynamic refinement module, captures pose-dependent change, e.g. the fine wrinkles and complex shadows that occur during articulation. Importantly, our hand-face interaction module models the subtle geometry and appearance dynamics that underlie common gestures.Through experiments of novel view synthesis, self reenactment and cross-identity reenactment, we demonstrate that InteractAvatar can reconstruct hand and hand-face interactions from multi-view videos with high-fidelity details and be animated with novel poses",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kefan Chen",
      "Sreyas Mohan",
      "Justin Theiss",
      "Sergiu Oprea",
      "Srinath Sridhar",
      "Aayush Prakash"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_LINR-PCGC_Lossless_Implicit_Neural_Representations_for_Point_Cloud_Geometry_Compression_ICCV_2025_paper.html": {
    "title": "LINR-PCGC: Lossless Implicit Neural Representations for Point Cloud Geometry Compression",
    "volume": "main",
    "abstract": "Existing AI-based point cloud compression methods struggle with dependence on specific training data distributions, which limits their real-world deployment. Implicit Neural Representation (INR) methods solve the above problem by encoding overfitted network parameters to the bitstream, resulting in more distribution-agnostic results. However, due to the limitation of encoding time and decoder size, current INR based methods only consider lossy geometry compression. In this paper, we propose the first lossless point cloud geometry compression method called Lossless Implicit Neural Representations for Point Cloud Geometry Compression (LINR-PCGC). To accelerate encoding speed, we design a group of point clouds level coding framework with an effective network initialization strategy, which can reduce around 60% encoding time. A lightweight coding network based on multiscale SparseConv, consisting of scale context extraction, child node prediction, and model compression modules, is proposed to realize fast inference and compact decoder size. Experimental results show that our method consistently outperforms traditional and AI-based methods: for example, with the convergence time in the MVUB dataset, our method reduces the bitstream by approximately 21.21% compared to G-PCC TMC13v23 and 21.95% compared to SparsePCGC",
    "checked": true,
    "id": "5a08d263bb6cd610bf9bf67065622d69b0048669",
    "semantic_title": "linr-pcgc: lossless implicit neural representations for point cloud geometry compression",
    "citation_count": 1,
    "authors": [
      "Wenjie Huang",
      "Qi Yang",
      "Shuting Xia",
      "He Huang",
      "Yiling Xu",
      "Zhu Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Du_RoCo-Sim_Enhancing_Roadside_Collaborative_Perception_through_Foreground_Simulation_ICCV_2025_paper.html": {
    "title": "RoCo-Sim: Enhancing Roadside Collaborative Perception through Foreground Simulation",
    "volume": "main",
    "abstract": "Roadside Collaborative Perception refers to a system where multiple roadside units collaborate to pool their perceptual data, assisting vehicles in enhancing their environmental awareness. Existing roadside perception methods concentrate on model design but overlook data issues like calibration errors, sparse information, and multi-view consistency, leading to poor performance on recent published datasets. To significantly enhance roadside collaborative perception and address critical data issues, we present the first simulation framework RoCo-Sim for road-side collaborative perception. RoCo-Sim is capable of generating diverse, multi-view consistent simulated roadside data through dynamic foreground editing and full-scene style transfer of a single image. RoCo-Sim consists of four components: (1) Camera Extrinsic Optimization ensures accurate 3D to 2D projection for roadside cameras; (2) A novel Multi-View Occlusion-Aware Sampler (MOAS) determines the placement of diverse digital assets within 3D space; (3) DepthSAM innovatively models foreground-background relationships from single-frame fixed-view images, ensuring multi-view consistency of foreground; and (4) Scalable Post-Processing Toolkit generates more realistic and enriched scenes through style transfer and other enhancements. RoCo-Sim significantly improves roadside 3D object detection, outperforming SOTA methods by 83.74% on Rcooper-Intersection and 83.12% on TUMTraf-V2X for AP70. RoCo-Sim fills a critical gap in roadside perception simulation. Code and pre-trained models will be released soon",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuwen Du",
      "Anning Hu",
      "Zichen Chao",
      "Yifan Lu",
      "Junhao Ge",
      "Genjia Liu",
      "Weitao Wu",
      "Lanjun Wang",
      "Siheng Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Bourigault_UKBOB_One_Billion_MRI_Labeled_Masks_for_Generalizable_3D_Medical_ICCV_2025_paper.html": {
    "title": "UKBOB: One Billion MRI Labeled Masks for Generalizable 3D Medical Image Segmentation",
    "volume": "main",
    "abstract": "In the medical imaging domain, it is a fundamental challenge to collect large-scale labeled data due to privacy, involved logistics, and the high cost of labeling medical images. In this work, we present the UK Biobank Organs and Bones (UKBOB), the largest labeled dataset of body organs of 51,761 MRI 3D samples (17.9 M 2D images) and a total of more than 1.37 billion 2D segmentation masks of 72 organs based on the UK Biobank MRI dataset. We utilize automatic labeling, filter the labels with organ-specific filters, and manually annotate a subset of 300 MRIs with 11 abdominal classes to validate the quality (UKBOB-manual). This approach allows for scaling up the dataset collection while maintaining confidence in the labels. We further confirm the validity of the labels by the zero-shot generalization of trained models on the filtered UKBOB to other small labeled datasets from a similar domain ( _E.g._ abdominal MRI). To further elevate the effect of the noisy labels, we propose a novel Entropy Test-time Adaptation (ETTA) to refine the segmentation output. We use UKBOB to train a foundation model (_Swin-BOB_) for 3D medical image segmentation based on Swin-UNetr, achieving state-of-the-art results in several benchmarks in 3D medical imaging, including BRATS brain MRI tumour challenge (+0.4% improvement), and BTCV abdominal CT scan benchmark (+1.3% improvement). Pre-trained model and our filtered labels will be made available with the UK Biobank",
    "checked": true,
    "id": "de9d49838f859d0692ce78902e0a9bce4b9998a7",
    "semantic_title": "ukbob: one billion mri labeled masks for generalizable 3d medical image segmentation",
    "citation_count": 1,
    "authors": [
      "Emmanuelle Bourigault",
      "Amir Jamaludin",
      "Abdullah Hamdi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Machnio_To_Label_or_Not_to_Label_PALM_-_A_Predictive_ICCV_2025_paper.html": {
    "title": "To Label or Not to Label: PALM - A Predictive Model for Evaluating Sample Efficiency in Active Learning Models",
    "volume": "main",
    "abstract": "Active learning (AL) seeks to reduce annotation costs by selecting the most informative samples for labeling, making it particularly valuable in resource-constrained settings. However, traditional evaluation methods, which focus solely on final accuracy, fail to capture the full dynamics of the learning process. To address this gap, we propose PALM (Performance Analysis of Active Learning Models), a unified and interpretable mathematical model that characterizes AL trajectories through four key parameters: achievable accuracy, coverage efficiency, early-stage performance, and scalability. PALM provides a predictive description of AL behavior from partial observations, enabling the estimation of future performance and facilitating principled comparisons across different strategies. We validate PALM through extensive experiments on CIFAR-10/100 and ImageNet-50/100/200, covering a wide range of AL methods and self-supervised embeddings. Our results demonstrate that PALM generalizes effectively across datasets, budgets, and strategies, accurately predicting full learning curves from limited labeled data. Importantly, PALM reveals crucial insights into learning efficiency, data space coverage, and the scalability of AL methods. By enabling the selection of cost-effective strategies and predicting performance under tight budget constraints, PALM lays the basis for more systematic, reproducible, and data-efficient evaluation of AL in both research and real-world applications. The code is available at: https://github.com/juliamachnio/PALM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julia Machnio",
      "Mads Nielsen",
      "Mostafa Mehdipour Ghazi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gan_GaussianOcc_Fully_Self-supervised_and_Efficient_3D_Occupancy_Estimation_with_Gaussian_ICCV_2025_paper.html": {
    "title": "GaussianOcc: Fully Self-supervised and Efficient 3D Occupancy Estimation with Gaussian Splatting",
    "volume": "main",
    "abstract": "We introduce GaussianOcc, a systematic method that investigates Gaussian splatting for fully self-supervised and efficient 3D occupancy estimation in surround views. First, traditional methods for self-supervised 3D occupancy estimation still require ground truth 6D poses from sensors during training. To address this limitation, we propose Gaussian Splatting for Projection (GSP) module to provide accurate scale information for fully self-supervised training from adjacent view projection. Additionally, existing methods rely on volume rendering for final 3D voxel representation learning using 2D signals (depth maps, semantic maps), which is both time-consuming and less effective. We propose Gaussian Splatting from Voxel space (GSV) to leverage the fast rendering properties of Gaussian splatting. As a result, the proposed GaussianOcc method enables fully self-supervised (no ground truth pose) 3D occupancy estimation in competitive performance with low computational cost (2.7 times faster in training and 5 times faster in rendering)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wanshui Gan",
      "Fang Liu",
      "Hongbin Xu",
      "Ningkai Mo",
      "Naoto Yokoya"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Free-MoRef_Instantly_Multiplexing_Context_Perception_Capabilities_of_Video-MLLMs_within_Single_ICCV_2025_paper.html": {
    "title": "Free-MoRef: Instantly Multiplexing Context Perception Capabilities of Video-MLLMs within Single Inference",
    "volume": "main",
    "abstract": "Video Multimodal Large Language Models (Video-MLLM) have achieved remarkable advancements in video understanding tasks. However, constrained by the context length limitation in the underlying LLMs, existing Video-MLLMs typically exhibit suboptimal performance on long video scenarios. To understand extended input frames, common solutions span token compression and streaming inference techniques, which sacrifice feature granularity or inference efficiency. Differently, to efficiently achieve comprehensive understanding of longer frame inputs, we draw ideas from MoE and propose a training-free approach Free-MoRef, which instantly multiplexes the context perception capabilities of Video-MLLMs within one inference pass. Specifically, Free-MoRef reconstructs the vision tokens into several short sequences as multi-references. Subsequently, we introduce MoRef-attention, which gathers clues from the multi-reference chunks in parallel to summarize unified query activations. After the shadow layers in LLMs, a reference fusion step is derived to compose a final mixed reasoning sequence with key tokens from parallel chunks, which compensates the cross-reference vision interactions that are neglected in MoRef-attention. By splitting and fusing the long vision token sequences, Free-MoRef achieves improved performance under much lower computing costs in reasoning multiplexed context length, demonstrating strong efficiency and effectiveness. Experiments on VideoMME, MLVU, LongVideoBench show that Free-MoRef achieves full perception of 2xto 8xlonger input frames without compression on a single A100 GPU while keeping instant responses, thereby bringing significant performance gains, even surpassing dedicatedly trained long-video-MLLMs. Codes are available at https://github.com/wkfdb/Free-MoRef",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kuo Wang",
      "Quanlong Zheng",
      "Junlin Xie",
      "Yanhao Zhang",
      "Jinguo Luo",
      "Haonan Lu",
      "Liang Lin",
      "Fan Zhou",
      "Guanbin Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_DC-ControlNet_Decoupling_Inter-_and_Intra-Element_Conditions_in_Image_Generation_with_ICCV_2025_paper.html": {
    "title": "DC-ControlNet: Decoupling Inter- and Intra-Element Conditions in Image Generation with Diffusion Models",
    "volume": "main",
    "abstract": "In this paper, we introduce DC (Decouple)-ControlNet, a highly flexible and precisely controllable framework for multi-condition image generation. The core idea behind DC-ControlNet is to decouple control conditions, transforming global control into a hierarchical system that integrates distinct elements, contents, and layouts. This enables users to mix these individual conditions with greater flexibility, leading to more efficient and accurate image generation control. Previous ControlNet-based models rely solely on global conditions, which affect the entire image and lack the ability of element- or region-specific control. This limitation reduces flexibility and can cause condition misunderstandings in multi-conditional image generation. To address these challenges, we propose both intra-element and inter-element Controllers in DC-ControlNet. The Intra-Element Controller handles different types of control signals within individual elements, accurately describing the content and layout characteristics of the object. For interactions between elements, we introduce the Inter-Element Controller, which accurately handles multi-element interactions and occlusion based on user-defined relationships. Extensive evaluations show that DC-ControlNet significantly outperforms existing ControlNet models and Layout-to-Image generative models in terms of control flexibility and precision in multi-condition control",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongji Yang",
      "Wencheng Han",
      "Yucheng Zhou",
      "Jianbing Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cai_EfficientMT_Efficient_Temporal_Adaptation_for_Motion_Transfer_in_Text-to-Video_Diffusion_ICCV_2025_paper.html": {
    "title": "EfficientMT: Efficient Temporal Adaptation for Motion Transfer in Text-to-Video Diffusion Models",
    "volume": "main",
    "abstract": "The progress on generative models has led to significant advances on text-to-video (T2V) generation, yet the motion controllability of generated videos remains limited. Existing motion transfer approaches explored the motion representations of reference videos to guide generation. Nevertheless, these methods typically rely on sample-specific optimization frameworks, resulting in high computational burdens. In this paper, we propose EfficientMT, a novel and efficient end-to-end framework for video motion transfer. By leveraging a small set of synthetic paired motion transfer samples, EfficientMT effectively adapts a pretrained T2V model into a general motion transfer framework that can accurately capture and reproduce diverse motion patterns. Specifically, we repurpose the backbone of the T2V model to extract temporal information from reference videos, and further propose a scaler module to distill motion-related information. Subsequently, we introduce a temporal integration mechanism that seamlessly incorporates reference motion features into the video generation process. After training on our self-collected synthetic paired samples, EfficientMT enables general video motion transfer without requiring test-time optimization. Extensive experiments demonstrate that our EfficientMT outperforms existing methods in efficiency while maintaining flexible motion controllability. Our code will be available at https://github.com/PrototypeNx/EfficientMT",
    "checked": true,
    "id": "cc5e37912d984d75b723199bf0ab1df9610c8b45",
    "semantic_title": "efficientmt: efficient temporal adaptation for motion transfer in text-to-video diffusion models",
    "citation_count": 0,
    "authors": [
      "Yufei Cai",
      "Hu Han",
      "Yuxiang Wei",
      "Shiguang Shan",
      "Xilin Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhu_Dynamic_Multimodal_Prototype_Learning_in_Vision-Language_Models_ICCV_2025_paper.html": {
    "title": "Dynamic Multimodal Prototype Learning in Vision-Language Models",
    "volume": "main",
    "abstract": "With the increasing attention to pre-trained vision-language models (VLMs), e.g., CLIP, substantial efforts have been devoted to many downstream tasks, especially in test-time adaptation (TTA). However, previous works focus on learning prototypes only in the textual modality while overlooking the ambiguous semantics in class names. These ambiguities lead to textual prototypes that are insufficient to capture visual concepts, resulting in limited performance. To address this issue, we introduce **ProtoMM**, a training-free framework that constructs multimodal prototypes to adapt VLMs during the test time. By viewing the prototype as a discrete distribution over the textual descriptions and visual particles, ProtoMM has the ability to combine the multimodal features for comprehensive prototype learning. More importantly, the visual particles are dynamically updated as the testing stream flows. This allows our multimodal prototypes to continually learn from the data, enhancing their generalizability in unseen scenarios. In addition, we quantify the importance of the prototypes and test images by formulating their semantic distance as an optimal transport problem. Extensive experiments on 15 zero-shot benchmarks demonstrate the effectiveness of our method, achieving a 1.03% average accuracy improvement over state-of-the-art methods on ImageNet and its variant datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyu Zhu",
      "Shuo Wang",
      "Beier Zhu",
      "Miaoge Li",
      "Yunfan Li",
      "Junfeng Fang",
      "Zhicai Wang",
      "Dongsheng Wang",
      "Hanwang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Pan_Liberated-GS_3D_Gaussian_Splatting_Independent_from_SfM_Point_Clouds_ICCV_2025_paper.html": {
    "title": "Liberated-GS: 3D Gaussian Splatting Independent from SfM Point Clouds",
    "volume": "main",
    "abstract": "3D Gaussian Splatting (3DGS) has demonstrated impressive performance in novel view synthesis and real-time rendering. However, it heavily relies on high-quality initial sparse points from Structure-from-Motion (SfM) which often struggles in textureless regions, degrading the geometry and visual quality of 3DGS. To address this limitation, we propose a novel initialization pipeline, achieving high-fidelity reconstruction from dense image sequences without relying on SfM-derived point clouds. Specifically, we first propose an effective depth alignment method to align the estimated monocular depth with depth rendered from an under-optimized coarse Gaussian model using an unbiased depth rasterization approach and ensemble them afterward. After that, to efficiently process dense image sequences, we incorporate a progressive segmented initialization process that to generate the initial points. Extensive experiments demonstrate the superiority of our method over previous approaches. Notably, our method outperforms the SfM-based method by a 14.4% improvement in LPIPS on the Mip-NeRF360 datasets and a 30.7% improvement on the Tanks and Temples datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weihong Pan",
      "Xiaoyu Zhang",
      "Hongjia Zhai",
      "Xiaojun Xiang",
      "Hanqing Jiang",
      "Guofeng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Guo_Client2Vec_Improving_Federated_Learning_by_Distribution_Shifts_Aware_Client_Indexing_ICCV_2025_paper.html": {
    "title": "Client2Vec: Improving Federated Learning by Distribution Shifts Aware Client Indexing",
    "volume": "main",
    "abstract": "Federated Learning (FL) is a privacy-preserving distributed machine learning paradigm. Nonetheless, the substantial distribution shifts among clients pose a considerable challenge to the performance of current FL algorithms. To mitigate this challenge, various methods have been proposed to enhance the FL training process. This paper endeavors to tackle the issue of data heterogeneity from another perspective---by improving FL algorithms prior to the actual training stage. Specifically, we introduce the Client2Vec mechanism, which generates a unique client index that contains clients' distribution shifts information for each client before the commencement of FL training. Subsequently, we leverage the generated client index to enhance the subsequent FL training process. To demonstrate the effectiveness of the proposed Client2Vec method, we conduct three case studies that assess the impact of the client index on the FL training process. These case studies encompass enhanced client sampling, model aggregation, and local training. Extensive experiments conducted on diverse datasets and model architectures show the efficacy of Client2Vec across all three case studies. Our code is available at https://github.com/LINs-lab/client2vec",
    "checked": true,
    "id": "c63fef18379494f22a153b5bcd56e78a763a14f6",
    "semantic_title": "client2vec: improving federated learning by distribution shifts aware client indexing",
    "citation_count": 0,
    "authors": [
      "Yongxin Guo",
      "Lin Wang",
      "Xiaoying Tang",
      "Tao Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_FuXi-RTM_A_Physics-Guided_Prediction_Framework_with_Radiative_Transfer_Modeling_ICCV_2025_paper.html": {
    "title": "FuXi-RTM: A Physics-Guided Prediction Framework with Radiative Transfer Modeling",
    "volume": "main",
    "abstract": "Similar to conventional video generation, current deep learning-based weather prediction frameworks often lack explicit physical constraints, leading to unphysical outputs that limit their reliability for operational forecasting. Among various physical processes requiring proper representation, radiation plays a fundamental role as it drives Earth's weather and climate systems. However, accurate simulation of radiative transfer processes remains challenging for traditional numerical weather prediction (NWP) models due to their inherent complexity and high computational costs. Here, we propose FuXi-RTM, a hybrid physics-guided deep learning framework designed to enhance weather forecast accuracy while enforcing physical consistency. FuXi-RTM integrates a primary forecasting model (FuXi) with a fixed deep learning-based radiative transfer model (DLRTM) surrogate that efficiently replaces conventional radiation parameterization schemes. This represents the first deep learning-based weather forecasting framework to explicitly incorporate physical process modeling. Evaluated over a comprehensive 5-year dataset, FuXi-RTM outperforms its unconstrained counterpart in 88.51% of 3320 variable and lead time combinations, with improvements in radiative flux predictions. By incorporating additional physical processes, FuXi-RTM paves the way for next-generation weather forecasting systems that are both accurate and physically consistent",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiusheng Huang",
      "Xiaohui Zhong",
      "Xu Fan",
      "Hao Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Enhanced_Pansharpening_via_Quaternion_Spatial-Spectral_Interactions_ICCV_2025_paper.html": {
    "title": "Enhanced Pansharpening via Quaternion Spatial-Spectral Interactions",
    "volume": "main",
    "abstract": "Pansharpening aims to generate high-resolution multispectral (MS) images by fusing panchromatic (PAN) images with corresponding low-resolution MS images. However, many existing methods struggle to fully capture spatial and spectral interactions, limiting their effectiveness. To address this, we propose a novel quaternion-based spatial-spectral interaction network that enhances pansharpening by leveraging the compact representation capabilities of quaternions for high-dimensional data. Our method consists of three key components: the quaternion global spectral interaction branch, the quaternion local spatial structure awareness branch, and the quaternion spatial-spectral interaction branch. The first applies the quaternion Fourier transform to convert multi-channel features into the frequency domain as a whole, enabling global information interaction while preserving inter-channel dependencies, which aids spectral fidelity. The second uses a customized spatial quaternion representation, combined with a window-shifting strategy, to maintain local spatial dependencies while promoting spatial interactions, which helps inject spatial details. The last integrates the two pathways within the quaternion framework to enrich spatial-spectral interactions for richer representations. By utilizing quaternion's multi-dimensional representation and parameter-sharing properties, our method achieves a more compact and efficient cross-resolution, multi-band information integration, significantly improving the quality of the fused image. Extensive experiments validate the proposed method's effectiveness and its superior performance over current SOTA techniques. The code is available at https://github.com/dongli8/QuatPanNet",
    "checked": false,
    "id": "9b2f732f1381a5ac49aad1f231c4ae906786fd81",
    "semantic_title": "stp-som: scale-transfer learning for pansharpening via estimating spectral observation model",
    "citation_count": 8,
    "authors": [
      "Dong Li",
      "Chunhui Luo",
      "Yuanfei Bao",
      "Gang Yang",
      "Jie Xiao",
      "Xueyang Fu",
      "Zheng-Jun Zha"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ji_Loss_Functions_for_Predictor-based_Neural_Architecture_Search_ICCV_2025_paper.html": {
    "title": "Loss Functions for Predictor-based Neural Architecture Search",
    "volume": "main",
    "abstract": "Evaluation is a critical but costly procedure in neural architecture search (NAS). Performance predictors have been widely adopted to reduce evaluation costs by directly estimating architecture performance. The effectiveness of predictors is heavily influenced by the choice of loss functions. While traditional predictors employ regression loss functions to evaluate the absolute accuracy of architectures, recent approaches have explored various ranking-based loss functions, such as pairwise and listwise ranking losses, to focus on the ranking of architecture performance. Despite their success in NAS, the effectiveness and characteristics of these loss functions have not been thoroughly investigated. In this paper, we conduct the first comprehensive study on loss functions in performance predictors, categorizing them into three main types: regression, ranking, and weighted loss functions. Specifically, we assess eight loss functions using a range of NAS-relevant metrics on 13 tasks across five search spaces. Our results reveal that specific categories of loss functions can be effectively combined to enhance predictor-based NAS. Furthermore, our findings could provide practical guidance for selecting appropriate loss functions for various tasks. We hope this work provides meaningful insights to guide the development of loss functions for predictor-based methods in the NAS community",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Ji",
      "Yuqi Feng",
      "Jiahao Fan",
      "Yanan Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yu_Learning_to_Generalize_without_Bias_for_Open-Vocabulary_Action_Recognition_ICCV_2025_paper.html": {
    "title": "Learning to Generalize without Bias for Open-Vocabulary Action Recognition",
    "volume": "main",
    "abstract": "Leveraging the effective visual-text alignment and static generalizability from CLIP, recent video learners adopt CLIP initialization with further regularization or recombination for generalization in open-vocabulary action recognition in-context. However, due to the static bias of CLIP, such video learners tend to overfit on shortcut static features, thereby compromising their generalizability, especially to novel out-of-context actions. To address this issue, we introduce Open-MeDe, a novel Meta-optimization framework with static Debiasing for Open-vocabulary action recognition. From a fresh perspective of generalization, Open-MeDe adopts a meta-learning approach to improve known-to-open generalizing and image-to-video debiasing in a cost-effective manner. Specifically, Open-MeDe introduces a cross-batch meta-optimization scheme that explicitly encourages video learners to quickly generalize to arbitrary subsequent data via virtual evaluation, steering a smoother optimization landscape. In effect, the free of CLIP regularization during optimization implicitly mitigates the inherent static bias of the video meta-learner. We further apply self-ensemble over the optimization trajectory to obtain generic optimal parameters that can achieve robust generalization to both in-context and out-of-context novel data. Extensive evaluations show that Open-MeDe not only surpasses state-of-the-art regularization methods tailored for in-context open-vocabulary action recognition but also substantially excels in out-of-context scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yating Yu",
      "Congqi Cao",
      "Yifan Zhang",
      "Yanning Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Dages_Metric_Convolutions_A_Unifying_Theory_to_Adaptive_Image_Convolutions_ICCV_2025_paper.html": {
    "title": "Metric Convolutions: A Unifying Theory to Adaptive Image Convolutions",
    "volume": "main",
    "abstract": "Standard convolutions are prevalent in image processing and deep learning, but their fixed kernels limits adaptability. Several deformation strategies of the reference kernel grid have been proposed. Yet, they lack a unified theoretical framework. By returning to a metric perspective for images, now seen as two-dimensional manifolds equipped with notions of local and geodesic distances, either symmetric (Riemannian) or not (Finsler), we provide a unifying principle: the kernel positions are samples of unit balls of implicit metrics. With this new perspective, we also propose metric convolutions, a novel approach that samples unit balls from explicit signal-dependent metrics, providing interpretable operators with geometric regularisation. This framework, compatible with gradient-based optimisation, can directly replace existing convolutions applied to either input images or deep features of neural networks. Metric convolutions typically require fewer parameters and provide better generalisation. Our approach shows competitive performance in standard denoising and classification tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Dagès",
      "Michael Lindenbaum",
      "Alfred M. Bruckstein"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_SDMatte_Grafting_Diffusion_Models_for_Interactive_Matting_ICCV_2025_paper.html": {
    "title": "SDMatte: Grafting Diffusion Models for Interactive Matting",
    "volume": "main",
    "abstract": "Recent interactive matting methods have demonstrated satisfactory performance in capturing the primary regions of objects, but they fall short in extracting fine-grained details in edge regions. Diffusion models trained on billions of image-text pairs, demonstrate exceptional capability in modeling highly complex data distributions and synthesizing realistic texture details, while exhibiting robust text-driven interaction capabilities, making them an attractive solution for interactive matting. To this end, we propose SDMatte, a diffusion-driven interactive matting model, with three key contributions. First, we exploit the powerful priors of the pre-trained U-Net within diffusion models and transform the text-driven interaction mechanism into a visual prompt-driven interaction mechanism to enable interactive matting. Second, we integrate coordinate embeddings of visual prompts and opacity embeddings of objects into U-Net, enhancing SDMatte's sensitivity to spatial position information and opacity information. Third, we propose a masked self-attention mechanism and a visual prompt-driven interaction mechanism that enable the model to focus on areas specified by visual prompts, leading to better performance. Extensive experiments on multiple datasets demonstrate the superior performance of our method, validating its effectiveness in interactive matting. Code will be made publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Longfei Huang",
      "Yu Liang",
      "Hao Zhang",
      "Jinwei Chen",
      "Wei Dong",
      "Lunde Chen",
      "Wanyu Liu",
      "Bo Li",
      "Peng-Tao Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Scaling_Inference-Time_Search_with_Vision_Value_Model_for_Improved_Visual_ICCV_2025_paper.html": {
    "title": "Scaling Inference-Time Search with Vision Value Model for Improved Visual Comprehension",
    "volume": "main",
    "abstract": "Despite significant advancements in vision-language models (VLMs), there lacks effective approaches to enhance response quality by scaling inference-time computation. This capability is known to be a core step towards the self-improving models in recent large language model studies. In this paper, we present Vision Value Model (VisVM) that can guide VLM inference-time search to generate responses with better visual comprehension. Specifically, VisVM not only evaluates the generated sentence quality in the current search step, but also anticipates the quality of subsequent sentences that may result from the current step, thus providing a long-term value. In this way, VisVM steers VLMs away from generating sentences prone to hallucinations or insufficient detail, thereby producing higher quality responses. Experimental results demonstrate that VisVM-guided search significantly enhances VLMs' ability to generate descriptive captions with richer visual details and fewer hallucinations, compared with greedy decoding and search methods with other visual reward signals. Furthermore, we find that self-training the model with the VisVM-guided captions improve VLM's performance across a wide range of multimodal benchmarks, indicating the potential for developing self-improving VLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiyao Wang",
      "Zhengyuan Yang",
      "Linjie Li",
      "Hongjin Lu",
      "Yuancheng Xu",
      "Chung-Ching Lin",
      "Kevin Lin",
      "Furong Huang",
      "Lijuan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Guo_Cooperative_Pseudo_Labeling_for_Unsupervised_Federated_Classification_ICCV_2025_paper.html": {
    "title": "Cooperative Pseudo Labeling for Unsupervised Federated Classification",
    "volume": "main",
    "abstract": "Unsupervised federated learning (UFL) aims to collaboratively train a global model across distributed clients without data sharing and label information. Previous UFL works have predominantly focused on representation learning and clustering tasks. Recently, vision language models (e.g., CLIP) have gained significant attention for their attractive zero-shot prediction capabilities. Leveraging this advancement, classification problems that were previously infeasible under the UFL paradigm now present new opportunities but remain largely unexplored. In this paper, we extend UFL to the classification problem with CLIP for the first time and propose a novel method, **Fed**erated **Co**operative **P**seudo **L**abeling (**FedCoPL**). Specifically, clients estimate and upload their pseudo label distribution, and the server adjusts and redistributes them to avoid global imbalance among categories. Moreover, we introduce a partial prompt aggregation protocol for effective collaboration and personalization. In particular, visual prompts containing general image features are aggregated at the server, while text prompts encoding personalized knowledge are retained locally. Extensive experiments on six datasets demonstrate the superior performance of our FedCoPL compared to baseline methods. Our code is available at https://github.com/krumpguo/FedCoPL",
    "checked": true,
    "id": "edf00a2af2aee95e521490b3ff38237e7b97a2c5",
    "semantic_title": "cooperative pseudo labeling for unsupervised federated classification",
    "citation_count": 0,
    "authors": [
      "Kuangpu Guo",
      "Lijun Sheng",
      "Yongcan Yu",
      "Jian Liang",
      "Zilei Wang",
      "Ran He"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xu_FreeDNA_Endowing_Domain_Adaptation_of_Diffusion-Based_Dense_Prediction_with_Training-Free_ICCV_2025_paper.html": {
    "title": "FreeDNA: Endowing Domain Adaptation of Diffusion-Based Dense Prediction with Training-Free Domain Noise Alignment",
    "volume": "main",
    "abstract": "Domain Adaptation (DA) for dense prediction tasks is an important topic, which enhances the dense prediction model's performance when tested on its unseen domain. Recently, with the development of Diffusion-based Dense Prediction (DDP) models, the exploration of DA designs tailored to this framework is worth exploring, since the diffusion model is effective in modeling the distribution transformation that comprises domain information. In this work, we propose a training-free mechanism for DDP frameworks, endowing them with DA capabilities. Our motivation arises from the observation that the exposure bias (e.g., noise statistics bias) in diffusion brings domain shift, and different domains in conditions of DDP models can also be effectively captured by the noise prediction statistics. Based on this, we propose a training-free Domain Noise Alignment (DNA) approach, which alleviates the variations of noise statistics to domain changes during the diffusion sampling process, thereby achieving domain adaptation. Specifically, when the source domain is available, we directly adopt the DNA method to achieve domain adaptation by aligning the noise statistics of the target domain with those of the source domain. For the more challenging source-free DA, inspired by the observation that regions closer to the source domain exhibit higher confidence meeting variations of sampling noise, we utilize the statistics from the high-confidence regions progressively to guide the noise statistic adjustment during the sampling process. Notably, our method demonstrates the effectiveness of enhancing the DA capability of DDP models across four common dense prediction tasks. Code is available at \\href https://github.com/xuhang07/FreeDNA https://github.com/xuhang07/FreeDNA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hang Xu",
      "Jie Huang",
      "Linjiang Huang",
      "Dong Li",
      "Yidi Liu",
      "Feng Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ding_DOLLAR_Few-Step_Video_Generation_via_Distillation_and_Latent_Reward_Optimization_ICCV_2025_paper.html": {
    "title": "DOLLAR: Few-Step Video Generation via Distillation and Latent Reward Optimization",
    "volume": "main",
    "abstract": "Diffusion probabilistic models have shown significant progress in video generation; however, their computational efficiency is limited by the large number of sampling steps required. Reducing sampling steps often compromises video quality or generation diversity. In this work, we introduce a distillation method that combines variational score distillation and consistency distillation to achieve few-step video generation, maintaining both high quality and diversity. We also propose a latent reward model fine-tuning approach to further enhance video generation performance according to any specified reward metric. This approach reduces memory usage and does not require the reward to be differentiable. Our method demonstrates state-of-the-art performance in few-step generation for 10-second videos (128 frames at 12 FPS). The distilled student model achieves a score of 82.57 on VBench, surpassing the teacher model as well as baseline models Gen-3, T2V-Turbo, and Kling. One-step distillation accelerates the teacher model's diffusion sampling by up to 278.6 times, enabling near real-time generation. Human evaluations further validate the superior performance of our 4-step student models compared to teacher model using 50-step DDIM sampling",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihan Ding",
      "Chi Jin",
      "Difan Liu",
      "Haitian Zheng",
      "Krishna Kumar Singh",
      "Qiang Zhang",
      "Yan Kang",
      "Zhe Lin",
      "Yuchen Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhong_Semantic_Alignment_and_Reinforcement_for_Data-Free_Quantization_of_Vision_Transformers_ICCV_2025_paper.html": {
    "title": "Semantic Alignment and Reinforcement for Data-Free Quantization of Vision Transformers",
    "volume": "main",
    "abstract": "Data-free quantization (DFQ) enables model quantization without accessing real data, addressing concerns regarding data security and privacy. With the growing adoption of Vision Transformers (ViTs), DFQ for ViTs has garnered significant attention. However, existing DFQ methods exhibit two limitations: (1) semantic distortion, where the semantics of synthetic images deviate substantially from those of real images, and (2) semantic inadequacy, where synthetic images contain extensive regions with limited content and oversimplified textures, leading to suboptimal quantization performance. To address these limitations, we propose SARDFQ, a novel Semantics Alignment and Reinforcement Data-Free Quantization method for ViTs. To address semantic distortion, SARDFQ incorporates Attention Priors Alignment (APA), which optimizes synthetic images to follow randomly generated structure attention priors. To mitigate semantic inadequacy, SARDFQ introduces Multi-Semantic Reinforcement (MSR), leveraging localized patch optimization to enhance semantic richness across synthetic images. Furthermore, SARDFQ employs Soft-Label Learning (SL), wherein multiple semantic targets are adapted to facilitate the learning of multi-semantic images augmented by MSR. Extensive experiments demonstrate the effectiveness of SARDFQ, significantly surpassing existing methods. For example, SARDFQ improves top-1 accuracy on ImageNet by 15.52% for W4A4 ViT-B",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunshan Zhong",
      "Yuyao Zhou",
      "Yuxin Zhang",
      "Wanchen Sui",
      "Shen Li",
      "Yong Li",
      "Fei Chao",
      "Rongrong Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yeo_Zero-AVSR_Zero-Shot_Audio-Visual_Speech_Recognition_with_LLMs_by_Learning_Language-Agnostic_ICCV_2025_paper.html": {
    "title": "Zero-AVSR: Zero-Shot Audio-Visual Speech Recognition with LLMs by Learning Language-Agnostic Speech Representations",
    "volume": "main",
    "abstract": "We explore a novel zero-shot Audio-Visual Speech Recognition (AVSR) framework, dubbed Zero-AVSR, which enables speech recognition in target languages without requiring any audio-visual speech data in those languages. Specifically, we introduce the Audio-Visual Speech Romanizer (AV-Romanizer), which learns language-agnostic speech representations by predicting Roman text. Then, by leveraging the strong multilingual modeling capabilities of Large Language Models (LLMs), we propose converting the predicted Roman text into language-specific graphemes, forming the proposed Cascaded Zero-AVSR. Taking it a step further, we explore a unified Zero-AVSR approach by directly integrating the audio-visual speech representations encoded by the AV-Romanizer into the LLM. This is achieved through finetuning the adapter and the LLM using our proposed multi-task learning scheme. To capture the wide spectrum of phonetic and linguistic diversity, we also introduce a Multilingual Audio-Visual Romanized Corpus (MARC) consisting of 2,916 hours of audio-visual speech data across 82 languages, along with transcriptions in both language-specific graphemes and Roman text. Extensive analysis and experiments confirm that the proposed Zero-AVSR framework has the potential to expand language support beyond the languages seen during the training of the AV-Romanizer. Code is available at https://bit.ly/zero-avsr",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeong Hun Yeo",
      "Minsu Kim",
      "Chae Won Kim",
      "Stavros Petridis",
      "Yong Man Ro"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jian_Supervised_Exploratory_Learning_for_Long-Tailed_Visual_Recognition_ICCV_2025_paper.html": {
    "title": "Supervised Exploratory Learning for Long-Tailed Visual Recognition",
    "volume": "main",
    "abstract": "Long-tailed data poses a significant challenge for deep learning models, which tend to prioritize accurate classification of head classes while largely neglecting tail classes. Existing techniques, such as class re-balancing, logit adjustment, and data augmentation, aim to enlarge decision regions of tail classes or achieve clear decision boundaries, leaving the robustness of decision regions under-considered. This paper proposes a simple yet effective Supervised Exploratory Learning (SEL) framework to achieve these goals simultaneously from space exploration perspectives. SEL employs the adaptive Optimal Foraging Algorithm (OFA) to generate diverse exploratory examples, integrating Class-biased Complement (CbC) for balanced class distribution and Fitness-weighted Sampling (FwS) for space exploration. Both theoretical analysis and empirical results demonstrate that SEL enhances class balance, sharpens decision boundaries, and strengthens decision regions. SEL is a plug-and-play training framework that can be seamlessly integrated into model training or classifier adjustment stages, making it highly adaptable and compatible with existing methods and facilitating further performance improvements. Extensive experiments on various long-tailed benchmarks demonstrate SEL's superiority",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongquan Jian",
      "Yanhao Chen",
      "Yancheng Wang",
      "Junfeng Yao",
      "Meihong Wang",
      "Qingqiang Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Enhanced_Event-based_Dense_Stereo_via_Cross-Sensor_Knowledge_Distillation_ICCV_2025_paper.html": {
    "title": "Enhanced Event-based Dense Stereo via Cross-Sensor Knowledge Distillation",
    "volume": "main",
    "abstract": "Accurate stereo matching under fast motion and extreme lighting conditions is a challenge for many vision applications. Event cameras have the advantages of low latency and high dynamic range, thus providing a reliable solution to this challenge. However, since events are sparse, this makes it an ill-posed problem to obtain dense disparity using only events. In this work, we propose a novel framework for event-based dense stereo via cross-sensor knowledge distillation. Specifically, a multi-level intensity-to-event distillation strategy is designed to maximize the potential of long-range information, local texture details, and task-related knowledge of the intensity images. Simultaneously, to enforce the cross-view consistency, an intensity-event joint left-right consistency module is proposed. With our framework, extensive dense and structural information contained in intensity images is distilled to the event branch. Therefore, retaining only the events can predict dense disparities during inference, preserving the low latency characteristics of the events. Adequate experiments conducted on the MVSEC and DSEC datasets demonstrate that our method exhibits superior stereo matching performance than baselines, both quantitatively and qualitatively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haihao Zhang",
      "Yunjian Zhang",
      "Jianing Li",
      "Lin Zhu",
      "Meng Lv",
      "Yao Zhu",
      "Yanwei Liu",
      "Xiangyang Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Disentangled_World_Models_Learning_to_Transfer_Semantic_Knowledge_from_Distracting_ICCV_2025_paper.html": {
    "title": "Disentangled World Models: Learning to Transfer Semantic Knowledge from Distracting Videos for Reinforcement Learning",
    "volume": "main",
    "abstract": "Training visual reinforcement learning (RL) in practical scenarios presents a significant challenge, i.e., RL agents suffer from low sample efficiency in environments with variations. While various approaches have attempted to alleviate this issue by disentangled representation learning, these methods usually start learning from scratch without prior knowledge of the world. This paper, in contrast, tries to learn and understand underlying semantic variations from distracting videos via offline-to-online latent distillation and flexible disentanglement constraints. To enable effective cross-domain semantic knowledge transfer, we introduce an interpretable model-based RL framework, dubbed Disentangled World Models (DisWM). Specifically, we pretrain the action-free video prediction model offline with disentanglement regularization to extract semantic knowledge from distracting videos. The disentanglement capability of the pretrained model is then transferred to the world model through latent distillation. For finetuning in the online environment, we exploit the knowledge from the pretrained model and introduce a disentanglement constraint to the world model. During the adaptation phase, the incorporation of actions and rewards from online environment interactions enriches the diversity of the data, which in turn strengthens the disentangled representation learning. Experimental results validate the superiority of our approach on various benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Wang",
      "Zhipeng Zhang",
      "Baao Xie",
      "Xin Jin",
      "Yunbo Wang",
      "Shiyu Wang",
      "Liaomo Zheng",
      "Xiaokang Yang",
      "Wenjun Zeng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kodaira_StreamDiffusion_A_Pipeline-level_Solution_for_Real-Time_Interactive_Generation_ICCV_2025_paper.html": {
    "title": "StreamDiffusion: A Pipeline-level Solution for Real-Time Interactive Generation",
    "volume": "main",
    "abstract": "We introduce StreamDiffusion, a real-time diffusion pipeline designed for streaming image generation. Existing diffusion models are adept at creating images from text or image prompts, yet they often fall short in real-time interaction. This limitation becomes particularly evident in scenarios involving continuous input, such as augmented/virtual reality, video game graphics rendering, live video streaming, and broadcasting, where high throughput is imperative. StreamDiffusion tackles this challenge through a novel pipeline-level system design. It employs unique strategies like batching the denoising process (Stream Batch), residual classifier-free guidance (R-CFG), and stochastic similarity filtering (SSF). Additionally, it seamlessly integrates advanced acceleration technologies for maximum efficiency. Specifically, Stream Batch reformulates the denoising process by eliminating the traditional wait-and-execute approach and utilizing a batching denoising approach, facilitating fluid and high-throughput streams. This results in 1.5x higher throughput compared to the conventional sequential denoising approach. R-CFG significantly addresses inefficiencies caused by repetitive computations during denoising. It optimizes the process to require minimal or no additional computations, leading to speed improvements of up to 2.05x compared to previous classifier-free methods. Besides, our stochastic similarity filtering dramatically lowers GPU activation frequency by halting computations for static image flows, achieving a remarkable reduction in computational consumption--2.39 times on an RTX 3060 GPU and 1.99 times on an RTX 4090 GPU, respectively. The synergy of our proposed strategies with established acceleration technologies enables image generation to reach speeds of up to 91.07 fps on a single RTX 4090 GPU, outperforming the throughput of AutoPipeline, developed by Diffusers, by more than 59.56x",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akio Kodaira",
      "Chenfeng Xu",
      "Toshiki Hazama",
      "Takanori Yoshimoto",
      "Kohei Ohno",
      "Shogo Mitsuhori",
      "Soichi Sugano",
      "Hanying Cho",
      "Zhijian Liu",
      "Masayoshi Tomizuka",
      "Kurt Keutzer"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Han_Toward_Better_Out-painting_Improving_the_Image_Composition_with_Initialization_Policy_ICCV_2025_paper.html": {
    "title": "Toward Better Out-painting: Improving the Image Composition with Initialization Policy Model",
    "volume": "main",
    "abstract": "With its extensive applications, Foreground Conditioned Out-painting (FCO) has attracted considerable attention in the research field. Through the utilization of text-driven FCO, users are enabled to generate diverse backgrounds for a given foreground by adjusting the text prompt, which considerably enhances the efficiency in fields like e-commerce. Since the foreground is fixed in FCO, a key concern is whether the generated background can match the foreground well to achieve a coherent composition. However, most existing methods are lacking in this regard. Artifacts and incorrect interactions are common defects in synthesized images. This issue is linked to the influence of the initial noise in the sampling process. As the initial noise is sampled independently, it's highly likely that the implied image composition will conflict with the given foreground. In this paper, a novel Initialization Policy Model (IPM) is proposed to address this problem. Its function is to replace the early denoising steps and directly predict the intermediate state that is conducive to the reasonable image composition. Since the IPM is designed to take only the foreground image and the text prompt as inputs, it isolates the impact of the initial noise. The subsequently proposed training paradigm that combines inversion-derived label supervision and diffusion reward supervision further ensures the efficient training of the IPM. The evaluation is conducted using the task-specific OpenImage-FCO dataset developed by us. The results verify that the introduction of the IPM can significantly improve the composition of the synthesized images and achieve advanced performance in the FCO task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan Han",
      "Yihao Zhao",
      "Yanhao Ge",
      "Mingyu You"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Luo_When_Large_Vision-Language_Model_Meets_Large_Remote_Sensing_Imagery_Coarse-to-Fine_ICCV_2025_paper.html": {
    "title": "When Large Vision-Language Model Meets Large Remote Sensing Imagery: Coarse-to-Fine Text-Guided Token Pruning",
    "volume": "main",
    "abstract": "Efficient vision-language understanding of large Remote Sensing Images (RSIs) is meaningful but challenging. Current Large Vision-Language Models (LVLMs) typically employ limited pre-defined grids to process images, leading to information loss when handling gigapixel RSIs. Conversely, using unlimited grids significantly increases computational costs. To preserve image details while reducing computational complexity, we propose a text-guided token pruning method with Dynamic Image Pyramid (DIP) integration. Our method introduces: (i) a Region Focus Module (RFM) that leverages text-aware region localization capability to identify critical vision tokens, and (ii) a coarse-to-fine image tile selection and vision token pruning strategy based on DIP, which is guided by RFM outputs and avoids directly processing the entire large imagery. Additionally, existing benchmarks for evaluating LVLMs' perception ability on large RSI suffer from limited question diversity and constrained image sizes. We construct a new benchmark named LRS-VQA, which contains 7,333 QA pairs across 8 categories, with image length up to 27,328 pixels. Our method outperforms existing high-resolution strategies on four datasets using the same data. Moreover, compared to existing token reduction methods, our approach demonstrates higher efficiency under high-resolution settings",
    "checked": true,
    "id": "ead64dfab4f3917a89ceb11159c36091ed286379",
    "semantic_title": "when large vision-language model meets large remote sensing imagery: coarse-to-fine text-guided token pruning",
    "citation_count": 4,
    "authors": [
      "Junwei Luo",
      "Yingying Zhang",
      "Xue Yang",
      "Kang Wu",
      "Qi Zhu",
      "Lei Liang",
      "Jingdong Chen",
      "Yansheng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ding_StreamMind_Unlocking_Full_Frame_Rate_Streaming_Video_Dialogue_through_Event-Gated_ICCV_2025_paper.html": {
    "title": "StreamMind: Unlocking Full Frame Rate Streaming Video Dialogue through Event-Gated Cognition",
    "volume": "main",
    "abstract": "With the rise of real-world human-AI interaction applications, such as AI assistants, the need for Streaming Video Dialogue is critical. To address this need, we introduce StreamMind, a video LLM framework that achieves ultra-FPS streaming video processing (100 fps on a single A100) and enables proactive, always-on responses in real time, without explicit user intervention. To solve the key challenge of the contradiction between linear video streaming speed and quadratic transformer computation cost, we propose a novel perception-cognition interleaving paradigm named \"event-gated LLM invocation\", in contrast to the existing per-time-step LLM invocation. By introducing a Cognition Gate network between the video encoder and the LLM, LLM is only invoked when relevant events occur. To realize the event feature extraction with constant cost, we propose Event-Preserving Feature Extractor (EPFE) based on state-space method, generating a single perception token for spatiotemporal features. These techniques enable the video LLM with full-FPS perception and real-time cognition response. Experiments on Ego4D and SoccerNet streaming tasks, as well as standard offline benchmarks, demonstrate state-of-the-art performance in both model capability and real-time efficiency, paving the way for ultra-high-FPS applications, such as Game AI Copilot and interactive media",
    "checked": true,
    "id": "226f859f2470e83b63e4deb38dd6a1e4e5d5264b",
    "semantic_title": "streammind: unlocking full frame rate streaming video dialogue through event-gated cognition",
    "citation_count": 5,
    "authors": [
      "Xin Ding",
      "Hao Wu",
      "Yifan Yang",
      "Shiqi Jiang",
      "Qianxi Zhang",
      "Donglin Bai",
      "Zhibo Chen",
      "Ting Cao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_DecAD_Decoupling_Anomalies_in_Latent_Space_for_Multi-Class_Unsupervised_Anomaly_ICCV_2025_paper.html": {
    "title": "DecAD: Decoupling Anomalies in Latent Space for Multi-Class Unsupervised Anomaly Detection",
    "volume": "main",
    "abstract": "Existing distillation-based and reconstruction-based methods have a critical limitation: Autoencoder-based frameworks trained exclusively on normal samples unexpectedly well reconstruct abnormal features, leadingto degraded detection performance. We identify this phenomenon as 'anomaly leakage' (AL): the decoder optimized by reconstruction loss tends to directly copy the encoded input, regardless of whether the input is a normal or abnormal feature. To address this issue, we propose a novel framework that explicitly decouples encoded features into normal and abnormal components through a special invertible mapping in a prior latent space. Next, we remove abnormal components and leverage normal remainders for feature reconstruction. Compared to previous methods, the invertible structure can eliminate anomalous information point-to-point without damaging the information of neighboring patches, improving reconstruction. In this process, effective synthetic abnormal features are essential for training the decoupling process. Therefore, we propose applying adversarial training to find suitable perturbations to simulate feature-level anomalies. Extensive experimental evaluations on benchmark datasets, including MVTec AD, VisA, and Real-IAD, demonstrate that our method achieves competitive performance compared to state-of-the-art approaches. Code is available at: DecAD",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaolei Wang",
      "Xiaoyang Wang",
      "Huihui Bai",
      "Eng Gee Lim",
      "Jimin Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Dunlap_Discovering_Divergent_Representations_between_Text-to-Image_Models_ICCV_2025_paper.html": {
    "title": "Discovering Divergent Representations between Text-to-Image Models",
    "volume": "main",
    "abstract": "In this paper, we investigate when and how visual representations learned by two different generative models diverge from each other. Specifically, given two text-to-image models, our goal is to discover visual attributes that appear in images generated by one model but not the other, along with the types of prompts that trigger these attribute differences. For example, 'flames' might appear in one model's outputs when given prompts expressing strong emotions, while the other model does not produce this attribute given the same prompts. We introduce CompCon (Comparing Concepts), an evolutionary search algorithm that discovers visual attributes more prevalent in one model's output than the other, and uncovers the prompt concepts linked to these visual differences. To evaluate CompCon's ability to find diverging representations, we create an automated data generation pipeline to produce ID^2, a dataset of 60 input-dependent differences, and compare our approach to several LLM- and VLM-powered baselines. Finally, we use CompCon to compare popular text-to-image models, finding divergent representations such as how PixArt depicts prompts mentioning loneliness with wet streets and Stable Diffusion 3.5 depicts African American people in media professions",
    "checked": true,
    "id": "91a3df8a78dc37e12e8df0978f3a57854fcad175",
    "semantic_title": "discovering divergent representations between text-to-image models",
    "citation_count": 0,
    "authors": [
      "Lisa Dunlap",
      "Joseph E. Gonzalez",
      "Trevor Darrell",
      "Fabian Caba Heilbron",
      "Josef Sivic",
      "Bryan Russell"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hofer_Kaputt_A_Large-Scale_Dataset_for_Visual_Defect_Detection_ICCV_2025_paper.html": {
    "title": "Kaputt: A Large-Scale Dataset for Visual Defect Detection",
    "volume": "main",
    "abstract": "We present a novel large-scale dataset for defect detection in a logistics setting. Recent work on industrial anomaly detection has primarily focused on manufacturing scenarios with highly controlled poses and a limited number of object categories. Existing benchmarks like MVTec-AD (Bergmann et al., 2021) and VisA (Zou et al., 2022) have reached saturation, with state-of-the-art methods achieving up to 99.9% AUROC scores. In contrast to manufacturing, anomaly detection in retail logistics faces new challenges, particularly in the diversity and variability of object pose and appearance. Leading anomaly detection methods fall short when applied to this new setting. To bridge this gap, we introduce a new benchmark that overcomes the current limitations of existing datasets. With over 230,000 images (and more than 29,000 defective instances), it is 40 times larger than MVTec and contains more than 48,000 distinct objects. To validate the difficulty of the problem, we conduct an extensive evaluation of multiple state-of-the-art anomaly detection methods, demonstrating that they do not surpass 56.96% AUROC on our dataset. Further qualitative analysis confirms that existing methods struggle to leverage normal samples under heavy pose and appearance variation. With our large-scale dataset, we set a new benchmark and encourage future research towards solving this challenging problem in retail logistics anomaly detection. The dataset is available for download under https://www.kaputt-dataset.com",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Höfer",
      "Dorian F. Henning",
      "Artemij Amiranashvili",
      "Douglas Morrison",
      "Mariliza Tzes",
      "Ingmar Posner",
      "Marc Matvienko",
      "Alessandro Rennola",
      "Anton Milan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhao_Self-Ensembling_Gaussian_Splatting_for_Few-Shot_Novel_View_Synthesis_ICCV_2025_paper.html": {
    "title": "Self-Ensembling Gaussian Splatting for Few-Shot Novel View Synthesis",
    "volume": "main",
    "abstract": "3D Gaussian Splatting (3DGS) has demonstrated remarkable effectiveness in novel view synthesis (NVS). However, 3DGS tends to overfit when trained with sparse views, limiting its generalization to novel viewpoints. In this paper, we address this overfitting issue by introducing Self-Ensembling Gaussian Splatting (SE-GS). We achieve self-ensembling by incorporating an uncertainty-aware perturbation strategy during training. A \\Delta-model and a \\Sigma-model are jointly trained on the available images. The \\Delta-model is dynamically perturbed based on rendering uncertainty across training steps, generating diverse perturbed models with negligible computational overhead. Discrepancies between the \\Sigma-model and these perturbed models are minimized throughout training, forming a robust ensemble of 3DGS models. This ensemble, represented by the \\Sigma-model, is then used to generate novel-view images during inference. Experimental results on the LLFF, Mip-NeRF360, DTU, and MVImgNet datasets demonstrate that our approach enhances NVS quality under few-shot training conditions, outperforming existing state-of-the-art methods. The code is released at: https://sailor-z.github.io/projects/SEGS.html",
    "checked": true,
    "id": "9bb8fdf5baf8fb4cf9d9870808de90e457ce8122",
    "semantic_title": "self-ensembling gaussian splatting for few-shot novel view synthesis",
    "citation_count": 1,
    "authors": [
      "Chen Zhao",
      "Xuan Wang",
      "Tong Zhang",
      "Saqib Javed",
      "Mathieu Salzmann"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Esteves_Spectral_Image_Tokenizer_ICCV_2025_paper.html": {
    "title": "Spectral Image Tokenizer",
    "volume": "main",
    "abstract": "Image tokenizers map images to sequences of discrete tokens, and are a crucial component of autoregressive transformer-based image generation. The tokens are typically associated with spatial locations in the input image, arranged in raster scan order, which is not ideal for autoregressive modeling. In this paper, we propose to tokenize the image spectrum instead, obtained from a discrete wavelet transform (DWT), such that the sequence of tokens represents the image in a coarse-to-fine fashion. Our tokenizer brings several advantages: 1) it leverages that natural images are more compressible at high frequencies, 2) it can take and reconstruct images of different resolutions without retraining, 3) it improves the conditioning for next-token prediction -- instead of conditioning on a partial line-by-line reconstruction of the image, it takes a coarse reconstruction of the full image, 4) it enables partial decoding where the first few generated tokens can reconstruct a coarse version of the image, 5) it enables autoregressive models to be used for image upsampling. We evaluate the tokenizer reconstruction metrics as well as multiscale image generation, text-guided image upsampling and editing",
    "checked": true,
    "id": "e3844668447a247b2447f681878ebed9765249e3",
    "semantic_title": "spectral image tokenizer",
    "citation_count": 9,
    "authors": [
      "Carlos Esteves",
      "Mohammed Suhail",
      "Ameesh Makadia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yu_GameFactory_Creating_New_Games_with_Generative_Interactive_Videos_ICCV_2025_paper.html": {
    "title": "GameFactory: Creating New Games with Generative Interactive Videos",
    "volume": "main",
    "abstract": "Generative videos have the potential to revolutionize game development by autonomously creating new content. In this paper, we present GameFactory, a framework for action-controlled scene-generalizable game video generation. We first address the fundamental challenge of action controllability by introducing GF-Minecraft, an action-annotated game video dataset without human bias, and developing an action control module that enables precise control over both keyboard and mouse inputs. We further extend to support autoregressive generation for unlimited-length interactive videos.More importantly, GameFactory tackles the critical challenge of scene-generalizable action control, which most existing methods fail to address. To enable the creation of entirely new and diverse games beyond fixed styles and scenes, we leverage the open-domain generative priors from pre-trained video diffusion models. To bridge the domain gap between open-domain priors and small-scale game datasets, we propose a multi-phase training strategy with a domain adapter that decouples game style learning from action control. This decoupling ensures that action control learning is no longer bound to specific game styles, thereby achieving scene-generalizable action control. Experimental results demonstrate that GameFactory effectively generates open-domain action-controllable game videos, representing a significant step forward in AI-driven game generation",
    "checked": true,
    "id": "892c3ab8feadad3bd7e76032befe923e521fe964",
    "semantic_title": "gamefactory: creating new games with generative interactive videos",
    "citation_count": 40,
    "authors": [
      "Jiwen Yu",
      "Yiran Qin",
      "Xintao Wang",
      "Pengfei Wan",
      "Di Zhang",
      "Xihui Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Leng_Occupancy_Learning_with_Spatiotemporal_Memory_ICCV_2025_paper.html": {
    "title": "Occupancy Learning with Spatiotemporal Memory",
    "volume": "main",
    "abstract": "3D occupancy becomes a promising perception representation for autonomous driving to model the surrounding environment at a fine-grained scale. However, it remains challenging to efficiently aggregate 3D occupancy over time across multiple input frames due to the high processing cost and the uncertainty and dynamics of voxels. To address this issue, we propose ST-Occ, a scene-level occupancy representation learning framework that effectively learns the spatiotemporal feature with temporal consistency. ST-Occ consists of two core designs: a spatiotemporal memory that captures comprehensive historical information and stores it efficiently through a scene-level representation, and a memory attention that conditions the current occupancy representation on the spatiotemporal memory with a model of uncertainty and dynamic awareness. Our method significantly enhances the spatiotemporal representation learned for 3D occupancy prediction tasks by exploiting the temporal dependency between multi-frame inputs. Experiments show that our approach outperforms the state-of-the-art methods by a margin of 3 mIoU and reduces the temporal inconsistency by 29%. The code and model are available at https://github.com/matthew-leng/ST-Occ",
    "checked": true,
    "id": "2cbe91b5c6c31d67b7ae1e4b6e2768e88982bc62",
    "semantic_title": "occupancy learning with spatiotemporal memory",
    "citation_count": 1,
    "authors": [
      "Ziyang Leng",
      "Jiawei Yang",
      "Wenlong Yi",
      "Bolei Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hegde_Understanding_Co-speech_Gestures_in-the-wild_ICCV_2025_paper.html": {
    "title": "Understanding Co-speech Gestures in-the-wild",
    "volume": "main",
    "abstract": "Co-speech gestures play a vital role in non-verbal communication. In this paper, we introduce a new framework for co-speech gesture understanding in the wild. Specifically, we propose three new tasks and benchmarks to evaluate a model's capability to comprehend gesture-speech-text associations: (i) gesture based retrieval, (ii) gestured word spotting, and (iii) active speaker detection using gestures. We present a new approach that learns a tri-modal video-gesture-speech-text representation to solve these tasks. By leveraging a combination of global phrase contrastive loss and local gesture-word coupling loss, we demonstrate that a strong gesture representation can be learned in a weakly supervised manner from videos in the wild. Our learned representations outperform previous methods, including large vision-language models (VLMs). Further analysis reveals that speech and text modalities capture distinct gesture related signals, underscoring the advantages of learning a shared tri-modal embedding space. The dataset, model, and code are available at: https://www.robots.ox.ac.uk/ vgg/research/jegal",
    "checked": true,
    "id": "eb9b4e55e2f09723e627745c476952722ac1bce4",
    "semantic_title": "understanding co-speech gestures in-the-wild",
    "citation_count": 0,
    "authors": [
      "Sindhu B Hegde",
      "K R Prajwal",
      "Taein Kwon",
      "Andrew Zisserman"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Rukhovich_CAD-Recode_Reverse_Engineering_CAD_Code_from_Point_Clouds_ICCV_2025_paper.html": {
    "title": "CAD-Recode: Reverse Engineering CAD Code from Point Clouds",
    "volume": "main",
    "abstract": "Computer-Aided Design (CAD) models are typically constructed by sequentially drawing parametric sketches and applying CAD operations to obtain a 3D model. The problem of 3D CAD reverse engineering consists of reconstructing the sketch and CAD operation sequences from 3D representations such as point clouds. In this paper, we address this challenge through novel contributions across three levels: CAD sequence representation, network design, and training dataset. In particular, we represent CAD sketch-extrude sequences as Python code. The proposed CAD-Recode translates a point cloud into Python code that, when executed, reconstructs the CAD model. Taking advantage of the exposure of pre-trained Large Language Models (LLMs) to Python code, we leverage a relatively small LLM as a decoder for CAD-Recode and combine it with a lightweight point cloud projector. CAD-Recode is trained on a procedurally generated dataset of one million CAD sequences. CAD-Recode significantly outperforms existing methods across the DeepCAD, Fusion360 and real-world CC3D datasets. Furthermore, we show that our CAD Python code output is interpretable by off-the-shelf LLMs, enabling CAD editing and CAD-specific question answering from point clouds",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Danila Rukhovich",
      "Elona Dupont",
      "Dimitrios Mallis",
      "Kseniya Cherenkova",
      "Anis Kacem",
      "Djamila Aouada"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Fu_ObjectRelator_Enabling_Cross-View_Object_Relation_Understanding_Across_Ego-Centric_and_Exo-Centric_ICCV_2025_paper.html": {
    "title": "ObjectRelator: Enabling Cross-View Object Relation Understanding Across Ego-Centric and Exo-Centric Perspectives",
    "volume": "main",
    "abstract": "Bridging the gap between ego-centric and exo-centric views has been a long-standing question in computer vision. In this paper, we focus on the emerging Ego-Exo object correspondence task, which aims to understand object relations across ego-exo perspectives through segmentation. While numerous segmentation models have been proposed, most operate on a single image (view), making them impractical for cross-view scenarios. PSALM, a recently proposed segmentation method, stands out as a notable exception with its demonstrated zero-shot ability on this task. However, due to the drastic viewpoint change between ego and exo, PSALM fails to accurately locate and segment objects, especially in complex backgrounds or when object appearances change significantly. To address these issues, we propose ObjectRelator, a novel approach featuring two key modules: Multimodal Condition Fusion (MCFuse) and SSL-based Cross-View Object Alignment (XObjAlign). MCFuse introduces language as an additional cue, integrating both visual masks and textual descriptions to improve object localization and prevent incorrect associations. XObjAlign enforces cross-view consistency through self-supervised alignment, enhancing robustness to object appearance variations. Extensive experiments demonstrate ObjectRelator's effectiveness on the large-scale Ego-Exo4D benchmark and HANDAL-X (an adapted dataset for cross-view segmentation) with state-of-the-art performance. Code is available at: http://yuqianfu.com/ObjectRelator",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuqian Fu",
      "Runze Wang",
      "Bin Ren",
      "Guolei Sun",
      "Biao Gong",
      "Yanwei Fu",
      "Danda Pani Paudel",
      "Xuanjing Huang",
      "Luc Van Gool"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhao_Tune-Your-Style_Intensity-tunable_3D_Style_Transfer_with_Gaussian_Splatting_ICCV_2025_paper.html": {
    "title": "Tune-Your-Style: Intensity-tunable 3D Style Transfer with Gaussian Splatting",
    "volume": "main",
    "abstract": "3D style transfer refers to the artistic stylization of 3D assets based on reference style images. Recently, 3DGS-based stylization methods have drawn considerable attention, primarily due to their markedly enhanced training and rendering speeds. However, a vital challenge for 3D style transfer is to strike a balance between the content and the patterns and colors of the style. Although the existing methods strive to achieve relatively balanced outcomes, the fixed-output paradigm struggles to adapt to the diverse content-style balance requirements from different users. In this work, we introduce a creative intensity-tunable 3D style transfer paradigm, dubbed Tune-Your-Style, which allows users to flexibly adjust the style intensity injected into the scene to match their desired content-style balance, thus enhancing the customizability of 3D style transfer. To achieve this goal, we first introduce Gaussian neurons to explicitly model the style intensity and parameterize a learnable style tuner to achieve intensity-tunable style injection. To facilitate the learning of tunable stylization, we further propose the tunable stylization guidance, which obtains multi-view consistent stylized views from diffusion models through cross-view style alignment, and then employs a two-stage optimization strategy to provide stable and efficient guidance by modulating the balance between full-style guidance from the stylized views and zero-style guidance from the initial rendering. Extensive experiments demonstrate that our method not only delivers visually appealing results, but also exhibits flexible customizability for 3D style transfer. Project page is available at https://zhao-yian.github.io/TuneStyle",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yian Zhao",
      "Rushi Ye",
      "Ruochong Zheng",
      "Zesen Cheng",
      "Chaoran Feng",
      "Jiashu Yang",
      "Pengchong Qiao",
      "Chang Liu",
      "Jie Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Peng_Stylized-Face_A_Million-level_Stylized_Face_Dataset_for_Face_Recognition_ICCV_2025_paper.html": {
    "title": "Stylized-Face: A Million-level Stylized Face Dataset for Face Recognition",
    "volume": "main",
    "abstract": "Stylized face recognition is the task of recognizing generated faces with the same ID across diverse stylistic domains (e.g., anime, painting, cyberpunk styles). This emerging field plays a vital role in the governance of generative image, serving the primary objective: Recognize the ID information of stylized faces to detect potential infringements of portrait rights. Despite its importance, progress in stylized face recognition has been hindered by the lack of large-scale, stylistically diverse datasets. To address this gap, we introduce the Stylized-Face dataset, which is the first dataset specifically designed for stylized face recognition. Stylized-Face dataset includes 4.6 million images across 62k IDs, specifically curated to enhance model performance in stylized face recognition tasks. To ensure data quality (i.e., ID preservation) at this massive scale, we implement a semi-automated pipeline for large-scale data cleaning. Based on the Stylized-Face dataset, we establish three benchmarks to evaluate the robustness and generalization of recognition models across various scenarios, including within-distribution performance, cross-prompt generalization, and cross-method generalization, which target key challenges in stylized face recognition. Experimental results demonstrate that models trained on Stylized-Face achieve remarkable improvements in both stylized face recognition performance (a 15.9% improvement in TAR at FAR=1e-4) and generalization (a 13.3% improvement in TAR at FAR=1e-3 in cross-method generalization)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengyuan Peng",
      "Jianqing Xu",
      "Yuge Huang",
      "Jinkun Hao",
      "Shouhong Ding",
      "Zhizhong Zhang",
      "Xin Tan",
      "Lizhuang Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Akada_Bring_Your_Rear_Cameras_for_Egocentric_3D_Human_Pose_Estimation_ICCV_2025_paper.html": {
    "title": "Bring Your Rear Cameras for Egocentric 3D Human Pose Estimation",
    "volume": "main",
    "abstract": "Egocentric 3D human pose estimation has been actively studied using cameras installed in front of a head-mounted device (HMD). While frontal placement is the optimal and the only option for some tasks, such as hand tracking, it remains unclear if the same holds for full-body tracking due to self-occlusion and limited field-of-view coverage. Notably, even the state-of-the-art methods often fail to estimate accurate 3D poses in many scenarios, such as when HMD users tilt their heads upward---a common motion in human activities. A key limitation of existing HMD designs is their neglect of the back of the body, despite its potential to provide crucial 3D reconstruction cues. Hence, this paper investigates the usefulness of rear cameras for full-body tracking. We also show that simply adding rear views to the frontal inputs is not optimal for existing methods due to their dependence on individual 2D joint detectors without effective multi-view integration. To address this issue, we propose a new transformer-based method that refines 2D joint heatmap estimation with multi-view information and heatmap uncertainty, thereby improving 3D pose tracking. Also, we introduce two new large-scale datasets, Ego4View-Syn and Ego4View-RW, for a rear-view evaluation. Our experiments show that the new camera configurations with back views provide superior support for 3D pose tracking compared to only frontal placements. The proposed method achieves significant improvement over the current state of the art (>10% on MPJPE). The source code, trained models, and datasets are available on our project page",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hiroyasu Akada",
      "Jian Wang",
      "Vladislav Golyanik",
      "Christian Theobalt"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Bagchi_ReferEverything_Towards_Segmenting_Everything_We_Can_Speak_of_in_Videos_ICCV_2025_paper.html": {
    "title": "ReferEverything: Towards Segmenting Everything We Can Speak of in Videos",
    "volume": "main",
    "abstract": "We present REM, a framework for segmenting a wide range of concepts in video that can be described through natural language. Our method leverages the universal visual-language mapping learned by video diffusion models on Internet-scale data by fine-tuning them on small-scale Referring Object Segmentation datasets. Our key insight is to preserve the entirety of the generative model's architecture by shifting its objective from predicting noise to predicting mask latents. The resulting model can accurately segment rare and unseen objects, despite only being trained on a limited set of categories. Additionally, it can effortlessly generalize to non-object dynamic concepts, such as smoke or raindrops, as demonstrated in our new benchmark for Referring Video Process Segmentation (Ref-VPS). REM performs on par with the state-of-the-art on in-domain datasets, like Ref-DAVIS, while outperforming them by up to 12 IoU points out-of-domain, leveraging the power of generative pre-training. We also show that advancements in video generation directly improve segmentation",
    "checked": true,
    "id": "d0b386bcb903866f10e7593007ecb19a621bb1b3",
    "semantic_title": "refereverything: towards segmenting everything we can speak of in videos",
    "citation_count": 2,
    "authors": [
      "Anurag Bagchi",
      "Zhipeng Bao",
      "Yu-Xiong Wang",
      "Pavel Tokmakov",
      "Martial Hebert"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_GestureHYDRA_Semantic_Co-speech_Gesture_Synthesis_via_Hybrid_Modality_Diffusion_Transformer_ICCV_2025_paper.html": {
    "title": "GestureHYDRA: Semantic Co-speech Gesture Synthesis via Hybrid Modality Diffusion Transformer and Cascaded-Synchronized Retrieval-Augmented Generation",
    "volume": "main",
    "abstract": "While increasing attention has been paid to co-speech gesture synthesis, most previous works neglect to investigate hand gestures with explicit and essential semantics. In this paper, we study co-speech gesture generation with an emphasis on specific hand gesture activation, which can deliver more instructional information than common body movements. To achieve this, we first build a high-quality dataset of 3D human body movements including a set of semantically explicit hand gestures that are commonly used by live streamers. Then we present a hybrid-modality gesture generation system GestureHYDRA built upon a hybrid-modality diffusion transformer architecture with novelly designed motion-style injective transformer layers, which enables advanced gesture modeling ability and versatile gesture operations. To guarantee these specific hand gestures can be activated, we introduce a cascaded retrieval-augmented generation strategy built upon a semantic gesture repository annotated for each subject and an adaptive audio-gesture synchronization mechanism, which substantially improves semantic gesture activation and production efficiency. Quantitative and qualitative experiments demonstrate that our proposed approach achieves superior performance over all the counterparts",
    "checked": true,
    "id": "5c88337909059829e45f5592fc17b4a190987965",
    "semantic_title": "gesturehydra: semantic co-speech gesture synthesis via hybrid modality diffusion transformer and cascaded-synchronized retrieval-augmented generation",
    "citation_count": 0,
    "authors": [
      "Quanwei Yang",
      "Luying  Huang",
      "Kaisiyuan Wang",
      "Jiazhi Guan",
      "Shengyi He",
      "Fengguo Li",
      "Hang Zhou",
      "Lingyun Yu",
      "Yingying Li",
      "Haocheng Feng",
      "Hongtao Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Make_Your_Training_Flexible_Towards_Deployment-Efficient_Video_Models_ICCV_2025_paper.html": {
    "title": "Make Your Training Flexible: Towards Deployment-Efficient Video Models",
    "volume": "main",
    "abstract": "Current video training methods rely on fixed spatiotemporal sampling grids to extract a predetermined number of tokens, limiting adaptability to diverse computational budgets and resulting in suboptimal accuracy-computation trade-offs. This rigidity constrains high-performance models trained in resource-rich environments from being efficiently deployed on resource-constrained devices. We hence introduce a novel paradigm for lossless adaptation across scenarios, enabling models to maintain optimal performance under high-resource conditions while seamlessly transferring to low-resource environments. Central to this is Token Optimization (TO), an adaptive inference framework that dynamically samples and selects input token set to optimize input information under varied computational constraints. To support this, we propose Flux, an augmentation tool that enables flexible sampling grids and token selection. It integrates seamlessly into popular video training frameworks, significantly enhancing model robustness and adaptability with negligible additional cost. Applied to large-scale video pretraining, our method produces FluxViT, which achieves state-of-the-art performance across multiple tasks under standard costs. Remarkably, with only 1/4 of the tokens, FluxViT matches prior state-of-the-art models under TO across tasks, achieving nearly 90% computational savings. Code and models will be available at https://github.com/OpenGVLab/FluxViT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenting Wang",
      "Kunchang Li",
      "Tianxiang Jiang",
      "Xiangyu Zeng",
      "Yi Wang",
      "Limin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Guo_Federated_Continual_Instruction_Tuning_ICCV_2025_paper.html": {
    "title": "Federated Continual Instruction Tuning",
    "volume": "main",
    "abstract": "A vast amount of instruction tuning data is crucial for the impressive performance of Large Multimodal Models (LMMs), but the associated computational costs and data collection demands during supervised fine-tuning make it impractical for most researchers. Federated learning (FL) has the potential to leverage all distributed data and training resources to reduce the overhead of joint training. However, most existing methods assume a fixed number of tasks, while in real-world scenarios, clients continuously encounter new knowledge and often struggle to retain old tasks due to memory constraints. In this work, we introduce the Federated Continual Instruction Tuning (FCIT) benchmark to model this real-world challenge. Our benchmark includes two realistic scenarios, encompassing four different settings and twelve carefully curated instruction tuning datasets. To address the challenges posed by FCIT, we propose dynamic knowledge organization to effectively integrate updates from different tasks during training and subspace selective activation to allocate task-specific output during inference. Extensive experimental results demonstrate that our proposed method significantly enhances model performance across varying levels of data heterogeneity and catastrophic forgetting. Code and dataset are released at https://github.com/Ghy0501/FCIT",
    "checked": true,
    "id": "90e9addbb373a654abf25a68068f10d2cfe1d435",
    "semantic_title": "federated continual instruction tuning",
    "citation_count": 5,
    "authors": [
      "Haiyang Guo",
      "Fanhu Zeng",
      "Fei Zhu",
      "Wenzhuo Liu",
      "Da-Han Wang",
      "Jian Xu",
      "Xu-Yao Zhang",
      "Cheng-Lin Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ran_HUST_High-Fidelity_Unbiased_Skin_Tone_Estimation_via_Texture_Quantization_ICCV_2025_paper.html": {
    "title": "HUST: High-Fidelity Unbiased Skin Tone Estimation via Texture Quantization",
    "volume": "main",
    "abstract": "Recent 3D facial reconstruction methods have made significant progress in shape estimation, but high-fidelity unbiased facial albedo estimation remains challenging. Existing methods rely on expensive light-stage captured data, and while they have made progress in either high-fidelity reconstruction or unbiased skin tone estimation, no work has yet achieved optimal results in both aspects simultaneously. In this paper, we present a novel high-fidelity unbiased facial diffuse albedo reconstruction method, HUST, which recovers the diffuse albedo map directly from a single image without the need for captured data. Our key insight is that the albedo map is the illumination-invariant texture map, which enables us to use inexpensive texture data for diffuse albedo estimation by eliminating illumination. To achieve this, we collect large-scale high-resolution facial images and train a VQGAN model in the image space. To adapt the pre-trained VQGAN model for UV texture generation, we fine-tune the encoder by using limited UV textures and our high-resolution faces under adversarial supervision in both image and latent space. Finally, we train a cross-attention module and utilize group identity loss for the domain adaptation from texture to albedo. Extensive experiments demonstrate that HUST can predict high-fidelity facial albedos for in-the-wild images. On the FAIR benchmark, HUST achieves the lowest average ITA error (11.20) and bias score (1.58), demonstrating superior accuracy and robust fairness across the entire spectrum of human skin tones. Our code, models, and training data will be made publicly available to facilitate future research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zimin Ran",
      "Xingyu Ren",
      "Xiang An",
      "Kaicheng Yang",
      "Ziyong Feng",
      "Jing Yang",
      "Rolandos Alexandros Potamias",
      "Linchao Zhu",
      "Jiankang Deng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yuan_DLFR-Gen_Diffusion-based_Video_Generation_with_Dynamic_Latent_Frame_Rate_ICCV_2025_paper.html": {
    "title": "DLFR-Gen: Diffusion-based Video Generation with Dynamic Latent Frame Rate",
    "volume": "main",
    "abstract": "Diffusion Transformer (DiT)-based generation models have achieved remarkable success in video generation. However, their inherent computational demands pose significant efficiency challenges. In this paper, we exploit the inherent temporal non-uniformity of real-world videos, and observe that videos exhibit dynamic information density, with high-motion segments demanding greater detail preservation than static scenes. Inspired by this temporal non-uniformity, we propose DLFR-Gen, a training-free approach for Diffusion-based Video Generation with Dynamic Latent Frame Rate. DLFR-Gen adaptively adjusts the number of elements in latent space based on the motion frequency of the latent space content, using fewer tokens for low-frequency segments while preserving detail in high-frequency segments. Specifically, our key contributions are: (1) A dynamic frame rate scheduler for DiT video generation that adaptively assigns frame rates for video segments. (2) A novel latent-space frame merging method to align latent representations with their denoised counterparts before merging those redundant in low-resolution space. (3) A preference analysis of Rotary Positional Embeddings (RoPE) across DiT layers, informing a tailored RoPE strategy optimized for semantic and local information capture. Experiments show that DLFR-Gen can achieve a speedup up to 3x for video generation with minimal quality degradation",
    "checked": false,
    "id": "0e57a1e6f02d76dde80243adf22f8d128c28ff2b",
    "semantic_title": "vgdfr: diffusion-based video generation with dynamic latent frame rate",
    "citation_count": 0,
    "authors": [
      "Zhihang Yuan",
      "Rui Xie",
      "Yuzhang Shang",
      "Hanling Zhang",
      "Siyuan Wang",
      "Shengen Yan",
      "Guohao Dai",
      "Yu Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Thomas_Whats_in_a_Latent_Leveraging_Diffusion_Latent_Space_for_Domain_ICCV_2025_paper.html": {
    "title": "What's in a Latent? Leveraging Diffusion Latent Space for Domain Generalization",
    "volume": "main",
    "abstract": "Domain Generalization aims to develop models that can generalize to novel and unseen data distributions. In this work, we study how model architectures and pre-training objectives impact feature richness and propose a method to effectively leverage them for domain generalization. Specifically, given a pre-trained feature space, we first discover latent domain structures, referred to as pseudo-domains, that capture domain-specific variations in an unsupervised manner. Next, we augment existing classifiers with these complementary pseudo-domain representations making them more amenable to diverse unseen test domains. We analyze how different pre-training feature spaces differ in the domain-specific variances they capture. Our empirical studies reveal that features from diffusion models excel at separating domains in the absence of explicit domain labels and capture nuanced domain-specific information. On 5 datasets, we show that our very simple framework improves generalization to unseen domains by a maximum test accuracy improvement of over 4% compared to the standard baseline Empirical Risk Minimization (ERM). Crucially, our method outperforms most algorithms that access domain labels during training. Code is available at: https://xthomasbu.github.io/GUIDE/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xavier Thomas",
      "Deepti Ghadiyaram"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Song_DiffSim_Taming_Diffusion_Models_for_Evaluating_Visual_Similarity_ICCV_2025_paper.html": {
    "title": "DiffSim: Taming Diffusion Models for Evaluating Visual Similarity",
    "volume": "main",
    "abstract": "Diffusion models have fundamentally transformed the field of generative models, making the assessment of similarity between customized model outputs and reference inputs critically important. However, traditional perceptual similarity metrics operate primarily at the pixel and patch levels, comparing low-level colors and textures but failing to capture mid-level similarities and differences in image layout, object pose, and semantic content. Contrastive learning-based CLIP and self-supervised learning-based DINO are often used to measure semantic similarity, but they highly compress image features, inadequately assessing appearance details. This paper is the first to discover that pretrained diffusion models can be utilized for measuring visual similarity and introduces the DiffSim method, addressing the limitations of traditional metrics in capturing perceptual consistency in custom generation tasks. By aligning features in the attention layers of the denoising U-Net, DiffSim evaluates both appearance and style similarity, showing superior alignment with human visual preferences. Additionally, we introduce the Sref and IP benchmarks to evaluate visual similarity at the level of style and instance, respectively. Comprehensive evaluations across multiple benchmarks demonstrate that DiffSim achieves state-of-the-art performance, providing a robust tool for measuring visual coherence in generative models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiren Song",
      "Xiaokang Liu",
      "Mike Zheng Shou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cho_Controllable_Feature_Whitening_for_Hyperparameter-Free_Bias_Mitigation_ICCV_2025_paper.html": {
    "title": "Controllable Feature Whitening for Hyperparameter-Free Bias Mitigation",
    "volume": "main",
    "abstract": "As the use of artificial intelligence rapidly increases, the development of trustworthy artificial intelligence has become important. However, recent studies have shown that deep neural networks are susceptible to learn spurious correlations present in datasets. To improve the reliability, we propose a simple yet effective framework called controllable feature whitening. We quantify the linear correlation between the target and bias features by the covariance matrix, and eliminate it through the whitening module. Our results systemically demonstrate that removing the linear correlations between features fed into the last linear classifier significantly mitigates the bias, while avoiding the need to model intractable higher-order dependencies. A particular advantage of the proposed method is that it does not require regularization terms or adversarial learning, which often leads to unstable optimization in practice. Furthermore, we show that two fairness criteria, demographic parity and equalized odds, can be effectively handled by whitening with the re-weighted covariance matrix. Consequently, our method controls the trade-off between the utility and fairness of algorithms by adjusting the weighting coefficient. Finally, we validate that our method outperforms existing approaches on four benchmark datasets: Corrupted CIFAR-10, Biased FFHQ, WaterBirds, and Celeb-A",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yooshin Cho",
      "Hanbyel Cho",
      "Janghyeon Lee",
      "HyeongGwon Hong",
      "Jaesung Ahn",
      "Junmo Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zheng_iManip_Skill-Incremental_Learning_for_Robotic_Manipulation_ICCV_2025_paper.html": {
    "title": "iManip: Skill-Incremental Learning for Robotic Manipulation",
    "volume": "main",
    "abstract": "The development of a generalist agent with adaptive multiple manipulation skills has been a long-standing goal in the robotics community.In this paper, we explore a crucial task, skill-incremental learning, in robotic manipulation, which is to endow the robots with the ability to learn new manipulation skills based on the previous learned knowledge without re-training. First, we build a skill-incremental environment based on the RLBench benchmark, and explore how traditional incremental methods perform in this setting. We find that they suffer from severe catastrophic forgetting due to the previous methods on classification overlooking the characteristics of temporality and action complexity in robotic manipulation tasks. Towards this end, we propose an incremental Manipulation framework, termed iManip, to mitigate the above issues. We firstly design a temporal replay strategy to maintain the integrity of old skills when learning new skill. Moreover, we propose the Extendable PerceiverIO, consisting of an action prompt with extendable weight to adapt to new action primitives in new skill. Extensive experiments show that our framework performs well in Skill-Incremental Learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zexin Zheng",
      "Jia-Feng Cai",
      "Xiao-Ming Wu",
      "Yi-Lin Wei",
      "Yu-Ming Tang",
      "Ancong Wu",
      "Wei-Shi Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kung_RadarSplat_Radar_Gaussian_Splatting_for_High-Fidelity_Data_Synthesis_and_3D_ICCV_2025_paper.html": {
    "title": "RadarSplat: Radar Gaussian Splatting for High-Fidelity Data Synthesis and 3D Reconstruction of Autonomous Driving Scenes",
    "volume": "main",
    "abstract": "High-fidelity 3D scene reconstruction plays a crucial role in autonomous driving by enabling novel data generation from existing datasets. This allows simulating safety-critical scenarios and augmenting training datasets without incurring further data collection costs. While recent advances in radiance fields have demonstrated promising results in 3D reconstruction and sensor data synthesis using cameras and LiDAR, their potential for radar remains largely unexplored. Radar is crucial for autonomous driving due to its robustness in adverse weather conditions like rain, fog, and snow, where optical sensors often struggle. Although the state-of-the-art radar-based neural representation shows promise for 3D driving scene reconstruction, it performs poorly in scenarios with significant radar noise, including receiver saturation and multipath reflection. Moreover, it is limited to synthesizing preprocessed, noise-excluded radar images, failing to address realistic radar data synthesis. To address these limitations, this paper proposes RadarSplat, which integrates Gaussian Splatting with novel radar noise modeling to enable realistic radar data synthesis and enhanced 3D reconstruction. Compared to the state-of-the-art, RadarSplat achieves superior radar image synthesis (+3.4 PSNR / 2.6x SSIM) and improved geometric reconstruction (-40% RMSE / 1.5x Accuracy), demonstrating its effectiveness in generating high-fidelity radar data and scene reconstruction",
    "checked": true,
    "id": "3d697a02982eb2d66c1c1c38f9b793f7acb2b39e",
    "semantic_title": "radarsplat: radar gaussian splatting for high-fidelity data synthesis and 3d reconstruction of autonomous driving scenes",
    "citation_count": 3,
    "authors": [
      "Pou-Chun Kung",
      "Skanda Harisha",
      "Ram Vasudevan",
      "Aline Eid",
      "Katherine A. Skinner"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Nyffeler_Hierarchical_3D_Scene_Graphs_Construction_Outdoors_ICCV_2025_paper.html": {
    "title": "Hierarchical 3D Scene Graphs Construction Outdoors",
    "volume": "main",
    "abstract": "Understanding and structuring outdoor environments in 3D is critical for numerous applications, including robotics, urban planning, and autonomous navigation. In this work, we propose a pipeline to construct hierarchical 3D scene graphs from outdoor data, consisting of posed images and 3D reconstructions. Our approach systematically extracts and organizes objects and their subcomponents, enabling representations that span from entire buildings to their facades and individual windows. By leveraging geometric and semantic relationships, our method efficiently groups objects into meaningful hierarchies while ensuring robust spatial consistency. We integrate efficient feature extraction, hierarchical object merging, and relationship inference to generate structured scene graphs that capture both global and local dependencies. Our approach scales to large outdoor environments while maintaining efficiency, and we demonstrate its effectiveness on real-world datasets. We also demonstrate that these constructed outdoor scene graphs are beneficial for downstream applications, such as 3D scene alignment. The code is available on GitHub",
    "checked": false,
    "id": "a0268bd71781076979273d556a32ca4e9115ab7d",
    "semantic_title": "terra: hierarchical terrain-aware 3d scene graph for task-agnostic outdoor mapping",
    "citation_count": 0,
    "authors": [
      "Jon Nyffeler",
      "Federico Tombari",
      "Daniel Barath"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wolf_SIC_Similarity-Based_Interpretable_Image_Classification_with_Neural_Networks_ICCV_2025_paper.html": {
    "title": "SIC: Similarity-Based Interpretable Image Classification with Neural Networks",
    "volume": "main",
    "abstract": "The deployment of deep learning models in critical domains necessitates a balance between high accuracy and interpretability.We introduce SIC, an inherently interpretable neural network that provides local and global explanations of its decision-making process.Leveraging the concept of case-based reasoning, SIC extracts class-representative support vectors from training images, ensuring they capture relevant features while suppressing irrelevant ones.Classification decisions are made by calculating and aggregating similarity scores between these support vectors and the input's latent feature vector. We employ B-Cos transformations, which align model weights with inputs, to yield coherent pixel-level explanations in addition to global explanations of case-based reasoning.We evaluate SIC on three tasks: fine-grained classification on Stanford Dogs and FunnyBirds, multi-label classification on Pascal VOC, and pathology detection on the RSNA dataset.Results indicate that SIC not only achieves competitive accuracy compared to state-of-the-art black-box and inherently interpretable models but also offers insightful explanations verified through practical evaluation on the FunnyBirds benchmark.Our theoretical analysis proves that these explanations fulfill established axioms for explanations. Our findings underscore SIC's potential for applications where understanding model decisions is as critical as the decisions themselves",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tom Nuno Wolf",
      "Emre Kavak",
      "Fabian Bongratz",
      "Christian Wachinger"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jang_Towards_Cross-modal_Backward-compatible_Representation_Learning_for_Vision-Language_Models_ICCV_2025_paper.html": {
    "title": "Towards Cross-modal Backward-compatible Representation Learning for Vision-Language Models",
    "volume": "main",
    "abstract": "Modern retrieval systems often struggle with upgrading to new and more powerful models due to the incompatibility of embeddings between the old and new models. This necessitates a costly process known as backfilling, which involves re-computing the embeddings for a large number of data samples. In vision, Backward-compatible Training (BT) has been proposed to ensure that the new model aligns with the old model's embeddings. This paper extends the concept of vision-only BT to the field of cross-modal retrieval, marking the first attempt to address Cross-modal BT (XBT). Our goal is to achieve backward-compatibility between Vision-Language Pretraining (VLP) models, such as CLIP, for the cross-modal retrieval task. To address XBT challenges, we propose an efficient solution: a projection module that maps the new model's embeddings to those of the old model. This module, pretrained solely with text data, significantly reduces the number of image-text pairs required for XBT learning, and, once it is pretrained, it avoids using the old model during training. Furthermore, we utilize parameter-efficient training strategies that improve efficiency and preserve the off-the-shelf new model's knowledge by avoiding any modifications. Experimental results on cross-modal retrieval datasets demonstrate the effectiveness of XBT and its potential to enable backfill-free upgrades when a new VLP model emerges",
    "checked": true,
    "id": "f95c85581731c0706ace60a01fb05bf4049bf90c",
    "semantic_title": "towards cross-modal backward-compatible representation learning for vision-language models",
    "citation_count": 2,
    "authors": [
      "Young Kyun Jang",
      "Ser-nam Lim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Fuse_Before_Transfer_Knowledge_Fusion_for_Heterogeneous_Distillation_ICCV_2025_paper.html": {
    "title": "Fuse Before Transfer: Knowledge Fusion for Heterogeneous Distillation",
    "volume": "main",
    "abstract": "Most knowledge distillation (KD) methods focus on teacher-student pairs with similar architectures, such as both being CNN models. The potential and flexibility of KD can be greatly improved by expanding it to Cross-Architecture KD (CAKD), where the knowledge of homogeneous and heterogeneous teachers can be distilled selectively. However, substantial feature gaps between heterogeneous models (e.g., ViT teacher v.s. CNN student) make CAKD extremely challenging, caused by the distinction of inherent inductive biases and module functions. To this end, we fuse heterogeneous knowledge before transferring it from teacher to student. This fusion combines the advantages of both cross-architecture inductive biases and module functions by merging different combinations of convolution, attention, and MLP modules derived directly from student and teacher module functions. Furthermore, heterogeneous features exhibit diverse spatial distributions, hindering the effectiveness of conventional pixel-wise MSE loss. Therefore, we replace it with a spatial-agnostic InfoNCE loss. Our method is evaluated across various homogeneous models and arbitrary heterogeneous combinations of CNNs, ViTs, and MLPs, yielding promising performance for distilled models with a maximum gain of 11.47% on CIFAR-100 and 3.67% on ImageNet-1K. Code is available at https://github.com/liguopeng0923/FBT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guopeng Li",
      "Qiang Wang",
      "Ke Yan",
      "Shouhong Ding",
      "Yuan Gao",
      "Gui-Song Xia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_CoLMDriver_LLM-based_Negotiation_Benefits_Cooperative_Autonomous_Driving_ICCV_2025_paper.html": {
    "title": "CoLMDriver: LLM-based Negotiation Benefits Cooperative Autonomous Driving",
    "volume": "main",
    "abstract": "Vehicle-to-vehicle (V2V) cooperative autonomous driving holds great promise for improving safety by addressing the perception and prediction uncertainties inherent in single-agent systems. However, traditional cooperative methods are constrained by rigid collaboration protocols and limited generalization to unseen interactive scenarios. While LLM-based approaches offer generalized reasoning capabilities, their challenges in spatial planning and unstable inference latency hinder their direct application in cooperative driving. To address these limitations, we propose CoLMDriver, the first full-pipeline LLM-based cooperative driving system, enabling effective language-based negotiation and real-time driving control. CoLMDriver features a parallel driving pipeline with two key components: (i) an LLM-based negotiation module under an critic-feedback paradigm, which continuously refines cooperation policies through feedback from previous decisions of all vehicles; and (ii) an intention-guided waypoint generator, which translates negotiation outcomes into executable waypoints. Additionally, we introduce InterDrive, a CARLA-based simulation benchmark comprising 10 challenging interactive driving scenarios for evaluating V2V cooperation. Experimental results demonstrate that CoLMDriver significantly outperforms existing approaches, achieving an 11% higher success rate across diverse highly interactive V2V driving scenarios. Code is released on https://github.com/cxliu0314/CoLMDriver",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changxing Liu",
      "Genjia Liu",
      "Zijun Wang",
      "Jinchang Yang",
      "Siheng Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sun_Dual_Domain_Control_via_Active_Learning_for_Remote_Sensing_Domain_ICCV_2025_paper.html": {
    "title": "Dual Domain Control via Active Learning for Remote Sensing Domain Incremental Object Detection",
    "volume": "main",
    "abstract": "Domain incremental object detection in remote sensing addresses the challenge of adapting to continuously emerging domains with distinct characteristics. Unlike natural images, remote sensing data vary significantly due to differences in sensors, altitudes, and geographic locations, leading to data distribution shifts and feature misalignments. These challenges make it difficult for models to generalize across domains while retaining knowledge from previous tasks, requiring effective adaptation strategies to mitigate catastrophic forgetting. To address these challenges, we propose the Dual Domain Control via Active Learning (Active-DDC) method, which integrates active learning strategies to handle data distribution and model feature shifts. The first component, the Data-based Active Learning Example Replay (ALER) module, combines a high-information sample selection strategy from active learning with the characteristic extreme foreground-background ratio in remote sensing images, enabling the selection of highly representative samples for storage in a memory bank. The second component, the Query-based Active Domain Shift Control (ADSC) module, leverages the query vector, a key element for DETR-based detectors, to implement query active preselection and optimal transport matching, thus facilitating effective cross-domain knowledge transfer. Our method achieves optimal performance in domain incremental tasks across four remote sensing datasets, and ablation studies further validate the effectiveness of both components",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiachen Sun",
      "De Cheng",
      "Xi Yang",
      "Nannan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Qiu_VideoSetDiff_Identifying_and_Reasoning_Similarities_and_Differences_in_Similar_Videos_ICCV_2025_paper.html": {
    "title": "VideoSetDiff: Identifying and Reasoning Similarities and Differences in Similar Videos",
    "volume": "main",
    "abstract": "Recognizing subtle similarities and differences among sets of similar activities is central to many real-world applications, including skill acquisition, sports performance evaluation, and anomaly detection. Humans excel at such fine-grained analysis, which requires comprehensive video understanding and cross-video reasoning about action attributes, poses, positions, and emotional states. Yet existing video-based large language models typically address only single-video recognition, leaving their capacity for multi-video reasoning largely unexplored. We introduce VideoSetDiff, a curated dataset designed to test detail-oriented recognition across diverse activities, from subtle action attributes to viewpoint transitions. Our evaluation of current video-based LLMs on VideoSetDiff reveals critical shortcomings, particularly in fine-grained detail recognition and multi-video reasoning. To mitigate these issues, we propose an automatically generated dataset for instruction tuning alongside a novel multi-video recognition framework. While instruction tuning and specialized multi-video reasoning improve performance, all tested models remain far from satisfactory. These findings underscore the need for more robust video-based LLMs capable of handling complex multi-video tasks, enabling diverse real-world applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Qiu",
      "Yanjun Sun",
      "Takuma Yagi",
      "Shusaku Egami",
      "Natsuki Miyata",
      "Ken Fukuda",
      "Kensho Hara",
      "Ryusuke Sagawa"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Roberts_GRAB_A_Challenging_GRaph_Analysis_Benchmark_for_Large_Multimodal_Models_ICCV_2025_paper.html": {
    "title": "GRAB: A Challenging GRaph Analysis Benchmark for Large Multimodal Models",
    "volume": "main",
    "abstract": "Large multimodal models (LMMs) have exhibited proficiencies across many visual tasks. Although numerous well-known benchmarks exist to evaluate model performance, they increasingly have insufficient headroom. As such, there is a pressing need for a new generation of benchmarks challenging enough for the next generation of LMMs. One area that LMMs show potential is graph analysis, specifically, the tasks an analyst might typically perform when interpreting figures such as estimating the mean, intercepts or correlations of functions and data series. In this work, we introduce GRAB, a graph analysis benchmark, fit for current and future frontier LMMs. Our benchmark is predominantly synthetic, ensuring high-quality, noise-free questions. GRAB is comprised of 3284 questions, covering five tasks and 23 graph properties. We evaluate 20 LMMs on GRAB, finding it to be a challenging benchmark, with the highest performing model attaining a score of just 21.0%. Finally, we conduct various ablations to investigate where the models succeed and struggle. We release GRAB and a lightweight GRAB-Lite to encourage progress in this important, growing domain",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonathan Roberts",
      "Kai Han",
      "Samuel Albanie"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Qi_Transparent_Vision_A_Theory_of_Hierarchical_Invariant_Representations_ICCV_2025_paper.html": {
    "title": "Transparent Vision: A Theory of Hierarchical Invariant Representations",
    "volume": "main",
    "abstract": "Developing robust and interpretable vision systems is a crucial step towards trustworthy artificial intelligence. One promising paradigm is to design transparent structures, e.g., geometric invariance, for fundamental representations. However, such invariants exhibit limited discriminability, limiting their applications in larger-scale tasks. For this open problem, we conduct a systematic investigation of hierarchical invariance, exploring this topic from theoretical, practical, and application perspectives. At the theoretical level, we show how to construct discriminative invariants with a Convolutional Neural Network (CNN)-like hierarchical architecture, yet in a fully transparent manner. The general blueprint, specific definitions, invariant properties, and numerical implementations are provided. At the practical level, we discuss how to customize this transparent framework into a given task. With the over-completeness, discriminative features w.r.t. the task can be adaptively formed in a Neural Architecture Search (NAS)-like manner. We demonstrate the above arguments with accuracy, invariance, and efficiency results on laboratory-style classification experiments. Furthermore, at the application level, our representations are explored in real-world forensic tasks on adversarial perturbations and generated content. Such applications reveal that our invariants exhibit competitive discriminability even in the era of deep learning. For robust and interpretable vision tasks at larger scales, hierarchical invariant representations can be considered as an effective alternative to traditional CNNs and invariants",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuren Qi",
      "Yushu Zhang",
      "Chao Wang",
      "Zhihua Xia",
      "Xiaochun Cao",
      "Fenglei Fan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Rajic_Multi-View_3D_Point_Tracking_ICCV_2025_paper.html": {
    "title": "Multi-View 3D Point Tracking",
    "volume": "main",
    "abstract": "We introduce the first data-driven multi-view 3D point tracker, designed to track arbitrary points in dynamic scenes using multiple camera views. Unlike existing monocular trackers, which struggle with depth ambiguities and occlusion, or prior multi-camera methods that require over 20 cameras and tedious per-sequence optimization, our feed-forward model directly predicts 3D correspondences using a practical number of cameras (e.g., four), enabling robust and accurate online tracking. Given known camera poses and either sensor-based or estimated multi-view depth, our tracker fuses multi-view features into a unified point cloud and applies k-nearest-neighbors correlation alongside a transformer-based update to reliably estimate long-range 3D correspondences, even under occlusion. We train on 5K synthetic multi-view Kubric sequences and evaluate on two real-world benchmarks--Panoptic Studio and DexYCB--achieving median trajectory errors of 3.1 cm and 2.0cm, respectively. Our method generalizes well to diverse camera setups of 1-8 views with varying vantage points and video lengths of 24-150 frames. By releasing our tracker alongside training and evaluation datasets, we aim to set a new standard for multi-view 3D tracking research and provide a practical tool for real-world applications. Project page: https://ethz-vlg.github.io/mvtracker",
    "checked": true,
    "id": "5ad35aeaabac3279edc94fe16245c9bee116165b",
    "semantic_title": "multi-view 3d point tracking",
    "citation_count": 0,
    "authors": [
      "Frano Rajič",
      "Haofei Xu",
      "Marko Mihajlovic",
      "Siyuan Li",
      "Irem Demir",
      "Emircan Gündoğdu",
      "Lei Ke",
      "Sergey Prokudin",
      "Marc Pollefeys",
      "Siyu Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lee_InsideOut_Integrated_RGB-Radiative_Gaussian_Splatting_for_Comprehensive_3D_Object_Representation_ICCV_2025_paper.html": {
    "title": "InsideOut: Integrated RGB-Radiative Gaussian Splatting for Comprehensive 3D Object Representation",
    "volume": "main",
    "abstract": "We introduce InsideOut, an extension of 3D Gaussian splatting (3DGS) that bridges the gap between high-fidelity RGB surface details and subsurface X-ray structures. The fusion of RGB and X-ray imaging is invaluable in fields such as medical diagnostics, cultural heritage restoration, and manufacturing. We collect new paired RGB and X-ray data, perform hierarchical fitting to align RGB and X-ray radiative Gaussian splats, and propose an X-ray reference loss to ensure consistent internal structures. InsideOut effectively addresses the challenges posed by disparate data representations between the two modalities and limited paired datasets. This approach significantly extends the applicability of 3DGS, enhancing visualization, simulation, and non-destructive testing capabilities across various domains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jungmin Lee",
      "Seonghyuk Hong",
      "Juyong Lee",
      "Jaeyoon Lee",
      "Jongwon Choi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Giacomini_Splat-LOAM_Gaussian_Splatting_LiDAR_Odometry_and_Mapping_ICCV_2025_paper.html": {
    "title": "Splat-LOAM: Gaussian Splatting LiDAR Odometry and Mapping",
    "volume": "main",
    "abstract": "LiDARs provide accurate geometric measurements, making them valuable for ego-motion estimation and reconstruction tasks.Although its success, managing an accurate and lightweight representation of the environment still poses challenges.Both classic and NeRF-based solutions have to trade off accuracy over memory and processing times.In this work, we build on recent advancements in Gaussian Splatting methods to develop a novel \\lidar odometry and mapping pipeline that exclusively relies on Gaussian primitives for its scene representation.Leveraging spherical projection, we drive the refinement of the primitives uniquely from LiDAR measurements.Experiments show that our approach matches the current registration performance, while achieving SOTA results for mapping tasks with minimal GPU requirements. This efficiency makes it a strong candidate for further exploration and potential adoption in real-time robotics estimation tasks",
    "checked": true,
    "id": "db8d84004e77265d1164b1e33feadf303658400f",
    "semantic_title": "splat-loam: gaussian splatting lidar odometry and mapping",
    "citation_count": 2,
    "authors": [
      "Emanuele Giacomini",
      "Luca Di Giammarino",
      "Lorenzo De Rebotti",
      "Giorgio Grisetti",
      "Martin R. Oswald"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chinchuthakun_LUSD_Localized_Update_Score_Distillation_for_Text-Guided_Image_Editing_ICCV_2025_paper.html": {
    "title": "LUSD: Localized Update Score Distillation for Text-Guided Image Editing",
    "volume": "main",
    "abstract": "While diffusion models show promising results in image editing given a target prompt, achieving both prompt fidelity and background preservation remains difficult. Recent works have introduced score distillation techniques that leverage the rich generative prior of text-to-image diffusion models to solve this task without additional fine-tuning. However, these methods often struggle with tasks such as object insertion. Our investigation of these failures reveals significant variations in gradient magnitude and spatial distribution, making hyperparameter tuning highly input-specific or unsuccessful. To address this, we propose two simple yet effective modifications: attention-based spatial regularization and gradient filtering-normalization, both aimed at reducing these variations during gradient updates. Experimental results show our method outperforms state-of-the-art score distillation techniques in prompt fidelity, improving successful edits while preserving the background. Users also preferred our method over state-of-the-art techniques across three metrics, and by 58-64% overall",
    "checked": true,
    "id": "6a6ea1c892ac9918dc172ff1b1a84049418a7fab",
    "semantic_title": "lusd: localized update score distillation for text-guided image editing",
    "citation_count": 0,
    "authors": [
      "Worameth Chinchuthakun",
      "Tossaporn Saengja",
      "Nontawat Tritrong",
      "Pitchaporn Rewatbowornwong",
      "Pramook Khungurn",
      "Supasorn Suwajanakorn"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lim_ChartCap_Mitigating_Hallucination_of_Dense_Chart_Captioning_ICCV_2025_paper.html": {
    "title": "ChartCap: Mitigating Hallucination of Dense Chart Captioning",
    "volume": "main",
    "abstract": "Generating accurate, informative, and hallucination-free captions for charts remains challenging for vision language models, primarily due to the lack of large-scale, high-quality datasets of real-world charts. However, existing real-world chart datasets suffer from the inclusion of extraneous information that cannot be inferred from the chart and failure to sufficiently capture structural elements and key insights. Therefore, we introduce ChartCap, a large-scale dataset of 565K real-world chart images paired with type-specific, dense captions that exclude extraneous information and highlight both structural elements and key insights in detail. To build ChartCap, we design a four-stage pipeline that generates captions using only the discernible data from the chart and employ a cycle consistency-based human verification, which accelerates quality control without sacrificing accuracy. Additionally, we propose a novel metric, the Visual Consistency Score, which evaluates caption quality by measuring the similarity between the chart regenerated from a caption and the original chart, independent of reference captions. Extensive experiments confirms that models fine-tuned on ChartCap consistently generate more accurate and informative captions with reduced hallucinations, surpassing both open-source and proprietary models and even human-annotated captions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyoung Lim",
      "Jaewoo Ahn",
      "Gunhee Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Du_RTMap_Real-Time_Recursive_Mapping_with_Change_Detection_and_Localization_ICCV_2025_paper.html": {
    "title": "RTMap: Real-Time Recursive Mapping with Change Detection and Localization",
    "volume": "main",
    "abstract": "While recent online HD mapping methods relieve burdened offline pipelines and solve map freshness, they remain limited by perceptual inaccuracies, occlusion in dense traffic, and an inability to fuse multi-agent observations. We propose RTMap to enhance these single-traversal methods by persistently crowdsourcing a multi-traversal HD map as a self-evolutional memory. On onboard agents, RTMap simultaneously addresses three core challenges in an end-to-end fashion: (1) Uncertainty-aware positional modeling for HD map elements, (2) probabilistic-aware localization w.r.t. the crowdsourced prior-map, and (3) real-time detection for possible road structural changes. Experiments on several public autonomous driving datasets demonstrate our solid performance on both the prior-aided map quality and the localization accuracy, demonstrating our effectiveness of robustly serving downstream prediction and planning modules while gradually improving the accuracy and freshness of the crowdsourced prior-map asynchronously. Our source-code will be made publicly available at https://github.com/CN-ADLab/RTMap",
    "checked": true,
    "id": "35c49c3e20e0ba91b3e5a99b38d7e7dd81e2beb9",
    "semantic_title": "rtmap: real-time recursive mapping with change detection and localization",
    "citation_count": 1,
    "authors": [
      "Yuheng Du",
      "Sheng Yang",
      "Lingxuan Wang",
      "Zhenghua Hou",
      "Chengying Cai",
      "Zhitao Tan",
      "Mingxia Chen",
      "Shi-Sheng Huang",
      "Qiang Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yuan_Scaling_3D_Compositional_Models_for_Robust_Classification_and_Pose_Estimation_ICCV_2025_paper.html": {
    "title": "Scaling 3D Compositional Models for Robust Classification and Pose Estimation",
    "volume": "main",
    "abstract": "Deep learning algorithms for object classification and 3D object pose estimation lack robustness to out-of-distribution factors such as synthetic stimuli, changes in weather conditions, and partial occlusion. Recently, a class of Neural Mesh Models have been developed where objects are represented in terms of 3D meshes with learned features at the vertices. These models have shown robustness in small-scale settings, involving 10 objects, but it is unclear that they can be scaled up to 100s of object classes. The main problem is that their training involves contrastive learning among the vertices of all object classes, which scales quadratically with the number of classes. We present a strategy which exploits the compositionality of the objects, i.e. the independence of the feature vectors of the vertices, which greatly reduces the training time while also improving the performance of the algorithms. We first restructure the per-vertex contrastive learning into contrasting within class and between classes. Then we propose a process that dynamically decouples the contrast between classes which are rarely confused, and enhances the contrast between the vertices of classes that are most confused. Our large-scale 3D compositional model not only achieves state-of-the-art performance on the task of predicting classification and pose estimation simultaneously, surpassing the Neural Mesh Models and standard DNNs, but is also more robust to out-of-distribution testing including occlusion, weather conditions, synthetic data, and generalization to unknown classes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoding Yuan",
      "Guofeng Zhang",
      "Prakhar Kaushik",
      "Artur Jesslen",
      "Adam Kortylewski",
      "Alan Yuille"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lee_MultiVerse_A_Multi-Turn_Conversation_Benchmark_for_Evaluating_Large_Vision_and_ICCV_2025_paper.html": {
    "title": "MultiVerse: A Multi-Turn Conversation Benchmark for Evaluating Large Vision and Language Models",
    "volume": "main",
    "abstract": "Vision-and-Language Models (VLMs) have shown impressive capabilities on single-turn benchmarks, yet real-world applications often demand more intricate multi-turn dialogues. Existing multi-turn datasets (e.g, MMDU, ConvBench) only partially capture the breadth and depth of conversational scenarios encountered by users. In this work, we introduce MultiVerse, a novel multi-turn conversation benchmark featuring 647 dialogues--each averaging four turns--derived from a diverse set of 12 popular VLM evaluation benchmarks. With 484 tasks and 484 interaction goals, MultiVerse covers a wide range of topics, from factual knowledge and perception to advanced reasoning tasks such as mathematics and coding. To facilitate robust assessment, we propose a checklist-based evaluation method that leverages GPT-4o as the automated evaluator, measuring performance across 37 key aspects, including perceptual accuracy, linguistic clarity, and factual correctness. We evaluate 18 VLMs on MultiVerse, revealing that even the strongest models (e.g., GPT-4o) achieve only a 50% success rate in complex multi-turn conversations, highlighting the dataset's challenging nature. Notably, we find that providing full dialogue context significantly enhances performance for smaller or weaker models, emphasizing the importance of in-context learning. We believe MultiVerse is a landscape of evaluating multi-turn interaction abilities for VLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Young-Jun Lee",
      "Byung-Kwan Lee",
      "Jianshu Zhang",
      "Yechan Hwang",
      "Byungsoo Ko",
      "Han-Gyu Kim",
      "Dongyu Yao",
      "Xuankun Rong",
      "Eojin Joo",
      "Seung-Ho Han",
      "Bowon Ko",
      "Ho-Jin Choi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Forte_Contact-Aware_Refinement_of_Human_Pose_Pseudo-Ground_Truth_via_Bioimpedance_Sensing_ICCV_2025_paper.html": {
    "title": "Contact-Aware Refinement of Human Pose Pseudo-Ground Truth via Bioimpedance Sensing",
    "volume": "main",
    "abstract": "Capturing accurate 3D human pose in the wild would provide valuable data for training motion-generation and pose-estimation methods. While video-based estimation approaches have become increasingly accurate, they often fail in common scenarios involving self-contact, such as a hand touching the face. In contrast, wearable bioimpedance sensing can cheaply and unobtrusively measure ground-truth skin-to-skin contact. Consequently, we propose a novel framework that combines visual pose estimators with bioimpedance sensing to capture the 3D pose of people by taking self-contact into account. Our method, BioTUCH, initializes the pose using an off-the-shelf estimator and introduces contact-aware pose optimization during measured self-contact: reprojection error and deviations from the input estimate are minimized while enforcing vertex proximity constraints. We validate our approach using a new dataset of synchronized RGB video, bioimpedance measurements, and 3D motion capture. Testing with three input pose estimators, we demonstrate an average of 11.7% improvement in reconstruction accuracy. We also present a miniature wearable bioimpedance sensor that enables efficient large-scale collection of contact-aware training data for improving pose estimation and generation using BioTUCH. Code and data are available at biotuch.is.tue.mpg.de",
    "checked": true,
    "id": "4b16ea10a0e01a7fb5c961e74107ca6820877cd0",
    "semantic_title": "contact-aware refinement of human pose pseudo-ground truth via bioimpedance sensing",
    "citation_count": 0,
    "authors": [
      "Maria-Paola Forte",
      "Nikos Athanasiou",
      "Giulia Ballardini",
      "Jan Ulrich Bartels",
      "Katherine J. Kuchenbecker",
      "Michael J. Black"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Fan_Embodied_VideoAgent_Persistent_Memory_from_Egocentric_Videos_and_Embodied_Sensors_ICCV_2025_paper.html": {
    "title": "Embodied VideoAgent: Persistent Memory from Egocentric Videos and Embodied Sensors Enables Dynamic Scene Understanding",
    "volume": "main",
    "abstract": "This paper investigates the problem of understanding dynamic 3D scenes from egocentric observations, a key challenge in robotics and embodied AI. Unlike prior studies that explored this as long-form video understanding and utilized egocentric video only, we instead propose an LLM-based agent, Embodied VideoAgent, which constructs scene memory from both egocentric video and embodied sensory inputs (e.g. depth and pose sensing). We further introduce a VLM-based approach to automatically update the memory when actions or activities over objects are perceived. Embodied VideoAgent attains significant advantages over counterparts in challenging reasoning and planning tasks in 3D scenes, achieving gains of 6.5% on Ego4D-VQ3D, 2.6% on OpenEQA, and 15.3% on EnvQA. We have also demonstrated its potential in various embodied AI tasks including generating embodied interactions and perception for robot manipulation. The code and demo will be made public",
    "checked": true,
    "id": "cd80793c36fa0e5e7e7e96e2b7044b4f90b8b7d5",
    "semantic_title": "embodied videoagent: persistent memory from egocentric videos and embodied sensors enables dynamic scene understanding",
    "citation_count": 8,
    "authors": [
      "Yue Fan",
      "Xiaojian Ma",
      "Rongpeng Su",
      "Jun Guo",
      "Rujie Wu",
      "Xi Chen",
      "Qing Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Rege_CuRe_Cultural_Gaps_in_the_Long_Tail_of_Text-to-Image_Systems_ICCV_2025_paper.html": {
    "title": "CuRe: Cultural Gaps in the Long Tail of Text-to-Image Systems",
    "volume": "main",
    "abstract": "Popular text-to-image (T2I) systems are trained on web-scraped data, which is heavily Amero and Euro-centric, underrepresenting the cultures of the Global South. To analyze these biases, we introduce CuRe, a novel and scalable benchmarking and scoring suite for cultural representativeness that leverages the marginal utility of attribute specification to T2I systems as a proxy for human judgments. Our CuRe benchmark dataset has a novel categorical hierarchy built from the crowdsourced Wikimedia knowledge graph, with 300 cultural artifacts across 32 cultural subcategories grouped into six broad cultural axes (food, art, fashion, architecture, celebrations, and people). Our dataset's categorical hierarchy enables CuRe scorers to evaluate T2I systems by analyzing their response to increasing the informativeness of text conditioning, enabling fine-grained cultural comparisons. We empirically observe much stronger correlations of our class of scorers to human judgments of perceptual similarity, image-text alignment, and cultural diversity across image encoders (SigLIP 2, AIMV2 and DINOv2), multimodal language models (OpenCLIP, SigLIP 2, Gemini 2.0 Flash) and state-of-the-art text-to-image systems, including three variants of Stable Diffusion (1.5, XL, 3.5 Large), FLUX.1 [dev], Ideogram 2.0, and DALL-E 3. The code and dataset is open-sourced and available at https://aniketrege.github.io/cure/",
    "checked": true,
    "id": "5553eeb71ca947cc1165475376fd249a1f8f7bbe",
    "semantic_title": "cure: cultural gaps in the long tail of text-to-image systems",
    "citation_count": 2,
    "authors": [
      "Aniket Rege",
      "Zinnia Nie",
      "Mahesh Ramesh",
      "Unmesh Raskar",
      "Zhuoran Yu",
      "Aditya Kusupati",
      "Yong Jae Lee",
      "Ramya Korlakai Vinayak"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jeon_Exploiting_Domain_Properties_in_Language-Driven_Domain_Generalization_for_Semantic_Segmentation_ICCV_2025_paper.html": {
    "title": "Exploiting Domain Properties in Language-Driven Domain Generalization for Semantic Segmentation",
    "volume": "main",
    "abstract": "Recent domain generalized semantic segmentation (DGSS) studies have achieved notable improvements by distilling semantic knowledge from Vision-Language Models (VLMs). However, they overlook the semantic misalignment between visual and textual contexts, which arises due to the rigidity of a fixed context prompt learned on a single source domain. To this end, we present a novel domain generalization framework for semantic segmentation, namely Domain-aware Prompt-driven Masked Transformer (DPMFormer). Firstly, we introduce domain-aware prompt learning to facilitate semantic alignment between visual and textual cues. To capture various domain-specific properties with a single source dataset, we propose domain-aware contrastive learning along with the texture perturbation that diversifies the observable domains. Lastly, to establish a framework resilient against diverse environmental changes, we have proposed the domain-robust consistency learning which guides the model to minimize discrepancies of prediction from original and the augmented images. Through experiments and analyses, we demonstrate the superiority of the proposed framework, which establishes a new state-of-the-art on various DGSS benchmarks",
    "checked": false,
    "id": "e4c7f711c8b600087d0eecf2665b9453f0eec59f",
    "semantic_title": "a simple recipe for language-guided domain generalized segmentation",
    "citation_count": 23,
    "authors": [
      "Seogkyu Jeon",
      "Kibeom Hong",
      "Hyeran Byun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zeng_OVG-HQ_Online_Video_Grounding_with_Hybrid-modal_Queries_ICCV_2025_paper.html": {
    "title": "OVG-HQ: Online Video Grounding with Hybrid-modal Queries",
    "volume": "main",
    "abstract": "Video grounding (VG) task focuses on locating specific moments in a video based on a query, usually in text form. However, traditional VG struggles with some scenarios like streaming video or queries using visual cues. To fill this gap, we present a new task named Online Video Grounding with Hybrid-modal Queries (OVG-HQ), which enables online segment localization using text, images, video segments, and their combinations. This task poses two new challenges: limited context in online settings and modality imbalance during training, where dominant modalities overshadow weaker ones. To address these, we propose OVG-HQ-Unify, a unified framework featuring a Parametric Memory Block (PMB) that uses neural network parameters to dynamically retain past context and a cross-modal distillation strategy that guides the learning of non-dominant modalities. This design enables a single model to effectively handle hybrid-modal queries. Due to the lack of suitable datasets, we construct QVHighlights-Unify, an expanded dataset with multi-modal queries. Besides, since offline metrics overlook prediction timeliness, we adapt them to the online setting, introducing oR@n, IoU=m, and online mean Average Precision (omAP) to evaluate both accuracy and efficiency. Experiments show that our OVG-HQ-Unify outperforms existing models, offering a robust solution for online, hybrid-modal video grounding. We will release our source code and dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runhao Zeng",
      "Jiaqi Mao",
      "Minghao Lai",
      "Minh Hieu Phan",
      "Yanjie Dong",
      "Wei Wang",
      "Qi Chen",
      "Xiping Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_ConformalSAM_Unlocking_the_Potential_of_Foundational_Segmentation_Models_in_Semi-Supervised_ICCV_2025_paper.html": {
    "title": "ConformalSAM: Unlocking the Potential of Foundational Segmentation Models in Semi-Supervised Semantic Segmentation with Conformal Prediction",
    "volume": "main",
    "abstract": "Pixel-level vision tasks, such as semantic segmentation, require extensive and high-quality annotated data, which is costly to obtain. Semi-supervised semantic segmentation (SSSS) has emerged as a solution to alleviate the labeling burden by leveraging both labeled and unlabeled data through self-training techniques. Meanwhile, the advent of foundational segmentation models pre-trained on massive data, has shown the potential to generalize across domains effectively. This work explores whether a foundational segmentation model can address label scarcity in the pixel-level vision task as an annotator for unlabeled images. Specifically, we investigate the efficacy of using SEEM, a Segment Anything Model (SAM) variant fine-tuned for textual input, to generate predictive masks for unlabeled data. To address the shortcomings of using SEEM-generated masks as supervision, we propose ConformalSAM, a novel SSSS framework which first calibrates the foundation model using the target domain's labeled data and then filters out unreliable pixel labels of unlabeled data so that only high-confidence labels are used as supervision. By leveraging conformal prediction (CP) to adapt foundation models to target data through uncertainty calibration, ConformalSAM exploits the strong capability of the foundational segmentation model reliably which benefits the early-stage learning, while a subsequent self-reliance training strategy mitigates overfitting to SEEM-generated masks in the later training stage. Our experiment demonstrates that, on three standard benchmarks of SSSS, ConformalSAM achieves superior performance compared to recent SSSS methods and helps boost the performance of those methods as a plug-in",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Danhui Chen",
      "Ziquan Liu",
      "Chuxi Yang",
      "Dan Wang",
      "Yan Yan",
      "Yi Xu",
      "Xiangyang Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ge_V2PE_Improving_Multimodal_Long-Context_Capability_of_Vision-Language_Models_with_Variable_ICCV_2025_paper.html": {
    "title": "V2PE: Improving Multimodal Long-Context Capability of Vision-Language Models with Variable Visual Position Encoding",
    "volume": "main",
    "abstract": "Vision-Language Models (VLMs) have shown promising capabilities in handling various multimodal tasks, yet they struggle in long-context scenarios, particularly tasks involving videos, high-resolution images, or lengthy image-text documents. In our work, we first conduct an empirical analysis of VLMs' long-context capabilities using our augmented long-context multimodal datasets. Our findings reveal that directly applying the positional encoding mechanism used for textual tokens to visual tokens is suboptimal, and VLM performance degrades sharply when the position encoding exceeds the model's context window. To address this, we propose Variable Visual Position Encoding (V2PE), a novel positional encoding approach that employs variable and smaller increments for visual tokens, enabling more efficient management of long multimodal sequences. Our experiments demonstrate the effectiveness of V2PE in enhancing VLMs' ability to effectively understand and reason over long multimodal contexts. We further integrate V2PE with our augmented long-context multimodal datasets to fine-tune the open-source VLMs. The fine-tuned model achieves strong performance on both standard and long-context multimodal tasks. Notably, when the sequence length of the training dataset is increased to 256K tokens, the model is capable of processing multimodal sequences up to 1M tokens, highlighting its potential for real-world long-context applications. We shall release the code, model weights, and datasets to facilitate further research",
    "checked": true,
    "id": "552a405c61de25451bb959cb14e2f2bded0e2884",
    "semantic_title": "v2pe: improving multimodal long-context capability of vision-language models with variable visual position encoding",
    "citation_count": 13,
    "authors": [
      "Junqi Ge",
      "Ziyi Chen",
      "Jintao Lin",
      "Jinguo Zhu",
      "Xihui Liu",
      "Jifeng Dai",
      "Xizhou Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shang_PRO-VPT_Distribution-Adaptive_Visual_Prompt_Tuning_via_Prompt_Relocation_ICCV_2025_paper.html": {
    "title": "PRO-VPT: Distribution-Adaptive Visual Prompt Tuning via Prompt Relocation",
    "volume": "main",
    "abstract": "Visual prompt tuning (VPT), i.e., fine-tuning some lightweight prompt tokens, provides an efficient and effective approach for adapting pre-trained models to various downstream tasks. However, most prior art indiscriminately uses a fixed prompt distribution across different tasks, neglecting the importance of each block varying depending on the task. In this paper, we introduce adaptive distribution optimization (ADO) by tackling two key questions: (1) How to appropriately and formally define ADO, and (2) How to design an adaptive distribution strategy guided by this definition? Through empirical analysis, we first confirm that properly adjusting the distribution significantly improves VPT performance, and further uncover a key insight that a nested relationship exists between ADO and VPT. Based on these findings, we propose a new VPT framework, termed PRO-VPT (iterative Prompt RelOcation-based VPT), which adaptively adjusts the distribution built upon a nested optimization formulation. Specifically, we develop a prompt relocation strategy derived from this formulation, comprising two steps: pruning idle prompts from prompt-saturated blocks, followed by allocating these prompts to the most prompt-needed blocks. By iteratively performing prompt relocation and VPT, our proposal can adaptively learn the optimal prompt distribution in a nested optimization-based manner, thereby unlocking the full potential of VPT. Extensive experiments demonstrate that our proposal significantly outperforms advanced VPT methods, e.g., PRO-VPT surpasses VPT by 1.6 pp and 2.0 pp average accuracy, leading prompt-based methods to state-of-the-art performance on VTAB-1k and FGVC benchmarks. The code is available at https://github.com/ckshang/PRO-VPT",
    "checked": true,
    "id": "4016630329dd9d270a3cf930c4fd0bbdfa91d57f",
    "semantic_title": "pro-vpt: distribution-adaptive visual prompt tuning via prompt relocation",
    "citation_count": 0,
    "authors": [
      "Chikai Shang",
      "Mengke Li",
      "Yiqun Zhang",
      "Zhen Chen",
      "Jinlin Wu",
      "Fangqing Gu",
      "Yang Lu",
      "Yiu-Ming Cheung"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_CoMPaSS_Enhancing_Spatial_Understanding_in_Text-to-Image_Diffusion_Models_ICCV_2025_paper.html": {
    "title": "CoMPaSS: Enhancing Spatial Understanding in Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "Text-to-image (T2I) diffusion models excel at generating photorealistic images, but commonly struggle to render accurate spatial relationships described in text prompts. We identify two core issues underlying this common failure: 1) the ambiguous nature of spatial-related data in existing datasets, and 2) the inability of current text encoders to accurately interpret the spatial semantics of input descriptions. We address these issues with CoMPaSS, a versatile training framework that enhances spatial understanding of any T2I diffusion model. CoMPaSS solves the ambiguity of spatial-related data with the Spatial Constraints-Oriented Pairing (SCOP) data engine, which curates spatially accurate training data through a set of principled spatial constraints. To better exploit the curated high-quality spatial priors, CoMPaSS further introduces a Token ENcoding ORdering (TENOR) module to allow better exploitation of high-quality spatial priors, effectively compensating for the shortcoming of text encoders. Extensive experiments on four popular open-weight T2I diffusion models covering both UNet- and MMDiT-based architectures demonstrate the effectiveness of CoMPaSS by setting new state-of-the-arts with substantial relative gains across well-known benchmarks on spatial relationships generation, including VISOR (+98%), T2I-CompBench Spatial (+67%), and GenEval Position (+131%)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gaoyang Zhang",
      "Bingtao Fu",
      "Qingnan Fan",
      "Qi Zhang",
      "Runxing Liu",
      "Hong Gu",
      "Huaqi Zhang",
      "Xinguo Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_AdaDrive_Self-Adaptive_Slow-Fast_System_for_Language-Grounded_Autonomous_Driving_ICCV_2025_paper.html": {
    "title": "AdaDrive: Self-Adaptive Slow-Fast System for Language-Grounded Autonomous Driving",
    "volume": "main",
    "abstract": "Effectively integrating Large Language Models (LLMs) into autonomous driving requires a balance between leveraging high-level reasoning and maintaining real-time efficiency. Existing approaches either activate LLMs too frequently, causing excessive computational overhead, or use fixed schedules, failing to adapt to dynamic driving conditions. To address these challenges, we propose AdaDrive, an adaptively collaborative slow-fast framework that optimally determines when and how LLMs contribute to decision-making. (1) When to activate the LLM: AdaDrive employs a novel adaptive activation loss that dynamically determines LLM invocation based on a comparative learning mechanism, ensuring activation only in complex or critical scenarios. (2) How to integrate LLM assistance: Instead of rigid binary activation, AdaDrive introduces an adaptive fusion strategy that modulates a continuous, scaled LLM influence based on scene complexity and prediction confidence, ensuring seamless collaboration with conventional planners. Through these strategies, AdaDrive provides a flexible, context-aware framework that maximizes decision accuracy without compromising real-time performance. Extensive experiments on language-grounded autonomous driving benchmarks demonstrate that AdaDrive state-of-the-art performance in terms of both driving accuracy and computational efficiency. Code is available at https://github.com/ReaFly/AdaDrive",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruifei Zhang",
      "Junlin Xie",
      "Wei Zhang",
      "Weikai Chen",
      "Xiao Tan",
      "Xiang Wan",
      "Guanbin Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ren_Multi-modal_Segment_Anything_Model_for_Camouflaged_Scene_Segmentation_ICCV_2025_paper.html": {
    "title": "Multi-modal Segment Anything Model for Camouflaged Scene Segmentation",
    "volume": "main",
    "abstract": "Camouflaged scenes, where objects blend seamlessly into their environments, pose significant challenges to both human observers and computer vision systems. These objects match the background in color, texture, and shape, making them difficult to detect. To this end, we propose leveraging the Segment Anything Model (SAM) to tackle this challenging task effectively. Specifically, we propose how to exploit SAM without requiring any manual prompts by proposing several ideas. At the core of our method lies the rich information extracted through multi-modal prompts. At first, we generate an image caption using the BLIP model and obtain its text embedding through the use of a text encoder. We then generate a visual embedding through the vision encoder of the BLIP model and use both as inputs to SAM to provide additional semantic information about the image. Finally, we propose a couple of architectural novelties, a) we effectively integrate the multi-modal information in SAM through a multi-level adapter and b) we replace the dense embedding of SAM with the image embedding of its image encoder. Our method achieves new state-of-the-art performance in 11 out of 12 metrics in three benchmark datasets for camouflaged detection. Additionally, our method can be successfully adapted to other tasks such as medical image segmentation performing on par or even outperforming the state-of-the-art methods. Our code is available in https://github.com/ic-qialanqian/Vision-Language-SAM",
    "checked": false,
    "id": "b8601f25f1d451ed3a7e209b45eb70e663615997",
    "semantic_title": "memorysam: memorize modalities and semantics with segment anything model 2 for multi-modal semantic segmentation",
    "citation_count": 10,
    "authors": [
      "Guangyu Ren",
      "Hengyan Liu",
      "Michalis Lazarou",
      "Tania Stathaki"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shang_LLaVA-PruMerge_Adaptive_Token_Reduction_for_Efficient_Large_Multimodal_Models_ICCV_2025_paper.html": {
    "title": "LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models",
    "volume": "main",
    "abstract": "Large Multimodal Models (LMMs) have shown significant visual reasoning capabilities by connecting a visual encoder and a large language model. LMMs typically take in a fixed and large amount of visual tokens, such as the penultimate layer features in the CLIP visual encoder, as the prefix content. Recent LMMs incorporate more complex visual inputs, such as high-resolution images and videos, which further increases the number of visual tokens significantly. However, due to the inherent design of the Transformer architecture, the computational costs of these models tend to increase quadratically with the number of input tokens. To tackle this problem, we explore a token reduction mechanism that identifies significant spatial redundancy among visual tokens. In response, we propose PruMerge, a novel adaptive visual token reduction strategy that significantly reduces the number of visual tokens without compromising the performance of LMMs. Specifically, to metric the importance of each token, we exploit the sparsity observed in the visual encoder, characterized by the sparse distribution of attention scores between the class token and visual tokens. This sparsity enables us to dynamically select the most crucial visual tokens to retain. Subsequently, we cluster the selected (unpruned) tokens based on their key similarity and merge them with the unpruned tokens, effectively supplementing and enhancing their informational content. Empirically, when applied to LLaVA-1.5 and Video-LLaVA, our approach can reduce the number of visual tokens by 4 times, and achieve comparable or better performance across diverse visual question-answering and reasoning tasks",
    "checked": true,
    "id": "c0ef72d02b93065e77c506e23ce9acbbcd945893",
    "semantic_title": "llava-prumerge: adaptive token reduction for efficient large multimodal models",
    "citation_count": 176,
    "authors": [
      "Yuzhang Shang",
      "Mu Cai",
      "Bingxin Xu",
      "Yong Jae Lee",
      "Yan Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xu_Event-boosted_Deformable_3D_Gaussians_for_Dynamic_Scene_Reconstruction_ICCV_2025_paper.html": {
    "title": "Event-boosted Deformable 3D Gaussians for Dynamic Scene Reconstruction",
    "volume": "main",
    "abstract": "Deformable 3D Gaussian Splatting (3D-GS) is limited by missing intermediate motion information due to the low temporal resolution of RGB cameras. To address this, we introduce the first approach combining event cameras, which capture high-temporal-resolution, continuous motion data, with deformable 3D-GS for dynamic scene reconstruction. We observe that threshold modeling for events plays a crucial role in achieving high-quality reconstruction. Therefore, we propose a GS-Threshold Joint Modeling strategy, creating a mutually reinforcing process that greatly improves both 3D reconstruction and threshold modeling. Moreover, we introduce a Dynamic-Static Decomposition strategy that first identifies dynamic areas by exploiting the inability of static Gaussians to represent motions, then applies a buffer-based soft decomposition to separate dynamic and static areas. This strategy accelerates rendering by avoiding unnecessary deformation in static areas, and focuses on dynamic areas to enhance fidelity. Additionally, we contribute the first event-inclusive 4D benchmark with synthetic and real-world dynamic scenes, on which our method achieves state-of-the-art performance",
    "checked": true,
    "id": "b10865d8483ddb5f3b511362058ee84c35130a13",
    "semantic_title": "event-boosted deformable 3d gaussians for dynamic scene reconstruction",
    "citation_count": 0,
    "authors": [
      "Wenhao Xu",
      "Wenming Weng",
      "Yueyi Zhang",
      "Ruikang Xu",
      "Zhiwei Xiong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liang_Gradient-Reweighted_Adversarial_Camouflage_for_Physical_Object_Detection_Evasion_ICCV_2025_paper.html": {
    "title": "Gradient-Reweighted Adversarial Camouflage for Physical Object Detection Evasion",
    "volume": "main",
    "abstract": "Object detection is widely used in real-world applications such as autonomous driving, yet adversarial camouflage poses a significant threat by deceiving detectors from multiple viewpoints. Existing techniques struggle to maintain consistent attack efficacy across different viewpoints. To address this, we propose GRAC, an adversarial camouflage framework that enhances attack effectiveness across viewpoints and distances. First, we identify conflicts in gradient updates across angles and introduce gradient reweighting to resolve them, enabling coordinated optimization. Second, we model light interactions to simulate illumination changes, improving robustness under varying lighting conditions. Additionally, we address non-uniform texture updates arising from inconsistent sampling density during rendering by applying pooling-based texture regularization to improve smoothness. Extensive experiments in both simulated and physical environments demonstrate that GRAC outperforms existing methods across diverse conditions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawei Liang",
      "Siyuan Liang",
      "Tianrui Lou",
      "Ming Zhang",
      "Wenjin Li",
      "Dunqiu Fan",
      "Xiaochun Cao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jing_Stereo_Any_Video_Temporally_Consistent_Stereo_Matching_ICCV_2025_paper.html": {
    "title": "Stereo Any Video: Temporally Consistent Stereo Matching",
    "volume": "main",
    "abstract": "This paper introduces Stereo Any Video, a powerful framework for video stereo matching. It can estimate spatially accurate and temporally consistent disparities without relying on auxiliary information such as camera poses or optical flow. The strong capability is driven by rich priors from monocular video depth models, which are integrated with convolutional features to produce stable representations. To further enhance performance, key architectural innovations are introduced: all-to-all-pairs correlation, which constructs smooth and robust matching cost volumes, and temporal convex upsampling, which improves temporal coherence. These components collectively ensure robustness, accuracy, and temporal consistency, setting a new standard in video stereo matching. Extensive experiments demonstrate that our method achieves state-of-the-art performance across multiple datasets both qualitatively and quantitatively in zero-shot settings, as well as strong generalization to real-world indoor and outdoor scenarios. Code and models will be publicly released",
    "checked": true,
    "id": "238831440e67929c2ca4787ea2a0048f7e38baee",
    "semantic_title": "stereo any video: temporally consistent stereo matching",
    "citation_count": 1,
    "authors": [
      "Junpeng Jing",
      "Weixun Luo",
      "Ye Mao",
      "Krystian Mikolajczyk"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Han_ASCENT_Annotation-free_Self-supervised_Contrastive_Embeddings_for_3D_Neuron_Tracking_in_ICCV_2025_paper.html": {
    "title": "ASCENT: Annotation-free Self-supervised Contrastive Embeddings for 3D Neuron Tracking in Fluorescence Microscopy",
    "volume": "main",
    "abstract": "We propose ASCENT, a novel framework for tracking neurons in 3D fluorescence microscopy recordings without relying on manual track annotations. ASCENT leverages self-supervised contrastive learning to learn robust, discriminative embeddings from detected neuron candidates. At its core is a volume compression module that transforms full 3D volumetric data into an efficient 2D representation by iteratively projecting along the z-axis and integrating positional information. This compressed representation is processed by a deep encoder (e.g., ResNet or Vision Transformer) to yield robust feature vectors that capture both appearance and spatial relationships among neurons. Extensive experiments on both in-house and public datasets demonstrate that ASCENT achieves state-of-the-art tracking performance with fast inference speed while removing the need for costly manual labeling and heavy pre- and post-processing. Our results suggest that this approach provides a scalable solution for 3D neuron tracking and holds promise for applications such as inter-individual neuron identity matching and demixing overlapping cells",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haejun Han",
      "Hang Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lafon_ViLU_Learning_Vision-Language_Uncertainties_for_Failure_Prediction_ICCV_2025_paper.html": {
    "title": "ViLU: Learning Vision-Language Uncertainties for Failure Prediction",
    "volume": "main",
    "abstract": "Reliable Uncertainty Quantification (UQ) and failure prediction remain open challenges for Vision-Language Models (VLMs). We introduce ViLU, a new Vision-Language Uncertainty quantification framework that contextualizes uncertainty estimates by leveraging all task-relevant textual representations. ViLU constructs an uncertainty-aware multi-modal representation by integrating the visual embedding, the predicted textual embedding, and an image-conditioned textual representation via cross-attention. Unlike traditional UQ methods based on loss prediction, ViLU trains an uncertainty predictor as a binary classifier to distinguish correct from incorrect predictions using a weighted binary cross-entropy loss, making it loss-agnostic. In particular, our proposed approach is well-suited for post-hoc settings, where only vision and text embeddings are available without direct access to the model itself. Extensive experiments on diverse datasets show the significant gains of our method compared to state-of-the-art failure prediction methods. We apply our method to standard classification datasets, such as ImageNet-1k, as well as large-scale image-caption datasets like CC12M and LAION-400M. Ablation studies highlight the critical role of our architecture and training in achieving effective uncertainty quantification",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marc Lafon",
      "Yannis Karmim",
      "Julio Silva-Rodríguez",
      "Paul Couairon",
      "Clément Rambour",
      "Raphael Fournier-Sniehotta",
      "Ismail Ben Ayed",
      "Jose Dolz",
      "Nicolas Thome"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Su_A_Linear_N-Point_Solver_for_Structure_and_Motion_from_Asynchronous_ICCV_2025_paper.html": {
    "title": "A Linear N-Point Solver for Structure and Motion from Asynchronous Tracks",
    "volume": "main",
    "abstract": "Structure and continuous motion estimation from point correspondences is a fundamental problem in computer vision that has been powered by well-known algorithms such as the familiar 5-point or 8-point algorithm. However, despite their acclaim, these algorithms are limited to processing point correspondences originating from a pair of views each one representing an instantaneous capture of the scene. Yet, in the case of rolling shutter cameras, or more recently, event cameras, this synchronization breaks down. In this work, we present a unified approach for structure and linear motion estimation from 2D point correspondences with arbitrary timestamps, from an arbitrary set of views. By formulating the problem in terms of first-order dynamics and leveraging a constant velocity motion model, we derive a novel, linear point incidence relation allowing for the efficient recovery of both linear velocity and 3D points with predictable degeneracies and solution multiplicities. Owing to its general formulation, it can handle correspondences from a wide range of sensing modalities such as global shutter, rolling shutter, and event cameras, and can even combine correspondences from different collocated sensors. We validate the effectiveness of our solver on both simulated and real-world data, where we show consistent improvement across all modalities when compared to recent approaches. We believe our work opens the door to efficient structure and motion estimation from asynchronous data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hang Su",
      "Yunlong Feng",
      "Daniel Gehrig",
      "Panfeng Jiang",
      "Ling Gao",
      "Xavier Lagorce",
      "Laurent Kneip"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cheng_Constraint-Aware_Feature_Learning_for_Parametric_Point_Cloud_ICCV_2025_paper.html": {
    "title": "Constraint-Aware Feature Learning for Parametric Point Cloud",
    "volume": "main",
    "abstract": "Parametric point clouds are sampled from CAD shapes and are becoming increasingly common in industrial manufacturing. Most CAD-specific deep learning methods focus on geometric features, while overlooking constraints inherent in CAD shapes. This limits their ability to discern CAD shapes with similar appearances but different constraints. To tackle this challenge, we first analyze the constraint importance via simple validation experiments. Then, we introduce a deep learning-friendly constraint representation with three components, and design a constraint-aware feature learning network (CstNet), which includes two stages. Stage 1 extracts constraint representation from BRep data or point cloud based on local features. It enables better generalization ability to unseen dataset after pre-training. Stage 2 employs attention layers to adaptively adjust the weights of three constraints' components. It facilitates the effective utilization of constraints. In addition, we built the first multi-modal parametric-purpose dataset, i.e. Param20K, comprising about 20K CAD instances of 75 classes. On this dataset, CstNet achieved 3.49% (classification) and 26.17% (rotation robustness) accuracy improvements over the state-of-the-art. To the best of our knowledge, CstNet is the first constraint-aware deep learning method tailored for parametric point cloud analysis",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xi Cheng",
      "Ruiqi Lei",
      "Di Huang",
      "Zhichao Liao",
      "Fengyuan Piao",
      "Yan Chen",
      "Pingfa Feng",
      "Long Zeng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lai_Unleashing_Vecset_Diffusion_Model_for_Fast_Shape_Generation_ICCV_2025_paper.html": {
    "title": "Unleashing Vecset Diffusion Model for Fast Shape Generation",
    "volume": "main",
    "abstract": "3D shape generation has greatly flourished through the development of so-called \"native\" 3D diffusion, particularly through the Vectset Diffusion Model (VDM). While recent advancements have shown promising results in generating high-resolution 3D shapes, VDM still struggles at high-speed generation. Challenges exist because of not only difficulties in accelerating diffusion sampling but also VAE decoding in VDM -- areas under-explored in previous works. To address these challenges, we present FlashVDM, a systematic framework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables flexible diffusion sampling with as few as 5 inference steps, while maintaining comparable quality, which is made possible by stabilizing consistency distillation with our newly introduced Progressive Flow Distillation technique. For VAE, we introduce a lightning vectset decoder equipped with Adaptive KV Selection, Hierarchical Volume Decoding,, and Efficient Network Design. By exploiting the locality of vectset and the sparsity of shape surface in the volume, the proposed decoder drastically lowers FLOPs, minimizing the overall decoding overhead. We apply FlashVDM to the current state-of-the-art open-source shape generation model Hunyuan3D-2, resulting in Hunyuan3D-2 Turbo. Through systematic evaluation for both generation and reconstruction, we demonstrate that our model outperforms existing fast 3D generation methods by a significant margin, achieving comparable performance to the state-of-the-art models while reducing inference time by over 45x for reconstruction and 32x for generation. Code and models are publicly available at https://github.com/Tencent-Hunyuan/FlashVDM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeqiang Lai",
      "Yunfei Zhao",
      "Zibo Zhao",
      "Haolin Liu",
      "Fuyun Wang",
      "Huiwen Shi",
      "Xianghui Yang",
      "Qingxiang Lin",
      "Jingwei Huang",
      "Yuhong Liu",
      "Jie Jiang",
      "Chunchao Guo",
      "Xiangyu Yue"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Pathak_Revisiting_Point_Cloud_Completion_Are_We_Ready_For_The_Real-World_ICCV_2025_paper.html": {
    "title": "Revisiting Point Cloud Completion: Are We Ready For The Real-World?",
    "volume": "main",
    "abstract": "Point clouds acquired in constrained, challenging, uncontrolled, and multi-sensor real-world settings are noisy, incomplete, and non-uniformly sparse. This presents acute challenges for the vital task of point cloud completion. Using tools from Algebraic Topology and Persistent Homology (PH), we demonstrate that current benchmark object point clouds lack rich topological features that are integral part of point clouds captured in realistic environments. To facilitate research in this direction, we contribute the first real-world industrial dataset for point cloud completion, RealPC - a diverse, rich and varied set of point clouds. It consists of 40,000 pairs across 21 categories of industrial structures in railway establishments. Benchmark results on several strong baselines reveal that existing methods fail in real-world scenarios. We discover a striking observation - unlike current datasets, RealPC consists of multiple 0- and 1-dimensional PH-based topological features. We prove that integrating these topological priors into existing works helps improve completion. We present how 0-dimensional PH priors extract the global topology of a complete shape in the form of a 3D skeleton and assist a model in generating topologically consistent complete shapes. Since computing Homology is expensive, we present a simple, yet effective Homology Sampler guided network, BOSHNet that bypasses the Homology computation by sampling proxy backbones akin to 0-dim PH. These backbones provide similar benefits of 0-dim PH right from the start of the training, unlike similar methods where accurate backbones are obtained only during later phases of the training",
    "checked": true,
    "id": "dc7dada39bb2094d7f232bafe4a0915fa02c1fc7",
    "semantic_title": "revisiting point cloud completion: are we ready for the real-world?",
    "citation_count": 1,
    "authors": [
      "Stuti Pathak",
      "Prashant Kumar",
      "Dheeraj Baiju",
      "Nicholus Mboga",
      "Gunther Steenackers",
      "Rudi Penne"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jiang_RayZer_A_Self-supervised_Large_View_Synthesis_Model_ICCV_2025_paper.html": {
    "title": "RayZer: A Self-supervised Large View Synthesis Model",
    "volume": "main",
    "abstract": "We present RayZer, a self-supervised multi-view 3D Vision model trained without any 3D supervision, i.e., camera poses and scene geometry, while exhibiting emerging 3D awareness. Concretely, RayZer takes unposed and uncalibrated images as input, recovers camera parameters, reconstructs a scene representation, and synthesizes novel views. During training, RayZer relies solely on its self-predicted camera poses to render target views, eliminating the need for any ground-truth camera annotations and allowing RayZer to be trained with 2D image supervision. The emerging 3D awareness of RayZer is attributed to two key factors. First, we design a self-supervised framework, which achieves 3D-aware auto-encoding of input images by disentangling camera and scene representations. Second, we design a transformer-based model in which the only 3D prior is the ray structure, connecting camera, pixel, and scene simultaneously. RayZer demonstrates comparable or even superior novel view synthesis performance than \"oracle\" methods that rely on pose annotations in both training and testing",
    "checked": true,
    "id": "1770fe638bd30e0ba1ba6efb1283ac40651b52e0",
    "semantic_title": "rayzer: a self-supervised large view synthesis model",
    "citation_count": 6,
    "authors": [
      "Hanwen Jiang",
      "Hao Tan",
      "Peng Wang",
      "Haian Jin",
      "Yue Zhao",
      "Sai Bi",
      "Kai Zhang",
      "Fujun Luan",
      "Kalyan Sunkavalli",
      "Qixing Huang",
      "Georgios Pavlakos"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Perception-as-Control_Fine-grained_Controllable_Image_Animation_with_3D-aware_Motion_Representation_ICCV_2025_paper.html": {
    "title": "Perception-as-Control: Fine-grained Controllable Image Animation with 3D-aware Motion Representation",
    "volume": "main",
    "abstract": "Motion-controllable image animation is a fundamental task with a wide range of potential applications. Recent works have made progress in controlling camera or object motion via various motion representations, while they still struggle to support collaborative camera and object motion control with adaptive control granularity. To this end, we introduce 3D-aware motion representation and propose an image animation framework, called Perception-as-Control, to achieve fine-grained collaborative motion control. Specifically, we construct 3D-aware motion representation from a reference image, manipulate it based on interpreted user instructions, and perceive it from different viewpoints. In this way, camera and object motions are transformed into intuitive and consistent visual changes. Then, our framework leverages the perception results as motion control signals, enabling it to support various motion-related video synthesis tasks in a unified and flexible way. Experiments demonstrate the superiority of the proposed approach",
    "checked": true,
    "id": "b80063e23a5d116c908cd86155fef779ab0c33b6",
    "semantic_title": "perception-as-control: fine-grained controllable image animation with 3d-aware motion representation",
    "citation_count": 8,
    "authors": [
      "Yingjie Chen",
      "Yifang Men",
      "Yuan Yao",
      "Miaomiao Cui",
      "Liefeng Bo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Pan_Structure-aware_Semantic_Discrepancy_and_Consistency_for_3D_Medical_Image_Self-supervised_ICCV_2025_paper.html": {
    "title": "Structure-aware Semantic Discrepancy and Consistency for 3D Medical Image Self-supervised Learning",
    "volume": "main",
    "abstract": "3D medical image self-supervised learning (mSSL) holds great promise for medical analysis. Effectively supporting broader applications requires considering anatomical structure variations in location, scale, and morphology, which are crucial for capturing meaningful distinctions. However, previous mSSL methods partition images with fixed-size patches, often ignoring the structure variations. In this work, we introduce a novel perspective on 3D medical images with the goal of learning structure-aware representations. We assume that patches within the same structure share the same semantics (semantic consistency) while those from different structures exhibit distinct semantics (semantic discrepancy). Based on this assumption, we propose an mSSL framework named S2DC, achieving Structure-aware Semantic Discrepancy and Consistency in two steps. First, S2DC enforces distinct representations for different patches to increase semantic discrepancy by leveraging an optimal transport strategy. Second, S2DC advances semantic consistency at the structural level based on neighborhood similarity distribution. By bridging patch-level and structure-level representations, S2DC achieves structure-aware representations. Thoroughly evaluated across 10 datasets, 4 tasks, and 3 modalities, our proposed method consistently outperforms the state-of-the-art methods in mSSL. The code is available at https://github.com/Ashespt/S2DC",
    "checked": true,
    "id": "1f53309db704054ea76ffab93904af5d5bb4d690",
    "semantic_title": "structure-aware semantic discrepancy and consistency for 3d medical image self-supervised learning",
    "citation_count": 2,
    "authors": [
      "Tan Pan",
      "Zhaorui Tan",
      "Kaiyu Guo",
      "Dongli Xu",
      "Weidi Xu",
      "Chen Jiang",
      "Xin Guo",
      "Yuan Qi",
      "Yuan Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Dunkel_Do_It_Yourself_Learning_Semantic_Correspondence_from_Pseudo-Labels_ICCV_2025_paper.html": {
    "title": "Do It Yourself: Learning Semantic Correspondence from Pseudo-Labels",
    "volume": "main",
    "abstract": "Finding correspondences between semantically similar points across images and object instances is one of the everlasting challenges in computer vision. While large pre-trained vision models have recently been demonstrated as effective priors for semantic matching, they still suffer from ambiguities for symmetric objects or repeated object parts. We propose improving semantic correspondence estimation through 3D-aware pseudo-labeling. Specifically, we train an adapter to refine off-the-shelf features using pseudo-labels obtained via 3D-aware chaining, filtering wrong labels through relaxed cyclic consistency, and 3D spherical prototype mapping constraints. While reducing the need for dataset-specific annotations compared to prior work, we establish a new state-of-the-art on SPair-71k, achieving an absolute gain of over 4% and of over 7% compared to methods with similar supervision requirements. The generality of our proposed approach simplifies the extension of training to other data sources, which we demonstrate in our experiments",
    "checked": true,
    "id": "55c627d2cea3e2f4b2464f9606ec9cd46c41b306",
    "semantic_title": "do it yourself: learning semantic correspondence from pseudo-labels",
    "citation_count": 3,
    "authors": [
      "Olaf Dünkel",
      "Thomas Wimmer",
      "Christian Theobalt",
      "Christian Rupprecht",
      "Adam Kortylewski"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gong_SA-LUT_Spatial_Adaptive_4D_Look-Up_Table_for_Photorealistic_Style_Transfer_ICCV_2025_paper.html": {
    "title": "SA-LUT: Spatial Adaptive 4D Look-Up Table for Photorealistic Style Transfer",
    "volume": "main",
    "abstract": "Photorealistic style transfer (PST) enables real-world color grading by adapting reference image colors while preserving content structure.Existing methods mainly follow either approaches: generation-based methods that prioritize stylistic fidelity at the cost of content integrity and efficiency, or global color transformation methods such as LUT, which preserve structure but lack local adaptability. To bridge this gap, we propose Spatial Adaptive 4D Look-Up Table (SA-LUT), combining LUT efficiency with neural network adaptability. SA-LUT features: (1) a Style-guided 4D LUT Generator that extracts multi-scale features from the style image to predict a 4D LUT, and (2) a Context Generator using content-style cross-attention to produce a context map. This context map enables spatially-adaptive adjustments, allowing our 4D LUT to apply precise color transformations while preserving structural integrity. To establish a rigorous evaluation framework for photorealistic style transfer, we introduce PST50, the first benchmark specifically designed for PST assessment. Experiments demonstrate that SA-LUT substantially outperforms state-of-the-art methods, achieving a 66.7% reduction in LPIPS score compared to 3D LUT approaches, while maintaining real-time performance at 16 FPS for video stylization. Code and benchmark will be released",
    "checked": true,
    "id": "49b6ff44426a04346576fd740cdd596b552b065a",
    "semantic_title": "sa-lut: spatial adaptive 4d look-up table for photorealistic style transfer",
    "citation_count": 1,
    "authors": [
      "Zerui Gong",
      "Zhonghua Wu",
      "Qingyi Tao",
      "Qinyue Li",
      "Chen Change Loy"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Monga_DuET_Dual_Incremental_Object_Detection_via_Exemplar-Free_Task_Arithmetic_ICCV_2025_paper.html": {
    "title": "DuET: Dual Incremental Object Detection via Exemplar-Free Task Arithmetic",
    "volume": "main",
    "abstract": "Real-world object detection systems, such as those in autonomous driving and surveillance, must continuously learn new object categories and simultaneously adapt to changing environmental conditions. Existing approaches, Class Incremental Object Detection (CIOD) and Domain Incremental Object Detection (DIOD)--only address one aspect of this challenge. CIOD struggles in unseen domains, while DIOD suffers from catastrophic forgetting when learning new classes, limiting their real-world applicability. To overcome these limitations, we introduce Dual Incremental Object Detection (DuIOD), a more practical setting that simultaneously handles class and domain shifts in an exemplar-free manner. We propose DuET, a Task Arithmetic-based model merging framework that enables stable incremental learning while mitigating sign conflicts through a novel Directional Consistency Loss. Unlike prior methods, DuET is detector-agnostic, allowing models like YOLO11 and RT-DETR to function as real-time incremental object detectors. To comprehensively evaluate both retention and adaptation, we introduce the Retention-Adaptability Index (RAI), which combines the Average Retention Index (Avg RI) for catastrophic forgetting and the Average Generalization Index for domain adaptability into a common ground. Extensive experiments on the Pascal Series and Diverse Weather Series demonstrate DuET's effectiveness, achieving a +13.12% RAI improvement while preserving 89.3% Avg RI on the Pascal Series (4 tasks), as well as a +11.39% RAI improvement with 88.57% Avg RI on the Diverse Weather Series (3 tasks), outperforming existing methods",
    "checked": true,
    "id": "4857062751b922c7ce3d166dd1c54d627b2056b8",
    "semantic_title": "duet: dual incremental object detection via exemplar-free task arithmetic",
    "citation_count": 0,
    "authors": [
      "Munish Monga",
      "Vishal Chudasama",
      "Pankaj Wasnik",
      "Biplab Banerjee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Maharana_BATCLIP_Bimodal_Online_Test-Time_Adaptation_for_CLIP_ICCV_2025_paper.html": {
    "title": "BATCLIP: Bimodal Online Test-Time Adaptation for CLIP",
    "volume": "main",
    "abstract": "Although open-vocabulary classification models like Contrastive Language Image Pretraining (CLIP) have demonstrated strong zero-shot learning capabilities, their robustness to common image corruptions remains poorly understood. Through extensive experiments, we show that zero-shot CLIP lacks robustness to common image corruptions during test-time, necessitating the adaptation of CLIP to unlabeled corrupted images using test-time adaptation (TTA). However, we found that existing TTA methods have severe limitations in adapting CLIP due to their unimodal nature. To address these limitations, we propose \\texttt BATCLIP , a bimodal online TTA method designed to improve CLIP's robustness to common image corruptions. The key insight of our approach is not only to adapt the visual encoders for improving image features but also to strengthen the alignment between image and text features by promoting a stronger association between the image class prototype, computed using pseudo-labels, and the corresponding text feature. We evaluate our approach on benchmark image corruption datasets and achieve state-of-the-art results in online TTA for CLIP. Furthermore, we evaluate our proposed TTA approach on various domain generalization datasets to demonstrate its generalization capabilities. Our code is available at https://github.com/sarthaxxxxx/BATCLIP",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sarthak Maharana",
      "Baoming Zhang",
      "Leonid Karlinsky",
      "Rogerio Feris",
      "Yunhui Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shao_MagShield_Towards_Better_Robustness_in_Sparse_Inertial_Motion_Capture_Under_ICCV_2025_paper.html": {
    "title": "MagShield: Towards Better Robustness in Sparse Inertial Motion Capture Under Magnetic Disturbances",
    "volume": "main",
    "abstract": "This paper proposes a novel method, named MagShield, designed to address the issue of magnetic disturbances in sparse inertial motion capture (MoCap) systems. Existing Inertial Measurement Units (IMUs) are prone to orientation estimation errors in magnetically disturbed environments, limiting the practical application of inertial Mocap systems in real-world scenarios. To address this problem, MagShield employs a \"detect-then-correct\" strategy, first detecting magnetic disturbances through multi-IMU joint analysis, and then correcting orientation errors using human motion priors. MagShield can be ntegrated with most existing sparse inertial MoCap systems, improving their performance in magnetically disturbed environments. Experimental results demonstrate that MagShield significantly enhances the accuracy of motion capture under magnetic interference and exhibits good compatibility across different sparse inertial MoCap systems. Code and dataset are available at https://github.com/YZ-Shiao/MagShield",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunzhe Shao",
      "Xinyu Yi",
      "Lu Yin",
      "Shihui Guo",
      "Junhai Yong",
      "Feng Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_SHIFT_Smoothing_Hallucinations_by_Information_Flow_Tuning_for_Multimodal_Large_ICCV_2025_paper.html": {
    "title": "SHIFT: Smoothing Hallucinations by Information Flow Tuning for Multimodal Large Language Models",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) are prone to hallucinations, which pose significant risks in their applications. Most existing hallucination detection methods rely on internal probabilities or external knowledge, and they are limited to identifying hallucinations at the sentence or passage level. In this paper, we introduce the first token-level, zero-resource hallucination detection framework, leveraging a novel approach inspired by the Mad Libs game. This method assesses the reliability of the input text by evaluating the consistency of information before and after the game. Building on this framework, we also propose an innovative automated hallucination generation technique and introduce a high-quality hallucination dataset, HalluWiki. Extensive experiments demonstrate that our approach achieves over 90% detection accuracy across different levels, establishing a new frontier in hallucination detection for LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sudong Wang",
      "Yunjian Zhang",
      "Yao Zhu",
      "Enci Liu",
      "Jianing Li",
      "Yanwei Liu",
      "Xiangyang Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Qiu_Bias-Resilient_Weakly_Supervised_Semantic_Segmentation_Using_Normalizing_Flows_ICCV_2025_paper.html": {
    "title": "Bias-Resilient Weakly Supervised Semantic Segmentation Using Normalizing Flows",
    "volume": "main",
    "abstract": "Weakly supervised semantic segmentation (WSSS) aims to generate dense labels using sparse annotations, such as image-level labels. Existing class activation map (CAM) generation methods have been able to locate rough objects. However, due to the limited information provided by image level labels, the bias activation problem, including over-activation, becomes another key obstacle in WSSS. To rectify such bias activation, we attempt to mine pixel level class feature distribution information from the entire dataset. Specifically, we propose to use normalizing flow to model the class feature distribution of all pixels across the entire dataset and design a Bias-Resilient WSSS framework based on Normalizing Flow (BRNF). Normalizing flow has the ability to map complex distributions to normal distributions. Building upon it, we designed an additional Gaussian mixture classifier which classifies pixels from the perspective of feature distributions, providing supplementary information to the conventional MLP based classifier. In addition, we use this distribution to sample low bias features as positive anchors for contrastive learning, thereby encouraging feature optimization toward the correct low-bias direction. Experimental results demonstrate that our method significantly outperforms existing baselines, achieving state-of-the-art performance on WSSS benchmarks. Code will be available at https://github.com/DpDark/BRNF",
    "checked": false,
    "id": "d1cfcd0b73f4d9e9c4067262be073c2752e94663",
    "semantic_title": "automated estimation of microcirculation capillary density using relative perfusion maps",
    "citation_count": 0,
    "authors": [
      "Xianglin Qiu",
      "Xiaoyang Wang",
      "Zhen Zhang",
      "Jimin Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_DSO_Aligning_3D_Generators_with_Simulation_Feedback_for_Physical_Soundness_ICCV_2025_paper.html": {
    "title": "DSO: Aligning 3D Generators with Simulation Feedback for Physical Soundness",
    "volume": "main",
    "abstract": "Most 3D object generators prioritize aesthetic quality, often neglecting the physical constraints necessary for practical applications. One such constraint is that a 3D object should be self-supporting, i.e., remain balanced under gravity. Previous approaches to generating stable 3D objects relied on differentiable physics simulators to optimize geometry at test time, which is slow, unstable, and prone to local optima. Inspired by the literature on aligning generative models with external feedback, we propose Direct Simulation Optimization (DSO). This framework leverages feedback from a (non-differentiable) simulator to increase the likelihood that the 3D generator directly outputs stable 3D objects. We construct a dataset of 3D objects labeled with stability scores obtained from the physics simulator. This dataset enables fine-tuning of the 3D generator using the stability score as an alignment metric, via direct preference optimization (DPO) or direct reward optimization (DRO)---a novel objective we introduce to align diffusion models without requiring pairwise preferences. Our experiments demonstrate that the fine-tuned feed-forward generator, using either the DPO or DRO objective, is significantly faster and more likely to produce stable objects than test-time optimization. Notably, the DSO framework functions even without any ground-truth 3D objects for training, allowing the 3D generator to self-improve by automatically collecting simulation feedback on its own outputs",
    "checked": true,
    "id": "6fe46c6f8183bc5cc22c43a57c215e71f6ad18e7",
    "semantic_title": "dso: aligning 3d generators with simulation feedback for physical soundness",
    "citation_count": 4,
    "authors": [
      "Ruining Li",
      "Chuanxia Zheng",
      "Christian Rupprecht",
      "Andrea Vedaldi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Fan_Align_Your_Rhythm_Generating_Highly_Aligned_Dance_Poses_with_Gating-Enhanced_ICCV_2025_paper.html": {
    "title": "Align Your Rhythm: Generating Highly Aligned Dance Poses with Gating-Enhanced Rhythm-Aware Feature Representation",
    "volume": "main",
    "abstract": "Automatically generating natural, diverse and rhythmic human dance movements driven by music is vital for virtual reality and film industries. However, generating dance that naturally follows music remains a challenge, as existing methods lack proper beat alignment and exhibit unnatural motion dynamics. In this paper, we propose Danceba, a novel framework that leverages gating mechanism to enhance rhythm-aware feature representation for music-driven dance generation, which achieves highly aligned dance poses with enhanced rhythmic sensitivity. Specifically, we introduce Phase-Based Rhythm Extraction (PRE) to precisely extract rhythmic information from musical phase data, capitalizing on the intrinsic periodicity and temporal structures of music. Additionally, we propose Temporal-Gated Causal Attention (TGCA) to focus on global rhythmic features, ensuring that dance movements closely follow the musical rhythm. We also introduce Parallel Mamba Motion Modeling (PMMM) architecture to separately model upper and lower body motions along with musical features, thereby improving the naturalness and diversity of generated dance movements. Extensive experiments confirm that Danceba outperforms state-of-the-art methods, achieving significantly better rhythmic alignment and motion diversity",
    "checked": true,
    "id": "c9ef51df624e67247853d6ccf8e79512cf81556e",
    "semantic_title": "align your rhythm: generating highly aligned dance poses with gating-enhanced rhythm-aware feature representation",
    "citation_count": 0,
    "authors": [
      "Congyi Fan",
      "Jian Guan",
      "Xuanjia Zhao",
      "Dongli Xu",
      "Youtian Lin",
      "Tong Ye",
      "Pengming Feng",
      "Haiwei Pan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhu_Stable_Score_Distillation_ICCV_2025_paper.html": {
    "title": "Stable Score Distillation",
    "volume": "main",
    "abstract": "Text-guided image and 3D editing have advanced with diffusion-based models, yet methods like Delta Denoising Score often struggle with stability, spatial control, and editing strength. These limitations stem from reliance on complex auxiliary structures, which introduce conflicting optimization signals and restrict precise, localized edits. We introduce Stable Score Distillation (SSD), a streamlined framework that enhances stability and alignment in the editing process by anchoring a single classifier to the source prompt. Specifically, SSD utilizes Classifier-Free Guidance (CFG) equation to achieve cross-prompt alignment, and introduces a constant term null-text branch to stabilize the optimization process. This approach preserves the original content's structure and ensures that editing trajectories are closely aligned with the source prompt, enabling smooth, prompt-specific modifications while maintaining coherence in surrounding regions. Additionally, SSD incorporates a prompt enhancement branch to boost editing strength, particularly for style transformations. Our method achieves state-of-the-art results in 2D and 3D editing tasks, including NeRF and text-driven style edits, with faster convergence and reduced complexity, providing a robust and efficient solution for text-guided editing. Code is available at: https://github.com/Alex-Zhu1/SSD",
    "checked": true,
    "id": "517942551283b1a7a5bcbaa8c601cefdcea0ff3b",
    "semantic_title": "stable score distillation",
    "citation_count": 1,
    "authors": [
      "Haiming Zhu",
      "Yangyang Xu",
      "Chenshu Xu",
      "Tingrui Shen",
      "Wenxi Liu",
      "Yong Du",
      "Jun Yu",
      "Shengfeng He"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_UAVScenes_A_Multi-Modal_Dataset_for_UAVs_ICCV_2025_paper.html": {
    "title": "UAVScenes: A Multi-Modal Dataset for UAVs",
    "volume": "main",
    "abstract": "Multi-modal perception is essential for unmanned aerial vehicle (UAV) operations, as it enables a comprehensive understanding of the UAVs' surrounding environment. However, most existing multi-modal UAV datasets are primarily biased toward localization and 3D reconstruction tasks, or only support map-level semantic segmentation due to the lack of frame-wise annotations for both camera images and LiDAR point clouds. This limitation prevents them from being used for high-level scene understanding tasks. To address this gap and advance multi-modal UAV perception, we introduce UAVScenes, a large-scale dataset designed to benchmark various tasks across both 2D and 3D modalities. Our benchmark dataset is built upon the well-calibrated multi-modal UAV dataset MARS-LVIG, originally developed only for simultaneous localization and mapping (SLAM). We enhance this dataset by providing manually labeled semantic annotations for both frame-wise images and LiDAR point clouds, along with accurate 6-degree-of-freedom (6-DoF) poses. These additions enable a wide range of UAV perception tasks, including segmentation, depth estimation, 6-DoF localization, place recognition, and novel view synthesis (NVS). Our dataset is available at https://github.com/sijieaaa/UAVScenes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sijie Wang",
      "Siqi Li",
      "Yawei Zhang",
      "Shangshu Yu",
      "Shenghai Yuan",
      "Rui She",
      "Quanjiang Guo",
      "JinXuan Zheng",
      "Ong Kang Howe",
      "Leonrich Chandra",
      "Shrivarshann Srijeyan",
      "Aditya Sivadas",
      "Toshan Aggarwal",
      "Heyuan Liu",
      "Hongming Zhang",
      "Chujie Chen",
      "Junyu Jiang",
      "Lihua Xie",
      "Wee Peng Tay"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_FoundIR_Unleashing_Million-scale_Training_Data_to_Advance_Foundation_Models_for_ICCV_2025_paper.html": {
    "title": "FoundIR: Unleashing Million-scale Training Data to Advance Foundation Models for Image Restoration",
    "volume": "main",
    "abstract": "Despite the significant progress made by all-in-one models in universal image restoration, existing methods suffer from a generalization bottleneck in real-world scenarios, as they are mostly trained on small-scale synthetic datasets with limited degradations. Therefore, large-scale high-quality real-world training data is urgently needed to facilitate the emergence of foundation models for image restoration. To advance this field, we spare no effort in contributing a million-scale dataset with two notable advantages over existing training data: larger-scale real-world samples, and higher-diversity data types. By adjusting internal camera settings and external imaging conditions, we can capture aligned image pairs using our well-designed data acquisition system over multiple rounds and our data alignment criterion. Moreover, we propose a robust model, FoundIR, to better address a broader range of restoration tasks in real-world scenarios, taking a further step toward foundation models. Specifically, we first utilize a diffusion-based generalist model to remove degradations by learning the degradation-agnostic common representations from diverse inputs, where incremental learning strategy is adopted to better guide model training. To refine the model's restoration capability in complex scenarios, we introduce degradation-aware specialist models for achieving final high-quality results. Extensive experiments show the value of our dataset and the effectiveness of our method",
    "checked": true,
    "id": "010a6d61b5490216a0bb174a7a9f9d1565861ef3",
    "semantic_title": "foundir: unleashing million-scale training data to advance foundation models for image restoration",
    "citation_count": 7,
    "authors": [
      "Hao Li",
      "Xiang Chen",
      "Jiangxin Dong",
      "Jinhui Tang",
      "Jinshan Pan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tsai_LightsOut_Diffusion-based_Outpainting_for_Enhanced_Lens_Flare_Removal_ICCV_2025_paper.html": {
    "title": "LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal",
    "volume": "main",
    "abstract": "Lens flare significantly degrades image quality, impacting critical computer vision tasks like object detection and autonomous driving. Recent Single Image Flare Removal (SIFR) methods perform poorly when off-frame light sources are incomplete or absent. We propose LightsOut, a diffusion-based outpainting framework tailored to enhance SIFR by reconstructing off-frame light sources. Our method leverages a multitask regression module and LoRA fine-tuned diffusion model to ensure realistic and physically consistent outpainting results. Comprehensive experiments demonstrate LightsOut consistently boosts the performance of existing SIFR methods across challenging scenarios without additional retraining, serving as a universally applicable plug-and-play preprocessing solution",
    "checked": true,
    "id": "9327240e1e857baef6a602b9edd28b7bd3834061",
    "semantic_title": "lightsout: diffusion-based outpainting for enhanced lens flare removal",
    "citation_count": 0,
    "authors": [
      "Shr-Ruei Tsai",
      "Wei-Cheng Chang",
      "Jie-Ying Lee",
      "Chih-Hai Su",
      "Yu-Lun Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cao_TPG-INR_Target_Prior-Guided_Implicit_3D_CT_Reconstruction_for_Enhanced_Sparse-view_ICCV_2025_paper.html": {
    "title": "TPG-INR: Target Prior-Guided Implicit 3D CT Reconstruction for Enhanced Sparse-view Imaging",
    "volume": "main",
    "abstract": "X-ray imaging, based on penetration, enables detailed visualization of internal structures. Building on this capability, existing implicit 3D reconstruction methods have adapted the NeRF model and its variants for internal CT reconstruction. However, these approaches often neglect the significance of objects' anatomical priors for implicit learning, limiting both reconstruction precision and learning efficiency, particularly in ultra-sparse view scenarios. To address these challenges, we propose a novel 3D CT reconstruction framework that employs a 'target prior' derived from the object's projection data to enhance implicit learning. Our approach integrates positional and structural encoding to facilitate voxel-wise implicit reconstruction, utilizing the target prior to guide voxel sampling and enrich structural encoding. This dual strategy significantly boosts both learning efficiency and reconstruction quality. Additionally, we introduce a CUDA-based algorithm for rapid estimation of high-quality 3D target priors from sparse-view projections. Experiments utilizing projection data from a complex abdominal dataset demonstrate that the proposed model substantially enhances learning efficiency, outperforming the current leading model, NAF, by a factor of ten. In terms of reconstruction quality, it also exceeds the most accurate model, NeRP, achieving PSNR improvements of 3.57 dB, 5.42 dB, and 5.70 dB with 10, 20, and 30 projections, respectively. The code is available upon request",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qinglei Cao",
      "Ziyao Tang",
      "Xiaoqin Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jiao_From_Holistic_to_Localized_Local_Enhanced_Adapters_for_Efficient_Visual_ICCV_2025_paper.html": {
    "title": "From Holistic to Localized: Local Enhanced Adapters for Efficient Visual Instruction Fine-Tuning",
    "volume": "main",
    "abstract": "Efficient Visual Instruction Fine-Tuning (EVIT) seeks to adapt Multimodal Large Language Models (MLLMs) to downstream tasks with minimal computational overhead. However, as task diversity and complexity increase, EVIT faces significant challenges in resolving data conflicts. To address this limitation, we propose the Dual Low-Rank Adaptation (Dual-LoRA), a holistic-to-local framework that enhances the adapter's capacity to address data conflict through dual structural optimization. Specifically, we utilize two subspaces: a skill space for stable, holistic knowledge retention, and a rank-rectified task space that locally activates the holistic knowledge. Additionally, we introduce Visual Cue Enhancement (VCE), a multi-level local feature aggregation module designed to enrich the vision-language projection with local details. Our approach is both memory- and time-efficient, requiring only 1.16xthe inference time of the standard LoRA method (with injection into the query and value projection layers), and just 73% of the inference time of a 4-expert LoRA-MoE. Extensive experiments on various downstream tasks and general MLLM benchmarks validate the effectiveness of our proposed methods",
    "checked": true,
    "id": "d321b6ab190d32a1b65ecb76f259027b3ab37cf8",
    "semantic_title": "from holistic to localized: local enhanced adapters for efficient visual instruction fine-tuning",
    "citation_count": 0,
    "authors": [
      "Pengkun Jiao",
      "Bin Zhu",
      "Jingjing Chen",
      "Chong-Wah  Ngo",
      "Yu-Gang Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sun_Ouroboros_Single-step_Diffusion_Models_for_Cycle-consistent_Forward_and_Inverse_Rendering_ICCV_2025_paper.html": {
    "title": "Ouroboros: Single-step Diffusion Models for Cycle-consistent Forward and Inverse Rendering",
    "volume": "main",
    "abstract": "While multi-step diffusion models have advanced both forward and inverse rendering, existing approaches often treat these problems independently, leading to cycle inconsistency and slow inference speed. In this work, we present Ouroboros, a framework composed of two single-step diffusion models that handle forward and inverse rendering with mutual reinforcement. Our approach extends intrinsic decomposition to both indoor and outdoor scenes and introduces a cycle consistency mechanism that ensures coherence between forward and inverse rendering outputs. Experimental results demonstrate state-of-the-art performance across diverse scenes while achieving substantially faster inference speed compared to other diffusion-based methods. We also demonstrate that Ouroboros can transfer to video decomposition in a training-free manner, reducing temporal inconsistency in video sequences while maintaining high-quality per-frame inverse rendering",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shanlin Sun",
      "Yifan Wang",
      "Hanwen Zhang",
      "Yifeng Xiong",
      "Qin Ren",
      "Ruogu Fang",
      "Xiaohui Xie",
      "Chenyu You"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_Photolithography_Overlay_Map_Generation_with_Implicit_Knowledge_Distillation_Diffusion_Transformer_ICCV_2025_paper.html": {
    "title": "Photolithography Overlay Map Generation with Implicit Knowledge Distillation Diffusion Transformer",
    "volume": "main",
    "abstract": "This paper presents the Implicit Knowledge Distillation Diffusion Transformer (IKDDiT), a groundbreaking model tailored for photolithography overlay map generation in semiconductor manufacturing. IKDDiT effectively addresses the challenges of open-vocabulary overlay map generation by integrating pre-trained image-text encoders, diffusion models, and masked transformers. Utilizing advanced text-to-image diffusion and image-text discriminative models, it generates high-fidelity overlay maps across multiple photolithography layers, significantly mitigating overlay misregistration errors and minimizing productivity losses caused by wafer rework. Key innovations include an implicit knowledge distillation framework that refines inter-image alignment by decoupling discriminative and generative tasks via an implicit discriminator, as well as a gated cross-attention mechanism to enhance generative performance. Experimental results demonstrate that IKDDiT achieves an optimal trade-off between efficiency and accuracy, providing a scalable, robust solution poised to advance overlay map generation in semiconductor processes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan-Fu Yang",
      "Hsiu-Hui Hsiao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yoon_S4M_Boosting_Semi-Supervised_Instance_Segmentation_with_SAM_ICCV_2025_paper.html": {
    "title": "S4M: Boosting Semi-Supervised Instance Segmentation with SAM",
    "volume": "main",
    "abstract": "Semi-supervised instance segmentation poses challenges due to limited labeled data, causing difficulties in accurately localizing distinct object instances. Current teacher-student frameworks still suffer from performance constraints due to unreliable pseudo-label quality stemming from limited labeled data. While the Segment Anything Model (SAM) offers robust segmentation capabilities at various granularities, directly applying SAM introduces challenges such as class-agnostic predictions and potential over-segmentation. To address these complexities, we carefully integrate SAM into the semi-supervised instance segmentation framework, developing a novel distillation method that effectively captures the precise localization capabilities of SAM without compromising semantic recognition. Furthermore, we incorporate pseudo-label refinement as well as a specialized data augmentation with the refined pseudo-labels, resulting in superior performance. We establish state-of-the-art performance, and provide comprehensive experiments and ablation studies to validate the effectiveness of our proposed approach",
    "checked": false,
    "id": "d1c94c54f6c227c6c1a71eb503486df13146f118",
    "semantic_title": "s^4m: boosting semi-supervised instance segmentation with sam",
    "citation_count": 1,
    "authors": [
      "Heeji Yoon",
      "Heeseong Shin",
      "Eunbeen Hong",
      "Hyunwook Choi",
      "Hansang Cho",
      "Daun Jeong",
      "Seungryong Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Debiased_Curriculum_Adaptation_for_Safe_Transfer_Learning_in_Chest_X-ray_ICCV_2025_paper.html": {
    "title": "Debiased Curriculum Adaptation for Safe Transfer Learning in Chest X-ray Classification",
    "volume": "main",
    "abstract": "Chest X-ray classification is extensively utilized within the field of medical image analysis. However, manually labeling chest X-ray images is time-consuming and costly. Domain adaptation, which is designed to transfer knowledge from related domains, could offer a promising solution. Existing methods employ feature adaptation or self-training for knowledge transfer. Nonetheless, negative transfer is observed due to the entanglement of class imbalance and distribution shift in chest X-ray classification. In this paper, we propose Debiased Curriculum Adaptation framework to mitigate negative transfer in two aspects: (1) Curriculum Adaptation, which is designed to transfer knowledge in an easy-to-hard way, is proposed to alleviate confirmation bias in self-training. (2) Spectral Debiasing is introduced to harmonize the feature space between the source and target domains, as well as balance the feature space of positive and negative samples. Extensive experiments on 72 transfer tasks (including 6 diseases and 4 domains) demonstrate our superiority over state-of-the-art methods. In comparison to advanced methods, our approach effectively mitigates negative transfer, ensuring safe knowledge transfer",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingyang Liu",
      "Xinyang Chen",
      "Yang Shu",
      "Xiucheng Li",
      "Weili Guan",
      "Liqiang Nie"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_Diorama_Unleashing_Zero-shot_Single-view_3D_Indoor_Scene_Modeling_ICCV_2025_paper.html": {
    "title": "Diorama: Unleashing Zero-shot Single-view 3D Indoor Scene Modeling",
    "volume": "main",
    "abstract": "Reconstructing structured 3D scenes from RGB images using CAD objects unlocks efficient and compact scene representations that maintain compositionality and interactability. Existing works propose training-heavy methods relying on either expensive yet inaccurate real-world annotations or controllable yet monotonous synthetic data that do not generalize well to unseen objects or domains. We present Diorama, the first zero-shot open-world system that holistically models 3D scenes from single-view RGB observations without requiring end-to-end training or human annotations. We show the feasibility of our approach by decomposing the problem into subtasks and introduce better solutions to each: architecture reconstruction, 3D shape retrieval, object pose estimation, and scene layout optimization. We evaluate our system on both synthetic and real-world data to show we significantly outperform baselines from prior work. We also demonstrate generalization to real-world internet images and the text-to-scene task",
    "checked": true,
    "id": "61d161e19a693b0fb6d3b80559aedfd0d30ceeb4",
    "semantic_title": "diorama: unleashing zero-shot single-view 3d indoor scene modeling",
    "citation_count": 1,
    "authors": [
      "Qirui Wu",
      "Denys Iliash",
      "Daniel Ritchie",
      "Manolis Savva",
      "Angel X. Chang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Boosting_Multi-View_Indoor_3D_Object_Detection_via_Adaptive_3D_Volume_ICCV_2025_paper.html": {
    "title": "Boosting Multi-View Indoor 3D Object Detection via Adaptive 3D Volume Construction",
    "volume": "main",
    "abstract": "This work presents SGCDet, a novel multi-view indoor 3D object detection framework based on adaptive 3D volume construction. Unlike previous approaches that restrict the receptive field of voxels to fixed locations on images, we introduce a geometry and context aware aggregation module to integrate geometric and contextual information within adaptive regions in each image and dynamically adjust the contributions from different views, enhancing the representation capability of voxel features. Furthermore, we propose a sparse volume construction strategy that adaptively identifies and selects voxels with high occupancy probabilities for feature refinement, minimizing redundant computation in free space. Benefiting from the above designs, our framework achieves effective and efficient volume construction in an adaptive way. Better still, our network can be supervised using only 3D bounding boxes, eliminating the dependence on ground-truth scene geometry. Experimental results demonstrate that SGCDet achieves state-of-the-art performance on the ScanNet, ScanNet200 and ARKitScenes datasets. The source code is available at https://github.com/RM-Zhang/SGCDet",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runmin Zhang",
      "Zhu Yu",
      "Si-Yuan Cao",
      "Lingyu Zhu",
      "Guangyi Zhang",
      "Xiaokai Bai",
      "Hui-Liang Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_VQ-VLA_Improving_Vision-Language-Action_Models_via_Scaling_Vector-Quantized_Action_Tokenizers_ICCV_2025_paper.html": {
    "title": "VQ-VLA: Improving Vision-Language-Action Models via Scaling Vector-Quantized Action Tokenizers",
    "volume": "main",
    "abstract": "In this paper, we introduce an innovative vector quantization based action tokenizer built upon the largest-scale action trajectory dataset to date, leveraging over 100 times more data than previous approaches. This extensive dataset enables our tokenizer to capture rich spatiotemporal dynamics, resulting in a model that not only accelerates inference but also generates smoother and more coherent action outputs. Once trained, the tokenizer can be seamlessly adapted to a wide range of downstream tasks in a zero-shot manner, from short-horizon reactive behaviors to long-horizon planning. A key finding of our work is that the domain gap between synthetic and real action trajectories is marginal, allowing us to effectively utilize a vast amount of synthetic data during training without compromising real-world performance. To validate our approach, we conducted extensive experiments in both simulated environments and on real robotic platforms. The results demonstrate that as the volume of synthetic trajectory data increases, the performance of our tokenizer on downstream tasks improves significantly-most notably, achieving up to a 30% higher success rate on two real-world tasks in long-horizon scenarios. These findings highlight the potential of our action tokenizer as a robust and scalable solution for real-time embodied intelligence systems, paving the way for more efficient and reliable robotic control in diverse application domains",
    "checked": true,
    "id": "94d0f8898314cb06ab4f8a5c9fdadac2e4f0272c",
    "semantic_title": "vq-vla: improving vision-language-action models via scaling vector-quantized action tokenizers",
    "citation_count": 7,
    "authors": [
      "Yating Wang",
      "Haoyi Zhu",
      "Mingyu Liu",
      "Jiange Yang",
      "Hao-Shu Fang",
      "Tong He"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Quadratic_Gaussian_Splatting_High_Quality_Surface_Reconstruction_with_Second-order_Geometric_ICCV_2025_paper.html": {
    "title": "Quadratic Gaussian Splatting: High Quality Surface Reconstruction with Second-order Geometric Primitives",
    "volume": "main",
    "abstract": "We propose Quadratic Gaussian Splatting (QGS), a novel representation that replaces static primitives with deformable quadric surfaces (e.g., ellipse, paraboloids) to capture intricate geometry. Unlike prior works that rely on Euclidean distance for primitive density modeling--a metric misaligned with surface geometry under deformation--QGS introduces geodesic distance-based density distributions. This innovation ensures that density weights adapt intrinsically to the primitive curvature, preserving consistency during shape changes (e.g., from planar disks to curved paraboloids). By solving geodesic distances in closed form on quadric surfaces, QGS enables surface-aware splatting, where a single primitive can represent complex curvature that previously required dozens of planar surfels, potentially reducing memory usage while maintaining efficient rendering via fast ray-quadric intersection. Experiments on DTU, Tanks and Temples, and MipNeRF360 datasets demonstrate state-of-the-art surface reconstruction, with QGS reducing geometric error (chamfer distance) by 33% over 2DGS and 27% over GOF on the DTU dataset. Crucially, QGS retains competitive appearance quality, bridging the gap between geometric precision and visual fidelity for applications like robotics and immersive reality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyu Zhang",
      "Binbin Huang",
      "Hanqing Jiang",
      "Liyang Zhou",
      "Xiaojun Xiang",
      "Shuhan Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ahn_UDC-VIT_A_Real-World_Video_Dataset_for_Under-Display_Cameras_ICCV_2025_paper.html": {
    "title": "UDC-VIT: A Real-World Video Dataset for Under-Display Cameras",
    "volume": "main",
    "abstract": "Even though an Under-Display Camera (UDC) is an advanced imaging system, the display panel significantly degrades captured images or videos, introducing low transmittance, blur, noise, and flare issues. Tackling such issues is challenging because of the complex degradation of UDCs, including diverse flare patterns. However, no dataset contains videos of real-world UDC degradation. In this paper, we propose a real-world UDC video dataset called UDC-VIT. Unlike existing datasets, UDC-VIT exclusively includes human motions for facial recognition. We propose a video-capturing system to acquire clean and UDC-degraded videos of the same scene simultaneously. Then, we align a pair of captured videos frame by frame, using discrete Fourier transform (DFT). We compare UDC-VIT with six representative UDC still image datasets and two existing UDC video datasets. Using six deep-learning models, we compare UDC-VIT and an existing synthetic UDC video dataset. The results indicate the ineffectiveness of models trained on earlier synthetic UDC video datasets, as they do not reflect the actual characteristics of UDC-degraded videos. We also demonstrate the importance of effective UDC restoration by evaluating face recognition accuracy concerning PSNR, SSIM, and LPIPS scores. UDC-VIT is available at our official GitHub repository",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kyusu Ahn",
      "JiSoo Kim",
      "Sangik Lee",
      "HyunGyu Lee",
      "Byeonghyun Ko",
      "Chanwoo Park",
      "Jaejin Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Prompt_Guidance_and_Human_Proximal_Perception_for_HOT_Prediction_with_ICCV_2025_paper.html": {
    "title": "Prompt Guidance and Human Proximal Perception for HOT Prediction with Regional Joint Loss",
    "volume": "main",
    "abstract": "The task of Human-Object conTact (HOT) detection involves identifying the specific areas of the human body that are touching objects. Nevertheless, current models are restricted to just one type of image, often leading to too much segmentation in areas with little interaction, and struggling to maintain category consistency within specific regions. To tackle this issue, a HOT framework, termed P3HOT, is proposed, which blends Prompt guidance and human Proximal Perception. To begin with, we utilize a semantic-driven prompt mechanism to direct the network's attention towards the relevant regions based on the correlation between image and text. Then a human proximal perception mechanism is employed to dynamically perceive key depth range around the human, using learnable parameters to effectively eliminate regions where interactions are not expected. Calculating depth resolves the uncertainty of the overlap between humans and objects in a 2D perspective, providing a quasi-3D viewpoint. Moreover, a Regional Joint Loss (RJLoss) has been created as a new loss to inhibit abnormal categories in the same area. A new evaluation metric called AD-Acc. is introduced to address the shortcomings of existing methods in addressing negative samples. Comprehensive experimental results demonstrate that our approach achieves state-of-the-art performance in four metrics across two benchmark datasets. Specifically, our model achieves an improvement of 0.7, 2.0, 1.6, and 11.0 in SC-Acc., mIoU, wIoU, and AD-Acc. metrics, respectively, on the HOT-Annotated dataset. The sources code are available at https://github.com/YuxiaoWang-AI/P3HOT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxiao Wang",
      "Yu Lei",
      "Zhenao Wei",
      "Weiying Xue",
      "Xinyu Jiang",
      "Nan Zhuang",
      "Qi Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ying_Towards_Omnimodal_Expressions_and_Reasoning_in_Referring_Audio-Visual_Segmentation_ICCV_2025_paper.html": {
    "title": "Towards Omnimodal Expressions and Reasoning in Referring Audio-Visual Segmentation",
    "volume": "main",
    "abstract": "Referring audio-visual segmentation (RAVS) has recently seen significant advancements, yet challenges remain in integrating multimodal information and deeply understanding and reasoning about audiovisual content. To extend the boundaries of RAVS and facilitate future research in this field, we propose Omnimodal Referring Audio-Visual Segmentation (OmniAVS), a new dataset containing 2,104 videos and 61,095 multimodal referring expressions. OmniAVS stands out with three key innovations: (1) 8 types of multimodal expressions that flexibly combine text, speech, sound, and visual cues; (2) an emphasis on understanding audio content beyond just detecting their presence; and (3) the inclusion of complex reasoning and world knowledge in expressions. Furthermore, we introduce Omnimodal Instructed Segmentation Assistant (OISA), to address the challenges of multimodal reasoning and fine-grained understanding of audiovisual content in OmniAVS. OISA uses MLLM to comprehend complex cues and perform reasoning-based segmentation. Extensive experiments show that OISA outperforms existing methods on OmniAVS and achieves competitive results on other related tasks",
    "checked": true,
    "id": "7906c35e32ab4cf217fe24b5315c9849de106417",
    "semantic_title": "towards omnimodal expressions and reasoning in referring audio-visual segmentation",
    "citation_count": 4,
    "authors": [
      "Kaining Ying",
      "Henghui Ding",
      "Guangquan Jie",
      "Yu-Gang Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Gait-X_Exploring_X_modality_for_Generalized_Gait_Recognition_ICCV_2025_paper.html": {
    "title": "Gait-X: Exploring X modality for Generalized Gait Recognition",
    "volume": "main",
    "abstract": "Modality exploration has been repeatedly mentioned in gait recognition, evolving from silhouette to parsing, mesh, point clouds, etc. These latest modalities agree that silhouette is less affected by background and clothing noises, but argue it loses too much valuable discriminative information. They seek to retain the strengths of silhouette while extracting more semantic or structural information through upstream estimation for better recognition. We agree with this principle but argue that these upstream estimations are usually unstable and the resulted modalities rely on pre-defined design. Moreover, the crucial aspect of modality generalization remains underexplored. To address this, we propose Gait-X to explore how to flexibly and stably develop a gait-specific generalized X modality from a frequency perspective. Specifically, 1) We replace upstream estimation with stable frequency decomposition and conduct a comprehensive analysis of how different frequencies impact modality and within-/cross-domain performance; 2) To enable flexible modality customization and mitigate the influence of noise and domain variations, we propose to remove irrelevant low-frequency noise and suppress high-frequency domain-specific information to form our X modality; 3) To further improve model generalization, we expand the representation across multiple frequencies to guide the model in balancing whole frequencies for enhanced generalization. Extensive experiments on CCPG, SUSTech1K, and CASIA-B datasets show superior generalization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zengbin Wang",
      "Saihui Hou",
      "Junjie Li",
      "Xu Liu",
      "Chunshui Cao",
      "Yongzhen Huang",
      "Siye Wang",
      "Man Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shi_Learning_Separable_Fine-Grained_Representation_via_Dendrogram_Construction_from_Coarse_Labels_ICCV_2025_paper.html": {
    "title": "Learning Separable Fine-Grained Representation via Dendrogram Construction from Coarse Labels for Fine-grained Visual Recognition",
    "volume": "main",
    "abstract": "Learning fine-grained representations from coarse labels for fine-grained visual recognition (FGVR) is a challenging yet valuable task, as it alleviates the reliance on labor-intensive fine-grained annotations. Early approaches focused primarily on minimizing intra-fine-grained-class variation but overlooked inter-fine-grained-class separability, resulting in limited FGVR performance. Subsequent studies employed a top-down paradigm to enhance separability via deep clustering, yet these methods require predefining the number of fine-grained classes, which is often impractical to obtain. Here, we introduce a bottom-up learning paradigm that constructs a hierarchical dendrogram by iteratively merging similar instances/clusters, inferring higher-level semantics from lowest-level instances without predefining class numbers. Leveraging this, we propose BuCSFR, a novel method that integrates a Bottom-up Construction (BuC) module to build the dendrogram based on a minimal information loss criterion, and a Separable Fine-grained Representation (SFR) module that treats dendrogram nodes as pseudo-labels to ensure representation separability. The synergistic interaction between these modules enables iterative enhancement, grounded theoretically in the Expectation-Maximization (EM) framework. Extensive experiments on five benchmark datasets demonstrate the superiority of our approach, showcasing its effectiveness in learning separable representations for FGVR. The source code is available at: https://github.com/BeCarefulOfYournaoke/BuCSFR",
    "checked": false,
    "id": "0abd728314fc39be766d4fc01cf0a265b2cdf284",
    "semantic_title": "vaemo: efficient representation learning for visual-audio emotion with knowledge injection",
    "citation_count": 1,
    "authors": [
      "Guanghui Shi",
      "Xuefeng Liang",
      "Wenjie Li",
      "Xiaoyu Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hu_DyGS-SLAM_Real-Time_Accurate_Localization_and_Gaussian_Reconstruction_for_Dynamic_Scenes_ICCV_2025_paper.html": {
    "title": "DyGS-SLAM: Real-Time Accurate Localization and Gaussian Reconstruction for Dynamic Scenes",
    "volume": "main",
    "abstract": "In dynamic scenes, achieving accurate camera localization and reconstructing a long-term consistent map containing only the static background are two major challenges faced by Visual Simultaneous Localization and Mapping (VSLAM). In current traditional dynamic VSLAM systems, the methods used to handle dynamic objects are primarily designed for localization; if applied to reconstruction, they are prone to introducing motion artifacts. Meanwhile, mask compensation strategies in NeRF- or 3DGS-based dynamic VSLAM systems also face challenges, such as the inability to completely eliminate dynamic object artifacts and low real-time performance. To address these issues, we leverage object detection to extract semantic information and propose a dynamic feature detection algorithm based on both geometry and appearance. This algorithm accurately identifies known and unknown moving objects and determines their actual motion states. To mitigate the issue of insufficient detection box coverage, we design a dynamic object box correction algorithm based on clustering and Gaussian mixture models to comprehensively identify moving object regions. Furthermore, to overcome the limitations of sparse features in texture-scarce environments, we introduce a feature densification strategy based on image texture complexity, enhancing reconstruction quality while maintaining real-time performance. Extensive experimental evaluations demonstrate that our system achieves state-of-the-art localization and reconstruction performance in dynamic scenes and can run in real time on resource-constrained devices",
    "checked": false,
    "id": "ad2aee2acf7f1ae447ec5cb95cf1c19a58aa9d40",
    "semantic_title": "monogs++: fast and accurate monocular rgb gaussian slam",
    "citation_count": 3,
    "authors": [
      "Xinggang Hu",
      "Chenyangguang Zhang",
      "Mingyuan Zhao",
      "Yuanze Gui",
      "Xiangkui Zhang",
      "Xiangyang Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_D3QE_Learning_Discrete_Distribution_Discrepancy-aware_Quantization_Error_for_Autoregressive-Generated_Image_ICCV_2025_paper.html": {
    "title": "D3QE: Learning Discrete Distribution Discrepancy-aware Quantization Error for Autoregressive-Generated Image Detection",
    "volume": "main",
    "abstract": "The emergence of visual autoregressive (AR) models has revolutionized image generation while presenting new challenges for synthetic image detection. Unlike previous GAN or diffusion-based methods, AR models generate images through discrete token prediction, exhibiting both marked improvements in image synthesis quality and unique characteristics in their vector-quantized representations. In this paper, we propose to leverage Discrete Distribution Discrepancy-aware Quantization Error (D^3QE) for autoregressive-generated image detection that exploits the distinctive patterns and the frequency distribution bias of the codebook existing in real and fake images. We introduce a discrete distribution discrepancy-aware transformer that integrates dynamic codebook frequency statistics into its attention mechanism, fusing semantic features and quantization error latent. To evaluate our method, we construct a comprehensive dataset termed ARForensics covering 7 mainstream visual AR models. Experiments demonstrate superior detection accuracy and strong generalization of D^3QE across different AR models, with robustness to real-world perturbations. Code is available at \\href https://github.com/Zhangyr2022/D3QE https://github.com/Zhangyr2022/D3QE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanran Zhang",
      "Bingyao Yu",
      "Yu Zheng",
      "Wenzhao Zheng",
      "Yueqi Duan",
      "Lei Chen",
      "Jie Zhou",
      "Jiwen Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tan_Towards_a_Universal_3D_Medical_Multi-modality_Generalization_via_Learning_Personalized_ICCV_2025_paper.html": {
    "title": "Towards a Universal 3D Medical Multi-modality Generalization via Learning Personalized Invariant Representation",
    "volume": "main",
    "abstract": "Variations in medical imaging modalities and individual anatomical differences pose challenges to cross-modality generalization in multi-modal tasks. Existing methods often concentrate exclusively on common anatomical patterns, thereby neglecting individual differences and consequently limiting their generalization performance. This paper emphasizes the critical role of learning individual-level invariance, i.e., personalized representation \\mathbb X _h, to enhance multi-modality generalization under both homogeneous and heterogeneous settings.It reveals that mappings from individual anatomy to different medical modalities remain static across the population, which is implied in the personalization process.We propose a two-stage approach: pre-training with invariant representation \\mathbb X _h for personalization, then fine-tuning for diverse downstream tasks.We provide both theoretical and empirical evidence demonstrating the feasibility and advantages of personalization, showing that our approach yields greater generalizability and transferability across diverse multi-modal medical tasks compared to methods lacking personalization. Extensive experiments further validate that our approach significantly enhances performance in various generalization scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaorui Tan",
      "Xi Yang",
      "Tan Pan",
      "Tianyi Liu",
      "Chen Jiang",
      "Xin Guo",
      "Qiufeng Wang",
      "Anh Nguyen",
      "Yuan Qi",
      "Kaizhu Huang",
      "Yuan Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sun_StrandHead_Text_to_Hair-Disentangled_3D_Head_Avatars_Using_Human-Centric_Priors_ICCV_2025_paper.html": {
    "title": "StrandHead: Text to Hair-Disentangled 3D Head Avatars Using Human-Centric Priors",
    "volume": "main",
    "abstract": "While haircut indicates distinct personality, existing avatar generation methods fail to model practical hair due to the data limitation or entangled representation. We propose StrandHead, a novel text-driven method capable of generating 3D hair strands and disentangled head avatars with strand-level attributes. Instead of using large-scale hair-text paired data for supervision, we demonstrate that realistic hair strands can be generated from prompts by distilling 2D generative models pre-trained on human mesh data. To this end, we propose a meshing approach guided by strand geometry to guarantee the gradient flow from the distillation objective to the neural strand representation. The optimization is then regularized by statistically significant haircut features, leading to stable updating of strands against unreasonable drifting. These employed 2D/3D human-centric priors contribute to text-aligned and realistic 3D strand generation. Extensive experiments show that StrandHead achieves the state-of-the-art performance on text to strand generation and disentangled 3D head avatar modeling. The generated 3D hair can be applied on avatars for strand-level editing, as well as implemented in the graphics engine for physical simulation or other applications. Project page: https://xiaokunsun.github.io/StrandHead.github.io/",
    "checked": true,
    "id": "73d18bd6e140185d949210f3435719ee72d59f55",
    "semantic_title": "strandhead: text to hair-disentangled 3d head avatars using human-centric priors",
    "citation_count": 1,
    "authors": [
      "Xiaokun Sun",
      "Zeyu Cai",
      "Ying Tai",
      "Jian Yang",
      "Zhenyu Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cheng_MCAM_Multimodal_Causal_Analysis_Model_for_Ego-Vehicle-Level_Driving_Video_Understanding_ICCV_2025_paper.html": {
    "title": "MCAM: Multimodal Causal Analysis Model for Ego-Vehicle-Level Driving Video Understanding",
    "volume": "main",
    "abstract": "Accurate driving behavior recognition and reasoning are critical for autonomous driving video understanding. However, existing methods often tend to dig out the shallow causal, fail to address spurious correlations across modalities, and ignore the ego-vehicle level causality modeling. To overcome these limitations, we propose a novel Multimodal Causal Analysis Model (MCAM) that constructs latent causal structures between visual and language modalities. Firstly, we design a multi-level feature extractor to capture long-range dependencies. Secondly, we design a causal analysis module that dynamically models driving scenarios using a directed acyclic graph (DAG) of driving states. Thirdly, we utilize a vision-language transformer to align critical visual features with their corresponding linguistic expressions. Extensive experiments on the BDD-X, and CoVLA datasets demonstrate that MCAM achieves SOTA performance in visual-language causal relationship learning. Furthermore, the model exhibits superior capability in capturing causal characteristics within video sequences, showcasing its effectiveness for autonomous driving applications. The code is available at https://github.com/SixCorePeach/MCAM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tongtong Cheng",
      "Rongzhen  Li",
      "Yixin  Xiong",
      "Tao  Zhang",
      "Jing  Wang",
      "Kai Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Mi_Adversarial_Robust_Memory-Based_Continual_Learner_ICCV_2025_paper.html": {
    "title": "Adversarial Robust Memory-Based Continual Learner",
    "volume": "main",
    "abstract": "Despite the remarkable advances that have been made in continual learning, the adversarial vulnerability of such methods has not been fully discussed. We delve into the adversarial robustness of memory-based continual learning algorithms and observe limited robustness improvement by directly applying adversarial training techniques. Our preliminary studies reveal the twin challenges for building adversarial robust continual learners: accelerated forgetting in continual learning and gradient obfuscation in adversarial robustness. In this study, we put forward a novel adversarial robust memory-based continual learner that adjusts data logits to mitigate the forgetting of pasts caused by adversarial samples. Furthermore, we devise a gradient-based data selection mechanism to overcome the gradient obfuscation caused by limited stored data. The proposed approach can widely integrate with existing memory-based continual learning and adversarial training algorithms in a plug-and-play way. Extensive experiments on Split-CIFAR10/100 and Split-Tiny-ImageNet demonstrate the effectiveness of our approach, achieving a maximum forgetting reduction of 34.17% in adversarial data for ResNet, and 20.10% for ViT",
    "checked": true,
    "id": "25124fe761c9bdec94d85e535929ee754b882227",
    "semantic_title": "adversarial robust memory-based continual learner",
    "citation_count": 5,
    "authors": [
      "Xiaoyue Mi",
      "Fan Tang",
      "Zonghan Yang",
      "Danding Wang",
      "Juan Cao",
      "Peng Li",
      "Yang Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gao_SurfaceSplat_Connecting_Surface_Reconstruction_and_Gaussian_Splatting_ICCV_2025_paper.html": {
    "title": "SurfaceSplat: Connecting Surface Reconstruction and Gaussian Splatting",
    "volume": "main",
    "abstract": "Surface reconstruction and novel view rendering from sparse-view images are challenging. Signed Distance Function (SDF)-based methods struggle with fine details, while 3D Gaussian Splatting (3DGS)-based approaches lack global geometry coherence. We propose a novel hybrid method that combines both strengths: SDF captures coarse geometry to enhance 3DGS-based rendering, while newly rendered images from 3DGS refine SDF details for accurate surface reconstruction. As a result, our method surpasses state-of-the-art approaches in surface reconstruction and novel view synthesis on DTU and MobileBrick datasets. Code will be released at https://github.com/aim-uofa/SurfaceSplat",
    "checked": true,
    "id": "a5025b5e4597d45dde76aa64952c8655de1048b0",
    "semantic_title": "surfacesplat: connecting surface reconstruction and gaussian splatting",
    "citation_count": 1,
    "authors": [
      "Zihui Gao",
      "Jia-Wang Bian",
      "Guosheng Lin",
      "Hao Chen",
      "Chunhua Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_TransiT_Transient_Transformer_for_Non-line-of-sight_Videography_ICCV_2025_paper.html": {
    "title": "TransiT: Transient Transformer for Non-line-of-sight Videography",
    "volume": "main",
    "abstract": "High quality and high speed videography using Non-Line-of-Sight (NLOS) imaging benefit autonomous navigation, collision prevention, and post-disaster search and rescue tasks. Current solutions have to balance between the frame rate and image quality. High frame rates, for example, can be achieved by reducing either per-point scanning time or scanning density, but at the cost of lowering the information density at individual frames. Fast scanning process further reduces the signal-to-noise ratio and different scanning systems exhibit different distortion characteristics. In this work, we design and employ a new Transient Transformer architecture called TransiT to achieve real-time NLOS recovery under fast scans. TransiT directly compresses the temporal dimension of input transients to extract features, reducing computation costs and meeting high frame rate requirements. It further adopts a feature fusion mechanism as well as employs a spatial-temporal Transformer to help capture features of NLOS transient videos. Moreover, TransiT applies transfer learning to bridge the gap between synthetic and real-measured data. In real experiments, TransiT manages to reconstruct from sparse transients of 16 x16 measured at an exposure time of 0.4 ms per point to NLOS videos at a 64 x64 resolution at 10 frames per second. We will make our code and dataset available to the community",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruiqian Li",
      "Siyuan Shen",
      "Suan Xia",
      "Ziheng Wang",
      "Xingyue Peng",
      "Chengxuan Song",
      "Yingsheng Zhu",
      "Tao Wu",
      "Shiying Li",
      "Jingyi Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xu_Penalizing_Boundary_Activation_for_Object_Completeness_in_Diffusion_Models_ICCV_2025_paper.html": {
    "title": "Penalizing Boundary Activation for Object Completeness in Diffusion Models",
    "volume": "main",
    "abstract": "Diffusion models have emerged as a powerful technique for text-to-image (T2I) generation, creating high-quality, diverse images across various domains. However, a common limitation in these models is the incomplete display of objects, where fragments or missing parts can undermine the model's performance in downstream applications such as dataset synthesis and video generation using 2D prior-based models. % that demand visual accuracy, such as e-commerce product imaging and realistic digital content creation.In this study, we conduct the in-depth analysis of this issue and reveal that the primary culprit behind incomplete object generation is RandomCrop. This data augmentation method, widely used in diffusion models, though enhances model generalization ability, disrupts object continuity during training. To address this, we propose a training-free solution that penalizes activation values occurring at image boundaries during the early denoising steps. Our method is easily applicable to pre-trained Stable Diffusion models with minimal modifications and negligible computational overhead. Extensive experiments demonstrate the effectiveness of our method, showing substantial improvements in object integrity and image quality",
    "checked": true,
    "id": "40908b3e251fde24b92cbcb7aefda1f7cea0d026",
    "semantic_title": "penalizing boundary activation for object completeness in diffusion models",
    "citation_count": 0,
    "authors": [
      "Haoyang Xu",
      "Tianhao Zhao",
      "Sibei Yang",
      "Yutian Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lee_DMQ_Dissecting_Outliers_of_Diffusion_Models_for_Post-Training_Quantization_ICCV_2025_paper.html": {
    "title": "DMQ: Dissecting Outliers of Diffusion Models for Post-Training Quantization",
    "volume": "main",
    "abstract": "Diffusion models have achieved remarkable success in image generation but come with significant computational costs, posing challenges for deployment in resource-constrained environments. Recent post-training quantization (PTQ) methods have attempted to mitigate this issue by focusing on the iterative nature of diffusion models. However, these approaches often overlook outliers, leading to degraded performance at low bit-widths. In this paper, we propose a DMQ which combines Learned Equivalent Scaling (LES) and channel-wise Power-of-Two Scaling (PTS) to effectively address these challenges. Learned Equivalent Scaling optimizes channel-wise scaling factors to redistribute quantization difficulty between weights and activations, reducing overall quantization error. Recognizing that early denoising steps, despite having small quantization errors, crucially impact the final output due to error accumulation, we incorporate an adaptive timestep weighting scheme to prioritize these critical steps during learning. Furthermore, identifying that layers such as skip connections exhibit high inter-channel variance, we introduce channel-wise Power-of-Two Scaling for activations. To ensure robust selection of PTS factors even with small calibration set, we introduce a voting algorithm that enhances reliability. Extensive experiments demonstrate that our method significantly outperforms existing works, especially at low bit-widths such as W4A6 (4-bit weight, 6-bit activation) and W4A8, maintaining high image generation quality and model stability. The code is available at https://github.com/LeeDongYeun/dmq",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongyeun Lee",
      "Jiwan Hur",
      "Hyounguk Shon",
      "Jae Young Lee",
      "Junmo Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shen_QK-Edit_Revisiting_Attention-based_Injection_in_MM-DiT_for_Image_and_Video_ICCV_2025_paper.html": {
    "title": "QK-Edit: Revisiting Attention-based Injection in MM-DiT for Image and Video Editing",
    "volume": "main",
    "abstract": "Multimodal Diffusion Transformers (MM-DiTs) have recently emerged as a powerful framework for unified text-vision synthesis, surpassing traditional U-Net architectures in generative tasks. One key innovation lies in its Multimodal Self-Attention (MM-SA) interaction where image and text tokens are concatenated and processed via self-attention.However, this mechanism poses significant challenges for editing, rendering conventional U-Net-based attention manipulation methods ineffective. To address this limitation, we propose QK-Edit, a training-free framework that exploits the unique attention dynamics of MM-DiTs for precise text-guided image and video editing. By introducing a novel query-key manipulation strategy, our method isolates and adjusts critical attention components to achieve an optimal balance between prompt fidelity and structural consistency. This enables seamless edits across various tasks, including object addition, object removal, object replacement, changing background, changing material, changing color, and style transformation. Notably, it can be easily implemented with feature replacement in inference.QK-Edit demonstrates superior editing performance on state-of-the-art models, such as FLUX and HunyuanVideo, effectively bridging the gap between generative power and editable flexibility in MM-DiTs, and paving the way for scalable multimodal content creation",
    "checked": true,
    "id": "0582e1b660ed063de19ef98260418d5a7f2fc2a7",
    "semantic_title": "qk-edit: revisiting attention-based injection in mm-dit for image and video editing",
    "citation_count": 0,
    "authors": [
      "Tiancheng Shen",
      "Zilong Huang",
      "Xiangtai Li",
      "Zhijie Lin",
      "Jiyang Liu",
      "Yitong Wang",
      "Jiashi Feng",
      "Ming-Hsuan Yang",
      "Jun Hao Liew"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Deng_Visual_Chronicles_Using_Multimodal_LLMs_to_Analyze_Massive_Collections_of_ICCV_2025_paper.html": {
    "title": "Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections of Images",
    "volume": "main",
    "abstract": "We present a system using Multimodal LLMs (MLLMs) to analyze a large database with tens of millions of images captured at different times, with the aim of discovering patterns in temporal changes. Specifically, we aim to capture frequent co-occurring changes (\"trends\") across a city over a certain period. Unlike previous visual analyses, our analysis answers open-ended queries (e.g., \"what are the frequent types of changes in the city?\") without any predetermined target subjects or training labels. These properties cast prior learning-based or unsupervised visual analysis tools unsuitable. We identify MLLMs as a novel tool for their open-ended semantic understanding capabilities. Yet, our datasets are four orders of magnitude too large for an MLLM to ingest as context. So we introduce a bottom-up procedure that decomposes the massive visual analysis problem into more tractable sub-problems. We carefully design MLLM-based solutions to each sub-problem. During experiments and ablation studies with our system, we find it significantly outperforms baselines and is able to discover interesting trends from images captured in large cities (e.g., \"addition of outdoor dining,\", \"overpass was painted blue,\" etc.)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boyang Deng",
      "Songyou Peng",
      "Kyle Genova",
      "Gordon Wetzstein",
      "Noah Snavely",
      "Leonidas Guibas",
      "Thomas Funkhouser"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Stotko_SAFT_Shape_and_Appearance_of_Fabrics_from_Template_via_Differentiable_ICCV_2025_paper.html": {
    "title": "SAFT: Shape and Appearance of Fabrics from Template via Differentiable Physical Simulations from Monocular Video",
    "volume": "main",
    "abstract": "The reconstruction of three-dimensional dynamic scenes is a well-established yet challenging task within the domain of computer vision. In this paper, we propose a novel approach that combines the domains of 3D geometry reconstruction and appearance estimation for physically based rendering and present a system that is able to perform both tasks for fabrics, utilizing only a single monocular RGB video sequence as input. In order to obtain realistic and high-quality deformations and renderings, a physical simulation of the cloth geometry and differentiable rendering are employed. In this paper, we introduce two novel regularization terms for the 3D reconstruction task that improve the plausibility of the reconstruction by addressing the depth ambiguity problem in monocular video. In comparison with the most recent methods in the field, we have reduced the error in the 3D reconstruction by a factor of 2.64 while requiring a medium runtime of 30 min per scene. Furthermore, the optimized motion achieves sufficient quality to perform an appearance estimation of the deforming object, recovering sharp details from this single monocular RGB video",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Stotko",
      "Reinhard Klein"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gu_Gradient_Short-Circuit_Efficient_Out-of-Distribution_Detection_via_Feature_Intervention_ICCV_2025_paper.html": {
    "title": "Gradient Short-Circuit: Efficient Out-of-Distribution Detection via Feature Intervention",
    "volume": "main",
    "abstract": "Out-of-Distribution (OOD) detection is critical for safely deploying deep models in open-world environments, where inputs may lie outside the training distribution. During inference on a model trained exclusively with In-Distribution (ID) data, we observe a salient gradient phenomenon: around an ID sample, the local gradient directions for \"enhancing\" that sample's predicted class remain relatively consistent, whereas OOD samples--unseen in training--exhibit disorganized or conflicting gradient directions in the same neighborhood. Motivated by this observation, we propose an inference-stage technique to short-circuit those feature coordinates that spurious gradients exploit to inflate OOD confidence, while leaving ID classification largely intact. To circumvent the expense of recomputing the logits after this gradient short-circuit, we further introduce a local first-order approximation that accurately captures the post-modification outputs without a second forward pass. Experiments on standard OOD benchmarks show our approach yields substantial improvements. Moreover, the method is lightweight and requires minimal changes to the standard inference pipeline, offering a practical path toward robust OOD detection in real-world applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawei Gu",
      "Ziyue Qiao",
      "Zechao Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xu_AD-GS_Object-Aware_B-Spline_Gaussian_Splatting_for_Self-Supervised_Autonomous_Driving_ICCV_2025_paper.html": {
    "title": "AD-GS: Object-Aware B-Spline Gaussian Splatting for Self-Supervised Autonomous Driving",
    "volume": "main",
    "abstract": "Modeling and rendering dynamic urban driving scenes is crucial for self-driving simulation. Current high-quality methods typically rely on costly manual object tracklet annotations, while self-supervised approaches fail to capture dynamic object motions accurately and decompose scenes properly, resulting in rendering artifacts. We introduce AD-GS, a novel self-supervised framework for high-quality free-viewpoint rendering of driving scenes from a single log. At its core is a novel learnable motion model that integrates locality-aware B-spline curves with global-aware trigonometric functions, enabling flexible yet precise dynamic object modeling. Rather than requiring comprehensive semantic labeling, AD-GS automatically segments scenes into objects and background with the simplified pseudo 2D segmentation, representing objects using dynamic Gaussians and bidirectional temporal visibility masks. Further, our model incorporates visibility reasoning and physically rigid regularization to enhance robustness. Extensive evaluations demonstrate that our annotation-free model significantly outperforms current state-of-the-art annotation-free methods and is competitive with annotation-dependent approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawei Xu",
      "Kai Deng",
      "Zexin Fan",
      "Shenlong Wang",
      "Jin Xie",
      "Jian Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_HRScene_How_Far_Are_VLMs_from_Effective_High-Resolution_Image_Understanding_ICCV_2025_paper.html": {
    "title": "HRScene: How Far Are VLMs from Effective High-Resolution Image Understanding?",
    "volume": "main",
    "abstract": "High-resolution image (HRI) understanding aims to process images with a large number of pixels, such as pathological images and agricultural aerial images, both of which can exceed 1 million pixels. Vision Large Language Models (VLMs) can allegedly handle HRIs, however, there is a lack of a comprehensive benchmark for VLMs to evaluate HRI understanding. To address this gap, we introduce HRScene, a novel unified benchmark for HRI understanding with rich scenes. HRScene incorporates 25 real-world datasets and 2 synthetic diagnostic datasets with resolutions ranging from 1,024 times 1,024 to 35,503 times 26,627. HRScene is collected and re-annotated by 10 graduate-level annotators, covering 25 scenarios, ranging from microscopic to radiology images, street views, long-range pictures, and telescope images. It includes HRIs of real-world objects, scanned documents, and composite multi-image. The two diagnostic evaluation datasets are synthesized by combining the target image with the gold answer and distracting images in different orders, assessing how well models utilize regions in HRI. We conduct extensive experiments involving 28 VLMs, including Gemini 2.0 Flash and GPT-4o. Experiments on HRScene show that current VLMs achieve an average accuracy of around 50% on real-world tasks, revealing significant gaps in HRI understanding. Results on synthetic datasets reveal that VLMs struggle to effectively utilize HRI regions, showing significant Regional Divergence and lost-in-middle, shedding light on future research",
    "checked": true,
    "id": "fd337dc4cb83330ae9d7593127eb899453335596",
    "semantic_title": "hrscene: how far are vlms from effective high-resolution image understanding?",
    "citation_count": 0,
    "authors": [
      "Yusen Zhang",
      "Wenliang Zheng",
      "Aashrith Madasu",
      "Peng Shi",
      "Ryo Kamoi",
      "Hao Zhou",
      "Zhuoyang Zou",
      "Shu Zhao",
      "Sarkar Snigdha Sarathi Das",
      "Vipul Gupta",
      "Xiaoxin Lu",
      "Nan Zhang",
      "Ranran Haoran Zhang",
      "Avitej Iyer",
      "Renze Lou",
      "Wenpeng Yin",
      "Rui Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tong_Decoding_Correlation-Induced_Misalignment_in_the_Stable_Diffusion_Workflow_for_Text-to-Image_ICCV_2025_paper.html": {
    "title": "Decoding Correlation-Induced Misalignment in the Stable Diffusion Workflow for Text-to-Image Generation",
    "volume": "main",
    "abstract": "The fundamental requirement for text-to-image generation is aligning the generated images with the provided text. With large-scale data, pre-trained Stable Diffusion (SD) models have achieved remarkable performance in this task. These models process an input prompt as text control, guiding a vision model to perform denoising operations that recover a clean image from pure noise. However, we observe that when there is correlation among text tokens, SD's generated images fail to accurately represent the semantics of the input prompt: simple yet crucial objects may be omitted, thereby disrupting text-image alignment. We refer to this problem as \"object omission\". Without additional external knowledge, previous methods have been ineffective at addressing this issue. To investigate this problem, we analyze the attention maps in SD and find that biased text representations mislead the visual denoising process when handling correlated tokens, impeding object generation. Moreover, we observe that even when two prompts share the same semantics, slight variations in token sequence significantly alter attention scores, consequently affecting the final generated images. Based on these findings, we propose a simple yet effective fine-tuning method that applies decorrelation to the self-attention maps in the text module, thus reducing dependencies between tokens. Our approach requires no external prior knowledge, is straightforward to implement, and operates solely on the text module of the SD model. Extensive experiments confirm that our method effectively alleviates the object omission problem under text correlations, thereby enhancing text-image alignment",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunze Tong",
      "Fengda Zhang",
      "Didi Zhu",
      "Jun Xiao",
      "Kun Kuang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lee_CityNav_A_Large-Scale_Dataset_for_Real-World_Aerial_Navigation_ICCV_2025_paper.html": {
    "title": "CityNav: A Large-Scale Dataset for Real-World Aerial Navigation",
    "volume": "main",
    "abstract": "Vision-and-language navigation (VLN) aims to develop agents capable of navigating in realistic environments. While recent cross-modal training approaches have significantly improved navigation performance in both indoor and outdoor scenarios, aerial navigation over real-world cities remains underexplored primarily due to limited datasets and the difficulty of integrating visual and geographic information. To fill this gap, we introduce CityNav, the first large-scale real-world dataset for aerial VLN. Our dataset consists of 32,637 human demonstration trajectories, each paired with a natural language description, covering 4.65 km2 across two real cities: Cambridge and Birmingham. In contrast to existing datasets composed of synthetic scenes such as AerialVLN, our dataset presents a unique challenge because agents must interpret spatial relationships between real-world landmarks and the navigation destination, making CityNav an essential benchmark for advancing aerial VLN. Furthermore, as an initial step toward addressing this challenge, we provide a methodology of creating geographic semantic maps that can be used as an auxiliary modality input during navigation. In our experiments, we compare performance of three representative aerial VLN agents (Seq2seq, CMA and AerialVLN models) and demonstrate that the semantic map representation significantly improves their navigation performance",
    "checked": true,
    "id": "103a79f7413e4b13eb54ca82c817d8aec491d72e",
    "semantic_title": "citynav: a large-scale dataset for real-world aerial navigation",
    "citation_count": 23,
    "authors": [
      "Jungdae Lee",
      "Taiki Miyanishi",
      "Shuhei Kurita",
      "Koya Sakamoto",
      "Daichi Azuma",
      "Yutaka Matsuo",
      "Nakamasa Inoue"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Arslan_Neuromanifold-Regularized_KANs_for_Shape-fair_Feature_Representations_ICCV_2025_paper.html": {
    "title": "Neuromanifold-Regularized KANs for Shape-fair Feature Representations",
    "volume": "main",
    "abstract": "Traditional deep networks struggle to acquire shape-fair representations due to their high expressivity. Kolmogorov-Arnold Networks (KANs) are promising candidates as they learn nonlinearities directly, a property that makes them more adaptive. However, KANs perform suboptimally in terms of shape-fairness because of unconstrained nonlinearities, a limitation we demonstrate for the first time. On the other hand, shape-fair networks reside on a neuromanifold of low-degree. Motivated by this, we investigate neuromanifold regularization of KANs to enable learning of shape-fair feature representations. The proposed method, NeuroManifold Regularized-KANs, is a novel regularization that addresses failure modes during the acquisition of local and global shape cues, separately. This is done by constraining the degree of the neuromanifolds of two jointly trained feature extractors. Additionally, we propose a novel Style Decorrelation Loss that promotes decorrelation of intermediate representations. Our experiments demonstrate that NMR-KAN improves shape bias over baseline convolutional KANs by 14.8% while also providing robustness under image corruptions and adversarial attacks. Code is avaliable at: http://www.github.com/kaptres/NMR-KAN",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mazlum Ferhat Arslan",
      "Weihong Guo",
      "Shuo Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lee_Diffusion_Guided_Adaptive_Augmentation_for_Generalization_in_Visual_Reinforcement_Learning_ICCV_2025_paper.html": {
    "title": "Diffusion Guided Adaptive Augmentation for Generalization in Visual Reinforcement Learning",
    "volume": "main",
    "abstract": "Reinforcement learning (RL) has proven its potential in complex decision-making tasks. Yet, many RL systems rely on manually crafted state representations, requiring effort in feature engineering. Visual Reinforcement Learning (VRL) offers a way to address this challenge by enabling agents to learn directly from raw visual input. Nonetheless, VRL continues to face generalization issues, as models often overfit to specific domain features.To tackle this issue, we propose Diffusion Guided Adaptive Augmentation (DGA2), an augmentation method that utilizes Stable Diffusion to enhance domain diversity.We introduce an Adaptive Domain Shift strategy that dynamically adjusts the degree of domain shift according to the agent's learning progress for effective augmentation with Stable Diffusion.Additionally, we employ saliency as the mask to preserve the semantics of data.Our experiments on the DMControl-GB, Adroit, and Procgen environments demonstrate that DGA2 improves generalization performance compared to existing data augmentation and generalization methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeong Woon Lee",
      "Hyoseok Hwang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shou_Graph_Domain_Adaptation_with_Dual-branch_Encoder_and_Two-level_Alignment_for_ICCV_2025_paper.html": {
    "title": "Graph Domain Adaptation with Dual-branch Encoder and Two-level Alignment for Whole Slide Image-based Survival Prediction",
    "volume": "main",
    "abstract": "In recent years, whole slide image (WSI)-based survival analysis has attracted much attention. In practice, WSIs usually come from different hospitals (or domains) and may have significant differences. These differences generally result in large gaps in distribution between different WSI domains and thus, the survival analysis models trained on one domain may fail to transfer to another. To address this issue, we propose a Dual-branch Encoder and Two-level Alignment (DETA) framework to explore both feature and category-level alignment between different WSI domains. Specifically, we first formulate the concerned problem as graph domain adaptation (GDA) using the graph representation of WSIs. Then, we construct a dual-branch graph encoder, including the message passing (MP) and the shortest path (SP) branches, to explicitly and implicitly extract semantic information from the graph-represented WSIs. To realize GDA, we propose a two-level alignment approach: at the category level, we develop a coupling technique by virtue of the dual-branch structure, leading to reduced divergence between the category distributions of the two domains; at the feature level, we introduce an adversarial perturbation strategy to better augment source domain feature, resulting in improved alignment in feature distribution. Extensive experiments have demonstrated the effectiveness of our proposed DETA framework in WSI-based survival analysis under the domain shift scenario",
    "checked": true,
    "id": "a8dff1dc0129844af5f3169331e71ad828a563ee",
    "semantic_title": "graph domain adaptation with dual-branch encoder and two-level alignment for whole slide image-based survival prediction",
    "citation_count": 8,
    "authors": [
      "Yuntao Shou",
      "Xiangyong Cao",
      "Peiqiang Yan",
      "Qiao Hui",
      "Qian Zhao",
      "Deyu Meng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Min_Vision-Language_Interactive_Relation_Mining_for_Open-Vocabulary_Scene_Graph_Generation_ICCV_2025_paper.html": {
    "title": "Vision-Language Interactive Relation Mining for Open-Vocabulary Scene Graph Generation",
    "volume": "main",
    "abstract": "To promote the deployment of scenario understanding in the real world, Open-Vocabulary Scene Graph Generation (OV-SGG) has attracted much attention recently, aiming to generalize beyond the limited number of relation categories labeled during training and detect those unseen relations during inference. Towards OV-SGG, one feasible solution is to leverage the large-scale pre-trained vision-language models (VLMs) containing plentiful category-level content to capture accurate correspondences between images and text. However, due to the lack of quadratic relation-aware knowledge in VLMs, directly using the category-level correspondence in the base dataset could not sufficiently represent generalized relations involved in open world. Therefore, designing an effective open-vocabulary relation mining framework is challenging and meaningful. To this end, we propose a novel Vision-Language Interactive Relation Mining model (VL-IRM) for OV-SGG, which explores learning generalized relation-aware knowledge through multi-modal interaction. Specifically, first, to enhance the generalization of the relation text to visual content, we present a generative relation model to make the text modality explore possible open-ended relations based on visual content. Then, we employ visual modality to guide the relation text for spatial and semantic extension. Extensive experiments demonstrate the superior OV-SGG performance of our method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yukuan Min",
      "Muli Yang",
      "Jinhao Zhang",
      "Yuxuan Wang",
      "Aming Wu",
      "Cheng Deng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_FedAGC_Federated_Continual_Learning_with_Asymmetric_Gradient_Correction_ICCV_2025_paper.html": {
    "title": "FedAGC: Federated Continual Learning with Asymmetric Gradient Correction",
    "volume": "main",
    "abstract": "Federated Continual Learning (FCL) has emerged as a prominent distributed learning paradigm and aims at addressing model learning challenges in both federated and continual learning settings. Efficient personalization in FCL remains a major challenge, as it must handle not only conflicts between old and new knowledge within parallel task streams but also heterogeneous knowledge conflicts from different clients. Recent approaches attempt to mitigate these issues through gradient correction. However, they often overlook the combined impact of gradient magnitude and direction, leading to unsatisfactory gradient solutions. To address these issues, we propose a novel federated continual learning method (called FedAGC) with asymmetric gradient correction, which performs memory rehearsal using representative samples selected via a centroid-based approach from historical tasks. By formulating the problem as a multi-objective optimization problem, FedAGC derives more effective gradients while incorporating group-level personalization to facilitate useful knowledge integration and irrelevant knowledge isolation, effectively mitigating both temporal and spatial catastrophic forgetting. Extensive experiments confirm the effectiveness of FedAGC",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengchao Zhang",
      "Fanhua Shang",
      "Hongying Liu",
      "Liang Wan",
      "Wei Feng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kitichotkul_Free-running_vs_Synchronous_Single-Photon_Lidar_for_High-flux_3D_Imaging_ICCV_2025_paper.html": {
    "title": "Free-running vs Synchronous: Single-Photon Lidar for High-flux 3D Imaging",
    "volume": "main",
    "abstract": "Conventional wisdom suggests that single-photon lidar (SPL) should operate in low-light conditions to minimize dead-time effects.Many methods have been developed to mitigate these effects in synchronous SPL systems. However, solutions for free-running SPL remain limited despite the advantage of reduced histogram distortion from dead times.To improve the accuracy of free-running SPL, we propose a computationally efficient joint maximum likelihood estimator of the signal flux, the background flux, and the depth, along with a complementary regularization framework that incorporates a learned point cloud score model as a prior.Simulations and experiments demonstrate that free-running SPL yields lower estimation errors than its synchronous counterpart under identical conditions, with our regularization further improving accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruangrawee Kitichotkul",
      "Shashwath Bharadwaj",
      "Joshua Rapp",
      "Yanting Ma",
      "Alexander Mehta",
      "Vivek K Goyal"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Continuous-Time_Human_Motion_Field_from_Event_Cameras_ICCV_2025_paper.html": {
    "title": "Continuous-Time Human Motion Field from Event Cameras",
    "volume": "main",
    "abstract": "This paper addresses the challenges of estimating a continuous-time field from a stream of events. Existing Human Mesh Recovery (HMR) methods rely predominantly on frame-based approaches, which are prone to aliasing and inaccuracies due to limited temporal resolution and motion blur. In this work, we predict a continuous-time human motion field from events caused by human motion. Prior state-of-the-art methods rely on computationally intensive optimization across a fixed number of poses at high frame rates, which becomes prohibitively expensive as we increase the temporal resolution. In comparison, our model leverages a recurrent feed-forward neural network to predict human motion in the latent space of possible human motions. We present the first work that replaces traditional event volume-based discrete-time pre-dictions with a continuous human motion field represented as a time-implicit function, enabling parallel pose queries at arbitrary temporal resolutions. To advance the evaluation of continuous-time human pose estimation, we introduce the Beam-splitter Event Agile Human Motion Dataset--a hardware-synchronized high-speed human dataset tailored for this purpose. EvHuman improves joint errors by 23.8 % compared to previous event human methods, while reducing the computational time by 69%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyun Wang",
      "Ruijun Zhang",
      "Zi-Yan Liu",
      "Yufu Wang",
      "Kostas Daniilidis"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_MagicMirror_ID-Preserved_Video_Generation_in_Video_Diffusion_Transformers_ICCV_2025_paper.html": {
    "title": "MagicMirror: ID-Preserved Video Generation in Video Diffusion Transformers",
    "volume": "main",
    "abstract": "We present MagicMirror, a framework for generating identity-preserved videos with cinematic-level quality and dynamic motion. While recent advances in video diffusion models have shown impressive capabilities in text-to-video generation, maintaining consistent identity while producing natural motion remains challenging. Previous methods either require person-specific fine-tuning or struggle to balance identity preservation with motion diversity. Built upon Video Diffusion Transformers, our method introduces three key components: (1) a dual-branch facial feature extractor that captures both identity and structural features, (2) a lightweight cross-modal adapter with Conditioned Adaptive Normalization for efficient identity integration, and (3) a two-stage training strategy combining synthetic identity pairs with video data. Extensive experiments demonstrate that MagicMirror effectively balances identity consistency with natural motion, outperforming existing methods across multiple metrics while requiring minimal parameters added. The code and model will be made publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuechen Zhang",
      "Yaoyang Liu",
      "Bin Xia",
      "Bohao Peng",
      "Zexin Yan",
      "Eric Lo",
      "Jiaya Jia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Fischer_Unified_Category-Level_Object_Detection_and_Pose_Estimation_from_RGB_Images_ICCV_2025_paper.html": {
    "title": "Unified Category-Level Object Detection and Pose Estimation from RGB Images using 3D Prototypes",
    "volume": "main",
    "abstract": "Recognizing objects in images is a fundamental problem in computer vision. Although detecting objects in 2D images is common, many applications require determining their pose in 3D space. Traditional category-level methods rely on RGB-D inputs, which may not always be available, or employ two-stage approaches that use separate models and representations for detection and pose estimation. For the first time, we introduce a unified model that integrates detection and pose estimation into a single framework for RGB images by leveraging neural mesh models with learned features and multi-model RANSAC. Our approach achieves state-of-the-art results for RGB category-level pose estimation on REAL275, improving on the current state-of-the-art by 22.9% averaged across all scale-agnostic metrics. Finally, we demonstrate that our unified method exhibits greater robustness compared to single-stage baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tom Fischer",
      "Xiaojie Zhang",
      "Eddy Ilg"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liao_GT-Mean_Loss_A_Simple_Yet_Effective_Solution_for_Brightness_Mismatch_ICCV_2025_paper.html": {
    "title": "GT-Mean Loss: A Simple Yet Effective Solution for Brightness Mismatch in Low-Light Image Enhancement",
    "volume": "main",
    "abstract": "Low-light image enhancement (LLIE) aims to improve the visual quality of images captured under poor lighting conditions. In supervised LLIE tasks, there exists a significant yet often overlooked inconsistency between the overall brightness of an enhanced image and its ground truth counterpart, referred to as brightness mismatch in this study. Brightness mismatch negatively impact supervised LLIE models by misleading model training. However, this issue is largely neglected in current research. In this context, we propose the GT-mean loss, a simple yet effective loss function directly modeling the mean values of images from a probabilistic perspective.The GT-mean loss is flexible, as it extends existing supervised LLIE loss functions into the GT-mean form with minimal additional computational costs. Extensive experiments demonstrate that the incorporation of the GT-mean loss results in consistent performance improvements across various methods and datasets",
    "checked": true,
    "id": "bf9ffc8ac5a771f376f2ec25bf1e7800a4f483a9",
    "semantic_title": "gt-mean loss: a simple yet effective solution for brightness mismatch in low-light image enhancement",
    "citation_count": 0,
    "authors": [
      "Jingxi Liao",
      "Shijie Hao",
      "Richang Hong",
      "Meng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_OpenVision_A_Fully-Open_Cost-Effective_Family_of_Advanced_Vision_Encoders_for_ICCV_2025_paper.html": {
    "title": "OpenVision: A Fully-Open, Cost-Effective Family of Advanced Vision Encoders for Multimodal Learning",
    "volume": "main",
    "abstract": "OpenAI's CLIP models, released in early 2021, have long been the only viable choice for the research community in building multimodal foundation models. This dominance has only recently been challenged by a few alternatives like SigLIP. However, to the best of our knowledge, all these solutions are still not fully open, e.g., their training data remains proprietary and/or their training frameworks are unreleased. In this paper, we address this challenge by introducing a family of fully open vision encoders that are as competitive as, or even surpass, OpenAI's CLIP in building multimodal foundation models like LLaVA. Moreover, due to their fully open nature, we offer these vision encoders in a wide range of sizes, from as few as 5.9 million parameters to 632.1 million parameters. We further demonstrate that these variable-sized vision encoders provide significant flexibility: larger models deliver enhanced multimodal performance, while smaller models enable efficient and portable multimodal foundation models suitable for edge device deployment. The training data, code and trained models will be released soon",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xianhang Li",
      "Yanqing Liu",
      "Haoqin Tu",
      "Cihang Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Unified_Multimodal_Understanding_via_Byte-Pair_Visual_Encoding_ICCV_2025_paper.html": {
    "title": "Unified Multimodal Understanding via Byte-Pair Visual Encoding",
    "volume": "main",
    "abstract": "Multimodal large language models (MLLMs) have made significant progress in vision-language understanding, yet effectively aligning different modalities remains a fundamental challenge. We present a framework that unifies multimodal understanding by applying byte-pair encoding to visual tokens. Unlike conventional approaches that rely on modality-specific encoders, our method directly incorporates structural information into visual tokens, mirroring successful tokenization strategies in text-only language models. We introduce a priority-guided encoding scheme that considers both frequency and spatial consistency, coupled with a multi-stage training procedure based on curriculum-driven data composition. These enhancements enable the transformer model to better capture cross-modal relationships and reason with visual information. Comprehensive experiments demonstrate improved performance across diverse vision-language tasks. By bridging the gap between visual and textual representations, our approach contributes to the advancement of more capable and efficient multimodal foundation models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wanpeng Zhang",
      "Yicheng Feng",
      "Hao Luo",
      "Yijiang Li",
      "Zihao Yue",
      "Sipeng Zheng",
      "Zongqing Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yan_MobileIE_An_Extremely_Lightweight_and_Effective_ConvNet_for_Real-Time_Image_ICCV_2025_paper.html": {
    "title": "MobileIE: An Extremely Lightweight and Effective ConvNet for Real-Time Image Enhancement on Mobile Devices",
    "volume": "main",
    "abstract": "Recent advancements in deep neural networks have driven significant progress in image enhancement (IE). However, deploying deep learning models on resource-constrained platforms, such as mobile devices, remains challenging due to high computation and memory demands. To address these challenges and facilitate real-time IE on mobile, we introduce an extremely lightweight Convolutional Neural Network (CNN) framework with around 4K parameters. Our approach integrates re-parameterization with an Incremental Weight Optimization strategy to ensure efficiency. Additionally, we enhance performance with a Feature Self-Transform module and a Hierarchical Dual-Path Attention mechanism, optimized with a Local Variance-Weighted loss. With this efficient framework, we are the first to achieve real-time IE inference at up to 1,100 frames per second (FPS) while delivering competitive image quality, achieving the best trade-off between speed and performance across multiple IE tasks. The code will be available at https://github.com/AVC2-UESTC/MobileIE.git",
    "checked": true,
    "id": "bd5105d144966179c020db3513a72f6b72c23eeb",
    "semantic_title": "mobileie: an extremely lightweight and effective convnet for real-time image enhancement on mobile devices",
    "citation_count": 4,
    "authors": [
      "Hailong Yan",
      "Ao Li",
      "Xiangtao Zhang",
      "Zhe Liu",
      "Zenglin Shi",
      "Ce Zhu",
      "Le Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lou_Learning_Pixel-adaptive_Multi-layer_Perceptrons_for_Real-time_Image_Enhancement_ICCV_2025_paper.html": {
    "title": "Learning Pixel-adaptive Multi-layer Perceptrons for Real-time Image Enhancement",
    "volume": "main",
    "abstract": "Deep learning-based bilateral grid processing has emerged as a promising solution for image enhancement, inherently encoding spatial and intensity information while enabling efficient full-resolution processing through slicing operations. However, existing approaches are limited to linear affine transformations, hindering their ability to model complex color relationships. Meanwhile, while multi-layer perceptrons (MLPs) excel at non-linear mappings, traditional MLP-based methods employ globally shared parameters, which is hard to deal with localized variations. To overcome these dual challenges, we propose a Bilateral Grid-based Pixel-Adaptive Multi-layer Perceptron (BPAM) framework. Our approach synergizes the spatial modeling of bilateral grids with the non-linear capabilities of MLPs. Specifically, we generate bilateral grids containing MLP parameters, where each pixel dynamically retrieves its unique transformation parameters and obtain a distinct MLP for color mapping based on spatial coordinates and intensity values. In addition, we propose a novel grid decomposition strategy that categorizes MLP parameters into distinct types stored in separate subgrids. Multi-channel guidance maps are used to extract category-specific parameters from corresponding subgrids, ensuring effective utilization of color information during slicing while guiding precise parameter generation. Extensive experiments on public datasets demonstrate that our method outperforms state-of-the-art methods in performance while maintaining real-time processing capabilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyu Lou",
      "Xiaorui Zhao",
      "Kexuan Shi",
      "Shuhang Gu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Bruns_ACE-G_Improving_Generalization_of_Scene_Coordinate_Regression_Through_Query_Pre-Training_ICCV_2025_paper.html": {
    "title": "ACE-G: Improving Generalization of Scene Coordinate Regression Through Query Pre-Training",
    "volume": "main",
    "abstract": "Scene coordinate regression (SCR) has established itself as a promising learning-based approach to visual relocalization. After mere minutes of scene-specific training, SCR models estimate camera poses of query images with high accuracy. Still, SCR methods fall short of the generalization capabilities of more classical feature-matching approaches. When imaging conditions of query images, such as lighting or viewpoint, are too different from the training views, SCR models fail. Failing to generalize is an inherent limitation of previous SCR frameworks, since their training objective is to encode the training views in the weights of the coordinate regressor itself. The regressor essentially overfits to the training views, by design. We propose to separate the coordinate regressor and the map representation into a generic transformer and a scene-specific map code. This separation allows us to pre-train the transformer on tens of thousands of scenes. More importantly, it allows us to train the transformer to generalize from mapping images to unseen query images during pre-training. We demonstrate on multiple challenging relocalization datasets that our method, ACE-G, leads to significantly increased robustness while keeping the computational footprint attractive",
    "checked": true,
    "id": "ef38f227f6ebacf4fb2537cb5eb27512cbc320f5",
    "semantic_title": "ace-g: improving generalization of scene coordinate regression through query pre-training",
    "citation_count": 0,
    "authors": [
      "Leonard Bruns",
      "Axel Barroso-Laguna",
      "Tommaso Cavallari",
      "Aron Monszpart",
      "Sowmya Munukutla",
      "Victor Adrian Prisacariu",
      "Eric Brachmann"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hudson_Everything_is_a_Video_Unifying_Modalities_through_Next-Frame_Prediction_ICCV_2025_paper.html": {
    "title": "Everything is a Video: Unifying Modalities through Next-Frame Prediction",
    "volume": "main",
    "abstract": "Multimodal learning, which involves integrating information from various modalities such as text, images, audio, and video, is pivotal for numerous complex tasks like visual question answering, cross-modal retrieval, and caption generation. Traditional approaches rely on modality-specific encoders and late fusion techniques, which can hinder flexibility when adapting to new tasks or modalities. To address these limitations, we introduce a novel framework that extends the concept of task reformulation beyond natural language processing (NLP) to multimodal learning. We propose to reformulate diverse multimodal tasks into a unified next-frame prediction problem, allowing a single model to handle different modalities without modality-specific components. This method treats all inputs and outputs as sequential frames in a video, enabling seamless integration of modalities and effective knowledge transfer across tasks. Our approach is evaluated on a range of tasks, including text-to-text, image-to-text, video-to-text, and audio-to-text, demonstrating the model's ability to generalize across modalities with minimal adaptation. We show that task reformulation can significantly simplify multimodal model design across various tasks, laying the groundwork for more generalized multimodal foundation models",
    "checked": true,
    "id": "8821fafedfd47a6b0db1374120061cc725623216",
    "semantic_title": "everything is a video: unifying modalities through next-frame prediction",
    "citation_count": 2,
    "authors": [
      "G. Thomas Hudson",
      "Dean Slack",
      "Thomas Winterbottom",
      "Jamie Sterling",
      "Chenghao Xiao",
      "Junjie Shentu",
      "Noura Al Moubayed"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cao_ViT-EnsembleAttack_Augmenting_Ensemble_Models_for_Stronger_Adversarial_Transferability_in_Vision_ICCV_2025_paper.html": {
    "title": "ViT-EnsembleAttack: Augmenting Ensemble Models for Stronger Adversarial Transferability in Vision Transformers",
    "volume": "main",
    "abstract": "Ensemble-based attacks have been proven to be effective in enhancing adversarial transferability by aggregating the output of models with various architectures. However, existing research primarily focuses on refining ensemble weights or optimizing the ensemble path, overlooking the exploration of ensemble models to enhance the transferability of adversarial attacks. To address this gap, we propose applying adversarial augmentation to the surrogate models, aiming to boost overall generalization of ensemble models and reduce the risk of adversarial overfitting. Meanwhile, observing that ensemble Vision Transformers (ViTs) gain less attention, we propose ViT-EnsembleAttack based on the idea of model adversarial augmentation, the first ensemble-based attack method tailored for ViTs to the best of our knowledge. Our approach generates augmented models for each surrogate ViT using three strategies: Multi-head dropping, Attention score scaling, and MLP feature mixing, with the associated parameters optimized by Bayesian optimization. These adversarially augmented models are ensembled to generate adversarial examples. Furthermore, we introduce Automatic Reweighting and Step Size Enlargement modules to boost transferability. Extensive experiments demonstrate that ViT-EnsembleAttack significantly enhances the adversarial transferability of ensemble-based attacks on ViTs, outperforming existing methods by a substantial margin. Code is available at https://github.com/Trustworthy-AI-Group/TransferAttack",
    "checked": true,
    "id": "10d7d1a137cb573067dbd334e84e823566e6a345",
    "semantic_title": "vit-ensembleattack: augmenting ensemble models for stronger adversarial transferability in vision transformers",
    "citation_count": 1,
    "authors": [
      "Hanwen Cao",
      "Haobo Lu",
      "Xiaosen Wang",
      "Kun He"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Song_LayerTracer_Cognitive-Aligned_Layered_SVG_Synthesis_via_Diffusion_Transformer_ICCV_2025_paper.html": {
    "title": "LayerTracer: Cognitive-Aligned Layered SVG Synthesis via Diffusion Transformer",
    "volume": "main",
    "abstract": "Generating cognitive-aligned layered SVGs remains challenging due to existing methods' tendencies toward either oversimplified single-layer outputs or optimization-induced shape redundancies. We propose LayerTracer, a DiT based framework that bridges this gap by learning designers' layered SVG creation processes from a novel dataset of sequential design operations. Our approach operates in two phases: First, a text-conditioned DiT generates multi-phase rasterized construction blueprints that simulate human design workflows. Second, layer-wise vectorization with path deduplication produces clean, editable SVGs. For image vectorization, we introduce a conditional diffusion mechanism that encodes reference images into latent tokens, guiding hierarchical reconstruction while preserving structural integrity. Extensive experiments show that LayerTracer surpasses optimization-based and neural baselines in generation quality and editability",
    "checked": true,
    "id": "b800eaf42586836c80de26adc9825a940d8a03ca",
    "semantic_title": "layertracer: cognitive-aligned layered svg synthesis via diffusion transformer",
    "citation_count": 11,
    "authors": [
      "Yiren Song",
      "Danze Chen",
      "Mike Zheng Shou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_TopoTTA_Topology-Enhanced_Test-Time_Adaptation_for_Tubular_Structure_Segmentation_ICCV_2025_paper.html": {
    "title": "TopoTTA: Topology-Enhanced Test-Time Adaptation for Tubular Structure Segmentation",
    "volume": "main",
    "abstract": "Tubular structure segmentation (TSS) is important for various applications, such as hemodynamic analysis and route navigation. Despite significant progress in TSS, domain shifts remain a major challenge, leading to performance degradation in unseen target domains. Unlike other segmentation tasks, TSS is more sensitive to domain shifts, as changes in topological structures can compromise segmentation integrity, and variations in local features distinguishing foreground from background (e.g., texture and contrast) may further disrupt topological continuity. To address these challenges, we propose Topology-enhanced Test-Time Adaptation (TopoTTA), the first test-time adaptation framework designed specifically for TSS. TopoTTA consists of two stages: Stage 1 adapts models to cross-domain topological discrepancies using the proposed Topological Meta Difference Convolutions (TopoMDCs), which enhance topological representation without altering pre-trained parameters; Stage 2 improves topological continuity by a novel Topology Hard sample Generation (TopoHG) strategy and prediction alignment on hard samples with pseudo-labels in the generated pseudo-break regions. Extensive experiments across four scenarios and ten datasets demonstrate TopoTTA's effectiveness in handling topological distribution shifts, achieving an average improvement of 31.81% in clDice. TopoTTA also serves as a plug-and-play TTA solution for CNN-based TSS models",
    "checked": true,
    "id": "4853e9ea66fb2b84df4075f39f2197cefe99f698",
    "semantic_title": "topotta: topology-enhanced test-time adaptation for tubular structure segmentation",
    "citation_count": 0,
    "authors": [
      "Jiale Zhou",
      "Wenhan Wang",
      "Shikun Li",
      "Xiaolei Qu",
      "Xin Guo",
      "Yizhong Liu",
      "Wenzhong Tang",
      "Xun Lin",
      "Yefeng Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Dumery_Counting_Stacked_Objects_ICCV_2025_paper.html": {
    "title": "Counting Stacked Objects",
    "volume": "main",
    "abstract": "Visual object counting is a fundamental computer vision task underpinning numerous real-world applications, from cell counting in biomedicine to traffic and wildlife monitoring. However, existing methods struggle to handle the challenge of stacked 3D objects in which most objects are hidden by those above them. To address this important yet underexplored problem, we propose a novel 3D counting approach that decomposes the task into two complementary subproblems - estimating the 3D geometry of the object stack and the occupancy ratio from multi-view images. By combining geometric reconstruction and deep learning-based depth analysis, our method can accurately count identical objects within containers, even when they are irregularly stacked. We validate our 3D Counting pipeline on large-scale synthetic and diverse real-world datasets with manually verified total counts",
    "checked": true,
    "id": "f9a6e79eefe0d14fb587bcbd167540fe10d812d5",
    "semantic_title": "counting stacked objects",
    "citation_count": 1,
    "authors": [
      "Corentin Dumery",
      "Noa Etté",
      "Aoxiang Fan",
      "Ren Li",
      "Jingyi Xu",
      "Hieu Le",
      "Pascal Fua"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Garg_VOccl3D_A_Video_Benchmark_Dataset_for_3D_Human_Pose_and_ICCV_2025_paper.html": {
    "title": "VOccl3D: A Video Benchmark Dataset for 3D Human Pose and Shape Estimation under real Occlusions",
    "volume": "main",
    "abstract": "Human pose and shape (HPS) estimation methods have been extensively studied, with many demonstrating high zero-shot performance on in-the-wild images and videos. However, these methods often struggle in challenging scenarios involving complex human poses or significant occlusions. Although some studies address 3D human pose estimation under occlusion, they typically evaluate performance on datasets that lack realistic or substantial occlusions, e.g., most existing datasets introduce occlusions with random patches over the human or clipart-style overlays, which may not reflect real-world challenges. To bridge this gap in realistic occlusion datasets, we introduce a novel benchmark dataset, VOccl3D, a Video-based human Occlusion dataset with 3D body pose and shape annotations. Inspired by works such as AGORA and BEDLAM, we constructed this dataset using advanced computer graphics rendering techniques, incorporating diverse real-world occlusion scenarios, clothing textures, and human motions. Additionally, we fine-tuned recent HPS methods, CLIFF and BEDLAM-CLIFF, on our dataset, demonstrating significant qualitative and quantitative improvements across multiple public datasets, as well as on the test split of our dataset, while comparing its performance with other state-of-the-art methods. Furthermore, we leveraged our dataset to enhance human detection performance under occlusion by fine-tuning an existing object detector, YOLO11, thus leading to a robust end-to-end HPS estimation system under occlusions. Overall, this dataset serves as a valuable resource for future research aimed at benchmarking methods designed to handle occlusions, offering a more realistic alternative to existing occlusion datasets",
    "checked": true,
    "id": "89196e9e963876f86a9036129e0867d1f22f2d96",
    "semantic_title": "voccl3d: a video benchmark dataset for 3d human pose and shape estimation under real occlusions",
    "citation_count": 0,
    "authors": [
      "Yash Garg",
      "Saketh Bachu",
      "Arindam Dutta",
      "Rohit Lal",
      "Sarosij Bose",
      "Calvin-Khang Ta",
      "M. Salman Asif",
      "Amit Roy-Chowdhury"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_PoseAnchor_Robust_Root_Position_Estimation_for_3D_Human_Pose_Estimation_ICCV_2025_paper.html": {
    "title": "PoseAnchor: Robust Root Position Estimation for 3D Human Pose Estimation",
    "volume": "main",
    "abstract": "Standard 3D human pose estimation (HPE) benchmarks employ root-centering, which normalizes poses relative to the pelvis but discards absolute root position information. While effective for evaluation, this approach limits real-world applications such as motion tracking, AR/VR, and human-computer interaction, where absolute root position is essential. Moreover, incorporating root position into these models often leads to performance degradation. To address these limitations, we introduce PoseAnchor, a unified framework that seamlessly integrates root position estimation while improving overall pose accuracy. PoseAnchor leverages Iterative Hard Thresholding Robust Least Squares Regression (ITRR), a novel robust regression approach introduced to 3D HPE for the first time. ITRR effectively mitigates the impact of noisy 2D detections, enabling more accurate root position estimation. With ITRR, PoseAnchor enables zero-shot root localization, allowing existing models to estimate absolute root positions without retraining or architectural modifications. ITRR identifies a support set of reliable joints based on their spatial relationships to achieve robust root estimation, effectively filtering out unreliable joints. Beyond zero-shot localization, PoseAnchor incorporates ITRR into a Data-Driven Training framework that selectively utilizes the support set to optimize pose learning. By dynamically filtering high-confidence joint data, PoseAnchor mitigates noise while improving robustness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun-Hee Kim",
      "Jumin Han",
      "Seong-Whan Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jena_Sparfels_Fast_Reconstruction_from_Sparse_Unposed_Imagery_ICCV_2025_paper.html": {
    "title": "Sparfels: Fast Reconstruction from Sparse Unposed Imagery",
    "volume": "main",
    "abstract": "We present a method for Sparse view reconstruction with surface element splatting that runs within 2 minutes on a consumer grade GPU. While few methods address sparse radiance field learning from noisy or unposed sparse cameras, shape recovery remains relatively underexplored in this setting. Several radiance and shape learning test-time optimization methods address the sparse posed setting by learning data priors or using combinations of external monocular geometry priors. Differently, we propose an efficient and simple pipeline harnessing a single recent 3D foundation model. We leverage its various task heads, notably point maps and camera initializations to instantiate a bundle adjusting 2D Gaussian Splatting (2DGS) model, and image correspondences to guide camera optimization midst 2DGS training. Key to our contribution is a novel formulation of splatted color variance along rays, which can be computed efficiently. Reducing this moment in training leads to more accurate shape reconstructions. We demonstrate stat-of-the-art performances in the sparse uncalibrated setting in reconstruction and novel view Benchmarks based on established multi-view datasets",
    "checked": true,
    "id": "d88e013c9949dfe77ceea6d7f5cc1662ea19ea7d",
    "semantic_title": "sparfels: fast reconstruction from sparse unposed imagery",
    "citation_count": 2,
    "authors": [
      "Shubhendu Jena",
      "Amine Ouasfi",
      "Mae Younes",
      "Adnane Boukhayma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhao_Unsupervised_Visual_Chain-of-Thought_Reasoning_via_Preference_Optimization_ICCV_2025_paper.html": {
    "title": "Unsupervised Visual Chain-of-Thought Reasoning via Preference Optimization",
    "volume": "main",
    "abstract": "Chain-of-thought (CoT) reasoning greatly improves the interpretability and problem-solving abilities of multimodal large language models (MLLMs). However, existing approaches are focused on text CoT, limiting their ability to leverage visual cues. Visual CoT remains underexplored, and the only work is based on supervised fine-tuning (SFT) that relies on extensive labeled bounding-box data and is hard to generalize to unseen cases. In this paper, we introduce Unsupervised Visual CoT (UV-CoT), a novel framework for image-level CoT reasoning via preference optimization. UV-CoT performs preference comparisons between model-generated bounding boxes (one is preferred and the other is dis-preferred), eliminating the need for bounding-box annotations. We get such preference data by introducing an automatic data generation pipeline. Given an image, our target MLLM (e.g., LLaVA-1.5-7B) generates seed bounding boxes using a template prompt and then answers the question using each bounded region as input. An evaluator MLLM (e.g., OmniLLM-12B) ranks the responses, and these rankings serve as supervision to train the target MLLM with UV-CoT by minimizing negative log-likelihood losses. By emulating human perception--identifying key regions and reasoning based on them--UV-CoT can improve visual comprehension, particularly in spatial reasoning tasks where textual descriptions alone fall short. Our experiments on six datasets demonstrate the superiority of UV-CoT, compared to the state-of-the-art textual and visual CoT methods. Our zero-shot testing on three unseen datasets shows the strong generalization of UV-CoT. The implementation code is available in the Appendix",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kesen Zhao",
      "Beier Zhu",
      "Qianru Sun",
      "Hanwang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Mehrpanah_On_the_Complexity-Faithfulness_Trade-off_of_Gradient-Based_Explanations_ICCV_2025_paper.html": {
    "title": "On the Complexity-Faithfulness Trade-off of Gradient-Based Explanations",
    "volume": "main",
    "abstract": "ReLU networks, while prevalent for visual data, have sharp transitions, sometimes relying on individual pixels for predictions, making vanilla gradient-based explanations noisy and difficult to interpret. Existing methods, such as GradCAM, smooth these explanations by producing surrogate models at the cost of faithfulness. We introduce a unifying spectral framework to systematically analyze and quantify smoothness, faithfulness, and their trade-off in explanations.Using this framework, we quantify and regularize the contribution of ReLU networks to high-frequency information, providing a principled approach to identifying this trade-off. Our analysis characterizes how surrogate-based smoothing distorts explanations, leading to an \"explanation gap\" that we formally define and measure for different post-hoc methods.Finally, we validate our theoretical findings across different design choices, datasets, and ablations",
    "checked": true,
    "id": "0000fa5e2c4f3d0280bad11871ef16ae3c8ac122",
    "semantic_title": "on the complexity-faithfulness trade-off of gradient-based explanations",
    "citation_count": 0,
    "authors": [
      "Amir Mehrpanah",
      "Matteo Gamba",
      "Kevin Smith",
      "Hossein Azizpour"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hu_Adaptive_Dual_Uncertainty_Optimization_Boosting_Monocular_3D_Object_Detection_under_ICCV_2025_paper.html": {
    "title": "Adaptive Dual Uncertainty Optimization: Boosting Monocular 3D Object Detection under Test-Time Shifts",
    "volume": "main",
    "abstract": "Accurate monocular 3D object detection (M3OD) is pivotal for safety-critical applications like autonomous driving, yet its reliability deteriorates significantly under real-world domain shifts caused by environmental or sensor variations. To address these shifts, Test-Time Adaptation (TTA) methods have emerged, enabling models to adapt to target distributions during inference. While prior TTA approaches recognize the positive correlation between low uncertainty and high generalization ability, they fail to address the dual uncertainty inherent to M3OD: semantic uncertainty (ambiguous class predictions) and geometric uncertainty (unstable spatial localization). To bridge this gap, we propose Dual Uncertainty Optimization (DUO), the first TTA framework designed to jointly minimize both uncertainties for robust M3OD. Through a convex optimization lens, we introduce an innovative convex structure of the focal loss and further derive a novel conjugate loss, enabling label-agnostic uncertainty weighting and balanced learning for high-uncertainty objects. In parallel, we design a semantic-aware normal field constraint that preserves geometric coherence in regions with clear semantic cues, reducing uncertainty from the unstable 3D representation. This dual-branch mechanism forms a complementary loop: enhanced spatial perception improves semantic classification, and robust semantic predictions further refine spatial understanding. Extensive experiments demonstrate the superiority of DUO over existing methods across various datasets and domain shift types. The source code is available at https://github.com/hzcar/DUO",
    "checked": true,
    "id": "0180d3daaf936091378c754d6ef9eece475b66c5",
    "semantic_title": "adaptive dual uncertainty optimization: boosting monocular 3d object detection under test-time shifts",
    "citation_count": 0,
    "authors": [
      "Zixuan Hu",
      "Dongxiao Li",
      "Xinzhu Ma",
      "Shixiang Tang",
      "Xiaotong Li",
      "Wenhan Yang",
      "Ling-Yu Duan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_RayPose_Ray_Bundling_Diffusion_for_Template_Views_in_Unseen_6D_ICCV_2025_paper.html": {
    "title": "RayPose: Ray Bundling Diffusion for Template Views in Unseen 6D Object Pose Estimation",
    "volume": "main",
    "abstract": "Typical template-based object pose pipelines estimate the pose by retrieving the closest matching template and aligning it with the observed image. However, failure to retrieve the correct template often leads to inaccurate pose predictions. To address this, we reformulate template-based object pose estimation as a ray alignment problem, where the viewing directions from multiple posed template images are learned to align with a non-posed query image. Inspired by recent progress in diffusion-based camera pose estimation, we embed this formulation into a diffusion transformer architecture that aligns a query image with a set of posed templates. We reparameterize object rotation using object-centered camera rays and model object translation by extending scale-invariant translation estimation to dense translation offsets. Our model leverages geometric priors from the templates to guide accurate query pose inference. A coarse-to-fine training strategy based on narrowed template sampling improves performance without modifying the network architecture. Extensive experiments across multiple benchmark datasets show competitive results of our method compared to state-of-the-art approaches in unseen object pose estimation",
    "checked": true,
    "id": "af2b00083552f81fede255dae36ecc641e34e815",
    "semantic_title": "raypose: ray bundling diffusion for template views in unseen 6d object pose estimation",
    "citation_count": 0,
    "authors": [
      "Junwen  Huang",
      "Shishir Reddy Vutukur",
      "Peter KT Yu",
      "Nassir Navab",
      "Slobodan Ilic",
      "Benjamin Busam"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Roy_DuoLoRA__Cycle-consistent_and_Rank-disentangled_Content-Style_Personalization_ICCV_2025_paper.html": {
    "title": "DuoLoRA : Cycle-consistent and Rank-disentangled Content-Style Personalization",
    "volume": "main",
    "abstract": "We tackle the challenge of jointly personalizing content and style from a few examples. A promising approach is to train separate Low-Rank Adapters (LoRA) and merge them effectively, preserving both content and style. Existing methods, such as ZipLoRA, treat content and style as independent entities, merging them by learning masks in LoRA's output dimensions. However, content and style are intertwined, not independent. To address this, we propose DuoLoRA--a content-style personalization framework featuring three key components: (1) rank-dimension mask learning, (2) effective merging via layer priors, and (3) Constyle loss, which leverages cycle-consistency in the merging process.First, we introduce ZipRank, which performs content-style merging within the rank dimension, offering adaptive rank flexibility and significantly reducing the number of learnable parameters. Additionally, we incorporate SDXL layer priors to apply implicit rank constraints informed by each layer's content-style bias and adaptive merger initialization, enhancing the integration of content and style. To further refine the merging process, we introduce Constyle loss, which leverages the cycle consistency between content and style.Our experimental results demonstrate that DuoLoRA outperforms state-of-the-art content-style merging methods across multiple benchmarks",
    "checked": true,
    "id": "8ef20ba47152ef766b9e4941b4a5b8e01d193579",
    "semantic_title": "duolora : cycle-consistent and rank-disentangled content-style personalization",
    "citation_count": 1,
    "authors": [
      "Aniket Roy",
      "Shubhankar Borse",
      "Shreya Kadambi",
      "Debasmit Das",
      "Shweta Mahajan",
      "Risheek Garrepalli",
      "Hyojin Park",
      "Ankita Nayak",
      "Rama Chellappa",
      "Munawar Hayat",
      "Fatih Porikli"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_F-Bench_Rethinking_Human_Preference_Evaluation_Metrics_for_Benchmarking_Face_Generation_ICCV_2025_paper.html": {
    "title": "F-Bench: Rethinking Human Preference Evaluation Metrics for Benchmarking Face Generation, Customization, and Restoration",
    "volume": "main",
    "abstract": "Recent artificial intelligence (AI) generative models have demonstrated remarkable capabilities in image production, and have been widely applied to face image generation, customization, and restoration. However, many AI-generated faces (AIGFs) still suffer from issues such as unique distortions, unrealistic details, and unexpected identity shifts, underscoring the need for a comprehensive quality evaluation method for AIGFs. To this end, we introduce **FaceQ**, the first comprehensive AI-generated Face image database with fine-grained Quality annotations aligned with human preferences, which consists of 12K images and 491K ratings across multiple dimensions. Using the FaceQ database, we establish **F-Bench**, a benchmark for comparing and evaluating face generation, customization, and restoration models, highlighting strengths and weaknesses across various prompts and evaluation dimensions. Additionally, we assess the performance of existing image quality assessment (IQA) methods on FaceQ, and further propose a large multimodal model (LMM) based Face quality Evaluator (**F-Eval**) to accurately assess the multi-dimensional quality of generated faces in a one-for-all manner. Extensive experimental results demonstrate the state-of-the-art performance of our F-Eval",
    "checked": true,
    "id": "5f438d96ca45788d7b9551240ffe7be1d9e58837",
    "semantic_title": "f-bench: rethinking human preference evaluation metrics for benchmarking face generation, customization, and restoration",
    "citation_count": 2,
    "authors": [
      "Lu Liu",
      "Huiyu Duan",
      "Qiang Hu",
      "Liu Yang",
      "Chunlei Cai",
      "Tianxiao Ye",
      "Huayu Liu",
      "Xiaoyun Zhang",
      "Guangtao Zhai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_MikuDance_Animating_Character_Art_with_Mixed_Motion_Dynamics_ICCV_2025_paper.html": {
    "title": "MikuDance: Animating Character Art with Mixed Motion Dynamics",
    "volume": "main",
    "abstract": "We propose MikuDance, a diffusion-based pipeline incorporating mixed motion dynamics to animate stylized character art. MikuDance consists of two key techniques: Mixed Motion Modeling and Mixed-Control Diffusion, to address the challenges of high-dynamic motion and reference-guidance misalignment in character art animation. Specifically, a Scene Motion Tracking strategy is presented to explicitly model the dynamic camera in pixel-wise space, enabling unified character-scene motion modeling. Building on this, the Mixed-Control Diffusion implicitly aligns the scale and body shape of diverse characters with motion guidance, allowing flexible control of local character motion. Subsequently, a Motion-Adaptive Normalization module is incorporated to effectively inject global scene motion, paving the way for comprehensive character art animation. Through extensive experiments, we demonstrate the effectiveness and generalizability of MikuDance across various character art and motion guidance, consistently producing high-quality animations with remarkable motion dynamics",
    "checked": true,
    "id": "a2f008048033893793720b8c7df15c8e2429d6d8",
    "semantic_title": "mikudance: animating character art with mixed motion dynamics",
    "citation_count": 4,
    "authors": [
      "Jiaxu Zhang",
      "Xianfang Zeng",
      "Xin Chen",
      "Wei Zuo",
      "Gang Yu",
      "Zhigang Tu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chambon_GaussRender_Learning_3D_Occupancy_with_Gaussian_Rendering_ICCV_2025_paper.html": {
    "title": "GaussRender: Learning 3D Occupancy with Gaussian Rendering",
    "volume": "main",
    "abstract": "Understanding the 3D geometry and semantics of driving scenes is critical for developing safe autonomous vehicles. Recent advances in 3D occupancy prediction have improved scene representation but often suffer from spatial inconsistencies, leading to floating artifacts and poor surface localization. Existing voxel-wise losses (e.g., cross-entropy) fail to enforce geometric coherence. In this paper, we propose GaussRender, a module that improves 3D occupancy learning by enforcing projective consistency. Our key idea is to project both predicted and ground-truth 3D occupancy into 2D camera views, where we apply supervision. Our method penalizes 3D configurations that produce inconsistent 2D projections, thereby enforcing a more coherent and geometrically plausible 3D structure. To achieve this efficiently, we leverage differentiable rendering with Gaussian splatting. GaussRender seamlessly integrates with existing architectures while maintaining efficiency and requiring no inference-time modifications. Extensive evaluations on multiple benchmarks (SurroundOcc-nuScenes, Occ3D nuScenes, SSCBench-KITTI360) demonstrate that GaussRender significantly improves geometric fidelity across various 3D occupancy models (TPVFormer, SurroundOcc, Symphonies), achieving state-of-the-art results, particularly on surface-sensitive metrics. The code and models will be open-sourced",
    "checked": true,
    "id": "82ad264d2a819f54154c47f4db3ac227adce0021",
    "semantic_title": "gaussrender: learning 3d occupancy with gaussian rendering",
    "citation_count": 5,
    "authors": [
      "Loick  Chambon",
      "Eloi Zablocki",
      "Alexandre  Boulch",
      "Mickael Chen",
      "Matthieu Cord"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liang_UniDxMD_Towards_Unified_Representation_for_Cross-Modal_Unsupervised_Domain_Adaptation_in_ICCV_2025_paper.html": {
    "title": "UniDxMD: Towards Unified Representation for Cross-Modal Unsupervised Domain Adaptation in 3D Semantic Segmentation",
    "volume": "main",
    "abstract": "Modality or domain distribution shifts pose formidable challenges in 3D semantic segmentation. Existing methods predominantly address either cross-modal or cross-domain adaptation in isolation, leading to insufficient exploration of semantic associations and complementary features in heterogeneous data. To bridge this gap, we present UniDxMD, a unified representation method for cross-modal unsupervised domain adaptation (UDA) in 3D semantic segmentation that simultaneously tackles both cross-modal and cross-domain adaptation objectives. Our core insight is deriving a unified discrete representation from heterogeneous data to mitigate distribution shifts, inspired by vector quantization. Specifically, we propose a differentiable, cluster-based soft quantization mechanism (CSQM) that maps heterogeneous data (spanning modalities and domains) into a shared discrete latent space. Then, we introduce latent space regularization (LSR), leveraging joint prototypes that satisfy semantic relation consistency as learnable anchors to enhance the compactness and semantic discriminability of the discrete latent space. Our method paves the way for advancing cross-modal UDA in 3D semantic segmentation towards the unified representation. Extensive results across four challenging cross-modal UDA scenarios demonstrate the superiority of our method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengyin Liang",
      "Hui Yin",
      "Min Liang",
      "Qianqian Du",
      "Ying Yang",
      "Hua Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xue_GFPack_Attention-Driven_Gradient_Fields_for_Optimizing_2D_Irregular_Packing_ICCV_2025_paper.html": {
    "title": "GFPack++: Attention-Driven Gradient Fields for Optimizing 2D Irregular Packing",
    "volume": "main",
    "abstract": "2D irregular packing is a classic combinatorial optimization problem with various applications, such as material utilization and texture atlas generation. Due to its NP-hard nature, conventional numerical approaches typically encounter slow convergence and high computational costs. Previous research GFPack introduced a generative method for gradient-based packing, providing early evidence of its feasibility but faced limitations such as insufficient rotation support, poor boundary adaptability, and high overlap ratios. In this paper, we propose GFPack++, a deeply investigated framework that adopts attention-based geometry and relation encoding, enabling more comprehensive modeling of complex packing relationships. We further design a constrained gradient and a weighting function to enhance both the feasibility of the produced solutions and the learning effectiveness. Experimental results on multiple datasets demonstrate that GFPack++ achieves higher space utilization, supports continuous rotation, generalizes well to arbitrary boundaries, and infers orders of magnitude faster than previous approaches. Codes for this paper are at https://github.com/TimHsue/GFPack-pp",
    "checked": false,
    "id": "9dba21229a4e3b68543ede60a7ffd767fcc0c9cf",
    "semantic_title": "gfpack++: improving 2d irregular packing by learning gradient field with attention",
    "citation_count": 0,
    "authors": [
      "Tianyang Xue",
      "Lin Lu",
      "Yang Liu",
      "Mingdong Wu",
      "Hao Dong",
      "Yanbin Zhang",
      "Renmin Han",
      "Baoquan Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lyu_PLA_Prompt_Learning_Attack_against_Text-to-Image_Generative_Models_ICCV_2025_paper.html": {
    "title": "PLA: Prompt Learning Attack against Text-to-Image Generative Models",
    "volume": "main",
    "abstract": "Text-to-Image (T2I) models have gained widespread adoption across various applications. Despite the success, the potential misuse of T2I models poses significant risks of generating Not-Safe-For-Work (NSFW) content. To investigate the vulnerability of T2I models, this paper delves into adversarial attacks to bypass the safety mechanisms under black-box settings. Most previous methods rely on word substitution to search adversarial prompts. Due to limited search space, this leads to suboptimal performance compared to gradient-based training. However, black-box settings present unique challenges to training gradient-driven attack methods, since there is no access to the internal architecture and parameters of T2I models. To facilitate the learning of adversarial prompts in black-box settings, we propose a novel prompt learning attack framework (PLA), where insightful gradient-based training tailored to black-box T2I models is designed by utilizing multimodal similarities. Experiments show that our new method can effectively attack the safety mechanisms of black-box T2I models including prompt filters and post-hoc safety checkers with a high success rate compared to state-of-the-art methods. Warning: This paper may contain offensive model-generated content",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinqi Lyu",
      "Yihao Liu",
      "Yanjie Li",
      "Bin Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Teufel_HumanOLAT_A_Large-Scale_Dataset_for_Full-Body_Human_Relighting_and_Novel-View_ICCV_2025_paper.html": {
    "title": "HumanOLAT: A Large-Scale Dataset for Full-Body Human Relighting and Novel-View Synthesis",
    "volume": "main",
    "abstract": "Simultaneous relighting and novel-view rendering of digital human representations is an important yet challenging task with numerous applications. However, progress in this area has been significantly limited due to the lack of publicly available, high-quality datasets, especially for full-body human captures. To address this critical gap, we introduce the HumanOLAT dataset, the first publicly accessible large-scale dataset providing multi-view One-Light-at-a-Time (OLAT) captures of full-body humans. The dataset includes HDR RGB frames under various illumination conditions, such as white light, environment maps, color gradients and fine-grained OLAT illuminations. Our evaluations on state-of-the-art relighting and novel-view synthesis methods underscore both the dataset's value and the significant challenges still present in accurately modeling complex human-centric appearance and lighting interactions. We believe that HumanOLAT will significantly facilitate future research, enabling rigorous benchmarking and advancements in both general and human-specific relighting and rendering techniques",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Timo Teufel",
      "Pulkit Gera",
      "Xilong Zhou",
      "Umar Iqbal",
      "Pramod Rao",
      "Jan Kautz",
      "Vladislav Golyanik",
      "Christian Theobalt"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Integrating_Task-Specific_and_Universal_Adapters_for_Pre-Trained_Model-based_Class-Incremental_Learning_ICCV_2025_paper.html": {
    "title": "Integrating Task-Specific and Universal Adapters for Pre-Trained Model-based Class-Incremental Learning",
    "volume": "main",
    "abstract": "Class-Incremental Learning (CIL) requires a learning system to continually learn new classes without forgetting. Existing pre-trained model-based CIL methods often freeze the pre-trained network and adapt to incremental tasks using additional lightweight modules such as adapters. However, incorrect module selection during inference hurts performance, and task-specific modules often overlook shared general knowledge, leading to errors on distinguishing between similar classes across tasks. To address the aforementioned challenges, we propose integrating Task-Specific and Universal Adapters (TUNA) in this paper. Specifically, we train task-specific adapters to capture the most crucial features relevant to their respective tasks and introduce an entropy-based selection mechanism to choose the most suitable adapter. Furthermore, we leverage an adapter fusion strategy to construct a universal adapter, which encodes the most discriminative features shared across tasks. We combine task-specific and universal adapter predictions to harness both specialized and general knowledge during inference. Extensive experiments on various benchmark datasets demonstrate the state-of-the-art performance of our approach. Code is available at https://github.com/LAMDA-CL/ICCV2025-TUNA",
    "checked": true,
    "id": "3393558cc5f4978f103436dfd672029e19754155",
    "semantic_title": "integrating task-specific and universal adapters for pre-trained model-based class-incremental learning",
    "citation_count": 0,
    "authors": [
      "Yan Wang",
      "Da-Wei Zhou",
      "Han-Jia Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Qin_RoboFactory_Exploring_Embodied_Agent_Collaboration_with_Compositional_Constraints_ICCV_2025_paper.html": {
    "title": "RoboFactory: Exploring Embodied Agent Collaboration with Compositional Constraints",
    "volume": "main",
    "abstract": "Designing effective embodied multi-agent systems is critical for solving complex real-world tasks across domains. Due to the complexity of multi-agent embodied systems, existing methods fail to automatically generate safe and efficient training data for such systems. To this end, we propose the concept of compositional constraints for embodied multi-agent systems, addressing the challenges arising from collaboration among embodied agents. We design various interfaces tailored to different types of constraints, enabling seamless interaction with the physical world. Leveraging compositional constraints and specifically designed interfaces, we develop an automated data collection framework for embodied multi-agent systems and introduce the first benchmark for embodied multi-agent manipulation, RoboFactory. Based on RoboFactory benchmark, we adapt and evaluate the method of imitation learning and analyzed its performance in different difficulty agent tasks. Furthermore, we explore the architectures and training strategies for multi-agent imitation learning, aiming to build safe and efficient embodied multi-agent systems",
    "checked": true,
    "id": "410eb41ade34d02f2db0b60a60dac7cd71d6180b",
    "semantic_title": "robofactory: exploring embodied agent collaboration with compositional constraints",
    "citation_count": 7,
    "authors": [
      "Yiran Qin",
      "Li Kang",
      "Xiufeng Song",
      "Zhenfei Yin",
      "Xiaohong Liu",
      "Xihui Liu",
      "Ruimao Zhang",
      "Lei Bai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Fan_Momentum-GS_Momentum_Gaussian_Self-Distillation_for_High-Quality_Large_Scene_Reconstruction_ICCV_2025_paper.html": {
    "title": "Momentum-GS: Momentum Gaussian Self-Distillation for High-Quality Large Scene Reconstruction",
    "volume": "main",
    "abstract": "3D Gaussian Splatting has demonstrated notable success in large-scale scene reconstruction, but challenges persist due to high training memory consumption and storage overhead. Hybrid representations that integrate implicit and explicit features offer a way to mitigate these limitations. However, when applied in parallelized block-wise training, two critical issues arise since reconstruction accuracy deteriorates due to reduced data diversity when training each block independently, and parallel training restricts the number of divided blocks to the available number of GPUs. To address these issues, we propose Momentum-GS, a novel approach that leverages momentum-based self-distillation to promote consistency and accuracy across the blocks while decoupling the number of blocks from the physical GPU count. Our method maintains a teacher Gaussian decoder updated with momentum, ensuring a stable reference during training. This teacher provides each block with global guidance in a self-distillation manner, promoting spatial consistency in reconstruction. To further ensure consistency across the blocks, we incorporate block weighting, dynamically adjusting each block's weight according to its reconstruction accuracy. Extensive experiments on large-scale scenes show that our method consistently outperforms existing techniques, achieving a 18.7% improvement in LPIPS over CityGaussian with much fewer divided blocks and establishing a new state of the art",
    "checked": true,
    "id": "ce90b76cc0a1fa006ff04bd5837d96f60b8e260e",
    "semantic_title": "momentum-gs: momentum gaussian self-distillation for high-quality large scene reconstruction",
    "citation_count": 10,
    "authors": [
      "Jixuan Fan",
      "Wanhua Li",
      "Yifei Han",
      "Tianru Dai",
      "Yansong Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_SEHDR_Single-Exposure_HDR_Novel_View_Synthesis_via_3D_Gaussian_Bracketing_ICCV_2025_paper.html": {
    "title": "SEHDR: Single-Exposure HDR Novel View Synthesis via 3D Gaussian Bracketing",
    "volume": "main",
    "abstract": "This paper presents SeHDR, a novel high dynamic range 3D Gaussian Splatting (HDR-3DGS) approach for generating HDR novel views given multi-view LDR images. Unlike existing methods that typically require the multi-view LDR input images to be captured from different exposures, which are tedious to capture and more likely to suffer from errors (e.g., object motion blurs and calibration/alignment inaccuracies), our approach learns the HDR scene representation from multi-view LDR images of a single exposure. Our key insight to this ill-posed problem is that by first estimating **Bracketed 3D Gaussians** (i.e., with different exposures) from single-exposure multi-view LDR images, we may then be able to merge these bracketed 3D Gaussians into an HDR scene representation. Specifically, SeHDR first learns base 3D Gaussians from single-exposure LDR inputs, where the spherical harmonics parameterize colors in a linear color space. We then estimate multiple 3D Gaussians with identical geometry but varying linear colors conditioned on exposure manipulations. Finally, we propose the Differentiable Neural Exposure Fusion (NeEF) to integrate the base and estimated 3D Gaussians into HDR Gaussians for novel view rendering. Extensive experiments demonstrate that SeHDR outperforms existing methods as well as carefully designed baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiyu Li",
      "Haoyuan Wang",
      "Ke Xu",
      "Gerhard Petrus Hancke",
      "Rynson W.H. Lau"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Petrov_TriDi_Trilateral_Diffusion_of_3D_Humans_Objects_and_Interactions_ICCV_2025_paper.html": {
    "title": "TriDi: Trilateral Diffusion of 3D Humans, Objects, and Interactions",
    "volume": "main",
    "abstract": "Modeling 3D human-object interaction (HOI) is a problem of great interest for computer vision and a key enabler for virtual and mixed-reality applications. Existing methods work in a one-way direction: some recover plausible human interactions conditioned on a 3D object; others recover the object pose conditioned on a human pose. Instead, we provide the first unified model - TriDi which works in any direction. Concretely, we generate Human, Object, and Interaction modalities simultaneously with a new three-way diffusion process, allowing to model seven distributions with one network. We implement TriDi as a transformer attending to the various modalities' tokens, thereby discovering conditional relations between them. The user can control the interaction either as a text description of HOI or a contact map. We embed these two representations onto a shared latent space, combining the practicality of text descriptions with the expressiveness of contact maps. Using a single network, TriDi unifies all the special cases of prior work and extends to new ones modeling a family of seven distributions. Remarkably, despite using a single model, TriDi generated samples surpass one-way specialized baselines on GRAB and BEHAVE in terms of both qualitative and quantitative metrics, and demonstrating better diversity. We show applicability of TriDi to scene population, generating object for human-contact datasets, and generalization to unseen object geometry",
    "checked": true,
    "id": "24fa9d70ecfb777592720767d23358ac72434243",
    "semantic_title": "tridi: trilateral diffusion of 3d humans, objects, and interactions",
    "citation_count": 3,
    "authors": [
      "Ilya A. Petrov",
      "Riccardo Marin",
      "Julian Chibane",
      "Gerard Pons-Moll"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Danish_GEOBench-VLM_Benchmarking_Vision-Language_Models_for_Geospatial_Tasks_ICCV_2025_paper.html": {
    "title": "GEOBench-VLM: Benchmarking Vision-Language Models for Geospatial Tasks",
    "volume": "main",
    "abstract": "While numerous recent benchmarks focus on evaluating generic Vision-Language Models (VLMs), they do not effectively address the specific challenges of geospatial applications.Generic VLM benchmarks are not designed to handle the complexities of geospatial data, an essential component for applications such as environmental monitoring, urban planning, and disaster management.Key challenges in the geospatial domain include temporal change detection, large-scale object counting, tiny object detection, and understanding relationships between entities in remote sensing imagery.To bridge this gap, we present GEOBench-VLM, a comprehensive benchmark specifically designed to evaluate VLMs on geospatial tasks, including scene understanding, object counting, localization, fine-grained categorization, segmentation, and temporal analysis. Our benchmark features over 10,000 manually verified instructions and spanning diverse visual conditions, object types, and scales.We evaluate several state-of-the-art VLMs to assess performance on geospatial-specific challenges. The results indicate that although existing VLMs demonstrate potential, they face challenges when dealing with geospatial-specific tasks, highlighting the room for further improvements. Notably, the best-performing LLaVa-OneVision achieves only 41.7% accuracy on MCQs, slightly more than GPT-4o, which is approximately double the random guess performance. Our benchmark will be publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Danish",
      "Muhammad Akhtar Munir",
      "Syed Roshaan Ali Shah",
      "Kartik Kuckreja",
      "Fahad Shahbaz Khan",
      "Paolo Fraccaro",
      "Alexandre Lacoste",
      "Salman Khan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ai_UPP_Unified_Point-Level_Prompting_for_Robust_Point_Cloud_Analysis_ICCV_2025_paper.html": {
    "title": "UPP: Unified Point-Level Prompting for Robust Point Cloud Analysis",
    "volume": "main",
    "abstract": "Pre-trained point cloud analysis models have shown promising advancements in various downstream tasks, yet their effectiveness is typically suffering from low-quality point cloud (i.e., noise and incompleteness), which is a common issue in real scenarios due to casual object occlusions and unsatisfactory data collected by 3D sensors. To this end, existing methods focus on enhancing point cloud quality by developing dedicated denoising and completion models. However, due to the isolation between the point cloud enhancement and downstream tasks, these methods fail to work in various real-world domains. In addition, the conflicting objectives between denoising and completing tasks further limit the ensemble paradigm to preserve critical geometric features. To tackle the above challenges, we propose a unified point-level prompting method that reformulates point cloud denoising and completion as a prompting mechanism, enabling robust analysis in a parameter-efficient manner. We start by introducing a Rectification Prompter to adapt to noisy points through the predicted rectification vector prompts, effectively filtering noise while preserving intricate geometric features essential for accurate analysis. Sequentially, we further incorporate a Completion Prompter to generate auxiliary point prompts based on the rectified point clouds, facilitating their robustness and adaptability. Finally, a Shape-Aware Unit module is exploited to efficiently unify and capture the filtered geometric features for the downstream point cloud analysis. Extensive experiments on four datasets demonstrate the superiority and robustness of our method when handling noisy and incomplete point cloud data against existing state-of-the-art methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixiang Ai",
      "Zhenyu Cui",
      "Yuxin Peng",
      "Jiahuan Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_HypDAE_Hyperbolic_Diffusion_Autoencoders_for_Hierarchical_Few-shot_Image_Generation_ICCV_2025_paper.html": {
    "title": "HypDAE: Hyperbolic Diffusion Autoencoders for Hierarchical Few-shot Image Generation",
    "volume": "main",
    "abstract": "Few-shot image generation aims to generate diverse and high-quality images for an unseen class given only a few examples in that class. A key challenge in this task is balancing category consistency and image diversity, which often compete with each other. Moreover, existing methods offer limited control over the attributes of newly generated images. In this work, we propose Hyperbolic Diffusion Autoencoders (HypDAE), a novel approach that operates in hyperbolic space to capture hierarchical relationships among images from seen categories. By leveraging pre-trained foundation models, HypDAE generates diverse new images for unseen categories with exceptional quality by varying stochastic subcodes or semantic codes. Most importantly, the hyperbolic representation introduces an additional degree of control over semantic diversity through the adjustment of radii within the hyperbolic disk. Extensive experiments and visualizations demonstrate that HypDAE significantly outperforms prior methods by achieving a better balance between preserving category-relevant features and promoting image diversity with limited data. Furthermore, HypDAE offers a highly controllable and interpretable generation process",
    "checked": true,
    "id": "bd4e41bb42d9a530292d60bb0687360c16dc95a7",
    "semantic_title": "hypdae: hyperbolic diffusion autoencoders for hierarchical few-shot image generation",
    "citation_count": 1,
    "authors": [
      "Lingxiao Li",
      "Kaixuan Fan",
      "Boqing Gong",
      "Xiangyu Yue"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Feng_High-Resolution_Spatiotemporal_Modeling_with_Global-Local_State_Space_Models_for_Video-Based_ICCV_2025_paper.html": {
    "title": "High-Resolution Spatiotemporal Modeling with Global-Local State Space Models for Video-Based Human Pose Estimation",
    "volume": "main",
    "abstract": "Modeling high-resolution spatiotemporal representations, including both global dynamic contexts (e.g., holistic human motion tendencies) and local motion details (e.g., high-frequency changes of keypoints), is essential for video-based human pose estimation (VHPE). Current state-of-the-art methods typically unify spatiotemporal learning within a single type of modeling structure (convolution or attention-based blocks), which inherently have difficulties in balancing global and local dynamic modeling and may bias the network to one of them, leading to suboptimal performance. Moreover, existing VHPE models suffer from quadratic complexity when capturing global dependencies, limiting their applicability especially for high-resolution sequences. Recently, the state space models (known as Mamba) have demonstrated significant potential in modeling long-range contexts with linear complexity; however, they are restricted to 1D sequential data. In this paper, we present a novel framework that extends Mamba from two aspects to separately learn global and local high-resolution spatiotemporal representations for VHPE. Specifically, we first propose a Global Spatiotemporal Mamba, which performs 6D selective space-time scan and spatial- and temporal-modulated scan merging to efficiently extract global representations from high-resolution sequences. We further introduce a windowed space-time scan-based Local Refinement Mamba to enhance the high-frequency details of localized keypoint motions. Extensive experiments on four benchmark datasets demonstrate that the proposed model outperforms state-of-the-art VHPE approaches while achieving better computational trade-offs",
    "checked": true,
    "id": "d79e4ce863edee19e9de25ae209e92e2a3d7b0cc",
    "semantic_title": "high-resolution spatiotemporal modeling with global-local state space models for video-based human pose estimation",
    "citation_count": 0,
    "authors": [
      "Runyang Feng",
      "Hyung Jin Chang",
      "Tze Ho Elden Tse",
      "Boeun Kim",
      "Yi Chang",
      "Yixing Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hu_ShadowHack_Hacking_Shadows_via_Luminance-Color_Divide_and_Conquer_ICCV_2025_paper.html": {
    "title": "ShadowHack: Hacking Shadows via Luminance-Color Divide and Conquer",
    "volume": "main",
    "abstract": "Shadows introduce challenges such as reduced brightness, texture deterioration, and color distortion in images, complicating a holistic solution. This study presents ShadowHack, a divide-and-conquer strategy that tackles these complexities by decomposing the original task into luminance recovery and color remedy. To brighten shadow regions and repair the corrupted textures in the luminance space, we customize LRNet, a U-shaped network with a rectified outreach attention module, to enhance information interaction and recalibrate contaminated attention maps. With luminance recovered, CRNet then leverages cross-attention mechanisms to revive vibrant colors, producing visually compelling results. Extensive experiments on multiple datasets are conducted to demonstrate the superiority of ShadowHack over existing state-of-the-art solutions both quantitatively and qualitatively, highlighting the effectiveness of our design. Our code will be made publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jin Hu",
      "Mingjia Li",
      "Xiaojie Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Leng_REPA-E_Unlocking_VAE_for_End-to-End_Tuning_of_Latent_Diffusion_Transformers_ICCV_2025_paper.html": {
    "title": "REPA-E: Unlocking VAE for End-to-End Tuning of Latent Diffusion Transformers",
    "volume": "main",
    "abstract": "In this paper we tackle a fundamental question: \"Can we train latent diffusion models together with the variational auto-encoder (VAE) tokenizer in an end-to-end manner?\" Traditional deep-learning wisdom dictates that end-to-end training is often preferable when possible. However, for latent diffusion transformers, it is observed that end-to-end training both VAE and diffusion-model using standard diffusion-loss is ineffective, even causing a degradation in final performance. We show that while diffusion loss is ineffective, end-to-end training can be unlocked through the representation-alignment (REPA) loss -- allowing both VAE and diffusion model to be jointly tuned during the training process. Despite its simplicity, the proposed training recipe (REPA-E) shows remarkable performance; speeding up diffusion model training by over 17x and 45x over REPA and vanilla training recipes, respectively. Interestingly, we observe that end-to-end tuning with REPA-E also improves the VAE itself; leading to improved latent space structure and downstream generation performance. In terms of final performance, our approach sets a new state-of-the-art; achieving FID of 1.26 and 1.83 with and without classifier-free guidance on ImageNet 256x256. Code is available at https://end2end-diffusion.github.io",
    "checked": false,
    "id": "27a028c9ad4145d52d5e00619a7672ca0429aac2",
    "semantic_title": "repa-e: unlocking vae for end-to-end tuning with latent diffusion transformers",
    "citation_count": 34,
    "authors": [
      "Xingjian Leng",
      "Jaskirat Singh",
      "Yunzhong Hou",
      "Zhenchang Xing",
      "Saining Xie",
      "Liang Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_MOSAIC_Generating_Consistent_Privacy-Preserving_Scenes_from_Multiple_Depth_Views_in_ICCV_2025_paper.html": {
    "title": "MOSAIC: Generating Consistent, Privacy-Preserving Scenes from Multiple Depth Views in Multi-Room Environments",
    "volume": "main",
    "abstract": "We introduce a diffusion-based approach for generating privacy-preserving digital twins of multi-room indoor environments from depth images only. Central to our approach is a novel Multi-view Overlapped Scene Alignment with Implicit Consistency (MOSAIC) model that explicitly considers cross-view dependencies within the same scene in the probabilistic sense. MOSAIC operates through a multi-channel inference-time optimization that avoids error accumulation common in sequential or single-room constraints in panorama-based approaches. MOSAIC scales to complex scenes with zero extra training and provably reduces the variance during denoising process when more overlapping views are added, leading to improved generation quality. Experiments show that MOSAIC outperforms state-of-the-art baselines on image fidelity metrics in reconstructing complex multi-room environments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhixuan Liu",
      "Haokun Zhu",
      "Rui Chen",
      "Jonathan Francis",
      "Soonmin Hwang",
      "Ji Zhang",
      "Jean Oh"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_RoBridge_A_Hierarchical_Architecture_Bridging_Cognition_and_Execution_for_General_ICCV_2025_paper.html": {
    "title": "RoBridge: A Hierarchical Architecture Bridging Cognition and Execution for General Robotic Manipulation",
    "volume": "main",
    "abstract": "Operating robots in open-ended scenarios with diverse tasks is a crucial research and application direction in robotics. While recent progress in natural language processing and large multimodal models has enhanced robots' ability to understand complex instructions, robot manipulation still faces the procedural skill dilemma and the declarative skill dilemma in open environments. Existing methods often compromise cognitive and executive capabilities. To address these challenges, in this paper, we propose RoBridge, a hierarchical intelligent architecture for general robotic manipulation. It consists of a high-level cognitive planner (HCP) based on a large-scale pre-trained vision-language model (VLM), an invariant operable representation (IOR) serving as a symbolic bridge, and a generalist embodied agent (GEA). RoBridge maintains the declarative skill of VLM and unleashes the procedural skill of reinforcement learning, effectively bridging the gap between cognition and execution. RoBridge demonstrates significant performance improvements over existing baselines, achieving a 75% success rate on new tasks and an 83% average success rate in sim-to-real generalization using only five real-world data samples per task. This work represents a significant step towards integrating cognitive reasoning with physical execution in robotic systems, offering a new paradigm for general robotic manipulation",
    "checked": true,
    "id": "6656caa93b491ad8f31de2c91ccef11fd07a32fd",
    "semantic_title": "robridge: a hierarchical architecture bridging cognition and execution for general robotic manipulation",
    "citation_count": 4,
    "authors": [
      "Kaidong Zhang",
      "Rongtao Xu",
      "Pengzhen Ren",
      "Junfan Lin",
      "Hefeng Wu",
      "Liang Lin",
      "Xiaodan Liang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_PersonaCraft_Personalized_and_Controllable_Full-Body_Multi-Human_Scene_Generation_Using_Occlusion-Aware_ICCV_2025_paper.html": {
    "title": "PersonaCraft: Personalized and Controllable Full-Body Multi-Human Scene Generation Using Occlusion-Aware 3D-Conditioned Diffusion",
    "volume": "main",
    "abstract": "We present PersonaCraft, a framework for controllable and occlusion-robust full-body personalized image synthesis of multiple individuals in complex scenes. Current methods struggle with occlusion-heavy scenarios and complete body personalization, as 2D pose conditioning lacks 3D geometry, often leading to ambiguous occlusions and anatomical distortions, and many approaches focus solely on facial identity. In contrast, our PersonaCraft integrates diffusion models with 3D human modeling, employing SMPLx-ControlNet, to utilize 3D geometry like depth and normal maps for robust 3D-aware pose conditioning and enhanced anatomical coherence. To handle fine-grained occlusions, we propose Occlusion Boundary Enhancer Network that exploits depth edge signals with occlusion-focused training, and Occlusion-Aware Classifier-Free Guidance strategy that selectively reinforces conditioning in occluded regions without affecting unoccluded areas. PersonaCraft can seamlessly be combined with Face Identity ControlNet, achieving full-body multi-human personalization and thus marking a significant advancement beyond prior approaches that concentrate only on facial identity. Our dual-pathway body shape representation with SMPLx-based shape parameters and textual refinement, enables precise full-body personalization and flexible user-defined body shape adjustments. Extensive quantitative experiments and user studies demonstrate that PersonaCraft significantly outperforms existing methods in generating high-quality, multi-person images with accurate personalization and robust occlusion handling",
    "checked": true,
    "id": "e4a864286d606947ee0f88312fa470d88d4d81ca",
    "semantic_title": "personacraft: personalized and controllable full-body multi-human scene generation using occlusion-aware 3d-conditioned diffusion",
    "citation_count": 0,
    "authors": [
      "Gwanghyun Kim",
      "Suh Yoon Jeon",
      "Seunggyu Lee",
      "Se Young Chun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ji_Controllable_and_Expressive_One-Shot_Video_Head_Swapping_ICCV_2025_paper.html": {
    "title": "Controllable and Expressive One-Shot Video Head Swapping",
    "volume": "main",
    "abstract": "In this paper, we propose a novel diffusion-based multi-condition controllable framework for video head swapping, which seamlessly transplant a human head from a static image into a dynamic video, while preserving the original body and background of target video, and further allowing to tweak head expressions and movements during swapping as needed. Existing face-swapping methods mainly focus on localized facial replacement neglecting holistic head morphology, while head-swapping approaches struggling with hairstyle diversity and complex backgrounds, and none of these methods allow users to modify the transplanted head's expressions after swapping. To tackle these challenges, our method incorporates several innovative strategies through a unified latent diffusion paradigm. 1) Identity-preserving context fusion: We propose a shape-agnostic mask strategy to explicitly disentangle foreground head identity features from background/body contexts, combining hair enhancement strategy to achieve robust holistic head identity preservation across diverse hair types and complex backgrounds. 2) Expression-aware landmark retargeting and editing: We propose a disentangled 3DMM-driven retargeting module that decouples identity, expression, and head poses, minimizing the impact of original expressions in input images and supporting expression editing. While a scale-aware retargeting strategy is further employed to minimize cross-identity expression distortion for higher transfer precision. Experimental results demonstrate that our method excels in seamless background integration while preserving the identity of the source portrait, as well as showcasing superior expression transfer capabilities applicable to both real and virtual characters",
    "checked": true,
    "id": "d2ab4188c8c05fab0baa7f49b2b455574333b6d1",
    "semantic_title": "controllable and expressive one-shot video head swapping",
    "citation_count": 0,
    "authors": [
      "Chaonan Ji",
      "Jinwei Qi",
      "Peng Zhang",
      "Bang Zhang",
      "Liefeng Bo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_SkySense_V2_A_Unified_Foundation_Model_for_Multi-modal_Remote_Sensing_ICCV_2025_paper.html": {
    "title": "SkySense V2: A Unified Foundation Model for Multi-modal Remote Sensing",
    "volume": "main",
    "abstract": "The multi-modal remote sensing foundation model (MM-RSFM) has significantly advanced various Earth observation tasks, such as urban planning, environmental monitoring, and natural disaster management. However, most existing approaches generally require the training of separate backbone networks for each data modality, leading to redundancy and inefficient parameter utilization. Moreover, prevalent pre-training methods typically apply self-supervised learning (SSL) techniques from natural images without adequately accommodating the characteristics of remote sensing (RS) images, such as the complicated semantic distribution within a single RS image. In this work, we present SkySense V2, a unified MM-RSFM that employs a single transformer backbone to handle multiple modalities. This backbone is pre-trained with a novel SSL strategy tailored to the distinct traits of RS data. In particular, SkySense V2 incorporates an innovative adaptive patch merging module and learnable modality prompt tokens to address challenges related to varying resolutions and limited feature diversity across modalities. In additional, we incorporate the mixture of experts (MoE) module to further enhance the performance of the foundation model. SkySense V2 demonstrates impressive generalization abilities through an extensive evaluation involving 16 datasets over 7 tasks, outperforming SkySense by an average of 1.8 points",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingying Zhang",
      "Lixiang Ru",
      "Kang Wu",
      "Lei Yu",
      "Lei Liang",
      "Yansheng Li",
      "Jingdong Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Towards_Video_Thinking_Test_A_Holistic_Benchmark_for_Advanced_Video_ICCV_2025_paper.html": {
    "title": "Towards Video Thinking Test: A Holistic Benchmark for Advanced Video Reasoning and Understanding",
    "volume": "main",
    "abstract": "Human intelligence requires both correctness and robustness, with the former being foundational for the latter. In video understanding, correctness ensures the accurate interpretation of visual content, and robustness maintains consistent performance in challenging conditions. Despite advances in video large language models (video LLMs), existing benchmarks inadequately reflect the gap between these models and human intelligence in maintaining correctness and robustness in video interpretation. We introduce the Video Turing Test (Video-TT), a benchmark designed to assess if video LLMs can interpret real-world videos as effectively as humans.Video-TT differentiates between errors due to inadequate frame sampling and 1) genuine gaps in understanding complex visual narratives, and 2) evaluates robustness against natural adversarial questions. Video-TT comprises 1,000 YouTube Shorts videos, each with one open-ended question and four adversarial questions that probe visual and narrative complexity. Our evaluation shows a significant gap between video LLMs and human performance, underscoring the need for benchmarks like Video-TT to advance video understanding",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanhan Zhang",
      "Yunice Chew",
      "Yuhao Dong",
      "Aria Leo",
      "Bo Hu",
      "Ziwei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wan_Intra-view_and_Inter-view_Correlation_Guided_Multi-view_Novel_Class_Discovery_ICCV_2025_paper.html": {
    "title": "Intra-view and Inter-view Correlation Guided Multi-view Novel Class Discovery",
    "volume": "main",
    "abstract": "In this paper, we address the problem of novel class discovery (NCD), which aims to cluster novel classes by leveraging knowledge from disjoint known classes. While recent advances have made significant progress in this area, existing NCD methods face two major limitations. First, they primarily focus on single-view data (e.g., images), overlooking the increasingly common multi-view data, such as multi-omics datasets used in disease diagnosis. Second, their reliance on pseudo-labels to supervise novel class clustering often results in unstable performance, as pseudo-label quality is highly sensitive to factors such as data noise and feature dimensionality. To address these challenges, we propose a novel framework named Intra-view and Inter-view Correlation Guided Multi-view Novel Class Discovery (IICMVNCD), which is the first attempt to explore NCD in multi-view setting so far. Specifically, at the intra-view level, leveraging the distributional similarity between known and novel classes, we employ matrix factorization to decompose features into view-specific shared base matrices and factor matrices. The base matrices capture distributional consistency among the two datasets, while the factor matrices model pairwise relationships between samples. At the inter-view level, we utilize view relationships among known classes to guide the clustering of novel classes. This includes generating predicted labels through the weighted fusion of factor matrices and dynamically adjusting view weights of known classes based on the supervision loss, which are then transferred to novel class learning. Experimental results validate the effectiveness of our proposed approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinhang Wan",
      "Jiyuan Liu",
      "Qian Qu",
      "Suyuan Liu",
      "Chuyu Zhang",
      "Fangdi Wang",
      "Xinwang Liu",
      "En Zhu",
      "Kunlun He"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_Decoupled_Diffusion_Sparks_Adaptive_Scene_Generation_ICCV_2025_paper.html": {
    "title": "Decoupled Diffusion Sparks Adaptive Scene Generation",
    "volume": "main",
    "abstract": "Controllable scene generation could reduce the cost of diverse data collection substantially for autonomous driving. Prior works formulate the traffic layout generation as a predictive progress, either by denoising entire sequences at once or by iteratively predicting the next frame. However, full sequence denoising hinders online reaction, while the latter's short-sighted next-frame prediction lacks precise goal-state guidance. Further, the learned model struggles to generate complex or challenging scenarios due to a large number of safe and ordinary driving behaviors from open datasets. To overcome these, we introduce Nexus, a decoupled scene generation framework that improves reactivity and goal conditioning by simulating both ordinal and challenging scenarios from fine-grained tokens with independent noise states. At the core of the decoupled pipeline is the integration of a partial noise-masking training strategy and a noise-aware schedule that ensures timely environmental updates throughout the denoising process. To complement challenging scenario generation, we collect a dataset consisting of complex corner cases. It covers 540 hours of simulated data, including high-risk interactions such as cut-in, sudden braking, and collision. Nexus achieves superior generation realism while preserving reactivity and goal orientation, with a 40% reduction in displacement error. We further demonstrate that Nexus improves closed-loop planning by 20% through data augmentation and showcase its capability in safety-critical data generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunsong Zhou",
      "Naisheng Ye",
      "William Ljungbergh",
      "Tianyu Li",
      "Jiazhi Yang",
      "Zetong Yang",
      "Hongzi Zhu",
      "Christoffer Petersson",
      "Hongyang Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Guo_Unsupervised_Joint_Learning_of_Optical_Flow_and_Intensity_with_Event_ICCV_2025_paper.html": {
    "title": "Unsupervised Joint Learning of Optical Flow and Intensity with Event Cameras",
    "volume": "main",
    "abstract": "Event cameras rely on motion to obtain information about scene appearance. This means that appearance and motion are inherently linked: either both are present and recorded in the event data, or neither is captured. Previous works treat the recovery of these two visual quantities as separate tasks, which does not fit with the above-mentioned nature of event cameras and overlooks the inherent relations between them. We propose an unsupervised learning framework that jointly estimates optical flow (motion) and image intensity (appearance) using a single network. From the data generation model, we newly derive the event-based photometric error as a function of optical flow and image intensity. This error is further combined with the contrast maximization framework to form a comprehensive loss function that provides proper constraints for both flow and intensity estimation. Exhaustive experiments show our method's state-of-the-art performance: in optical flow estimation, it reduces EPE by 20% and AE by 25% compared to unsupervised approaches, while delivering competitive intensity estimation results, particularly in high dynamic range scenarios. Our method also achieves shorter inference time than all other optical flow methods and many of the image reconstruction methods, while they output only one quantity. Project page: https://github.com/tub-rip/E2FAI",
    "checked": true,
    "id": "069b800a16960e7d4be618432f8abe74f640bf6e",
    "semantic_title": "unsupervised joint learning of optical flow and intensity with event cameras",
    "citation_count": 4,
    "authors": [
      "Shuang Guo",
      "Friedhelm Hamann",
      "Guillermo Gallego"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Anshul_Intra-modal_and_Cross-modal_Synchronization_for_Audio-visual_Deepfake_Detection_and_Temporal_ICCV_2025_paper.html": {
    "title": "Intra-modal and Cross-modal Synchronization for Audio-visual Deepfake Detection and Temporal Localization",
    "volume": "main",
    "abstract": "Recent deepfake detection algorithms focus solely on uni-modal or cross-modal inconsistencies. While the former disregards audio-visual correspondence entirely rendering them less effective against multimodal attacks, the latter overlooks inconsistencies in a particular modality. Moreover, many models are single-stage supervised frameworks, effective on specific training data but less generalizable to new manipulations. To address these gaps, we propose a two-stage multimodal framework that first learns intra-modal and cross-modal temporal synchronization on real videos, capturing audio-visual correspondences crucial for deepfake detection and localization. We introduce a Gaussian-targeted loss in our pretraining model to focus on learning relative synchronization patterns across multimodal pairs. Using pretrained features, our approach not only enables classification on fully manipulated videos but also supports a localization module for partial deepfakes with only specific segments spoofed. Moreover, the pretraining stage does not require fine-tuning, thus reducing complexity. Our model, tested on various benchmark datasets, demonstrates strong generalization and precise temporal localization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashutosh Anshul",
      "Shreyas Gopal",
      "Deepu Rajan",
      "Eng Siong Chng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zeng_YOLO-Count_Differentiable_Object_Counting_for_Text-to-Image_Generation_ICCV_2025_paper.html": {
    "title": "YOLO-Count: Differentiable Object Counting for Text-to-Image Generation",
    "volume": "main",
    "abstract": "We propose YOLO-Count, a differentiable open-vocabulary object counting model that tackles both general counting challenges and enables precise quantity control for text-to-image (T2I) generation. A core contribution is the 'cardinality' map, a novel regression target that accounts for variations in object size and spatial distribution. Leveraging representation alignment and a hybrid strong-weak supervision scheme, YOLO-Count bridges the gap between open-vocabulary counting and T2I generation control. Its fully differentiable architecture facilitates gradient-based optimization, enabling accurate object count estimation and fine-grained guidance for generative models. Extensive experiments demonstrate that YOLO-Count achieves state-of-the-art counting accuracy while providing robust and effective quantity control for T2I systems",
    "checked": true,
    "id": "ad7fbca3303db0ab776231ed17527cb3a00d59bb",
    "semantic_title": "yolo-count: differentiable object counting for text-to-image generation",
    "citation_count": 4,
    "authors": [
      "Guanning Zeng",
      "Xiang Zhang",
      "Zirui Wang",
      "Haiyang Xu",
      "Zeyuan Chen",
      "Bingnan Li",
      "Zhuowen Tu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Qian_PriorMotion_Generative_Class-Agnostic_Motion_Prediction_with_Raster-Vector_Motion_Field_Priors_ICCV_2025_paper.html": {
    "title": "PriorMotion: Generative Class-Agnostic Motion Prediction with Raster-Vector Motion Field Priors",
    "volume": "main",
    "abstract": "Reliable spatial and motion perception is essential for safe autonomous navigation. Recently, class-agnostic motion prediction on bird's-eye view (BEV) cell grids derived from LiDAR point clouds has gained significant attention. However, existing frameworks typically perform cell classification and motion prediction on a per-pixel basis, neglecting important motion field priors such as rigidity constraints, temporal consistency, and future interactions between agents. These limitations lead to degraded performance, particularly in sparse and distant regions. To address these challenges, we introduce PriorMotion, an innovative generative framework designed for class-agnostic motion prediction that integrates essential motion priors by modeling them as distributions within a structured latent space. Specifically, our method captures structured motion priors using raster-vector representations and employs a variational autoencoder with distinct dynamic and static components to learn future motion distributions in the latent space. Experiments on the nuScenes dataset demonstrate that PriorMotion outperforms state-of-the-art methods across both traditional metrics and our newly proposed evaluation criteria. Notably, we achieve improvements of approximately 15.24% in accuracy for fast-moving objects, an 3.59% increase in generalization, a reduction of 0.0163 in motion stability, and a 31.52% reduction in prediction errors in distant regions. Further validation on FMCW LiDAR sensors confirms the robustness of our approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kangan Qian",
      "Jinyu Miao",
      "Xinyu Jiao",
      "Ziang Luo",
      "Zheng Fu",
      "Yining Shi",
      "Yunlong Wang",
      "Kun Jiang",
      "Diange Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Scaling_Tumor_Segmentation_Best_Lessons_from_Real_and_Synthetic_Data_ICCV_2025_paper.html": {
    "title": "Scaling Tumor Segmentation: Best Lessons from Real and Synthetic Data",
    "volume": "main",
    "abstract": "AI for tumor segmentation is limited by the lack of large, voxel-wise annotated datasets, which are hard to create and require medical experts. In our proprietary JHH dataset of 3,000 annotated pancreatic tumor scans, we found that AI performance stopped improving after 1,500 scans. With synthetic data, we reached the same performance using only 500 real scans. This finding suggests that synthetic data can steepen data scaling laws, enabling more efficient model training than real data alone. Motivated by these lessons, we created AbdomenAtlas 2.0---a dataset of 10,134 CT scans with a total of 13,223 tumor instances per-voxel manually annotated in six organs (pancreas, liver, kidney, colon, esophagus, and uterus) and 6,511 control scans. Annotated by 23 expert radiologists, it is several orders of magnitude larger than existing public tumor datasets. While we continue expanding the dataset, the current version of AbdomenAtlas 2.0 already provides a strong foundation---based on lessons from the JHH dataset---for training AI to segment tumors in six organs. It achieves notable improvements over public datasets, with a +7% DSC gain on in-distribution tests and +16% on out-of-distribution tests",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Chen",
      "Xinze Zhou",
      "Chen Liu",
      "Hao Chen",
      "Wenxuan Li",
      "Zekun Jiang",
      "Ziyan Huang",
      "Yuxuan Zhao",
      "Dexin Yu",
      "Junjun He",
      "Yefeng Zheng",
      "Ling Shao",
      "Alan Yuille",
      "Zongwei Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Bala_DCT-Shield_A_Robust_Frequency_Domain_Defense_against_Malicious_Image_Editing_ICCV_2025_paper.html": {
    "title": "DCT-Shield: A Robust Frequency Domain Defense against Malicious Image Editing",
    "volume": "main",
    "abstract": "Advancements in diffusion models have enabled effortless image editing via text prompts, raising concerns about image security. Attackers with access to user images can exploit these tools for malicious edits. Recent defenses attempt to protect images by adding a limited noise in the pixel space to disrupt the functioning of diffusion-based editing models. However, the adversarial noise added by previous methods is easily noticeable to the human eye. Moreover, most of these methods are not robust to purification techniques like JPEG compression under a feasible pixel budget. We propose a novel optimization approach that introduces adversarial perturbations directly in the frequency domain by modifying the Discrete Cosine Transform (DCT) coefficients of the input image. By leveraging the JPEG pipeline, our method generates adversarial images that effectively prevent malicious image editing. Extensive experiments across a variety of tasks and datasets demonstrate that our approach introduces fewer visual artifacts while maintaining similar levels of edit protection and robustness to noise purification techniques",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aniruddha Bala",
      "Rohit Chowdhury",
      "Rohan Jaiswal",
      "Siddharth Roheda"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_MobileViCLIP_An_Efficient_Video-Text_Model_for_Mobile_Devices_ICCV_2025_paper.html": {
    "title": "MobileViCLIP: An Efficient Video-Text Model for Mobile Devices",
    "volume": "main",
    "abstract": "Efficient lightweight neural networks have received increasing attention due to their faster reasoning speed and easier deployment on mobile devices. However, existing video models still focus on the larger ViT architecture, and few works attempt to build efficient architecture. Since many efficient contrastive language-image pre-training (CLIP) models have shown strong zero-shot classification and retrieval capability, we attempt to fill the gap in video-text understanding models and propose a fast and efficient video-text model MobileViCLIP with strong zero-shot reasoning capability that can be deployed on mobile devices. In particular, our MobileViCLIP-Small obtains similar zero-shot retrieval performance as InternVideo2-L14 on text-to-video dataset MSR-VTT while being 46.7x faster when deployed on the mobile device. Furthermore, MobileViCLIP-Small can generalize to zero-shot action recognition task and obtains 1.0% better Top-1 accuracy than InternVideo2-S14 while being 5.6x faster on the mobile device. The code is available at https://github.com/MCG-NJU/MobileViCLIP",
    "checked": true,
    "id": "d01e055269348ac18e39a942057d0da0cb60501a",
    "semantic_title": "mobileviclip: an efficient video-text model for mobile devices",
    "citation_count": 0,
    "authors": [
      "Min Yang",
      "Zihan Jia",
      "Zhilin Dai",
      "Sheng Guo",
      "Limin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wong_SignRep_Enhancing_Self-Supervised_Sign_Representations_ICCV_2025_paper.html": {
    "title": "SignRep: Enhancing Self-Supervised Sign Representations",
    "volume": "main",
    "abstract": "Sign language representation learning presents unique challenges due to the complex spatio-temporal nature of signs and the scarcity of labeled datasets. Existing methods often rely either on models pre-trained on general visual tasks, that lack sign-specific features, or use complex multimodal and multi-branch architectures. To bridge this gap, we introduce a scalable, self-supervised framework for sign representation learning. We leverage important inductive (sign) priors during the training of our RGB model. To do this, we leverage simple but important cues based on skeletons while pretraining a masked autoencoder. These sign specific priors alongside feature regularization and an adversarial style agnostic loss provide a powerful backbone. Notably, our model does not require skeletal keypoints during inference, avoiding the limitations of keypoint-based models during downstream tasks. When finetuned, we achieve state-of-the-art performance for sign recognition on the WLASL, ASL-Citizen and NMFs-CSL datasets, using a simpler architecture and with only a single-modality. Beyond recognition, our frozen model excels in sign dictionary retrieval and sign translation, surpassing standard MAE pretraining and skeletal-based representations in retrieval. It also reduces computational costs for training existing sign translation models while maintaining strong performance on Phoenix2014T, CSL-Daily and How2Sign",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryan Wong",
      "Necati Cihan Camgoz",
      "Richard Bowden"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Towards_Comprehensive_Lecture_Slides_Understanding_Large-scale_Dataset_and_Effective_Method_ICCV_2025_paper.html": {
    "title": "Towards Comprehensive Lecture Slides Understanding: Large-scale Dataset and Effective Method",
    "volume": "main",
    "abstract": "Online education has been widespread in worldwide universities and educational institutions. Lecture slides, a fundamental component of online education, contain a wealth of information, playing a crucial role in learning.However, previous works have not yet paid sufficient attention to understanding lecture slides, including the absence of the large-scale dataset and comprehensive understanding tasks. To facilitate the research about lecture slides understanding, we establish the LecSlides-370K, which consists of 25,542 lectures with 370,078 slides across 15 areas. We also introduce two comprehensive tasks, Lecture Summary and Lecture Question Answering (QA), for providing different perspectives of slides understanding. Furthermore, complex and flexible text relations can hinder the understanding of the internal logic of slides. To address this challenge, we propose a novel method, named SlideParser, which includes an auxiliary branch to predict text relations within slides and enhance attention between related texts, thereby improving slides understanding. With extensive experiments, we show the superiority of our proposed method on both LecSlides-370k and SlideVQA. Dataset and code will be released soon",
    "checked": false,
    "id": "79a36c53cc757d22a0426252429136cf141c8939",
    "semantic_title": "velocity and concentration measurements of supersonic underexpanded jets",
    "citation_count": 0,
    "authors": [
      "Enming Zhang",
      "Yuzhe Li",
      "Yuliang Liu",
      "Yingying Zhu",
      "Xiang Bai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cao_Visual_Relation_Diffusion_for_Human-Object_Interaction_Detection_ICCV_2025_paper.html": {
    "title": "Visual Relation Diffusion for Human-Object Interaction Detection",
    "volume": "main",
    "abstract": "Human-object interaction (HOI) detection relies on fine-grained visual understanding to distinguish complex relationships between humans and objects. While recent generative diffusion models have demonstrated remarkable capability in learning detailed visual concepts through pixel-level generation, their potential for interaction-level relationship modeling remains largely unexplored. To bridge this gap, we propose a Visual Relation Diffusion model (VRDiff), which introduces dense visual relation conditions to guide interaction understanding. Specifically, we encode interaction-aware condition representations that capture both spatial responsiveness and contextual semantics of human-object pairs, conditioning the diffusion process purely on visual features rather than text-based inputs. Furthermore, we refine these relation representations through generative feedback from the diffusion model, enhancing HOI detection without requiring image synthesis. Extensive experiments on the HICO-DET benchmark demonstrate that VRDiff achieves competitive results under both standard and zero-shot HOI detection settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ping Cao",
      "Yepeng Tang",
      "Chunjie Zhang",
      "Xiaolong Zheng",
      "Chao Liang",
      "Yunchao Wei",
      "Yao Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Egocentric_Action-aware_Inertial_Localization_in_Point_Clouds_with_Vision-Language_Guidance_ICCV_2025_paper.html": {
    "title": "Egocentric Action-aware Inertial Localization in Point Clouds with Vision-Language Guidance",
    "volume": "main",
    "abstract": "This paper presents a novel inertial localization framework named Egocentric Action-aware Inertial Localization (EAIL), which leverages egocentric action cues from head-mounted IMU signals to localize the target individual within a 3D point cloud. Human inertial localization is challenging due to IMU sensor noise that causes trajectory drift over time. The diversity of human actions further complicates IMU signal processing by introducing various motion patterns. Nevertheless, we observe that some actions captured by the head-mounted IMU correlate with spatial environmental structures (e.g., bending down to look inside an oven, washing dishes next to a sink), thereby serving as spatial anchors to compensate for the localization drift. The proposed EAIL framework learns such correlations via hierarchical multi-modal alignment with vision-language guidance. By assuming that the 3D point cloud of the environment is available, it contrastively learns modality encoders that align short-term egocentric action cues in IMU signals with local environmental features in the point cloud. The learning process is enhanced using concurrently collected vision and language signals to improve multimodal alignment. The learned encoders are then used in reasoning the IMU data and the point cloud over time and space to perform inertial localization. Interestingly, these encoders can further be utilized to recognize the corresponding sequence of actions as a by-product. Extensive experiments demonstrate the effectiveness of the proposed framework over state-of-the-art inertial localization and inertial action recognition baselines",
    "checked": false,
    "id": "153de6cd8c13907d937010719ea6d747351d1aef",
    "semantic_title": "egocentric action-aware inertial localization in point clouds",
    "citation_count": 0,
    "authors": [
      "Mingfang Zhang",
      "Ryo Yonetani",
      "Yifei Huang",
      "Liangyang Ouyang",
      "Ruicong Liu",
      "Yoichi Sato"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Park_Hybrid-TTA_Continual_Test-time_Adaptation_via_Dynamic_Domain_Shift_Detection_ICCV_2025_paper.html": {
    "title": "Hybrid-TTA: Continual Test-time Adaptation via Dynamic Domain Shift Detection",
    "volume": "main",
    "abstract": "Continual Test Time Adaptation (CTTA) has emerged as a critical approach to bridge the domain gap between controlled training environments and real-world scenarios.Since it is important to balance the trade-off between adaptation and stabilization, many studies have tried to accomplish it by either introducing a regulation to fully trainable models or updating a limited portion of the models.This paper proposes Hybrid-TTA, a holistic approach that dynamically selects the instance-wise tuning method for optimal adaptation. Our approach introduces Dynamic Domain Shift Detection (DDSD), which identifies domain shifts by leveraging temporal correlations in input sequences, and dynamically switches between Full or Efficient Tuning for effective adaptation toward varying domain shifts. To maintain model stability, Masked Image Modeling Adaptation (MIMA) leverages auxiliary reconstruction task for enhanced generalization and robustness with minimal computational overhead.Hybrid-TTA achieves 0.6%p gain on the Cityscapes-to-ACDC benchmark dataset for semantic segmentation, surpassing previous state-of-the-art methods. It also delivers about 20-fold increase in FPS compared to the recently proposed fastest methods, offering a robust solution for real-world continual adaptation challenges",
    "checked": true,
    "id": "98bbd3f466902aab584a1f26c5e8ac86a80225ae",
    "semantic_title": "hybrid-tta: continual test-time adaptation via dynamic domain shift detection",
    "citation_count": 0,
    "authors": [
      "Hyewon Park",
      "Hyejin Park",
      "Jueun Ko",
      "Dongbo Min"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xu_NavQ_Learning_a_Q-Model_for_Foresighted_Vision-and-Language_Navigation_ICCV_2025_paper.html": {
    "title": "NavQ: Learning a Q-Model for Foresighted Vision-and-Language Navigation",
    "volume": "main",
    "abstract": "In this work we concentrate on the task of goal-oriented Vision-and-Language Navigation (VLN). Existing methods often make decisions based on historical information, overlooking the future implications and long-term outcomes of the actions. In contrast, we aim to develop a foresighted agent. Specifically, we draw upon Q-learning to train a Q-model using large-scale unlabeled trajectory data, in order to learn the general knowledge regarding the layout and object relations within indoor scenes. This model can generate a Q-feature, analogous to the Q-value in traditional Q-network, for each candidate action, which describes the potential future information that may be observed after taking the specific action. Subsequently, a cross-modal future encoder integrates the task-agnostic Q-feature with navigation instructions to produce a set of action scores reflecting future prospects. These scores, when combined with the original scores based on history, facilitate an A*-style searching strategy to effectively explore the regions that are more likely to lead to the destination. Extensive experiments conducted on widely used goal-oriented VLN datasets validate the effectiveness of the proposed method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peiran Xu",
      "Xicheng Gong",
      "Yadong Mu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_PartField_Learning_3D_Feature_Fields_for_Part_Segmentation_and_Beyond_ICCV_2025_paper.html": {
    "title": "PartField: Learning 3D Feature Fields for Part Segmentation and Beyond",
    "volume": "main",
    "abstract": "We propose PartField, a feedforward approach for learning part-based 3D features, which captures the general concept of parts and their hierarchy without relying on predefined templates or text-based names, and can be applied to open-world 3D shapes across various modalities. PartField requires only a 3D feedforward pass at inference time, significantly improving runtime and robustness compared to prior approaches. Our model is trained by distilling 2D and 3D part proposals from a mix of labeled datasets and image segmentations on large unsupervised datasets, via a contrastive learning formulation. It produces a continuous feature field which can be clustered to yield a hierarchical part decomposition. Comparisons show that PartField is up to 20% more accurate and often orders of magnitude faster than other recent class-agnostic part-segmentation methods. Beyond single-shape part decomposition, consistency in the learned field emerges across shapes, enabling tasks such as co-segmentation and correspondence, which we demonstrate in several applications of these general-purpose, hierarchical, and consistent 3D feature fields. Check our Webpage! https://research.nvidia.com/labs/toronto-ai/partfield-release/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minghua Liu",
      "Mikaela Angelina Uy",
      "Donglai Xiang",
      "Hao Su",
      "Sanja Fidler",
      "Nicholas Sharp",
      "Jun Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Anti-Tamper_Protection_for_Unauthorized_Individual_Image_Generation_ICCV_2025_paper.html": {
    "title": "Anti-Tamper Protection for Unauthorized Individual Image Generation",
    "volume": "main",
    "abstract": "With the advancement of personalized image generation technologies, concerns about forgery attacks that infringe on portrait rights and privacy are growing. To address these concerns, protection perturbation algorithms have been developed to disrupt forgery generation. However, the protection algorithms would become ineffective when forgery attackers apply purification techniques to bypass the protection. To address this issue, we present a novel approach, Anti-Tamper Perturbation (ATP). ATP introduces a tamper-proof mechanism within the perturbation. It consists of protection and authorization perturbations, where the protection perturbation defends against forgery attacks, while the authorization perturbation detects purification-based tampering. Both protection and authorization perturbations are applied in the frequency domain under the guidance of a mask, ensuring that the protection perturbation does not disrupt the authorization perturbation. This design also enables the authorization perturbation to be distributed across all image pixels, preserving its sensitivity to purification-based tampering. ATP demonstrates its effectiveness in defending forgery attacks across various attack settings through extensive experiments, providing a robust solution for protecting individuals' portrait rights and privacy. Our code is available at: https://github.com/Seeyn/Anti-Tamper-Perturbation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zelin Li",
      "Ruohan Zong",
      "Yifan Liu",
      "Ruichen Yao",
      "Yaokun Liu",
      "Yang Zhang",
      "Dong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chowdhury_EgoAdapt_Adaptive_Multisensory_Distillation_and_Policy_Learning_for_Efficient_Egocentric_ICCV_2025_paper.html": {
    "title": "EgoAdapt: Adaptive Multisensory Distillation and Policy Learning for Efficient Egocentric Perception",
    "volume": "main",
    "abstract": "Modern perception models, particularly those designed for multisensory egocentric tasks, have achieved remarkable performance but often come with substantial computational costs. These high demands pose challenges for real-world deployment, especially in resource-constrained environments. In this paper, we introduce EGOADAPT, a framework that adaptively performs cross-modal distillation and policy learning to enable efficient inference across different egocentric perception tasks, including egocentric action recognition, active speaker localization, and behavior anticipation. Our proposed policy module is adaptable to task-specific action spaces, making it broadly applicable. Experimental results on three challenging egocentric datasets - EPIC-Kitchens, EasyCom, and Aria Everyday Activities - demonstrate that our method significantly enhances efficiency, reducing GMACs by up to 89.09%, parameters up to 82.02%, and energy up to 9.6x, while still on-par and in many cases outperforming the performance of corresponding state-of-the-art models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanjoy Chowdhury",
      "Subrata Biswas",
      "Sayan Nag",
      "Tushar Nagarajan",
      "Calvin Murdock",
      "Ishwarya Ananthabhotla",
      "Yijun Qian",
      "Vamsi Krishna Ithapu",
      "Dinesh Manocha",
      "Ruohan Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lee_EVT_Efficient_View_Transformation_for_Multi-Modal_3D_Object_Detection_ICCV_2025_paper.html": {
    "title": "EVT: Efficient View Transformation for Multi-Modal 3D Object Detection",
    "volume": "main",
    "abstract": "Multi-modal sensor fusion in Bird's Eye View (BEV) representation has become the leading approach for 3D object detection. However, existing methods often rely on depth estimators or transformer encoders to transform image features into BEV space, which reduces robustness or introduces significant computational overhead. Moreover, the insufficient geometric guidance in view transformation results in ray-directional misalignments, limiting the effectiveness of BEV representations. To address these challenges, we propose Efficient View Transformation (EVT), a novel 3D object detection framework that constructs a well-structured BEV representation, improving both accuracy and efficiency. Our approach focuses on two key aspects. First, Adaptive Sampling and Adaptive Projection (ASAP), which utilizes LiDAR guidance to generate 3D sampling points and adaptive kernels, enables more effective transformation of image features into BEV space and a refined BEV representation. Second, an improved query-based detection framework, incorporating group-wise mixed query selection and geometry-aware cross-attention, effectively captures both the common properties and the geometric structure of objects in the transformer decoder. On the nuScenes test set, EVT achieves state-of-the-art performance of 75.3% NDS with real-time inference speed",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongjin Lee",
      "Hyeon-Mun Jeong",
      "Yurim Jeon",
      "Sanghyun Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Peng_Mitigating_Object_Hallucinations_via_Sentence-Level_Early_Intervention_ICCV_2025_paper.html": {
    "title": "Mitigating Object Hallucinations via Sentence-Level Early Intervention",
    "volume": "main",
    "abstract": "Multimodal large language models (MLLMs) have revolutionized cross-modal understanding but continue to struggle with hallucinations - fabricated content contradicting visual inputs. Existing hallucination mitigation methods either incur prohibitive computational costs or introduce distribution mismatches between training data and model outputs. We identify a critical insight: hallucinations predominantly emerge at the early stages of text generation and propagate through subsequent outputs. To address this, we propose SENTINEL (Sentence-level Early iNtervention Through IN-domain prEference Learning), a framework that eliminates dependency on human annotations. Specifically, we first bootstrap high-quality in-domain preference pairs by iteratively sampling model outputs, validating object existence through cross-checking with two open-vocabulary detectors, and classifying sentences into hallucinated/non-hallucinated categories. Subsequently, we use context-coherent positive samples and hallucinated negative samples to iteratively build context-aware preference data. Finally, we train models using a context-aware preference loss (C-DPO) that emphasizes discriminative learning at the sentence level where hallucinations initially manifest. Experimental results show that SENTINEL can reduce hallucinations by 90% over the original model and outperforms the previous state-of-the-art method on both the hallucination benchmarks and general capabilities benchmarks, manifesting its superiority and generalization ability. The models, datasets, and code are available at https://github.com/pspdada/SENTINEL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shangpin Peng",
      "Senqiao Yang",
      "Li Jiang",
      "Zhuotao Tian"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sun_RobAVA_A_Large-scale_Dataset_and_Baseline_Towards_Video_based_Robotic_ICCV_2025_paper.html": {
    "title": "RobAVA: A Large-scale Dataset and Baseline Towards Video based Robotic Arm Action Understanding",
    "volume": "main",
    "abstract": "Understanding the behaviors of robotic arms is essential for various robotic applications such as logistics management, precision agriculture, and automated manufacturing. However, the lack of large-scale and diverse datasets significantly hinders progress in video-based robotic arm action understanding, highlighting the need for collecting a new large-scale dataset. In particular, our RobAVA contains 40k video sequences with video-level fine-grained annotations, covering basic actions such as picking, pushing, and placing, as well as their combinations in different orders and interactions with various objects. Distinguished to existing action recognition benchmarks, RobAVA includes instances of both normal and anomalous executions for each action category. Our further analysis reveals that the primary challenge in robotic arm action recognition lies in the fact that a complete action consists of a sequence of fundamental, atomic behaviors, requiring models to learn the inter-relationships among them. To this end, we propose a novel baseline approach, AGPT-Net, which re-defines the problem of understanding robotic arm actions as a task of aligning video sequences with atomic attributes.To enhance AGPT-Net's ability to distinguish normal and anomalous action instances, we introduce a joint semantic space constraint between category and attribute semantics, thereby amplifying the separation between normal and anomalous attribute representations for each action. We conduct extensive experiments to demonstrate AGPT-Net's superiority over other mainstream recognition models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baoli Sun",
      "Ning Wang",
      "Xinzhu Ma",
      "Anqi Zou",
      "Yihang Lu",
      "Chuixuan Fan",
      "Zhihui Wang",
      "Kun Lu",
      "Zhiyong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Azad_DisenQ_Disentangling_Q-Former_for_Activity-Biometrics_ICCV_2025_paper.html": {
    "title": "DisenQ: Disentangling Q-Former for Activity-Biometrics",
    "volume": "main",
    "abstract": "In this work, we address activity-biometrics, which involves identifying individuals across diverse set of activities. Unlike traditional person identification, this setting introduces additional challenges as identity cues become entangled with motion dynamics and appearance variations, making biometrics feature learning more complex. While additional visual data like pose and/or silhouette help, they often struggle from extraction inaccuracies. To overcome this, we propose a multimodal language-guided framework that replaces reliance on additional visual data with structured textual supervision. At its core, we introduce **DisenQ** (**Disen**tangling **Q**-Former), a unified querying transformer that disentangles biometrics, motion, and non-biometrics features by leveraging structured language guidance. This ensures identity cues remain independent of appearance and motion variations, preventing misidentifications. We evaluate our approach on three activity-based video benchmarks, achieving state-of-the-art performance. Additionally, we demonstrate strong generalization to complex real-world scenario with competitive performance on a traditional video-based identification benchmark, showing the effectiveness of our framework",
    "checked": true,
    "id": "c16ddd40adf30fb22a6fb0844f6f4bd713f8d048",
    "semantic_title": "disenq: disentangling q-former for activity-biometrics",
    "citation_count": 2,
    "authors": [
      "Shehreen Azad",
      "Yogesh Singh Rawat"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hassan_Vision-Language_Neural_Graph_Featurization_for_Extracting_Retinal_Lesions_ICCV_2025_paper.html": {
    "title": "Vision-Language Neural Graph Featurization for Extracting Retinal Lesions",
    "volume": "main",
    "abstract": "Retinopathy comprises a group of retinal disorders that can lead to severe visual impairments or blindness. The heterogeneous morphology of lesions poses a significant challenge in developing robust diagnostic systems. Supervised approaches rely on large labeled datasets and often struggle with generalization. To address these limitations, we propose an unsupervised vision-language neural graph featurization method. This method first segments fundus images into a set of superpixels via Simple Linear Iterative Clustering (SLIC). The superpixels are then decomposed into an undirected graph where each superpixel serves as a node, and spatially adjacent nodes are connected by edges. A Hamiltonian path systematically traverses the graph and iteratively updates and propagates node and edge latent space embeddings throughout the graph until convergence is achieved. Then, a normalized cut separates the converged embeddings into two clusters within a latent space that represent the lesion and healthy superpixels of the input scans. The lesion superpixels are further classified into lesion categories using a prompt-based zero-shot vision-language model. The proposed method is rigorously tested on four public datasets, dubbed ODIR, FIVES, BIOMISA, and IDRiD, achieving F1-scores of 0.89, 0.92, 0.93, and 0.92, respectively, with significant performance gains over state-of-the-art methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taimur Hassan",
      "Anabia Sohail",
      "Muzammal Naseer",
      "Naoufel Werghi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhao_PhysSplat_Efficient_Physics_Simulation_for_3D_Scenes_via_MLLM-Guided_Gaussian_ICCV_2025_paper.html": {
    "title": "PhysSplat: Efficient Physics Simulation for 3D Scenes via MLLM-Guided Gaussian Splatting",
    "volume": "main",
    "abstract": "Recent advancements in 3D generation models have opened new possibilities for simulating dynamic 3D object movements and customizing behaviors, yet creating this content remains challenging. Current methods often require manual assignment of precise physical properties for simulations or rely on video generation models to predict them, which is computationally intensive. In this paper, we rethink the usage of multi-modal large language model (MLLM) in physics-based simulation, and present PhysSplat, a physics-based approach that efficiently endows static 3D objects with interactive dynamics. We begin with detailed scene reconstruction and object-level 3D open-vocabulary segmentation, progressing to multi-view image in-painting. Inspired by human visual reasoning, we propose MLLM-based Physical Property Perception (MLLM-P3) to predict the mean physical properties of objects in a zero-shot manner. The Material Property Distribution Prediction model (MPDP) then estimates physical property distributions via geometry-conditioned probabilistic sampling of MLLM-P3 outputs, reformulating the problem as probability distribution estimation to reduce computational costs. Finally, we simulate objects in 3D scenes with particles sampled via the Physical-Geometric Adaptive Sampling (PGAS) strategy, efficiently capturing complex deformations and significantly reducing computational costs. Extensive experiments and user studies demonstrate that our PhysSplat achieves more realistic motion than state-of-the-art methods within 2 minutes on a single GPU",
    "checked": false,
    "id": "55364d49c1042fccd7e93902f378577418e2f521",
    "semantic_title": "efficient physics simulation for 3d scenes via mllm-guided gaussian splatting",
    "citation_count": 5,
    "authors": [
      "Haoyu Zhao",
      "Hao Wang",
      "Xingyue Zhao",
      "Hao Fei",
      "Hongqiu Wang",
      "Chengjiang Long",
      "Hua Zou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Parihar_Zero-Shot_Depth_Aware_Image_Editing_with_Diffusion_Models_ICCV_2025_paper.html": {
    "title": "Zero-Shot Depth Aware Image Editing with Diffusion Models",
    "volume": "main",
    "abstract": "Diffusion models have transformed image editing but struggle with precise depth-aware control, such as placing objects at a specified depth. Layered representations offer fine-grained control by decomposing an image into separate editable layers. However, existing methods simplistically represent a scene via a set of background and transparent foreground layers while ignoring the scene geometry - limiting their effectiveness for depth-aware editing. We propose Depth-Guided Layer Decomposition - a layering method that decomposes an image into foreground and background layers based on a user-specified depth value, enabling precise depth-aware edits. We further propose Feature Guided Layer Compositing - a zero-shot approach for realistic layer compositing by leveraging generative priors from pretrained diffusion models. Specifically, we guide the internal U-Net features to progressively fuse individual layers into a composite latent at each denoising step. This preserves the structure of individual layers while generating realistic outputs with appropriate color and lighting adjustments without a need for post-hoc harmonization models. We demonstrate our method on two key depth-aware editing tasks: 1) scene compositing by blending the foreground of one scene with the background of another at a specified depth, and; 2) object insertion at a user-defined depth. Our zero-shot approach achieves precise depth ordering and high-quality edits, surpassing specialized scene compositing and object placement baselines, as validated across benchmarks and user studies",
    "checked": false,
    "id": "138b1dbfdb6db6aafe60b8d7484131db12e3542a",
    "semantic_title": "one-shot learning meets depth diffusion in multi-object videos",
    "citation_count": 1,
    "authors": [
      "Rishubh Parihar",
      "Sachidanand VS",
      "R. Venkatesh Babu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_OrderChain_Towards_General_Instruct-Tuning_for_Stimulating_the_Ordinal_Understanding_Ability_ICCV_2025_paper.html": {
    "title": "OrderChain: Towards General Instruct-Tuning for Stimulating the Ordinal Understanding Ability of MLLM",
    "volume": "main",
    "abstract": "Despite the remarkable progress of multimodal large language models (MLLMs), they continue to face challenges in achieving competitive performance on ordinal regression (OR; a.k.a. ordinal classification). To address this issue, this paper presents OrderChain, a novel and general prompting paradigm that improves the ordinal understanding ability of MLLMs by specificity and commonality modeling. Specifically, our OrderChain consists of a set of task-aware prompts to facilitate the specificity modeling of diverse OR tasks and a new range optimization Chain-of-Thought (RO-CoT), which learns a commonality way of thinking about OR tasks by uniformly decomposing them into multiple small-range optimization subtasks. Further, we propose a category recursive division (CRD) method to generate instruction candidate category prompts to support RO-CoT automatic optimization. Comprehensive experiments show that LLaVA model with our OrderChain improves baseline LLaVA significantly on diverse OR datasets, e.g., from 47.5% to 93.2% accuracy on the Adience dataset for age estimation, and from 30.0% to 85.7% accuracy on the Diabetic Retinopathy dataset. Notably, LLaVA with our OrderChain also remarkably outperforms state-of-the-art methods by 27% on accuracy and 0.24 on MAE on the Adience dataset. To our best knowledge, our OrderChain is the first work that augments MLLMs for OR tasks, and the effectiveness is witnessed across a spectrum of OR datasets. Project Page: https://order-chain.github.io/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinhong Wang",
      "Shuo Tong",
      "Jian Liu",
      "Dongqi Tang",
      "Weiqiang Wang",
      "Wentong Li",
      "Hongxia Xu",
      "Danny Z. Chen",
      "Jintai Chen",
      "Jian Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_V2XPnP_Vehicle-to-Everything_Spatio-Temporal_Fusion_for_Multi-Agent_Perception_and_Prediction_ICCV_2025_paper.html": {
    "title": "V2XPnP: Vehicle-to-Everything Spatio-Temporal Fusion for Multi-Agent Perception and Prediction",
    "volume": "main",
    "abstract": "Vehicle-to-everything (V2X) technologies offer a promising paradigm to mitigate the limitations of constrained observability in single-vehicle systems. Prior work primarily focuses on single-frame cooperative perception, which fuses agents' information across different spatial locations but ignores temporal cues and temporal tasks (e.g., temporal perception and prediction). In this paper, we focus on the spatio-temporal fusion in V2X scenarios and design one-step and multi-step communication strategies (when to transmit) as well as examine their integration with three fusion strategies - early, late, and intermediate (what to transmit), providing comprehensive benchmarks with 11 fusion models (how to fuse). Furthermore, we propose V2XPnP, a novel intermediate fusion framework within one-step communication for end-to-end perception and prediction. Our framework employs a unified Transformer-based architecture to effectively model complex spatio-temporal relationships across multiple agents, frames, and high-definition maps. Moreover, we introduce the V2XPnP Sequential Dataset that supports all V2X collaboration modes and addresses the limitations of existing real-world datasets, which are restricted to single-frame or single-mode cooperation. Extensive experiments demonstrate that our framework outperforms state-of-the-art methods in both perception and prediction tasks",
    "checked": true,
    "id": "1ee5717b631c04cfc5df5ef1f86024171afa8dcd",
    "semantic_title": "v2xpnp: vehicle-to-everything spatio-temporal fusion for multi-agent perception and prediction",
    "citation_count": 17,
    "authors": [
      "Zewei Zhou",
      "Hao Xiang",
      "Zhaoliang Zheng",
      "Seth Z. Zhao",
      "Mingyue Lei",
      "Yun Zhang",
      "Tianhui Cai",
      "Xinyi Liu",
      "Johnson Liu",
      "Maheswari Bajji",
      "Xin Xia",
      "Zhiyu Huang",
      "Bolei Zhou",
      "Jiaqi Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Agrawal_Scaling_Action_Detection_AdaTAD_with_Transformer-Enhanced_Temporal-Spatial_Adaptation_ICCV_2025_paper.html": {
    "title": "Scaling Action Detection: AdaTAD++ with Transformer-Enhanced Temporal-Spatial Adaptation",
    "volume": "main",
    "abstract": "Temporal Action Detection (TAD) is essential for analyzing long-form videos by identifying and segmenting actions within untrimmed sequences. While recent innovations like Temporal Informative Adapters (TIA) have improved resolution, memory constraints still limit large video processing. To address this, we introduce AdaTAD++, an enhanced framework that decouples temporal and spatial processing within adapters, organizing them into independently trainable modules. Our novel two-step training strategy first optimizes for high temporal and low spatial resolution, then vice versa, allowing the model to utilize both high spatial and temporal resolutions during inference, while maintaining training efficiency. Additionally, we incorporate a more sophisticated temporal module capable of capturing long-range dependencies more effectively than previous methods. Experiments on benchmark datasets, including ActivityNet-1.3, THUMOS14, and EPIC-Kitchens 100, demonstrate that AdaTAD++ achieves state-of-the-art performance. We also explore various adapter configurations, discussing their trade-offs regarding resource constraints and performance, providing valuable insights into their optimal application",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tanay Agrawal",
      "Abid Ali",
      "Antitza Dantcheva",
      "Francois Bremond"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Fix-CLIP_Dual-Branch_Hierarchical_Contrastive_Learning_via_Synthetic_Captions_for_Better_ICCV_2025_paper.html": {
    "title": "Fix-CLIP: Dual-Branch Hierarchical Contrastive Learning via Synthetic Captions for Better Understanding of Long Text",
    "volume": "main",
    "abstract": "CLIP has shown promising performance across many short-text tasks in a zero-shot manner. However, limited by the input length of the text encoder, CLIP struggles on under-stream tasks with long-text inputs (>77 tokens). To remedy this issue, we propose FIX-CLIP, which includes three novel modules: (1) A dual-branch training pipeline that aligns short and long texts with masked and raw images, respectively, which boosts the long-text representation while preserving the short-text ability. (2) Multiple learnable regional prompts with unidirectional masks in Transformer layers for regional information extraction. (3) A hierarchical feature alignment module in the intermediate encoder layers to promote the consistency of multi-scale features. Furthermore, we collect 30M images and utilize existing MLLMs to synthesize long-text captions for training. Extensive experiments show that FIX-CLIP achieves state-of-the-art performance on both long-text and short-text retrieval benchmarks. For downstream applications, we reveal that FIX-CLIP's text encoder delivers promising performance in a plug-and-play manner for diffusion models with long-text input. The code is available at https://github.com/bcwang-sjtu/Fix-CLIP",
    "checked": true,
    "id": "0aca700936777cb4a35e7c718bbf1dab02b31574",
    "semantic_title": "fix-clip: dual-branch hierarchical contrastive learning via synthetic captions for better understanding of long text",
    "citation_count": 2,
    "authors": [
      "Bingchao Wang",
      "Zhiwei Ning",
      "Jianyu Ding",
      "Xuanang Gao",
      "Yin Li",
      "Dongsheng Jiang",
      "Jie Yang",
      "Wei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cao_MotionCtrl_A_Real-time_Controllable_Vision-Language-Motion_Model_ICCV_2025_paper.html": {
    "title": "MotionCtrl: A Real-time Controllable Vision-Language-Motion Model",
    "volume": "main",
    "abstract": "Human motion generation involves synthesizing coherent human motion sequences conditioned on diverse multimodal inputs and holds significant potential for real-world applications. Despite recent advancements, existing vision-language-motion models (VLMMs) remain limited in achieving this goal. In this paper, we identify the lack of controllability as a critical bottleneck, where VLMMs struggle with diverse human commands, pose initialization, generation of long-term or unseen cases, and fine-grained control over individual body parts. To address these challenges, we introduce MotionCtrl, the first real-time, controllable VLMM with state-of-the-art performance. MotionCtrl achieves its controllability through training on HuMo100M, the largest human motion dataset to date, featuring over 5 million self-collected motions, 100 million multi-task instructional instances, and detailed part-level descriptions that address a long-standing gap in the field. Additionally, we propose a novel part-aware residual quantization technique for motion tokenization, enabling precise control over individual body parts during motion generation. Extensive experiments demonstrate MotionCtrl's superior performance across a wide range of motion benchmarks. Furthermore, we provide strategic design insights and a detailed time efficiency analysis to guide the development of practical motion generators",
    "checked": false,
    "id": "c5c5e7a7e20eef64bb6ad32332a3d22285cb43b6",
    "semantic_title": "being-m0.5: a real-time controllable vision-language-motion model",
    "citation_count": 0,
    "authors": [
      "Bin Cao",
      "Sipeng Zheng",
      "Ye Wang",
      "Lujie Xia",
      "Qianshan Wei",
      "Qin Jin",
      "Jing Liu",
      "Zongqing Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Qiu_Benchmarking_Multimodal_Large_Language_Models_Against_Image_Corruptions_ICCV_2025_paper.html": {
    "title": "Benchmarking Multimodal Large Language Models Against Image Corruptions",
    "volume": "main",
    "abstract": "Multimodal Large Language Models (MLLMs) have made significant strides in visual and language tasks. However, despite their impressive performance on standard datasets, these models encounter considerable robustness challenges when processing corrupted images, raising concerns about their reliability in safety-critical applications. To address this issue, we introduce the MLLM-IC benchmark, specifically designed to assess the performance of MLLMs under image corruption scenarios. MLLM-IC offers a more comprehensive evaluation of corruption robustness compared to existing benchmarks, enabling a multi-dimensional assessment of various MLLM capabilities across a broad range of corruption types. It includes 40 distinct corruption types and 34 low-level multimodal capabilities, each organized into a three-level hierarchical structure. Notably, it is the first corruption robustness benchmark designed to facilitate the evaluation of fine-grained MLLM capabilities. We further evaluate several prominent MLLMs and derive valuable insights into their characteristics. We believe the MLLM-IC benchmark will provide crucial insights into the robustness of MLLMs in handling corrupted images and contribute to the development of more resilient MLLMs",
    "checked": false,
    "id": "9dad4d7c6da4f3bb56a24742998c76a2629ac9fd",
    "semantic_title": "benchmarking large multimodal models against common corruptions",
    "citation_count": 17,
    "authors": [
      "Xinkuan Qiu",
      "Meina Kan",
      "Yongbin Zhou",
      "Shiguang Shan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Guan_ETVA_Evaluation_of_Text-to-Video_Alignment_via_Fine-grained_Question_Generation_and_ICCV_2025_paper.html": {
    "title": "ETVA: Evaluation of Text-to-Video Alignment via Fine-grained Question Generation and Answering",
    "volume": "main",
    "abstract": "Precisely evaluating semantic alignment between text prompts and generated videos remains a challenge in Text-to-Video (T2V) Generation. Existing text-to-video alignment metrics like CLIPScore only generate coarse-grained scores without fine-grained alignment details, failing to align with human preference. To address this limitation, we propose ETVA, a novel Evaluation method of Text-to-Video Alignment via fine-grained question generation and answering. First, a multi-agent system parses prompts into semantic scene graphs to generate atomic questions. Then we design a knowledge-augmented multi-stage reasoning framework for question answering, where an auxiliary LLM first retrieves relevant common-sense knowledge (e.g., physical laws), and then video LLM answers the generated questions through a multi-stage reasoning mechanism. Extensive experiments demonstrate that ETVA achieves a Spearman's correlation coefficient of 58.47, showing a much higher correlation with human judgment than existing metrics, which attain only 31.0. We also construct a comprehensive benchmark specifically designed for text-to-video alignment evaluation, featuring 2k diverse prompts and 12k atomic questions spanning 10 categories. Through a systematic evaluation of 15 existing text-to-video models, we identify their key capabilities and limitations, paving the way for next-generation T2V generation. All codes and datasets will be publicly available soon",
    "checked": true,
    "id": "0085fdf014af3261f4352a24921713d23343966f",
    "semantic_title": "etva: evaluation of text-to-video alignment via fine-grained question generation and answering",
    "citation_count": 2,
    "authors": [
      "Kaisi Guan",
      "Zhengfeng Lai",
      "Yuchong Sun",
      "Peng Zhang",
      "Wei Liu",
      "Kieran Liu",
      "Meng Cao",
      "Ruihua Song"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xin_Adaptive_Routing_of_Text-to-Image_Generation_Requests_Between_Large_Cloud_Model_ICCV_2025_paper.html": {
    "title": "Adaptive Routing of Text-to-Image Generation Requests Between Large Cloud Model and Light-Weight Edge Model",
    "volume": "main",
    "abstract": "Large text-to-image models demonstrate impressive generation capabilities; however, their substantial size necessitates expensive cloud servers for deployment. Conversely, light-weight models can be deployed on edge devices at lower cost but often with inferior generation quality for complex user prompts. To strike a balance between performance and cost, we propose a routing framework, called RouteT2I, which dynamically selects either the large cloud model or the light-weight edge model for each user prompt. Since generated image quality is challenging to measure and compare directly, RouteT2I establishes multi-dimensional quality metrics, particularly, by evaluating the similarity between the generated images and both positive and negative texts that describe each specific quality metric. RouteT2I then predicts the expected quality of the generated images by identifying key tokens in the prompt and comparing their impact on the quality. RouteT2I further introduces the Pareto relative superiority to compare the multi-metric quality of the generated images. Based on this comparison and predefined cost constraints, RouteT2I allocates prompts to either the edge or the cloud. Evaluation reveals that RouteT2I significantly reduces the number of requesting large cloud model while maintaining high-quality image generation",
    "checked": true,
    "id": "e7ebbf0f68620ff7382f1d85c3c32b9e779137b5",
    "semantic_title": "adaptive routing of text-to-image generation requests between large cloud model and light-weight edge model",
    "citation_count": 2,
    "authors": [
      "Zewei Xin",
      "Qinya Li",
      "Chaoyue Niu",
      "Fan Wu",
      "Guihai Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cheng_Outdoor_Monocular_SLAM_with_Global_Scale-Consistent_3D_Gaussian_Pointmaps_ICCV_2025_paper.html": {
    "title": "Outdoor Monocular SLAM with Global Scale-Consistent 3D Gaussian Pointmaps",
    "volume": "main",
    "abstract": "3D Gaussian Splatting (3DGS) has become a popular solution in SLAM due to its high-fidelity and real-time novel view synthesis performance. However, some previous 3DGS SLAM methods employ a differentiable rendering pipeline for tracking, lack geometric priors in outdoor scenes. Other approaches introduce separate tracking modules, but they accumulate errors with significant camera movement, leading to scale drift. To address these challenges, we propose a robust RGB-only outdoor 3DGS SLAM method: S3PO-GS. Technically, we establish a self-consistent tracking module anchored in the 3DGS pointmap, which avoids cumulative scale drift and achieves more precise and robust tracking with fewer iterations. Additionally, we design a patch-based pointmap dynamic mapping module, which introduces geometric priors while avoiding scale ambiguity. This significantly enhances tracking accuracy and the quality of scene reconstruction, making it particularly suitable for complex outdoor environments. Our experiments on the Waymo, KITTI, and DL3DV datasets demonstrate that S3PO-GS achieves state-of-the-art results in novel view synthesis and outperforms other 3DGS SLAM methods in tracking accuracy",
    "checked": true,
    "id": "a1d8c229be51b8a1f41e080607b07c15158f4929",
    "semantic_title": "outdoor monocular slam with global scale-consistent 3d gaussian pointmaps",
    "citation_count": 4,
    "authors": [
      "Chong Cheng",
      "Sicheng Yu",
      "Zijian Wang",
      "Yifan Zhou",
      "Hao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gong_FlowStyler_Artistic_Video_Stylization_via_Transformation_Fields_Transports_ICCV_2025_paper.html": {
    "title": "FlowStyler: Artistic Video Stylization via Transformation Fields Transports",
    "volume": "main",
    "abstract": "Contemporary video stylization approaches struggle to achieve artistic stylization while preserving temporal consistency. While generator-based methods produce visually striking stylized results, they suffer from flickering artifacts in dynamic motion scenarios and require prohibitive computational resources. Conversely, non-generative techniques frequently show either temporal inconsistency or inadequate style preservation.We address these limitations by adapting the physics-inspired transport principles from the Transport-based Neural Style Transfer (TNST) framework (originally developed for volumetric fluid stylization) to enforce inter-frame consistency in video stylization.Our framework employs two complementary transformation fields for artistic stylization: a geometric stylization velocity field governing deformation and an orthogonality-regularized color transfer field managing color adaptations. We further strengthen temporal consistency through two key enhancements to our field architecture: a momentum-preserving strategy mitigating vibration artifacts, and an occlusion-aware temporal lookup strategy addressing motion trailing artifacts. Extensive experiments demonstrate FlowStyler's superior performance across dual dimensions: Compared to generator-based approaches, we achieve 4xlower short-term warping errors, while maintaining comparable style fidelity; Against non-generative methods, FlowStyler attains 22% higher style fidelity with slightly improved temporal stability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuning Gong",
      "Jiaming Chen",
      "Xiaohua Ren",
      "Yuanjun Liao",
      "Yanci Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gao_7DGS_Unified_Spatial-Temporal-Angular_Gaussian_Splatting_ICCV_2025_paper.html": {
    "title": "7DGS: Unified Spatial-Temporal-Angular Gaussian Splatting",
    "volume": "main",
    "abstract": "Real-time rendering of dynamic scenes with view-dependent effects remains a fundamental challenge in computer graphics. While recent advances in Gaussian Splatting have shown promising results separately handling dynamic scenes (4DGS) and view-dependent effects (6DGS), no existing method unifies these capabilities while maintaining real-time performance. We present 7D Gaussian Splatting (7DGS), a unified framework representing scene elements as seven-dimensional Gaussians spanning position (3D), time (1D), and viewing direction (3D). Our key contribution is an efficient conditional slicing mechanism that transforms 7D Gaussians into view- and time-conditioned 3D Gaussians, maintaining compatibility with existing 3D Gaussian Splatting pipelines while enabling joint optimization. Experiments demonstrate that 7DGS outperforms prior methods by up to 7.36 dB in PSNR while achieving real-time rendering (401 FPS) on challenging dynamic scenes with complex view-dependent effects",
    "checked": true,
    "id": "33d27d333e8d51f7f9c33e7eac6990fcae5603d9",
    "semantic_title": "7dgs: unified spatial-temporal-angular gaussian splatting",
    "citation_count": 2,
    "authors": [
      "Zhongpai Gao",
      "Benjamin Planche",
      "Meng  Zheng",
      "Anwesa  Choudhuri",
      "Terrence  Chen",
      "Ziyan Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yuan_Robust_and_Efficient_3D_Gaussian_Splatting_for_Urban_Scene_Reconstruction_ICCV_2025_paper.html": {
    "title": "Robust and Efficient 3D Gaussian Splatting for Urban Scene Reconstruction",
    "volume": "main",
    "abstract": "We present a framework that enables fast reconstruction and real-time rendering of urban-scale scenes while maintaining robustness against appearance variations across multi-view captures. Our approach begins with scene partitioning for parallel training, employing a visibility-based image selection strategy to optimize training efficiency. A controllable level-of-detail (LOD) strategy explicitly regulates Gaussian density under a user-defined budget, enabling efficient training and rendering while maintaining high visual fidelity. The appearance transformation module mitigates the negative effects of appearance inconsistencies across images while enabling flexible adjustments. Additionally, we utilize enhancement modules, such as depth regularization, scale regularization, and antialiasing, to improve reconstruction fidelity. Experimental results demonstrate that our method effectively reconstructs urban-scale scenes and outperforms previous approaches in both efficiency and quality. The source code is available at: https://yzslab.github.io/REUrbanGS",
    "checked": true,
    "id": "5c7daffb93626e88da97fcd8232d59ff73b71cd7",
    "semantic_title": "robust and efficient 3d gaussian splatting for urban scene reconstruction",
    "citation_count": 1,
    "authors": [
      "Zhensheng Yuan",
      "Haozhi Huang",
      "Zhen Xiong",
      "Di Wang",
      "Guanghua Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ke_DWIM_Towards_Tool-aware_Visual_Reasoning_via_Discrepancy-aware_Workflow_Generation__ICCV_2025_paper.html": {
    "title": "DWIM: Towards Tool-aware Visual Reasoning via Discrepancy-aware Workflow Generation & Instruct-Masking Tuning",
    "volume": "main",
    "abstract": "Visual reasoning (VR), which is crucial in many fields for enabling human-like visual understanding, remains highly challenging. Recently, compositional visual reasoning approaches, which leverage the reasoning abilities of large language models (LLMs) with integrated tools to solve problems, have shown promise as more effective strategies than end-to-end VR methods. However, these approaches face limitations, as frozen LLMs lack tool awareness in VR, leading to performance bottlenecks. While leveraging LLMs for reasoning is widely used in other domains, they are not directly applicable to VR due to limited training data, imperfect tools that introduce errors and reduce data collection efficiency in VR, and challenging in fine-tuning on noisy workflows. To address these challenges, we propose DWIM: i) Discrepancy-aware training Workflow generation, which assesses tool usage and extracts more viable workflows for training; and ii) Instruct-Masking fine-tuning, which guides the model to only clone effective actions, enabling the generation of more practical solutions. Our experiments demonstrate that DWIM achieves state-of-the-art performance across various VR tasks, exhibiting strong generalization on multiple widely-used datasets",
    "checked": true,
    "id": "35ef7184a35e2225849ec9414d51360564596b4a",
    "semantic_title": "dwim: towards tool-aware visual reasoning via discrepancy-aware workflow generation & instruct-masking tuning",
    "citation_count": 4,
    "authors": [
      "Fucai Ke",
      "Vijay Kumar B G",
      "Xingjian Leng",
      "Zhixi Cai",
      "Zaid Khan",
      "Weiqing Wang",
      "Pari Delir Haghighi",
      "Hamid Rezatofighi",
      "Manmohan Chandraker"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ge_Iris_Breaking_GUI_Complexity_with_Adaptive_Focus_and_Self-Refining_ICCV_2025_paper.html": {
    "title": "Iris: Breaking GUI Complexity with Adaptive Focus and Self-Refining",
    "volume": "main",
    "abstract": "Digital agents are increasingly employed to automate tasks in interactive digital environments such as web pages, software applications, and operating systems. While text-based agents built on Large Language Models (LLMs) often require frequent updates due to platform-specific APIs, visual agents leveraging Multimodal Large Language Models (MLLMs) offer enhanced adaptability by interacting directly with Graphical User Interfaces (GUIs). However, these agents face significant challenges in visual perception, particularly when handling high-resolution, visually complex digital environments. This paper introduces Iris, a foundational visual agent that addresses these challenges through two key innovations: Information-Sensitive Cropping (ISC) and Self-Refining Dual Learning (SRDL). ISC dynamically identifies and prioritizes visually dense regions using an edge detection algorithm, enabling efficient processing by allocating more computational resources to areas with higher information density. SRDL enhances the agent's ability to handle complex tasks by leveraging a dual-learning loop, where improvements in referring (describing UI elements) reinforce grounding (locating elements) and vice versa, all without requiring additional annotated data. Empirical evaluations demonstrate that Iris achieves state-of-the-art performance across multiple benchmarks with only 850K GUI annotations, outperforming methods using 10x more training data. These improvements further translate to significant gains in both web and OS agent downstream tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiqi Ge",
      "Juncheng Li",
      "Xinglei Pang",
      "Minghe Gao",
      "Kaihang Pan",
      "Wang Lin",
      "Hao Fei",
      "Wenqiao Zhang",
      "Siliang Tang",
      "Yueting Zhuang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xuan_Rethink_Sparse_Signals_for_Pose-guided_Text-to-image_Generation_ICCV_2025_paper.html": {
    "title": "Rethink Sparse Signals for Pose-guided Text-to-image Generation",
    "volume": "main",
    "abstract": "Recent works favored dense signals (e.g., depth, DensePose), as an alternative to sparse signals (e.g., OpenPose), to provide detailed spatial guidance for pose-guided text-to-image generation. However, dense representations raised new challenges including editing difficulties and potential inconsistencies with textual prompts. This fact motivates us to revisit sparse signals for pose guidance, owing to their simplicity and shape-agnostic nature, which remains underexplored. This paper proposes a novel Spatial-Pose ControlNet (SP-Ctrl), equipping sparse signals with robust controllability for pose-guided image generation. Specifically, we extend OpenPose to a learnable spatial representation, making keypoint embeddings discriminative and expressive. Additionally, we introduce keypoint concept learning, which encourages keypoint tokens to attend to the spatial positions of each keypoint, thus improving pose alignment. Experiments on animal- and human-centric image generation tasks demonstrate that our method outperforms recent spatially controllable text-to-image generation approaches under the sparse-pose guidance and even matches the performance of dense signal-based methods. Moreover, SP-Ctrl shows promising capabilities in diverse and cross-species generation through sparse signals. Code is available at https://github.com/DREAMXFAR/SP-Ctrl",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenjie Xuan",
      "Jing Zhang",
      "Juhua Liu",
      "Bo Du",
      "Dacheng Tao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhong_RoboTrom-Nav_A_Unified_Framework_for_Embodied_Navigation_Integrating_Perception_Planning_ICCV_2025_paper.html": {
    "title": "RoboTrom-Nav: A Unified Framework for Embodied Navigation Integrating Perception, Planning, and Prediction",
    "volume": "main",
    "abstract": "In language-guided visual navigation, agents locate target objects in unseen environments using natural language instructions. For reliable navigation in unfamiliar scenes, agents should possess strong perception, planning, and prediction capabilities. Additionally, when agents revisit previously explored areas during long-term navigation, they may retain irrelevant and redundant historical perceptions, leading to suboptimal results. In this work, we propose RoboTron-Nav, a unified framework that integrates p erception, p lanning, and p rediction capabilities through multitask collaborations on navigation and embodied question answering tasks, thereby enhancing navigation performances. Furthermore, RoboTron-Nav employs an adaptive 3D-aware history sampling strategy to effectively and efficiently utilize historical observations. By leveraging large language model, RoboTron-Nav comprehends diverse commands and complex visual scenes, resulting in appropriate navigation actions. RoboTron-Nav achieves an 81.1% success rate in object goal navigation on the \\mathrm CHORES -\\mathbb S benchmark, setting a new state-of-the-art performance",
    "checked": false,
    "id": "1db4673e6e9fa3b79021e5fceaed78d83f88e71c",
    "semantic_title": "robotron-nav: a unified framework for embodied navigation integrating perception, planning, and prediction",
    "citation_count": 3,
    "authors": [
      "Yufeng Zhong",
      "Chengjian Feng",
      "Feng Yan",
      "Fanfan Liu",
      "Liming Zheng",
      "Lin Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_PatchScaler_An_Efficient_Patch-Independent_Diffusion_Model_for_Image_Super-Resolution_ICCV_2025_paper.html": {
    "title": "PatchScaler: An Efficient Patch-Independent Diffusion Model for Image Super-Resolution",
    "volume": "main",
    "abstract": "While diffusion models significantly improve the perceptual quality of super-resolved images, they usually require a large number of sampling steps, resulting in high computational costs and long inference times. Recent efforts have explored reasonable acceleration schemes by reducing the number of sampling steps. However, these approaches treat all regions of the image equally, overlooking the fact that regions with varying levels of reconstruction difficulty require different sampling steps. To address this limitation, we propose PatchScaler, an efficient patch-independent diffusion pipeline for single image super-resolution. Specifically, PatchScaler introduces a Patch-adaptive Group Sampling (PGS) strategy that groups feature patches by quantifying their reconstruction difficulty and establishes shortcut paths with different sampling configurations for each group. To further optimize the patch-level reconstruction process of PGS, we propose a texture prompt that provides rich texture conditional information to the diffusion model. The texture prompt adaptively retrieves texture priors for the target patch from a common reference texture memory. Extensive experiments show that our PatchScaler achieves favorable performance in both quantitative and qualitative evaluations, while significantly speeding up inference. Project page: https://github.com/yongliuy/PatchScaler",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yong Liu",
      "Hang Dong",
      "Jinshan Pan",
      "Qingji Dong",
      "Kai Chen",
      "Rongxiang Zhang",
      "Lean Fu",
      "Fei Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Pathiraja_RefEdit_A_Benchmark_and_Method_for_Improving_Instruction-based_Image_Editing_ICCV_2025_paper.html": {
    "title": "RefEdit: A Benchmark and Method for Improving Instruction-based Image Editing Model on Referring Expressions",
    "volume": "main",
    "abstract": "Despite recent advances in inversion and instruction-based image editing, existing approaches primarily excel at editing single, prominent objects but significantly struggle when applied to complex scenes containing multiple entities. To quantify this gap, we first introduce **`RefEdit-Bench`**, a rigorous real-world benchmark rooted in RefCOCO, where even baselines trained on millions of samples perform poorly.To overcome this limitation, we introduce **`RefEdit`** -- an instruction-based editing model trained on our scalable synthetic data generation pipeline.Our **`RefEdit`**, trained on only 20,000 editing triplets, outperforms the Flux/SD3 model-based baselines trained on millions of data. Extensive evaluations across various benchmarks demonstrate that our model not only excels in referring expression tasks but also enhances performance on traditional benchmarks, achieving state-of-the-art results comparable to closed-source methods.We will release our code, data, and checkpoints",
    "checked": true,
    "id": "e29ae31ea72a7115c5848c9ec9b7b90e772b8a7c",
    "semantic_title": "refedit: a benchmark and method for improving instruction-based image editing model on referring expressions",
    "citation_count": 1,
    "authors": [
      "Bimsara Pathiraja",
      "Maitreya Patel",
      "Shivam Singh",
      "Yezhou Yang",
      "Chitta Baral"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chung_ETA_Energy-based_Test-time_Adaptation_for_Depth_Completion_ICCV_2025_paper.html": {
    "title": "ETA: Energy-based Test-time Adaptation for Depth Completion",
    "volume": "main",
    "abstract": "We propose a method of adapting pretrained depth completion models to test time data in an unsupervised manner. Depth completion models are (pre)trained to produce dense depth maps from pairs of RGB image and sparse depth maps in ideal capture conditions (source domain), e.g., well-illuminated, high signal-to-noise. When models are transferred to capture conditions out of ideal case (target domain), they produce erroneous output dense depth maps due to the covariate shift. To identify cases of out-of-distribution errors, we propose an learn an energy model in the source domain that assigns scalars that represent the likelihood of the output dense depth maps. This energy model is further used to adapt the pretrained depth completion models at test time, leading to our method: Energy-based Test-time Adaptation (ETA). ETA can localize regions of errors as high energy; test-time adaptation involves updating the model weights to minimize the energy, which in turn mitigates the covariate shift. We evaluate ETA across 3 indoor and 3 outdoor datasets, where ETA improves over the previous state of the art by an average of 6.94% on outdoor and 10.23% on indoor settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Younjoon Chung",
      "Hyoungseob Park",
      "Patrick Rim",
      "Xiaoran Zhang",
      "Jihe He",
      "Ziyao Zeng",
      "Safa Cicek",
      "Byung-Woo Hong",
      "James S. Duncan",
      "Alex Wong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_Amodal3R_Amodal_3D_Reconstruction_from_Occluded_2D_Images_ICCV_2025_paper.html": {
    "title": "Amodal3R: Amodal 3D Reconstruction from Occluded 2D Images",
    "volume": "main",
    "abstract": "Most existing image-to-3D models assume that objects are fully visible, ignoring occlusions that commonly occur in real-world scenarios. In this paper, we introduce Amodal3R, a conditional image-to-3D model designed to reconstruct plausible 3D geometry and appearance from partial observations. We extend a \"foundation\" 3D generator by introducing a visible mask-weighted attention mechanism and an occlusion-aware attention layer that explicitly leverages visible and occlusion priors to guide the reconstruction process. We demonstrate that, by training solely on synthetic data, Amodal3R learns to recover full 3D objects even in the presence of occlusions in real scenes. It substantially outperforms state-of-the-art methods that independently perform 2D amodal completion followed by 3D reconstruction, thereby establishing a new benchmark for occlusion-aware 3D reconstruction",
    "checked": true,
    "id": "4f7af27daff614eb4e7c58461743f6405f7c265f",
    "semantic_title": "amodal3r: amodal 3d reconstruction from occluded 2d images",
    "citation_count": 10,
    "authors": [
      "Tianhao Wu",
      "Chuanxia Zheng",
      "Frank Guan",
      "Andrea Vedaldi",
      "Tat-Jen Cham"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Park_A_Unified_Framework_for_Motion_Reasoning_and_Generation_in_Human_ICCV_2025_paper.html": {
    "title": "A Unified Framework for Motion Reasoning and Generation in Human Interaction",
    "volume": "main",
    "abstract": "Recent advancements in large language models (LLMs) have significantly improved their ability to generate natural and contextually appropriate text, enabling more human-like interactions. However, understanding and generating interactive human-like motion, especially involving coordinated interactive motion, remains a challenging problem due to its inherent complexity. To address this, we present MoLaM, the Interactive Motion-LAnguage Model, a unified architecture that jointly processes language and motion modalities for understanding, generating, and controlling interactive motions in multi-turn conversational settings. Unlike prior approaches limited to uni-directional tasks (e.g., text-to-motion or motion-to-text), MoLaM supports a wide range of bi-directional and multi-turn tasks. We also introduce Inter-MT2, a large-scale instruction tuning dataset containing 82.7K multi-turn interactive motion instructions and 153K motion samples, covering tasks such as editing, question answering, and story generation. Leveraging LLMs and motion diffusion models, MoLaM demonstrates strong performance across five interactive motion tasks: motion-to-text, text-to-motion, reaction generation, motion editing, and motion reasoning, outperforming or matching task-specific baselines with a single, unified model",
    "checked": true,
    "id": "f5641a658af8d31a0794b222cd90ffe0edecefd0",
    "semantic_title": "a unified framework for motion reasoning and generation in human interaction",
    "citation_count": 1,
    "authors": [
      "Jeongeun Park",
      "Sungjoon Choi",
      "Sangdoo Yun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yokoyama_Dynamic_Group_Detection_using_VLM-augmented_Temporal_Groupness_Graph_ICCV_2025_paper.html": {
    "title": "Dynamic Group Detection using VLM-augmented Temporal Groupness Graph",
    "volume": "main",
    "abstract": "This paper proposes dynamic human group detection in videos. For detecting complex groups, not only the local appearance features of in-group members but also the global context of the scene are important. Such local and global appearance features in each frame are extracted using a Vision-Language Model (VLM) augmented for group detection in our method. For further improvement, the group structure should be consistent over time. While previous methods are stabilized on the assumption that groups are not changed in a video, our method detects dynamically changing groups by global optimization using a graph with all frames' groupness probabilities estimated by our groupness-augmented CLIP features. Our experimental results demonstrate that our method outperforms state-of-the-art group detection methods on public datasets. Code: https://github.com/irajisamurai/VLM-GroupDetection.git",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaname Yokoyama",
      "Chihiro Nakatani",
      "Norimichi Ukita"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hasson_SciVid_Cross-Domain_Evaluation_of_Video_Models_in_Scientific_Applications_ICCV_2025_paper.html": {
    "title": "SciVid: Cross-Domain Evaluation of Video Models in Scientific Applications",
    "volume": "main",
    "abstract": "In recent years, there has been a proliferation of spatiotemporal foundation models in different scientific disciplines. While promising, these models are often domain-specific and are only assessed within the particular applications for which they are designed. Given that many tasks can be represented as video modeling problems, video foundation models (ViFMs) hold considerable promise as general-purpose domain-agnostic approaches. However, it is not known whether the knowledge acquired on large-scale but potentially out-of-domain data can be effectively transferred across diverse scientific disciplines, and if a single, pretrained ViFM can be competitive with domain-specific baselines. To address this, we introduce SciVid, a comprehensive benchmark comprising five *Sci*entific *Vid*eo tasks, across medical computer vision, animal behavior, and weather forecasting. We adapt six leading ViFMs to SciVid using simple trainable readout modules, establishing strong baselines and demonstrating the potential for effective transfer learning. Specifically, we show that state-of-the-art results can be obtained in several applications by leveraging the general-purpose representations from ViFM backbones. Furthermore, our results reveal the limitations of existing ViFMs, and highlight opportunities for the development of generalizable models for high-impact scientific applications. We release our code at github.com/google-deepmind/scivid to facilitate further research in the development of ViFMs",
    "checked": true,
    "id": "d4448fa7319b61e57539cb564cd687acc84fdb55",
    "semantic_title": "scivid: cross-domain evaluation of video models in scientific applications",
    "citation_count": 1,
    "authors": [
      "Yana Hasson",
      "Pauline Luc",
      "Liliane Momeni",
      "Maks Ovsjanikov",
      "Guillaume Le Moing",
      "Alina Kuznetsova",
      "Ira Ktena",
      "Jennifer J. Sun",
      "Skanda Koppula",
      "Dilara Gokay",
      "Joseph Heyward",
      "Etienne Pot",
      "Andrew Zisserman"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tao_RoboPearls_Editable_Video_Simulation_for_Robot_Manipulation_ICCV_2025_paper.html": {
    "title": "RoboPearls: Editable Video Simulation for Robot Manipulation",
    "volume": "main",
    "abstract": "The development of generalist robot manipulation policies has seen significant progress, driven by large-scale demonstration data across diverse environments. However, the high cost and inefficiency of collecting real-world demonstrations hinder the scalability of data acquisition. While existing simulation platforms enable controlled environments for robotic learning, the challenge of bridging the sim-to-real gap remains. To address these challenges, we propose RoboPearls, an editable video simulation framework for robotic manipulation. Built on 3D Gaussian Splatting (3DGS), RoboPearls enables the construction of photo-realistic, view-consistent simulations from demonstration videos, and supports a wide range of simulation operators, including various object manipulations, powered by advanced modules like Incremental Semantic Distillation (ISD) and 3D regularized NNFM Loss (3D-NNFM). Moreover, by incorporating large language models (LLMs), RoboPearls automates the simulation production process in a user-friendly manner through flexible command interpretation and execution. Furthermore, RoboPearls employs a vision-language model (VLM) to analyze robotic learning issues to close the simulation loop for performance enhancement. To demonstrate the effectiveness of RoboPearls, we conduct extensive experiments on multiple datasets and scenes, including RLBench, COLOSSEUM, Ego4D, Open X-Embodiment, and a real-world robot, which demonstrate our satisfactory simulation performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tang Tao",
      "Likui Zhang",
      "Youpeng Wen",
      "Kaidong Zhang",
      "Jia-Wang Bian",
      "Xia Zhou",
      "Tianyi Yan",
      "Kun Zhan",
      "Peng Jia",
      "Hefeng Wu",
      "Liang Lin",
      "Xiaodan Liang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cao_FreeMorph_Tuning-Free_Generalized_Image_Morphing_with_Diffusion_Model_ICCV_2025_paper.html": {
    "title": "FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model",
    "volume": "main",
    "abstract": "We present FreeMorph, the first tuning-free method for image morphing that accommodates inputs with varying semantics or layouts. Unlike existing methods, which rely on fine-tuning pre-trained diffusion models and are limited by time constraints and semantic/layout discrepancies, FreeMorph delivers high-fidelity image morphing without extensive training. Despite its efficiency and potential, tuning-free methods still face challenges in maintaining high-quality image morphing due to the non-linear nature of the multi-step denoising process and bias inherited from the pre-trained diffusion model. In this paper, we introduce FreeMorph to address this challenge by integrating two key innovations. 1) We first propose a guidance-aware spherical interpolation design that incorporates the explicit guidance from the input images by modifying the self-attention modules, addressing identity loss, and ensuring directional transitions throughout the generated sequences. 2) We further introduce a step-oriented variation trend that blends self-attention modules derived from each input image to achieve controlled and consistent transitions that respect both input images. Our extensive evaluations demonstrate that FreeMorph outperforms existing methods with training that is 10X 50X faster, establishing a new state-of-the-art for image morphing",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yukang Cao",
      "Chenyang Si",
      "Jinghao Wang",
      "Ziwei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tran_ConstStyle_Robust_Domain_Generalization_with_Unified_Style_Transformation_ICCV_2025_paper.html": {
    "title": "ConstStyle: Robust Domain Generalization with Unified Style Transformation",
    "volume": "main",
    "abstract": "Deep neural networks often suffer performance drops when test data distribution differs from training data. Domain Generalization (DG) aims to address this by focusing on domain-invariant features or augmenting data for greater diversity. However, these methods often struggle with limited training domains or significant gaps between seen (training) and unseen (test) domains. To enhance DG robustness, we hypothesize that it is essential for the model to be trained on data from domains that closely resemble unseen test domains--an inherently difficult task due to the absence of prior knowledge about the unseen domains. Accordingly, we propose ConstStyle, a novel approach that leverages a unified domain to capture domain-invariant features and bridge the domain gap with theoretical analysis. During training, all samples are mapped onto this unified domain, optimized for seen domains. During testing, unseen domain samples are projected similarly before predictions. By aligning both training and testing data within this unified domain, ConstStyle effectively reduces the impact of domain shifts, even with large domain gaps or few seen domains. Extensive experiments demonstrate that ConstStyle consistently outperforms existing methods across diverse scenarios. Notably, when only a limited number of seen domains are available, ConstStyle can boost accuracy up to 19.82% compared to the next best approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nam Duong Tran",
      "Nam Nguyen Phuong",
      "Hieu H. Pham",
      "Phi Le Nguyen",
      "My T. Thai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Huynh_Vision-Language_Models_Cant_See_the_Obvious_ICCV_2025_paper.html": {
    "title": "Vision-Language Models Can't See the Obvious",
    "volume": "main",
    "abstract": "We present Saliency Benchmark (SalBench), a novel benchmark designed to assess the capability of Large Vision-Language Models (LVLM) in detecting visually salient features that are readily apparent to humans, such as a large circle amidst a grid of smaller ones. This benchmark focuses on low-level features including color, intensity, and orientation, which are fundamental to human visual processing. Our SalBench consists of images that highlight rare, unusual, or unexpected elements within scenes, and naturally draw human attention. It comprises three novel tasks for evaluating the perceptual capabilities of LVLM: Odd-One-Out Detection, Referring Odd-One-Out, and Visual Referring Odd-One-Out. We perform a comprehensive evaluation of state-of-the-art LVLM using SalBench and our findings reveal a surprising limitation: LVLM struggle to identify seemingly obvious visual anomalies, with even the advanced GPT-4o achieving only 47.6% accuracy on such a simple task. SalBench will be an important step in measuring the capabilities of LVLM that align with the subtle definition of human attention",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ngoc Dung Huynh",
      "Phuc H Le-Khac",
      "Wamiq Reyaz  Para",
      "Ankit Singh",
      "Sanath Narayan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lin_Perspective-Aware_Teaching_Adapting_Knowledge_for_Heterogeneous_Distillation_ICCV_2025_paper.html": {
    "title": "Perspective-Aware Teaching: Adapting Knowledge for Heterogeneous Distillation",
    "volume": "main",
    "abstract": "Knowledge distillation (KD) involves transferring knowledge from a pre-trained heavy teacher model to a lighter student model, thereby reducing the inference cost while maintaining comparable effectiveness. Prior KD techniques typically assume homogeneity between the teacher and student models. However, as technology advances, a wide variety of architectures have emerged, ranging from initial Convolutional Neural Networks (CNNs) to Vision Transformers (ViTs), and Multi-Level Perceptrons (MLPs). Consequently, developing a universal KD framework compatible with any architecture has become an important research topic. In this paper, we introduce a perspective-aware teaching (PAT) KD framework to enable feature distillation across diverse architectures. Our framework comprises two key components. First, we design prompt tuning blocks that incorporate student feedback, allowing teacher features to adapt to the student model's learning process. Second, we propose region-aware attention to mitigate the view mismatch problem between heterogeneous architectures. By leveraging these two modules, effective distillation of intermediate features can be achieved across heterogeneous architectures. Extensive experiments on CIFAR, ImageNet, and COCO demonstrate the superiority of the proposed method. Our code is available at https://github.com/jimmylin0979/PAT.git",
    "checked": true,
    "id": "68f75c9deb8d3f3fe8f5b4f00077d82f105097e8",
    "semantic_title": "perspective-aware teaching: adapting knowledge for heterogeneous distillation",
    "citation_count": 1,
    "authors": [
      "Jhe-Hao Lin",
      "Yi Yao",
      "Chan-Feng Hsu",
      "Hong-Xia Xie",
      "Hong-Han Shuai",
      "Wen-Huang Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wei_GazeGaussian_High-Fidelity_Gaze_Redirection_with_3D_Gaussian_Splatting_ICCV_2025_paper.html": {
    "title": "GazeGaussian: High-Fidelity Gaze Redirection with 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "Gaze estimation encounters generalization challenges when dealing with out-of-distribution data. To address this problem, recent methods use neural radiance fields (NeRF) to generate augmented data. However, existing methods based on NeRF are computationally expensive and lack facial details. 3D Gaussian Splatting (3DGS) has become the prevailing representation of neural fields. While 3DGS has been extensively examined in head avatars, it faces challenges with accurate gaze control and generalization across different subjects. In this work, we propose GazeGaussian, the first high-fidelity gaze redirection method that uses a two-stream 3DGS model to represent the face and eye regions separately. Leveraging the unstructured nature of 3DGS, we develop a novel representation of the eye for rigid eye rotation based on the target gaze direction. To enable synthesis generalization across various subjects, we integrate an expression-guided module to inject subject-specific information into the neural renderer. Comprehensive experiments show that GazeGaussian outperforms existing methods in rendering speed, gaze redirection accuracy, and facial synthesis across multiple datasets. The code is available at: https://ucwxb.github.io/GazeGaussian",
    "checked": true,
    "id": "281e6ebaa31ab31872b6cef82042fd68ea3394a4",
    "semantic_title": "gazegaussian: high-fidelity gaze redirection with 3d gaussian splatting",
    "citation_count": 3,
    "authors": [
      "Xiaobao Wei",
      "Peng Chen",
      "Guangyu Li",
      "Ming Lu",
      "Hui Chen",
      "Feng Tian"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hu_Balancing_Task-invariant_Interaction_and_Task-specific_Adaptation_for_Unified_Image_Fusion_ICCV_2025_paper.html": {
    "title": "Balancing Task-invariant Interaction and Task-specific Adaptation for Unified Image Fusion",
    "volume": "main",
    "abstract": "Unified image fusion aims to integrate complementary information from multi-source images, enhancing image quality through a unified framework applicable to diverse fusion tasks. While treating all fusion tasks as a unified problem facilitates task-invariant knowledge sharing, it often overlooks task-specific characteristics, thereby limiting the overall performance. Existing general image fusion methods incorporate explicit task identification to enable adaptation to different fusion tasks. However, this dependence during inference restricts the model's generalization to unseen fusion tasks. To address these issues, we propose a novel unified image fusion framework named \"TITA\", which dynamically balances both Task-invariant Interaction and Task-specific Adaptation. For task-invariant interaction, we introduce the Interaction-enhanced Pixel Attention (IPA) module to enhance pixel-wise interactions for better multi-source complementary information extraction. For task-specific adaptation, the Operation-based Adaptive Fusion (OAF) module dynamically adjusts operation weights based on task properties. Additionally, we incorporate the Fast Adaptive Multitask Optimization (FAMO) strategy to mitigate the impact of gradient conflicts across tasks during joint training. Extensive experiments demonstrate that TITA not only achieves competitive performance compared to specialized methods across three image fusion scenarios but also exhibits strong generalization to unseen fusion tasks",
    "checked": true,
    "id": "ebde164582b1f2434675b2682773b01f56480ddf",
    "semantic_title": "balancing task-invariant interaction and task-specific adaptation for unified image fusion",
    "citation_count": 0,
    "authors": [
      "Xingyu Hu",
      "Junjun Jiang",
      "Chenyang Wang",
      "Kui Jiang",
      "Xianming Liu",
      "Jiayi Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_Stroke2Sketch_Harnessing_Stroke_Attributes_for_Training-Free_Sketch_Generation_ICCV_2025_paper.html": {
    "title": "Stroke2Sketch: Harnessing Stroke Attributes for Training-Free Sketch Generation",
    "volume": "main",
    "abstract": "Generating sketches guided by reference styles requires precise transfer of stroke attributes, such as line thickness, deformation, and texture sparsity, while preserving semantic structure and content fidelity. To this end, we propose Stroke2Sketch, a novel training-free framework that introduces cross-image stroke attention, a mechanism embedded within self-attention layers to establish fine-grained semantic correspondences and enable accurate stroke attribute transfer. This allows our method to adaptively integrate reference stroke characteristics into content images while maintaining structural integrity. Additionally, we develop adaptive contrast enhancement and semantic-focused attention to reinforce content preservation and foreground emphasis. Stroke2Sketch effectively synthesizes stylistically faithful sketches that closely resemble handcrafted results, outperforming existing methods in expressive stroke control and semantic coherence",
    "checked": true,
    "id": "ba343fe238bedcb229bee7da39c6285ad70937d7",
    "semantic_title": "stroke2sketch: harnessing stroke attributes for training-free sketch generation",
    "citation_count": 0,
    "authors": [
      "Rui Yang",
      "Huining Li",
      "Yiyi Long",
      "Xiaojun Wu",
      "Shengfeng He"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Pan_Mitigating_Catastrophic_Overfitting_in_Fast_Adversarial_Training_via_Label_Information_ICCV_2025_paper.html": {
    "title": "Mitigating Catastrophic Overfitting in Fast Adversarial Training via Label Information Elimination",
    "volume": "main",
    "abstract": "Fast Adversarial Training (FAT) employs the single-step Fast Gradient Sign Method (FGSM) to generate adversarial examples, reducing the computational costs of traditional adversarial training. However, FAT suffers from Catastrophic Overfitting (CO), where models' robust accuracy against multi-step attacks plummets to zero during training. Recent studies indicate that CO occurs because single-step adversarial perturbations contain label information that models exploit for prediction, leading to overfitting and diminished robustness against more complex attacks. In this paper, we discover that after CO occurs, the label information of certain samples can transfer across different samples, significantly increasing the likelihood of modified images being classified as the intended label. This discovery offers a new perspective on why various adversarial initialization strategies are effective. To address this issue, we introduce an innovative FAT strategy that leverages special samples to capture transferable label information and proactively removes potential label information during training, complemented by a non-uniform label smoothing technique to further eliminate label information. Experimental results across three datasets demonstrate that our method maintains competitive robustness against several attacks compared to other FAT approaches, with ablation studies confirming the effectiveness of our methodology. The code is available at https://github.com/fzjcdt/LIET",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Pan",
      "Ke Tang",
      "Qing Li",
      "Xin Yao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhao_Pi-GPS_Enhancing_Geometry_Problem_Solving_by_Unleashing_the_Power_of_ICCV_2025_paper.html": {
    "title": "Pi-GPS: Enhancing Geometry Problem Solving by Unleashing the Power of Diagrammatic Information",
    "volume": "main",
    "abstract": "Geometry problem solving has garnered increasing attention due to its potential applications in intelligent education field. Inspired by the observation that text often introduces ambiguities that diagrams can clarify, this paper presents Pi-GPS, a novel framework that unleashes the power of diagrammatic information to resolve textual ambiguities, an aspect largely overlooked in prior research. Specifically, we design a micro module comprising a rectifier and verifier: the rectifier employs MLLMs to disambiguate text based on the diagrammatic context, while the verifier ensures the rectified output adherence to geometric rules, mitigating model hallucinations. Additionally, we explore the impact of LLMs in theorem predictor based on the disambiguated formal language. Empirical results demonstrate that Pi-GPS surpasses state-of-the-art models, achieving a nearly 10% improvement on Geometry3K over prior neural-symbolic approaches. We hope this work highlights the significance of resolving textual ambiguity in multimodal mathematical reasoning, a crucial factor limiting performance",
    "checked": true,
    "id": "49f454440d008a0234df3a6862378b64d42d1667",
    "semantic_title": "pi-gps: enhancing geometry problem solving by unleashing the power of diagrammatic information",
    "citation_count": 4,
    "authors": [
      "Junbo Zhao",
      "Ting Zhang",
      "Jiayu Sun",
      "Mi Tian",
      "Hua Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_CountSE_Soft_Exemplar_Open-set_Object_Counting_ICCV_2025_paper.html": {
    "title": "CountSE: Soft Exemplar Open-set Object Counting",
    "volume": "main",
    "abstract": "Open-set counting is garnering increasing attention due to its capability to enumerate objects of arbitrary category. It can be generally categorized into two methodologies: text-guided zero-shot counting methods and exemplar-guided few-shot counting methods. Previous text-guided zero-shot methods only provide limited object information through text, resulting in poor performance. Besides, though exemplar-guided few-shot approaches gain better results, they rely heavily on manually annotated visual exemplars, resulting in low efficiency and high labor intensity. Therefore, we propose CountSE, which simultaneously achieves high efficiency and high performance. CountSE is a new text-guided zero-shot object counting algorithm that generates multiple precise soft exemplars at different scales to enhance counting models driven solely by semantics. Specifically, to obtain richer object information and address the diversity in object scales, we introduce Semantic-guided Exemplar Selection, a module that generates candidate soft exemplars at various scales and selects those with high similarity scores. Then, to ensure accuracy and representativeness, Clustering-based Exemplar Filtering is introduced to refine the candidate exemplars by effectively eliminating inaccurate exemplars through clustering analysis. In the text-guided zero-shot setting, CountSE outperforms all state-of-the-art methods on the FSC-147 benchmark by at least 15%. Additionally, experiments on two other widely used datasets demonstrate that CountSE significantly outperforms all previous text-guided zero-shot counting methods and is competitive with the most advanced exemplar-guided few-shot methods. Codes will be available. Code is available at https://github.com/pppppz22/CountSE",
    "checked": false,
    "id": "5c69c3f30252d44bd5c953d887b47c8b681ddb95",
    "semantic_title": "omnicount: multi-label object counting with semantic-geometric priors",
    "citation_count": 6,
    "authors": [
      "Shuai Liu",
      "Peng Zhang",
      "Shiwei Zhang",
      "Wei Ke"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Dai_Beyond_Isolated_Words_Diffusion_Brush_for_Handwritten_Text-Line_Generation_ICCV_2025_paper.html": {
    "title": "Beyond Isolated Words: Diffusion Brush for Handwritten Text-Line Generation",
    "volume": "main",
    "abstract": "Existing handwritten text generation methods primarily focus on isolated words. However, realistic handwritten text demands attention not only to individual words but also to the relationships between them, such as vertical alignment and horizontal spacing. Therefore, generating entire text line emerges as a more promising and comprehensive task. However, this task poses significant challenges, including the accurate modeling of complex style patterns--encompassing both intra- and inter-word relationships--and maintaining content accuracy across numerous characters. To address these challenges, we propose DiffBrush, a novel diffusion-based model for handwritten text-line generation. Unlike existing methods, DiffBrush excels in both style imitation and content accuracy through two key strategies: (1) content-decoupled style learning, which disentangles style from content to better capture intra-word and inter-word style patterns by using column- and row-wise masking; and (2) multi-scale content learning, which employs line and word discriminators to ensure global coherence and local accuracy of textual content. Extensive experiments show that DiffBrush excels in generating high-quality text-lines, particularly in style reproduction and content preservation. Code is available at https://github.com/dailenson/DiffBrush",
    "checked": true,
    "id": "ac398bbb1c0922a196d94f72944bda65c9b2d150",
    "semantic_title": "beyond isolated words: diffusion brush for handwritten text-line generation",
    "citation_count": 0,
    "authors": [
      "Gang Dai",
      "Yifan Zhang",
      "Yutao Qin",
      "Qiangya Guo",
      "Shuangping Huang",
      "Shuicheng Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yu_Wasserstein_Style_Distribution_Analysis_and_Transform_for_Stylized_Image_Generation_ICCV_2025_paper.html": {
    "title": "Wasserstein Style Distribution Analysis and Transform for Stylized Image Generation",
    "volume": "main",
    "abstract": "Large-scale text-to-image diffusion models have achieved remarkable success in image generation, thereby driving the development of stylized image generation technologies. Recent studies introduce style information by empirically replacing specific features in attention blocks with style features. However, the relationship between features and style remains unclear. In this paper, we systematically analyze the relationship between features in attention blocks and style. By quantifying the distribution discrepancy induced by style variations using the Wasserstein distance, we find that features in self-attention blocks exhibit high sensitivity to style compared to features in cross-attention blocks. Our analysis provides valuable insights into the contribution of different features to style. Based on our findings, we propose a novel Wasserstein Style Distribution Transform (WSDT) method, which generates stylized images by transforming the distribution of style-sensitive features to align with that of style features. WSDT applies channel adaptive distribution transform to ensure that information not related to the style is not introduced. Our approach is simple yet efficient, optimization-free, and can be seamlessly integrated into attention-based text-to-image diffusion models. Extensive experiments demonstrate the effectiveness of our approach in stylized image generation tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xi Yu",
      "Xiang Gu",
      "Zhihao Shi",
      "Jian Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ruan_PRE-Mamba_A_4D_State_Space_Model_for_Ultra-High-Frequent_Event_Camera_ICCV_2025_paper.html": {
    "title": "PRE-Mamba: A 4D State Space Model for Ultra-High-Frequent Event Camera Deraining",
    "volume": "main",
    "abstract": "Event cameras excel in high temporal resolution and dynamic range but suffer from dense noise in rainy conditions. Existing event deraining methods face trade-offs between temporal precision, deraining effectiveness, and computational efficiency. In this paper, we propose PRE-Mamba, a novel point-based event camera deraining framework that fully exploits the spatiotemporal characteristics of raw event and rain. Our framework introduces a 4D event cloud representation that integrates dual temporal scales to preserve high temporal precision, a Spatio-Temporal Decoupling and Fusion module (STDF) that enhances deraining capability by enabling shallow decoupling and interaction of temporal and spatial information, and a Multi-Scale State Space Model (MS3M) that captures deeper rain dynamics across dual-temporal and multi-spatial scales with linear computational complexity. Enhanced by frequency-domain regularization, PRE-Mamba achieves superior performance (0.95 SR, 0.91 NR, and 0.4s/M events) with only 0.26M parameters on EventRain-27K, a comprehensive dataset with labeled synthetic and real-world sequences. Moreover, our method generalizes well across varying rain intensities, viewpoints, and even snowy conditions. Code and dataset: https://github.com/softword-tt/PRE-Mamba",
    "checked": true,
    "id": "eaf847ccba1179018e5b52b83e7710686b46b886",
    "semantic_title": "pre-mamba: a 4d state space model for ultra-high-frequent event camera deraining",
    "citation_count": 4,
    "authors": [
      "Ciyu Ruan",
      "Ruishan Guo",
      "Zihang Gong",
      "Jingao Xu",
      "Wenhan Yang",
      "Xinlei Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Future-Aware_Interaction_Network_For_Motion_Forecasting_ICCV_2025_paper.html": {
    "title": "Future-Aware Interaction Network For Motion Forecasting",
    "volume": "main",
    "abstract": "Motion forecasting is a crucial component of autonomous driving systems, enabling the generation of accurate and smooth future trajectories to ensure safe navigation to the destination. In previous methods, potential future trajectories are often absent in the scene encoding stage, which may lead to suboptimal outcomes. Additionally, prior approaches typically employ transformer architectures for spatiotemporal modeling of trajectories and map information, which suffer from the quadratic scaling complexity of the transformer architecture. In this work, we propose an interaction-based method, named Future-Aware Interaction Network, that introduces potential future trajectories into scene encoding for a comprehensive traffic representation. Furthermore, a State Space Model (SSM), specifically Mamba, is introduced for both spatial and temporal modeling. To adapt Mamba for spatial interaction modeling, we propose an adaptive reordering strategy that transforms unordered data into a structured sequence. Additionally, Mamba is employed to refine generated future trajectories temporally, ensuring more consistent predictions. These enhancements not only improve model efficiency but also enhance the accuracy and diversity of predictions. We conduct comprehensive experiments on the widely used Argoverse 1 and Argoverse 2 datasets, demonstrating that the proposed method achieves superior performance compared to previous approaches in a more efficient way. The code is available here",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shijie Li",
      "Chunyu Liu",
      "Xun Xu",
      "Si Yong Yeo",
      "Xulei Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Nguyen_SuMa_A_Subspace_Mapping_Approach_for_Robust_and_Effective_Concept_ICCV_2025_paper.html": {
    "title": "SuMa: A Subspace Mapping Approach for Robust and Effective Concept Erasure in Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "The rapid growth of text-to-image diffusion models has raised concerns about their potential misuse in generat- ing harmful or unauthorized contents. To address these issues, several Concept Erasure methods have been pro- posed. However, most of them fail to achieve both robust- ness, i.e., the ability to robustly remove the target concept., and effectiveness, i.e., maintaining image quality. While few recent techniques successfully achieve these goals for NSFW concepts, none could handle narrow concepts such as copyrighted characters or celebrities. Erasing these nar- row concepts is critical in addressing copyright and legal concerns. However, erasing them is challenging due to their close distances to non-target neighboring concepts, requir- ing finer-grained manipulation. In this paper, we introduce Subspace Mapping (SuMa), a novel method specifically de- signed to achieve both robustness and effectiveness in eas- ing these narrow concepts. SuMa first derives a target sub- space representing the concept to be erased and then neu- tralizes it by mapping it to a reference subspace that mini- mizes the distance between the two. This mapping ensures the target concept is robustly erased while preserving im- age quality. We conduct extensive experiments with SuMa across four tasks: subclass erasure, celebrity erasure, artis- tic style erasure, and instance erasure and compare the results with current state-of-the-art methods. Our method achieves image quality comparable to approaches focused on effectiveness, while also yielding results that are on par with methods targeting completeness",
    "checked": true,
    "id": "13c15ba00c55e1e00580ea68c55c69e32a32a137",
    "semantic_title": "suma: a subspace mapping approach for robust and effective concept erasure in text-to-image diffusion models",
    "citation_count": 0,
    "authors": [
      "Kien Nguyen",
      "Anh Tran",
      "Cuong Pham"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chang_Hierarchical-aware_Orthogonal_Disentanglement_Framework_for_Fine-grained_Skeleton-based_Action_Recognition_ICCV_2025_paper.html": {
    "title": "Hierarchical-aware Orthogonal Disentanglement Framework for Fine-grained Skeleton-based Action Recognition",
    "volume": "main",
    "abstract": "In recent years, skeleton-based action recognition has gained significant attention due to its robustness in varying environmental conditions. However, most existing methods struggle to distinguish fine-grained actions due to subtle motion features, minimal inter-class variation, and they often fail to consider the underlying similarity relationships between action classes. To address these limitations, we propose a Hierarchical-aware Orthogonal Disentanglement framework (HiOD). We disentangle coarse-grained and fine-grained features by employing independent spatial-temporal granularity-aware bases, which encode semantic representations at varying levels of granularity. Additionally, we design a cross-granularity feature interaction mechanism that leverages complementary information between coarse-grained and fine-grained features. We further enhance the learning process through hierarchical prototype contrastive learning, which utilizes the parent class hierarchy to guide the learning of coarse-grained features while ensuring the distinguishability of fine-grained features within child classes. Extensive experiments on FineGYM, FSD-10, NTU RGB+D, and NTU RGB+D 120 datasets demonstrate the superiority of our method in fine-grained action recognition tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haochen Chang",
      "Pengfei Ren",
      "Haoyang Zhang",
      "Liang Xie",
      "Hongbo Chen",
      "Erwei Yin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ma_Stochastic_Interpolants_for_Revealing_Stylistic_Flows_across_the_History_of_ICCV_2025_paper.html": {
    "title": "Stochastic Interpolants for Revealing Stylistic Flows across the History of Art",
    "volume": "main",
    "abstract": "Generative models have made rapid progress in content creation, particularly in synthesizing artworks and capturing stylistic variation. However, most methods operate at the level of individual images, limiting their ability to reveal broader stylistic trends and temporal transitions. We address this by introducing a framework that models stylistic evolution as an optimal transport problem in a learned style space, using stochastic interpolants and dual diffusion implicit bridges to align artistic distributions across time without requiring paired data. A central contribution is a diverse dataset of over 650,000 artworks spanning 500 years, curated with metadata across multiple genres. Together, our method and dataset enable tracing long-range stylistic transitions and plausible futures of individual artworks, supporting fine-grained temporal analysis. This offers a new tool for modeling historical patterns in visual culture and opens up promising directions in visual understanding. Code and dataset: https://github.com/CompVis/Art-fm",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pingchuan Ma",
      "Ming Gui",
      "Johannes Schusterbauer",
      "Xiaopei Yang",
      "Olga Grebenkova",
      "Vincent Tao Hu",
      "Björn Ommer"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ciubotariu_MIORe__VAR-MIORe_Benchmarks_to_Push_the_Boundaries_of_Restoration_ICCV_2025_paper.html": {
    "title": "MIORe & VAR-MIORe: Benchmarks to Push the Boundaries of Restoration",
    "volume": "main",
    "abstract": "We introduce MIORe and VAR-MIORe, two novel multi-task datasets that address critical limitations in current motion restoration benchmarks. Designed with high-frame-rate (1000 FPS) acquisition and professional-grade optics, our datasets capture a broad spectrum of motion scenarios, which include complex ego-camera movements, dynamic multi-subject interactions, and depth-dependent blur effects. By adaptively averaging frames based on computed optical flow metrics, MIORe generates consistent motion blur, and preserves sharp inputs for video frame interpolation and optical flow estimation. VAR-MIORe further extends by spanning a variable range of motion magnitudes, from minimal to extreme, establishing the first benchmark to offer explicit control over motion amplitude. We provide high-resolution, scalable ground truths that challenge existing algorithms under both controlled and adverse conditions, paving the way for next-generation research of various image and video restoration tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "George Ciubotariu",
      "Zhuyun Zhou",
      "Zongwei Wu",
      "Radu Timofte"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Bartolomei_Depth_AnyEvent_A_Cross-Modal_Distillation_Paradigm_for_Event-Based_Monocular_Depth_ICCV_2025_paper.html": {
    "title": "Depth AnyEvent: A Cross-Modal Distillation Paradigm for Event-Based Monocular Depth Estimation",
    "volume": "main",
    "abstract": "Event cameras capture sparse, high-temporal-resolution visual information, making them particularly suitable for challenging environments with high-speed motion and strongly varying lighting conditions. However, the lack of large datasets with dense ground-truth depth annotations hinders learning-based monocular depth estimation from event data. To address this limitation, we propose a cross-modal distillation paradigm to generate dense proxy labels leveraging a Vision Foundation Model (VFM). Our strategy requires an event stream spatially aligned with RGB frames, a simple setup even available off-the-shelf, and exploits the robustness of large-scale VFMs.Additionally, we propose to adapt VFMs, either a vanilla one like Depth Anything v2 (DAv2), or deriving from it a novel recurrent architecture to infer depth from monocular event cameras. We evaluate our approach using synthetic and real-world datasets, demonstrating that i) our cross-modal paradigm achieves competitive performance compared to fully supervised methods without requiring expensive depth annotations, and ii) our VFM-based models achieve state-of-the-art performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luca Bartolomei",
      "Enrico Mannocci",
      "Fabio Tosi",
      "Matteo Poggi",
      "Stefano Mattoccia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kushida_Thermal_Polarimetric_Multi-view_Stereo_ICCV_2025_paper.html": {
    "title": "Thermal Polarimetric Multi-view Stereo",
    "volume": "main",
    "abstract": "This paper introduces a novel method for detailed 3D shape reconstruction utilizing thermal polarization cues. Unlike state-of-the-art methods, the proposed approach is independent of illumination and material properties. In this paper, we formulate a general theory of polarization observation and show that long-wave infrared (LWIR) polarimetric imaging is free from the ambiguities that affect visible polarization analyses. Subsequently, we propose a method for recovering detailed 3D shapes using multi-view thermal polarimetric images. Experimental results demonstrate that our approach effectively reconstructs fine details in transparent, translucent, and heterogeneous objects, outperforming existing techniques",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Takahiro Kushida",
      "Kenichiro Tanaka"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yoshiyasu_MeshMamba_State_Space_Models_for_Articulated_3D_Mesh_Generation_and_ICCV_2025_paper.html": {
    "title": "MeshMamba: State Space Models for Articulated 3D Mesh Generation and Reconstruction",
    "volume": "main",
    "abstract": "In this paper, we introduce MeshMamba, a neural network model for learning 3D articulated mesh models by employing the recently proposed Mamba State Space Models (Mamba-SSMs). MeshMamba is efficient and scalable in handling a large number of input tokens, enabling the generation and reconstruction of body mesh models with more than 10,000 vertices, capturing clothing and hand geometries. The key to effectively learning MeshMamba is the serialization technique of mesh vertices into orderings that are easily processed by Mamba. This is achieved by sorting the vertices based on body part annotations or the 3D vertex locations of a template mesh, such that the ordering respects the structure of articulated shapes. Based on MeshMamba, we design 1) MambaDiff3D, a denoising diffusion model for generating 3D articulated meshes and 2) Mamba-HMR, a 3D human mesh recovery model that reconstructs a human body shape and pose from a single image. Experimental results showed that MambaDiff3D can generate dense 3D human meshes in clothes, with grasping hands, etc., and outperforms previous approaches in the 3D human shape generation task. Additionally, Mamba-HMR extends the capabilities of previous non-parametric human mesh recovery approaches, which were limited to handling body-only poses using around 500 vertex tokens, to the whole-body setting with face and hands, while achieving competitive performance in (near) real-time",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yusuke Yoshiyasu",
      "Leyuan  Sun",
      "Ryusuke Sagawa"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Phung_MH-LVC_Multi-Hypothesis_Temporal_Prediction_for_Learned_Conditional_Residual_Video_Coding_ICCV_2025_paper.html": {
    "title": "MH-LVC: Multi-Hypothesis Temporal Prediction for Learned Conditional Residual Video Coding",
    "volume": "main",
    "abstract": "This work, termed MH-LVC, presents a multi-hypothesis temporal prediction scheme that employs long- and short-term reference frames in a conditional residual video coding framework. Recent temporal context mining approaches to conditional video coding offer superior coding performance. However, the need to store and access a large amount of implicit contextual information extracted from past decoded frames in decoding a video frame poses a challenge due to excessive memory access. Our MH-LVC overcomes this issue by storing multiple long- and short-term reference frames but limiting the number of reference frames used at a time for temporal prediction to two. Our decoded frame buffer management allows the encoder to flexibly utilize the long-term key frames to mitigate temporal cascading errors and the short-term reference frames to minimize prediction errors. Moreover, our buffering scheme enables the temporal prediction structure to be adapted to individual input videos. While this flexibility is common in traditional video codecs, it has not been fully explored for learned video codecs. Extensive experiments show that the proposed method outperforms VTM-17.0 under the low-delay B configuration in terms of PSNR-RGB across commonly used test datasets, and performs comparably to the state-of-the-art learned codecs (e.g. DCVC-FM) while requiring less decoded frame buffer and similar decoding time",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huu-Tai Phung",
      "Zong-Lin Gao",
      "Yi-Chen Yao",
      "Kuan-Wei Ho",
      "Yi-Hsin Chen",
      "Yu-Hsiang Lin",
      "Alessandro Gnutti",
      "Wen-Hsiao Peng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hosu_Image_Intrinsic_Scale_Assessment_Bridging_the_Gap_Between_Quality_and_ICCV_2025_paper.html": {
    "title": "Image Intrinsic Scale Assessment: Bridging the Gap Between Quality and Resolution",
    "volume": "main",
    "abstract": "Image Quality Assessment (IQA) measures and predicts perceived image quality by human observers. Although recent studies have highlighted the critical influence that variations in the scale of an image have on its perceived quality, this relationship has not been systematically quantified.To bridge this gap, we introduce the Image Intrinsic Scale (IIS), defined as the largest scale where an image exhibits its highest perceived quality. We also present the Image Intrinsic Scale Assessment (IISA) task, which involves subjectively measuring and predicting the IIS based on human judgments. We develop a subjective annotation methodology and create the IISA-DB dataset, comprising 785 image-IIS pairs annotated by experts in a rigorously controlled crowdsourcing study with verified reliability. Furthermore, we propose WIISA (Weak-labeling for Image Intrinsic Scale Assessment), a strategy that leverages how the IIS of an image varies with downscaling to generate weak labels. Experiments show that applying WIISA during the training of several IQA methods adapted for IISA consistently improves the performance compared to using only ground-truth labels. We will release the code, dataset, and pre-trained models upon acceptance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vlad Hosu",
      "Lorenzo Agnolucci",
      "Daisuke Iso",
      "Dietmar Saupe"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Meng_Temporal_Rate_Reduction_Clustering_for_Human_Motion_Segmentation_ICCV_2025_paper.html": {
    "title": "Temporal Rate Reduction Clustering for Human Motion Segmentation",
    "volume": "main",
    "abstract": "Human Motion Segmentation (HMS), which aims to partition videos into non-overlapping human motions, has attracted increasing research attention recently. Existing approaches for HMS are mainly dominated by subspace clustering methods, which are grounded on the assumption that high-dimensional temporal data align with a Union-of-Subspaces (UoS) distribution. However, the frames in video capturing complex human motions with cluttered backgrounds may not align well with the UoS distribution. In this paper, we propose a novel approach for HMS, named Temporal Rate Reduction Clustering (\\text TR ^2\\text C ), which jointly learns structured representations and affinity to segment the sequences of frames in video. Specifically, the structured representations learned by \\text TR ^2\\text C enjoy temporally consistency and are aligned well with a UoS structure, which is favorable for addressing the HMS task. We conduct extensive experiments on five benchmark HMS datasets and achieve state-of-the-art performances with different feature extractors. The code is available at: https://github.com/mengxianghan123/TR2C",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xianghan Meng",
      "Zhengyu Tong",
      "Zhiyuan Huang",
      "Chun-Guang Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Khaki_SparseVILA_Decoupling_Visual_Sparsity_for_Efficient_VLM_Inference_ICCV_2025_paper.html": {
    "title": "SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference",
    "volume": "main",
    "abstract": "Vision language models have received increasing attention for their ability to integrate visual and textual understanding, with some capable of processing native-resolution images and long videos. While the capacity to process large visual data unlocks numerous downstream applications, it often introduces significant latency challenges, as the visual tokens dominate the resource consumption. In this work, we introduce SparseVILA, a novel method of query-aware token retrieval to dynamically accelerate the underlying LLM by pruning tokens in the prefill stage while attending to a sparse subset of visual tokens during the decoding phase. By decoupling the context and generation compression, we can migrate the majority of sparsity into the generation stage, enabling query-aware support for multi-turn conversation while achieving a 1.4x speedup on image benchmarks. This approach leads to +5.9% average accuracy improvements on image-centric benchmarks over previous works. Finally, SparseVILA enables efficient long-context/long-generation tasks by achieving a 3.6x and 1.7x speedup in prefill and decoding, respectively",
    "checked": true,
    "id": "a71c8e40586179b75fa9d1d241a0dc249db81354",
    "semantic_title": "sparsevila: decoupling visual sparsity for efficient vlm inference",
    "citation_count": 0,
    "authors": [
      "Samir Khaki",
      "Junxian Guo",
      "Jiaming Tang",
      "Shang Yang",
      "Yukang Chen",
      "Konstantinos N. Plataniotis",
      "Yao Lu",
      "Song Han",
      "Zhijian Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Prabhu_Trust_but_Verify_Programmatic_VLM_Evaluation_in_the_Wild_ICCV_2025_paper.html": {
    "title": "Trust but Verify: Programmatic VLM Evaluation in the Wild",
    "volume": "main",
    "abstract": "Vision-Language Models (VLMs) frequently hallucinate responses to visual queries, undermining their reliability for critical applications. However, quantifying the effect of such hallucinations in free-form responses to open-ended queries requires visually verifying each claim within the response, which is highly challenging. We propose Programmatic VLM Evaluation (PROVE), a new benchmarking paradigm for evaluating VLM responses to open-ended queries. To construct PROVE, we provide a large language model with a high-fidelity scene-graph representation constructed from a detailed image caption, and prompt it to generate i) diverse and challenging question-answer (QA) pairs that test a range of image understanding capabilities, and ii) programs that can be executed over the scene graph object to verify each QA pair. We thus construct a benchmark of 10.6k challenging but grounded visual QA pairs. Next, we propose a scene graph-based evaluation framework to programmatically measure both the helpfulness and truthfulness of a free-form model response without relying on subjective LLM judgments. We extensively benchmark a range of VLMs on PROVE, and uncover a concerning tradeoff where models that provide more helpful responses often hallucinate more, whereas truthful models tend to be less informative. PROVE serves as a foundation for developing next-generation VLMs that balance helpfulness with truthfulness. A snapshot of our dataset is available at https://prove-explorer-anon.netlify.app/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Viraj Prabhu",
      "Senthil Purushwalkam",
      "An Yan",
      "Caiming Xiong",
      "Ran Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/She_Relative_Illumination_Fields_Learning_Medium_and_Light_Independent_Underwater_Scenes_ICCV_2025_paper.html": {
    "title": "Relative Illumination Fields: Learning Medium and Light Independent Underwater Scenes",
    "volume": "main",
    "abstract": "We address the challenge of constructing a consistent and photorealistic Neural Radiance Field in inhomogeneously illuminated, scattering environments with unknown, co-moving light sources. While most existing works on underwater scene representation focus on a static homogeneous illumination, limited attention has been paid to scenarios such as when a robot explores water deeper than a few tens of meters, where sunlight becomes insufficient. To address this, we propose a novel illumination field locally attached to the camera, enabling the capture of uneven lighting effects within the viewing frustum. We combine this with a volumetric medium representation to an overall method that effectively handles interaction between dynamic illumination field and static scattering medium. Evaluation results demonstrate the effectiveness and flexibility of our approach",
    "checked": true,
    "id": "9bea0b6679fe663cf877df43abfde3a858c317cc",
    "semantic_title": "relative illumination fields: learning medium and light independent underwater scenes",
    "citation_count": 0,
    "authors": [
      "Mengkun She",
      "Felix Seegräber",
      "David Nakath",
      "Patricia Schöntag",
      "Kevin Köser"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhu_Embodied_Representation_Alignment_with_Mirror_Neurons_ICCV_2025_paper.html": {
    "title": "Embodied Representation Alignment with Mirror Neurons",
    "volume": "main",
    "abstract": "Mirror neurons are a class of neurons that activate both when an individual observes an action and when they perform the same action. This mechanism reveals a fundamental interplay between action understanding and embodied execution, suggesting that these two abilities are inherently connected. Nonetheless, existing machine learning methods largely overlook this interplay, treating these abilities as separate tasks. In this study, we provide a unified perspective in modeling them through the lens of representation learning. We first observe that their intermediate representations spontaneously align. Inspired by mirror neurons, we further introduce an approach that explicitly aligns the representations of observed and executed actions. Specifically, we employ two linear layers to map the representations to a shared latent space, where contrastive learning enforces the alignment of corresponding representations, effectively maximizing their mutual information. Experiments demonstrate that this simple approach fosters mutual synergy between the two tasks, effectively improving representation quality and generalization",
    "checked": true,
    "id": "eaca32b44037ef524243dac843c8414cd312731b",
    "semantic_title": "embodied representation alignment with mirror neurons",
    "citation_count": 0,
    "authors": [
      "Wentao Zhu",
      "Zhining Zhang",
      "Yuwei Ren",
      "Yin Huang",
      "Hao Xu",
      "Yizhou Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Wave-MambaAD_Wavelet-driven_State_Space_Model_for_Multi-class_Unsupervised_Anomaly_Detection_ICCV_2025_paper.html": {
    "title": "Wave-MambaAD: Wavelet-driven State Space Model for Multi-class Unsupervised Anomaly Detection",
    "volume": "main",
    "abstract": "The Mamba model excels in anomaly detection through efficient long-range dependency modeling and linear complexity. However, Mamba-based anomaly detectors still face two critical challenges: (1) insufficient modeling of diverse local features leading to inaccurate detection of subtle anomalies; (2) spatial-wise scanning mechanism disrupting the spatial continuity of large-scale anomalies, resulting in incomplete localization. To address these challenges, we propose Wave-MambaAD, a wavelet-driven state space model for unified subtle and large-scale anomaly detection. Firstly, to capture subtle anomalies, we design a high-frequency state space model that employs horizontal, vertical, and diagonal scanning mechanisms for processing directionally aligned high-frequency components, enabling precise anomaly detection through multidimensional feature extraction. Secondly, for comprehensive localization of large-scale anomalies, we propose a low-frequency state space model implementing channel-adaptive dynamic scanning mechanisms to maintain structural coherence in global contexts, which facilitates large-scale anomaly detection via adaptive feature integration. Finally, we develop a dynamic spatial enhancement block to improve anomalous feature representation by enhancing feature diversity through coordinated inter-channel communication and adaptive gating mechanisms. Comprehensive experiments on benchmark anomaly detection datasets show that Wave-MambaAD achieves competitive performance at lower parameters and computational costs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiao Zhang",
      "Mingwen Shao",
      "Xinyuan Chen",
      "Xiang Lv",
      "Kai Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Skvrna_MonoSOWA_Scalable_Monocular_3D_Object_Detector_Without_Human_Annotations_ICCV_2025_paper.html": {
    "title": "MonoSOWA: Scalable Monocular 3D Object Detector Without Human Annotations",
    "volume": "main",
    "abstract": "Inferring object 3D position and orientation from a single RGB camera is a foundational task in computer vision with many important applications. Traditionally, 3D object detection methods are trained in a fully-supervised setup, requiring LiDAR and vast amounts of human annotations, which are laborious, costly, and do not scale well with the ever-increasing amounts of data being captured.We present a novel method to train a 3D object detector from a single RGB camera without domain-specific human annotations, making orders of magnitude more data available for training. The method uses newly proposed Local Object Motion Model to disentangle object movement source between subsequent frames, is approximately 700 times faster than previous work and compensates camera focal length differences to aggregate multiple datasets.The method is evaluated on three public datasets, where despite using no human labels, it outperforms prior work by a significant margin. It also shows its versatility as a pre-training tool for fully-supervised training and shows that combining pseudo-labels from multiple datasets can achieve comparable accuracy to using human labels from a single dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jan Skvrna",
      "Lukas Neumann"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_Measuring_the_Impact_of_Rotation_Equivariance_on_Aerial_Object_Detection_ICCV_2025_paper.html": {
    "title": "Measuring the Impact of Rotation Equivariance on Aerial Object Detection",
    "volume": "main",
    "abstract": "Due to the arbitrary orientation of objects in aerial images, rotation equivariance is a critical property for aerial object detectors. However, recent studies on rotation-equivariant aerial object detection remain scarce. Most detectors rely on data augmentation to enable models to learn approximately rotation-equivariant features. A few detectors have constructed rotation-equivariant networks, but due to the breaking of strict rotation equivariance by typical downsampling processes, these networks only achieve approximately rotation-equivariant backbones. Whether strict rotation equivariance is necessary for aerial image object detection remains an open question. In this paper, we implement a strictly rotation-equivariant backbone and neck network with a more advanced network structure and compare it with approximately rotation-equivariant networks to quantitatively measure the impact of rotation equivariance on the performance of aerial image detectors. Additionally, leveraging the inherently grouped nature of rotation-equivariant features, we propose a multi-branch head network that reduces the parameter count while improving detection accuracy. Based on the aforementioned improvements, this study proposes the Multi-branch head rotation-equivariant single-stage Detector (MessDet), which achieves state-of-the-art performance on the challenging aerial image datasets DOTA-v1.0, DOTA-v1.5 and DIOR-R with an exceptionally low parameter count",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiuyu Wu",
      "Xinhao Wang",
      "Xiubin Zhu",
      "Lan Yang",
      "Jiyuan Liu",
      "Xingchen Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_Inter2Former_Dynamic_Hybrid_Attention_for_Efficient_High-Precision_Interactive_Segmentation_ICCV_2025_paper.html": {
    "title": "Inter2Former: Dynamic Hybrid Attention for Efficient High-Precision Interactive Segmentation",
    "volume": "main",
    "abstract": "Interactive segmentation (IS) improves annotation efficiency by segmenting target regions from user prompts, with widespread applications in real-world scenarios. Current approaches face a critical trade-off: dense-token methods achieve superior accuracy and detail preservation but suffer from prohibitively slow processing on CPU devices, while the Segment Anything Model (SAM) advances the field with sparse prompt tokens for fast inference but compromises segmentation quality. In this paper, we propose Inter2Former to address this challenge by optimizing computation allocation in dense-token processing, which introduces four key enhancements. First, we propose Dynamic Prompt Embedding (DPE) that adaptively processes only regions of interest while avoiding additional overhead from background tokens. Second, we introduce Dynamic Hybrid Attention (DHA), which leverages previous segmentation masks to route tokens through either full attention (O(N^2)) for boundary regions or our proposed efficient BSQ attention (O(N)) for non-boundary regions. Third, we develop Hybrid Mixture of Experts (HMoE), which applies similar adaptive computation strategies in FFN modules with CPU-optimized parallel processing. Finally, we present Dynamic Local Upsampling (DLU), a reverse operation of DPE, which localizes objects with a lightweight MLP and performs fine-grained upsampling only in detected regions. Experimental results on high-precision IS benchmarks demonstrate that Inter2Former achieves SOTA performance with high efficiency on CPU devices",
    "checked": false,
    "id": "b7c6b43885291b58051c7c88d5613daf0e75ab77",
    "semantic_title": "inter2former: dynamic hybrid attention for efficient high-precision interactive",
    "citation_count": 0,
    "authors": [
      "You Huang",
      "Lichao Chen",
      "Jiayi Ji",
      "Liujuan Cao",
      "Shengchuan Zhang",
      "Rongrong Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kang_LEGION_Learning_to_Ground_and_Explain_for_Synthetic_Image_Detection_ICCV_2025_paper.html": {
    "title": "LEGION: Learning to Ground and Explain for Synthetic Image Detection",
    "volume": "main",
    "abstract": "The rapid advancements in generative technology have emerged as a double-edged sword. While offering powerful tools that enhance convenience, they also pose significant social concerns. As defenders, current synthetic image detection methods often lack artifact-level textual interpretability and are overly focused on image manipulation detection, and current datasets usually suffer from outdated generators and a lack of fine-grained annotations. In this paper, we introduce SynthScars, a high-quality and diverse dataset consisting of 12,236 fully synthetic images with human-expert annotations. It features 4 distinct image content types, 3 categories of artifacts, and fine-grained annotations covering pixel-level segmentation, detailed textual explanations, and artifact category labels. Furthermore, we propose LEGION (LEarning to Ground and explain for Synthetic Image detectiON), a multimodal large language model (MLLM)-based image forgery analysis framework that integrates artifact detection, segmentation, and explanation. Building upon this capability, we further explore LEGION as a controller, integrating it into image refinement pipelines to guide the generation of higher-quality and more realistic images. Extensive experiments show that LEGION outperforms existing methods across multiple benchmarks, particularly surpassing the second-best traditional expert on SynthScars by 3.31% in mIoU and 7.75% in F1 score. Moreover, the refined images generated under its guidance exhibit stronger alignment with human preferences. More information about LEGION can be found at https://opendatalab.github.io/LEGION",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hengrui Kang",
      "Siwei Wen",
      "Zichen Wen",
      "Junyan Ye",
      "Weijia Li",
      "Peilin Feng",
      "Baichuan Zhou",
      "Bin Wang",
      "Dahua Lin",
      "Linfeng Zhang",
      "Conghui He"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ran_KDA_Knowledge_Diffusion_Alignment_with_Enhanced_Context_for_Video_Temporal_ICCV_2025_paper.html": {
    "title": "KDA: Knowledge Diffusion Alignment with Enhanced Context for Video Temporal Grounding",
    "volume": "main",
    "abstract": "Video Temporal Grounding (VTG) confronts the challenge of bridging the semantic gap between concise textual queries and the rich complexity of video content, further compounded by the difficulty of capturing discriminative features without explicit target cues. To address these challenges, we propose Knowledge Diffusion Alignment (KDA), a framework that leverages the generative prowess of diffusion models. KDA introduces a multi-layer video knowledge extraction module alongside a background residual diffusion model that progressively prunes irrelevant background information from global video features, thereby distilling query-relevant moment knowledge enriched with visual context. By a three-stage training approach that harnesses annotated moment guidance, KDA guarantees that the extracted moment knowledge incorporates the discriminative features necessary for accurate localization. A knowledge prompt reasoning module facilitates the comprehensive interaction and utilization of moment knowledge and multimodal features. Moreover, we introduce a spans-enhanced decoder that selectively integrates spans from multi-modal features, capitalizing on intrinsic alignment cues. Comprehensive experiments on three datasets demonstrate performance that surpasses state-of-the-art methods, attesting to the effectiveness of the proposed framework",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ran Ran",
      "Jiwei Wei",
      "Shiyuan He",
      "Zeyu Ma",
      "Chaoning Zhang",
      "Ning Xie",
      "Yang Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Bin_NormalCrafter_Learning_Temporally_Consistent_Normals_from_Video_Diffusion_Priors_ICCV_2025_paper.html": {
    "title": "NormalCrafter: Learning Temporally Consistent Normals from Video Diffusion Priors",
    "volume": "main",
    "abstract": "Surface normal estimation serves as a cornerstone for a spectrum of computer vision applications. While numerous efforts have been devoted to static image scenarios, ensuring temporal coherence in video-based normal estimation remains a formidable challenge. Instead of merely augmenting existing methods with temporal components, we present NormalCrafter to leverage the inherent temporal priors of Video Diffusion Models (VDMs). We identify the reason for blurry predictions when directly applying VDMs and introduce Semantic Feature Regularization (SFR) to encourage the model to concentrate on geometric details by aligning diffusion features with fine-grained semantic cues. Moreover, we introduce a two-stage training protocol that leverages both latent and pixel space learning to preserve spatial accuracy while maintaining long temporal context. Extensive evaluations demonstrate the efficacy of our method, showcasing a superior performance in generating temporally consistent normal sequences with intricate details from diverse videos. Code and models are publicly available at https://normalcrafter.github.io/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanrui Bin",
      "Wenbo Hu",
      "Haoyuan Wang",
      "Xinya Chen",
      "Bing Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_TrackAny3D_Transferring_Pretrained_3D_Models_for_Category-unified_3D_Point_Cloud_ICCV_2025_paper.html": {
    "title": "TrackAny3D: Transferring Pretrained 3D Models for Category-unified 3D Point Cloud Tracking",
    "volume": "main",
    "abstract": "3D LiDAR-based single object tracking (SOT) relies on sparse and irregular point clouds, posing challenges from geometric variations in scale, motion patterns, and structural complexity across object categories. Current category-specific approaches achieve good accuracy but are impractical for real-world use, requiring separate models for each category and showing limited generalization. To tackle these issues, we propose TrackAny3D, the first framework to transfer large-scale pretrained 3D models for category-agnostic 3D SOT. We first integrate parameter-efficient adapters to bridge the gap between pretraining and tracking tasks while preserving geometric priors. Then, we introduce a Mixture-of-Geometry-Experts (MoGE) architecture that adaptively activates specialized subnetworks based on distinct geometric characteristics. Additionally, we design a temporal context optimization strategy that incorporates learnable temporal tokens and a dynamic mask weighting module to propagate historical information and mitigate temporal drift. Experiments on three commonly-used benchmarks show that TrackAny3D establishes new state-of-the-art performance on category-agnostic 3D SOT, demonstrating strong generalization and competitiveness. We hope this work will enlighten the community on the importance of unified models and further expand the use of large-scale pretrained models in this field",
    "checked": true,
    "id": "c26a1fc32d81000a9cae663dee4ab4ee5eb84f3d",
    "semantic_title": "trackany3d: transferring pretrained 3d models for category-unified 3d point cloud tracking",
    "citation_count": 0,
    "authors": [
      "Mengmeng Wang",
      "Haonan Wang",
      "Yulong Li",
      "Xiangjie Kong",
      "Jiaxin Du",
      "Guojiang Shen",
      "Feng Xia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Nikonorov_Color_Matching_Using_Hypernetwork-Based_Kolmogorov-Arnold_Networks_ICCV_2025_paper.html": {
    "title": "Color Matching Using Hypernetwork-Based Kolmogorov-Arnold Networks",
    "volume": "main",
    "abstract": "We present cmKAN, a versatile framework for color matching. Given an input image with colors from a source color distribution, our method effectively and accurately maps these colors to match a target color distribution in both supervised and unsupervised settings. Our framework leverages the spline capabilities of Kolmogorov-Arnold Networks (KANs) to model the color matching between source and target distributions. Specifically, we developed a hypernetwork that generates spatially varying weight maps to control the nonlinear splines of a KAN, enabling accurate color matching. As part of this work, we introduce a first large-scale dataset of paired images captured by two distinct cameras and evaluate the efficacy of our and existing methods in matching colors. We evaluated our approach across various color-matching tasks, including: (1) raw-to-raw mapping, where the source color distribution is in one camera's raw color space and the target in another camera's raw space; (2) raw-to-sRGB mapping, where the source color distribution is in a camera's raw space and the target is in the display sRGB space, emulating the color rendering of a camera ISP; and (3) sRGB-to-sRGB mapping, where the goal is to transfer colors from a source sRGB space (e.g., produced by a source camera ISP) to a target sRGB space (e.g., from a different camera ISP). The results show that our method outperforms existing approaches by 37.3% on average for supervised and unsupervised cases while remaining lightweight compared to other methods",
    "checked": true,
    "id": "6e16bd4aa79d4a45c9c7dfad3782e608084ed698",
    "semantic_title": "color matching using hypernetwork-based kolmogorov-arnold networks",
    "citation_count": 0,
    "authors": [
      "Artem Nikonorov",
      "Georgy Perevozchikov",
      "Andrei Korepanov",
      "Nancy Mehta",
      "Mahmoud Afifi",
      "Egor Ershov",
      "Radu Timofte"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Min_S2M2_Scalable_Stereo_Matching_Model_for_Reliable_Depth_Estimation_ICCV_2025_paper.html": {
    "title": "S2M2: Scalable Stereo Matching Model for Reliable Depth Estimation",
    "volume": "main",
    "abstract": "The pursuit of a generalizable stereo matching model, capable of performing well across varying resolutions and disparity ranges without dataset-specific fine-tuning, has revealed a fundamental trade-off. Iterative local search methods achieve high scores on constrained benchmarks, but their core mechanism inherently limits the global consistency required for true generalization. However, global matching architectures, while theoretically more robust, have historically been rendered infeasible by prohibitive computational and memory costs. We resolve this dilemma with S2M2: a global matching architecture that achieves state-of-the-art accuracy and high efficiency without relying on cost volume filtering or deep refinement stacks. Our design integrates a multi-resolution transformer for robust long-range correspondence, trained with a novel loss function that concentrates probability on feasible matches. This approach enables a more robust joint estimation of disparity, occlusion, and confidence. S2M2 establishes a new state of the art on Middlebury v3 and ETH3D benchmarks, significantly outperforming prior methods in most metrics while reconstructing high-quality details with competitive efficiency",
    "checked": false,
    "id": "1b5b887e4f9626583ddadaa8c998078b7ff4c1d1",
    "semantic_title": "{s\\textsuperscript{2}m\\textsuperscript{2}}: scalable stereo matching model for reliable depth estimation",
    "citation_count": 0,
    "authors": [
      "Junhong Min",
      "Youngpil Jeon",
      "Jimin Kim",
      "Minyong Choi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_NETracer_A_Topology-Aware_Iterative_Tracing_Approach_for_Tubular_Structure_Extraction_ICCV_2025_paper.html": {
    "title": "NETracer: A Topology-Aware Iterative Tracing Approach for Tubular Structure Extraction",
    "volume": "main",
    "abstract": "Extracting tubular structures from images is a widespread and challenging task in computer vision. To explore these continuous structures, iterative tracing methods offer a promising direction. However, in scenes with dense and blurred branches, existing tracing methods tend to jump to adjacent branches during tracing process, leading a significant topological mistake. The reason of this shortcoming is that the tracing model only focuses on the estimation of discrete nodes and ignores their connection attribution. To solve this problem, we introduce NETracer, a topology-aware iterative tracing method to improve the continuity and topological accuracy. In our approach, a node-edge estimation network with local connectivity loss is trained to produce the future nodes and their connective edges. Then, a geodesic distance-based search strategy is employed with the help of predicted edge cues to trace the future branches more accurately. Additionally, to comprehensively assess the effect of the tracing model, an new tracing metric is proposed to evaluate the local accuracy, continuity, and topological correctness of the traced branches. We demonstrate that our proposed method outperforms existing segmentation and tracing methods on five 2D road, vessel and 3D neuron datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Liu",
      "Yangbo Jiang",
      "Nenggan Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tao_MGSfM_Multi-Camera_Geometry_Driven_Global_Structure-from-Motion_ICCV_2025_paper.html": {
    "title": "MGSfM: Multi-Camera Geometry Driven Global Structure-from-Motion",
    "volume": "main",
    "abstract": "Multi-camera systems are increasingly vital in the environmental perception of autonomous vehicles and robotics. Their physical configuration offers inherent fixed relative pose constraints that benefit Structure-from-Motion (SfM). However, traditional global SfM systems struggle with robustness due to their optimization framework.We propose a novel global motion averaging framework for multi-camera systems, featuring two core components: a decoupled rotation averaging module and a hybrid translation averaging module.Our rotation averaging employs a hierarchical strategy by first estimating relative rotations within rigid camera units and then computing global rigid unit rotations.To enhance the robustness of translation averaging, we incorporate both camera-to-camera and camera-to-point constraints to initialize camera positions and 3D points with a convex distance-based objective function and refine them with an unbiased non-bilinear angle-based objective function.Experiments on large-scale datasets show that our system matches or exceeds incremental SfM accuracy while significantly improving efficiency.Our framework outperforms existing global SfM methods, establishing itself as a robust solution for real-world multi-camera SfM applications. We will share our system as an open-source implementation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peilin Tao",
      "Hainan Cui",
      "Diantao Tu",
      "Shuhan Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lyu_FaceLift_Learning_Generalizable_Single_Image_3D_Face_Reconstruction_from_Synthetic_ICCV_2025_paper.html": {
    "title": "FaceLift: Learning Generalizable Single Image 3D Face Reconstruction from Synthetic Heads",
    "volume": "main",
    "abstract": "We present FaceLift, a novel feed-forward approach for generalizable high-quality 360-degree 3D head reconstruction from a single image. Our pipeline first employs a multi-view latent diffusion model to generate consistent side and back views from a single facial input, which then feed into a transformer-based reconstructor that produces a comprehensive 3D Gaussian splats representation. Previous methods for monocular 3D face reconstruction often lack full view coverage or view consistency due to insufficient multi-view supervision. We address this by creating a high-quality synthetic head dataset that enables consistent supervision across viewpoints. To bridge the domain gap between synthetic training data and real-world images, we propose a simple yet effective technique that ensures the view generation process maintains fidelity to the input by learning to reconstruct the input image alongside the view generation. Despite being trained exclusively on synthetic data, our method demonstrates remarkable generalization to real-world images. Through extensive qualitative and quantitative evaluations, we show that FaceLift outperforms state-of-the-art 3D face reconstruction methods on identity preservation, detail recovery and rendering quality",
    "checked": true,
    "id": "35e239466ef9d68264970ade62ae7347cf84c932",
    "semantic_title": "facelift: learning generalizable single image 3d face reconstruction from synthetic heads",
    "citation_count": 4,
    "authors": [
      "Weijie Lyu",
      "Yi Zhou",
      "Ming-Hsuan Yang",
      "Zhixin Shu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Qiao_Towards_Open-World_Generation_of_Stereo_Images_and_Unsupervised_Matching_ICCV_2025_paper.html": {
    "title": "Towards Open-World Generation of Stereo Images and Unsupervised Matching",
    "volume": "main",
    "abstract": "Stereo images are fundamental to numerous applications, including extended reality (XR) devices, autonomous driving, and robotics. Unfortunately, acquiring high-quality stereo images remains challenging due to the precise calibration requirements of dual-camera setups and the complexity of obtaining accurate, dense disparity maps. Existing stereo image generation methods typically focus on either visual quality for viewing or geometric accuracy for matching, but not both. We introduce GenStereo, a diffusion-based approach, to bridge this gap. The method includes two primary innovations (1) conditioning the diffusion process on a disparity-aware coordinate embedding and a warped input image, allowing for more precise stereo alignment than previous methods, and (2) an adaptive fusion mechanism that intelligently combines the diffusion-generated image with a warped image, improving both realism and disparity consistency. Through extensive training on 11 diverse stereo datasets, GenStereo demonstrates strong generalization ability. GenStereo achieves state-of-the-art performance in both stereo image generation and unsupervised stereo matching tasks. Project page is available at https://qjizhi.github.io/genstereo",
    "checked": true,
    "id": "6159d8e39deecd9cef4be97e914727609200aa22",
    "semantic_title": "towards open-world generation of stereo images and unsupervised matching",
    "citation_count": 1,
    "authors": [
      "Feng Qiao",
      "Zhexiao Xiong",
      "Eric Xing",
      "Nathan Jacobs"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_PolarAnything_Diffusion-based_Polarimetric_Image_Synthesis_ICCV_2025_paper.html": {
    "title": "PolarAnything: Diffusion-based Polarimetric Image Synthesis",
    "volume": "main",
    "abstract": "Polarization images facilitate image enhancement and 3D reconstruction tasks, but the limited accessibility of polarization cameras hinders their broader application. This gap drives the need for synthesizing photorealistic polarization images. The existing polarization simulator Mitsuba relies on a parametric polarization image formation model and requires extensive 3D assets covering shape and PBR materials, preventing it from generating large-scale photorealistic images. To address this problem, we propose PolarAnything, capable of synthesizing polarization images from a single RGB input with both photorealism and physical accuracy, eliminating the dependency on 3D asset collections. Drawing inspiration from the zero-shot performance of pretrained diffusion models, we introduce a diffusion-based generative framework with an effective representation strategy that preserves the fidelity of polarization properties. Experiments show that our model generates high-quality polarization images and supports downstream tasks like shape from polarization",
    "checked": true,
    "id": "94b348f79feae99208cf7b76aca7fb39e1932320",
    "semantic_title": "polaranything: diffusion-based polarimetric image synthesis",
    "citation_count": 0,
    "authors": [
      "Kailong Zhang",
      "Youwei Lyu",
      "Heng Guo",
      "Si Li",
      "Zhanyu Ma",
      "Boxin Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Choi_Whats_Making_That_Sound_Right_Now_Video-centric_Audio-Visual_Localization_ICCV_2025_paper.html": {
    "title": "What's Making That Sound Right Now? Video-centric Audio-Visual Localization",
    "volume": "main",
    "abstract": "Audio-Visual Localization (AVL) aims to identify sound-emitting sources within a visual scene. However, existing studies focus on image-level audio-visual associations, failing to capture temporal dynamics. Moreover, they assume simplified scenarios where sound sources are always visible and involve only a single object. To address these limitations, we propose AVATAR, a video-centric AVL benchmark that incorporates high-resolution temporal information. AVATAR introduces four distinct scenarios -- Single-sound, Mixed-sound, Multi-entity, and Off-screen -- enabling a more comprehensive evaluation of AVL models. Additionally, we present TAVLO, a novel video-centric AVL model that explicitly integrates temporal information. Experimental results show that conventional methods struggle to track temporal variations due to their reliance on global audio features and frame-level mappings. In contrast, TAVLO achieves robust and precise audio-visual alignment by leveraging high-resolution temporal modeling. Our work empirically demonstrates the importance of temporal dynamics in AVL and establishes a new standard for video-centric audio-visual localization",
    "checked": true,
    "id": "d8be1d911a08e8207d7a84e34eae57918866bab1",
    "semantic_title": "what's making that sound right now? video-centric audio-visual localization",
    "citation_count": 0,
    "authors": [
      "Hahyeon Choi",
      "Junhoo Lee",
      "Nojun Kwak"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_WonderPlay_Dynamic_3D_Scene_Generation_from_a_Single_Image_and_ICCV_2025_paper.html": {
    "title": "WonderPlay: Dynamic 3D Scene Generation from a Single Image and Actions",
    "volume": "main",
    "abstract": "WonderPlay is a novel framework integrating physics simulation with video generation for generating action-conditioned dynamic 3D scenes from a single image. Our hybrid generative simulator first uses a physics solver to simulate coarse 3D dynamics, which subsequently conditions a video generator to produce a video with finer, more realistic motion. The generated video is then used to update the simulated dynamic 3D scene, closing the loop between the physics solver and the video generator. This approach enables intuitive user control to be combined with the accurate dynamics of physics-based simulators and the expressivity of diffusion-based video generators. Experimental results demonstrate that WonderPlay enables users to interact with various scenes of diverse content, including cloth, sand, snow, liquid, smoke, elasticity, and rigid bodies -- all using a single image input. Code will be made public",
    "checked": true,
    "id": "483b0fdb9421410b7d6288a0e74ebe879bf16953",
    "semantic_title": "wonderplay: dynamic 3d scene generation from a single image and actions",
    "citation_count": 5,
    "authors": [
      "Zizhang Li",
      "Hong-Xing Yu",
      "Wei Liu",
      "Yin Yang",
      "Charles Herrmann",
      "Gordon Wetzstein",
      "Jiajun Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xiao_SRefiner_Soft-Braid_Attention_for_Multi-Agent_Trajectory_Refinement_ICCV_2025_paper.html": {
    "title": "SRefiner: Soft-Braid Attention for Multi-Agent Trajectory Refinement",
    "volume": "main",
    "abstract": "Accurate prediction of multi-agent future trajectories is crucial for autonomous driving systems to make safe and efficient decisions. Trajectory refinement has emerged as a key strategy to enhance prediction accuracy. However, existing refinement methods often overlook the topological relationships between trajectories, which are vital for improving prediction precision. Inspired by braid theory, we propose a novel trajectory refinement approach, Soft-Braid Refiner (SRefiner), guided by the soft-braid topological structure of trajectories using Soft-Braid Attention. Soft-Braid Attention captures spatio-temporal topological relationships between trajectories by considering both spatial proximity and vehicle motion states at \"soft intersection points\". Additionally, we extend this approach to model interactions between trajectories and lanes, further improving the prediction accuracy. SRefiner is a multi-iteration, multi-agent framework that iteratively refines trajectories, incorporating topological information to enhance interactions within traffic scenarios. SRefiner achieves significant performance improvements over four baseline methods across two datasets, establishing a new state-of-the-art in trajectory refinement",
    "checked": true,
    "id": "0dff8c8ce0c26d7cb517f3ae1e8599146ffd8a51",
    "semantic_title": "srefiner: soft-braid attention for multi-agent trajectory refinement",
    "citation_count": 0,
    "authors": [
      "Liwen Xiao",
      "Zhiyu Pan",
      "Zhicheng Wang",
      "Zhiguo Cao",
      "Wei Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lee_Scheduling_Weight_Transitions_for_Quantization-Aware_Training_ICCV_2025_paper.html": {
    "title": "Scheduling Weight Transitions for Quantization-Aware Training",
    "volume": "main",
    "abstract": "Quantization-aware training (QAT) simulates a quantization process during training to lower bit-precision of weights/activations. It learns quantized weights indirectly by updating latent weights, i.e., full-precision inputs to a quantizer, using gradient-based optimizers. We claim that coupling a user-defined learning rate (LR) with these optimizers is sub-optimal for QAT. Quantized weights transit discrete levels of a quantizer, only if corresponding latent weights pass transition points, where the quantizer changes discrete states. This suggests that the changes of quantized weights are affected by both the LR for latent weights and their distributions. It is thus difficult to control the degree of changes for quantized weights by scheduling the LR manually. We conjecture that the degree of parameter changes in QAT is related to the number of quantized weights transiting discrete levels. Based on this, we introduce a transition rate (TR) scheduling technique that controls the number of transitions of quantized weights explicitly. Instead of scheduling a LR for latent weights, we schedule a target TR of quantized weights, and update the latent weights with a novel transition-adaptive LR (TALR), enabling considering the degree of changes for the quantized weights during QAT. Experimental results demonstrate the effectiveness of our approach on standard benchmarks",
    "checked": true,
    "id": "e12d63ab00032e0fad636e3d54351df355082ec2",
    "semantic_title": "scheduling weight transitions for quantization-aware training",
    "citation_count": 0,
    "authors": [
      "Junghyup Lee",
      "Jeimin Jeon",
      "Dohyung Kim",
      "Bumsub Ham"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kuang_Cross-Granularity_Online_Optimization_with_Masked_Compensated_Information_for_Learned_Image_ICCV_2025_paper.html": {
    "title": "Cross-Granularity Online Optimization with Masked Compensated Information for Learned Image Compression",
    "volume": "main",
    "abstract": "Learned image compression aims to reduce redundancy by accurately modeling the complex signal distribution inherent in images with network parameters. However, existing practices that train models on entire dataset offline face a limitation, as the estimated distribution only approximates the general image signal distribution and fails to capture image-specific characteristics. To address this issue, we propose a cross-granularity online optimization strategy to mitigate information loss from two key aspects: statistical distribution gaps and local structural gaps. This strategy introduces additional fitted bitstream to push the estimated signal distribution closer to the real one at both coarse-grained and fine-grained levels. For coarse-grained optimization, we relax the common bitrate constraints during gradient descent and reduce bitrate cost via adaptive QP (Quantization Parameter) selection, preventing information collapse and narrowing the statistical distribution gaps. For fine-grained optimization, a Mask-based Selective Compensation Module is designed to sparsely encode structural characteristics at low bitrates, enhancing local distribution alignment. By jointly optimizing global and local distributions, our method achieves closer alignment to real image statistics and significantly enhances the performance. Extensive experiments validate the superiority of our method as well as the design of our module. Our project is publicly available at: https://ellisonkuang.github.io/CGOO.github.io/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haowei Kuang",
      "Wenhan Yang",
      "Zongming Guo",
      "Jiaying Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_Efficient_Spiking_Point_Mamba_for_Point_Cloud_Analysis_ICCV_2025_paper.html": {
    "title": "Efficient Spiking Point Mamba for Point Cloud Analysis",
    "volume": "main",
    "abstract": "Bio-inspired Spiking Neural Networks (SNNs) provide an energy-efficient way to extract 3D spatio-temporal features. However, existing 3D SNNs have struggled with long-range dependencies until the recent emergence of Mamba, which offers superior computational efficiency and sequence modeling capability. In this work, we propose Spiking Point Mamba (SPM), the first Mamba-based SNN in the 3D domain. Naively adapting Mamba to 3D SNNs, though, is hindered by temporal dynamics mismatch and spike-induced information loss. Thus, we first introduce Hierarchical Dynamic Encoding (HDE), an improved direct encoding method that effectively introduces dynamic temporal mechanism. Then, we propose Spiking Mamba Block (SMB), which builds upon Mamba while learning inter-time-step features and minimizing information loss caused by spikes. Finally, to further boost performance, we adopt an asymmetric SNN-ANN architecture for spike-based pre-training and finetune. Compared with the previous state-of-the-art SNN models, SPM improves overall accuracy by +6.2%, +6.1%, and +7.4% on three variants of ScanObjectNN, and boosts instance mIOU by +1.9% on ShapeNetPart. Meanwhile, its energy consumption is at most 12.6x lower than that of its ANN counterpart. Code: https://github.com/PeppaWu/SPM",
    "checked": true,
    "id": "55a4753ffce6d88a6432d0488d59e680fcb3e472",
    "semantic_title": "efficient spiking point mamba for point cloud analysis",
    "citation_count": 1,
    "authors": [
      "Peixi Wu",
      "Bosong Chai",
      "Menghua Zheng",
      "Wei Li",
      "Zhangchi Hu",
      "Jie Chen",
      "Zheyu Zhang",
      "Hebei Li",
      "Xiaoyan Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xing_VideoVAE_Large_Motion_Video_Autoencoding_with_Cross-modal_Video_VAE_ICCV_2025_paper.html": {
    "title": "VideoVAE+: Large Motion Video Autoencoding with Cross-modal Video VAE",
    "volume": "main",
    "abstract": "Learning a robust video Variational Autoencoder (VAE) is essential for reducing video redundancy and facilitating efficient video generation. Directly applying image VAEs to individual frames in isolation results in temporal inconsistencies and fails to compress temporal redundancy effectively. Existing works on Video VAEs compress temporal redundancy but struggle to handle videos with large motion effectively. They suffer from issues such as severe image blur and loss of detail in scenarios with large motion. In this paper, we present a powerful video VAE named VideoVAE+ that effectively reconstructs videos with large motion. First, we investigate two architecture choices and propose our simple yet effective architecture with better spatiotemporal joint modeling performance. Second, we propose to leverage the textual information in existing text-to-video datasets and incorporate text guidance during training. The textural guidance is optional during inference. We find that this design enhances the reconstruction quality and preservation of detail. Finally, our models achieve strong performance compared with various baseline approaches in both general videos and large motion videos, demonstrating its effectiveness on the challenging large motion scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yazhou Xing",
      "Yang Fei",
      "Yingqing He",
      "Jingye Chen",
      "Jiaxin Xie",
      "Xiaowei Chi",
      "Qifeng Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Baik_Learning_3D_Object_Spatial_Relationships_from_Pre-trained_2D_Diffusion_Models_ICCV_2025_paper.html": {
    "title": "Learning 3D Object Spatial Relationships from Pre-trained 2D Diffusion Models",
    "volume": "main",
    "abstract": "We present a method for learning 3D spatial relationships between object pairs, referred to as object-object spatial relationships (OOR), by leveraging synthetically generated 3D samples from pre-trained 2D diffusion models. We hypothesize that images synthesized by 2D diffusion models inherently capture realistic OOR cues, enabling efficient collection of a 3D dataset to learn OOR for various unbounded object categories. Our approach synthesizes diverse images that capture plausible OOR cues, which we then uplift into 3D samples. Leveraging our diverse collection of 3D samples for the object pairs, we train a score-based OOR diffusion model to learn the distribution of their relative spatial relationships. Additionally, we extend our pairwise OORtomulti-object OOR by enforcing consistency across pairwise relations and preventing object collisions. Extensive experiments demonstrate the robustness of our method across various object-object spatial relationships, along with its applicability to 3D scene arrangement tasks and human motion synthesis using our OOR diffusion model",
    "checked": true,
    "id": "0ec9fbf32f8c68f85606f412a2136f4f1bc060fc",
    "semantic_title": "learning 3d object spatial relationships from pre-trained 2d diffusion models",
    "citation_count": 1,
    "authors": [
      "Sangwon Baik",
      "Hyeonwoo Kim",
      "Hanbyul Joo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_Event-guided_Unified_Framework_for_Low-light_Video_Enhancement_Frame_Interpolation_and_ICCV_2025_paper.html": {
    "title": "Event-guided Unified Framework for Low-light Video Enhancement, Frame Interpolation, and Deblurring",
    "volume": "main",
    "abstract": "In low-light environments, longer exposure times are commonly used to enhance image visibility; however, this inevitably leads to motion blur. Even with a long exposure time, videos captured in low-light environments still suffer from issues such as low visibility, low contrast, and color distortion. Additionally, the long exposure time results in videos with a low frame rate. Therefore, videos captured in low-light exhibit low visibility and motion blur, as well as low frame rates. To overcome these limitations, we propose a novel problem aimed at transforming motion-blurred, low-frame-rate videos with poor visibility in low-light environments into high-frame-rate videos while simultaneously enhancing their visibility. To tackle this challenge, we leverage the unique advantages of event cameras, which capture scene changes asynchronously, providing superior temporal resolution and a wider dynamic range compared to conventional frame-based cameras. These properties make event cameras particularly effective in reducing motion blur, compensating for low frame rates, and enhancing visibility in low-light conditions. To this end, we developed a hybrid camera system that integrates two RGB cameras and an event camera, capturing a dedicated dataset for this task and proposing novel network architectures to effectively address this problem. The project pages are available at https://sites.google.com/view/eledi-iccv",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taewoo Kim",
      "Kuk-Jin Yoon"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gong_FE-CLIP_Frequency_Enhanced_CLIP_Model_for_Zero-Shot_Anomaly_Detection_and_ICCV_2025_paper.html": {
    "title": "FE-CLIP: Frequency Enhanced CLIP Model for Zero-Shot Anomaly Detection and Segmentation",
    "volume": "main",
    "abstract": "Zero-shot anomaly detection (ZSAD) requires detection models trained using auxiliary data to detect anomalies without any training sample in a target dataset. It is challenging since the models need to generalize to anomalies across different domains. Recently, CLIP-based anomaly detection methods, such as WinCLIP and AnomalyCLIP, have demonstrated superior performance in the ZSAD task, due to the strong zero-shot recognition of the CLIP model. However, they overlook the utilization of frequency information of images. In this paper, we find that frequency information could benefit the ZSAD task, since some properties of the anomaly area, such as appearance defects, can also be reflected based on its frequency information. To this end, We propose Frequency Enhanced CLIP (FE-CLIP), taking advantage of two different but complementary frequency-aware clues, (1) Frequency-aware Feature Extraction adapter, and (2) Local Frequency Statistics adapter, in the visual encoder of CLIP, to deeply mine frequency information for the ZSAD task. We apply DCT as the frequency-domain transformation. Through comprehensive experiments, we show that the proposed FE-CLIP has good generalization across different domains and achieves superior zero-shot performance of detecting and segmenting anomalies in 10 datasets of highly diverse class semantics from various defect inspections and medical domains. Besides, the proposed FE-CLIP also achieves superior performance under the few-normal-shot anomaly detection settings",
    "checked": false,
    "id": "0b4f84c7dc88b391f60f14a9e4227238cb926e17",
    "semantic_title": "zero-shot eggshell crack detection using grounding dino and fft-based outer-to-inner ring energy ratio",
    "citation_count": 0,
    "authors": [
      "Tao Gong",
      "Qi Chu",
      "Bin Liu",
      "Wei Zhou",
      "Nenghai Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ma_MotionDiff_Training-free_Zero-shot_Interactive_Motion_Editing_via_Flow-assisted_Multi-view_Diffusion_ICCV_2025_paper.html": {
    "title": "MotionDiff: Training-free Zero-shot Interactive Motion Editing via Flow-assisted Multi-view Diffusion",
    "volume": "main",
    "abstract": "Generative models have made remarkable advancements and are capable of producing high-quality content. However, performing controllable editing with generative models remains challenging, due to their inherent uncertainty in outputs. This challenge is particularly pronounced in motion editing, which involves the processing of spatial information. While some physics-based generative methods have attempted to implement motion editing, they typically operate on single-view images with simple motions, such as translation and dragging. These methods struggle to handle complex rotation and stretching motions and ensure multi-view consistency, often necessitating resource-intensive retraining. To address these challenges, we propose a training-free zero-shot diffusion method that leverages optical flow for complex multi-view motion editing. Specifically, given a static scene, users can interactively select objects of interest to add motion priors. The proposed Point Kinematic Model then estimates corresponding multi-view optical flows during the Multi-view Flow Estimation Stage. Subsequently, these optical flows are utilized to generate multi-view motion results through decoupled motion representation in the Multi-view Motion Diffusion Stage. Extensive experiments demonstrate that our method outperforms other physics-based generative motion editing methods in achieving high-quality multi-view consistent motion results. Notably, our method does not require retraining, enabling users to conveniently adapt it for various down-stream tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yikun Ma",
      "Yiqing Li",
      "Jiawei Wu",
      "Xing Luo",
      "Zhi Jin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ye_Alleviating_Textual_Reliance_in_Medical_Language-guided_Segmentation_via_Prototype-driven_Semantic_ICCV_2025_paper.html": {
    "title": "Alleviating Textual Reliance in Medical Language-guided Segmentation via Prototype-driven Semantic Approximation",
    "volume": "main",
    "abstract": "Medical language-guided segmentation, integrating textual clinical reports as auxiliary guidance to enhance image segmentation, has demonstrated significant improvements over unimodal approaches. However, its inherent reliance on paired image-text input, which we refer to as \"textual reliance\", presents two fundamental limitations: 1) many medical segmentation datasets lack paired reports, leaving a substantial portion of image-only data underutilized for training; and 2) inference is limited to retrospective analysis of cases with paired reports, limiting its applicability in most clinical scenarios where segmentation typically precedes reporting. To address these limitations, we propose ProLearn, the first Prototype-driven Learning framework for language-guided segmentation that fundamentally alleviates textual reliance. At its core, we introduce a novel Prototype-driven Semantic Approximation (PSA) module to enable approximation of semantic guidance from textual input. PSA initializes a discrete and compact prototype space by distilling segmentation-relevant semantics from textual reports. Once initialized, it supports a query-and-respond mechanism which approximates semantic guidance for images without textual input, thereby alleviating textual reliance. Extensive experiments on QaTa-COV19, MosMedData+ and Kvasir-SEG demonstrate that ProLearn outperforms state-of-the-art language-guided methods when limited text is available",
    "checked": true,
    "id": "8de339a894f13f805658fbb4a2a7b6d5eaf2558a",
    "semantic_title": "alleviating textual reliance in medical language-guided segmentation via prototype-driven semantic approximation",
    "citation_count": 0,
    "authors": [
      "Shuchang Ye",
      "Usman Naseem",
      "Mingyuan Meng",
      "Jinman Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sun_CoTMR_Chain-of-Thought_Multi-Scale_Reasoning_for_Training-Free_Zero-Shot_Composed_Image_Retrieval_ICCV_2025_paper.html": {
    "title": "CoTMR: Chain-of-Thought Multi-Scale Reasoning for Training-Free Zero-Shot Composed Image Retrieval",
    "volume": "main",
    "abstract": "Zero-Shot Composed Image Retrieval (ZS-CIR) aims to retrieve target images by integrating information from a composed query (reference image and modification text) without training samples. Existing methods primarily combine caption models and Large Language Models (LLMs) to generate target captions from composed queries but face various issues such as incompatibility, visual information loss, and insufficient reasoning. In this work, we propose CoTMR, a training-free framework with novel Chain-of-thought (CoT) and Multi-scale Reasoning. Instead of relying on caption models for modality transformation, CoTMR directly employs the Large Vision-Language Model (LVLM) to achieve unified understanding and reasoning of composed queries. To enhance reasoning reliability, we devise CIRCoT, which guides the LVLM to perform step-by-step reasoning by following predefined subtasks. Additionally, while most existing approaches focus solely on global-level reasoning, CoTMR introduces fine-grained predictions about the presence or absence of key elements at the object scale for more comprehensive reasoning. Furthermore, we design a Multi-Grained Scoring (MGS) mechanism, which integrates CLIP similarity scores of the above reasoning outputs with candidate images to realize precise retrieval. Extensive experiments demonstrate that our CoTMR not only drastically outperforms previous methods across four prominent benchmarks but also offers appealing interpretability",
    "checked": true,
    "id": "f501db5ce9296d01b9f41e4f99258d42062a4869",
    "semantic_title": "cotmr: chain-of-thought multi-scale reasoning for training-free zero-shot composed image retrieval",
    "citation_count": 4,
    "authors": [
      "Zelong Sun",
      "Dong Jing",
      "Zhiwu Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lee_PASTA_Part-Aware_Sketch-to-3D_Shape_Generation_with_Text-Aligned_Prior_ICCV_2025_paper.html": {
    "title": "PASTA: Part-Aware Sketch-to-3D Shape Generation with Text-Aligned Prior",
    "volume": "main",
    "abstract": "A fundamental challenge in conditional 3D shape generation is to minimize the information loss and maximize the intention of user input. Existing approaches have predominantly focused on two types of isolated conditional signals, i.e., user sketches and text descriptions, each of which does not offer flexible control of the generated shape. In this paper, we introduce PASTA, the flexible approach that seamlessly integrates a user sketch and a text description for 3D shape generation. The key idea is to use text embeddings from a vision-language model to enrich the semantic representation of sketches. Specifically, these text-derived priors specify the part components of the object, compensating for missing visual cues from ambiguous sketches. In addition, we introduce ISG-Net which employs two types of graph convolutional networks: IndivGCN, which processes fine-grained details, and PartGCN, which aggregates these details into parts and refines the structure of objects. Extensive experiments demonstrate that PASTA outperforms existing methods in part-level editing and achieves state-of-the-art results in sketch-to-3D shape generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seunggwan Lee",
      "Hwanhee Jung",
      "Byoungsoo Koh",
      "Qixing Huang",
      "Sang Ho Yoon",
      "Sangpil Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tang_BASIC_Boosting_Visual_Alignment_with_Intrinsic_Refined_Embeddings_in_Multimodal_ICCV_2025_paper.html": {
    "title": "BASIC: Boosting Visual Alignment with Intrinsic Refined Embeddings in Multimodal Large Language Models",
    "volume": "main",
    "abstract": "Mainstream Multimodal Large Language Models (MLLMs) achieve visual understanding by using a vision projector to bridge well-pretrained vision encoders and large language models (LLMs). The inherent gap between visual and textual modalities makes the embeddings from the vision projector critical for visual comprehension. However, current alignment approaches treat visual embeddings as contextual cues and merely apply auto-regressive supervision to textual outputs, neglecting the necessity of introducing equivalent direct visual supervision, which hinders the potential finer alignment of visual embeddings. In this paper, based on our analysis of the refinement process of visual embeddings in the LLM's shallow layers, we propose BASIC, a method that utilizes refined visual embeddings within the LLM as supervision to directly guide the projector in generating initial visual embeddings. Specifically, the guidance is conducted from two perspectives: (i) optimizing embedding directions by reducing angles between initial and supervisory embeddings in semantic space; (ii) improving semantic matching by minimizing disparities between the logit distributions of both visual embeddings. Without additional supervisory models or artificial annotations, BASIC significantly improves the performance of MLLMs across a wide range of benchmarks, demonstrating the effectiveness of our introduced direct visual supervision",
    "checked": true,
    "id": "61a51a3df02998090ffd06e2560ac266e2b37bb7",
    "semantic_title": "basic: boosting visual alignment with intrinsic refined embeddings in multimodal large language models",
    "citation_count": 0,
    "authors": [
      "Jianting Tang",
      "Yubo Wang",
      "Haoyu Cao",
      "Linli Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_Long-term_Traffic_Simulation_with_Interleaved_Autoregressive_Motion_and_Scenario_Generation_ICCV_2025_paper.html": {
    "title": "Long-term Traffic Simulation with Interleaved Autoregressive Motion and Scenario Generation",
    "volume": "main",
    "abstract": "An ideal traffic simulator replicates the realistic long-term point-to-point trip that a self-driving system experiences during deployment. Prior models and benchmarks focus on closed-loop motion simulation for initial agents in a scene. This is problematic for long-term simulation. Agents enter and exit the scene as the ego vehicle enters new regions. We propose InfGen, a unified next-token prediction model that performs interleaved closed-loop motion simulation and scene generation. InfGen automatically switches between closed-loop motion simulation and scene generation mode. It enables stable long-term rollout simulation. InfGen performs at the state-of-the-art in short-term (9s) traffic simulation, and significantly outperforms all other methods in long-term (30s) simulation. The code and model of InfGen will be released at https://orangesodahub.github.io/InfGen",
    "checked": true,
    "id": "9348f5bba205d4cb3f959deb4f3a22cb7c62cb99",
    "semantic_title": "long-term traffic simulation with interleaved autoregressive motion and scenario generation",
    "citation_count": 1,
    "authors": [
      "Xiuyu Yang",
      "Shuhan Tan",
      "Philipp Krähenbühl"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ling_VMBench_A_Benchmark_for_Perception-Aligned_Video_Motion_Generation_ICCV_2025_paper.html": {
    "title": "VMBench: A Benchmark for Perception-Aligned Video Motion Generation",
    "volume": "main",
    "abstract": "Video generation has advanced rapidly, improving evaluation methods, yet assessing video's motion remains a major challenge. Specifically, there are two key issues: 1) current motion metrics do not fully align with human perceptions; 2) the existing motion prompts are limited. Based these findings, we introduce VMBench--a comprehensive Video Motion Benchmark that has perception-aligned motion metrics and features the most diverse types of motion. VMBench has several appealing properties: (1) Perception-Driven Motion Evaluation Metrics, we identify five dimensions based on human perception in motion video assessment and develop fine-grained evaluation metrics, providing deeper insights into models' strengths and weaknesses in motion quality. (2) Meta-Guided Motion Prompt Generation, a structured method that extracts meta-information, generates diverse motion prompts with LLMs, and refines them through human-AI validation, resulting in a multi-level prompt library covering six key dynamic scene dimensions. (3) Human-Aligned Validation Mechanism, we provide human preference annotations to validate our benchmarks, with our metrics achieving an average 35.3% improvement in Spearman's correlation over baseline methods. This is the first time that the quality of motion in videos has been evaluated from the perspective of human perception alignment. Additionally, we release VMBench at https://github.com/AMAP-ML/VMBench, setting a new standard for evaluating and advancing motion generation models",
    "checked": true,
    "id": "e8702839751f58621cae3a80bafba164a1c7e303",
    "semantic_title": "vmbench: a benchmark for perception-aligned video motion generation",
    "citation_count": 9,
    "authors": [
      "Xinran Ling",
      "Chen Zhu",
      "Meiqi Wu",
      "Hangyu Li",
      "Xiaokun Feng",
      "Cundian Yang",
      "Aiming Hao",
      "Jiashu Zhu",
      "Jiahong Wu",
      "Xiangxiang Chu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Balazadeh_Physics_Context_Builders_A_Modular_Framework_for_Physical_Reasoning_in_ICCV_2025_paper.html": {
    "title": "Physics Context Builders: A Modular Framework for Physical Reasoning in Vision-Language Models",
    "volume": "main",
    "abstract": "Physical reasoning remains a significant challenge for Vision-Language Models (VLMs). This limitation arises from an inability to translate learned knowledge into predictions about physical behavior. Although continual fine-tuning can mitigate this issue, it is expensive for large models and impractical to perform repeatedly for every task. This necessitates the creation of modular and scalable ways to teach VLMs about physical reasoning. To that end, we introduce Physics Context Builders (PCBs), a modular framework where specialized smaller VLMs are fine-tuned to generate detailed physical scene descriptions. These can be used as physical contexts to enhance the reasoning capabilities of larger VLMs. PCBs enable the separation of visual perception from reasoning, allowing us to analyze their relative contributions to physical understanding. We perform experiments on CLEVRER and on Falling Tower, a stability detection dataset with both simulated and real-world scenes, to demonstrate that PCBs provide substantial performance improvements, increasing average accuracy by up to 13.8% on complex physical reasoning tasks. Notably, PCBs also show strong Sim2Real transfer, successfully generalizing from simulated training data to real-world scenes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vahid Balazadeh",
      "Mohammadmehdi Ataei",
      "Hyunmin Cheong",
      "Amir Hosein Khasahmadi",
      "Rahul G. Krishnan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lu_RhythmGuassian_Repurposing_Generalizable_Gaussian_Model_For_Remote_Physiological_Measurement_ICCV_2025_paper.html": {
    "title": "RhythmGuassian: Repurposing Generalizable Gaussian Model For Remote Physiological Measurement",
    "volume": "main",
    "abstract": "Remote Photoplethysmography (rPPG) enables non-contact extraction of physiological signals, providing significant advantages in medical monitoring, emotion recognition, and face anti-spoofing. However, the extraction of reliable rPPG signals is hindered by motion variations in real-world environments, leading to entanglement issue. To address the challenge, we employ the Generalizable Gaussian Model (GGM) to disentangle geometry and chroma components with 4D Gaussian representations. Employing the GGM for robust rPPG estimation is non-trivial. Firstly, there are no camera parameters in the dataset, resulting in the inability to render video from 4D Gaussian. The \"4D virtual camera\" is proposed to construct extra Gaussian parameters to describe view and motion changes, giving the ability to render video with the fixed virtual camera parameters. Further, the chroma component is still not explicitly decoupled in 4D Gaussian representation. Explicit motion modeling (EMM) is designed to decouple the motion variation in an unsupervised manner. Explicit chroma modeling (ECM) is tailored to decouple specular, physiological, and noise signals, respectively. To validate our approach, we expand existing rPPG datasets to include various motion and illumination interference scenarios, demonstrating the effectiveness of our method in real-world settings. The code is available at https://github.com/LuPaoPao/RhythmGuassian",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Lu",
      "Yuting Zhang",
      "Jiaqi Tang",
      "Bowen Fu",
      "Wenhang Ge",
      "Wei Wei",
      "Kaishun Wu",
      "Yingcong Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Khalid_Bridging_the_Sky_and_Ground_Towards_View-Invariant_Feature_Learning_for_ICCV_2025_paper.html": {
    "title": "Bridging the Sky and Ground: Towards View-Invariant Feature Learning for Aerial-Ground Person Re-Identification",
    "volume": "main",
    "abstract": "Aerial-Ground Person Re-Identification (AG-ReID) is a practical yet challenging task that involves cross-platform matching between aerial and ground cameras. Existing person Re-Identification (Re-ID) methods are primarily designed for homogeneous camera settings, such as ground-to-ground or aerial-to-aerial matching. Therefore, these conventional Re-ID approaches underperform due to the significant viewpoint discrepancies introduced by cross-platform cameras in the AG-ReID task. To address this limitation, we propose a novel and efficient approach, termed View-Invariant Feature Learning for Aerial-Ground Person Re-Identification (VIF-AGReID), which explores view-invariant features without leveraging any auxiliary information. Our approach introduces two key components: (1) Patch-Level RotateMix (PLRM), an augmentation strategy that enhances rotational diversity within local regions of training samples, enabling the model to capture fine-grained view-invariant features, and (2) View-Invariant Angular Loss (VIAL), which mitigates the impact of perspective variations by imposing angular constraints that exponentially penalize large angular deviations, optimizing the similarity of positive pairs while enhancing dissimilarity for hard negatives. These components interact synergistically to drive view-invariant feature learning, enhancing robustness across diverse viewpoints. Extensive experiments on the CARGO, AG-ReIDv1, and AG-ReIDv2 benchmarks demonstrate the effectiveness of our method in addressing the AG-ReID task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wajahat Khalid",
      "Bin Liu",
      "Xulin Li",
      "Muhammad Waqas",
      "Muhammad Sher Afgan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Amula_Prototype_Guided_Backdoor_Defense_via_Activation_Space_Manipulation_ICCV_2025_paper.html": {
    "title": "Prototype Guided Backdoor Defense via Activation Space Manipulation",
    "volume": "main",
    "abstract": "Deep learning models are susceptible to backdoor attacks involving malicious perturbation of some training data with a trigger to force misclassification to a target class. Various triggers have been used including semantic triggers that are easily realizable. We present Prototype Guided Backdoor Defense (PGBD), a robust post-hoc defense that scales across different trigger types, including previously unsolved semantic triggers. PPGBD exploits displacements in the geometric spaces of activations to penalize movements towards the trigger. This is done using a novel sanitization loss of a post-hoc fine-tuning step. This approach scales to all types of attacks and triggers, and achieves better performance across settings. We also present the first defense against semantic attacks on a new celebrity face images dataset. Activation spaces can provide rich clues to enhance DL models in different ways",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Venkat Adithya Amula",
      "Sunayana Samavedam",
      "Saurabh Saini",
      "Avani Gupta",
      "P J Narayanan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhu_EvolvingGrasp_Evolutionary_Grasp_Generation_via_Efficient_Preference_Alignment_ICCV_2025_paper.html": {
    "title": "EvolvingGrasp: Evolutionary Grasp Generation via Efficient Preference Alignment",
    "volume": "main",
    "abstract": "Dexterous robotic hands often struggle to generalize effectively in complex environments due to models trained on low-diversity data. However, the real world presents an inherently unbounded range of scenarios. A natural solution is to enable robots learning from experience in complex environments--an approach akin to evolution, where systems improve through learning from both failures and successes. Motivated by this, we propose EvolvingGrasp, an evolutionary grasp generation method that continuously enhances grasping performance through efficient preference alignment. Specifically, we introduce Handpose-wise Preference Optimization (HPO), which allows the model to continuously align with preferences from both positive and negative feedback while progressively refining its grasping strategies. To further enhance efficiency and reliability during online adjustments, we incorporate a Physics-aware Consistency Model within HPO, which accelerates inference, reduces the number of timesteps needed for preference fine-tuning, and ensures physical plausibility throughout the process. Our results validate that EvolvingGrasp enables evolutionary grasp generation, ensuring robust, physically feasible, and preference-aligned grasping in both simulation and real scenarios",
    "checked": true,
    "id": "2ca509423718504511b040d19b3d3b1e40b694a0",
    "semantic_title": "evolvinggrasp: evolutionary grasp generation via efficient preference alignment",
    "citation_count": 2,
    "authors": [
      "Yufei Zhu",
      "Yiming Zhong",
      "Zemin Yang",
      "Peishan Cong",
      "Jingyi Yu",
      "Xinge Zhu",
      "Yuexin Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_HyTIP_Hybrid_Temporal_Information_Propagation_for_Masked_Conditional_Residual_Video_ICCV_2025_paper.html": {
    "title": "HyTIP: Hybrid Temporal Information Propagation for Masked Conditional Residual Video Coding",
    "volume": "main",
    "abstract": "Most frame-based learned video codecs can be interpreted as recurrent neural networks (RNNs) propagating reference information along the temporal dimension. This work revisits the limitations of the current approaches from an RNN perspective. The output-recurrence methods, which propagate decoded frames, are intuitive but impose dual constraints on the output decoded frames, leading to suboptimal rate-distortion performance. In contrast, the hidden-to-hidden connection approaches, which propagate latent features within the RNN, offer greater flexibility but require large buffer sizes. To address these issues, we propose HyTIP, a learned video coding framework that combines both mechanisms. Our hybrid buffering strategy uses explicit decoded frames and a small number of implicit latent features to achieve competitive coding performance. Experimental results show that our HyTIP outperforms the sole use of either output-recurrence or hidden-to-hidden approaches. Furthermore, it achieves comparable performance to state-of-the-art methods but with a much smaller buffer size, and outperforms VTM 17.0 (Low-delay B) in terms of PSNR-RGB and MS-SSIM-RGB. The source code of HyTIP is available at https://github.com/NYCU-MAPL/HyTIP",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi-Hsin Chen",
      "Yi-Chen Yao",
      "Kuan-Wei Ho",
      "Chun-Hung Wu",
      "Huu-Tai Phung",
      "Martin Benjak",
      "Jörn Ostermann",
      "Wen-Hsiao Peng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Achieving_More_with_Less_Additive_Prompt_Tuning_for_Rehearsal-Free_Class-Incremental_ICCV_2025_paper.html": {
    "title": "Achieving More with Less: Additive Prompt Tuning for Rehearsal-Free Class-Incremental Learning",
    "volume": "main",
    "abstract": "Class-incremental learning (CIL) enables models to learn new classes progressively while preserving knowledge of previously learned ones. Recent advances in this field have shifted towards parameter-efficient fine-tuning techniques, with many approaches building upon the framework that maintains a pool of learnable prompts. Although effective, these methods introduce substantial computational overhead, primarily due to prompt pool querying and increased input sequence lengths from prompt concatenation. In this work, we present a novel prompt-based approach that addresses this limitation. Our method trains a single set of shared prompts across all tasks and, rather than concatenating prompts to the input, directly modifies the CLS token's attention computation by adding the prompts to it. This simple and lightweight design not only significantly reduces computational complexity--both in terms of inference costs and the number of trainable parameters--but also eliminates the need to optimize prompt lengths for different downstream tasks, offering a more efficient yet powerful solution for rehearsal-free class-incremental learning. Extensive experiments across a diverse range of CIL benchmarks demonstrate the effectiveness of our approach, highlighting its potential to establish a new prompt-based CIL paradigm. Furthermore, experiments on general recognition benchmarks beyond the CIL setting also show strong performance, positioning our method as a promising candidate for a general parameter-efficient fine-tuning approach",
    "checked": true,
    "id": "02ef3e8ad6e355423ac98e1ace376f4a4e557a94",
    "semantic_title": "achieving more with less: additive prompt tuning for rehearsal-free class-incremental learning",
    "citation_count": 0,
    "authors": [
      "Haoran Chen",
      "Ping Wang",
      "Zihan Zhou",
      "Xu Zhang",
      "Zuxuan Wu",
      "Yu-Gang Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_PromptDresser_Improving_the_Quality_and_Controllability_of_Virtual_Try-On_via_ICCV_2025_paper.html": {
    "title": "PromptDresser: Improving the Quality and Controllability of Virtual Try-On via Generative Textual Prompt and Prompt-aware Mask",
    "volume": "main",
    "abstract": "Recent virtual try-on approaches have advanced by finetuning pre-trained text-to-image diffusion models to leverage their powerful generative ability; however, the use of text prompts in virtual try-on remains underexplored. This paper tackles a text-editable virtual try-on task that modifies the clothing based on the provided clothing image while editing the wearing style (e.g., tucking style, fit) according to the text descriptions. In the text-editable virtual try-on, three key aspects exist: (i) designing rich text descriptions for paired person-clothing data to train the model, (ii) addressing the conflicts where textual information of the existing person's clothing interferes the generation of the new clothing, and (iii) adaptively adjust the inpainting mask aligned with the text descriptions, ensuring proper editing areas while preserving the original person's appearance irrelevant to the new clothing. To address these aspects, we propose PromptDresser, a text-editable virtual try-on model that leverages large multimodal model (LMM) assistance to enable high-quality and versatile manipulation based on generative text prompts. Our approach utilizes LMMs via in-context learning to generate detailed text descriptions for person and clothing images independently, including pose details and editing attributes using minimal human cost. Moreover, to ensure the editing areas, we adjust the inpainting mask depending on the text prompts adaptively. Our approach enhances text editability while effectively conveying clothing details that are difficult to capture through images alone, leading to improved image quality. Experiments show that PromptDressersignificantly outperforms baselines, demonstrating superior text-driven control and versatile clothing manipulation",
    "checked": true,
    "id": "6bd9bc76807efc08ed7300967442483bc10bf9c1",
    "semantic_title": "promptdresser: improving the quality and controllability of virtual try-on via generative textual prompt and prompt-aware mask",
    "citation_count": 2,
    "authors": [
      "Jeongho Kim",
      "Hoiyeong Jin",
      "Sunghyun Park",
      "Jaegul Choo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ruan_VLRMBench_A_Comprehensive_and_Challenging_Benchmark_for_Vision-Language_Reward_Models_ICCV_2025_paper.html": {
    "title": "VLRMBench: A Comprehensive and Challenging Benchmark for Vision-Language Reward Models",
    "volume": "main",
    "abstract": "Although large visual-language models (LVLMs) have demonstrated strong performance in multimodal tasks, errors may occasionally arise due to biases during the reasoning process. Recently, reward models (RMs) have become increasingly pivotal in the reasoning process. Specifically, process RMs evaluate each reasoning step, outcome RMs focus on the assessment of reasoning results, and critique RMs perform error analysis on the entire reasoning process, followed by corrections. However, existing benchmarks for vision-language RMs (VLRMs) typically assess only a single aspect of their capabilities (e.g., distinguishing between two answers), thus limiting the all-round evaluation and restricting the development of RMs in the visual-language domain. To address this gap, we propose a comprehensive and challenging benchmark, dubbed as VLRMBench, encompassing 12,634 questions. VLRMBench is constructed based on three distinct types of datasets, covering mathematical reasoning, hallucination understanding, and multi-image understanding. We design 12 tasks across three major categories, focusing on evaluating VLRMs in the aspects of process understanding, outcome judgment, and critique generation. Extensive experiments are conducted on 21 open-source models and 5 advanced closed-source models, highlighting the challenges posed by VLRMBench. For instance, in the `Forecasting Future', a binary classification task, the advanced GPT-4o achieves only a 76.0% accuracy. The code is available at https://github.com/JCruan519/VLRMBench",
    "checked": true,
    "id": "495f60ebc60056205dbcf0779311ea0c4af948dd",
    "semantic_title": "vlrmbench: a comprehensive and challenging benchmark for vision-language reward models",
    "citation_count": 9,
    "authors": [
      "Jiacheng Ruan",
      "Wenzhen Yuan",
      "Xian Gao",
      "Ye Guo",
      "Daoxin Zhang",
      "Zhe Xu",
      "Yao Hu",
      "Ting Liu",
      "Yuzhuo Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_SITE_towards_Spatial_Intelligence_Thorough_Evaluation_ICCV_2025_paper.html": {
    "title": "SITE: towards Spatial Intelligence Thorough Evaluation",
    "volume": "main",
    "abstract": "Spatial intelligence (SI) represents a cognitive ability encompassing the visualization, manipulation, and reasoning about spatial relationships, underpinning disciplines from neuroscience to robotics. We introduce SITE, a benchmark dataset towards SI Thorough Evaluation in a standardized format of multi-choice visual question-answering, designed to assess large vision-language models' spatial intelligence across diverse visual modalities (single-image, multi-image, and video) and SI factors (figural to environmental scales, spatial visualization and orientation, intrinsic and extrinsic, static and dynamic). Our approach to curating the benchmark combines a bottom-up survey of existing datasets and a top-down strategy drawing upon three classification systems in cognitive science, which prompt us to design two novel types of tasks about view-taking and dynamic scenes. Extensive experiments reveal that leading models fall behind human experts, especially in spatial orientation, a fundamental SI factor. Moreover, we demonstrate a positive correlation between a model's spatial reasoning proficiency and its performance on an embodied AI task",
    "checked": true,
    "id": "0f794af47f950ceece9ebc7c92fc30d7abaecc13",
    "semantic_title": "site: towards spatial intelligence thorough evaluation",
    "citation_count": 3,
    "authors": [
      "Wenqi Wang",
      "Reuben Tan",
      "Pengyue Zhu",
      "Jianwei Yang",
      "Zhengyuan Yang",
      "Lijuan Wang",
      "Andrey Kolobov",
      "Jianfeng Gao",
      "Boqing Gong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Augustin_DASH_Detection_and_Assessment_of_Systematic_Hallucinations_of_VLMs_ICCV_2025_paper.html": {
    "title": "DASH: Detection and Assessment of Systematic Hallucinations of VLMs",
    "volume": "main",
    "abstract": "Vision-language models (VLMs) are prone to object hal- lucinations, where they erroneously indicate the presence of certain objects in an image. Existing benchmarks quantify hallucinations using relatively small, labeled datasets. However, this approach is i) insufficient to assess hallucinations that arise in open-world settings, where VLMs are widely used, and ii) inadequate for detecting systematic errors in VLMs. We propose DASH (Detection and Assess- ment of Systematic Hallucinations), an automatic, large-scale pipeline designed to identify systematic hallucinations of VLMs on real-world images in an open-world setting. A key component is DASH-OPT for image-based retrieval, where we optimize over the \"natural image manifold\" to generate images that mislead the VLM. The output of DASH consists of clusters of real and semantically similar images for which the VLM hallucinates an object. We apply DASH to PaliGemma and two LLaVA-NeXT models across 380 object classes and, in total, find more than 19k clusters with 950k images. We study the transfer of the identified systematic hallucinations to other VLMs and show that fine-tuning PaliGemma with the model-specific images obtained with DASH mitigates object hallucinations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maximilian Augustin",
      "Yannic Neuhaus",
      "Matthias Hein"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_FW-Merging_Scaling_Model_Merging_with_Frank-Wolfe_Optimization_ICCV_2025_paper.html": {
    "title": "FW-Merging: Scaling Model Merging with Frank-Wolfe Optimization",
    "volume": "main",
    "abstract": "Model merging has emerged as a promising approach for multi-task learning (MTL) in large language models (LLMs), providing a training- and data-efficient alternative to conventional fine-tuning. However, with the rapid development of the open-source AI ecosystem and the increasing availability of fine-tuned LLMs, existing model merging methods face two key limitations: (i) they are primarily designed for in-house fine-tuned models, making them less adaptable to diverse model sources with partially unknown model and task information, (ii) they struggle to scale effectively when merging numerous model checkpoints.To address these challenges, we formulate model merging as a constrained optimization problem and introduce a novel approach: Frank-Wolfe Merging (FW-Merging). Inspired by the Frank-Wolfe optimization, our approach iteratively selects the most relevant model parameters to minimize a linear approximation of the objective function, merging them through a predefined merging function. The objective function is designed to capture the desired behavior of the target merged model, while the fine-tuned candidate models defines the constraint set.More importantly, FW-Merging serves as an orthogonal technique to existing merging methods, seamlessly integrating with them to further enhance performance.Our experiments show that FW-Merging scales across diverse model sources, remaining stable with 16 irrelevant models and improving by 15.3% with 16 relevant models on 20 CV tasks, all while maintaining constant memory overhead--unlike the linear overhead of data-informed methods.Compared with the state-of-the-art methods, FW-Merging surpasses the data-free merging method by 32.8% and outperforms the data-informed Adamerging by 8.39% when merging 20 ViT models. Our code is attached with this submission",
    "checked": true,
    "id": "a86b1e59362d7e78d7ac0548db11748f14bf11d5",
    "semantic_title": "fw-merging: scaling model merging with frank-wolfe optimization",
    "citation_count": 1,
    "authors": [
      "Hao Mark Chen",
      "Shell Xu Hu",
      "Wayne Luk",
      "Timothy Hospedales",
      "Hongxiang Fan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Peng_On_the_Provable_Importance_of_Gradients_for_Autonomous_Language-Assisted_Image_ICCV_2025_paper.html": {
    "title": "On the Provable Importance of Gradients for Autonomous Language-Assisted Image Clustering",
    "volume": "main",
    "abstract": "This paper investigates the recently emerged problem of Language-assisted Image Clustering (LaIC), where textual semantics are leveraged to improve the discriminability of visual representations to facilitate image clustering. Due to the unavailability of true class names, one of core challenges of LaIC lies in how to filter positive nouns, i.e., those semantically close to the images of interest, from unlabeled wild corpus data. Existing filtering strategies are predominantly based on the off-the-shelf feature space learned by CLIP; however, despite being intuitive, these strategies lack a rigorous theoretical foundation. To fill this gap, we propose a novel gradient-based framework, termed as GradNorm, which is theoretically guaranteed and shows strong empirical performance. In particular, we measure the positiveness of each noun based on the magnitude of gradients back-propagated from the cross-entropy between the predicted target distribution and the softmax output. Theoretically, we provide a rigorous error bound to quantify the separability of positive nouns by GradNorm and prove that GradNorm naturally subsumes existing filtering strategies as extremely special cases of itself. Empirically, extensive experiments show that GradNorm achieves the state-of-the-art clustering performance on various benchmarks",
    "checked": false,
    "id": "e9d2b4a702b8a07aaf9d30561fb669570619f27a",
    "semantic_title": "on the provable importance of gradients for language-assisted image clustering",
    "citation_count": 0,
    "authors": [
      "Bo Peng",
      "Jie Lu",
      "Guangquan Zhang",
      "Zhen Fang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/He_ZipVL_Accelerating_Vision-Language_Models_through_Dynamic_Token_Sparsity_ICCV_2025_paper.html": {
    "title": "ZipVL: Accelerating Vision-Language Models through Dynamic Token Sparsity",
    "volume": "main",
    "abstract": "The efficiency of large vision-language models (LVLMs) is constrained by the computational bottleneck of the attention mechanism during the prefill phase and the memory bottleneck of fetching the key-value (KV) cache in the decoding phase, particularly in scenarios involving high-resolution images or videos. Visual content often exhibits substantial redundancy, resulting in highly sparse attention maps within LVLMs. This sparsity can be leveraged to accelerate attention computation or compress the KV cache through various approaches. However, most studies focus on addressing only one of these bottlenecks and do not adequately support dynamic adjustment of sparsity concerning distinct layers or tasks. In this paper, we present ZipVL, an efficient inference framework designed for LVLMs through a dynamic ratio allocation strategy of important tokens. This ratio is adaptively determined based on the layer-specific distribution of attention scores, rather than fixed hyper-parameters, thereby improving efficiency for less complex tasks while maintaining high performance for more challenging ones. Then we select important tokens based on their normalized attention scores and perform sparse attention mechanism solely on those important tokens, reducing the latency in the prefill phase. Tokens deemed less important will be discarded to reduce KV cache size, alleviating the memory bottleneck in the decoding phase. Our experiments demonstrate that ZipVL can accelerate the prefill phase by 2.3x and improve decoding throughput by 2.8x, with a minimal accuracy reduction of only 0.5% on VQAv2 benchmark over LLaVA-Next-13B model, effectively enhancing the generation efficiency of LVLMs",
    "checked": false,
    "id": "7bf7b9d24eaef1d30f77cda4f4489e36a8329ee9",
    "semantic_title": "zipvl: efficient large vision-language models with dynamic token sparsification and kv cache compression",
    "citation_count": 27,
    "authors": [
      "Yefei He",
      "Feng Chen",
      "Jing Liu",
      "Wenqi Shao",
      "Hong Zhou",
      "Kaipeng Zhang",
      "Bohan Zhuang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wei_RayletDF_Raylet_Distance_Fields_for_Generalizable_3D_Surface_Reconstruction_from_ICCV_2025_paper.html": {
    "title": "RayletDF: Raylet Distance Fields for Generalizable 3D Surface Reconstruction from Point Clouds or Gaussians",
    "volume": "main",
    "abstract": "In this paper, we present a generalizable method for 3D surface reconstruction from raw point clouds or pre-estimated 3D Gaussians by 3DGS from RGB images. Unlike existing coordinate-based methods which are often computationally intensive when rendering explicit surfaces, our proposed method, named **RayletDF**, introduces a new technique called raylet distance field, which aims to directly predict surface points from query rays. Our pipeline consists of three key modules: a raylet feature extractor, a raylet distance field predictor, and a multi-raylet blender. These components work together to extract fine-grained local geometric features, predict raylet distances, and aggregate multiple predictions to reconstruct precise surface points. We extensively evaluate our method on multiple public real-world datasets, demonstrating superior performance in surface reconstruction from point clouds or 3D Gaussians. Most notably, our method achieves exceptional generalization ability, successfully recovering 3D surfaces in a single-forward pass across unseen datasets in testing",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shenxing Wei",
      "Jinxi Li",
      "Yafei Yang",
      "Siyuan Zhou",
      "Bo Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Qian_VOVTrack_Exploring_the_Potentiality_in_Raw_Videos_for_Open-Vocabulary_Multi-Object_ICCV_2025_paper.html": {
    "title": "VOVTrack: Exploring the Potentiality in Raw Videos for Open-Vocabulary Multi-Object Tracking",
    "volume": "main",
    "abstract": "Open-vocabulary multi-object tracking (OVMOT) represents a critical new challenge involving the detection and tracking of diverse object categories in videos, encompassing both seen categories (base classes) and unseen categories (novel classes). This issue amalgamates the complexities of open-vocabulary object detection (OVD) and multi-object tracking (MOT). Existing approaches to OVMOT often merge OVD and MOT methodologies as separate modules, not fully leveraging the video information. In this work, we propose VOVTrack, a novel method that integrates object states relevant to MOT and video-centric training to address this challenge from a video analysis standpoint. First, we consider the tracking-related state of the objects during tracking and propose a new prompt-guided attention mechanism for more accurate detection (localization and classification) of time-varying objects. Subsequently, we leverage raw video data without annotations for training by formulating a self-supervised object similarity learning technique to facilitate temporal object tracking (association). Experimental results underscore that VOVTrack establishes itself as a state-of-the-art solution for the open-vocabulary tracking task",
    "checked": false,
    "id": "d4724537c36d3c7f4936de99d7710ae316f48f6b",
    "semantic_title": "vovtrack: exploring the potentiality in videos for open-vocabulary object tracking",
    "citation_count": 3,
    "authors": [
      "Zekun Qian",
      "Ruize Han",
      "Junhui Hou",
      "Linqi Song",
      "Wei Feng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Nissan_Planar_Affine_Rectification_from_Local_Change_of_Scale_and_Orientation_ICCV_2025_paper.html": {
    "title": "Planar Affine Rectification from Local Change of Scale and Orientation",
    "volume": "main",
    "abstract": "We propose a method for affine rectification of an image plane by leveraging changes in local scales and orientations under projective distortion. Specifically, we derive a novel linear constraint that directly relates pairs of points with orientations to the parameters of a projective transformation. This constraint is combined with an existing linear constraint on local scales, leading to highly robust rectification. The method reduces to solving a system of linear equations, enabling an efficient algebraic least-squares solution. It requires only two local scales and two local orientations, which can be extracted from, e.g., SIFT features. Unlike prior approaches, our method does not impose restrictions on individual features, does not require class segmentation, and makes no assumptions about feature interrelations. It is compatible with any feature detector that provides local scale or orientation. Furthermore, combining scaled and oriented points with line segments yields a highly robust algorithm that outperforms baselines. Extensive experiments show the effectiveness of our approach on real-world images, including repetitive patterns, building facades, and text-based content",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuval Nissan",
      "Marc Pollefeys",
      "Daniel Barath"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/So_Grouped_Speculative_Decoding_for_Autoregressive_Image_Generation_ICCV_2025_paper.html": {
    "title": "Grouped Speculative Decoding for Autoregressive Image Generation",
    "volume": "main",
    "abstract": "Recently, autoregressive (AR) image models have demonstrated remarkable generative capabilities, positioning themselves as a compelling alternative to diffusion models. However, their sequential nature leads to long inference times, limiting their practical scalability. In this work, we introduce Grouped Speculative Decoding (GSD), a novel, training-free acceleration method for AR image models. While recent studies have explored Speculative Decoding (SD) as a means to speed up AR image generation, existing approaches either provide only modest acceleration or require additional training. Our in-depth analysis reveals a fundamental difference between language and image tokens: image tokens exhibit inherent redundancy and diversity, meaning multiple tokens can convey valid semantics. However, traditional SD methods are designed to accept only a single most-likley token, which fails to leverage this difference, leading to excessive false-negative rejections. To address this, we propose a new SD strategy that evaluates clusters of visually valid tokens rather than relying on a single target token. Additionally, we observe that static clustering based on embedding distance is ineffective, which motivates our dynamic GSD approach. Extensive experiments show that GSD accelerates AR image models by an average of 3.7x while preserving image quality--all without requiring any additional training",
    "checked": true,
    "id": "9a179a78d8e028e74d2656f52ee056f86ae00cb0",
    "semantic_title": "grouped speculative decoding for autoregressive image generation",
    "citation_count": 2,
    "authors": [
      "Junhyuk So",
      "Juncheol Shin",
      "Hyunho Kook",
      "Eunhyeok Park"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Si_You_Share_Beliefs_I_Adapt_Progressive_Heterogeneous_Collaborative_Perception_ICCV_2025_paper.html": {
    "title": "You Share Beliefs, I Adapt: Progressive Heterogeneous Collaborative Perception",
    "volume": "main",
    "abstract": "Collaborative perception enables vehicles to overcome individual perception limitations by sharing information, allowing them to see further and through occlusions. In real-world scenarios, models on different vehicles are often heterogeneous due to manufacturer variations. Existing methods for heterogeneous collaborative perception address this challenge by fine-tuning adapters or the entire network to bridge the domain gap. However, these methods are impractical in real-world applications, as each new collaborator must undergo joint training with the ego vehicle on a dataset before inference, or the ego vehicle stores models for all potential collaborators in advance. Therefore, we pose a new question: Can we tackle this challenge directly during inference, eliminating the need for joint training? To answer this, we introduce Progressive Heterogeneous Collaborative Perception (PHCP), a novel framework that formulates the problem as few-shot unsupervised domain adaptation. Unlike previous work, PHCP dynamically aligns features by self-training an adapter during inference, eliminating the need for labeled data and joint training. Extensive experiments on the OPV2V dataset demonstrate that PHCP achieves strong performance across diverse heterogeneous scenarios. Notably, PHCP achieves performance comparable to SOTA methods trained on the entire dataset while using only a small amount of unlabeled data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Si",
      "Ehsan Javanmardi",
      "Manabu Tsukada"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Music-Aligned_Holistic_3D_Dance_Generation_via_Hierarchical_Motion_Modeling_ICCV_2025_paper.html": {
    "title": "Music-Aligned Holistic 3D Dance Generation via Hierarchical Motion Modeling",
    "volume": "main",
    "abstract": "Well-coordinated, music-aligned holistic dance enhances emotional expressiveness and audience engagement. However, generating such dances remains challenging due to the scarcity of holistic 3D dance datasets, the difficulty of achieving cross-modal alignment between music and dance, and the complexity of modeling interdependent motion across the body, hands, and face. To address these challenges, we introduce SoulDance, a high-precision music-dance paired dataset captured via professional motion capture systems, featuring meticulously annotated holistic dance movements. Building on this dataset, we propose SoulNet, a framework designed to generate music-aligned, kinematically coordinated holistic dance sequences. SoulNet consists of three principal components: (1) Hierarchical Residual Vector Quantization, which models complex, fine-grained motion dependencies across the body, hands, and face; (2) Music-Aligned Generative Model, which composes these hierarchical motion units into expressive and coordinated holistic dance; (3) Music-Motion Retrieval Module, a pre-trained cross-modal model that functions as a music-dance alignment prior, ensuring temporal synchronization and semantic coherence between generated dance and input music throughout the generation process. Extensive experiments demonstrate that SoulNet significantly surpasses existing approaches in generating high-quality, music-coordinated, and well-aligned holistic 3D dance sequences. Additional resources are available on our project: https://xjli360.github.io/SoulDance",
    "checked": true,
    "id": "1c2a61e7cdbaa67b0ea9278a639eb196f37d3db6",
    "semantic_title": "music-aligned holistic 3d dance generation via hierarchical motion modeling",
    "citation_count": 0,
    "authors": [
      "Xiaojie Li",
      "Ronghui Li",
      "Shukai Fang",
      "Shuzhao Xie",
      "Xiaoyang Guo",
      "Jiaqing Zhou",
      "Junkun Peng",
      "Zhi Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xiang_Advancing_Visual_Large_Language_Model_for_Multi-granular_Versatile_Perception_ICCV_2025_paper.html": {
    "title": "Advancing Visual Large Language Model for Multi-granular Versatile Perception",
    "volume": "main",
    "abstract": "Perception is a fundamental task in the field of computer vision, encompassing a diverse set of subtasks that can be systematically categorized into four distinct groups based on two dimensions: prediction type and instruction type. Notably, existing researches often focus solely on a limited subset of these potential combinations, which constrains their applicability and versatility across various contexts. In response to this challenge, we present MVL-LM, a Multi-granular and Versatile Perception framework incorporating Visual Large Language Model. Our framework is designed to integrate both word-based and sentence-based perception tasks alongside box and mask predictions within a single architecture. MVL-LM features an innovative multi-granularity decoder in conjunction with a CoT-inspired dataset unification strategy, enabling seamless supervised fine-tuning across a wide spectrum of tasks, including but not limited to panoptic segmentation, detection, grounding, and referring expression segmentation. Furthermore, we introduce a query enhancement strategy aimed at harnessing the decoding and generative capabilities inherent in VLLMs. Extensive experiments conducted across a range of benchmarks in both word-based and sentence-based perception tasks substantiate the efficacy of our framework. The code will be available at https://github.com/xiangwentao666/MVP-LM",
    "checked": true,
    "id": "551a750af451763b5486c5d9b273ef26951207b4",
    "semantic_title": "advancing visual large language model for multi-granular versatile perception",
    "citation_count": 0,
    "authors": [
      "Wentao Xiang",
      "Haoxian Tan",
      "Yujie Zhong",
      "Cong Wei",
      "Dengjie Li",
      "Yujiu Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Fu_Neural_Solver_of_Dichromatic_Reflection_Model_for_Specular_Highlight_Removal_ICCV_2025_paper.html": {
    "title": "Neural Solver of Dichromatic Reflection Model for Specular Highlight Removal",
    "volume": "main",
    "abstract": "The Dichromatic Reflection Model (DRM), a widely used physical image formation model, has been extensively applied to specular highlight removal. However, traditional DRM solvers fail to effectively recover the missing content underneath specular highlights and are prone to producing visual artifacts. Additionally, existing deep learning-based methods do not fully exploit the full set of the DRM variables. Instead, they typically learn to translate an input image into its diffuse image (and specular residue image). As a result, their performance remains somewhat limited. To overcome these limitations, we propose a neural DRM solver for specular highlight removal. Our pipeline consists of three networks: Highlight Detection Network (HDNet), Alpha-Chrom Estimation Network (ACENet), and Refinement Network (RNet). Specifically, HDNet is first used to detect specular highlights. Meanwhile, guided by multi-level contextual contrasted features from HDNet, ACENet estimates the DRM variables. These estimated variables are then used by our reconstruction models to generate the specular-free and specular residue images. Finally, we introduce RNet, a model-free network, to further refine the results. Extensive experiments on existing datasets and our collected images demonstrate that our neural solver is superior to previous methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gang Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Pardo_MatchDiffusion_Training-free_Generation_of_Match-Cuts_ICCV_2025_paper.html": {
    "title": "MatchDiffusion: Training-free Generation of Match-Cuts",
    "volume": "main",
    "abstract": "Match-cuts are powerful cinematic tools that create seamless transitions between scenes, delivering strong visual and metaphorical connections. However, crafting impactful match-cuts is a challenging and resource-intensive process that requires deliberate artistic planning throughout the production pipeline. In this work, we introduce MatchDiffusion, a training-free method that uses text-to-video diffusion models to automatically generate match-cuts. As such, MatchDiffusion is the first method for match-cut generation. Our method leverages an inherent property of diffusion models, whereby the early denoising steps determine the broad appearance of the scene, while the latter steps add details. Motivated by this property, MatchDiffusion first performs \"Joint Diffusion\", by initializing generation for two prompts from a shared noise sample, and following a shared denoising path for the first denoising steps.This process results in the two videos sharing structural and motion characteristics. After Joint Diffusion, we then conduct \"Disjoint Diffusion\", allowing the videos' denoising paths to diverge and introduce their unique details. MatchDiffusion thus yields visually coherent videos that are amenable to match-cuts. We demonstrate the effectiveness of our method through user studies and metrics, showing its potential to democratize match-cut creation",
    "checked": true,
    "id": "ed87f3d1aa3f0d22e78464bc1ceb8c863720baa5",
    "semantic_title": "matchdiffusion: training-free generation of match-cuts",
    "citation_count": 1,
    "authors": [
      "Alejandro Pardo",
      "Fabio Pizzati",
      "Tong Zhang",
      "Alexander Pondaven",
      "Philip Torr",
      "Juan Camilo Perez",
      "Bernard Ghanem"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_STD-GS_Exploring_Frame-Event_Interaction_for_SpatioTemporal-Disentangled_Gaussian_Splatting_to_Reconstruct_ICCV_2025_paper.html": {
    "title": "STD-GS: Exploring Frame-Event Interaction for SpatioTemporal-Disentangled Gaussian Splatting to Reconstruct High-Dynamic Scene",
    "volume": "main",
    "abstract": "High-dynamic scene reconstruction aims to represent static background with rigid spatial features and dynamic objects with deformed continuous spatiotemporal features. Typically, existing methods adopt unified representation model (e.g., Gaussian) to directly match the spatiotemporal features of dynamic scene from frame camera. However, this unified paradigm fails in the potential discontinuous temporal features of objects due to frame imaging and the heterogeneous spatial features between background and objects. In this work, we introduce event camera to compensate for frame camera, and propose a spatiotemporal-disentangled Gaussian splatting framework for high-dynamic scene reconstruction. As for dynamic scene, we figure out that background and objects have appearance discrepancy in frame-based spatial features and motion discrepancy in event-based temporal features, which motivates us to distinguish the spatiotemporal features between background and objects via clustering. As for dynamic object, we discover that Gaussian representations and event data share the consistent spatiotemporal characteristic, which could serve as a prior to guide the spatiotemporal disentanglement of object Gaussians. Within Gaussian splatting framework, the cumulative scene-object disentanglement can improve the spatiotemporal discrimination between background and objects to render the time-continuous dynamic scene. Extensive experiments are performed to verify the superiority of our method",
    "checked": true,
    "id": "936bc2f5b359ffe70ad3278ba7c99eda3d3dd6b6",
    "semantic_title": "std-gs: exploring frame-event interaction for spatiotemporal-disentangled gaussian splatting to reconstruct high-dynamic scene",
    "citation_count": 0,
    "authors": [
      "Hanyu Zhou",
      "Haonan Wang",
      "Haoyue Liu",
      "Yuxing Duan",
      "Luxin Yan",
      "Gim Hee Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cen_C2MIL_Synchronizing_Semantic_and_Topological_Causalities_in_Multiple_Instance_Learning_ICCV_2025_paper.html": {
    "title": "C2MIL: Synchronizing Semantic and Topological Causalities in Multiple Instance Learning for Robust and Interpretable Survival Analysis",
    "volume": "main",
    "abstract": "Graph-based Multiple Instance Learning (MIL) is widely used in survival analysis with Hematoxylin and Eosin (H&E)-stained whole slide images (WSIs) due to its ability to capture topological information. However, variations in staining and scanning can introduce semantic bias, while topological subgraphs that are not relevant to the causal relationships can create noise, resulting in biased slide-level representations. These issues can hinder both the interpretability and generalization of the analysis. To tackle this, we introduce a dual structural causal model as the theoretical foundation and propose a novel and interpretable dual causal graph-based MIL model, C2MIL. C2MIL incorporates a novel cross-scale adaptive feature disentangling module for semantic causal intervention and a new Bernoulli differentiable causal subgraph sampling method for topological causal discovery. A joint optimization strategy combining disentangling supervision and contrastive learning enables simultaneous refinement of both semantic and topological causalities. Experiments demonstrate that C2MIL consistently improves generalization and interpretability over existing methods and can serve as a causal enhancement for diverse MIL baselines. The code is available at https://github.com/mimic0127/C2MIL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Min Cen",
      "Zhenfeng Zhuang",
      "Yuzhe Zhang",
      "Min Zeng",
      "Baptiste Magnier",
      "Lequan Yu",
      "Hong Zhang",
      "Liansheng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Stojanov_Weakly-Supervised_Learning_of_Dense_Functional_Correspondences_ICCV_2025_paper.html": {
    "title": "Weakly-Supervised Learning of Dense Functional Correspondences",
    "volume": "main",
    "abstract": "Establishing dense correspondences across image pairs is essential for tasks such as shape reconstruction and robot manipulation. In the challenging setting of matching across different categories, the function of an object, i.e., the effect that an object can cause on other objects, can guide how correspondences should be established. This is because object parts that enable specific functions often share similarities in shape and appearance. We derive the definition of dense functional correspondence based on this observation and propose a weakly-supervised learning paradigm to tackle the prediction task. The main insight behind our approach is that we can leverage vision-language models to pseudo-label multi-view images to obtain functional parts. We then integrate this with dense contrastive learning from pixel correspondences to distill both functional and spatial knowledge into a new model that can establish dense functional correspondence. Further, we curate synthetic and real evaluation datasets as task benchmarks. Our results demonstrate the advantages of our approach over baseline solutions consisting of off-the-shelf self-supervised image representations and grounded vision language models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stefan Stojanov",
      "Linan Zhao",
      "Yunzhi Zhang",
      "Daniel L. K. Yamins",
      "Jiajun Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Fang_Proxy-Bridged_Game_Transformer_for_Interactive_Extreme_Motion_Prediction_ICCV_2025_paper.html": {
    "title": "Proxy-Bridged Game Transformer for Interactive Extreme Motion Prediction",
    "volume": "main",
    "abstract": "Multi-person motion prediction becomes particularly challenging when handling highly interactive scenarios involving extreme motions. Previous works focused more on the case of `moderate' motions (e.g., walking together), where predicting each pose in isolation often yields reasonable results. However, these approaches fall short in modeling extreme motions like lindy-hop dances, as they require a more comprehensive understanding of cross-person dependencies. To bridge this gap, we introduce Proxy-bridged Game Transformer (PGformer), a Transformer-based foundation model that captures the interactions driving extreme multi-person motions. PGformer incorporates a novel cross-query attention module to learn bidirectional dependencies between pose sequences and a proxy unit that subtly controls bidirectional spatial information flow. We evaluated PGformer on the challenging ExPI dataset, which involves large collaborative movements. Both quantitative and qualitative results demonstrate the superiority of PGformer in both short- and long-term predictions. We also test the proposed method on moderate movement datasets CMU-Mocap and MuPoTS-3D, generalizing PGformer to scenarios with more than two individuals with promising results. Code of PGformer is available at https://github.com/joyfang1106/pgformer",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanwen Fang",
      "Wenqi Jia",
      "Xu Cao",
      "Peng-Tao Jiang",
      "Guodong Li",
      "Jintai Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_ReFlex_Text-Guided_Editing_of_Real_Images_in_Rectified_Flow_via_ICCV_2025_paper.html": {
    "title": "ReFlex: Text-Guided Editing of Real Images in Rectified Flow via Mid-Step Feature Extraction and Attention Adaptation",
    "volume": "main",
    "abstract": "Rectified Flow text-to-image models surpass diffusion models in image quality and text alignment, but adapting ReFlow for real-image editing remains challenging. We propose a new real-image editing method for ReFlow by analyzing the intermediate representations of multimodal transformer blocks and identifying three key features. To extract these features from real images with sufficient structural preservation, we leverage mid-step latent, which is inverted only up to the mid-step. We then adapt attention during injection to improve editability and enhance alignment to the target text. Our method is training-free, requires no user-provided mask, and can be applied even without a source prompt. Extensive experiments on two benchmarks with nine baselines demonstrate its superior performance over prior methods, further validated by human evaluations confirming a strong user preference for our approach",
    "checked": true,
    "id": "7e84486d158beb9980883b22abfe220000ca1213",
    "semantic_title": "reflex: text-guided editing of real images in rectified flow via mid-step feature extraction and attention adaptation",
    "citation_count": 1,
    "authors": [
      "Jimyeong Kim",
      "Jungwon Park",
      "Yeji Song",
      "Nojun Kwak",
      "Wonjong Rhee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Fu_Co-Painter_Fine-Grained_Controllable_Image_Stylization_via_Implicit_Decoupling_and_Adaptive_ICCV_2025_paper.html": {
    "title": "Co-Painter: Fine-Grained Controllable Image Stylization via Implicit Decoupling and Adaptive Injection",
    "volume": "main",
    "abstract": "Controllable diffusion models have been widely applied in image stylization. However, existing methods often treat the style in the reference image as a single, indivisible entity, which makes it difficult to transfer specific stylistic attributes. To address this issue, we propose a fine-grained controllable image stylization framework, Co-Painter, to decouple multiple attributes embedded in the reference image and adaptively inject it into the diffusion model. We first build a multi-condition image stylization framework based on the text-to-image generation model. Then, to drive it, we develop a fine-grained decoupling mechanism to implicitly separate the attributes from the image. Finally, we design a gated feature injection mechanism to adaptively regulate the importance of multiple attributes. To support the above procedure, we also build a dataset with fine-grained styles. It comprises nearly 48,000 image-text pairs samples. Extensive experiments demonstrate that the proposed model achieves an optimal balance between text alignment and style similarity to reference images, both in standard and fine-grained settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowen Fu",
      "Wei Wei",
      "Jiaqi Tang",
      "Jiangtao Nie",
      "Yanyu Ye",
      "Xiaogang Xu",
      "Ying-Cong Chen",
      "Lei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wen_Object-level_Correlation_for_Few-Shot_Segmentation_ICCV_2025_paper.html": {
    "title": "Object-level Correlation for Few-Shot Segmentation",
    "volume": "main",
    "abstract": "Few-shot semantic segmentation (FSS) aims to segment objects of novel categories in the query images given only a few annotated support samples. Existing methods primarily build the image-level correlation between the support target object and the entire query image. However, this correlation contains the hard pixel noise, i.e., irrelevant background objects, that is intractable to trace and suppress, leading to the overfitting of the background. To address the limitation of this correlation, we imitate the biological vision process to identify novel objects in the object-level information. Target identification in the general objects is more valid than in the entire image, especially in the low-data regime. Inspired by this, we design an Object-level Correlation Network (OCNet) by establishing the object-level correlation between the support target object and query general objects, which is mainly composed of the General Object Mining Module (GOMM) and Correlation Construction Module (CCM). Specifically, GOMM constructs the query general object feature by learning saliency and high-level similarity cues, where the general objects include the irrelevant background objects and the target foreground object. Then, CCM establishes the object-level correlation by allocating the target prototypes to match the general object feature. The generated object-level correlation can mine the query target feature and suppress the hard pixel noise for the final prediction. Extensive experiments on PASCAL-5i and COCO-20i show that our model achieves the state-of-the-art performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chunlin Wen",
      "Yu Zhang",
      "Jie Fan",
      "Hongyuan Zhu",
      "Xiu-Shen Wei",
      "Yijun Wang",
      "Zhiqiang Kou",
      "Shuzhou Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xu_Hipandas_Hyperspectral_Image_Joint_Denoising_and_Super-Resolution_by_Image_Fusion_ICCV_2025_paper.html": {
    "title": "Hipandas: Hyperspectral Image Joint Denoising and Super-Resolution by Image Fusion with the Panchromatic Image",
    "volume": "main",
    "abstract": "Hyperspectral images (HSIs) are frequently noisy and of low resolution due to the constraints of imaging devices. Recently launched satellites can concurrently acquire HSIs and panchromatic (PAN) images, enabling the restoration of HSIs to generate clean and high-resolution imagery through fusing PAN images for denoising and super-resolution. However, previous studies treated these two tasks as independent processes, resulting in accumulated errors. This paper introduces Hyperspectral Image Joint Pandenoising and Pansharpening (Hipandas), a novel learning paradigm that reconstructs high quality HSIs from noisy low-resolution HSIs (NLRHS) and high-resolution PAN images. The proposed unsupervised Hipandas framework consists of a guided denoising network, a guided super-resolution network, and a PAN reconstruction network, utilizing an HSI low-rank prior and a newly introduced detail-oriented low-rank prior. The interconnection of these networks complicates the training process, necessitating a two-stage training strategy to ensure effective training. Experimental results on both simulated and real-world datasets indicate that the proposed method surpasses state-of-the-art algorithms, yielding more accurate and visually pleasing HRHS images. The code is available at https://github.com/shuangxu96/Hipandas",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuang Xu",
      "Zixiang Zhao",
      "Haowen Bai",
      "Chang Yu",
      "Jiangjun Peng",
      "Xiangyong Cao",
      "Deyu Meng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ban_SAMO_A_Lightweight_Sharpness-Aware_Approach_for_Multi-Task_Optimization_with_Joint_ICCV_2025_paper.html": {
    "title": "SAMO: A Lightweight Sharpness-Aware Approach for Multi-Task Optimization with Joint Global-Local Perturbation",
    "volume": "main",
    "abstract": "Multi-task learning (MTL) enables a joint model to capture commonalities across multiple tasks, reducing computation costs and improving data efficiency. However, a major challenge in MTL optimization is task conflicts, where the task gradients differ in direction or magnitude, limiting model performance compared to single-task counterparts. Sharpness-aware minimization (SAM) minimizes task loss while simultaneously reducing the sharpness of the loss landscape. Our empirical observations show that SAM effectively mitigates task conflicts in MTL. Motivated by these findings, we explore integrating SAM into MTL but face two key challenges. On one hand, both the average loss gradient and individual task gradients--referred to as global and local information--contribute to SAM, but how to combine them remains unclear. On the other hand, directly computing each task gradient introduces significant computational and memory overheads. To address these challenges, we propose SAMO, a lightweight **S**harpness-**A**ware **M**ulti-task **O**ptimization approach, that leverages a joint global-local perturbation. The local perturbations are approximated using only forward passes and are layerwise normalized to improve efficiency. Extensive experiments on a suite of multi-task benchmarks demonstrate both the effectiveness and efficiency of our method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Ban",
      "Gokul Ram Subramani",
      "Kaiyi Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yan_RoboTron-Mani_All-in-One_Multimodal_Large_Model_for_Robotic_Manipulation_ICCV_2025_paper.html": {
    "title": "RoboTron-Mani: All-in-One Multimodal Large Model for Robotic Manipulation",
    "volume": "main",
    "abstract": "Recently, robotics has advanced significantly through the integration of larger models and large-scale datasets. However, challenges remain in applying these models to 3D spatial interactions and managing data collection costs. To address these issues, we propose the multimodal robotic manipulation model RoboTron-Mani and the comprehensive dataset RoboData. RoboTron-Mani, on one hand, enhances 3D perception through camera parameters and occupancy supervision. On the other hand, it further incorporates Modality-Isolation-Mask and multimodal decoder blocks based on OpenFlamingo, improving modality fusion and fine-grained perception. RoboData integrats several publicly-available datasets, achieving the first fusion of multi-view images, camera parameters, depth maps, actions, and space alignment, which facilitates comprehensive learning from diverse robotic datasets and offers one complete evaluation system. Trained on RoboData, RoboTron-Mani is the first generalist policy that surpasses expert models, enabling simultaneous evaluation of all tasks across multiple datasets, rather than being limited to specific data or task selections. Specifically, RoboTron-Mani boosts manipulation performance by increasing the average sequence length on CALVIN from 1.7 to 3.5, enabling cross-embodiment generalization, and achieving state-of-the-art results on both simulated and real-world datasets",
    "checked": false,
    "id": "e520836218312a33a32cc92c11e16dd006c2a251",
    "semantic_title": "robomm: all-in-one multimodal large model for robotic manipulation",
    "citation_count": 12,
    "authors": [
      "Feng Yan",
      "Fanfan Liu",
      "Yiyang Huang",
      "Zechao Guan",
      "Liming Zheng",
      "Yufeng Zhong",
      "Chengjian Feng",
      "Lin Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gao_FastJSMA_Accelerating_Jacobian-based_Saliency_Map_Attacks_through_Gradient_Decoupling_ICCV_2025_paper.html": {
    "title": "FastJSMA: Accelerating Jacobian-based Saliency Map Attacks through Gradient Decoupling",
    "volume": "main",
    "abstract": "Adversarial attack plays a critical role in evaluating the robustness of deep learning models. Jacobian-based Saliency Map Attack (JSMA) is an interpretable adversarial method that offers excellent pixel-level control and provides valuable insights into model vulnerabilities. However, its quadratic computational complexity O(M^2 xN) renders it impractical for large-scale datasets, limiting its application despite its inherent value. This paper proposes FastJSMA, an efficient attack method that addresses these computational limitations. Our approach introduces a gradient decoupling mechanism that decomposes the Jacobian calculation into complementary class suppression (g^-) and class excitation (g^+) gradients, reducing complexity to O(M\\sqrt N ). Additionally, we implement a class probing mechanism and an adaptive saliency threshold to further optimize the process. Experimental results across multiple datasets demonstrate that FastJSMA maintains comparable attack success rates while dramatically reducing computation time--requiring only 2.9% of JSMA's processing time on CIFAR-10 and 1.2% on CIFAR-100, and successfully operating on ImageNet where traditional JSMA fails due to memory constraints. This advancement enables the practical application of interpretable saliency map-based attacks on large-scale datasets, balancing effectiveness with computational efficiency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenghao Gao",
      "Shengjie Xu",
      "Zijing Li",
      "Meixi Chen",
      "Chaojian Yu",
      "Yuanjie Shao",
      "Changxin Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_TAR3D_Creating_High-Quality_3D_Assets_via_Next-Part_Prediction_ICCV_2025_paper.html": {
    "title": "TAR3D: Creating High-Quality 3D Assets via Next-Part Prediction",
    "volume": "main",
    "abstract": "We present TAR3D, a novel framework that consists of a 3D-aware Vector Quantized-Variational AutoEncoder (VQVAE) and a Generative Pre-trained Transformer (GPT) to generate high-quality 3D assets. The core insight of this work is to migrate the multimodal unification and promising learning capabilities of the next-token prediction paradigm to conditional 3D object generation. To achieve this, the3D VQ-VAE first encodes a wide range of 3D shapes into a compact triplane latent space and utilizes a set of discrete representations from a trainable codebook to reconstruct fine-grained geometries under the supervision of query point occupancy. Then, the 3D GPT, equipped with a custom triplane position embedding called TriPE, predicts the codebook index sequence with prefilling prompt tokensin an autoregressive manner so that the composition of 3D geometries can be modeled part by part. Extensive experiments on ShapeNet and Objaverse demonstrate that TAR3D can achieve superior generation quality over existing methods in text-to-3D and image-to-3D tasks",
    "checked": true,
    "id": "991ba376f6ee44d4b399423dcb60f86c0663ac1e",
    "semantic_title": "tar3d: creating high-quality 3d assets via next-part prediction",
    "citation_count": 6,
    "authors": [
      "Xuying Zhang",
      "Yutong Liu",
      "Yangguang Li",
      "Renrui Zhang",
      "Yufei Liu",
      "Kai Wang",
      "Wanli Ouyang",
      "Zhiwei Xiong",
      "Peng Gao",
      "Qibin Hou",
      "Ming-Ming Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhong_CVFusion_Cross-View_Fusion_of_4D_Radar_and_Camera_for_3D_ICCV_2025_paper.html": {
    "title": "CVFusion: Cross-View Fusion of 4D Radar and Camera for 3D Object Detection",
    "volume": "main",
    "abstract": "4D radar has received significant attention in autonomous driving thanks to its robustness under adverse weathers. Due to the sparse points and noisy measurements of the 4D radar, most of the research perform the 3D object detection task by integrating images from camera and perform modality fusion in BEV space. However, the potential of the radar and the fusion mechanism is still largely unexplored, hindering the performance improvement. In this study, we propose a cross-view two-stage fusion network called CVFusion. In the first stage, we design a radar guided iterative (RGIter) BEV fusion module to generate high-recall 3D proposal boxes. In the second stage, we aggregate features from multiple heterogeneous views including points, image, and BEV for each proposal. These comprehensive instance level features greatly help refine the proposals and generate high-quality predictions. Extensive experiments on public datasets show that our method outperforms the previous state-of-the-art methods by a large margin, with 9.10% and 3.68% mAP improvements on View-of-Delft (VoD) and TJ4DRadSet, respectively. Our code will be available at https://github.com/zhzhzhzhzhz/CVFusion",
    "checked": true,
    "id": "841d3f508cb81b819f27b77ca05d418105f49dbf",
    "semantic_title": "cvfusion: cross-view fusion of 4d radar and camera for 3d object detection",
    "citation_count": 1,
    "authors": [
      "Hanzhi Zhong",
      "Zhiyu Xiang",
      "Ruoyu Xu",
      "Jingyun Fu",
      "Peng Xu",
      "Shaohong Wang",
      "Zhihao Yang",
      "Tianyu Pu",
      "Eryun Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhai_Efficient_Input-level_Backdoor_Defense_on_Text-to-Image_Synthesis_via_Neuron_Activation_ICCV_2025_paper.html": {
    "title": "Efficient Input-level Backdoor Defense on Text-to-Image Synthesis via Neuron Activation Variation",
    "volume": "main",
    "abstract": "In recent years, text-to-image (T2I) diffusion models have gained significant attention for their ability to generate high-quality images reflecting text prompts. However, their growing popularity has also led to the emergence of backdoor threats, posing substantial risks. Currently, effective defense strategies against such threats are lacking due to the diversity of backdoor targets in T2I synthesis. In this paper, we propose NaviT2I, an efficient input-level backdoor defense framework against diverse T2I backdoors. Our approach is based on the new observation that trigger tokens tend to induce significant neuron activation variation in the early stage of the diffusion generation process, a phenomenon we term Early-step Activation Variation. Leveraging this insight, NaviT2I navigates T2I models to prevent malicious inputs by analyzing Neuron activation variations caused by input tokens. Extensive experiments show that NaviT2I significantly outperforms the baselines in both effectiveness and efficiency across diverse datasets, various T2I backdoors, and different model architectures including UNet and DiT. Furthermore, we show that our method remains effective under potential adaptive attacks",
    "checked": true,
    "id": "87afd6ad631c431d0649ca6aa600862b253cfaf4",
    "semantic_title": "efficient input-level backdoor defense on text-to-image synthesis via neuron activation variation",
    "citation_count": 0,
    "authors": [
      "Shengfang Zhai",
      "Jiajun Li",
      "Yue Liu",
      "Huanran Chen",
      "Zhihua Tian",
      "Wenjie Qu",
      "Qingni Shen",
      "Ruoxi Jia",
      "Yinpeng Dong",
      "Jiaheng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_MoMa-Kitchen_A_100K_Benchmark_for_Affordance-Grounded_Last-Mile_Navigation_in_Mobile_ICCV_2025_paper.html": {
    "title": "MoMa-Kitchen: A 100K+ Benchmark for Affordance-Grounded Last-Mile Navigation in Mobile Manipulation",
    "volume": "main",
    "abstract": "In mobile manipulation, navigation and manipulation are often treated as separate problems, resulting in a significant gap between merely approaching an object and engaging with it effectively. Many navigation approaches primarily define success by proximity to the target, often overlooking the necessity for optimal positioning that facilitates subsequent manipulation. To address this, we introduce \\ours, a benchmark dataset comprising over 100k samples that provide training data for models to learn optimal final navigation positions for seamless transition to manipulation. Our dataset includes affordance-grounded floor labels collected from diverse kitchen environments, in which robotic mobile manipulators of different models attempt to grasp target objects amidst clutter. Using a fully automated pipeline, we simulate diverse real-world scenarios and generate affordance labels for optimal manipulation positions. Visual data are collected from RGB-D inputs captured by a first-person view camera mounted on the robotic arm, ensuring consistency in viewpoint during data collection. We also develop a lightweight baseline model, \\ourmodel, for navigation affordance grounding that demonstrates promising performance on the \\ours benchmark. Our approach enables models to learn affordance-based final positioning that accommodates different arm types and platform heights, thereby paving the way for more robust and generalizable integration of navigation and manipulation in embodied AI",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pingrui Zhang",
      "Xianqiang Gao",
      "Yuhan Wu",
      "Kehui Liu",
      "Dong Wang",
      "Zhigang Wang",
      "Bin Zhao",
      "Yan Ding",
      "Xuelong Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wei_ViT-Linearizer_Distilling_Quadratic_Knowledge_into_Linear-Time_Vision_Models_ICCV_2025_paper.html": {
    "title": "ViT-Linearizer: Distilling Quadratic Knowledge into Linear-Time Vision Models",
    "volume": "main",
    "abstract": "Vision Transformers (ViTs) have delivered remarkable performence through global self-attention, yet their quadratic complexity can become prohibitive for high-resolution inputs. In this work, we present ViT-Linearizer, a cross-architecture distillation framework that transfers rich ViT representations into a linear-time, recurrent-style model. Our approach leverages 1) activation matching, an intermediate constraint that encourages a student to align its token-wise dependencies with those produced by the teacher, and 2) masked prediction, a contextual reconstruction objective that requires the student to predict the teacher's representations for unseen (masked) tokens, to effectively distill the quadratic self-attention knowledge into the student while maintaining efficient complexity. Empirically, our method provides notable speedups particularly for high-resolution tasks, significantly addressing the hardware challenges in inference. Additionally, it also elevates Mamba-based architectures' performance on standard vision benchmarks, achieving a competitive 84.3% top-1 accuracy on ImageNet with a base-sized model. Our results underscore the good potential of RNN-based solutions for large-scale visual tasks, bridging the gap between theoretical efficiency and real-world effectiveness",
    "checked": true,
    "id": "b41683fb03b5efe6cc1990513e362e22b4e1426f",
    "semantic_title": "vit-linearizer: distilling quadratic knowledge into linear-time vision models",
    "citation_count": 2,
    "authors": [
      "Guoyizhe Wei",
      "Rama Chellappa"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Fu_RobustSplat_Decoupling_Densification_and_Dynamics_for_Transient-Free_3DGS_ICCV_2025_paper.html": {
    "title": "RobustSplat: Decoupling Densification and Dynamics for Transient-Free 3DGS",
    "volume": "main",
    "abstract": "3D Gaussian Splatting (3DGS) has gained significant attention for its real-time, photo-realistic rendering in novel-view synthesis and 3D modeling. However, existing methods struggle with accurately modeling scenes affected by transient objects, leading to artifacts in the rendered images. We identify that the Gaussian densification process, while enhancing scene detail capture, unintentionally contributes to these artifacts by growing additional Gaussians that model transient disturbances. To address this, we propose RobustSplat, a robust solution based on two critical designs. First, we introduce a delayed Gaussian growth strategy that prioritizes optimizing static scene structure before allowing Gaussian splitting/cloning, mitigating overfitting to transient objects in early optimization. Second, we design a scale-cascaded mask bootstrapping approach that first leverages lower-resolution feature similarity supervision for reliable initial transient mask estimation, taking advantage of its stronger semantic consistency and robustness to noise, and then progresses to high-resolution supervision to achieve more precise mask prediction. Extensive experiments on multiple challenging datasets show that our method outperforms existing methods, clearly demonstrating the robustness and effectiveness of our method. Our project page is https://fcyycf.github.io/RobustSplat/",
    "checked": true,
    "id": "703c220e4255180e2d6e1405cdb250a87b5f83d3",
    "semantic_title": "robustsplat: decoupling densification and dynamics for transient-free 3dgs",
    "citation_count": 1,
    "authors": [
      "Chuanyu Fu",
      "Yuqi Zhang",
      "Kunbin Yao",
      "Guanying Chen",
      "Yuan Xiong",
      "Chuan Huang",
      "Shuguang Cui",
      "Xiaochun Cao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_Generalized_Deep_Multi-view_Clustering_via_Causal_Learning_with_Partially_Aligned_ICCV_2025_paper.html": {
    "title": "Generalized Deep Multi-view Clustering via Causal Learning with Partially Aligned Cross-view Correspondence",
    "volume": "main",
    "abstract": "Multi-view clustering (MVC) aims to explore the common clustering structure across multiple views. Many existing MVC methods heavily rely on the assumption of view consistency, where alignments for corresponding samples across different views are ordered in advance. However, real-world scenarios often present a challenge as only partial data is consistently aligned across different views, restricting the overall clustering performance. In this work, we consider the model performance decreasing phenomenon caused by data order shift (i.e., from fully to partially aligned) as a generalized multi-view clustering problem. To tackle this problem, we design a causal multi-view clustering network, termed CauMVC. We adopt a causal modeling approach to understand multi-view clustering procedure. To be specific, we formulate the partially aligned data as an intervention and multi-view clustering with partially aligned data as an post-intervention inference. However, obtaining invariant features directly can be challenging. Thus, we design a Variational Auto-Encoder for causal learning by incorporating an encoder from existing information to estimate the invariant features. Moreover, a decoder is designed to perform the post-intervention inference. Lastly, we design a contrastive regularizer to capture sample correlations. To the best of our knowledge, this paper is the first work to deal generalized multi-view clustering via causal learning. Empirical experiments on both fully and partially aligned data illustrate the strong generalization and effectiveness of CauMVC",
    "checked": true,
    "id": "7c83bceb59d906b06d7eaa18b635a35b829b966a",
    "semantic_title": "generalized deep multi-view clustering via causal learning with partially aligned cross-view correspondence",
    "citation_count": 0,
    "authors": [
      "Xihong Yang",
      "Siwei Wang",
      "Jiaqi Jin",
      "Fangdi Wang",
      "Tianrui Liu",
      "Yueming Jin",
      "Xinwang Liu",
      "En Zhu",
      "Kunlun He"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhao_MeasureXpert_Automatic_Anthropometric_Measurement_Extraction_from_Two_Unregistered_Partial_Posed_ICCV_2025_paper.html": {
    "title": "MeasureXpert: Automatic Anthropometric Measurement Extraction from Two Unregistered, Partial, Posed, and Dressed Body Scans",
    "volume": "main",
    "abstract": "While automatic anthropometric measurement extraction has witnessed growth in recent years, effective, non-contact, and precise measurement methods for dressed humans in arbitrary poses are still lacking, limiting the widespread application of this technology. The occlusion caused by clothing and the adverse influence of posture on body shape significantly increase the complexity of this task. Additionally, current methods often assume the availability of a complete 3D body mesh in a canonical pose (e.g., \"A\" or \"T\" pose), which is not always the case in practice. To address these challenges, we propose MeasureXpert, a novel learning-based model that requires only two unregistered, partial, and dressed body scans as input, and accommodates entirely independent and arbitrary poses for each scan. MeasureXpert computes a comprehensive representation of the naked body shape by synergistically fusing features from the front- and back-view partial point clouds. The comprehensive representation obtained is mapped onto a 3D undressed body shape space, assuming a canonical posture and incorporating predefined measurement landmarks. A point-based offset optimization is also developed to refine the reconstructed complete body shape, enabling accurate regression of measurement values. To train the proposed model, a new large-scale dataset, consisting of 300K samples, was synthesized. The proposed model was validated using two publicly available real-world datasets and was compared with different relevant methods. Extensive experimental results demonstrate that MeasureXpert achieves superior performance compared to the reference methods. The code and dataset are available at: MeasureXpertProject",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ran Zhao",
      "Xinxin Dai",
      "Pengpeng Hu",
      "Vasile Palade",
      "Adrian Munteanu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yun_FedMeNF_Privacy-Preserving_Federated_Meta-Learning_for_Neural_Fields_ICCV_2025_paper.html": {
    "title": "FedMeNF: Privacy-Preserving Federated Meta-Learning for Neural Fields",
    "volume": "main",
    "abstract": "Neural fields provide a memory-efficient representation of data, which can effectively handle diverse modalities and large-scale data. However, learning to map neural fields often requires large amounts of training data and computations, which can be limited to resource-constrained edge devices. One approach to tackle this limitation is to leverage Federated Meta-Learning (FML), but traditional FML approaches suffer from privacy leakage. To address these issues, we introduce a novel FML approach called FedMeNF. FedMeNF utilizes a new privacy-preserving loss function that regulates privacy leakage in the local meta-optimization. This enables the local meta-learner to optimize quickly and efficiently without retaining the client's private data. Our experiments demonstrate that FedMeNF achieves fast optimization speed and robust reconstruction performance, even with few-shot or non-IID data across diverse data modalities, while preserving client data privacy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junhyeog Yun",
      "Minui Hong",
      "Gunhee Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Fang_MeshLLM_Empowering_Large_Language_Models_to_Progressively_Understand_and_Generate_ICCV_2025_paper.html": {
    "title": "MeshLLM: Empowering Large Language Models to Progressively Understand and Generate 3D Mesh",
    "volume": "main",
    "abstract": "We present MeshLLM, a novel framework that leverages large language models (LLMs) to understand and generate text-serialized 3D meshes. Our approach addresses key limitations in existing methods, including the limited dataset scale when catering to LLMs' token length and the loss of 3D structural information during mesh serialization. We introduce a Primitive-Mesh decomposition strategy, which divides 3D meshes into structurally meaningful subunits. This enables the creation of a large-scale dataset with 1500k+ samples, almost 50x larger than previous methods, which aligns better with the LLM scaling law principles. Furthermore, we propose inferring face connectivity from vertices and local mesh assembly training strategies, significantly enhancing the LLMs' ability to capture mesh topology and spatial structures. Experiments show that MeshLLM outperforms the state-of-the-art LLaMA-Mesh in both mesh generation quality and shape understanding, highlighting its great potential in processing text-serialized 3D meshes",
    "checked": true,
    "id": "29b97763ddaa8621672753e91a6b2ef1c002784c",
    "semantic_title": "meshllm: empowering large language models to progressively understand and generate 3d mesh",
    "citation_count": 0,
    "authors": [
      "Shuangkang Fang",
      "I-Chao Shen",
      "Yufeng Wang",
      "Yi-Hsuan Tsai",
      "Yi Yang",
      "Shuchang Zhou",
      "Wenrui Ding",
      "Takeo Igarashi",
      "Ming-Hsuan Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Split-and-Combine_Enhancing_Style_Augmentation_for_Single_Domain_Generalization_ICCV_2025_paper.html": {
    "title": "Split-and-Combine: Enhancing Style Augmentation for Single Domain Generalization",
    "volume": "main",
    "abstract": "Single domain generalization aims to learn a model with good generalization ability from a single source domain. Recent advances in this field have focused on increasing the diversity of the training data through style (e.g., color and texture) augmentation. However, most existing methods apply uniform perturbations to the entire image, failing to simulate complex images with multiple distinct stylistic regions. To address this, we propose a \"Split-And-Combine\" (SAC) strategy to enhance style diversity. Specifically, SAC first performs patch-aware augmentation, which splits an image into multiple patches and applies style augmentation independently to each patch, enabling distinct color variations across regions. Then, SAC combines these patches to reconstruct a complete image and applies adaptive random convolutions, which utilizes a deformable convolution layer with random and Gaussian filters to enhance texture diversity while preserving object integrity. Notably, SAC leverages entropy as a risk assessment criterion to adaptively determine whether a sample should undergo augmentation within the iterative process of random convolutions, preventing excessive augmentation. Furthermore, SAC introduces an energy-based distribution discrepancy score to quantify out-of-distribution likelihood, systematically expanding the augmented data's distribution. SAC can serve as a plug-and-play component to improve the performance of recent methods. Extensive experiments on four datasets demonstrate the effectiveness of SAC",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Zhang",
      "Shuai Yang",
      "Qianlong Dang",
      "Zhize Wu",
      "Lichuan Gu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_FOLDER_Accelerating_Multi-Modal_Large_Language_Models_with_Enhanced_Performance_ICCV_2025_paper.html": {
    "title": "FOLDER: Accelerating Multi-Modal Large Language Models with Enhanced Performance",
    "volume": "main",
    "abstract": "Recently, Multi-modal Large Language Models (MLLMs) have shown remarkable effectiveness for multi-modal tasks due to their ability of cross-modal understanding. However, processing long sequences of visual tokens extracted from visual backbones poses challenges for deployment in real-time applications. To address this issue, we introduce FOLDER, a simple yet effective plug-and-play module designed to reduce the length of the visual token sequence, mitigating computational and memory demands during both training and inference. Through a comprehensive analysis of the token reduction process in the vision encoder, we analyze the information loss introduced by different reduction strategies and develop FOLDER to preserve key information while removing visual redundancy. We show the effectiveness of FOLDER by integrating it into the visual backbone of various MLLMs, significantly accelerating the inference phase. Furthermore, we evaluate its utility as a training accelerator or even performance booster for MLLMs. FOLDER achieves comparable or even better performance than the original models, while dramatically reducing complexity by removing up to 70% of visual tokens. Our code is available at https://github.com/anakin-skywalker-Joseph/Folder",
    "checked": true,
    "id": "0f172f3e813bdcba1f68a93e418752da452cc034",
    "semantic_title": "folder: accelerating multi-modal large language models with enhanced performance",
    "citation_count": 11,
    "authors": [
      "Haicheng Wang",
      "Zhemeng Yu",
      "Gabriele Spadaro",
      "Chen Ju",
      "Victor Quétu",
      "Shuai Xiao",
      "Enzo Tartaglione"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Malone_A_Hyperdimensional_One_Place_Signature_to_Represent_Them_All_Stackable_ICCV_2025_paper.html": {
    "title": "A Hyperdimensional One Place Signature to Represent Them All: Stackable Descriptors For Visual Place Recognition",
    "volume": "main",
    "abstract": "Visual Place Recognition (VPR) enables coarse localization by comparing query images to a reference database of geo-tagged images. Recent breakthroughs in deep learning architectures and training regimes have led to methods with improved robustness to factors like environment appearance change, but with the downside that the required training and/or matching compute scales with the number of distinct environmental conditions encountered. Here, we propose Hyperdimensional One Place Signatures (HOPS) to simultaneously improve the performance, compute and scalability of these state-of-the-art approaches by fusing the descriptors from multiple reference sets captured under different conditions. HOPS scales to any number of environmental conditions by leveraging the Hyperdimensional Computing framework. Extensive evaluations demonstrate that our approach is highly generalizable and consistently improves recall performance across all evaluated VPR methods and datasets by large margins. Arbitrarily fusing reference images without compute penalty enables numerous other useful possibilities, three of which we demonstrate here: improved performance with reduced dimensionality descriptors, stacking synthetic images, and coarse localization to an entire traverse or environmental section",
    "checked": true,
    "id": "7aa7b1e1a1f56b15785ae7beee67748aea6b8cd4",
    "semantic_title": "a hyperdimensional one place signature to represent them all: stackable descriptors for visual place recognition",
    "citation_count": 0,
    "authors": [
      "Connor Malone",
      "Somayeh Hussaini",
      "Tobias Fischer",
      "Michael Milford"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Large_Multi-modal_Models_Can_Interpret_Features_in_Large_Multi-modal_Models_ICCV_2025_paper.html": {
    "title": "Large Multi-modal Models Can Interpret Features in Large Multi-modal Models",
    "volume": "main",
    "abstract": "Recent advances in Large Multimodal Models (LMMs) lead to significant breakthroughs in both academia and industry. One question that arises is how we, as humans, can understand their internal neural representations. This paper takes an initial step towards addressing this question by presenting a versatile framework to identify and interpret the semantics within LMMs. Specifically, 1) we first apply a Sparse Autoencoder(SAE) to disentangle the representations into human understandable features. 2) We then present an automatic interpretation framework to interpreted the open-semantic features learned in SAE by the LMMs themselves. We employ this framework to analyze the LLaVA-NeXT-8B model using the LLaVA-OV-72B model, demonstrating that these features can effectively steer the model's behavior. Our results contribute to a deeper understanding of why LMMs excel in specific tasks, including EQ tests, and illuminate the nature of their mistakes along with potential strategies for their rectification. These findings offer new insights into the internal mechanisms of LMMs and suggest parallels with the cognitive processes of the human brain",
    "checked": true,
    "id": "b2dfbda2dc8bc8954d64db8f0c43d187490456bd",
    "semantic_title": "large multi-modal models can interpret features in large multi-modal models",
    "citation_count": 7,
    "authors": [
      "Kaichen Zhang",
      "Yifei Shen",
      "Bo Li",
      "Ziwei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Song_Progressive_Artwork_Outpainting_via_Latent_Diffusion_Models_ICCV_2025_paper.html": {
    "title": "Progressive Artwork Outpainting via Latent Diffusion Models",
    "volume": "main",
    "abstract": "Latent diffusion models have demonstrated superior performance over traditional methods in generating highly detailed and aesthetically pleasing images, which makes them widely used for various image generation and editing tasks, including outpainting. However, most LDM-based outpainting methods impose constraints on resolution and aspect ratio, often leading to the loss of local details and blurring. One way to address these issues is progressive outpainting, where the image is extended outward incrementally. However, naive progressive outpainting suffers from two key challenges: (1) difficulty in effectively capturing global context, making it hard to maintain the original context, and (2) a tendency to generate unnatural patterns. These challenges are particularly pronounced in art, where artists pre-design the composition before painting. As a result, existing methods often introduce visual inconsistencies that distract the viewer and diminish the intended artistic emphasis. To address these limitations, we propose two types of composition planning modules that enhance progressive outpainting by leveraging global structural guidance. These modules guide a pre-trained stable diffusion model to consider the overall composition, enabling realistic and contextually appropriate artwork completion without labor-intensive user prompts. Through experiments on diverse artwork images, we show the effectiveness of our proposed method both quantitatively and qualitatively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dae-Young Song",
      "Jung-Jae Yu",
      "Donghyeon Cho"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xie_Are_VLMs_Ready_for_Autonomous_Driving_An_Empirical_Study_from_ICCV_2025_paper.html": {
    "title": "Are VLMs Ready for Autonomous Driving? An Empirical Study from the Reliability, Data and Metric Perspectives",
    "volume": "main",
    "abstract": "Recent advancements in Vision-Language Models (VLMs) have fueled interest in autonomous driving applications, particularly for interpretable decision-making. However, the assumption that VLMs provide visually grounded and reliable driving explanations remains unexamined. To address this, we introduce DriveBench, a benchmark evaluating 12 VLMs across 17 settings, covering 19,200 images, 20,498 QA pairs, and four key driving tasks. Our findings reveal that VLMs often generate plausible responses from general knowledge or textual cues rather than true visual grounding, especially under degraded or missing visual inputs. This behavior, concealed by dataset imbalances and insufficient evaluation metrics, poses significant risks in safety-critical scenarios like autonomous driving. We further observe that VLMs possess inherent corruption-awareness but only explicitly acknowledge these issues when directly prompted. Given the challenges and inspired by the inherent corruption awareness, we propose Robust Agentic Utilization (RAU), leveraging VLMs' corruption awareness and agentic planning with external tools to enhance perception reliability for downstream tasks. Our study challenges existing evaluation paradigms and provides a roadmap toward more robust and interpretable autonomous driving systems",
    "checked": false,
    "id": "7b96f2ebbc8664394584056fcd9ffbd0257c849e",
    "semantic_title": "are vlms ready for autonomous driving? an empirical study from the reliability, data, and metric perspectives",
    "citation_count": 44,
    "authors": [
      "Shaoyuan Xie",
      "Lingdong Kong",
      "Yuhao Dong",
      "Chonghao Sima",
      "Wenwei Zhang",
      "Qi Alfred Chen",
      "Ziwei Liu",
      "Liang Pan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hu_Learn2Synth_Learning_Optimal_Data_Synthesis_Using_Hypergradients_for_Brain_Image_ICCV_2025_paper.html": {
    "title": "Learn2Synth: Learning Optimal Data Synthesis Using Hypergradients for Brain Image Segmentation",
    "volume": "main",
    "abstract": "Domain randomization through synthesis is a powerful strategy to train networks that are unbiased with respect to the domain of the input images. Randomization allows networks to see a virtually infinite range of intensities and artifacts during training, thereby minimizing overfitting to appearance and maximizing generalization to unseen data. Although powerful, this approach relies on the accurate tuning of a large set of hyperparameters that govern the probabilistic distribution of the synthesized images. Instead of manually tuning these parameters, we introduce Learn2Synth, a novel procedure in which synthesis parameters are learned using a small set of real labeled data. Unlike methods that impose constraints to align synthetic data with real data (e.g., contrastive or adversarial techniques), which risk misaligning the image and its label map, we tune an augmentation engine such that a segmentation network trained on synthetic data has optimal accuracy when applied to real data. This approach allows the training procedure to benefit from real labeled examples, without ever using these real examples to train the segmentation network, which avoids biasing the network towards the properties of the training set. Specifically, we develop parametric and nonparametric strategies to enhance synthetic images in a way that improves the performance of the segmentation network. We demonstrate the effectiveness of this learning strategy on synthetic and real-world brain scans. Code is available at: https://github.com/HuXiaoling/Learn2Synth",
    "checked": false,
    "id": "16784a2c77a8a114cb11c8c25dd65845f8180ddc",
    "semantic_title": "learn2synth: learning optimal data synthesis using hypergradients",
    "citation_count": 1,
    "authors": [
      "Xiaoling Hu",
      "Xiangrui Zeng",
      "Oula Puonti",
      "Juan Eugenio Iglesias",
      "Bruce Fischl",
      "Yaël Balbastre"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jin_Deep_Incomplete_Multi-view_Clustering_with_Distribution_Dual-Consistency_Recovery_Guidance_ICCV_2025_paper.html": {
    "title": "Deep Incomplete Multi-view Clustering with Distribution Dual-Consistency Recovery Guidance",
    "volume": "main",
    "abstract": "Multi-view clustering leverages complementary representations from diverse sources to enhance performance. However, real-world data often suffer incomplete cases due to factors like privacy concerns and device malfunctions. A key challenge is effectively utilizing available instances to recover missing views. Existing methods frequently overlook the heterogeneity among views during recovery, leading to significant distribution discrepancies between recovered and true data. Additionally, many approaches focus on cross-view correlations, neglecting insights from intra-view reliable structure and cross-view clustering structure. To address these issues, we propose BURG, a novel method for incomplete multi-view clustering with distriBution dUal-consistency Recovery Guidance. We treat each sample as a distinct category and perform cross-view distribution transfer to predict the distribution space of missing views. To compensate for the lack of reliable category information, we design a dual-consistency guided recovery strategy that includes intra-view alignment guided by neighbor-aware consistency and cross-view alignment guided by prototypical consistency. Extensive experiments on benchmarks demonstrate the superiority of BURG in the incomplete multi-view scenario",
    "checked": true,
    "id": "fcd4f46f708e403dd8ced9263788d6f079d1a63a",
    "semantic_title": "deep incomplete multi-view clustering with distribution dual-consistency recovery guidance",
    "citation_count": 1,
    "authors": [
      "Jiaqi Jin",
      "Siwei Wang",
      "Zhibin Dong",
      "Xihong Yang",
      "Xinwang Liu",
      "En Zhu",
      "Kunlun He"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gao_3D_Mesh_Editing_using_Masked_LRMs_ICCV_2025_paper.html": {
    "title": "3D Mesh Editing using Masked LRMs",
    "volume": "main",
    "abstract": "We present a novel approach to mesh shape editing, building on recent progress in 3D reconstruction from multi-view images. We formulate shape editing as a conditional reconstruction problem, where the model must reconstruct the input shape with the exception of a specified 3D region, in which the geometry should be generated from the conditional signal. To this end, we train a conditional Large Reconstruction Model (LRM) for masked reconstruction, using multi-view consistent masks rendered from a randomly generated 3D occlusion, and using one clean viewpoint as the conditional signal. During inference, we manually define a 3D region to edit and provide an edited image from a canonical viewpoint to fill that region. We demonstrate that, in just a single forward pass, our method not only preserves the input geometry in the unmasked region through reconstruction capabilities on par with SoTA, but is also expressive enough to perform a variety of mesh edits from a single image guidance that past works struggle with, while being 2-10 times faster than the top-performing prior work",
    "checked": true,
    "id": "90935ef4cdebe4270c42236c8bd3158f96c53d09",
    "semantic_title": "3d mesh editing using masked lrms",
    "citation_count": 8,
    "authors": [
      "Will Gao",
      "Dilin Wang",
      "Yuchen Fan",
      "Aljaz Bozic",
      "Tuur Stuyck",
      "Zhengqin Li",
      "Zhao Dong",
      "Rakesh Ranjan",
      "Nikolaos Sarafianos"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Acknowledging_Focus_Ambiguity_in_Visual_Questions_ICCV_2025_paper.html": {
    "title": "Acknowledging Focus Ambiguity in Visual Questions",
    "volume": "main",
    "abstract": "No published work on visual question answering (VQA) accounts for ambiguity regarding where the content described in the question is located in the image. To fill this gap, we introduce VQ-FocusAmbiguity, the first VQA dataset that visually grounds each plausible image region a question could refer to when arriving at valid answers. We next analyze and compare our dataset to existing datasets to reveal its unique properties. Finally, we benchmark modern models for two novel tasks related to acknowledging focus ambiguity: recognizing whether a visual question has focus ambiguity and locating all plausible focus regions within the image. Results show that the dataset is challenging for modern models. To facilitate future progress on these tasks, we publicly share the dataset with an evaluation server at https://vizwiz.org/tasks-and-datasets/focus-ambiguity-in-visual-questions/",
    "checked": true,
    "id": "5900a0475484dfaf4ea8ac0395e46a4da00b2cb8",
    "semantic_title": "acknowledging focus ambiguity in visual questions",
    "citation_count": 0,
    "authors": [
      "Chongyan Chen",
      "Yu-Yun Tseng",
      "Zhuoheng Li",
      "Anush Venkatesh",
      "Danna Gurari"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lin_DRaM-LHM_A_Quaternion_Framework_for_Iterative_Camera_Pose_Estimation_ICCV_2025_paper.html": {
    "title": "DRaM-LHM: A Quaternion Framework for Iterative Camera Pose Estimation",
    "volume": "main",
    "abstract": "We explore a quaternion adjugate matrix-based representation for rotational motion in the Perspective-n-Point (PnP) problem. Leveraging quadratic quaternion terms within a Determinant Ratio Matrix (DRaM) estimation framework, we extend its application to perspective scenarios, providing a robust and efficient initialization for iterative PnP pose estimation. Notably, by solving the orthographic projection least-squares problem, DRaM provides a reliable initialization that enhances the accuracy and stability of iterative PnP solvers. Experiments on synthetic and real data demonstrate its efficiency, accuracy, and robustness, particularly under high noise conditions. Furthermore, our non-minimal formulation ensures numerical stability, making it effective for real-world applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Lin",
      "Weizhi  Du",
      "Zhixiang Min",
      "Baochen She",
      "Enrique Dunn",
      "Sonya M. Hanson"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Szymanowicz_Bolt3D_Generating_3D_Scenes_in_Seconds_ICCV_2025_paper.html": {
    "title": "Bolt3D: Generating 3D Scenes in Seconds",
    "volume": "main",
    "abstract": "We present a latent diffusion model for fast feed-forward 3D scene generation. Given one or more images, our model Bolt3D directly samples a 3D scene representation in less than seven seconds on a single GPU. We achieve this by leveraging powerful and scalable existing 2D diffusion network architectures to produce consistent high-fidelity 3D scene representations. To train this model, we create a large-scale multiview-consistent dataset of 3D geometry and appearance by applying state-of-the-art dense 3D reconstruction techniques to existing multiview image datasets. Compared to prior multiview generative models that require per-scene optimization for 3D reconstruction, Bolt3D reduces the inference cost by a factor of 300 times. Project website: szymanowiczs.github.io/bolt3d",
    "checked": true,
    "id": "c7398e15b3eb8247ff235ce4d58d94c548e67404",
    "semantic_title": "bolt3d: generating 3d scenes in seconds",
    "citation_count": 16,
    "authors": [
      "Stanislaw Szymanowicz",
      "Jason Y. Zhang",
      "Pratul Srinivasan",
      "Ruiqi Gao",
      "Arthur Brussee",
      "Aleksander Holynski",
      "Ricardo Martin-Brualla",
      "Jonathan T. Barron",
      "Philipp Henzler"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Serrano-Lozano_Revisiting_Image_Fusion_for_Multi-Illuminant_White-Balance_Correction_ICCV_2025_paper.html": {
    "title": "Revisiting Image Fusion for Multi-Illuminant White-Balance Correction",
    "volume": "main",
    "abstract": "White balance (WB) correction in scenes with multiple illuminants remains a persistent challenge in computer vision. Recent methods explored fusion-based approaches, where a neural network linearly blends multiple sRGB versions of an input image, each processed with predefined WB presets. However, we demonstrate that these methods are suboptimal for common multi-illuminant scenarios. Additionally, existing fusion-based methods rely on sRGB WB datasets lacking dedicated multi-illuminant images, limiting both training and evaluation. To address these challenges, we introduce two key contributions. First, we propose an efficient transformer-based model that effectively captures spatial dependencies across sRGB WB presets, substantially improving upon linear fusion techniques. Second, we introduce a large-scale multi-illuminant dataset comprising over 16,000 sRGB images rendered with five different WB settings, along with WB-corrected images. Our method achieves up to 100% improvement over existing techniques on our new multi-illuminant image fusion dataset. We will release our code and dataset upon acceptance",
    "checked": true,
    "id": "5c710ee47047837d8aad6691161341e214780a9f",
    "semantic_title": "revisiting image fusion for multi-illuminant white-balance correction",
    "citation_count": 0,
    "authors": [
      "David Serrano-Lozano",
      "Aditya Arora",
      "Luis Herranz",
      "Konstantinos G. Derpanis",
      "Michael S. Brown",
      "Javier Vazquez-Corral"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lee_Combinative_Matching_for_Geometric_Shape_Assembly_ICCV_2025_paper.html": {
    "title": "Combinative Matching for Geometric Shape Assembly",
    "volume": "main",
    "abstract": "This paper introduces a new shape-matching methodology, combinative matching, to combine interlocking parts for geometric shape assembly. Previous methods for geometric assembly typically rely on aligning parts by finding identical surfaces between the parts as in conventional shape matching and registration. Specifically, we explicitly model two distinct properties of interlocking shapes: 'identical surface shape' and 'opposite volume occupancy.' Our method thus learns to establish correspondences across regions where their surface shapes appear identical but their volumes occupy the inverted space to each other. To facilitate this process, we also learn to align regions in rotation by estimating their shape orientations via equivariant neural networks. The proposed approach significantly reduces local ambiguities in matching and allows a robust combination of parts in assembly. Experimental results on geometric assembly benchmarks demonstrate the efficacy of our method, consistently outperforming the state of the art",
    "checked": true,
    "id": "f85d56a8bfd0795dd2ec2f6b419956b53fc835b3",
    "semantic_title": "combinative matching for geometric shape assembly",
    "citation_count": 0,
    "authors": [
      "Nahyuk Lee",
      "Juhong Min",
      "Junhong Lee",
      "Chunghyun Park",
      "Minsu Cho"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hanna_Know_Your_Attention_Maps_Class-specific_Token_Masking_for_Weakly_Supervised_ICCV_2025_paper.html": {
    "title": "Know Your Attention Maps: Class-specific Token Masking for Weakly Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "Weakly Supervised Semantic Segmentation (WSSS) is a challenging problem that has been extensively studied in recent years. Traditional approaches often rely on external modules like Class Activation Maps to highlight regions of interest and generate pseudo segmentation masks. In this work, we propose an end-to-end method that directly utilizes the attention maps learned by a Vision Transformer (ViT) for WSSS. We propose training a sparse ViT with multiple [CLS] tokens (one for each class), using a random masking strategy to promote [CLS] token - class assignment. At inference time, we aggregate the different self-attention maps of each [CLS] token corresponding to the predicted labels to generate pseudo segmentation masks. Our proposed approach enhances the interpretability of self-attention maps and ensures accurate class assignments. Extensive experiments on two standard benchmarks and three specialized datasets demonstrate that our method generates accurate pseudo-masks, outperforming related works. Those pseudo-masks can be used to train a segmentation model which achieves results comparable to fully-supervised models, significantly reducing the need for fine-grained labeled data",
    "checked": true,
    "id": "0dbf7107235da5675b01afebcd7b5f0bce36e62e",
    "semantic_title": "know your attention maps: class-specific token masking for weakly supervised semantic segmentation",
    "citation_count": 0,
    "authors": [
      "Joëlle Hanna",
      "Damian Borth"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Saleh_DAViD_Data-efficient_and_Accurate_Vision_Models_from_Synthetic_Data_ICCV_2025_paper.html": {
    "title": "DAViD: Data-efficient and Accurate Vision Models from Synthetic Data",
    "volume": "main",
    "abstract": "The state of the art in human-centric computer vision achieves high accuracy and robustness across a diverse range of tasks. The most effective models in this domain have billions of parameters, thus requiring extremely large datasets, expensive training regimes, and compute-intensive inference. In this paper, we demonstrate that it is possible to train models on much smaller but high-fidelity synthetic datasets, with no loss in accuracy and higher efficiency. Using synthetic training data provides us with excellent levels of detail and perfect labels, while providing strong guarantees for data provenance, usage rights, and user consent. Procedural data synthesis also provides us with explicit control on data diversity, that we can use to address unfairness in the models we train. Extensive quantitative assessment on real input images demonstrates accuracy of our models on three dense prediction tasks: depth estimation, surface normal estimation, and soft foreground segmentation. Our models require only a fraction of the cost of training and inference when compared with foundational models of similar accuracy. Our human-centric synthetic dataset and trained models are available at https://aka.ms/DAViD",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fatemeh Saleh",
      "Sadegh Aliakbarian",
      "Charlie Hewitt",
      "Lohit Petikam",
      "Xian Xiao",
      "Antonio Criminisi",
      "Thomas J. Cashman",
      "Tadas Baltrusaitis"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zheng_Reducing_Unimodal_Bias_in_Multi-Modal_Semantic_Segmentation_with_Multi-Scale_Functional_ICCV_2025_paper.html": {
    "title": "Reducing Unimodal Bias in Multi-Modal Semantic Segmentation with Multi-Scale Functional Entropy Regularization",
    "volume": "main",
    "abstract": "Fusing and balancing multi-modal inputs from novel sensors for dense prediction tasks, particularly semantic segmentation, is critically important yet remains a significant challenge. One major limitation is the tendency of multi-modal frameworks to over-rely on easily learnable modalities, a phenomenon referred to as unimodal dominance or bias. This issue becomes especially problematic in real-world scenarios where the dominant modality may be unavailable, resulting in severe performance degradation. To this end, we apply a simple but effective plug-and-play regularization term based on functional entropy, which introduces no additional parameters or modules. This term is designed to intuitively balance the contribution of each visual modality to the segmentation results. Specifically, we leverage the log-Sobolev inequality to bound functional entropy using functional-Fisher-information. By maximizing the information contributed by each visual modality, our approach mitigates unimodal dominance and establishes a more balanced and robust segmentation framework. A multi-scale regularization module is proposed to apply our proposed plug-and-paly term on high-level features and also segmentation predictions for more balanced multi-modal learning. Extensive experiments on three datasets demonstrate that our proposed method achieves superior performance, i.e., +13.94%, +3.25% and +3.64%, without introducing any additional parameters",
    "checked": true,
    "id": "314cd62411afee3f5ea41bf29e7fefc9f7fed51b",
    "semantic_title": "reducing unimodal bias in multi-modal semantic segmentation with multi-scale functional entropy regularization",
    "citation_count": 5,
    "authors": [
      "Xu Zheng",
      "Yuanhuiyi Lyu",
      "Lutao Jiang",
      "Danda Pani Paudel",
      "Luc Van Gool",
      "Xuming Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hamdan_ETA_Efficiency_through_Thinking_Ahead_A_Dual_Approach_to_Self-Driving_ICCV_2025_paper.html": {
    "title": "ETA: Efficiency through Thinking Ahead, A Dual Approach to Self-Driving with Large Models",
    "volume": "main",
    "abstract": "How can we benefit from large models without sacrificing inference speed, a common dilemma in self-driving systems? A prevalent solution is a dual-system architecture, employing a small model for rapid, reactive decisions and a larger model for slower but more informative analyses. Existing dual-system designs often implement parallel architectures where inference is either directly conducted using the large model at each current frame or retrieved from previously stored inference results. However, these works still struggle to enable large models for a timely response to every online frame. Our key insight is to shift intensive computations of the current frame to previous time steps and perform a batch inference of multiple time steps to make large models respond promptly to each time step. To achieve the shifting, we introduce Efficiency through Thinking Ahead (ETA), an asynchronous system designed to: (1) propagate informative features from the past to the current frame using future predictions from the large model, (2) extract current frame features using a small model for real-time responsiveness, and (3) integrate these dual features via an action mask mechanism that emphasizes action-critical image regions. Evaluated on the Bench2Drive CARLA Leaderboard-v2 benchmark, ETA advances state-of-the-art performance by 8% with a driving score of 69.53 while maintaining a near-real-time inference speed at 50 ms",
    "checked": true,
    "id": "589751352b212a66007d78669d4bf10c10f6ff42",
    "semantic_title": "eta: efficiency through thinking ahead, a dual approach to self-driving with large models",
    "citation_count": 0,
    "authors": [
      "Shadi Hamdan",
      "Chonghao Sima",
      "Zetong Yang",
      "Hongyang Li",
      "Fatma Guney"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_SCORE_Scene_Context_Matters_in_Open-Vocabulary_Remote_Sensing_Instance_Segmentation_ICCV_2025_paper.html": {
    "title": "SCORE: Scene Context Matters in Open-Vocabulary Remote Sensing Instance Segmentation",
    "volume": "main",
    "abstract": "Most existing remote sensing instance segmentation approaches are designed for close-vocabulary prediction, limiting their ability to recognize novel categories or generalize across datasets. This restricts their applicability in diverse Earth observation scenarios. To address this, we introduce open-vocabulary (OV) learning for remote sensing instance segmentation. While current OV segmentation models perform well on natural image datasets, their direct application to remote sensing faces challenges such as diverse landscapes, seasonal variations, and the presence of small or ambiguous objects in aerial imagery. To overcome these challenges, we propose SCORE (Scene Context matters in Open-vocabulary REmote sensing instance segmentation), a framework that integrates multi-granularity scene context, i.e., regional context and global context, to enhance both visual and textual representations. Specifically, we introduce Region-Aware Integration, which refines class embeddings with regional context to improve object distinguishability. Additionally, we propose Global Context Adaptation, which enriches naive text embeddings with remote sensing global context, creating a more adaptable and expressive linguistic latent space for the classifier. We establish new benchmarks for OV remote sensing instance segmentation across diverse datasets. Experimental results demonstrate that, our proposed method achieves SOTA performance, which provides a robust solution for large-scale, real-world geospatial analysis. Our code is available at https://github.com/HuangShiqi128/SCORE",
    "checked": true,
    "id": "ffad873bf91d2de3b49d36940b0db61a537b0aeb",
    "semantic_title": "score: scene context matters in open-vocabulary remote sensing instance segmentation",
    "citation_count": 0,
    "authors": [
      "Shiqi Huang",
      "Shuting He",
      "Huaiyuan Qin",
      "Bihan Wen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_LEGO-Maker_A_Semantic-Driven_Algorithm_for_Text-to-3D_Generation_ICCV_2025_paper.html": {
    "title": "LEGO-Maker: A Semantic-Driven Algorithm for Text-to-3D Generation",
    "volume": "main",
    "abstract": "Driven by large-scale model iterations, the inference speed and generalization ability of 3D model generation have improved significantly. However, the quality of existing methods still falls short of enabling direct use without post-processing. Common issues include insufficient texture clarity, loss of semantic information, lack of fine-grained detail, and the generation of redundant artifacts. Moreover, current approaches focus solely on producing static structures, where individual components remain non-movable, without considering functional applications in the generation process. To address these limitations, we draw inspiration from LEGO-like modular construction and decompose complex models into semantically functional components. We propose LEGO-Maker, a novel framework that reformulates the text-to-3D task into a three-stage process: target image generation, functional semantic decomposition, and multi-task 3D generation with structured fusion. Leveraging a reorganized high-quality 3D dataset, we train a Diffusion model and a semantic segmentation model tailored for 3D generation tasks. Additionally, we design a motion-driven mechanism to introduce action sequences for functionally interactive modules after model fusion. Experimental results demonstrate that, compared to existing methods, our approach significantly enhances semantic understanding, model detail quality, and text consistency while showcasing direct applicability across various scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei Zhang",
      "Lei Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kolek_Learning_Interpretable_Queries_for_Explainable_Image_Classification_with_Information_Pursuit_ICCV_2025_paper.html": {
    "title": "Learning Interpretable Queries for Explainable Image Classification with Information Pursuit",
    "volume": "main",
    "abstract": "Information Pursuit (IP) is a recently introduced learning framework to construct classifiers that are interpretable-by-design. Given a set of task-relevant and interpretable data queries, IP selects a small subset of the most informative queries and makes predictions based on the gathered query-answer pairs. However, a key limitation of IP is its dependency on task-relevant interpretable queries, which typically require considerable data annotation and curation efforts. While previous approaches have explored using general-purpose large language models to generate these query sets, they rely on prompt engineering heuristics and often yield suboptimal query sets, resulting in a performance gap between IP and non-interpretable black-box predictors. In this work, we propose parameterizing IP queries as a learnable dictionary defined in the latent space of vision-language models such as CLIP. We formulate an optimization objective to learn IP queries and propose an alternating optimization algorithm that shares appealing connections with classic sparse dictionary learning algorithms. Our learned dictionary outperforms baseline methods based on handcrafted or prompted dictionaries across several image classification benchmarks",
    "checked": true,
    "id": "07ca8cb526da40601c49d03e9dcf9f505ef458c4",
    "semantic_title": "learning interpretable queries for explainable image classification with information pursuit",
    "citation_count": 3,
    "authors": [
      "Stefan Kolek",
      "Aditya Chattopadhyay",
      "Kwan Ho Ryan Chan",
      "Hector Andrade-Loarca",
      "Gitta Kutyniok",
      "René Vidal"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Bian_LoRA-FAIR_Federated_LoRA_Fine-Tuning_with_Aggregation_and_Initialization_Refinement_ICCV_2025_paper.html": {
    "title": "LoRA-FAIR: Federated LoRA Fine-Tuning with Aggregation and Initialization Refinement",
    "volume": "main",
    "abstract": "Foundation models (FMs) achieve strong performance across diverse tasks with task-specific fine-tuning, yet full parameter fine-tuning is often computationally prohibitive for large models. Parameter-efficient fine-tuning (PEFT) methods like Low-Rank Adaptation (LoRA) reduce this cost by introducing low-rank matrices for tuning fewer parameters. While LoRA allows for efficient fine-tuning, it requires significant data for adaptation, making Federated Learning (FL) an appealing solution due to its privacy-preserving collaborative framework. However, combining LoRA with FL introduces two key challenges: the Server-Side Aggregation Bias, where server-side averaging of LoRA matrices diverges from the ideal global update, and the Client-Side Initialization Lag, emphasizing the need for consistent initialization across rounds. Existing approaches address these challenges individually, limiting their effectiveness. We propose LoRA-FAIR, a novel method that tackles both issues by introducing a correction term on the server, enhancing aggregation efficiency and accuracy. LoRA-FAIR maintains computational and communication efficiency, yielding superior performance over state-of-the-art methods. Experimental results on ViT and MLP-Mixer models across large-scale datasets demonstrate that LoRA-FAIR consistently achieves performance improvements in FL settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jieming Bian",
      "Lei Wang",
      "Letian Zhang",
      "Jie Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_DiffDoctor_Diagnosing_Image_Diffusion_Models_Before_Treating_ICCV_2025_paper.html": {
    "title": "DiffDoctor: Diagnosing Image Diffusion Models Before Treating",
    "volume": "main",
    "abstract": "In spite of recent progress, image diffusion models still produce artifacts. A common solution is to leverage the feedback provided by quality assessment systems or human annotators to optimize the model, where images are generally rated in their entirety. In this work, we believe problem-solving starts with identification, yielding the request that the model should be aware of not just the presence of defects in an image, but their specific locations. Motivated by this, we propose DiffDoctor, a two-stage pipeline to assist image diffusion models in generating fewer artifacts. Concretely, the first stage targets developing a robust artifact detector, for which we collect a dataset of over 1M flawed synthesized images and set up an efficient human-in-the-loop annotation process, incorporating a carefully designed class-balance strategy. The learned artifact detector is then involved in the second stage to optimize the diffusion model by providing pixel-level feedback. Extensive experiments on text-to-image diffusion models demonstrate the effectiveness of our artifact detector as well as the soundness of our diagnose-then-treat design",
    "checked": true,
    "id": "5b3e3806ff87245ee015326ab18b6a37d60a5b41",
    "semantic_title": "diffdoctor: diagnosing image diffusion models before treating",
    "citation_count": 0,
    "authors": [
      "Yiyang Wang",
      "Xi Chen",
      "Xiaogang Xu",
      "Sihui Ji",
      "Yu Liu",
      "Yujun Shen",
      "Hengshuang Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_AIRA_Activation-Informed_Low-Rank_Adaptation_for_Large_Models_ICCV_2025_paper.html": {
    "title": "AIRA: Activation-Informed Low-Rank Adaptation for Large Models",
    "volume": "main",
    "abstract": "Low-Rank Adaptation (LoRA) is a widely used method for efficiently fine-tuning large models by introducing low-rank matrices into weight updates. However, existing LoRA techniques fail to account for activation information, such as outliers, which significantly impact model performance. This omission leads to suboptimal adaptation and slower convergence. To address this limitation, we present Activation-Informed Low-Rank Adaptation (AIRA), a novel approach that integrates activation information into initialization, training, and rank assignment to enhance model performance. Specifically, AIRA introduces: (1) Outlier-weighted SVD decomposition to reduce approximation errors in low-rank weight initialization, (2) Outlier-driven dynamic rank assignment using offline optimization for better layer-wise adaptation, and (3) Activation-informed training to amplify updates on significant weights. This cascaded activation-informed paradigm enables faster convergence and fewer fine-tuned parameters while maintaining high performance. Extensive experiments on multiple large models demonstrate that AIRA outperforms state-of-the-art LoRA variants, achieving superior performance-efficiency trade-offs in vision-language instruction tuning, few-shot learning, and image generation. Codes are available at https://github.com/lliai/LoRA-Zoo",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lujun Li",
      "Dezhi Li",
      "Cheng Lin",
      "Wei Li",
      "Wei Xue",
      "Sirui Han",
      "Yike Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Park_Understanding_Personal_Concept_in_Open-Vocabulary_Semantic_Segmentation_ICCV_2025_paper.html": {
    "title": "Understanding Personal Concept in Open-Vocabulary Semantic Segmentation",
    "volume": "main",
    "abstract": "While open-vocabulary semantic segmentation (OVSS) can segment an image into semantic regions based on arbitrarily given text descriptions even for classes unseen during training, it fails to understand personal texts (e.g. 'my mug cup') for segmenting regions of specific interest to users. This paper addresses challenges like recognizing 'my mug cup' among 'multiple mug cups'. To overcome this challenge, we introduce a novel task termed personalized open-vocabulary semantic segmentation and propose a text prompt tuning-based plug-in method designed to recognize personal visual concepts using a few pairs of images and masks, while maintaining the performance of the original OVSS. Based on the observation that reducing false predictions is essential when applying text prompt tuning to this task, our proposed method employs 'negative mask proposal' that captures visual concepts other than the personalized concept. We further improve the performance by enriching the representation of text prompts by injecting visual embeddings of the personal concept into them. This approach enhances personalized OVSS without compromising the original OVSS performance. We demonstrate the superiority of our method on our newly established benchmarks for this task, including FSS^per, CUB^per, and ADE^per",
    "checked": false,
    "id": "32f8c4d6ed13107008858fb138957195e1a40d74",
    "semantic_title": "personalized ovss: understanding personal concept in open-vocabulary semantic segmentation",
    "citation_count": 0,
    "authors": [
      "Sunghyun Park",
      "Jungsoo Lee",
      "Shubhankar  Borse",
      "Munawar Hayat",
      "Sungha Choi",
      "Kyuwoong Hwang",
      "Fatih Porikli"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hou_4D_Visual_Pre-training_for_Robot_Learning_ICCV_2025_paper.html": {
    "title": "4D Visual Pre-training for Robot Learning",
    "volume": "main",
    "abstract": "General visual representations learned from web-scale datasets for robotics have achieved great success in recent years, enabling data-efficient robot learning on manipulation tasks; yet these pre-trained representations are mostly on 2D images, neglecting the inherent 3D nature of the world. However, due to the scarcity of large-scale 3D data, it is still hard to extract a universal 3D representation from web datasets. Instead, we are seeking a general visual pre-training framework that could improve all 3D representations as an alternative. Our framework, called FVP, is a novel 4D Visual Pre-training framework for real-world robot learning. FVP frames the visual pre-training objective as a next-point-cloud-prediction problem, models the prediction model as a diffusion model, and pre-trains the model on the larger public datasets directly. Across twelve real-world manipulation tasks, FVP boosts the average success rate of 3D Diffusion Policy (DP3) for these tasks by 28%. The FVP pre-trained DP3 achieves state-of-the-art performance across imitation learning methods. Moreover, the efficacy of \\ours adapts across various point cloud encoders and datasets. Finally, we apply FVP to the RDT-1B, a larger Vision-Language-Action robotic model, enhancing its performance on various robot tasks. Our project page is available at: https://4d-visual-pretraining.github.io/",
    "checked": true,
    "id": "d193022d24f4ff723f00a14e970bb3769453fcf8",
    "semantic_title": "4d visual pre-training for robot learning",
    "citation_count": 1,
    "authors": [
      "Chengkai Hou",
      "Yanjie Ze",
      "Yankai Fu",
      "Zeyu Gao",
      "Songbo Hu",
      "Yue Yu",
      "Shanghang Zhang",
      "Huazhe Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_SemTalk_Holistic_Co-speech_Motion_Generation_with_Frame-level_Semantic_Emphasis_ICCV_2025_paper.html": {
    "title": "SemTalk: Holistic Co-speech Motion Generation with Frame-level Semantic Emphasis",
    "volume": "main",
    "abstract": "A good co-speech motion generation cannot be achieved without a careful integration of common rhythmic motion and rare yet essential semantic motion. In this work, we propose SemTalk for holistic co-speech motion generation with frame-level semantic emphasis. Our key insight is to separately learn base motions and sparse motions, and then adaptively fuse them. In particular, coarse2fine cross-attention module and rhythmic consistency learning are explored to establish rhythm-related base motion, ensuring a coherent foundation that synchronizes gestures with the speech rhythm. Subsequently, semantic emphasis learning is designed to generate semantic-aware sparse motion, focusing on frame-level semantic cues. Finally, to integrate sparse motion into the base motion and generate semantic-emphasized co-speech gestures, we further leverage a learned semantic score for adaptive synthesis. Qualitative and quantitative comparisons on two public datasets demonstrate that our method outperforms the state-of-the-art, delivering high-quality co-speech motion with enhanced semantic richness over a stable base motion",
    "checked": true,
    "id": "5e091e031bda1f4d016b00207feadacf02bec39d",
    "semantic_title": "semtalk: holistic co-speech motion generation with frame-level semantic emphasis",
    "citation_count": 1,
    "authors": [
      "Xiangyue Zhang",
      "Jianfang Li",
      "Jiaxu Zhang",
      "Ziqiang Dang",
      "Jianqiang Ren",
      "Liefeng Bo",
      "Zhigang Tu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_Textured_3D_Regenerative_Morphing_with_3D_Diffusion_Prior_ICCV_2025_paper.html": {
    "title": "Textured 3D Regenerative Morphing with 3D Diffusion Prior",
    "volume": "main",
    "abstract": "Textured 3D morphing creates smooth and plausible interpolation sequences between two 3D objects, focusing on transitions in both shape and texture. This is important for creative applications like visual effects in filmmaking. Previous methods rely on establishing point-to-point correspondences and determining smooth deformation trajectories, which inherently restrict them to shape-only morphing on untextured, topologically aligned datasets. This restriction leads to labor-intensive preprocessing and poor generalization. To overcome these challenges, we propose a method for 3D regenerative morphing using a 3D diffusion prior. Unlike previous methods that depend on explicit correspondences and deformations, our method eliminates the additional need for obtaining correspondence and uses the 3D diffusion prior to generate morphing. Specifically, we first introduce a 3D diffusion model and interpolate the source and target information at three levels: initial noise, model parameters, and condition features. We then explore an Attention Fusion strategy to generate smoother morphing sequences. To further improve the plausibility of semantic interpolation and the generated 3D surfaces, we propose two strategies: (a) Token Reordering, where we match approximate tokens based on semantic analysis to guide implicit correspondences in the denoising process of the diffusion model, and (b) Low-Frequency Enhancement, where we enhance low-frequency signals in the tokens to improve the quality of generated surfaces. Experimental results show that our method achieves superior smoothness and plausibility in 3D morphing across diverse cross-category object pairs, offering a novel regenerative method for 3D morphing with textured representations",
    "checked": true,
    "id": "8b5be2400aaa434181b450363398b3bdbb1177d7",
    "semantic_title": "textured 3d regenerative morphing with 3d diffusion prior",
    "citation_count": 2,
    "authors": [
      "Songlin Yang",
      "Yushi Lan",
      "Honghua Chen",
      "Xingang Pan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_LONG3R_Long_Sequence_Streaming_3D_Reconstruction_ICCV_2025_paper.html": {
    "title": "LONG3R: Long Sequence Streaming 3D Reconstruction",
    "volume": "main",
    "abstract": "Recent advancements in multi-view scene reconstruction have been significant, yet existing methods face limitations when processing streams of input images. These methods either rely on time-consuming offline optimization or are restricted to shorter sequences, hindering their applicability in real-time scenarios. In this work, we propose LONG3R (LOng sequence streamiNG 3D Reconstruction), a novel model designed for streaming multi-view 3D scene reconstruction over longer sequences. Our model achieves real-time processing by operating recurrently, maintaining and updating memory with each new observation. We first employ a memory gating mechanism to filter relevant memory, which, together with a new observation, is fed into a dual-source refined decoder for coarse-to-fine interaction. To effectively capture long-sequence memory, we propose a 3D spatio-temporal memory that dynamically prunes redundant spatial information while adaptively adjusting resolution along the scene. To enhance our model's performance on long sequences while maintaining training efficiency, we employ a two-stage curriculum training strategy, each stage targeting specific capabilities. Experiments demonstrate that LONG3R outperforms state-of-the-art streaming methods, particularly for longer sequences, while maintaining real-time inference speed. Project page: https://zgchen33.github.io/LONG3R/",
    "checked": true,
    "id": "07e4d5b813b9bf53efa308d07a57e534cef9ceeb",
    "semantic_title": "long3r: long sequence streaming 3d reconstruction",
    "citation_count": 6,
    "authors": [
      "Zhuoguang Chen",
      "Minghui Qin",
      "Tianyuan Yuan",
      "Zhe Liu",
      "Hang Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Luo_DreamActor-M1_Holistic_Expressive_and_Robust_Human_Image_Animation_with_Hybrid_ICCV_2025_paper.html": {
    "title": "DreamActor-M1: Holistic, Expressive and Robust Human Image Animation with Hybrid Guidance",
    "volume": "main",
    "abstract": "While recent image-based human animation methods achieve realistic body and facial motion synthesis, critical gaps remain in fine-grained holistic controllability, multi-scale adaptability, and long-term temporal coherence, which leads to their lower expressiveness and robustness. We propose a diffusion transformer (DiT) based framework, HERA, with hybrid guidance to overcome these limitations. For motion guidance, our hybrid control signals that integrate implicit facial representations, 3D head spheres, and 3D body skeletons achieve robust control of facial expressions and body movements, while producing expressive and identity-preserving animations.For scale adaptation, to handle various body poses and image scales ranging from portraits to full-body views, we employ a progressive training strategy using data with varying resolutions and scales.For appearance guidance, we integrate motion patterns from sequential frames with complementary visual references, ensuring long-term temporal coherence for unseen regions during complex movements.Experiments demonstrate that our method outperforms the state-of-the-art works, delivering expressive results for portraits, upper-body, and full-body generation with robust long-term consistency",
    "checked": true,
    "id": "afc7d348624a75c1c2e34a37279d42ffdc321b30",
    "semantic_title": "dreamactor-m1: holistic, expressive and robust human image animation with hybrid guidance",
    "citation_count": 14,
    "authors": [
      "Yuxuan Luo",
      "Zhengkun Rong",
      "Lizhen Wang",
      "Longhao Zhang",
      "Tianshu Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Blind2Sound_Self-Supervised_Image_Denoising_without_Residual_Noise_ICCV_2025_paper.html": {
    "title": "Blind2Sound: Self-Supervised Image Denoising without Residual Noise",
    "volume": "main",
    "abstract": "Self-supervised blind denoising for Poisson-Gaussian noise remains a challenging task. Pseudo-supervised pairs constructed from single noisy images re-corrupt the signal and degrade the performance. The visible blindspots solve the information loss in masked inputs. However, without explicitly noise sensing, mean square error as an objective function cannot adjust denoising intensities for dynamic noise levels, leading to noticeable residual noise. In this paper, we propose Blind2Sound, a simple yet effective approach to overcome residual noise in denoised images. The proposed adaptive re-visible loss senses noise levels and performs personalized denoising without noise residues while retaining the signal lossless. The theoretical analysis of intermediate medium gradients guarantees stable training, while the Cramer Gaussian loss acts as a regularization to facilitate the accurate perception of noise levels and improve the performance of the denoiser. Experiments on synthetic and real-world datasets show the superior performance of our method, especially for single-channel images. The code is publicly available from this link",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiazheng Liu",
      "Zejin Wang",
      "Bohao Chen",
      "Hua Han"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Mrabah_Sparsity_Outperforms_Low-Rank_Projections_in_Few-Shot_Adaptation_ICCV_2025_paper.html": {
    "title": "Sparsity Outperforms Low-Rank Projections in Few-Shot Adaptation",
    "volume": "main",
    "abstract": "Adapting Vision-Language Models (VLMs) to new domains with few labeled samples remains a significant challenge due to severe overfitting and computational constraints. State-of-the-art solutions, such as low-rank reparameterization, mitigate these issues but often struggle with generalization and require extensive hyperparameter tuning. In this paper, a novel Sparse Optimization (SO) framework is proposed. Unlike low-rank approaches that typically constrain updates to a fixed subspace, our SO method leverages high sparsity to dynamically adjust very few parameters. We introduce two key paradigms. First, we advocate for local sparsity and global density, which updates a minimal subset of parameters per iteration while maintaining overall model expressiveness. As a second paradigm, we advocate for local randomness and global importance, which sparsifies the gradient using random selection while pruning the first moment based on importance. This combination significantly mitigates overfitting and ensures stable adaptation in low-data regimes. Extensive experiments on 11 diverse datasets show that SO achieves state-of-the-art few-shot adaptation performance while reducing memory overhead",
    "checked": true,
    "id": "3d6251f47deafb680d0e48424ac3df4faad8ff5a",
    "semantic_title": "sparsity outperforms low-rank projections in few-shot adaptation",
    "citation_count": 0,
    "authors": [
      "Nairouz Mrabah",
      "Nicolas Richet",
      "Ismail Ben Ayed",
      "Eric Granger"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Bie_Hyper-Depth_Hypergraph-based_Multi-Scale_Representation_Fusion_for_Monocular_Depth_Estimation_ICCV_2025_paper.html": {
    "title": "Hyper-Depth: Hypergraph-based Multi-Scale Representation Fusion for Monocular Depth Estimation",
    "volume": "main",
    "abstract": "Monocular depth estimation (MDE) is a fundamental problem in computer vision with wide-ranging applications in various downstream tasks. While multi-scale features are perceptually critical for MDE, existing transformer-based methods have yet to leverage them explicitly. To address this limitation, we propose a hypergraph-based multi-scale representation fusion framework, Hyper-Depth.The proposed Hyper-Depth incorporates two key components: a Semantic Consistency Enhancement (SCE) module and a Geometric Consistency Constraint (GCC) module. The SCE module, designed based on hypergraph convolution, aggregates global information and enhances the representation of multi-scale patch features. Meanwhile, the GCC module provides geometric guidance to reduce over-fitting errors caused by excessive reliance on local features. In addition, we introduce a Correlation-based Conditional Random Fields (C-CRFs) module as the decoder to filter correlated patches and compute attention weights more effectively.Extensive experiments demonstrate that our method significantly outperforms state-of-the-art approaches across all evaluation metrics on the KITTI and NYU-Depth-v2 datasets, achieving improvements of 6.21% and 3.32% on the main metric RMSE, respectively. Furthermore, zero-shot evaluations on the nuScenes and SUN-RGBD datasets validate the generalizability of our approach",
    "checked": false,
    "id": "857b2f0cc754fc68de051563ba62a43d1b4fbacb",
    "semantic_title": "multi-space representation fusion enhanced monocular depth estimation via virtual point cloud",
    "citation_count": 0,
    "authors": [
      "Lin Bie",
      "Siqi Li",
      "Yifan Feng",
      "Yue Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xie_PVMamba_Parallelizing_Vision_Mamba_via_Dynamic_State_Aggregation_ICCV_2025_paper.html": {
    "title": "PVMamba: Parallelizing Vision Mamba via Dynamic State Aggregation",
    "volume": "main",
    "abstract": "Mamba, an architecture with RNN-like sequence modeling of State Space Model (SSM), has demonstrated promising capabilities in long-range modeling with high efficiency. However, Mamba models struggle with structured 2D visual data using sequential computing, thereby lagging behind their attention-based counterparts. In this paper, we propose a Parallel Vision Mamba (PVMamba), a novel SSM architecture tailored for visual data. PVMamba encompasses two key designs: 1) Based on the sparsity and adjacency of visual signals, we parallelize the sequential computing through three core steps, termed Dynamic State Aggregation (DSA), i.e., parallelization, alignment, and aggregation. DSA generates the hidden state in SSM by a feasible spatial aggregation, thereby overcoming the inherent sequential constraints. 2) Along with maintaining linear computational complexity, we apply a dynamic operator to learn the spatial samplings for each hidden state. To further boost the local modeling capability, we restrict the dynamic operator to the neighboring pixels in shallow layers. We also devise a layer multiplexing technique to stabilize the training and reduce the learning redundancy. PVMamba is a versatile backbone network with dynamic operators for various vision tasks, such as image classification and dense prediction. Extensive experiments show that PVMamba achieves state-of-the-art performance on a range of benchmarks",
    "checked": false,
    "id": "5dc287d6daf14022a983835e834bd18408cb1306",
    "semantic_title": "remote sensing image dehazing using content-driven state space modeling with scale-aware aggregation",
    "citation_count": 0,
    "authors": [
      "Fei Xie",
      "Zhongdao Wang",
      "Weijia Zhang",
      "Chao Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_Fuzzy_Contrastive_Decoding_to_Alleviate_Object_Hallucination_in_Large_Vision-Language_ICCV_2025_paper.html": {
    "title": "Fuzzy Contrastive Decoding to Alleviate Object Hallucination in Large Vision-Language Models",
    "volume": "main",
    "abstract": "Large vision-language models (LVLMs) often exhibit object hallucination, a phenomenon where models generate descriptions of non-existent objects within images. Prior methods have sought to mitigate this issue by adjusting model logits to reduce linguistic bias, but they often lack precise control over visual uncertainty, sometimes exacerbating hallucinations instead of mitigating them. To address this limitation, we propose a novel decoding strategy called fuzzy contrastive decoding (FuzzyCD) that uses Takagi-Sugeno fuzzy inference to refine hallucination control. FuzzyCD adaptively assigns weights to high-hallucination logits while mitigating unnecessary linguistic bias. Specifically, it transforms the log-probabilities of top-1 tokens from both standard and hallucination logits into a confidence linguistic fuzzy set. Through Takagi-Sugeno fuzzy inference, it dynamically adjusts hallucination logits to prevent the model from over-relying on spurious linguistic patterns. Experimental results on object hallucination datasets demonstrate that hallucination is mitigated by 11%p compared to conventional LVLMs. In-depth analyses highlight the effectiveness of FuzzyCD in enhancing the reliability of vision-language models",
    "checked": false,
    "id": "17af020890152d332497fd17691ccc55ea1bf886",
    "semantic_title": "retrieval visual contrastive decoding to mitigate object hallucinations in large vision-language models",
    "citation_count": 0,
    "authors": [
      "Jieun Kim",
      "Jinmyeong Kim",
      "Yoonji Kim",
      "Sung-Bae Cho"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sumbul_SMARTIES_Spectrum-Aware_Multi-Sensor_Auto-Encoder_for_Remote_Sensing_Images_ICCV_2025_paper.html": {
    "title": "SMARTIES: Spectrum-Aware Multi-Sensor Auto-Encoder for Remote Sensing Images",
    "volume": "main",
    "abstract": "From optical sensors to microwave radars, leveraging the complementary strengths of remote sensing (RS) sensors is crucial for achieving dense spatio-temporal monitoring of our planet. In contrast, recent deep learning models, whether task-specific or foundational, are often specific to single sensors or to fixed combinations: adapting such models to different sensory inputs requires both architectural changes and re-training, limiting scalability and generalization across multiple RS sensors. On the contrary, a single model able to modulate its feature representations to accept diverse sensors as input would pave the way to agile and flexible multi-sensor RS data processing. To address this, we introduce SMARTIES, a generic and versatile foundation model lifting sensor-specific/dependent efforts and enabling scalability and generalization to diverse RS sensors: SMARTIES projects data from heterogeneous sensors into a shared spectrum-aware space, enabling the use of arbitrary combinations of bands both for training and inference. To obtain sensor-agnostic representations, we train a single, unified transformer model reconstructing masked multi-sensor data with cross-sensor token mixup. On both single- and multi-modal tasks across diverse sensors, SMARTIES outperforms previous models that rely on sensor-specific pretraining. Our code and pretrained models are available at https://gsumbul.github.io/SMARTIES",
    "checked": true,
    "id": "a00d31dfc92fde0fdb8431e2d78824d882f90155",
    "semantic_title": "smarties: spectrum-aware multi-sensor auto-encoder for remote sensing images",
    "citation_count": 2,
    "authors": [
      "Gencer Sumbul",
      "Chang Xu",
      "Emanuele Dalsasso",
      "Devis Tuia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/He_SynFER_Towards_Boosting_Facial_Expression_Recognition_with_Synthetic_Data_ICCV_2025_paper.html": {
    "title": "SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data",
    "volume": "main",
    "abstract": "Facial expression datasets remain limited in scale due to privacy concerns, the subjectivity of annotations, and the labor-intensive nature of data collection. This limitation poses a significant challenge for developing modern deep learning-based facial expression analysis models, particularly foundation models, that rely on large-scale data for optimal performance. To tackle the overarching and complex challenge, we introduce SynFER (Synthesis of Facial Expressions with Refined Control), a novel framework for synthesizing facial expression image data based on high-level textual descriptions as well as more fine-grained and precise control through facial action units. To ensure the quality and reliability of the synthetic data, we propose a semantic guidance technique to steer the generation process and a pseudo-label generator to help rectify the facial expression labels for the synthetic images. To demonstrate the generation fidelity and the effectiveness of the synthetic data from SynFER, we conduct extensive experiments on representation learning using both synthetic data and real-world data. Results validate the efficacy of our approach and the synthetic data. Notably, our approach achieves a 67.23% classification accuracy on AffectNet when training solely with synthetic data equivalent to the AffectNet training set size, which increases to 69.84% when scaling up to five times the original size",
    "checked": true,
    "id": "c8ea2049f8cbb586879939f240072c0c4790b856",
    "semantic_title": "synfer: towards boosting facial expression recognition with synthetic data",
    "citation_count": 6,
    "authors": [
      "Xilin He",
      "Cheng Luo",
      "Xiaole Xian",
      "Bing Li",
      "Muhammad Haris Khan",
      "Zongyuan Ge",
      "Weicheng Xie",
      "Siyang Song",
      "Linlin Shen",
      "Bernard Ghanem",
      "Xiangyu Yue"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wei_Boosting_Multimodal_Learning_via_Disentangled_Gradient_Learning_ICCV_2025_paper.html": {
    "title": "Boosting Multimodal Learning via Disentangled Gradient Learning",
    "volume": "main",
    "abstract": "Multimodal learning often encounters the under-optimized problem and may have worse performance than unimodal learning. Existing methods attribute this problem to the imbalanced learning between modalities and rebalance them through gradient modulation. However, they fail to explain why the dominant modality in multimodal models also underperforms that in unimodal learning. In this work, we reveal the optimization conflict between the modality encoder and modality fusion module in multimodal models. Specifically, we prove that the cross-modal fusion in multimodal models decreases the gradient passed back to each modality encoder compared with unimodal models. Consequently, the performance of each modality in the multimodal model is inferior to that in the unimodal model. To this end, we propose a disentangled gradient learning (DGL) framework to decouple the optimization of the modality encoder and modality fusion module in the multimodal model. DGL truncates the gradient back-propagated from the multimodal loss to the modality encoder and replaces it with the gradient from unimodal loss. Besides, DGL removes the gradient back-propagated from the unimodal loss to the modality fusion module. This helps eliminate the gradient interference between the modality encoder and modality fusion module while ensuring their respective optimization processes. Finally, extensive experiments on multiple types of modalities, tasks, and frameworks with dense cross-modal interaction demonstrate the effectiveness and versatility of the proposed DGL",
    "checked": true,
    "id": "788ba48295d061fe461def3edb02ffcc04f80509",
    "semantic_title": "boosting multimodal learning via disentangled gradient learning",
    "citation_count": 1,
    "authors": [
      "Shicai Wei",
      "Chunbo Luo",
      "Yang Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Pun_Generating_Physically_Stable_and_Buildable_Brick_Structures_from_Text_ICCV_2025_paper.html": {
    "title": "Generating Physically Stable and Buildable Brick Structures from Text",
    "volume": "main",
    "abstract": "We introduce BrickGPT, the first approach for generating physically stable interconnecting brick assembly models from text prompts. To achieve this, we construct a large-scale, physically stable dataset of brick structures, along with their associated captions, and train an autoregressive large language model to predict the next brick to add via next-token prediction. To improve the stability of the resulting designs, we employ an efficient validity check and physics-aware rollback during autoregressive inference, which prunes infeasible token predictions using physics laws and assembly constraints. Our experiments show that BrickGPT produces stable, diverse, and aesthetically pleasing brick structures that align closely with the input text prompts. We also develop a text-based brick texturing method to generate colored and textured designs. We show that our designs can be assembled manually by humans and automatically by robotic arms. We release our new dataset, StableText2Brick, containing over 47,000 brick structures of over 28,000 unique 3D objects accompanied by detailed captions, along with our code and models at the project website: https://avalovelace1.github.io/BrickGPT/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ava Pun",
      "Kangle Deng",
      "Ruixuan Liu",
      "Deva Ramanan",
      "Changliu Liu",
      "Jun-Yan Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xia_Bootstrapping_Grounded_Chain-of-Thought_in_Multimodal_LLMs_for_Data-Efficient_Model_Adaptation_ICCV_2025_paper.html": {
    "title": "Bootstrapping Grounded Chain-of-Thought in Multimodal LLMs for Data-Efficient Model Adaptation",
    "volume": "main",
    "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in interpreting images using natural language. However, without using large-scale datasets for retraining, these models are difficult to adapt to specialized vision tasks, e.g., chart understanding. This problem is caused by a mismatch between pre-training and downstream datasets: pre-training datasets primarily concentrate on scenes and objects but contain limited information about specialized, non-object images, such as charts and tables. In this paper, we share an interesting finding that training an MLLM with chain-of-thought (CoT) reasoning data can facilitate model adaptation in specialized vision tasks, especially under data-limited regimes. However, we identify a critical issue within CoT data distilled from pre-trained MLLMs, i.e., the data often contains multiple factual errors in the reasoning steps. To address the problem, we propose Grounded Chain-of-Thought (GCoT), a simple bootstrapping-based approach that aims to inject grounding information (i.e., bounding boxes) into CoT data, essentially making the reasoning steps more faithful to input images. We evaluate our approach on five specialized vision tasks, which cover a variety of visual formats including charts, tables, receipts, and reports. The results demonstrate that under data-limited regimes our approach significantly improves upon fine-tuning and distillation",
    "checked": true,
    "id": "b709a4359b6809c5cd93f58ac8d85879faeb8df8",
    "semantic_title": "bootstrapping grounded chain-of-thought in multimodal llms for data-efficient model adaptation",
    "citation_count": 1,
    "authors": [
      "Jiaer Xia",
      "Bingkui Tong",
      "Yuhang Zang",
      "Rui Shao",
      "Kaiyang Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zeng_DisTime_Distribution-based_Time_Representation_for_Video_Large_Language_Models_ICCV_2025_paper.html": {
    "title": "DisTime: Distribution-based Time Representation for Video Large Language Models",
    "volume": "main",
    "abstract": "Despite advances in general video understanding, Video Large Language Models (Video-LLMs) face challenges in precise temporal localization due to discrete time representations and limited temporally aware datasets. Existing methods for temporal expression either conflate time with text-based numerical values, add a series of dedicated temporal tokens, or regress time using specialized temporal grounding heads. To address these issues, we introduce DisTime, a lightweight framework designed to enhance temporal comprehension in Video-LLMs. DisTime employs a learnable token to create a continuous temporal embedding space and incorporates a Distribution-based Time Decoder that generates temporal probability distributions, effectively mitigating boundary ambiguities and maintaining temporal continuity. Additionally, the Distribution-based Time Encoder re-encodes timestamps to provide time markers for Video-LLMs. To overcome temporal granularity limitations in existing datasets, we propose an automated annotation paradigm that combines the captioning capabilities of Video-LLMs with the localization expertise of dedicated temporal models. This leads to the creation of InternVid-TG, a substantial dataset with 1.25M temporally grounded events across 179k videos, surpassing ActivityNet-Caption by 55 times. Extensive experiments demonstrate that DisTime achieves state-of-the-art performance across benchmarks in three time-sensitive tasks while maintaining competitive performance in Video QA tasks. DisTime is released at https://github.com/josephzpng/DisTime",
    "checked": true,
    "id": "e2effbf56084cd6596eb09a16a48f6424a932d74",
    "semantic_title": "distime: distribution-based time representation for video large language models",
    "citation_count": 1,
    "authors": [
      "Yingsen Zeng",
      "Zepeng Huang",
      "Yujie Zhong",
      "Chengjian Feng",
      "Jie Hu",
      "Lin Ma",
      "Yang Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Engage_for_All_Making_Ordinary_Image_Descriptions_Appealing_Again_ICCV_2025_paper.html": {
    "title": "Engage for All: Making Ordinary Image Descriptions Appealing Again!",
    "volume": "main",
    "abstract": "In recent years, multi-modal large language models (MLLMs) have been successfully adopted to generate humorous and engaging descriptions for internet memes. While, it is challenging for the same approaches to apply to ordinary images which lack of inherent funny or exaggerated contents. Thus, crafting appealing descriptions for ordinary image demands imaginative efforts to discover or create intriguing connections between words to image contents. To address this gap, we introduce AppealImage, a large-scale dataset consisting of ordinary images paired with appealing descriptions. AppealImage allows us to define four distinct tasks with quantitative metrics to enable objective evaluation. Subsequently, we propose CharmNet, an innovative framework designed to generate appealing descriptions for ordinary images. CharmNet combines instruction tuning with heuristic active learning, guided by a referee model. Experimental results demonstrate that CharmNet outperforms the state-of-the-art method by 11.4% in generating appealing descriptions. Furthermore, CharmNet delivers impressive performance across various creative applications, including visual storytelling and situational dialogue generation. These results highlight CharmNet's potential to enhance social media engagement and to empower strong brand presence in competitive markets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuyan  Chen",
      "Yifan Jiang",
      "Li Zhou",
      "Jinghan Cao",
      "Yu Guan",
      "Ming Yang",
      "Qingpei Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_QuantCache_Adaptive_Importance-Guided_Quantization_with_Hierarchical_Latent_and_Layer_Caching_ICCV_2025_paper.html": {
    "title": "QuantCache: Adaptive Importance-Guided Quantization with Hierarchical Latent and Layer Caching for Video Generation",
    "volume": "main",
    "abstract": "Recently, Diffusion Transformers (DiTs) have emerged as a dominant architecture in video generation, surpassing U-Net-based models in terms of performance. However, the enhanced capabilities of DiTs come with significant drawbacks, including increased computational and memory costs, which hinder their deployment on resource-constrained devices. Current acceleration techniques, such as quantization and cache mechanism, offer limited speedup and are often applied in isolation, failing to fully address the complexities of DiT architectures. In this paper, we propose QuantCache, a novel training-free inference acceleration framework that jointly optimizes hierarchical latent caching, adaptive importance-guided quantization, and structural redundancy-aware pruning. QuantCache achieves an end-to-end latency speedup of 6.72x on Open-Sora with minimal loss in generation quality. Extensive evaluations across multiple video generation benchmarks demonstrate the effectiveness of our method, setting a new standard for efficient DiT inference. We will release all code and models to facilitate further research",
    "checked": true,
    "id": "c94e5f4081240ec825168aae552b76ab02a2f515",
    "semantic_title": "quantcache: adaptive importance-guided quantization with hierarchical latent and layer caching for video generation",
    "citation_count": 6,
    "authors": [
      "Junyi Wu",
      "Zhiteng Li",
      "Zheng Hui",
      "Yulun Zhang",
      "Linghe Kong",
      "Xiaokang Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Di_DH-FaceVid-1K_A_Large-Scale_High-Quality_Dataset_for_Face_Video_Generation_ICCV_2025_paper.html": {
    "title": "DH-FaceVid-1K: A Large-Scale High-Quality Dataset for Face Video Generation",
    "volume": "main",
    "abstract": "Human-centric generative models are becoming increasingly popular, giving rise to various innovative tools and applications, such as talking face videos conditioned on text or audio prompts. The core of these capabilities lies in powerful pre-trained foundation models, trained on large-scale, high-quality datasets. However, many advanced methods rely on in-house data subject to various constraints, and other current studies fail to generate high-resolution face videos, which is mainly attributed to the significant lack of large-scale, high-quality face video datasets. In this paper, we introduce a human face video dataset, DH-FaceVid-1K. Our collection spans 1,200 hours in total, encompassing 270,043 video clips from over 20,000 individuals. Each sample includes corresponding speech audio, facial keypoints, and text annotations. Compared to other publicly available datasets, ours distinguishes itself through its multi-ethnic coverage and high-quality, comprehensive individual attributes. We establish multiple face video generation models supporting tasks such as text-to-video and image-to-video generation. In addition, we develop comprehensive benchmarks to validate the scaling law when using different proportions of the proposed dataset. Our primary aim is to contribute a face video dataset, particularly addressing the underrepresentation of Asian faces in existing curated datasets and thereby enriching the global spectrum of face-centric data and mitigating demographic biases. Project Page: https://luna-ai-lab.github.io/DH-FaceVid-1K/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Donglin Di",
      "He Feng",
      "Wenzhang Sun",
      "Yongjia Ma",
      "Hao Li",
      "Wei Chen",
      "Lei Fan",
      "Tonghua Su",
      "Xun Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jin_Instruction-Grounded_Visual_Projectors_for_Continual_Learning_of_Generative_Vision-Language_Models_ICCV_2025_paper.html": {
    "title": "Instruction-Grounded Visual Projectors for Continual Learning of Generative Vision-Language Models",
    "volume": "main",
    "abstract": "Continual learning enables pre-trained generative vision-language models (VLMs) to incorporate knowledge from new tasks without retraining data from previous ones. Recent methods update a visual projector to translate visual information for new tasks, connecting pre-trained vision encoders with large language models. However, such adjustments may cause the models to prioritize visual inputs over language instructions, particularly learning tasks with repetitive types of textual instructions. To address the neglect of language instructions, we propose a novel framework that grounds the translation of visual information on instructions for language models. We introduce a mixture of visual projectors, each serving as a specialized visual-to-language translation expert based on the given instruction context to adapt to new tasks. To avoid using experts for irrelevant instruction contexts, we propose an expert recommendation strategy that reuses experts for tasks similar to those previously learned. Additionally, we introduce expert pruning to alleviate interference from the use of experts that cumulatively activated in previous tasks. Extensive experiments on diverse vision-language tasks demonstrate that our method outperforms existing continual learning approaches by generating instruction-following responses",
    "checked": true,
    "id": "8b940b7685d1dd57b0c815e79c0b616b5a937134",
    "semantic_title": "instruction-grounded visual projectors for continual learning of generative vision-language models",
    "citation_count": 0,
    "authors": [
      "Hyundong Jin",
      "Hyung Jin Chang",
      "Eunwoo Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jo_COIN_Confidence_Score-Guided_Distillation_for_Annotation-Free_Cell_Segmentation_ICCV_2025_paper.html": {
    "title": "COIN: Confidence Score-Guided Distillation for Annotation-Free Cell Segmentation",
    "volume": "main",
    "abstract": "Cell instance segmentation (CIS) is crucial for identifying individual cell morphologies in histopathological images, providing valuable insights for biological and medical research. While unsupervised CIS (UCIS) models aim to reduce the heavy reliance on labor-intensive image annotations, they fail to accurately capture cell boundaries, causing missed detections and poor performance. Recognizing the absence of error-free instances as a key limitation, we present COIN (COnfidence score-guided INstance distillation), a novel annotation-free framework with three key steps: (1) Increasing the sensitivity for the presence of error-free instances via unsupervised semantic segmentation with optimal transport, leveraging its ability to discriminate spatially minor instances, (2) Instance-level confidence scoring to measure the consistency between model prediction and refined mask and identify highly confident instances, offering an alternative to ground truth annotations, and (3) Progressive expansion of confidence with recursive self-distillation. Extensive experiments across six datasets show COIN outperforming existing UCIS methods, even surpassing semi- and weakly-supervised approaches across all metrics on the MoNuSeg and TNBC datasets. Project Page: \\href https://shjo-april.github.io/COIN/ https://shjo-april.github.io/COIN/",
    "checked": true,
    "id": "84895ff46eedc3e92240adc0543cb0a184caa02e",
    "semantic_title": "coin: confidence score-guided distillation for annotation-free cell segmentation",
    "citation_count": 0,
    "authors": [
      "Sanghyun Jo",
      "Seo Jin Lee",
      "Seungwoo Lee",
      "Seohyung Hong",
      "Hyungseok Seo",
      "Kyungsu Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Proactive_Scene_Decomposition_and_Reconstruction_ICCV_2025_paper.html": {
    "title": "Proactive Scene Decomposition and Reconstruction",
    "volume": "main",
    "abstract": "Human behaviors are the major causes of scene dynamics and inherently contain rich cues regarding the dynamics. This paper formalizes a new task of proactive scene decomposition and reconstruction, an online approach that leverages human-object interactions to iteratively disassemble and reconstruct the environment. By observing these intentional interactions, we can dynamically refine the decomposition and reconstruction process, addressing inherent ambiguities in static object-level reconstruction. The proposed system effectively integrates multiple tasks in dynamic environments such as accurate camera and object pose estimation, instance decomposition, and online map updating, capitalizing on cues from human-object interactions in egocentric live streams for a flexible, progressive alternative to conventional object-level reconstruction methods. Aided by the Gaussian splatting technique, accurate and consistent dynamic scene modeling is achieved with photorealistic and efficient rendering. The efficacy is validated in multiple real-world scenarios with promising advantages",
    "checked": true,
    "id": "fa049cb1429cadc73fd3f95f32228a4c82fa9b60",
    "semantic_title": "proactive scene decomposition and reconstruction",
    "citation_count": 0,
    "authors": [
      "Baicheng Li",
      "Zike Yan",
      "Dong Wu",
      "Hongbin Zha"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Estimating_2D_Camera_Motion_with_Hybrid_Motion_Basis_ICCV_2025_paper.html": {
    "title": "Estimating 2D Camera Motion with Hybrid Motion Basis",
    "volume": "main",
    "abstract": "Estimating 2D camera motion is a fundamental computer vision task that models the projection of 3D camera movements onto the 2D image plane. Current methods rely on either homography-based approaches, limited to planar scenes, or meshflow techniques that use grid-based local homographies but struggle with complex non-linear transformations. We introduce CamFlow, a novel framework that represents camera motion using hybrid motion bases: physical bases derived from camera geometry and stochastic bases for complex scenarios. Our approach includes a hybrid probabilistic loss function based on the Laplace distribution that enhances training robustness. For evaluation, we create a new benchmark by masking dynamic objects in existing optical flow datasets to isolate pure camera motion. Experiments show CamFlow outperforms state-of-the-art methods across diverse scenarios, demonstrating superior robustness and generalization in zero-shot settings. Code and datasets are available at our project page: https://lhaippp.github.io/CamFlow/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haipeng Li",
      "Tianhao Zhou",
      "Zhanglei Yang",
      "Yi Wu",
      "Yan Chen",
      "Zijing Mao",
      "Shen Cheng",
      "Bing Zeng",
      "Shuaicheng Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Adaptive_Articulated_Object_Manipulation_On_The_Fly_with_Foundation_Model_ICCV_2025_paper.html": {
    "title": "Adaptive Articulated Object Manipulation On The Fly with Foundation Model Reasoning and Part Grounding",
    "volume": "main",
    "abstract": "Articulated objects pose diverse manipulation challenges for robots. Since their internal structures are not directly observable, robots must adaptively explore and refine actions to generate successful manipulation trajectories. While existing works have attempted cross-category generalization in adaptive articulated object manipulation, two major challenges persist: (1) the geometric diversity of real-world articulated objects complicates visual perception and understanding, and (2) variations in object functions and mechanisms hinder the development of a unified adaptive manipulation strategy.To address these challenges, we propose AdaRPG, a novel framework that leverages foundation models to extract object parts, which exhibit greater local geometric similarity than entire objects, thereby enhancing visual affordance generalization for functional primitive skills. To support this, we construct a part-level affordance annotation dataset to train the affordance model. Additionally, AdaRPG utilizes the common knowledge embedded in foundation models to reason about complex mechanisms and generate high-level control codes that invoke primitive skill functions based on part affordance inference.Simulation and real-world experiments demonstrate AdaRPG's strong generalization ability across novel articulated object categories",
    "checked": true,
    "id": "5157d2a42823e24c53437ea9f09bd6e147746d72",
    "semantic_title": "adaptive articulated object manipulation on the fly with foundation model reasoning and part grounding",
    "citation_count": 1,
    "authors": [
      "Xiaojie Zhang",
      "Yuanfei Wang",
      "Ruihai Wu",
      "Kunqi Xu",
      "Yu Li",
      "Liuyu Xiang",
      "Hao Dong",
      "Zhaofeng He"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_Predict-Optimize-Distill_A_Self-Improving_Cycle_for_4D_Object_Understanding_ICCV_2025_paper.html": {
    "title": "Predict-Optimize-Distill: A Self-Improving Cycle for 4D Object Understanding",
    "volume": "main",
    "abstract": "Whether snipping with scissors or opening a box, humans can quickly understand the 3D configurations of familiar objects. For novel objects, we can resort to long-form inspection to build intuition. The more we observe the object, the better we get at predicting its 3D state immediately. Existing systems, however, are limited to either optimizing underlying representations from multi-view observations or training a feed-forward predictor from supervised datasets. We introduce Predict-Optimize-Distill (POD), a self-improving framework that interleaves prediction and optimization in a mutually reinforcing cycle to achieve better 4D object understanding with increasing observation time. Given a multi-view object scan and a long-form monocular video of human-object interaction, POD iteratively trains a neural network to predict local part poses from RGB frames, uses this predictor to initialize a global optimization which refines output poses through inverse rendering, then finally distills the results of optimization back into the model by generating synthetic self-labeled training data from novel viewpoints. Each iteration improves both the predictive model and the optimized motion trajectory, creating a virtuous cycle that bootstraps its own training data to learn about the pose configurations of an object. We also introduce a quasi-multiview mining strategy for reducing depth ambiguity by leveraging long video. We evaluate POD on 14 real-world and 5 synthetic objects with various joint types, including revolute and prismatic joints as well as multi-body configurations where parts detach or reattach independently. POD demonstrates significant improvement over a pure optimization baseline which gets stuck in local minima, particularly for longer videos. We also find that POD's performance improves with both video length and successive iterations of the self-improving cycle, highlighting its ability to scale performance with additional observations and compute",
    "checked": true,
    "id": "752e1971b840d6ee61b0e1347d8013111f2003e3",
    "semantic_title": "predict-optimize-distill: a self-improving cycle for 4d object understanding",
    "citation_count": 3,
    "authors": [
      "Mingxuan Wu",
      "Huang Huang",
      "Justin Kerr",
      "Chung Min Kim",
      "Anthony Zhang",
      "Brent Yi",
      "Angjoo Kanazawa"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Qiu_LHM_Large_Animatable_Human_Reconstruction_Model_for_Single_Image_to_ICCV_2025_paper.html": {
    "title": "LHM: Large Animatable Human Reconstruction Model for Single Image to 3D in Seconds",
    "volume": "main",
    "abstract": "Animatable 3D human reconstruction from a single image is a challenging problem due to the ambiguity in decoupling geometry, appearance, and deformation. Recent advances in 3D human reconstruction mainly focus on static human modeling, and the reliance of using synthetic 3D scans for training limits their generalization ability. Conversely, optimization-based video methods achieve higher fidelity but demand controlled capture conditions and computationally intensive refinement processes. Motivated by the emergence of large reconstruction models for efficient static reconstruction, we propose LHM (Large Animatable Human Reconstruction Model) to infer high-fidelity avatars represented as 3D Gaussian splatting in a feed-forward pass. Our model leverages a multimodal transformer architecture to effectively encode the human body positional features and image features with attention mechanism, enabling detailed preservation of clothing geometry and texture. To further boost the face identity preservation and fine detail recovery, we propose a head feature pyramid encoding scheme to aggregate multi-scale features of the head regions. Extensive experiments demonstrate that our LHM generates plausible animatable human in seconds without post-processing for face and hands, outperforming existing methods in both reconstruction accuracy and generalization ability",
    "checked": false,
    "id": "83239bbbe6742c1f905cc78cdae044f463eb38aa",
    "semantic_title": "lhm: large animatable human reconstruction model from a single image in seconds",
    "citation_count": 18,
    "authors": [
      "Lingteng Qiu",
      "Xiaodong Gu",
      "Peihao Li",
      "Qi Zuo",
      "Weichao Shen",
      "Junfei Zhang",
      "Kejie Qiu",
      "Weihao Yuan",
      "Guanying Chen",
      "Zilong Dong",
      "Liefeng Bo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liang_Fine-grained_Spatiotemporal_Grounding_on_Egocentric_Videos_ICCV_2025_paper.html": {
    "title": "Fine-grained Spatiotemporal Grounding on Egocentric Videos",
    "volume": "main",
    "abstract": "Spatiotemporal video grounding aims to localize target entities in videos based on textual queries. While existing research has made significant progress in exocentric videos, the egocentric setting remains relatively underexplored, despite its growing importance in applications such as augmented reality and robotics. In this work, we conduct a systematic analysis of the discrepancies between egocentric and exocentric videos, revealing key challenges such as shorter object durations, sparser trajectories, smaller object sizes, and larger positional shifts. To address these challenges, we introduce EgoMask, the first pixel-level benchmark for fine-grained spatiotemporal grounding in egocentric videos. It is constructed by our proposed automatic annotation pipeline, which annotates referring expressions and object masks across short-, medium-, and long-term videos. Additionally, we create EgoMask-Train, a large-scale training dataset to facilitate model development. Experiments demonstrate that the state-of-the-art spatiotemporal grounding models perform poorly on our benchmark EgoMask, but fine-tuning on EgoMask-Train yields significant improvements, while preserving performance on exocentric datasets. Our work thus provides essential resources and insights for advancing egocentric video understanding",
    "checked": true,
    "id": "467b5fb31ac39c9297e0f7e8371815f259b7912c",
    "semantic_title": "fine-grained spatiotemporal grounding on egocentric videos",
    "citation_count": 1,
    "authors": [
      "Shuo Liang",
      "Yiwu Zhong",
      "Zi-Yuan Hu",
      "Yeyao Tao",
      "Liwei Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wei_3D_Test-time_Adaptation_via_Graph_Spectral_Driven_Point_Shift_ICCV_2025_paper.html": {
    "title": "3D Test-time Adaptation via Graph Spectral Driven Point Shift",
    "volume": "main",
    "abstract": "While test-time adaptation (TTA) methods effectively address domain shifts by dynamically adapting pre-trained models to target domain data during online inference, their application to 3D point clouds is hindered by their irregular and unordered structure. Current 3D TTA methods often rely on computationally expensive spatial-domain optimizations and may require additional training data. In contrast, we propose Graph Spectral Domain Test-Time Adaptation (GSDTTA), a novel approach for 3D point cloud classification that shifts adaptation to the graph spectral domain, enabling more efficient adaptation by capturing global structural properties with fewer parameters. Point clouds in target domain are represented as outlier-aware graphs and transformed into graph spectral domain by Graph Fourier Transform (GFT). For efficiency, adaptation is performed by optimizing only the lowest 10% of frequency components, which capture the majority of the point cloud's energy. An inverse GFT (IGFT) is then applied to reconstruct the adapted point cloud with the graph spectral-driven point shift. This process is enhanced by an eigenmap-guided self-training strategy that iteratively refines both the spectral adjustments and the model parameters. Experimental results and ablation studies on benchmark datasets demonstrate the effectiveness of GSDTTA, outperforming existing TTA methods for 3D point cloud classification",
    "checked": true,
    "id": "4198ffe8c80c4652ce2528530ac71c3c108b8355",
    "semantic_title": "3d test-time adaptation via graph spectral driven point shift",
    "citation_count": 0,
    "authors": [
      "Xin Wei",
      "Qin Yang",
      "Yijie Fang",
      "Mingrui Zhu",
      "Nannan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Rajasegaran_An_Empirical_Study_of_Autoregressive_Pre-training_from_Videos_ICCV_2025_paper.html": {
    "title": "An Empirical Study of Autoregressive Pre-training from Videos",
    "volume": "main",
    "abstract": "We empirically study autoregressive pre-training from videos. To perform our study, we construct a series of autoregressive video models, called Toto. We treat videos as sequences of visual tokens and train transformer models to autoregressively predict future tokens. Our models are pre-trained on a diverse dataset of videos and images comprising over 1 trillion visual tokens. We explore different architectural, training, and inference design choices. We evaluate the learned visual representations on a range of downstream tasks including image recognition, video classification, object tracking, and robotics. Our results demonstrate that, despite minimal inductive biases, autoregressive pre-training leads to competitive performance across all benchmarks. Finally, we find that scaling our video models results in similar scaling curves to those seen in language models, albeit with a different rate",
    "checked": true,
    "id": "3b668cf5fefdf62b66e07eed52292a0ba13d690f",
    "semantic_title": "an empirical study of autoregressive pre-training from videos",
    "citation_count": 13,
    "authors": [
      "Jathushan Rajasegaran",
      "Ilija Radosavovic",
      "Rahul Ravishankar",
      "Yossi Gandelsman",
      "Christoph Feichtenhofer",
      "Jitendra Malik"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lee_Latent_Diffusion_Models_with_Masked_AutoEncoders_ICCV_2025_paper.html": {
    "title": "Latent Diffusion Models with Masked AutoEncoders",
    "volume": "main",
    "abstract": "In spite of the remarkable potential of Latent Diffusion Models (LDMs) in image generation, the desired properties and optimal design of the autoencoders have been underexplored. In this work, we analyze the role of autoencoders in LDMs and identify three key properties: latent smoothness, perceptual compression quality, and reconstruction quality. We demonstrate that existing autoencoders fail to simultaneously satisfy all three properties, and propose Variational Masked AutoEncoders (VMAEs), taking advantage of the hierarchical features maintained by Masked AutoEncoder. We integrate VMAEs into the LDM framework, introducing Latent Diffusion Models with Masked AutoEncoders (LDMAEs). Through comprehensive experiments, we demonstrate significantly enhanced image generation quality and computational efficiency",
    "checked": true,
    "id": "5f282aac9de171c92e8b91ae4ffc22b69f441a45",
    "semantic_title": "latent diffusion models with masked autoencoders",
    "citation_count": 1,
    "authors": [
      "Junho Lee",
      "Jeongwoo Shin",
      "Hyungwook Choi",
      "Joonseok Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Abdelreheem_PlaceIt3D_Language-Guided_Object_Placement_in_Real_3D_Scenes_ICCV_2025_paper.html": {
    "title": "PlaceIt3D: Language-Guided Object Placement in Real 3D Scenes",
    "volume": "main",
    "abstract": "We introduce the task of Language-Guided Object Placement in Real 3D Scenes. Given a 3D reconstructed point-cloud scene, a 3D asset, and a natural-language instruction, the goal is to place the asset so that the instruction is satisfied. The task demands tackling four intertwined challenges: (a) one-to-many ambiguity in valid placements; (b) precise geometric and physical reasoning; (c) joint understanding across the scene, the asset, and language; and (d) robustness to noisy point clouds with no privileged metadata at test time. The first three challenges mirror the complexities of synthetic scene generation, while the metadata-free, noisy-scan scenario is inherited from language-guided 3D visual grounding. We inaugurate this task by introducing a benchmark and evaluation protocol, releasing a dataset for training multi-modal large language models (MLLMs), and establishing a first nontrivial baseline. We believe this challenging setup and benchmark will provide a foundation for evaluating and advancing MLLMs in 3D understanding",
    "checked": true,
    "id": "3877f4c812b3091b43fba381fd7a6cbcd81df3ee",
    "semantic_title": "placeit3d: language-guided object placement in real 3d scenes",
    "citation_count": 1,
    "authors": [
      "Ahmed Abdelreheem",
      "Filippo Aleotti",
      "Jamie Watson",
      "Zawar Qureshi",
      "Abdelrahman Eldesokey",
      "Peter Wonka",
      "Gabriel Brostow",
      "Sara Vicente",
      "Guillermo Garcia-Hernando"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Luo_TR-PTS_Task-Relevant_Parameter_and_Token_Selection_for_Efficient_Tuning_ICCV_2025_paper.html": {
    "title": "TR-PTS: Task-Relevant Parameter and Token Selection for Efficient Tuning",
    "volume": "main",
    "abstract": "Large pre-trained models achieve remarkable performance in vision tasks but are impractical for fine-tuning due to high computational and storage costs. Parameter-Efficient Fine-Tuning (PEFT) methods mitigate this issue by updating only a subset of parameters; however, most existing approaches are task-agnostic, failing to fully exploit task-specific adaptations, which leads to suboptimal efficiency and performance. To address this limitation, we propose Task-Relevant Parameter and Token Selection (TR-PTS), a task-driven framework that enhances both computational efficiency and accuracy. Specifically, we introduce Task-Relevant Parameter Selection, which utilizes the Fisher Information Matrix (FIM) to identify and fine-tune only the most informative parameters in a layer-wise manner, while keeping the remaining parameters frozen. Simultaneously, Task-Relevant Token Selection dynamically preserves the most informative tokens and merges redundant ones, reducing computational overhead. By jointly optimizing parameters and tokens, TR-PTS enables the model to concentrate on task-discriminative information. We evaluate TR-PTS on benchmark, including FGVC and VTAB-1k, where it achieves state-of-the-art performance, surpassing full fine-tuning by 3.40% and 10.35%, respectively. The code are available at https://github.com/synbol/TR-PTS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siqi Luo",
      "Haoran Yang",
      "Yi Xin",
      "Mingyang Yi",
      "Guangyang Wu",
      "Guangtao Zhai",
      "Xiaohong Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_StruMamba3D_Exploring_Structural_Mamba_for_Self-supervised_Point_Cloud_Representation_Learning_ICCV_2025_paper.html": {
    "title": "StruMamba3D: Exploring Structural Mamba for Self-supervised Point Cloud Representation Learning",
    "volume": "main",
    "abstract": "Recently, Mamba-based methods have demonstrated impressive performance in point cloud representation learning by leveraging State Space Model (SSM) with the efficient context modeling ability and linear complexity. However, these methods still face two key issues that limit the potential of SSM: Destroying the adjacency of 3D points during SSM processing and failing to retain long-sequence memory as the input length increases in downstream tasks. To address these issues, we propose StruMamba3D, a novel paradigm for self-supervised point cloud representation learning. It enjoys several merits. First, we design spatial states and use them as proxies to preserve spatial dependencies among points. Second, we enhance the SSM with a state-wise update strategy and incorporate a lightweight convolution to facilitate interactions between spatial states for efficient structure modeling. Third, our method reduces the sensitivity of pre-trained Mamba-based models to varying input lengths by introducing a sequence length-adaptive strategy. Experimental results across four downstream tasks showcase the superior performance of our method. In addition, our method attains the SOTA 95.1% accuracy on ModelNet40 and 92.75% accuracy on the most challenging split of ScanObjectNN without voting strategy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuxin Wang",
      "Yixin Zha",
      "Wenfei Yang",
      "Tianzhu Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Deng_Open-World_Skill_Discovery_from_Unsegmented_Demonstration_Videos_ICCV_2025_paper.html": {
    "title": "Open-World Skill Discovery from Unsegmented Demonstration Videos",
    "volume": "main",
    "abstract": "Learning skills in open-world environments is essential for developing agents capable of handling a variety of tasks by combining basic skills. Online demonstration videos are typically long but unsegmented, making them difficult to segment and label with skill identifiers. Unlike existing methods that rely on random splitting or human labeling, we have developed a self-supervised learning-based approach to segment these long videos into a series of semantic-aware and skill-consistent segments. Drawing inspiration from human cognitive event segmentation theory, we introduce Skill Boundary Detection (SBD), an annotation-free temporal video segmentation algorithm. SBD detects skill boundaries in a video by leveraging prediction errors from a pretrained unconditional action-prediction model. This approach is based on the assumption that a significant increase in prediction error indicates a shift in the skill being executed. We evaluated our method in Minecraft, a rich open-world simulator with extensive gameplay videos available online. The SBD-generated segments yielded relative performance improvements of 63.7% and 52.1% for conditioned policies on short-term atomic tasks, and 11.3% and 20.8% for their corresponding hierarchical agents on long-horizon tasks, compared to unsegmented baselines. Our method can leverage the diverse YouTube videos to train instruction-following agents. The project page is at https://craftjarvis.github.io/SkillDiscovery/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingwen Deng",
      "Zihao Wang",
      "Shaofei Cai",
      "Anji Liu",
      "Yitao Liang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sutton_Staining_and_Locking_Computer_Vision_Models_Without_Retraining_ICCV_2025_paper.html": {
    "title": "Staining and Locking Computer Vision Models Without Retraining",
    "volume": "main",
    "abstract": "We introduce new methods of staining and locking computer vision models, to protect their owners' intellectual property. Staining, also known as watermarking, embeds secret behaviour into a model which can later be used to identify it, while locking aims to make a model unusable unless a secret trigger is inserted into input images. Unlike existing methods, our algorithms can be used to stain and lock pre-trained models without requiring fine-tuning or retraining, and come with provable, computable guarantees bounding their worst-case false positive rates. The stain and lock are implemented by directly modifying a small number of the model's weights and have minimal impact on the (unlocked) model's performance. Locked models are unlocked by inserting a small 'trigger patch' into the corner of the input image. We present experimental results showing the efficacy of our methods and demonstrating their practical performance on a variety of computer vision models",
    "checked": true,
    "id": "03c0814b6d88520dde7da981a41617aea11200de",
    "semantic_title": "staining and locking computer vision models without retraining",
    "citation_count": 1,
    "authors": [
      "Oliver J. Sutton",
      "Qinghua Zhou",
      "George Leete",
      "Alexander N. Gorban",
      "Ivan Y. Tyukin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Consistent_Time-of-Flight_Depth_Denoising_via_Graph-Informed_Geometric_Attention_ICCV_2025_paper.html": {
    "title": "Consistent Time-of-Flight Depth Denoising via Graph-Informed Geometric Attention",
    "volume": "main",
    "abstract": "Depth images captured by Time-of-Flight (ToF) sensors are prone to noise, requiring denoising for reliable downstream applications. Previous works either focus on single-frame processing, or perform multi-frame processing without considering depth variations at corresponding pixels across frames, leading to undesirable temporal inconsistency and spatial ambiguity. In this paper, we propose a novel ToF depth denoising network leveraging motion-invariant graph fusion to simultaneously enhance temporal stability and spatial sharpness. Specifically, despite depth shifts across frames, graph structures exhibit temporal self-similarity, enabling cross-frame geometric attention for graph fusion. Then, by incorporating an image smoothness prior on the fused graph and data fidelity term derived from ToF noise distribution, we formulate a maximum a posterior problem for ToF denoising. Finally, the solution is unrolled into iterative filters whose weights are adaptively learned from the graph-informed geometric attention, producing a high-performance yet interpretable network. Experimental results demonstrate that the proposed scheme achieves state-of-the-art performance in terms of accuracy and consistency on synthetic DVToF dataset and exhibits robust generalization on the real Kinectv2 dataset. Source code is available at https://github.com/davidweidawang/GIGA-ToF",
    "checked": true,
    "id": "a3795663ddd519229bdc857157ccf6e80c503789",
    "semantic_title": "consistent time-of-flight depth denoising via graph-informed geometric attention",
    "citation_count": 0,
    "authors": [
      "Weida Wang",
      "Changyong He",
      "Jin Zeng",
      "Di Qiu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jia_Can_Generative_Geospatial_Diffusion_Models_Excel_as_Discriminative_Geospatial_Foundation_ICCV_2025_paper.html": {
    "title": "Can Generative Geospatial Diffusion Models Excel as Discriminative Geospatial Foundation Models?",
    "volume": "main",
    "abstract": "Self-supervised learning (SSL) has revolutionized representation learning in Remote Sensing (RS), advancing Geospatial Foundation Models (GFMs) to leverage vast unlabeled satellite imagery for diverse downstream tasks. Currently, GFMs primarily employ objectives like contrastive learning or masked image modeling, owing to their proven success in learning transferable representations. However, generative diffusion models, which demonstrate the potential to capture multi-grained semantics essential for RS tasks during image generation, remain underexplored for discriminative applications. This prompts the question: can generative diffusion models also excel and serve as GFMs with sufficient discriminative power? In this work, we answer this question with SatDiFuser, a framework that transforms a diffusion-based generative geospatial foundation model into a powerful pretraining tool for discriminative RS. By systematically analyzing multi-stage, noise-dependent diffusion features, we develop three fusion strategies to effectively leverage these diverse representations. Extensive experiments on remote sensing benchmarks show that SatDiFuser outperforms state-of-the-art GFMs, achieving gains of up to +5.7% mIoU in semantic segmentation and +7.9% F1-score in classification, demonstrating the capacity of diffusion-based generative foundation models to rival or exceed discriminative GFMs. The source code is available at: https://github.com/yurujaja/SatDiFuser",
    "checked": true,
    "id": "c08d831b37526f9634b0bc8377a86c1178442558",
    "semantic_title": "can generative geospatial diffusion models excel as discriminative geospatial foundation models?",
    "citation_count": 5,
    "authors": [
      "Yuru Jia",
      "Valerio Marsocci",
      "Ziyang Gong",
      "Xue Yang",
      "Maarten Vergauwen",
      "Andrea Nascetti"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_End-to-End_Entity-Predicate_Association_Reasoning_for_Dynamic_Scene_Graph_Generation_ICCV_2025_paper.html": {
    "title": "End-to-End Entity-Predicate Association Reasoning for Dynamic Scene Graph Generation",
    "volume": "main",
    "abstract": "Dynamic Scene Graph Generation (DSGG) aims to comprehensively understand videos by abstracting them into visual triplets <subject, predicate, object>. Most existing methods focus on capturing temporal dependencies, but overlook crucial visual relationship dependencies between entities and predicates, as well as among predicate subclasses. These dependencies are essential for a deeper contextual understanding of scenarios. Additionally, current approaches do not support end-to-end training and instead rely on a two-stage pipeline, which incurs higher computational costs. To address these issues, we propose an end-to-end Association Reasoning Network (ARN) for DSGG. ARN leverages CLIP's semantic priors to model fine-grained triplet cues to generate scene graph. In addition, we design a Predicate Association Parsing (PAP) module that employs a conditional weight mapping mechanism to structure entity and predicate representations. We further introduce a Hierarchical Attention (HA) mechanism to integrate spatio-temporal context with entity and predicate representations, enabling effective associative reasoning. Extensive experiments on the Action Genome dataset demonstrate significant performance improvements over existing methods. The source code is available in \\hyperref[URL] https://github.com/wlw951226/ARN",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liwei Wang",
      "Yanduo Zhang",
      "Tao Lu",
      "Fang Liu",
      "Huiqin Zhang",
      "Jiayi Ma",
      "Huabing Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cho_AJAHR_Amputated_Joint_Aware_3D_Human_Mesh_Recovery_ICCV_2025_paper.html": {
    "title": "AJAHR: Amputated Joint Aware 3D Human Mesh Recovery",
    "volume": "main",
    "abstract": "Existing human mesh recovery methods assume a standard human body structure, overlooking diverse anatomical conditions such as limb loss. This assumption introduces bias with applied to individuals with amputations, a limitation further exacerbated by the scarcity of suitable datasets. To address this gap, we propose Amputated Joint Aware 3D Human Mesh Recovery (AJAHR), which is an adaptive pose estimation framework that improves mesh reconstruction for individuals with limb loss. Our model integrates a body-part amputation classifier, jointly trained with the mesh recovery network, to detect potential amputations. We also introduce Amputee 3D (A3D), which is a synthetic dataset offering a wide range of amputee poses for robust training. While maintaining competitive performance on non-amputees, our approach achieves state-of-the-art results for amputated individuals. Additional materials can be found at: https://chojinie.github.io/project_AJAHR/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyunjin Cho",
      "Giyun Choi",
      "Jongwon Choi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Do_Bridging_the_Skeleton-Text_Modality_Gap_Diffusion-Powered_Modality_Alignment_for_Zero-shot_ICCV_2025_paper.html": {
    "title": "Bridging the Skeleton-Text Modality Gap: Diffusion-Powered Modality Alignment for Zero-shot Skeleton-based Action Recognition",
    "volume": "main",
    "abstract": "In zero-shot skeleton-based action recognition (ZSAR), aligning skeleton features with the text features of action labels is essential for accurately predicting unseen actions. ZSAR faces a fundamental challenge in bridging the modality gap between the two-kind features, which severely limits generalization to unseen actions. Previous methods focus on direct alignment between skeleton and text latent spaces, but the modality gaps between these spaces hinder robust generalization learning. Motivated by the success of diffusion models in multi-modal alignment (e.g., text-to-image, text-to-video), we firstly present a diffusion-based skeleton-text alignment framework for ZSAR. Our approach, Triplet Diffusion for Skeleton-Text Matching (TDSM), focuses on cross-alignment power of diffusion models rather than their generative capability. Specifically, TDSM aligns skeleton features with text prompts by incorporating text features into the reverse diffusion process, where skeleton features are denoised under text guidance, forming a unified skeleton-text latent space for robust matching. To enhance discriminative power, we introduce a triplet diffusion (TD) loss that encourages our TDSM to correct skeleton-text matches while pushing them apart for different action classes. Our TDSM significantly outperforms very recent state-of-the-art methods with significantly large margins of 2.36%-point to 13.05%-point, demonstrating superior accuracy and scalability in zero-shot settings through effective skeleton-text matching",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeonghyeok Do",
      "Munchurl Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shen_Fish2Mesh_Transformer_3D_Human_Mesh_Recovery_from_Egocentric_Vision_ICCV_2025_paper.html": {
    "title": "Fish2Mesh Transformer: 3D Human Mesh Recovery from Egocentric Vision",
    "volume": "main",
    "abstract": "Egocentric human body estimation allows for the inference of user body pose and shape from a wearable camera's first-person perspective. Although research has used pose estimation techniques to overcome self-occlusions and image distortions caused by head-mounted fisheye images, similar advances in 3D human mesh recovery (HMR) techniques have been limited. We introduce Fish2Mesh, a fisheye-aware transformer-based model designed for 3D egocentric human mesh recovery. We propose an egocentric position embedding block to generate an ego-specific position table for the Swin Transformer to reduce fisheye image distortion. Our model utilizes multi-task heads for SMPL parametric regression and camera translations, estimating 3D and 2D joints as auxiliary loss to support model training. To address the scarcity of egocentric camera data, we create a training dataset by employing the pre-trained 4D-Human model and third-person cameras for weak supervision. Our experiments demonstrate that Fish2Mesh outperforms previous state-of-the-art 3D HMR models. Egocentric human body estimation allows for the inference of user body pose and shape from a wearable camera's first-person perspective. Although research has used pose estimation techniques to overcome self-occlusions and image distortions caused by head-mounted fisheye images, similar advances in 3D human mesh recovery (HMR) techniques have been limited. We introduce Fish2Mesh, a fisheye-aware transformer-based model designed for 3D egocentric human mesh recovery. We propose an egocentric position embedding block to generate an ego-specific position table for the Swin Transformer to reduce fisheye image distortion. Our model utilizes multi-task heads for SMPL parametric regression and camera translations, estimating 3D and 2D joints as auxiliary loss to support model training. To address the scarcity of egocentric camera data, we create a training dataset by employing the pre-trained 4D-Human model and third-person cameras for weak supervision. Our experiments demonstrate that Fish2Mesh outperforms previous state-of-the-art 3D HMR models",
    "checked": true,
    "id": "3fd4e3890a9e8325f1ad4305514872deb8b14ddb",
    "semantic_title": "fish2mesh transformer: 3d human mesh recovery from egocentric vision",
    "citation_count": 0,
    "authors": [
      "Tianma Shen",
      "Aditya Puranik",
      "James Vong",
      "Vrushabh Deogirikar",
      "Ryan Fell",
      "Julianna Dietrich",
      "Maria Kyrarini",
      "Christopher Kitts",
      "David C. Jeong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Simonelli_Easy3D_A_Simple_Yet_Effective_Method_for_3D_Interactive_Segmentation_ICCV_2025_paper.html": {
    "title": "Easy3D: A Simple Yet Effective Method for 3D Interactive Segmentation",
    "volume": "main",
    "abstract": "The increasing availability of digital 3D environments, whether through image reconstruction, generation, or scans obtained via lasers or robots, is driving innovation across various fields. Among the numerous applications, there is a significant demand for those that enable 3D interaction, such as 3D Interactive Segmentation, which is useful for tasks like object selection and manipulation. Additionally, there is a persistent need for solutions that are efficient, precise, and consistently perform well across diverse settings, particularly in unseen environments and with unfamiliar objects. In this work, we introduce a method that consistently surpasses previous state-of-the-art techniques on both in-domain and out-of-domain datasets. Our simple approach integrates a voxel-based sparse encoder with a lightweight transformer-based decoder that implements implicit click fusion, achieving superior performance and maximizing efficiency. Our method demonstrates substantial improvements on benchmark datasets, including ScanNet, ScanNet++, S3DIS, and KITTI-360, and also on unseen geometric distributions such as Gaussian Splatting",
    "checked": true,
    "id": "e5edc4c67777dacc2365a8dc8461cceae09dfdb3",
    "semantic_title": "easy3d: a simple yet effective method for 3d interactive segmentation",
    "citation_count": 1,
    "authors": [
      "Andrea Simonelli",
      "Norman Müller",
      "Peter Kontschieder"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Bao_DynImg_Key_Frames_with_Visual_Prompts_are_Good_Representation_for_ICCV_2025_paper.html": {
    "title": "DynImg: Key Frames with Visual Prompts are Good Representation for Multi-Modal Video Understanding",
    "volume": "main",
    "abstract": "In recent years, the introduction of Multi-modal Large Language Models (MLLMs) into video understanding tasks has become increasingly prevalent. However, how to effectively integrate temporal information remains a critical research focus. Traditional approaches treat spatial and temporal information separately. Due to issues like motion blur, it is challenging to accurately represent the spatial information of rapidly moving objects. This can lead to temporally important regions being underemphasized during spatial feature extraction, which in turn hinders accurate spatio-temporal interaction and video understanding. To address this limitation, we propose an innovative video representation method called Dynamic-Image (DynImg). Specifically, we introduce a set of non-key frames as temporal prompts to highlight the spatial areas containing fast-moving objects. During the process of visual feature extraction, these prompts guide the model to pay additional attention to the fine-grained spatial features corresponding to these regions. Moreover, to maintain the correct sequence for DynImg, we employ a corresponding 4D video Rotary Position Embedding. This retains both the temporal and spatial adjacency of DynImg, helping MLLM understand the spatio-temporal order within this combined format. Experimental evaluations reveal that DynImg surpasses the state-of-the-art methods by approximately 2% across multiple video understanding benchmarks, proving the effectiveness of our temporal prompts in enhancing video comprehension",
    "checked": true,
    "id": "5ca273d57c83069111939406f6d454761d7436ed",
    "semantic_title": "dynimg: key frames with visual prompts are good representation for multi-modal video understanding",
    "citation_count": 0,
    "authors": [
      "Xiaoyi Bao",
      "Chenwei Xie",
      "Hao Tang",
      "Tingyu Weng",
      "Xiaofeng Wang",
      "Yun Zheng",
      "Xingang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Interpretable_Zero-Shot_Learning_with_Locally-Aligned_Vision-Language_Model_ICCV_2025_paper.html": {
    "title": "Interpretable Zero-Shot Learning with Locally-Aligned Vision-Language Model",
    "volume": "main",
    "abstract": "Large-scale vision-language models (VLMs), such as CLIP, have achieved remarkable success in zero-shot learning (ZSL) by leveraging large-scale visual-text pair datasets. However, these methods often lack interpretability, as they compute the similarity between an entire query image and the embedded category words, making it difficult to explain their predictions. One approach to address this issue is to develop interpretable models by integrating language, where classifiers are built using discrete attributes, similar to human perception. This introduces a new challenge: how to effectively align local visual features with corresponding attributes based on pre-trained VLMs. To tackle this, we propose LaZSL, a locally-aligned vision-language model for interpretable ZSL. LaZSL employs local visual-semantic alignment via optimal transport to perform interaction between visual regions and their associated attributes, facilitating effective alignment and providing interpretable similarity without the need for additional training. Extensive experiments demonstrate that our method offers several advantages, including enhanced interpretability, improved accuracy, and strong domain generalization. Codes available at: https://github.com/shiming-chen/LaZSL",
    "checked": true,
    "id": "e5f7848ce1cf04d4e6959e2d0cf56c92157000a9",
    "semantic_title": "interpretable zero-shot learning with locally-aligned vision-language model",
    "citation_count": 0,
    "authors": [
      "Shiming Chen",
      "Bowen Duan",
      "Salman Khan",
      "Fahad Shahbaz Khan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wasim_MixANT_Observation-dependent_Memory_Propagation_for_Stochastic_Dense_Action_Anticipation_ICCV_2025_paper.html": {
    "title": "MixANT: Observation-dependent Memory Propagation for Stochastic Dense Action Anticipation",
    "volume": "main",
    "abstract": "We present MixANT, a novel architecture for stochastic long-term dense anticipation of human activities. While recent State Space Models (SSMs) like Mamba have shown promise through input-dependent selectivity on three key parameters, the critical forget-gate (A matrix) controlling temporal memory remains static. We address this limitation by introducing a mixture of experts approach that dynamically selects contextually relevant A matrices based on input features, enhancing representational capacity without sacrificing computational efficiency. Extensive experiments on the 50Salads, Breakfast, and Assembly101 datasets demonstrate that MixANT consistently outperforms state-of-the-art methods across all evaluation settings. Our results highlight the importance of input-dependent forget-gate mechanisms for reliable prediction of human behavior in diverse real-world scenarios. The project page is available at https://talalwasim.github.io/MixANT/",
    "checked": true,
    "id": "f1e076750c64ef311e2febf055f63234cf3d3cf7",
    "semantic_title": "mixant: observation-dependent memory propagation for stochastic dense action anticipation",
    "citation_count": 0,
    "authors": [
      "Syed Talal Wasim",
      "Hamid Suleman",
      "Olga Zatsarynna",
      "Muzammal Naseer",
      "Juergen Gall"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ma_MaGS_Reconstructing_and_Simulating_Dynamic_3D_Objects_with_Mesh-adsorbed_Gaussian_ICCV_2025_paper.html": {
    "title": "MaGS: Reconstructing and Simulating Dynamic 3D Objects with Mesh-adsorbed Gaussian Splatting",
    "volume": "main",
    "abstract": "3D reconstruction and simulation, although interrelated, have distinct objectives: reconstruction requires a flexible 3D representation that can adapt to diverse scenes, while simulation needs a structured representation to model motion principles effectively. This paper introduces the Mesh-adsorbed Gaussian Splatting (MaGS) method to address this challenge. MaGS constrains 3D Gaussians to roam near the mesh, creating a mutually adsorbed mesh-Gaussian 3D representation. Such representation harnesses both the rendering flexibility of 3D Gaussians and the structured property of meshes. To achieve this, we introduce RMD-Net, a network that learns motion priors from video data to refine mesh deformations, alongside RGD-Net, which models the relative displacement between the mesh and Gaussians to enhance rendering fidelity under mesh constraints. To generalize to novel, user-defined deformations beyond input video without reliance on temporal data, we propose MPE-Net, which leverages inherent mesh information to bootstrap RMD-Net and RGD-Net. Due to the universality of meshes, MaGS is compatible with various deformation priors such as ARAP, SMPL, and soft physics simulation. Extensive experiments on the D-NeRF, DG-Mesh, and PeopleSnapshot datasets demonstrate that MaGS achieves state-of-the-art performance in both reconstruction and simulation",
    "checked": true,
    "id": "59908ee53ad7cd44c0a48c1f5682db9c69635a4f",
    "semantic_title": "mags: reconstructing and simulating dynamic 3d objects with mesh-adsorbed gaussian splatting",
    "citation_count": 2,
    "authors": [
      "Shaojie Ma",
      "Yawei Luo",
      "Wei Yang",
      "Yi Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hwang_SceneMI_Motion_In-betweening_for_Modeling_Human-Scene_Interaction_ICCV_2025_paper.html": {
    "title": "SceneMI: Motion In-betweening for Modeling Human-Scene Interaction",
    "volume": "main",
    "abstract": "Modeling human-scene interactions (HSI) is essential for understanding and simulating everyday human behaviors. Recent approaches utilizing generative modeling have made progress in this domain; however, they are limited in controllability and flexibility for real-world applications. To address these challenges, we propose reformulating the HSI modeling problem as Scene-aware Motion In-betweening---a more tractable and practical task. We introduce SceneMI, a framework that supports several practical applications, including keyframe-guided character animation in 3D scenes and enhancing the motion quality of imperfect HSI data. SceneMI employs dual scene descriptors to comprehensively encode global and local scene context. Furthermore, our framework leverages the inherent denoising nature of diffusion models to generalize on noisy keyframes. Experimental results demonstrate SceneMI's effectiveness in scene-aware keyframe in-betweening and generalization to the real-world GIMO dataset, where motions and scenes are acquired by noisy IMU sensors and smartphones. We further showcase SceneMI's applicability in HSI reconstruction from monocular videos",
    "checked": false,
    "id": "da5325f51455f7d4581c42b27547199b031eacc0",
    "semantic_title": "scenemi: motion in-betweening for modeling human-scene interactions",
    "citation_count": 2,
    "authors": [
      "Inwoo Hwang",
      "Bing Zhou",
      "Young Min Kim",
      "Jian Wang",
      "Chuan Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_SMoLoRA_Exploring_and_Defying_Dual_Catastrophic_Forgetting_in_Continual_Visual_ICCV_2025_paper.html": {
    "title": "SMoLoRA: Exploring and Defying Dual Catastrophic Forgetting in Continual Visual Instruction Tuning",
    "volume": "main",
    "abstract": "Visual instruction tuning (VIT) enables multimodal large language models (MLLMs) to effectively handle a wide range of vision tasks by framing them as language-based instructions. Building on this, continual visual instruction tuning (CVIT) extends the capability of MLLMs to incrementally learn new tasks, accommodating evolving functionalities. While prior work has advanced CVIT through the development of new benchmarks and approaches to mitigate catastrophic forgetting, these efforts largely follow traditional continual learning paradigms, neglecting the unique challenges specific to CVIT. We identify a dual form of catastrophic forgetting in CVIT, where MLLMs not only forget previously learned visual understanding but also experience a decline in instruction following abilities as they acquire new tasks. To address this, we introduce the Separable Mixture of Low-Rank Adaptation (SMoLoRA) framework, which employs separable routing through two distinct modules--one for visual understanding and another for instruction following. This dual-routing design enables specialized adaptation in both domains, preventing forgetting while improving performance. Furthermore, we propose a new CVIT benchmark that goes beyond existing benchmarks by additionally evaluating a model's ability to generalize to unseen tasks and handle diverse instructions across various tasks. Extensive experiments demonstrate that SMoLoRA outperforms existing methods in mitigating dual forgetting, improving generalization to unseen tasks, and ensuring robustness in following diverse instructions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziqi Wang",
      "Chang Che",
      "Qi Wang",
      "Yangyang Li",
      "Zenglin Shi",
      "Meng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Palladin_Self-Supervised_Sparse_Sensor_Fusion_for_Long_Range_Perception_ICCV_2025_paper.html": {
    "title": "Self-Supervised Sparse Sensor Fusion for Long Range Perception",
    "volume": "main",
    "abstract": "Outside of urban hubs, autonomous cars and trucks have to master driving on intercity highways. Safe, long-distance highway travel at speeds exceeding 100 km/h demands perception distances of at least 250 m, which is about five times the 50-100m typically addressed in city driving, to allow sufficient planning and braking margins. Increasing the perception ranges also allows to extend autonomy from light two-ton passenger vehicles to large-scale forty-ton trucks, which need a longer planning horizon due to their high inertia. However, most existing perception approaches focus on shorter ranges and rely on Bird's Eye View (BEV) representations, which incur quadratic increases in memory and compute costs as distance grows. To overcome this limitation, we built on top of a sparse representation and introduced an efficient 3D encoding of multi-modal and temporal features, along with a novel self-supervised pre-training scheme that enables large-scale learning from unlabeled camera-LiDAR data. Our approach extends perception distances to 250 meters and achieves an 26.6% improvement in mAP in object detection and a decrease of 30.5% in Chamfer Distance in LiDAR forecasting compared to existing methods, reaching distances up to 250 meters",
    "checked": true,
    "id": "4f0e107705d39bd84ca46df5d08353c63debb9e0",
    "semantic_title": "self-supervised sparse sensor fusion for long range perception",
    "citation_count": 0,
    "authors": [
      "Edoardo Palladin",
      "Samuel Brucker",
      "Filippo Ghilotti",
      "Praveen Narayanan",
      "Mario Bijelic",
      "Felix Heide"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tong_GeoDistill_Geometry-Guided_Self-Distillation_for_Weakly_Supervised_Cross-View_Localization_ICCV_2025_paper.html": {
    "title": "GeoDistill: Geometry-Guided Self-Distillation for Weakly Supervised Cross-View Localization",
    "volume": "main",
    "abstract": "Cross-view localization, the task of estimating a camera's 3-degrees-of-freedom (3-DoF) pose by aligning ground-level images with aerial images, is crucial for large-scale outdoor applications like autonomous navigation and augmented reality. Existing methods often rely on fully supervised learning, which requires costly ground-truth pose annotations. In this work, we propose GeoDistill, a Geometry guided weakly supervised self Distillation framework that uses teacher-student learning with Field-of-View (FoV)-based masking to enhance local feature learning for robust cross-view localization. In GeoDistill, the teacher model localizes a full view image, while the student model predicts locations from a limited FoV counterpart created by FoV-based masking. By aligning the student's predictions with those of the teacher, the student focuses on key features like lane lines and ignores textureless regions, such as roads. This results in more accurate predictions and reduced uncertainty. Our experiments show that GeoDistill significantly improves localization performance across different frameworks. Additionally, we introduce a novel orientation estimation network that predicts relative orientation without requiring precise planar position ground truth. GeoDistill provides a scalable and efficient solution for real-world cross-view localization challenges. Code and model can be found at https://github.com/tongshw/GeoDistill",
    "checked": true,
    "id": "626abc2d9159b19ca67d73bc5b7aec1bec43a405",
    "semantic_title": "geodistill: geometry-guided self-distillation for weakly supervised cross-view localization",
    "citation_count": 0,
    "authors": [
      "Shaowen Tong",
      "Zimin Xia",
      "Alexandre Alahi",
      "Xuming He",
      "Yujiao Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tourani_Leveraging_2D_Priors_and_SDF_Guidance_for_Urban_Scene_Rendering_ICCV_2025_paper.html": {
    "title": "Leveraging 2D Priors and SDF Guidance for Urban Scene Rendering",
    "volume": "main",
    "abstract": "Dynamic scene rendering and reconstruction play a crucial role in computer vision and augmented reality. Recent methods based on 3D Gaussian Splatting (3DGS), have enabled accurate modeling of dynamic urban scenes, but for urban scenes they require both camera and LiDAR data, ground-truth 3D segmentations and motion data in the form of tracklets or pre-defined object templates such as SMPL. In this work, we explore whether a combination of 2D object agnostic priors in the form of depth and point tracking coupled with a signed distance function (SDF) representation for dynamic objects can be used to relax some of these requirements. We present a novel approach that integrates Signed Distance Functions (SDFs) with 3D Gaussian Splatting (3DGS) to create a more robust object representation by harnessing the strengths of both methods. Our unified optimization framework enhances the geometric accuracy of 3D Gaussian splatting and improves deformation modeling within the SDF, resulting in a more adaptable and precise representation. We demonstrate that our method achieves near state-of-the-art performance in rendering metrics even without LiDAR data on urban scenes. Furthermore, when incorporating LiDAR, our approach surpasses existing methods in reconstructing and generating novel views across diverse object categories, without ground-truth 3D motion annotation. Additionally, our method enables various scene editing tasks including scene decomposition, and scene composition",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siddharth Tourani",
      "Jayaram Reddy",
      "Akash Kumbar",
      "Satyajit Tourani",
      "Nishant Goyal",
      "Madhava Krishna",
      "N Dinesh Reddy",
      "Muhammad Haris Khan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Du_SVTRv2_CTC_Beats_Encoder-Decoder_Models_in_Scene_Text_Recognition_ICCV_2025_paper.html": {
    "title": "SVTRv2: CTC Beats Encoder-Decoder Models in Scene Text Recognition",
    "volume": "main",
    "abstract": "Connectionist temporal classification (CTC)-based scene text recognition (STR) methods, e.g., SVTR, are widely employed in OCR applications, mainly due to their simple architecture, which only contains a visual model and a CTC-aligned linear classifier, and therefore fast inference. However, they generally exhibit worse accuracy than encoder-decoder-based methods (EDTRs) due to struggling with text irregularity and linguistic missing. To address these challenges, we propose SVTRv2, a CTC model endowed with the ability to handle text irregularities and model linguistic context. First, a multi-size resizing strategy is proposed to resize text instances to appropriate predefined sizes, effectively avoiding severe text distortion. Meanwhile, we introduce a feature rearrangement module to ensure that visual features accommodate the requirement of CTC, thus alleviating the alignment puzzle. Second, we propose a semantic guidance module. It integrates linguistic context into the visual features, allowing CTC model to leverage language information for accuracy improvement. This module can be omitted at the inference stage and would not increase the time cost. We extensively evaluate SVTRv2 in both standard and recent challenging benchmarks, where SVTRv2 is fairly compared to popular STR models across multiple scenarios, including different types of text irregularity, languages, long text, and whether employing pretraining. SVTRv2 surpasses most EDTRs across the scenarios in terms of accuracy and inference speed. Code: https://github.com/Topdu/OpenOCR",
    "checked": true,
    "id": "cd3d24a78d18ea44404850e37bf172e74b99742e",
    "semantic_title": "svtrv2: ctc beats encoder-decoder models in scene text recognition",
    "citation_count": 6,
    "authors": [
      "Yongkun Du",
      "Zhineng Chen",
      "Hongtao Xie",
      "Caiyan Jia",
      "Yu-Gang Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Blind_Noisy_Image_Deblurring_Using_Residual_Guidance_Strategy_ICCV_2025_paper.html": {
    "title": "Blind Noisy Image Deblurring Using Residual Guidance Strategy",
    "volume": "main",
    "abstract": "Blind deblurring is an ill-posed inverse problem that involves recovering both the clear image and the blur kernel from a single blurry image. In real photography, longer exposure time results in lots of noise in the blurry image. Although existing blind deblurring methods produce satisfactory results on blurred images with little or no noise, they struggle to handle high noise levels. Strong noise compromises the accuracy of the estimated kernel and significantly reduces the quality of the deblurring results. To address this challenge, we propose a Residual Guidance Strategy (RGS) to suppress the influence of noise. Our method leverages adjacent coarser-scale information in the image pyramid to guide the blur kernel estimation in the current scale. Therefore, for blurred images with unknown noise levels and types, our method still estimates more accurate blur kernels, which are essential for subsequent non-blind restoration. Extensive experiments on both synthetic and real datasets have demonstrated that our method consistently outperforms numerous state-of-the-art methods under high levels of noise quantitatively and qualitatively",
    "checked": false,
    "id": "8d2bbfd6c2d11a816aa40a755002e49442217a04",
    "semantic_title": "multi-objective reptile search algorithm based effective image deblurring and restoration",
    "citation_count": 11,
    "authors": [
      "Heyan Liu",
      "Jianing Sun",
      "Jun Liu",
      "Xi-Le Zhao",
      "Tingting Wu",
      "Tieyong Zeng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_DeGauss_Dynamic-Static_Decomposition_with_Gaussian_Splatting_for_Distractor-free_3D_Reconstruction_ICCV_2025_paper.html": {
    "title": "DeGauss: Dynamic-Static Decomposition with Gaussian Splatting for Distractor-free 3D Reconstruction",
    "volume": "main",
    "abstract": "Reconstructing clean, distractor-free 3D scenes from real-world captures remains a significant challenge, particularly in highly dynamic and cluttered settings such as egocentric videos. To tackle this problem, we introduce DeGauss, a simple and robust self-supervised framework for dynamic scene reconstruction based on a decoupled dynamic-static Gaussian Splatting design. DeGauss models dynamic elements with foreground Gaussians and static content with background Gaussians, using a probabilistic mask to coordinate their composition and enable independent yet complementary optimization. DeGauss generalizes robustly across a wide range of real-world scenarios, from casual image collections to long, dynamic egocentric videos, without relying on complex heuristics or extensive supervision. Experiments on benchmarks including NeRF-on-the-go, ADT, AEA, Hot3D, and EPIC-Fields demonstrate that DeGauss consistently outperforms existing methods, establishing a strong baseline for generalizable, distractor-free 3D reconstruction in highly dynamic, interaction-rich environments",
    "checked": true,
    "id": "f6db5500ae0d588ff7e0244d25cacf1f6064e09e",
    "semantic_title": "degauss: dynamic-static decomposition with gaussian splatting for distractor-free 3d reconstruction",
    "citation_count": 2,
    "authors": [
      "Rui Wang",
      "Quentin Lohmeyer",
      "Mirko Meboldt",
      "Siyu Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Afifi_Time-Aware_Auto_White_Balance_in_Mobile_Photography_ICCV_2025_paper.html": {
    "title": "Time-Aware Auto White Balance in Mobile Photography",
    "volume": "main",
    "abstract": "Cameras rely on auto white balance (AWB) to correct undesirable color casts caused by scene illumination and the camera's spectral sensitivity. This is typically achieved using an illuminant estimator that determines the global color cast solely from the color information in the camera's raw sensor image. Mobile devices provide valuable additional metadata---such as capture timestamp and geolocation---that offers strong contextual clues to help narrow down the possible illumination solutions. This paper proposes a lightweight illuminant estimation method that incorporates such contextual metadata, along with additional capture information and image colors, into a lightweight model (~5K parameters), achieving promising results, matching or surpassing larger models. To validate our method, we introduce a dataset of 3,224 smartphone images with contextual metadata collected at various times of day and under diverse lighting conditions. The dataset includes ground-truth illuminant colors, determined using a color chart, and user-preferred illuminants validated through a user study, providing a comprehensive benchmark for AWB evaluation",
    "checked": true,
    "id": "baf1198a5251f0b6b4eb1f31dc5283167fa65ade",
    "semantic_title": "time-aware auto white balance in mobile photography",
    "citation_count": 2,
    "authors": [
      "Mahmoud Afifi",
      "Luxi Zhao",
      "Abhijith Punnappurath",
      "Mohamed A. Abdelsalam",
      "Ran Zhang",
      "Michael S. Brown"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Miao_Unlocking_the_Potential_of_Diffusion_Priors_in_Blind_Face_Restoration_ICCV_2025_paper.html": {
    "title": "Unlocking the Potential of Diffusion Priors in Blind Face Restoration",
    "volume": "main",
    "abstract": "Although diffusion prior is rising as a powerful solution for blind face restoration (BFR), the inherent gap between the vanilla diffusion model and BFR settings hinders its seamless adaptation. The gap mainly stems from the discrepancy between 1) high-quality (HQ) and low-quality (LQ) images and 2) synthesized and real-world images.The vanilla diffusion model is trained on images with no or less degredations, while BFR handles moderately to severely degraded images.Additionally, LQ images used for training are synthesized by a naive degradation model with limited degradation patterns, which fails to simulate the complex and unknown degradations in real-world scenarios.In this work, we use a unified network FLIPNET that switches between two modes to address specific gaps.In restoration mode, the model gradually integrates BFR-oriented features and face embeddings from LQ images to achieve authentic and faithful face restoration.In degradation mode, the model synthesizes real-world like degraded images based on the knowledge learned from real-world degradation datasets.Extensive evaluations on benchmark datasets show that our model 1) outperforms previous diffusion prior based BFR methods in terms of authenticity and fidelity, and 2) outperforms the naive degradation model in modeling the real-world degradations",
    "checked": true,
    "id": "7b895546a4ba7548a6e43abb61e43845b69e5fc5",
    "semantic_title": "unlocking the potential of diffusion priors in blind face restoration",
    "citation_count": 0,
    "authors": [
      "Yunqi Miao",
      "Zhiyu Qu",
      "Mingqi Gao",
      "Changrui Chen",
      "Jifei Song",
      "Jungong Han",
      "Jiankang Deng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_InstaScene_Towards_Complete_3D_Instance_Decomposition_and_Reconstruction_from_Cluttered_ICCV_2025_paper.html": {
    "title": "InstaScene: Towards Complete 3D Instance Decomposition and Reconstruction from Cluttered Scenes",
    "volume": "main",
    "abstract": "Humans can naturally identify and mentally complete occluded objects in cluttered environments. However, imparting similar cognitive ability to robotics remains challenging even with advanced reconstruction techniques, which models scenes as undifferentiated wholes and fails to recognize complete object from partial observations. In this paper, we propose InstaScene, a new paradigm towards holistic 3D perception of complex scenes with a primary goal: decomposing arbitrary instances while ensuring complete reconstruction. To achieve precise decomposition, we develop a novel spatial contrastive learning by tracing rasterization of each instance across views, significantly enhancing semantic supervision in cluttered scenes. To overcome incompleteness from limited observations, we introduce in-situ generation that harnesses valuable observations and geometric cues, effectively guiding 3D generative models to reconstruct complete instances that seamlessly align with the real world. Experiments on scene decomposition and object completion across complex real-world and synthetic scenes demonstrate that our method achieves superior decomposition accuracy while producing geometrically faithful and visually intact objects",
    "checked": true,
    "id": "487a5a513e5e0b4cfd0ad457608d92279ccc448c",
    "semantic_title": "instascene: towards complete 3d instance decomposition and reconstruction from cluttered scenes",
    "citation_count": 1,
    "authors": [
      "Zesong Yang",
      "Bangbang Yang",
      "Wenqi Dong",
      "Chenxuan Cao",
      "Liyuan Cui",
      "Yuewen Ma",
      "Zhaopeng Cui",
      "Hujun Bao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hang_Improved_Noise_Schedule_for_Diffusion_Training_ICCV_2025_paper.html": {
    "title": "Improved Noise Schedule for Diffusion Training",
    "volume": "main",
    "abstract": "Diffusion models have emerged as the de facto choice for generating high-quality visual signals across various domains.However, training a single model to predict noise across various levels poses significant challenges, necessitating numerous iterations and incurring significant computational costs.Various approaches, such as loss weighting strategy design and architectural refinements, have been introduced to expedite convergence and improve model performance.In this study, we propose a novel approach to design the noise schedule for enhancing the training of diffusion models. Our key insight is that the importance sampling of the logarithm of the Signal-to-Noise ratio (\\log \\text SNR ), theoretically equivalent to a modified noise schedule, is particularly beneficial for training efficiency when increasing the sample frequency around \\log \\text SNR =0. This strategic sampling allows the model to focus on the critical transition point between signal dominance and noise dominance, potentially leading to more robust and accurate predictions.We empirically demonstrate the superiority of our noise schedule over the standard cosine schedule.Furthermore, we highlight the advantages of our noise schedule design on the ImageNet benchmark, showing that the designed schedule consistently benefits different prediction targets.Our findings contribute to the ongoing efforts to optimize diffusion models, potentially paving the way for more efficient and effective training paradigms in the field of generative AI",
    "checked": true,
    "id": "8e764b91d7c0bab9f21508969d1abbf84f409bc6",
    "semantic_title": "improved noise schedule for diffusion training",
    "citation_count": 25,
    "authors": [
      "Tiankai Hang",
      "Shuyang Gu",
      "Jianmin Bao",
      "Fangyun Wei",
      "Dong Chen",
      "Xin Geng",
      "Baining Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kundu_ProbRes_Probabilistic_Jump_Diffusion_for_Open-World_Egocentric_Activity_Recognition_ICCV_2025_paper.html": {
    "title": "ProbRes: Probabilistic Jump Diffusion for Open-World Egocentric Activity Recognition",
    "volume": "main",
    "abstract": "Open-world egocentric activity recognition poses a fundamental challenge due to its unconstrained nature, requiring models to infer unseen activities from an expansive, partially observed search space. We introduce ProbRes, a Probabilistic Residual search framework based on jump-diffusion that efficiently navigates this space by balancing prior-guided exploration with likelihood-driven exploitation. Our approach integrates structured commonsense priors to construct a semantically coherent search space, adaptively refines predictions using Vision-Language Models (VLMs) and employs a stochastic search mechanism to locate high-likelihood activity labels while minimizing exhaustive enumeration efficiently. We systematically evaluate ProbRes across multiple openness levels (L0-L3), demonstrating its adaptability to increasing search space complexity. In addition to achieving state-of-the-art performance on benchmark datasets (GTEA Gaze, GTEA Gaze+, EPIC-Kitchens, and Charades-Ego), we establish a clear taxonomy for open-world recognition, delineating the challenges and methodological advancements necessary for egocentric activity understanding. Our results highlight the importance of structured search strategies, paving the way for scalable and efficient open-world activity recognition",
    "checked": true,
    "id": "2bf66320a7dae428c1f017e889d1782db1922e71",
    "semantic_title": "probres: probabilistic jump diffusion for open-world egocentric activity recognition",
    "citation_count": 2,
    "authors": [
      "Sanjoy Kundu",
      "Shanmukha Vellamcheti",
      "Sathyanarayanan N. Aakur"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Information_Density_Principle_for_MLLM_Benchmarks_ICCV_2025_paper.html": {
    "title": "Information Density Principle for MLLM Benchmarks",
    "volume": "main",
    "abstract": "With the emergence of Multimodal Large Language Models (MLLMs), hundreds of benchmarks have been developed to ensure the reliability of MLLMs in downstream tasks. However, the evaluation mechanism itself may not be reliable. For developers of MLLMs, questions remain about which benchmark to use and whether the test results meet their requirements. Therefore, we propose a critical principle of Information Density, which examines **how much insight a benchmark can provide for the development of MLLMs.** We characterize it from four key dimensions: (1) Fallacy, (2) Difficulty, (3) Redundancy, (4) Diversity. Through a comprehensive analysis of more than 10,000 samples, we measured the information density of 19 MLLM benchmarks. Experiments show that using the latest benchmarks in testing can provide more insight compared to previous ones, but there is still room for improvement in their information density. We hope this principle can promote the development and application of future MLLM benchmarks",
    "checked": true,
    "id": "d47263cab61fde9503c127745c4e0ec527f21870",
    "semantic_title": "information density principle for mllm benchmarks",
    "citation_count": 3,
    "authors": [
      "Chunyi Li",
      "Xiaozhe Li",
      "Zicheng Zhang",
      "Yuan Tian",
      "Ziheng Jia",
      "Xiaohong Liu",
      "Xiongkuo Min",
      "Jia Wang",
      "Haodong Duan",
      "Kai Chen",
      "Guangtao Zhai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Dunkel_CNS-Bench_Benchmarking_Image_Classifier_Robustness_Under_Continuous_Nuisance_Shifts_ICCV_2025_paper.html": {
    "title": "CNS-Bench: Benchmarking Image Classifier Robustness Under Continuous Nuisance Shifts",
    "volume": "main",
    "abstract": "An important challenge when using computer vision models in the real world is to evaluate their performance in potential out-of-distribution (OOD) scenarios. While simple synthetic corruptions are commonly applied to test OOD robustness, they often fail to capture nuisance shifts that occur in the real world. Recently, diffusion models have been applied to generate realistic images for benchmarking, but they are restricted to binary nuisance shifts. In this work, we introduce CNS-Bench, a Continuous Nuisance Shift Benchmark to quantify OOD robustness of image classifiers for continuous and realistic generative nuisance shifts. CNS-Bench allows generating a wide range of individual nuisance shifts in continuous severities by applying LoRA adapters to diffusion models. To address failure cases, we propose a filtering mechanism that outperforms previous methods, thereby enabling reliable benchmarking with generative models. With the proposed benchmark, we perform a large-scale study to evaluate the robustness of more than 40 classifiers under various nuisance shifts. Through carefully designed comparisons and analyses, we find that model rankings can change for varying shifts and shift scales, which cannot be captured when applying common binary shifts. Additionally, we show that evaluating the model performance on a continuous scale allows the identification of model failure points, providing a more nuanced understanding of model robustness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Olaf Dünkel",
      "Artur Jesslen",
      "Jiahao Xie",
      "Christian Theobalt",
      "Christian Rupprecht",
      "Adam Kortylewski"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gou_Knowledge-Guided_Part_Segmentation_ICCV_2025_paper.html": {
    "title": "Knowledge-Guided Part Segmentation",
    "volume": "main",
    "abstract": "In real-world scenarios, objects and their parts inherently possess both coarse-grained differences and intricate fine-grained structural relationships. These characteristics can be formalized as knowledge, leveraged for fine-grained part comprehension. However, existing part segmentation models consistently fail to capture these complex inter-part relationships, treating parts as independent entities and disregarding object-level distinctions. To address these limitations, we propose a novel Knowledge-Guided Part Segmentation (KPS) framework. Our approach automatically extracts structural relationships between parts using a large language model (LLM) and integrates them into a knowledge graph. Subsequently, a structural knowledge guidance module employs a graph convolutional network (GCN) to model these relationships. Furthermore, a coarse-grained object guidance module captures object-specific distinctions and integrates them as visual guidance. The integrated insights from the part structure and object differentiation guide the fine-grained part segmentation. Our KPS achieves notable improvements in segmentation performance, with a 4.96% mIoU gain on PartImageNet and a 3.73% gain on Pascal-Part. Moreover, in the open-vocabulary setting on Pascal-Part-116, it improves hIoU by 3.25%, highlighting the effectiveness of knowledge guidance in enhancing fine-grained part segmentation",
    "checked": false,
    "id": "17e9f03051788e9814616f3397bc9389138d554b",
    "semantic_title": "knowledge-guided prompt learning for lifespan brain mr image segmentation",
    "citation_count": 4,
    "authors": [
      "Xuejian Gou",
      "Fang Liu",
      "Licheng Jiao",
      "Shuo Li",
      "Lingling Li",
      "Hao Wang",
      "Xu Liu",
      "Puhua Chen",
      "Wenping Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Mall_CRAM_Large_Scale_Video_Continual_Learning_with_Bootstrapped_Compression_ICCV_2025_paper.html": {
    "title": "CRAM: Large Scale Video Continual Learning with Bootstrapped Compression",
    "volume": "main",
    "abstract": "Continual learning (CL) promises to allow neural networks to learn from continuous streams of inputs, instead of IID (independent and identically distributed) sampling, which requires random access to a full dataset. This would allow for much smaller storage requirements and self-sufficiency of deployed systems that cope with natural distribution shifts, similarly to biological learning.We focus on video CL employing a rehearsal-based approach, which reinforces past samples from a memory buffer. We posit that part of the reason why practical video CL is challenging is the high memory requirements of video, further exacerbated by long-videos and continual streams, which are at odds with the common rehearsal-buffer size constraints. To address this, we propose to use compressed vision, i.e. store video codes (embeddings) instead of raw inputs, and train a video classifier by IID sampling from this rolling buffer. Training a video compressor online (so not depending on any pre-trained networks) means that it is also subject to catastrophic forgetting. We propose a scheme to deal with this forgetting by refreshing video codes, which requires careful decompression with a previous version of the network and recompression with a new one. We name our method Continually Refreshed Amodal Memory (CRAM). We expand current video CL benchmarks to large-scale settings, namely EpicKitchens-100 and Kinetics-700, with thousands of relatively long videos, and demonstrate empirically that our video CL method outperforms prior art with a significantly reduced memory footprint",
    "checked": false,
    "id": "d5252e01598b18e5573414f06758dc38af8d6a1e",
    "semantic_title": "cram: large-scale video continual learning with bootstrapped compression",
    "citation_count": 0,
    "authors": [
      "Shivani Mall",
      "Joao F. Henriques"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kreutz_DeSPITE_Exploring_Contrastive_Deep_Skeleton-Pointcloud-IMU-Text_Embeddings_for_Advanced_Point_Cloud_ICCV_2025_paper.html": {
    "title": "DeSPITE: Exploring Contrastive Deep Skeleton-Pointcloud-IMU-Text Embeddings for Advanced Point Cloud Human Activity Understanding",
    "volume": "main",
    "abstract": "Despite LiDAR (Light Detection and Ranging) being an effective privacy-preserving alternative to RGB cameras to perceive human activities, it remains largely underexplored in the context of multi-modal contrastive pre-training for human activity understanding tasks, such as human activity recognition (HAR), retrieval, or person re-identification (RE-ID). To close this gap, our work explores learning the correspondence between LiDAR point clouds, human skeleton poses, IMU data, and text in a joint embedding space. More specifically, we present DeSPITE, a \\underline De ep \\underline S keleton-\\underline P ointcloud-\\underline I MU-\\underline T ext \\underline E mbedding model, which effectively learns a joint embedding space across these four modalities. At the heart of our empirical exploration, we have combined the existing LIPD and Babel datasets, which enabled us to synchronize data of all four modalities, allowing us to explore the learning of a new joint embedding space. Our experiments demonstrate novel human activity understanding tasks for point cloud sequences enabled through DeSPITE, including Skeleton\\leftrightarrowPointcloud\\leftrightarrowIMU matching, retrieval, and temporal moment retrieval. Furthermore, we show that DeSPITE is an effective pre-training strategy for point cloud HAR through experiments in MSR-Action3D and HMPEAR. Code and models are publicly available at https://github.com/thkreutz/despite",
    "checked": true,
    "id": "f2f4342e37efbe33b1571d072907b6e7ab9c62d9",
    "semantic_title": "despite: exploring contrastive deep skeleton-pointcloud-imu-text embeddings for advanced point cloud human activity understanding",
    "citation_count": 0,
    "authors": [
      "Thomas Kreutz",
      "Max Mühlhäuser",
      "Alejandro Sanchez Guinea"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Deng_Leveraging_Panoptic_Scene_Graph_for_Evaluating_Fine-Grained_Text-to-Image_Generation_ICCV_2025_paper.html": {
    "title": "Leveraging Panoptic Scene Graph for Evaluating Fine-Grained Text-to-Image Generation",
    "volume": "main",
    "abstract": "Text-to-image (T2I) models have advanced rapidly with diffusion-based breakthroughs, yet their evaluation remains challenging. Human assessments are costly, and existing automated metrics lack accurate compositional understanding. To address these limitations, we introduce PSG-Bench, a novel benchmark featuring 5K text prompts designed to evaluate the capabilities of advanced T2I models. Additionally, we propose PSGEval, a scene graph-based evaluation metric that converts generated images into structured representations and applies graph matching techniques for accurate and scalable assessment. PSGEval is a detection based evaluation metric without relying on QA generations. Our experimental results demonstrate that PSGEval aligns well with human evaluations, mitigating biases present in existing automated metrics. We further provide a detailed ranking and analysis of recent T2I models, offering a robust framework for future research in T2I evaluation",
    "checked": false,
    "id": "d3de65575c063a1a5d843b78672a6908aa0b10b3",
    "semantic_title": "opensgen: fine-grained relation-aware prompt for open-vocabulary scene graph generation",
    "citation_count": 0,
    "authors": [
      "Xueqing Deng",
      "Linjie Yang",
      "Qihang Yu",
      "Chenglin Yang",
      "Liang-Chieh Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wei_Noise2Score3D_Tweedies_Approach_for_Unsupervised_Point_Cloud_Denoising_ICCV_2025_paper.html": {
    "title": "Noise2Score3D: Tweedie's Approach for Unsupervised Point Cloud Denoising",
    "volume": "main",
    "abstract": "Building on recent advances in Bayesian statistics and image denoising, we propose Noise2Score3D, a fully unsupervised framework for point cloud denoising. Noise2Score3D learns the score function of the underlying point cloud distribution directly from noisy data, eliminating the need for clean data during training. Using Tweedie's formula, our method performs denoising in a single step, avoiding the iterative processes used in existing unsupervised methods, thus improving both accuracy and efficiency. Additionally, we introduce Total Variation for Point Clouds as a denoising quality metric, which allows for the estimation of unknown noise parameters. Experimental results demonstrate that Noise2Score3D achieves state-of-the-art performance on standard benchmarks among unsupervised learning methods in Chamfer distance and point-to-mesh metrics. Noise2Score3D also demonstrates strong generalization ability beyond training datasets. Our method, by addressing the generalization issue and challenge of the absence of clean data in learning-based methods, paves the way for learning-based point cloud denoising methods in real-world applications",
    "checked": true,
    "id": "b8da393b07eb12c38688cc9b35f790205a9ef281",
    "semantic_title": "noise2score3d: tweedie's approach for unsupervised point cloud denoising",
    "citation_count": 2,
    "authors": [
      "Xiangbin Wei",
      "Yuanfeng Wang",
      "Ao Xu",
      "Lingyu Zhu",
      "Dongyong Sun",
      "Keren Li",
      "Yang Li",
      "Qi Qin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liao_I2-World_Intra-Inter_Tokenization_for_Efficient_Dynamic_4D_Scene_Forecasting_ICCV_2025_paper.html": {
    "title": "I2-World: Intra-Inter Tokenization for Efficient Dynamic 4D Scene Forecasting",
    "volume": "main",
    "abstract": "Forecasting the evolution of 3D scenes and generating unseen scenarios through occupancy-based world models offers substantial potential to enhance the safety of autonomous driving systems. While tokenization has revolutionized image and video generation, efficiently tokenizing complex 3D scenes remains a critical challenge for 3D world models. To address this, we propose I^ 2 -World, an efficient framework for 4D occupancy forecasting. Our method decouples scene tokenization into intra-scene and inter-scene tokenizers. The intra-scene tokenizer employs a multi-scale residual quantization strategy to hierarchically compress 3D scenes while preserving spatial details. The inter-scene tokenizer residually aggregates temporal dependencies across timesteps. This dual design retains the compactness of 3D tokenizers while capturing the dynamic expressiveness of 4D approaches. Unlike decoder-only GPT-style autoregressive models, I^ 2 -World adopts an encoder-decoder architecture. The encoder aggregates spatial context from the current scene and predicts a transformation matrix to guide future scene generation. The decoder, conditioned on this matrix and historical tokens, ensures temporal consistency during generation. Experiments demonstrate that I^ 2 -World achieves state-of-the-art performance, surpassing existing approaches by 41.8% in 4D occupancy forecasting with exceptional efficiency--requiring only 2.9 GB of training memory and achieving real-time inference at 94.8 FPS",
    "checked": true,
    "id": "5f7a5167a9a17a3c2d82de827657ee5f9eda773d",
    "semantic_title": "i2-world: intra-inter tokenization for efficient dynamic 4d scene forecasting",
    "citation_count": 1,
    "authors": [
      "Zhimin Liao",
      "Ping Wei",
      "Ruijie Zhang",
      "Shuaijia Chen",
      "Haoxuan Wang",
      "Ziyang Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gao_Meta-Unlearning_on_Diffusion_Models_Preventing_Relearning_Unlearned_Concepts_ICCV_2025_paper.html": {
    "title": "Meta-Unlearning on Diffusion Models: Preventing Relearning Unlearned Concepts",
    "volume": "main",
    "abstract": "With the rapid progress of diffusion models (DMs), significant efforts are being made to unlearn harmful or copyrighted concepts from pretrained DMs to prevent potential model misuse. However, it is observed that even when DMs are properly unlearned before release, malicious finetuning can compromise this process, causing DMs to relearn the unlearned concepts. This occurs partly because certain benign concepts (e.g., \"skin\") retained in DMs are related to the unlearned ones (e.g., \"nudity\"), facilitating their relearning via finetuning. To address this, we propose meta-unlearning on DMs. Intuitively, a meta-unlearned DM should behave like an unlearned DM when used as is; moreover, if the meta-unlearned DM undergoes malicious finetuning on unlearned concepts, the related benign concepts retained within it will be triggered to self-destruct, hindering the relearning of unlearned concepts. Our meta-unlearning framework is compatible with most existing unlearning methods, requiring only the addition of an easy-to-implement meta objective. We validate our approach through empirical experiments on meta-unlearning concepts from Stable Diffusion models (SD-v1-4 and SDXL), supported by extensive ablation studies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongcheng Gao",
      "Tianyu Pang",
      "Chao Du",
      "Taihang Hu",
      "Zhijie Deng",
      "Min Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ulger_Auto-Vocabulary_Semantic_Segmentation_ICCV_2025_paper.html": {
    "title": "Auto-Vocabulary Semantic Segmentation",
    "volume": "main",
    "abstract": "Open-Vocabulary Segmentation (OVS) methods are capable of performing semantic segmentation without relying on a fixed vocabulary, and in some cases, without training or fine-tuning. However, OVS methods typically require a human in the loop to specify the vocabulary based on the task or dataset at hand. In this paper, we introduce Auto-Vocabulary Semantic Segmentation (AVS), advancing open-ended image understanding by eliminating the necessity to predefine object categories for segmentation. Our approach, AutoSeg, presents a framework that autonomously identifies relevant class names using semantically enhanced BLIP embeddings and segments them afterwards. Given that open-ended object category predictions cannot be directly compared with a fixed ground truth, we develop a Large Language Model-based Auto-Vocabulary Evaluator (LAVE) to efficiently evaluate the automatically generated classes and their corresponding segments. With AVS, our method sets new benchmarks on datasets PASCAL VOC, Context, ADE20K, and Cityscapes, while showing competitive performance to OVS methods that require specified class names. All code is released at https://github.com/ozzyou/AutoSeg",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Osman Ülger",
      "Maksymilian Kulicki",
      "Yuki Asano",
      "Martin R. Oswald"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Galanakis_SpinMeRound_Consistent_Multi-View_Identity_Generation_Using_Diffusion_Models_ICCV_2025_paper.html": {
    "title": "SpinMeRound: Consistent Multi-View Identity Generation Using Diffusion Models",
    "volume": "main",
    "abstract": "Despite recent progress in diffusion models, generating realistic head portraits from novel viewpoints remains a significant challenge in computer vision. Most current approaches are constrained to limited angular ranges, predominantly focusing on frontal or near-frontal views. Moreover, although the recent emerging large-scale diffusion models have been proven robust in handling 3D scenes, they underperform on facial data, given their complex structure and the uncanny valley pitfalls. In this paper, we propose SpinMeRound, a diffusion-based approach designed to generate consistent and accurate head portraits from novel viewpoints. By leveraging a number of input views alongside an identity embedding, our method effectively synthesizes diverse viewpoints of a subject whilst robustly maintaining its unique identity features. Through experimentation, we showcase our model's generation capabilities in full head synthesis, while beating current state-of-the-art multi-view diffusion models",
    "checked": true,
    "id": "23d34f1d6eb41398aefd1d930b25c1d1dfdf0791",
    "semantic_title": "spinmeround: consistent multi-view identity generation using diffusion models",
    "citation_count": 0,
    "authors": [
      "Stathis Galanakis",
      "Alexandros Lattas",
      "Stylianos Moschoglou",
      "Bernhard Kainz",
      "Stefanos Zafeiriou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_MSA2_Multi-task_Framework_with_Structure-aware_and_Style-adaptive_Character_Representation_for_ICCV_2025_paper.html": {
    "title": "MSA2: Multi-task Framework with Structure-aware and Style-adaptive Character Representation for Open-set Chinese Text Recognition",
    "volume": "main",
    "abstract": "Most existing methods regard open-set Chinese text recognition (CTR) as a single-task problem, primarily focusing on prototype learning of linguistic components or glyphs to identify unseen characters. In contrast, humans identify characters by integrating multiple perspectives, including linguistic and visual cues. Inspired by this, we propose a multi-task framework termed MSA^2, which considers multi-view character representations for open-set CTR. Within MSA^2, we introduce two novel strategies for character representation: structure-aware component encoding (SACE) and style-adaptive glyph embedding (SAGE). SACE utilizes a binary tree with dynamic representation space to emphasize the primary linguistic components, thereby generating structure-aware and discriminative linguistic representations for each character. Meanwhile, SAGE employs a glyph-centric contrastive learning to aggregate features from diverse forms, yielding robust glyph representations for the CTR model to adapt to the style variations among various fonts. Extensive experiments demonstrate that our proposed MSA^2 outperforms state-of-the-art CTR methods, achieving an average improvement of 1.3% and 6.0% in accuracy under closed-set and open-set settings on the BCTR dataset, respectively. The code will be available soon",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yangfu Li",
      "Hongjian Zhan",
      "Qi Liu",
      "Li Sun",
      "Yu-Jie Xiong",
      "Yue Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Guo_IGL-Nav_Incremental_3D_Gaussian_Localization_for_Image-goal_Navigation_ICCV_2025_paper.html": {
    "title": "IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation",
    "volume": "main",
    "abstract": "Visual navigation with an image as goal is a fundamental and challenging problem. Conventional methods either rely on end-to-end RL learning or modular-based policy with topological graph or BEV map as memory, which cannot fully model the geometric relationship between the explored 3D environment and the goal image. In order to efficiently and accurately localize the goal image in 3D space, we build our navigation system upon the renderable 3D gaussian (3DGS) representation. However, due to the computational intensity of 3DGS optimization and the large search space of 6-DoF camera pose, directly leveraging 3DGS for image localization during agent exploration process is prohibitively inefficient. To this end, we propose IGL-Nav, an Incremental 3D Gaussian Localization framework for efficient and 3D-aware image-goal navigation. Specifically, we incrementally update the scene representation as new images arrive with feed-forward monocular prediction. Then we coarsely localize the goal by leveraging the geometric information for discrete space matching, which can be equivalent to efficient 3D convolution. When the agent is close to the goal, we finally solve the fine target pose with optimization via differentiable rendering. The proposed IGL-Nav outperforms existing state-of-the-art methods by a large margin across diverse experimental configurations. It can also handle the more challenging free-view image-goal setting and be deployed on real-world robotic platform using a cellphone to capture goal image at arbitrary pose. Project page: https://gwxuan.github.io/IGL-Nav/",
    "checked": true,
    "id": "125f00e60943a7dadc2ff22ed5bce4bd071216f3",
    "semantic_title": "igl-nav: incremental 3d gaussian localization for image-goal navigation",
    "citation_count": 1,
    "authors": [
      "Wenxuan Guo",
      "Xiuwei Xu",
      "Hang Yin",
      "Ziwei Wang",
      "Jianjiang Feng",
      "Jie Zhou",
      "Jiwen Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wei_PCR-GS_COLMAP-Free_3D_Gaussian_Splatting_via_Pose_Co-Regularizations_ICCV_2025_paper.html": {
    "title": "PCR-GS: COLMAP-Free 3D Gaussian Splatting via Pose Co-Regularizations",
    "volume": "main",
    "abstract": "COLMAP-free 3D Gaussian Splatting (3D-GS) has recently attracted increasing attention due to its remarkable performance in reconstructing high-quality 3D scenes from unposed images or videos. However, it often struggles to handle scenes with complex camera trajectories as featured by drastic rotation and translation across adjacent camera views, leading to degraded estimation of camera poses and further local minima in joint optimization of camera poses and 3D-GS. We propose PCR-GS, an innovative COLMAP-free 3DGS technique that achieves superior 3D scene modeling and camera pose estimation via camera pose co-regularization. PCR-GS achieves regularization from two perspectives. The first is feature reprojection regularization which extracts view-robust DINO features from adjacent camera views and aligns their semantic information for camera pose regularization. The second is wavelet-based frequency regularization which exploits discrepancy in high-frequency details to further optimize the rotation matrix in camera poses. Extensive experiments over multiple real-world scenes show that the proposed PCR-GS achieves superior pose-free 3D-GS scene modeling under dramatic changes of camera trajectories",
    "checked": true,
    "id": "49256952e2243cf646e11a6c16224dec3f760922",
    "semantic_title": "pcr-gs: colmap-free 3d gaussian splatting via pose co-regularizations",
    "citation_count": 1,
    "authors": [
      "Yu Wei",
      "Jiahui Zhang",
      "Xiaoqin Zhang",
      "Ling Shao",
      "Shijian Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Su_EA-KD_Entropy-based_Adaptive_Knowledge_Distillation_ICCV_2025_paper.html": {
    "title": "EA-KD: Entropy-based Adaptive Knowledge Distillation",
    "volume": "main",
    "abstract": "Knowledge distillation (KD) enables a smaller \"student\" model to mimic a larger \"teacher\" model by transferring knowledge from the teacher's output or features. However, most KD methods treat all samples uniformly, overlooking the varying learning value of each sample and thereby limiting effectiveness. In this paper, we propose Entropy-based Adaptive Knowledge Distillation (EA-KD), a simple yet effective plug-and-play KD method that prioritizes learning from valuable samples. EA-KD quantifies each sample's learning value by strategically combining the entropy of the teacher and student output, then dynamically reweights the distillation loss to place greater emphasis on high-entropy samples. Extensive experiments across diverse KD frameworks and tasks--including image classification, object detection, and large language model (LLM) distillation--demonstrate that EA-KD consistently enhances performance, achieving state-of-the-art results with negligible computational cost. Our code is available at: https://github.com/cpsu00/EA-KD",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chi-Ping Su",
      "Ching-Hsun Tseng",
      "Bin Pu",
      "Lei Zhao",
      "Jiewen Yang",
      "Zhuangzhuang Chen",
      "Shin-Jye Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zheng_D3_Training-Free_AI-Generated_Video_Detection_Using_Second-Order_Features_ICCV_2025_paper.html": {
    "title": "D3: Training-Free AI-Generated Video Detection Using Second-Order Features",
    "volume": "main",
    "abstract": "The evolution of video generation techniques, such as Sora, has made it increasingly easy to produce high-fidelity AI-generated videos, raising public concern over the dissemination of synthetic content. However, existing detection methodologies remain limited by their insufficient exploration of temporal artifacts in synthetic videos. To bridge this gap, we establish a theoretical framework through second-order dynamical analysis under Newtonian mechanics, subsequently extending the Second-order Central Difference features tailored for temporal artifact detection. Building on this theoretical foundation, we reveal a fundamental divergence in second-order feature distributions between real and AI-generated videos. Concretely, we propose Detection by Difference of Differences (D3), a novel training-free detection method that leverages the above second-order temporal discrepancies. We validate the superiority of our D3 on 4 open-source datasets (Gen-Video, VideoPhy, EvalCrafter, VidProM), 40 subsets in total. For example, on GenVideo, D3 outperforms the previous best method by 10.39% (absolute) mean Average Precision. Additional experiments on time cost and post-processing operations demonstrate D3's exceptional computational efficiency and strong robust performance. Our code is available at https://github.com/Zig-HS/D3",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chende Zheng",
      "Ruiqi Suo",
      "Chenhao Lin",
      "Zhengyu Zhao",
      "Le Yang",
      "Shuai Liu",
      "Minghui Yang",
      "Cong Wang",
      "Chao Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liao_Background_Invariance_Testing_According_to_Semantic_Proximity_ICCV_2025_paper.html": {
    "title": "Background Invariance Testing According to Semantic Proximity",
    "volume": "main",
    "abstract": "In many applications, machine-learned (ML) models are required to hold some invariance qualities, such as rotation, size, and intensity invariance. Among these, testing for background invariance presents a significant challenge due to the vast and complex data space it encompasses. To evaluate invariance qualities, we first use a visualization-based testing framework which allows human analysts to assess and make informed decisions about the invariance properties of ML models. We show that such informative testing framework is preferred as ML models with the same global statistics (e.g., accuracy scores) can behave differently and have different visualized testing patterns. However, such human analysts might not lead to consistent decisions without a systematic sampling approach to select representative testing suites. In this work, we present a technical solution for selecting background scenes according to their semantic proximity to a target image that contains a foreground object being tested. We construct an ontology for storing knowledge about relationships among different objects using association analysis. This ontology enables an efficient and meaningful search for background scenes of different semantic distances to a target image, enabling the selection of a test suite that is both diverse and reasonable. Compared with other testing techniques, e.g., random sampling, nearest neighbors, or other sampled test suites by visual-language models (VLMs), our method achieved a superior balance between diversity and consistency of human annotations, thereby enhancing the reliability and comprehensiveness of background invariance testing",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zukang Liao",
      "Min Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jeong_Resolving_Token-Space_Gradient_Conflicts_Token_Space_Manipulation_for_Transformer-Based_Multi-Task_ICCV_2025_paper.html": {
    "title": "Resolving Token-Space Gradient Conflicts: Token Space Manipulation for Transformer-Based Multi-Task Learning",
    "volume": "main",
    "abstract": "Multi-Task Learning (MTL) enables multiple tasks to be learned within a shared network, but differences in objectives across tasks can cause negative transfer, where the learning of one task degrades another task's performance. While pre-trained transformers significantly improve MTL performance, their fixed network capacity and rigid structure limit adaptability. Previous dynamic network architectures attempt to address this but are inefficient as they directly convert shared parameters into task-specific ones. We propose Dynamic Token Modulation and Expansion (DTME-MTL), a framework applicable to any transformer-based MTL architecture. DTME-MTL enhances adaptability and reduces overfitting by identifying gradient conflicts in token space and applying adaptive solutions based on conflict type. Unlike prior methods that mitigate negative transfer by duplicating network parameters, DTME-MTL operates entirely in token space, enabling efficient adaptation without excessive parameter growth. Extensive experiments demonstrate that DTME-MTL consistently improves multi-task performance with minimal computational overhead, offering a scalable and effective solution for enhancing transformer-based MTL models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wooseong Jeong",
      "Kuk-Jin Yoon"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_ViT-Split_Unleashing_the_Power_of_Vision_Foundation_Models_via_Efficient_ICCV_2025_paper.html": {
    "title": "ViT-Split: Unleashing the Power of Vision Foundation Models via Efficient Splitting Heads",
    "volume": "main",
    "abstract": "Vision foundation models (VFMs) have demonstrated remarkable performance across a wide range of downstream tasks. While several VFM adapters have shown promising results by leveraging the prior knowledge of VFMs, we identify two inefficiencies in these approaches. First, the interaction between convolutional neural network (CNN) and VFM backbone triggers early layer gradient backpropagation. Second, existing methods require tuning all components, adding complexity. Besides, these adapters alter VFM features, underutilizing the prior knowledge. To tackle these challenges, we propose a new approach called ViT-Split, based on a key observation: the layers of several VFMs, like DINOv2, can be divided into two distinct components: an extractor for learning low-level features and an adapter for learning task-specific features. Leveraging this insight, we eliminate the CNN branch and introduce two heads, task head and prior head, to the frozen VFM. The task head is designed to learn task-specific features, mitigating the early gradient propagation issue. The prior head is used to leverage the multi-scale prior features from the frozen VFM, reducing tuning parameters and overfitting. Extensive experiments on various tasks (e.g., segmentation, detection, depth estimation, and visual question answering) validate the effectiveness and efficiency of ViT-Split. Specifically, ViT-Split reduces training time up to 4xwhile achieving comparable or even better results on ADE20K, compared to other VFM adapters",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Li",
      "Xin Li",
      "Tianqin Li",
      "Wenbin He",
      "Yu Kong",
      "Liu Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Fusion_Meets_Diverse_Conditions_A_High-diversity_Benchmark_and_Baseline_for_ICCV_2025_paper.html": {
    "title": "Fusion Meets Diverse Conditions: A High-diversity Benchmark and Baseline for UAV-based Multimodal Object Detection with Condition Cues",
    "volume": "main",
    "abstract": "Unmanned aerial vehicles (UAV)-based object detection with visible (RGB) and infrared (IR) images facilitates robust around-the-clock detection, driven by advancements in deep learning techniques and the availability of high-quality dataset. However, the existing dataset struggles to fully capture real-world complexity for limited imaging conditions. To this end, we introduce a high-diversity dataset ATR-UMOD covering varying scenarios, spanning altitudes from 80m to 300m, angles from 0deg to 75deg, and all-day, all-year time variations in rich weather and illumination conditions. Moreover, each RGB-IR image pair is annotated with 6 condition attributes, offering valuable high-level contextual information. To meet the challenge raised by such diverse conditions, we propose a novel prompt-guided condition-aware dynamic fusion (PCDF) to adaptively reassign multimodal contributions by leveraging annotated condition cues. By encoding imaging conditions as text prompts, PCDF effectively models the relationship between conditions and multimodal contributions through a task-specific soft-gating transformation. A prompt-guided condition-decoupling module further ensures the availability in practice without condition annotations. Experiments on ATR-UMOD dataset reveal the effectiveness of PCDF",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Chen",
      "Kangcheng Bin",
      "Ting Hu",
      "Jiahao Qi",
      "Xingyue Liu",
      "Tianpeng Liu",
      "Zhen Liu",
      "Yongxiang Liu",
      "Ping Zhong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_Stealthy_Backdoor_Attack_in_Federated_Learning_via_Adaptive_Layer-wise_Gradient_ICCV_2025_paper.html": {
    "title": "Stealthy Backdoor Attack in Federated Learning via Adaptive Layer-wise Gradient Alignment",
    "volume": "main",
    "abstract": "The distributed nature of federated learning exposes it to significant security threats, among which backdoor attacks are one of the most prevalent. However, existing backdoor attacks face a trade-off between attack strength and stealthiness: attacks maximizing the attack strength are often detectable, while stealthier approaches significantly reduce the effectiveness of the attack itself. Both of them result in ineffective backdoor injection. In this paper, we propose an adaptive layer-wise gradient alignment strategy to effectively evade various robust defense mechanisms while preserving attack strength. Without requiring additional knowledge, we leverage the previous global update as a reference for alignment to ensure stealthiness during dynamic FL training. This fine-grained alignment strategy applies appropriate constraints to each layer, which helps significantly maintain attack strength. To demonstrate the effectiveness of our method, we conduct extensive evaluations across a wide range of datasets and networks. Our experimental results show that the proposed attack effectively bypasses eight state-of-the-art defenses and achieves high backdoor accuracy, outperforming existing attacks by up to 54.76%. Additionally, it significantly preserves attack strength and maintains robust performance across diverse scenarios, highlighting its adaptability and generalizability. Code implementation is available at https://github.com/yqqhyqq/LGA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingqian Yang",
      "Peishen Yan",
      "Xiaoyu Wu",
      "Jiaru Zhang",
      "Tao Song",
      "Yang Hua",
      "Hao Wang",
      "Liangliang Wang",
      "Haibing Guan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_Hybrid_Layout_Control_for_Diffusion_Transformer_Fewer_Annotations_Superior_Aesthetics_ICCV_2025_paper.html": {
    "title": "Hybrid Layout Control for Diffusion Transformer: Fewer Annotations, Superior Aesthetics",
    "volume": "main",
    "abstract": "Text-to-image generation models often struggle to interpret spatially aware text prompts effectively. To overcome this, existing approaches typically require millions of high-quality semantic layout annotations consisting of bounding boxes and regional prompts. This paper shows that the large amounts of regional prompts are non-necessary for the latest diffusion transformers like SD3 or FLUX.In this paper, we propose an efficient hybrid layout framework for diffusion transformers. Our approach drastically reduces need for extensive layout annotations and minimizes reliance on regional prompt annotations--incurring only minimal additional computational cost during inference--while maintaining high-quality layout adherence. Our key insight is to break the layout-control task into two sequential stages: first, generating the target objects within the designated regions specified by an anonymous layout, and second, refining these outputs to ensure they strictly adhere to the regional prompts in the semantic layout. Building on this insight, we propose a hybrid layout control scheme that first fine-tunes the DiTs (e.g., SD3) to follow an anonymous layout, then continues fine-tuning the DiTs to follow the semantic layout, and finally includes a quality-tuning stage to enhance visual aesthetics. We show that this hybrid design is highly data-efficient, as we find only using a small amount of semantic layout annotations is sufficient, thereby significantly reducing dependency on regional prompts. In addition, we propose an efficient regional diffusion transformer to encode the spatial layout information using just a set of lower-resolution regional tokens instead of various carefully designed layout tokens. The region-wise diffusion loss over these regional tokens can guide the diffusion transformer learn to follow the given layout implicitly. We empirically validate the effectiveness of our approach by comparing it with the latest version of SiamLayout and show that our method achieves better results while being more than 10 times more data efficient and ensuring superior aesthetics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keming Wu",
      "Junwen Chen",
      "Zhanhao Liang",
      "Yinuo Wang",
      "Ji Li",
      "Chao Zhang",
      "Bin Wang",
      "Yuhui Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Seo_BUFFER-X_Towards_Zero-Shot_Point_Cloud_Registration_in_Diverse_Scenes_ICCV_2025_paper.html": {
    "title": "BUFFER-X: Towards Zero-Shot Point Cloud Registration in Diverse Scenes",
    "volume": "main",
    "abstract": "Recent advances in deep learning-based point cloud registration have improved generalization, yet most methods still require retraining or manual parameter tuning for each new environment. In this paper, we identify three key factors limiting generalization: (a) reliance on environment-specific voxel size and search radius, (b) poor out-of-domain robustness of learning-based keypoint detectors,and (c) raw coordinate usage, which exacerbates scale discrepancies. To address these issues, we present a zero-shot registration pipeline called BUFFER-X by (a) adaptively determining voxel size/search radii, (b) using farthest point sampling to bypass learned detectors, and (c) leveraging patch-wise scale normalization for consistent coordinate bounds. In particular, we present a multi-scale patch-based descriptor generation and a hierarchical inlier search across scales to improve robustness in diverse scenes. We also propose a novel generalizability benchmark using 11 datasets that cover various indoor/outdoor scenarios and sensor modalities, demonstrating that BUFFER-X achieves substantial generalization without prior information or manual parameter tuning for the test datasets. Our code is available at https://github.com/MIT-SPARK/BUFFER-X",
    "checked": true,
    "id": "f0dfc8998180228c5effd4a7fe19cdf0a33d441d",
    "semantic_title": "buffer-x: towards zero-shot point cloud registration in diverse scenes",
    "citation_count": 2,
    "authors": [
      "Minkyun Seo",
      "Hyungtae Lim",
      "Kanghee Lee",
      "Luca Carlone",
      "Jaesik Park"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gorgun_VITAL_More_Understandable_Feature_Visualization_through_Distribution_Alignment_and_Relevant_ICCV_2025_paper.html": {
    "title": "VITAL: More Understandable Feature Visualization through Distribution Alignment and Relevant Information Flow",
    "volume": "main",
    "abstract": "Neural networks are widely adopted to solve complex and challenging tasks. Especially in high-stakes decision-making, understanding their reasoning process is crucial, yet proves challenging for modern deep networks. Feature visualization (FV) is a powerful tool to decode what information neurons are responding to and hence to better understand the reasoning behind such networks. In particular, in FV we generate human-understandable images that reflect the information detected by neurons of interest. However, current methods often yield unrecognizable visualizations, exhibiting repetitive patterns and visual artifacts that are hard to understand for a human. To address these problems, we propose to guide FV through **statistics of real image features** combined with measures of **relevant network flow** to generate prototypical images. Our approach yields human-understandable visualizations that both qualitatively and quantitatively improve over state-of-the-art FVs across various architectures. As such, it can be used to decode **which** information the network uses, complementing mechanistic circuits that identify **where** it is encoded",
    "checked": true,
    "id": "ae540e8cc44c78cf58d96767acc78107b9b660a9",
    "semantic_title": "vital: more understandable feature visualization through distribution alignment and relevant information flow",
    "citation_count": 0,
    "authors": [
      "Ada Görgün",
      "Bernt Schiele",
      "Jonas Fischer"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Caetano_DisCoPatch_Taming_Adversarially-driven_Batch_Statistics_for_Improved_Out-of-Distribution_Detection_ICCV_2025_paper.html": {
    "title": "DisCoPatch: Taming Adversarially-driven Batch Statistics for Improved Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "Out-of-distribution (OOD) detection holds significant importance across many applications. While semantic and domain-shift OOD problems are well-studied, this work focuses on covariate shifts - subtle variations in the data distribution that can degrade machine learning performance. We hypothesize that detecting these subtle shifts can improve our understanding of in-distribution boundaries, ultimately improving OOD detection. In adversarial discriminators trained with Batch Normalization (BN), real and adversarial samples form distinct domains with unique batch statistics - a property we exploit for OOD detection. We introduce DisCoPatch, an unsupervised Adversarial Variational Autoencoder (VAE) framework that harnesses this mechanism. During inference, batches consist of patches from the same image, ensuring a consistent data distribution that allows the model to rely on batch statistics. DisCoPatch uses the VAE's suboptimal outputs (generated and reconstructed) as negative samples to train the discriminator, thereby improving its ability to delineate the boundary between in-distribution samples and covariate shifts. By tightening this boundary, DisCoPatch achieves state-of-the-art results in public OOD detection benchmarks. The proposed model not only excels in detecting covariate shifts, achieving 95.5% AUROC on ImageNet-1K(-C), but also outperforms all prior methods on public Near-OOD (95.0%) benchmarks. With a compact model size of 25MB, it achieves high OOD detection performance at notably lower latency than existing methods, making it an efficient and practical solution for real-world OOD detection applications",
    "checked": true,
    "id": "a7bb9ed35126caef3262743182bb92ed10aa294d",
    "semantic_title": "discopatch: taming adversarially-driven batch statistics for improved out-of-distribution detection",
    "citation_count": 0,
    "authors": [
      "Francisco Caetano",
      "Christiaan Viviers",
      "Luis A. Zavala-Mondragón",
      "Peter H.N. De With",
      "Fons van der Sommen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_EC-Flow_Enabling_Versatile_Robotic_Manipulation_from_Action-Unlabeled_Videos_via_Embodiment-Centric_ICCV_2025_paper.html": {
    "title": "EC-Flow: Enabling Versatile Robotic Manipulation from Action-Unlabeled Videos via Embodiment-Centric Flow",
    "volume": "main",
    "abstract": "Current language-guided robotic manipulation systems often require low-level action-labeled datasets for imitation learning. While object-centric flow prediction methods mitigate this issue, they remain limited to scenarios involving rigid objects with clear displacement and minimal occlusion. In this work, we present Embodiment-Centric Flow (EC-Flow), a framework that directly learns manipulation from action-unlabeled videos by predicting embodiment-centric flow. Our key insight is that incorporating the embodiment's inherent kinematics significantly enhances generalization to versatile manipulation scenarios, including deformable object handling, occlusions, and non-object-displacement tasks. To connect the EC-Flow with language instructions and object interactions, we further introduce a goal-alignment module by jointly optimizing movement consistency and goal-image prediction. Moreover, translating EC-Flow to executable robot actions only requires a standard robot URDF (Unified Robot Description Format) file to specify kinematic constraints across joints, which makes it easy to use in practice. We validate EC-Flow on both simulation (Meta-World) and real-world tasks, demonstrating its state-of-the-art performance in occluded object handling (62% improvement), deformable object manipulation (45% improvement), and non-object-displacement tasks (80% improvement) than prior state-of-the-art object-centric flow methods. More results can be found on our project website: https://ec-flow1.github.io/",
    "checked": true,
    "id": "ccf013b8feab4a2581dbad5fe5fe60c083abd8ea",
    "semantic_title": "ec-flow: enabling versatile robotic manipulation from action-unlabeled videos via embodiment-centric flow",
    "citation_count": 0,
    "authors": [
      "Yixiang Chen",
      "Peiyan Li",
      "Yan Huang",
      "Jiabing Yang",
      "Kehan Chen",
      "Liang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kwon_One_Look_is_Enough_Seamless_Patchwise_Refinement_for_Zero-Shot_Monocular_ICCV_2025_paper.html": {
    "title": "One Look is Enough: Seamless Patchwise Refinement for Zero-Shot Monocular Depth Estimation on High-Resolution Images",
    "volume": "main",
    "abstract": "Zero-shot depth estimation (DE) models exhibit strong generalization performance as they are trained on large-scale datasets. However, existing models struggle with high-resolution images due to the discrepancy in image resolutions of training (with smaller resolutions) and inference (for high resolutions). Processing them at full resolution leads to decreased estimation accuracy on depth with tremendous memory consumption, while downsampling to the training resolution results in blurred edges in the estimated depth images. Prevailing high-resolution depth estimation methods adopt a patch-based approach, which introduces depth discontinuity issues when reassembling the estimated depth patches, resulting in test-time inefficiency. Additionally, to obtain fine-grained depth details, these methods rely on synthetic datasets due to the real-world sparse ground truth depth, leading to poor generalizability. To tackle these limitations, we propose Patch Refine Once (PRO), an efficient and generalizable tile-based framework. Our PRO consists of two key components: (i) Grouped Patch Consistency Training that enhances test-time efficiency while mitigating the depth discontinuity problem by jointly processing four overlapping patches and enforcing a consistency loss on their overlapping regions within a single backpropagation step, and (ii) Bias Free Masking that prevents the DE models from overfitting to dataset-specific biases, enabling better generalization to real-world datasets even after training on synthetic data. Zero-shot evaluations on Booster, ETH3D, Middlebury 2014, and NuScenes demonstrate that our PRO can be seamlessly integrated into existing depth estimation models. It preserves the performance of original depth estimation models even under grid-based inference on high-resolution images, exhibiting minimal depth discontinuities along patch boundaries. Moreover, our PRO achieves significantly faster inference speed compared to prior patch-based methods",
    "checked": true,
    "id": "127936df0883febb3ba06c09e425e56ecc4e79c3",
    "semantic_title": "one look is enough: seamless patchwise refinement for zero-shot monocular depth estimation on high-resolution images",
    "citation_count": 0,
    "authors": [
      "Byeongjun Kwon",
      "Munchurl Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hou_Omegance_A_Single_Parameter_for_Various_Granularities_in_Diffusion-Based_Synthesis_ICCV_2025_paper.html": {
    "title": "Omegance: A Single Parameter for Various Granularities in Diffusion-Based Synthesis",
    "volume": "main",
    "abstract": "In this work, we show that we only need a single parameter \\omega to effectively control granularity in diffusion-based synthesis. This parameter is incorporated during the denoising steps of the diffusion model's reverse process. This simple approach does not require model retraining or architectural modifications and incurs negligible computational overhead, yet enables precise control over the level of details in the generated outputs. Moreover, spatial masks or denoising schedules with varying \\omega values can be applied to achieve region-specific or timestep-specific granularity control. External control signals or reference images can guide the creation of precise \\omega masks, allowing targeted granularity adjustments. Despite its simplicity, the method demonstrates impressive performance across various image and video synthesis tasks and is adaptable to advanced diffusion models. The code is available at https://github.com/itsmag11/Omegance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyu Hou",
      "Zongsheng Yue",
      "Xiaoming Li",
      "Chen Change Loy"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_Frequency-Semantic_Enhanced_Variational_Autoencoder_for_Zero-Shot_Skeleton-based_Action_Recognition_ICCV_2025_paper.html": {
    "title": "Frequency-Semantic Enhanced Variational Autoencoder for Zero-Shot Skeleton-based Action Recognition",
    "volume": "main",
    "abstract": "Zero-shot skeleton-based action recognition aims to develop models capable of identifying actions beyond the categories encountered during training. Previous approaches have primarily focused on aligning visual and semantic representations but often overlooked the importance of fine-grained action patterns in the semantic space (e.g., the hand movements in drinking water and brushing teeth). To address these limitations, we propose a Frequency-Semantic Enhanced Variational Autoencoder (FS-VAE) to explore the skeleton semantic representation learning with frequency decomposition. FS-VAE consists of three key components: 1) a frequency-based enhancement module with high- and low-frequency adjustments to enrich the skeletal semantics learning and improve the robustness of zero-shot action recognition; 2) a semantic-based action description with multilevel alignment to capture both local details and global correspondence, effectively bridging the semantic gap and compensating for the inherent loss of information in skeleton sequences; 3) a calibrated cross-alignment loss that enables valid skeleton-text pairs to counterbalance ambiguous ones, mitigating discrepancies and ambiguities in skeleton and text features, thereby ensuring robust alignment. Evaluations on the benchmarks demonstrate the effectiveness of our approach, validating that frequency-enhanced semantic features enable robust differentiation of visually and semantically similar action clusters, thereby improving zero-shot action recognition",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhan Wu",
      "Zhishuai Guo",
      "Chen Chen",
      "Hongfei Xue",
      "Aidong Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lin_CleanPose_Category-Level_Object_Pose_Estimation_via_Causal_Learning_and_Knowledge_ICCV_2025_paper.html": {
    "title": "CleanPose: Category-Level Object Pose Estimation via Causal Learning and Knowledge Distillation",
    "volume": "main",
    "abstract": "In the effort to achieve robust and generalizable category-level object pose estimation, recent methods primarily focus on learning fundamental representations from data. However, the inherent biases within the data are often overlooked: the repeated training samples and similar environments may mislead the models to over-rely on specific patterns, hindering models' performance on novel instances. In this paper, we present CleanPose, a novel method that mitigates the data biases to enhance category-level pose estimation by integrating causal learning and knowledge distillation. By incorporating key causal variables (structural information and hidden confounders) into causal modeling, we propose the causal inference module based on front-door adjustment, which promotes unbiased estimation by reducing potential spurious correlations. Additionally, to further confront the data bias at the feature level, we devise a residual-based knowledge distillation approach to transfer unbiased semantic knowledge from 3D foundation model, providing comprehensive causal supervision. Extensive experiments across multiple benchmarks (REAL275, CAMERA25 and HouseCat6D) hightlight the superiority of proposed CleanPose over state-of-the-art methods. Code will be available at https://github.com/chrislin0621/CleanPose",
    "checked": true,
    "id": "9743f846202458937306b0e07459be17ea9d0d77",
    "semantic_title": "cleanpose: category-level object pose estimation via causal learning and knowledge distillation",
    "citation_count": 2,
    "authors": [
      "Xiao Lin",
      "Yun Peng",
      "Liuyi Wang",
      "Xianyou Zhong",
      "Minghao Zhu",
      "Yi Feng",
      "Jingwei Yang",
      "Chengju Liu",
      "Qijun Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shi_Lark_Low-Rank_Updates_After_Knowledge_Localization_for_Few-shot_Class-Incremental_Learning_ICCV_2025_paper.html": {
    "title": "Lark: Low-Rank Updates After Knowledge Localization for Few-shot Class-Incremental Learning",
    "volume": "main",
    "abstract": "For Few-Shot Class-Incremental Learning (FSCIL), direct fine-tuning causes significant parameter shifts, resulting in catastrophic forgetting and increased resource consumption. While, freezing the pre-trained backbone exacerbates the inconsistency between the backbone and the evolving classifier. To overcome these challenges, we introduce a method called Low-Rank updates after knowledge localization (Lark). In the knowledge localization phase, the Fisher Information Matrix is calculated to measure the sensitivity of parameters in different layers to previously acquired knowledge. This phase ultimately identifies the parameters within the model that are most suitable for learning new knowledge. In the subsequent incremental editing phase, a low-rank incremental update strategy is applied. This strategy ensures that the model parameter updates adhere to a Rank-One matrix structure. By doing so, it minimizes alterations to the original parameters, thereby enabling the model to integrate new knowledge while retaining as much of the previous knowledge as possible. Extensive experimental results demonstrate that the Lark method achieves significant performance improvements on the CIFAR100, mini-ImageNet, and CUB200 datasets, surpassing current state-of-the-art methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinxin Shi",
      "Jiabao Zhao",
      "Yifan Yang",
      "Xingjiao Wu",
      "Jiawen Li",
      "Liang He"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Halacheva_Articulate3D_Holistic_Understanding_of_3D_Scenes_as_Universal_Scene_Description_ICCV_2025_paper.html": {
    "title": "Articulate3D: Holistic Understanding of 3D Scenes as Universal Scene Description",
    "volume": "main",
    "abstract": "3D scene understanding is a long-standing challenge in computer vision and a key component in enabling mixed reality, wearable computing, and embodied AI. Providing a solution to these applications requires a multifaceted approach that covers scene-centric, object-centric, as well as interaction-centric capabilities. While there exist numerous datasets approaching the former two problems, the task of understanding interactable and articulated objects is underrepresented and only partly covered in the research field. In this work, we address this shortcoming by introducing: (1) Articulate3D, an expertly curated 3D dataset featuring high-quality manual annotations on 280 indoor scenes. Articulate3D provides 8 types of annotations for articulated objects, covering parts and detailed motion information,all stored in a standardized scene representation format designed for scalable 3D content creation, exchange and seamless integration into simulation environments. (2) USDNet, a novel unified framework capable of simultaneously predicting part segmentation along with a full specification of motion attributes for articulated objects. We evaluate USDNet on Articulate3D as well as two existing datasets, demonstrating the advantage of our unified dense prediction approach. Furthermore, we highlight the value of Articulate3D through cross-dataset and cross-domain evaluations and showcase its applicability in downstream tasks such as scene editing through LLM prompting and robotic policy training for articulated object manipulation. We provide open access to our dataset, benchmark, and method's source code",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anna-Maria Halacheva",
      "Yang Miao",
      "Jan-Nico Zaech",
      "Xi Wang",
      "Luc Van Gool",
      "Danda Pani Paudel"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_DiffPCI_Large_Motion_Point_Cloud_frame_Interpolation_with_Diffusion_Model_ICCV_2025_paper.html": {
    "title": "DiffPCI: Large Motion Point Cloud frame Interpolation with Diffusion Model",
    "volume": "main",
    "abstract": "Point cloud interpolation aims to recover intermediate frames for temporally smoothing a point cloud sequence. However, real-world challenges, such as uneven or large scene motions, cause existing methods to struggle with limited interpolation precision. To address this, we introduce DiffPCI, a novel diffusion interpolation model that formulates the frame interpolation task as a progressive denoising diffusion process. Training DiffPCI involves two key stages: a forward interpolation diffusion process and a reverse interpolation denoising process. In the forward process, the clean intermediate frame is progressively transformed into a noisy one through continuous Gaussian noise injection. The reverse process then focuses on training a denoiser to gradually refine this noisy frame back to the ground-truth frame. In particular, we derive a point cloud interpolation-specific variational lower bound as our optimization objective for denoiser training. Furthermore, to alleviate the interpolation error especially in highly dynamic scenes, we develop a novel full-scale, dual-branch denoiser that enables more comprehensive front-back frame information fusion for robust bi-directional interpolation. Extensive experiments demonstrate that DiffPCI significantly outperforms current state-of-the-art frame interpolation methods (e.g. 27% and 860% reduction in the Chamfer Distance and Earth Mover's Distance on Nuscenes)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyu Zhang",
      "Haobo Jiang",
      "Jian Yang",
      "Jin Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Qian_TryOn-Refiner_Conditional_Rectified-flow-based_TryOn_Refiner_for_More_Accurate_Detail_Reconstruction_ICCV_2025_paper.html": {
    "title": "TryOn-Refiner: Conditional Rectified-flow-based TryOn Refiner for More Accurate Detail Reconstruction",
    "volume": "main",
    "abstract": "Diffusion techniques has significantly advanced the development of virtual try-on. However, these methods often struggle to preserve intricate details, such as patterns, texts and faces, etc. To tackle this challenge, we introduce a plug-and-play module named as \"TryOn-Refiner\", which can refine the detailed artifacts for any try-on results in only 1~10 steps.Instead of the previous diffusion-based refine module, TryOn-Refiner employs the conditional rectified-flow-based mechanism for better leveraging prior information from coarse try-on results. Specifically, TryOn-Refiner transforms the traditional refinement framework from a noise-to-image paradigm into a flow mapping framework that directly maps coarse images to refined images, essentially avoiding introducing uncertainty in the refinement process.Moreover, we propose a training data construction pipeline, which can efficiently generate paired training data and includes a data smoothing strategy to overcome the blocking artifact.Extended experimental results demonstrate our TryOn-Refiner consistently improves performance with only a few inference steps for all evaluated existing try-on methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wen Qian"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jin_AM-Adapter_Appearance_Matching_Adapter_for_Exemplar-based_Semantic_Image_Synthesis_in-the-Wild_ICCV_2025_paper.html": {
    "title": "AM-Adapter: Appearance Matching Adapter for Exemplar-based Semantic Image Synthesis in-the-Wild",
    "volume": "main",
    "abstract": "Exemplar-based semantic image synthesis generates images aligned with semantic content while preserving the appearance of an exemplar. Conventional structure-guidance models like ControlNet, are limited as they rely solely on text prompts to control appearance and cannot utilize exemplar images as input. Recent tuning-free approaches address this by transferring local appearance via implicit cross-image matching in the augmented self-attention mechanism of pre-trained diffusion models. However, prior works are often restricted to single-object cases or foreground object appearance transfer, struggling with complex scenes involving multiple objects. To overcome this, we propose AM-Adapter (Appearance Matching Adapter) to address exemplar-based semantic image synthesis in-the-wild, enabling multi-object appearance transfer from a single scene-level image. AM-Adapter automatically transfers local appearances from the scene-level input. AM-Adapter alternatively provides controllability to map user-defined object details to specific locations in the synthesized images. Our learnable framework enhances cross-image matching within augmented self-attention by integrating semantic information from segmentation maps. To disentangle generation and matching, we adopt stage-wise training. We first train the structure-guidance and generation networks, followed by training the matching adapter while keeping the others frozen. During inference, we introduce an automated exemplar retrieval method for selecting exemplar image-segmentation pairs efficiently. Despite utilizing minimal learnable parameters, AM-Adapter achieves state-of-the-art performance, excelling in both semantic alignment and local appearance fidelity. Extensive ablations validate our design choices. Code and weights will be released",
    "checked": false,
    "id": "bdc6fcf50cacedff02252b9161d09752239d05c5",
    "semantic_title": "appearance matching adapter for exemplar-based semantic image synthesis in-the-wild",
    "citation_count": 0,
    "authors": [
      "Siyoon Jin",
      "Jisu Nam",
      "Jiyoung Kim",
      "Dahyun Chung",
      "Yeong-Seok Kim",
      "Joonhyung Park",
      "Heonjeong Chu",
      "Seungryong Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yu_Auto-Controlled_Image_Perception_in_MLLMs_via_Visual_Perception_Tokens_ICCV_2025_paper.html": {
    "title": "Auto-Controlled Image Perception in MLLMs via Visual Perception Tokens",
    "volume": "main",
    "abstract": "In MLLMs, Visual perception refers to the process by which MLLMs encode visual inputs, such as images, and align them with the text embedding space. Currently, MLLMs still lack the capability to autonomously control their own visual perception processes. For example, they cannot selectively re-encode specific regions of an image or focus on information related to specific object categories. In this work, we propose the concept of Visual Perception Token, aiming to empower MLLM with a mechanism to control its visual perception processes. We design two types of Visual Perception Tokens, termed the Region Selection Token and the Vision Re-Encoding Token. MLLMs autonomously generate these tokens, just as they generate natural language tokens, and use them to trigger additional visual perception process. The Region Selection Token explicitly identifies regions of interest that require further processing, while the Vision Re-Encoding Token utilizes its hidden states to guide an additional vision encoding process. Extensive experiments highlight the effectiveness of these tokens in enhancing spatial reasoning, fine-grained understanding, Text/OCR-related VQA, and a wide range of other visual tasks. On average, the introduction of Visual Perception Tokens improves the performance of a 2B model by 30.9%, increasing its score from 0.572 to 0.749, and even outperforms a 7B parameter model by 20.0% (from 0.624)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runpeng Yu",
      "Xinyin Ma",
      "Xinchao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhu_Addressing_Representation_Collapse_in_Vector_Quantized_Models_with_One_Linear_ICCV_2025_paper.html": {
    "title": "Addressing Representation Collapse in Vector Quantized Models with One Linear Layer",
    "volume": "main",
    "abstract": "Vector Quantization (VQ) is essential for discretizing continuous representations in unsupervised learning but suffers from representation collapse, causing low codebook utilization and limiting scalability. Existing solutions often rely on complex optimizations or reduce latent dimensionality, which compromises model capacity and fails to fully solve the problem. We identify the root cause as disjoint codebook optimization, where only a few code vectors are updated via gradient descent. To fix this, we propose SimpleVQ, which reparameterizes code vectors through a learnable linear transformation layer over a latent basis, optimizing the entire linear space rather than nearest individual code vectors. Although the multiplication of two linear matrices is equivalent to applying a single linear layer, this simple approach effectively prevents collapse. Extensive experiments on image and audio tasks demonstrate that SimVQ improves codebook usage, is easy to implement, and generalizes well across modalities and architectures",
    "checked": true,
    "id": "106497116e585b2a4ff67fcf0ec7b0a5cc6bf8e4",
    "semantic_title": "addressing representation collapse in vector quantized models with one linear layer",
    "citation_count": 31,
    "authors": [
      "Yongxin Zhu",
      "Bocheng Li",
      "Yifei Xin",
      "Zhihua Xia",
      "Linli Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Guo_MosaicDiff_Training-free_Structural_Pruning_for_Diffusion_Model_Acceleration_Reflecting_Pretraining_ICCV_2025_paper.html": {
    "title": "MosaicDiff: Training-free Structural Pruning for Diffusion Model Acceleration Reflecting Pretraining Dynamics",
    "volume": "main",
    "abstract": "Diffusion models are renowned for their generative capabilities, yet their pretraining processes exhibit distinct phases of learning speed that have been entirely overlooked in prior post-training acceleration efforts in the community. In this study, we introduce a novel framework called ***MosaicDiff*** that aligns diffusion pretraining dynamics with post-training sampling acceleration via trajectory-aware structural pruning. Our approach leverages the observation that the middle, fast-learning stage of diffusion pretraining requires more conservative pruning to preserve critical model features, while the early and later, slow-learning stages benefit from a more aggressive pruning strategy. This adaptive pruning mechanism is the first to explicitly mirror the inherent learning speed variations of diffusion pretraining, thereby harmonizing the model's inner training dynamics with its accelerated sampling process. Extensive experiments on DiT and SDXL demonstrate that our method achieves significant speed-ups in sampling without compromising output quality, outperforming previous state-of-the-art methods by large margins, also providing a new viewpoint for more efficient and robust training-free diffusion acceleration",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowei Guo",
      "Shengkun Tang",
      "Cong Zeng",
      "Zhiqiang Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yuan_Not_Only_Vision_Evolve_Visual_Speech_Recognition_via_Peripheral_Information_ICCV_2025_paper.html": {
    "title": "Not Only Vision: Evolve Visual Speech Recognition via Peripheral Information",
    "volume": "main",
    "abstract": "Is visual information alone sufficient for visual speech recognition (VSR) in challenging real-world scenarios? Humans do not rely solely on visual information for lip-reading but also incorporate additional cues, such as speech-related context and prior knowledge about the task. However, existing methods have largely overlooked such external information in automatic VSR systems. To systematically explore the role of such information for VSR, we introduce the concept of Peripheral Information. We categorize it into three types based on the relevance to the spoken content: (1) Contextual Guidance (e.g., topic or description of speech), (2) Task Expertise (e.g., human prior experience in lip-reading), and (3) Linguistic Perturbation (irrelevant signals processed alongside meaningful information). Considering the disparity that peripheral information provides additional clues with varying significance while visual input serves as the most direct source for VSR, we propose a framework that introduces a hierarchical processing strategy to handle different modalities. With visual-specific adaptation and a dynamic routing mechanism for multi-modal information, our approach reduces the impact of modality conflicts effectively and enables selective utilization of peripheral information with varying relevance. Leveraging readily available peripheral information, our model achieves a WER of 22.03% on LRS3. Further experiments on AVSpeech demonstrate its generalization in real-world scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoxin Yuan",
      "Shuang Yang",
      "Shiguang Shan",
      "Xilin Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hong_Borrowing_Eyes_for_the_Blind_Spot_Overcoming_Data_Scarcity_in_ICCV_2025_paper.html": {
    "title": "Borrowing Eyes for the Blind Spot: Overcoming Data Scarcity in Malicious Video Detection via Cross-Domain Retrieval Augmentation",
    "volume": "main",
    "abstract": "The rapid proliferation of online video-sharing platforms has accelerated the spread of malicious videos, creating an urgent need for robust detection methods. However, the performance and generalizability of existing detection approaches are severely limited by the scarcity of annotated video data, as manually curating large-scale malicious detection datasets is both labor-intensive and impractical. To address this challenge, we propose CRAVE, a novel CRoss-domAin retrieVal augmEntation framework that transfers knowledge from resource-rich image-text domain to enhance malicious video detection. Specifically, CRAVE introduces a Pseudo-Pair Retriever to identify semantically relevant image-text data for high-quality cross-domain augmentation. Additionally, a Contrastive Cross-Domain Augmenter is designed to disentangle domain-shared and -unique representations, effectively bridging the domain gaps during knowledge transfer. These shared image-text representations are then leveraged to refine video representations, yielding more discriminative features for accurate malicious content detection. Experiments on four video datasets demonstrate that CRAVE largely outperforms competitive baselines in both performance and generalization, providing an innovative and strong solution to the issue of video data-scarcity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rongpei Hong",
      "Jian Lang",
      "Ting Zhong",
      "Fan Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Dai_Trans-Adapter_A_Plug-and-Play_Framework_for_Transparent_Image_Inpainting_ICCV_2025_paper.html": {
    "title": "Trans-Adapter: A Plug-and-Play Framework for Transparent Image Inpainting",
    "volume": "main",
    "abstract": "RGBA images, with the additional alpha channel, are crucial for any application that needs blending, masking, or transparency effects, making them more versatile than standard RGB images. Nevertheless, existing image inpainting methods are designed exclusively for RGB images. Conventional approaches to transparent image inpainting typically involve placing a background underneath RGBA images and employing a two-stage process: image inpainting followed by image matting. This pipeline, however, struggles to preserve transparency consistency in edited regions, and matting can introduce jagged edges along transparency boundaries. To address these challenges, we propose Trans-Adapter, a plug-and-play adapter that enables diffusion-based inpainting models to process transparent images directly. Trans-Adapter also supports controllable editing via ControlNet and can be seamlessly integrated into various community models. To evaluate our method, we introduce LayerBench, along with a novel non-reference alpha edge quality evaluation metric for assessing transparency edge quality. We conduct extensive experiments on LayerBench to demonstrate the effectiveness of our approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuekun Dai",
      "Haitian Li",
      "Shangchen Zhou",
      "Chen Change Loy"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tong_MetaMorph_Multimodal_Understanding_and_Generation_via_Instruction_Tuning_ICCV_2025_paper.html": {
    "title": "MetaMorph: Multimodal Understanding and Generation via Instruction Tuning",
    "volume": "main",
    "abstract": "In this work, we propose Visual-Predictive Instruction Tuning (VPiT) - a simple and effective extension to visual instruction tuning that enables a pretrained LLM to quickly morph into an unified autoregressive model capable of generating both text and visual tokens. VPiT teaches an LLM to predict discrete text tokens and continuous visual tokens from any input sequence of image and text data curated in an instruction-following format. Our empirical investigation reveals several intriguing properties of VPiT: (1) visual generation ability emerges as a natural byproduct of improved visual understanding, and can be unlocked efficiently with a small amount of generation data; (2) while we find understanding and generation to be mutually beneficial, understanding data contributes to both capabilities more effectively than generation data. Building upon these findings, we train our MetaMorph model and achieve competitive performance on both visual understanding and generation. In visual generation, MetaMorph can leverage the world knowledge and reasoning abilities gained from LLM pretraining, and overcome common failure modes exhibited by other generation models. Our results suggest that LLMs may have strong \"prior\" vision capabilities that can be efficiently adapted to both visual understanding and generation with a relatively simple instruction tuning process",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengbang Tong",
      "David Fan",
      "Jiachen Li",
      "Yunyang Xiong",
      "Xinlei Chen",
      "Koustuv Sinha",
      "Michael Rabbat",
      "Yann LeCun",
      "Saining Xie",
      "Zhuang Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kumar_CHARM3R_Towards_Unseen_Camera_Height_Robust_Monocular_3D_Detector_ICCV_2025_paper.html": {
    "title": "CHARM3R: Towards Unseen Camera Height Robust Monocular 3D Detector",
    "volume": "main",
    "abstract": "Monocular 3D object detectors, while effective on data from one ego camera height, struggle with unseen or out-of-distribution camera heights. Existing methods often rely on Plucker embeddings, image transformations or data augmentation. This paper takes a step towards this understudied problem by investigating the impact of camera height variations on state-of-the-art (SoTA) Mono3D models. With a systematic analysis on the extended CARLA dataset with multiple camera heights, we observe that depth estimation is a primary factor influencing performance under height variations. We mathematically prove and also empirically observe consistent negative and positive trends in mean depth error of regressed and ground-based depth models, respectively, under camera height changes. To mitigate this, we propose Camera Height Robust Monocular 3D Detector (CHARM3R), which averages both depth estimates within the model. CHARM3R significantly improves generalization to unseen camera heights, by more than 45%, achieving SoTA performance on the CARLA dataset",
    "checked": true,
    "id": "68a25c1003d1506ffe3390bbfb95d9770508f25c",
    "semantic_title": "charm3r: towards unseen camera height robust monocular 3d detector",
    "citation_count": 0,
    "authors": [
      "Abhinav Kumar",
      "Yuliang Guo",
      "Zhihao Zhang",
      "Xinyu Huang",
      "Liu Ren",
      "Xiaoming Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Fedele_SuperDec_3D_Scene_Decomposition_with_Superquadrics_Primitives_ICCV_2025_paper.html": {
    "title": "SuperDec: 3D Scene Decomposition with Superquadrics Primitives",
    "volume": "main",
    "abstract": "We present SuperDec, an approach for compact 3D scene representations based on geometric primitives, namely superquadrics.While most recent works leverage geometric primitives to obtain photorealistic 3D scene representations, we propose to leverage them to obtain a compact yet expressive representation. We propose to solve the problem locally on individual objects and leverage the capabilities of instance segmentation methods to scale our solution to full 3D scenes. In doing that, we design a new architecture which efficiently decompose point clouds of arbitrary objects in a compact set of superquadrics. We train our architecture on ShapeNet and we prove its generalization capabilities on object instances extracted from the ScanNet++ dataset as well as on full Replica scenes. Finally, we show how a compact representation based on superquadrics can be useful for a diverse range of downstream applications, including robotic tasks and controllable visual content generation and editing",
    "checked": false,
    "id": "0dfa2f94cace24de82641a5d1f0cb2f9ec9cf671",
    "semantic_title": "superdec: 3d scene decomposition with superquadric primitives",
    "citation_count": 3,
    "authors": [
      "Elisabetta Fedele",
      "Boyang Sun",
      "Leonidas Guibas",
      "Marc Pollefeys",
      "Francis Engelmann"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Sculpting_Memory_Multi-Concept_Forgetting_in_Diffusion_Models_via_Dynamic_Mask_ICCV_2025_paper.html": {
    "title": "Sculpting Memory: Multi-Concept Forgetting in Diffusion Models via Dynamic Mask and Concept-Aware Optimization",
    "volume": "main",
    "abstract": "Text-to-image (T2I) diffusion models have achieved remarkable success in generating high-quality images from textual prompts. However, their ability to store vast amounts of knowledge raises concerns in scenarios where selective forgetting is necessary, such as removing copyrighted content, reducing biases, or eliminating harmful concepts. While existing unlearning methods can remove certain concepts, they struggle with multi-concept forgetting due to instability, residual knowledge persistence, and generation quality degradation. To address these challenges, we propose **Dynamic Mask coupled with Concept-Aware Loss**, a novel unlearning framework designed for multi-concept forgetting in diffusion models. Our **Dynamic Mask** mechanism adaptively updates gradient masks based on current optimization states, allowing selective weight modifications that prevent interference with unrelated knowledge. Additionally, our **Concept-Aware Loss** explicitly guides the unlearning process by enforcing semantic consistency through superclass alignment, while a regularization loss based on knowledge distillation ensures that previously unlearned concepts remain forgotten during sequential unlearning. We conduct extensive experiments to evaluate our approach. Results demonstrate that our method outperforms existing unlearning techniques in forgetting effectiveness, output fidelity, and semantic coherence, particularly in multi-concept scenarios. Our work provides a principled and flexible framework for stable and high-fidelity unlearning in generative models. Code available in https://github.com/coulsonlee/Sculpting-Memory-ICCV-2025",
    "checked": true,
    "id": "725546fc945da535b6601b8872ba8fe192b78792",
    "semantic_title": "sculpting memory: multi-concept forgetting in diffusion models via dynamic mask and concept-aware optimization",
    "citation_count": 1,
    "authors": [
      "Gen Li",
      "Yang Xiao",
      "Jie Ji",
      "Kaiyuan Deng",
      "Bo Hui",
      "Linke Guo",
      "Xiaolong Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhan_LaRender_Training-Free_Occlusion_Control_in_Image_Generation_via_Latent_Rendering_ICCV_2025_paper.html": {
    "title": "LaRender: Training-Free Occlusion Control in Image Generation via Latent Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaohang Zhan",
      "Dingming Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jiang_TimeFormer_Capturing_Temporal_Relationships_of_Deformable_3D_Gaussians_for_Robust_ICCV_2025_paper.html": {
    "title": "TimeFormer: Capturing Temporal Relationships of Deformable 3D Gaussians for Robust Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dadong Jiang",
      "Zhi Hou",
      "Zhihui Ke",
      "Xianghui Yang",
      "Xiaobo Zhou",
      "Tie Qiu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_LLM_Thought_Divergence_and_Convergence_for_Dialogue-Based_Image_Generation_Control_ICCV_2025_paper.html": {
    "title": "LLM Thought Divergence and Convergence for Dialogue-Based Image Generation Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hui Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Spagnoletti_LATINO-PRO_LAtent_consisTency_INverse_sOlver_with_PRompt_Optimization_ICCV_2025_paper.html": {
    "title": "LATINO-PRO: LAtent consisTency INverse sOlver with PRompt Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alessio Spagnoletti",
      "Jean Prost",
      "Andrés Almansa",
      "Nicolas Papadakis",
      "Marcelo Pereyra"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_Lightweight_and_Fast_Real-time_Image_Enhancement_via_Decomposition_of_the_ICCV_2025_paper.html": {
    "title": "Lightweight and Fast Real-time Image Enhancement via Decomposition of the Spatial-aware Lookup Tables",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wontae Kim",
      "Keuntek Lee",
      "Nam Ik Cho"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shan_Geminio_Language-Guided_Gradient_Inversion_Attacks_in_Federated_Learning_ICCV_2025_paper.html": {
    "title": "Geminio: Language-Guided Gradient Inversion Attacks in Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junjie Shan",
      "Ziqi Zhao",
      "Jialin Lu",
      "Rui Zhang",
      "Siu Ming Yiu",
      "Ka-Ho Chow"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Knoche_DONUT_A_Decoder-Only_Model_for_Trajectory_Prediction_ICCV_2025_paper.html": {
    "title": "DONUT: A Decoder-Only Model for Trajectory Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Markus Knoche",
      "Daan de Geus",
      "Bastian Leibe"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhao_Training-free_Generation_of_Temporally_Consistent_Rewards_from_VLMs_ICCV_2025_paper.html": {
    "title": "Training-free Generation of Temporally Consistent Rewards from VLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinuo Zhao",
      "Jiale Yuan",
      "Zhiyuan Xu",
      "Xiaoshuai Hao",
      "Xinyi Zhang",
      "Kun Wu",
      "Zhengping Che",
      "Chi Harold Liu",
      "Jian Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Guo_Compression-Aware_One-Step_Diffusion_Model_for_JPEG_Artifact_Removal_ICCV_2025_paper.html": {
    "title": "Compression-Aware One-Step Diffusion Model for JPEG Artifact Removal",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinpei Guo",
      "Zheng Chen",
      "Wenbo Li",
      "Yong Guo",
      "Yulun Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gao_AnyPortal_Zero-Shot_Consistent_Video_Background_Replacement_ICCV_2025_paper.html": {
    "title": "AnyPortal: Zero-Shot Consistent Video Background Replacement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenshuo Gao",
      "Xicheng Lan",
      "Shuai Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Bian_Scene_Coordinate_Reconstruction_Priors_ICCV_2025_paper.html": {
    "title": "Scene Coordinate Reconstruction Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenjing Bian",
      "Axel Barroso-Laguna",
      "Tommaso Cavallari",
      "Victor Adrian Prisacariu",
      "Eric Brachmann"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_Towards_a_Universal_Image_Degradation_Model_via_Content-Degradation_Disentanglement_ICCV_2025_paper.html": {
    "title": "Towards a Universal Image Degradation Model via Content-Degradation Disentanglement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenbo Yang",
      "Zhongling Wang",
      "Zhou Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jiang_Balanced_Image_Stylization_with_Style_Matching_Score_ICCV_2025_paper.html": {
    "title": "Balanced Image Stylization with Style Matching Score",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxin Jiang",
      "Liming Jiang",
      "Shuai Yang",
      "Jia-Wei Liu",
      "Ivor W. Tsang",
      "Mike Zheng Shou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_MultiModal_Action_Conditioned_Video_Simulation_ICCV_2025_paper.html": {
    "title": "MultiModal Action Conditioned Video Simulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yichen Li",
      "Antonio Torralba"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Breaking_Rectangular_Shackles_Cross-View_Object_Segmentation_for_Fine-Grained_Object_Geo-Localization_ICCV_2025_paper.html": {
    "title": "Breaking Rectangular Shackles: Cross-View Object Segmentation for Fine-Grained Object Geo-Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingwang Zhang",
      "Yingying Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Miao_Temporal_Overlapping_Prediction_A_Self-supervised_Pre-training_Method_for_LiDAR_Moving_ICCV_2025_paper.html": {
    "title": "Temporal Overlapping Prediction: A Self-supervised Pre-training Method for LiDAR Moving Object Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziliang Miao",
      "Runjian Chen",
      "Yixi Cai",
      "Buwei He",
      "Wenquan Zhao",
      "Wenqi Shao",
      "Bo Zhang",
      "Fu Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_No_More_Sibling_Rivalry_Debiasing_Human-Object_Interaction_Detection_ICCV_2025_paper.html": {
    "title": "No More Sibling Rivalry: Debiasing Human-Object Interaction Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bin Yang",
      "Yulin Zhang",
      "Hong-Yu Zhou",
      "Sibei Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shi_Imbalance_in_Balance_Online_Concept_Balancing_in_Generation_Models_ICCV_2025_paper.html": {
    "title": "Imbalance in Balance: Online Concept Balancing in Generation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yukai Shi",
      "Jiarong Ou",
      "Rui Chen",
      "Haotian Yang",
      "Jiahao Wang",
      "Xin Tao",
      "Pengfei Wan",
      "Di Zhang",
      "Kun Gai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Unleashing_High-Quality_Image_Generation_in_Diffusion_Sampling_Using_Second-Order_Levenberg-Marquardt-Langevin_ICCV_2025_paper.html": {
    "title": "Unleashing High-Quality Image Generation in Diffusion Sampling Using Second-Order Levenberg-Marquardt-Langevin",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangyikang Wang",
      "Hubery Yin",
      "Lei Qian",
      "Yinan Li",
      "Shaobin Zhuang",
      "Huminhao Zhu",
      "Yilin Zhang",
      "Yanlong Tang",
      "Chao Zhang",
      "Hanbin Zhao",
      "Hui Qian",
      "Chen Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kobayashi_Temperature_in_Cosine-based_Softmax_Loss_ICCV_2025_paper.html": {
    "title": "Temperature in Cosine-based Softmax Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Takumi Kobayashi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_SD2Actor_Continuous_State_Decomposition_via_Diffusion_Embeddings_for_Robotic_Manipulation_ICCV_2025_paper.html": {
    "title": "SD2Actor: Continuous State Decomposition via Diffusion Embeddings for Robotic Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayi Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shuai_Free-Form_Motion_Control_Controlling_the_6D_Poses_of_Camera_and_ICCV_2025_paper.html": {
    "title": "Free-Form Motion Control: Controlling the 6D Poses of Camera and Objects in Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xincheng Shuai",
      "Henghui Ding",
      "Zhenyuan Qin",
      "Hao Luo",
      "Xingjun Ma",
      "Dacheng Tao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xu_GaussianProperty_Integrating_Physical_Properties_to_3D_Gaussians_with_LMMs_ICCV_2025_paper.html": {
    "title": "GaussianProperty: Integrating Physical Properties to 3D Gaussians with LMMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinli Xu",
      "Wenhang Ge",
      "Dicong Qiu",
      "ZhiFei Chen",
      "Dongyu Yan",
      "Zhuoyun Liu",
      "Haoyu Zhao",
      "Hanfeng Zhao",
      "Shunsi Zhang",
      "Junwei Liang",
      "Ying-Cong Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Exploring_Probabilistic_Modeling_Beyond_Domain_Generalization_for_Semantic_Segmentation_ICCV_2025_paper.html": {
    "title": "Exploring Probabilistic Modeling Beyond Domain Generalization for Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "I-Hsiang Chen",
      "Hua-En Chang",
      "Wei-Ting Chen",
      "Jenq-Neng Hwang",
      "Sy-Yen Kuo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Learning_Robust_Stereo_Matching_in_the_Wild_with_Selective_Mixture-of-Experts_ICCV_2025_paper.html": {
    "title": "Learning Robust Stereo Matching in the Wild with Selective Mixture-of-Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yun Wang",
      "Longguang Wang",
      "Chenghao Zhang",
      "Yongjian Zhang",
      "Zhanjie Zhang",
      "Ao Ma",
      "Chenyou Fan",
      "Tin Lun Lam",
      "Junjie Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jiang_Token-Efficient_VLM_High-Resolution_Image_Understanding_via_Dynamic_Region_Proposal_ICCV_2025_paper.html": {
    "title": "Token-Efficient VLM: High-Resolution Image Understanding via Dynamic Region Proposal",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yitong Jiang",
      "Jinwei Gu",
      "Tianfan Xue",
      "Ka Chun Cheung",
      "Pavlo Molchanov",
      "Hongxu Yin",
      "Sifei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Manas_Controlling_Multimodal_LLMs_via_Reward-guided_Decoding_ICCV_2025_paper.html": {
    "title": "Controlling Multimodal LLMs via Reward-guided Decoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oscar Mañas",
      "Pierluca D'Oro",
      "Koustuv Sinha",
      "Adriana Romero-Soriano",
      "Michal Drozdzal",
      "Aishwarya Agrawal"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lu_Multi-Modal_Few-Shot_Temporal_Action_Segmentation_ICCV_2025_paper.html": {
    "title": "Multi-Modal Few-Shot Temporal Action Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijia Lu",
      "Ehsan Elhamifar"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Baumann_What_If_Understanding_Motion_Through_Sparse_Interactions_ICCV_2025_paper.html": {
    "title": "What If: Understanding Motion Through Sparse Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stefan Andreas Baumann",
      "Nick Stracke",
      "Timy Phan",
      "Björn Ommer"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Goyal_Robust_3D_Object_Detection_using_Probabilistic_Point_Clouds_from_Single-Photon_ICCV_2025_paper.html": {
    "title": "Robust 3D Object Detection using Probabilistic Point Clouds from Single-Photon LiDARs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bhavya Goyal",
      "Felipe Gutierrez-Barragan",
      "Wei Lin",
      "Andreas Velten",
      "Yin Li",
      "Mohit Gupta"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_SAMPLE_Semantic_Alignment_through_Temporal-Adaptive_Multimodal_Prompt_Learning_for_Event-Based_ICCV_2025_paper.html": {
    "title": "SAMPLE: Semantic Alignment through Temporal-Adaptive Multimodal Prompt Learning for Event-Based Open-Vocabulary Action Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Wang",
      "Rui Zhao",
      "Ruiqin Xiong",
      "Xingtao Wang",
      "Xiaopeng Fan",
      "Tiejun Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lee_Semantic_Watermarking_Reinvented_Enhancing_Robustness_and_Generation_Quality_with_Fourier_ICCV_2025_paper.html": {
    "title": "Semantic Watermarking Reinvented: Enhancing Robustness and Generation Quality with Fourier Integrity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sung Ju Lee",
      "Nam Ik Cho"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hu_Auto-Regressively_Generating_Multi-View_Consistent_Images_ICCV_2025_paper.html": {
    "title": "Auto-Regressively Generating Multi-View Consistent Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "JiaKui Hu",
      "Yuxiao Yang",
      "Jialun Liu",
      "Jinbo Wu",
      "Chen Zhao",
      "Yanye Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_TimeExpert_An_Expert-Guided_Video_LLM_for_Video_Temporal_Grounding_ICCV_2025_paper.html": {
    "title": "TimeExpert: An Expert-Guided Video LLM for Video Temporal Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zuhao Yang",
      "Yingchen Yu",
      "Yunqing Zhao",
      "Shijian Lu",
      "Song Bai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Alshami_AIM_Amending_Inherent_Interpretability_via_Self-Supervised_Masking_ICCV_2025_paper.html": {
    "title": "AIM: Amending Inherent Interpretability via Self-Supervised Masking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eyad Alshami",
      "Shashank Agnihotri",
      "Bernt Schiele",
      "Margret Keuper"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kelly_I_Am_Big_You_Are_Little_I_Am_Right_You_ICCV_2025_paper.html": {
    "title": "I Am Big, You Are Little; I Am Right, You Are Wrong",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David A. Kelly",
      "Akchunya Chanchal",
      "Nathan Blake"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lu_Jigsaw_Imagining_Complete_Shape_Priors_for_Object_Reassembly_ICCV_2025_paper.html": {
    "title": "Jigsaw++: Imagining Complete Shape Priors for Object Reassembly",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxin Lu",
      "Gang Hua",
      "Qixing Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ye_GS-Occ3D_Scaling_Vision-only_Occupancy_Reconstruction_with_Gaussian_Splatting_ICCV_2025_paper.html": {
    "title": "GS-Occ3D: Scaling Vision-only Occupancy Reconstruction with Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baijun Ye",
      "Minghui Qin",
      "Saining Zhang",
      "Moonjun Gong",
      "Shaoting Zhu",
      "Hao Zhao",
      "Hang Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_CoMatch_Dynamic_Covisibility-Aware_Transformer_for_Bilateral_Subpixel-Level_Semi-Dense_Image_Matching_ICCV_2025_paper.html": {
    "title": "CoMatch: Dynamic Covisibility-Aware Transformer for Bilateral Subpixel-Level Semi-Dense Image Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zizhuo Li",
      "Yifan Lu",
      "Linfeng Tang",
      "Shihua Zhang",
      "Jiayi Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cheng_A_Unified_Interpretation_of_Training-Time_Out-of-Distribution_Detection_ICCV_2025_paper.html": {
    "title": "A Unified Interpretation of Training-Time Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xu Cheng",
      "Xin Jiang",
      "Zechao Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shao_Memory-Efficient_Generative_Models_via_Product_Quantization_ICCV_2025_paper.html": {
    "title": "Memory-Efficient Generative Models via Product Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Shao",
      "Hanxiao Zhang",
      "Hao Yu",
      "Jianxin Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yao_Unsupervised_Visible-Infrared_Person_Re-identification_under_Unpaired_Settings_ICCV_2025_paper.html": {
    "title": "Unsupervised Visible-Infrared Person Re-identification under Unpaired Settings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu Yao",
      "Bin Yang",
      "Wenke Huang",
      "Bo Du",
      "Mang Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Fang_Soft_Separation_and_Distillation_Toward_Global_Uniformity_in_Federated_Unsupervised_ICCV_2025_paper.html": {
    "title": "Soft Separation and Distillation: Toward Global Uniformity in Federated Unsupervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hung-Chieh Fang",
      "Hsuan-Tien Lin",
      "Irwin King",
      "Yifei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Monsefi_TaxaDiffusion_Progressively_Trained_Diffusion_Model_for_Fine-Grained_Species_Generation_ICCV_2025_paper.html": {
    "title": "TaxaDiffusion: Progressively Trained Diffusion Model for Fine-Grained Species Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amin Karimi Monsefi",
      "Mridul Khurana",
      "Rajiv Ramnath",
      "Anuj Karpatne",
      "Wei-Lun Chao",
      "Cheng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_Multimodal_LLMs_as_Customized_Reward_Models_for_Text-to-Image_Generation_ICCV_2025_paper.html": {
    "title": "Multimodal LLMs as Customized Reward Models for Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shijie Zhou",
      "Ruiyi Zhang",
      "Huaisheng Zhu",
      "Branislav Kveton",
      "Yufan Zhou",
      "Jiuxiang Gu",
      "Jian Chen",
      "Changyou Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Bargatin_MEMFOF_High-Resolution_Training_for_Memory-Efficient_Multi-Frame_Optical_Flow_Estimation_ICCV_2025_paper.html": {
    "title": "MEMFOF: High-Resolution Training for Memory-Efficient Multi-Frame Optical Flow Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vladislav Bargatin",
      "Egor Chistov",
      "Alexander Yakovenko",
      "Dmitriy Vatolin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yao_Diving_into_the_Fusion_of_Monocular_Priors_for_Generalized_Stereo_ICCV_2025_paper.html": {
    "title": "Diving into the Fusion of Monocular Priors for Generalized Stereo Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengtang Yao",
      "Lidong Yu",
      "Zhidan Liu",
      "Jiaxi Zeng",
      "Yuwei Wu",
      "Yunde Jia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_RAGNet_Large-scale_Reasoning-based_Affordance_Segmentation_Benchmark_towards_General_Grasping_ICCV_2025_paper.html": {
    "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongming Wu",
      "Yanping Fu",
      "Saike Huang",
      "Yingfei Liu",
      "Fan Jia",
      "Nian Liu",
      "Feng Dai",
      "Tiancai Wang",
      "Rao Muhammad Anwer",
      "Fahad Shahbaz Khan",
      "Jianbing Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Mirlach_R-LiViT_A_LiDAR-Visual-Thermal_Dataset_Enabling_Vulnerable_Road_User_Focused_Roadside_ICCV_2025_paper.html": {
    "title": "R-LiViT: A LiDAR-Visual-Thermal Dataset Enabling Vulnerable Road User Focused Roadside Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonas Mirlach",
      "Lei Wan",
      "Andreas Wiedholz",
      "Hannan Ejaz Keen",
      "Andreas Eich"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Qin_Spatially-Varying_Autofocus_ICCV_2025_paper.html": {
    "title": "Spatially-Varying Autofocus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingsi Qin",
      "Aswin C. Sankaranarayanan",
      "Matthew O'Toole"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_ZeroStereo_Zero-shot_Stereo_Matching_from_Single_Images_ICCV_2025_paper.html": {
    "title": "ZeroStereo: Zero-shot Stereo Matching from Single Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xianqi Wang",
      "Hao Yang",
      "Gangwei Xu",
      "Junda Cheng",
      "Min Lin",
      "Yong Deng",
      "Jinliang Zang",
      "Yurui Chen",
      "Xin Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Fang_A3GS_Arbitrary_Artistic_Style_into_Arbitrary_3D_Gaussian_Splatting_ICCV_2025_paper.html": {
    "title": "A3GS: Arbitrary Artistic Style into Arbitrary 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyuan Fang",
      "Rengan Xie",
      "Xuancheng Jin",
      "Qi Ye",
      "Wei Chen",
      "Wenting Zheng",
      "Rui Wang",
      "Yuchi Huo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Josi_SEREP_Semantic_Facial_Expression_Representation_for_Robust_In-the-Wild_Capture_and_ICCV_2025_paper.html": {
    "title": "SEREP: Semantic Facial Expression Representation for Robust In-the-Wild Capture and Retargeting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arthur Josi",
      "Luiz Gustavo Hafemann",
      "Abdallah Dib",
      "Emeline Got",
      "Rafael M. O. Cruz",
      "Marc-André Carbonneau"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Medeiros_Visual_Modality_Prompt_for_Adapting_Vision-Language_Object_Detectors_ICCV_2025_paper.html": {
    "title": "Visual Modality Prompt for Adapting Vision-Language Object Detectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heitor R. Medeiros",
      "Atif Belal",
      "Srikanth Muralidharan",
      "Eric Granger",
      "Marco Pedersoli"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_The_Silent_Assistant_NoiseQuery_as_Implicit_Guidance_for_Goal-Driven_Image_ICCV_2025_paper.html": {
    "title": "The Silent Assistant: NoiseQuery as Implicit Guidance for Goal-Driven Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruoyu Wang",
      "Huayang Huang",
      "Ye Zhu",
      "Olga Russakovsky",
      "Yu Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lu_AnyBimanual_Transferring_Unimanual_Policy_for_General_Bimanual_Manipulation_ICCV_2025_paper.html": {
    "title": "AnyBimanual: Transferring Unimanual Policy for General Bimanual Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guanxing Lu",
      "Tengbo Yu",
      "Haoyuan Deng",
      "Season Si Chen",
      "Yansong Tang",
      "Ziwei Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xu_Training-Free_Industrial_Defect_Generation_with_Diffusion_Models_ICCV_2025_paper.html": {
    "title": "Training-Free Industrial Defect Generation with Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruyi Xu",
      "Yen-Tzu Chiu",
      "Tai-I Chen",
      "Oscar Chew",
      "Yung-Yu Chuang",
      "Wen-Huang Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zheng_Hierarchical_Event_Memory_for_Accurate_and_Low-latency_Online_Video_Temporal_ICCV_2025_paper.html": {
    "title": "Hierarchical Event Memory for Accurate and Low-latency Online Video Temporal Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minghang Zheng",
      "Yuxin Peng",
      "Benyuan Sun",
      "Yi Yang",
      "Yang Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Qiu_Spatial_Preference_Rewarding_for_MLLMs_Spatial_Understanding_ICCV_2025_paper.html": {
    "title": "Spatial Preference Rewarding for MLLMs Spatial Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Qiu",
      "Peng Gao",
      "Lewei Lu",
      "Xiaoqin Zhang",
      "Ling Shao",
      "Shijian Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Correspondence_as_Video_Test-Time_Adaption_on_SAM2_for_Reference_Segmentation_ICCV_2025_paper.html": {
    "title": "Correspondence as Video: Test-Time Adaption on SAM2 for Reference Segmentation in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoran Wang",
      "Zekun Li",
      "Jian Zhang",
      "Lei Qi",
      "Yinghuan Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lee_Interaction-Merged_Motion_Planning_Effectively_Leveraging_Diverse_Motion_Datasets_for_Robust_ICCV_2025_paper.html": {
    "title": "Interaction-Merged Motion Planning: Effectively Leveraging Diverse Motion Datasets for Robust Planning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giwon Lee",
      "Wooseong Jeong",
      "Daehee Park",
      "Jaewoo Jeong",
      "Kuk-Jin Yoon"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wei_FreeFlux_Understanding_and_Exploiting_Layer-Specific_Roles_in_RoPE-Based_MMDiT_for_ICCV_2025_paper.html": {
    "title": "FreeFlux: Understanding and Exploiting Layer-Specific Roles in RoPE-Based MMDiT for Versatile Image Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyi Wei",
      "Yifan Zhou",
      "Dongdong Chen",
      "Xingang Pan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yuan_WalkVLM_Aid_Visually_Impaired_People_Walking_by_Vision_Language_Model_ICCV_2025_paper.html": {
    "title": "WalkVLM: Aid Visually Impaired People Walking by Vision Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiqiang Yuan",
      "Ting Zhang",
      "Yeshuang Zhu",
      "Jiapei Zhang",
      "Ying Deng",
      "Zexi Jia",
      "Peixiang Luo",
      "Xiaoyue Duan",
      "Jie Zhou",
      "Jinchao Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Guo_WildSeg3D_Segment_Any_3D_Objects_in_the_Wild_from_2D_ICCV_2025_paper.html": {
    "title": "WildSeg3D: Segment Any 3D Objects in the Wild from 2D Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yansong Guo",
      "Jie Hu",
      "Yansong Qu",
      "Liujuan Cao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Pothiraj_CAPTURE_Evaluating_Spatial_Reasoning_in_Vision_Language_Models_via_Occluded_ICCV_2025_paper.html": {
    "title": "CAPTURE: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Atin Pothiraj",
      "Elias Stengel-Eskin",
      "Jaemin Cho",
      "Mohit Bansal"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Dai_DeRIS_Decoupling_Perception_and_Cognition_for_Enhanced_Referring_Image_Segmentation_ICCV_2025_paper.html": {
    "title": "DeRIS: Decoupling Perception and Cognition for Enhanced Referring Image Segmentation through Loopback Synergy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ming Dai",
      "Wenxuan Cheng",
      "Jiang-jiang Liu",
      "Sen Yang",
      "Wenxiao Cai",
      "Yanpeng Sun",
      "Wankou Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hao_PersPose_3D_Human_Pose_Estimation_with_Perspective_Encoding_and_Perspective_ICCV_2025_paper.html": {
    "title": "PersPose: 3D Human Pose Estimation with Perspective Encoding and Perspective Rotation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyang Hao",
      "Han Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shin_Exploring_Multimodal_Diffusion_Transformers_for_Enhanced_Prompt-based_Image_Editing_ICCV_2025_paper.html": {
    "title": "Exploring Multimodal Diffusion Transformers for Enhanced Prompt-based Image Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joonghyuk Shin",
      "Alchan Hwang",
      "Yujin Kim",
      "Daneul Kim",
      "Jaesik Park"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Park_IM-LUT_Interpolation_Mixing_Look-Up_Tables_for_Image_Super-Resolution_ICCV_2025_paper.html": {
    "title": "IM-LUT: Interpolation Mixing Look-Up Tables for Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sejin Park",
      "Sangmin Lee",
      "Kyong Hwan Jin",
      "Seung-Won Jung"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Fu_TeEFusion_Blending_Text_Embeddings_to_Distill_Classifier-Free_Guidance_ICCV_2025_paper.html": {
    "title": "TeEFusion: Blending Text Embeddings to Distill Classifier-Free Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minghao Fu",
      "Guo-Hua Wang",
      "Xiaohao Chen",
      "Qing-Guo Chen",
      "Zhao Xu",
      "Weihua Luo",
      "Kaifu Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Scaling_Omni-modal_Pretraining_with_Multimodal_Context_Advancing_Universal_Representation_Learning_ICCV_2025_paper.html": {
    "title": "Scaling Omni-modal Pretraining with Multimodal Context: Advancing Universal Representation Learning Across Modalities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiyuan Zhang",
      "Handong Li",
      "Jing Liu",
      "Xiangyu Yue"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_ZIM_Zero-Shot_Image_Matting_for_Anything_ICCV_2025_paper.html": {
    "title": "ZIM: Zero-Shot Image Matting for Anything",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Beomyoung Kim",
      "Chanyong Shin",
      "Joonhyun Jeong",
      "Hyungsik Jung",
      "Se-Yun Lee",
      "Sewhan Chun",
      "Dong-Hyun Hwang",
      "Joonsang Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Suzuki_LayerD_Decomposing_Raster_Graphic_Designs_into_Layers_ICCV_2025_paper.html": {
    "title": "LayerD: Decomposing Raster Graphic Designs into Layers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tomoyuki Suzuki",
      "Kang-Jun Liu",
      "Naoto Inoue",
      "Kota Yamaguchi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_ALOcc_Adaptive_Lifting-Based_3D_Semantic_Occupancy_and_Cost_Volume-Based_Flow_ICCV_2025_paper.html": {
    "title": "ALOcc: Adaptive Lifting-Based 3D Semantic Occupancy and Cost Volume-Based Flow Predictions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dubing Chen",
      "Jin Fang",
      "Wencheng Han",
      "Xinjing Cheng",
      "Junbo Yin",
      "Chengzhong Xu",
      "Fahad Shahbaz Khan",
      "Jianbing Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ma_Heuristic-Induced_Multimodal_Risk_Distribution_Jailbreak_Attack_for_Multimodal_Large_Language_ICCV_2025_paper.html": {
    "title": "Heuristic-Induced Multimodal Risk Distribution Jailbreak Attack for Multimodal Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Teng Ma",
      "Xiaojun Jia",
      "Ranjie Duan",
      "Xinfeng Li",
      "Yihao Huang",
      "Xiaoshuang Jia",
      "Zhixuan Chu",
      "Wenqi Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zeng_Skip-Vision_Efficient_and_Scalable_Acceleration_of_Vision-Language_Models_via_Adaptive_ICCV_2025_paper.html": {
    "title": "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language Models via Adaptive Token Skipping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weili Zeng",
      "Ziyuan Huang",
      "Kaixiang Ji",
      "Yichao Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Pramanick_Enrich_and_Detect_Video_Temporal_Grounding_with_Multimodal_LLMs_ICCV_2025_paper.html": {
    "title": "Enrich and Detect: Video Temporal Grounding with Multimodal LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shraman Pramanick",
      "Effrosyni Mavroudi",
      "Yale Song",
      "Rama Chellappa",
      "Lorenzo Torresani",
      "Triantafyllos Afouras"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Class_Token_as_Proxy_Optimal_Transport-assisted_Proxy_Learning_for_Weakly_ICCV_2025_paper.html": {
    "title": "Class Token as Proxy: Optimal Transport-assisted Proxy Learning for Weakly Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Wang",
      "Tianhong Dai",
      "Bingfeng Zhang",
      "Siyue Yu",
      "Eng Gee Lim",
      "Jimin Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/He_DexVLG_Dexterous_Vision-Language-Grasp_Model_at_Scale_ICCV_2025_paper.html": {
    "title": "DexVLG: Dexterous Vision-Language-Grasp Model at Scale",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawei He",
      "Danshi Li",
      "Xinqiang Yu",
      "Zekun Qi",
      "Wenyao Zhang",
      "Jiayi Chen",
      "Zhaoxiang Zhang",
      "Zhizheng Zhang",
      "Li Yi",
      "He Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_FIND_Few-Shot_Anomaly_Inspection_with_Normal-Only_Multi-Modal_Data_ICCV_2025_paper.html": {
    "title": "FIND: Few-Shot Anomaly Inspection with Normal-Only Multi-Modal Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiting Li",
      "Fayao Liu",
      "Jingyi Liao",
      "Sichao Tian",
      "Chuan-Sheng Foo",
      "Xulei Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xie_MaskSAM_Auto-prompt_SAM_with_Mask_Classification_for_Volumetric_Medical_Image_ICCV_2025_paper.html": {
    "title": "MaskSAM: Auto-prompt SAM with Mask Classification for Volumetric Medical Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bin Xie",
      "Hao Tang",
      "Bin Duan",
      "Dawen Cai",
      "Yan Yan",
      "Gady Agam"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hui_Boundary_Probing_for_Input_Privacy_Protection_When_Using_LMM_Services_ICCV_2025_paper.html": {
    "title": "Boundary Probing for Input Privacy Protection When Using LMM Services",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaofei Hui",
      "Haoxuan Qu",
      "Ping Hu",
      "Hossein Rahmani",
      "Jun Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sun_Knowledge_Distillation_with_Refined_Logits_ICCV_2025_paper.html": {
    "title": "Knowledge Distillation with Refined Logits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wujie Sun",
      "Defang Chen",
      "Siwei Lyu",
      "Genlang Chen",
      "Chun Chen",
      "Can Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Tracking_Tiny_Drones_against_Clutter_Large-Scale_Infrared_Benchmark_with_Motion-Centric_ICCV_2025_paper.html": {
    "title": "Tracking Tiny Drones against Clutter: Large-Scale Infrared Benchmark with Motion-Centric Adaptive Algorithm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahao Zhang",
      "Zongli Jiang",
      "Jinli Zhang",
      "Yixin Wei",
      "Liang Li",
      "Yizheng Wang",
      "Gang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xing_AID_Adapting_Image2Video_Diffusion_Models_for_Instruction-guided_Video_Prediction_ICCV_2025_paper.html": {
    "title": "AID: Adapting Image2Video Diffusion Models for Instruction-guided Video Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Xing",
      "Qi Dai",
      "Zejia Weng",
      "Zuxuan Wu",
      "Yu-Gang Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Mitra_Enhancing_Few-Shot_Vision-Language_Classification_with_Large_Multimodal_Model_Features_ICCV_2025_paper.html": {
    "title": "Enhancing Few-Shot Vision-Language Classification with Large Multimodal Model Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chancharik Mitra",
      "Brandon Huang",
      "Tianning Chai",
      "Zhiqiu Lin",
      "Assaf Arbelle",
      "Rogerio Feris",
      "Leonid Karlinsky",
      "Trevor Darrell",
      "Deva Ramanan",
      "Roei Herzig"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cai_DeepShield_Fortifying_Deepfake_Video_Detection_with_Local_and_Global_Forgery_ICCV_2025_paper.html": {
    "title": "DeepShield: Fortifying Deepfake Video Detection with Local and Global Forgery Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinqi Cai",
      "Jichang Li",
      "Zhaolun Li",
      "Weikai Chen",
      "Rushi Lan",
      "Xi Xie",
      "Xiaonan Luo",
      "Guanbin Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Synthesizing_Near-Boundary_OOD_Samples_for_Out-of-Distribution_Detection_ICCV_2025_paper.html": {
    "title": "Synthesizing Near-Boundary OOD Samples for Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinglun Li",
      "Kaixun Jiang",
      "Zhaoyu Chen",
      "Bo Lin",
      "Yao Tang",
      "Weifeng Ge",
      "Wenqiang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jang_Splat-based_3D_Scene_Reconstruction_with_Extreme_Motion-blur_ICCV_2025_paper.html": {
    "title": "Splat-based 3D Scene Reconstruction with Extreme Motion-blur",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyeonjoong Jang",
      "Dongyoung Choi",
      "Donggun Kim",
      "Woohyun Kang",
      "Min H. Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Son_Towards_Robustness_of_Person_Search_against_Corruptions_ICCV_2025_paper.html": {
    "title": "Towards Robustness of Person Search against Corruptions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Woojung Son",
      "Yoonki Cho",
      "Guoyuan An",
      "Chanmi Lee",
      "Sung-Eui Yoon"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xu_INSTINCT_Instance-Level_Interaction_Architecture_for_Query-Based_Collaborative_Perception_ICCV_2025_paper.html": {
    "title": "INSTINCT: Instance-Level Interaction Architecture for Query-Based Collaborative Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunjiang Xu",
      "Lingzhi Li",
      "Jin Wang",
      "Yupeng Ouyang",
      "Benyuan Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shi_DriveX_Omni_Scene_Modeling_for_Learning_Generalizable_World_Knowledge_in_ICCV_2025_paper.html": {
    "title": "DriveX: Omni Scene Modeling for Learning Generalizable World Knowledge in Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Shi",
      "Shaoshuai Shi",
      "Kehua Sheng",
      "Bo Zhang",
      "Li Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yao_MagicCity_Geometry-Aware_3D_City_Generation_from_Satellite_Imagery_with_Multi-View_ICCV_2025_paper.html": {
    "title": "MagicCity: Geometry-Aware 3D City Generation from Satellite Imagery with Multi-View Consistency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingbo Yao",
      "Xuanmin Wang",
      "Hao Wu",
      "Chengliang Ping",
      "Doudou Zhang",
      "Hui Xiong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhu_EDFFDNet_Towards_Accurate_and_Efficient_Unsupervised_Multi-Grid_Image_Registration_ICCV_2025_paper.html": {
    "title": "EDFFDNet: Towards Accurate and Efficient Unsupervised Multi-Grid Image Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haokai Zhu",
      "Bo Qu",
      "Si-Yuan Cao",
      "Runmin Zhang",
      "Shujie Chen",
      "Bailin Yang",
      "Hui-Liang Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Bassi_RadGPT_Constructing_3D_Image-Text_Tumor_Datasets_ICCV_2025_paper.html": {
    "title": "RadGPT: Constructing 3D Image-Text Tumor Datasets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pedro R.A.S. Bassi",
      "Mehmet Can Yavuz",
      "Ibrahim Ethem Hamamci",
      "Sezgin Er",
      "Xiaoxi Chen",
      "Wenxuan Li",
      "Bjoern Menze",
      "Sergio Decherchi",
      "Andrea Cavalli",
      "Kang Wang",
      "Yang Yang",
      "Alan Yuille",
      "Zongwei Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Adaptive_Prompt_Learning_via_Gaussian_Outlier_Synthesis_for_Out-of-distribution_Detection_ICCV_2025_paper.html": {
    "title": "Adaptive Prompt Learning via Gaussian Outlier Synthesis for Out-of-distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongkang Zhang",
      "Dongyu She",
      "Zhong Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liao_LangBridge_Interpreting_Image_as_a_Combination_of_Language_Embeddings_ICCV_2025_paper.html": {
    "title": "LangBridge: Interpreting Image as a Combination of Language Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqi Liao",
      "Yuwei Niu",
      "Fanqing Meng",
      "Hao Li",
      "Changyao Tian",
      "Yinuo Du",
      "Yuwen Xiong",
      "Dianqi Li",
      "Xizhou Zhu",
      "Li Yuan",
      "Jifeng Dai",
      "Yu Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hu_Anchor_Token_Matching_Implicit_Structure_Locking_for_Training-free_AR_Image_ICCV_2025_paper.html": {
    "title": "Anchor Token Matching: Implicit Structure Locking for Training-free AR Image Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taihang Hu",
      "Linxuan Li",
      "Kai Wang",
      "Yaxing Wang",
      "Jian Yang",
      "Ming-Ming Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wild_ArgoTweak_Towards_Self-Updating_HD_Maps_through_Structured_Priors_ICCV_2025_paper.html": {
    "title": "ArgoTweak: Towards Self-Updating HD Maps through Structured Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lena Wild",
      "Rafael Valencia",
      "Patric Jensfelt"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Singha_FedMVP_Federated_Multimodal_Visual_Prompt_Tuning_for_Vision-Language_Models_ICCV_2025_paper.html": {
    "title": "FedMVP: Federated Multimodal Visual Prompt Tuning for Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mainak Singha",
      "Subhankar Roy",
      "Sarthak Mehrotra",
      "Ankit Jha",
      "Moloud Abdar",
      "Biplab Banerjee",
      "Elisa Ricci"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_Robust_Dataset_Condensation_using_Supervised_Contrastive_Learning_ICCV_2025_paper.html": {
    "title": "Robust Dataset Condensation using Supervised Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicole Hee-Yeon Kim",
      "Hwanjun Song"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_TIP-I2V_A_Million-Scale_Real_Text_and_Image_Prompt_Dataset_for_ICCV_2025_paper.html": {
    "title": "TIP-I2V: A Million-Scale Real Text and Image Prompt Dataset for Image-to-Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhao Wang",
      "Yi Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_From_Reusing_to_Forecasting_Accelerating_Diffusion_Models_with_TaylorSeers_ICCV_2025_paper.html": {
    "title": "From Reusing to Forecasting: Accelerating Diffusion Models with TaylorSeers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiacheng Liu",
      "Chang Zou",
      "Yuanhuiyi Lyu",
      "Junjie Chen",
      "Linfeng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Weng_VisNumBench_Evaluating_Number_Sense_of_Multimodal_Large_Language_Models_ICCV_2025_paper.html": {
    "title": "VisNumBench: Evaluating Number Sense of Multimodal Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tengjin Weng",
      "Jingyi Wang",
      "Wenhao Jiang",
      "Zhong Ming"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sun_Invisible_Watermarks_Visible_Gains_Steering_Machine_Unlearning_with_Bi-Level_Watermarking_ICCV_2025_paper.html": {
    "title": "Invisible Watermarks, Visible Gains: Steering Machine Unlearning with Bi-Level Watermarking Design",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhao Sun",
      "Yihua Zhang",
      "Gaowen Liu",
      "Hongtao Xie",
      "Sijia Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_FICGen_Frequency-Inspired_Contextual_Disentanglement_for_Layout-driven_Degraded_Image_Generation_ICCV_2025_paper.html": {
    "title": "FICGen: Frequency-Inspired Contextual Disentanglement for Layout-driven Degraded Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenzhuang Wang",
      "Yifan Zhao",
      "Mingcan Ma",
      "Ming Liu",
      "Zhonglin Jiang",
      "Yong Chen",
      "Jia Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Quantifying_and_Narrowing_the_Unknown_Interactive_Text-to-Video_Retrieval_via_Uncertainty_ICCV_2025_paper.html": {
    "title": "Quantifying and Narrowing the Unknown: Interactive Text-to-Video Retrieval via Uncertainty Minimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bingqing Zhang",
      "Zhuo Cao",
      "Heming Du",
      "Yang Li",
      "Xue Li",
      "Jiajun Liu",
      "Sen Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yuan_ShortV_Efficient_Multimodal_Large_Language_Models_by_Freezing_Visual_Tokens_ICCV_2025_paper.html": {
    "title": "ShortV: Efficient Multimodal Large Language Models by Freezing Visual Tokens in Ineffective Layers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qianhao Yuan",
      "Qingyu Zhang",
      "Yanjiang Liu",
      "Jiawei Chen",
      "Yaojie Lu",
      "Hongyu Lin",
      "Jia Zheng",
      "Xianpei Han",
      "Le Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_SpikeDiff_Zero-shot_High-Quality_Video_Reconstruction_from_Chromatic_Spike_Camera_and_ICCV_2025_paper.html": {
    "title": "SpikeDiff: Zero-shot High-Quality Video Reconstruction from Chromatic Spike Camera and Sub-millisecond Spike Streams",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siqi Yang",
      "Jinxiu Liang",
      "Zhaojun Huang",
      "Yeliduosi Xiaokaiti",
      "Yakun Chang",
      "Zhaofei Yu",
      "Boxin Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Moon_Selective_Contrastive_Learning_for_Weakly_Supervised_Affordance_Grounding_ICCV_2025_paper.html": {
    "title": "Selective Contrastive Learning for Weakly Supervised Affordance Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "WonJun Moon",
      "Hyun Seok Seong",
      "Jae-Pil Heo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kapse_GECKO_Gigapixel_Vision-Concept_Contrastive_Pretraining_in_Histopathology_ICCV_2025_paper.html": {
    "title": "GECKO: Gigapixel Vision-Concept Contrastive Pretraining in Histopathology",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saarthak Kapse",
      "Pushpak Pati",
      "Srikar Yellapragada",
      "Srijan Das",
      "Rajarsi R. Gupta",
      "Joel Saltz",
      "Dimitris Samaras",
      "Prateek Prasanna"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hruby_Single-Scanline_Relative_Pose_Estimation_for_Rolling_Shutter_Cameras_ICCV_2025_paper.html": {
    "title": "Single-Scanline Relative Pose Estimation for Rolling Shutter Cameras",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Petr Hruby",
      "Marc Pollefeys"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lee_Compression_of_3D_Gaussian_Splatting_with_Optimized_Feature_Planes_and_ICCV_2025_paper.html": {
    "title": "Compression of 3D Gaussian Splatting with Optimized Feature Planes and Standard Video Codecs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soonbin Lee",
      "Fangwen Shu",
      "Yago Sanchez",
      "Thomas Schierl",
      "Cornelius Hellge"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zheng_CULTURE3D_A_Large-Scale_and_Diverse_Dataset_of_Cultural_Landmarks_and_ICCV_2025_paper.html": {
    "title": "CULTURE3D: A Large-Scale and Diverse Dataset of Cultural Landmarks and Terrains for Gaussian-Based Scene Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyi Zheng",
      "Steve Zhang",
      "Weizhe Lin",
      "Aaron Zhang",
      "Walterio W. Mayol-Cuevas",
      "Yunze Liu",
      "Junxiao Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jiang_Real3D_Towards_Scaling_Large_Reconstruction_Models_with_Real_Images_ICCV_2025_paper.html": {
    "title": "Real3D: Towards Scaling Large Reconstruction Models with Real Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanwen Jiang",
      "Qixing Huang",
      "Georgios Pavlakos"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lu_FA_Forced_Prompt_Learning_of_Vision-Language_Models_for_Out-of-Distribution_Detection_ICCV_2025_paper.html": {
    "title": "FA: Forced Prompt Learning of Vision-Language Models for Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinhua Lu",
      "Runhe Lai",
      "Yanqi Wu",
      "Kanghao Chen",
      "Wei-Shi Zheng",
      "Ruixuan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jiang_Geo4D_Leveraging_Video_Generators_for_Geometric_4D_Scene_Reconstruction_ICCV_2025_paper.html": {
    "title": "Geo4D: Leveraging Video Generators for Geometric 4D Scene Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeren Jiang",
      "Chuanxia Zheng",
      "Iro Laina",
      "Diane Larlus",
      "Andrea Vedaldi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Han_REPARO_Compositional_3D_Assets_Generation_with_Differentiable_3D_Layout_Alignment_ICCV_2025_paper.html": {
    "title": "REPARO: Compositional 3D Assets Generation with Differentiable 3D Layout Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haonan Han",
      "Rui Yang",
      "Huan Liao",
      "Jiankai Xing",
      "Zunnan Xu",
      "Xiaoming Yu",
      "Junwei Zha",
      "Xiu Li",
      "Wanhua Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jiang_VACE_All-in-One_Video_Creation_and_Editing_ICCV_2025_paper.html": {
    "title": "VACE: All-in-One Video Creation and Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyinzi Jiang",
      "Zhen Han",
      "Chaojie Mao",
      "Jingfeng Zhang",
      "Yulin Pan",
      "Yu Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_SparseMM_Head_Sparsity_Emerges_from_Visual_Concept_Responses_in_MLLMs_ICCV_2025_paper.html": {
    "title": "SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahui Wang",
      "Zuyan Liu",
      "Yongming Rao",
      "Jiwen Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Focal_Plane_Visual_Feature_Generation_and_Matching_on_a_Pixel_ICCV_2025_paper.html": {
    "title": "Focal Plane Visual Feature Generation and Matching on a Pixel Processor Array",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyi Zhang",
      "Laurie Bose",
      "Jianing Chen",
      "Piotr Dudek",
      "Walterio Mayol-Cuevas"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Q-Frame_Query-aware_Frame_Selection_and_Multi-Resolution_Adaptation_for_Video-LLMs_ICCV_2025_paper.html": {
    "title": "Q-Frame: Query-aware Frame Selection and Multi-Resolution Adaptation for Video-LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaojie Zhang",
      "Jiahui Yang",
      "Jianqin Yin",
      "Zhenbo Luo",
      "Jian Luan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Bian_Prompt-driven_Transferable_Adversarial_Attack_on_Person_Re-Identification_with_Attribute-aware_Textual_ICCV_2025_paper.html": {
    "title": "Prompt-driven Transferable Adversarial Attack on Person Re-Identification with Attribute-aware Textual Inversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Bian",
      "Min Liu",
      "Yunqi Yi",
      "Xueping Wang",
      "Shuai Jiang",
      "Yaonan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chow_OV-SCAN_Semantically_Consistent_Alignment_for_Novel_Object_Discovery_in_Open-Vocabulary_ICCV_2025_paper.html": {
    "title": "OV-SCAN: Semantically Consistent Alignment for Novel Object Discovery in Open-Vocabulary 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adrian Chow",
      "Evelien Riddell",
      "Yimu Wang",
      "Sean Sedwards",
      "Krzysztof Czarnecki"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_ContextFace_Generating_Facial_Expressions_from_Emotional_Contexts_ICCV_2025_paper.html": {
    "title": "ContextFace: Generating Facial Expressions from Emotional Contexts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Min-jung Kim",
      "Minsang Kim",
      "Seung Jun Baek"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Son_MDP-Omni_Parameter-free_Multimodal_Depth_Prior-based_Sampling_for_Omnidirectional_Stereo_Matching_ICCV_2025_paper.html": {
    "title": "MDP-Omni: Parameter-free Multimodal Depth Prior-based Sampling for Omnidirectional Stereo Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eunjin Son",
      "HyungGi Jo",
      "Wookyong Kwon",
      "Sang Jun Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Niss_The_Inter-Intra_Modal_Measure_A_Predictive_Lens_on_Fine-Tuning_Outcomes_ICCV_2025_paper.html": {
    "title": "The Inter-Intra Modal Measure: A Predictive Lens on Fine-Tuning Outcomes in Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Laura Niss",
      "Kevin Vogt-Lowell",
      "Theodoros Tsiligkaridis"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lee_Error_Recognition_in_Procedural_Videos_using_Generalized_Task_Graph_ICCV_2025_paper.html": {
    "title": "Error Recognition in Procedural Videos using Generalized Task Graph",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shih-Po Lee",
      "Ehsan Elhamifar"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jeong_Reangle-A-Video_4D_Video_Generation_as_Video-to-Video_Translation_ICCV_2025_paper.html": {
    "title": "Reangle-A-Video: 4D Video Generation as Video-to-Video Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyeonho Jeong",
      "Suhyeon Lee",
      "Jong Chul Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_DualReal_Adaptive_Joint_Training_for_Lossless_Identity-Motion_Fusion_in_Video_ICCV_2025_paper.html": {
    "title": "DualReal: Adaptive Joint Training for Lossless Identity-Motion Fusion in Video Customization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenchuan Wang",
      "Mengqi Huang",
      "Yijing Tu",
      "Zhendong Mao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xiao_RoboTron-Sim_Improving_Real-World_Driving_via_Simulated_Hard-Case_ICCV_2025_paper.html": {
    "title": "RoboTron-Sim: Improving Real-World Driving via Simulated Hard-Case",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baihui Xiao",
      "Chengjian Feng",
      "Zhijian Huang",
      "Feng Yan",
      "Yujie Zhong",
      "Lin Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Miao_Towards_Scalable_Spatial_Intelligence_via_2D-to-3D_Data_Lifting_ICCV_2025_paper.html": {
    "title": "Towards Scalable Spatial Intelligence via 2D-to-3D Data Lifting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyu Miao",
      "Haoran Duan",
      "Quanhao Qian",
      "Jiuniu Wang",
      "Yang Long",
      "Ling Shao",
      "Deli Zhao",
      "Ran Xu",
      "Gongjie Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Bernasconi_LDIP_Long_Distance_Information_Propagation_for_Video_Super-Resolution_ICCV_2025_paper.html": {
    "title": "LDIP: Long Distance Information Propagation for Video Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Bernasconi",
      "Abdelaziz Djelouah",
      "Yang Zhang",
      "Markus Gross",
      "Christopher Schroers"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Cross-Architecture_Distillation_Made_Simple_with_Redundancy_Suppression_ICCV_2025_paper.html": {
    "title": "Cross-Architecture Distillation Made Simple with Redundancy Suppression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weijia Zhang",
      "Yuehao Liu",
      "Wu Ran",
      "Chao Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_M2EIT_Multi-Domain_Mixture_of_Experts_for_Robust_Neural_Inertial_Tracking_ICCV_2025_paper.html": {
    "title": "M2EIT: Multi-Domain Mixture of Experts for Robust Neural Inertial Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Li",
      "Yang Xu",
      "Changhao Chen",
      "Zhongchen Shi",
      "Wei Chen",
      "Liang Xie",
      "Hongbo Chen",
      "Erwei Yin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Guo_ShortFT_Diffusion_Model_Alignment_via_Shortcut-based_Fine-Tuning_ICCV_2025_paper.html": {
    "title": "ShortFT: Diffusion Model Alignment via Shortcut-based Fine-Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiefan Guo",
      "Miaomiao Cui",
      "Liefeng Bo",
      "Di Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hu_Who_Controls_the_Authorization_Invertible_Networks_for_Copyright_Protection_in_ICCV_2025_paper.html": {
    "title": "Who Controls the Authorization? Invertible Networks for Copyright Protection in Text-to-Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baoyue Hu",
      "Yang Wei",
      "Junhao Xiao",
      "Wendong Huang",
      "Xiuli Bi",
      "Bin Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jang_Identity-aware_Language_Gaussian_Splatting_for_Open-vocabulary_3D_Semantic_Segmentation_ICCV_2025_paper.html": {
    "title": "Identity-aware Language Gaussian Splatting for Open-vocabulary 3D Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "SungMin Jang",
      "Wonjun Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_IDEATOR_Jailbreaking_and_Benchmarking_Large_Vision-Language_Models_Using_Themselves_ICCV_2025_paper.html": {
    "title": "IDEATOR: Jailbreaking and Benchmarking Large Vision-Language Models Using Themselves",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruofan Wang",
      "Juncheng Li",
      "Yixu Wang",
      "Bo Wang",
      "Xiaosen Wang",
      "Yan Teng",
      "Yingchun Wang",
      "Xingjun Ma",
      "Yu-Gang Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Degradation-Modeled_Multipath_Diffusion_for_Tunable_Metalens_Photography_ICCV_2025_paper.html": {
    "title": "Degradation-Modeled Multipath Diffusion for Tunable Metalens Photography",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianing Zhang",
      "Jiayi Zhu",
      "Feiyu Ji",
      "Xiaokang Yang",
      "Xiaoyun Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_Driving_View_Synthesis_on_Free-form_Trajectories_with_Generative_Prior_ICCV_2025_paper.html": {
    "title": "Driving View Synthesis on Free-form Trajectories with Generative Prior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyu Yang",
      "Zijie Pan",
      "Yuankun Yang",
      "Xiatian Zhu",
      "Li Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cheng_AnimeGamer_Infinite_Anime_Life_Simulation_with_Next_Game_State_Prediction_ICCV_2025_paper.html": {
    "title": "AnimeGamer: Infinite Anime Life Simulation with Next Game State Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junhao Cheng",
      "Yuying Ge",
      "Yixiao Ge",
      "Jing Liao",
      "Ying Shan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yu_OmniPaint_Mastering_Object-Oriented_Editing_via_Disentangled_Insertion-Removal_Inpainting_ICCV_2025_paper.html": {
    "title": "OmniPaint: Mastering Object-Oriented Editing via Disentangled Insertion-Removal Inpainting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongsheng Yu",
      "Ziyun Zeng",
      "Haitian Zheng",
      "Jiebo Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_UIPro_Unleashing_Superior_Interaction_Capability_For_GUI_Agents_ICCV_2025_paper.html": {
    "title": "UIPro: Unleashing Superior Interaction Capability For GUI Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongxin Li",
      "Jingran Su",
      "Jingfan Chen",
      "Zheng Ju",
      "Yuntao Chen",
      "Qing Li",
      "Zhaoxiang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lee_Class-Wise_Federated_Averaging_for_Efficient_Personalization_ICCV_2025_paper.html": {
    "title": "Class-Wise Federated Averaging for Efficient Personalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gyuejeong Lee",
      "Daeyoung Choi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kheradmand_StochasticSplats_Stochastic_Rasterization_for_Sorting-Free_3D_Gaussian_Splatting_ICCV_2025_paper.html": {
    "title": "StochasticSplats: Stochastic Rasterization for Sorting-Free 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shakiba Kheradmand",
      "Delio Vicini",
      "George Kopanas",
      "Dmitry Lagun",
      "Kwang Moo Yi",
      "Mark Matthews",
      "Andrea Tagliasacchi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xu_A0_An_Affordance-Aware_Hierarchical_Model_for_General_Robotic_Manipulation_ICCV_2025_paper.html": {
    "title": "A0: An Affordance-Aware Hierarchical Model for General Robotic Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rongtao Xu",
      "Jian Zhang",
      "Minghao Guo",
      "Youpeng Wen",
      "Haoting Yang",
      "Min Lin",
      "Jianzheng Huang",
      "Zhe Li",
      "Kaidong Zhang",
      "Liqiong Wang",
      "Yuxuan Kuang",
      "Meng Cao",
      "Feng Zheng",
      "Xiaodan Liang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tang_FinMMR_Make_Financial_Numerical_Reasoning_More_Multimodal_Comprehensive_and_Challenging_ICCV_2025_paper.html": {
    "title": "FinMMR: Make Financial Numerical Reasoning More Multimodal, Comprehensive, and Challenging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zichen Tang",
      "Haihong E",
      "Jiacheng Liu",
      "Zhongjun Yang",
      "Rongjin Li",
      "Zihua Rong",
      "Haoyang He",
      "Zhuodi Hao",
      "Xinyang Hu",
      "Kun Ji",
      "Ziyan Ma",
      "Mengyuan Ji",
      "Jun Zhang",
      "Chenghao Ma",
      "Qianhe Zheng",
      "Yang Liu",
      "Yiling Huang",
      "Xinyi Hu",
      "Qing Huang",
      "Zijian Xie",
      "Shiyao Peng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lai_A_Tiny_Change_A_Giant_Leap_Long-Tailed_Class-Incremental_Learning_via_ICCV_2025_paper.html": {
    "title": "A Tiny Change, A Giant Leap: Long-Tailed Class-Incremental Learning via Geometric Prototype Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyi Lai",
      "Luojun Lin",
      "Weijie Chen",
      "Yuanlong Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sun_X-Prompt_Generalizable_Auto-Regressive_Visual_Learning_with_In-Context_Prompting_ICCV_2025_paper.html": {
    "title": "X-Prompt: Generalizable Auto-Regressive Visual Learning with In-Context Prompting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyi Sun",
      "Ziyang Chu",
      "Pan Zhang",
      "Tong Wu",
      "Yuhang Zang",
      "Xiaoyi Dong",
      "Yuanjun Xiong",
      "Dahua Lin",
      "Jiaqi Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_HORT_Monocular_Hand-held_Objects_Reconstruction_with_Transformers_ICCV_2025_paper.html": {
    "title": "HORT: Monocular Hand-held Objects Reconstruction with Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zerui Chen",
      "Rolandos Alexandros Potamias",
      "Shizhe Chen",
      "Cordelia Schmid"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_Early_Timestep_Zero-Shot_Candidate_Selection_for_Instruction-Guided_Image_Editing_ICCV_2025_paper.html": {
    "title": "Early Timestep Zero-Shot Candidate Selection for Instruction-Guided Image Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joowon Kim",
      "Ziseok Lee",
      "Donghyeon Cho",
      "Sanghyun Jo",
      "Yeonsung Jung",
      "Kyungsu Kim",
      "Eunho Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Bilecen_Identity_Preserving_3D_Head_Stylization_with_Multiview_Score_Distillation_ICCV_2025_paper.html": {
    "title": "Identity Preserving 3D Head Stylization with Multiview Score Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bahri Batuhan Bilecen",
      "Ahmet Berke Gökmen",
      "Furkan Guzelant",
      "Aysegul Dundar"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Aligning_Information_Capacity_Between_Vision_and_Language_via_Dense-to-Sparse_Feature_ICCV_2025_paper.html": {
    "title": "Aligning Information Capacity Between Vision and Language via Dense-to-Sparse Feature Distillation for Image-Text Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Liu",
      "Wentao Feng",
      "Zhuoyao Liu",
      "Shudong Huang",
      "Jiancheng Lv"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Peng_A_Lesson_in_Splats_Teacher-Guided_Diffusion_for_3D_Gaussian_Splats_ICCV_2025_paper.html": {
    "title": "A Lesson in Splats: Teacher-Guided Diffusion for 3D Gaussian Splats Generation with 2D Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chensheng Peng",
      "Ido Sobol",
      "Masayoshi Tomizuka",
      "Kurt Keutzer",
      "Chenfeng Xu",
      "Or Litany"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cao_Boosting_Vision_Semantic_Density_with_Anatomy_Normality_Modeling_for_Medical_ICCV_2025_paper.html": {
    "title": "Boosting Vision Semantic Density with Anatomy Normality Modeling for Medical Vision-language Pre-training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiwei Cao",
      "Jianpeng Zhang",
      "Zhongyi Shui",
      "Sinuo Wang",
      "Zeli Chen",
      "Xi Li",
      "Le Lu",
      "Xianghua Ye",
      "Qi Zhang",
      "Tingbo Liang",
      "Ling Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_GAP_Gaussianize_Any_Point_Clouds_with_Text_Guidance_ICCV_2025_paper.html": {
    "title": "GAP: Gaussianize Any Point Clouds with Text Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiqi Zhang",
      "Junsheng Zhou",
      "Haotian Geng",
      "Wenyuan Zhang",
      "Yu-Shen Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Flow4Agent_Long-form_Video_Understanding_via_Motion_Prior_from_Optical_Flow_ICCV_2025_paper.html": {
    "title": "Flow4Agent: Long-form Video Understanding via Motion Prior from Optical Flow",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruyang Liu",
      "Shangkun Sun",
      "Haoran Tang",
      "Wei Gao",
      "Ge Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Moto_Latent_Motion_Token_as_the_Bridging_Language_for_Learning_ICCV_2025_paper.html": {
    "title": "Moto: Latent Motion Token as the Bridging Language for Learning Robot Manipulation from Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Chen",
      "Yuying Ge",
      "Weiliang Tang",
      "Yizhuo Li",
      "Yixiao Ge",
      "Mingyu Ding",
      "Ying Shan",
      "Xihui Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Causal-Entity_Reflected_Egocentric_Traffic_Accident_Video_Synthesis_ICCV_2025_paper.html": {
    "title": "Causal-Entity Reflected Egocentric Traffic Accident Video Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei-Lei Li",
      "Jianwu Fang",
      "Junbin Xiao",
      "Shanmin Pang",
      "Hongkai Yu",
      "Chen Lv",
      "Jianru Xue",
      "Tat-Seng Chua"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ma_BezierGS_Dynamic_Urban_Scene_Reconstruction_with_Bezier_Curve_Gaussian_Splatting_ICCV_2025_paper.html": {
    "title": "BezierGS: Dynamic Urban Scene Reconstruction with Bezier Curve Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zipei Ma",
      "Junzhe Jiang",
      "Yurui Chen",
      "Li Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_RealCam-I2V_Real-World_Image-to-Video_Generation_with_Interactive_Complex_Camera_Control_ICCV_2025_paper.html": {
    "title": "RealCam-I2V: Real-World Image-to-Video Generation with Interactive Complex Camera Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Teng Li",
      "Guangcong Zheng",
      "Rui Jiang",
      "Shuigen Zhan",
      "Tao Wu",
      "Yehao Lu",
      "Yining Lin",
      "Chuanyun Deng",
      "Yepan Xiong",
      "Min Chen",
      "Lin Cheng",
      "Xi Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Breaking_the_Encoder_Barrier_for_Seamless_Video-Language_Understanding_ICCV_2025_paper.html": {
    "title": "Breaking the Encoder Barrier for Seamless Video-Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Handong Li",
      "Yiyuan Zhang",
      "Longteng Guo",
      "Xiangyu Yue",
      "Jing Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/He_Simulating_Dual-Pixel_Images_From_Ray_Tracing_For_Depth_Estimation_ICCV_2025_paper.html": {
    "title": "Simulating Dual-Pixel Images From Ray Tracing For Depth Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fengchen He",
      "Dayang Zhao",
      "Hao Xu",
      "Tingwei Quan",
      "Shaoqun Zeng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Saad_How_Would_It_Sound_Material-Controlled_Multimodal_Acoustic_Profile_Generation_for_ICCV_2025_paper.html": {
    "title": "How Would It Sound? Material-Controlled Multimodal Acoustic Profile Generation for Indoor Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mahnoor Fatima Saad",
      "Ziad Al-Halah"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Thai_SplatTalk_3D_VQA_with_Gaussian_Splatting_ICCV_2025_paper.html": {
    "title": "SplatTalk: 3D VQA with Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anh Thai",
      "Songyou Peng",
      "Kyle Genova",
      "Leonidas Guibas",
      "Thomas Funkhouser"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_LocalDyGS_Multi-view_Global_Dynamic_Scene_Modeling_via_Adaptive_Local_Implicit_ICCV_2025_paper.html": {
    "title": "LocalDyGS: Multi-view Global Dynamic Scene Modeling via Adaptive Local Implicit Feature Decoupling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahao Wu",
      "Rui Peng",
      "Jianbo Jiao",
      "Jiayu Yang",
      "Luyang Tang",
      "Kaiqiang Xiong",
      "Jie Liang",
      "Jinbo Yan",
      "Runling Liu",
      "Ronggang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zheng_Teeth_Reconstruction_and_Performance_Capture_Using_a_Phone_Camera_ICCV_2025_paper.html": {
    "title": "Teeth Reconstruction and Performance Capture Using a Phone Camera",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weixi Zheng",
      "Jingwang Ling",
      "Zhibo Wang",
      "Quan Wang",
      "Feng Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Knowledge_Distillation_for_Learned_Image_Compression_ICCV_2025_paper.html": {
    "title": "Knowledge Distillation for Learned Image Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunuo Chen",
      "Zezheng Lyu",
      "Bing He",
      "Ning Cao",
      "Gang Chen",
      "Guo Lu",
      "Wenjun Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Training-Free_Class_Purification_for_Open-Vocabulary_Semantic_Segmentation_ICCV_2025_paper.html": {
    "title": "Training-Free Class Purification for Open-Vocabulary Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Chen",
      "Lingxiao Yang",
      "Yun Chen",
      "Nailong Zhao",
      "Jianhuang Lai",
      "Jie Shao",
      "Xiaohua Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Uddin_Unsupervised_Identification_of_Protein_Compositions_and_Conformations_via_Implicit_Content-Transformation_ICCV_2025_paper.html": {
    "title": "Unsupervised Identification of Protein Compositions and Conformations via Implicit Content-Transformation Disentanglement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mostofa Rafid Uddin",
      "Jana Armouti",
      "Min Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_VIGFace_Virtual_Identity_Generation_for_Privacy-Free_Face_Recognition_Dataset_ICCV_2025_paper.html": {
    "title": "VIGFace: Virtual Identity Generation for Privacy-Free Face Recognition Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minsoo Kim",
      "Min-Cheol Sagong",
      "Gi Pyo Nam",
      "Junghyun Cho",
      "Ig-Jae Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_Generalizable_Object_Re-Identification_via_Visual_In-Context_Prompting_ICCV_2025_paper.html": {
    "title": "Generalizable Object Re-Identification via Visual In-Context Prompting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhizhong Huang",
      "Xiaoming Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_GenHaze_Pioneering_Controllable_One-Step_Realistic_Haze_Generation_for_Real-World_Dehazing_ICCV_2025_paper.html": {
    "title": "GenHaze: Pioneering Controllable One-Step Realistic Haze Generation for Real-World Dehazing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sixiang Chen",
      "Tian Ye",
      "Yunlong Lin",
      "Yeying Jin",
      "Yijun  Yang",
      "Haoyu Chen",
      "Jianyu Lai",
      "Song Fei",
      "Zhaohu Xing",
      "Fugee Tsung",
      "Lei Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xu_LLaVA-CoT_Let_Vision_Language_Models_Reason_Step-by-Step_ICCV_2025_paper.html": {
    "title": "LLaVA-CoT: Let Vision Language Models Reason Step-by-Step",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guowei Xu",
      "Peng Jin",
      "Ziang Wu",
      "Hao Li",
      "Yibing Song",
      "Lichao Sun",
      "Li Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Mur-Labadia_O-MaMa_Learning_Object_Mask_Matching_between_Egocentric_and_Exocentric_Views_ICCV_2025_paper.html": {
    "title": "O-MaMa: Learning Object Mask Matching between Egocentric and Exocentric Views",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lorenzo Mur-Labadia",
      "Maria Santos-Villafranca",
      "Jesus Bermudez-Cameo",
      "Alejandro Perez-Yus",
      "Ruben Martinez-Cantin",
      "Jose J. Guerrero"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Rethinking_the_Embodied_Gap_in_Vision-and-Language_Navigation_A_Holistic_Study_ICCV_2025_paper.html": {
    "title": "Rethinking the Embodied Gap in Vision-and-Language Navigation: A Holistic Study of Physical and Visual Disparities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liuyi Wang",
      "Xinyuan Xia",
      "Hui Zhao",
      "Hanqing Wang",
      "Tai Wang",
      "Yilun Chen",
      "Chengju Liu",
      "Qijun Chen",
      "Jiangmiao Pang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhao_Pseudo-SD_Pseudo_Controlled_Stable_Diffusion_for_Semi-Supervised_and_Cross-Domain_Semantic_ICCV_2025_paper.html": {
    "title": "Pseudo-SD: Pseudo Controlled Stable Diffusion for Semi-Supervised and Cross-Domain Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dong Zhao",
      "Qi Zang",
      "Shuang Wang",
      "Nicu Sebe",
      "Zhun Zhong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_MaTVLM_Hybrid_Mamba-Transformer_for_Efficient_Vision-Language_Modeling_ICCV_2025_paper.html": {
    "title": "MaTVLM: Hybrid Mamba-Transformer for Efficient Vision-Language Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingyue Li",
      "Bencheng Liao",
      "Wenyu Liu",
      "Xinggang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_ETCH_Generalizing_Body_Fitting_to_Clothed_Humans_via_Equivariant_Tightness_ICCV_2025_paper.html": {
    "title": "ETCH: Generalizing Body Fitting to Clothed Humans via Equivariant Tightness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boqian Li",
      "Haiwen Feng",
      "Zeyu Cai",
      "Michael J. Black",
      "Yuliang Xiu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_SVIP_Semantically_Contextualized_Visual_Patches_for_Zero-Shot_Learning_ICCV_2025_paper.html": {
    "title": "SVIP: Semantically Contextualized Visual Patches for Zero-Shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhi Chen",
      "Zecheng Zhao",
      "Jingcai Guo",
      "Jingjing Li",
      "Zi Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Deng_MamTiff-CAD_Multi-Scale_Latent_Diffusion_with_Mamba_for_Complex_Parametric_Sequence_ICCV_2025_paper.html": {
    "title": "MamTiff-CAD: Multi-Scale Latent Diffusion with Mamba+ for Complex Parametric Sequence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liyuan Deng",
      "Yunpeng Bai",
      "Yongkang Dai",
      "Xiaoshui Huang",
      "Hongping Gan",
      "Dongshuo Huang",
      "Hao Jiacheng",
      "Yilei Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Guo_Forgetting_Through_Transforming_Enabling_Federated_Unlearning_via_Class-Aware_Representation_Transformation_ICCV_2025_paper.html": {
    "title": "Forgetting Through Transforming: Enabling Federated Unlearning via Class-Aware Representation Transformation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Guo",
      "Zhen Tian",
      "Minghao Yao",
      "Saiyu Qi",
      "Yong Qi",
      "Bingyi Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tran_ReasonVQA_A_Multi-hop_Reasoning_Benchmark_with_Structural_Knowledge_for_Visual_ICCV_2025_paper.html": {
    "title": "ReasonVQA: A Multi-hop Reasoning Benchmark with Structural Knowledge for Visual Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Duong T. Tran",
      "Trung-Kien Tran",
      "Manfred Hauswirth",
      "Danh Le Phuoc"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_HiNeuS_High-fidelity_Neural_Surface_Mitigating_Low-texture_and_Reflective_Ambiguity_ICCV_2025_paper.html": {
    "title": "HiNeuS: High-fidelity Neural Surface Mitigating Low-texture and Reflective Ambiguity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yida Wang",
      "Xueyang Zhang",
      "Kun Zhan",
      "Peng Jia",
      "Xianpeng Lang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_Rethinking_DPO-style_Diffusion_Aligning_Frameworks_ICCV_2025_paper.html": {
    "title": "Rethinking DPO-style Diffusion Aligning Frameworks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xun Wu",
      "Shaohan Huang",
      "Lingjie Jiang",
      "Furu Wei"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Dat_VSC_Visual_Search_Compositional_Text-to-Image_Diffusion_Model_ICCV_2025_paper.html": {
    "title": "VSC: Visual Search Compositional Text-to-Image Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Do Huu Dat",
      "Nam Hyeon-Woo",
      "Po-Yuan Mao",
      "Tae-Hyun Oh"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/You_FRET_Feature_Redundancy_Elimination_for_Test_Time_Adaptation_ICCV_2025_paper.html": {
    "title": "FRET: Feature Redundancy Elimination for Test Time Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linjing You",
      "Jiabao Lu",
      "Xiayuan Huang",
      "Xiangli Nie"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Dong_From_One_to_More_Contextual_Part_Latents_for_3D_Generation_ICCV_2025_paper.html": {
    "title": "From One to More: Contextual Part Latents for 3D Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaocong Dong",
      "Lihe Ding",
      "Xiao Chen",
      "Yaokun Li",
      "Yuxin Wang",
      "Yucheng Wang",
      "Qi Wang",
      "Jaehyeok Kim",
      "Chenjian Gao",
      "Zhanpeng Huang",
      "Zibin Wang",
      "Tianfan Xue",
      "Dan Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Saito_CaptionSmiths_Flexibly_Controlling_Language_Pattern_in_Image_Captioning_ICCV_2025_paper.html": {
    "title": "CaptionSmiths: Flexibly Controlling Language Pattern in Image Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kuniaki Saito",
      "Donghyun Kim",
      "Kwanyong Park",
      "Atsushi Hashimoto",
      "Yoshitaka Ushiku"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Luo_CoHD_A_Counting-Aware_Hierarchical_Decoding_Framework_for_Generalized_Referring_Expression_ICCV_2025_paper.html": {
    "title": "CoHD: A Counting-Aware Hierarchical Decoding Framework for Generalized Referring Expression Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuoyan Luo",
      "Yinghao Wu",
      "Tianheng Cheng",
      "Yong Liu",
      "Yicheng Xiao",
      "Hongfa Wang",
      "Xiao-Ping Zhang",
      "Yujiu Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_PersonalVideo_High_ID-Fidelity_Video_Customization_without_Dynamic_and_Semantic_Degradation_ICCV_2025_paper.html": {
    "title": "PersonalVideo: High ID-Fidelity Video Customization without Dynamic and Semantic Degradation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hengjia Li",
      "Haonan Qiu",
      "Shiwei Zhang",
      "Xiang Wang",
      "Yujie Wei",
      "Zekun Li",
      "Yingya Zhang",
      "Boxi Wu",
      "Deng Cai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_MP-HSIR_A_Multi-Prompt_Framework_for_Universal_Hyperspectral_Image_Restoration_ICCV_2025_paper.html": {
    "title": "MP-HSIR: A Multi-Prompt Framework for Universal Hyperspectral Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhehui Wu",
      "Yong Chen",
      "Naoto Yokoya",
      "Wei He"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_Learning_A_Unified_Template_for_Gait_Recognition_ICCV_2025_paper.html": {
    "title": "Learning A Unified Template for Gait Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Panjian Huang",
      "Saihui Hou",
      "Junzhou Huang",
      "Yongzhen Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chang_LANGTRAJ_Diffusion_Model_and_Dataset_for_Language-Conditioned_Trajectory_Simulation_ICCV_2025_paper.html": {
    "title": "LANGTRAJ: Diffusion Model and Dataset for Language-Conditioned Trajectory Simulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei-Jer Chang",
      "Wei Zhan",
      "Masayoshi Tomizuka",
      "Manmohan Chandraker",
      "Francesco  Pittaluga"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Park_Generative_Active_Learning_for_Long-tail_Trajectory_Prediction_via_Controllable_Diffusion_ICCV_2025_paper.html": {
    "title": "Generative Active Learning for Long-tail Trajectory Prediction via Controllable Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daehee Park",
      "Monu Surana",
      "Pranav Desai",
      "Ashish Mehta",
      "Reuben MV John",
      "Kuk-Jin Yoon"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_REGEN_Learning_Compact_Video_Embedding_with_Re-Generative_Decoder_ICCV_2025_paper.html": {
    "title": "REGEN: Learning Compact Video Embedding with (Re-)Generative Decoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yitian Zhang",
      "Long Mai",
      "Aniruddha Mahapatra",
      "David Bourgin",
      "Yicong Hong",
      "Jonah Casebeer",
      "Feng Liu",
      "Yun Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yu_DADet_Safeguarding_Image_Conditional_Diffusion_Models_against_Adversarial_and_Backdoor_ICCV_2025_paper.html": {
    "title": "DADet: Safeguarding Image Conditional Diffusion Models against Adversarial and Backdoor Attacks via Diffusion Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongwei Yu",
      "Xinlong Ding",
      "Jiawei Li",
      "Jinlong Wang",
      "Yudong Zhang",
      "Rongquan Wang",
      "Huimin Ma",
      "Jiansheng Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ackermann_CL-Splats_Continual_Learning_of_Gaussian_Splatting_with_Local_Optimization_ICCV_2025_paper.html": {
    "title": "CL-Splats: Continual Learning of Gaussian Splatting with Local Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jan Ackermann",
      "Jonas Kulhanek",
      "Shengqu Cai",
      "Haofei Xu",
      "Marc Pollefeys",
      "Gordon Wetzstein",
      "Leonidas J. Guibas",
      "Songyou Peng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Qiu_Bridging_Local_Inductive_Bias_and_Long-Range_Dependencies_with_Pixel-Mamba_for_ICCV_2025_paper.html": {
    "title": "Bridging Local Inductive Bias and Long-Range Dependencies with Pixel-Mamba for End-to-end Whole Slide Image Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongwei Qiu",
      "Hanqing Chao",
      "Tiancheng Lin",
      "Wanxing Chang",
      "Zijiang Yang",
      "Wenpei Jiao",
      "Yixuan Shen",
      "Yunshuo Zhang",
      "Yelin Yang",
      "Wenbin Liu",
      "Hui Jiang",
      "Yun Bian",
      "Ke Yan",
      "Dakai Jin",
      "Le Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lu_Inpaint4Drag_Repurposing_Inpainting_Models_for_Drag-Based_Image_Editing_via_Bidirectional_ICCV_2025_paper.html": {
    "title": "Inpaint4Drag: Repurposing Inpainting Models for Drag-Based Image Editing via Bidirectional Warping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyi Lu",
      "Kai Han"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tao_Transformed_Low-rank_Adaptation_via_Tensor_Decomposition_and_Its_Applications_to_ICCV_2025_paper.html": {
    "title": "Transformed Low-rank Adaptation via Tensor Decomposition and Its Applications to Text-to-image Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zerui Tao",
      "Yuhta Takida",
      "Naoki Murata",
      "Qibin Zhao",
      "Yuki Mitsufuji"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_PlaneRAS_Learning_Planar_Primitives_for_3D_Plane_Recovery_ICCV_2025_paper.html": {
    "title": "PlaneRAS: Learning Planar Primitives for 3D Plane Recovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fang Zhang",
      "Wenzhao Zheng",
      "Linqing Zhao",
      "Zelan Zhu",
      "Jiwen Lu",
      "Xiuzhuang Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_TARS_Traffic-Aware_Radar_Scene_Flow_Estimation_ICCV_2025_paper.html": {
    "title": "TARS: Traffic-Aware Radar Scene Flow Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jialong Wu",
      "Marco Braun",
      "Dominic Spata",
      "Matthias Rottmann"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Multi-Cache_Enhanced_Prototype_Learning_for_Test-Time_Generalization_of_Vision-Language_Models_ICCV_2025_paper.html": {
    "title": "Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyu Chen",
      "Haotian Zhai",
      "Can Zhang",
      "Xiupeng Shi",
      "Ruirui Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tu_MotionFollower_Editing_Video_Motion_via_Score-Guided_Diffusion_ICCV_2025_paper.html": {
    "title": "MotionFollower: Editing Video Motion via Score-Guided Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuyuan Tu",
      "Qi Dai",
      "Zihao Zhang",
      "Sicheng Xie",
      "Zhi-Qi Cheng",
      "Chong Luo",
      "Xintong Han",
      "Zuxuan Wu",
      "Yu-Gang Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Luo_Learning_Few-Step_Diffusion_Models_by_Trajectory_Distribution_Matching_ICCV_2025_paper.html": {
    "title": "Learning Few-Step Diffusion Models by Trajectory Distribution Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihong Luo",
      "Tianyang Hu",
      "Jiacheng Sun",
      "Yujun Cai",
      "Jing Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_Unified_Multi-Agent_Trajectory_Modeling_with_Masked_Trajectory_Diffusion_ICCV_2025_paper.html": {
    "title": "Unified Multi-Agent Trajectory Modeling with Masked Trajectory Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Songru Yang",
      "Zhenwei Shi",
      "Zhengxia Zou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Olsson_Certifiably_Optimal_Anisotropic_Rotation_Averaging_ICCV_2025_paper.html": {
    "title": "Certifiably Optimal Anisotropic Rotation Averaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Carl Olsson",
      "Yaroslava Lochman",
      "Johan Malmport",
      "Christopher Zach"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_TeRA_Rethinking_Text-guided_Realistic_3D_Avatar_Generation_ICCV_2025_paper.html": {
    "title": "TeRA: Rethinking Text-guided Realistic 3D Avatar Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanwen Wang",
      "Yiyu Zhuang",
      "Jiawei Zhang",
      "Li Wang",
      "Yifei Zeng",
      "Xun Cao",
      "Xinxin Zuo",
      "Hao Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lee_Understanding_Flatness_in_Generative_Models_Its_Role_and_Benefits_ICCV_2025_paper.html": {
    "title": "Understanding Flatness in Generative Models: Its Role and Benefits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taehwan Lee",
      "Kyeongkook Seo",
      "Jaejun Yoo",
      "Sung Whan Yoon"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_UniGlyph_Unified_Segmentation-Conditioned_Diffusion_for_Precise_Visual_Text_Synthesis_ICCV_2025_paper.html": {
    "title": "UniGlyph: Unified Segmentation-Conditioned Diffusion for Precise Visual Text Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanrui Wang",
      "Cong Han",
      "Yafei Li",
      "Zhipeng Jin",
      "Xiawei Li",
      "SiNan Du",
      "Wen Tao",
      "Shuanglong Li",
      "Yi Yang",
      "Chun Yuan",
      "Liu Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ma_DCHM_Depth-Consistent_Human_Modeling_for_Multiview_Detection_ICCV_2025_paper.html": {
    "title": "DCHM: Depth-Consistent Human Modeling for Multiview Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahao Ma",
      "Tianyu Wang",
      "Miaomiao Liu",
      "David Ahmedt-Aristizabal",
      "Chuong Nguyen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Niu_ChatReID_Open-ended_Interactive_Person_Retrieval_via_Hierarchical_Progressive_Tuning_for_ICCV_2025_paper.html": {
    "title": "ChatReID: Open-ended Interactive Person Retrieval via Hierarchical Progressive Tuning for Vision Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ke Niu",
      "Haiyang Yu",
      "Mengyang Zhao",
      "Teng Fu",
      "Siyang Yi",
      "Wei Lu",
      "Bin Li",
      "Xuelin Qian",
      "Xiangyang Xue"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Luo_TAViS_Text-bridged_Audio-Visual_Segmentation_with_Foundation_Models_ICCV_2025_paper.html": {
    "title": "TAViS: Text-bridged Audio-Visual Segmentation with Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyang Luo",
      "Nian Liu",
      "Xuguang Yang",
      "Salman Khan",
      "Rao Muhammad Anwer",
      "Hisham Cholakkal",
      "Fahad Shahbaz Khan",
      "Junwei Han"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Fu_ORION_A_Holistic_End-to-End_Autonomous_Driving_Framework_by_Vision-Language_Instructed_ICCV_2025_paper.html": {
    "title": "ORION: A Holistic End-to-End Autonomous Driving Framework by Vision-Language Instructed Action Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu Fu",
      "Diankun Zhang",
      "Zongchuang Zhao",
      "Jianfeng Cui",
      "Dingkang Liang",
      "Chong Zhang",
      "Dingyuan Zhang",
      "Hongwei Xie",
      "Bing Wang",
      "Xiang Bai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Mi_GeoExplorer_Active_Geo-localization_with_Curiosity-Driven_Exploration_ICCV_2025_paper.html": {
    "title": "GeoExplorer: Active Geo-localization with Curiosity-Driven Exploration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li Mi",
      "Manon Béchaz",
      "Zeming Chen",
      "Antoine Bosselut",
      "Devis Tuia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Quetu_LaCoOT_Layer_Collapse_through_Optimal_Transport_ICCV_2025_paper.html": {
    "title": "LaCoOT: Layer Collapse through Optimal Transport",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Victor Quétu",
      "Zhu Liao",
      "Nour Hezbri",
      "Fabio Pizzati",
      "Enzo Tartaglione"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Back_on_Track_Bundle_Adjustment_for_Dynamic_Scene_Reconstruction_ICCV_2025_paper.html": {
    "title": "Back on Track: Bundle Adjustment for Dynamic Scene Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weirong Chen",
      "Ganlin Zhang",
      "Felix Wimbauer",
      "Rui Wang",
      "Nikita Araslanov",
      "Andrea Vedaldi",
      "Daniel Cremers"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liang_ReferDINO_Referring_Video_Object_Segmentation_with_Visual_Grounding_Foundations_ICCV_2025_paper.html": {
    "title": "ReferDINO: Referring Video Object Segmentation with Visual Grounding Foundations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianming Liang",
      "Kun-Yu Lin",
      "Chaolei Tan",
      "Jianguo Zhang",
      "Wei-Shi Zheng",
      "Jian-Fang Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_LA-MOTR_End-to-End_Multi-Object_Tracking_by_Learnable_Association_ICCV_2025_paper.html": {
    "title": "LA-MOTR: End-to-End Multi-Object Tracking by Learnable Association",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peng Wang",
      "Yongcai Wang",
      "Hualong Cao",
      "Wang Chen",
      "Deying Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lee_NuiScene_Exploring_Efficient_Generation_of_Unbounded_Outdoor_Scenes_ICCV_2025_paper.html": {
    "title": "NuiScene: Exploring Efficient Generation of Unbounded Outdoor Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han-Hung Lee",
      "Qinghong Han",
      "Angel X. Chang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ansari_NegRefine_Refining_Negative_Label-Based_Zero-Shot_OOD_Detection_ICCV_2025_paper.html": {
    "title": "NegRefine: Refining Negative Label-Based Zero-Shot OOD Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amirhossein Ansari",
      "Ke Wang",
      "Pulei Xiong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sharma_DM-EFS_Dynamically_Multiplexed_Expanded_Features_Set_Form_for_Robust_and_ICCV_2025_paper.html": {
    "title": "DM-EFS: Dynamically Multiplexed Expanded Features Set Form for Robust and Efficient Small Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aashish Sharma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_OmniVTON_Training-Free_Universal_Virtual_Try-On_ICCV_2025_paper.html": {
    "title": "OmniVTON: Training-Free Universal Virtual Try-On",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaotong Yang",
      "Yuhui Li",
      "Shengfeng He",
      "Xinzhe Li",
      "Yangyang Xu",
      "Junyu Dong",
      "Yong Du"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lan_ACAM-KD_Adaptive_and_Cooperative_Attention_Masking_for_Knowledge_Distillation_ICCV_2025_paper.html": {
    "title": "ACAM-KD: Adaptive and Cooperative Attention Masking for Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qizhen Lan",
      "Qing Tian"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Han_InfGen_A_Resolution-Agnostic_Paradigm_for_Scalable_Image_Synthesis_ICCV_2025_paper.html": {
    "title": "InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Han",
      "Wanghan Xu",
      "Junchao Gong",
      "Xiaoyu Yue",
      "Song Guo",
      "Luping Zhou",
      "Lei Bai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/He_PlanGen_Towards_Unified_Layout_Planning_and_Image_Generation_in_Auto-Regressive_ICCV_2025_paper.html": {
    "title": "PlanGen: Towards Unified Layout Planning and Image Generation in Auto-Regressive Vision Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runze He",
      "Bo Cheng",
      "Yuhang Ma",
      "Qingxiang Jia",
      "Shanyuan Liu",
      "Ao Ma",
      "Xiaoyu Wu",
      "Liebucha Wu",
      "Dawei Leng",
      "Yuhui Yin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_AirCache_Activating_Inter-modal_Relevancy_KV_Cache_Compression_for_Efficient_Large_ICCV_2025_paper.html": {
    "title": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for Efficient Large Vision-Language Model Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Huang",
      "Hao Zou",
      "Bochen Wang",
      "Ye Xi",
      "Zhen Xie",
      "Hao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Completing_3D_Partial_Assemblies_with_View-Consistent_2D-3D_Correspondence_ICCV_2025_paper.html": {
    "title": "Completing 3D Partial Assemblies with View-Consistent 2D-3D Correspondence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weihao Wang",
      "Yu Lan",
      "Mingyu You",
      "Bin He"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Bao_Latte_Collaborative_Test-Time_Adaptation_of_Vision-Language_Models_in_Federated_Learning_ICCV_2025_paper.html": {
    "title": "Latte: Collaborative Test-Time Adaptation of Vision-Language Models in Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenxuan Bao",
      "Ruxi Deng",
      "Ruizhong Qiu",
      "Tianxin Wei",
      "Hanghang Tong",
      "Jingrui He"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_SSVQ_Unleashing_the_Potential_of_Vector_Quantization_with_Sign-Splitting_ICCV_2025_paper.html": {
    "title": "SSVQ: Unleashing the Potential of Vector Quantization with Sign-Splitting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuaiting Li",
      "Juncan Deng",
      "Chengxuan Wang",
      "Kedong Xu",
      "Rongtao Deng",
      "Hong Gu",
      "Haibin Shen",
      "Kejie Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Bisulco_From_Linearity_to_Non-Linearity_How_Masked_Autoencoders_Capture_Spatial_Correlations_ICCV_2025_paper.html": {
    "title": "From Linearity to Non-Linearity: How Masked Autoencoders Capture Spatial Correlations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anthony Bisulco",
      "Rahul Ramesh",
      "Randall Balestriero",
      "Pratik Chaudhari"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gandikota_SliderSpace_Decomposing_the_Visual_Capabilities_of_Diffusion_Models_ICCV_2025_paper.html": {
    "title": "SliderSpace: Decomposing the Visual Capabilities of Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rohit Gandikota",
      "Zongze Wu",
      "Richard Zhang",
      "David Bau",
      "Eli Shechtman",
      "Nick Kolkin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Bui_TrustMark_Robust_Watermarking_and_Watermark_Removal_for_Arbitrary_Resolution_Images_ICCV_2025_paper.html": {
    "title": "TrustMark: Robust Watermarking and Watermark Removal for Arbitrary Resolution Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tu Bui",
      "Shruti Agarwal",
      "John Collomosse"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Belouadi_TikZero_Zero-Shot_Text-Guided_Graphics_Program_Synthesis_ICCV_2025_paper.html": {
    "title": "TikZero: Zero-Shot Text-Guided Graphics Program Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonas Belouadi",
      "Eddy Ilg",
      "Margret Keuper",
      "Hideki Tanaka",
      "Masao Utiyama",
      "Raj Dabre",
      "Steffen Eger",
      "Simone Ponzetto"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_DrivingGPT_Unifying_Driving_World_Modeling_and_Planning_with_Multi-modal_Autoregressive_ICCV_2025_paper.html": {
    "title": "DrivingGPT: Unifying Driving World Modeling and Planning with Multi-modal Autoregressive Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuntao Chen",
      "Yuqi Wang",
      "Zhaoxiang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_One-Shot_Knowledge_Transfer_for_Scalable_Person_Re-Identification_ICCV_2025_paper.html": {
    "title": "One-Shot Knowledge Transfer for Scalable Person Re-Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Longhua Li",
      "Lei Qi",
      "Xin Geng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Du_GS-ID_Illumination_Decomposition_on_Gaussian_Splatting_via_Adaptive_Light_Aggregation_ICCV_2025_paper.html": {
    "title": "GS-ID: Illumination Decomposition on Gaussian Splatting via Adaptive Light Aggregation and Diffusion-Guided Material Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kang Du",
      "Zhihao Liang",
      "Yulin Shen",
      "Zeyu Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Qiu_FreeScale_Unleashing_the_Resolution_of_Diffusion_Models_via_Tuning-Free_Scale_ICCV_2025_paper.html": {
    "title": "FreeScale: Unleashing the Resolution of Diffusion Models via Tuning-Free Scale Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haonan Qiu",
      "Shiwei Zhang",
      "Yujie Wei",
      "Ruihang Chu",
      "Hangjie Yuan",
      "Xiang Wang",
      "Yingya Zhang",
      "Ziwei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_TokenUnify_Scaling_Up_Autoregressive_Pretraining_for_Neuron_Segmentation_ICCV_2025_paper.html": {
    "title": "TokenUnify: Scaling Up Autoregressive Pretraining for Neuron Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinda Chen",
      "Haoyuan Shi",
      "Xiaoyu Liu",
      "Te Shi",
      "Ruobing Zhang",
      "Dong Liu",
      "Zhiwei Xiong",
      "Feng Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_SAS_Segment_Any_3D_Scene_with_Integrated_2D_Priors_ICCV_2025_paper.html": {
    "title": "SAS: Segment Any 3D Scene with Integrated 2D Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuoyuan Li",
      "Jiahao Lu",
      "Jiacheng Deng",
      "Hanzhi Chang",
      "Lifan Wu",
      "Yanzhe Liang",
      "Tianzhu Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zuo_OMNI-DC_Highly_Robust_Depth_Completion_with_Multiresolution_Depth_Integration_ICCV_2025_paper.html": {
    "title": "OMNI-DC: Highly Robust Depth Completion with Multiresolution Depth Integration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiming Zuo",
      "Willow Yang",
      "Zeyu Ma",
      "Jia Deng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_4D_Gaussian_Splatting_SLAM_ICCV_2025_paper.html": {
    "title": "4D Gaussian Splatting SLAM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanyan Li",
      "Youxu Fang",
      "Zunjie Zhu",
      "Kunyi Li",
      "Yong  Ding",
      "Federico Tombari"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ho_PHD_Personalized_3D_Human_Body_Fitting_with_Point_Diffusion_ICCV_2025_paper.html": {
    "title": "PHD: Personalized 3D Human Body Fitting with Point Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hsuan-I Ho",
      "Chen Guo",
      "Po-Chen Wu",
      "Ivan Shugurov",
      "Chengcheng Tang",
      "Abhay Mittal",
      "Sizhe An",
      "Manuel Kaufmann",
      "Linguang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Park_SteerX_Creating_Any_Camera-Free_3D_and_4D_Scenes_with_Geometric_ICCV_2025_paper.html": {
    "title": "SteerX: Creating Any Camera-Free 3D and 4D Scenes with Geometric Steering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Byeongjun Park",
      "Hyojun Go",
      "Hyelin Nam",
      "Byung-Hoon Kim",
      "Hyungjin Chung",
      "Changick Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Beveridge_Hierarchical_Material_Recognition_from_Local_Appearance_ICCV_2025_paper.html": {
    "title": "Hierarchical Material Recognition from Local Appearance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthew Beveridge",
      "Shree K. Nayar"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kang_Unleashing_the_Temporal_Potential_of_Stereo_Event_Cameras_for_Continuous-Time_ICCV_2025_paper.html": {
    "title": "Unleashing the Temporal Potential of Stereo Event Cameras for Continuous-Time 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jae-Young Kang",
      "Hoonhee Cho",
      "Kuk-Jin Yoon"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tan_SweetTok_Semantic-Aware_Spatial-Temporal_Tokenizer_for_Compact_Video_Discretization_ICCV_2025_paper.html": {
    "title": "SweetTok: Semantic-Aware Spatial-Temporal Tokenizer for Compact Video Discretization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhentao Tan",
      "Ben Xue",
      "Jian Jia",
      "Junhao Wang",
      "Wencai Ye",
      "Shaoyun Shi",
      "Mingjie Sun",
      "Wenjin Wu",
      "Quan Chen",
      "Peng Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kuo_D-Attn_Decomposed_Attention_for_Large_Vision-and-Language_Model_ICCV_2025_paper.html": {
    "title": "D-Attn: Decomposed Attention for Large Vision-and-Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chia-Wen Kuo",
      "Sijie Zhu",
      "Fan Chen",
      "Xiaohui Shen",
      "Longyin Wen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_R1-VL_Learning_to_Reason_with_Multimodal_Large_Language_Models_via_ICCV_2025_paper.html": {
    "title": "R1-VL: Learning to Reason with Multimodal Large Language Models via Step-wise Group Relative Policy Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyi Zhang",
      "Jiaxing Huang",
      "Huanjin Yao",
      "Shunyu Liu",
      "Xikun Zhang",
      "Shijian Lu",
      "Dacheng Tao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Not_All_Degradations_Are_Equal_A_Targeted_Feature_Denoising_Framework_ICCV_2025_paper.html": {
    "title": "Not All Degradations Are Equal: A Targeted Feature Denoising Framework for Generalizable Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongjun Wang",
      "Jiyuan Chen",
      "Zhengwei Yin",
      "Xuan Song",
      "Yinqiang Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Rep-MTL_Unleashing_the_Power_of_Representation-level_Task_Saliency_for_Multi-Task_ICCV_2025_paper.html": {
    "title": "Rep-MTL: Unleashing the Power of Representation-level Task Saliency for Multi-Task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zedong Wang",
      "Siyuan Li",
      "Dan Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lyu_TLB-VFI_Temporal-Aware_Latent_Brownian_Bridge_Diffusion_for_Video_Frame_Interpolation_ICCV_2025_paper.html": {
    "title": "TLB-VFI: Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zonglin Lyu",
      "Chen Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhu_Gaussian_Splatting_with_Discretized_SDF_for_Relightable_Assets_ICCV_2025_paper.html": {
    "title": "Gaussian Splatting with Discretized SDF for Relightable Assets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zuo-Liang Zhu",
      "Jian Yang",
      "Beibei Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xie_Region-based_Cluster_Discrimination_for_Visual_Representation_Learning_ICCV_2025_paper.html": {
    "title": "Region-based Cluster Discrimination for Visual Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yin Xie",
      "Kaicheng Yang",
      "Xiang An",
      "Kun Wu",
      "Yongle Zhao",
      "Weimo Deng",
      "Zimin Ran",
      "Yumeng Wang",
      "Ziyong Feng",
      "Roy Miles",
      "Ismail Elezi",
      "Jiankang Deng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Steiner_AAA-Gaussians_Anti-Aliased_and_Artifact-Free_3D_Gaussian_Rendering_ICCV_2025_paper.html": {
    "title": "AAA-Gaussians: Anti-Aliased and Artifact-Free 3D Gaussian Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Steiner",
      "Thomas Köhler",
      "Lukas Radl",
      "Felix Windisch",
      "Dieter Schmalstieg",
      "Markus Steinberger"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Torbunov_EvRT-DETR_Latent_Space_Adaptation_of_Image_Detectors_for_Event-based_Vision_ICCV_2025_paper.html": {
    "title": "EvRT-DETR: Latent Space Adaptation of Image Detectors for Event-based Vision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dmitrii Torbunov",
      "Yihui Ren",
      "Animesh Ghose",
      "Odera Dim",
      "Yonggang Cui"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_MeshPad_Interactive_Sketch-Conditioned_Artist-Reminiscent_Mesh_Generation_and_Editing_ICCV_2025_paper.html": {
    "title": "MeshPad: Interactive Sketch-Conditioned Artist-Reminiscent Mesh Generation and Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoxuan Li",
      "Ziya Erkoç",
      "Lei Li",
      "Daniele Sirigatti",
      "Vladislav Rosov",
      "Angela Dai",
      "Matthias Nießner"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Soldan_ResidualViT_for_Efficient_Temporally_Dense_Video_Encoding_ICCV_2025_paper.html": {
    "title": "ResidualViT for Efficient Temporally Dense Video Encoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mattia Soldan",
      "Fabian Caba Heilbron",
      "Bernard Ghanem",
      "Josef Sivic",
      "Bryan Russell"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_UniCombine_Unified_Multi-Conditional_Combination_with_Diffusion_Transformer_ICCV_2025_paper.html": {
    "title": "UniCombine: Unified Multi-Conditional Combination with Diffusion Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoxuan Wang",
      "Jinlong Peng",
      "Qingdong He",
      "Hao Yang",
      "Ying Jin",
      "Jiafu Wu",
      "Xiaobin Hu",
      "Yanjie Pan",
      "Zhenye Gan",
      "Mingmin Chi",
      "Bo Peng",
      "Yabiao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Easi3R_Estimating_Disentangled_Motion_from_DUSt3R_Without_Training_ICCV_2025_paper.html": {
    "title": "Easi3R: Estimating Disentangled Motion from DUSt3R Without Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyu Chen",
      "Yue Chen",
      "Yuliang Xiu",
      "Andreas Geiger",
      "Anpei Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Mun_Addressing_Text_Embedding_Leakage_in_Diffusion-based_Image_Editing_ICCV_2025_paper.html": {
    "title": "Addressing Text Embedding Leakage in Diffusion-based Image Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sunung Mun",
      "Jinhwan Nam",
      "Sunghyun Cho",
      "Jungseul Ok"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_CaO2_Rectifying_Inconsistencies_in_Diffusion-Based_Dataset_Distillation_ICCV_2025_paper.html": {
    "title": "CaO2: Rectifying Inconsistencies in Diffusion-Based Dataset Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoxuan Wang",
      "Zhenghao Zhao",
      "Junyi Wu",
      "Yuzhang Shang",
      "Gaowen Liu",
      "Yan Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wei_HQ-CLIP_Leveraging_Large_Vision-Language_Models_to_Create_High-Quality_Image-Text_Datasets_ICCV_2025_paper.html": {
    "title": "HQ-CLIP: Leveraging Large Vision-Language Models to Create High-Quality Image-Text Datasets and CLIP Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhixiang Wei",
      "Guangting Wang",
      "Xiaoxiao Ma",
      "Ke Mei",
      "Huaian Chen",
      "Yi Jin",
      "Fengyun Rao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Bridging_the_Gap_Between_Ideal_and_Real-world_Evaluation_Benchmarking_AI-Generated_ICCV_2025_paper.html": {
    "title": "Bridging the Gap Between Ideal and Real-world Evaluation: Benchmarking AI-Generated Image Detection in Challenging Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chunxiao Li",
      "Xiaoxiao Wang",
      "Meiling Li",
      "Boming Miao",
      "Peng Sun",
      "Yunjian Zhang",
      "Xiangyang Ji",
      "Yao Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Progressive_Test_Time_Energy_Adaptation_for_Medical_Image_Segmentation_ICCV_2025_paper.html": {
    "title": "Progressive Test Time Energy Adaptation for Medical Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoran Zhang",
      "Byung-Woo Hong",
      "Hyoungseob Park",
      "Daniel H. Pak",
      "Anne-Marie Rickmann",
      "Lawrence H. Staib",
      "James S. Duncan",
      "Alex Wong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xu_Sequential_Gaussian_Avatars_with_Hierarchical_Motion_Context_ICCV_2025_paper.html": {
    "title": "Sequential Gaussian Avatars with Hierarchical Motion Context",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wangze Xu",
      "Yifan Zhan",
      "Zhihang Zhong",
      "Xiao Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Phan_Beyond_Losses_Reweighting_Empowering_Multi-Task_Learning_via_the_Generalization_Perspective_ICCV_2025_paper.html": {
    "title": "Beyond Losses Reweighting: Empowering Multi-Task Learning via the Generalization Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hoang Phan",
      "Lam Tran",
      "Quyen Tran",
      "Ngoc Tran",
      "Tuan Truong",
      "Qi Lei",
      "Nhat Ho",
      "Dinh Phung",
      "Trung Le"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Arsomngern_Zero-shot_Inexact_CAD_Model_Alignment_from_a_Single_Image_ICCV_2025_paper.html": {
    "title": "Zero-shot Inexact CAD Model Alignment from a Single Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pattaramanee Arsomngern",
      "Sasikarn Khwanmuang",
      "Matthias Nießner",
      "Supasorn Suwajanakorn"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Nguyen_Vulnerability-Aware_Spatio-Temporal_Learning_for_Generalizable_Deepfake_Video_Detection_ICCV_2025_paper.html": {
    "title": "Vulnerability-Aware Spatio-Temporal Learning for Generalizable Deepfake Video Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dat Nguyen",
      "Marcella Astrid",
      "Anis Kacem",
      "Enjie Ghorbel",
      "Djamila Aouada"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Deng_Efficient_Autoregressive_Shape_Generation_via_Octree-Based_Adaptive_Tokenization_ICCV_2025_paper.html": {
    "title": "Efficient Autoregressive Shape Generation via Octree-Based Adaptive Tokenization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kangle Deng",
      "Hsueh-Ti Derek Liu",
      "Yiheng Zhu",
      "Xiaoxia Sun",
      "Chong Shang",
      "Kiran S. Bhat",
      "Deva Ramanan",
      "Jun-Yan Zhu",
      "Maneesh Agrawala",
      "Tinghui Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cai_Cycle-Consistent_Learning_for_Joint_Layout-to-Image_Generation_and_Object_Detection_ICCV_2025_paper.html": {
    "title": "Cycle-Consistent Learning for Joint Layout-to-Image Generation and Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinhao Cai",
      "Qiuxia Lai",
      "Gensheng Pei",
      "Xiangbo Shu",
      "Yazhou Yao",
      "Wenguan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_MUG_Pseudo_Labeling_Augmented_Audio-Visual_Mamba_Network_for_Audio-Visual_Video_ICCV_2025_paper.html": {
    "title": "MUG: Pseudo Labeling Augmented Audio-Visual Mamba Network for Audio-Visual Video Parsing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Langyu Wang",
      "Bingke Zhu",
      "Yingying Chen",
      "Yiyuan Zhang",
      "Ming Tang",
      "Jinqiao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kazerouni_LIFT_Latent_Implicit_Functions_for_Task-_and_Data-Agnostic_Encoding_ICCV_2025_paper.html": {
    "title": "LIFT: Latent Implicit Functions for Task- and Data-Agnostic Encoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amirhossein Kazerouni",
      "Soroush Mehraban",
      "Michael Brudno",
      "Babak Taati"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Po_Long-Context_State-Space_Video_World_Models_ICCV_2025_paper.html": {
    "title": "Long-Context State-Space Video World Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryan Po",
      "Yotam Nitzan",
      "Richard Zhang",
      "Berlin Chen",
      "Tri Dao",
      "Eli Shechtman",
      "Gordon Wetzstein",
      "Xun Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Dataset_Distillation_via_the_Wasserstein_Metric_ICCV_2025_paper.html": {
    "title": "Dataset Distillation via the Wasserstein Metric",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyang Liu",
      "Yijiang Li",
      "Tiancheng Xing",
      "Peiran Wang",
      "Vibhu Dalal",
      "Luwei Li",
      "Jingrui He",
      "Haohan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cao_IRGPT_Understanding_Real-world_Infrared_Image_with_Bi-cross-modal_Curriculum_on_Large-scale_ICCV_2025_paper.html": {
    "title": "IRGPT: Understanding Real-world Infrared Image with Bi-cross-modal Curriculum on Large-scale Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhe Cao",
      "Jin Zhang",
      "Ruiheng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Luo_Mixed_Signals_A_Diverse_Point_Cloud_Dataset_for_Heterogeneous_LiDAR_ICCV_2025_paper.html": {
    "title": "Mixed Signals: A Diverse Point Cloud Dataset for Heterogeneous LiDAR V2X Collaboration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Katie Z Luo",
      "Minh-Quan Dao",
      "Zhenzhen Liu",
      "Mark Campbell",
      "Wei-Lun Chao",
      "Kilian Q Weinberger",
      "Ezio Malis",
      "Vincent Fremont",
      "Bharath Hariharan",
      "Mao Shan",
      "Stewart Worrall",
      "Julie Stephany Berrio Perez"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Pan_Exploring_Weather-aware_Aggregation_and_Adaptation_for_Semantic_Segmentation_under_Adverse_ICCV_2025_paper.html": {
    "title": "Exploring Weather-aware Aggregation and Adaptation for Semantic Segmentation under Adverse Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuwen Pan",
      "Rui Sun",
      "Wangkai Li",
      "Tianzhu Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_OracleFusion_Assisting_the_Decipherment_of_Oracle_Bone_Script_with_Structurally_ICCV_2025_paper.html": {
    "title": "OracleFusion: Assisting the Decipherment of Oracle Bone Script with Structurally Constrained Semantic Typography",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Caoshuo Li",
      "Zengmao Ding",
      "Xiaobin Hu",
      "Bang Li",
      "Donghao Luo",
      "AndyPian Wu",
      "Chaoyang Wang",
      "Chengjie Wang",
      "Taisong Jin",
      "Seven Shu",
      "Yunsheng Wu",
      "Yongge Liu",
      "Rongrong Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chatterjee_Streaming_VideoLLMs_for_Real-Time_Procedural_Video_Understanding_ICCV_2025_paper.html": {
    "title": "Streaming VideoLLMs for Real-Time Procedural Video Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dibyadip Chatterjee",
      "Edoardo Remelli",
      "Yale Song",
      "Bugra Tekin",
      "Abhay Mittal",
      "Bharat Bhatnagar",
      "Necati Cihan Camgoz",
      "Shreyas Hampali",
      "Eric Sauser",
      "Shugao Ma",
      "Angela Yao",
      "Fadime Sener"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Efficient_Visual_Place_Recognition_Through_Multimodal_Semantic_Knowledge_Integration_ICCV_2025_paper.html": {
    "title": "Efficient Visual Place Recognition Through Multimodal Semantic Knowledge Integration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sitao Zhang",
      "Hongda Mao",
      "Qingshuang Chen",
      "Yelin Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhao_Rethinking_Multi-modal_Object_Detection_from_the_Perspective_of_Mono-Modality_Feature_ICCV_2025_paper.html": {
    "title": "Rethinking Multi-modal Object Detection from the Perspective of Mono-Modality Feature Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyi Zhao",
      "Boyang Liu",
      "Yanglei Gao",
      "Yiming Sun",
      "Maoxun Yuan",
      "Xingxing Wei"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yu_VEGGIE_Instructional_Editing_and_Reasoning_Video_Concepts_with_Grounded_Generation_ICCV_2025_paper.html": {
    "title": "VEGGIE: Instructional Editing and Reasoning Video Concepts with Grounded Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shoubin Yu",
      "Difan Liu",
      "Ziqiao Ma",
      "Yicong Hong",
      "Yang Zhou",
      "Hao Tan",
      "Joyce Chai",
      "Mohit Bansal"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ke_Task-Aware_Prompt_Gradient_Projection_for_Parameter-Efficient_Tuning_Federated_Class-Incremental_Learning_ICCV_2025_paper.html": {
    "title": "Task-Aware Prompt Gradient Projection for Parameter-Efficient Tuning Federated Class-Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hualong Ke",
      "Jiangming Shi",
      "Yachao Zhang",
      "Fangyong Wang",
      "Yuan Xie",
      "Yanyun Qu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_Mixture-of-Scores_Robust_Image-Text_Data_Valuation_via_Three_Lines_of_Code_ICCV_2025_paper.html": {
    "title": "Mixture-of-Scores: Robust Image-Text Data Valuation via Three Lines of Code",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sitong Wu",
      "Haoru Tan",
      "Yukang Chen",
      "Shaofeng Zhang",
      "Jingyao Li",
      "Bei Yu",
      "Xiaojuan Qi",
      "Jiaya Jia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Son_DMesh_An_Efficient_Differentiable_Mesh_for_Complex_Shapes_ICCV_2025_paper.html": {
    "title": "DMesh++: An Efficient Differentiable Mesh for Complex Shapes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanghyun Son",
      "Matheus Gadelha",
      "Yang Zhou",
      "Matthew Fisher",
      "Zexiang Xu",
      "Yi-Ling Qiao",
      "Ming C. Lin",
      "Yi Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jia_From_Imitation_to_Innovation_The_Emergence_of_AIs_Unique_Artistic_ICCV_2025_paper.html": {
    "title": "From Imitation to Innovation: The Emergence of AI's Unique Artistic Styles and the Challenge of Copyright Protection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zexi Jia",
      "Chuanwei Huang",
      "Yeshuang Zhu",
      "Hongyan Fei",
      "Ying Deng",
      "Zhiqiang Yuan",
      "Jiapei Zhang",
      "Jinchao Zhang",
      "Jie Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cheng_VPO_Aligning_Text-to-Video_Generation_Models_with_Prompt_Optimization_ICCV_2025_paper.html": {
    "title": "VPO: Aligning Text-to-Video Generation Models with Prompt Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiale Cheng",
      "Ruiliang Lyu",
      "Xiaotao Gu",
      "Xiao Liu",
      "Jiazheng Xu",
      "Yida Lu",
      "Jiayan Teng",
      "Zhuoyi Yang",
      "Yuxiao Dong",
      "Jie Tang",
      "Hongning Wang",
      "Minlie Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xiong_Intervening_in_Black_Box_Concept_Bottleneck_Model_for_Enhancing_Human_ICCV_2025_paper.html": {
    "title": "Intervening in Black Box: Concept Bottleneck Model for Enhancing Human Neural Network Mutual Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nuoye Xiong",
      "Anqi Dong",
      "Ning Wang",
      "Cong Hua",
      "Guangming Zhu",
      "Lin Mei",
      "Peiyi Shen",
      "Liang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_Versatile_Transition_Generation_with_Image-to-Video_Diffusion_ICCV_2025_paper.html": {
    "title": "Versatile Transition Generation with Image-to-Video Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zuhao Yang",
      "Jiahui Zhang",
      "Yingchen Yu",
      "Shijian Lu",
      "Song Bai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_SMGDiff_Soccer_Motion_Generation_using_Diffusion_Probabilistic_Models_ICCV_2025_paper.html": {
    "title": "SMGDiff: Soccer Motion Generation using Diffusion Probabilistic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongdi Yang",
      "Chengyang Li",
      "Zhenxuan Wu",
      "Gaozheng Li",
      "Jingya Wang",
      "Jingyi Yu",
      "Zhuo Su",
      "Lan Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Han_Extrapolated_Urban_View_Synthesis_Benchmark_ICCV_2025_paper.html": {
    "title": "Extrapolated Urban View Synthesis Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangyu Han",
      "Zhen Jia",
      "Boyi Li",
      "Yan Wang",
      "Boris Ivanovic",
      "Yurong You",
      "Lingjie Liu",
      "Yue Wang",
      "Marco Pavone",
      "Chen Feng",
      "Yiming Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lee_Emulating_Self-attention_with_Convolution_for_Efficient_Image_Super-Resolution_ICCV_2025_paper.html": {
    "title": "Emulating Self-attention with Convolution for Efficient Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongheon Lee",
      "Seokju Yun",
      "Youngmin Ro"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Song_CoDa-4DGS_Dynamic_Gaussian_Splatting_with_Context_and_Deformation_Awareness_for_ICCV_2025_paper.html": {
    "title": "CoDa-4DGS: Dynamic Gaussian Splatting with Context and Deformation Awareness for Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Song",
      "Chenwei Liang",
      "Yan Xia",
      "Walter Zimmer",
      "Hu Cao",
      "Holger Caesar",
      "Andreas Festag",
      "Alois Knoll"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Paliwal_RI3D_Few-Shot_Gaussian_Splatting_With_Repair_and_Inpainting_Diffusion_Priors_ICCV_2025_paper.html": {
    "title": "RI3D: Few-Shot Gaussian Splatting With Repair and Inpainting Diffusion Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Avinash Paliwal",
      "Xilong Zhou",
      "Wei Ye",
      "Jinhui Xiong",
      "Rakesh Ranjan",
      "Nima Khademi Kalantari"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lee_Text_Embedding_Knows_How_to_Quantize_Text-Guided_Diffusion_Models_ICCV_2025_paper.html": {
    "title": "Text Embedding Knows How to Quantize Text-Guided Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongjae Lee",
      "Myungjun Son",
      "Dongjea Kang",
      "Seung-Won Jung"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cao_UniVerse_Unleashing_the_Scene_Prior_of_Video_Diffusion_Models_for_ICCV_2025_paper.html": {
    "title": "UniVerse: Unleashing the Scene Prior of Video Diffusion Models for Robust Radiance Field Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jin Cao",
      "Hongrui Wu",
      "Ziyong Feng",
      "Hujun Bao",
      "Xiaowei Zhou",
      "Sida Peng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Shape_of_Motion_4D_Reconstruction_from_a_Single_Video_ICCV_2025_paper.html": {
    "title": "Shape of Motion: 4D Reconstruction from a Single Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qianqian Wang",
      "Vickie Ye",
      "Hang Gao",
      "Weijia Zeng",
      "Jake Austin",
      "Zhengqi Li",
      "Angjoo Kanazawa"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gao_From_Gallery_to_Wrist_Realistic_3D_Bracelet_Insertion_in_Videos_ICCV_2025_paper.html": {
    "title": "From Gallery to Wrist: Realistic 3D Bracelet Insertion in Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenjian Gao",
      "Lihe Ding",
      "Rui Han",
      "Zhanpeng Huang",
      "Zibin Wang",
      "Tianfan Xue"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cho_Representing_3D_Shapes_with_64_Latent_Vectors_for_3D_Diffusion_ICCV_2025_paper.html": {
    "title": "Representing 3D Shapes with 64 Latent Vectors for 3D Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "In Cho",
      "Youngbeom Yoo",
      "Subin Jeon",
      "Seon Joo Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yu_Backdoor_Defense_via_Enhanced_Splitting_and_Trap_Isolation_ICCV_2025_paper.html": {
    "title": "Backdoor Defense via Enhanced Splitting and Trap Isolation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongrui Yu",
      "Lu Qi",
      "Wanyu Lin",
      "Jian Chen",
      "Hailong Sun",
      "Chengbin Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Miao_FedVLA_Federated_Vision-Language-Action_Learning_with_Dual_Gating_Mixture-of-Experts_for_Robotic_ICCV_2025_paper.html": {
    "title": "FedVLA: Federated Vision-Language-Action Learning with Dual Gating Mixture-of-Experts for Robotic Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cui Miao",
      "Tao Chang",
      "Meihan Wu",
      "Hongbin Xu",
      "Chun Li",
      "Ming Li",
      "Xiaodong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhu_BokehDiff_Neural_Lens_Blur_with_One-Step_Diffusion_ICCV_2025_paper.html": {
    "title": "BokehDiff: Neural Lens Blur with One-Step Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengxuan Zhu",
      "Qingnan Fan",
      "Qi Zhang",
      "Jinwei Chen",
      "Huaqi Zhang",
      "Chao Xu",
      "Boxin Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sick_CutS3D_Cutting_Semantics_in_3D_for_2D_Unsupervised_Instance_Segmentation_ICCV_2025_paper.html": {
    "title": "CutS3D: Cutting Semantics in 3D for 2D Unsupervised Instance Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leon Sick",
      "Dominik Engel",
      "Sebastian Hartwig",
      "Pedro Hermosilla",
      "Timo Ropinski"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lee_Perspective-Aware_Reasoning_in_Vision-Language_Models_via_Mental_Imagery_Simulation_ICCV_2025_paper.html": {
    "title": "Perspective-Aware Reasoning in Vision-Language Models via Mental Imagery Simulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Phillip Y. Lee",
      "Jihyeon Je",
      "Chanho Park",
      "Mikaela Angelina Uy",
      "Leonidas Guibas",
      "Minhyuk Sung"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Fan_Salvaging_the_Overlooked_Leveraging_Class-Aware_Contrastive_Learning_for_Multi-Class_Anomaly_ICCV_2025_paper.html": {
    "title": "Salvaging the Overlooked: Leveraging Class-Aware Contrastive Learning for Multi-Class Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Fan",
      "Junjie Huang",
      "Donglin Di",
      "Anyang Su",
      "Tianyou Song",
      "Maurice Pagnucco",
      "Yang Song"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_GUAVA_Generalizable_Upper_Body_3D_Gaussian_Avatar_ICCV_2025_paper.html": {
    "title": "GUAVA: Generalizable Upper Body 3D Gaussian Avatar",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongbin Zhang",
      "Yunfei Liu",
      "Lijian Lin",
      "Ye Zhu",
      "Yang Li",
      "Minghan Qin",
      "Yu Li",
      "Haoqian Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhu_PASG_A_Closed-Loop_Framework_for_Automated_Geometric_Primitive_Extraction_and_ICCV_2025_paper.html": {
    "title": "PASG: A Closed-Loop Framework for Automated Geometric Primitive Extraction and Semantic Anchoring in Robotic Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihao Zhu",
      "Yifan Zheng",
      "Siyu Pan",
      "Yaohui Jin",
      "Yao Mu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Guo_FastVAR_Linear_Visual_Autoregressive_Modeling_via_Cached_Token_Pruning_ICCV_2025_paper.html": {
    "title": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hang Guo",
      "Yawei Li",
      "Taolin Zhang",
      "Jiangshan Wang",
      "Tao Dai",
      "Shu-Tao Xia",
      "Luca Benini"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jung_Online_Generic_Event_Boundary_Detection_ICCV_2025_paper.html": {
    "title": "Online Generic Event Boundary Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyungrok Jung",
      "Daneul Kim",
      "Seunggyun Lim",
      "Jeany Son",
      "Jonghyun Choi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shin_Registration_beyond_Points_General_Affine_Subspace_Alignment_via_Geodesic_Distance_ICCV_2025_paper.html": {
    "title": "Registration beyond Points: General Affine Subspace Alignment via Geodesic Distance on Grassmann Manifold",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaeho Shin",
      "Hyeonjae Gil",
      "Junwoo Jang",
      "Maani Ghaffari",
      "Ayoung Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Disentangling_Instance_and_Scene_Contexts_for_3D_Semantic_Scene_Completion_ICCV_2025_paper.html": {
    "title": "Disentangling Instance and Scene Contexts for 3D Semantic Scene Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Enyu Liu",
      "En Yu",
      "Sijia Chen",
      "Wenbing Tao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zheng_World4Drive_End-to-End_Autonomous_Driving_via_Intention-aware_Physical_Latent_World_Model_ICCV_2025_paper.html": {
    "title": "World4Drive: End-to-End Autonomous Driving via Intention-aware Physical Latent World Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yupeng Zheng",
      "Pengxuan Yang",
      "Zebin Xing",
      "Qichao Zhang",
      "Yuhang Zheng",
      "Yinfeng Gao",
      "Pengfei Li",
      "Teng Zhang",
      "Zhongpu Xia",
      "Peng Jia",
      "XianPeng Lang",
      "Dongbin Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Katragadda_Online_Language_Splatting_ICCV_2025_paper.html": {
    "title": "Online Language Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saimouli Katragadda",
      "Cho-Ying Wu",
      "Yuliang Guo",
      "Xinyu Huang",
      "Guoquan Huang",
      "Liu Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Amodal_Depth_Anything_Amodal_Depth_Estimation_in_the_Wild_ICCV_2025_paper.html": {
    "title": "Amodal Depth Anything: Amodal Depth Estimation in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenyu Li",
      "Mykola Lavreniuk",
      "Jian Shi",
      "Shariq Farooq Bhat",
      "Peter Wonka"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_Hierarchical_Variational_Test-Time_Prompt_Generation_for_Zero-Shot_Generalization_ICCV_2025_paper.html": {
    "title": "Hierarchical Variational Test-Time Prompt Generation for Zero-Shot Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoyang Wu",
      "Fang Liu",
      "Licheng Jiao",
      "Shuo Li",
      "Lingling Li",
      "Xu Liu",
      "Puhua Chen",
      "Wenping Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_VTimeCoT_Thinking_by_Drawing_for_Video_Temporal_Grounding_and_Reasoning_ICCV_2025_paper.html": {
    "title": "VTimeCoT: Thinking by Drawing for Video Temporal Grounding and Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinglei Zhang",
      "Yuanfan Guo",
      "Rolandos Alexandros Potamias",
      "Jiankang Deng",
      "Hang Xu",
      "Chao Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_RA-BUSSeg_Relation-aware_Semi-supervised_Breast_Ultrasound_Image_Segmentation_via_Adjacent_Propagation_ICCV_2025_paper.html": {
    "title": "RA-BUSSeg: Relation-aware Semi-supervised Breast Ultrasound Image Segmentation via Adjacent Propagation and Cross-layer Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wanting Zhang",
      "Zhenhui Ding",
      "Guilian Chen",
      "Huisi Wu",
      "Jing Qin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zeng_Factorized_Learning_for_Temporally_Grounded_Video-Language_Models_ICCV_2025_paper.html": {
    "title": "Factorized Learning for Temporally Grounded Video-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenzheng Zeng",
      "Difei Gao",
      "Mike Zheng Shou",
      "Hwee Tou Ng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Decouple_to_Reconstruct_High_Quality_UHD_Restoration_via_Active_Feature_ICCV_2025_paper.html": {
    "title": "Decouple to Reconstruct: High Quality UHD Restoration via Active Feature Disentanglement and Reversible Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yidi Liu",
      "Dong Li",
      "Yuxin Ma",
      "Jie Huang",
      "Wenlong Zhang",
      "Xueyang Fu",
      "Zheng-Jun Zha"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kumari_Generating_Multi-Image_Synthetic_Data_for_Text-to-Image_Customization_ICCV_2025_paper.html": {
    "title": "Generating Multi-Image Synthetic Data for Text-to-Image Customization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nupur Kumari",
      "Xi Yin",
      "Jun-Yan Zhu",
      "Ishan Misra",
      "Samaneh Azadi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_WeaveSeg_Iterative_Contrast-weaving_and_Spectral_Feature-refining_for_Nuclei_Instance_Segmentation_ICCV_2025_paper.html": {
    "title": "WeaveSeg: Iterative Contrast-weaving and Spectral Feature-refining for Nuclei Instance Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiajia Li",
      "Huisi Wu",
      "Jing Qin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Towards_Long-Horizon_Vision-Language-Action_System_Reasoning_Acting_and_Memory_ICCV_2025_paper.html": {
    "title": "Towards Long-Horizon Vision-Language-Action System: Reasoning, Acting and Memory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daixun Li",
      "Yusi Zhang",
      "Mingxiang Cao",
      "Donglai Liu",
      "Weiying Xie",
      "Tianlin Hui",
      "Lunkai Lin",
      "Zhiqiang Xie",
      "Yunsong Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ou_MR-FIQA_Face_Image_Quality_Assessment_with_Multi-Reference_Representations_from_Synthetic_ICCV_2025_paper.html": {
    "title": "MR-FIQA: Face Image Quality Assessment with Multi-Reference Representations from Synthetic Data Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fu-Zhao Ou",
      "Chongyi Li",
      "Shiqi Wang",
      "Sam Kwong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_IDFace_Face_Template_Protection_for_Efficient_and_Secure_Identification_ICCV_2025_paper.html": {
    "title": "IDFace: Face Template Protection for Efficient and Secure Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sunpill Kim",
      "Seunghun Paik",
      "Chanwoo Hwang",
      "Dongsoo Kim",
      "Junbum Shin",
      "Jae Hong Seo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Choi_A_Real-world_Display_Inverse_Rendering_Dataset_ICCV_2025_paper.html": {
    "title": "A Real-world Display Inverse Rendering Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seokjun Choi",
      "Hoon-Gyu Chung",
      "Yujin Jeon",
      "Giljoo Nam",
      "Seung-Hwan Baek"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhong_Lyra_An_Efficient_and_Speech-Centric_Framework_for_Omni-Cognition_ICCV_2025_paper.html": {
    "title": "Lyra: An Efficient and Speech-Centric Framework for Omni-Cognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhisheng Zhong",
      "Chengyao Wang",
      "Yuqi Liu",
      "Senqiao Yang",
      "Longxiang Tang",
      "Yuechen Zhang",
      "Jingyao Li",
      "Tianyuan Qu",
      "Yanwei Li",
      "Yukang Chen",
      "Shaozuo Yu",
      "Sitong Wu",
      "Eric Lo",
      "Shu Liu",
      "Jiaya Jia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Golyadkin_MEH_A_Multi-Style_Dataset_and_Toolkit_for_Advancing_Egyptian_Hieroglyph_ICCV_2025_paper.html": {
    "title": "MEH: A Multi-Style Dataset and Toolkit for Advancing Egyptian Hieroglyph Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maksim Golyadkin",
      "Valeria Rubanova",
      "Aleksandr Utkov",
      "Dmitry Nikolotov",
      "Ilya Makarov"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Pei_D2ST-Adapter_Disentangled-and-Deformable_Spatio-Temporal_Adapter_for_Few-shot_Action_Recognition_ICCV_2025_paper.html": {
    "title": "D2ST-Adapter: Disentangled-and-Deformable Spatio-Temporal Adapter for Few-shot Action Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenjie Pei",
      "Qizhong Tan",
      "Guangming Lu",
      "Jiandong Tian",
      "Jun Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Naaman_Synchronization_of_Multiple_Videos_ICCV_2025_paper.html": {
    "title": "Synchronization of Multiple Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Avihai Naaman",
      "Ron Shapira Weber",
      "Oren Freifeld"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xie_Hi-Gaussian_Hierarchical_Gaussians_under_Normalized_Spherical_Projection_for_Single-View_3D_ICCV_2025_paper.html": {
    "title": "Hi-Gaussian: Hierarchical Gaussians under Normalized Spherical Projection for Single-View 3D Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Binjian Xie",
      "Pengju Zhang",
      "Hao Wei",
      "Yihong Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liao_MotionAgent_Fine-grained_Controllable_Video_Generation_via_Motion_Field_Agent_ICCV_2025_paper.html": {
    "title": "MotionAgent: Fine-grained Controllable Video Generation via Motion Field Agent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyao Liao",
      "Xianfang Zeng",
      "Liao Wang",
      "Gang Yu",
      "Guosheng Lin",
      "Chi Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lei_The_Scalability_of_Simplicity_Empirical_Analysis_of_Vision-Language_Learning_with_ICCV_2025_paper.html": {
    "title": "The Scalability of Simplicity: Empirical Analysis of Vision-Language Learning with a Single Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weixian Lei",
      "Jiacong Wang",
      "Haochen Wang",
      "Xiangtai Li",
      "Jun Hao Liew",
      "Jiashi Feng",
      "Zilong Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_PacGDC_Label-Efficient_Generalizable_Depth_Completion_with_Projection_Ambiguity_and_Consistency_ICCV_2025_paper.html": {
    "title": "PacGDC: Label-Efficient Generalizable Depth Completion with Projection Ambiguity and Consistency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haotian Wang",
      "Aoran Xiao",
      "Xiaoqin Zhang",
      "Meng Yang",
      "Shijian Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tao_GSV3D_Gaussian_Splatting-based_Geometric_Distillation_with_Stable_Video_Diffusion_for_ICCV_2025_paper.html": {
    "title": "GSV3D: Gaussian Splatting-based Geometric Distillation with Stable Video Diffusion for Single-Image 3D Object Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ye Tao",
      "Jiawei Zhang",
      "Yahao Shi",
      "Dongqing Zou",
      "Bin Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hou_OpenAnimals_Revisiting_Person_Re-Identification_for_Animals_Towards_Better_Generalization_ICCV_2025_paper.html": {
    "title": "OpenAnimals: Revisiting Person Re-Identification for Animals Towards Better Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saihui Hou",
      "Panjian Huang",
      "Zengbin Wang",
      "Yuan Liu",
      "Zeyu Li",
      "Man Zhang",
      "Yongzhen Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tran_VSRM_A_Robust_Mamba-Based_Framework_for_Video_Super-Resolution_ICCV_2025_paper.html": {
    "title": "VSRM: A Robust Mamba-Based Framework for Video Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dinh Phu Tran",
      "Dao Duy Hung",
      "Daeyoung Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lu_End-to-End_Multi-Modal_Diffusion_Mamba_ICCV_2025_paper.html": {
    "title": "End-to-End Multi-Modal Diffusion Mamba",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chunhao Lu",
      "Qiang Lu",
      "Meichen Dong",
      "Jake Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kreber_Guiding_Diffusion-Based_Articulated_Object_Generation_by_Partial_Point_Cloud_Alignment_ICCV_2025_paper.html": {
    "title": "Guiding Diffusion-Based Articulated Object Generation by Partial Point Cloud Alignment and Physical Plausibility Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jens U. Kreber",
      "Joerg Stueckler"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_PoseSyn_Synthesizing_Diverse_3D_Pose_Data_from_In-the-Wild_2D_Data_ICCV_2025_paper.html": {
    "title": "PoseSyn: Synthesizing Diverse 3D Pose Data from In-the-Wild 2D Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "ChangHee Yang",
      "Hyeonseop Song",
      "Seokhun Choi",
      "Seungwoo Lee",
      "Jaechul Kim",
      "Hoseok Do"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Backdoor_Attacks_on_Neural_Networks_via_One-Bit_Flip_ICCV_2025_paper.html": {
    "title": "Backdoor Attacks on Neural Networks via One-Bit Flip",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Li",
      "Lannan Luo",
      "Qiang Zeng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/He_Vector_Contrastive_Learning_For_Pixel-Wise_Pretraining_In_Medical_Vision_ICCV_2025_paper.html": {
    "title": "Vector Contrastive Learning For Pixel-Wise Pretraining In Medical Vision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuting He",
      "Shuo Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Benefit_From_Seen_Enhancing_Open-Vocabulary_Object_Detection_by_Bridging_Visual_ICCV_2025_paper.html": {
    "title": "Benefit From Seen: Enhancing Open-Vocabulary Object Detection by Bridging Visual and Textual Co-Occurrence Knowledge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanqi Li",
      "Jianwei Niu",
      "Tao Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yan_EEdit__Rethinking_the_Spatial_and_Temporal_Redundancy_for_Efficient_ICCV_2025_paper.html": {
    "title": "EEdit : Rethinking the Spatial and Temporal Redundancy for Efficient Image Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zexuan Yan",
      "Yue Ma",
      "Chang Zou",
      "Wenteng Chen",
      "Qifeng Chen",
      "Linfeng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Niu_Enhancing_Adversarial_Transferability_by_Balancing_Exploration_and_Exploitation_with_Gradient-Guided_ICCV_2025_paper.html": {
    "title": "Enhancing Adversarial Transferability by Balancing Exploration and Exploitation with Gradient-Guided Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zenghao Niu",
      "Weicheng Xie",
      "Siyang Song",
      "Zitong Yu",
      "Feng Liu",
      "Linlin Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_SuperEdit_Rectifying_and_Facilitating_Supervision_for_Instruction-Based_Image_Editing_ICCV_2025_paper.html": {
    "title": "SuperEdit: Rectifying and Facilitating Supervision for Instruction-Based Image Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ming Li",
      "Xin Gu",
      "Fan Chen",
      "Xiaoying Xing",
      "Longyin Wen",
      "Chen Chen",
      "Sijie Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ma_HPSv3_Towards_Wide-Spectrum_Human_Preference_Score_ICCV_2025_paper.html": {
    "title": "HPSv3: Towards Wide-Spectrum Human Preference Score",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhang Ma",
      "Xiaoshi Wu",
      "Keqiang Sun",
      "Hongsheng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_NullSwap_Proactive_Identity_Cloaking_Against_Deepfake_Face_Swapping_ICCV_2025_paper.html": {
    "title": "NullSwap: Proactive Identity Cloaking Against Deepfake Face Swapping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyi Wang",
      "Shuaicheng Niu",
      "Harry Cheng",
      "Xiao Zhang",
      "Yinglong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_SynAD_Enhancing_Real-World_End-to-End_Autonomous_Driving_Models_through_Synthetic_Data_ICCV_2025_paper.html": {
    "title": "SynAD: Enhancing Real-World End-to-End Autonomous Driving Models through Synthetic Data Integration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jongsuk Kim",
      "Jaeyoung Lee",
      "Gyojin Han",
      "Dong-Jae Lee",
      "Minki Jeong",
      "Junmo Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yazdanpanah_Purge-Gate_Backpropagation-Free_Test-Time_Adaptation_for_Point_Clouds_Classification_via_Token_ICCV_2025_paper.html": {
    "title": "Purge-Gate: Backpropagation-Free Test-Time Adaptation for Point Clouds Classification via Token purging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Moslem Yazdanpanah",
      "Ali Bahri",
      "Mehrdad Noori",
      "Sahar Dastani",
      "Gustavo Adolfo Vargas Hakim",
      "David Osowiechi",
      "Ismail Ben Ayed",
      "Christian Desrosiers"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_External_Knowledge_Injection_for_CLIP-Based_Class-Incremental_Learning_ICCV_2025_paper.html": {
    "title": "External Knowledge Injection for CLIP-Based Class-Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Da-Wei Zhou",
      "Kai-Wen Li",
      "Jingyi Ning",
      "Han-Jia Ye",
      "Lijun Zhang",
      "De-Chuan Zhan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yasuki_GeoProg3D_Compositional_Visual_Reasoning_for_City-Scale_3D_Language_Fields_ICCV_2025_paper.html": {
    "title": "GeoProg3D: Compositional Visual Reasoning for City-Scale 3D Language Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shunsuke Yasuki",
      "Taiki Miyanishi",
      "Nakamasa Inoue",
      "Shuhei Kurita",
      "Koya Sakamoto",
      "Daichi Azuma",
      "Masato Taki",
      "Yutaka Matsuo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Geometry_Distributions_ICCV_2025_paper.html": {
    "title": "Geometry Distributions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Biao Zhang",
      "Jing Ren",
      "Peter Wonka"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shi_PVChat_Personalized_Video_Chat_with_One-Shot_Learning_ICCV_2025_paper.html": {
    "title": "PVChat: Personalized Video Chat with One-Shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufei Shi",
      "Weilong Yan",
      "Gang Xu",
      "Yumeng Li",
      "Yucheng Chen",
      "Zhenxi Li",
      "Fei Yu",
      "Ming Li",
      "Si Yong Yeo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_VertexRegen_Mesh_Generation_with_Continuous_Level_of_Detail_ICCV_2025_paper.html": {
    "title": "VertexRegen: Mesh Generation with Continuous Level of Detail",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Zhang",
      "Yawar Siddiqui",
      "Armen Avetisyan",
      "Chris Xie",
      "Jakob Engel",
      "Henry Howard-Jenkins"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sun_Low-Light_Image_Enhancement_Using_Event-Based_Illumination_Estimation_ICCV_2025_paper.html": {
    "title": "Low-Light Image Enhancement Using Event-Based Illumination Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Sun",
      "Yuhan Bao",
      "Jiajun Zhai",
      "Jingyun Liang",
      "Yulun Zhang",
      "Kaiwei Wang",
      "Danda Pani Paudel",
      "Luc Van Gool"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Fu_FontAnimate_High_Quality_Few-shot_Font_Generation_via_Animating_Font_Transfer_ICCV_2025_paper.html": {
    "title": "FontAnimate: High Quality Few-shot Font Generation via Animating Font Transfer Process",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bin Fu",
      "Zixuan Wang",
      "Kainan Yan",
      "Shitian Zhao",
      "Qi Qin",
      "Jie Wen",
      "Junjun He",
      "Peng Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Qian_A_Good_Teacher_Adapts_Their_Knowledge_for_Distillation_ICCV_2025_paper.html": {
    "title": "A Good Teacher Adapts Their Knowledge for Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengyao Qian",
      "Trung Le",
      "Mehrtash Harandi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yu_GenFlowRL_Shaping_Rewards_with_Generative_Object-Centric_Flow_in_Visual_Reinforcement_ICCV_2025_paper.html": {
    "title": "GenFlowRL: Shaping Rewards with Generative Object-Centric Flow in Visual Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kelin Yu",
      "Sheng Zhang",
      "Harshit Soora",
      "Furong Huang",
      "Heng Huang",
      "Pratap Tokekar",
      "Ruohan Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cho_MAVFlow_Preserving_Paralinguistic_Elements_with_Conditional_Flow_Matching_for_Zero-Shot_ICCV_2025_paper.html": {
    "title": "MAVFlow: Preserving Paralinguistic Elements with Conditional Flow Matching for Zero-Shot AV2AV Multilingual Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sungwoo Cho",
      "Jeongsoo Choi",
      "Sungnyun Kim",
      "Se-Young Yun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shin_Video_Color_Grading_via_Look-Up_Table_Generation_ICCV_2025_paper.html": {
    "title": "Video Color Grading via Look-Up Table Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seunghyun Shin",
      "Dongmin Shin",
      "Jisu Shin",
      "Hae-Gon Jeon",
      "Joon-Young Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_NeuraLeaf_Neural_Parametric_Leaf_Models_with_Shape_and_Deformation_Disentanglement_ICCV_2025_paper.html": {
    "title": "NeuraLeaf: Neural Parametric Leaf Models with Shape and Deformation Disentanglement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Yang",
      "Dongni Mao",
      "Hiroaki Santo",
      "Yasuyuki Matsushita",
      "Fumio Okura"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_DC-AR_Efficient_Masked_Autoregressive_Image_Generation_with_Deep_Compression_Hybrid_ICCV_2025_paper.html": {
    "title": "DC-AR: Efficient Masked Autoregressive Image Generation with Deep Compression Hybrid Tokenizer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yecheng Wu",
      "Han Cai",
      "Junyu Chen",
      "Zhuoyang Zhang",
      "Enze Xie",
      "Jincheng Yu",
      "Junsong Chen",
      "Jinyi Hu",
      "Yao Lu",
      "Song Han"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_BVINet_Unlocking_Blind_Video_Inpainting_with_Zero_Annotations_ICCV_2025_paper.html": {
    "title": "BVINet: Unlocking Blind Video Inpainting with Zero Annotations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiliang Wu",
      "Kerui Chen",
      "Kun Li",
      "Hehe Fan",
      "Yi Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Biswas_Semi-supervised_Deep_Transfer_for_Regression_without_Domain_Alignment_ICCV_2025_paper.html": {
    "title": "Semi-supervised Deep Transfer for Regression without Domain Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mainak Biswas",
      "Ambedkar Dukkipati",
      "Devarajan Sridharan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhu_Evading_Data_Provenance_in_Deep_Neural_Networks_ICCV_2025_paper.html": {
    "title": "Evading Data Provenance in Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyu Zhu",
      "Sichu Liang",
      "Wenwen Wang",
      "Zhuomeng Zhang",
      "Fangqi Li",
      "Shi-Lin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ren_Vamba_Understanding_Hour-Long_Videos_with_Hybrid_Mamba-Transformers_ICCV_2025_paper.html": {
    "title": "Vamba: Understanding Hour-Long Videos with Hybrid Mamba-Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiming Ren",
      "Wentao Ma",
      "Huan Yang",
      "Cong Wei",
      "Ge Zhang",
      "Wenhu Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Manual-PA_Learning_3D_Part_Assembly_from_Instruction_Diagrams_ICCV_2025_paper.html": {
    "title": "Manual-PA: Learning 3D Part Assembly from Instruction Diagrams",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahao Zhang",
      "Anoop Cherian",
      "Cristian Rodriguez",
      "Weijian Deng",
      "Stephen Gould"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_RMultiplex200K_Toward_Reliable_Multimodal_Process_Supervision_for_Visual_Language_Models_ICCV_2025_paper.html": {
    "title": "RMultiplex200K: Toward Reliable Multimodal Process Supervision for Visual Language Models on Telecommunications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sijia Chen",
      "Bin Song"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Video-T1_Test-time_Scaling_for_Video_Generation_ICCV_2025_paper.html": {
    "title": "Video-T1: Test-time Scaling for Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangfu Liu",
      "Hanyang Wang",
      "Yimo Cai",
      "Kaiyan Zhang",
      "Xiaohang Zhan",
      "Yueqi Duan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_MemoryTalker_Personalized_Speech-Driven_3D_Facial_Animation_via_Audio-Guided_Stylization_ICCV_2025_paper.html": {
    "title": "MemoryTalker: Personalized Speech-Driven 3D Facial Animation via Audio-Guided Stylization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyung Kyu Kim",
      "Sangmin Lee",
      "Hak Gu Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhai_Text2Outfit_Controllable_Outfit_Generation_with_Multimodal_Language_Models_ICCV_2025_paper.html": {
    "title": "Text2Outfit: Controllable Outfit Generation with Multimodal Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanhao Zhai",
      "Yen-Liang Lin",
      "Minxu Peng",
      "Larry S. Davis",
      "Ashwin Chandramouli",
      "Junsong Yuan",
      "David Doermann"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Suo_From_Trial_to_Triumph_Advancing_Long_Video_Understanding_via_Visual_ICCV_2025_paper.html": {
    "title": "From Trial to Triumph: Advancing Long Video Understanding via Visual Context Sample Scaling and Self-reward Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yucheng Suo",
      "Fan Ma",
      "Linchao Zhu",
      "Tianyi Wang",
      "Fengyun Rao",
      "Yi Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sheng_CA2C_A_Prior-Knowledge-Free_Approach_for_Robust_Label_Noise_Learning_via_ICCV_2025_paper.html": {
    "title": "CA2C: A Prior-Knowledge-Free Approach for Robust Label Noise Learning via Asymmetric Co-learning and Co-training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengmeng Sheng",
      "Zeren Sun",
      "Tianfei Zhou",
      "Xiangbo Shu",
      "Jinshan Pan",
      "Yazhou Yao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_HairCUP_Hair_Compositional_Universal_Prior_for_3D_Gaussian_Avatars_ICCV_2025_paper.html": {
    "title": "HairCUP: Hair Compositional Universal Prior for 3D Gaussian Avatars",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Byungjun Kim",
      "Shunsuke Saito",
      "Giljoo Nam",
      "Tomas Simon",
      "Jason Saragih",
      "Hanbyul Joo",
      "Junxuan Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tan_Prototype-based_Contrastive_Learning_with_Stage-wise_Progressive_Augmentation_for_Self-Supervised_Fine-Grained_ICCV_2025_paper.html": {
    "title": "Prototype-based Contrastive Learning with Stage-wise Progressive Augmentation for Self-Supervised Fine-Grained Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baofeng Tan",
      "Xiu-Shen Wei",
      "Lin Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_4DSegStreamer_Streaming_4D_Panoptic_Segmentation_via_Dual_Threads_ICCV_2025_paper.html": {
    "title": "4DSegStreamer: Streaming 4D Panoptic Segmentation via Dual Threads",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ling Liu",
      "Jun Tian",
      "Li Yi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xie_GS-LIVM_Real-Time_Photo-Realistic_LiDAR-Inertial-Visual_Mapping_with_Gaussian_Splatting_ICCV_2025_paper.html": {
    "title": "GS-LIVM: Real-Time Photo-Realistic LiDAR-Inertial-Visual Mapping with Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yusen Xie",
      "Zhenmin Huang",
      "Jin Wu",
      "Jun Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_Temporal_Unlearnable_Examples_Preventing_Personal_Video_Data_from_Unauthorized_Exploitation_ICCV_2025_paper.html": {
    "title": "Temporal Unlearnable Examples: Preventing Personal Video Data from Unauthorized Exploitation by Object Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiangqiang Wu",
      "Yi Yu",
      "Chenqi Kong",
      "Ziquan Liu",
      "Jia Wan",
      "Haoliang Li",
      "Alex C. Kot",
      "Antoni B. Chan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Deng_Self-Calibrating_Gaussian_Splatting_for_Large_Field-of-View_Reconstruction_ICCV_2025_paper.html": {
    "title": "Self-Calibrating Gaussian Splatting for Large Field-of-View Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youming Deng",
      "Wenqi Xian",
      "Guandao Yang",
      "Leonidas Guibas",
      "Gordon Wetzstein",
      "Steve Marschner",
      "Paul Debevec"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Bhowmik_TWIST__SCOUT_Grounding_Multimodal_LLM-Experts_by_Forget-Free_Tuning_ICCV_2025_paper.html": {
    "title": "TWIST & SCOUT: Grounding Multimodal LLM-Experts by Forget-Free Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aritra Bhowmik",
      "Mohammad Mahdi Derakhshani",
      "Dennis Koelma",
      "Yuki M. Asano",
      "Martin R. Oswald",
      "Cees G. M. Snoek"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yoo_Automated_Model_Evaluation_for_Object_Detection_via_Prediction_Consistency_and_ICCV_2025_paper.html": {
    "title": "Automated Model Evaluation for Object Detection via Prediction Consistency and Reliability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seungju Yoo",
      "Hyuk Kwon",
      "Joong-Won Hwang",
      "Kibok Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_VSP_Diagnosing_the_Dual_Challenges_of_Perception_and_Reasoning_in_ICCV_2025_paper.html": {
    "title": "VSP: Diagnosing the Dual Challenges of Perception and Reasoning in Spatial Planning Tasks for MLLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiucheng Wu",
      "Handong Zhao",
      "Michael Saxon",
      "Trung Bui",
      "William Yang Wang",
      "Yang Zhang",
      "Shiyu Chang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liang_Instance-Level_Video_Depth_in_Groups_Beyond_Occlusions_ICCV_2025_paper.html": {
    "title": "Instance-Level Video Depth in Groups Beyond Occlusions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Liang",
      "Yang Zhou",
      "Ziming Sun",
      "Tianyi Xiang",
      "Guiqing Li",
      "Shengfeng He"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yu_ODP-Bench_Benchmarking_Out-of-Distribution_Performance_Prediction_ICCV_2025_paper.html": {
    "title": "ODP-Bench: Benchmarking Out-of-Distribution Performance Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Yu",
      "Kehan Li",
      "Dongbai Li",
      "Yue He",
      "Xingxuan Zhang",
      "Peng Cui"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Stepping_Out_of_Similar_Semantic_Space_for_Open-Vocabulary_Segmentation_ICCV_2025_paper.html": {
    "title": "Stepping Out of Similar Semantic Space for Open-Vocabulary Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yong Liu",
      "Song-Li Wu",
      "Sule Bai",
      "Jiahao Wang",
      "Yitong Wang",
      "Yansong Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gotz_Sibai_A_Few-Shot_Meta-Classifier_for_Poisoning_Detection_in_Federated_Learning_ICCV_2025_paper.html": {
    "title": "Sibai: A Few-Shot Meta-Classifier for Poisoning Detection in Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Melanie Götz",
      "Torsten Krauß",
      "Alexandra Dmitrienko"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jeong_StyleKeeper_Prevent_Content_Leakage_using_Negative_Visual_Query_Guidance_ICCV_2025_paper.html": {
    "title": "StyleKeeper: Prevent Content Leakage using Negative Visual Query Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaeseok Jeong",
      "Junho Kim",
      "Gayoung Lee",
      "Yunjey Choi",
      "Youngjung Uh"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ohkawa_Generative_Modeling_of_Shape-Dependent_Self-Contact_Human_Poses_ICCV_2025_paper.html": {
    "title": "Generative Modeling of Shape-Dependent Self-Contact Human Poses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Takehiko Ohkawa",
      "Jihyun Lee",
      "Shunsuke Saito",
      "Jason Saragih",
      "Fabian Prada",
      "Yichen Xu",
      "Shoou-I Yu",
      "Ryosuke Furuta",
      "Yoichi Sato",
      "Takaaki Shiratori"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gao_Frequency-Guided_Diffusion_for_Training-Free_Text-Driven_Image_Translation_ICCV_2025_paper.html": {
    "title": "Frequency-Guided Diffusion for Training-Free Text-Driven Image Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheng Gao",
      "Jifei Song",
      "Zhensong Zhang",
      "Jiankang Deng",
      "Ioannis Patras"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Qu_DictAS_A_Framework_for_Class-Generalizable_Few-Shot_Anomaly_Segmentation_via_Dictionary_ICCV_2025_paper.html": {
    "title": "DictAS: A Framework for Class-Generalizable Few-Shot Anomaly Segmentation via Dictionary Lookup",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Qu",
      "Xian Tao",
      "Xinyi Gong",
      "ShiChen Qu",
      "Xiaopei Zhang",
      "Xingang Wang",
      "Fei Shen",
      "Zhengtao Zhang",
      "Mukesh Prasad",
      "Guiguang Ding"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gao_DAP-MAE_Domain-Adaptive_Point_Cloud_Masked_Autoencoder_for_Effective_Cross-Domain_Learning_ICCV_2025_paper.html": {
    "title": "DAP-MAE: Domain-Adaptive Point Cloud Masked Autoencoder for Effective Cross-Domain Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziqi Gao",
      "Qiufu Li",
      "Linlin Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jin_6DOPE-GS_Online_6D_Object_Pose_Estimation_using_Gaussian_Splatting_ICCV_2025_paper.html": {
    "title": "6DOPE-GS: Online 6D Object Pose Estimation using Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufeng Jin",
      "Vignesh Prasad",
      "Snehal Jauhri",
      "Mathias Franzius",
      "Georgia Chalvatzaki"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wan_Incremental_Few-Shot_Semantic_Segmentation_via_Multi-Level_Switchable_Visual_Prompts_ICCV_2025_paper.html": {
    "title": "Incremental Few-Shot Semantic Segmentation via Multi-Level Switchable Visual Prompts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maoxian Wan",
      "Kaige Li",
      "Qichuan Geng",
      "Weimin Shi",
      "Zhong Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Faster_and_Better_3D_Splatting_via_Group_Training_ICCV_2025_paper.html": {
    "title": "Faster and Better 3D Splatting via Group Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengbo Wang",
      "Guozheng Ma",
      "Yifei Xue",
      "Yizhen Lao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kiehn_PLMP_-_Point-Line_Minimal_Problems_for_Projective_SfM_ICCV_2025_paper.html": {
    "title": "PLMP - Point-Line Minimal Problems for Projective SfM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kim Kiehn",
      "Albin Ahlbäck",
      "Kathlén Kohn"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ma_Flow-MIL_Constructing_Highly-expressive_Latent_Feature_Space_For_Whole_Slide_Image_ICCV_2025_paper.html": {
    "title": "Flow-MIL: Constructing Highly-expressive Latent Feature Space For Whole Slide Image Classification Using Normalizing Flow",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingfan Ma",
      "Bohan An",
      "Ao Shen",
      "Mingzhi Yuan",
      "Minghong Duan",
      "Manning Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Du_3DRealCar_An_In-the-wild_RGB-D_Car_Dataset_with_360-degree_Views_ICCV_2025_paper.html": {
    "title": "3DRealCar: An In-the-wild RGB-D Car Dataset with 360-degree Views",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaobiao Du",
      "Yida Wang",
      "Haiyang Sun",
      "Zhuojie Wu",
      "Hongwei Sheng",
      "Shuyun Wang",
      "Jiaying Ying",
      "Ming Lu",
      "Tianqing Zhu",
      "Kun Zhan",
      "Xin Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Enhancing_Transferability_of_Targeted_Adversarial_Examples_via_Inverse_Target_Gradient_ICCV_2025_paper.html": {
    "title": "Enhancing Transferability of Targeted Adversarial Examples via Inverse Target Gradient Competition and Spatial Distance Stretching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhankai Li",
      "Weiping Wang",
      "Jie Li",
      "Shigeng Zhang",
      "Yunan Hu",
      "Song Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_MotionShot_Adaptive_Motion_Transfer_across_Arbitrary_Objects_for_Text-to-Video_Generation_ICCV_2025_paper.html": {
    "title": "MotionShot: Adaptive Motion Transfer across Arbitrary Objects for Text-to-Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanchen Liu",
      "Yanan Sun",
      "Zhening Xing",
      "Junyao Gao",
      "Kai Chen",
      "Wenjie Pei"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lin_STIV_Scalable_Text_and_Image_Conditioned_Video_Generation_ICCV_2025_paper.html": {
    "title": "STIV: Scalable Text and Image Conditioned Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zongyu Lin",
      "Wei Liu",
      "Chen Chen",
      "Jiasen Lu",
      "Wenze Hu",
      "Tsu-Jui Fu",
      "Jesse Allardice",
      "Zhengfeng Lai",
      "Liangchen Song",
      "Bowen Zhang",
      "Cha Chen",
      "Yiran Fei",
      "Lezhi Li",
      "Yinfei Yang",
      "Yizhou Sun",
      "Kai-Wei Chang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xiang_Repurposing_2D_Diffusion_Models_with_Gaussian_Atlas_for_3D_Generation_ICCV_2025_paper.html": {
    "title": "Repurposing 2D Diffusion Models with Gaussian Atlas for 3D Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tiange Xiang",
      "Kai Li",
      "Chengjiang Long",
      "Christian Häne",
      "Peihong Guo",
      "Scott Delp",
      "Ehsan Adeli",
      "Li Fei-Fei"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Dedhia_Generating_Fast_and_Slow_Scalable_Parallel_Video_Generation_with_Video_ICCV_2025_paper.html": {
    "title": "Generating, Fast and Slow: Scalable Parallel Video Generation with Video Interface Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bhishma Dedhia",
      "David Bourgin",
      "Krishna Kumar Singh",
      "Yuheng Li",
      "Yan Kang",
      "Zhan Xu",
      "Niraj K. Jha",
      "Yuchen Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_The_Devil_is_in_the_Spurious_Correlations_Boosting_Moment_Retrieval_ICCV_2025_paper.html": {
    "title": "The Devil is in the Spurious Correlations: Boosting Moment Retrieval with Dynamic Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyang Zhou",
      "Fanyue Wei",
      "Lixin Duan",
      "Angela Yao",
      "Wen Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cong_Guiding_Noisy_Label_Conditional_Diffusion_Models_with_Score-based_Discriminator_Correction_ICCV_2025_paper.html": {
    "title": "Guiding Noisy Label Conditional Diffusion Models with Score-based Discriminator Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dat Nguyen Cong",
      "Hieu Tran Bao",
      "Tung Hoang-Thanh"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_A_Conditional_Probability_Framework_for_Compositional_Zero-shot_Learning_ICCV_2025_paper.html": {
    "title": "A Conditional Probability Framework for Compositional Zero-shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peng Wu",
      "Qiuxia Lai",
      "Hao Fang",
      "Guo-Sen Xie",
      "Yilong Yin",
      "Xiankai Lu",
      "Wenguan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Dutta_IAP_Invisible_Adversarial_Patch_Attack_through_Perceptibility-Aware_Localization_and_Perturbation_ICCV_2025_paper.html": {
    "title": "IAP: Invisible Adversarial Patch Attack through Perceptibility-Aware Localization and Perturbation Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Subrat Kishore Dutta",
      "Xiao Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Advancing_Textual_Prompt_Learning_with_Anchored_Attributes_ICCV_2025_paper.html": {
    "title": "Advancing Textual Prompt Learning with Anchored Attributes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheng Li",
      "Yibing Song",
      "Ming-Ming Cheng",
      "Xiang Li",
      "Jian Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_Event-based_Visual_Vibrometry_ICCV_2025_paper.html": {
    "title": "Event-based Visual Vibrometry",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyu Zhou",
      "Peiqi Duan",
      "Yeliduosi Xiaokaiti",
      "Chao Xu",
      "Boxin Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhao_Keep_Your_Friends_Close_and_Your_Enemies_Farther_Distance-aware_Voxel-wise_ICCV_2025_paper.html": {
    "title": "Keep Your Friends Close, and Your Enemies Farther: Distance-aware Voxel-wise Contrastive Learning for Semi-supervised Multi-organ Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haochen Zhao",
      "Jianwei Niu",
      "Xuefeng Liu",
      "Xiaozheng Xie",
      "Li Kuang",
      "Haotian Yang",
      "Bin Dai",
      "Hui Meng",
      "Yong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shen_Online_Reasoning_Video_Segmentation_with_Just-in-Time_Digital_Twins_ICCV_2025_paper.html": {
    "title": "Online Reasoning Video Segmentation with Just-in-Time Digital Twins",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiqing Shen",
      "Bohan Liu",
      "Chenjia Li",
      "Lalithkumar Seenivasan",
      "Mathias Unberath"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xu_Robust_Multi-View_Learning_via_Representation_Fusion_of_Sample-Level_Attention_and_ICCV_2025_paper.html": {
    "title": "Robust Multi-View Learning via Representation Fusion of Sample-Level Attention and Alignment of Simulated Perturbation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Xu",
      "Na Zhao",
      "Gang Niu",
      "Masashi Sugiyama",
      "Xiaofeng Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ma_InterSyn_Interleaved_Learning_for_Dynamic_Motion_Synthesis_in_the_Wild_ICCV_2025_paper.html": {
    "title": "InterSyn: Interleaved Learning for Dynamic Motion Synthesis in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiyi Ma",
      "Yuanzhi Liang",
      "Xiu Li",
      "Chi Zhang",
      "Xuelong Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhu_SegmentDreamer_Towards_High-fidelity_Text-to-3D_Synthesis_with_Segmented_Consistency_Trajectory_Distillation_ICCV_2025_paper.html": {
    "title": "SegmentDreamer: Towards High-fidelity Text-to-3D Synthesis with Segmented Consistency Trajectory Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahao Zhu",
      "Zixuan Chen",
      "Guangcong Wang",
      "Xiaohua Xie",
      "Yi Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wen_InterGSEdit_Interactive_3D_Gaussian_Splatting_Editing_with_3D_Geometry-Consistent_Attention_ICCV_2025_paper.html": {
    "title": "InterGSEdit: Interactive 3D Gaussian Splatting Editing with 3D Geometry-Consistent Attention Prior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minghao Wen",
      "Shengjie Wu",
      "Kangkan Wang",
      "Dong Liang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Fine-Grained_Evaluation_of_Large_Vision-Language_Models_in_Autonomous_Driving_ICCV_2025_paper.html": {
    "title": "Fine-Grained Evaluation of Large Vision-Language Models in Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Li",
      "Meng Tian",
      "Zhenyu Lin",
      "Jiangtong Zhu",
      "Dechang Zhu",
      "Haiqiang Liu",
      "Yueyi Zhang",
      "Zhiwei Xiong",
      "Xinhai Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_MaTe_Images_Are_All_You_Need_for_Material_Transfer_via_ICCV_2025_paper.html": {
    "title": "MaTe: Images Are All You Need for Material Transfer via Diffusion Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nisha Huang",
      "Henglin Liu",
      "Yizhou Lin",
      "Kaer Huang",
      "Chubin Chen",
      "Jie Guo",
      "Tong-yee Lee",
      "Xiu Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Ph-GAN_Physics-Inspired_GAN_for_Generating_SAR_Images_Under_Limited_Data_ICCV_2025_paper.html": {
    "title": "Ph-GAN: Physics-Inspired GAN for Generating SAR Images Under Limited Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xidan Zhang",
      "Yihan Zhuang",
      "Qian Guo",
      "Haodong Yang",
      "Xuelin Qian",
      "Gong Cheng",
      "Junwei Han",
      "Zhongling Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/De_Vries_Interpretable_point_cloud_classification_using_multiple_instance_learning_ICCV_2025_paper.html": {
    "title": "Interpretable point cloud classification using multiple instance learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matt De Vries",
      "Reed Naidoo",
      "Olga Fourkioti",
      "Lucas G. Dent",
      "Nathan Curry",
      "Chris Dunsby",
      "Chris Bakal"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Efficient_Fine-Tuning_of_Large_Models_via_Nested_Low-Rank_Adaptation_ICCV_2025_paper.html": {
    "title": "Efficient Fine-Tuning of Large Models via Nested Low-Rank Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lujun Li",
      "Cheng  Lin",
      "Dezhi Li",
      "You-Liang Huang",
      "Wei  Li",
      "Tianyu  Wu",
      "Jie  Zou",
      "Wei  Xue",
      "Sirui  Han",
      "Yike  Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_CoSMIC_Continual_Self-supervised_Learning_for_Multi-Domain_Medical_Imaging_via_Conditional_ICCV_2025_paper.html": {
    "title": "CoSMIC: Continual Self-supervised Learning for Multi-Domain Medical Imaging via Conditional Mutual Information Maximization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihang Liu",
      "Ying Wen",
      "Longzhen Yang",
      "Lianghua He",
      "Heng Tao Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xie_STAR_Spatial-Temporal_Augmentation_with_Text-to-Video_Models_for_Real-World_Video_Super-Resolution_ICCV_2025_paper.html": {
    "title": "STAR: Spatial-Temporal Augmentation with Text-to-Video Models for Real-World Video Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Xie",
      "Yinhong Liu",
      "Penghao Zhou",
      "Chen Zhao",
      "Jun Zhou",
      "Kai Zhang",
      "Zhenyu Zhang",
      "Jian Yang",
      "Zhenheng Yang",
      "Ying Tai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Guo_SCAN_Bootstrapping_Contrastive_Pre-training_for_Data_Efficiency_ICCV_2025_paper.html": {
    "title": "SCAN: Bootstrapping Contrastive Pre-training for Data Efficiency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yangyang Guo",
      "Mohan Kankanhalli"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_Learnable_Retrieval_Enhanced_Visual-Text_Alignment_and_Fusion_for_Radiology_Report_ICCV_2025_paper.html": {
    "title": "Learnable Retrieval Enhanced Visual-Text Alignment and Fusion for Radiology Report Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qin Zhou",
      "Guoyan Liang",
      "Xindi Li",
      "Jingyuan Chen",
      "Zhe Wang",
      "Chang Yao",
      "Sai Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zheng_Revisiting_Adversarial_Patch_Defenses_on_Object_Detectors_Unified_Evaluation_Large-Scale_ICCV_2025_paper.html": {
    "title": "Revisiting Adversarial Patch Defenses on Object Detectors: Unified Evaluation, Large-Scale Dataset, and New Insights",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junhao Zheng",
      "Jiahao Sun",
      "Chenhao Lin",
      "Zhengyu Zhao",
      "Chen Ma",
      "Chong Zhang",
      "Cong Wang",
      "Qian Wang",
      "Chao Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_V.I.P.__Iterative_Online_Preference_Distillation_for_Efficient_Video_Diffusion_ICCV_2025_paper.html": {
    "title": "V.I.P. : Iterative Online Preference Distillation for Efficient Video Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jisoo Kim",
      "Wooseok Seo",
      "Junwan Kim",
      "Seungho Park",
      "Sooyeon Park",
      "Youngjae Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jiang_Diffusion-based_Source-biased_Model_for_Single_Domain_Generalized_Object_Detection_ICCV_2025_paper.html": {
    "title": "Diffusion-based Source-biased Model for Single Domain Generalized Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Jiang",
      "Wenfei Yang",
      "Tianzhu Zhang",
      "Yongdong Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Domain_Generalizable_Portrait_Style_Transfer_ICCV_2025_paper.html": {
    "title": "Domain Generalizable Portrait Style Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinbo Wang",
      "Wenju Xu",
      "Qing Zhang",
      "Wei-Shi Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhu_4D-Bench_Benchmarking_Multi-modal_Large_Language_Models_for_4D_Object_Understanding_ICCV_2025_paper.html": {
    "title": "4D-Bench: Benchmarking Multi-modal Large Language Models for 4D Object Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenxuan Zhu",
      "Bing Li",
      "Cheng Zheng",
      "Jinjie Mai",
      "Jun Chen",
      "Letian Jiang",
      "Abdullah Hamdi",
      "Sara Rojas Martinez",
      "Chia-Wen Lin",
      "Mohamed Elhoseiny",
      "Bernard Ghanem"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lee_How_Do_Optical_Flow_and_Textual_Prompts_Collaborate_to_Assist_ICCV_2025_paper.html": {
    "title": "How Do Optical Flow and Textual Prompts Collaborate to Assist in Audio-Visual Semantic Segmentation?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujian Lee",
      "Peng Gao",
      "Yongqi Xu",
      "Wentao Fan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_Devil_is_in_the_Uniformity_Exploring_Diverse_Learners_within_Transformer_ICCV_2025_paper.html": {
    "title": "Devil is in the Uniformity: Exploring Diverse Learners within Transformer for Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shihao Zhou",
      "Dayu Li",
      "Jinshan Pan",
      "Juncheng Zhou",
      "Jinglei Shi",
      "Jufeng Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Epona_Autoregressive_Diffusion_World_Model_for_Autonomous_Driving_ICCV_2025_paper.html": {
    "title": "Epona: Autoregressive Diffusion World Model for Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaiwen Zhang",
      "Zhenyu Tang",
      "Xiaotao Hu",
      "Xingang Pan",
      "Xiaoyang Guo",
      "Yuan Liu",
      "Jingwei Huang",
      "Li Yuan",
      "Qian Zhang",
      "Xiao-Xiao Long",
      "Xun Cao",
      "Wei Yin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Diffusion-based_3D_Hand_Motion_Recovery_with_Intuitive_Physics_ICCV_2025_paper.html": {
    "title": "Diffusion-based 3D Hand Motion Recovery with Intuitive Physics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufei Zhang",
      "Zijun Cui",
      "Jeffrey O. Kephart",
      "Qiang Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ogren_Visual_Surface_Wave_Elastography_Revealing_Subsurface_Physical_Properties_via_Visible_ICCV_2025_paper.html": {
    "title": "Visual Surface Wave Elastography: Revealing Subsurface Physical Properties via Visible Surface Waves",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander C. Ogren",
      "Berthy T. Feng",
      "Jihoon Ahn",
      "Katherine L. Bouman",
      "Chiara Daraio"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_Leveraging_Spatial_Invariance_to_Boost_Adversarial_Transferability_ICCV_2025_paper.html": {
    "title": "Leveraging Spatial Invariance to Boost Adversarial Transferability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihan Zhou",
      "Li Li",
      "Yanli Ren",
      "Chuan Qin",
      "Guorui Feng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Salehi_MoSiC_Optimal-Transport_Motion_Trajectory_for_Dense_Self-Supervised_Learning_ICCV_2025_paper.html": {
    "title": "MoSiC: Optimal-Transport Motion Trajectory for Dense Self-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammadreza Salehi",
      "Shashanka Venkataramanan",
      "Ioana Simion",
      "Efstratios Gavves",
      "Cees G. M. Snoek",
      "Yuki M Asano"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_GDKVM_Echocardiography_Video_Segmentation_via_Spatiotemporal_Key-Value_Memory_with_Gated_ICCV_2025_paper.html": {
    "title": "GDKVM: Echocardiography Video Segmentation via Spatiotemporal Key-Value Memory with Gated Delta Rule",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Wang",
      "Yimu Sun",
      "Jingxing Guo",
      "Huisi Wu",
      "Jing Qin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Susladkar_ViCTr_Vital_Consistency_Transfer_for_Pathology_Aware_Image_Synthesis_ICCV_2025_paper.html": {
    "title": "ViCTr: Vital Consistency Transfer for Pathology Aware Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Onkar Susladkar",
      "Gayatri Deshmukh",
      "Yalcin Tur",
      "Gorkem Durak",
      "Ulas Bagci"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_DanceEditor_Towards_Iterative_Editable_Music-driven_Dance_Generation_with_Open-Vocabulary_Descriptions_ICCV_2025_paper.html": {
    "title": "DanceEditor: Towards Iterative Editable Music-driven Dance Generation with Open-Vocabulary Descriptions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hengyuan Zhang",
      "Zhe Li",
      "Xingqun Qi",
      "Mengze Li",
      "Muyi Sun",
      "Siye Wang",
      "Man Zhang",
      "Sirui Han"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_When_Pixel_Difference_Patterns_Meet_ViT_PiDiViT_for_Few-Shot_Object_ICCV_2025_paper.html": {
    "title": "When Pixel Difference Patterns Meet ViT: PiDiViT for Few-Shot Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongliang Zhou",
      "Yongxiang Liu",
      "Canyu Mo",
      "Weijie Li",
      "Bowen Peng",
      "Li Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shinoda_AnimalClue_Recognizing_Animals_by_their_Traces_ICCV_2025_paper.html": {
    "title": "AnimalClue: Recognizing Animals by their Traces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Risa Shinoda",
      "Nakamasa Inoue",
      "Iro Laina",
      "Christian Rupprecht",
      "Hirokatsu Kataoka"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zeng_From_Objects_to_Events_Unlocking_Complex_Visual_Understanding_in_Object_ICCV_2025_paper.html": {
    "title": "From Objects to Events: Unlocking Complex Visual Understanding in Object Detectors via LLM-guided Symbolic Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhui Zeng",
      "Haoxiang Wu",
      "Wenjie Nie",
      "Guangyao Chen",
      "Xiawu Zheng",
      "Yunhang Shen",
      "Jun Peng",
      "Yonghong Tian",
      "Rongrong Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Suo_Pruning_All-Rounder_Rethinking_and_Improving_Inference_Efficiency_for_Large_Vision_ICCV_2025_paper.html": {
    "title": "Pruning All-Rounder: Rethinking and Improving Inference Efficiency for Large Vision Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Suo",
      "Ji Ma",
      "Mengyang Sun",
      "Lin Yuanbo Wu",
      "Peng Wang",
      "Yanning Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Milano_Discontinuity-aware_Normal_Integration_for_Generic_Central_Camera_Models_ICCV_2025_paper.html": {
    "title": "Discontinuity-aware Normal Integration for Generic Central Camera Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francesco Milano",
      "Manuel López-Antequera",
      "Naina Dhingra",
      "Roland Siegwart",
      "Robert Thiel"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yan_LazyMAR_Accelerating_Masked_Autoregressive_Models_via_Feature_Caching_ICCV_2025_paper.html": {
    "title": "LazyMAR: Accelerating Masked Autoregressive Models via Feature Caching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feihong Yan",
      "Qingyan Wei",
      "Jiayi Tang",
      "Jiajun Li",
      "Yulin Wang",
      "Xuming Hu",
      "Huiqi Li",
      "Linfeng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_OmniDiff_A_Comprehensive_Benchmark_for_Fine-grained_Image_Difference_Captioning_ICCV_2025_paper.html": {
    "title": "OmniDiff: A Comprehensive Benchmark for Fine-grained Image Difference Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Liu",
      "Saihui Hou",
      "Saijie Hou",
      "Jiabao Du",
      "Shibei Meng",
      "Yongzhen Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kang_MAESTRO_Task-Relevant_Optimization_via_Adaptive_Feature_Enhancement_and_Suppression_for_ICCV_2025_paper.html": {
    "title": "MAESTRO: Task-Relevant Optimization via Adaptive Feature Enhancement and Suppression for Multi-task 3D Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changwon Kang",
      "Jisong Kim",
      "Hongjae Shin",
      "Junseo Park",
      "Jun Won Choi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Daxberger_MM-Spatial_Exploring_3D_Spatial_Understanding_in_Multimodal_LLMs_ICCV_2025_paper.html": {
    "title": "MM-Spatial: Exploring 3D Spatial Understanding in Multimodal LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Erik Daxberger",
      "Nina Wenzel",
      "David Griffiths",
      "Haiming Gang",
      "Justin Lazarow",
      "Gefen Kohavi",
      "Kai Kang",
      "Marcin Eichner",
      "Yinfei Yang",
      "Afshin Dehghan",
      "Peter Grasch"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_CasP_Improving_Semi-Dense_Feature_Matching_Pipeline_Leveraging_Cascaded_Correspondence_Priors_ICCV_2025_paper.html": {
    "title": "CasP: Improving Semi-Dense Feature Matching Pipeline Leveraging Cascaded Correspondence Priors for Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peiqi Chen",
      "Lei Yu",
      "Yi Wan",
      "Yingying Pei",
      "Xinyi Liu",
      "Yongxiang Yao",
      "Yingying Zhang",
      "Lixiang Ru",
      "Liheng Zhong",
      "Jingdong Chen",
      "Ming Yang",
      "Yongjun Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Multimodal_Latent_Diffusion_Model_for_Complex_Sewing_Pattern_Generation_ICCV_2025_paper.html": {
    "title": "Multimodal Latent Diffusion Model for Complex Sewing Pattern Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengqi Liu",
      "Yuhao Cheng",
      "Zhuo Chen",
      "Xingyu Ren",
      "Wenhan Zhu",
      "Lincheng Li",
      "Mengxiao Bi",
      "Xiaokang Yang",
      "Yichao Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Damaraju_CObL_Toward_Zero-Shot_Ordinal_Layering_without_User_Prompting_ICCV_2025_paper.html": {
    "title": "CObL: Toward Zero-Shot Ordinal Layering without User Prompting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aneel Damaraju",
      "Dean Hazineh",
      "Todd Zickler"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cong_Rethinking_the_Upsampling_Process_in_Light_Field_Super-Resolution_with_Spatial-Epipolar_ICCV_2025_paper.html": {
    "title": "Rethinking the Upsampling Process in Light Field Super-Resolution with Spatial-Epipolar Implicit Image Function",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruixuan Cong",
      "Yu Wang",
      "Mingyuan Zhao",
      "Da Yang",
      "Rongshan Chen",
      "Hao Sheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hu_GGTalker_Talking_Head_Systhesis_with_Generalizable_Gaussian_Priors_and_Identity-Specific_ICCV_2025_paper.html": {
    "title": "GGTalker: Talking Head Systhesis with Generalizable Gaussian Priors and Identity-Specific Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wentao Hu",
      "Shunkai Li",
      "Ziqiao Peng",
      "Haoxian Zhang",
      "Fan Shi",
      "Xiaoqiang Liu",
      "Pengfei Wan",
      "Di Zhang",
      "Hui Tian"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Piedade_SAC-GNC_SAmple_Consensus_for_adaptive_Graduated_Non-Convexity_ICCV_2025_paper.html": {
    "title": "SAC-GNC: SAmple Consensus for adaptive Graduated Non-Convexity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Valter Piedade",
      "Chitturi Sidhartha",
      "José Gaspar",
      "Venu Madhav Govindu",
      "Pedro Miraldo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zverev_VGGSounder_Audio-Visual_Evaluations_for_Foundation_Models_ICCV_2025_paper.html": {
    "title": "VGGSounder: Audio-Visual Evaluations for Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniil Zverev",
      "Thaddäus Wiedemer",
      "Ameya Prabhu",
      "Matthias Bethge",
      "Wieland Brendel",
      "A. Sophia Koepke"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/DiBrita_ResQ_A_Novel_Framework_to_Implement_Residual_Neural_Networks_on_ICCV_2025_paper.html": {
    "title": "ResQ: A Novel Framework to Implement Residual Neural Networks on Analog Rydberg Atom Quantum Computers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicholas S. DiBrita",
      "Jason Han",
      "Tirthak Patel"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lin_Make_Me_Happier_Evoking_Emotions_Through_Image_Diffusion_Models_ICCV_2025_paper.html": {
    "title": "Make Me Happier: Evoking Emotions Through Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qing Lin",
      "Jingfeng Zhang",
      "Yew-Soon Ong",
      "Mengmi Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cheng_SimpleVQA_Multimodal_Factuality_Evaluation_for_Multimodal_Large_Language_Models_ICCV_2025_paper.html": {
    "title": "SimpleVQA: Multimodal Factuality Evaluation for Multimodal Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xianfu Cheng",
      "Wei Zhang",
      "Shiwei Zhang",
      "Jian Yang",
      "Xiangyuan Guan",
      "Xianjie Wu",
      "Xiang Li",
      "Ge Zhang",
      "Jiaheng Liu",
      "Yuying Mai",
      "Yutao Zeng",
      "Zhoufutu Wen",
      "Ke Jin",
      "Baorui Wang",
      "Weixiao Zhou",
      "Yunhong Lu",
      "Hangyuan Ji",
      "Tongliang Li",
      "Wenhao Huang",
      "Zhoujun Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_DiTFastAttnV2_Head-wise_Attention_Compression_for_Multi-Modality_Diffusion_Transformers_ICCV_2025_paper.html": {
    "title": "DiTFastAttnV2: Head-wise Attention Compression for Multi-Modality Diffusion Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanling Zhang",
      "Rundong Su",
      "Zhihang Yuan",
      "Pengtao Chen",
      "Mingzhu Shen",
      "Yibo Fan",
      "Shengen Yan",
      "Guohao Dai",
      "Yu Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Mo_Find_a_Scapegoat_Poisoning_Membership_Inference_Attack_and_Defense_to_ICCV_2025_paper.html": {
    "title": "Find a Scapegoat: Poisoning Membership Inference Attack and Defense to Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenjin Mo",
      "Zhiyuan Li",
      "Minghong Fang",
      "Mingwei Fang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Si_Generalized_Tensor-based_Parameter-Efficient_Fine-Tuning_via_Lie_Group_Transformations_ICCV_2025_paper.html": {
    "title": "Generalized Tensor-based Parameter-Efficient Fine-Tuning via Lie Group Transformations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chongjie Si",
      "Zhiyi Shi",
      "Xuehui Wang",
      "Yichen Xiao",
      "Xiaokang Yang",
      "Wei Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lu_GAS_Generative_Avatar_Synthesis_from_a_Single_Image_ICCV_2025_paper.html": {
    "title": "GAS: Generative Avatar Synthesis from a Single Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixing Lu",
      "Junting Dong",
      "Youngjoong Kwon",
      "Qin Zhao",
      "Bo Dai",
      "Fernando De la Torre"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shi_GenM3_Generative_Pretrained_Multi-path_Motion_Model_for_Text_Conditional_Human_ICCV_2025_paper.html": {
    "title": "GenM3: Generative Pretrained Multi-path Motion Model for Text Conditional Human Motion Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyu Shi",
      "Lijiang Liu",
      "Yong Sun",
      "Zhiyuan Zhang",
      "Jinni Zhou",
      "Qiang Nie"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Coordinate-based_Speed_of_Sound_Recovery_for_Aberration-Corrected_Photoacoustic_Computed_Tomography_ICCV_2025_paper.html": {
    "title": "Coordinate-based Speed of Sound Recovery for Aberration-Corrected Photoacoustic Computed Tomography",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianao Li",
      "Manxiu Cui",
      "Cheng Ma",
      "Emma Alexander"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kwon_Granular_Concept_Circuits_Toward_a_Fine-Grained_Circuit_Discovery_for_Concept_ICCV_2025_paper.html": {
    "title": "Granular Concept Circuits: Toward a Fine-Grained Circuit Discovery for Concept Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dahee Kwon",
      "Sehyun Lee",
      "Jaesik Choi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Diversity-Enhanced_Distribution_Alignment_for_Dataset_Distillation_ICCV_2025_paper.html": {
    "title": "Diversity-Enhanced Distribution Alignment for Dataset Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongcheng Li",
      "Yucan Zhou",
      "Xiaoyan Gu",
      "Bo Li",
      "Weiping Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_OCR_Hinders_RAG_Evaluating_the_Cascading_Impact_of_OCR_on_ICCV_2025_paper.html": {
    "title": "OCR Hinders RAG: Evaluating the Cascading Impact of OCR on Retrieval-Augmented Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyuan Zhang",
      "Qintong Zhang",
      "Bin Wang",
      "Linke Ouyang",
      "Zichen Wen",
      "Ying Li",
      "Ka-Ho Chow",
      "Conghui He",
      "Wentao Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_HarmonySeg_Tubular_Structure_Segmentation_with_Deep-Shallow_Feature_Fusion_and_Growth-Suppression_ICCV_2025_paper.html": {
    "title": "HarmonySeg: Tubular Structure Segmentation with Deep-Shallow Feature Fusion and Growth-Suppression Balanced Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Huang",
      "Ke Zhang",
      "Wei Liu",
      "Yuanyuan Wang",
      "Vishal M. Patel",
      "Le Lu",
      "Xu Han",
      "Dakai Jin",
      "Ke Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hu_Semi-supervised_Concept_Bottleneck_Models_ICCV_2025_paper.html": {
    "title": "Semi-supervised Concept Bottleneck Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lijie Hu",
      "Tianhao Huang",
      "Huanyi Xie",
      "Xilin Gong",
      "Chenyang Ren",
      "Zhengyu Hu",
      "Lu Yu",
      "Ping Ma",
      "Di Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Fang_Can_Knowledge_be_Transferred_from_Unimodal_to_Multimodal_Investigating_the_ICCV_2025_paper.html": {
    "title": "Can Knowledge be Transferred from Unimodal to Multimodal? Investigating the Transitivity of Multimodal Knowledge Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingyong Fang",
      "Xinzhong Wang",
      "Depeng Wang",
      "Zongru Wu",
      "Ya Guo",
      "Huijia Zhu",
      "Zhuosheng Zhang",
      "Gongshen Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Beyond_Training_Dynamic_Token_Merging_for_Zero-Shot_Video_Understanding_ICCV_2025_paper.html": {
    "title": "Beyond Training: Dynamic Token Merging for Zero-Shot Video Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiming Zhang",
      "Zhuokai Zhao",
      "Zhaorun Chen",
      "Zenghui Ding",
      "Xianjun Yang",
      "Yining Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_CounterPC_Counterfactual_Feature_Realignment_for_Unsupervised_Domain_Adaptation_on_Point_ICCV_2025_paper.html": {
    "title": "CounterPC: Counterfactual Feature Realignment for Unsupervised Domain Adaptation on Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feng Yang",
      "Yichao Cao",
      "Xiu Su",
      "Dan Niu",
      "Xuanpeng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Rong_MPG-SAM_2_Adapting_SAM_2_with_Mask_Priors_and_Global_ICCV_2025_paper.html": {
    "title": "MPG-SAM 2: Adapting SAM 2 with Mask Priors and Global Context for Referring Video Object Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fu Rong",
      "Meng Lan",
      "Qian Zhang",
      "Lefei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xu_FreeSplatter_Pose-free_Gaussian_Splatting_for_Sparse-view_3D_Reconstruction_ICCV_2025_paper.html": {
    "title": "FreeSplatter: Pose-free Gaussian Splatting for Sparse-view 3D Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiale Xu",
      "Shenghua Gao",
      "Ying Shan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Su_FreqPDE_Rethinking_Positional_Depth_Embedding_for_Multi-View_3D_Object_Detection_ICCV_2025_paper.html": {
    "title": "FreqPDE: Rethinking Positional Depth Embedding for Multi-View 3D Object Detection Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haisheng Su",
      "Junjie Zhang",
      "Feixiang Song",
      "Sanping Zhou",
      "Wei Wu",
      "Junchi Yan",
      "Nanning Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Hierarchy-Aware_Pseudo_Word_Learning_with_Text_Adaptation_for_Zero-Shot_Composed_ICCV_2025_paper.html": {
    "title": "Hierarchy-Aware Pseudo Word Learning with Text Adaptation for Zero-Shot Composed Image Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhe Li",
      "Lei Zhang",
      "Zheren Fu",
      "Kun Zhang",
      "Zhendong Mao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liang_WSI-LLaVA_A_Multimodal_Large_Language_Model_for_Whole_Slide_Image_ICCV_2025_paper.html": {
    "title": "WSI-LLaVA: A Multimodal Large Language Model for Whole Slide Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuci Liang",
      "Xinheng Lyu",
      "Wenting Chen",
      "Meidan Ding",
      "Jipeng Zhang",
      "Xiangjian He",
      "Song Wu",
      "Xiaohan Xing",
      "Sen Yang",
      "Xiyue Wang",
      "Linlin Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_GLEAM_Learning_Generalizable_Exploration_Policy_for_Active_Mapping_in_Complex_ICCV_2025_paper.html": {
    "title": "GLEAM: Learning Generalizable Exploration Policy for Active Mapping in Complex 3D Indoor Scene",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Chen",
      "Tai Wang",
      "Quanyi Li",
      "Tao Huang",
      "Jiangmiao Pang",
      "Tianfan Xue"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Han_DialNav_Multi-turn_Dialog_Navigation_with_a_Remote_Guide_ICCV_2025_paper.html": {
    "title": "DialNav: Multi-turn Dialog Navigation with a Remote Guide",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leekyeung Han",
      "Hyunji Min",
      "Gyeom Hwangbo",
      "Jonghyun Choi",
      "Paul Hongsuck Seo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_VLDrive_Vision-Augmented_Lightweight_MLLMs_for_Efficient_Language-grounded_Autonomous_Driving_ICCV_2025_paper.html": {
    "title": "VLDrive: Vision-Augmented Lightweight MLLMs for Efficient Language-grounded Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruifei Zhang",
      "Wei Zhang",
      "Xiao Tan",
      "Sibei Yang",
      "Xiang Wan",
      "Xiaonan Luo",
      "Guanbin Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Ponimator_Unfolding_Interactive_Pose_for_Versatile_Human-human_Interaction_Animation_ICCV_2025_paper.html": {
    "title": "Ponimator: Unfolding Interactive Pose for Versatile Human-human Interaction Animation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaowei Liu",
      "Chuan Guo",
      "Bing Zhou",
      "Jian Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gholami_Streamlining_Image_Editing_with_Layered_Diffusion_Brushes_ICCV_2025_paper.html": {
    "title": "Streamlining Image Editing with Layered Diffusion Brushes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peyman Gholami",
      "Robert Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Winter_ObjectMate_A_Recurrence_Prior_for_Object_Insertion_and_Subject-Driven_Generation_ICCV_2025_paper.html": {
    "title": "ObjectMate: A Recurrence Prior for Object Insertion and Subject-Driven Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Winter",
      "Asaf Shul",
      "Matan Cohen",
      "Dana Berman",
      "Yael Pritch",
      "Alex Rav-Acha",
      "Yedid Hoshen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_CycleVAR_Repurposing_Autoregressive_Model_for_Unsupervised_One-Step_Image_Translation_ICCV_2025_paper.html": {
    "title": "CycleVAR: Repurposing Autoregressive Model for Unsupervised One-Step Image Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Liu",
      "Shengqian Li",
      "Zuzeng Lin",
      "Feng Wang",
      "Si Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhao_Synthetic_Video_Enhances_Physical_Fidelity_in_Video_Synthesis_ICCV_2025_paper.html": {
    "title": "Synthetic Video Enhances Physical Fidelity in Video Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Zhao",
      "Xingyu Ni",
      "Ziyu Wang",
      "Feng Cheng",
      "Ziyan Yang",
      "Lu Jiang",
      "Bohan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Enhancing_Partially_Relevant_Video_Retrieval_with_Hyperbolic_Learning_ICCV_2025_paper.html": {
    "title": "Enhancing Partially Relevant Video Retrieval with Hyperbolic Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Li",
      "Jinpeng Wang",
      "Chaolei Tan",
      "Niu Lian",
      "Long Chen",
      "Yaowei Wang",
      "Min Zhang",
      "Shu-Tao Xia",
      "Bin Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Ultra_High-Resolution_Image_Inpainting_with_Patch-Based_Content_Consistency_Adapter_ICCV_2025_paper.html": {
    "title": "Ultra High-Resolution Image Inpainting with Patch-Based Content Consistency Adapter",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianhui Zhang",
      "Shen Cheng",
      "Qirui Sun",
      "Jia Liu",
      "Wang Luyang",
      "Chaoyu Feng",
      "Chen Fang",
      "Lei Lei",
      "Jue Wang",
      "Shuaicheng Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xu_OURO_A_Self-Bootstrapped_Framework_for_Enhancing_Multimodal_Scene_Understanding_ICCV_2025_paper.html": {
    "title": "OURO: A Self-Bootstrapped Framework for Enhancing Multimodal Scene Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianrun Xu",
      "Guanyu Chen",
      "Ye Li",
      "Yuxin Xi",
      "Zeyu Mu",
      "Ruichen Wang",
      "Tianren Zhang",
      "Haichuan Gao",
      "Feng Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Pan_DiGA3D_Coarse-to-Fine_Diffusional_Propagation_of_Geometry_and_Appearance_for_Versatile_ICCV_2025_paper.html": {
    "title": "DiGA3D: Coarse-to-Fine Diffusional Propagation of Geometry and Appearance for Versatile 3D Inpainting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyi Pan",
      "Dan Xu",
      "Qiong Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Text-to-Any-Skeleton_Motion_Generation_Without_Retargeting_ICCV_2025_paper.html": {
    "title": "Text-to-Any-Skeleton Motion Generation Without Retargeting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingyuan Liu",
      "Ke Lv",
      "Kun Dong",
      "Jian Xue",
      "Zehai Niu",
      "Jinbao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sun_EVDM_Event-based_Real-world_Video_Deblurring_with_Mamba_ICCV_2025_paper.html": {
    "title": "EVDM: Event-based Real-world Video Deblurring with Mamba",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhijing Sun",
      "Senyan Xu",
      "Kean Liu",
      "Runze Tian",
      "Xueyang Fu",
      "Zheng-Jun Zha"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_Learning_Yourself_Class-Incremental_Semantic_Segmentation_with_Language-Inspired_Bootstrapped_Disentanglement_ICCV_2025_paper.html": {
    "title": "Learning Yourself: Class-Incremental Semantic Segmentation with Language-Inspired Bootstrapped Disentanglement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruitao Wu",
      "Yifan Zhao",
      "Jia Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yao_Towards_Fine-grained_Interactive_Segmentation_in_Images_and_Videos_ICCV_2025_paper.html": {
    "title": "Towards Fine-grained Interactive Segmentation in Images and Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Yao",
      "Qiushi Yang",
      "Miaomiao Cui",
      "Liefeng Bo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Narayan_FaceXFormer_A_Unified_Transformer_for_Facial_Analysis_ICCV_2025_paper.html": {
    "title": "FaceXFormer: A Unified Transformer for Facial Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kartik Narayan",
      "Vibashan VS",
      "Rama Chellappa",
      "Vishal M. Patel"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hermann_Puzzle_Similarity_A_Perceptually-guided_Cross-Reference_Metric_for_Artifact_Detection_in_ICCV_2025_paper.html": {
    "title": "Puzzle Similarity: A Perceptually-guided Cross-Reference Metric for Artifact Detection in 3D Scene Reconstructions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicolai Hermann",
      "Jorge Condor",
      "Piotr Didyk"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gong_InstantEdit_Text-Guided_Few-Step_Image_Editing_with_Piecewise_Rectified_Flow_ICCV_2025_paper.html": {
    "title": "InstantEdit: Text-Guided Few-Step Image Editing with Piecewise Rectified Flow",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiming Gong",
      "Zhen Zhu",
      "Minjia Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_The_Source_Image_is_the_Best_Attention_for_Infrared_and_ICCV_2025_paper.html": {
    "title": "The Source Image is the Best Attention for Infrared and Visible Image Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Song Wang",
      "Xie Han",
      "Liqun Kuang",
      "Boying Wang",
      "Zhongyu Chen",
      "Zherui Qiao",
      "Fan Yang",
      "Xiaoxia Liu",
      "Bingyu Zhang",
      "Zhixun Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kayan_Princeton365_A_Diverse_Dataset_with_Accurate_Camera_Pose_ICCV_2025_paper.html": {
    "title": "Princeton365: A Diverse Dataset with Accurate Camera Pose",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karhan Kayan",
      "Stamatis Alexandropoulos",
      "Rishabh Jain",
      "Yiming Zuo",
      "Erich Liang",
      "Jia Deng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ji_Towards_Immersive_Human-X_Interaction_A_Real-Time_Framework_for_Physically_Plausible_ICCV_2025_paper.html": {
    "title": "Towards Immersive Human-X Interaction: A Real-Time Framework for Physically Plausible Motion Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaiyang Ji",
      "Ye Shi",
      "Zichen Jin",
      "Kangyi Chen",
      "Lan Xu",
      "Yuexin Ma",
      "Jingyi Yu",
      "Jingya Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Generative_Video_Bi-flow_ICCV_2025_paper.html": {
    "title": "Generative Video Bi-flow",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Liu",
      "Tobias Ritschel"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xie_AlignDiff_Learning_Physically-Grounded_Camera_Alignment_via_Diffusion_ICCV_2025_paper.html": {
    "title": "AlignDiff: Learning Physically-Grounded Camera Alignment via Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liuyue Xie",
      "Jiancong Guo",
      "Ozan Cakmakci",
      "Andre Araujo",
      "László A. Jeni",
      "Zhiheng Jia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_EFTViT_Efficient_Federated_Training_of_Vision_Transformers_with_Masked_Images_ICCV_2025_paper.html": {
    "title": "EFTViT: Efficient Federated Training of Vision Transformers with Masked Images on Resource-Constrained Clients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meihan Wu",
      "Tao Chang",
      "Cui Miao",
      "Jie Zhou",
      "Chun Li",
      "Xiangyu Xu",
      "Ming Li",
      "Xiaodong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lu_A_Structure-aware_and_Motion-adaptive_Framework_for_3D_Human_Pose_Estimation_ICCV_2025_paper.html": {
    "title": "A Structure-aware and Motion-adaptive Framework for 3D Human Pose Estimation with Mamba",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ye Lu",
      "Jie Wang",
      "Jianjun Gao",
      "Rui Gong",
      "Chen Cai",
      "Kim-Hui Yap"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Toward_Fair_and_Accurate_Cross-Domain_Medical_Image_Segmentation_A_VLM-Driven_ICCV_2025_paper.html": {
    "title": "Toward Fair and Accurate Cross-Domain Medical Image Segmentation: A VLM-Driven Active Domain Adaptation Paradigm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongqiu Wang",
      "Wu Chen",
      "Xiangde Luo",
      "Zhaohu Xing",
      "Lihao Liu",
      "Jing Qin",
      "Shaozhi Wu",
      "Lei Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Nair_Scaling_Transformer-Based_Novel_View_Synthesis_with_Models_Token_Disentanglement_and_ICCV_2025_paper.html": {
    "title": "Scaling Transformer-Based Novel View Synthesis with Models Token Disentanglement and Synthetic Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nithin Gopalakrishnan Nair",
      "Srinivas Kaza",
      "Xuan Luo",
      "Vishal M. Patel",
      "Stephen Lombardi",
      "Jungyeon Park"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_FED-PsyAU_Privacy-Preserving_Micro-Expression_Recognition_via_Psychological_AU_Coordination_and_Dynamic_ICCV_2025_paper.html": {
    "title": "FED-PsyAU: Privacy-Preserving Micro-Expression Recognition via Psychological AU Coordination and Dynamic Facial Motion Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingting Li",
      "Yu Qian",
      "Lin Zhao",
      "Su-Jing Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gopal_SAFER_Sharpness_Aware_layer-selective_Finetuning_for_Enhanced_Robustness_in_vision_ICCV_2025_paper.html": {
    "title": "SAFER: Sharpness Aware layer-selective Finetuning for Enhanced Robustness in vision transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bhavna Gopal",
      "Huanrui Yang",
      "Mark Horton",
      "Yiran Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lin_ClearSight_Human_Vision-Inspired_Solutions_for_Event-Based_Motion_Deblurring_ICCV_2025_paper.html": {
    "title": "ClearSight: Human Vision-Inspired Solutions for Event-Based Motion Deblurring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaopeng Lin",
      "Yulong Huang",
      "Hongwei Ren",
      "Zunchang Liu",
      "Hongxiang Huang",
      "Yue Zhou",
      "Haotian Fu",
      "Bojun Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wan_Event-aided_Dense_and_Continuous_Point_Tracking_Everywhere_and_Anytime_ICCV_2025_paper.html": {
    "title": "Event-aided Dense and Continuous Point Tracking: Everywhere and Anytime",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhexiong Wan",
      "Jianqin Luo",
      "Yuchao Dai",
      "Gim Hee Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xu_Stable-Sim2Real_Exploring_Simulation_of_Real-Captured_3D_Data_with_Two-Stage_Depth_ICCV_2025_paper.html": {
    "title": "Stable-Sim2Real: Exploring Simulation of Real-Captured 3D Data with Two-Stage Depth Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mutian Xu",
      "Chongjie Ye",
      "Haolin Liu",
      "Yushuang Wu",
      "Jiahao Chang",
      "Xiaoguang Han"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Aqeel_Towards_Real_Unsupervised_Anomaly_Detection_Via_Confident_Meta-Learning_ICCV_2025_paper.html": {
    "title": "Towards Real Unsupervised Anomaly Detection Via Confident Meta-Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Aqeel",
      "Shakiba Sharifi",
      "Marco Cristani",
      "Francesco Setti"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shi_Multi-Schema_Proximity_Network_for_Composed_Image_Retrieval_ICCV_2025_paper.html": {
    "title": "Multi-Schema Proximity Network for Composed Image Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiangming Shi",
      "Xiangbo Yin",
      "Yeyun Chen",
      "Yachao Zhang",
      "Zhizhong Zhang",
      "Yuan Xie",
      "Yanyun Qu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Feng_Unified_Video_Generation_via_Next-Set_Prediction_in_Continuous_Domain_ICCV_2025_paper.html": {
    "title": "Unified Video Generation via Next-Set Prediction in Continuous Domain",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhanzhou Feng",
      "Qingpei Guo",
      "Xinyu Xiao",
      "Ruihan Xu",
      "Ming Yang",
      "Shiliang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_Latent-Reframe_Enabling_Camera_Control_for_Video_Diffusion_Models_without_Training_ICCV_2025_paper.html": {
    "title": "Latent-Reframe: Enabling Camera Control for Video Diffusion Models without Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenghong Zhou",
      "Jie An",
      "Jiebo Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_EditCLIP_Representation_Learning_for_Image_Editing_ICCV_2025_paper.html": {
    "title": "EditCLIP: Representation Learning for Image Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qian Wang",
      "Aleksandar Cvejić",
      "Abdelrahman Eldesokey",
      "Peter Wonka"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liang_Efficient_Event_Camera_Data_Pretraining_with_Adaptive_Prompt_Fusion_ICCV_2025_paper.html": {
    "title": "Efficient Event Camera Data Pretraining with Adaptive Prompt Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quanmin Liang",
      "Qiang Li",
      "Shuai Liu",
      "Xinzi Cao",
      "Jinyi Lu",
      "Feidiao Yang",
      "Wei Zhang",
      "Kai Huang",
      "Yonghong Tian"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Buglakova_Tiling_artifacts_and_trade-offs_of_feature_normalization_in_the_segmentation_ICCV_2025_paper.html": {
    "title": "Tiling artifacts and trade-offs of feature normalization in the segmentation of large biological images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elena Buglakova",
      "Anwai Archit",
      "Edoardo D'Imprima",
      "Julia Mahamid",
      "Constantin Pape",
      "Anna Kreshuk"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Harnessing_Text-to-Image_Diffusion_Models_for_Point_Cloud_Self-Supervised_Learning_ICCV_2025_paper.html": {
    "title": "Harnessing Text-to-Image Diffusion Models for Point Cloud Self-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiyang Chen",
      "Shanshan Zhao",
      "Lunhao Duan",
      "Changxing Ding",
      "Dacheng Tao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Bridging_the_Gap_between_Brain_and_Machine_in_Interpreting_Visual_ICCV_2025_paper.html": {
    "title": "Bridging the Gap between Brain and Machine in Interpreting Visual Semantics: Towards Self-adaptive Brain-to-Text Decoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxuan Chen",
      "Yu Qi",
      "Yueming Wang",
      "Gang Pan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chae_Doppler-Aware_LiDAR-RADAR_Fusion_for_Weather-Robust_3D_Detection_ICCV_2025_paper.html": {
    "title": "Doppler-Aware LiDAR-RADAR Fusion for Weather-Robust 3D Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujeong Chae",
      "Heejun Park",
      "Hyeonseong Kim",
      "Kuk-Jin Yoon"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kou_RoboAnnotatorX_A_Comprehensive_and_Universal_Annotation_Framework_for_Accurate_Understanding_ICCV_2025_paper.html": {
    "title": "RoboAnnotatorX: A Comprehensive and Universal Annotation Framework for Accurate Understanding of Long-horizon Robot Demonstration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Longxin Kou",
      "Fei Ni",
      "Yan Zheng",
      "Peilong Han",
      "Jinyi Liu",
      "Haiqin Cui",
      "Rui Liu",
      "Jianye Hao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_On-Device_Diffusion_Transformer_Policy_for_Efficient_Robot_Manipulation_ICCV_2025_paper.html": {
    "title": "On-Device Diffusion Transformer Policy for Efficient Robot Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiming Wu",
      "Huan Wang",
      "Zhenghao Chen",
      "Jianxin Pang",
      "Dong Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xia_Unsupervised_Part_Discovery_via_Descriptor-Based_Masked_Image_Restoration_with_Optimized_ICCV_2025_paper.html": {
    "title": "Unsupervised Part Discovery via Descriptor-Based Masked Image Restoration with Optimized Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahao Xia",
      "Yike Wu",
      "Wenjian Huang",
      "Jianguo Zhang",
      "Jian Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Feng_VideoOrion_Tokenizing_Object_Dynamics_in_Videos_ICCV_2025_paper.html": {
    "title": "VideoOrion: Tokenizing Object Dynamics in Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yicheng Feng",
      "Yijiang Li",
      "Wanpeng Zhang",
      "Sipeng Zheng",
      "Hao Luo",
      "Zihao Yue",
      "Zongqing Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Training-Free_Text-Guided_Image_Editing_with_Visual_Autoregressive_Model_ICCV_2025_paper.html": {
    "title": "Training-Free Text-Guided Image Editing with Visual Autoregressive Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufei Wang",
      "Lanqing Guo",
      "Zhihao Li",
      "Jiaxing Huang",
      "Pichao Wang",
      "Bihan Wen",
      "Jian Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Cracking_Instance_Jigsaw_Puzzles_An_Alternative_to_Multiple_Instance_Learning_ICCV_2025_paper.html": {
    "title": "Cracking Instance Jigsaw Puzzles: An Alternative to Multiple Instance Learning for Whole Slide Image Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiwen Chen",
      "Peijie Qiu",
      "Wenhui Zhu",
      "Hao Wang",
      "Huayu Li",
      "Xuanzhao Dong",
      "Xiaotong Sun",
      "Xiaobing Yu",
      "Yalin Wang",
      "Abolfazl Razi",
      "Aristeidis Sotiras"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yuan_SPD_Shallow_Backdoor_Protecting_Deep_Backdoor_Against_Backdoor_Detection_ICCV_2025_paper.html": {
    "title": "SPD: Shallow Backdoor Protecting Deep Backdoor Against Backdoor Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shunjie Yuan",
      "Xinghua Li",
      "Xuelin Cao",
      "Haiyan Zhang",
      "Mengyao Zhu",
      "Robert H. Deng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lin_RealGeneral_Unifying_Visual_Generation_via_Temporal_In-Context_Learning_with_Video_ICCV_2025_paper.html": {
    "title": "RealGeneral: Unifying Visual Generation via Temporal In-Context Learning with Video Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yijing Lin",
      "Mengqi Huang",
      "Shuhan Zhuang",
      "Zhendong Mao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Mamba-3VL_Taming_State_Space_Model_for_3D_Vision_Language_Learning_ICCV_2025_paper.html": {
    "title": "Mamba-3VL: Taming State Space Model for 3D Vision Language Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Wang",
      "Yuxin Chen",
      "Zhongang Qi",
      "Lijun Liu",
      "Jile Jiao",
      "Xuetao Feng",
      "Yujia Liang",
      "Ying Shan",
      "Zhipeng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yi_Fine-structure_Preserved_Real-world_Image_Super-resolution_via_Transfer_VAE_Training_ICCV_2025_paper.html": {
    "title": "Fine-structure Preserved Real-world Image Super-resolution via Transfer VAE Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiaosi Yi",
      "Shuai Li",
      "Rongyuan Wu",
      "Lingchen Sun",
      "Yuhui Wu",
      "Lei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Fischer_FlowR_Flowing_from_Sparse_to_Dense_3D_Reconstructions_ICCV_2025_paper.html": {
    "title": "FlowR: Flowing from Sparse to Dense 3D Reconstructions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tobias Fischer",
      "Samuel Rota Bulò",
      "Yung-Hsu Yang",
      "Nikhil Keetha",
      "Lorenzo Porzi",
      "Norman Müller",
      "Katja Schwarz",
      "Jonathon Luiten",
      "Marc Pollefeys",
      "Peter Kontschieder"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/H._Augmented_Mass-Spring_Model_for_Real-Time_Dense_Hair_Simulation_ICCV_2025_paper.html": {
    "title": "Augmented Mass-Spring Model for Real-Time Dense Hair Simulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "J. Alejandro Amador H.",
      "Yi Zhou",
      "Xin Sun",
      "Zhixin Shu",
      "Chengan He",
      "Soren Pirk",
      "Dominik L. Michels"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Abud_IQA-Adapter_Exploring_Knowledge_Transfer_from_Image_Quality_Assessment_to_Diffusion-based_ICCV_2025_paper.html": {
    "title": "IQA-Adapter: Exploring Knowledge Transfer from Image Quality Assessment to Diffusion-based Generative Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Khaled Abud",
      "Sergey Lavrushkin",
      "Alexey Kirillov",
      "Dmitriy Vatolin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhu_SPA_Efficient_User-Preference_Alignment_against_Uncertainty_in_Medical_Image_Segmentation_ICCV_2025_paper.html": {
    "title": "SPA: Efficient User-Preference Alignment against Uncertainty in Medical Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayuan Zhu",
      "Junde Wu",
      "Cheng Ouyang",
      "Konstantinos Kamnitsas",
      "J. Alison Noble"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yu_GaussianReg_Rapid_2D3D_Registration_for_Emergency_Surgery_via_Explicit_3D_ICCV_2025_paper.html": {
    "title": "GaussianReg: Rapid 2D/3D Registration for Emergency Surgery via Explicit 3D Modeling with Gaussian Primitives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weihao Yu",
      "Xiaoqing Guo",
      "Xinyu Liu",
      "Yifan Liu",
      "Hao Zheng",
      "Yawen Huang",
      "Yixuan Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Dong_Robustifying_Zero-Shot_Vision_Language_Models_by_Subspaces_Alignment_ICCV_2025_paper.html": {
    "title": "Robustifying Zero-Shot Vision Language Models by Subspaces Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junhao Dong",
      "Piotr Koniusz",
      "Liaoyuan Feng",
      "Yifei Zhang",
      "Hao Zhu",
      "Weiming Liu",
      "Xinghua Qu",
      "Yew-Soon Ong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Performing_Defocus_Deblurring_by_Modeling_its_Formation_Process_ICCV_2025_paper.html": {
    "title": "Performing Defocus Deblurring by Modeling its Formation Process",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengbo Zhang",
      "Lin Geng Foo",
      "Hossein Rahmani",
      "Jun Liu",
      "De Wen Soh"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_Towards_Effective_Foundation_Model_Adaptation_for_Extreme_Cross-Domain_Few-Shot_Learning_ICCV_2025_paper.html": {
    "title": "Towards Effective Foundation Model Adaptation for Extreme Cross-Domain Few-Shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fei Zhou",
      "Peng Wang",
      "Lei Zhang",
      "Wei Wei",
      "Chen Ding",
      "Guosheng Lin",
      "Yanning Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Bai_GaRe_Relightable_3D_Gaussian_Splatting_for_Outdoor_Scenes_from_Unconstrained_ICCV_2025_paper.html": {
    "title": "GaRe: Relightable 3D Gaussian Splatting for Outdoor Scenes from Unconstrained Photo Collections",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiyang Bai",
      "Jiaqi Zhu",
      "Songru Jiang",
      "Wei Huang",
      "Tao Lu",
      "Yuanqi Li",
      "Jie Guo",
      "Runze Fu",
      "Yanwen Guo",
      "Lijun Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhu_LoD-Loc_v2_Aerial_Visual_Localization_over_Low_Level-of-Detail_City_Models_ICCV_2025_paper.html": {
    "title": "LoD-Loc v2: Aerial Visual Localization over Low Level-of-Detail City Models using Explicit Silhouette Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juelin Zhu",
      "Shuaibang Peng",
      "Long Wang",
      "Hanlin Tan",
      "Yu Liu",
      "Maojun Zhang",
      "Shen Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Feng_RomanTex_Decoupling_3D-aware_Rotary_Positional_Embedded_Multi-Attention_Network_for_Texture_ICCV_2025_paper.html": {
    "title": "RomanTex: Decoupling 3D-aware Rotary Positional Embedded Multi-Attention Network for Texture Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei Feng",
      "Mingxin Yang",
      "Shuhui Yang",
      "Sheng Zhang",
      "Jiaao Yu",
      "Zibo Zhao",
      "Yuhong Liu",
      "Jie Jiang",
      "Chunchao Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Phantom_Subject-Consistent_Video_Generation_via_Cross-Modal_Alignment_ICCV_2025_paper.html": {
    "title": "Phantom: Subject-Consistent Video Generation via Cross-Modal Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lijie Liu",
      "Tianxiang Ma",
      "Bingchuan Li",
      "Zhuowei Chen",
      "Jiawei Liu",
      "Gen Li",
      "Siyu Zhou",
      "Qian He",
      "Xinglong Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Song_EYE3Turn_Anything_into_Naked-eye_3D_ICCV_2025_paper.html": {
    "title": "EYE3:Turn Anything into Naked-eye 3D",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingde Song",
      "Zongyuan Yang",
      "Baolin Liu",
      "Yongping Xiong",
      "Sai Chen",
      "Lan Yi",
      "Zhaohe Zhang",
      "Xunbo Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Boosting_Class_Representation_via_Semantically_Related_Instances_for_Robust_Long-Tailed_ICCV_2025_paper.html": {
    "title": "Boosting Class Representation via Semantically Related Instances for Robust Long-Tailed Learning with Noisy Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhang Li",
      "Zhuying Li",
      "Yuheng Jia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yan_You_Are_Your_Own_Best_Teacher_Achieving_Centralized-level_Performance_in_ICCV_2025_paper.html": {
    "title": "You Are Your Own Best Teacher: Achieving Centralized-level Performance in Federated Learning under Heterogeneous and Long-tailed Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shanshan Yan",
      "Zexi Li",
      "Chao Wu",
      "Meng Pang",
      "Yang Lu",
      "Yan Yan",
      "Hanzi Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_IFAdapter_Instance_Feature_Control_for_Grounded_Text-to-Image_Generation_ICCV_2025_paper.html": {
    "title": "IFAdapter: Instance Feature Control for Grounded Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinwei Wu",
      "Xianpan Zhou",
      "Bing Ma",
      "Xuefeng Su",
      "Kai Ma",
      "Xinchao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_StableDepth_Scene-Consistent_and_Scale-Invariant_Monocular_Depth_ICCV_2025_paper.html": {
    "title": "StableDepth: Scene-Consistent and Scale-Invariant Monocular Depth",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheng Zhang",
      "Lihe Yang",
      "Tianyu Yang",
      "Chaohui Yu",
      "Xiaoyang Guo",
      "Yixing Lao",
      "Hengshuang Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Naiman_LV-MAE_Learning_Long_Video_Representations_through_Masked-Embedding_Autoencoders_ICCV_2025_paper.html": {
    "title": "LV-MAE: Learning Long Video Representations through Masked-Embedding Autoencoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ilan Naiman",
      "Emanuel Ben-Baruch",
      "Oron Anschel",
      "Alon Shoshan",
      "Igor Kviatkovsky",
      "Manoj Aggarwal",
      "Gerard Medioni"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Personalized_Federated_Learning_under_Local_Supervision_ICCV_2025_paper.html": {
    "title": "Personalized Federated Learning under Local Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiqi Liu",
      "Jiaqiang Li",
      "Yuchen Liu",
      "Yaochu Jin",
      "Lingjuan Lyu",
      "Xiaohu Wu",
      "Han Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_STDDNet_Harnessing_Mamba_for_Video_Polyp_Segmentation_via_Spatial-aligned_Temporal_ICCV_2025_paper.html": {
    "title": "STDDNet: Harnessing Mamba for Video Polyp Segmentation via Spatial-aligned Temporal Modeling and Discriminative Dynamic Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guilian Chen",
      "Huisi Wu",
      "Jing Qin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Mou_DIMO_Diverse_3D_Motion_Generation_for_Arbitrary_Objects_ICCV_2025_paper.html": {
    "title": "DIMO: Diverse 3D Motion Generation for Arbitrary Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linzhan Mou",
      "Jiahui Lei",
      "Chen Wang",
      "Lingjie Liu",
      "Kostas Daniilidis"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ning_Enhancing_Spatial_Reasoning_in_Multimodal_Large_Language_Models_through_Reasoning-based_ICCV_2025_paper.html": {
    "title": "Enhancing Spatial Reasoning in Multimodal Large Language Models through Reasoning-based Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenhua Ning",
      "Zhuotao Tian",
      "Shaoshuai Shi",
      "Guangming Lu",
      "Daojing He",
      "Wenjie Pei",
      "Li Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_MMAD_Multi-label_Micro-Action_Detection_in_Videos_ICCV_2025_paper.html": {
    "title": "MMAD: Multi-label Micro-Action Detection in Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kun Li",
      "Pengyu Liu",
      "Dan Guo",
      "Fei Wang",
      "Zhiliang Wu",
      "Hehe Fan",
      "Meng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xie_Human-in-the-Loop_Local_Corrections_of_3D_Scene_Layouts_via_Infilling_ICCV_2025_paper.html": {
    "title": "Human-in-the-Loop Local Corrections of 3D Scene Layouts via Infilling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christopher Xie",
      "Armen Avetisyan",
      "Henry Howard-Jenkins",
      "Yawar Siddiqui",
      "Julian Straub",
      "Richard Newcombe",
      "Vasileios Balntas",
      "Jakob Engel"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_2D_Gaussian_Splatting-based_Sparse-view_Transparent_Object_Depth_Reconstruction_via_Physics_ICCV_2025_paper.html": {
    "title": "2D Gaussian Splatting-based Sparse-view Transparent Object Depth Reconstruction via Physics Simulation for Scene Update",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeongyun Kim",
      "Seunghoon Jeong",
      "Giseop Kim",
      "Myung-Hwan Jeon",
      "Eunji Jun",
      "Ayoung Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_HouseCrafter_Lifting_Floorplans_to_3D_Scenes_with_2D_Diffusion_Models_ICCV_2025_paper.html": {
    "title": "HouseCrafter: Lifting Floorplans to 3D Scenes with 2D Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiwen Chen",
      "Hieu T. Nguyen",
      "Vikram Voleti",
      "Varun Jampani",
      "Huaizu Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Roetzer_Fast_Globally_Optimal_and_Geometrically_Consistent_3D_Shape_Matching_ICCV_2025_paper.html": {
    "title": "Fast Globally Optimal and Geometrically Consistent 3D Shape Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paul Roetzer",
      "Florian Bernard"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Desai_Guiding_Diffusion_Models_with_Adaptive_Negative_Sampling_Without_External_Resources_ICCV_2025_paper.html": {
    "title": "Guiding Diffusion Models with Adaptive Negative Sampling Without External Resources",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alakh Desai",
      "Nuno Vasconcelos"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_AlignGuard_Scalable_Safety_Alignment_for_Text-to-Image_Generation_ICCV_2025_paper.html": {
    "title": "AlignGuard: Scalable Safety Alignment for Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runtao Liu",
      "I Chieh Chen",
      "Jindong Gu",
      "Jipeng Zhang",
      "Renjie Pi",
      "Qifeng Chen",
      "Philip Torr",
      "Ashkan Khakzar",
      "Fabio Pizzati"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Function-centric_Bayesian_Network_for_Zero-Shot_Object_Goal_Navigation_ICCV_2025_paper.html": {
    "title": "Function-centric Bayesian Network for Zero-Shot Object Goal Navigation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sixian Zhang",
      "Xinyao Yu",
      "Xinhang Song",
      "Yiyao Wang",
      "Shuqiang Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sharma_Preserve_Anything_Controllable_Image_Synthesis_with_Object_Preservation_ICCV_2025_paper.html": {
    "title": "Preserve Anything: Controllable Image Synthesis with Object Preservation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prasen Kumar Sharma",
      "Neeraj Matiyali",
      "Siddharth Srivastava",
      "Gaurav Sharma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liao_Leveraging_Local_Patch_Alignment_to_Seam-cutting_for_Large_Parallax_Image_ICCV_2025_paper.html": {
    "title": "Leveraging Local Patch Alignment to Seam-cutting for Large Parallax Image Stitching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianli Liao",
      "Chenyang Zhao",
      "Lei Li",
      "Heling Cao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Revisiting_Efficient_Semantic_Segmentation_Learning_Offsets_for_Better_Spatial_and_ICCV_2025_paper.html": {
    "title": "Revisiting Efficient Semantic Segmentation: Learning Offsets for Better Spatial and Class Feature Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shi-Chen Zhang",
      "Yunheng Li",
      "Yu-Huan Wu",
      "Qibin Hou",
      "Ming-Ming Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gao_GIViC_Generative_Implicit_Video_Compression_ICCV_2025_paper.html": {
    "title": "GIViC: Generative Implicit Video Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ge Gao",
      "Siyue Teng",
      "Tianhao Peng",
      "Fan Zhang",
      "David Bull"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_TAD-E2E_A_Large-scale_End-to-end_Autonomous_Driving_Dataset_ICCV_2025_paper.html": {
    "title": "TAD-E2E: A Large-scale End-to-end Autonomous Driving Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chang Liu",
      "Mingxu Zhu",
      "Zheyuan Zhang",
      "Linna Song",
      "Xiao Zhao",
      "Qingliang Luo",
      "Qi Wang",
      "Chufan Guo",
      "Kuifeng Su"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jiang_Rethinking_Bimanual_Robotic_Manipulation_Learning_with_Decoupled_Interaction_Framework_ICCV_2025_paper.html": {
    "title": "Rethinking Bimanual Robotic Manipulation: Learning with Decoupled Interaction Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian-Jian Jiang",
      "Xiao-Ming Wu",
      "Yi-Xiang He",
      "Ling-An Zeng",
      "Yi-Lin Wei",
      "Dandan Zhang",
      "Wei-Shi Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_LVAgent_Long_Video_Understanding_by_Multi-Round_Dynamical_Collaboration_of_MLLM_ICCV_2025_paper.html": {
    "title": "LVAgent: Long Video Understanding by Multi-Round Dynamical Collaboration of MLLM Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boyu Chen",
      "Zhengrong Yue",
      "Siran Chen",
      "Zikang Wang",
      "Yang Liu",
      "Peng Li",
      "Yali Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kirschstein_Avat3r_Large_Animatable_Gaussian_Reconstruction_Model_for_High-fidelity_3D_Head_ICCV_2025_paper.html": {
    "title": "Avat3r: Large Animatable Gaussian Reconstruction Model for High-fidelity 3D Head Avatars",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tobias Kirschstein",
      "Javier Romero",
      "Artem Sevastopolsky",
      "Matthias Nießner",
      "Shunsuke Saito"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Mai_EVER_Exact_Volumetric_Ellipsoid_Rendering_for_Real-time_View_Synthesis_ICCV_2025_paper.html": {
    "title": "EVER: Exact Volumetric Ellipsoid Rendering for Real-time View Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Mai",
      "Peter Hedman",
      "George Kopanas",
      "Dor Verbin",
      "David Futschik",
      "Qiangeng Xu",
      "Falko Kuester",
      "Jonathan T. Barron",
      "Yinda Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_Democratizing_High-Fidelity_Co-Speech_Gesture_Video_Generation_ICCV_2025_paper.html": {
    "title": "Democratizing High-Fidelity Co-Speech Gesture Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xu Yang",
      "Shaoli Huang",
      "Shenbo Xie",
      "Xuelin Chen",
      "Yifei Liu",
      "Changxing Ding"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tang_Dissecting_Generalized_Category_Discovery_Multiplex_Consensus_under_Self-Deconstruction_ICCV_2025_paper.html": {
    "title": "Dissecting Generalized Category Discovery: Multiplex Consensus under Self-Deconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luyao Tang",
      "Kunze Huang",
      "Chaoqi Chen",
      "Yuxuan Yuan",
      "Chenxin Li",
      "Xiaotong Tu",
      "Xinghao Ding",
      "Yue Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_DiffIP_Representation_Fingerprints_for_Robust_IP_Protection_of_Diffusion_Models_ICCV_2025_paper.html": {
    "title": "DiffIP: Representation Fingerprints for Robust IP Protection of Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuoling Li",
      "Haoxuan Qu",
      "Jason Kuen",
      "Jiuxiang Gu",
      "Qiuhong Ke",
      "Jun Liu",
      "Hossein Rahmani"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Duan_Divide-and-Conquer_for_Enhancing_Unlabeled_Learning_Stability_and_Plasticity_in_Semi-supervised_ICCV_2025_paper.html": {
    "title": "Divide-and-Conquer for Enhancing Unlabeled Learning, Stability, and Plasticity in Semi-supervised Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Duan",
      "Taicai Chen",
      "Lei Qi",
      "Yinghuan Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Du_All_Parts_Matter_A_Unified_Mask-Free_Virtual_Try-On_Framework_ICCV_2025_paper.html": {
    "title": "All Parts Matter: A Unified Mask-Free Virtual Try-On Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenghu Du",
      "Shengwu Xiong",
      "Yi Rong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cao_CogNav_Cognitive_Process_Modeling_for_Object_Goal_Navigation_with_LLMs_ICCV_2025_paper.html": {
    "title": "CogNav: Cognitive Process Modeling for Object Goal Navigation with LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihan Cao",
      "Jiazhao Zhang",
      "Zhinan Yu",
      "Shuzhen Liu",
      "Zheng Qin",
      "Qin Zou",
      "Bo Du",
      "Kai Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ishihara_Spatio-Spectral_Pattern_Illumination_for_Direct_and_Indirect_Separation_from_a_ICCV_2025_paper.html": {
    "title": "Spatio-Spectral Pattern Illumination for Direct and Indirect Separation from a Single Hyperspectral Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shin Ishihara",
      "Imari Sato"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Langerman_Explaining_Human_Preferences_via_Metrics_for_Structured_3D_Reconstruction_ICCV_2025_paper.html": {
    "title": "Explaining Human Preferences via Metrics for Structured 3D Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jack Langerman",
      "Denys Rozumnyi",
      "Yuzhong Huang",
      "Dmytro Mishkin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Soni_LOCATEdit_Graph_Laplacian_Optimized_Cross_Attention_for_Localized_Text-Guided_Image_ICCV_2025_paper.html": {
    "title": "LOCATEdit: Graph Laplacian Optimized Cross Attention for Localized Text-Guided Image Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Achint Soni",
      "Meet Soni",
      "Sirisha Rambhatla"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_VistaDream_Sampling_multiview_consistent_images_for_single-view_scene_reconstruction_ICCV_2025_paper.html": {
    "title": "VistaDream: Sampling multiview consistent images for single-view scene reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiping Wang",
      "Yuan Liu",
      "Ziwei Liu",
      "Wenping Wang",
      "Zhen Dong",
      "Bisheng Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_LangScene-X_Reconstruct_Generalizable_3D_Language-Embedded_Scenes_with_TriMap_Video_Diffusion_ICCV_2025_paper.html": {
    "title": "LangScene-X: Reconstruct Generalizable 3D Language-Embedded Scenes with TriMap Video Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangfu Liu",
      "Hao Li",
      "Jiawei Chi",
      "Hanyang Wang",
      "Minghui Yang",
      "Fudong Wang",
      "Yueqi Duan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Krishnan_Orchid_Image_Latent_Diffusion_for_Joint_Appearance_and_Geometry_Generation_ICCV_2025_paper.html": {
    "title": "Orchid: Image Latent Diffusion for Joint Appearance and Geometry Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akshay Krishnan",
      "Xinchen Yan",
      "Vincent Casser",
      "Abhijit Kundu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jiang_MonoMVSNet_Monocular_Priors_Guided_Multi-View_Stereo_Network_ICCV_2025_paper.html": {
    "title": "MonoMVSNet: Monocular Priors Guided Multi-View Stereo Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianfei Jiang",
      "Qiankun Liu",
      "Haochen Yu",
      "Hongyuan Liu",
      "Liyong Wang",
      "Jiansheng Chen",
      "Huimin Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Top2Pano_Learning_to_Generate_Indoor_Panoramas_from_Top-Down_View_ICCV_2025_paper.html": {
    "title": "Top2Pano: Learning to Generate Indoor Panoramas from Top-Down View",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zitong Zhang",
      "Suranjan Gautam",
      "Rui Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Highlight_What_You_Want_Weakly-Supervised_Instance-Level_Controllable_Infrared-Visible_Image_Fusion_ICCV_2025_paper.html": {
    "title": "Highlight What You Want: Weakly-Supervised Instance-Level Controllable Infrared-Visible Image Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyu Wang",
      "Jizheng Zhang",
      "Haiyu Song",
      "Mingyu Ge",
      "Jiayu Wang",
      "Haoran Duan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Dai_Unbiased_Missing-modality_Multimodal_Learning_ICCV_2025_paper.html": {
    "title": "Unbiased Missing-modality Multimodal Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruiting Dai",
      "Chenxi Li",
      "Yandong Yan",
      "Lisi Mo",
      "Ke Qin",
      "Tao He"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_You_Think_You_ACT_The_New_Task_of_Arbitrary_Text_ICCV_2025_paper.html": {
    "title": "You Think, You ACT: The New Task of Arbitrary Text to Motion Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runqi Wang",
      "Caoyuan Ma",
      "Guopeng Li",
      "Hanrui Xu",
      "Yuke Li",
      "Zheng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Intermediate_Connectors_and_Geometric_Priors_for_Language-Guided_Affordance_Segmentation_on_ICCV_2025_paper.html": {
    "title": "Intermediate Connectors and Geometric Priors for Language-Guided Affordance Segmentation on Unseen Object Categories",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yicong Li",
      "Yiyang Chen",
      "Zhenyuan Ma",
      "Junbin Xiao",
      "Xiang Wang",
      "Angela Yao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_FALCON_Resolving_Visual_Redundancy_and_Fragmentation_in_High-resolution_Multimodal_Large_ICCV_2025_paper.html": {
    "title": "FALCON: Resolving Visual Redundancy and Fragmentation in High-resolution Multimodal Large Language Models via Visual Registers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Renshan Zhang",
      "Rui Shao",
      "Gongwei Chen",
      "Miao Zhang",
      "Kaiwen Zhou",
      "Weili Guan",
      "Liqiang Nie"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_Verbalized_Representation_Learning_for_Interpretable_Few-Shot_Generalization_ICCV_2025_paper.html": {
    "title": "Verbalized Representation Learning for Interpretable Few-Shot Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng-Fu Yang",
      "Da Yin",
      "Wenbo Hu",
      "Heng Ji",
      "Nanyun Peng",
      "Bolei Zhou",
      "Kai-Wei Chang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yu_HERO_Human_Reaction_Generation_from_Videos_ICCV_2025_paper.html": {
    "title": "HERO: Human Reaction Generation from Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengjun Yu",
      "Wei Zhai",
      "Yuhang Yang",
      "Yang Cao",
      "Zheng-Jun Zha"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Dong_Transformer-based_Tooth_Alignment_Prediction_with_Occlusion_and_Collision_Constraints_ICCV_2025_paper.html": {
    "title": "Transformer-based Tooth Alignment Prediction with Occlusion and Collision Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenxing Dong",
      "Jiazhou Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Thaker_Frequency-Guided_Posterior_Sampling_for_Diffusion-Based_Image_Restoration_ICCV_2025_paper.html": {
    "title": "Frequency-Guided Posterior Sampling for Diffusion-Based Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Darshan Thaker",
      "Abhishek Goyal",
      "Rene Vidal"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_Beyond_Walking_A_Large-Scale_Image-Text_Benchmark_for_Text-based_Person_Anomaly_ICCV_2025_paper.html": {
    "title": "Beyond Walking: A Large-Scale Image-Text Benchmark for Text-based Person Anomaly Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuyu Yang",
      "Yaxiong Wang",
      "Li Zhu",
      "Zhedong Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xiong_GigaTok_Scaling_Visual_Tokenizers_to_3_Billion_Parameters_for_Autoregressive_ICCV_2025_paper.html": {
    "title": "GigaTok: Scaling Visual Tokenizers to 3 Billion Parameters for Autoregressive Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianwei Xiong",
      "Jun Hao Liew",
      "Zilong Huang",
      "Jiashi Feng",
      "Xihui Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cheng_Perspective-aware_3D_Gaussian_Inpainting_with_Multi-view_Consistency_ICCV_2025_paper.html": {
    "title": "Perspective-aware 3D Gaussian Inpainting with Multi-view Consistency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxin Cheng",
      "Binxiao Huang",
      "Taiqiang Wu",
      "Wenyong Zhou",
      "Chenchen Ding",
      "Zhengwu Liu",
      "Graziano Chesi",
      "Ngai Wong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yu_DistillDrive_End-to-End_Multi-Mode_Autonomous_Driving_Distillation_by_Isomorphic_Hetero-Source_Planning_ICCV_2025_paper.html": {
    "title": "DistillDrive: End-to-End Multi-Mode Autonomous Driving Distillation by Isomorphic Hetero-Source Planning Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Yu",
      "Xianghang Zhang",
      "Runkai Zhao",
      "Huaicheng Yan",
      "Meng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Mohammadi_TESPEC_Temporally-Enhanced_Self-Supervised_Pretraining_for_Event_Cameras_ICCV_2025_paper.html": {
    "title": "TESPEC: Temporally-Enhanced Self-Supervised Pretraining for Event Cameras",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Mohammadi",
      "Ziyi Wu",
      "Igor Gilitschenski"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Banerjee_Wide2Long_Learning_Lens_Compression_and_Perspective_Adjustment_for_Wide-Angle_to_ICCV_2025_paper.html": {
    "title": "Wide2Long: Learning Lens Compression and Perspective Adjustment for Wide-Angle to Telephoto Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soumyadipta Banerjee",
      "Jiaul H. Paik",
      "Debashis Sen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ju_FullDiT_Video_Generative_Foundation_Models_with_Multimodal_Control_via_Full_ICCV_2025_paper.html": {
    "title": "FullDiT: Video Generative Foundation Models with Multimodal Control via Full Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan Ju",
      "Weicai Ye",
      "Quande Liu",
      "Qiulin Wang",
      "Xintao Wang",
      "Pengfei Wan",
      "Di Zhang",
      "Kun Gai",
      "Qiang Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jevtic_Feed-Forward_SceneDINO_for_Unsupervised_Semantic_Scene_Completion_ICCV_2025_paper.html": {
    "title": "Feed-Forward SceneDINO for Unsupervised Semantic Scene Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aleksandar Jevtić",
      "Christoph Reich",
      "Felix Wimbauer",
      "Oliver Hahn",
      "Christian Rupprecht",
      "Stefan Roth",
      "Daniel Cremers"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_V2XScenes_A_Multiple_Challenging_Traffic_Conditions_Dataset_for_Large-Range_Vehicle-Infrastructure_ICCV_2025_paper.html": {
    "title": "V2XScenes: A Multiple Challenging Traffic Conditions Dataset for Large-Range Vehicle-Infrastructure Collaborative Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowen Wang",
      "Yafei Wang",
      "Wei Gong",
      "Siheng Chen",
      "Genjia Liu",
      "Minhao Xiong",
      "Chin Long Ng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xiao_VideoAuteur_Towards_Long_Narrative_Video_Generation_ICCV_2025_paper.html": {
    "title": "VideoAuteur: Towards Long Narrative Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junfei Xiao",
      "Feng Cheng",
      "Lu Qi",
      "Liangke Gui",
      "Yang Zhao",
      "Shanchuan Lin",
      "Jiepeng Cen",
      "Zhibei Ma",
      "Alan Yuille",
      "Lu Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kang_Is_CLIP_ideal_No._Can_we_fix_it_Yes_ICCV_2025_paper.html": {
    "title": "Is CLIP ideal? No. Can we fix it? Yes!",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raphi Kang",
      "Yue Song",
      "Georgia Gkioxari",
      "Pietro Perona"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_LaneDiffusion_Improving_Centerline_Graph_Learning_via_Prior_Injected_BEV_Feature_ICCV_2025_paper.html": {
    "title": "LaneDiffusion: Improving Centerline Graph Learning via Prior Injected BEV Feature Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijie Wang",
      "Weiming Zhang",
      "Wei Zhang",
      "Xiao Tan",
      "Hongxing Liu",
      "Yaowei Wang",
      "Guanbin Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_Federated_Domain_Generalization_with_Domain-specific_Soft_Prompts_Generation_ICCV_2025_paper.html": {
    "title": "Federated Domain Generalization with Domain-specific Soft Prompts Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianhan Wu",
      "Xiaoyang Qu",
      "Zhangcheng Huang",
      "Jianzong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Webster_Multi-modal_Identity_Extraction_ICCV_2025_paper.html": {
    "title": "Multi-modal Identity Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryan Webster",
      "Teddy Furon"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Height-Fidelity_Dense_Global_Fusion_for_Multi-modal_3D_Object_Detection_ICCV_2025_paper.html": {
    "title": "Height-Fidelity Dense Global Fusion for Multi-modal 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanshi Wang",
      "Jin Gao",
      "Weiming Hu",
      "Zhipeng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Duan_DIH-CLIP_Unleashing_the_Diversity_of_Multi-Head_Self-Attention_for_Training-Free_Open-Vocabulary_ICCV_2025_paper.html": {
    "title": "DIH-CLIP: Unleashing the Diversity of Multi-Head Self-Attention for Training-Free Open-Vocabulary Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Songsong Duan",
      "Xi Yang",
      "Nannan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_ExploreGS_Explorable_3D_Scene_Reconstruction_with_Virtual_Camera_Samplings_and_ICCV_2025_paper.html": {
    "title": "ExploreGS: Explorable 3D Scene Reconstruction with Virtual Camera Samplings and Diffusion Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minsu Kim",
      "Subin Jeon",
      "In Cho",
      "Mijin Yoo",
      "Seon Joo Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Moon_Prototypes_are_Balanced_Units_for_Efficient_and_Effective_Partially_Relevant_ICCV_2025_paper.html": {
    "title": "Prototypes are Balanced Units for Efficient and Effective Partially Relevant Video Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "WonJun Moon",
      "Cheol-Ho Cho",
      "Woojin Jun",
      "Taeoh Kim",
      "Inwoong Lee",
      "Dongyoon Wee",
      "Minho Shim",
      "Jae-Pil Heo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_Diffusion_Epistemic_Uncertainty_with_Asymmetric_Learning_for_Diffusion-Generated_Image_Detection_ICCV_2025_paper.html": {
    "title": "Diffusion Epistemic Uncertainty with Asymmetric Learning for Diffusion-Generated Image Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingsong Huang",
      "Hui Guo",
      "Jing Huang",
      "Bing Bai",
      "Qi Xiong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zheng_SpiLiFormer_Enhancing_Spiking_Transformers_with_Lateral_Inhibition_ICCV_2025_paper.html": {
    "title": "SpiLiFormer: Enhancing Spiking Transformers with Lateral Inhibition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeqi Zheng",
      "Yanchen Huang",
      "Yingchao Yu",
      "Zizheng Zhu",
      "Junfeng Tang",
      "Zhaofei Yu",
      "Yaochu Jin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Dong_PS-Mamba_Spatial-Temporal_Graph_Mamba_for_Pose_Sequence_Refinement_ICCV_2025_paper.html": {
    "title": "PS-Mamba: Spatial-Temporal Graph Mamba for Pose Sequence Refinement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoye Dong",
      "Gim Hee Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ma_Model_Reveals_What_to_Cache_Profiling-Based_Feature_Reuse_for_Video_ICCV_2025_paper.html": {
    "title": "Model Reveals What to Cache: Profiling-Based Feature Reuse for Video Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuran Ma",
      "Yexin Liu",
      "Yaofu Liu",
      "Xianfeng Wu",
      "Mingzhe Zheng",
      "Zihao Wang",
      "Ser-Nam Lim",
      "Harry Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhu_IRASim_A_Fine-Grained_World_Model_for_Robot_Manipulation_ICCV_2025_paper.html": {
    "title": "IRASim: A Fine-Grained World Model for Robot Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangqi Zhu",
      "Hongtao Wu",
      "Song Guo",
      "Yuxiao Liu",
      "Chilam Cheang",
      "Tao Kong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xu_Diffusion-Based_Imaginative_Coordination_for_Bimanual_Manipulation_ICCV_2025_paper.html": {
    "title": "Diffusion-Based Imaginative Coordination for Bimanual Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huilin Xu",
      "Jian Ding",
      "Jiakun Xu",
      "Ruixiang Wang",
      "Jun Chen",
      "Jinjie Mai",
      "Yanwei Fu",
      "Bernard Ghanem",
      "Feng Xu",
      "Mohamed Elhoseiny"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Huy_Seeing_the_Trees_for_the_Forest_Rethinking_Weakly-Supervised_Medical_Visual_ICCV_2025_paper.html": {
    "title": "Seeing the Trees for the Forest: Rethinking Weakly-Supervised Medical Visual Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ta Duc Huy",
      "Duy Anh Huynh",
      "Yutong Xie",
      "Yuankai Qi",
      "Qi Chen",
      "Phi Le Nguyen",
      "Sen Kim Tran",
      "Son Lam Phung",
      "Anton van den Hengel",
      "Zhibin Liao",
      "Minh-Son To",
      "Johan W. Verjans",
      "Vu Minh Hieu Phan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Bao_Dataset_Distillation_as_Data_Compression_A_Rate-Utility_Perspective_ICCV_2025_paper.html": {
    "title": "Dataset Distillation as Data Compression: A Rate-Utility Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youneng Bao",
      "Yiping Liu",
      "Zhuo Chen",
      "Yongsheng Liang",
      "Mu Li",
      "Kede Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hu_GroundingSuite_Measuring_Complex_Multi-Granular_Pixel_Grounding_ICCV_2025_paper.html": {
    "title": "GroundingSuite: Measuring Complex Multi-Granular Pixel Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Hu",
      "Lianghui Zhu",
      "Yuxuan Zhang",
      "Tianheng Cheng",
      "Lei Liu",
      "Heng Liu",
      "Longjin Ran",
      "Xiaoxin Chen",
      "Wenyu Liu",
      "Xinggang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kay_Consensus-Driven_Active_Model_Selection_ICCV_2025_paper.html": {
    "title": "Consensus-Driven Active Model Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Justin Kay",
      "Grant Van Horn",
      "Subhransu Maji",
      "Daniel Sheldon",
      "Sara Beery"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jia_Learning_an_Implicit_Physics_Model_for_Image-based_Fluid_Simulation_ICCV_2025_paper.html": {
    "title": "Learning an Implicit Physics Model for Image-based Fluid Simulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emily Yue-Ting Jia",
      "Jiageng Mao",
      "Zhiyuan Gao",
      "Yajie Zhao",
      "Yue Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_Task-Decoupled_Bezier_Surface_Constraint_for_Uneven_Low-Light_Image_Enhancement_ICCV_2025_paper.html": {
    "title": "Task-Decoupled Bezier Surface Constraint for Uneven Low-Light Image Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingxiang Zhou",
      "Xiangdong Su",
      "Haoran Zhang",
      "Wei Chen",
      "Guanglai Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Go_VideoRFSplat_Direct_Scene-Level_Text-to-3D_Gaussian_Splatting_Generation_with_Flexible_Pose_ICCV_2025_paper.html": {
    "title": "VideoRFSplat: Direct Scene-Level Text-to-3D Gaussian Splatting Generation with Flexible Pose and Multi-View Joint Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyojun Go",
      "Byeongjun Park",
      "Hyelin Nam",
      "Byung-Hoon Kim",
      "Hyungjin Chung",
      "Changick Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Molahasani_PRISM_Reducing_Spurious_Implicit_Biases_in_Vision-Language_Models_with_LLM-Guided_ICCV_2025_paper.html": {
    "title": "PRISM: Reducing Spurious Implicit Biases in Vision-Language Models with LLM-Guided Embedding Projection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mahdiyar Molahasani",
      "Azadeh Motamedi",
      "Michael Greenspan",
      "Il-Min Kim",
      "Ali Etemad"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hu_Animate_Anyone_2_High-Fidelity_Character_Image_Animation_with_Environment_Affordance_ICCV_2025_paper.html": {
    "title": "Animate Anyone 2: High-Fidelity Character Image Animation with Environment Affordance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li Hu",
      "Guangyuan Wang",
      "Zhen Shen",
      "Xin Gao",
      "Dechao Meng",
      "Lian Zhuo",
      "Peng Zhang",
      "Bang Zhang",
      "Liefeng Bo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tian_DATA_Domain-And-Time_Alignment_for_High-Quality_Feature_Fusion_in_Collaborative_Perception_ICCV_2025_paper.html": {
    "title": "DATA: Domain-And-Time Alignment for High-Quality Feature Fusion in Collaborative Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengchang Tian",
      "Jianwei Ma",
      "Yan Huang",
      "Zhanye Chen",
      "Honghao Wei",
      "Hui Zhang",
      "Wei Hong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_LongAnimation_Long_Animation_Generation_with_Dynamic_Global-Local_Memory_ICCV_2025_paper.html": {
    "title": "LongAnimation: Long Animation Generation with Dynamic Global-Local Memory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nan Chen",
      "Mengqi Huang",
      "Yihao Meng",
      "Zhendong Mao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Dunnhofer_Is_Tracking_Really_More_Challenging_in_First_Person_Egocentric_Vision_ICCV_2025_paper.html": {
    "title": "Is Tracking Really More Challenging in First Person Egocentric Vision?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matteo Dunnhofer",
      "Zaira Manigrasso",
      "Christian Micheloni"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xu_Bringing_RNNs_Back_to_Efficient_Open-Ended_Video_Understanding_ICCV_2025_paper.html": {
    "title": "Bringing RNNs Back to Efficient Open-Ended Video Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weili Xu",
      "Enxin Song",
      "Wenhao Chai",
      "Xuexiang Wen",
      "Tian Ye",
      "Gaoang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Deng_ViM-VQ_Efficient_Post-Training_Vector_Quantization_for_Visual_Mamba_ICCV_2025_paper.html": {
    "title": "ViM-VQ: Efficient Post-Training Vector Quantization for Visual Mamba",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juncan Deng",
      "Shuaiting Li",
      "Zeyu Wang",
      "Kedong Xu",
      "Hong Gu",
      "Kejie Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hashmi_TorchAdapt_Towards_Light-Agnostic_Real-Time_Visual_Perception_ICCV_2025_paper.html": {
    "title": "TorchAdapt: Towards Light-Agnostic Real-Time Visual Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Khurram Azeem Hashmi",
      "Karthik Palyakere Suresh",
      "Didier  Stricker",
      "Muhammad Zeshan  Afzal"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_UniversalBooth_Model-Agnostic_Personalized_Text-to-Image_Generation_ICCV_2025_paper.html": {
    "title": "UniversalBooth: Model-Agnostic Personalized Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Songhua Liu",
      "Ruonan Yu",
      "Xinchao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Diao_EVEv2_Improved_Baselines_for_Encoder-Free_Vision-Language_Models_ICCV_2025_paper.html": {
    "title": "EVEv2: Improved Baselines for Encoder-Free Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiwen Diao",
      "Xiaotong Li",
      "Yufeng Cui",
      "Yueze Wang",
      "Haoge Deng",
      "Ting Pan",
      "Wenxuan Wang",
      "Huchuan Lu",
      "Xinlong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_End-to-End_Driving_with_Online_Trajectory_Evaluation_via_BEV_World_Model_ICCV_2025_paper.html": {
    "title": "End-to-End Driving with Online Trajectory Evaluation via BEV World Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingyan Li",
      "Yuqi Wang",
      "Yang Liu",
      "Jiawei He",
      "Lue Fan",
      "Zhaoxiang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_EEGMirror_Leveraging_EEG_Data_in_the_Wild_via_Montage-Agnostic_Self-Supervision_ICCV_2025_paper.html": {
    "title": "EEGMirror: Leveraging EEG Data in the Wild via Montage-Agnostic Self-Supervision for EEG to Video Decoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan-Hao Liu",
      "Bao-Liang Lu",
      "Wei-Long Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_Edit360_2D_Image_Edits_to_3D_Assets_from_Any_Angle_ICCV_2025_paper.html": {
    "title": "Edit360: 2D Image Edits to 3D Assets from Any Angle",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junchao Huang",
      "Xinting Hu",
      "Shaoshuai Shi",
      "Zhuotao Tian",
      "Li Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_VehicleMAE_View-asymmetry_Mutual_Learning_for_Vehicle_Re-identification_Pre-training_via_Masked_ICCV_2025_paper.html": {
    "title": "VehicleMAE: View-asymmetry Mutual Learning for Vehicle Re-identification Pre-training via Masked AutoEncoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Wang",
      "Zeyu Zhang",
      "Dong Wang",
      "Di Gai",
      "Xin Xiong",
      "Jiyang Xu",
      "Ruihua Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hu_MiDSummer_Multi-Guidance_Diffusion_for_Controllable_Zero-Shot_Immersive_Gaussian_Splatting_Scene_ICCV_2025_paper.html": {
    "title": "MiDSummer: Multi-Guidance Diffusion for Controllable Zero-Shot Immersive Gaussian Splatting Scene Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anjun Hu",
      "Richard Tomsett",
      "Valentin Gourmet",
      "Massimo Camplani",
      "Jas Kandola",
      "Hanting Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Haji-Ali_AV-Link_Temporally-Aligned_Diffusion_Features_for_Cross-Modal_Audio-Video_Generation_ICCV_2025_paper.html": {
    "title": "AV-Link: Temporally-Aligned Diffusion Features for Cross-Modal Audio-Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Moayed Haji-Ali",
      "Willi Menapace",
      "Aliaksandr Siarohin",
      "Ivan Skorokhodov",
      "Alper Canberk",
      "Kwot Sin Lee",
      "Vicente Ordonez",
      "Sergey Tulyakov"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_RANKCLIP_Ranking-Consistent_Language-Image_Pretraining_ICCV_2025_paper.html": {
    "title": "RANKCLIP: Ranking-Consistent Language-Image Pretraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiming Zhang",
      "Zhuokai Zhao",
      "Zhaorun Chen",
      "Zhili Feng",
      "Zenghui Ding",
      "Yining Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Reiss_Is_Visual_in-Context_Learning_for_Compositional_Medical_Tasks_within_Reach_ICCV_2025_paper.html": {
    "title": "Is Visual in-Context Learning for Compositional Medical Tasks within Reach?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simon Reiß",
      "Zdravko Marinov",
      "Alexander Jaus",
      "Constantin Seibold",
      "M. Saquib Sarfraz",
      "Erik Rodner",
      "Rainer Stiefelhagen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jung_Details_Matter_for_Indoor_Open-vocabulary_3D_Instance_Segmentation_ICCV_2025_paper.html": {
    "title": "Details Matter for Indoor Open-vocabulary 3D Instance Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanghun Jung",
      "Jingjing Zheng",
      "Ke Zhang",
      "Nan Qiao",
      "Albert Y. C. Chen",
      "Lu Xia",
      "Chi Liu",
      "Yuyin Sun",
      "Xiao Zeng",
      "Hsiang-Wei Huang",
      "Byron Boots",
      "Min Sun",
      "Cheng-Hao Kuo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tsai_Differentially_Private_Fine-Tuning_of_Diffusion_Models_ICCV_2025_paper.html": {
    "title": "Differentially Private Fine-Tuning of Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu-Lin Tsai",
      "Yizhe Li",
      "Chia-Mu Yu",
      "Xuebin Ren",
      "Po-Yu Chen",
      "Zekai Chen",
      "Francois Buet-Golfouse"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Barsbey_Large_Learning_Rates_Simultaneously_Achieve_Robustness_to_Spurious_Correlations_and_ICCV_2025_paper.html": {
    "title": "Large Learning Rates Simultaneously Achieve Robustness to Spurious Correlations and Compressibility",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Melih Barsbey",
      "Lucas Prieto",
      "Stefanos Zafeiriou",
      "Tolga Birdal"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Global_Regulation_and_Excitation_via_Attention_Tuning_for_Stereo_Matching_ICCV_2025_paper.html": {
    "title": "Global Regulation and Excitation via Attention Tuning for Stereo Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahao Li",
      "Xinhong Chen",
      "Zhengmin Jiang",
      "Qian Zhou",
      "Yung-Hui Li",
      "Jianping Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Duan_TruthPrInt_Mitigating_Large_Vision-Language_Models_Object_Hallucination_Via_Latent_Truthful-Guided_ICCV_2025_paper.html": {
    "title": "TruthPrInt: Mitigating Large Vision-Language Models Object Hallucination Via Latent Truthful-Guided Pre-Intervention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinhao Duan",
      "Fei Kong",
      "Hao Cheng",
      "James Diffenderfer",
      "Bhavya Kailkhura",
      "Lichao Sun",
      "Xiaofeng Zhu",
      "Xiaoshuang Shi",
      "Kaidi Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ji_Customizing_Domain_Adapters_for_Domain_Generalization_ICCV_2025_paper.html": {
    "title": "Customizing Domain Adapters for Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuyang Ji",
      "Zeyi Huang",
      "Haohan Wang",
      "Yong Jae Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wong_ADCD-Net_Robust_Document_Image_Forgery_Localization_via_Adaptive_DCT_Feature_ICCV_2025_paper.html": {
    "title": "ADCD-Net: Robust Document Image Forgery Localization via Adaptive DCT Feature and Hierarchical Content Disentanglement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kahim Wong",
      "Jicheng Zhou",
      "Haiwei Wu",
      "Yain-Whar Si",
      "Jiantao Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_STaR_Seamless_Spatial-Temporal_Aware_Motion_Retargeting_with_Penetration_and_Consistency_ICCV_2025_paper.html": {
    "title": "STaR: Seamless Spatial-Temporal Aware Motion Retargeting with Penetration and Consistency Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaohang Yang",
      "Qing Wang",
      "Jiahao Yang",
      "Gregory Slabaugh",
      "Shanxin Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sun_Structured_Policy_Optimization_Enhance_Large_Vision-Language_Model_via_Self-referenced_Dialogue_ICCV_2025_paper.html": {
    "title": "Structured Policy Optimization: Enhance Large Vision-Language Model via Self-referenced Dialogue",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guohao Sun",
      "Can Qin",
      "Yihao Feng",
      "Zeyuan Chen",
      "Ran Xu",
      "Sohail Dianat",
      "Majid Rabbani",
      "Raghuveer Rao",
      "Zhiqiang Tao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhong_UnrealZoo_Enriching_Photo-realistic_Virtual_Worlds_for_Embodied_AI_ICCV_2025_paper.html": {
    "title": "UnrealZoo: Enriching Photo-realistic Virtual Worlds for Embodied AI",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangwei Zhong",
      "Kui Wu",
      "Churan Wang",
      "Hao Chen",
      "Hai Ci",
      "Zhoujun Li",
      "Yizhou Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Luo_Visual_Test-time_Scaling_for_GUI_Agent_Grounding_ICCV_2025_paper.html": {
    "title": "Visual Test-time Scaling for GUI Agent Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tiange Luo",
      "Lajanugen Logeswaran",
      "Justin Johnson",
      "Honglak Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Atmis_One-Step_Specular_Highlight_Removal_with_Adapted_Diffusion_Models_ICCV_2025_paper.html": {
    "title": "One-Step Specular Highlight Removal with Adapted Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mahir Atmis",
      "Levent Karacan",
      "Mehmet Sarıgül"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chu_GraspCoT_Integrating_Physical_Property_Reasoning_for_6-DoF_Grasping_under_Flexible_ICCV_2025_paper.html": {
    "title": "GraspCoT: Integrating Physical Property Reasoning for 6-DoF Grasping under Flexible Language Instructions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaomeng Chu",
      "Jiajun Deng",
      "Guoliang You",
      "Wei Liu",
      "Xingchen Li",
      "Jianmin Ji",
      "Yanyong Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Nagashima_Deep_Space_Weather_Model_Long-Range_Solar_Flare_Prediction_from_Multi-Wavelength_ICCV_2025_paper.html": {
    "title": "Deep Space Weather Model: Long-Range Solar Flare Prediction from Multi-Wavelength Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shunya Nagashima",
      "Komei Sugiura"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Hydra-NeXt_Robust_Closed-Loop_Driving_with_Open-Loop_Training_ICCV_2025_paper.html": {
    "title": "Hydra-NeXt: Robust Closed-Loop Driving with Open-Loop Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenxin Li",
      "Shihao Wang",
      "Shiyi Lan",
      "Zhiding Yu",
      "Zuxuan Wu",
      "Jose M. Alvarez"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yue_Zero-Shot_Vision_Encoder_Grafting_via_LLM_Surrogates_ICCV_2025_paper.html": {
    "title": "Zero-Shot Vision Encoder Grafting via LLM Surrogates",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaiyu Yue",
      "Vasu Singla",
      "Menglin Jia",
      "John Kirchenbauer",
      "Rifaa Qadri",
      "Zikui Cai",
      "Abhinav Bhatele",
      "Furong Huang",
      "Tom Goldstein"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhu_Depth_Any_Event_Stream_Enhancing_Event-based_Monocular_Depth_Estimation_via_ICCV_2025_paper.html": {
    "title": "Depth Any Event Stream: Enhancing Event-based Monocular Depth Estimation via Dense-to-Sparse Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinjing Zhu",
      "Tianbo Pan",
      "Zidong Cao",
      "Yexin Liu",
      "James T. Kwok",
      "Hui Xiong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_Uncover_Treasures_in_DCT_Advancing_JPEG_Quality_Enhancement_by_Exploiting_ICCV_2025_paper.html": {
    "title": "Uncover Treasures in DCT: Advancing JPEG Quality Enhancement by Exploiting Latent Correlations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Yang",
      "Qunliang Xing",
      "Mai Xu",
      "Minglang Qiao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Layer-wise_Vision_Injection_with_Disentangled_Attention_for_Efficient_LVLMs_ICCV_2025_paper.html": {
    "title": "Layer-wise Vision Injection with Disentangled Attention for Efficient LVLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuange Zhang",
      "Dengjie Li",
      "Bo Liu",
      "Zenghao Bao",
      "Yao Zhou",
      "Baisong Yang",
      "Zhongying Liu",
      "Yujie Zhong",
      "Tongtong Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_Heatmap_Regression_without_Soft-Argmax_for_Facial_Landmark_Detection_ICCV_2025_paper.html": {
    "title": "Heatmap Regression without Soft-Argmax for Facial Landmark Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chiao-An Yang",
      "Raymond A. Yeh"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Pan_ICE-Bench_A_Unified_and_Comprehensive_Benchmark_for_Image_Creating_and_ICCV_2025_paper.html": {
    "title": "ICE-Bench: A Unified and Comprehensive Benchmark for Image Creating and Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yulin Pan",
      "Xiangteng He",
      "Chaojie Mao",
      "Zhen Han",
      "Zeyinzi Jiang",
      "Jingfeng Zhang",
      "Yu Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jin_Unified_Adversarial_Augmentation_for_Improving_Palmprint_Recognition_ICCV_2025_paper.html": {
    "title": "Unified Adversarial Augmentation for Improving Palmprint Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianlong Jin",
      "Chenglong Zhao",
      "Ruixin Zhang",
      "Sheng Shang",
      "Yang Zhao",
      "Jun Wang",
      "Jingyun Zhang",
      "Shouhong Ding",
      "Wei Jia",
      "Yunsheng Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_PlugMark_A_Plug-in_Zero-Watermarking_Framework_for_Diffusion_Models_ICCV_2025_paper.html": {
    "title": "PlugMark: A Plug-in Zero-Watermarking Framework for Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengzhen Chen",
      "Yanwei Liu",
      "Xiaoyan Gu",
      "Enci Liu",
      "Zhuoyi Shang",
      "Xiangyang Ji",
      "Wu Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Su_FLSeg_Enhancing_Privacy_and_Robustness_in_Federated_Learning_under_Heterogeneous_ICCV_2025_paper.html": {
    "title": "FLSeg: Enhancing Privacy and Robustness in Federated Learning under Heterogeneous Data via Model Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zichun Su",
      "Zhi Lu",
      "Yutong Wu",
      "Renfei Shen",
      "Songfeng Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Feng_Partially_Matching_Submap_Helps_Uncertainty_Modeling_and_Propagation_for_Text_ICCV_2025_paper.html": {
    "title": "Partially Matching Submap Helps: Uncertainty Modeling and Propagation for Text to Point Cloud Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingtao Feng",
      "Longlong Mei",
      "Zijie Wu",
      "Jianqiao Luo",
      "Fenghao Tian",
      "Jie Feng",
      "Weisheng Dong",
      "Yaonan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_Human-Object_Interaction_from_Human-Level_Instructions_ICCV_2025_paper.html": {
    "title": "Human-Object Interaction from Human-Level Instructions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Wu",
      "Jiaman Li",
      "Pei Xu",
      "C. Karen Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ding_SAM2Long_Enhancing_SAM_2_for_Long_Video_Segmentation_with_a_ICCV_2025_paper.html": {
    "title": "SAM2Long: Enhancing SAM 2 for Long Video Segmentation with a Training-Free Memory Tree",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuangrui Ding",
      "Rui Qian",
      "Xiaoyi Dong",
      "Pan Zhang",
      "Yuhang Zang",
      "Yuhang Cao",
      "Yuwei Guo",
      "Dahua Lin",
      "Jiaqi Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Fang_Adapting_Vehicle_Detectors_for_Aerial_Imagery_to_Unseen_Domains_with_ICCV_2025_paper.html": {
    "title": "Adapting Vehicle Detectors for Aerial Imagery to Unseen Domains with Weak Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Fang",
      "Minhyek Jeon",
      "Zheyang Qin",
      "Stanislav Panev",
      "Celso De Melo",
      "Shuowen Hu",
      "Shayok Chakraborty",
      "Fernando De La Torre"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Neural_Shell_Texture_Splatting_More_Details_and_Fewer_Primitives_ICCV_2025_paper.html": {
    "title": "Neural Shell Texture Splatting: More Details and Fewer Primitives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Zhang",
      "Anpei Chen",
      "Jincheng Xiong",
      "Pinxuan Dai",
      "Yujun Shen",
      "Weiwei Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ma_3DSRBench_A_Comprehensive_3D_Spatial_Reasoning_Benchmark_ICCV_2025_paper.html": {
    "title": "3DSRBench: A Comprehensive 3D Spatial Reasoning Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wufei Ma",
      "Haoyu Chen",
      "Guofeng Zhang",
      "Yu-Cheng Chou",
      "Jieneng Chen",
      "Celso de Melo",
      "Alan Yuille"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gao_Causality-guided_Prompt_Learning_for_Vision-language_Models_via_Visual_Granulation_ICCV_2025_paper.html": {
    "title": "Causality-guided Prompt Learning for Vision-language Models via Visual Granulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengyu Gao",
      "Qiulei Dong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sun_Arti-PG_A_Toolbox_for_Procedurally_Synthesizing_Large-Scale_and_Diverse_Articulated_ICCV_2025_paper.html": {
    "title": "Arti-PG: A Toolbox for Procedurally Synthesizing Large-Scale and Diverse Articulated Objects with Rich Annotations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianhua Sun",
      "Yuxuan Li",
      "Jiude Wei",
      "Longfei Xu",
      "Nange Wang",
      "Yining Zhang",
      "Cewu Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ma_X2I_Seamless_Integration_of_Multimodal_Understanding_into_Diffusion_Transformer_via_ICCV_2025_paper.html": {
    "title": "X2I: Seamless Integration of Multimodal Understanding into Diffusion Transformer via Attention Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Ma",
      "Qirong Peng",
      "Xu Guo",
      "Chen Chen",
      "Haonan Lu",
      "Zhenyu Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_One_Last_Attention_for_Your_Vision-Language_Model_ICCV_2025_paper.html": {
    "title": "One Last Attention for Your Vision-Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liang Chen",
      "Ghazi Shazan Ahmad",
      "Tianjun Yao",
      "Lingqiao Liu",
      "Zhiqiang Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tian_MMCR_Benchmarking_Cross-Source_Reasoning_in_Scientific_Papers_ICCV_2025_paper.html": {
    "title": "MMCR: Benchmarking Cross-Source Reasoning in Scientific Papers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Tian",
      "Zheng Lu",
      "Mingqi Gao",
      "Zheng Liu",
      "Bo Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gong_ZeroKey_Point-Level_Reasoning_and_Zero-Shot_3D_Keypoint_Detection_from_Large_ICCV_2025_paper.html": {
    "title": "ZeroKey: Point-Level Reasoning and Zero-Shot 3D Keypoint Detection from Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bingchen Gong",
      "Diego Gomez",
      "Abdullah Hamdi",
      "Abdelrahman Eldesokey",
      "Ahmed Abdelreheem",
      "Peter Wonka",
      "Maks Ovsjanikov"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hong_Audio-visual_Controlled_Video_Diffusion_with_Masked_Selective_State_Spaces_Modeling_ICCV_2025_paper.html": {
    "title": "Audio-visual Controlled Video Diffusion with Masked Selective State Spaces Modeling for Natural Talking Head Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fa-Ting Hong",
      "Zunnan Xu",
      "Zixiang Zhou",
      "Jun Zhou",
      "Xiu Li",
      "Qin Lin",
      "Qinglin Lu",
      "Dan Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_DreamCube_RGB-D_Panorama_Generation_via_Multi-plane_Synchronization_ICCV_2025_paper.html": {
    "title": "DreamCube: RGB-D Panorama Generation via Multi-plane Synchronization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yukun Huang",
      "Yanning Zhou",
      "Jianan Wang",
      "Kaiyi Huang",
      "Xihui Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Raphaeli_SILO_Solving_Inverse_Problems_with_Latent_Operators_ICCV_2025_paper.html": {
    "title": "SILO: Solving Inverse Problems with Latent Operators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ron Raphaeli",
      "Sean Man",
      "Michael Elad"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yan_Entropy-Adaptive_Diffusion_Policy_Optimization_with_Dynamic_Step_Alignment_ICCV_2025_paper.html": {
    "title": "Entropy-Adaptive Diffusion Policy Optimization with Dynamic Step Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "RenYe Yan",
      "Jikang Cheng",
      "Yaozhong Gan",
      "Shikun Sun",
      "You Wu",
      "Yunfan Yang",
      "Liang Ling",
      "Jinlong Lin",
      "Yeshuang Zhu",
      "Jie Zhou",
      "Jinchao Zhang",
      "Junliang Xing",
      "Yimao Cai",
      "Ru Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Boudjoghra_ScanEdit_Hierarchically-Guided_Functional_3D_Scan_Editing_ICCV_2025_paper.html": {
    "title": "ScanEdit: Hierarchically-Guided Functional 3D Scan Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohamed El Amine Boudjoghra",
      "Ivan Laptev",
      "Angela Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Qin_DeFSS_Image-to-Mask_Denoising_Learning_for_Few-shot_Segmentation_ICCV_2025_paper.html": {
    "title": "DeFSS: Image-to-Mask Denoising Learning for Few-shot Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zishu Qin",
      "Junhao Xu",
      "Weifeng Ge"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Seizinger_Bokehlicious_Photorealistic_Bokeh_Rendering_with_Controllable_Apertures_ICCV_2025_paper.html": {
    "title": "Bokehlicious: Photorealistic Bokeh Rendering with Controllable Apertures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tim Seizinger",
      "Florin-Alexandru Vasluianu",
      "Marcos V. Conde",
      "Zongwei Wu",
      "Radu Timofte"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ren_Turbo2K_Towards_Ultra-Efficient_and_High-Quality_2K_Video_Synthesis_ICCV_2025_paper.html": {
    "title": "Turbo2K: Towards Ultra-Efficient and High-Quality 2K Video Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingjing Ren",
      "Wenbo Li",
      "Zhongdao Wang",
      "Haoze Sun",
      "Bangzhen Liu",
      "Haoyu Chen",
      "Jiaqi Xu",
      "Aoxue Li",
      "Shifeng Zhang",
      "Bin Shao",
      "Yong Guo",
      "Lei Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ju_Video2BEV_Transforming_Drone_Videos_to_BEVs_for_Video-based_Geo-localization_ICCV_2025_paper.html": {
    "title": "Video2BEV: Transforming Drone Videos to BEVs for Video-based Geo-localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Ju",
      "Shaofei Huang",
      "Si Liu",
      "Zhedong Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Puppet-Master_Scaling_Interactive_Video_Generation_as_a_Motion_Prior_for_ICCV_2025_paper.html": {
    "title": "Puppet-Master: Scaling Interactive Video Generation as a Motion Prior for Part-Level Dynamics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruining Li",
      "Chuanxia Zheng",
      "Christian Rupprecht",
      "Andrea Vedaldi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gao_MMAT-1M_A_Large_Reasoning_Dataset_for_Multimodal_Agent_Tuning_ICCV_2025_paper.html": {
    "title": "MMAT-1M: A Large Reasoning Dataset for Multimodal Agent Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianhong Gao",
      "Yannian Fu",
      "Weiqun Wu",
      "Haixiao Yue",
      "Shanshan Liu",
      "Gang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Patel_FlowChef_Steering_of_Rectified_Flow_Models_for_Controlled_Generations_ICCV_2025_paper.html": {
    "title": "FlowChef: Steering of Rectified Flow Models for Controlled Generations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maitreya Patel",
      "Song Wen",
      "Dimitris N. Metaxas",
      "Yezhou Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Doveh_Teaching_VLMs_to_Localize_Specific_Objects_from_In-context_Examples_ICCV_2025_paper.html": {
    "title": "Teaching VLMs to Localize Specific Objects from In-context Examples",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sivan Doveh",
      "Nimrod Shabtay",
      "Eli Schwartz",
      "Hilde Kuehne",
      "Raja Giryes",
      "Rogerio Feris",
      "Leonid Karlinsky",
      "James Glass",
      "Assaf Arbelle",
      "Shimon Ullman",
      "M. Jehanzeb Mirza"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xu_A_Hidden_Stumbling_Block_in_Generalized_Category_Discovery_Distracted_Attention_ICCV_2025_paper.html": {
    "title": "A Hidden Stumbling Block in Generalized Category Discovery: Distracted Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiyu Xu",
      "Zhanxuan Hu",
      "Yu Duan",
      "Ercheng Pei",
      "Yonghang Tai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_High-Precision_3D_Measurement_of_Complex_Textured_Surfaces_Using_Multiple_Filtering_ICCV_2025_paper.html": {
    "title": "High-Precision 3D Measurement of Complex Textured Surfaces Using Multiple Filtering Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchong Chen",
      "Jian Yu",
      "Shaoyan Gai",
      "Zeyu Cai",
      "Feipeng Da"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kuhn_Efficient_Unsupervised_Shortcut_Learning_Detection_and_Mitigation_in_Transformers_ICCV_2025_paper.html": {
    "title": "Efficient Unsupervised Shortcut Learning Detection and Mitigation in Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lukas Kuhn",
      "Sari Sadiya",
      "Jörg Schlötterer",
      "Florian Buettner",
      "Christin Seifert",
      "Gemma Roig"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kahatapitiya_Adaptive_Caching_for_Faster_Video_Generation_with_Diffusion_Transformers_ICCV_2025_paper.html": {
    "title": "Adaptive Caching for Faster Video Generation with Diffusion Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kumara Kahatapitiya",
      "Haozhe Liu",
      "Sen He",
      "Ding Liu",
      "Menglin Jia",
      "Chenyang Zhang",
      "Michael S. Ryoo",
      "Tian Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Du_RGE-GS_Reward-Guided_Expansive_Driving_Scene_Reconstruction_via_Diffusion_Priors_ICCV_2025_paper.html": {
    "title": "RGE-GS: Reward-Guided Expansive Driving Scene Reconstruction via Diffusion Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sicong Du",
      "Jiarun Liu",
      "Qifeng Chen",
      "Hao-Xiang Chen",
      "Tai-Jiang Mu",
      "Sheng Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Nguyen_EgoMusic-driven_Human_Dance_Motion_Estimation_with_Skeleton_Mamba_ICCV_2025_paper.html": {
    "title": "EgoMusic-driven Human Dance Motion Estimation with Skeleton Mamba",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quang Nguyen",
      "Nhat Le",
      "Baoru Huang",
      "Minh Nhat Vu",
      "Chengcheng Tang",
      "Van Nguyen",
      "Ngan Le",
      "Thieu Vo",
      "Anh Nguyen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chu_OmniCache_A_Trajectory-Oriented_Global_Perspective_on_Training-Free_Cache_Reuse_for_ICCV_2025_paper.html": {
    "title": "OmniCache: A Trajectory-Oriented Global Perspective on Training-Free Cache Reuse for Diffusion Transformer Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huanpeng Chu",
      "Wei Wu",
      "Guanyu Feng",
      "Yutao Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_VIPerson_Flexibly_Generating_Virtual_Identity_for_Person_Re-Identification_ICCV_2025_paper.html": {
    "title": "VIPerson: Flexibly Generating Virtual Identity for Person Re-Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao-Wen Zhang",
      "Delong Zhang",
      "Yi-Xing Peng",
      "Zhi Ouyang",
      "Jingke Meng",
      "Wei-Shi Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Feng_St4RTrack_Simultaneous_4D_Reconstruction_and_Tracking_in_the_World_ICCV_2025_paper.html": {
    "title": "St4RTrack: Simultaneous 4D Reconstruction and Tracking in the World",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiwen Feng",
      "Junyi Zhang",
      "Qianqian Wang",
      "Yufei Ye",
      "Pengcheng Yu",
      "Michael J. Black",
      "Trevor Darrell",
      "Angjoo Kanazawa"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ge_PRM_Photometric_Stereo_based_Large_Reconstruction_Model_ICCV_2025_paper.html": {
    "title": "PRM: Photometric Stereo based Large Reconstruction Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhang Ge",
      "Jiantao Lin",
      "Guibao Shen",
      "Jiawei Feng",
      "Tao Hu",
      "Xinli Xu",
      "Ying-Cong Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cao_Exploiting_Frequency_Dynamics_for_Enhanced_Multimodal_Event-based_Action_Recognition_ICCV_2025_paper.html": {
    "title": "Exploiting Frequency Dynamics for Enhanced Multimodal Event-based Action Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meiqi Cao",
      "Xiangbo Shu",
      "Xin Jiang",
      "Rui Yan",
      "Yazhou Yao",
      "Jinhui Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_AerialVG_A_Challenging_Benchmark_for_Aerial_Visual_Grounding_by_Exploring_ICCV_2025_paper.html": {
    "title": "AerialVG: A Challenging Benchmark for Aerial Visual Grounding by Exploring Positional Relations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junli Liu",
      "Qizhi Chen",
      "Zhigang Wang",
      "Yiwen Tang",
      "Yiting Zhang",
      "Chi Yan",
      "Dong Wang",
      "Xuelong Li",
      "Bin Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_DriveArena_A_Closed-loop_Generative_Simulation_Platform_for_Autonomous_Driving_ICCV_2025_paper.html": {
    "title": "DriveArena: A Closed-loop Generative Simulation Platform for Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuemeng Yang",
      "Licheng Wen",
      "Tiantian Wei",
      "Yukai Ma",
      "Jianbiao Mei",
      "Xin Li",
      "Wenjie Lei",
      "Daocheng Fu",
      "Pinlong Cai",
      "Min Dou",
      "Liang He",
      "Yong Liu",
      "Botian Shi",
      "Yu Qiao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_InvRGBL_Inverse_Rendering_of_Complex_Scenes_with_Unified_Color_and_ICCV_2025_paper.html": {
    "title": "InvRGB+L: Inverse Rendering of Complex Scenes with Unified Color and LiDAR Reflectance Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoxue Chen",
      "Bhargav Chandaka",
      "Chih-Hao Lin",
      "Ya-Qin Zhang",
      "David Forsyth",
      "Hao Zhao",
      "Shenlong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_DreamFuse_Adaptive_Image_Fusion_with_Diffusion_Transformer_ICCV_2025_paper.html": {
    "title": "DreamFuse: Adaptive Image Fusion with Diffusion Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junjia Huang",
      "Pengxiang Yan",
      "Jiyang Liu",
      "Jie Wu",
      "Zhao Wang",
      "Yitong Wang",
      "Liang Lin",
      "Guanbin Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_DALIP_Distribution_Alignment-based_Language-Image_Pre-Training_for_Domain-Specific_Data_ICCV_2025_paper.html": {
    "title": "DALIP: Distribution Alignment-based Language-Image Pre-Training for Domain-Specific Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junjie Wu",
      "Jiangtao Xie",
      "Zhaolin Zhang",
      "Qilong Wang",
      "Qinghua Hu",
      "Peihua Li",
      "Sen Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/He_Joint_Semantic_and_Rendering_Enhancements_in_3D_Gaussian_Modeling_with_ICCV_2025_paper.html": {
    "title": "Joint Semantic and Rendering Enhancements in 3D Gaussian Modeling with Anisotropic Local Encoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingming He",
      "Chongyi Li",
      "Shiqi Wang",
      "Sam Kwong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Unveiling_the_Invisible_Reasoning_Complex_Occlusions_Amodally_with_AURA_ICCV_2025_paper.html": {
    "title": "Unveiling the Invisible: Reasoning Complex Occlusions Amodally with AURA",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhixuan Li",
      "Hyunse Yoon",
      "Sanghoon Lee",
      "Weisi Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_AGO_Adaptive_Grounding_for_Open_World_3D_Occupancy_Prediction_ICCV_2025_paper.html": {
    "title": "AGO: Adaptive Grounding for Open World 3D Occupancy Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peizheng Li",
      "Shuxiao Ding",
      "You Zhou",
      "Qingwen Zhang",
      "Onat Inak",
      "Larissa Triess",
      "Niklas Hanselmann",
      "Marius Cordts",
      "Andreas Zell"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_Free2Guide_Training-Free_Text-to-Video_Alignment_using_Image_LVLM_ICCV_2025_paper.html": {
    "title": "Free2Guide: Training-Free Text-to-Video Alignment using Image LVLM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaemin Kim",
      "Bryan Sangwoo Kim",
      "Jong Chul Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_T2I-Copilot_A_Training-Free_Multi-Agent_Text-to-Image_System_for_Enhanced_Prompt_Interpretation_ICCV_2025_paper.html": {
    "title": "T2I-Copilot: A Training-Free Multi-Agent Text-to-Image System for Enhanced Prompt Interpretation and Interactive Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chieh-Yun Chen",
      "Min Shi",
      "Gong Zhang",
      "Humphrey Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Qin_Lumina-Image_2.0_A_Unified_and_Efficient_Image_Generative_Framework_ICCV_2025_paper.html": {
    "title": "Lumina-Image 2.0: A Unified and Efficient Image Generative Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Qin",
      "Le Zhuo",
      "Yi Xin",
      "Ruoyi Du",
      "Zhen Li",
      "Bin Fu",
      "Yiting Lu",
      "Xinyue Li",
      "Dongyang Liu",
      "Xiangyang Zhu",
      "Will Beddow",
      "Erwann Millon",
      "Victor Perez",
      "Wenhai Wang",
      "Yu Qiao",
      "Bo Zhang",
      "Xiaohong Liu",
      "Hongsheng Li",
      "Chang Xu",
      "Peng Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_mmCooper_A_Multi-agent_Multi-stage_Communication-efficient_and_Collaboration-robust_Cooperative_Perception_Framework_ICCV_2025_paper.html": {
    "title": "mmCooper: A Multi-agent Multi-stage Communication-efficient and Collaboration-robust Cooperative Perception Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bingyi Liu",
      "Jian Teng",
      "Hongfei Xue",
      "Enshu Wang",
      "Chuanhui Zhu",
      "Pu Wang",
      "Libing Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yuan_LightBSR_Towards_Lightweight_Blind_Super-Resolution_via_Discriminative_Implicit_Degradation_Representation_ICCV_2025_paper.html": {
    "title": "LightBSR: Towards Lightweight Blind Super-Resolution via Discriminative Implicit Degradation Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiang Yuan",
      "Ji Ma",
      "Bo Wang",
      "Guanzhou Ke",
      "Weiming Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Harnessing_Uncertainty-aware_Bounding_Boxes_for_Unsupervised_3D_Object_Detection_ICCV_2025_paper.html": {
    "title": "Harnessing Uncertainty-aware Bounding Boxes for Unsupervised 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruiyang Zhang",
      "Hu Zhang",
      "Zhedong Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ling_OCSplats_Observation_Completeness_Quantification_and_Label_Noise_Separation_in_3DGS_ICCV_2025_paper.html": {
    "title": "OCSplats: Observation Completeness Quantification and Label Noise Separation in 3DGS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Ling",
      "Xian Xu",
      "Yinghui Sun",
      "Quansen Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Dutta_One_Encoder_to_Rule_them_All_Representation_Learning_for_Model-free_ICCV_2025_paper.html": {
    "title": "One Encoder to Rule them All: Representation Learning for Model-free Visual Reinforcement Learning using Fourier Neural Operators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Parag Dutta",
      "Mohd Ayyoob",
      "Shalabh Bhatnagar",
      "Ambedkar Dukkipati"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lee_LOMM_Latest_Object_Memory_Management_for_Temporally_Consistent_Video_Instance_ICCV_2025_paper.html": {
    "title": "LOMM: Latest Object Memory Management for Temporally Consistent Video Instance Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seunghun Lee",
      "Jiwan Seo",
      "Minwoo Choi",
      "Kiljoon Han",
      "Jahoon Jeong",
      "Zane Durante",
      "Ehsan Adeli",
      "Sang Hyun Park",
      "Sunghoon Im"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lei_Open-Vocabulary_HOI_Detection_with_Interaction-aware_Prompt_and_Concept_Calibration_ICCV_2025_paper.html": {
    "title": "Open-Vocabulary HOI Detection with Interaction-aware Prompt and Concept Calibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ting Lei",
      "Shaofeng Yin",
      "Qingchao Chen",
      "Yuxin Peng",
      "Yang Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yan_DuCos_Duality_Constrained_Depth_Super-Resolution_via_Foundation_Model_ICCV_2025_paper.html": {
    "title": "DuCos: Duality Constrained Depth Super-Resolution via Foundation Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiqiang Yan",
      "Zhengxue Wang",
      "Haoye Dong",
      "Jun Li",
      "Jian Yang",
      "Gim Hee Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Peng_ROVI_A_VLM-LLM_Re-Captioned_Dataset_for_Open-Vocabulary_Instance-Grounded_Text-to-Image_Generation_ICCV_2025_paper.html": {
    "title": "ROVI: A VLM-LLM Re-Captioned Dataset for Open-Vocabulary Instance-Grounded Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cihang Peng",
      "Qiming Hou",
      "Zhong Ren",
      "Kun Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhu_PBFG_A_New_Physically-Based_Dataset_and_Removal_of_Lens_Flares_ICCV_2025_paper.html": {
    "title": "PBFG: A New Physically-Based Dataset and Removal of Lens Flares and Glares",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Zhu",
      "Sungkil Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Qin_Active_Perception_Meets_Rule-Guided_RL_A_Two-Phase_Approach_for_Precise_ICCV_2025_paper.html": {
    "title": "Active Perception Meets Rule-Guided RL: A Two-Phase Approach for Precise Object Navigation in Complex Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liang Qin",
      "Min Wang",
      "Peiwei Li",
      "Wengang Zhou",
      "Houqiang Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yuan_CAT_A_Unified_Click-and-Track_Framework_for_Realistic_Tracking_ICCV_2025_paper.html": {
    "title": "CAT: A Unified Click-and-Track Framework for Realistic Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongsheng Yuan",
      "Jie Zhao",
      "Dong Wang",
      "Huchuan Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhao_Differential-informed_Sample_Selection_Accelerates_Multimodal_Contrastive_Learning_ICCV_2025_paper.html": {
    "title": "Differential-informed Sample Selection Accelerates Multimodal Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihua Zhao",
      "Feng Hong",
      "Mengxi Chen",
      "Pengyi Chen",
      "Benyuan Liu",
      "Jiangchao Yao",
      "Ya Zhang",
      "Yanfeng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Fan_A_View-consistent_Sampling_Method_for_Regularized_Training_of_Neural_Radiance_ICCV_2025_paper.html": {
    "title": "A View-consistent Sampling Method for Regularized Training of Neural Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aoxiang Fan",
      "Corentin Dumery",
      "Nicolas Talabot",
      "Pascal Fua"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yi_LUT-Fuse_Towards_Extremely_Fast_Infrared_and_Visible_Image_Fusion_via_ICCV_2025_paper.html": {
    "title": "LUT-Fuse: Towards Extremely Fast Infrared and Visible Image Fusion via Distillation to Learnable Look-Up Tables",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xunpeng Yi",
      "Yibing Zhang",
      "Xinyu Xiang",
      "Qinglong Yan",
      "Han Xu",
      "Jiayi Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Long_AdsQA_Towards_Advertisement_Video_Understanding_ICCV_2025_paper.html": {
    "title": "AdsQA: Towards Advertisement Video Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinwei Long",
      "Kai Tian",
      "Peng Xu",
      "Guoli Jia",
      "Jingxuan Li",
      "Sa Yang",
      "Yihua Shao",
      "Kaiyan Zhang",
      "Che Jiang",
      "Hao Xu",
      "Yang Liu",
      "Jiaheng Ma",
      "Bowen Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhu_Fine-grained_Abnormality_Prompt_Learning_for_Zero-shot_Anomaly_Detection_ICCV_2025_paper.html": {
    "title": "Fine-grained Abnormality Prompt Learning for Zero-shot Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawen Zhu",
      "Yew-Soon Ong",
      "Chunhua Shen",
      "Guansong Pang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liang_Learning_Dense_Feature_Matching_via_Lifting_Single_2D_Image_to_ICCV_2025_paper.html": {
    "title": "Learning Dense Feature Matching via Lifting Single 2D Image to 3D Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingping Liang",
      "Yutao Hu",
      "Wenqi Shao",
      "Ying Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zheng_DNF-Intrinsic_Deterministic_Noise-Free_Diffusion_for_Indoor_Inverse_Rendering_ICCV_2025_paper.html": {
    "title": "DNF-Intrinsic: Deterministic Noise-Free Diffusion for Indoor Inverse Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rongjia Zheng",
      "Qing Zhang",
      "Chengjiang Long",
      "Wei-Shi Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Unified_Open-World_Segmentation_with_Multi-Modal_Prompts_ICCV_2025_paper.html": {
    "title": "Unified Open-World Segmentation with Multi-Modal Prompts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Liu",
      "Yufei Yin",
      "Chenchen Jing",
      "Muzhi Zhu",
      "Hao Chen",
      "Yuling Xi",
      "Bo Feng",
      "Hao Wang",
      "Shiyu Li",
      "Chunhua Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shao_GausSim_Foreseeing_Reality_by_Gaussian_Simulator_for_Elastic_Objects_ICCV_2025_paper.html": {
    "title": "GausSim: Foreseeing Reality by Gaussian Simulator for Elastic Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yidi Shao",
      "Mu Huang",
      "Chen Change Loy",
      "Bo Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Beche_ClaraVid_A_Holistic_Scene_Reconstruction_Benchmark_From_Aerial_Perspective_With_ICCV_2025_paper.html": {
    "title": "ClaraVid: A Holistic Scene Reconstruction Benchmark From Aerial Perspective With Delentropy-Based Complexity Profiling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Radu Beche",
      "Sergiu Nedevschi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yan_TextMaster_A_Unified_Framework_for_Realistic_Text_Editing_via_Glyph-Style_ICCV_2025_paper.html": {
    "title": "TextMaster: A Unified Framework for Realistic Text Editing via Glyph-Style Dual-Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenyu Yan",
      "Jian Wang",
      "Aoqiang Wang",
      "Yuhan Li",
      "Wenxiang Shang",
      "Zhu Hangcheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yu_X2-Gaussian_4D_Radiative_Gaussian_Splatting_for_Continuous-time_Tomographic_Reconstruction_ICCV_2025_paper.html": {
    "title": "X2-Gaussian: 4D Radiative Gaussian Splatting for Continuous-time Tomographic Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weihao Yu",
      "Yuanhao Cai",
      "Ruyi Zha",
      "Zhiwen Fan",
      "Chenxin Li",
      "Yixuan Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Fu_FrameFusion_Combining_Similarity_and_Importance_for_Video_Token_Reduction_on_ICCV_2025_paper.html": {
    "title": "FrameFusion: Combining Similarity and Importance for Video Token Reduction on Large Vision Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyu Fu",
      "Tengxuan Liu",
      "Qinghao Han",
      "Guohao Dai",
      "Shengen Yan",
      "Huazhong Yang",
      "Xuefei Ning",
      "Yu Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chowdhury_AURELIA_Test-time_Reasoning_Distillation_in_Audio-Visual_LLMs_ICCV_2025_paper.html": {
    "title": "AURELIA: Test-time Reasoning Distillation in Audio-Visual LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanjoy Chowdhury",
      "Hanan Gani",
      "Nishit Anand",
      "Sayan Nag",
      "Ruohan Gao",
      "Mohamed Elhoseiny",
      "Salman Khan",
      "Dinesh Manocha"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liang_Unraveling_the_Smoothness_Properties_of_Diffusion_Models_A_Gaussian_Mixture_ICCV_2025_paper.html": {
    "title": "Unraveling the Smoothness Properties of Diffusion Models: A Gaussian Mixture Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingyu Liang",
      "Zhizhou Sha",
      "Zhenmei Shi",
      "Zhao Song",
      "Mingda Wan",
      "Yufa Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/You_FB-Diff_Fourier_Basis-guided_Diffusion_for_Temporal_Interpolation_of_4D_Medical_ICCV_2025_paper.html": {
    "title": "FB-Diff: Fourier Basis-guided Diffusion for Temporal Interpolation of 4D Medical Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin You",
      "Runze Yang",
      "Chuyan Zhang",
      "Zhongliang Jiang",
      "Jie Yang",
      "Nassir Navab"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Deng_UNIS_A_Unified_Framework_for_Achieving_Unbiased_Neural_Implicit_Surfaces_ICCV_2025_paper.html": {
    "title": "UNIS: A Unified Framework for Achieving Unbiased Neural Implicit Surfaces in Volume Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junkai Deng",
      "Hanting Niu",
      "Jiaze Li",
      "Fei Hou",
      "Ying He"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ospanov_Scendi_Score_Prompt-Aware_Diversity_Evaluation_via_Schur_Complement_of_CLIP_ICCV_2025_paper.html": {
    "title": "Scendi Score: Prompt-Aware Diversity Evaluation via Schur Complement of CLIP Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Azim Ospanov",
      "Mohammad Jalali",
      "Farzan Farnia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ren_Seeing_the_Unseen_A_Semantic_Alignment_and_Context-Aware_Prompt_Framework_ICCV_2025_paper.html": {
    "title": "Seeing the Unseen: A Semantic Alignment and Context-Aware Prompt Framework for Open-Vocabulary Camouflaged Object Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peng Ren",
      "Tian Bai",
      "Jing Sun",
      "Fuming Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lei_HOLa_Zero-Shot_HOI_Detection_with_Low-Rank_Decomposed_VLM_Feature_Adaptation_ICCV_2025_paper.html": {
    "title": "HOLa: Zero-Shot HOI Detection with Low-Rank Decomposed VLM Feature Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qinqian Lei",
      "Bo Wang",
      "Robby T. Tan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhao_Context-Aware_Academic_Emotion_Dataset_and_Benchmark_ICCV_2025_paper.html": {
    "title": "Context-Aware Academic Emotion Dataset and Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luming Zhao",
      "Jingwen Xuan",
      "Jiamin Lou",
      "Yonghui Yu",
      "Wenwu Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Meral_Contrastive_Test-Time_Composition_of_Multiple_LoRA_Models_for_Image_Generation_ICCV_2025_paper.html": {
    "title": "Contrastive Test-Time Composition of Multiple LoRA Models for Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tuna Han Salih Meral",
      "Enis Simsar",
      "Federico Tombari",
      "Pinar Yanardag"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shin_DiffRefine_Diffusion-based_Proposal_Specific_Point_Cloud_Densification_for_Cross-Domain_Object_ICCV_2025_paper.html": {
    "title": "DiffRefine: Diffusion-based Proposal Specific Point Cloud Densification for Cross-Domain Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sangyun Shin",
      "Yuhang He",
      "Xinyu Hou",
      "Samuel Hodgson",
      "Andrew Markham",
      "Niki Trigoni"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hu_Seeing_Through_Deepfakes_A_Human-Inspired_Framework_for_Multi-Face_Detection_ICCV_2025_paper.html": {
    "title": "Seeing Through Deepfakes: A Human-Inspired Framework for Multi-Face Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juan Hu",
      "Shaojing Fan",
      "Terence Sim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Test-time_Adaptation_for_Foundation_Medical_Segmentation_Model_Without_Parametric_Updates_ICCV_2025_paper.html": {
    "title": "Test-time Adaptation for Foundation Medical Segmentation Model Without Parametric Updates",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kecheng  Chen",
      "Xinyu  Luo",
      "Tiexin Qin",
      "Jie Liu",
      "Hui Liu",
      "Victor Ho Fun  Lee",
      "Hong  Yan",
      "Haoliang Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sun_Hierarchy_UGP_Hierarchy_Unified_Gaussian_Primitive_for_Large-Scale_Dynamic_Scene_ICCV_2025_paper.html": {
    "title": "Hierarchy UGP: Hierarchy Unified Gaussian Primitive for Large-Scale Dynamic Scene Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyang Sun",
      "Qinglin Yang",
      "Jiawei Wang",
      "Zhen Xu",
      "Chen Liu",
      "Yida Wang",
      "Kun Zhan",
      "Hujun Bao",
      "Xiaowei Zhou",
      "Sida Peng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chan_SMSTracker_Tri-path_Score_Mask_Sigma_Fusion_for_Multi-Modal_Tracking_ICCV_2025_paper.html": {
    "title": "SMSTracker: Tri-path Score Mask Sigma Fusion for Multi-Modal Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sixian  Chan",
      "Zedong Li",
      "Wenhao Li",
      "Shijian Lu",
      "Chunhua Shen",
      "Xiaoqin  Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhu_Distilling_Parallel_Gradients_for_Fast_ODE_Solvers_of_Diffusion_Models_ICCV_2025_paper.html": {
    "title": "Distilling Parallel Gradients for Fast ODE Solvers of Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Beier Zhu",
      "Ruoyu Wang",
      "Tong Zhao",
      "Hanwang Zhang",
      "Chi Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Casey_Aligning_Constraint_Generation_with_Design_Intent_in_Parametric_CAD_ICCV_2025_paper.html": {
    "title": "Aligning Constraint Generation with Design Intent in Parametric CAD",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Evan Casey",
      "Tianyu Zhang",
      "Shu Ishida",
      "John Roger Thompson",
      "Amir Khasahmadi",
      "Joseph George Lambourne",
      "Pradeep Kumar Jayaraman",
      "Karl D.D. Willis"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhao_DisCo_Towards_Distinct_and_Coherent_Visual_Encapsulation_in_Video_MLLMs_ICCV_2025_paper.html": {
    "title": "DisCo: Towards Distinct and Coherent Visual Encapsulation in Video MLLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahe Zhao",
      "Rongkun Zheng",
      "Yi Wang",
      "Helin Wang",
      "Hengshuang Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Fang_PUMA_Empowering_Unified_MLLM_with_Multi-granular_Visual_Generation_ICCV_2025_paper.html": {
    "title": "PUMA: Empowering Unified MLLM with Multi-granular Visual Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rongyao Fang",
      "Chengqi Duan",
      "Kun Wang",
      "Hao Li",
      "Linjiang Huang",
      "Hao Tian",
      "Xingyu Zeng",
      "Rui Zhao",
      "Jifeng Dai",
      "Hongsheng Li",
      "Xihui Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yu_Axis-level_Symmetry_Detection_with_Group-Equivariant_Representation_ICCV_2025_paper.html": {
    "title": "Axis-level Symmetry Detection with Group-Equivariant Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wongyun Yu",
      "Ahyun Seo",
      "Minsu Cho"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_SMP-Attack_Boosting_the_Transferability_of_Feature_Importance-based_Adversarial_Attack_with_ICCV_2025_paper.html": {
    "title": "SMP-Attack: Boosting the Transferability of Feature Importance-based Adversarial Attack with Semantics-aware Multi-granularity Patchout",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wen Yang",
      "Guodong Liu",
      "Di Ming"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jiang_Referring_to_Any_Person_ICCV_2025_paper.html": {
    "title": "Referring to Any Person",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qing Jiang",
      "Lin Wu",
      "Zhaoyang Zeng",
      "Tianhe Ren",
      "Yuda Xiong",
      "Yihao Chen",
      "Liu Qin",
      "Lei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lu_GWM_Towards_Scalable_Gaussian_World_Models_for_Robotic_Manipulation_ICCV_2025_paper.html": {
    "title": "GWM: Towards Scalable Gaussian World Models for Robotic Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guanxing Lu",
      "Baoxiong Jia",
      "Puhao Li",
      "Yixin Chen",
      "Ziwei Wang",
      "Yansong Tang",
      "Siyuan Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yeo_Statistical_Confidence_Rescoring_for_Robust_3D_Scene_Graph_Generation_from_ICCV_2025_paper.html": {
    "title": "Statistical Confidence Rescoring for Robust 3D Scene Graph Generation from Multi-View Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Xun Yeo",
      "Yanyan Li",
      "Gim Hee Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_Draw_Your_Mind_Personalized_Generation_via_Condition-Level_Modeling_in_Text-to-Image_ICCV_2025_paper.html": {
    "title": "Draw Your Mind: Personalized Generation via Condition-Level Modeling in Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyungjin Kim",
      "Seokho Ahn",
      "Young-Duk Seo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Guo_CopyrightShield_Enhancing_Diffusion_Model_Security_Against_Copyright_Infringement_Attacks_ICCV_2025_paper.html": {
    "title": "CopyrightShield: Enhancing Diffusion Model Security Against Copyright Infringement Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhixiang Guo",
      "Siyuan Liang",
      "Aishan Liu",
      "Dacheng Tao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Conde_PixTalk_Controlling_Photorealistic_Image_Processing_and_Editing_with_Language_ICCV_2025_paper.html": {
    "title": "PixTalk: Controlling Photorealistic Image Processing and Editing with Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marcos V. Conde",
      "Zihao Lu",
      "Radu Timofte"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yan_Learning_Streaming_Video_Representation_via_Multitask_Training_ICCV_2025_paper.html": {
    "title": "Learning Streaming Video Representation via Multitask Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yibin Yan",
      "Jilan Xu",
      "Shangzhe Di",
      "Yikun Liu",
      "Yudi Shi",
      "Qirui Chen",
      "Zeqian Li",
      "Yifei Huang",
      "Weidi Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_DASH_4D_Hash_Encoding_with_Self-Supervised_Decomposition_for_Real-Time_Dynamic_ICCV_2025_paper.html": {
    "title": "DASH: 4D Hash Encoding with Self-Supervised Decomposition for Real-Time Dynamic Scene Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Chen",
      "Zhangchi Hu",
      "Peixi Wu",
      "Huyue Zhu",
      "Hebei Li",
      "Xiaoyan Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Dong_Pose-Star_Anatomy-Aware_Editing_for_Open-World_Fashion_Images_ICCV_2025_paper.html": {
    "title": "Pose-Star: Anatomy-Aware Editing for Open-World Fashion Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuran Dong",
      "Mang Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhu_MamV2XCalib_V2X-based_Target-less_Infrastructure_Camera_Calibration_with_State_Space_Model_ICCV_2025_paper.html": {
    "title": "MamV2XCalib: V2X-based Target-less Infrastructure Camera Calibration with State Space Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaoye Zhu",
      "Zhe Wang",
      "Yan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jiang_Beyond_the_Destination_A_Novel_Benchmark_for_Exploration-Aware_Embodied_Question_ICCV_2025_paper.html": {
    "title": "Beyond the Destination: A Novel Benchmark for Exploration-Aware Embodied Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaixuan Jiang",
      "Yang Liu",
      "Weixing Chen",
      "Jingzhou Luo",
      "Ziliang Chen",
      "Ling Pan",
      "Guanbin Li",
      "Liang Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Fan_Semantic_Equitable_Clustering_A_Simple_and_Effective_Strategy_for_Clustering_ICCV_2025_paper.html": {
    "title": "Semantic Equitable Clustering: A Simple and Effective Strategy for Clustering Vision Tokens",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qihang Fan",
      "Huaibo Huang",
      "Mingrui Chen",
      "Ran He"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tang_HiP-AD_Hierarchical_and_Multi-Granularity_Planning_with_Deformable_Attention_for_Autonomous_ICCV_2025_paper.html": {
    "title": "HiP-AD: Hierarchical and Multi-Granularity Planning with Deformable Attention for Autonomous Driving in a Single Decoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingqi Tang",
      "Zhuoran Xu",
      "Zhaotie Meng",
      "Erkang Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_Visual_Textualization_for_Image_Prompted_Object_Detection_ICCV_2025_paper.html": {
    "title": "Visual Textualization for Image Prompted Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongjian Wu",
      "Yang Zhou",
      "Jiya Saiyin",
      "Bingzheng Wei",
      "Yan Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Qin_Sliced_Wasserstein_Bridge_for_Open-Vocabulary_Video_Instance_Segmentation_ICCV_2025_paper.html": {
    "title": "Sliced Wasserstein Bridge for Open-Vocabulary Video Instance Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheyun Qin",
      "Deng Yu",
      "Chuanchen Luo",
      "Zhumin Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jiang_Moderating_the_Generalization_of_Score-based_Generative_Model_ICCV_2025_paper.html": {
    "title": "Moderating the Generalization of Score-based Generative Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wan Jiang",
      "He Wang",
      "Xin Zhang",
      "Dan Guo",
      "Zhaoxin Fan",
      "Yunfeng Diao",
      "Richang Hong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Guan_A_Token-level_Text_Image_Foundation_Model_for_Document_Understanding_ICCV_2025_paper.html": {
    "title": "A Token-level Text Image Foundation Model for Document Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tongkun Guan",
      "Zining Wang",
      "Pei Fu",
      "Zhengtao Guo",
      "Wei Shen",
      "Kai Zhou",
      "Tiezhu Yue",
      "Chen Duan",
      "Hao Sun",
      "Qianyi Jiang",
      "Junfeng Luo",
      "Xiaokang Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Song_Riemannian-Geometric_Fingerprints_of_Generative_Models_ICCV_2025_paper.html": {
    "title": "Riemannian-Geometric Fingerprints of Generative Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hae Jin Song",
      "Laurent Itti"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Fang_NeRF_Is_a_Valuable_Assistant_for_3D_Gaussian_Splatting_ICCV_2025_paper.html": {
    "title": "NeRF Is a Valuable Assistant for 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuangkang Fang",
      "I-Chao Shen",
      "Takeo Igarashi",
      "Yufeng Wang",
      "ZeSheng Wang",
      "Yi Yang",
      "Wenrui Ding",
      "Shuchang Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jaenal_Towards_Visual_Localization_Interoperability_Cross-Feature_for_Collaborative_Visual_Localization_and_ICCV_2025_paper.html": {
    "title": "Towards Visual Localization Interoperability: Cross-Feature for Collaborative Visual Localization and Mapping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alberto Jaenal",
      "Paula Carbó Cubero",
      "José Araújo",
      "André Mateus"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_EasyControl_Adding_Efficient_and_Flexible_Control_for_Diffusion_Transformer_ICCV_2025_paper.html": {
    "title": "EasyControl: Adding Efficient and Flexible Control for Diffusion Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Zhang",
      "Yirui Yuan",
      "Yiren Song",
      "Haofan Wang",
      "Jiaming Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_PerLDiff_Controllable_Street_View_Synthesis_Using_Perspective-Layout_Diffusion_Model_ICCV_2025_paper.html": {
    "title": "PerLDiff: Controllable Street View Synthesis Using Perspective-Layout Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinhua Zhang",
      "Hualian Sheng",
      "Sijia Cai",
      "Bing Deng",
      "Qiao Liang",
      "Wen Li",
      "Ying Fu",
      "Jieping Ye",
      "Shuhang Gu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yu_DocThinker_Explainable_Multimodal_Large_Language_Models_with_Rule-based_Reinforcement_Learning_ICCV_2025_paper.html": {
    "title": "DocThinker: Explainable Multimodal Large Language Models with Rule-based Reinforcement Learning for Document Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenwen Yu",
      "Zhibo Yang",
      "Yuliang Liu",
      "Xiang Bai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Feng_I2VControl_Disentangled_and_Unified_Video_Motion_Synthesis_Control_ICCV_2025_paper.html": {
    "title": "I2VControl: Disentangled and Unified Video Motion Synthesis Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wanquan Feng",
      "Tianhao Qi",
      "Jiawei Liu",
      "Mingzhen Sun",
      "Pengqi Tu",
      "Tianxiang Ma",
      "Fei Dai",
      "Songtao Zhao",
      "Siyu Zhou",
      "Qian He"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_Democratizing_Text-to-Image_Masked_Generative_Models_with_Compact_Text-Aware_One-Dimensional_Tokens_ICCV_2025_paper.html": {
    "title": "Democratizing Text-to-Image Masked Generative Models with Compact Text-Aware One-Dimensional Tokens",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongwon Kim",
      "Ju He",
      "Qihang Yu",
      "Chenglin Yang",
      "Xiaohui Shen",
      "Suha Kwak",
      "Liang-Chieh Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Aligning_Global_Semantics_and_Local_Textures_in_Generative_Video_Enhancement_ICCV_2025_paper.html": {
    "title": "Aligning Global Semantics and Local Textures in Generative Video Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhikai Chen",
      "Fuchen Long",
      "Zhaofan Qiu",
      "Ting Yao",
      "Wengang Zhou",
      "Jiebo Luo",
      "Tao Mei"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_VISO_Accelerating_In-orbit_Object_Detection_with_Language-Guided_Mask_Learning_and_ICCV_2025_paper.html": {
    "title": "VISO: Accelerating In-orbit Object Detection with Language-Guided Mask Learning and Sparse Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meiqi Wang",
      "Han Qiu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_Reverse_Convolution_and_Its_Applications_to_Image_Restoration_ICCV_2025_paper.html": {
    "title": "Reverse Convolution and Its Applications to Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuhong Huang",
      "Shiqi Liu",
      "Kai Zhang",
      "Ying Tai",
      "Jian Yang",
      "Hui Zeng",
      "Lei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Feng_ATCTrack_Aligning_Target-Context_Cues_with_Dynamic_Target_States_for_Robust_ICCV_2025_paper.html": {
    "title": "ATCTrack: Aligning Target-Context Cues with Dynamic Target States for Robust Vision-Language Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaokun Feng",
      "Shiyu Hu",
      "Xuchen Li",
      "Dailing Zhang",
      "Meiqi Wu",
      "Jing Zhang",
      "Xiaotang Chen",
      "Kaiqi Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xiao_Reminiscence_Attack_on_Residuals_Exploiting_Approximate_Machine_Unlearning_for_Privacy_ICCV_2025_paper.html": {
    "title": "Reminiscence Attack on Residuals: Exploiting Approximate Machine Unlearning for Privacy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaxin Xiao",
      "Qingqing Ye",
      "Li Hu",
      "Huadi Zheng",
      "Haibo Hu",
      "Zi Liang",
      "Haoyang Li",
      "Yijie Jiao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Dai_SeaS_Few-shot_Industrial_Anomaly_Image_Generation_with_Separation_and_Sharing_ICCV_2025_paper.html": {
    "title": "SeaS: Few-shot Industrial Anomaly Image Generation with Separation and Sharing Fine-tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhewei Dai",
      "Shilei Zeng",
      "Haotian Liu",
      "Xurui Li",
      "Feng Xue",
      "Yu Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_CreatiLayout_Siamese_Multimodal_Diffusion_Transformer_for_Creative_Layout-to-Image_Generation_ICCV_2025_paper.html": {
    "title": "CreatiLayout: Siamese Multimodal Diffusion Transformer for Creative Layout-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hui Zhang",
      "Dexiang Hong",
      "Yitong Wang",
      "Jie Shao",
      "Xinglong Wu",
      "Zuxuan Wu",
      "Yu-Gang Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Rao_AMD_Adaptive_Momentum_and_Decoupled_Contrastive_Learning_Framework_for_Robust_ICCV_2025_paper.html": {
    "title": "AMD: Adaptive Momentum and Decoupled Contrastive Learning Framework for Robust Long-Tail Trajectory Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bin Rao",
      "Haicheng Liao",
      "Yanchen Guan",
      "Chengyue Wang",
      "Bonan Wang",
      "Jiaxun Zhang",
      "Zhenning Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xin_Music_Grounding_by_Short_Video_ICCV_2025_paper.html": {
    "title": "Music Grounding by Short Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijie Xin",
      "Minquan Wang",
      "Jingyu Liu",
      "Quan Chen",
      "Ye Ma",
      "Peng Jiang",
      "Xirong Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lyu_DyWA_Dynamics-adaptive_World_Action_Model_for_Generalizable_Non-prehensile_Manipulation_ICCV_2025_paper.html": {
    "title": "DyWA: Dynamics-adaptive World Action Model for Generalizable Non-prehensile Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiangran Lyu",
      "Ziming Li",
      "Xuesong Shi",
      "Chaoyi Xu",
      "Yizhou Wang",
      "He Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_POMATO_Marrying_Pointmap_Matching_with_Temporal_Motions_for_Dynamic_3D_ICCV_2025_paper.html": {
    "title": "POMATO: Marrying Pointmap Matching with Temporal Motions for Dynamic 3D Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Songyan Zhang",
      "Yongtao Ge",
      "Jinyuan Tian",
      "Guangkai Xu",
      "Hao Chen",
      "Chen Lv",
      "Chunhua Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/De_Sousa_Ribeiro_Flow_Stochastic_Segmentation_Networks_ICCV_2025_paper.html": {
    "title": "Flow Stochastic Segmentation Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fabio De Sousa Ribeiro",
      "Omar Todd",
      "Charles Jones",
      "Avinash Kori",
      "Raghav Mehta",
      "Ben Glocker"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cho_Learning_Large_Motion_Estimation_from_Intermediate_Representations_with_a_High-Resolution_ICCV_2025_paper.html": {
    "title": "Learning Large Motion Estimation from Intermediate Representations with a High-Resolution Optical Flow Dataset Featuring Long-Range Dynamic Motion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hoonhee Cho",
      "Yuhwan Jeong",
      "Kuk-Jin Yoon"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Albert_Towards_Higher_Effective_Rank_in_Parameter-Efficient_Fine-tuning_using_Khatri-Rao_Product_ICCV_2025_paper.html": {
    "title": "Towards Higher Effective Rank in Parameter-Efficient Fine-tuning using Khatri-Rao Product",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paul Albert",
      "Frederic Z. Zhang",
      "Hemanth Saratchandran",
      "Anton van den Hengel",
      "Ehsan Abbasnejad"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chu_USP_Unified_Self-Supervised_Pretraining_for_Image_Generation_and_Understanding_ICCV_2025_paper.html": {
    "title": "USP: Unified Self-Supervised Pretraining for Image Generation and Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangxiang Chu",
      "Renda Li",
      "Yong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Fan_Go_to_Zero_Towards_Zero-shot_Motion_Generation_with_Million-scale_Data_ICCV_2025_paper.html": {
    "title": "Go to Zero: Towards Zero-shot Motion Generation with Million-scale Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ke Fan",
      "Shunlin Lu",
      "Minyue Dai",
      "Runyi Yu",
      "Lixing Xiao",
      "Zhiyang Dou",
      "Junting Dong",
      "Lizhuang Ma",
      "Jingbo Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jing_Beyond_Brain_Decoding_Visual-Semantic_Reconstructions_to_Mental_Creation_Extension_Based_ICCV_2025_paper.html": {
    "title": "Beyond Brain Decoding: Visual-Semantic Reconstructions to Mental Creation Extension Based on fMRI",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haodong Jing",
      "Dongyao Jiang",
      "Yongqiang Ma",
      "Haibo Hua",
      "Bo Huang",
      "Nanning Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_DAViD_Modeling_Dynamic_Affordance_of_3D_Objects_Using_Pre-trained_Video_ICCV_2025_paper.html": {
    "title": "DAViD: Modeling Dynamic Affordance of 3D Objects Using Pre-trained Video Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyeonwoo Kim",
      "Sangwon Baik",
      "Hanbyul Joo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lu_SKALD_Learning-Based_Shot_Assembly_for_Coherent_Multi-Shot_Video_Creation_ICCV_2025_paper.html": {
    "title": "SKALD: Learning-Based Shot Assembly for Coherent Multi-Shot Video Creation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen-Yi Lu",
      "Md Mehrab Tanjim",
      "Ishita Dasgupta",
      "Somdeb Sarkhel",
      "Gang Wu",
      "Saayan Mitra",
      "Somali Chaterji"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_When_Confidence_Fails_Revisiting_Pseudo-Label_Selection_in_Semi-supervised_Semantic_Segmentation_ICCV_2025_paper.html": {
    "title": "When Confidence Fails: Revisiting Pseudo-Label Selection in Semi-supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pan Liu",
      "Jinshi Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Preacher_Paper-to-Video_Agentic_System_ICCV_2025_paper.html": {
    "title": "Preacher: Paper-to-Video Agentic System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingwei Liu",
      "Ling Yang",
      "Hao Luo",
      "Fan Wang",
      "Hongyan Li",
      "Mengdi Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tran_More_Reliable_Pseudo-labels_Better_Performance_A_Generalized_Approach_to_Single_ICCV_2025_paper.html": {
    "title": "More Reliable Pseudo-labels, Better Performance: A Generalized Approach to Single Positive Multi-label Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luong Tran",
      "Thieu Vo",
      "Anh Nguyen",
      "Sang Dinh",
      "Van Nguyen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Purkrabek_Detection_Pose_Estimation_and_Segmentation_for_Multiple_Bodies_Closing_the_ICCV_2025_paper.html": {
    "title": "Detection, Pose Estimation and Segmentation for Multiple Bodies: Closing the Virtuous Circle",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miroslav Purkrabek",
      "Jiri Matas"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Luo_CalliReader_Contextualizing_Chinese_Calligraphy_via_an_Embedding-Aligned_Vision-Language_Model_ICCV_2025_paper.html": {
    "title": "CalliReader: Contextualizing Chinese Calligraphy via an Embedding-Aligned Vision-Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Luo",
      "Jiaqi Tang",
      "Chenyi Huang",
      "Feiyang Hao",
      "Zhouhui Lian"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Polaczek_NeuralSVG_An_Implicit_Representation_for_Text-to-Vector_Generation_ICCV_2025_paper.html": {
    "title": "NeuralSVG: An Implicit Representation for Text-to-Vector Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sagi Polaczek",
      "Yuval Alaluf",
      "Elad Richardson",
      "Yael Vinker",
      "Daniel Cohen-Or"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zheng_Hierarchical_Cross-modal_Prompt_Learning_for_Vision-Language_Models_ICCV_2025_paper.html": {
    "title": "Hierarchical Cross-modal Prompt Learning for Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Zheng",
      "Shunzhi Yang",
      "Zhuoxin He",
      "Jinfeng Yang",
      "Zhenhua Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kunzel_RIPE_Reinforcement_Learning_on_Unlabeled_Image_Pairs_for_Robust_Keypoint_ICCV_2025_paper.html": {
    "title": "RIPE: Reinforcement Learning on Unlabeled Image Pairs for Robust Keypoint Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Johannes Künzel",
      "Anna Hilsmann",
      "Peter Eisert"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Dong_Can_We_Achieve_Efficient_Diffusion_Without_Self-Attention_Distilling_Self-Attention_into_ICCV_2025_paper.html": {
    "title": "Can We Achieve Efficient Diffusion Without Self-Attention? Distilling Self-Attention into Convolutions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyi Dong",
      "Chengxing Zhou",
      "Weijian Deng",
      "Pengxu Wei",
      "Xiangyang Ji",
      "Liang Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zongur_Activation_Subspaces_for_Out-of-Distribution_Detection_ICCV_2025_paper.html": {
    "title": "Activation Subspaces for Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Barış Zöngür",
      "Robin Hesse",
      "Stefan Roth"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_SIMS_Simulating_Stylized_Human-Scene_Interactions_with_Retrieval-Augmented_Script_Generation_ICCV_2025_paper.html": {
    "title": "SIMS: Simulating Stylized Human-Scene Interactions with Retrieval-Augmented Script Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenjia Wang",
      "Liang Pan",
      "Zhiyang Dou",
      "Jidong Mei",
      "Zhouyingcheng Liao",
      "Yuke Lou",
      "Yifan Wu",
      "Lei Yang",
      "Jingbo Wang",
      "Taku Komura"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Girella_LOTS_of_Fashion_Multi-Conditioning_for_Image_Generation_via_Sketch-Text_Pairing_ICCV_2025_paper.html": {
    "title": "LOTS of Fashion! Multi-Conditioning for Image Generation via Sketch-Text Pairing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Federico Girella",
      "Davide Talon",
      "Ziyue Liu",
      "Zanxi Ruan",
      "Yiming Wang",
      "Marco Cristani"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tian_DuoCLR_Dual-Surrogate_Contrastive_Learning_for_Skeleton-based_Human_Action_Segmentation_ICCV_2025_paper.html": {
    "title": "DuoCLR: Dual-Surrogate Contrastive Learning for Skeleton-based Human Action Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haitao Tian"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chi_Plug-in_Feedback_Self-adaptive_Attention_in_CLIP_for_Training-free_Open-Vocabulary_Segmentation_ICCV_2025_paper.html": {
    "title": "Plug-in Feedback Self-adaptive Attention in CLIP for Training-free Open-Vocabulary Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhixiang Chi",
      "Yanan Wu",
      "Li Gu",
      "Huan Liu",
      "Ziqiang Wang",
      "Yang Zhang",
      "Yang Wang",
      "Konstantinos Plataniotis"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Street_Gaussians_without_3D_Object_Tracker_ICCV_2025_paper.html": {
    "title": "Street Gaussians without 3D Object Tracker",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruida Zhang",
      "Chengxi Li",
      "Chenyangguang Zhang",
      "Xingyu Liu",
      "Haili Yuan",
      "Yanyan Li",
      "Xiangyang Ji",
      "Gim Hee Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cao_AllGCD_Leveraging_All_Unlabeled_Data_for_Generalized_Category_Discovery_ICCV_2025_paper.html": {
    "title": "AllGCD: Leveraging All Unlabeled Data for Generalized Category Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinzi Cao",
      "Ke Chen",
      "Feidiao Yang",
      "Xiawu Zheng",
      "Yonghong Tian",
      "Yutong Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hao_Principles_of_Visual_Tokens_for_Efficient_Video_Understanding_ICCV_2025_paper.html": {
    "title": "Principles of Visual Tokens for Efficient Video Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyue Hao",
      "Gen Li",
      "Shreyank N Gowda",
      "Robert B. Fisher",
      "Jonathan Huang",
      "Anurag Arnab",
      "Laura Sevilla-Lara"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/He_Boosting_Domain_Generalized_and_Adaptive_Detection_with_Diffusion_Models_Fitness_ICCV_2025_paper.html": {
    "title": "Boosting Domain Generalized and Adaptive Detection with Diffusion Models: Fitness, Generalization, and Transferability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boyong He",
      "Yuxiang Ji",
      "Zhuoyue Tan",
      "Liaoni Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lee_Power_of_Cooperative_Supervision_Multiple_Teachers_Framework_for_Advanced_3D_ICCV_2025_paper.html": {
    "title": "Power of Cooperative Supervision: Multiple Teachers Framework for Advanced 3D Semi-Supervised Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jin-Hee Lee",
      "Jae-Keun Lee",
      "Jeseok Kim",
      "Kwon Soon"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Qin_SAM_Encoder_Breach_by_Adversarial_Simplicial_Complex_Triggers_Downstream_Model_ICCV_2025_paper.html": {
    "title": "SAM Encoder Breach by Adversarial Simplicial Complex Triggers Downstream Model Failures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Qin",
      "Rui Wang",
      "Tao Huang",
      "Tong Xiao",
      "Liping Jing"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Islam_ReassembleNet_Learnable_Keypoints_and_Diffusion_for_2D_Fresco_Reconstruction_ICCV_2025_paper.html": {
    "title": "ReassembleNet: Learnable Keypoints and Diffusion for 2D Fresco Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adeela Islam",
      "Stefano Fiorini",
      "Stuart James",
      "Pietro Morerio",
      "Alessio Del Bue"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tang_Similarity_Memory_Prior_is_All_You_Need_for_Medical_Image_ICCV_2025_paper.html": {
    "title": "Similarity Memory Prior is All You Need for Medical Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Tang",
      "Zhiqing Guo",
      "Liejun Wang",
      "Chao Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ji_MMGeo_Multimodal_Compositional_Geo-Localization_for_UAVs_ICCV_2025_paper.html": {
    "title": "MMGeo: Multimodal Compositional Geo-Localization for UAVs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxiang Ji",
      "Boyong He",
      "Zhuoyue Tan",
      "Liaoni Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lu_Tracing_Copied_Pixels_and_Regularizing_Patch_Affinity_in_Copy_Detection_ICCV_2025_paper.html": {
    "title": "Tracing Copied Pixels and Regularizing Patch Affinity in Copy Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yichen Lu",
      "Siwei Nie",
      "Minlong Lu",
      "Xudong Yang",
      "Xiaobo Zhang",
      "Peng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wei_InstructSeg_Unifying_Instructed_Visual_Segmentation_with_Multi-modal_Large_Language_Models_ICCV_2025_paper.html": {
    "title": "InstructSeg: Unifying Instructed Visual Segmentation with Multi-modal Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cong Wei",
      "Yujie Zhong",
      "Haoxian Tan",
      "Yingsen Zeng",
      "Yong Liu",
      "Hongfa Wang",
      "Yujiu Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tedla_Multispectral_Demosaicing_via_Dual_Cameras_ICCV_2025_paper.html": {
    "title": "Multispectral Demosaicing via Dual Cameras",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "SaiKiran Tedla",
      "Junyong Lee",
      "Beixuan Yang",
      "Mahmoud Afifi",
      "Michael S. Brown"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Reflect-DiT_Inference-Time_Scaling_for_Text-to-Image_Diffusion_Transformers_via_In-Context_Reflection_ICCV_2025_paper.html": {
    "title": "Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion Transformers via In-Context Reflection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shufan Li",
      "Konstantinos Kallidromitis",
      "Akash Gokul",
      "Arsh Koneru",
      "Yusuke Kato",
      "Kazuki Kozuka",
      "Aditya Grover"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Su_HAMoBE_Hierarchical_and_Adaptive_Mixture_of_Biometric_Experts_for_Video-based_ICCV_2025_paper.html": {
    "title": "HAMoBE: Hierarchical and Adaptive Mixture of Biometric Experts for Video-based Person ReID",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiyang Su",
      "Yunping Shi",
      "Feng Liu",
      "Xiaoming Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_TopicGeo_An_Efficient_Unified_Framework_for_Geolocation_ICCV_2025_paper.html": {
    "title": "TopicGeo: An Efficient Unified Framework for Geolocation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Wang",
      "Xinlin Wang",
      "Shuiping Gou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/He_Neighboring_Autoregressive_Modeling_for_Efficient_Visual_Generation_ICCV_2025_paper.html": {
    "title": "Neighboring Autoregressive Modeling for Efficient Visual Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yefei He",
      "Yuanyu He",
      "Shaoxuan He",
      "Feng Chen",
      "Hong Zhou",
      "Kaipeng Zhang",
      "Bohan Zhuang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Thawakar_Beyond_Simple_Edits_Composed_Video_Retrieval_with_Dense_Modifications_ICCV_2025_paper.html": {
    "title": "Beyond Simple Edits: Composed Video Retrieval with Dense Modifications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Omkar Thawakar",
      "Dmitry Demidov",
      "Ritesh Thawkar",
      "Rao Muhammad Anwer",
      "Mubarak Shah",
      "Fahad Shahbaz Khan",
      "Salman Khan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_MagicColor_Multi-Instance_Sketch_Colorization_ICCV_2025_paper.html": {
    "title": "MagicColor: Multi-Instance Sketch Colorization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinhan Zhang",
      "Yue Ma",
      "Bingyuan Wang",
      "Qifeng Chen",
      "Zeyu Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Burges_Active_Learning_Meets_Foundation_Models_Fast_Remote_Sensing_Data_Annotation_ICCV_2025_paper.html": {
    "title": "Active Learning Meets Foundation Models: Fast Remote Sensing Data Annotation for Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marvin Burges",
      "Philipe Ambrozio Dias",
      "Carson Woody",
      "Sarah Walters",
      "Dalton Lunga"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sklyarova_Im2Haircut_Single-view_Strand-based_Hair_Reconstruction_for_Human_Avatars_ICCV_2025_paper.html": {
    "title": "Im2Haircut: Single-view Strand-based Hair Reconstruction for Human Avatars",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vanessa Sklyarova",
      "Egor Zakharov",
      "Malte Prinzler",
      "Giorgio Becherini",
      "Michael J. Black",
      "Justus Thies"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_Leveraging_Prior_Knowledge_of_Diffusion_Model_for_Person_Search_ICCV_2025_paper.html": {
    "title": "Leveraging Prior Knowledge of Diffusion Model for Person Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giyeol Kim",
      "Sooyoung Yang",
      "Jihyong Oh",
      "Myungjoo Kang",
      "Chanho Eom"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Raza_PS3_A_Multimodal_Transformer_Integrating_Pathology_Reports_with_Histology_Images_ICCV_2025_paper.html": {
    "title": "PS3: A Multimodal Transformer Integrating Pathology Reports with Histology Images and Biological Pathways for Cancer Survival Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manahil Raza",
      "Ayesha Azam",
      "Talha Qaiser",
      "Nasir Rajpoot"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Debiasing_Trace_Guidance_Top-down_Trace_Distillation_and_Bottom-up_Velocity_Alignment_ICCV_2025_paper.html": {
    "title": "Debiasing Trace Guidance: Top-down Trace Distillation and Bottom-up Velocity Alignment for Unsupervised Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingjian Wang",
      "Li Chai",
      "Jiming Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_EDM_Efficient_Deep_Feature_Matching_ICCV_2025_paper.html": {
    "title": "EDM: Efficient Deep Feature Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xi Li",
      "Tong Rao",
      "Cihui Pan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Mahapatra_Progressive_Growing_of_Video_Tokenizers_for_Temporally_Compact_Latent_Spaces_ICCV_2025_paper.html": {
    "title": "Progressive Growing of Video Tokenizers for Temporally Compact Latent Spaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aniruddha Mahapatra",
      "Long Mai",
      "David Bourgin",
      "Yitian Zhang",
      "Feng Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Stronger_Steadier__Superior_Geometric_Consistency_in_Depth_VFM_Forges_ICCV_2025_paper.html": {
    "title": "Stronger, Steadier & Superior: Geometric Consistency in Depth VFM Forges Domain Generalized Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyu Chen",
      "Ting Han",
      "Changshe Zhang",
      "Xin Luo",
      "Meiliu Wu",
      "Guorong Cai",
      "Jinhe Su"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_CorrCLIP_Reconstructing_Patch_Correlations_in_CLIP_for_Open-Vocabulary_Semantic_Segmentation_ICCV_2025_paper.html": {
    "title": "CorrCLIP: Reconstructing Patch Correlations in CLIP for Open-Vocabulary Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dengke Zhang",
      "Fagui Liu",
      "Quan Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chadebec_LBM_Latent_Bridge_Matching_for_Fast_Image-to-Image_Translation_ICCV_2025_paper.html": {
    "title": "LBM: Latent Bridge Matching for Fast Image-to-Image Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Clément Chadebec",
      "Onur Tasar",
      "Sanjeev Sreetharan",
      "Benjamin Aubin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_DreamRenderer_Taming_Multi-Instance_Attribute_Control_in_Large-Scale_Text-to-Image_Models_ICCV_2025_paper.html": {
    "title": "DreamRenderer: Taming Multi-Instance Attribute Control in Large-Scale Text-to-Image Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dewei Zhou",
      "Mingwei Li",
      "Zongxin Yang",
      "Yi Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Pang_Towards_a_3D_Transfer-based_Black-box_Attack_via_Critical_Feature_Guidance_ICCV_2025_paper.html": {
    "title": "Towards a 3D Transfer-based Black-box Attack via Critical Feature Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuchao Pang",
      "Zhenghan Chen",
      "Shen Zhang",
      "Liming Lu",
      "Siyuan Liang",
      "Anan Du",
      "Yongbin Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_TurboVSR_Fantastic_Video_Upscalers_and_Where_to_Find_Them_ICCV_2025_paper.html": {
    "title": "TurboVSR: Fantastic Video Upscalers and Where to Find Them",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongdao Wang",
      "Guodongfang Zhao",
      "Jingjing Ren",
      "Bailan Feng",
      "Shifeng Zhang",
      "Wenbo Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liao_LLM-Assisted_Semantic_Guidance_for_Sparsely_Annotated_Remote_Sensing_Object_Detection_ICCV_2025_paper.html": {
    "title": "LLM-Assisted Semantic Guidance for Sparsely Annotated Remote Sensing Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Liao",
      "Chunyan Xu",
      "Chenxu Wang",
      "Zhen Cui"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Park_NuPlanQA_A_Large-Scale_Dataset_and_Benchmark_for_Multi-View_Driving_Scene_ICCV_2025_paper.html": {
    "title": "NuPlanQA: A Large-Scale Dataset and Benchmark for Multi-View Driving Scene Understanding in Multi-Modal Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sung-Yeon Park",
      "Can Cui",
      "Yunsheng Ma",
      "Ahmadreza Moradipari",
      "Rohit Gupta",
      "Kyungtae Han",
      "Ziran Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xiu_Geometric_Alignment_and_Prior_Modulation_for_View-Guided_Point_Cloud_Completion_ICCV_2025_paper.html": {
    "title": "Geometric Alignment and Prior Modulation for View-Guided Point Cloud Completion on Unseen Categories",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingqiao Xiu",
      "Yicong Li",
      "Na Zhao",
      "Han Fang",
      "Xiang Wang",
      "Angela Yao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chattopadhyay_ODDR_Outlier_Detection__Dimension_Reduction_Based_Defense_Against_Adversarial_ICCV_2025_paper.html": {
    "title": "ODDR: Outlier Detection & Dimension Reduction Based Defense Against Adversarial Patches",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nandish Chattopadhyay",
      "Amira Guesmi",
      "Muhammad Abdullah Hanif",
      "Bassem Ouni",
      "Muhammad Shafique"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_Mind_the_Gap_Preserving_and_Compensating_for_the_Modality_Gap_ICCV_2025_paper.html": {
    "title": "Mind the Gap: Preserving and Compensating for the Modality Gap in CLIP-Based Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linlan Huang",
      "Xusheng Cao",
      "Haori Lu",
      "Yifan Meng",
      "Fei Yang",
      "Xialei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Stochastic_Gradient_Estimation_for_Higher-Order_Differentiable_Rendering_ICCV_2025_paper.html": {
    "title": "Stochastic Gradient Estimation for Higher-Order Differentiable Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zican Wang",
      "Michael Fischer",
      "Tobias Ritschel"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Han_PolGS_Polarimetric_Gaussian_Splatting_for_Fast_Reflective_Surface_Reconstruction_ICCV_2025_paper.html": {
    "title": "PolGS: Polarimetric Gaussian Splatting for Fast Reflective Surface Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufei Han",
      "Bowen Tie",
      "Heng Guo",
      "Youwei Lyu",
      "Si Li",
      "Boxin Shi",
      "Yunpeng Jia",
      "Zhanyu Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ruan_AdvDreamer_Unveils_Are_Vision-Language_Models_Truly_Ready_for_Real-World_3D_ICCV_2025_paper.html": {
    "title": "AdvDreamer Unveils: Are Vision-Language Models Truly Ready for Real-World 3D Variations?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shouwei Ruan",
      "Hanqing Liu",
      "Yao Huang",
      "Xiaoqi Wang",
      "Caixin Kang",
      "Hang Su",
      "Yinpeng Dong",
      "Xingxing Wei"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/He_ERNet_Efficient_Non-Rigid_Registration_Network_for_Point_Sequences_ICCV_2025_paper.html": {
    "title": "ERNet: Efficient Non-Rigid Registration Network for Point Sequences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangzhao He",
      "Yuxi Xiao",
      "Zhen Xu",
      "Xiaowei Zhou",
      "Sida Peng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Qu_Does_Your_Vision-Language_Model_Get_Lost_in_the_Long_Video_ICCV_2025_paper.html": {
    "title": "Does Your Vision-Language Model Get Lost in the Long Video Sampling Dilemma?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyuan Qu",
      "Longxiang Tang",
      "Bohao Peng",
      "Senqiao Yang",
      "Bei Yu",
      "Jiaya Jia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Michalkiewicz_Not_all_Views_are_Created_Equal_Analyzing_Viewpoint_Instabilities_in_ICCV_2025_paper.html": {
    "title": "Not all Views are Created Equal: Analyzing Viewpoint Instabilities in Vision Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mateusz Michalkiewicz",
      "Sheena Bai",
      "Mahsa Baktashmotlagh",
      "Varun Jampani",
      "Guha Balakrishnan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jain_HumorDB_Can_AI_understand_graphical_humor_ICCV_2025_paper.html": {
    "title": "HumorDB: Can AI understand graphical humor?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vedaant V Jain",
      "Gabriel Kreiman",
      "Felipe dos Santos Alves Feitosa"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Karuppasamy_Towards_Safer_and_Understandable_Driver_Intention_Prediction_ICCV_2025_paper.html": {
    "title": "Towards Safer and Understandable Driver Intention Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mukilan Karuppasamy",
      "Shankar Gangisetty",
      "Shyam Nandan Rai",
      "Carlo Masone",
      "C V Jawahar"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Schwarz_Generative_Gaussian_Splatting_Generating_3D_Scenes_with_Video_Diffusion_Priors_ICCV_2025_paper.html": {
    "title": "Generative Gaussian Splatting: Generating 3D Scenes with Video Diffusion Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Katja Schwarz",
      "Norman Müller",
      "Peter Kontschieder"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wei_AffordDexGrasp_Open-set_Language-guided_Dexterous_Grasp_with_Generalizable-Instructive_Affordance_ICCV_2025_paper.html": {
    "title": "AffordDexGrasp: Open-set Language-guided Dexterous Grasp with Generalizable-Instructive Affordance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi-Lin Wei",
      "Mu Lin",
      "Yuhao Lin",
      "Jian-Jian Jiang",
      "Xiao-Ming Wu",
      "Ling-An Zeng",
      "Wei-Shi Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_AR-1-to-3_Single_Image_to_Consistent_3D_Object_via_Next-View_Prediction_ICCV_2025_paper.html": {
    "title": "AR-1-to-3: Single Image to Consistent 3D Object via Next-View Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuying Zhang",
      "Yupeng Zhou",
      "Kai Wang",
      "Yikai Wang",
      "Zhen Li",
      "Shaohui Jiao",
      "Daquan Zhou",
      "Qibin Hou",
      "Ming-Ming Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ichbiah_Inverse_3D_Microscopy_Rendering_for_Cell_Shape_Inference_with_Active_ICCV_2025_paper.html": {
    "title": "Inverse 3D Microscopy Rendering for Cell Shape Inference with Active Mesh",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sacha Ichbiah",
      "Anshuman Sinha",
      "Fabrice Delbary",
      "Hervé Turlier"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xu_NeurOp-Diff_Continuous_Remote_Sensing_Image_Super-Resolution_via_Neural_Operator_Diffusion_ICCV_2025_paper.html": {
    "title": "NeurOp-Diff: Continuous Remote Sensing Image Super-Resolution via Neural Operator Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihao Xu",
      "Yuzhi Tang",
      "Bowen Xu",
      "Qingquan Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_DC-TTA_Divide-and-Conquer_Framework_for_Test-Time_Adaptation_of_Interactive_Segmentation_ICCV_2025_paper.html": {
    "title": "DC-TTA: Divide-and-Conquer Framework for Test-Time Adaptation of Interactive Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jihun Kim",
      "Hoyong Kwon",
      "Hyeokjun  Kweon",
      "Wooseong Jeong",
      "Kuk-Jin Yoon"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Masum_PROL__Rehearsal_Free_Continual_Learning_in_Streaming_Data_via_ICCV_2025_paper.html": {
    "title": "PROL : Rehearsal Free Continual Learning in Streaming Data via Prompt Online Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "M. Anwar Ma'sum",
      "Mahardhika Pratama",
      "Savitha Ramasamy",
      "Lin Liu",
      "Habibullah Habibullah",
      "Ryszard Kowalczyk"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Generalization-Preserved_Learning_Closing_the_Backdoor_to_Catastrophic_Forgetting_in_Continual_ICCV_2025_paper.html": {
    "title": "Generalization-Preserved Learning: Closing the Backdoor to Catastrophic Forgetting in Continual Deepfake Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xueyi Zhang",
      "Peiyin Zhu",
      "Chengwei Zhang",
      "Zhiyuan Yan",
      "Jikang Cheng",
      "Mingrui Lao",
      "Siqi Cai",
      "Yanming Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jin_JailbreakDiffBench_A_Comprehensive_Benchmark_for_Jailbreaking_Diffusion_Models_ICCV_2025_paper.html": {
    "title": "JailbreakDiffBench: A Comprehensive Benchmark for Jailbreaking Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaolong Jin",
      "Zixuan  Weng",
      "Hanxi Guo",
      "Chenlong Yin",
      "Siyuan Cheng",
      "Guangyu Shen",
      "Xiangyu Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jiang_InfiniteYou_Flexible_Photo_Recrafting_While_Preserving_Your_Identity_ICCV_2025_paper.html": {
    "title": "InfiniteYou: Flexible Photo Recrafting While Preserving Your Identity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liming Jiang",
      "Qing Yan",
      "Yumin Jia",
      "Zichuan Liu",
      "Hao Kang",
      "Xin Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_GeoMan_Temporally_Consistent_Human_Geometry_Estimation_using_Image-to-Video_Diffusion_ICCV_2025_paper.html": {
    "title": "GeoMan: Temporally Consistent Human Geometry Estimation using Image-to-Video Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gwanghyun Kim",
      "Xueting Li",
      "Ye Yuan",
      "Koki Nagano",
      "Tianye Li",
      "Jan Kautz",
      "Se Young Chun",
      "Umar Iqbal"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cai_NAVER_A_Neuro-Symbolic_Compositional_Automaton_for_Visual_Grounding_with_Explicit_ICCV_2025_paper.html": {
    "title": "NAVER: A Neuro-Symbolic Compositional Automaton for Visual Grounding with Explicit Logic Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhixi Cai",
      "Fucai Ke",
      "Simindokht Jahangard",
      "Maria Garcia de la Banda",
      "Reza Haffari",
      "Peter J. Stuckey",
      "Hamid Rezatofighi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jun_Generative_Adversarial_Diffusion_ICCV_2025_paper.html": {
    "title": "Generative Adversarial Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "U-Chae Jun",
      "Jaeeun Ko",
      "Jiwoo Kang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Bond_GaussianVideo_Efficient_Video_Representation_via_Hierarchical_Gaussian_Splatting_ICCV_2025_paper.html": {
    "title": "GaussianVideo: Efficient Video Representation via Hierarchical Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrew Bond",
      "Jui-Hsien Wang",
      "Long Mai",
      "Erkut Erdem",
      "Aykut Erdem"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xu_MergeOcc_Bridge_the_Domain_Gap_between_Different_LiDARs_for_Robust_ICCV_2025_paper.html": {
    "title": "MergeOcc: Bridge the Domain Gap between Different LiDARs for Robust Occupancy Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zikun Xu",
      "Shaobing Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_AutoScape_Geometry-Consistent_Long-Horizon_Scene_Generation_ICCV_2025_paper.html": {
    "title": "AutoScape: Geometry-Consistent Long-Horizon Scene Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiacheng Chen",
      "Ziyu Jiang",
      "Mingfu Liang",
      "Bingbing Zhuang",
      "Jong-Chyi Su",
      "Sparsh Garg",
      "Ying Wu",
      "Manmohan Chandraker"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Few-Shot_Image_Quality_Assessment_via_Adaptation_of_Vision-Language_Models_ICCV_2025_paper.html": {
    "title": "Few-Shot Image Quality Assessment via Adaptation of Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xudong Li",
      "Zihao Huang",
      "Yan Zhang",
      "Yunhang Shen",
      "Ke Li",
      "Xiawu Zheng",
      "Liujuan Cao",
      "Rongrong Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Simsar_UIP2P_Unsupervised_Instruction-based_Image_Editing_via_Edit_Reversibility_Constraint_ICCV_2025_paper.html": {
    "title": "UIP2P: Unsupervised Instruction-based Image Editing via Edit Reversibility Constraint",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Enis Simsar",
      "Alessio Tonioni",
      "Yongqin Xian",
      "Thomas Hofmann",
      "Federico Tombari"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_Beyond_Spatial_Frequency_Pixel-wise_Temporal_Frequency-based_Deepfake_Video_Detection_ICCV_2025_paper.html": {
    "title": "Beyond Spatial Frequency: Pixel-wise Temporal Frequency-based Deepfake Video Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taehoon Kim",
      "Jongwook Choi",
      "Yonghyun Jeong",
      "Haeun Noh",
      "Jaejun Yoo",
      "Seungryul Baek",
      "Jongwon Choi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jeong_Synchronizing_Task_Behavior_Aligning_Multiple_Tasks_during_Test-Time_Training_ICCV_2025_paper.html": {
    "title": "Synchronizing Task Behavior: Aligning Multiple Tasks during Test-Time Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wooseong Jeong",
      "Jegyeong Cho",
      "Youngho Yoon",
      "Kuk-Jin Yoon"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Benigmim_FLOSS_Free_Lunch_in_Open-vocabulary_Semantic_Segmentation_ICCV_2025_paper.html": {
    "title": "FLOSS: Free Lunch in Open-vocabulary Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yasser Benigmim",
      "Mohammad Fahes",
      "Tuan-Hung Vu",
      "Andrei Bursuc",
      "Raoul de Charette"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_p-MoD_Building_Mixture-of-Depths_MLLMs_via_Progressive_Ratio_Decay_ICCV_2025_paper.html": {
    "title": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Zhang",
      "Desen Meng",
      "Zhengming Zhang",
      "Zhenpeng Huang",
      "Tao Wu",
      "Limin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_SAME_Learning_Generic_Language-Guided_Visual_Navigation_with_State-Adaptive_Mixture_of_ICCV_2025_paper.html": {
    "title": "SAME: Learning Generic Language-Guided Visual Navigation with State-Adaptive Mixture of Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gengze Zhou",
      "Yicong Hong",
      "Zun Wang",
      "Chongyang Zhao",
      "Mohit Bansal",
      "Qi Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Conti_ToF-Splatting_Dense_SLAM_using_Sparse_Time-of-Flight_Depth_and_Multi-Frame_Integration_ICCV_2025_paper.html": {
    "title": "ToF-Splatting: Dense SLAM using Sparse Time-of-Flight Depth and Multi-Frame Integration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrea Conti",
      "Matteo Poggi",
      "Valerio Cambareri",
      "Martin R. Oswald",
      "Stefano Mattoccia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jin_PASD_A_Pixel-Adaptive_Swarm_Dynamics_Approach_for_Unsupervised_Low-Light_Image_ICCV_2025_paper.html": {
    "title": "PASD: A Pixel-Adaptive Swarm Dynamics Approach for Unsupervised Low-Light Image Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuai Jin",
      "Yuhua Qian",
      "Feijiang Li",
      "Guoqing Liu",
      "Xinyan Liang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Das_ConsNoTrainLoRA_Data-driven_Weight_Initialization_of_Low-rank_Adapters_using_Constraints_ICCV_2025_paper.html": {
    "title": "ConsNoTrainLoRA: Data-driven Weight Initialization of Low-rank Adapters using Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Debasmit Das",
      "Hyoungwoo  Park",
      "Munawar Hayat",
      "Seokeon Choi",
      "Sungrack Yun",
      "Fatih Porikli"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sarker_Correspondence-Free_Fast_and_Robust_Spherical_Point_Pattern_Registration_ICCV_2025_paper.html": {
    "title": "Correspondence-Free Fast and Robust Spherical Point Pattern Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anik Sarker",
      "Alan T. Asbeck"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cagatan_Adversarial_Robustness_of_Discriminative_Self-Supervised_Learning_in_Vision_ICCV_2025_paper.html": {
    "title": "Adversarial Robustness of Discriminative Self-Supervised Learning in Vision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ömer Veysel Çağatan",
      "Ömer Faruk Tal",
      "M. Emre Gursoy"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Pham_CT-ScanGaze_A_Dataset_and_Baselines_for_3D_Volumetric_Scanpath_Modeling_ICCV_2025_paper.html": {
    "title": "CT-ScanGaze: A Dataset and Baselines for 3D Volumetric Scanpath Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Trong Thang Pham",
      "Akash Awasthi",
      "Saba Khan",
      "Esteban Duran Marti",
      "Tien-Phat Nguyen",
      "Khoa Vo",
      "Minh Tran",
      "Son Nguyen",
      "Cuong Tran",
      "Yuki Ikebe",
      "Anh Totti Nguyen",
      "Anh Nguyen",
      "Zhigang Deng",
      "Carol C. Wu",
      "Hien Nguyen",
      "Ngan Le"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Segu_MOBIUS_Big-to-Mobile_Universal_Instance_Segmentation_via_Multi-modal_Bottleneck_Fusion_and_ICCV_2025_paper.html": {
    "title": "MOBIUS: Big-to-Mobile Universal Instance Segmentation via Multi-modal Bottleneck Fusion and Calibrated Decoder Pruning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mattia Segu",
      "Marta Tintore Gazulla",
      "Yongqin Xian",
      "Luc Van Gool",
      "Federico Tombari"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shi_VoxelKP_A_Voxel-based_Network_Architecture_for_Human_Keypoint_Estimation_in_ICCV_2025_paper.html": {
    "title": "VoxelKP: A Voxel-based Network Architecture for Human Keypoint Estimation in LiDAR Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Shi",
      "Peter Wonka"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gong_TemCoCo_Temporally_Consistent_Multi-modal_Video_Fusion_with_Visual-Semantic_Collaboration_ICCV_2025_paper.html": {
    "title": "TemCoCo: Temporally Consistent Multi-modal Video Fusion with Visual-Semantic Collaboration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meiqi Gong",
      "Hao Zhang",
      "Xunpeng Yi",
      "Linfeng Tang",
      "Jiayi Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Song_Normal_and_Abnormal_Pathology_Knowledge-Augmented_Vision-Language_Model_for_Anomaly_Detection_ICCV_2025_paper.html": {
    "title": "Normal and Abnormal Pathology Knowledge-Augmented Vision-Language Model for Anomaly Detection in Pathology Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinsol Song",
      "Jiamu Wang",
      "Anh Tien Nguyen",
      "Keunho Byeon",
      "Sangjeong Ahn",
      "Sung Hak Lee",
      "Jin Tae Kwak"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_COME_Dual_Structure-Semantic_Learning_with_Collaborative_MoE_for_Universal_Lesion_ICCV_2025_paper.html": {
    "title": "COME: Dual Structure-Semantic Learning with Collaborative MoE for Universal Lesion Detection Across Heterogeneous Ultrasound Datasets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingyu Chen",
      "Yawen Zeng",
      "Yue Wang",
      "Peng Wan",
      "Guochen Ning",
      "Hongen Liao",
      "Daoqiang Zhang",
      "Fang Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chung_Fine-Tuning_Visual_Autogressive_Models_for_Subject-Driven_Generation_ICCV_2025_paper.html": {
    "title": "Fine-Tuning Visual Autogressive Models for Subject-Driven Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiwoo Chung",
      "Sangeek Hyun",
      "Hyunjun Kim",
      "Eunseo Koh",
      "MinKyu Lee",
      "Jae-Pil Heo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tian_Voyaging_into_Perpetual_Dynamic_Scenes_from_a_Single_View_ICCV_2025_paper.html": {
    "title": "Voyaging into Perpetual Dynamic Scenes from a Single View",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fengrui Tian",
      "Tianjiao Ding",
      "Jinqi Luo",
      "Hancheng Min",
      "Rene Vidal"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Qiu_Accelerating_Diffusion_Transformer_via_Gradient-Optimized_Cache_ICCV_2025_paper.html": {
    "title": "Accelerating Diffusion Transformer via Gradient-Optimized Cache",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junxiang Qiu",
      "Lin Liu",
      "Shuo Wang",
      "Jinda Lu",
      "Kezhou Chen",
      "Yanbin Hao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_SceneSplat_Gaussian_Splatting-based_Scene_Understanding_with_Vision-Language_Pretraining_ICCV_2025_paper.html": {
    "title": "SceneSplat: Gaussian Splatting-based Scene Understanding with Vision-Language Pretraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Li",
      "Qi Ma",
      "Runyi Yang",
      "Huapeng Li",
      "Mengjiao Ma",
      "Bin Ren",
      "Nikola Popovic",
      "Nicu Sebe",
      "Ender Konukoglu",
      "Theo Gevers",
      "Luc Van Gool",
      "Martin R. Oswald",
      "Danda Pani Paudel"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kang_Beyond_the_Limits_Overcoming_Negative_Correlation_of_Activation-Based_Training-Free_NAS_ICCV_2025_paper.html": {
    "title": "Beyond the Limits: Overcoming Negative Correlation of Activation-Based Training-Free NAS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haidong Kang",
      "Lianbo Ma",
      "Pengjun Chen",
      "Guo Yu",
      "Xingwei Wang",
      "Min Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ma_Unknown_Text_Learning_for_CLIP-based_Few-Shot_Open-set_Recognition_ICCV_2025_paper.html": {
    "title": "Unknown Text Learning for CLIP-based Few-Shot Open-set Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Ma",
      "Qilong Wang",
      "Bing Cao",
      "Qinghua Hu",
      "Yahong Han"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jiang_VoteSplat_Hough_Voting_Gaussian_Splatting_for_3D_Scene_Understanding_ICCV_2025_paper.html": {
    "title": "VoteSplat: Hough Voting Gaussian Splatting for 3D Scene Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minchao Jiang",
      "Shunyu  Jia",
      "Jiaming Gu",
      "Xiaoyuan Lu",
      "Guangming Zhu",
      "Anqi  Dong",
      "Liang  Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_C4D_4D_Made_from_3D_through_Dual_Correspondences_ICCV_2025_paper.html": {
    "title": "C4D: 4D Made from 3D through Dual Correspondences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shizun Wang",
      "Zhenxiang Jiang",
      "Xingyi Yang",
      "Xinchao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zeng_An_Efficient_Hybrid_Vision_Transformer_for_TinyML_Applications_ICCV_2025_paper.html": {
    "title": "An Efficient Hybrid Vision Transformer for TinyML Applications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fanhong Zeng",
      "Huanan Li",
      "Juntao Guan",
      "Rui Fan",
      "Tong Wu",
      "Xilong Wang",
      "Rui Lai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Object-centric_Video_Question_Answering_with_Visual_Grounding_and_Referring_ICCV_2025_paper.html": {
    "title": "Object-centric Video Question Answering with Visual Grounding and Referring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haochen Wang",
      "Qirui Chen",
      "Cilin Yan",
      "Jiayin Cai",
      "Xiaolong Jiang",
      "Yao Hu",
      "Weidi Xie",
      "Stratis Gavves"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xu_Face_Retouching_with_Diffusion_Data_Generation_and_Spectral_Restorement_ICCV_2025_paper.html": {
    "title": "Face Retouching with Diffusion Data Generation and Spectral Restorement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhidan Xu",
      "Xiaoqin Zhang",
      "Shijian Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_AG2aussian_Anchor-Graph_Structured_Gaussian_Splatting_for_Instance-Level_3D_Scene_Understanding_ICCV_2025_paper.html": {
    "title": "AG2aussian: Anchor-Graph Structured Gaussian Splatting for Instance-Level 3D Scene Understanding and Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaonan Wang",
      "Manyi Li",
      "Changhe Tu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gruszczynski_Beyond_Blur_A_Fluid_Perspective_on_Generative_Diffusion_Models_ICCV_2025_paper.html": {
    "title": "Beyond Blur: A Fluid Perspective on Generative Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Grzegorz Gruszczynski",
      "Jakub Meixner",
      "Michal Wlodarczyk",
      "Przemyslaw Musialski"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Luo_CanonSwap_High-Fidelity_and_Consistent_Video_Face_Swapping_via_Canonical_Space_ICCV_2025_paper.html": {
    "title": "CanonSwap: High-Fidelity and Consistent Video Face Swapping via Canonical Space Modulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangyang Luo",
      "Ye Zhu",
      "Yunfei Liu",
      "Lijian Lin",
      "Cong Wan",
      "Zijian Cai",
      "Yu Li",
      "Shao-Lun Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Haddad_Soft_Local_Completeness_Rethinking_Completeness_in_XAI_ICCV_2025_paper.html": {
    "title": "Soft Local Completeness: Rethinking Completeness in XAI",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziv Weiss Haddad",
      "Oren Barkan",
      "Yehonatan  Elisha",
      "Noam Koenigstein"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xu_Perceiving_and_Acting_in_First-Person_A_Dataset_and_Benchmark_for_ICCV_2025_paper.html": {
    "title": "Perceiving and Acting in First-Person: A Dataset and Benchmark for Egocentric Human-Object-Human Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liang Xu",
      "Chengqun Yang",
      "Zili Lin",
      "Fei Xu",
      "Yifan Liu",
      "Congsheng Xu",
      "Yiyi Zhang",
      "Jie Qin",
      "Xingdong Sheng",
      "Yunhui Liu",
      "Xin Jin",
      "Yichao Yan",
      "Wenjun Zeng",
      "Xiaokang Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kang_Open-ended_Hierarchical_Streaming_Video_Understanding_with_Vision_Language_Models_ICCV_2025_paper.html": {
    "title": "Open-ended Hierarchical Streaming Video Understanding with Vision Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyolim Kang",
      "Yunsu Park",
      "Youngbeom Yoo",
      "Yeeun Choi",
      "Seon Joo Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Bader_SUB_Benchmarking_CBM_Generalization_via_Synthetic_Attribute_Substitutions_ICCV_2025_paper.html": {
    "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jessica Bader",
      "Leander Girrbach",
      "Stephan Alaniz",
      "Zeynep Akata"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jian_G-DexGrasp_Generalizable_Dexterous_Grasping_Synthesis_Via_Part-Aware_Prior_Retrieval_and_ICCV_2025_paper.html": {
    "title": "G-DexGrasp: Generalizable Dexterous Grasping Synthesis Via Part-Aware Prior Retrieval and Prior-Assisted Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juntao Jian",
      "Xiuping Liu",
      "Zixuan Chen",
      "Manyi Li",
      "Jian Liu",
      "Ruizhen Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Autoregressive_Denoising_Score_Matching_is_a_Good_Video_Anomaly_Detector_ICCV_2025_paper.html": {
    "title": "Autoregressive Denoising Score Matching is a Good Video Anomaly Detector",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanwen Zhang",
      "Congqi Cao",
      "Qinyi Lv",
      "Lingtong Min",
      "Yanning Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Marrie_LUDVIG_Learning-Free_Uplifting_of_2D_Visual_Features_to_Gaussian_Splatting_ICCV_2025_paper.html": {
    "title": "LUDVIG: Learning-Free Uplifting of 2D Visual Features to Gaussian Splatting Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juliette Marrie",
      "Romain Menegaux",
      "Michael Arbel",
      "Diane Larlus",
      "Julien Mairal"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Nam_PARTE_Part-Guided_Texturing_for_3D_Human_Reconstruction_from_a_Single_ICCV_2025_paper.html": {
    "title": "PARTE: Part-Guided Texturing for 3D Human Reconstruction from a Single Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyeongjin Nam",
      "Donghwan Kim",
      "Gyeongsik Moon",
      "Kyoung Mu Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lou_LLaVA-SP_Enhancing_Visual_Representation_with_Visual_Spatial_Tokens_for_MLLMs_ICCV_2025_paper.html": {
    "title": "LLaVA-SP: Enhancing Visual Representation with Visual Spatial Tokens for MLLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoran Lou",
      "Chunxiao Fan",
      "Ziyan Liu",
      "Yuexin Wu",
      "Xinliang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_3D-MOOD_Lifting_2D_to_3D_for_Monocular_Open-Set_Object_Detection_ICCV_2025_paper.html": {
    "title": "3D-MOOD: Lifting 2D to 3D for Monocular Open-Set Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yung-Hsu Yang",
      "Luigi Piccinelli",
      "Mattia Segu",
      "Siyuan Li",
      "Rui Huang",
      "Yuqian Fu",
      "Marc Pollefeys",
      "Hermann Blum",
      "Zuria Bauer"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tian_LLM-enhanced_Action-aware_Multi-modal_Prompt_Tuning_for_Image-Text_Matching_ICCV_2025_paper.html": {
    "title": "LLM-enhanced Action-aware Multi-modal Prompt Tuning for Image-Text Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengxiao Tian",
      "Xinxiao Wu",
      "Shuo Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_PointGAC_Geometric-Aware_Codebook_for_Masked_Point_Modeling_ICCV_2025_paper.html": {
    "title": "PointGAC: Geometric-Aware Codebook for Masked Point Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abiao Li",
      "Chenlei Lv",
      "Yuming Fang",
      "Yifan Zuo",
      "Jian Zhang",
      "Guofeng Mei"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Unbiased_Region-Language_Alignment_for_Open-Vocabulary_Dense_Prediction_ICCV_2025_paper.html": {
    "title": "Unbiased Region-Language Alignment for Open-Vocabulary Dense Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunheng Li",
      "Yuxuan Li",
      "Quan-Sheng Zeng",
      "Wenhai Wang",
      "Qibin Hou",
      "Ming-Ming Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sun_CLIPer_Hierarchically_Improving_Spatial_Representation_of_CLIP_for_Open-Vocabulary_Semantic_ICCV_2025_paper.html": {
    "title": "CLIPer: Hierarchically Improving Spatial Representation of CLIP for Open-Vocabulary Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lin Sun",
      "Jiale Cao",
      "Jin Xie",
      "Xiaoheng Jiang",
      "Yanwei Pang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Adversarial_Reconstruction_Feedback_for_Robust_Fine-grained_Generalization_ICCV_2025_paper.html": {
    "title": "Adversarial Reconstruction Feedback for Robust Fine-grained Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shijie Wang",
      "Jian Shi",
      "Haojie Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lim_ConceptSplit_Decoupled_Multi-Concept_Personalization_of_Diffusion_Models_via_Token-wise_Adaptation_ICCV_2025_paper.html": {
    "title": "ConceptSplit: Decoupled Multi-Concept Personalization of Diffusion Models via Token-wise Adaptation and Attention Disentanglement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Habin Lim",
      "Yeongseob Won",
      "Juwon Seo",
      "Gyeong-Moon Park"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/You_Consistency_Trajectory_Matching_for_One-Step_Generative_Super-Resolution_ICCV_2025_paper.html": {
    "title": "Consistency Trajectory Matching for One-Step Generative Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiyi You",
      "Mingyang Zhang",
      "Leheng Zhang",
      "Xingyu Zhou",
      "Kexuan Shi",
      "Shuhang Gu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_SGAD_Semantic_and_Geometric-aware_Descriptor_for_Local_Feature_Matching_ICCV_2025_paper.html": {
    "title": "SGAD: Semantic and Geometric-aware Descriptor for Local Feature Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangzeng Liu",
      "Chi Wang",
      "Guanglu Shi",
      "Xiaodong Zhang",
      "Qiguang Miao",
      "Miao Fan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_SpectralAR_Spectral_Autoregressive_Visual_Generation_ICCV_2025_paper.html": {
    "title": "SpectralAR: Spectral Autoregressive Visual Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanhui Huang",
      "Weiliang Chen",
      "Wenzhao Zheng",
      "Yueqi Duan",
      "Jie Zhou",
      "Jiwen Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kong_RogSplat_Robust_Gaussian_Splatting_via_Generative_Priors_ICCV_2025_paper.html": {
    "title": "RogSplat: Robust Gaussian Splatting via Generative Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanyang Kong",
      "Xingyi Yang",
      "Xinchao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lan_When_Schrodinger_Bridge_Meets_Real-World_Image_Dehazing_with_Unpaired_Training_ICCV_2025_paper.html": {
    "title": "When Schrodinger Bridge Meets Real-World Image Dehazing with Unpaired Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunwei Lan",
      "Zhigao Cui",
      "Xin Luo",
      "Chang Liu",
      "Nian Wang",
      "Menglin Zhang",
      "Yanzhao Su",
      "Dong Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ki_CARIM_Caption-Based_Autonomous_Driving_Scene_Retrieval_via_Inclusive_Text_Matching_ICCV_2025_paper.html": {
    "title": "CARIM: Caption-Based Autonomous Driving Scene Retrieval via Inclusive Text Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minjoo Ki",
      "Daejung Kim",
      "Kisung Kim",
      "Seon Joo Kim",
      "Jinhan Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Abdullah_Punching_Bag_vs._Punching_Person_Motion_Transferability_in_Videos_ICCV_2025_paper.html": {
    "title": "Punching Bag vs. Punching Person: Motion Transferability in Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raiyaan Abdullah",
      "Jared Claypoole",
      "Michael Cogswell",
      "Ajay Divakaran",
      "Yogesh Rawat"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Apolinario_CODE-CL_Conceptor-Based_Gradient_Projection_for_Deep_Continual_Learning_ICCV_2025_paper.html": {
    "title": "CODE-CL: Conceptor-Based Gradient Projection for Deep Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marco P. E. Apolinario",
      "Sakshi Choudhary",
      "Kaushik Roy"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_Boosting_Generative_Adversarial_Transferability_with_Self-supervised_Vision_Transformer_Features_ICCV_2025_paper.html": {
    "title": "Boosting Generative Adversarial Transferability with Self-supervised Vision Transformer Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shangbo Wu",
      "Yu-an Tan",
      "Ruinan Ma",
      "Wencong Ma",
      "Dehua Zhu",
      "Yuanzhang Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jung_Zero-Shot_Compositional_Video_Learning_with_Coding_Rate_Reduction_ICCV_2025_paper.html": {
    "title": "Zero-Shot Compositional Video Learning with Coding Rate Reduction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heeseok Jung",
      "Jun-Hyeon Bak",
      "Yujin Jeong",
      "Gyugeun Lee",
      "Jinwoo Ahn",
      "Eun-Sol Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chowdhury_AVTrustBench_Assessing_and_Enhancing_Reliability_and_Robustness_in_Audio-Visual_LLMs_ICCV_2025_paper.html": {
    "title": "AVTrustBench: Assessing and Enhancing Reliability and Robustness in Audio-Visual LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanjoy Chowdhury",
      "Sayan Nag",
      "Subhrajyoti Dasgupta",
      "Yaoting Wang",
      "Mohamed Elhoseiny",
      "Ruohan Gao",
      "Dinesh Manocha"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_LOTA_Bit-Planes_Guided_AI-Generated_Image_Detection_ICCV_2025_paper.html": {
    "title": "LOTA: Bit-Planes Guided AI-Generated Image Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongsong  Wang",
      "Renxi  Cheng",
      "Yang  Zhang",
      "Chaolei  Han",
      "Jie  Gui"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Krishnan_Benchmarking_Egocentric_Visual-Inertial_SLAM_at_City_Scale_ICCV_2025_paper.html": {
    "title": "Benchmarking Egocentric Visual-Inertial SLAM at City Scale",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anusha Krishnan",
      "Shaohui Liu",
      "Paul-Edouard Sarlin",
      "Oscar Gentilhomme",
      "David Caruso",
      "Maurizio Monge",
      "Richard Newcombe",
      "Jakob Engel",
      "Marc Pollefeys"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Roh_CATSplat_Context-Aware_Transformer_with_Spatial_Guidance_for_Generalizable_3D_Gaussian_ICCV_2025_paper.html": {
    "title": "CATSplat: Context-Aware Transformer with Spatial Guidance for Generalizable 3D Gaussian Splatting from A Single-View Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wonseok Roh",
      "Hwanhee Jung",
      "Jong Wook Kim",
      "Seunggwan Lee",
      "Innfarn Yoo",
      "Andreas Lugmayr",
      "Seunggeun Chi",
      "Karthik Ramani",
      "Sangpil Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Nguyen_Supercharged_One-step_Text-to-Image_Diffusion_Models_with_Negative_Prompts_ICCV_2025_paper.html": {
    "title": "Supercharged One-step Text-to-Image Diffusion Models with Negative Prompts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Viet Nguyen",
      "Anh Nguyen",
      "Trung Dao",
      "Khoi Nguyen",
      "Cuong Pham",
      "Toan Tran",
      "Anh Tran"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Learning_Normals_of_Noisy_Points_by_Local_Gradient-Aware_Surface_Filtering_ICCV_2025_paper.html": {
    "title": "Learning Normals of Noisy Points by Local Gradient-Aware Surface Filtering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qing Li",
      "Huifang Feng",
      "Xun Gong",
      "Yu-Shen Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Karaev_CoTracker3_Simpler_and_Better_Point_Tracking_by_Pseudo-Labelling_Real_Videos_ICCV_2025_paper.html": {
    "title": "CoTracker3: Simpler and Better Point Tracking by Pseudo-Labelling Real Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikita Karaev",
      "Yuri Makarov",
      "Jianyuan Wang",
      "Natalia Neverova",
      "Andrea Vedaldi",
      "Christian Rupprecht"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Guo_IMG_Calibrating_Diffusion_Models_via_Implicit_Multimodal_Guidance_ICCV_2025_paper.html": {
    "title": "IMG: Calibrating Diffusion Models via Implicit Multimodal Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayi Guo",
      "Chuanhao Yan",
      "Xingqian Xu",
      "Yulin Wang",
      "Kai Wang",
      "Gao Huang",
      "Humphrey Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yu_VRBench_A_Benchmark_for_Multi-Step_Reasoning_in_Long_Narrative_Videos_ICCV_2025_paper.html": {
    "title": "VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiashuo Yu",
      "Yue Wu",
      "Meng Chu",
      "Zhifei Ren",
      "Zizheng Huang",
      "Pei Chu",
      "Ruijie Zhang",
      "Yinan He",
      "Qirui Li",
      "Songze Li",
      "Zhenxiang Li",
      "Zhongying Tu",
      "Conghui He",
      "Yu Qiao",
      "Yali Wang",
      "Yi Wang",
      "Limin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Mo_PUMPS_Skeleton-Agnostic_Point-based_Universal_Motion_Pre-Training_for_Synthesis_in_Human_ICCV_2025_paper.html": {
    "title": "PUMPS: Skeleton-Agnostic Point-based Universal Motion Pre-Training for Synthesis in Human Motion Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Clinton Ansun Mo",
      "Kun Hu",
      "Chengjiang Long",
      "Dong Yuan",
      "Wan-Chi Siu",
      "Zhiyong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Styborski_When_and_Where_do_Data_Poisons_Attack_Textual_Inversion_ICCV_2025_paper.html": {
    "title": "When and Where do Data Poisons Attack Textual Inversion?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeremy Styborski",
      "Mingzhi Lyu",
      "Jiayou Lu",
      "Nupur Kapur",
      "Adams Wai-Kin Kong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_RAGD_Regional-Aware_Diffusion_Model_for_Text-to-Image_Generation_ICCV_2025_paper.html": {
    "title": "RAGD: Regional-Aware Diffusion Model for Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhennan Chen",
      "Yajie Li",
      "Haofan Wang",
      "Zhibo Chen",
      "Zhengkai Jiang",
      "Jun Li",
      "Qian Wang",
      "Jian Yang",
      "Ying Tai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hsu_OpenM3D_Open_Vocabulary_Multi-view_Indoor_3D_Object_Detection_without_Human_ICCV_2025_paper.html": {
    "title": "OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peng-Hao Hsu",
      "Ke Zhang",
      "Fu-En Wang",
      "Tao Tu",
      "Ming-Feng Li",
      "Yu-Lun Liu",
      "Albert Y. C. Chen",
      "Min Sun",
      "Cheng-Hao Kuo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lo_From_Prompt_to_Progression_Taming_Video_Diffusion_Models_for_Seamless_ICCV_2025_paper.html": {
    "title": "From Prompt to Progression: Taming Video Diffusion Models for Seamless Attribute Transition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ling Lo",
      "Kelvin C.K. Chan",
      "Wen-Huang Cheng",
      "Ming-Hsuan Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gao_Benchmarking_Multimodal_CoT_Reward_Model_Stepwise_by_Visual_Program_ICCV_2025_paper.html": {
    "title": "Benchmarking Multimodal CoT Reward Model Stepwise by Visual Program",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minghe Gao",
      "Xuqi Liu",
      "Zhongqi Yue",
      "Yang Wu",
      "Shuang Chen",
      "Juncheng Li",
      "Siliang Tang",
      "Fei Wu",
      "Tat-Seng Chua",
      "Yueting Zhuang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Qraitem_Web_Artifact_Attacks_Disrupt_Vision_Language_Models_ICCV_2025_paper.html": {
    "title": "Web Artifact Attacks Disrupt Vision Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maan Qraitem",
      "Piotr Teterwak",
      "Kate Saenko",
      "Bryan A. Plummer"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_GlassWizard_Harvesting_Diffusion_Priors_for_Glass_Surface_Detection_ICCV_2025_paper.html": {
    "title": "GlassWizard: Harvesting Diffusion Priors for Glass Surface Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenxue Li",
      "Tian Ye",
      "Xinyu Xiong",
      "Jinbin Bai",
      "Feilong Tang",
      "Wenxuan Song",
      "Zhaohu Xing",
      "Lie Ju",
      "Guanbin Li",
      "Lei Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sundar_Quanta_Neural_Networks_From_Photons_to_Perception_ICCV_2025_paper.html": {
    "title": "Quanta Neural Networks: From Photons to Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Varun Sundar",
      "Tianyi Zhang",
      "Sacha Jungerman",
      "Mohit Gupta"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xu_Cross-Subject_Mind_Decoding_from_Inaccurate_Representations_ICCV_2025_paper.html": {
    "title": "Cross-Subject Mind Decoding from Inaccurate Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yangyang Xu",
      "Bangzhen Liu",
      "Wenqi Shao",
      "Yong Du",
      "Shengfeng He",
      "Tingting Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tang_How_Can_Objects_Help_Video-Language_Understanding_ICCV_2025_paper.html": {
    "title": "How Can Objects Help Video-Language Understanding?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zitian Tang",
      "Shijie Wang",
      "Junho Cho",
      "Jaewook Yoo",
      "Chen Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liao_ImageGen-CoT_Enhancing_Text-to-Image_In-context_Learning_with_Chain-of-Thought_Reasoning_ICCV_2025_paper.html": {
    "title": "ImageGen-CoT: Enhancing Text-to-Image In-context Learning with Chain-of-Thought Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqi Liao",
      "Zhengyuan Yang",
      "Linjie Li",
      "Dianqi Li",
      "Kevin Lin",
      "Yu Cheng",
      "Lijuan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lee_Auto-Regressive_Transformation_for_Image_Alignment_ICCV_2025_paper.html": {
    "title": "Auto-Regressive Transformation for Image Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kanggeon Lee",
      "Soochahn Lee",
      "Kyoung Mu Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gamrian_Beyond_RGB_Adaptive_Parallel_Processing_for_RAW_Object_Detection_ICCV_2025_paper.html": {
    "title": "Beyond RGB: Adaptive Parallel Processing for RAW Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shani Gamrian",
      "Hila Barel",
      "Feiran Li",
      "Masakazu Yoshimura",
      "Daisuke Iso"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_Stable_Virtual_Camera_Generative_View_Synthesis_with_Diffusion_Models_ICCV_2025_paper.html": {
    "title": "Stable Virtual Camera: Generative View Synthesis with Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jensen Zhou",
      "Hang Gao",
      "Vikram Voleti",
      "Aaryaman Vasishta",
      "Chun-Han Yao",
      "Mark Boss",
      "Philip Torr",
      "Christian Rupprecht",
      "Varun Jampani"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ulmer_Conditional_Latent_Diffusion_Models_for_Zero-Shot_Instance_Segmentation_ICCV_2025_paper.html": {
    "title": "Conditional Latent Diffusion Models for Zero-Shot Instance Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maximilian Ulmer",
      "Wout Boerdijk",
      "Rudolph Triebel",
      "Maximilian Durner"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zheng_Learning_Counterfactually_Decoupled_Attention_for_Open-World_Model_Attribution_ICCV_2025_paper.html": {
    "title": "Learning Counterfactually Decoupled Attention for Open-World Model Attribution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Zheng",
      "Boyang Gong",
      "Fanye Kong",
      "Yueqi Duan",
      "Bingyao Yu",
      "Wenzhao Zheng",
      "Lei Chen",
      "Jiwen Lu",
      "Jie Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhao_DepR_Depth_Guided_Single-view_Scene_Reconstruction_with_Instance-level_Diffusion_ICCV_2025_paper.html": {
    "title": "DepR: Depth Guided Single-view Scene Reconstruction with Instance-level Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingcheng Zhao",
      "Xiang Zhang",
      "Haiyang Xu",
      "Zeyuan Chen",
      "Jianwen Xie",
      "Yuan Gao",
      "Zhuowen Tu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Bai_Edicho_Consistent_Image_Editing_in_the_Wild_ICCV_2025_paper.html": {
    "title": "Edicho: Consistent Image Editing in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingyan Bai",
      "Hao Ouyang",
      "Yinghao Xu",
      "Qiuyu Wang",
      "Ceyuan Yang",
      "Ka Leong Cheng",
      "Yujun Shen",
      "Qifeng Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Oorloff_Stable_Diffusion_Models_are_Secretly_Good_at_Visual_In-Context_Learning_ICCV_2025_paper.html": {
    "title": "Stable Diffusion Models are Secretly Good at Visual In-Context Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Trevine Oorloff",
      "Vishwanath Sindagi",
      "Wele Gedara Chaminda Bandara",
      "Ali Shafahi",
      "Amin Ghiasi",
      "Charan Prakash",
      "Reza Ardekani"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Luo_T2Bs_Text-to-Character_Blendshapes_via_Video_Generation_ICCV_2025_paper.html": {
    "title": "T2Bs: Text-to-Character Blendshapes via Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahao Luo",
      "Chaoyang Wang",
      "Michael Vasilkovsky",
      "Vladislav Shakhrai",
      "Di Liu",
      "Peiye Zhuang",
      "Sergey Tulyakov",
      "Peter Wonka",
      "Hsin-Ying Lee",
      "James Davis",
      "Jian Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_QuEST_Low-bit_Diffusion_Model_Quantization_via_Efficient_Selective_Finetuning_ICCV_2025_paper.html": {
    "title": "QuEST: Low-bit Diffusion Model Quantization via Efficient Selective Finetuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoxuan Wang",
      "Yuzhang Shang",
      "Zhihang Yuan",
      "Junyi Wu",
      "Junchi Yan",
      "Yan Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Task-Specific_Zero-shot_Quantization-Aware_Training_for_Object_Detection_ICCV_2025_paper.html": {
    "title": "Task-Specific Zero-shot Quantization-Aware Training for Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changhao Li",
      "Xinrui Chen",
      "Ji Wang",
      "Kang Zhao",
      "Jianfei Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xie_Shot-by-Shot_Film-Grammar-Aware_Training-Free_Audio_Description_Generation_ICCV_2025_paper.html": {
    "title": "Shot-by-Shot: Film-Grammar-Aware Training-Free Audio Description Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyu Xie",
      "Tengda Han",
      "Max Bain",
      "Arsha Nagrani",
      "Eshika Khandelwal",
      "Gül Varol",
      "Weidi Xie",
      "Andrew Zisserman"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lu_Snakes_and_Ladders_Two_Steps_Up_for_VideoMamba_ICCV_2025_paper.html": {
    "title": "Snakes and Ladders: Two Steps Up for VideoMamba",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hui Lu",
      "Albert A. Salah",
      "Ronald Poppe"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Generalized_Few-Shot_Point_Cloud_Segmentation_via_LLM-Assisted_Hyper-Relation_Matching_ICCV_2025_paper.html": {
    "title": "Generalized Few-Shot Point Cloud Segmentation via LLM-Assisted Hyper-Relation Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoyang Li",
      "Yuan Wang",
      "Guoxin Xiong",
      "Wangkai Li",
      "Yuwen Pan",
      "Tianzhu Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gangopadhyay_Extending_Foundational_Monocular_Depth_Estimators_to_Fisheye_Cameras_with_Calibration_ICCV_2025_paper.html": {
    "title": "Extending Foundational Monocular Depth Estimators to Fisheye Cameras with Calibration Tokens",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suchisrit Gangopadhyay",
      "Jung-Hee Kim",
      "Xien Chen",
      "Patrick Rim",
      "Hyoungseob Park",
      "Alex Wong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Qu_IGD_Instructional_Graphic_Design_with_Multimodal_Layer_Generation_ICCV_2025_paper.html": {
    "title": "IGD: Instructional Graphic Design with Multimodal Layer Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yadong Qu",
      "Shancheng Fang",
      "Yuxin Wang",
      "Xiaorui Wang",
      "Zhineng Chen",
      "Hongtao Xie",
      "Yongdong Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xu_AdaptiveAE_An_Adaptive_Exposure_Strategy_for_HDR_Capturing_in_Dynamic_ICCV_2025_paper.html": {
    "title": "AdaptiveAE: An Adaptive Exposure Strategy for HDR Capturing in Dynamic Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyi Xu",
      "Fan Zhang",
      "Boxin Shi",
      "Tianfan Xue",
      "Yujin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Agrawal_UINavBench_A_Framework_for_Comprehensive_Evaluation_of_Interactive_Digital_Agents_ICCV_2025_paper.html": {
    "title": "UINavBench: A Framework for Comprehensive Evaluation of Interactive Digital Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Harsh Agrawal",
      "Eldon Schoop",
      "Xinlei Pan",
      "Anuj Mahajan",
      "Ari Seff",
      "Di Feng",
      "Ruijia Cheng",
      "Andres Romero Mier Y Teran",
      "Esteban Gomez",
      "Abhishek Sundararajan",
      "Forrest Huang",
      "Amanda Swearngin",
      "Mohana Prasad Sathya Moorthy",
      "Jeff Nichols",
      "Alexander Toshev"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_FedWSQ_Efficient_Federated_Learning_with_Weight_Standardization_and_Distribution-Aware_Non-Uniform_ICCV_2025_paper.html": {
    "title": "FedWSQ: Efficient Federated Learning with Weight Standardization and Distribution-Aware Non-Uniform Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seung-Wook Kim",
      "Seongyeol Kim",
      "Jiah Kim",
      "Seowon Ji",
      "Se-Ho Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gao_Knowledge_Transfer_from_Interaction_Learning_ICCV_2025_paper.html": {
    "title": "Knowledge Transfer from Interaction Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yilin Gao",
      "Kangyi Chen",
      "Zhongxing Peng",
      "Hengjie Lu",
      "Shugong Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ji_Instruction-based_Image_Editing_with_Planning_Reasoning_and_Generation_ICCV_2025_paper.html": {
    "title": "Instruction-based Image Editing with Planning, Reasoning, and Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liya Ji",
      "Chenyang Qi",
      "Qifeng Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cao_Enhancing_Mamba_Decoder_with_Bidirectional_Interaction_in_Multi-Task_Dense_Prediction_ICCV_2025_paper.html": {
    "title": "Enhancing Mamba Decoder with Bidirectional Interaction in Multi-Task Dense Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mang Cao",
      "Sanping Zhou",
      "Yizhe Li",
      "Ye Deng",
      "Wenli Huang",
      "Le Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gao_CityGS-X_A_Scalable_Architecture_for_Efficient_and_Geometrically_Accurate_Large-Scale_ICCV_2025_paper.html": {
    "title": "CityGS-X: A Scalable Architecture for Efficient and Geometrically Accurate Large-Scale Scene Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanyuan Gao",
      "Hao Li",
      "Jiaqi Chen",
      "Zhengyu Zou",
      "Zhihang Zhong",
      "Dingwen Zhang",
      "Xiao Sun",
      "Junwei Han"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zou_Dataset_Distillation_via_Vision-Language_Category_Prototype_ICCV_2025_paper.html": {
    "title": "Dataset Distillation via Vision-Language Category Prototype",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yawen Zou",
      "Guang Li",
      "Duo Su",
      "Zi Wang",
      "Jun Yu",
      "Chao Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Token_Activation_Map_to_Visually_Explain_Multimodal_LLMs_ICCV_2025_paper.html": {
    "title": "Token Activation Map to Visually Explain Multimodal LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Li",
      "Hualiang Wang",
      "Xinpeng Ding",
      "Haonan Wang",
      "Xiaomeng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ji_OcRFDet_Object-Centric_Radiance_Fields_for_Multi-View_3D_Object_Detection_in_ICCV_2025_paper.html": {
    "title": "OcRFDet: Object-Centric Radiance Fields for Multi-View 3D Object Detection in Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingqian Ji",
      "Shanshan Zhang",
      "Jian Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jiang_A_Unified_Framework_to_BRIDGE_Complete_and_Incomplete_Deep_Multi-View_ICCV_2025_paper.html": {
    "title": "A Unified Framework to BRIDGE Complete and Incomplete Deep Multi-View Clustering under Non-IID Missing Patterns",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaorui Jiang",
      "Buyun He",
      "Peng Yuan Zhou",
      "Xinyue Chen",
      "Jingcai Guo",
      "Jie Xu",
      "Yong Liao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_FairHuman_Boosting_Hand_and_Face_Quality_in_Human_Image_Generation_ICCV_2025_paper.html": {
    "title": "FairHuman: Boosting Hand and Face Quality in Human Image Generation with Minimum Potential Delay Fairness in Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Wang",
      "Tianwei Cao",
      "Huayu Zhang",
      "Zhongjiang He",
      "Kongming Liang",
      "Zhanyu Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_CC-OCR_A_Comprehensive_and_Challenging_OCR_Benchmark_for_Evaluating_Large_ICCV_2025_paper.html": {
    "title": "CC-OCR: A Comprehensive and Challenging OCR Benchmark for Evaluating Large Multimodal Models in Literacy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhibo Yang",
      "Jun Tang",
      "Zhaohai Li",
      "Pengfei Wang",
      "Jianqiang Wan",
      "Humen Zhong",
      "Xuejing Liu",
      "Mingkun Yang",
      "Peng Wang",
      "Shuai Bai",
      "Lianwen Jin",
      "Junyang Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Khoba_PEFTDiff_Diffusion-Guided_Transferability_Estimation_for_Parameter-Efficient_Fine-Tuning_ICCV_2025_paper.html": {
    "title": "PEFTDiff: Diffusion-Guided Transferability Estimation for Parameter-Efficient Fine-Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prafful Kumar Khoba",
      "Zijian Wang",
      "Chetan Arora",
      "Mahsa  Baktashmotlagh"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_CODA_Repurposing_Continuous_VAEs_for_Discrete_Tokenization_ICCV_2025_paper.html": {
    "title": "CODA: Repurposing Continuous VAEs for Discrete Tokenization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyu Liu",
      "Zanlin Ni",
      "Yeguo Hua",
      "Xin Deng",
      "Xiao Ma",
      "Cheng Zhong",
      "Gao Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Moon_GeoAvatar_Adaptive_Geometrical_Gaussian_Splatting_for_3D_Head_Avatar_ICCV_2025_paper.html": {
    "title": "GeoAvatar: Adaptive Geometrical Gaussian Splatting for 3D Head Avatar",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "SeungJun Moon",
      "Hah Min Lew",
      "Seungeun Lee",
      "Ji-Su Kang",
      "Gyeong-Moon Park"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jung_Group-wise_Scaling_and_Orthogonal_Decomposition_for_Domain-Invariant_Feature_Extraction_in_ICCV_2025_paper.html": {
    "title": "Group-wise Scaling and Orthogonal Decomposition for Domain-Invariant Feature Extraction in Face Anti-Spoofing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seungjin Jung",
      "Kanghee Lee",
      "Yonghyun Jeong",
      "Haeun Noh",
      "Jungmin Lee",
      "Jongwon Choi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Bang_RCTDistill_Cross-Modal_Knowledge_Distillation_Framework_for_Radar-Camera_3D_Object_Detection_ICCV_2025_paper.html": {
    "title": "RCTDistill: Cross-Modal Knowledge Distillation Framework for Radar-Camera 3D Object Detection with Temporal Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Geonho Bang",
      "Minjae Seong",
      "Jisong Kim",
      "Geunju Baek",
      "Daye Oh",
      "Junhyung Kim",
      "Junho Koh",
      "Jun Won Choi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Towards_Annotation-Free_Evaluation_KPAScore_for_Human_Keypoint_Detection_ICCV_2025_paper.html": {
    "title": "Towards Annotation-Free Evaluation: KPAScore for Human Keypoint Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoxiao Wang",
      "Chunxiao Li",
      "Peng Sun",
      "Boming Miao",
      "Yunjian Zhang",
      "Yao Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ashraf_TITAN_Query-Token_based_Domain_Adaptive_Adversarial_Learning_ICCV_2025_paper.html": {
    "title": "TITAN: Query-Token based Domain Adaptive Adversarial Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tajamul Ashraf",
      "Janibul Bashir"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tirado-Garin_AnyCalib_On-Manifold_Learning_for_Model-Agnostic_Single-View_Camera_Calibration_ICCV_2025_paper.html": {
    "title": "AnyCalib: On-Manifold Learning for Model-Agnostic Single-View Camera Calibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Javier Tirado-Garín",
      "Javier Civera"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_SEGA_A_Stepwise_Evolution_Paradigm_for_Content-Aware_Layout_Generation_with_ICCV_2025_paper.html": {
    "title": "SEGA: A Stepwise Evolution Paradigm for Content-Aware Layout Generation with Design Prior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoran Wang",
      "Bo Zhao",
      "Jinghui Wang",
      "Hanzhang Wang",
      "Huan Yang",
      "Wei Ji",
      "Hao Liu",
      "Xinyan Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Goyal_GEOPARD_Geometric_Pretraining_for_Articulation_Prediction_in_3D_Shapes_ICCV_2025_paper.html": {
    "title": "GEOPARD: Geometric Pretraining for Articulation Prediction in 3D Shapes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pradyumn Goyal",
      "Dmitry Petrov",
      "Sheldon Andrews",
      "Yizhak Ben-Shabat",
      "Hsueh-Ti Derek Liu",
      "Evangelos Kalogerakis"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Fu_ViSpeak_Visual_Instruction_Feedback_in_Streaming_Videos_ICCV_2025_paper.html": {
    "title": "ViSpeak: Visual Instruction Feedback in Streaming Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shenghao Fu",
      "Qize Yang",
      "Yuan-Ming Li",
      "Yi-Xing Peng",
      "Kun-Yu Lin",
      "Xihan Wei",
      "Jian-Fang Hu",
      "Xiaohua Xie",
      "Wei-Shi  Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gao_Feature_Coding_in_the_Era_of_Large_Models_Dataset_Test_ICCV_2025_paper.html": {
    "title": "Feature Coding in the Era of Large Models: Dataset, Test Conditions, and Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changsheng Gao",
      "Yifan Ma",
      "Qiaoxi Chen",
      "Yenan Xu",
      "Dong Liu",
      "Weisi Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_VideoAds_for_Fast-Paced_Video_Understanding_ICCV_2025_paper.html": {
    "title": "VideoAds for Fast-Paced Video Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheyuan Zhang",
      "Wanying Dou",
      "Linkai Peng",
      "Hongyi Pan",
      "Ulas Bagci",
      "Boqing Gong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_MGSR_2D3D_Mutual-boosted_Gaussian_Splatting_for_High-fidelity_Surface_Reconstruction_under_ICCV_2025_paper.html": {
    "title": "MGSR: 2D/3D Mutual-boosted Gaussian Splatting for High-fidelity Surface Reconstruction under Various Light Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingyuan Zhou",
      "Yuehu Gong",
      "Weidong Yang",
      "Jiaze Li",
      "Yeqi Luo",
      "Baixin Xu",
      "Shuhao Li",
      "Ben Fei",
      "Ying He"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cartella_Modeling_Human_Gaze_Behavior_with_Diffusion_Models_for_Unified_Scanpath_ICCV_2025_paper.html": {
    "title": "Modeling Human Gaze Behavior with Diffusion Models for Unified Scanpath Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giuseppe Cartella",
      "Vittorio Cuculo",
      "Alessandro D'Amelio",
      "Marcella Cornia",
      "Giuseppe Boccignone",
      "Rita Cucchiara"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zheng_Efficient_Multi-Person_Motion_Prediction_by_Lightweight_Spatial_and_Temporal_Interactions_ICCV_2025_paper.html": {
    "title": "Efficient Multi-Person Motion Prediction by Lightweight Spatial and Temporal Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanhong Zheng",
      "Ruixuan Yu",
      "Jian Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_I2V3D_Controllable_Image-to-video_Generation_with_3D_Guidance_ICCV_2025_paper.html": {
    "title": "I2V3D: Controllable Image-to-video Generation with 3D Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyuan Zhang",
      "Dongdong Chen",
      "Jing Liao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_TAG-WM_Tamper-Aware_Generative_Image_Watermarking_via_Diffusion_Inversion_Sensitivity_ICCV_2025_paper.html": {
    "title": "TAG-WM: Tamper-Aware Generative Image Watermarking via Diffusion Inversion Sensitivity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuzhuo Chen",
      "Zehua Ma",
      "Han Fang",
      "Weiming Zhang",
      "Nenghai Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xiao_MIEB_Massive_Image_Embedding_Benchmark_ICCV_2025_paper.html": {
    "title": "MIEB: Massive Image Embedding Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenghao Xiao",
      "Isaac Chung",
      "Imene Kerboua",
      "Jamie Stirling",
      "Xin Zhang",
      "Márton Kardos",
      "Roman Solomatin",
      "Noura Al Moubayed",
      "Kenneth Enevoldsen",
      "Niklas Muennighoff"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_IMoRe_Implicit_Program-Guided_Reasoning_for_Human_Motion_QA_ICCV_2025_paper.html": {
    "title": "IMoRe: Implicit Program-Guided Reasoning for Human Motion Q&A",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Li",
      "Chinthani Sugandhika",
      "Yeo Keat Ee",
      "Eric Peh",
      "Hao Zhang",
      "Hong Yang",
      "Deepu Rajan",
      "Basura Fernando"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Engelhardt_SViM3D_Stable_Video_Material_Diffusion_for_Single_Image_3D_Generation_ICCV_2025_paper.html": {
    "title": "SViM3D: Stable Video Material Diffusion for Single Image 3D Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andreas Engelhardt",
      "Mark Boss",
      "Vikram Voleti",
      "Chun-Han Yao",
      "Hendrik P. A. Lensch",
      "Varun Jampani"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Nam_VAGUE_Visual_Contexts_Clarify_Ambiguous_Expressions_ICCV_2025_paper.html": {
    "title": "VAGUE: Visual Contexts Clarify Ambiguous Expressions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heejeong Nam",
      "Jinwoo Ahn",
      "Keummin Ka",
      "Jiwan Chung",
      "Youngjae Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Enhancing_Numerical_Prediction_of_MLLMs_with_Soft_Labeling_ICCV_2025_paper.html": {
    "title": "Enhancing Numerical Prediction of MLLMs with Soft Labeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pei Wang",
      "Zhaowei Cai",
      "Hao Yang",
      "Davide Modolo",
      "Ashwin Swaminathan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sun_Fine-Grained_3D_Gaussian_Head_Avatars_Modeling_from_Static_Captures_via_ICCV_2025_paper.html": {
    "title": "Fine-Grained 3D Gaussian Head Avatars Modeling from Static Captures via Joint Reconstruction and Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Sun",
      "Xuan Wang",
      "Cong Wang",
      "WeiLi Zhang",
      "Yanbo Fan",
      "Yu Guo",
      "Fei Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_DynamicFace_High-Quality_and_Consistent_Face_Swapping_for_Image_and_Video_ICCV_2025_paper.html": {
    "title": "DynamicFace: High-Quality and Consistent Face Swapping for Image and Video using Composable 3D Facial Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runqi Wang",
      "Yang Chen",
      "Sijie Xu",
      "Tianyao He",
      "Wei Zhu",
      "Dejia Song",
      "Nemo Chen",
      "Xu Tang",
      "Yao Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lee_CoMoGaussian_Continuous_Motion-Aware_Gaussian_Splatting_from_Motion-Blurred_Images_ICCV_2025_paper.html": {
    "title": "CoMoGaussian: Continuous Motion-Aware Gaussian Splatting from Motion-Blurred Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jungho Lee",
      "Donghyeong Kim",
      "Dogyoon Lee",
      "Suhwan Cho",
      "Minhyeok Lee",
      "Wonjoon Lee",
      "Taeoh Kim",
      "Dongyoon Wee",
      "Sangyoun Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Open-Unfairness_Adversarial_Mitigation_for_Generalized_Deepfake_Detection_ICCV_2025_paper.html": {
    "title": "Open-Unfairness Adversarial Mitigation for Generalized Deepfake Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoyang Li",
      "Zhu Teng",
      "Baopeng Zhang",
      "Jianping Fan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Recognizing_Actions_from_Robotic_View_for_Natural_Human-Robot_Interaction_ICCV_2025_paper.html": {
    "title": "Recognizing Actions from Robotic View for Natural Human-Robot Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyi Wang",
      "Peiming Li",
      "Hong Liu",
      "Zhichao Deng",
      "Can Wang",
      "Jun Liu",
      "Junsong Yuan",
      "Mengyuan Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_WIPES_Wavelet-based_Visual_Primitives_ICCV_2025_paper.html": {
    "title": "WIPES: Wavelet-based Visual Primitives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhao Zhang",
      "Hao Zhu",
      "Delong Wu",
      "Di Kang",
      "Linchao Bao",
      "Xun Cao",
      "Zhan Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Frequency_Domain-Based_Diffusion_Model_for_Unpaired_Image_Dehazing_ICCV_2025_paper.html": {
    "title": "Frequency Domain-Based Diffusion Model for Unpaired Image Dehazing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengxu Liu",
      "Lu Qi",
      "Jinshan Pan",
      "Xueming Qian",
      "Ming-Hsuan Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cai_Unsupervised_Histopathological_Image_Semantic_Segmentation_with_Overlapping_Patches_Consistency_Constraint_ICCV_2025_paper.html": {
    "title": "Unsupervised Histopathological Image Semantic Segmentation with Overlapping Patches Consistency Constraint",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wentian Cai",
      "Weizhao Weng",
      "Zihao Huang",
      "Yandan Chen",
      "Siquan Huang",
      "Ping Gao",
      "Victor C. M. Leung",
      "Ying Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Giakoumoglou_SAGI_Semantically_Aligned_and_Uncertainty_Guided_AI_Image_Inpainting_ICCV_2025_paper.html": {
    "title": "SAGI: Semantically Aligned and Uncertainty Guided AI Image Inpainting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paschalis Giakoumoglou",
      "Dimitrios Karageorgiou",
      "Symeon Papadopoulos",
      "Panagiotis C. Petrantonakis"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Park_Adversarial_Purification_via_Super-Resolution_and_Diffusion_ICCV_2025_paper.html": {
    "title": "Adversarial Purification via Super-Resolution and Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mincheol Park",
      "Cheonjun Park",
      "Seungseop Lim",
      "Mijin Koo",
      "Hyunwuk Lee",
      "Won Woo Ro",
      "Suhyun Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_RAGDiffusion_Faithful_Cloth_Generation_via_External_Knowledge_Assimilation_ICCV_2025_paper.html": {
    "title": "RAGDiffusion: Faithful Cloth Generation via External Knowledge Assimilation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhan Li",
      "Xianfeng Tan",
      "Wenxiang Shang",
      "Yubo Wu",
      "Jian Wang",
      "Xuanhong Chen",
      "Yi Zhang",
      "Hangcheng Zhu",
      "Bingbing Ni"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Mu_Diff2I2P_Differentiable_Image-to-Point_Cloud_Registration_with_Diffusion_Prior_ICCV_2025_paper.html": {
    "title": "Diff2I2P: Differentiable Image-to-Point Cloud Registration with Diffusion Prior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juncheng Mu",
      "Chengwei Ren",
      "Weixiang Zhang",
      "Liang Pan",
      "Xiao-Ping Zhang",
      "Yue Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Dynamic_Typography_Bringing_Text_to_Life_via_Video_Diffusion_Prior_ICCV_2025_paper.html": {
    "title": "Dynamic Typography: Bringing Text to Life via Video Diffusion Prior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zichen Liu",
      "Yihao Meng",
      "Hao Ouyang",
      "Yue Yu",
      "Bolin Zhao",
      "Daniel Cohen-Or",
      "Huamin Qu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Litman_LightSwitch_Multi-view_Relighting_with_Material-guided_Diffusion_ICCV_2025_paper.html": {
    "title": "LightSwitch: Multi-view Relighting with Material-guided Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yehonathan Litman",
      "Fernando De la Torre",
      "Shubham Tulsiani"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_CanFields_Consolidating_Diffeomorphic_Flows_for_Non-Rigid_4D_Interpolation_from_Arbitrary-Length_ICCV_2025_paper.html": {
    "title": "CanFields: Consolidating Diffeomorphic Flows for Non-Rigid 4D Interpolation from Arbitrary-Length Sequences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miaowei Wang",
      "Changjian Li",
      "Amir Vaxman"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yoon_Occlusion-robust_Stylization_for_Drawing-based_3D_Animation_ICCV_2025_paper.html": {
    "title": "Occlusion-robust Stylization for Drawing-based 3D Animation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sunjae Yoon",
      "Gwanhyeong Koo",
      "Younghwan Lee",
      "Ji Woo Hong",
      "Chang D. Yoo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_Towards_Foundational_Models_for_Single-Chip_Radar_ICCV_2025_paper.html": {
    "title": "Towards Foundational Models for Single-Chip Radar",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianshu Huang",
      "Akarsh Prabhakara",
      "Chuhan Chen",
      "Jay Karhade",
      "Deva Ramanan",
      "Matthew O'toole",
      "Anthony Rowe"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Pang_Augmented_and_Softened_Matching_for_Unsupervised_Visible-Infrared_Person_Re-Identification_ICCV_2025_paper.html": {
    "title": "Augmented and Softened Matching for Unsupervised Visible-Infrared Person Re-Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiqi Pang",
      "Chunyu Wang",
      "Lingling Zhao",
      "Junjie Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liang_Spatial_Alignment_and_Temporal_Matching_Adapter_for_Video-Radar_Remote_Physiological_ICCV_2025_paper.html": {
    "title": "Spatial Alignment and Temporal Matching Adapter for Video-Radar Remote Physiological Measurement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qian Liang",
      "Ruixu Geng",
      "Jinbo Chen",
      "Haoyu Wang",
      "Yan Chen",
      "Yang Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_StableCodec_Taming_One-Step_Diffusion_for_Extreme_Image_Compression_ICCV_2025_paper.html": {
    "title": "StableCodec: Taming One-Step Diffusion for Extreme Image Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyu Zhang",
      "Xin Luo",
      "Li Li",
      "Dong Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sabolic_Seal_Your_Backdoor_with_Variational_Defense_ICCV_2025_paper.html": {
    "title": "Seal Your Backdoor with Variational Defense",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ivan Sabolić",
      "Matej Grcić",
      "Siniša Šegvić"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_From_Abyssal_Darkness_to_Blinding_Glare_A_Benchmark_on_Extreme_ICCV_2025_paper.html": {
    "title": "From Abyssal Darkness to Blinding Glare: A Benchmark on Extreme Exposure Correction in Real World",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Wang",
      "Huiyuan Fu",
      "Zhiye Huang",
      "Siru Zhang",
      "Xin Wang",
      "Huadong Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yamane_MVTrajecter_Multi-View_Pedestrian_Tracking_with_Trajectory_Motion_Cost_and_Trajectory_ICCV_2025_paper.html": {
    "title": "MVTrajecter: Multi-View Pedestrian Tracking with Trajectory Motion Cost and Trajectory Appearance Cost",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taiga Yamane",
      "Ryo Masumura",
      "Satoshi Suzuki",
      "Shota Orihashi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Mueller_GeoDiffusion_A_Training-Free_Framework_for_Accurate_3D_Geometric_Conditioning_in_ICCV_2025_paper.html": {
    "title": "GeoDiffusion: A Training-Free Framework for Accurate 3D Geometric Conditioning in Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Phillip Mueller",
      "Talip Uenlue",
      "Sebastian Schmidt",
      "Marcel Kollovieh",
      "Jiajie Fan",
      "Stephan Günnemann",
      "Lars Mikelsons"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Thomas_VALLR_Visual_ASR_Language_Model_for_Lip_Reading_ICCV_2025_paper.html": {
    "title": "VALLR: Visual ASR Language Model for Lip Reading",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marshall Thomas",
      "Edward Fish",
      "Richard Bowden"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Learning_Efficient_and_Generalizable_Human_Representation_with_Human_Gaussian_Model_ICCV_2025_paper.html": {
    "title": "Learning Efficient and Generalizable Human Representation with Human Gaussian Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Liu",
      "Shengjun Zhang",
      "Chensheng Dai",
      "Yang Chen",
      "Hao Liu",
      "Chen Li",
      "Yueqi Duan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_DexH2R_A_Benchmark_for_Dynamic_Dexterous_Grasping_in_Human-to-Robot_Handover_ICCV_2025_paper.html": {
    "title": "DexH2R: A Benchmark for Dynamic Dexterous Grasping in Human-to-Robot Handover",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youzhuo Wang",
      "Jiayi Ye",
      "Chuyang Xiao",
      "Yiming Zhong",
      "Heng Tao",
      "Hang Yu",
      "Yumeng Liu",
      "Jingyi Yu",
      "Yuexin Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yahia_Mobile_Video_Diffusion_ICCV_2025_paper.html": {
    "title": "Mobile Video Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haitam Ben Yahia",
      "Denis Korzhenkov",
      "Ioannis Lelekas",
      "Amir Ghodrati",
      "Amirhossein Habibian"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_Importance-Based_Token_Merging_for_Efficient_Image_and_Video_Generation_ICCV_2025_paper.html": {
    "title": "Importance-Based Token Merging for Efficient Image and Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu Wu",
      "Jingyi Xu",
      "Hieu Le",
      "Dimitris Samaras"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/He_Semi-ViM_Bidirectional_State_Space_Model_for_Mitigating_Label_Imbalance_in_ICCV_2025_paper.html": {
    "title": "Semi-ViM: Bidirectional State Space Model for Mitigating Label Imbalance in Semi-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyang He",
      "Hongyang Xie",
      "Haochen You",
      "Victor Sanchez"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Du_Diffusion_Transformer_meets_Multi-level_Wavelet_Spectrum_for_Single_Image_Super-Resolution_ICCV_2025_paper.html": {
    "title": "Diffusion Transformer meets Multi-level Wavelet Spectrum for Single Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peng Du",
      "Hui Li",
      "Han Xu",
      "Paul Barom Jeon",
      "Dongwook Lee",
      "Daehyun Ji",
      "Ran Yang",
      "Feng Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liang_Towards_Human-like_Virtual_Beings_Simulating_Human_Behavior_in_3D_Scenes_ICCV_2025_paper.html": {
    "title": "Towards Human-like Virtual Beings: Simulating Human Behavior in 3D Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Liang",
      "Wenguan Wang",
      "Yi Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Guo_HiGarment_Cross-modal_Harmony_Based_Diffusion_Model_for_Flat_Sketch_to_ICCV_2025_paper.html": {
    "title": "HiGarment: Cross-modal Harmony Based Diffusion Model for Flat Sketch to Realistic Garment Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyi Guo",
      "Jingxuan Zhang",
      "Fangyu Wu",
      "Huanda Lu",
      "Qiufeng Wang",
      "Wenmian Yang",
      "Eng Gee Lim",
      "Dongming Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Robust_Unfolding_Network_for_HDR_Imaging_with_Modulo_Cameras_ICCV_2025_paper.html": {
    "title": "Robust Unfolding Network for HDR Imaging with Modulo Cameras",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhile Chen",
      "Hui Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xi_Player-Centric_Multimodal_Prompt_Generation_for_Large_Language_Model_Based_Identity-Aware_ICCV_2025_paper.html": {
    "title": "Player-Centric Multimodal Prompt Generation for Large Language Model Based Identity-Aware Basketball Video Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyu Xi",
      "Haoying Sun",
      "Yaofei Wu",
      "Junchi Yan",
      "Haoran Zhang",
      "Lifang Wu",
      "Liang Wang",
      "Changwen Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yin_ArchiSet_Benchmarking_Editable_and_Consistent_Single-View_3D_Reconstruction_of_Buildings_ICCV_2025_paper.html": {
    "title": "ArchiSet: Benchmarking Editable and Consistent Single-View 3D Reconstruction of Buildings with Specific Window-to-Wall Ratios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Yin",
      "Pengyu Zeng",
      "Licheng Shen",
      "Miao Zhang",
      "Jing Zhong",
      "Yuxing Han",
      "Shuai Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_Scale_Your_Instructions_Enhance_the_Instruction-Following_Fidelity_of_Unified_Image_ICCV_2025_paper.html": {
    "title": "Scale Your Instructions: Enhance the Instruction-Following Fidelity of Unified Image Generation Model by Self-Adaptive Attention Scaling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Zhou",
      "Tianyi Wei",
      "Nenghai  Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_From_Enhancement_to_Understanding_Build_a_Generalized_Bridge_for_Low-light_ICCV_2025_paper.html": {
    "title": "From Enhancement to Understanding: Build a Generalized Bridge for Low-light Vision via Semantically Consistent Unsupervised Fine-tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sen Wang",
      "Shao Zeng",
      "Tianjun Gu",
      "Zhizhong Zhang",
      "Ruixin Zhang",
      "Shouhong Ding",
      "Jingyun Zhang",
      "Jun Wang",
      "Xin Tan",
      "Yuan Xie",
      "Lizhuang Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Koh_Translation_of_Text_Embedding_via_Delta_Vector_to_Suppress_Strongly_ICCV_2025_paper.html": {
    "title": "Translation of Text Embedding via Delta Vector to Suppress Strongly Entangled Content in Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eunseo Koh",
      "Seunghoo Hong",
      "Tae-Young Kim",
      "Simon S. Woo",
      "Jae-Pil Heo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sun_ARMO_Autoregressive_Rigging_for_Multi-Category_Objects_ICCV_2025_paper.html": {
    "title": "ARMO: Autoregressive Rigging for Multi-Category Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingze Sun",
      "Shiwei Mao",
      "Keyi Chen",
      "Yurun Chen",
      "Shunlin Lu",
      "Jingbo Wang",
      "Junting Dong",
      "Ruqi Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sirko-Galouchenko_DIP_Unsupervised_Dense_In-Context_Post-training_of_Visual_Representations_ICCV_2025_paper.html": {
    "title": "DIP: Unsupervised Dense In-Context Post-training of Visual Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sophia Sirko-Galouchenko",
      "Spyros Gidaris",
      "Antonin Vobecky",
      "Andrei Bursuc",
      "Nicolas Thome"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lu_Think_Twice_Test-Time_Reasoning_for_Robust_CLIP_Zero-Shot_Classification_ICCV_2025_paper.html": {
    "title": "Think Twice: Test-Time Reasoning for Robust CLIP Zero-Shot Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shenyu Lu",
      "Zhaoying Pan",
      "Xiaoqian Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhu_Training-free_Geometric_Image_Editing_on_Diffusion_Models_ICCV_2025_paper.html": {
    "title": "Training-free Geometric Image Editing on Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanshen Zhu",
      "Zhen Zhu",
      "Kaile Zhang",
      "Yiming Gong",
      "Yuliang Liu",
      "Xiang Bai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_HyperGCT_A_Dynamic_Hyper-GNN-Learned_Geometric_Constraint_for_3D_Registration_ICCV_2025_paper.html": {
    "title": "HyperGCT: A Dynamic Hyper-GNN-Learned Geometric Constraint for 3D Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiyu Zhang",
      "Jiayi Ma",
      "Jianwei Guo",
      "Wei Hu",
      "Zhaoshuai Qi",
      "Fei Hui",
      "Jiaqi Yang",
      "Yanning Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Guo_Dark-ISP_Enhancing_RAW_Image_Processing_for_Low-Light_Object_Detection_ICCV_2025_paper.html": {
    "title": "Dark-ISP: Enhancing RAW Image Processing for Low-Light Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiasheng Guo",
      "Xin Gao",
      "Yuxiang Yan",
      "Guanghao Li",
      "Jian Pu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_Harmonizing_Visual_Representations_for_Unified_Multimodal_Understanding_and_Generation_ICCV_2025_paper.html": {
    "title": "Harmonizing Visual Representations for Unified Multimodal Understanding and Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Size Wu",
      "Wenwei Zhang",
      "Lumin Xu",
      "Sheng Jin",
      "Zhonghua Wu",
      "Qingyi Tao",
      "Wentao Liu",
      "Wei Li",
      "Chen Change Loy"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Environment-Agnostic_Pose_Generating_Environment-independent_Object_Representations_for_6D_Pose_Estimation_ICCV_2025_paper.html": {
    "title": "Environment-Agnostic Pose: Generating Environment-independent Object Representations for 6D Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaobo Zhang",
      "Yuhang Huang",
      "Wanqing Zhao",
      "Wei Zhao",
      "Ziyu Guan",
      "Jinye Peng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Perez_UnMix-NeRF_Spectral_Unmixing_Meets_Neural_Radiance_Fields_ICCV_2025_paper.html": {
    "title": "UnMix-NeRF: Spectral Unmixing Meets Neural Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fabian Perez",
      "Sara Rojas",
      "Carlos Hinojosa",
      "Hoover Rueda-Chacón",
      "Bernard Ghanem"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yin_Progressive_Homeostatic_and_Plastic_Prompt_Tuning_for_Audio-Visual_Multi-Task_Incremental_ICCV_2025_paper.html": {
    "title": "Progressive Homeostatic and Plastic Prompt Tuning for Audio-Visual Multi-Task Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiong Yin",
      "Liang Li",
      "Jiehua Zhang",
      "Yuhan Gao",
      "Chenggang Yan",
      "Xichun Sheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Elsner_Multidimensional_Byte_Pair_Encoding_Shortened_Sequences_for_Improved_Visual_Data_ICCV_2025_paper.html": {
    "title": "Multidimensional Byte Pair Encoding: Shortened Sequences for Improved Visual Data Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tim Elsner",
      "Paula Usinger",
      "Julius Nehring-Wirxel",
      "Gregor Kobsik",
      "Victor Czech",
      "Yanjiang He",
      "Isaak Lim",
      "Leif Kobbelt"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Han_All_in_One_Visual-Description-Guided_Unified_Point_Cloud_Segmentation_ICCV_2025_paper.html": {
    "title": "All in One: Visual-Description-Guided Unified Point Cloud Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zongyan Han",
      "Mohamed El Amine Boudjoghra",
      "Jiahua Dong",
      "Jinhong Wang",
      "Rao Muhammad Anwer"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sun_Controllable-LPMoE_Adapting_to_Challenging_Object_Segmentation_via_Dynamic_Local_Priors_ICCV_2025_paper.html": {
    "title": "Controllable-LPMoE: Adapting to Challenging Object Segmentation via Dynamic Local Priors from Mixture-of-Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanguang Sun",
      "Jiawei Lian",
      "Jian Yang",
      "Lei Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Frequency-Dynamic_Attention_Modulation_For_Dense_Prediction_ICCV_2025_paper.html": {
    "title": "Frequency-Dynamic Attention Modulation For Dense Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linwei Chen",
      "Lin Gu",
      "Ying Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cui_Enpowering_Your_Pansharpening_Models_with_Generalizability_Unified_Distribution_is_All_ICCV_2025_paper.html": {
    "title": "Enpowering Your Pansharpening Models with Generalizability: Unified Distribution is All You Need",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongchuan Cui",
      "Peng Liu",
      "Hui Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tian_Semantic_versus_Identity_A_Divide-and-Conquer_Approach_towards_Adjustable_Medical_Image_ICCV_2025_paper.html": {
    "title": "Semantic versus Identity: A Divide-and-Conquer Approach towards Adjustable Medical Image De-Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Tian",
      "Shuo Wang",
      "Rongzhao Zhang",
      "Zijian Chen",
      "Yankai Jiang",
      "Chunyi Li",
      "Xiangyang Zhu",
      "Fang Yan",
      "Qiang Hu",
      "XiaoSong Wang",
      "Guangtao Zhai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xiang_DEPTHOR_Depth_Enhancement_from_a_Practical_Light-Weight_dToF_Sensor_and_ICCV_2025_paper.html": {
    "title": "DEPTHOR: Depth Enhancement from a Practical Light-Weight dToF Sensor and RGB Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jijun Xiang",
      "Xuan Zhu",
      "Xianqi Wang",
      "Yu Wang",
      "Hong Zhang",
      "Fei Guo",
      "Xin Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lian_Describe_Anything_Detailed_Localized_Image_and_Video_Captioning_ICCV_2025_paper.html": {
    "title": "Describe Anything: Detailed Localized Image and Video Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Long Lian",
      "Yifan Ding",
      "Yunhao Ge",
      "Sifei Liu",
      "Hanzi Mao",
      "Boyi Li",
      "Marco Pavone",
      "Ming-Yu Liu",
      "Trevor Darrell",
      "Adam Yala",
      "Yin Cui"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Point_Cloud_Self-supervised_Learning_via_3D_to_Multi-view_Masked_Learner_ICCV_2025_paper.html": {
    "title": "Point Cloud Self-supervised Learning via 3D to Multi-view Masked Learner",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhimin Chen",
      "Xuewei  Chen",
      "Xiao  Guo",
      "Yingwei  Li",
      "Longlong  Jing",
      "Liang  Yang",
      "Bing Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Probabilistic_Prototype_Calibration_of_Vision-language_Models_for_Generalized_Few-shot_Semantic_ICCV_2025_paper.html": {
    "title": "Probabilistic Prototype Calibration of Vision-language Models for Generalized Few-shot Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Liu",
      "Jiayi Shen",
      "Pan Zhou",
      "Jan-Jakob Sonke",
      "Efstratios Gavves"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhu_KV-Edit_Training-Free_Image_Editing_for_Precise_Background_Preservation_ICCV_2025_paper.html": {
    "title": "KV-Edit: Training-Free Image Editing for Precise Background Preservation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianrui Zhu",
      "Shiyi Zhang",
      "Jiawei Shao",
      "Yansong Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Triad_Empowering_LMM-based_Anomaly_Detection_with_Expert-guided_Region-of-Interest_Tokenizer_and_ICCV_2025_paper.html": {
    "title": "Triad: Empowering LMM-based Anomaly Detection with Expert-guided Region-of-Interest Tokenizer and Manufacturing Process",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanze Li",
      "Shihao Yuan",
      "Haolin Wang",
      "Qizhang Li",
      "Ming Liu",
      "Chen Xu",
      "Guangming Shi",
      "Wangmeng Zuo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_When_Anchors_Meet_Cold_Diffusion_A_Multi-Stage_Approach_to_Lane_ICCV_2025_paper.html": {
    "title": "When Anchors Meet Cold Diffusion: A Multi-Stage Approach to Lane Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo-Lun Huang",
      "Zi-Xiang Ni",
      "Feng-Kai Huang",
      "Hong-Han Shuai",
      "Wen-Huang Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wulff_Dream-to-Recon_Monocular_3D_Reconstruction_with_Diffusion-Depth_Distillation_from_Single_Images_ICCV_2025_paper.html": {
    "title": "Dream-to-Recon: Monocular 3D Reconstruction with Diffusion-Depth Distillation from Single Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Philipp Wulff",
      "Felix Wimbauer",
      "Dominik Muhle",
      "Daniel Cremers"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xiao_Breaking_Grid_Constraints_Dynamic_Graph_Reconstruction_Network_for_Multi-organ_Segmentation_ICCV_2025_paper.html": {
    "title": "Breaking Grid Constraints: Dynamic Graph Reconstruction Network for Multi-organ Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junhao Xiao",
      "Yang Wei",
      "Jingyu Wang",
      "Yongchao Wang",
      "Xiuli Bi",
      "Bin Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lin_SplArt_Articulation_Estimation_and_Part-Level_Reconstruction_with_3D_Gaussian_Splatting_ICCV_2025_paper.html": {
    "title": "SplArt: Articulation Estimation and Part-Level Reconstruction with 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengjie Lin",
      "Jiading Fang",
      "Muhammad Zubair Irshad",
      "Vitor Campagnolo Guizilini",
      "Rares Andrei Ambrus",
      "Greg Shakhnarovich",
      "Matthew R. Walter"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hoefler_FedXDS_Leveraging_Model_Attribution_Methods_to_counteract_Data_Heterogeneity_in_ICCV_2025_paper.html": {
    "title": "FedXDS: Leveraging Model Attribution Methods to counteract Data Heterogeneity in Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maximilian Andreas Hoefler",
      "Karsten Mueller",
      "Wojciech Samek"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Bai_Retinex-MEF_Retinex-based_Glare_Effects_Aware_Unsupervised_Multi-Exposure_Image_Fusion_ICCV_2025_paper.html": {
    "title": "Retinex-MEF: Retinex-based Glare Effects Aware Unsupervised Multi-Exposure Image Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haowen Bai",
      "Jiangshe Zhang",
      "Zixiang Zhao",
      "Lilun Deng",
      "Yukun Cui",
      "Shuang Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Qin_Structure_Matters_Revisiting_Boundary_Refinement_in_Video_Object_Segmentation_ICCV_2025_paper.html": {
    "title": "Structure Matters: Revisiting Boundary Refinement in Video Object Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guanyi Qin",
      "Ziyue Wang",
      "Daiyun Shen",
      "Haofeng Liu",
      "Hantao Zhou",
      "Junde Wu",
      "Runze Hu",
      "Yueming Jin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hu_Bilateral_Collaboration_with_Large_Vision-Language_Models_for_Open_Vocabulary_Human-Object_ICCV_2025_paper.html": {
    "title": "Bilateral Collaboration with Large Vision-Language Models for Open Vocabulary Human-Object Interaction Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yupeng Hu",
      "Changxing Ding",
      "Chang Sun",
      "Shaoli Huang",
      "Xiangmin Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Bolduc_GaSLight_Gaussian_Splats_for_Spatially-Varying_Lighting_in_HDR_ICCV_2025_paper.html": {
    "title": "GaSLight: Gaussian Splats for Spatially-Varying Lighting in HDR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christophe Bolduc",
      "Yannick Hold-Geoffroy",
      "Jean-François Lalonde"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Qu_Hate_in_Plain_Sight_On_the_Risks_of_Moderating_AI-Generated_ICCV_2025_paper.html": {
    "title": "Hate in Plain Sight: On the Risks of Moderating AI-Generated Hateful Illusions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiting Qu",
      "Ziqing Yang",
      "Yihan Ma",
      "Michael Backes",
      "Savvas Zannettou",
      "Yang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Park_Steering_Guidance_for_Personalized_Text-to-Image_Diffusion_Models_ICCV_2025_paper.html": {
    "title": "Steering Guidance for Personalized Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sunghyun Park",
      "Seokeon Choi",
      "Hyoungwoo Park",
      "Sungrack Yun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hu_Improving_Rectified_Flow_with_Boundary_Conditions_ICCV_2025_paper.html": {
    "title": "Improving Rectified Flow with Boundary Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xixi Hu",
      "Runlong Liao",
      "Keyang Xu",
      "Bo Liu",
      "Yeqing Li",
      "Eugene Ie",
      "Hongliang Fei",
      "Qiang Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yeo_ATAS_Any-to-Any_Self-Distillation_for_Enhanced_Open-Vocabulary_Dense_Prediction_ICCV_2025_paper.html": {
    "title": "ATAS: Any-to-Any Self-Distillation for Enhanced Open-Vocabulary Dense Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juan Yeo",
      "Soonwoo Cha",
      "Jiwoo Song",
      "Hyunbin Jin",
      "Taesup Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ma_Multimodal_Prompt_Alignment_for_Facial_Expression_Recognition_ICCV_2025_paper.html": {
    "title": "Multimodal Prompt Alignment for Facial Expression Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fuyan Ma",
      "Yiran He",
      "Bin Sun",
      "Shutao Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xu_Laboring_on_less_labors_RPCA_Paradigm_for_Pan-sharpening_ICCV_2025_paper.html": {
    "title": "Laboring on less labors: RPCA Paradigm for Pan-sharpening",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Honghui Xu",
      "Chuangjie Fang",
      "Yibin Wang",
      "Jie Wu",
      "Jianwei Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Mi_MVQA_Mamba_with_Unified_Sampling_for_Efficient_Video_Quality_Assessment_ICCV_2025_paper.html": {
    "title": "MVQA: Mamba with Unified Sampling for Efficient Video Quality Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yachun Mi",
      "Yu Li",
      "Weicheng Meng",
      "Chaofeng Chen",
      "Chen Hui",
      "Shaohui Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Semantic-guided_Camera_Ray_Regression_for_Visual_Localization_ICCV_2025_paper.html": {
    "title": "Semantic-guided Camera Ray Regression for Visual Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yesheng Zhang",
      "Xu Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Rojas_HAMSt3R_Human-Aware_Multi-view_Stereo_3D_Reconstruction_ICCV_2025_paper.html": {
    "title": "HAMSt3R: Human-Aware Multi-view Stereo 3D Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sara Rojas",
      "Matthieu Armando",
      "Bernard Ghanem",
      "Philippe Weinzaepfel",
      "Vincent Leroy",
      "Grégory Rogez"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yu_TrajectoryCrafter_Redirecting_Camera_Trajectory_for_Monocular_Videos_via_Diffusion_Models_ICCV_2025_paper.html": {
    "title": "TrajectoryCrafter: Redirecting Camera Trajectory for Monocular Videos via Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mark Yu",
      "Wenbo Hu",
      "Jinbo Xing",
      "Ying Shan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chatziagapi_AV-Flow_Transforming_Text_to_Audio-Visual_Human-like_Interactions_ICCV_2025_paper.html": {
    "title": "AV-Flow: Transforming Text to Audio-Visual Human-like Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aggelina Chatziagapi",
      "Louis-Philippe Morency",
      "Hongyu Gong",
      "Michael Zollhöfer",
      "Dimitris Samaras",
      "Alexander Richard"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yu_Multi-View_Slot_Attention_Using_Paraphrased_Texts_for_Face_Anti-Spoofing_ICCV_2025_paper.html": {
    "title": "Multi-View Slot Attention Using Paraphrased Texts for Face Anti-Spoofing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeongmin Yu",
      "Susang Kim",
      "Kisu Lee",
      "Taekyoung Kwon",
      "Won-Yong Shin",
      "Ha Young Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yuan_Self-Supervised_Monocular_4D_Scene_Reconstruction_for_Egocentric_Videos_ICCV_2025_paper.html": {
    "title": "Self-Supervised Monocular 4D Scene Reconstruction for Egocentric Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengbo Yuan",
      "Geng Chen",
      "Li Yi",
      "Yang Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_What_we_need_is_explicit_controllability_Training_3D_gaze_estimator_ICCV_2025_paper.html": {
    "title": "What we need is explicit controllability: Training 3D gaze estimator using only facial images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tingwei Li",
      "Jun Bao",
      "Zhenzhong Kuang",
      "Buyu Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhu_WaveMamba_Wavelet-Driven_Mamba_Fusion_for_RGB-Infrared_Object_Detection_ICCV_2025_paper.html": {
    "title": "WaveMamba: Wavelet-Driven Mamba Fusion for RGB-Infrared Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haodong Zhu",
      "Wenhao Dong",
      "Linlin Yang",
      "Hong Li",
      "Yuguang Yang",
      "Yangyang Ren",
      "Qingcheng Zhu",
      "Zichao Feng",
      "Changbai Li",
      "Shaohui Lin",
      "Runqi Wang",
      "Xiaoyan Luo",
      "Baochang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tang_CoST_Efficient_Collaborative_Perception_From_Unified_Spatiotemporal_Perspective_ICCV_2025_paper.html": {
    "title": "CoST: Efficient Collaborative Perception From Unified Spatiotemporal Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zongheng Tang",
      "Yi Liu",
      "Yifan Sun",
      "Yulu Gao",
      "Jinyu Chen",
      "Runsheng Xu",
      "Si Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_GCRayDiffusion_Pose-Free_Surface_Reconstruction_via_Geometric_Consistent_Ray_Diffusion_ICCV_2025_paper.html": {
    "title": "GCRayDiffusion: Pose-Free Surface Reconstruction via Geometric Consistent Ray Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li-Heng Chen",
      "Zi-Xin Zou",
      "Chang Liu",
      "Tianjiao Jing",
      "Yan-Pei Cao",
      "Shi-Sheng Huang",
      "Hongbo Fu",
      "Hua Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_AU-Blendshape_for_Fine-grained_Stylized_3D_Facial_Expression_Manipulation_ICCV_2025_paper.html": {
    "title": "AU-Blendshape for Fine-grained Stylized 3D Facial Expression Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Li",
      "Ju Dai",
      "Feng Zhou",
      "Kaida  Ning",
      "Lei Li",
      "Junjun Pan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Noise-Modeled_Diffusion_Models_for_Low-Light_Spike_Image_Restoration_ICCV_2025_paper.html": {
    "title": "Noise-Modeled Diffusion Models for Low-Light Spike Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruonan Liu",
      "Lin Zhu",
      "Xijie Xiang",
      "Lizhi Wang",
      "Hua Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zheng_Rethinking_Discrete_Tokens_Treating_Them_as_Conditions_for_Continuous_Autoregressive_ICCV_2025_paper.html": {
    "title": "Rethinking Discrete Tokens: Treating Them as Conditions for Continuous Autoregressive Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peng Zheng",
      "Junke Wang",
      "Yi Chang",
      "Yizhou Yu",
      "Rui Ma",
      "Zuxuan Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Guo_MotionLab_Unified_Human_Motion_Generation_and_Editing_via_the_Motion-Condition-Motion_ICCV_2025_paper.html": {
    "title": "MotionLab: Unified Human Motion Generation and Editing via the Motion-Condition-Motion Paradigm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyan Guo",
      "Zeyu Hu",
      "De Wen Soh",
      "Na Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lee_Continual_Multiple_Instance_Learning_with_Enhanced_Localization_for_Histopathological_Whole_ICCV_2025_paper.html": {
    "title": "Continual Multiple Instance Learning with Enhanced Localization for Histopathological Whole Slide Image Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Byung Hyun Lee",
      "Wongi Jeong",
      "Woojae Han",
      "Kyoungbun Lee",
      "Se Young Chun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cho_DisCoRD_Discrete_Tokens_to_Continuous_Motion_via_Rectified_Flow_Decoding_ICCV_2025_paper.html": {
    "title": "DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jungbin Cho",
      "Junwan Kim",
      "Jisoo Kim",
      "Minseo Kim",
      "Mingu Kang",
      "Sungeun Hong",
      "Tae-Hyun Oh",
      "Youngjae Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Krause_TREAD_Token_Routing_for_Efficient_Architecture-agnostic_Diffusion_Training_ICCV_2025_paper.html": {
    "title": "TREAD: Token Routing for Efficient Architecture-agnostic Diffusion Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Felix Krause",
      "Timy Phan",
      "Ming Gui",
      "Stefan Andreas Baumann",
      "Vincent Tao Hu",
      "Björn Ommer"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Leveraging_Debiased_Cross-modal_Attention_Maps_and_Code-based_Reasoning_for_Zero-shot_ICCV_2025_paper.html": {
    "title": "Leveraging Debiased Cross-modal Attention Maps and Code-based Reasoning for Zero-shot Referring Expression Comprehension",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juntao Chen",
      "Wen Shen",
      "Zhihua Wei",
      "Lijun Sun",
      "Hongyun Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lin_Controllable_Weather_Synthesis_and_Removal_with_Video_Diffusion_Models_ICCV_2025_paper.html": {
    "title": "Controllable Weather Synthesis and Removal with Video Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chih-Hao Lin",
      "Zian Wang",
      "Ruofan Liang",
      "Yuxuan Zhang",
      "Sanja Fidler",
      "Shenlong Wang",
      "Zan Gojcic"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hirota_Bias_in_Gender_Bias_Benchmarks_How_Spurious_Features_Distort_Evaluation_ICCV_2025_paper.html": {
    "title": "Bias in Gender Bias Benchmarks: How Spurious Features Distort Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yusuke Hirota",
      "Ryo Hachiuma",
      "Boyi Li",
      "Ximing Lu",
      "Michael Ross Boone",
      "Boris Ivanovic",
      "Yejin Choi",
      "Marco Pavone",
      "Yu-Chiang Frank Wang",
      "Noa Garcia",
      "Yuta Nakashima",
      "Chao-Han Huck Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_Clink_Chop_Thud_-_Learning_Object_Sounds_from_Real-World_Interactions_ICCV_2025_paper.html": {
    "title": "Clink! Chop! Thud! - Learning Object Sounds from Real-World Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengyu Yang",
      "Yiming Chen",
      "Haozheng Pei",
      "Siddhant Agarwal",
      "Arun Balajee Vasudevan",
      "James Hays"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zeng_Instant_GaussianImage_A_Generalizable_and_Self-Adaptive_Image_Representation_via_2D_ICCV_2025_paper.html": {
    "title": "Instant GaussianImage: A Generalizable and Self-Adaptive Image Representation via 2D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaojie Zeng",
      "Yuesong Wang",
      "Tao Guan",
      "Chao Yang",
      "Lili Ju"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xu_Learning_to_Inference_Adaptively_for_Multimodal_Large_Language_Models_ICCV_2025_paper.html": {
    "title": "Learning to Inference Adaptively for Multimodal Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuoyan Xu",
      "Khoi Duc Nguyen",
      "Preeti Mukherjee",
      "Saurabh Bagchi",
      "Somali Chaterji",
      "Yingyu Liang",
      "Yin Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shin_HazeFlow_Revisit_Haze_Physical_Model_as_ODE_and_Non-Homogeneous_Haze_ICCV_2025_paper.html": {
    "title": "HazeFlow: Revisit Haze Physical Model as ODE and Non-Homogeneous Haze Generation for Real-World Dehazing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junseong Shin",
      "Seungwoo Chung",
      "Yunjeong Yang",
      "Tae Hyun Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tang_AcZeroTS_Active_Learning_for_Zero-shot_Tissue_Segmentation_in_Pathology_Images_ICCV_2025_paper.html": {
    "title": "AcZeroTS: Active Learning for Zero-shot Tissue Segmentation in Pathology Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiao Tang",
      "Junjie Zhou",
      "Bo Qian",
      "Peng Wan",
      "Yingli Zuo",
      "Wei Shao",
      "Daoqiang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jeong_FaceShield_Defending_Facial_Image_against_Deepfake_Threats_ICCV_2025_paper.html": {
    "title": "FaceShield: Defending Facial Image against Deepfake Threats",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaehwan Jeong",
      "Sumin In",
      "Sieun Kim",
      "Hannie Shin",
      "Jongheon Jeong",
      "Sang Ho Yoon",
      "Jaewook Chung",
      "Sangpil Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Arabi_SEAL_Semantic_Aware_Image_Watermarking_ICCV_2025_paper.html": {
    "title": "SEAL: Semantic Aware Image Watermarking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kasra Arabi",
      "R. Teal Witter",
      "Chinmay Hegde",
      "Niv Cohen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kummerer_Modeling_Saliency_Dataset_Bias_ICCV_2025_paper.html": {
    "title": "Modeling Saliency Dataset Bias",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthias Kümmerer",
      "Harneet Singh Khanuja",
      "Matthias Bethge"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lin_MCOP_Multi-UAV_Collaborative_Occupancy_Prediction_ICCV_2025_paper.html": {
    "title": "MCOP: Multi-UAV Collaborative Occupancy Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zefu Lin",
      "Wenbo Chen",
      "Xiaojuan Jin",
      "Yuran Yang",
      "Lue Fan",
      "Yixin Zhang",
      "Yufeng Zhang",
      "Zhaoxiang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Bastian_Forecasting_Continuous_Non-Conservative_Dynamical_Systems_in_SO3_ICCV_2025_paper.html": {
    "title": "Forecasting Continuous Non-Conservative Dynamical Systems in SO(3)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lennart Bastian",
      "Mohammad Rashed",
      "Nassir Navab",
      "Tolga Birdal"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_HoliTracer_Holistic_Vectorization_of_Geographic_Objects_from_Large-Size_Remote_Sensing_ICCV_2025_paper.html": {
    "title": "HoliTracer: Holistic Vectorization of Geographic Objects from Large-Size Remote Sensing Imagery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Wang",
      "Bo Dang",
      "Wanchun Li",
      "Wei Chen",
      "Yansheng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Tensor-aggregated_LoRA_in_Federated_Fine-tuning_ICCV_2025_paper.html": {
    "title": "Tensor-aggregated LoRA in Federated Fine-tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhixuan Li",
      "Binqian Xu",
      "Xiangbo Shu",
      "Jiachao Zhang",
      "Yazhou Yao",
      "Guo-Sen Xie",
      "Jinhui Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ye_TextSSR_Diffusion-based_Data_Synthesis_for_Scene_Text_Recognition_ICCV_2025_paper.html": {
    "title": "TextSSR: Diffusion-based Data Synthesis for Scene Text Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingsong Ye",
      "Yongkun Du",
      "Yunbo Tao",
      "Zhineng Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Su_OVA-Fields_Weakly_Supervised_Open-Vocabulary_Affordance_Fields_for_Robot_Operational_Part_ICCV_2025_paper.html": {
    "title": "OVA-Fields: Weakly Supervised Open-Vocabulary Affordance Fields for Robot Operational Part Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heng Su",
      "Mengying Xie",
      "Nieqing Cao",
      "Yan Ding",
      "Beichen Shao",
      "Xianlei Long",
      "Fuqiang Gu",
      "Chao Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Marks_Tree_Skeletonization_from_3D_Point_Clouds_by_Denoising_Diffusion_ICCV_2025_paper.html": {
    "title": "Tree Skeletonization from 3D Point Clouds by Denoising Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elias Ariel Marks",
      "Lucas Nunes",
      "Federico Magistri",
      "Matteo Sodano",
      "Rodrigo Marcuzzi",
      "Lars Zimmermann",
      "Jens Behley",
      "Cyrill Stachniss"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Rahman_DiMPLe_-_Disentangled_Multi-Modal_Prompt_Learning_Enhancing_Out-Of-Distribution_Alignment_with_ICCV_2025_paper.html": {
    "title": "DiMPLe - Disentangled Multi-Modal Prompt Learning: Enhancing Out-Of-Distribution Alignment with Invariant and Spurious Feature Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Umaima Rahman",
      "Mohammad Yaqub",
      "Dwarikanath Mahapatra"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sachdeva_From_Panels_to_Prose_Generating_Literary_Narratives_from_Comics_ICCV_2025_paper.html": {
    "title": "From Panels to Prose: Generating Literary Narratives from Comics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ragav Sachdeva",
      "Andrew Zisserman"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Taioli_Collaborative_Instance_Object_Navigation_Leveraging_Uncertainty-Awareness_to_Minimize_Human-Agent_Dialogues_ICCV_2025_paper.html": {
    "title": "Collaborative Instance Object Navigation: Leveraging Uncertainty-Awareness to Minimize Human-Agent Dialogues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francesco Taioli",
      "Edoardo Zorzi",
      "Gianni Franchi",
      "Alberto Castellini",
      "Alessandro Farinelli",
      "Marco Cristani",
      "Yiming Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_VMem_Consistent_Interactive_Video_Scene_Generation_with_Surfel-Indexed_View_Memory_ICCV_2025_paper.html": {
    "title": "VMem: Consistent Interactive Video Scene Generation with Surfel-Indexed View Memory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runjia Li",
      "Philip Torr",
      "Andrea Vedaldi",
      "Tomas Jakab"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sun_Learning_Null_Geodesics_for_Gravitational_Lensing_Rendering_in_General_Relativity_ICCV_2025_paper.html": {
    "title": "Learning Null Geodesics for Gravitational Lensing Rendering in General Relativity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingyuan Sun",
      "Zheng Fang",
      "Jiaxu Wang",
      "Kunyi Zhang",
      "Qiang Zhang",
      "Renjing Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Clarke_X-Capture_An_Open-Source_Portable_Device_for_Multi-Sensory_Learning_ICCV_2025_paper.html": {
    "title": "X-Capture: An Open-Source Portable Device for Multi-Sensory Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuel Clarke",
      "Suzannah Wistreich",
      "Yanjie Ze",
      "Jiajun Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Towards_Accurate_and_Efficient_3D_Object_Detection_for_Autonomous_Driving_ICCV_2025_paper.html": {
    "title": "Towards Accurate and Efficient 3D Object Detection for Autonomous Driving: A Mixture of Experts Computing System on Edge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linshen Liu",
      "Boyan Su",
      "Junyue  Jiang",
      "Guanlin Wu",
      "Cong Guo",
      "Ceyu  Xu",
      "Hao Frank Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Feng_UrbanLLaVA_A_Multi-modal_Large_Language_Model_for_Urban_Intelligence_ICCV_2025_paper.html": {
    "title": "UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Feng",
      "Shengyuan Wang",
      "Tianhui Liu",
      "Yanxin Xi",
      "Yong Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Simon_TITAN-Guide_Taming_Inference-Time_Alignment_for_Guided_Text-to-Video_Diffusion_Models_ICCV_2025_paper.html": {
    "title": "TITAN-Guide: Taming Inference-Time Alignment for Guided Text-to-Video Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christian Simon",
      "Masato Ishii",
      "Akio Hayakawa",
      "Zhi Zhong",
      "Shusuke Takahashi",
      "Takashi Shibuya",
      "Yuki Mitsufuji"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sudhakaran_ART_Adaptive_Relation_Tuning_for_Generalized_Relation_Prediction_ICCV_2025_paper.html": {
    "title": "ART: Adaptive Relation Tuning for Generalized Relation Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gopika Sudhakaran",
      "Hikaru Shindo",
      "Patrick Schramowski",
      "Simone Schaub-Meyer",
      "Kristian Kersting",
      "Stefan Roth"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_VLM4D_Towards_Spatiotemporal_Awareness_in_Vision_Language_Models_ICCV_2025_paper.html": {
    "title": "VLM4D: Towards Spatiotemporal Awareness in Vision Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shijie Zhou",
      "Alexander Vilesov",
      "Xuehai He",
      "Ziyu Wan",
      "Shuwang Zhang",
      "Aditya Nagachandra",
      "Di Chang",
      "Dongdong Chen",
      "Xin Eric Wang",
      "Achuta Kadambi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sun_DimensionX_Create_Any_3D_and_4D_Scenes_from_a_Single_ICCV_2025_paper.html": {
    "title": "DimensionX: Create Any 3D and 4D Scenes from a Single Image with Decoupled Video Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenqiang Sun",
      "Shuo Chen",
      "Fangfu Liu",
      "Zilong Chen",
      "Yueqi Duan",
      "Jun Zhu",
      "Jun Zhang",
      "Yikai Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liang_Uncertainty-Driven_Expert_Control_Enhancing_the_Reliability_of_Medical_Vision-Language_Models_ICCV_2025_paper.html": {
    "title": "Uncertainty-Driven Expert Control: Enhancing the Reliability of Medical Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Liang",
      "Di  Wang",
      "Zhicheng Jiao",
      "Ronghan Li",
      "Pengfei Yang",
      "Quan Wang",
      "Tat-Seng Chua"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sun_Mitigating_Geometric_Degradation_in_Fast_DownSampling_via_FastAdapter_for_Point_ICCV_2025_paper.html": {
    "title": "Mitigating Geometric Degradation in Fast DownSampling via FastAdapter for Point Cloud Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuofeng Sun",
      "Haibin Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Dual-S3D_Hierarchical_Dual-Path_Selective_SSM-CNN_for_High-Fidelity_Implicit_Reconstruction_ICCV_2025_paper.html": {
    "title": "Dual-S3D: Hierarchical Dual-Path Selective SSM-CNN for High-Fidelity Implicit Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luoxi Zhang",
      "Pragyan Shrestha",
      "Yu Zhou",
      "Chun Xie",
      "Itaru Kitahara"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/He_SparseFlex_High-Resolution_and_Arbitrary-Topology_3D_Shape_Modeling_ICCV_2025_paper.html": {
    "title": "SparseFlex: High-Resolution and Arbitrary-Topology 3D Shape Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xianglong He",
      "Zi-Xin Zou",
      "Chia-Hao Chen",
      "Yuan-Chen Guo",
      "Ding Liang",
      "Chun Yuan",
      "Wanli Ouyang",
      "Yan-Pei Cao",
      "Yangguang Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jiang_Multimodal_LLM_Guided_Exploration_and_Active_Mapping_using_Fisher_Information_ICCV_2025_paper.html": {
    "title": "Multimodal LLM Guided Exploration and Active Mapping using Fisher Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wen Jiang",
      "Boshu Lei",
      "Katrina Ashton",
      "Kostas Daniilidis"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Scheuble_Lidar_Waveforms_are_Worth_40x128x33_Words_ICCV_2025_paper.html": {
    "title": "Lidar Waveforms are Worth 40x128x33 Words",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dominik Scheuble",
      "Hanno Holzhüter",
      "Steven Peters",
      "Mario Bijelic",
      "Felix Heide"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shenaj_LoRA.rar_Learning_to_Merge_LoRAs_via_Hypernetworks_for_Subject-Style_Conditioned_ICCV_2025_paper.html": {
    "title": "LoRA.rar: Learning to Merge LoRAs via Hypernetworks for Subject-Style Conditioned Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Donald Shenaj",
      "Ondrej Bohdal",
      "Mete Ozay",
      "Pietro Zanuttigh",
      "Umberto Michieli"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Mo_X-Fusion_Introducing_New_Modality_to_Frozen_Large_Language_Models_ICCV_2025_paper.html": {
    "title": "X-Fusion: Introducing New Modality to Frozen Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sicheng Mo",
      "Thao Nguyen",
      "Xun  Huang",
      "Siddharth Srinivasan Iyer",
      "Yijun Li",
      "Yuchen Liu",
      "Abhishek  Tandon",
      "Eli Shechtman",
      "Krishna Kumar  Singh",
      "Yong Jae  Lee",
      "Bolei  Zhou",
      "Yuheng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_KOEnsAttack_Towards_Efficient_Data-Free_Black-Box_Adversarial_Attacks_via_Knowledge-Orthogonalized_Substitute_ICCV_2025_paper.html": {
    "title": "KOEnsAttack: Towards Efficient Data-Free Black-Box Adversarial Attacks via Knowledge-Orthogonalized Substitute Ensembles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaoyong Yang",
      "Jia-Li Yin",
      "Bin Chen",
      "Zhaozhe Hu",
      "Xiaolei Liu",
      "Wei Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sun_Multimodal_Large_Language_Model-Guided_ISP_Hyperparameter_Optimization_with_Dynamic_Preference_ICCV_2025_paper.html": {
    "title": "Multimodal Large Language Model-Guided ISP Hyperparameter Optimization with Dynamic Preference Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyu Sun",
      "Zhikun Zhao",
      "Congyan Lang",
      "Bing Li",
      "Juan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Fucka_SALAD_--_Semantics-Aware_Logical_Anomaly_Detection_ICCV_2025_paper.html": {
    "title": "SALAD -- Semantics-Aware Logical Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matic Fučka",
      "Vitjan Zavrtanik",
      "Danijel Skočaj"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Federated_Continuous_Category_Discovery_and_Learning_ICCV_2025_paper.html": {
    "title": "Federated Continuous Category Discovery and Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lixu Wang",
      "Chenxi Liu",
      "Junfeng Guo",
      "Qingqing Ye",
      "Heng Huang",
      "Haibo Hu",
      "Wei Dong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hwang_Motion_Synthesis_with_Sparse_and_Flexible_Keyjoint_Control_ICCV_2025_paper.html": {
    "title": "Motion Synthesis with Sparse and Flexible Keyjoint Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Inwoo Hwang",
      "Jinseok Bae",
      "Donggeun Lim",
      "Young Min Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Baek_EVOLVE_Event-Guided_Deformable_Feature_Transfer_and_Dual-Memory_Refinement_for_Low-Light_ICCV_2025_paper.html": {
    "title": "EVOLVE: Event-Guided Deformable Feature Transfer and Dual-Memory Refinement for Low-Light Video Object Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jong-Hyeon Baek",
      "Jiwon Oh",
      "Yeong Jun Koh"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Rawal_ARGUS_Hallucination_and_Omission_Evaluation_in_Video-LLMs_ICCV_2025_paper.html": {
    "title": "ARGUS: Hallucination and Omission Evaluation in Video-LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruchit Rawal",
      "Reza Shirkavand",
      "Heng Huang",
      "Gowthami Somepalli",
      "Tom Goldstein"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Siniukov_DiTaiListener_Controllable_High_Fidelity_Listener_Video_Generation_with_Diffusion_ICCV_2025_paper.html": {
    "title": "DiTaiListener: Controllable High Fidelity Listener Video Generation with Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maksim Siniukov",
      "Di Chang",
      "Minh Tran",
      "Hongkun Gong",
      "Ashutosh Chaubey",
      "Mohammad Soleymani"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Choi_Humans_as_a_Calibration_Pattern_Dynamic_3D_Scene_Reconstruction_from_ICCV_2025_paper.html": {
    "title": "Humans as a Calibration Pattern: Dynamic 3D Scene Reconstruction from Unsynchronized and Uncalibrated Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changwoon Choi",
      "Jeongjun Kim",
      "Geonho Cha",
      "Minkwan Kim",
      "Dongyoon Wee",
      "Young Min Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_ISP2HRNet_Learning_to_Reconstruct_High_Resolution_Image_from_Irregularly_Sampled_ICCV_2025_paper.html": {
    "title": "ISP2HRNet: Learning to Reconstruct High Resolution Image from Irregularly Sampled Pixels via Hierarchical Gradient Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanlin Wang",
      "Ruiqin Xiong",
      "Rui Zhao",
      "Jin Wang",
      "Xiaopeng Fan",
      "Tiejun Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xue_Group_Inertial_Poser_Multi-Person_Pose_and_Global_Translation_from_Sparse_ICCV_2025_paper.html": {
    "title": "Group Inertial Poser: Multi-Person Pose and Global Translation from Sparse Inertial Sensors and Ultra-Wideband Ranging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ying Xue",
      "Jiaxi Jiang",
      "Rayan Armani",
      "Dominik Hollidt",
      "Yi-Chi Liao",
      "Christian Holz"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Abe_NormalLoc_Visual_Localization_on_Textureless_3D_Models_using_Surface_Normals_ICCV_2025_paper.html": {
    "title": "NormalLoc: Visual Localization on Textureless 3D Models using Surface Normals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiro Abe",
      "Gaku Nakano",
      "Kazumine Ogura"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_Light-A-Video_Training-free_Video_Relighting_via_Progressive_Light_Fusion_ICCV_2025_paper.html": {
    "title": "Light-A-Video: Training-free Video Relighting via Progressive Light Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujie Zhou",
      "Jiazi Bu",
      "Pengyang Ling",
      "Pan Zhang",
      "Tong Wu",
      "Qidong Huang",
      "Jinsong Li",
      "Xiaoyi Dong",
      "Yuhang Zang",
      "Yuhang Cao",
      "Anyi Rao",
      "Jiaqi Wang",
      "Li Niu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Luo_Decoupled_Multi-Predictor_Optimization_for_Inference-Efficient_Model_Tuning_ICCV_2025_paper.html": {
    "title": "Decoupled Multi-Predictor Optimization for Inference-Efficient Model Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liwei Luo",
      "Shuaitengyuan Li",
      "Dongwei Ren",
      "Qilong Wang",
      "Pengfei Zhu",
      "Qinghua Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Deterministic_Object_Pose_Confidence_Region_Estimation_ICCV_2025_paper.html": {
    "title": "Deterministic Object Pose Confidence Region Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinghao Wang",
      "Zhang Li",
      "Zi Wang",
      "Banglei Guan",
      "Yang Shang",
      "Qifeng Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_MedVSR_Medical_Video_Super-Resolution_with_Cross_State-Space_Propagation_ICCV_2025_paper.html": {
    "title": "MedVSR: Medical Video Super-Resolution with Cross State-Space Propagation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyu Liu",
      "Guolei Sun",
      "Cheng Wang",
      "Yixuan Yuan",
      "Ender Konukoglu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tan_MPBR_Multimodal_Progressive_Bidirectional_Reasoning_for_Open-Set_Fine-Grained_Recognition_ICCV_2025_paper.html": {
    "title": "MPBR: Multimodal Progressive Bidirectional Reasoning for Open-Set Fine-Grained Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junfu Tan",
      "Peiguang Jing",
      "Yu Zhu",
      "Yu Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/He_Domain-aware_Category-level_Geometry_Learning_Segmentation_for_3D_Point_Clouds_ICCV_2025_paper.html": {
    "title": "Domain-aware Category-level Geometry Learning Segmentation for 3D Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pei He",
      "Lingling Li",
      "Licheng Jiao",
      "Ronghua Shang",
      "Fang Liu",
      "Shuang Wang",
      "Xu Liu",
      "Wenping Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhu_DiMO_Distilling_Masked_Diffusion_Models_into_One-step_Generator_ICCV_2025_paper.html": {
    "title": "Di[M]O: Distilling Masked Diffusion Models into One-step Generator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanzhi Zhu",
      "Xi Wang",
      "Stéphane Lathuilière",
      "Vicky Kalogeiton"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jakubik_TerraMind_Large-Scale_Generative_Multimodality_for_Earth_Observation_ICCV_2025_paper.html": {
    "title": "TerraMind: Large-Scale Generative Multimodality for Earth Observation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Johannes Jakubik",
      "Felix Yang",
      "Benedikt Blumenstiel",
      "Erik Scheurer",
      "Rocco Sedona",
      "Stefano Maurogiovanni",
      "Jente Bosmans",
      "Nikolaos Dionelis",
      "Valerio Marsocci",
      "Niklas Kopp",
      "Rahul Ramachandran",
      "Paolo Fraccaro",
      "Thomas Brunschwiler",
      "Gabriele Cavallaro",
      "Juan Bernabe-Moreno",
      "Nicolas Longépé"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Byun_An_Efficient_Post-hoc_Framework_for_Reducing_Task_Discrepancy_of_Text_ICCV_2025_paper.html": {
    "title": "An Efficient Post-hoc Framework for Reducing Task Discrepancy of Text Encoders for Composed Image Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaeseok Byun",
      "Seokhyeon Jeong",
      "Wonjae Kim",
      "Sanghyuk Chun",
      "Taesup Moon"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Q-Norm_Robust_Representation_Learning_via_Quality-Adaptive_Normalization_ICCV_2025_paper.html": {
    "title": "Q-Norm: Robust Representation Learning via Quality-Adaptive Normalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lanning Zhang",
      "Ying Zhou",
      "Fei Gao",
      "Ziyun Li",
      "Maoying Qiao",
      "Jinlan Xu",
      "Nannan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sung-Bin_VoiceCraft-Dub_Automated_Video_Dubbing_with_Neural_Codec_Language_Models_ICCV_2025_paper.html": {
    "title": "VoiceCraft-Dub: Automated Video Dubbing with Neural Codec Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kim Sung-Bin",
      "Jeongsoo Choi",
      "Puyuan Peng",
      "Joon Son Chung",
      "Tae-Hyun Oh",
      "David Harwath"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lv_SUV_Suppressing_Undesired_Video_Content_via_Semantic_Modulation_Based_on_ICCV_2025_paper.html": {
    "title": "SUV: Suppressing Undesired Video Content via Semantic Modulation Based on Text Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Lv",
      "Mingwen Shao",
      "Lingzhuang Meng",
      "Chang Liu",
      "Yecong Wan",
      "Xinyuan Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shi_FonTS_Text_Rendering_With_Typography_and_Style_Controls_ICCV_2025_paper.html": {
    "title": "FonTS: Text Rendering With Typography and Style Controls",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenda Shi",
      "Yiren Song",
      "Dengming Zhang",
      "Jiaming Liu",
      "Xingxing Zou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Improving_SAM_for_Camouflaged_Object_Detection_via_Dual_Stream_Adapters_ICCV_2025_paper.html": {
    "title": "Improving SAM for Camouflaged Object Detection via Dual Stream Adapters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaming Liu",
      "Linghe Kong",
      "Guihai Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Petit_DiSCO-3D__Discovering_and_Segmenting_Sub-Concepts_from_Open-vocabulary_Queries_in_ICCV_2025_paper.html": {
    "title": "DiSCO-3D : Discovering and Segmenting Sub-Concepts from Open-vocabulary Queries in NeRF",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Doriand Petit",
      "Steve Bourgeois",
      "Vincent Gay-Bellile",
      "Florian Chabot",
      "Loïc Barthe"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_PRIMAL_Physically_Reactive_and_Interactive_Motor_Model_for_Avatar_Learning_ICCV_2025_paper.html": {
    "title": "PRIMAL: Physically Reactive and Interactive Motor Model for Avatar Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Zhang",
      "Yao Feng",
      "Alpár Cseke",
      "Nitin Saini",
      "Nathan Bajandas",
      "Nicolas Heron",
      "Michael J. Black"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_VCA_Video_Curious_Agent_for_Long_Video_Understanding_ICCV_2025_paper.html": {
    "title": "VCA: Video Curious Agent for Long Video Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyuan Yang",
      "Delin Chen",
      "Xueyang Yu",
      "Maohao Shen",
      "Chuang Gan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xu_Self-Reinforcing_Prototype_Evolution_with_Dual-Knowledge_Cooperation_for_Semi-Supervised_Lifelong_Person_ICCV_2025_paper.html": {
    "title": "Self-Reinforcing Prototype Evolution with Dual-Knowledge Cooperation for Semi-Supervised Lifelong Person Re-Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kunlun Xu",
      "Fan Zhuo",
      "Jiangmeng Li",
      "Xu Zou",
      "Jiahuan Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chae_What_to_Distill_Fast_Knowledge_Distillation_with_Adaptive_Sampling_ICCV_2025_paper.html": {
    "title": "What to Distill? Fast Knowledge Distillation with Adaptive Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Byungchul Chae",
      "Seonyeong Heo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jiang_Revisiting_Pool-based_Prompt_Learning_for_Few-shot_Class-incremental_Learning_ICCV_2025_paper.html": {
    "title": "Revisiting Pool-based Prompt Learning for Few-shot Class-incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongwei Jiang",
      "Yixiong Zou",
      "Yuhua Li",
      "Ruixuan Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gao_HOMO-Feature_Cross-Arbitrary-Modal_Image_Matching_with_Homomorphism_of_Organized_Major_Orientation_ICCV_2025_paper.html": {
    "title": "HOMO-Feature: Cross-Arbitrary-Modal Image Matching with Homomorphism of Organized Major Orientation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenzhong Gao",
      "Wei Li",
      "Desheng Weng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Papais_ForeSight_Multi-View_Streaming_Joint_Object_Detection_and_Trajectory_Forecasting_ICCV_2025_paper.html": {
    "title": "ForeSight: Multi-View Streaming Joint Object Detection and Trajectory Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sandro Papais",
      "Letian Wang",
      "Brian Cheong",
      "Steven L. Waslander"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xiang_SG-LDM_Semantic-Guided_LiDAR_Generation_via_Latent-Aligned_Diffusion_ICCV_2025_paper.html": {
    "title": "SG-LDM: Semantic-Guided LiDAR Generation via Latent-Aligned Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengkang Xiang",
      "Zizhao Li",
      "Amir Khodabandeh",
      "Kourosh Khoshelham"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kang_Dynamic_Multi-Layer_Null_Space_Projection_for_Vision-Language_Continual_Learning_ICCV_2025_paper.html": {
    "title": "Dynamic Multi-Layer Null Space Projection for Vision-Language Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Borui Kang",
      "Lei Wang",
      "Zhiping Wu",
      "Tao Feng",
      "Yawen Li",
      "Yang Gao",
      "Wenbin Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_AFUNet_Cross-Iterative_Alignment-Fusion_Synergy_for_HDR_Reconstruction_via_Deep_Unfolding_ICCV_2025_paper.html": {
    "title": "AFUNet: Cross-Iterative Alignment-Fusion Synergy for HDR Reconstruction via Deep Unfolding Paradigm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyue Li",
      "Zhangkai Ni",
      "Wenhan Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Duan_DiT4SR_Taming_Diffusion_Transformer_for_Real-World_Image_Super-Resolution_ICCV_2025_paper.html": {
    "title": "DiT4SR: Taming Diffusion Transformer for Real-World Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheng-Peng Duan",
      "Jiawei Zhang",
      "Xin Jin",
      "Ziheng Zhang",
      "Zheng Xiong",
      "Dongqing Zou",
      "Jimmy S. Ren",
      "Chunle Guo",
      "Chongyi Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Balanced_Sharpness-Aware_Minimization_for_Imbalanced_Regression_ICCV_2025_paper.html": {
    "title": "Balanced Sharpness-Aware Minimization for Imbalanced Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yahao Liu",
      "Qin Wang",
      "Lixin Duan",
      "Wen Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lu_Dynamic-DINO_Fine-Grained_Mixture_of_Experts_Tuning_for_Real-time_Open-Vocabulary_Object_ICCV_2025_paper.html": {
    "title": "Dynamic-DINO: Fine-Grained Mixture of Experts Tuning for Real-time Open-Vocabulary Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yehao Lu",
      "Minghe Weng",
      "Zekang Xiao",
      "Rui Jiang",
      "Wei Su",
      "Guangcong Zheng",
      "Ping Lu",
      "Xi Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Fan_Video_Individual_Counting_for_Moving_Drones_ICCV_2025_paper.html": {
    "title": "Video Individual Counting for Moving Drones",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaowu Fan",
      "Jia Wan",
      "Tao Han",
      "Antoni B. Chan",
      "Andy J. Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Guan_BridgeDepth_Bridging_Monocular_and_Stereo_Reasoning_with_Latent_Alignment_ICCV_2025_paper.html": {
    "title": "BridgeDepth: Bridging Monocular and Stereo Reasoning with Latent Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tongfan Guan",
      "Jiaxin Guo",
      "Chen Wang",
      "Yun-Hui Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Burkhardt_SuperEvent_Cross-Modal_Learning_of_Event-based_Keypoint_Detection_for_SLAM_ICCV_2025_paper.html": {
    "title": "SuperEvent: Cross-Modal Learning of Event-based Keypoint Detection for SLAM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yannick Burkhardt",
      "Simon Schaefer",
      "Stefan Leutenegger"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zheng_ViLLa_Video_Reasoning_Segmentation_with_Large_Language_Model_ICCV_2025_paper.html": {
    "title": "ViLLa: Video Reasoning Segmentation with Large Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rongkun Zheng",
      "Lu Qi",
      "Xi Chen",
      "Yi Wang",
      "Kun Wang",
      "Hengshuang Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Bridging_Class_Imbalance_and_Partial_Labeling_via_Spectral-Balanced_Energy_Propagation_ICCV_2025_paper.html": {
    "title": "Bridging Class Imbalance and Partial Labeling via Spectral-Balanced Energy Propagation for Skeleton-based Action Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yandan Wang",
      "Chenqi Guo",
      "Yinglong Ma",
      "Jiangyan Chen",
      "Yuan Gao",
      "Weiming Dong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Fan_PRVQL_Progressive_Knowledge-guided_Refinement_for_Robust_Egocentric_Visual_Query_Localization_ICCV_2025_paper.html": {
    "title": "PRVQL: Progressive Knowledge-guided Refinement for Robust Egocentric Visual Query Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bing Fan",
      "Yunhe Feng",
      "Yapeng Tian",
      "James Chenhao Liang",
      "Yuewei Lin",
      "Yan Huang",
      "Heng Fan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Guo_ARIG_Autoregressive_Interactive_Head_Generation_for_Real-time_Conversations_ICCV_2025_paper.html": {
    "title": "ARIG: Autoregressive Interactive Head Generation for Real-time Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ying Guo",
      "Xi Liu",
      "Cheng Zhen",
      "Pengfei Yan",
      "Xiaoming Wei"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zheng_RARE_Refine_Any_Registration_of_Pairwise_Point_Clouds_via_Zero-Shot_ICCV_2025_paper.html": {
    "title": "RARE: Refine Any Registration of Pairwise Point Clouds via Zero-Shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengyu Zheng",
      "Jin Huang",
      "Honghua Chen",
      "Mingqiang Wei"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gauen_Bayesian-Inspired_Space-Time_Superpixels_ICCV_2025_paper.html": {
    "title": "Bayesian-Inspired Space-Time Superpixels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kent Gauen",
      "Stanley Chan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Isaac-Medina_FEVER-OOD_Free_Energy_Vulnerability_Elimination_for_Robust_Out-of-Distribution_Detection_ICCV_2025_paper.html": {
    "title": "FEVER-OOD: Free Energy Vulnerability Elimination for Robust Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brian K.S. Isaac-Medina",
      "Mauricio Che",
      "Yona Falinie A. Gaus",
      "Samet Akcay",
      "Toby P. Breckon"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_TRNAS_A_Training-Free_Robust_Neural_Architecture_Search_ICCV_2025_paper.html": {
    "title": "TRNAS: A Training-Free Robust Neural Architecture Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yeming Yang",
      "Qingling Zhu",
      "Jianping Luo",
      "Ka-Chun Wong",
      "Qiuzhen Lin",
      "Jianqiang Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lu_Hierarchical_Divide-and-Conquer_Grouping_for_Classification_Adaptation_of_Pre-Trained_Models_ICCV_2025_paper.html": {
    "title": "Hierarchical Divide-and-Conquer Grouping for Classification Adaptation of Pre-Trained Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziqian Lu",
      "Yunlong Yu",
      "Qinyue Tong",
      "Jun Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_Text-IRSTD_Leveraging_Semantic_Text_to_Promote_Infrared_Small_Target_Detection_ICCV_2025_paper.html": {
    "title": "Text-IRSTD: Leveraging Semantic Text to Promote Infrared Small Target Detection in Complex Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feng Huang",
      "Shuyuan Zheng",
      "Zhaobing Qiu",
      "Huanxian Liu",
      "Huanxin Bai",
      "Liqiong Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lu_IntrinsicControlNet_Cross-distribution_Image_Generation_with_Real_and_Unreal_ICCV_2025_paper.html": {
    "title": "IntrinsicControlNet: Cross-distribution Image Generation with Real and Unreal",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayuan Lu",
      "Rengan Xie",
      "Zixuan Xie",
      "Zhizhen Wu",
      "Dianbing Xi",
      "Qi Ye",
      "Rui Wang",
      "Hujun Bao",
      "Yuchi Huo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Hypergraph_Clustering_Network_with_Partial_Attribute_Imputation_ICCV_2025_paper.html": {
    "title": "Hypergraph Clustering Network with Partial Attribute Imputation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qianqian Wang",
      "Bowen Zhao",
      "Zhengming Ding",
      "Wei Feng",
      "Quanxue Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Flash-VStream_Efficient_Real-Time_Understanding_for_Long_Video_Streams_ICCV_2025_paper.html": {
    "title": "Flash-VStream: Efficient Real-Time Understanding for Long Video Streams",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoji Zhang",
      "Yiqin Wang",
      "Yansong Tang",
      "Yong Liu",
      "Jiashi Feng",
      "Xiaojie Jin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_RoboTron-Drive_All-in-One_Large_Multimodal_Model_for_Autonomous_Driving_ICCV_2025_paper.html": {
    "title": "RoboTron-Drive: All-in-One Large Multimodal Model for Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhijian Huang",
      "Chengjian Feng",
      "Feng Yan",
      "Baihui Xiao",
      "Zequn Jie",
      "Yujie Zhong",
      "Xiaodan Liang",
      "Lin Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Luo_Dual-Process_Image_Generation_ICCV_2025_paper.html": {
    "title": "Dual-Process Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Grace Luo",
      "Jonathan Granskog",
      "Aleksander Holynski",
      "Trevor Darrell"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lin_GroundFlow_A_Plug-in_Module_for_Temporal_Reasoning_on_3D_Point_ICCV_2025_paper.html": {
    "title": "GroundFlow: A Plug-in Module for Temporal Reasoning on 3D Point Cloud Sequential Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijun Lin",
      "Shuting He",
      "Cheston Tan",
      "Bihan Wen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Long-Tailed_Classification_with_Multi-Granularity_Semantics_ICCV_2025_paper.html": {
    "title": "Long-Tailed Classification with Multi-Granularity Semantics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuting Liu",
      "Liu Yang",
      "Yu Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xu_DAA_Deep_Angular_A_Star_for_Image-based_Path_Planning_ICCV_2025_paper.html": {
    "title": "DAA*: Deep Angular A Star for Image-based Path Planning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiwei Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wei_TrackVerse_A_Large-Scale_Object-Centric_Video_Dataset_for_Image-Level_Representation_Learning_ICCV_2025_paper.html": {
    "title": "TrackVerse: A Large-Scale Object-Centric Video Dataset for Image-Level Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yibing Wei",
      "Samuel Church",
      "Victor Suciu",
      "Jinhong Lin",
      "Cheng-En Wu",
      "Pedro Morgado"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liang_Perspective-Invariant_3D_Object_Detection_ICCV_2025_paper.html": {
    "title": "Perspective-Invariant 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ao Liang",
      "Lingdong Kong",
      "Dongyue Lu",
      "Youquan Liu",
      "Jian Fang",
      "Huaici Zhao",
      "Wei Tsang Ooi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ziwen_Long-LRM_Long-sequence_Large_Reconstruction_Model_for_Wide-coverage_Gaussian_Splats_ICCV_2025_paper.html": {
    "title": "Long-LRM: Long-sequence Large Reconstruction Model for Wide-coverage Gaussian Splats",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Ziwen",
      "Hao Tan",
      "Kai Zhang",
      "Sai Bi",
      "Fujun Luan",
      "Yicong Hong",
      "Li Fuxin",
      "Zexiang Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Han_Unlearning_the_Noisy_Correspondence_Makes_CLIP_More_Robust_ICCV_2025_paper.html": {
    "title": "Unlearning the Noisy Correspondence Makes CLIP More Robust",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haochen Han",
      "Alex Jinpeng Wang",
      "Peijun Ye",
      "Fangming Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_HumanSAM_Classifying_Human-centric_Forgery_Videos_in_Human_Spatial_Appearance_and_ICCV_2025_paper.html": {
    "title": "HumanSAM: Classifying Human-centric Forgery Videos in Human Spatial, Appearance, and Motion Anomaly",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chang Liu",
      "Yunfan Ye",
      "Fan Zhang",
      "Qingyang Zhou",
      "Yuchuan Luo",
      "Zhiping Cai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_An_Information-Theoretic_Regularizer_for_Lossy_Neural_Image_Compression_ICCV_2025_paper.html": {
    "title": "An Information-Theoretic Regularizer for Lossy Neural Image Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingwen Zhang",
      "Meng Wang",
      "Xihua Sheng",
      "Peilin Chen",
      "Junru Li",
      "Li Zhang",
      "Shiqi Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tan_ReTracker_Exploring_Image_Matching_for_Robust_Online_Any_Point_Tracking_ICCV_2025_paper.html": {
    "title": "ReTracker: Exploring Image Matching for Robust Online Any Point Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongli Tan",
      "Xingyi He",
      "Sida Peng",
      "Yiqing Gong",
      "Xing Zhu",
      "Jiaming Sun",
      "Ruizhen Hu",
      "Yujun Shen",
      "Hujun Bao",
      "Xiaowei Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_DC-AE_1.5_Accelerating_Diffusion_Model_Convergence_with_Structured_Latent_Space_ICCV_2025_paper.html": {
    "title": "DC-AE 1.5: Accelerating Diffusion Model Convergence with Structured Latent Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyu Chen",
      "Dongyun Zou",
      "Wenkun He",
      "Junsong Chen",
      "Enze Xie",
      "Song Han",
      "Han Cai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_V2M4_4D_Mesh_Animation_Reconstruction_from_a_Single_Monocular_Video_ICCV_2025_paper.html": {
    "title": "V2M4: 4D Mesh Animation Reconstruction from a Single Monocular Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianqi Chen",
      "Biao Zhang",
      "Xiangjun Tang",
      "Peter Wonka"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ahmed_MixA_A_Mixed_Attention_approach_with_Stable_Lightweight_Linear_Attention_ICCV_2025_paper.html": {
    "title": "MixA: A Mixed Attention approach with Stable Lightweight Linear Attention to enhance Efficiency of Vision Transformers at the Edge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sabbir Ahmed",
      "Jingtao Li",
      "Weiming Zhuang",
      "Chen Chen",
      "Lingjuan Lyu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Peng_MUSE_Multi-Subject_Unified_Synthesis_via_Explicit_Layout_Semantic_Expansion_ICCV_2025_paper.html": {
    "title": "MUSE: Multi-Subject Unified Synthesis via Explicit Layout Semantic Expansion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fei Peng",
      "Junqiang Wu",
      "Yan Li",
      "Tingting Gao",
      "Di Zhang",
      "Huiyuan Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Pei_OpenSubstance_A_High-quality_Measured_Dataset_of_Multi-View_and_-Lighting_Images_ICCV_2025_paper.html": {
    "title": "OpenSubstance: A High-quality Measured Dataset of Multi-View and -Lighting Images and Shapes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan Pei",
      "Jinchen Bai",
      "Xiang Feng",
      "Zoubin Bi",
      "Kun Zhou",
      "Hongzhi Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Overcoming_Dual_Drift_for_Continual_Long-Tailed_Visual_Question_Answering_ICCV_2025_paper.html": {
    "title": "Overcoming Dual Drift for Continual Long-Tailed Visual Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feifei Zhang",
      "Zhihao Wang",
      "Xi Zhang",
      "Changsheng Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_LIRA_Inferring_Segmentation_in_Large_Multi-modal_Models_with_Local_Interleaved_ICCV_2025_paper.html": {
    "title": "LIRA: Inferring Segmentation in Large Multi-modal Models with Local Interleaved Region Assistance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhang Li",
      "Biao Yang",
      "Qiang Liu",
      "Shuo Zhang",
      "Zhiyin Ma",
      "Liang Yin",
      "Linger Deng",
      "Yabo Sun",
      "Yuliang Liu",
      "Xiang Bai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_GARF_Learning_Generalizable_3D_Reassembly_for_Real-World_Fractures_ICCV_2025_paper.html": {
    "title": "GARF: Learning Generalizable 3D Reassembly for Real-World Fractures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sihang Li",
      "Zeyu Jiang",
      "Grace Chen",
      "Chenyang Xu",
      "Siqi Tan",
      "Xue Wang",
      "Irving Fang",
      "Kristof Zyskowski",
      "Shannon P. McPherron",
      "Radu Iovita",
      "Chen Feng",
      "Jing Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Fan_Adapting_In-Domain_Few-Shot_Segmentation_to_New_Domains_without_Source_Domain_ICCV_2025_paper.html": {
    "title": "Adapting In-Domain Few-Shot Segmentation to New Domains without Source Domain Retraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Fan",
      "Kaiqi Liu",
      "Nian Liu",
      "Hisham Cholakkal",
      "Rao Muhammad Anwer",
      "Wenbin Li",
      "Yang Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Exploring_the_Adversarial_Vulnerabilities_of_Vision-Language-Action_Models_in_Robotics_ICCV_2025_paper.html": {
    "title": "Exploring the Adversarial Vulnerabilities of Vision-Language-Action Models in Robotics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taowen Wang",
      "Cheng Han",
      "James Liang",
      "Wenhao Yang",
      "Dongfang Liu",
      "Luna Xinyu Zhang",
      "Qifan Wang",
      "Jiebo Luo",
      "Ruixiang Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Fan_Scaling_Language-Free_Visual_Representation_Learning_ICCV_2025_paper.html": {
    "title": "Scaling Language-Free Visual Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Fan",
      "Shengbang Tong",
      "Jiachen Zhu",
      "Koustuv Sinha",
      "Zhuang Liu",
      "Xinlei Chen",
      "Michael Rabbat",
      "Nicolas Ballas",
      "Yann LeCun",
      "Amir Bar",
      "Saining Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Su_HUG_Hierarchical_Urban_Gaussian_Splatting_with_Block-Based_Reconstruction_for_Large-Scale_ICCV_2025_paper.html": {
    "title": "HUG: Hierarchical Urban Gaussian Splatting with Block-Based Reconstruction for Large-Scale Aerial Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mai Su",
      "Zhongtao Wang",
      "Huishan Au",
      "Yilong Li",
      "Xizhe Cao",
      "Chengwei Pan",
      "Yisong Chen",
      "Guoping Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lee_ESSENTIAL_Episodic_and_Semantic_Memory_Integration_for_Video_Class-Incremental_Learning_ICCV_2025_paper.html": {
    "title": "ESSENTIAL: Episodic and Semantic Memory Integration for Video Class-Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jongseo Lee",
      "Kyungho Bae",
      "Kyle Min",
      "Gyeong-Moon Park",
      "Jinwoo Choi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhu_Move_to_Understand_a_3D_Scene_Bridging_Visual_Grounding_and_ICCV_2025_paper.html": {
    "title": "Move to Understand a 3D Scene: Bridging Visual Grounding and Exploration for Efficient and Versatile Embodied Navigation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyu Zhu",
      "Xilin Wang",
      "Yixuan Li",
      "Zhuofan Zhang",
      "Xiaojian Ma",
      "Yixin Chen",
      "Baoxiong Jia",
      "Wei Liang",
      "Qian Yu",
      "Zhidong Deng",
      "Siyuan Huang",
      "Qing Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xia_Exploring_The_Visual_Feature_Space_for_Multimodal_Neural_Decoding_ICCV_2025_paper.html": {
    "title": "Exploring The Visual Feature Space for Multimodal Neural Decoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weihao Xia",
      "Cengiz Oztireli"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lin_INS-MMBench_A_Comprehensive_Benchmark_for_Evaluating_LVLMs_Performance_in_Insurance_ICCV_2025_paper.html": {
    "title": "INS-MMBench: A Comprehensive Benchmark for Evaluating LVLMs' Performance in Insurance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenwei Lin",
      "Hanjia Lyu",
      "Xian Xu",
      "Jiebo Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liao_Continual_Personalization_for_Diffusion_Models_ICCV_2025_paper.html": {
    "title": "Continual Personalization for Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu-Chien Liao",
      "Jr-Jen Chen",
      "Chi-Pin Huang",
      "Ci-Siang Lin",
      "Meng-Lin Wu",
      "Yu-Chiang Frank Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Fang_Creation-MMBench_Assessing_Context-Aware_Creative_Intelligence_in_MLLMs_ICCV_2025_paper.html": {
    "title": "Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyu Fang",
      "Zhijian Chen",
      "Kai Lan",
      "Lixin Ma",
      "Shengyuan Ding",
      "Yingji Liang",
      "Xiangyu Zhao",
      "Farong Wen",
      "Zicheng Zhang",
      "Guofeng Zhang",
      "Haodong Duan",
      "Kai Chen",
      "Dahua Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shin_Seam360GS_Seamless_360deg_Gaussian_Splatting_from_Real-World_Omnidirectional_Images_ICCV_2025_paper.html": {
    "title": "Seam360GS: Seamless 360deg Gaussian Splatting from Real-World Omnidirectional Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changha Shin",
      "Woong Oh Cho",
      "Seon Joo Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Peng_Boosting_Adversarial_Transferability_via_Residual_Perturbation_Attack_ICCV_2025_paper.html": {
    "title": "Boosting Adversarial Transferability via Residual Perturbation Attack",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinjia Peng",
      "Zeze Tao",
      "Huibing Wang",
      "Meng Wang",
      "Yang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ying_LDPose_Towards_Inclusive_Human_Pose_Estimation_for_Limb-Deficient_Individuals_in_ICCV_2025_paper.html": {
    "title": "LDPose: Towards Inclusive Human Pose Estimation for Limb-Deficient Individuals in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaying Ying",
      "Heming Du",
      "Kaihao Zhang",
      "Lincheng Li",
      "Xin Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Barsellotti_Talking_to_DINO_Bridging_Self-Supervised_Vision_Backbones_with_Language_for_ICCV_2025_paper.html": {
    "title": "Talking to DINO: Bridging Self-Supervised Vision Backbones with Language for Open-Vocabulary Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luca Barsellotti",
      "Lorenzo Bianchi",
      "Nicola Messina",
      "Fabio Carrara",
      "Marcella Cornia",
      "Lorenzo Baraldi",
      "Fabrizio Falchi",
      "Rita Cucchiara"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Language-Driven_Multi-Label_Zero-Shot_Learning_with_Semantic_Granularity_ICCV_2025_paper.html": {
    "title": "Language-Driven Multi-Label Zero-Shot Learning with Semantic Granularity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shouwen Wang",
      "Qian Wan",
      "Junbin Gao",
      "Zhigang Zeng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Regensky_Beyond_Perspective_Neural_360-Degree_Video_Compression_ICCV_2025_paper.html": {
    "title": "Beyond Perspective: Neural 360-Degree Video Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andy Regensky",
      "Marc Windsheimer",
      "Fabian Brand",
      "Andre Kaup"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jin_Differentiable_Room_Acoustic_Rendering_with_Multi-View_Vision_Priors_ICCV_2025_paper.html": {
    "title": "Differentiable Room Acoustic Rendering with Multi-View Vision Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Derong Jin",
      "Ruohan Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ma_An_Inversion-based_Measure_of_Memorization_for_Diffusion_Models_ICCV_2025_paper.html": {
    "title": "An Inversion-based Measure of Memorization for Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhe Ma",
      "Qingming Li",
      "Xuhong Zhang",
      "Tianyu Du",
      "Ruixiao Lin",
      "Zonghui Wang",
      "Shouling Ji",
      "Wenzhi Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shinoda_AgroBench_Vision-Language_Model_Benchmark_in_Agriculture_ICCV_2025_paper.html": {
    "title": "AgroBench: Vision-Language Model Benchmark in Agriculture",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Risa Shinoda",
      "Nakamasa Inoue",
      "Hirokatsu Kataoka",
      "Masaki Onishi",
      "Yoshitaka Ushiku"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_ROAR_Reducing_Inversion_Error_in_Generative_Image_Watermarking_ICCV_2025_paper.html": {
    "title": "ROAR: Reducing Inversion Error in Generative Image Watermarking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanyi Wang",
      "Han Fang",
      "Shi-Lin Wang",
      "Ee-Chien Chang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Luo_Adding_Additional_Control_to_One-Step_Diffusion_with_Joint_Distribution_Matching_ICCV_2025_paper.html": {
    "title": "Adding Additional Control to One-Step Diffusion with Joint Distribution Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihong Luo",
      "Tianyang Hu",
      "Yifan Song",
      "Jiacheng Sun",
      "Zhenguo Li",
      "Jing Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Su_TimeBooth_Disentangled_Facial_Invariant_Representation_for_Diverse_and_Personalized_Face_ICCV_2025_paper.html": {
    "title": "TimeBooth: Disentangled Facial Invariant Representation for Diverse and Personalized Face Aging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zepeng Su",
      "Zhulin Liu",
      "Zongyan Zhang",
      "Tong Zhang",
      "C.L.Philip Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xie_Flexi-FSCIL_Adaptive_Knowledge_Retention_for_Breaking_the_Stability-Plasticity_Dilemma_in_ICCV_2025_paper.html": {
    "title": "Flexi-FSCIL: Adaptive Knowledge Retention for Breaking the Stability-Plasticity Dilemma in Few-Shot Class-Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wufei Xie",
      "Yalin Wang",
      "Chenliang Liu",
      "Zhaohui Jiang",
      "Xue Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Learning_Beyond_Still_Frames_Scaling_Vision-Language_Models_with_Video_ICCV_2025_paper.html": {
    "title": "Learning Beyond Still Frames: Scaling Vision-Language Models with Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiyuan Zhang",
      "Handong Li",
      "Jing Liu",
      "Xiangyu Yue"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sadikaj_MultiADS_Defect-aware_Supervision_for_Multi-type_Anomaly_Detection_and_Segmentation_in_ICCV_2025_paper.html": {
    "title": "MultiADS: Defect-aware Supervision for Multi-type Anomaly Detection and Segmentation in Zero-Shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ylli Sadikaj",
      "Hongkuan Zhou",
      "Lavdim Halilaj",
      "Stefan Schmid",
      "Steffen Staab",
      "Claudia Plant"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wen_SEGS-SLAM_Structure-enhanced_3D_Gaussian_Splatting_SLAM_with_Appearance_Embedding_ICCV_2025_paper.html": {
    "title": "SEGS-SLAM: Structure-enhanced 3D Gaussian Splatting SLAM with Appearance Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianci Wen",
      "Zhiang Liu",
      "Yongchun Fang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Images_as_Noisy_Labels_Unleashing_the_Potential_of_the_Diffusion_ICCV_2025_paper.html": {
    "title": "Images as Noisy Labels: Unleashing the Potential of the Diffusion Model for Open-Vocabulary Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan Li",
      "Xuanbin Wang",
      "Xuan Wang",
      "Zhaoxiang Zhang",
      "Yuelei Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Tile-wise_vs._Image-wise_Random-Tile_Loss_and_Training_Paradigm_for_Gaussian_ICCV_2025_paper.html": {
    "title": "Tile-wise vs. Image-wise: Random-Tile Loss and Training Paradigm for Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyu Zhang",
      "Weihong Pan",
      "Xiaojun Xiang",
      "Hongjia Zhai",
      "Liyang Zhou",
      "Hanqing Jiang",
      "Guofeng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Paul_How_To_Make_Your_Cell_Tracker_Say_I_dunno_ICCV_2025_paper.html": {
    "title": "How To Make Your Cell Tracker Say \"I dunno!",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Richard D. Paul",
      "Johannes Seiffarth",
      "David Rügamer",
      "Katharina Nöh",
      "Hanno Scharr"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xia_Training-free_and_Adaptive_Sparse_Attention_for_Efficient_Long_Video_Generation_ICCV_2025_paper.html": {
    "title": "Training-free and Adaptive Sparse Attention for Efficient Long Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei Xia",
      "Suhan Ling",
      "Fangcheng Fu",
      "Yujie Wang",
      "Huixia Li",
      "Xuefeng Xiao",
      "Bin Cui"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Swanson_Super_Resolved_Imaging_with_Adaptive_Optics_ICCV_2025_paper.html": {
    "title": "Super Resolved Imaging with Adaptive Optics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Robin Swanson",
      "Esther Y. H. Lin",
      "Masen Lamb",
      "Suresh Sivanandam",
      "Kiriakos N. Kutulakos"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Svitov_BillBoard_Splatting_BBSplat_Learnable_Textured_Primitives_for_Novel_View_Synthesis_ICCV_2025_paper.html": {
    "title": "BillBoard Splatting (BBSplat): Learnable Textured Primitives for Novel View Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Svitov",
      "Pietro Morerio",
      "Lourdes Agapito",
      "Alessio Del Bue"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Antic_SDFit_3D_Object_Pose_and_Shape_by_Fitting_a_Morphable_ICCV_2025_paper.html": {
    "title": "SDFit: 3D Object Pose and Shape by Fitting a Morphable SDF to a Single Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dimitrije Antić",
      "Georgios Paschalidis",
      "Shashank Tripathi",
      "Theo Gevers",
      "Sai Kumar Dwivedi",
      "Dimitrios Tzionas"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_VRM_Knowledge_Distillation_via_Virtual_Relation_Matching_ICCV_2025_paper.html": {
    "title": "VRM: Knowledge Distillation via Virtual Relation Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weijia Zhang",
      "Fei Xie",
      "Weidong Cai",
      "Chao Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Park_SC-Lane_Slope-aware_and_Consistent_Road_Height_Estimation_Framework_for_3D_ICCV_2025_paper.html": {
    "title": "SC-Lane: Slope-aware and Consistent Road Height Estimation Framework for 3D Lane Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaesong Park",
      "Eunbin Seo",
      "Jihyeon Hwang",
      "Jongwoo Lim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cai_Parametric_Shadow_Control_for_Portrait_Generation_in_Text-to-Image_Diffusion_Models_ICCV_2025_paper.html": {
    "title": "Parametric Shadow Control for Portrait Generation in Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoming Cai",
      "Tsung-Wei Huang",
      "Shiv Gehlot",
      "Brandon Y. Feng",
      "Sachin Shah",
      "Guan-Ming Su",
      "Christopher Metzler"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yook_ZIUM_Zero-Shot_Intent-Aware_Adversarial_Attack_on_Unlearned_Models_ICCV_2025_paper.html": {
    "title": "ZIUM: Zero-Shot Intent-Aware Adversarial Attack on Unlearned Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyun Jun Yook",
      "Ga San Jhun",
      "Jae Hyun Cho",
      "Min Jeon",
      "Donghyun Kim",
      "Tae Hyung Kim",
      "Youn Kyu Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ren_Beyond_Next-Token_Next-X_Prediction_for_Autoregressive_Visual_Generation_ICCV_2025_paper.html": {
    "title": "Beyond Next-Token: Next-X Prediction for Autoregressive Visual Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sucheng Ren",
      "Qihang Yu",
      "Ju He",
      "Xiaohui Shen",
      "Alan Yuille",
      "Liang-Chieh Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hartwig_GECO_Geometrically_Consistent_Embedding_with_Lightspeed_Inference_ICCV_2025_paper.html": {
    "title": "GECO: Geometrically Consistent Embedding with Lightspeed Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Regine Hartwig",
      "Dominik Muhle",
      "Riccardo Marin",
      "Daniel Cremers"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Viola_Marigold-DC_Zero-Shot_Monocular_Depth_Completion_with_Guided_Diffusion_ICCV_2025_paper.html": {
    "title": "Marigold-DC: Zero-Shot Monocular Depth Completion with Guided Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Massimiliano Viola",
      "Kevin Qu",
      "Nando Metzger",
      "Bingxin Ke",
      "Alexander Becker",
      "Konrad Schindler",
      "Anton Obukhov"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Mao_MedSegFactory_Text-Guided_Generation_of_Medical_Image-Mask_Pairs_ICCV_2025_paper.html": {
    "title": "MedSegFactory: Text-Guided Generation of Medical Image-Mask Pairs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawei Mao",
      "Yuhan Wang",
      "Yucheng Tang",
      "Daguang Xu",
      "Kang Wang",
      "Yang Yang",
      "Zongwei Zhou",
      "Yuyin Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_TurboTrain_Towards_Efficient_and_Balanced_Multi-Task_Learning_for_Multi-Agent_Perception_ICCV_2025_paper.html": {
    "title": "TurboTrain: Towards Efficient and Balanced Multi-Task Learning for Multi-Agent Perception and Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zewei Zhou",
      "Seth Z. Zhao",
      "Tianhui Cai",
      "Zhiyu Huang",
      "Bolei Zhou",
      "Jiaqi Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_FiVE-Bench_A_Fine-grained_Video_Editing_Benchmark_for_Evaluating_Emerging_Diffusion_ICCV_2025_paper.html": {
    "title": "FiVE-Bench: A Fine-grained Video Editing Benchmark for Evaluating Emerging Diffusion and Rectified Flow Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minghan Li",
      "Chenxi Xie",
      "Yichen Wu",
      "Lei Zhang",
      "Mengyu Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lu_ReAL-AD_Towards_Human-Like_Reasoning_in_End-to-End_Autonomous_Driving_ICCV_2025_paper.html": {
    "title": "ReAL-AD: Towards Human-Like Reasoning in End-to-End Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhang Lu",
      "Jiadong Tu",
      "Yuexin Ma",
      "Xinge Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xuan_ReME_A_Data-Centric_Framework_for_Training-Free_Open-Vocabulary_Segmentation_ICCV_2025_paper.html": {
    "title": "ReME: A Data-Centric Framework for Training-Free Open-Vocabulary Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiwei Xuan",
      "Ziquan Deng",
      "Kwan-Liu Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Sparse_Fine-Tuning_of_Transformers_for_Generative_Tasks_ICCV_2025_paper.html": {
    "title": "Sparse Fine-Tuning of Transformers for Generative Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Chen",
      "Jingxi Yu",
      "Zichen Miao",
      "Qiang Qiu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cao_Unlocking_Constraints_Source-Free_Occlusion-Aware_Seamless_Segmentation_ICCV_2025_paper.html": {
    "title": "Unlocking Constraints: Source-Free Occlusion-Aware Seamless Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihong Cao",
      "Jiaming Zhang",
      "Xu Zheng",
      "Hao Shi",
      "Kunyu Peng",
      "Hang Liu",
      "Kailun Yang",
      "Hui Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jia_A_Visual_Leap_in_CLIP_Compositionality_Reasoning_through_Generation_of_ICCV_2025_paper.html": {
    "title": "A Visual Leap in CLIP Compositionality Reasoning through Generation of Counterfactual Sets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zexi Jia",
      "Chuanwei Huang",
      "Hongyan Fei",
      "Yeshuang Zhu",
      "Zhiqiang Yuan",
      "Ying Deng",
      "Jiapei Zhang",
      "Jinchao Zhang",
      "Jie Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Long_Boosting_Adversarial_Transferability_via_Negative_Hessian_Trace_Regularization_ICCV_2025_paper.html": {
    "title": "Boosting Adversarial Transferability via Negative Hessian Trace Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunfei Long",
      "Zilin Tian",
      "Liguo Zhang",
      "Huosheng Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_AutoOcc_Automatic_Open-Ended_Semantic_Occupancy_Annotation_via_Vision-Language_Guided_Gaussian_ICCV_2025_paper.html": {
    "title": "AutoOcc: Automatic Open-Ended Semantic Occupancy Annotation via Vision-Language Guided Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyu Zhou",
      "Jingqi Wang",
      "Yongtao Wang",
      "Yufei Wei",
      "Nan Dong",
      "Ming-Hsuan Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Fei_Scalable_Dual_Fingerprinting_for_Hierarchical_Attribution_of_Text-to-Image_Models_ICCV_2025_paper.html": {
    "title": "Scalable Dual Fingerprinting for Hierarchical Attribution of Text-to-Image Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianwei Fei",
      "Yunshu Dai",
      "Peipeng Yu",
      "Zhe Kong",
      "Jiantao Zhou",
      "Zhihua Xia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zheng_Improving_Noise_Efficiency_in_Privacy-preserving_Dataset_Distillation_ICCV_2025_paper.html": {
    "title": "Improving Noise Efficiency in Privacy-preserving Dataset Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runkai Zheng",
      "Vishnu Asutosh Dasu",
      "Yinong Oliver Wang",
      "Haohan Wang",
      "Fernando De La Torre"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Thoreau_Parameter-Efficient_Adaptation_of_Geospatial_Foundation_Models_through_Embedding_Deflection_ICCV_2025_paper.html": {
    "title": "Parameter-Efficient Adaptation of Geospatial Foundation Models through Embedding Deflection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Romain Thoreau",
      "Valerio Marsocci",
      "Dawa Derksen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Su_UniFuse_A_Unified_All-in-One_Framework_for_Multi-Modal_Medical_Image_Fusion_ICCV_2025_paper.html": {
    "title": "UniFuse: A Unified All-in-One Framework for Multi-Modal Medical Image Fusion Under Diverse Degradations and Misalignments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dayong Su",
      "Yafei Zhang",
      "Huafeng Li",
      "Jinxing Li",
      "Yu Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Coupling_the_Generator_with_Teacher_for_Effective_Data-Free_Knowledge_Distillation_ICCV_2025_paper.html": {
    "title": "Coupling the Generator with Teacher for Effective Data-Free Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xu Chen",
      "Yang Li",
      "Yahong Han",
      "Guangquan Xu",
      "Jialie Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zameni_MOSCATO_Predicting_Multiple_Object_State_Change_Through_Actions_ICCV_2025_paper.html": {
    "title": "MOSCATO: Predicting Multiple Object State Change Through Actions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Parnian Zameni",
      "Yuhan Shen",
      "Ehsan Elhamifar"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kang_Harnessing_Input-Adaptive_Inference_for_Efficient_VLN_ICCV_2025_paper.html": {
    "title": "Harnessing Input-Adaptive Inference for Efficient VLN",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongwoo Kang",
      "Akhil Perincherry",
      "Zachary Coalson",
      "Aiden Gabriel",
      "Stefan Lee",
      "Sanghyun Hong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_LoftUp_Learning_a_Coordinate-Based_Feature_Upsampler_for_Vision_Foundation_Models_ICCV_2025_paper.html": {
    "title": "LoftUp: Learning a Coordinate-Based Feature Upsampler for Vision Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiwen Huang",
      "Anpei Chen",
      "Volodymyr Havrylov",
      "Andreas Geiger",
      "Dan Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_EMatch_A_Unified_Framework_for_Event-based_Optical_Flow_and_Stereo_ICCV_2025_paper.html": {
    "title": "EMatch: A Unified Framework for Event-based Optical Flow and Stereo Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengjie Zhang",
      "Lin Zhu",
      "Xiao Wang",
      "Lizhi Wang",
      "Hua Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Garcia-Fernandez_Superpowering_Open-Vocabulary_Object_Detectors_for_X-ray_Vision_ICCV_2025_paper.html": {
    "title": "Superpowering Open-Vocabulary Object Detectors for X-ray Vision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pablo Garcia-Fernandez",
      "Lorenzo Vaquero",
      "Mingxuan Liu",
      "Feng Xue",
      "Daniel Cores",
      "Nicu Sebe",
      "Manuel Mucientes",
      "Elisa Ricci"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_LVBench_An_Extreme_Long_Video_Understanding_Benchmark_ICCV_2025_paper.html": {
    "title": "LVBench: An Extreme Long Video Understanding Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weihan Wang",
      "Zehai He",
      "Wenyi Hong",
      "Yean Cheng",
      "Xiaohan Zhang",
      "Ji Qi",
      "Ming Ding",
      "Xiaotao Gu",
      "Shiyu Huang",
      "Bin Xu",
      "Yuxiao Dong",
      "Jie Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shi_Scalable_Image_Tokenization_with_Index_Backpropagation_Quantization_ICCV_2025_paper.html": {
    "title": "Scalable Image Tokenization with Index Backpropagation Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fengyuan Shi",
      "Zhuoyan Luo",
      "Yixiao Ge",
      "Yujiu Yang",
      "Ying Shan",
      "Limin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_InsViE-1M_Effective_Instruction-based_Video_Editing_with_Elaborate_Dataset_Construction_ICCV_2025_paper.html": {
    "title": "InsViE-1M: Effective Instruction-based Video Editing with Elaborate Dataset Construction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhui Wu",
      "Liyi Chen",
      "Ruibin Li",
      "Shihao Wang",
      "Chenxi Xie",
      "Lei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_SpatialCrafter_Unleashing_the_Imagination_of_Video_Diffusion_Models_for_Scene_ICCV_2025_paper.html": {
    "title": "SpatialCrafter: Unleashing the Imagination of Video Diffusion Models for Scene Reconstruction from Limited Observations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Songchun Zhang",
      "Huiyao Xu",
      "Sitong Guo",
      "Zhongwei Xie",
      "Hujun Bao",
      "Weiwei Xu",
      "Changqing Zou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hyun_Multi-Granular_Spatio-Temporal_Token_Merging_for_Training-Free_Acceleration_of_Video_LLMs_ICCV_2025_paper.html": {
    "title": "Multi-Granular Spatio-Temporal Token Merging for Training-Free Acceleration of Video LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeongseok Hyun",
      "Sukjun Hwang",
      "Su Ho Han",
      "Taeoh Kim",
      "Inwoong Lee",
      "Dongyoon Wee",
      "Joon-Young Lee",
      "Seon Joo Kim",
      "Minho Shim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wan_ONLY_One-Layer_Intervention_Sufficiently_Mitigates_Hallucinations_in_Large_Vision-Language_Models_ICCV_2025_paper.html": {
    "title": "ONLY: One-Layer Intervention Sufficiently Mitigates Hallucinations in Large Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zifu Wan",
      "Ce Zhang",
      "Silong Yong",
      "Martin Q. Ma",
      "Simon Stepputtis",
      "Louis-Philippe Morency",
      "Deva Ramanan",
      "Katia Sycara",
      "Yaqi Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jung_Inverse_Image-Based_Rendering_for_Light_Field_Generation_from_Single_Images_ICCV_2025_paper.html": {
    "title": "Inverse Image-Based Rendering for Light Field Generation from Single Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyunjun Jung",
      "Hae-Gon Jeon"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Morph_A_Motion-free_Physics_Optimization_Framework_for_Human_Motion_Generation_ICCV_2025_paper.html": {
    "title": "Morph: A Motion-free Physics Optimization Framework for Human Motion Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuo Li",
      "Mingshuang Luo",
      "Ruibing Hou",
      "Xin Zhao",
      "Hao Liu",
      "Hong Chang",
      "Zimo Liu",
      "Chen Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Generalized_and_Efficient_2D_Gaussian_Splatting_for_Arbitrary-scale_Super-Resolution_ICCV_2025_paper.html": {
    "title": "Generalized and Efficient 2D Gaussian Splatting for Arbitrary-scale Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Du Chen",
      "Liyi Chen",
      "Zhengqiang Zhang",
      "Lei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lin_Pretend_Benign_A_Stealthy_Adversarial_Attack_by_Exploiting_Vulnerabilities_in_ICCV_2025_paper.html": {
    "title": "Pretend Benign: A Stealthy Adversarial Attack by Exploiting Vulnerabilities in Cooperative Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongwei Lin",
      "Dongyu Pan",
      "Qiming Xia",
      "Hai Wu",
      "Cheng Wang",
      "Siqi Shen",
      "Chenglu Wen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gao_ProbMED_A_Probabilistic_Framework_for_Medical_Multimodal_Binding_ICCV_2025_paper.html": {
    "title": "ProbMED: A Probabilistic Framework for Medical Multimodal Binding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Gao",
      "Sangwook Kim",
      "Jianzhong You",
      "Chris McIntosh"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ahmed_Kestrel_3D_Multimodal_LLM_for_Part-Aware_Grounded_Description_ICCV_2025_paper.html": {
    "title": "Kestrel: 3D Multimodal LLM for Part-Aware Grounded Description",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mahmoud Ahmed",
      "Junjie Fei",
      "Jian Ding",
      "Eslam Mohamed Bakr",
      "Mohamed Elhoseiny"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Precise_Action-to-Video_Generation_Through_Visual_Action_Prompts_ICCV_2025_paper.html": {
    "title": "Precise Action-to-Video Generation Through Visual Action Prompts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuang Wang",
      "Chao Wen",
      "Haoyu Guo",
      "Sida Peng",
      "Minghan Qin",
      "Hujun Bao",
      "Xiaowei Zhou",
      "Ruizhen Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_QuickSplat_Fast_3D_Surface_Reconstruction_via_Learned_Gaussian_Initialization_ICCV_2025_paper.html": {
    "title": "QuickSplat: Fast 3D Surface Reconstruction via Learned Gaussian Initialization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yueh-Cheng Liu",
      "Lukas Höllein",
      "Matthias Nießner",
      "Angela Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Konstantinou_EquiCaps_Predictor-Free_Pose-Aware_Pre-Trained_Capsule_Networks_ICCV_2025_paper.html": {
    "title": "EquiCaps: Predictor-Free Pose-Aware Pre-Trained Capsule Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Athinoulla Konstantinou",
      "Georgios Leontidis",
      "Mamatha Thota",
      "Aiden Durrant"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xue_Feature_Decomposition-Recomposition_in_Large_Vision-Language_Model_for_Few-Shot_Class-Incremental_Learning_ICCV_2025_paper.html": {
    "title": "Feature Decomposition-Recomposition in Large Vision-Language Model for Few-Shot Class-Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zongyao Xue",
      "Meina Kan",
      "Shiguang Shan",
      "Xilin Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cao_Taming_Flow_Matching_with_Unbalanced_Optimal_Transport_into_Fast_Pansharpening_ICCV_2025_paper.html": {
    "title": "Taming Flow Matching with Unbalanced Optimal Transport into Fast Pansharpening",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihan Cao",
      "Yu Zhong",
      "Liang-Jian Deng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Patel_Learning_to_Unlearn_while_Retaining_Combating_Gradient_Conflicts_in_Machine_ICCV_2025_paper.html": {
    "title": "Learning to Unlearn while Retaining: Combating Gradient Conflicts in Machine Unlearning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gaurav Patel",
      "Qiang Qiu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_Scaling_and_Taming_Adversarial_Training_with_Synthetic_Data_ICCV_2025_paper.html": {
    "title": "Scaling and Taming Adversarial Training with Synthetic Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juntao Wu",
      "Xianting Huang",
      "Yu Chen",
      "Shuai Pang",
      "Ke Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_FramePainter_Endowing_Interactive_Image_Editing_with_Video_Diffusion_Priors_ICCV_2025_paper.html": {
    "title": "FramePainter: Endowing Interactive Image Editing with Video Diffusion Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yabo Zhang",
      "Xinpeng Zhou",
      "Yihan Zeng",
      "Hang Xu",
      "Hui Li",
      "Wangmeng Zuo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_MetaScope_Optics-Driven_Neural_Network_for_Ultra-Micro_Metalens_Endoscopy_ICCV_2025_paper.html": {
    "title": "MetaScope: Optics-Driven Neural Network for Ultra-Micro Metalens Endoscopy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wuyang Li",
      "Wentao Pan",
      "Xiaoyuan Liu",
      "Zhendong Luo",
      "Chenxin Li",
      "Hengyu Liu",
      "Din Ping Tsai",
      "Mu Ku Chen",
      "Yixuan Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kiefhaber_Removing_Cost_Volumes_from_Optical_Flow_Estimators_ICCV_2025_paper.html": {
    "title": "Removing Cost Volumes from Optical Flow Estimators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simon Kiefhaber",
      "Stefan Roth",
      "Simone Schaub-Meyer"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Han_SparseRecon_Neural_Implicit_Surface_Reconstruction_from_Sparse_Views_with_Feature_ICCV_2025_paper.html": {
    "title": "SparseRecon: Neural Implicit Surface Reconstruction from Sparse Views with Feature and Depth Consistencies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liang Han",
      "Xu Zhang",
      "Haichuan Song",
      "Kanle Shi",
      "Yu-Shen Liu",
      "Zhizhong Han"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_LMM4LMM_Benchmarking_and_Evaluating_Large-multimodal_Image_Generation_with_LMMs_ICCV_2025_paper.html": {
    "title": "LMM4LMM: Benchmarking and Evaluating Large-multimodal Image Generation with LMMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiarui Wang",
      "Huiyu Duan",
      "Yu Zhao",
      "Juntong Wang",
      "Guangtao Zhai",
      "Xiongkuo Min"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_Leveraging_the_Power_of_MLLMs_for_Gloss-Free_Sign_Language_Translation_ICCV_2025_paper.html": {
    "title": "Leveraging the Power of MLLMs for Gloss-Free Sign Language Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jungeun Kim",
      "Hyeongwoo Jeon",
      "Jongseong Bae",
      "Ha Young Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Cross-modal_Ship_Re-Identification_via_Optical_and_SAR_Imagery_A_Novel_ICCV_2025_paper.html": {
    "title": "Cross-modal Ship Re-Identification via Optical and SAR Imagery: A Novel Dataset and Method",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Wang",
      "Shengyang Li",
      "Jian Yang",
      "Yuxuan Liu",
      "Yixuan Lv",
      "Zhuang Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Hybrid-grained_Feature_Aggregation_with_Coarse-to-fine_Language_Guidance_for_Self-supervised_Monocular_ICCV_2025_paper.html": {
    "title": "Hybrid-grained Feature Aggregation with Coarse-to-fine Language Guidance for Self-supervised Monocular Depth Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenyao Zhang",
      "Hongsi Liu",
      "Bohan Li",
      "Jiawei He",
      "Zekun Qi",
      "Yunnan Wang",
      "Shengyang Zhao",
      "Xinqiang Yu",
      "Wenjun Zeng",
      "Xin Jin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Aneja_GaussianSpeech_Audio-Driven_Personalized_3D_Gaussian_Avatars_ICCV_2025_paper.html": {
    "title": "GaussianSpeech: Audio-Driven Personalized 3D Gaussian Avatars",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shivangi Aneja",
      "Artem Sevastopolsky",
      "Tobias Kirschstein",
      "Justus Thies",
      "Angela Dai",
      "Matthias Nießner"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_MonoFusion_Sparse-View_4D_Reconstruction_via_Monocular_Fusion_ICCV_2025_paper.html": {
    "title": "MonoFusion: Sparse-View 4D Reconstruction via Monocular Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihan Wang",
      "Jeff Tan",
      "Tarasha Khurana",
      "Neehar Peri",
      "Deva Ramanan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_SIGMAN_Scaling_3D_Human_Gaussian_Generation_with_Millions_of_Assets_ICCV_2025_paper.html": {
    "title": "SIGMAN: Scaling 3D Human Gaussian Generation with Millions of Assets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhang Yang",
      "Fengqi Liu",
      "Yixing Lu",
      "Qin Zhao",
      "Pingyu Wu",
      "Wei Zhai",
      "Ran Yi",
      "Yang Cao",
      "Lizhuang Ma",
      "Zheng-Jun Zha",
      "Junting Dong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Decatur_Reusing_Computation_in_Text-to-Image_Diffusion_for_Efficient_Generation_of_Image_ICCV_2025_paper.html": {
    "title": "Reusing Computation in Text-to-Image Diffusion for Efficient Generation of Image Sets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dale Decatur",
      "Thibault Groueix",
      "Wang Yifan",
      "Rana Hanocka",
      "Vladimir Kim",
      "Matheus Gadelha"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hou_Dita_Scaling_Diffusion_Transformer_for_Generalist_Vision-Language-Action_Policy_ICCV_2025_paper.html": {
    "title": "Dita: Scaling Diffusion Transformer for Generalist Vision-Language-Action Policy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhi Hou",
      "Tianyi Zhang",
      "Yuwen Xiong",
      "Haonan Duan",
      "Hengjun Pu",
      "Ronglei Tong",
      "Chengyang Zhao",
      "Xizhou Zhu",
      "Yu Qiao",
      "Jifeng Dai",
      "Yuntao Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhao_Tree-NeRV_Efficient_Non-Uniform_Sampling_for_Neural_Video_Representation_via_Tree-Structured_ICCV_2025_paper.html": {
    "title": "Tree-NeRV: Efficient Non-Uniform Sampling for Neural Video Representation via Tree-Structured Feature Grids",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiancheng Zhao",
      "Yifan Zhan",
      "Qingtian Zhu",
      "Mingze Ma",
      "Muyao Niu",
      "Zunian Wan",
      "Xiang Ji",
      "Yinqiang Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_GLEAM_Enhanced_Transferable_Adversarial_Attacks_for_Vision-Language_Pre-training_Models_via_ICCV_2025_paper.html": {
    "title": "GLEAM: Enhanced Transferable Adversarial Attacks for Vision-Language Pre-training Models via Global-Local Transformations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunqi Liu",
      "Xue Ouyang",
      "Xiaohui Cui"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_UniMLVG_Unified_Framework_for_Multi-view_Long_Video_Generation_with_Comprehensive_ICCV_2025_paper.html": {
    "title": "UniMLVG: Unified Framework for Multi-view Long Video Generation with Comprehensive Control Capabilities for Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Chen",
      "Zehuan Wu",
      "Yichen Liu",
      "Yuxin Guo",
      "Jingcheng Ni",
      "Haifeng Xia",
      "Siyu Xia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Bae_CaliMatch_Adaptive_Calibration_for_Improving_Safe_Semi-supervised_Learning_ICCV_2025_paper.html": {
    "title": "CaliMatch: Adaptive Calibration for Improving Safe Semi-supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinsoo Bae",
      "Seoung Bum Kim",
      "Hyungrok Do"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_CogCM_Cognition-Inspired_Contextual_Modeling_for_Audio-Visual_Speech_Enhancement_ICCV_2025_paper.html": {
    "title": "CogCM: Cognition-Inspired Contextual Modeling for Audio-Visual Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feixiang Wang",
      "Shuang Yang",
      "Shiguang Shan",
      "Xilin Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Karthik_Scalable_Ranked_Preference_Optimization_for_Text-to-Image_Generation_ICCV_2025_paper.html": {
    "title": "Scalable Ranked Preference Optimization for Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shyamgopal Karthik",
      "Huseyin Coskun",
      "Zeynep Akata",
      "Sergey Tulyakov",
      "Jian Ren",
      "Anil Kag"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Aligning_Vision_to_Language_Annotation-Free_Multimodal_Knowledge_Graph_Construction_for_ICCV_2025_paper.html": {
    "title": "Aligning Vision to Language: Annotation-Free Multimodal Knowledge Graph Construction for Enhanced LLMs Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junming Liu",
      "Siyuan Meng",
      "Yanting Gao",
      "Song Mao",
      "Pinlong Cai",
      "Guohang Yan",
      "Yirong Chen",
      "Zilin Bian",
      "Ding Wang",
      "Botian Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_QR-LoRA_Efficient_and_Disentangled_Fine-tuning_via_QR_Decomposition_for_Customized_ICCV_2025_paper.html": {
    "title": "QR-LoRA: Efficient and Disentangled Fine-tuning via QR Decomposition for Customized Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahui Yang",
      "Yongjia Ma",
      "Donglin Di",
      "Jianxun Cui",
      "Hao Li",
      "Wei Chen",
      "Yan Xie",
      "Xun Yang",
      "Wangmeng Zuo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Bose_Uncertainty-Aware_Diffusion-Guided_Refinement_of_3D_Scenes_ICCV_2025_paper.html": {
    "title": "Uncertainty-Aware Diffusion-Guided Refinement of 3D Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sarosij Bose",
      "Arindam Dutta",
      "Sayak Nag",
      "Junge Zhang",
      "Jiachen Li",
      "Konstantinos Karydis",
      "Amit K. Roy-Chowdhury"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_GestureLSM_Latent_Shortcut_based_Co-Speech_Gesture_Generation_with_Spatial-Temporal_Modeling_ICCV_2025_paper.html": {
    "title": "GestureLSM: Latent Shortcut based Co-Speech Gesture Generation with Spatial-Temporal Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pinxin Liu",
      "Luchuan Song",
      "Junhua Huang",
      "Haiyang Liu",
      "Chenliang Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_LLaFEA_Frame-Event_Complementary_Fusion_for_Fine-Grained_Spatiotemporal_Understanding_in_LMMs_ICCV_2025_paper.html": {
    "title": "LLaFEA: Frame-Event Complementary Fusion for Fine-Grained Spatiotemporal Understanding in LMMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanyu Zhou",
      "Gim Hee Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/He_GCAV_A_Global_Concept_Activation_Vector_Framework_for_Cross-Layer_Consistency_ICCV_2025_paper.html": {
    "title": "GCAV: A Global Concept Activation Vector Framework for Cross-Layer Consistency in Interpretability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenghao He",
      "Sanchit Sinha",
      "Guangzhi Xiong",
      "Aidong Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lee_FastPoint_Accelerating_3D_Point_Cloud_Model_Inference_via_Sample_Point_ICCV_2025_paper.html": {
    "title": "FastPoint: Accelerating 3D Point Cloud Model Inference via Sample Point Distance Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Donghyun Lee",
      "Dawoon Jeong",
      "Jae W. Lee",
      "Hongil Yoon"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_GSRecon_Efficient_Generalizable_Gaussian_Splatting_for_Surface_Reconstruction_from_Sparse_ICCV_2025_paper.html": {
    "title": "GSRecon: Efficient Generalizable Gaussian Splatting for Surface Reconstruction from Sparse Views",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hang Yang",
      "Le Hui",
      "Jianjun Qian",
      "Jin Xie",
      "Jian Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Park_WAVE_Warp-Based_View_Guidance_for_Consistent_Novel_View_Synthesis_Using_ICCV_2025_paper.html": {
    "title": "WAVE: Warp-Based View Guidance for Consistent Novel View Synthesis Using a Single Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiwoo Park",
      "Tae Eun Choi",
      "Youngjun Jun",
      "Seong Jae Hwang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sella_Blended_Point_Cloud_Diffusion_for_Localized_Text-guided_Shape_Editing_ICCV_2025_paper.html": {
    "title": "Blended Point Cloud Diffusion for Localized Text-guided Shape Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Etai Sella",
      "Noam Atia",
      "Ron Mokady",
      "Hadar Averbuch-Elor"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Przewiezlikowski_Beyond_cls_Exploring_the_True_Potential_of_Masked_Image_Modeling_ICCV_2025_paper.html": {
    "title": "Beyond [cls]: Exploring the True Potential of Masked Image Modeling Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marcin Przewięźlikowski",
      "Randall Balestriero",
      "Wojciech Jasiński",
      "Marek Śmieja",
      "Bartosz Zieliński"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_AnimateAnyMesh_A_Feed-Forward_4D_Foundation_Model_for_Text-Driven_Universal_Mesh_ICCV_2025_paper.html": {
    "title": "AnimateAnyMesh: A Feed-Forward 4D Foundation Model for Text-Driven Universal Mesh Animation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijie Wu",
      "Chaohui Yu",
      "Fan Wang",
      "Xiang Bai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_PhysRig_Differentiable_Physics-Based_Skinning_and_Rigging_Framework_for_Realistic_Articulated_ICCV_2025_paper.html": {
    "title": "PhysRig: Differentiable Physics-Based Skinning and Rigging Framework for Realistic Articulated Object Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Zhang",
      "Haolan Xu",
      "Chun Feng",
      "Varun Jampani",
      "Narendra Ahuja"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shen_SpikePack_Enhanced_Information_Flow_in_Spiking_Neural_Networks_with_High_ICCV_2025_paper.html": {
    "title": "SpikePack: Enhanced Information Flow in Spiking Neural Networks with High Hardware Compatibility",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guobin Shen",
      "Jindong Li",
      "Tenglong Li",
      "Dongcheng Zhao",
      "Yi Zeng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Frequency-Aware_Autoregressive_Modeling_for_Efficient_High-Resolution_Image_Synthesis_ICCV_2025_paper.html": {
    "title": "Frequency-Aware Autoregressive Modeling for Efficient High-Resolution Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuokun Chen",
      "Jugang Fan",
      "Zhuowei Yu",
      "Bohan Zhuang",
      "Mingkui Tan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xiong_Efficient_Track_Anything_ICCV_2025_paper.html": {
    "title": "Efficient Track Anything",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunyang Xiong",
      "Chong Zhou",
      "Xiaoyu Xiang",
      "Lemeng Wu",
      "Chenchen Zhu",
      "Zechun Liu",
      "Saksham Suri",
      "Balakrishnan Varadarajan",
      "Ramya Akula",
      "Forrest Iandola",
      "Raghuraman Krishnamoorthi",
      "Bilge Soran",
      "Vikas Chandra"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_VFlowOpt_A_Token_Pruning_Framework_for_LMMs_with_Visual_Information_ICCV_2025_paper.html": {
    "title": "VFlowOpt: A Token Pruning Framework for LMMs with Visual Information Flow-Guided Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sihan Yang",
      "Runsen Xu",
      "Chenhang Cui",
      "Tai Wang",
      "Dahua Lin",
      "Jiangmiao Pang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/He_CameraCtrl_II_Dynamic_Scene_Exploration_via_Camera-controlled_Video_Diffusion_Models_ICCV_2025_paper.html": {
    "title": "CameraCtrl II: Dynamic Scene Exploration via Camera-controlled Video Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao He",
      "Ceyuan Yang",
      "Shanchuan Lin",
      "Yinghao Xu",
      "Meng Wei",
      "Liangke Gui",
      "Qi Zhao",
      "Gordon Wetzstein",
      "Lu Jiang",
      "Hongsheng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Guan_Text-guided_Visual_Prompt_DINO_for_Generic_Segmentation_ICCV_2025_paper.html": {
    "title": "Text-guided Visual Prompt DINO for Generic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchen Guan",
      "Chong Sun",
      "Canmiao Fu",
      "Zhipeng Huang",
      "Chun Yuan",
      "Chen Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_Open-set_Cross_Modal_Generalization_via_Multimodal_Unified_Representation_ICCV_2025_paper.html": {
    "title": "Open-set Cross Modal Generalization via Multimodal Unified Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hai Huang",
      "Yan Xia",
      "Shulei Wang",
      "Hanting Wang",
      "Minghui Fang",
      "Shengpeng Ji",
      "Sashuai Zhou",
      "Tao Jin",
      "Zhou Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_PLADIS_Pushing_the_Limits_of_Attention_in_Diffusion_Models_at_ICCV_2025_paper.html": {
    "title": "PLADIS: Pushing the Limits of Attention in Diffusion Models at Inference Time by Leveraging Sparsity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kwanyoung Kim",
      "Byeongsu Sim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shatwell_GT-Loc_Unifying_When_and_Where_in_Images_Through_a_Joint_ICCV_2025_paper.html": {
    "title": "GT-Loc: Unifying When and Where in Images Through a Joint Embedding Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David G. Shatwell",
      "Ishan Rajendrakumar Dave",
      "Sirnam Swetha",
      "Mubarak Shah"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ma_Robust_Test-Time_Adaptation_for_Single_Image_Denoising_Using_Deep_Gaussian_ICCV_2025_paper.html": {
    "title": "Robust Test-Time Adaptation for Single Image Denoising Using Deep Gaussian Prior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qing Ma",
      "Pengwei Liang",
      "Xiong Zhou",
      "Jiayi Ma",
      "Junjun Jiang",
      "Zhe Peng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Han_MATE_Motion-Augmented_Temporal_Consistency_for_Event-based_Point_Tracking_ICCV_2025_paper.html": {
    "title": "MATE: Motion-Augmented Temporal Consistency for Event-based Point Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Han",
      "Wei Zhai",
      "Yang Cao",
      "Bin Li",
      "Zheng-jun Zha"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Meng_Diffusion-Based_Extreme_High-speed_Scenes_Reconstruction_with_the_Complementary_Vision_Sensor_ICCV_2025_paper.html": {
    "title": "Diffusion-Based Extreme High-speed Scenes Reconstruction with the Complementary Vision Sensor",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yapeng Meng",
      "Yihan Lin",
      "Taoyi Wang",
      "Yuguo Chen",
      "Lijian Wang",
      "Rong Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Tandi_RnGCam_High-speed_video_from_rolling__global_shutter_measurements_ICCV_2025_paper.html": {
    "title": "RnGCam: High-speed video from rolling & global shutter measurements",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kevin Tandi",
      "Xiang Dai",
      "Chinmay Talegaonkar",
      "Gal Mishne",
      "Nick Antipa"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_OV3D-CG_Open-vocabulary_3D_Instance_Segmentation_with_Contextual_Guidance_ICCV_2025_paper.html": {
    "title": "OV3D-CG: Open-vocabulary 3D Instance Segmentation with Contextual Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingquan Zhou",
      "Chen He",
      "Ruiping Wang",
      "Xilin Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Feng_FG-OrIU_Towards_Better_Forgetting_via_Feature-Gradient_Orthogonality_for_Incremental_Unlearning_ICCV_2025_paper.html": {
    "title": "FG-OrIU: Towards Better Forgetting via Feature-Gradient Orthogonality for Incremental Unlearning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qian Feng",
      "JiaHang Tu",
      "Mintong Kang",
      "Hanbin Zhao",
      "Chao Zhang",
      "Hui Qian"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lu_InfiniCube_Unbounded_and_Controllable_Dynamic_3D_Driving_Scene_Generation_with_ICCV_2025_paper.html": {
    "title": "InfiniCube: Unbounded and Controllable Dynamic 3D Driving Scene Generation with World-Guided Video Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Lu",
      "Xuanchi Ren",
      "Jiawei Yang",
      "Tianchang Shen",
      "Zhangjie Wu",
      "Jun Gao",
      "Yue Wang",
      "Siheng Chen",
      "Mike Chen",
      "Sanja Fidler",
      "Jiahui Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Shen_Scene_Graph_Guided_Generation_Enable_Accurate_Relations_Generation_in_Text-to-Image_ICCV_2025_paper.html": {
    "title": "Scene Graph Guided Generation: Enable Accurate Relations Generation in Text-to-Image Models via Textural Rectification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guibao Shen",
      "Luozhou Wang",
      "Jiantao Lin",
      "Wenhang Ge",
      "Chaozhe Zhang",
      "Xin Tao",
      "Di Zhang",
      "Pengfei Wan",
      "Guangyong Chen",
      "Yijun Li",
      "Ying-cong Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Vasluianu_After_the_Party_Navigating_the_Mapping_From_Color_to_Ambient_ICCV_2025_paper.html": {
    "title": "After the Party: Navigating the Mapping From Color to Ambient Lighting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Florin-Alexandru Vasluianu",
      "Tim Seizinger",
      "Zongwei Wu",
      "Radu Timofte"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Disentangled_Clothed_Avatar_Generation_with_Layered_Representation_ICCV_2025_paper.html": {
    "title": "Disentangled Clothed Avatar Generation with Layered Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weitian Zhang",
      "Yichao Yan",
      "Sijing Wu",
      "Manwen Liao",
      "Xiaokang Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gao_Teleportraits_Training-Free_People_Insertion_into_Any_Scene_ICCV_2025_paper.html": {
    "title": "Teleportraits: Training-Free People Insertion into Any Scene",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jialu Gao",
      "K J Joseph",
      "Fernando De La Torre"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Dong_DepthSync_Diffusion_Guidance-Based_Depth_Synchronization_for_Scale-_and_Geometry-Consistent_Video_ICCV_2025_paper.html": {
    "title": "DepthSync: Diffusion Guidance-Based Depth Synchronization for Scale- and Geometry-Consistent Video Depth Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue-Jiang Dong",
      "Wang Zhao",
      "Jiale Xu",
      "Ying Shan",
      "Song-Hai Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_InfoBridge_Balanced_Multimodal_Integration_through_Conditional_Dependency_Modeling_ICCV_2025_paper.html": {
    "title": "InfoBridge: Balanced Multimodal Integration through Conditional Dependency Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenxin Li",
      "Yifan Liu",
      "Panwang Pan",
      "Hengyu Liu",
      "Xinyu Liu",
      "Wuyang Li",
      "Cheng Wang",
      "Weihao Yu",
      "Yiyang Lin",
      "Yixuan Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhan_Towards_Explicit_Exoskeleton_for_the_Reconstruction_of_Complicated_3D_Human_ICCV_2025_paper.html": {
    "title": "Towards Explicit Exoskeleton for the Reconstruction of Complicated 3D Human Avatars",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Zhan",
      "Qingtian Zhu",
      "Muyao Niu",
      "Mingze Ma",
      "Jiancheng Zhao",
      "Zhihang Zhong",
      "Xiao Sun",
      "Yu Qiao",
      "Yinqiang Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Salari_CABLD_Contrast-Agnostic_Brain_Landmark_Detection_with_Consistency-Based_Regularization_ICCV_2025_paper.html": {
    "title": "CABLD: Contrast-Agnostic Brain Landmark Detection with Consistency-Based Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soorena Salari",
      "Arash Harirpoush",
      "Hassan Rivaz",
      "Yiming Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhao_One_Object_Multiple_Lies_A_Benchmark_for_Cross-task_Adversarial_Attack_ICCV_2025_paper.html": {
    "title": "One Object, Multiple Lies: A Benchmark for Cross-task Adversarial Attack on Unified Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiale Zhao",
      "Xinyang Jiang",
      "Junyao Gao",
      "Yuhao Xue",
      "Cairong Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ho_A_Differentiable_Wave_Optics_Model_for_End-to-End_Computational_Imaging_System_ICCV_2025_paper.html": {
    "title": "A Differentiable Wave Optics Model for End-to-End Computational Imaging System Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chi-Jui Ho",
      "Yash Belhe",
      "Steve Rotenberg",
      "Ravi Ramamoorthi",
      "Tzu-Mao Li",
      "Nicholas Antipa"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Dutta_CHROME_Clothed_Human_Reconstruction_with_Occlusion-Resilience_and_Multiview-Consistency_from_a_ICCV_2025_paper.html": {
    "title": "CHROME: Clothed Human Reconstruction with Occlusion-Resilience and Multiview-Consistency from a Single Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arindam Dutta",
      "Meng Zheng",
      "Zhongpai Gao",
      "Benjamin Planche",
      "Anwesa Choudhuri",
      "Terrence Chen",
      "Amit K. Roy-Chowdhury",
      "Ziyan Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Jia_H3R_Hybrid_Multi-view_Correspondence_for_Generalizable_3D_Reconstruction_ICCV_2025_paper.html": {
    "title": "H3R: Hybrid Multi-view Correspondence for Generalizable 3D Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heng Jia",
      "Linchao Zhu",
      "Na Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/He_Capturing_head_avatar_with_hand_contacts_from_a_monocular_video_ICCV_2025_paper.html": {
    "title": "Capturing head avatar with hand contacts from a monocular video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haonan He",
      "Yufeng Zheng",
      "Jie Song"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Haitman_DoppDrive_Doppler-Driven_Temporal_Aggregation_for_Improved_Radar_Object_Detection_ICCV_2025_paper.html": {
    "title": "DoppDrive: Doppler-Driven Temporal Aggregation for Improved Radar Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuval Haitman",
      "Oded Bialer"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gao_3D_Gaussian_Map_with_Open-Set_Semantic_Grouping_for_Vision-Language_Navigation_ICCV_2025_paper.html": {
    "title": "3D Gaussian Map with Open-Set Semantic Grouping for Vision-Language Navigation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianzhe Gao",
      "Rui Liu",
      "Wenguan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_M-SpecGene_Generalized_Foundation_Model_for_RGBT_Multispectral_Vision_ICCV_2025_paper.html": {
    "title": "M-SpecGene: Generalized Foundation Model for RGBT Multispectral Vision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kailai Zhou",
      "Fuqiang Yang",
      "Shixian Wang",
      "Bihan Wen",
      "Chongde Zi",
      "Linsen Chen",
      "Qiu Shen",
      "Xun Cao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_YOLOE_Real-Time_Seeing_Anything_ICCV_2025_paper.html": {
    "title": "YOLOE: Real-Time Seeing Anything",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ao Wang",
      "Lihao Liu",
      "Hui Chen",
      "Zijia Lin",
      "Jungong Han",
      "Guiguang Ding"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lee_Joint_Learning_of_Pose_Regression_and_Denoising_Diffusion_with_Score_ICCV_2025_paper.html": {
    "title": "Joint Learning of Pose Regression and Denoising Diffusion with Score Scaling Sampling for Category-level 6D Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seunghyun Lee",
      "Tae-Kyun Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Beyond_Text-Visual_Attention_Exploiting_Visual_Cues_for_Effective_Token_Pruning_ICCV_2025_paper.html": {
    "title": "Beyond Text-Visual Attention: Exploiting Visual Cues for Effective Token Pruning in VLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qizhe Zhang",
      "Aosong Cheng",
      "Ming Lu",
      "Renrui Zhang",
      "Zhiyong Zhuo",
      "Jiajun Cao",
      "Shaobo Guo",
      "Qi She",
      "Shanghang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_Event-guided_HDR_Reconstruction_with_Diffusion_Priors_ICCV_2025_paper.html": {
    "title": "Event-guided HDR Reconstruction with Diffusion Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixin Yang",
      "Jiawei Zhang",
      "Yang Zhang",
      "Yunxuan Wei",
      "Dongqing Zou",
      "Jimmy S. Ren",
      "Boxin Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Continual_Adaptation_Environment-Conditional_Parameter_Generation_for_Object_Detection_in_Dynamic_ICCV_2025_paper.html": {
    "title": "Continual Adaptation: Environment-Conditional Parameter Generation for Object Detection in Dynamic Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Deng Li",
      "Aming Wu",
      "Yang Li",
      "Yaowei Wang",
      "Yahong Han"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_CATP-LLM_Empowering_Large_Language_Models_for_Cost-Aware_Tool_Planning_ICCV_2025_paper.html": {
    "title": "CATP-LLM: Empowering Large Language Models for Cost-Aware Tool Planning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Duo Wu",
      "Jinghe Wang",
      "Yuan Meng",
      "Yanning Zhang",
      "Le Sun",
      "Zhi Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lei_MoMaps_Semantics-Aware_Scene_Motion_Generation_with_Motion_Maps_ICCV_2025_paper.html": {
    "title": "MoMaps: Semantics-Aware Scene Motion Generation with Motion Maps",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahui Lei",
      "Kyle Genova",
      "George Kopanas",
      "Noah Snavely",
      "Leonidas Guibas"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yun_WarpHE4D_Dense_4D_Head_Map_toward_Full_Head_Reconstruction_ICCV_2025_paper.html": {
    "title": "WarpHE4D: Dense 4D Head Map toward Full Head Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jongseob Yun",
      "Yong-Hoon Kwon",
      "Min-Gyu Park",
      "Ju-Mi Kang",
      "Min-Ho Lee",
      "Inho Chang",
      "Ju Hong Yoon",
      "Kuk-Jin Yoon"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wei_Perceive_Understand_and_Restore_Real-World_Image_Super-Resolution_with_Autoregressive_Multimodal_ICCV_2025_paper.html": {
    "title": "Perceive, Understand and Restore: Real-World Image Super-Resolution with Autoregressive Multimodal Generative Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyang Wei",
      "Shuaizheng Liu",
      "Chun Yuan",
      "Lei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xu_TRKT_Weakly_Supervised_Dynamic_Scene_Graph_Generation_with_Temporal-enhanced_Relation-aware_ICCV_2025_paper.html": {
    "title": "TRKT: Weakly Supervised Dynamic Scene Graph Generation with Temporal-enhanced Relation-aware Knowledge Transferring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhu Xu",
      "Ting Lei",
      "Zhimin Li",
      "Guan Wang",
      "Qingchao Chen",
      "Yuxin Peng",
      "Yang Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/He_Progressive_Distribution_Bridging_Unsupervised_Adaptation_for_Large-scale_Pre-trained_Models_via_ICCV_2025_paper.html": {
    "title": "Progressive Distribution Bridging: Unsupervised Adaptation for Large-scale Pre-trained Models via Adaptive Auxiliary Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weinan He",
      "Yixin Zhang",
      "Zilei Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhan_Griffon_v2_Advancing_Multimodal_Perception_with_High-Resolution_Scaling_and_Visual-Language_ICCV_2025_paper.html": {
    "title": "Griffon v2: Advancing Multimodal Perception with High-Resolution Scaling and Visual-Language Co-Referring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufei Zhan",
      "Shurong Zheng",
      "Yousong Zhu",
      "Hongyin Zhao",
      "Fan Yang",
      "Ming Tang",
      "Jinqiao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ton_TARO_Timestep-Adaptive_Representation_Alignment_with_Onset-Aware_Conditioning_for_Synchronized_Video-to-Audio_ICCV_2025_paper.html": {
    "title": "TARO: Timestep-Adaptive Representation Alignment with Onset-Aware Conditioning for Synchronized Video-to-Audio Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tri Ton",
      "Ji Woo Hong",
      "Chang D. Yoo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Dong_VPR-Cloak_A_First_Look_at_Privacy_Cloak_Against_Visual_Place_ICCV_2025_paper.html": {
    "title": "VPR-Cloak: A First Look at Privacy Cloak Against Visual Place Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuting Dong",
      "Mingzhi Chen",
      "Feng Lu",
      "Hao Yu",
      "Guanghao Li",
      "Zhe Wu",
      "Ming Tang",
      "Chun Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xue_SDFormer_Vision-based_3D_Semantic_Scene_Completion_via_SAM-assisted_Dual-channel_Voxel_ICCV_2025_paper.html": {
    "title": "SDFormer: Vision-based 3D Semantic Scene Completion via SAM-assisted Dual-channel Voxel Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujie Xue",
      "Huilong Pi",
      "Jiapeng Zhang",
      "Yunchuan Qin",
      "Zhuo Tang",
      "Kenli Li",
      "Ruihui Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_ModSkill_Physical_Character_Skill_Modularization_ICCV_2025_paper.html": {
    "title": "ModSkill: Physical Character Skill Modularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiming Huang",
      "Zhiyang Dou",
      "Lingjie Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wei_Improving_Multimodal_Learning_via_Imbalanced_Learning_ICCV_2025_paper.html": {
    "title": "Improving Multimodal Learning via Imbalanced Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shicai Wei",
      "Chunbo Luo",
      "Yang Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Dong_INTER_Mitigating_Hallucination_in_Large_Vision-Language_Models_by_Interaction_Guidance_ICCV_2025_paper.html": {
    "title": "INTER: Mitigating Hallucination in Large Vision-Language Models by Interaction Guidance Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Dong",
      "Shichao Dong",
      "Jin Wang",
      "Jing Huang",
      "Li Zhou",
      "Zenghui Sun",
      "Lihua Jing",
      "Jinsong Lan",
      "Xiaoyong Zhu",
      "Bo Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xiang_ForestFormer3D_A_Unified_Framework_for_End-to-End_Segmentation_of_Forest_LiDAR_ICCV_2025_paper.html": {
    "title": "ForestFormer3D: A Unified Framework for End-to-End Segmentation of Forest LiDAR 3D Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Binbin Xiang",
      "Maciej Wielgosz",
      "Stefano Puliti",
      "Kamil Král",
      "Martin Krůček",
      "Azim Missarov",
      "Rasmus Astrup"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_Medical_World_Model_ICCV_2025_paper.html": {
    "title": "Medical World Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yijun Yang",
      "Zhao-Yang Wang",
      "Qiuping Liu",
      "Shuwen Sun",
      "Kang Wang",
      "Rama Chellappa",
      "Zongwei Zhou",
      "Alan Yuille",
      "Lei Zhu",
      "Yu-Dong Zhang",
      "Jieneng Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Mao_AccidentalGS_3D_Gaussian_Splatting_from_Accidental_Camera_Motion_ICCV_2025_paper.html": {
    "title": "AccidentalGS: 3D Gaussian Splatting from Accidental Camera Motion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mao Mao",
      "Xujie Shen",
      "Guyuan Chen",
      "Boming Zhao",
      "Jiarui Hu",
      "Hujun Bao",
      "Zhaopeng Cui"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Goli_RoMo_Robust_Motion_Segmentation_Improves_Structure_from_Motion_ICCV_2025_paper.html": {
    "title": "RoMo: Robust Motion Segmentation Improves Structure from Motion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lily Goli",
      "Sara Sabour",
      "Mark Matthews",
      "Marcus A. Brubaker",
      "Dmitry Lagun",
      "Alec Jacobson",
      "David J. Fleet",
      "Saurabh Saxena",
      "Andrea Tagliasacchi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Benita_CAFA_a_Controllable_Automatic_Foley_Artist_ICCV_2025_paper.html": {
    "title": "CAFA: a Controllable Automatic Foley Artist",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roi Benita",
      "Michael Finkelson",
      "Tavi Halperin",
      "Gleb Sterkin",
      "Yossi Adi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hu_Neuroverse3D_Developing_In-Context_Learning_Universal_Model_for_Neuroimaging_in_3D_ICCV_2025_paper.html": {
    "title": "Neuroverse3D: Developing In-Context Learning Universal Model for Neuroimaging in 3D",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiesi Hu",
      "Hanyang Peng",
      "Yanwu Yang",
      "Xutao Guo",
      "Yang Shang",
      "Pengcheng Shi",
      "Chenfei Ye",
      "Ting Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_ProSAM_Enhancing_the_Robustness_of_SAM-based_Visual_Reference_Segmentation_with_ICCV_2025_paper.html": {
    "title": "ProSAM: Enhancing the Robustness of SAM-based Visual Reference Segmentation with Probabilistic Prompts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoqi Wang",
      "Clint Sebastian",
      "Wenbin He",
      "Liu Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Hsiao_TF-TI2I_Training-Free_Text-and-Image-to-Image_Generation_via_Multi-Modal_Implicit-Context_Learning_In_Text-to-Image_ICCV_2025_paper.html": {
    "title": "TF-TI2I: Training-Free Text-and-Image-to-Image Generation via Multi-Modal Implicit-Context Learning In Text-to-Image Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Teng-Fang Hsiao",
      "Bo-Kai Ruan",
      "Yi-Lun Wu",
      "Tzu-Ling Lin",
      "Hong-Han Shuai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xie_Allowing_Oscillation_Quantization_Overcoming_Solution_Space_Limitation_in_Low_Bit-Width_ICCV_2025_paper.html": {
    "title": "Allowing Oscillation Quantization: Overcoming Solution Space Limitation in Low Bit-Width Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiying Xie",
      "Zihan Meng",
      "Jitao Ma",
      "Wenjin Guo",
      "Haowei Li",
      "Haonan Qin",
      "Leyuan Fang",
      "Yunsong Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Gong_CARP_Visuomotor_Policy_Learning_via_Coarse-to-Fine_Autoregressive_Prediction_ICCV_2025_paper.html": {
    "title": "CARP: Visuomotor Policy Learning via Coarse-to-Fine Autoregressive Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhefei Gong",
      "Pengxiang Ding",
      "Shangke Lyu",
      "Siteng Huang",
      "Mingyang Sun",
      "Wei Zhao",
      "Zhaoxin Fan",
      "Donglin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_SAMora_Enhancing_SAM_through_Hierarchical_Self-Supervised_Pre-Training_for_Medical_Images_ICCV_2025_paper.html": {
    "title": "SAMora: Enhancing SAM through Hierarchical Self-Supervised Pre-Training for Medical Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuhang Chen",
      "Hangjie Yuan",
      "Pengwei Liu",
      "Hanxue Gu",
      "Tao Feng",
      "Dong Ni"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_MagicMotion_Controllable_Video_Generation_with_Dense-to-Sparse_Trajectory_Guidance_ICCV_2025_paper.html": {
    "title": "MagicMotion: Controllable Video Generation with Dense-to-Sparse Trajectory Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quanhao Li",
      "Zhen Xing",
      "Rui Wang",
      "Hui Zhang",
      "Qi Dai",
      "Zuxuan Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xia_Less_Static_More_Private_Towards_Transferable_Privacy-Preserving_Action_Recognition_by_ICCV_2025_paper.html": {
    "title": "Less Static, More Private: Towards Transferable Privacy-Preserving Action Recognition by Generative Decoupled Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhi-Wei Xia",
      "Kun-Yu Lin",
      "Yuan-Ming Li",
      "Wei-Jin Huang",
      "Xian-Tuo Tan",
      "Wei-Shi Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Bhosale_PathDiff_Histopathology_Image_Synthesis_with_Unpaired_Text_and_Mask_Conditions_ICCV_2025_paper.html": {
    "title": "PathDiff: Histopathology Image Synthesis with Unpaired Text and Mask Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mahesh Bhosale",
      "Abdul Wasi",
      "Yuanhao Zhai",
      "Yunjie Tian",
      "Samuel Border",
      "Nan Xi",
      "Pinaki Sarder",
      "Junsong Yuan",
      "David Doermann",
      "Xuan Gong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Robust_Low-light_Scene_Restoration_via_Illumination_Transition_ICCV_2025_paper.html": {
    "title": "Robust Low-light Scene Restoration via Illumination Transition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ze Li",
      "Feng Zhang",
      "Xiatian Zhu",
      "Meng Zhang",
      "Yanghong Zhou",
      "P. Y. Mok"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Koledic_GVDepth_Zero-Shot_Monocular_Depth_Estimation_for_Ground_Vehicles_based_on_ICCV_2025_paper.html": {
    "title": "GVDepth: Zero-Shot Monocular Depth Estimation for Ground Vehicles based on Probabilistic Cue Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karlo Koledić",
      "Luka Petrović",
      "Ivan Marković",
      "Ivan Petrović"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kwon_VISION-XL_High_Definition_Video_Inverse_Problem_Solver_using_Latent_Image_ICCV_2025_paper.html": {
    "title": "VISION-XL: High Definition Video Inverse Problem Solver using Latent Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taesung Kwon",
      "Jong Chul Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kang_Robin3D_Improving_3D_Large_Language_Model_via_Robust_Instruction_Tuning_ICCV_2025_paper.html": {
    "title": "Robin3D: Improving 3D Large Language Model via Robust Instruction Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weitai Kang",
      "Haifeng Huang",
      "Yuzhang Shang",
      "Mubarak Shah",
      "Yan Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Amicantonio_Mixture_of_Experts_Guided_by_Gaussian_Splatters_Matters_A_new_ICCV_2025_paper.html": {
    "title": "Mixture of Experts Guided by Gaussian Splatters Matters: A new Approach to Weakly-Supervised Video Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giacomo D' Amicantonio",
      "Snehashis Majhi",
      "Quan Kong",
      "Lorenzo Garattoni",
      "Gianpiero Francesca",
      "Francois Bremond",
      "Egor Bondarev"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ai_ProJudge_A_Multi-Modal_Multi-Discipline_Benchmark_and_Instruction-Tuning_Dataset_for_MLLM-based_ICCV_2025_paper.html": {
    "title": "ProJudge: A Multi-Modal Multi-Discipline Benchmark and Instruction-Tuning Dataset for MLLM-based Process Judges",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxin Ai",
      "Pengfei Zhou",
      "Zhaopan Xu",
      "Ming Li",
      "Fanrui Zhang",
      "Zizhen Li",
      "Jianwen Sun",
      "Yukang Feng",
      "Baojin Huang",
      "Zhongyuan Wang",
      "Kaipeng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_GloPER_Unsupervised_Animal_Pattern_Extraction_from_Local_Reconstruction_ICCV_2025_paper.html": {
    "title": "GloPER: Unsupervised Animal Pattern Extraction from Local Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowen Chen",
      "Yun Sing Koh",
      "Gillian Dobbie"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zheng_One_Trajectory_One_Token_Grounded_Video_Tokenization_via_Panoptic_Sub-object_ICCV_2025_paper.html": {
    "title": "One Trajectory, One Token: Grounded Video Tokenization via Panoptic Sub-object Trajectory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenhao Zheng",
      "Jieyu Zhang",
      "Mohammadreza Salehi",
      "Ziqi Gao",
      "Vishnu Iyengar",
      "Norimasa Kobori",
      "Quan Kong",
      "Ranjay Krishna"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ding_Kaleidoscopic_Background_Attack_Disrupting_Pose_Estimation_with_Multi-Fold_Radial_Symmetry_ICCV_2025_paper.html": {
    "title": "Kaleidoscopic Background Attack: Disrupting Pose Estimation with Multi-Fold Radial Symmetry Textures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinlong Ding",
      "Hongwei Yu",
      "Jiawei Li",
      "Feifan Li",
      "Yu Shang",
      "Bochao Zou",
      "Huimin Ma",
      "Jiansheng Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Rabinowitz_COSTARR_Consolidated_Open_Set_Technique_with_Attenuation_for_Robust_Recognition_ICCV_2025_paper.html": {
    "title": "COSTARR: Consolidated Open Set Technique with Attenuation for Robust Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryan Rabinowitz",
      "Steve Cruz",
      "Walter Scheirer",
      "Terrance E. Boult"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zeng_Agreement_aware_and_dissimilarity_oriented_GLOM_ICCV_2025_paper.html": {
    "title": "Agreement aware and dissimilarity oriented GLOM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ru Zeng",
      "Yan Song",
      "Yang Zhang",
      "Yanling Hu",
      "Hui Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Cho_Att-Adapter_A_Robust_and_Precise_Domain-Specific_Multi-Attributes_T2I_Diffusion_Adapter_ICCV_2025_paper.html": {
    "title": "Att-Adapter: A Robust and Precise Domain-Specific Multi-Attributes T2I Diffusion Adapter via Conditional Variational Autoencoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wonwoong Cho",
      "Yan-Ying Chen",
      "Matthew Klenk",
      "David I. Inouye",
      "Yanxia Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Sim_PERSONA_Personalized_Whole-Body_3D_Avatar_with_Pose-Driven_Deformations_from_a_ICCV_2025_paper.html": {
    "title": "PERSONA: Personalized Whole-Body 3D Avatar with Pose-Driven Deformations from a Single Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Geonhee Sim",
      "Gyeongsik Moon"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Ren_PanoSplatt3R_Leveraging_Perspective_Pretraining_for_Generalized_Unposed_Wide-Baseline_Panorama_Reconstruction_ICCV_2025_paper.html": {
    "title": "PanoSplatt3R: Leveraging Perspective Pretraining for Generalized Unposed Wide-Baseline Panorama Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahui Ren",
      "Mochu Xiang",
      "Jiajun Zhu",
      "Yuchao Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_CCMNet_Leveraging_Calibrated_Color_Correction_Matrices_for_Cross-Camera_Color_Constancy_ICCV_2025_paper.html": {
    "title": "CCMNet: Leveraging Calibrated Color Correction Matrices for Cross-Camera Color Constancy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongyoung Kim",
      "Mahmoud Afifi",
      "Dongyun Kim",
      "Michael S. Brown",
      "Seon Joo Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Xiang_Seeing_3D_Through_2D_Lenses_3D_Few-Shot_Class-Incremental_Learning_via_ICCV_2025_paper.html": {
    "title": "Seeing 3D Through 2D Lenses: 3D Few-Shot Class-Incremental Learning via Cross-Modal Geometric Rectification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tuo Xiang",
      "Xuemiao Xu",
      "Bangzhen Liu",
      "Jinyi Li",
      "Yong Li",
      "Shengfeng He"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Secure_On-Device_Video_OOD_Detection_Without_Backpropagation_ICCV_2025_paper.html": {
    "title": "Secure On-Device Video OOD Detection Without Backpropagation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shawn Li",
      "Peilin Cai",
      "Yuxiao Zhou",
      "Zhiyu Ni",
      "Renjie Liang",
      "You Qin",
      "Yi Nian",
      "Zhengzhong Tu",
      "Xiyang Hu",
      "Yue Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Nassar_SmolDocling_An_ultra-compact_vision-language_model_for_end-to-end_multi-modal_document_conversion_ICCV_2025_paper.html": {
    "title": "SmolDocling: An ultra-compact vision-language model for end-to-end multi-modal document conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ahmed Nassar",
      "Matteo Omenetti",
      "Maksym Lysak",
      "Nikolaos Livathinos",
      "Christoph Auer",
      "Lucas Morin",
      "Rafael Teixeira de Lima",
      "Yusik Kim",
      "A. Said Gurbuz",
      "Michele Dolfi",
      "Peter W. J. Staar"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhong_Zero-Shot_Composed_Image_Retrieval_via_Dual-Stream_Instruction-Aware_Distillation_ICCV_2025_paper.html": {
    "title": "Zero-Shot Composed Image Retrieval via Dual-Stream Instruction-Aware Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenliang Zhong",
      "Rob Barton",
      "Weizhi An",
      "Feng Jiang",
      "Hehuan Ma",
      "Yuzhi Guo",
      "Abhishek Dan",
      "Shioulin Sam",
      "Karim Bouyarmane",
      "Junzhou Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Underwater_Visual_SLAM_with_Depth_Uncertainty_and_Medium_Modeling_ICCV_2025_paper.html": {
    "title": "Underwater Visual SLAM with Depth Uncertainty and Medium Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Liu",
      "Sheng Fan",
      "Wenguan Wang",
      "Yi Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhu_Blind_Video_Super-Resolution_based_on_Implicit_Kernels_ICCV_2025_paper.html": {
    "title": "Blind Video Super-Resolution based on Implicit Kernels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiang Zhu",
      "Yuxuan Jiang",
      "Shuyuan Zhu",
      "Fan Zhang",
      "David Bull",
      "Bing Zeng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhong_AMDANet_Attention-Driven_Multi-Perspective_Discrepancy_Alignment_for_RGB-Infrared_Image_Fusion_and_ICCV_2025_paper.html": {
    "title": "AMDANet: Attention-Driven Multi-Perspective Discrepancy Alignment for RGB-Infrared Image Fusion and Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haifeng Zhong",
      "Fan Tang",
      "Zhuo Chen",
      "Hyung Jin Chang",
      "Yixing Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Rahman_Local_Scale_Equivariance_with_Latent_Deep_Equilibrium_Canonicalizer_ICCV_2025_paper.html": {
    "title": "Local Scale Equivariance with Latent Deep Equilibrium Canonicalizer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md Ashiqur Rahman",
      "Chiao-An Yang",
      "Michael N. Cheng",
      "Lim Jun Hao",
      "Jeremiah Jiang",
      "Teck-Yian Lim",
      "Raymond A. Yeh"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Miao_Auxiliary_Prompt_Tuning_of_Vision-Language_Models_for_Few-Shot_Out-of-Distribution_Detection_ICCV_2025_paper.html": {
    "title": "Auxiliary Prompt Tuning of Vision-Language Models for Few-Shot Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenjun Miao",
      "Guansong Pang",
      "Zihan Wang",
      "Jin Zheng",
      "Xiao Bai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Lu_TACO_Taming_Diffusion_for_in-the-wild_Video_Amodal_Completion_ICCV_2025_paper.html": {
    "title": "TACO: Taming Diffusion for in-the-wild Video Amodal Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruijie Lu",
      "Yixin Chen",
      "Yu Liu",
      "Jiaxiang Tang",
      "Junfeng Ni",
      "Diwen Wan",
      "Gang Zeng",
      "Siyuan Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_UniGS_Modeling_Unitary_3D_Gaussians_for_Novel_View_Synthesis_from_ICCV_2025_paper.html": {
    "title": "UniGS: Modeling Unitary 3D Gaussians for Novel View Synthesis from Sparse-view Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiamin Wu",
      "Kenkun Liu",
      "Xiaoke Jiang",
      "Yuan Yao",
      "Lei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Zhu_ObjectGS_Object-aware_Scene_Reconstruction_and_Scene_Understanding_via_Gaussian_Splatting_ICCV_2025_paper.html": {
    "title": "ObjectGS: Object-aware Scene Reconstruction and Scene Understanding via Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruijie Zhu",
      "Mulin Yu",
      "Linning Xu",
      "Lihan Jiang",
      "Yixuan Li",
      "Tianzhu Zhang",
      "Jiangmiao Pang",
      "Bo Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Wan_SP2T_Sparse_Proxy_Attention_for_Dual-stream_Point_Transformer_ICCV_2025_paper.html": {
    "title": "SP2T: Sparse Proxy Attention for Dual-stream Point Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxu Wan",
      "Hong Zhang",
      "Ziqi He",
      "Yangyan Deng",
      "Qishu Wang",
      "Ding Yuan",
      "Yifan Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Feng_Scoring_Remember_and_Reference_Catching_Camouflaged_Objects_in_Videos_ICCV_2025_paper.html": {
    "title": "Scoring, Remember, and Reference: Catching Camouflaged Objects in Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu'ang Feng",
      "Shuyong Gao",
      "Fuzhen Yan",
      "Yicheng Song",
      "Lingyi Hong",
      "Junjie Hu",
      "Wenqiang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Task-Oriented_Human_Grasp_Synthesis_via_Context-_and_Task-Aware_Diffusers_ICCV_2025_paper.html": {
    "title": "Task-Oriented Human Grasp Synthesis via Context- and Task-Aware Diffusers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "An-Lun Liu",
      "Yu-Wei Chao",
      "Yi-Ting Chen"
    ]
  }
}