{
  "https://aclanthology.org/2021.eacl-main.1": {
    "title": "Unsupervised Sentence-embeddings by Manifold Approximation and Projection",
    "volume": "main",
    "abstract": "The concept of unsupervised universal sentence encoders has gained traction recently, wherein pre-trained models generate effective task-agnostic fixed-dimensional representations for phrases, sentences and paragraphs. Such methods are of varying complexity, from simple weighted-averages of word vectors to complex language-models based on bidirectional transformers. In this work we propose a novel technique to generate sentence-embeddings in an unsupervised fashion by projecting the sentences onto a fixed-dimensional manifold with the objective of preserving local neighbourhoods in the original space. To delineate such neighbourhoods we experiment with several set-distance metrics, including the recently proposed Word Mover's distance, while the fixed-dimensional projection is achieved by employing a scalable and efficient manifold approximation method rooted in topological data analysis. We test our approach, which we term EMAP or Embeddings by Manifold Approximation and Projection, on six publicly available text-classification datasets of varying size and complexity. Empirical results show that our method consistently performs similar to or better than several alternative state-of-the-art approaches",
    "checked": true,
    "id": "f3cf96a9d825b9575f7f9dbd1112b4ee3f19ba2d",
    "semantic_title": "unsupervised sentence-embeddings by manifold approximation and projection",
    "citation_count": 5,
    "authors": [
      "Subhradeep Kayal"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.2": {
    "title": "Contrastive Multi-document Question Generation",
    "volume": "main",
    "abstract": "Multi-document question generation focuses on generating a question that covers the common aspect of multiple documents. Such a model is useful in generating clarifying options. However, a naive model trained only using the targeted (‘positive') document set may generate too generic questions that cover a larger scope than delineated by the document set. To address this challenge, we introduce the contrastive learning strategy where given ‘positive' and ‘negative' sets of documents, we generate a question that is closely related to the ‘positive' set but is far away from the ‘negative' set. This setting allows generated questions to be more specific and related to the target document set. To generate such specific questions, we propose Multi-Source Coordinated Question Generator (MSCQG), a novel framework that includes a supervised learning (SL) stage and a reinforcement learning (RL) stage. In the SL stage, a single-document question generator is trained. In the RL stage, a coordinator model is trained to find optimal attention weights to align multiple single-document generators, by optimizing a reward designed to promote specificity of generated questions. We also develop an effective auxiliary objective, named Set-induced Contrastive Regularization (SCR) that improves the coordinator's contrastive learning during the RL stage. We show that our model significantly outperforms several strong baselines, as measured by automatic metrics and human evaluation. The source repository is publicly available at ‘www.github.com/woonsangcho/contrast_qgen'",
    "checked": true,
    "id": "7e3ab7e411a7038b653d6b76c8badf58b0794e15",
    "semantic_title": "contrastive multi-document question generation",
    "citation_count": 25,
    "authors": [
      "Woon Sang Cho",
      "Yizhe Zhang",
      "Sudha Rao",
      "Asli Celikyilmaz",
      "Chenyan Xiong",
      "Jianfeng Gao",
      "Mengdi Wang",
      "Bill Dolan"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.3": {
    "title": "Disambiguatory Signals are Stronger in Word-initial Positions",
    "volume": "main",
    "abstract": "Psycholinguistic studies of human word processing and lexical access provide ample evidence of the preferred nature of word-initial versus word-final segments, e.g., in terms of attention paid by listeners (greater) or the likelihood of reduction by speakers (lower). This has led to the conjecture—as in Wedel et al. (2019b), but common elsewhere—that languages have evolved to provide more information earlier in words than later. Information-theoretic methods to establish such tendencies in lexicons have suffered from several methodological shortcomings that leave open the question of whether this high word-initial informativeness is actually a property of the lexicon or simply an artefact of the incremental nature of recognition. In this paper, we point out the confounds in existing methods for comparing the informativeness of segments early in the word versus later in the word, and present several new measures that avoid these confounds. When controlling for these confounds, we still find evidence across hundreds of languages that indeed there is a cross-linguistic tendency to front-load information in words",
    "checked": true,
    "id": "db758b45e704c7d83d3299d3b2d0cec60c14a5b9",
    "semantic_title": "disambiguatory signals are stronger in word-initial positions",
    "citation_count": 5,
    "authors": [
      "Tiago Pimentel",
      "Ryan Cotterell",
      "Brian Roark"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.4": {
    "title": "On the (In)Effectiveness of Images for Text Classification",
    "volume": "main",
    "abstract": "Images are core components of multi-modal learning in natural language processing (NLP), and results have varied substantially as to whether images improve NLP tasks or not. One confounding effect has been that previous NLP research has generally focused on sophisticated tasks (in varying settings), generally applied to English only. We focus on text classification, in the context of assigning named entity classes to a given Wikipedia page, where images generally complement the text and the Wikipedia page can be in one of a number of different languages. Our experiments across a range of languages show that images complement NLP models (including BERT) trained without external pre-training, but when combined with BERT models pre-trained on large-scale external data, images contribute nothing",
    "checked": true,
    "id": "53307b18c9830006a050b252f13b2ab314a33c31",
    "semantic_title": "on the (in)effectiveness of images for text classification",
    "citation_count": 5,
    "authors": [
      "Chunpeng Ma",
      "Aili Shen",
      "Hiyori Yoshikawa",
      "Tomoya Iwakura",
      "Daniel Beck",
      "Timothy Baldwin"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.5": {
    "title": "If you've got it, flaunt it: Making the most of fine-grained sentiment annotations",
    "volume": "main",
    "abstract": "Fine-grained sentiment analysis attempts to extract sentiment holders, targets and polar expressions and resolve the relationship between them, but progress has been hampered by the difficulty of annotation. Targeted sentiment analysis, on the other hand, is a more narrow task, focusing on extracting sentiment targets and classifying their polarity. In this paper, we explore whether incorporating holder and expression information can improve target extraction and classification and perform experiments on eight English datasets. We conclude that jointly predicting target and polarity BIO labels improves target extraction, and that augmenting the input text with gold expressions generally improves targeted polarity classification. This highlights the potential importance of annotating expressions for fine-grained sentiment datasets. At the same time, our results show that performance of current models for predicting polar expressions is poor, hampering the benefit of this information in practice",
    "checked": true,
    "id": "74d20da7d1fc48e0433bd4527d502e72458f6507",
    "semantic_title": "if you've got it, flaunt it: making the most of fine-grained sentiment annotations",
    "citation_count": 3,
    "authors": [
      "Jeremy Barnes",
      "Lilja Øvrelid",
      "Erik Velldal"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.6": {
    "title": "Keep Learning: Self-supervised Meta-learning for Learning from Inference",
    "volume": "main",
    "abstract": "A common approach in many machine learning algorithms involves self-supervised learning on large unlabeled data before fine-tuning on downstream tasks to further improve performance. A new approach for language modelling, called dynamic evaluation, further fine-tunes a trained model during inference using trivially-present ground-truth labels, giving a large improvement in performance. However, this approach does not easily extend to classification tasks, where ground-truth labels are absent during inference. We propose to solve this issue by utilizing self-training and back-propagating the loss from the model's own class-balanced predictions (pseudo-labels), adapting the Reptile algorithm from meta-learning, combined with an inductive bias towards pre-trained weights to improve generalization. Our method improves the performance of standard backbones such as BERT, Electra, and ResNet-50 on a wide variety of tasks, such as question answering on SQuAD and NewsQA, benchmark task SuperGLUE, conversation response selection on Ubuntu Dialog corpus v2.0, as well as image classification on MNIST and ImageNet without any changes to the underlying models. Our proposed method outperforms previous approaches, enables self-supervised fine-tuning during inference of any classifier model to better adapt to target domains, can be easily adapted to any model, and is also effective in online and transfer-learning settings",
    "checked": true,
    "id": "9d4e23f9fe66a82c60e21664f9d720e996a45699",
    "semantic_title": "keep learning: self-supervised meta-learning for learning from inference",
    "citation_count": 8,
    "authors": [
      "Akhil Kedia",
      "Sai Chetan Chinthakindi"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.7": {
    "title": "ResPer: Computationally Modelling Resisting Strategies in Persuasive Conversations",
    "volume": "main",
    "abstract": "Modelling persuasion strategies as predictors of task outcome has several real-world applications and has received considerable attention from the computational linguistics community. However, previous research has failed to account for the resisting strategies employed by an individual to foil such persuasion attempts. Grounded in prior literature in cognitive and social psychology, we propose a generalised framework for identifying resisting strategies in persuasive conversations. We instantiate our framework on two distinct datasets comprising persuasion and negotiation conversations. We also leverage a hierarchical sequence-labelling neural architecture to infer the aforementioned resisting strategies automatically. Our experiments reveal the asymmetry of power roles in non-collaborative goal-directed conversations and the benefits accrued from incorporating resisting strategies on the final conversation outcome. We also investigate the role of different resisting strategies on the conversation outcome and glean insights that corroborate with past findings. We also make the code and the dataset of this work publicly available at https://github.com/americast/resper",
    "checked": true,
    "id": "17fdf700dcd6e8621ef4d9aa4ae69d1ad7a2043e",
    "semantic_title": "resper: computationally modelling resisting strategies in persuasive conversations",
    "citation_count": 14,
    "authors": [
      "Ritam Dutt",
      "Sayan Sinha",
      "Rishabh Joshi",
      "Surya Shekhar Chakraborty",
      "Meredith Riggs",
      "Xinru Yan",
      "Haogang Bao",
      "Carolyn Rose"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.8": {
    "title": "BERxiT: Early Exiting for BERT with Better Fine-Tuning and Extension to Regression",
    "volume": "main",
    "abstract": "The slow speed of BERT has motivated much research on accelerating its inference, and the early exiting idea has been proposed to make trade-offs between model quality and efficiency. This paper aims to address two weaknesses of previous work: (1) existing fine-tuning strategies for early exiting models fail to take full advantage of BERT; (2) methods to make exiting decisions are limited to classification tasks. We propose a more advanced fine-tuning strategy and a learning-to-exit module that extends early exiting to tasks other than classification. Experiments demonstrate improved early exiting for BERT, with better trade-offs obtained by the proposed fine-tuning strategy, successful application to regression tasks, and the possibility to combine it with other acceleration methods. Source code can be found at https://github.com/castorini/berxit",
    "checked": true,
    "id": "7b37c0a4976c4d2a5a440d494fbb0f3daede2a00",
    "semantic_title": "berxit: early exiting for bert with better fine-tuning and extension to regression",
    "citation_count": 95,
    "authors": [
      "Ji Xin",
      "Raphael Tang",
      "Yaoliang Yu",
      "Jimmy Lin"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.9": {
    "title": "Telling BERT's Full Story: from Local Attention to Global Aggregation",
    "volume": "main",
    "abstract": "We take a deep look into the behaviour of self-attention heads in the transformer architecture. In light of recent work discouraging the use of attention distributions for explaining a model's behaviour, we show that attention distributions can nevertheless provide insights into the local behaviour of attention heads. This way, we propose a distinction between local patterns revealed by attention and global patterns that refer back to the input, and analyze BERT from both angles. We use gradient attribution to analyze how the output of an attention head depends on the input tokens, effectively extending the local attention-based analysis to account for the mixing of information throughout the transformer layers. We find that there is a significant mismatch between attention and attribution distributions, caused by the mixing of context inside the model. We quantify this discrepancy and observe that interestingly, there are some patterns that persist across all layers despite the mixing",
    "checked": true,
    "id": "3d9605a8b6364d24b58be561ea1ce4233c088cee",
    "semantic_title": "telling bert's full story: from local attention to global aggregation",
    "citation_count": 19,
    "authors": [
      "Damian Pascual",
      "Gino Brunner",
      "Roger Wattenhofer"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.10": {
    "title": "Effects of Pre- and Post-Processing on type-based Embeddings in Lexical Semantic Change Detection",
    "volume": "main",
    "abstract": "Lexical semantic change detection is a new and innovative research field. The optimal fine-tuning of models including pre- and post-processing is largely unclear. We optimize existing models by (i) pre-training on large corpora and refining on diachronic target corpora tackling the notorious small data problem, and (ii) applying post-processing transformations that have been shown to improve performance on synchronic tasks. Our results provide a guide for the application and optimization of lexical semantic change detection models across various learning scenarios",
    "checked": true,
    "id": "7dc158f7d659d24e0b421c7bef54f3d28ea341f8",
    "semantic_title": "effects of pre- and post-processing on type-based embeddings in lexical semantic change detection",
    "citation_count": 6,
    "authors": [
      "Jens Kaiser",
      "Sinan Kurtyigit",
      "Serge Kotchourko",
      "Dominik Schlechtweg"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.11": {
    "title": "The Gutenberg Dialogue Dataset",
    "volume": "main",
    "abstract": "Large datasets are essential for neural modeling of many NLP tasks. Current publicly available open-domain dialogue datasets offer a trade-off between quality (e.g., DailyDialog) and size (e.g., Opensubtitles). We narrow this gap by building a high-quality dataset of 14.8M utterances in English, and smaller datasets in German, Dutch, Spanish, Portuguese, Italian, and Hungarian. We extract and process dialogues from public-domain books made available by Project Gutenberg. We describe our dialogue extraction pipeline, analyze the effects of the various heuristics used, and present an error analysis of extracted dialogues. Finally, we conduct experiments showing that better response quality can be achieved in zero-shot and finetuning settings by training on our data than on the larger but much noisier Opensubtitles dataset. Our open-source pipeline (https://github.com/ricsinaruto/gutenberg-dialog) can be extended to further languages with little additional effort. Researchers can also build their versions of existing datasets by adjusting various trade-off parameters",
    "checked": true,
    "id": "e78e58378f717374b8342d55533cd43c6fb8f4df",
    "semantic_title": "the gutenberg dialogue dataset",
    "citation_count": 12,
    "authors": [
      "Richard Csaky",
      "Gábor Recski"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.12": {
    "title": "On the Calibration and Uncertainty of Neural Learning to Rank Models for Conversational Search",
    "volume": "main",
    "abstract": "According to the Probability Ranking Principle (PRP), ranking documents in decreasing order of their probability of relevance leads to an optimal document ranking for ad-hoc retrieval. The PRP holds when two conditions are met: [C1] the models are well calibrated, and, [C2] the probabilities of relevance are reported with certainty. We know however that deep neural networks (DNNs) are often not well calibrated and have several sources of uncertainty, and thus [C1] and [C2] might not be satisfied by neural rankers. Given the success of neural Learning to Rank (LTR) approaches—and here, especially BERT-based approaches—we first analyze under which circumstances deterministic neural rankers are calibrated for conversational search problems. Then, motivated by our findings we use two techniques to model the uncertainty of neural rankers leading to the proposed stochastic rankers, which output a predictive distribution of relevance as opposed to point estimates. Our experimental results on the ad-hoc retrieval task of conversation response ranking reveal that (i) BERT-based rankers are not robustly calibrated and that stochastic BERT-based rankers yield better calibration; and (ii) uncertainty estimation is beneficial for both risk-aware neural ranking, i.e. taking into account the uncertainty when ranking documents, and for predicting unanswerable conversational contexts",
    "checked": true,
    "id": "2d423f81b3552fe596160780a7d4d4c08842a838",
    "semantic_title": "on the calibration and uncertainty of neural learning to rank models for conversational search",
    "citation_count": 29,
    "authors": [
      "Gustavo Penha",
      "Claudia Hauff"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.13": {
    "title": "Frequency-Guided Word Substitutions for Detecting Textual Adversarial Examples",
    "volume": "main",
    "abstract": "Recent efforts have shown that neural text processing models are vulnerable to adversarial examples, but the nature of these examples is poorly understood. In this work, we show that adversarial attacks against CNN, LSTM and Transformer-based classification models perform word substitutions that are identifiable through frequency differences between replaced words and their corresponding substitutions. Based on these findings, we propose frequency-guided word substitutions (FGWS), a simple algorithm exploiting the frequency properties of adversarial word substitutions for the detection of adversarial examples. FGWS achieves strong performance by accurately detecting adversarial examples on the SST-2 and IMDb sentiment datasets, with F1 detection scores of up to 91.4% against RoBERTa-based classification models. We compare our approach against a recently proposed perturbation discrimination framework and show that we outperform it by up to 13.0% F1",
    "checked": true,
    "id": "0d5acd9ba016a7d4793d293b11c0057fb2edacd8",
    "semantic_title": "frequency-guided word substitutions for detecting textual adversarial examples",
    "citation_count": 90,
    "authors": [
      "Maximilian Mozes",
      "Pontus Stenetorp",
      "Bennett Kleinberg",
      "Lewis Griffin"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.14": {
    "title": "Maximal Multiverse Learning for Promoting Cross-Task Generalization of Fine-Tuned Language Models",
    "volume": "main",
    "abstract": "Language modeling with BERT consists of two phases of (i) unsupervised pre-training on unlabeled text, and (ii) fine-tuning for a specific supervised task. We present a method that leverages the second phase to its fullest, by applying an extensive number of parallel classifier heads, which are enforced to be orthogonal, while adaptively eliminating the weaker heads during training. We conduct an extensive inter- and intra-dataset evaluation, showing that our method improves the generalization ability of BERT, sometimes leading to a +9% gain in accuracy. These results highlight the importance of a proper fine-tuning procedure, especially for relatively smaller-sized datasets. Our code is attached as supplementary",
    "checked": true,
    "id": "431ad023149287abc496d61570ba167fb014cf54",
    "semantic_title": "maximal multiverse learning for promoting cross-task generalization of fine-tuned language models",
    "citation_count": 8,
    "authors": [
      "Itzik Malkiel",
      "Lior Wolf"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.15": {
    "title": "Unification-based Reconstruction of Multi-hop Explanations for Science Questions",
    "volume": "main",
    "abstract": "This paper presents a novel framework for reconstructing multi-hop explanations in science Question Answering (QA). While existing approaches for multi-hop reasoning build explanations considering each question in isolation, we propose a method to leverage explanatory patterns emerging in a corpus of scientific explanations. Specifically, the framework ranks a set of atomic facts by integrating lexical relevance with the notion of unification power, estimated analysing explanations for similar questions in the corpus. An extensive evaluation is performed on the Worldtree corpus, integrating k-NN clustering and Information Retrieval (IR) techniques. We present the following conclusions: (1) The proposed method achieves results competitive with Transformers, yet being orders of magnitude faster, a feature that makes it scalable to large explanatory corpora (2) The unification-based mechanism has a key role in reducing semantic drift, contributing to the reconstruction of many hops explanations (6 or more facts) and the ranking of complex inference facts (+12.0 Mean Average Precision) (3) Crucially, the constructed explanations can support downstream QA models, improving the accuracy of BERT by up to 10% overall",
    "checked": true,
    "id": "92fe70e5c7c5e53786ea75b0331672dc7d96ca99",
    "semantic_title": "unification-based reconstruction of multi-hop explanations for science questions",
    "citation_count": 24,
    "authors": [
      "Marco Valentino",
      "Mokanarangan Thayaparan",
      "André Freitas"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.16": {
    "title": "Dictionary-based Debiasing of Pre-trained Word Embeddings",
    "volume": "main",
    "abstract": "Word embeddings trained on large corpora have shown to encode high levels of unfair discriminatory gender, racial, religious and ethnic biases. In contrast, human-written dictionaries describe the meanings of words in a concise, objective and an unbiased manner. We propose a method for debiasing pre-trained word embeddings using dictionaries, without requiring access to the original training resources or any knowledge regarding the word embedding algorithms used. Unlike prior work, our proposed method does not require the types of biases to be pre-defined in the form of word lists, and learns the constraints that must be satisfied by unbiased word embeddings automatically from dictionary definitions of the words. Specifically, we learn an encoder to generate a debiased version of an input word embedding such that it (a) retains the semantics of the pre-trained word embedding, (b) agrees with the unbiased definition of the word according to the dictionary, and (c) remains orthogonal to the vector space spanned by any biased basis vectors in the pre-trained word embedding space. Experimental results on standard benchmark datasets show that the proposed method can accurately remove unfair biases encoded in pre-trained word embeddings, while preserving useful semantics",
    "checked": true,
    "id": "3713672a73ddccbc75e24e99cde182a78772da21",
    "semantic_title": "dictionary-based debiasing of pre-trained word embeddings",
    "citation_count": 36,
    "authors": [
      "Masahiro Kaneko",
      "Danushka Bollegala"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.17": {
    "title": "Belief-based Generation of Argumentative Claims",
    "volume": "main",
    "abstract": "In this work, we argue that augmenting argument generation technology with the ability to encode beliefs is of twofold. First, it gives more control on the generated arguments leading to better reach for audience. Second, it is one way of modeling the human process of synthesizing arguments. Therefore, we propose the task of belief-based claim generation, and study the research question of how to model and encode a user's beliefs into a generated argumentative text. To this end, we model users' beliefs via their stances on big issues, and extend state of the art text generation models with extra input reflecting user's beliefs. Through an automatic evaluation we show empirical evidence of the applicability to encode beliefs into argumentative text. In our manual evaluation, we highlight that the low effectiveness of our approach stems from the noise produced by the automatic collection of bag-of-words, which was mitigated by removing this noise. The finding of this paper lays the ground work to further investigate the role of beliefs in generating better reaching arguments",
    "checked": true,
    "id": "7baa2737a66c48707e7850c6054c251902072c44",
    "semantic_title": "belief-based generation of argumentative claims",
    "citation_count": 21,
    "authors": [
      "Milad Alshomary",
      "Wei-Fan Chen",
      "Timon Gurcke",
      "Henning Wachsmuth"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.18": {
    "title": "Non-Autoregressive Text Generation with Pre-trained Language Models",
    "volume": "main",
    "abstract": "Non-autoregressive generation (NAG) has recently attracted great attention due to its fast inference speed. However, the generation quality of existing NAG models still lags behind their autoregressive counterparts. In this work, we show that BERT can be employed as the backbone of a NAG model for a greatly improved performance. Additionally, we devise two mechanisms to alleviate the two common problems of vanilla NAG models: the inflexibility of prefixed output length and the conditional independence of individual token predictions. To further strengthen the speed advantage of the proposed model, we propose a new decoding strategy, ratio-first, for applications where the output lengths can be approximately estimated beforehand. For a comprehensive evaluation, we test the proposed model on three text generation tasks, including text summarization, sentence compression and machine translation. Experimental results show that our model significantly outperforms existing non-autoregressive baselines and achieves competitive performance with many strong autoregressive models. In addition, we also conduct extensive analysis experiments to reveal the effect of each proposed component",
    "checked": true,
    "id": "3e8f037d1b2c893f4df37deda1da3dcc577a8bac",
    "semantic_title": "non-autoregressive text generation with pre-trained language models",
    "citation_count": 42,
    "authors": [
      "Yixuan Su",
      "Deng Cai",
      "Yan Wang",
      "David Vandyke",
      "Simon Baker",
      "Piji Li",
      "Nigel Collier"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.19": {
    "title": "Multi-split Reversible Transformers Can Enhance Neural Machine Translation",
    "volume": "main",
    "abstract": "Large-scale transformers have been shown the state-of-the-art on neural machine translation. However, training these increasingly wider and deeper models could be tremendously memory intensive. We reduce the memory burden by employing the idea of reversible networks that a layer's input can be reconstructed from its output. We design three types of multi-split based reversible transformers. We also devise a corresponding backpropagation algorithm, which does not need to store activations for most layers. Furthermore, we present two fine-tuning techniques: splits shuffle and self ensemble, to boost translation accuracy. Specifically, our best models surpass the vanilla transformer by at least 1.4 BLEU points in three datasets. Our large-scale reversible models achieve 30.0 BLEU in WMT'14 En-De and 43.5 BLEU in WMT'14 En-Fr, beating several very strong baselines with less than half of the training memory",
    "checked": true,
    "id": "38a9c05a32931a2bf9ab08b56d55355524974d09",
    "semantic_title": "multi-split reversible transformers can enhance neural machine translation",
    "citation_count": 3,
    "authors": [
      "Yuekai Zhao",
      "Shuchang Zhou",
      "Zhihua Zhang"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.20": {
    "title": "Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference",
    "volume": "main",
    "abstract": "Some NLP tasks can be solved in a fully unsupervised fashion by providing a pretrained language model with \"task descriptions\" in natural language (e.g., Radford et al., 2019). While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples as cloze-style phrases to help language models understand a given task. These phrases are then used to assign soft labels to a large set of unlabeled examples. Finally, standard supervised training is performed on the resulting training set. For several tasks and languages, PET outperforms supervised training and strong semi-supervised approaches in low-resource settings by a large margin",
    "checked": true,
    "id": "8ae9a17c87a4518b513e860683a0ef7824be994d",
    "semantic_title": "exploiting cloze-questions for few-shot text classification and natural language inference",
    "citation_count": 1358,
    "authors": [
      "Timo Schick",
      "Hinrich Schütze"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.21": {
    "title": "CDˆ2CR: Co-reference resolution across documents and domains",
    "volume": "main",
    "abstract": "Cross-document co-reference resolution (CDCR) is the task of identifying and linking mentions to entities and concepts across many text documents. Current state-of-the-art models for this task assume that all documents are of the same type (e.g. news articles) or fall under the same theme. However, it is also desirable to perform CDCR across different domains (type or theme). A particular use case we focus on in this paper is the resolution of entities mentioned across scientific work and newspaper articles that discuss them. Identifying the same entities and corresponding concepts in both scientific articles and news can help scientists understand how their work is represented in mainstream media. We propose a new task and English language dataset for cross-document cross-domain co-reference resolution (CDˆ2CR). The task aims to identify links between entities across heterogeneous document types. We show that in this cross-domain, cross-document setting, existing CDCR models do not perform well and we provide a baseline model that outperforms current state-of-the-art CDCR models on CDˆ2CR. Our data set, annotation tool and guidelines as well as our model for cross-document cross-domain co-reference are all supplied as open access open source resources",
    "checked": true,
    "id": "33e8e5d0801aab695efc21fad20bd710bb22cb6d",
    "semantic_title": "cdˆ2cr: co-reference resolution across documents and domains",
    "citation_count": 7,
    "authors": [
      "James Ravenscroft",
      "Amanda Clare",
      "Arie Cattan",
      "Ido Dagan",
      "Maria Liakata"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.22": {
    "title": "AREDSUM: Adaptive Redundancy-Aware Iterative Sentence Ranking for Extractive Document Summarization",
    "volume": "main",
    "abstract": "Redundancy-aware extractive summarization systems score the redundancy of the sentences to be included in a summary either jointly with their salience information or separately as an additional sentence scoring step. Previous work shows the efficacy of jointly scoring and selecting sentences with neural sequence generation models. It is, however, not well-understood if the gain is due to better encoding techniques or better redundancy reduction approaches. Similarly, the contribution of salience versus diversity components on the created summary is not studied well. Building on the state-of-the-art encoding methods for summarization, we present two adaptive learning models: AREDSUM-SEQ that jointly considers salience and novelty during sentence selection; and a two-step AREDSUM-CTX that scores salience first, then learns to balance salience and redundancy, enabling the measurement of the impact of each aspect. Empirical results on CNN/DailyMail and NYT50 datasets show that by modeling diversity explicitly in a separate step, AREDSUM-CTX achieves significantly better performance than AREDSUM-SEQ as well as state-of-the-art extractive summarization baselines",
    "checked": true,
    "id": "d97fd64ee820aeeee8e1a51e98084a30ef5d64cd",
    "semantic_title": "aredsum: adaptive redundancy-aware iterative sentence ranking for extractive document summarization",
    "citation_count": 18,
    "authors": [
      "Keping Bi",
      "Rahul Jha",
      "Bruce Croft",
      "Asli Celikyilmaz"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.23": {
    "title": "Talk to me with left, right, and angles\": Lexical entrainment in spoken Hebrew dialogue",
    "volume": "main",
    "abstract": "It has been well-documented for several languages that human interlocutors tend to adapt their linguistic productions to become more similar to each other. This behavior, known as entrainment, affects lexical choice as well, both with regard to specific words, such as referring expressions, and overall style. We offer what we believe to be the first investigation of such lexical entrainment in Hebrew. Using two existing measures, we analyze Hebrew speakers interacting in a Map Task, a popular experimental setup, and find rich evidence of lexical entrainment. Analyzing speaker pairs by the combination of their genders as well as speakers by their individual gender, we find no clear pattern of differences. We do, however, find that speakers in a position of less power entrain more than those with greater power, which matches theoretical accounts. Overall, our results mostly accord with those for American English, with a lack of entrainment on hedge words being the main difference",
    "checked": true,
    "id": "4b8ccc488f2f449777e6dfff1a006960e6ea014c",
    "semantic_title": "talk to me with left, right, and angles\": lexical entrainment in spoken hebrew dialogue",
    "citation_count": 5,
    "authors": [
      "Andreas Weise",
      "Vered Silber-Varod",
      "Anat Lerner",
      "Julia Hirschberg",
      "Rivka Levitan"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.24": {
    "title": "Recipes for Building an Open-Domain Chatbot",
    "volume": "main",
    "abstract": "Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we highlight other ingredients. Good conversation requires blended skills: providing engaging talking points, and displaying knowledge, empathy and personality appropriately, while maintaining a consistent persona. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter models, and make our models and code publicly available. Human evaluations show our best models outperform existing approaches in multi-turn dialogue on engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models",
    "checked": true,
    "id": "9b539d413393047b28bb7be9b195f142aaf7a80e",
    "semantic_title": "recipes for building an open-domain chatbot",
    "citation_count": 917,
    "authors": [
      "Stephen Roller",
      "Emily Dinan",
      "Naman Goyal",
      "Da Ju",
      "Mary Williamson",
      "Yinhan Liu",
      "Jing Xu",
      "Myle Ott",
      "Eric Michael Smith",
      "Y-Lan Boureau",
      "Jason Weston"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.25": {
    "title": "Evaluating the Evaluation of Diversity in Natural Language Generation",
    "volume": "main",
    "abstract": "Despite growing interest in natural language generation (NLG) models that produce diverse outputs, there is currently no principled method for evaluating the diversity of an NLG system. In this work, we propose a framework for evaluating diversity metrics. The framework measures the correlation between a proposed diversity metric and a diversity parameter, a single parameter that controls some aspect of diversity in generated text. For example, a diversity parameter might be a binary variable used to instruct crowdsourcing workers to generate text with either low or high content diversity. We demonstrate the utility of our framework by: (a) establishing best practices for eliciting diversity judgments from humans, (b) showing that humans substantially outperform automatic metrics in estimating content diversity, and (c) demonstrating that existing methods for controlling diversity by tuning a \"decoding parameter\" mostly affect form but not meaning. Our framework can advance the understanding of different diversity metrics, an essential step on the road towards better NLG systems",
    "checked": true,
    "id": "627327dcedf58c78ad8c0a1ce02f4a70ebf61c45",
    "semantic_title": "evaluating the evaluation of diversity in natural language generation",
    "citation_count": 69,
    "authors": [
      "Guy Tevet",
      "Jonathan Berant"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.26": {
    "title": "Retrieval, Re-ranking and Multi-task Learning for Knowledge-Base Question Answering",
    "volume": "main",
    "abstract": "Question answering over knowledge bases (KBQA) usually involves three sub-tasks, namely topic entity detection, entity linking and relation detection. Due to the large number of entities and relations inside knowledge bases (KB), previous work usually utilized sophisticated rules to narrow down the search space and managed only a subset of KBs in memory. In this work, we leverage a retrieve-and-rerank framework to access KBs via traditional information retrieval (IR) method, and re-rank retrieved candidates with more powerful neural networks such as the pre-trained BERT model. Considering the fact that directly assigning a different BERT model for each sub-task may incur prohibitive costs, we propose to share a BERT encoder across all three sub-tasks and define task-specific layers on top of the shared layer. The unified model is then trained under a multi-task learning framework. Experiments show that: (1) Our IR-based retrieval method is able to collect high-quality candidates efficiently, thus enables our method adapt to large-scale KBs easily; (2) the BERT model improves the accuracy across all three sub-tasks; and (3) benefiting from multi-task learning, the unified model obtains further improvements with only 1/3 of the original parameters. Our final model achieves competitive results on the SimpleQuestions dataset and superior performance on the FreebaseQA dataset",
    "checked": true,
    "id": "489efd419d5690bdf8a255a9de8458e320f306c2",
    "semantic_title": "retrieval, re-ranking and multi-task learning for knowledge-base question answering",
    "citation_count": 28,
    "authors": [
      "Zhiguo Wang",
      "Patrick Ng",
      "Ramesh Nallapati",
      "Bing Xiang"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.27": {
    "title": "Implicitly Abusive Comparisons – A New Dataset and Linguistic Analysis",
    "volume": "main",
    "abstract": "We examine the task of detecting implicitly abusive comparisons (e.g. \"Your hair looks like you have been electrocuted\"). Implicitly abusive comparisons are abusive comparisons in which abusive words (e.g. \"dumbass\" or \"scum\") are absent. We detail the process of creating a novel dataset for this task via crowdsourcing that includes several measures to obtain a sufficiently representative and unbiased set of comparisons. We also present classification experiments that include a range of linguistic features that help us better understand the mechanisms underlying abusive comparisons",
    "checked": true,
    "id": "dbf9fbc13ff56225024f5565278a89a5f22da7f8",
    "semantic_title": "implicitly abusive comparisons – a new dataset and linguistic analysis",
    "citation_count": 16,
    "authors": [
      "Michael Wiegand",
      "Maja Geulig",
      "Josef Ruppenhofer"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.28": {
    "title": "Exploiting Emojis for Abusive Language Detection",
    "volume": "main",
    "abstract": "We propose to use abusive emojis, such as the \"middle finger\" or \"face vomiting\", as a proxy for learning a lexicon of abusive words. Since it represents extralinguistic information, a single emoji can co-occur with different forms of explicitly abusive utterances. We show that our approach generates a lexicon that offers the same performance in cross-domain classification of abusive microposts as the most advanced lexicon induction method. Such an approach, in contrast, is dependent on manually annotated seed words and expensive lexical resources for bootstrapping (e.g. WordNet). We demonstrate that the same emojis can also be effectively used in languages other than English. Finally, we also show that emojis can be exploited for classifying mentions of ambiguous words, such as \"fuck\" and \"bitch\", into generally abusive and just profane usages",
    "checked": true,
    "id": "f799f680756ece4097806ee28819b54d49c9e319",
    "semantic_title": "exploiting emojis for abusive language detection",
    "citation_count": 9,
    "authors": [
      "Michael Wiegand",
      "Josef Ruppenhofer"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.29": {
    "title": "A Systematic Review of Reproducibility Research in Natural Language Processing",
    "volume": "main",
    "abstract": "Against the background of what has been termed a reproducibility crisis in science, the NLP field is becoming increasingly interested in, and conscientious about, the reproducibility of its results. The past few years have seen an impressive range of new initiatives, events and active research in the area. However, the field is far from reaching a consensus about how reproducibility should be defined, measured and addressed, with diversity of views currently increasing rather than converging. With this focused contribution, we aim to provide a wide-angle, and as near as possible complete, snapshot of current work on reproducibility in NLP,",
    "checked": true,
    "id": "59e7ed6132ce9992a6790a0a179b9eed73959780",
    "semantic_title": "a systematic review of reproducibility research in natural language processing",
    "citation_count": 50,
    "authors": [
      "Anya Belz",
      "Shubham Agarwal",
      "Anastasia Shimorina",
      "Ehud Reiter"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.30": {
    "title": "Bootstrapping Multilingual AMR with Contextual Word Alignments",
    "volume": "main",
    "abstract": "We develop high performance multilingual Abstract Meaning Representation (AMR) systems by projecting English AMR annotations to other languages with weak supervision. We achieve this goal by bootstrapping transformer-based multilingual word embeddings, in particular those from cross-lingual RoBERTa (XLM-R large). We develop a novel technique for foreign-text-to-English AMR alignment, using the contextual word alignment between English and foreign language tokens. This word alignment is weakly supervised and relies on the contextualized XLM-R word embeddings. We achieve a highly competitive performance that surpasses the best published results for German, Italian, Spanish and Chinese",
    "checked": true,
    "id": "b8e01f7f2db416ae00441796a2365a5d0ab0ee3c",
    "semantic_title": "bootstrapping multilingual amr with contextual word alignments",
    "citation_count": 10,
    "authors": [
      "Janaki Sheth",
      "Young-Suk Lee",
      "Ramón Fernandez Astudillo",
      "Tahira Naseem",
      "Radu Florian",
      "Salim Roukos",
      "Todd Ward"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.31": {
    "title": "Semantic Oppositeness Assisted Deep Contextual Modeling for Automatic Rumor Detection in Social Networks",
    "volume": "main",
    "abstract": "Social networks face a major challenge in the form of rumors and fake news, due to their intrinsic nature of connecting users to millions of others, and of giving any individual the power to post anything. Given the rapid, widespread dissemination of information in social networks, manually detecting suspicious news is sub-optimal. Thus, research on automatic rumor detection has become a necessity. Previous works in the domain have utilized the reply relations between posts, as well as the semantic similarity between the main post and its context, consisting of replies, in order to obtain state-of-the-art performance. In this work, we demonstrate that semantic oppositeness can improve the performance on the task of rumor detection. We show that semantic oppositeness captures elements of discord, which are not properly covered by previous efforts, which only utilize semantic similarity or reply structure. We show, with extensive experiments on recent data sets for this problem, that our proposed model achieves state-of-the-art performance. Further, we show that our model is more resistant to the variances in performance introduced by randomness",
    "checked": true,
    "id": "fac9c6548d1644669ac142465d3060f0bc6992bd",
    "semantic_title": "semantic oppositeness assisted deep contextual modeling for automatic rumor detection in social networks",
    "citation_count": 7,
    "authors": [
      "Nisansa de Silva",
      "Dejing Dou"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.32": {
    "title": "Polarized-VAE: Proximity Based Disentangled Representation Learning for Text Generation",
    "volume": "main",
    "abstract": "Learning disentangled representations of realworld data is a challenging open problem. Most previous methods have focused on either supervised approaches which use attribute labels or unsupervised approaches that manipulate the factorization in the latent space of models such as the variational autoencoder (VAE) by training with task-specific losses. In this work, we propose polarized-VAE, an approach that disentangles select attributes in the latent space based on proximity measures reflecting the similarity between data points with respect to these attributes. We apply our method to disentangle the semantics and syntax of sentences and carry out transfer experiments. Polarized-VAE outperforms the VAE baseline and is competitive with state-of-the-art approaches, while being more a general framework that is applicable to other attribute disentanglement tasks",
    "checked": true,
    "id": "bde93bfbe651a294398979c48e753c0258c38292",
    "semantic_title": "polarized-vae: proximity based disentangled representation learning for text generation",
    "citation_count": 18,
    "authors": [
      "Vikash Balasubramanian",
      "Ivan Kobyzev",
      "Hareesh Bahuleyan",
      "Ilya Shapiro",
      "Olga Vechtomova"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.33": {
    "title": "ParaSCI: A Large Scientific Paraphrase Dataset for Longer Paraphrase Generation",
    "volume": "main",
    "abstract": "We propose ParaSCI, the first large-scale paraphrase dataset in the scientific field, including 33,981 paraphrase pairs from ACL (ParaSCI-ACL) and 316,063 pairs from arXiv (ParaSCI-arXiv). Digging into characteristics and common patterns of scientific papers, we construct this dataset though intra-paper and inter-paper methods, such as collecting citations to the same paper or aggregating definitions by scientific terms. To take advantage of sentences paraphrased partially, we put up PDBERT as a general paraphrase discovering method. The major advantages of paraphrases in ParaSCI lie in the prominent length and textual diversity, which is complementary to existing paraphrase datasets. ParaSCI obtains satisfactory results on human evaluation and downstream tasks, especially long paraphrase generation",
    "checked": true,
    "id": "56b4a292ddfbc2b910bbf5f9ddcfff7b6d6e778a",
    "semantic_title": "parasci: a large scientific paraphrase dataset for longer paraphrase generation",
    "citation_count": 28,
    "authors": [
      "Qingxiu Dong",
      "Xiaojun Wan",
      "Yue Cao"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.34": {
    "title": "Discourse Understanding and Factual Consistency in Abstractive Summarization",
    "volume": "main",
    "abstract": "We introduce a general framework for abstractive summarization with factual consistency and distinct modeling of the narrative flow in an output summary. Our work addresses current limitations of models for abstractive summarization that often hallucinate information or generate summaries with coherence issues. To generate abstractive summaries with factual consistency and narrative flow, we propose Cooperative Generator-Discriminator Networks (Co-opNet), a novel transformer-based framework where the generator works with a discriminator architecture to compose coherent long-form summaries. We explore four different discriminator objectives which each capture a different aspect of coherence, including whether salient spans of generated abstracts are hallucinated or appear in the input context, and the likelihood of sentence adjacency in generated abstracts. We measure the ability of Co-opNet to learn these objectives with arXiv scientific papers, using the abstracts as a proxy for gold long-form scientific article summaries. Empirical results from automatic and human evaluations demonstrate that Co-opNet learns to summarize with considerably improved global coherence compared to competitive baselines",
    "checked": true,
    "id": "cf6c88c892b7f4b1e92cb15a067fa49d1e089d64",
    "semantic_title": "discourse understanding and factual consistency in abstractive summarization",
    "citation_count": 20,
    "authors": [
      "Saadia Gabriel",
      "Antoine Bosselut",
      "Jeff Da",
      "Ari Holtzman",
      "Jan Buys",
      "Kyle Lo",
      "Asli Celikyilmaz",
      "Yejin Choi"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.35": {
    "title": "Knowledge Base Question Answering through Recursive Hypergraphs",
    "volume": "main",
    "abstract": "Knowledge Base Question Answering (KBQA) is the problem of predicting an answer for a factoid question over a given knowledge base (KB). Answering questions typically requires reasoning over multiple links in the given KB. Humans tend to answer questions by grouping different objects to perform reasoning over acquired knowledge. Hypergraphs provide a natural tool to model group relationships. In this work, inspired by typical human intelligence, we propose a new method for KBQA based on hypergraphs. Existing methods for KBQA, though effective, do not explicitly incorporate the recursive relational group structure in the given KB. Our method, which we name RecHyperNet (Recursive Hypergraph Network), exploits a new way of modelling KBs through recursive hypergraphs to organise such group relationships in KBs. Experiments on multiple KBQA benchmarks demonstrate the effectiveness of the proposed RecHyperNet. We have released the code",
    "checked": true,
    "id": "04b8b0dc6f92f27617ac27fa4a432715587c7eed",
    "semantic_title": "knowledge base question answering through recursive hypergraphs",
    "citation_count": 11,
    "authors": [
      "Naganand Yadati",
      "Dayanidhi R S",
      "Vaishnavi S",
      "Indira K M",
      "Srinidhi G"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.36": {
    "title": "FEWS: Large-Scale, Low-Shot Word Sense Disambiguation with the Dictionary",
    "volume": "main",
    "abstract": "Current models for Word Sense Disambiguation (WSD) struggle to disambiguate rare senses, despite reaching human performance on global WSD metrics. This stems from a lack of data for both modeling and evaluating rare senses in existing WSD datasets. In this paper, we introduce FEWS (Few-shot Examples of Word Senses), a new low-shot WSD dataset automatically extracted from example sentences in Wiktionary. FEWS has high sense coverage across different natural language domains and provides: (1) a large training set that covers many more senses than previous datasets and (2) a comprehensive evaluation set containing few- and zero-shot examples of a wide variety of senses. We establish baselines on FEWS with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with FEWS better capture rare senses in existing WSD datasets. Finally, we find humans outperform the best baseline models on FEWS, indicating that FEWS will support significant future work on low-shot WSD",
    "checked": true,
    "id": "00ae9b2ab1d5cade91c91b8c10f59e939e981f71",
    "semantic_title": "fews: large-scale, low-shot word sense disambiguation with the dictionary",
    "citation_count": 15,
    "authors": [
      "Terra Blevins",
      "Mandar Joshi",
      "Luke Zettlemoyer"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.37": {
    "title": "MONAH: Multi-Modal Narratives for Humans to analyze conversations",
    "volume": "main",
    "abstract": "In conversational analyses, humans manually weave multimodal information into the transcripts, which is significantly time-consuming. We introduce a system that automatically expands the verbatim transcripts of video-recorded conversations using multimodal data streams. This system uses a set of preprocessing rules to weave multimodal annotations into the verbatim transcripts and promote interpretability. Our feature engineering contributions are two-fold: firstly, we identify the range of multimodal features relevant to detect rapport-building; secondly, we expand the range of multimodal annotations and show that the expansion leads to statistically significant improvements in detecting rapport-building",
    "checked": true,
    "id": "b16416021b4149794df9408ddfa3e374d838f0f6",
    "semantic_title": "monah: multi-modal narratives for humans to analyze conversations",
    "citation_count": 6,
    "authors": [
      "Joshua Y. Kim",
      "Kalina Yacef",
      "Greyson Kim",
      "Chunfeng Liu",
      "Rafael Calvo",
      "Silas Taylor"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.38": {
    "title": "Does Typological Blinding Impede Cross-Lingual Sharing?",
    "volume": "main",
    "abstract": "Bridging the performance gap between high- and low-resource languages has been the focus of much previous work. Typological features from databases such as the World Atlas of Language Structures (WALS) are a prime candidate for this, as such data exists even for very low-resource languages. However, previous work has only found minor benefits from using typological information. Our hypothesis is that a model trained in a cross-lingual setting will pick up on typological cues from the input data, thus overshadowing the utility of explicitly using such features. We verify this hypothesis by blinding a model to typological information, and investigate how cross-lingual sharing and performance is impacted. Our model is based on a cross-lingual architecture in which the latent weights governing the sharing between languages is learnt during training. We show that (i) preventing this model from exploiting typology severely reduces performance, while a control experiment reaffirms that (ii) encouraging sharing according to typology somewhat improves performance",
    "checked": true,
    "id": "2ff6d2b3114dec4cbbeb815a0bee70a9e09a82b6",
    "semantic_title": "does typological blinding impede cross-lingual sharing?",
    "citation_count": 17,
    "authors": [
      "Johannes Bjerva",
      "Isabelle Augenstein"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.39": {
    "title": "AdapterFusion: Non-Destructive Task Composition for Transfer Learning",
    "volume": "main",
    "abstract": "Sequential fine-tuning and multi-task learning are methods aiming to incorporate knowledge from multiple tasks; however, they suffer from catastrophic forgetting and difficulties in dataset balancing. To address these shortcomings, we propose AdapterFusion, a new two stage learning algorithm that leverages knowledge from multiple tasks. First, in the knowledge extraction stage we learn task specific parameters called adapters, that encapsulate the task-specific information. We then combine the adapters in a separate knowledge composition step. We show that by separating the two stages, i.e., knowledge extraction and knowledge composition, the classifier can effectively exploit the representations learned from multiple tasks in a non-destructive manner. We empirically evaluate AdapterFusion on 16 diverse NLU tasks, and find that it effectively combines various types of knowledge at different layers of the model. We show that our approach outperforms traditional strategies such as full fine-tuning as well as multi-task learning. Our code and adapters are available at AdapterHub.ml",
    "checked": true,
    "id": "98ef0db84e62aef969629264c9de1f4d0013f3b9",
    "semantic_title": "adapterfusion: non-destructive task composition for transfer learning",
    "citation_count": 651,
    "authors": [
      "Jonas Pfeiffer",
      "Aishwarya Kamath",
      "Andreas Rücklé",
      "Kyunghyun Cho",
      "Iryna Gurevych"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.40": {
    "title": "CHOLAN: A Modular Approach for Neural Entity Linking on Wikipedia and Wikidata",
    "volume": "main",
    "abstract": "In this paper, we propose CHOLAN, a modular approach to target end-to-end entity linking (EL) over knowledge bases. CHOLAN consists of a pipeline of two transformer-based models integrated sequentially to accomplish the EL task. The first transformer model identifies surface forms (entity mentions) in a given text. For each mention, a second transformer model is employed to classify the target entity among a predefined candidates list. The latter transformer is fed by an enriched context captured from the sentence (i.e. local context), and entity description gained from Wikipedia. Such external contexts have not been used in state of the art EL approaches. Our empirical study was conducted on two well-known knowledge bases (i.e., Wikidata and Wikipedia). The empirical results suggest that CHOLAN outperforms state-of-the-art approaches on standard datasets such as CoNLL-AIDA, MSNBC, AQUAINT, ACE2004, and T-REx",
    "checked": true,
    "id": "2f1b6164b8ee2f354a9ed3b7699e9f13985ba5b4",
    "semantic_title": "cholan: a modular approach for neural entity linking on wikipedia and wikidata",
    "citation_count": 32,
    "authors": [
      "Manoj Prabhakar Kannan Ravi",
      "Kuldeep Singh",
      "Isaiah Onando Mulang’",
      "Saeedeh Shekarpour",
      "Johannes Hoffart",
      "Jens Lehmann"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.41": {
    "title": "Grounding as a Collaborative Process",
    "volume": "main",
    "abstract": "Collaborative grounding is a fundamental aspect of human-human dialog which allows people to negotiate meaning. In this paper we argue that it is missing from current deep learning approaches to dialog. Our central point is that making mistakes and being able to recover from them collaboratively is a key ingredient in grounding meaning. We illustrate the pitfalls of being unable to ground collaboratively, discuss what can be learned from the language acquisition and dialog systems literature, and reflect on how to move forward",
    "checked": true,
    "id": "1c8bda8ac573c1496aa91088e22d0b41215ddb7e",
    "semantic_title": "grounding as a collaborative process",
    "citation_count": 17,
    "authors": [
      "Luciana Benotti",
      "Patrick Blackburn"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.42": {
    "title": "Does She Wink or Does She Nod? A Challenging Benchmark for Evaluating Word Understanding of Language Models",
    "volume": "main",
    "abstract": "Recent progress in pretraining language models on large corpora has resulted in significant performance gains on many NLP tasks. These large models acquire linguistic knowledge during pretraining, which helps to improve performance on downstream tasks via fine-tuning. To assess what kind of knowledge is acquired, language models are commonly probed by querying them with ‘fill in the blank' style cloze questions. Existing probing datasets mainly focus on knowledge about relations between words and entities. We introduce WDLMPro (Word Definitions Language Model Probing) to evaluate word understanding directly using dictionary definitions of words. In our experiments, three popular pretrained language models struggle to match words and their definitions. This indicates that they understand many words poorly and that our new probing task is a difficult challenge that could help guide research on LMs in the future",
    "checked": true,
    "id": "cbd4f06a08eb4223b97c9079007a87dda4339afe",
    "semantic_title": "does she wink or does she nod? a challenging benchmark for evaluating word understanding of language models",
    "citation_count": 5,
    "authors": [
      "Lutfi Kerem Senel",
      "Hinrich Schütze"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.43": {
    "title": "Joint Coreference Resolution and Character Linking for Multiparty Conversation",
    "volume": "main",
    "abstract": "Character linking, the task of linking mentioned people in conversations to the real world, is crucial for understanding the conversations. For the efficiency of communication, humans often choose to use pronouns (e.g., \"she\") or normal entities (e.g., \"that girl\") rather than named entities (e.g., \"Rachel\") in the spoken language, which makes linking those mentions to real people a much more challenging than a regular entity linking task. To address this challenge, we propose to incorporate the richer context from the coreference relations among different mentions to help the linking. On the other hand, considering that finding coreference clusters itself is not a trivial task and could benefit from the global character information, we propose to jointly solve these two tasks. Specifically, we propose Cˆ2, the joint learning model of Coreference resolution and Character linking. The experimental results demonstrate that Cˆ2 can significantly outperform previous works on both tasks. Further analyses are conducted to analyze the contribution of all modules in the proposed model and the effect of all hyper-parameters",
    "checked": true,
    "id": "13e0600466c10f1cd7be55c5c78ee62b6d073a80",
    "semantic_title": "joint coreference resolution and character linking for multiparty conversation",
    "citation_count": 7,
    "authors": [
      "Jiaxin Bai",
      "Hongming Zhang",
      "Yangqiu Song",
      "Kun Xu"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.44": {
    "title": "Improving Factual Consistency Between a Response and Persona Facts",
    "volume": "main",
    "abstract": "Neural models for response generation produce responses that are semantically plausible but not necessarily factually consistent with facts describing the speaker's persona. These models are trained with fully supervised learning where the objective function barely captures factual consistency. We propose to fine-tune these models by reinforcement learning and an efficient reward function that explicitly captures the consistency between a response and persona facts as well as semantic plausibility. Our automatic and human evaluations on the PersonaChat corpus confirm that our approach increases the rate of responses that are factually consistent with persona facts over its supervised counterpart while retains the language quality of responses",
    "checked": true,
    "id": "3f4bef1f909468bf7179472b9b160d5463b76e22",
    "semantic_title": "improving factual consistency between a response and persona facts",
    "citation_count": 25,
    "authors": [
      "Mohsen Mesgar",
      "Edwin Simpson",
      "Iryna Gurevych"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.45": {
    "title": "PolyLM: Learning about Polysemy through Language Modeling",
    "volume": "main",
    "abstract": "To avoid the \"meaning conflation deficiency\" of word embeddings, a number of models have aimed to embed individual word senses. These methods at one time performed well on tasks such as word sense induction (WSI), but they have since been overtaken by task-specific techniques which exploit contextualized embeddings. However, sense embeddings and contextualization need not be mutually exclusive. We introduce PolyLM, a method which formulates the task of learning sense embeddings as a language modeling problem, allowing contextualization techniques to be applied. PolyLM is based on two underlying assumptions about word senses: firstly, that the probability of a word occurring in a given context is equal to the sum of the probabilities of its individual senses occurring; and secondly, that for a given occurrence of a word, one of its senses tends to be much more plausible in the context than the others. We evaluate PolyLM on WSI, showing that it performs considerably better than previous sense embedding techniques, and matches the current state-of-the-art specialized WSI method despite having six times fewer parameters. Code and pre-trained models are available at https://github.com/AlanAnsell/PolyLM",
    "checked": true,
    "id": "5ea131f148299adb97e36a2001b08c468b4bfff0",
    "semantic_title": "polylm: learning about polysemy through language modeling",
    "citation_count": 6,
    "authors": [
      "Alan Ansell",
      "Felipe Bravo-Marquez",
      "Bernhard Pfahringer"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.46": {
    "title": "Predicting Treatment Outcome from Patient Texts:The Case of Internet-Based Cognitive Behavioural Therapy",
    "volume": "main",
    "abstract": "We investigate the feasibility of applying standard text categorisation methods to patient text in order to predict treatment outcome in Internet-based cognitive behavioural therapy. The data set is unique in its detail and size for regular care for depression, social anxiety, and panic disorder. Our results indicate that there is a signal in the depression data, albeit a weak one. We also perform terminological and sentiment analysis, which confirm those results",
    "checked": true,
    "id": "19e3e09042b4f1adae79d799b9bb4db492ea3884",
    "semantic_title": "predicting treatment outcome from patient texts:the case of internet-based cognitive behavioural therapy",
    "citation_count": 5,
    "authors": [
      "Evangelia Gogoulou",
      "Magnus Boman",
      "Fehmi Ben Abdesslem",
      "Nils Hentati Isacsson",
      "Viktor Kaldo",
      "Magnus Sahlgren"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.47": {
    "title": "Scalable Evaluation and Improvement of Document Set Expansion via Neural Positive-Unlabeled Learning",
    "volume": "main",
    "abstract": "We consider the situation in which a user has collected a small set of documents on a cohesive topic, and they want to retrieve additional documents on this topic from a large collection. Information Retrieval (IR) solutions treat the document set as a query, and look for similar documents in the collection. We propose to extend the IR approach by treating the problem as an instance of positive-unlabeled (PU) learning—i.e., learning binary classifiers from only positive (the query documents) and unlabeled (the results of the IR engine) data. Utilizing PU learning for text with big neural networks is a largely unexplored field. We discuss various challenges in applying PU learning to the setting, showing that the standard implementations of state-of-the-art PU solutions fail. We propose solutions for each of the challenges and empirically validate them with ablation tests. We demonstrate the effectiveness of the new method using a series of experiments of retrieving PubMed abstracts adhering to fine-grained topics, showing improvements over the common IR solution and other baselines",
    "checked": true,
    "id": "886467f05dbf3e18aecd7e994226b1ecd7fb27f1",
    "semantic_title": "scalable evaluation and improvement of document set expansion via neural positive-unlabeled learning",
    "citation_count": 7,
    "authors": [
      "Alon Jacovi",
      "Gang Niu",
      "Yoav Goldberg",
      "Masashi Sugiyama"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.48": {
    "title": "The Role of Syntactic Planning in Compositional Image Captioning",
    "volume": "main",
    "abstract": "Image captioning has focused on generalizing to images drawn from the same distribution as the training set, and not to the more challenging problem of generalizing to different distributions of images. Recently, Nikolaus et al. (2019) introduced a dataset to assess compositional generalization in image captioning, where models are evaluated on their ability to describe images with unseen adjective–noun and noun–verb compositions. In this work, we investigate different methods to improve compositional generalization by planning the syntactic structure of a caption. Our experiments show that jointly modeling tokens and syntactic tags enhances generalization in both RNN- and Transformer-based models, while also improving performance on standard metrics",
    "checked": true,
    "id": "10161db52bfa53bdab84ae97b47cef2f22119131",
    "semantic_title": "the role of syntactic planning in compositional image captioning",
    "citation_count": 10,
    "authors": [
      "Emanuele Bugliarello",
      "Desmond Elliott"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.49": {
    "title": "Is \"hot pizza\" Positive or Negative? Mining Target-aware Sentiment Lexicons",
    "volume": "main",
    "abstract": "Modelling a word's polarity in different contexts is a key task in sentiment analysis. Previous works mainly focus on domain dependencies, and assume words' sentiments are invariant within a specific domain. In this paper, we relax this assumption by binding a word's sentiment to its collocation words instead of domain labels. This finer view of sentiment contexts is particularly useful for identifying commonsense sentiments expressed in neural words such as \"big\" and \"long\". Given a target (e.g., an aspect), we propose an effective \"perturb-and-see\" method to extract sentiment words modifying it from large-scale datasets. The reliability of the obtained target-aware sentiment lexicons is extensively evaluated both manually and automatically. We also show that a simple application of the lexicon is able to achieve highly competitive performances on the unsupervised opinion relation extraction task",
    "checked": true,
    "id": "fece36e3b2fd4e8d4efa99190cb1bbcb671d1b97",
    "semantic_title": "is \"hot pizza\" positive or negative? mining target-aware sentiment lexicons",
    "citation_count": 2,
    "authors": [
      "Jie Zhou",
      "Yuanbin Wu",
      "Changzhi Sun",
      "Liang He"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.50": {
    "title": "Quality Estimation without Human-labeled Data",
    "volume": "main",
    "abstract": "Quality estimation aims to measure the quality of translated content without access to a reference translation. This is crucial for machine translation systems in real-world scenarios where high-quality translation is needed. While many approaches exist for quality estimation, they are based on supervised machine learning requiring costly human labelled data. As an alternative, we propose a technique that does not rely on examples from human-annotators and instead uses synthetic training data. We train off-the-shelf architectures for supervised quality estimation on our synthetic data and show that the resulting models achieve comparable performance to models trained on human-annotated data, both for sentence and word-level prediction",
    "checked": true,
    "id": "b4bfabb2b8dc1c41038fbf26d2d74196e6fd75bc",
    "semantic_title": "quality estimation without human-labeled data",
    "citation_count": 22,
    "authors": [
      "Yi-Lin Tuan",
      "Ahmed El-Kishky",
      "Adithya Renduchintala",
      "Vishrav Chaudhary",
      "Francisco Guzmán",
      "Lucia Specia"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.51": {
    "title": "How Fast can BERT Learn Simple Natural Language Inference?",
    "volume": "main",
    "abstract": "This paper empirically studies whether BERT can really learn to conduct natural language inference (NLI) without utilizing hidden dataset bias; and how efficiently it can learn if it could. This is done via creating a simple entailment judgment case which involves only binary predicates in plain English. The results show that the learning process of BERT is very slow. However, the efficiency of learning can be greatly improved (data reduction by a factor of 1,500) if task-related features are added. This suggests that domain knowledge greatly helps when conducting NLI with neural networks",
    "checked": true,
    "id": "8acc75693f781a0475ddc18d0c3d44a2101cd462",
    "semantic_title": "how fast can bert learn simple natural language inference?",
    "citation_count": 9,
    "authors": [
      "Yi-Chung Lin",
      "Keh-Yih Su"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.52": {
    "title": "GRIT: Generative Role-filler Transformers for Document-level Event Entity Extraction",
    "volume": "main",
    "abstract": "We revisit the classic problem of document-level role-filler entity extraction (REE) for template filling. We argue that sentence-level approaches are ill-suited to the task and introduce a generative transformer-based encoder-decoder framework (GRIT) that is designed to model context at the document level: it can make extraction decisions across sentence boundaries; is implicitly aware of noun phrase coreference structure, and has the capacity to respect cross-role dependencies in the template structure. We evaluate our approach on the MUC-4 dataset, and show that our model performs substantially better than prior work. We also show that our modeling choices contribute to model performance, e.g., by implicitly capturing linguistic knowledge such as recognizing coreferent entity mentions",
    "checked": true,
    "id": "eb54c33b170782eb3c16f206a263abfc551b9c51",
    "semantic_title": "grit: generative role-filler transformers for document-level event entity extraction",
    "citation_count": 61,
    "authors": [
      "Xinya Du",
      "Alexander Rush",
      "Claire Cardie"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.53": {
    "title": "Cross-lingual Entity Alignment with Incidental Supervision",
    "volume": "main",
    "abstract": "Much research effort has been put to multilingual knowledge graph (KG) embedding methods to address the entity alignment task, which seeks to match entities in different languagespecific KGs that refer to the same real-world object. Such methods are often hindered by the insufficiency of seed alignment provided between KGs. Therefore, we propose a new model, JEANS , which jointly represents multilingual KGs and text corpora in a shared embedding scheme, and seeks to improve entity alignment with incidental supervision signals from text. JEANS first deploys an entity grounding process to combine each KG with the monolingual text corpus. Then, two learning processes are conducted: (i) an embedding learning process to encode the KG and text of each language in one embedding space, and (ii) a self-learning based alignment learning process to iteratively induce the correspondence of entities and that of lexemes between embeddings. Experiments on benchmark datasets show that JEANS leads to promising improvement on entity alignment with incidental supervision, and significantly outperforms state-of-the-art methods that solely rely on internal information of KGs",
    "checked": true,
    "id": "d49b8f319d41bd97a85155efdff54b0e98dd3a98",
    "semantic_title": "cross-lingual entity alignment with incidental supervision",
    "citation_count": 36,
    "authors": [
      "Muhao Chen",
      "Weijia Shi",
      "Ben Zhou",
      "Dan Roth"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.54": {
    "title": "Query Generation for Multimodal Documents",
    "volume": "main",
    "abstract": "This paper studies the problem of generatinglikely queries for multimodal documents withimages. Our application scenario is enablingefficient \"first-stage retrieval\" of relevant doc-uments, by attaching generated queries to doc-uments before indexing. We can then indexthis expanded text to efficiently narrow downto candidate matches using inverted index, sothat expensive reranking can follow. Our eval-uation results show that our proposed multi-modal representation meaningfully improvesrelevance ranking. More importantly, ourframework can achieve the state of the art inthe first stage retrieval scenarios",
    "checked": true,
    "id": "31b5b8b3e90305262def70df41e264eb2992f77c",
    "semantic_title": "query generation for multimodal documents",
    "citation_count": 5,
    "authors": [
      "Kyungho Kim",
      "Kyungjae Lee",
      "Seung-won Hwang",
      "Young-In Song",
      "Seungwook Lee"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.55": {
    "title": "End-to-End Argument Mining as Biaffine Dependency Parsing",
    "volume": "main",
    "abstract": "Non-neural approaches to argument mining (AM) are often pipelined and require heavy feature-engineering. In this paper, we propose a neural end-to-end approach to AM which is based on dependency parsing, in contrast to the current state-of-the-art which relies on relation extraction. Our biaffine AM dependency parser significantly outperforms the state-of-the-art, performing at F1 = 73.5% for component identification and F1 = 46.4% for relation identification. One of the advantages of treating AM as biaffine dependency parsing is the simple neural architecture that results. The idea of treating AM as dependency parsing is not new, but has previously been abandoned as it was lagging far behind the state-of-the-art. In a thorough analysis, we investigate the factors that contribute to the success of our model: the biaffine model itself, our representation for the dependency structure of arguments, different encoders in the biaffine model, and syntactic information additionally fed to the model. Our work demonstrates that dependency parsing for AM, an overlooked idea from the past, deserves more attention in the future",
    "checked": true,
    "id": "74ffab9da62788160681e86d3d36ae578df7c977",
    "semantic_title": "end-to-end argument mining as biaffine dependency parsing",
    "citation_count": 23,
    "authors": [
      "Yuxiao Ye",
      "Simone Teufel"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.56": {
    "title": "FakeFlow: Fake News Detection by Modeling the Flow of Affective Information",
    "volume": "main",
    "abstract": "Fake news articles often stir the readers' attention by means of emotional appeals that arouse their feelings. Unlike in short news texts, authors of longer articles can exploit such affective factors to manipulate readers by adding exaggerations or fabricating events, in order to affect the readers' emotions. To capture this, we propose in this paper to model the flow of affective information in fake news articles using a neural architecture. The proposed model, FakeFlow, learns this flow by combining topic and affective information extracted from text. We evaluate the model's performance with several experiments on four real-world datasets. The results show that FakeFlow achieves superior results when compared against state-of-the-art methods, thus confirming the importance of capturing the flow of the affective information in news articles",
    "checked": true,
    "id": "28625648fae0a1ba915c5d2a19c312841afc1b1c",
    "semantic_title": "fakeflow: fake news detection by modeling the flow of affective information",
    "citation_count": 57,
    "authors": [
      "Bilal Ghanem",
      "Simone Paolo Ponzetto",
      "Paolo Rosso",
      "Francisco Rangel"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.57": {
    "title": "CTC-based Compression for Direct Speech Translation",
    "volume": "main",
    "abstract": "Previous studies demonstrated that a dynamic phone-informed compression of the input audio is beneficial for speech translation (ST). However, they required a dedicated model for phone recognition and did not test this solution for direct ST, in which a single model translates the input audio into the target language without intermediate representations. In this work, we propose the first method able to perform a dynamic compression of the input in direct ST models. In particular, we exploit the Connectionist Temporal Classification (CTC) to compress the input sequence according to its phonetic characteristics. Our experiments demonstrate that our solution brings a 1.3-1.5 BLEU improvement over a strong baseline on two language pairs (English-Italian and English-German), contextually reducing the memory footprint by more than 10%",
    "checked": true,
    "id": "62316c538d3d9dcb6ae8a3567433f7417eab1f32",
    "semantic_title": "ctc-based compression for direct speech translation",
    "citation_count": 47,
    "authors": [
      "Marco Gaido",
      "Mauro Cettolo",
      "Matteo Negri",
      "Marco Turchi"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.58": {
    "title": "A Crowdsourced Open-Source Kazakh Speech Corpus and Initial Speech Recognition Baseline",
    "volume": "main",
    "abstract": "We present an open-source speech corpus for the Kazakh language. The Kazakh speech corpus (KSC) contains around 332 hours of transcribed audio comprising over 153,000 utterances spoken by participants from different regions and age groups, as well as both genders. It was carefully inspected by native Kazakh speakers to ensure high quality. The KSC is the largest publicly available database developed to advance various Kazakh speech and language processing applications. In this paper, we first describe the data collection and preprocessing procedures followed by a description of the database specifications. We also share our experience and challenges faced during the database construction, which might benefit other researchers planning to build a speech corpus for a low-resource language. To demonstrate the reliability of the database, we performed preliminary speech recognition experiments. The experimental results imply that the quality of audio and transcripts is promising (2.8% character error rate and 8.7% word error rate on the test set). To enable experiment reproducibility and ease the corpus usage, we also released an ESPnet recipe for our speech recognition models",
    "checked": true,
    "id": "cccbc895835037feb95eec73cf7c17106f7aa96a",
    "semantic_title": "a crowdsourced open-source kazakh speech corpus and initial speech recognition baseline",
    "citation_count": 27,
    "authors": [
      "Yerbolat Khassanov",
      "Saida Mussakhojayeva",
      "Almas Mirzakhmetov",
      "Alen Adiyev",
      "Mukhamet Nurpeiissov",
      "Huseyin Atakan Varol"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.59": {
    "title": "TDMSci: A Specialized Corpus for Scientific Literature Entity Tagging of Tasks Datasets and Metrics",
    "volume": "main",
    "abstract": "Tasks, Datasets and Evaluation Metrics are important concepts for understanding experimental scientific papers. However, previous work on information extraction for scientific literature mainly focuses on the abstracts only, and does not treat datasets as a separate type of entity (Zadeh and Schumann, 2016; Luan et al., 2018). In this paper, we present a new corpus that contains domain expert annotations for Task (T), Dataset (D), Metric (M) entities 2,000 sentences extracted from NLP papers. We report experiment results on TDM extraction using a simple data augmentation strategy and apply our tagger to around 30,000 NLP papers from the ACL Anthology. The corpus is made publicly available to the community for fostering research on scientific publication summarization (Erera et al., 2019) and knowledge discovery",
    "checked": true,
    "id": "3e8fa31c5da0c0e452d7ee4cbab8d0248afe318e",
    "semantic_title": "tdmsci: a specialized corpus for scientific literature entity tagging of tasks datasets and metrics",
    "citation_count": 36,
    "authors": [
      "Yufang Hou",
      "Charles Jochim",
      "Martin Gleize",
      "Francesca Bonin",
      "Debasis Ganguly"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.60": {
    "title": "Top-down Discourse Parsing via Sequence Labelling",
    "volume": "main",
    "abstract": "We introduce a top-down approach to discourse parsing that is conceptually simpler than its predecessors (Kobayashi et al., 2020; Zhang et al., 2020). By framing the task as a sequence labelling problem where the goal is to iteratively segment a document into individual discourse units, we are able to eliminate the decoder and reduce the search space for splitting points. We explore both traditional recurrent models and modern pre-trained transformer models for the task, and additionally introduce a novel dynamic oracle for top-down parsing. Based on the Full metric, our proposed LSTM model sets a new state-of-the-art for RST parsing",
    "checked": true,
    "id": "447ecc4deb0ab7b112e6341cd5aad9ee220e986f",
    "semantic_title": "top-down discourse parsing via sequence labelling",
    "citation_count": 25,
    "authors": [
      "Fajri Koto",
      "Jey Han Lau",
      "Timothy Baldwin"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.61": {
    "title": "Does the Order of Training Samples Matter? Improving Neural Data-to-Text Generation with Curriculum Learning",
    "volume": "main",
    "abstract": "Recent advancements in data-to-text generation largely take on the form of neural end-to-end systems. Efforts have been dedicated to improving text generation systems by changing the order of training samples in a process known as curriculum learning. Past research on sequence-to-sequence learning showed that curriculum learning helps to improve both the performance and convergence speed. In this work, we delve into the same idea surrounding the training samples consisting of structured data and text pairs, where at each update, the curriculum framework selects training samples based on the model's competence. Specifically, we experiment with various difficulty metrics and put forward a soft edit distance metric for ranking training samples. On our benchmarks, it shows faster convergence speed where training time is reduced by 38.7% and performance is boosted by 4.84 BLEU",
    "checked": true,
    "id": "c1bf19dcbdda44149369f0c9e4d2b3c4bccaa2f3",
    "semantic_title": "does the order of training samples matter? improving neural data-to-text generation with curriculum learning",
    "citation_count": 24,
    "authors": [
      "Ernie Chang",
      "Hui-Syuan Yeh",
      "Vera Demberg"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.62": {
    "title": "TrNews: Heterogeneous User-Interest Transfer Learning for News Recommendation",
    "volume": "main",
    "abstract": "We investigate how to solve the cross-corpus news recommendation for unseen users in the future. This is a problem where traditional content-based recommendation techniques often fail. Luckily, in real-world recommendation services, some publisher (e.g., Daily news) may have accumulated a large corpus with lots of consumers which can be used for a newly deployed publisher (e.g., Political news). To take advantage of the existing corpus, we propose a transfer learning model (dubbed as TrNews) for news recommendation to transfer the knowledge from a source corpus to a target corpus. To tackle the heterogeneity of different user interests and of different word distributions across corpora, we design a translator-based transfer-learning strategy to learn a representation mapping between source and target corpora. The learned translator can be used to generate representations for unseen users in the future. We show through experiments on real-world datasets that TrNews is better than various baselines in terms of four metrics. We also show that our translator is effective among existing transfer strategies",
    "checked": true,
    "id": "4f511cf7a06eed7952a425c4d8064689ba283b9c",
    "semantic_title": "trnews: heterogeneous user-interest transfer learning for news recommendation",
    "citation_count": 2,
    "authors": [
      "Guangneng Hu",
      "Qiang Yang"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.63": {
    "title": "Dialogue Act-based Breakdown Detection in Negotiation Dialogues",
    "volume": "main",
    "abstract": "Thanks to the success of goal-oriented negotiation dialogue systems, studies of negotiation dialogue have gained momentum in terms of both human-human negotiation support and dialogue systems. However, the field suffers from a paucity of available negotiation corpora, which hinders further development and makes it difficult to test new methodologies in novel negotiation settings. Here, we share a human-human negotiation dialogue dataset in a job interview scenario that features increased complexities in terms of the number of possible solutions and a utility function. We test the proposed corpus using a breakdown detection task for human-human negotiation support. We also introduce a dialogue act-based breakdown detection method, focusing on dialogue flow that is applicable to various corpora. Our results show that our proposed method features comparable detection performance to text-based approaches in existing corpora and better results in the proposed dataset",
    "checked": true,
    "id": "fb367084876f836bcb33b710f0dd366af69513a1",
    "semantic_title": "dialogue act-based breakdown detection in negotiation dialogues",
    "citation_count": 11,
    "authors": [
      "Atsuki Yamaguchi",
      "Kosui Iwasa",
      "Katsuhide Fujita"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.64": {
    "title": "Neural Data-to-Text Generation with LM-based Text Augmentation",
    "volume": "main",
    "abstract": "For many new application domains for data-to-text generation, the main obstacle in training neural models consists of a lack of training data. While usually large numbers of instances are available on the data side, often only very few text samples are available. To address this problem, we here propose a novel few-shot approach for this setting. Our approach automatically augments the data available for training by (i) generating new text samples based on replacing specific values by alternative ones from the same category, (ii) generating new text samples based on GPT-2, and (iii) proposing an automatic method for pairing the new text samples with data samples. As the text augmentation can introduce noise to the training data, we use cycle consistency as an objective, in order to make sure that a given data sample can be correctly reconstructed after having been formulated as text (and that text samples can be reconstructed from data). On both the E2E and WebNLG benchmarks, we show that this weakly supervised training paradigm is able to outperform fully supervised sequence-to-sequence models with less than 10% of the training set. By utilizing all annotated data, our model can boost the performance of a standard sequence-to-sequence model by over 5 BLEU points, establishing a new state-of-the-art on both datasets",
    "checked": true,
    "id": "c7ab9761a25f730de840e9b140bd77551746314a",
    "semantic_title": "neural data-to-text generation with lm-based text augmentation",
    "citation_count": 45,
    "authors": [
      "Ernie Chang",
      "Xiaoyu Shen",
      "Dawei Zhu",
      "Vera Demberg",
      "Hui Su"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.65": {
    "title": "Self-Training Pre-Trained Language Models for Zero- and Few-Shot Multi-Dialectal Arabic Sequence Labeling",
    "volume": "main",
    "abstract": "A sufficient amount of annotated data is usually required to fine-tune pre-trained language models for downstream tasks. Unfortunately, attaining labeled data can be costly, especially for multiple language varieties and dialects. We propose to self-train pre-trained language models in zero- and few-shot scenarios to improve performance on data-scarce varieties using only resources from data-rich ones. We demonstrate the utility of our approach in the context of Arabic sequence labeling by using a language model fine-tuned on Modern Standard Arabic (MSA) only to predict named entities (NE) and part-of-speech (POS) tags on several dialectal Arabic (DA) varieties. We show that self-training is indeed powerful, improving zero-shot MSA-to-DA transfer by as large as ˷10% F1 (NER) and 2% accuracy (POS tagging). We acquire even better performance in few-shot scenarios with limited amounts of labeled data. We conduct an ablation study and show that the performance boost observed directly results from training data augmentation possible with DA examples via self-training. This opens up opportunities for developing DA models exploiting only MSA resources. Our approach can also be extended to other languages and tasks",
    "checked": true,
    "id": "7f1f473e6042275149d3589022b76232e3c4c2f5",
    "semantic_title": "self-training pre-trained language models for zero- and few-shot multi-dialectal arabic sequence labeling",
    "citation_count": 13,
    "authors": [
      "Muhammad Khalifa",
      "Muhammad Abdul-Mageed",
      "Khaled Shaalan"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.66": {
    "title": "Multiple Tasks Integration: Tagging, Syntactic and Semantic Parsing as a Single Task",
    "volume": "main",
    "abstract": "Departing from both sequential pipelines and monotask systems, we propose Multiple Tasks Integration (MTI), a multitask paradigm orthogonal to weight sharing. The essence of MTI is to process the input iteratively but concurrently at multiple levels of analysis, where each decision is based on all of the structures that are already inferred and free from usual ordering constraints. We illustrate MTI with a system that performs part-of-speech tagging, syntactic dependency parsing and semantic dependency parsing. We observe that both the use of reinforcement learning and the release from sequential constraints are beneficial to the quality of the syntactic and semantic parses. We also observe that our model adopts an easy-first strategy that consists, on average, of predicting shorter dependencies before longer ones, but that syntax is not always tackled before semantics",
    "checked": true,
    "id": "5a6577d1da19ef9c284cea8dd97e8c23b30490d4",
    "semantic_title": "multiple tasks integration: tagging, syntactic and semantic parsing as a single task",
    "citation_count": 3,
    "authors": [
      "Timothée Bernard"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.67": {
    "title": "Coordinate Constructions in English Enhanced Universal Dependencies: Analysis and Computational Modeling",
    "volume": "main",
    "abstract": "In this paper, we address the representation of coordinate constructions in Enhanced Universal Dependencies (UD), where relevant dependency links are propagated from conjunction heads to other conjuncts. English treebanks for enhanced UD have been created from gold basic dependencies using a heuristic rule-based converter, which propagates only core arguments. With the aim of determining which set of links should be propagated from a semantic perspective, we create a large-scale dataset of manually edited syntax graphs. We identify several systematic errors in the original data, and propose to also propagate adjuncts. We observe high inter-annotator agreement for this semantic annotation task. Using our new manually verified dataset, we perform the first principled comparison of rule-based and (partially novel) machine-learning based methods for conjunction propagation for English. We show that learning propagation rules is more effective than hand-designing heuristic rules. When using automatic parses, our neural graph-parser based edge predictor outperforms the currently predominant pipelines using a basic-layer tree parser plus converters",
    "checked": true,
    "id": "47c6b91b7713bde3a54abd022f3af77c7c0c6750",
    "semantic_title": "coordinate constructions in english enhanced universal dependencies: analysis and computational modeling",
    "citation_count": 8,
    "authors": [
      "Stefan Grünewald",
      "Prisca Piccirilli",
      "Annemarie Friedrich"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.68": {
    "title": "Ellipsis Resolution as Question Answering: An Evaluation",
    "volume": "main",
    "abstract": "Most, if not all forms of ellipsis (e.g., so does Mary) are similar to reading comprehension questions (what does Mary do), in that in order to resolve them, we need to identify an appropriate text span in the preceding discourse. Following this observation, we present an alternative approach for English ellipsis resolution relying on architectures developed for question answering (QA). We present both single-task models, and joint models trained on auxiliary QA and coreference resolution datasets, clearly outperforming the current state of the art for Sluice Ellipsis (from 70.00 to 86.01 F1) and Verb Phrase Ellipsis (from 72.89 to 78.66 F1)",
    "checked": true,
    "id": "64485a223bdeaf9870c040d7bfc439f436233841",
    "semantic_title": "ellipsis resolution as question answering: an evaluation",
    "citation_count": 13,
    "authors": [
      "Rahul Aralikatte",
      "Matthew Lamm",
      "Daniel Hardt",
      "Anders Søgaard"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.69": {
    "title": "Jointly Improving Language Understanding and Generation with Quality-Weighted Weak Supervision of Automatic Labeling",
    "volume": "main",
    "abstract": "Neural natural language generation (NLG) and understanding (NLU) models are data-hungry and require massive amounts of annotated data to be competitive. Recent frameworks address this bottleneck with generative models that synthesize weak labels at scale, where a small amount of training labels are expert-curated and the rest of the data is automatically annotated. We follow that approach, by automatically constructing a large-scale weakly-labeled data with a fine-tuned GPT-2, and employ a semi-supervised framework to jointly train the NLG and NLU models. The proposed framework adapts the parameter updates to the models according to the estimated label-quality. On both the E2E and Weather benchmarks, we show that this weakly supervised training paradigm is an effective approach under low resource scenarios with as little as 10 data instances, and outperforming benchmark systems on both datasets when 100% of the training data is used",
    "checked": true,
    "id": "641bf94ced41d80500ba95b9551edcd6bb05bdaf",
    "semantic_title": "jointly improving language understanding and generation with quality-weighted weak supervision of automatic labeling",
    "citation_count": 23,
    "authors": [
      "Ernie Chang",
      "Vera Demberg",
      "Alex Marin"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.70": {
    "title": "Continuous Learning in Neural Machine Translation using Bilingual Dictionaries",
    "volume": "main",
    "abstract": "While recent advances in deep learning led to significant improvements in machine translation, neural machine translation is often still not able to continuously adapt to the environment. For humans, as well as for machine translation, bilingual dictionaries are a promising knowledge source to continuously integrate new knowledge. However, their exploitation poses several challenges: The system needs to be able to perform one-shot learning as well as model the morphology of source and target language. In this work, we proposed an evaluation framework to assess the ability of neural machine translation to continuously learn new phrases. We integrate one-shot learning methods for neural machine translation with different word representations and show that it is important to address both in order to successfully make use of bilingual dictionaries. By addressing both challenges we are able to improve the ability to translate new, rare words and phrases from 30% to up to 70%. The correct lemma is even generated by more than 90%",
    "checked": true,
    "id": "ba7d8b907f5693564bc3589c909ccf55a7209355",
    "semantic_title": "continuous learning in neural machine translation using bilingual dictionaries",
    "citation_count": 13,
    "authors": [
      "Jan Niehues"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.71": {
    "title": "Adv-OLM: Generating Textual Adversaries via OLM",
    "volume": "main",
    "abstract": "Deep learning models are susceptible to adversarial examples that have imperceptible perturbations in the original input, resulting in adversarial attacks against these models. Analysis of these attacks on the state of the art transformers in NLP can help improve the robustness of these models against such adversarial inputs. In this paper, we present Adv-OLM, a black-box attack method that adapts the idea of Occlusion and Language Models (OLM) to the current state of the art attack methods. OLM is used to rank words of a sentence, which are later substituted using word replacement strategies. We experimentally show that our approach outperforms other attack methods for several text classification tasks",
    "checked": true,
    "id": "a5b61adeeec0b24c0536c4abfe6e06b5fd0ffd8b",
    "semantic_title": "adv-olm: generating textual adversaries via olm",
    "citation_count": 5,
    "authors": [
      "Vijit Malik",
      "Ashwani Bhat",
      "Ashutosh Modi"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.72": {
    "title": "Conversational Question Answering over Knowledge Graphs with Transformer and Graph Attention Networks",
    "volume": "main",
    "abstract": "This paper addresses the task of (complex) conversational question answering over a knowledge graph. For this task, we propose LASAGNE (muLti-task semAntic parSing with trAnsformer and Graph atteNtion nEworks). It is the first approach, which employs a transformer architecture extended with Graph Attention Networks for multi-task neural semantic parsing. LASAGNE uses a transformer model for generating the base logical forms, while the Graph Attention model is used to exploit correlations between (entity) types and predicates to produce node representations. LASAGNE also includes a novel entity recognition module which detects, links, and ranks all relevant entities in the question context. We evaluate LASAGNE on a standard dataset for complex sequential question answering, on which it outperforms existing baselines averaged on all question types. Specifically, we show that LASAGNE improves the F1-score on eight out of ten question types; in some cases, the increase is more than 20% compared to state of the art (SotA)",
    "checked": true,
    "id": "068caac54d999dca8f8d627ddf685ce16fa49c34",
    "semantic_title": "conversational question answering over knowledge graphs with transformer and graph attention networks",
    "citation_count": 49,
    "authors": [
      "Endri Kacupaj",
      "Joan Plepi",
      "Kuldeep Singh",
      "Harsh Thakkar",
      "Jens Lehmann",
      "Maria Maleshkova"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.73": {
    "title": "DRAG: Director-Generator Language Modelling Framework for Non-Parallel Author Stylized Rewriting",
    "volume": "main",
    "abstract": "Author stylized rewriting is the task of rewriting an input text in a particular author's style. Recent works in this area have leveraged Transformer-based language models in a denoising autoencoder setup to generate author stylized text without relying on a parallel corpus of data. However, these approaches are limited by the lack of explicit control of target attributes and being entirely data-driven. In this paper, we propose a Director-Generator framework to rewrite content in the target author's style, specifically focusing on certain target attributes. We show that our proposed framework works well even with a limited-sized target author corpus. Our experiments on corpora consisting of relatively small-sized text authored by three distinct authors show significant improvements upon existing works to rewrite input texts in target author's style. Our quantitative and qualitative analyses further show that our model has better meaning retention and results in more fluent generations",
    "checked": true,
    "id": "e2df67a98b51116cafca7ee869daea27e7b18a65",
    "semantic_title": "drag: director-generator language modelling framework for non-parallel author stylized rewriting",
    "citation_count": 4,
    "authors": [
      "Hrituraj Singh",
      "Gaurav Verma",
      "Aparna Garimella",
      "Balaji Vasan Srinivasan"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.74": {
    "title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering",
    "volume": "main",
    "abstract": "Generative models for open domain question answering have proven to be competitive, without resorting to external knowledge. While promising, this approach requires to use models with billions of parameters, which are expensive to train and query. In this paper, we investigate how much these models can benefit from retrieving text passages, potentially containing evidence. We obtain state-of-the-art results on the Natural Questions and TriviaQA open benchmarks. Interestingly, we observe that the performance of this method significantly improves when increasing the number of retrieved passages. This is evidence that sequence-to-sequence models offers a flexible framework to efficiently aggregate and combine evidence from multiple passages",
    "checked": true,
    "id": "ea8c46e193d5121e440daf96edfd15a47151c293",
    "semantic_title": "leveraging passage retrieval with generative models for open domain question answering",
    "citation_count": 842,
    "authors": [
      "Gautier Izacard",
      "Edouard Grave"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.75": {
    "title": "Clinical Outcome Prediction from Admission Notes using Self-Supervised Knowledge Integration",
    "volume": "main",
    "abstract": "Outcome prediction from clinical text can prevent doctors from overlooking possible risks and help hospitals to plan capacities. We simulate patients at admission time, when decision support can be especially valuable, and contribute a novel *admission to discharge* task with four common outcome prediction targets: Diagnoses at discharge, procedures performed, in-hospital mortality and length-of-stay prediction. The ideal system should infer outcomes based on symptoms, pre-conditions and risk factors of a patient. We evaluate the effectiveness of language models to handle this scenario and propose *clinical outcome pre-training* to integrate knowledge about patient outcomes from multiple public sources. We further present a simple method to incorporate ICD code hierarchy into the models. We show that our approach improves performance on the outcome tasks against several baselines. A detailed analysis reveals further strengths of the model, including transferability, but also weaknesses such as handling of vital values and inconsistencies in the underlying data",
    "checked": true,
    "id": "8601cb23d63dba4a6d769308e3990def126eccdc",
    "semantic_title": "clinical outcome prediction from admission notes using self-supervised knowledge integration",
    "citation_count": 58,
    "authors": [
      "Betty van Aken",
      "Jens-Michalis Papaioannou",
      "Manuel Mayrdorfer",
      "Klemens Budde",
      "Felix Gers",
      "Alexander Loeser"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.76": {
    "title": "Combining Deep Generative Models and Multi-lingual Pretraining for Semi-supervised Document Classification",
    "volume": "main",
    "abstract": "Semi-supervised learning through deep generative models and multi-lingual pretraining techniques have orchestrated tremendous success across different areas of NLP. Nonetheless, their development has happened in isolation, while the combination of both could potentially be effective for tackling task-specific labelled data shortage. To bridge this gap, we combine semi-supervised deep generative models and multi-lingual pretraining to form a pipeline for document classification task. Compared to strong supervised learning baselines, our semi-supervised classification framework is highly competitive and outperforms the state-of-the-art counterparts in low-resource settings across several languages",
    "checked": true,
    "id": "cba62b06f560d3a51ed40738dae2faf33b93fb80",
    "semantic_title": "combining deep generative models and multi-lingual pretraining for semi-supervised document classification",
    "citation_count": 4,
    "authors": [
      "Yi Zhu",
      "Ehsan Shareghi",
      "Yingzhen Li",
      "Roi Reichart",
      "Anna Korhonen"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.77": {
    "title": "Multi-facet Universal Schema",
    "volume": "main",
    "abstract": "Universal schema (USchema) assumes that two sentence patterns that share the same entity pairs are similar to each other. This assumption is widely adopted for solving various types of relation extraction (RE) tasks. Nevertheless, each sentence pattern could contain multiple facets, and not every facet is similar to all the facets of another sentence pattern co-occurring with the same entity pair. To address the violation of the USchema assumption, we propose multi-facet universal schema that uses a neural model to represent each sentence pattern as multiple facet embeddings and encourage one of these facet embeddings to be close to that of another sentence pattern if they co-occur with the same entity pair. In our experiments, we demonstrate that multi-facet embeddings significantly outperform their single-facet embedding counterpart, compositional universal schema (CUSchema) (Verga et al., 2016), in distantly supervised relation extraction tasks. Moreover, we can also use multiple embeddings to detect the entailment relation between two sentence patterns when no manual label is available",
    "checked": true,
    "id": "8bd83f4db2c58c5a20edc8f9d100200557ce3cb6",
    "semantic_title": "multi-facet universal schema",
    "citation_count": 1,
    "authors": [
      "Rohan Paul",
      "Haw-Shiuan Chang",
      "Andrew McCallum"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.78": {
    "title": "Exploring Transitivity in Neural NLI Models through Veridicality",
    "volume": "main",
    "abstract": "Despite the recent success of deep neural networks in natural language processing, the extent to which they can demonstrate human-like generalization capacities for natural language understanding remains unclear. We explore this issue in the domain of natural language inference (NLI), focusing on the transitivity of inference relations, a fundamental property for systematically drawing inferences. A model capturing transitivity can compose basic inference patterns and draw new inferences. We introduce an analysis method using synthetic and naturalistic NLI datasets involving clause-embedding verbs to evaluate whether models can perform transitivity inferences composed of veridical inferences and arbitrary inference types. We find that current NLI models do not perform consistently well on transitivity inference tasks, suggesting that they lack the generalization capacity for drawing composite inferences from provided training examples. The data and code for our analysis are publicly available at https://github.com/verypluming/transitivity",
    "checked": true,
    "id": "0d39d525f30609d0541330f933007025cd457a83",
    "semantic_title": "exploring transitivity in neural nli models through veridicality",
    "citation_count": 22,
    "authors": [
      "Hitomi Yanaka",
      "Koji Mineshima",
      "Kentaro Inui"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.79": {
    "title": "A Neural Few-Shot Text Classification Reality Check",
    "volume": "main",
    "abstract": "Modern classification models tend to struggle when the amount of annotated data is scarce. To overcome this issue, several neural few-shot classification models have emerged, yielding significant progress over time, both in Computer Vision and Natural Language Processing. In the latter, such models used to rely on fixed word embeddings, before the advent of transformers. Additionally, some models used in Computer Vision are yet to be tested in NLP applications. In this paper, we compare all these models, first adapting those made in the field of image processing to NLP, and second providing them access to transformers. We then test these models equipped with the same transformer-based encoder on the intent detection task, known for having a large amount of classes. Our results reveal that while methods perform almost equally on the ARSC dataset, this is not the case for the Intent Detection task, where most recent and supposedly best competitors perform worse than older and simpler ones (while all are are given access to transformers). We also show that a simple baseline is surprisingly strong. All the new developed models as well as the evaluation framework are made publicly available",
    "checked": true,
    "id": "c8b35d9b54d517c3304b86ef78211934c8443b47",
    "semantic_title": "a neural few-shot text classification reality check",
    "citation_count": 16,
    "authors": [
      "Thomas Dopierre",
      "Christophe Gravier",
      "Wilfried Logerais"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.80": {
    "title": "Multilingual Machine Translation: Closing the Gap between Shared and Language-specific Encoder-Decoders",
    "volume": "main",
    "abstract": "State-of-the-art multilingual machine translation relies on a universal encoder-decoder, which requires retraining the entire system to add new languages. In this paper, we propose an alternative approach that is based on language-specific encoder-decoders, and can thus be more easily extended to new languages by learning their corresponding modules. So as to encourage a common interlingua representation, we simultaneously train the N initial languages. Our experiments show that the proposed approach outperforms the universal encoder-decoder by 3.28 BLEU points on average, while allowing to add new languages without the need to retrain the rest of the modules. All in all, our work closes the gap between shared and language-specific encoderdecoders, advancing toward modular multilingual machine translation systems that can be flexibly extended in lifelong learning settings",
    "checked": true,
    "id": "cec1176cc61d7dd643d3b16adc9acd87b1fbcd7e",
    "semantic_title": "multilingual machine translation: closing the gap between shared and language-specific encoder-decoders",
    "citation_count": 48,
    "authors": [
      "Carlos Escolano",
      "Marta R. Costa-jussà",
      "José A. R. Fonollosa",
      "Mikel Artetxe"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.81": {
    "title": "Clustering Word Embeddings with Self-Organizing Maps. Application on LaRoSeDa - A Large Romanian Sentiment Data Set",
    "volume": "main",
    "abstract": "Romanian is one of the understudied languages in computational linguistics, with few resources available for the development of natural language processing tools. In this paper, we introduce LaRoSeDa, a Large Romanian Sentiment Data Set, which is composed of 15,000 positive and negative reviews collected from the largest Romanian e-commerce platform. We employ two sentiment classification methods as baselines for our new data set, one based on low-level features (character n-grams) and one based on high-level features (bag-of-word-embeddings generated by clustering word embeddings with k-means). As an additional contribution, we replace the k-means clustering algorithm with self-organizing maps (SOMs), obtaining better results because the generated clusters of word embeddings are closer to the Zipf's law distribution, which is known to govern natural language. We also demonstrate the generalization capacity of using SOMs for the clustering of word embeddings on another recently-introduced Romanian data set, for text categorization by topic",
    "checked": true,
    "id": "27a9bdedcc2c3c0bc9116d7cd56fb72744b6a26a",
    "semantic_title": "clustering word embeddings with self-organizing maps. application on laroseda - a large romanian sentiment data set",
    "citation_count": 18,
    "authors": [
      "Anca Tache",
      "Gaman Mihaela",
      "Radu Tudor Ionescu"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.82": {
    "title": "Elastic weight consolidation for better bias inoculation",
    "volume": "main",
    "abstract": "The biases present in training datasets have been shown to affect models for sentence pair classification tasks such as natural language inference (NLI) and fact verification. While fine-tuning models on additional data has been used to mitigate them, a common issue is that of catastrophic forgetting of the original training dataset. In this paper, we show that elastic weight consolidation (EWC) allows fine-tuning of models to mitigate biases while being less susceptible to catastrophic forgetting. In our evaluation on fact verification and NLI stress tests, we show that fine-tuning with EWC dominates standard fine-tuning, yielding models with lower levels of forgetting on the original (biased) dataset for equivalent gains in accuracy on the fine-tuning (unbiased) dataset",
    "checked": true,
    "id": "2a7fa0ae03c98be2b03e5849b7a67f10e6c876dd",
    "semantic_title": "elastic weight consolidation for better bias inoculation",
    "citation_count": 9,
    "authors": [
      "James Thorne",
      "Andreas Vlachos"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.83": {
    "title": "Hierarchical Multi-head Attentive Network for Evidence-aware Fake News Detection",
    "volume": "main",
    "abstract": "The widespread of fake news and misinformation in various domains ranging from politics, economics to public health has posed an urgent need to automatically fact-check information. A recent trend in fake news detection is to utilize evidence from external sources. However, existing evidence-aware fake news detection methods focused on either only word-level attention or evidence-level attention, which may result in suboptimal performance. In this paper, we propose a Hierarchical Multi-head Attentive Network to fact-check textual claims. Our model jointly combines multi-head word-level attention and multi-head document-level attention, which aid explanation in both word-level and evidence-level. Experiments on two real-word datasets show that our model outperforms seven state-of-the-art baselines. Improvements over baselines are from 6% to 18%. Our source code and datasets are released at https://github.com/nguyenvo09/EACL2021",
    "checked": true,
    "id": "5c6ff5836639e87e8afeaad47e64d0e2234566e8",
    "semantic_title": "hierarchical multi-head attentive network for evidence-aware fake news detection",
    "citation_count": 36,
    "authors": [
      "Nguyen Vo",
      "Kyumin Lee"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.84": {
    "title": "Identifying Named Entities as they are Typed",
    "volume": "main",
    "abstract": "Identifying named entities in written text is an essential component of the text processing pipeline used in applications such as text editors to gain a better understanding of the semantics of the text. However, the typical experimental setup for evaluating Named Entity Recognition (NER) systems is not directly applicable to systems that process text in real time as the text is being typed. Evaluation is performed on a sentence level assuming the end-user is willing to wait until the entire sentence is typed for entities to be identified and further linked to identifiers or co-referenced. We introduce a novel experimental setup for NER systems for applications where decisions about named entity boundaries need to be performed in an online fashion. We study how state-of-the-art methods perform under this setup in multiple languages and propose adaptations to these models to suit this new experimental setup. Experimental results show that the best systems that are evaluated on each token after its typed, reach performance within 1–5 F1 points of systems that are evaluated at the end of the sentence. These show that entity recognition can be performed in this setup and open up the development of other NLP tools in a similar setup",
    "checked": true,
    "id": "8e50888480170637a570a3bebea37eadf292d982",
    "semantic_title": "identifying named entities as they are typed",
    "citation_count": 1,
    "authors": [
      "Ravneet Arora",
      "Chen-Tse Tsai",
      "Daniel Preotiuc-Pietro"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.85": {
    "title": "SANDI: Story-and-Images Alignment",
    "volume": "main",
    "abstract": "The Internet contains a multitude of social media posts and other of stories where text is interspersed with images. In these contexts, images are not simply used for general illustration, but are judiciously placed in certain spots of a story for multimodal descriptions and narration. In this work we analyze the problem of text-image alignment, and present SANDI, a methodology for automatically selecting images from an image collection and aligning them with text paragraphs of a story. SANDI combines visual tags, user-provided tags and background knowledge, and uses an Integer Linear Program to compute alignments that are semantically meaningful. Experiments show that SANDI can select and align images with texts with high quality of semantic fit",
    "checked": true,
    "id": "92457eea46f096d69750b53aeab0c01589e82331",
    "semantic_title": "sandi: story-and-images alignment",
    "citation_count": 5,
    "authors": [
      "Sreyasi Nag Chowdhury",
      "Simon Razniewski",
      "Gerhard Weikum"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.86": {
    "title": "Question and Answer Test-Train Overlap in Open-Domain Question Answering Datasets",
    "volume": "main",
    "abstract": "Ideally Open-Domain Question Answering models should exhibit a number of competencies, ranging from simply memorizing questions seen at training time, to answering novel question formulations with answers seen during training, to generalizing to completely novel questions with novel answers. However, single aggregated test set scores do not show the full picture of what capabilities models truly have. In this work, we perform a detailed study of the test sets of three popular open-domain benchmark datasets with respect to these competencies. We find that 30% of test-set questions have a near-duplicate paraphrase in their corresponding train sets. In addition, we find that 60-70% of answers in the test sets are also present in the train sets. Using these findings, we evaluate a variety of popular open-domain models to obtain greater insight into what extent they can generalize, and what drives their overall performance. We find that all models perform substantially worse on questions that cannot be memorized from train sets, with a mean absolute performance difference of 61% between repeated and non-repeated data. Finally we show that simple nearest-neighbor models outperform a BART closed-book QA model, further highlighting the role that train set memorization plays in these benchmarks",
    "checked": true,
    "id": "cb58542c94ce83b09f5d3809e69518ba52709c92",
    "semantic_title": "question and answer test-train overlap in open-domain question answering datasets",
    "citation_count": 174,
    "authors": [
      "Patrick Lewis",
      "Pontus Stenetorp",
      "Sebastian Riedel"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.87": {
    "title": "El Volumen Louder Por Favor: Code-switching in Task-oriented Semantic Parsing",
    "volume": "main",
    "abstract": "Being able to parse code-switched (CS) utterances, such as Spanish+English or Hindi+English, is essential to democratize task-oriented semantic parsing systems for certain locales. In this work, we focus on Spanglish (Spanish+English) and release a dataset, CSTOP, containing 5800 CS utterances alongside their semantic parses. We examine the CS generalizability of various Cross-lingual (XL) models and exhibit the advantage of pre-trained XL language models when data for only one language is present. As such, we focus on improving the pre-trained models for the case when only English corpus alongside either zero or a few CS training instances are available. We propose two data augmentation methods for the zero-shot and the few-shot settings: fine-tune using translate-and-align and augment using a generation model followed by match-and-filter. Combining the few-shot setting with the above improvements decreases the initial 30-point accuracy gap between the zero-shot and the full-data settings by two thirds",
    "checked": true,
    "id": "b03ff41f181b01fb0a168677a311005701e39bf7",
    "semantic_title": "el volumen louder por favor: code-switching in task-oriented semantic parsing",
    "citation_count": 8,
    "authors": [
      "Arash Einolghozati",
      "Abhinav Arora",
      "Lorena Sainz-Maza Lecanda",
      "Anuj Kumar",
      "Sonal Gupta"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.88": {
    "title": "Generating Syntactically Controlled Paraphrases without Using Annotated Parallel Pairs",
    "volume": "main",
    "abstract": "Paraphrase generation plays an essential role in natural language process (NLP), and it has many downstream applications. However, training supervised paraphrase models requires many annotated paraphrase pairs, which are usually costly to obtain. On the other hand, the paraphrases generated by existing unsupervised approaches are usually syntactically similar to the source sentences and are limited in diversity. In this paper, we demonstrate that it is possible to generate syntactically various paraphrases without the need for annotated paraphrase pairs. We propose Syntactically controlled Paraphrase Generator (SynPG), an encoder-decoder based model that learns to disentangle the semantics and the syntax of a sentence from a collection of unannotated texts. The disentanglement enables SynPG to control the syntax of output paraphrases by manipulating the embedding in the syntactic space. Extensive experiments using automatic metrics and human evaluation show that SynPG performs better syntactic control than unsupervised baselines, while the quality of the generated paraphrases is competitive. We also demonstrate that the performance of SynPG is competitive or even better than supervised models when the unannotated data is large. Finally, we show that the syntactically controlled paraphrases generated by SynPG can be utilized for data augmentation to improve the robustness of NLP models",
    "checked": true,
    "id": "46dfe93b284c929e7340e3f59303f25ad1a9df88",
    "semantic_title": "generating syntactically controlled paraphrases without using annotated parallel pairs",
    "citation_count": 60,
    "authors": [
      "Kuan-Hao Huang",
      "Kai-Wei Chang"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.89": {
    "title": "Data Augmentation for Hypernymy Detection",
    "volume": "main",
    "abstract": "The automatic detection of hypernymy relationships represents a challenging problem in NLP. The successful application of state-of-the-art supervised approaches using distributed representations has generally been impeded by the limited availability of high quality training data. We have developed two novel data augmentation techniques which generate new training examples from existing ones. First, we combine the linguistic principles of hypernym transitivity and intersective modifier-noun composition to generate additional pairs of vectors, such as \"small dog - dog\" or \"small dog - animal\", for which a hypernymy relationship can be assumed. Second, we use generative adversarial networks (GANs) to generate pairs of vectors for which the hypernymy relation can also be assumed. We furthermore present two complementary strategies for extending an existing dataset by leveraging linguistic resources such as WordNet. Using an evaluation across 3 different datasets for hypernymy detection and 2 different vector spaces, we demonstrate that both of the proposed automatic data augmentation and dataset extension strategies substantially improve classifier performance",
    "checked": true,
    "id": "3bfe39dc5d241f40f96a5fa3918f91c2ea299a17",
    "semantic_title": "data augmentation for hypernymy detection",
    "citation_count": 16,
    "authors": [
      "Thomas Kober",
      "Julie Weeds",
      "Lorenzo Bertolini",
      "David Weir"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.90": {
    "title": "Few-shot learning through contextual data augmentation",
    "volume": "main",
    "abstract": "Machine translation (MT) models used in industries with constantly changing topics, such as translation or news agencies, need to adapt to new data to maintain their performance over time. Our aim is to teach a pre-trained MT model to translate previously unseen words accurately, based on very few examples. We propose (i) an experimental setup allowing us to simulate novel vocabulary appearing in human-submitted translations, and (ii) corresponding evaluation metrics to compare our approaches. We extend a data augmentation approach using a pretrained language model to create training examples with similar contexts for novel words. We compare different fine-tuning and data augmentation approaches and show that adaptation on the scale of one to five examples is possible. Combining data augmentation with randomly selected training sentences leads to the highest BLEU score and accuracy improvements. Impressively, with only 1 to 5 examples, our model reports better accuracy scores than a reference system trained with on average 313 parallel examples",
    "checked": true,
    "id": "b21624decc42afaf5914c4939f4868b2b463b4f6",
    "semantic_title": "few-shot learning through contextual data augmentation",
    "citation_count": 8,
    "authors": [
      "Farid Arthaud",
      "Rachel Bawden",
      "Alexandra Birch"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.91": {
    "title": "Zero-shot Generalization in Dialog State Tracking through Generative Question Answering",
    "volume": "main",
    "abstract": "Dialog State Tracking (DST), an integral part of modern dialog systems, aims to track user preferences and constraints (slots) in task-oriented dialogs. In real-world settings with constantly changing services, DST systems must generalize to new domains and unseen slot types. Existing methods for DST do not generalize well to new slot names and many require known ontologies of slot types and values for inference. We introduce a novel ontology-free framework that supports natural language queries for unseen constraints and slots in multi-domain task-oriented dialogs. Our approach is based on generative question-answering using a conditional language model pre-trained on substantive English sentences. Our model improves joint goal accuracy in zero-shot domain adaptation settings by up to 9% (absolute) over the previous state-of-the-art on the MultiWOZ 2.1 dataset",
    "checked": true,
    "id": "f64a26335a432052820ed29654a7f88759171631",
    "semantic_title": "zero-shot generalization in dialog state tracking through generative question answering",
    "citation_count": 37,
    "authors": [
      "Shuyang Li",
      "Jin Cao",
      "Mukund Sridhar",
      "Henghui Zhu",
      "Shang-Wen Li",
      "Wael Hamza",
      "Julian McAuley"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.92": {
    "title": "Zero-shot Neural Passage Retrieval via Domain-targeted Synthetic Question Generation",
    "volume": "main",
    "abstract": "A major obstacle to the wide-spread adoption of neural retrieval models is that they require large supervised training sets to surpass traditional term-based techniques, which are constructed from raw corpora. In this paper, we propose an approach to zero-shot learning for passage retrieval that uses synthetic question generation to close this gap. The question generation system is trained on general domain data, but is applied to documents in the targeted domain. This allows us to create arbitrarily large, yet noisy, question-passage relevance pairs that are domain specific. Furthermore, when this is coupled with a simple hybrid term-neural model, first-stage retrieval performance can be improved further. Empirically, we show that this is an effective strategy for building neural passage retrieval models in the absence of large training corpora. Depending on the domain, this technique can even approach the accuracy of supervised models",
    "checked": true,
    "id": "196e1b5ee39219904427dbe636d154559d4f84b3",
    "semantic_title": "zero-shot neural passage retrieval via domain-targeted synthetic question generation",
    "citation_count": 112,
    "authors": [
      "Ji Ma",
      "Ivan Korotkov",
      "Yinfei Yang",
      "Keith Hall",
      "Ryan McDonald"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.93": {
    "title": "Discourse-Aware Unsupervised Summarization for Long Scientific Documents",
    "volume": "main",
    "abstract": "We propose an unsupervised graph-based ranking model for extractive summarization of long scientific documents. Our method assumes a two-level hierarchical graph representation of the source document, and exploits asymmetrical positional cues to determine sentence importance. Results on the PubMed and arXiv datasets show that our approach outperforms strong unsupervised baselines by wide margins in automatic metrics and human evaluation. In addition, it achieves performance comparable to many state-of-the-art supervised approaches which are trained on hundreds of thousands of examples. These results suggest that patterns in the discourse structure are a strong signal for determining importance in scientific articles",
    "checked": true,
    "id": "b9a21c2bf389ba693cd4692a028c7f2821b1804e",
    "semantic_title": "discourse-aware unsupervised summarization for long scientific documents",
    "citation_count": 46,
    "authors": [
      "Yue Dong",
      "Andrei Mircea",
      "Jackie Chi Kit Cheung"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.94": {
    "title": "MIDAS: A Dialog Act Annotation Scheme for Open Domain HumanMachine Spoken Conversations",
    "volume": "main",
    "abstract": "Dialog act prediction in open-domain conversations is an essential language comprehension task for both dialog system building and discourse analysis. Previous dialog act schemes, such as SWBD-DAMSL, are designed mainly for discourse analysis in human-human conversations. In this paper, we present a dialog act annotation scheme, MIDAS (Machine Interaction Dialog Act Scheme), targeted at open-domain human-machine conversations. MIDAS is designed to assist machines to improve their ability to understand human partners. MIDAS has a hierarchical structure and supports multi-label annotations. We collected and annotated a large open-domain human-machine spoken conversation dataset (consisting of 24K utterances). To validate our scheme, we leveraged transfer learning methods to train a multi-label dialog act prediction model and reached an F1 score of 0.79",
    "checked": true,
    "id": "6809664eef929960249d5bf76968c72740de6dff",
    "semantic_title": "midas: a dialog act annotation scheme for open domain humanmachine spoken conversations",
    "citation_count": 41,
    "authors": [
      "Dian Yu",
      "Zhou Yu"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.95": {
    "title": "Analyzing the Forgetting Problem in Pretrain-Finetuning of Open-domain Dialogue Response Models",
    "volume": "main",
    "abstract": "In this work, we study how the finetuning stage in the pretrain-finetune framework changes the behavior of a pretrained neural language generator. We focus on the transformer encoder-decoder model for the open-domain dialogue response generation task. Our major finding is that after standard finetuning, the model forgets some of the important language generation skills acquired during large-scale pretraining. We demonstrate the forgetting phenomenon through a set of detailed behavior analysis from the perspectives of knowledge transfer, context sensitivity, and function space projection. As a preliminary attempt to alleviate the forgetting problem, we propose an intuitive finetuning strategy named \"mix-review\". We find that mix-review effectively regularizes the finetuning process, and the forgetting problem is alleviated to some extent. Finally, we discuss interesting behavior of the resulting dialogue model and its implications",
    "checked": true,
    "id": "291a00d8433fecd2dd10f7f13b62dae8ce500043",
    "semantic_title": "analyzing the forgetting problem in pretrain-finetuning of open-domain dialogue response models",
    "citation_count": 44,
    "authors": [
      "Tianxing He",
      "Jun Liu",
      "Kyunghyun Cho",
      "Myle Ott",
      "Bing Liu",
      "James Glass",
      "Fuchun Peng"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.96": {
    "title": "Leveraging End-to-End ASR for Endangered Language Documentation: An Empirical Study on Yolóxochitl Mixtec",
    "volume": "main",
    "abstract": "Transcription bottlenecks\", created by a shortage of effective human transcribers (i.e., transcriber shortage), are one of the main challenges to endangered language (EL) documentation. Automatic speech recognition (ASR) has been suggested as a tool to overcome such bottlenecks. Following this suggestion, we investigated the effectiveness for EL documentation of end-to-end ASR, which unlike Hidden Markov Model ASR systems, eschews linguistic resources but is instead more dependent on large-data settings. We open source a Yoloxóchitl Mixtec EL corpus. First, we review our method in building an end-to-end ASR system in a way that would be reproducible by the ASR community. We then propose a novice transcription correction task and demonstrate how ASR systems and novice transcribers can work together to improve EL documentation. We believe this combinatory methodology would mitigate the transcription bottleneck and transcriber shortage that hinders EL documentation",
    "checked": true,
    "id": "8c4d1e81c277f71cd9e3c9a0af356203c7948dca",
    "semantic_title": "leveraging end-to-end asr for endangered language documentation: an empirical study on yolóxochitl mixtec",
    "citation_count": 40,
    "authors": [
      "Jiatong Shi",
      "Jonathan D. Amith",
      "Rey Castillo García",
      "Esteban Guadalupe Sierra",
      "Kevin Duh",
      "Shinji Watanabe"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.97": {
    "title": "Mode Effects' Challenge to Authorship Attribution",
    "volume": "main",
    "abstract": "The success of authorship attribution relies on the presence of linguistic features specific to individual authors. There is, however, limited research assessing to what extent authorial style remains constant when individuals switch from one writing modality to another. We measure the effect of writing mode on writing style in the context of authorship attribution research using a corpus of documents composed online (in a web browser) and documents composed offline using a traditional word processor. The results confirm the existence of a \"mode effect\" on authorial style. Online writing differs systematically from offline writing in terms of sentence length, word use, readability, and certain part-of-speech ratios. These findings have implications for research design and feature engineering in authorship attribution studies",
    "checked": true,
    "id": "b8ec8c0b5b15708a9d562db8b67c363a6092d021",
    "semantic_title": "mode effects' challenge to authorship attribution",
    "citation_count": 6,
    "authors": [
      "Haining Wang",
      "Allen Riddell",
      "Patrick Juola"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.98": {
    "title": "Generative Text Modeling through Short Run Inference",
    "volume": "main",
    "abstract": "Latent variable models for text, when trained successfully, accurately model the data distribution and capture global semantic and syntactic features of sentences. The prominent approach to train such models is variational autoencoders (VAE). It is nevertheless challenging to train and often results in a trivial local optimum where the latent variable is ignored and its posterior collapses into the prior, an issue known as posterior collapse. Various techniques have been proposed to mitigate this issue. Most of them focus on improving the inference model to yield latent codes of higher quality. The present work proposes a short run dynamics for inference. It is initialized from the prior distribution of the latent variable and then runs a small number (e.g., 20) of Langevin dynamics steps guided by its posterior distribution. The major advantage of our method is that it does not require a separate inference model or assume simple geometry of the posterior distribution, thus rendering an automatic, natural and flexible inference engine. We show that the models trained with short run dynamics more accurately model the data, compared to strong language model and VAE baselines, and exhibit no sign of posterior collapse. Analyses of the latent space show that interpolation in the latent space is able to generate coherent sentences with smooth transition and demonstrate improved classification over strong baselines with latent features from unsupervised pretraining. These results together expose a well-structured latent space of our generative model",
    "checked": true,
    "id": "899e491acad2d31a03abc2f62ec988e9da7f17b7",
    "semantic_title": "generative text modeling through short run inference",
    "citation_count": 4,
    "authors": [
      "Bo Pang",
      "Erik Nijkamp",
      "Tian Han",
      "Ying Nian Wu"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.99": {
    "title": "Detecting Extraneous Content in Podcasts",
    "volume": "main",
    "abstract": "Podcast episodes often contain material extraneous to the main content, such as advertisements, interleaved within the audio and the written descriptions. We present classifiers that leverage both textual and listening patterns in order to detect such content in podcast descriptions and audio transcripts. We demonstrate that our models are effective by evaluating them on the downstream task of podcast summarization and show that we can substantively improve ROUGE scores and reduce the extraneous content generated in the summaries",
    "checked": true,
    "id": "0035bf712b1ce5da56f8d6c04ff9ebbfcf01485d",
    "semantic_title": "detecting extraneous content in podcasts",
    "citation_count": 16,
    "authors": [
      "Sravana Reddy",
      "Yongze Yu",
      "Aasish Pappu",
      "Aswin Sivaraman",
      "Rezvaneh Rezapour",
      "Rosie Jones"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.100": {
    "title": "Randomized Deep Structured Prediction for Discourse-Level Processing",
    "volume": "main",
    "abstract": "Expressive text encoders such as RNNs and Transformer Networks have been at the center of NLP models in recent work. Most of the effort has focused on sentence-level tasks, capturing the dependencies between words in a single sentence, or pairs of sentences. However, certain tasks, such as argumentation mining, require accounting for longer texts and complicated structural dependencies between them. Deep structured prediction is a general framework to combine the complementary strengths of expressive neural encoders and structured inference for highly structured domains. Nevertheless, when the need arises to go beyond sentences, most work relies on combining the output scores of independently trained classifiers. One of the main reasons for this is that constrained inference comes at a high computational cost. In this paper, we explore the use of randomized inference to alleviate this concern and show that we can efficiently leverage deep structured prediction and expressive neural encoders for a set of tasks involving complicated argumentative structures",
    "checked": true,
    "id": "ee2c559a1a74b1a0192914eae6fab4195639e2c7",
    "semantic_title": "randomized deep structured prediction for discourse-level processing",
    "citation_count": 7,
    "authors": [
      "Manuel Widmoser",
      "Maria Leonor Pacheco",
      "Jean Honorio",
      "Dan Goldwasser"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.101": {
    "title": "Automatic Data Acquisition for Event Coreference Resolution",
    "volume": "main",
    "abstract": "We propose to leverage lexical paraphrases and high precision rules informed by news discourse structure to automatically collect coreferential and non-coreferential event pairs from unlabeled English news articles. We perform both manual validation and empirical evaluation on multiple evaluation datasets with different event domains and text genres to assess the quality of our acquired event pairs. We found that a model trained on our acquired event pairs performs comparably as the supervised model when applied to new data out of the training data domains. Further, augmenting human-annotated data with the acquired event pairs provides empirical performance gains on both in-domain and out-of-domain evaluation datasets",
    "checked": true,
    "id": "0af570942946c2cfe350d73e9a60f9b5942f1ef4",
    "semantic_title": "automatic data acquisition for event coreference resolution",
    "citation_count": 6,
    "authors": [
      "Prafulla Kumar Choubey",
      "Ruihong Huang"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.102": {
    "title": "Joint Learning of Representations for Web-tables, Entities and Types using Graph Convolutional Network",
    "volume": "main",
    "abstract": "Existing approaches for table annotation with entities and types either capture the structure of table using graphical models, or learn embeddings of table entries without accounting for the complete syntactic structure. We propose TabGCN, that uses Graph Convolutional Networks to capture the complete structure of tables, knowledge graph and the training annotations, and jointly learns embeddings for table elements as well as the entities and types. To account for knowledge incompleteness, TabGCN's embeddings can be used to discover new entities and types. Using experiments on 5 benchmark datasets, we show that TabGCN significantly outperforms multiple state-of-the-art baselines for table annotation, while showing promising performance on downstream table-related applications",
    "checked": true,
    "id": "274c1809d8f9ccea543e305b0d56497b4ce49b57",
    "semantic_title": "joint learning of representations for web-tables, entities and types using graph convolutional network",
    "citation_count": 13,
    "authors": [
      "Aniket Pramanick",
      "Indrajit Bhattacharya"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.103": {
    "title": "Multimodal Text Style Transfer for Outdoor Vision-and-Language Navigation",
    "volume": "main",
    "abstract": "One of the most challenging topics in Natural Language Processing (NLP) is visually-grounded language understanding and reasoning. Outdoor vision-and-language navigation (VLN) is such a task where an agent follows natural language instructions and navigates in real-life urban environments. With the lack of human-annotated instructions that illustrate the intricate urban scenes, outdoor VLN remains a challenging task to solve. In this paper, we introduce a Multimodal Text Style Transfer (MTST) learning approach and leverage external multimodal resources to mitigate data scarcity in outdoor navigation tasks. We first enrich the navigation data by transferring the style of the instructions generated by Google Maps API, then pre-train the navigator with the augmented external outdoor navigation dataset. Experimental results show that our MTST learning approach is model-agnostic, and our MTST approach significantly outperforms the baseline models on the outdoor VLN task, improving task completion rate by 8.7% relatively on the test set",
    "checked": true,
    "id": "6fe9177f0dd44764b15fe4d42bd96b50dc5fb144",
    "semantic_title": "multimodal text style transfer for outdoor vision-and-language navigation",
    "citation_count": 28,
    "authors": [
      "Wanrong Zhu",
      "Xin Wang",
      "Tsu-Jui Fu",
      "An Yan",
      "Pradyumna Narayana",
      "Kazoo Sone",
      "Sugato Basu",
      "William Yang Wang"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.104": {
    "title": "ECOL-R: Encouraging Copying in Novel Object Captioning with Reinforcement Learning",
    "volume": "main",
    "abstract": "Novel Object Captioning is a zero-shot Image Captioning task requiring describing objects not seen in the training captions, but for which information is available from external object detectors. The key challenge is to select and describe all salient detected novel objects in the input images. In this paper, we focus on this challenge and propose the ECOL-R model (Encouraging Copying of Object Labels with Reinforced Learning), a copy-augmented transformer model that is encouraged to accurately describe the novel object labels. This is achieved via a specialised reward function in the SCST reinforcement learning framework (Rennie et al., 2017) that encourages novel object mentions while maintaining the caption quality. We further restrict the SCST training to the images where detected objects are mentioned in reference captions to train the ECOL-R model. We additionally improve our copy mechanism via Abstract Labels, which transfer knowledge from known to novel object types, and a Morphological Selector, which determines the appropriate inflected forms of novel object labels. The resulting model sets new state-of-the-art on the nocaps (Agrawal et al., 2019) and held-out COCO (Hendricks et al., 2016) benchmarks",
    "checked": true,
    "id": "cd919b85d465c6cd91f6b2579c597ca5077e7543",
    "semantic_title": "ecol-r: encouraging copying in novel object captioning with reinforcement learning",
    "citation_count": 5,
    "authors": [
      "Yufei Wang",
      "Ian Wood",
      "Stephen Wan",
      "Mark Johnson"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.105": {
    "title": "Enriching Non-Autoregressive Transformer with Syntactic and Semantic Structures for Neural Machine Translation",
    "volume": "main",
    "abstract": "The non-autoregressive models have boosted the efficiency of neural machine translation through parallelized decoding at the cost of effectiveness, when comparing with the autoregressive counterparts. In this paper, we claim that the syntactic and semantic structures among natural language are critical for non-autoregressive machine translation and can further improve the performance. However, these structures are rarely considered in the existing non-autoregressive models. Inspired by this intuition, we propose to incorporate the explicit syntactic and semantic structure of languages into a non-autoregressive Transformer, for the task of neural machine translation. Moreover, we also consider the intermediate latent alignment within target sentences to better learn the long-term token dependencies. Experimental results on two real-world datasets (i.e., WMT14 En-De and WMT16 En- Ro) show that our model achieves a significantly faster speed, as well as keeps the translation quality when compared with several state-of-the-art non-autoregressive models",
    "checked": true,
    "id": "fdeeafdb51ddfeddc5573294cae5f7d5700147f2",
    "semantic_title": "enriching non-autoregressive transformer with syntactic and semantic structures for neural machine translation",
    "citation_count": 21,
    "authors": [
      "Ye Liu",
      "Yao Wan",
      "Jianguo Zhang",
      "Wenting Zhao",
      "Philip Yu"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.106": {
    "title": "NLQuAD: A Non-Factoid Long Question Answering Data Set",
    "volume": "main",
    "abstract": "We introduce NLQuAD, the first data set with baseline methods for non-factoid long question answering, a task requiring document-level language understanding. In contrast to existing span detection question answering data sets, NLQuAD has non-factoid questions that are not answerable by a short span of text and demanding multiple-sentence descriptive answers and opinions. We show the limitation of the F1 score for evaluation of long answers and introduce Intersection over Union (IoU), which measures position-sensitive overlap between the predicted and the target answer spans. To establish baseline performances, we compare BERT, RoBERTa, and Longformer models. Experimental results and human evaluations show that Longformer outperforms the other architectures, but results are still far behind a human upper bound, leaving substantial room for improvements. NLQuAD's samples exceed the input limitation of most pre-trained Transformer-based models, encouraging future research on long sequence language models",
    "checked": true,
    "id": "0371c6376d08d7368e4feea161b4909032b59a35",
    "semantic_title": "nlquad: a non-factoid long question answering data set",
    "citation_count": 17,
    "authors": [
      "Amir Soleimani",
      "Christof Monz",
      "Marcel Worring"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.107": {
    "title": "Debiasing Pre-trained Contextualised Embeddings",
    "volume": "main",
    "abstract": "In comparison to the numerous debiasing methods proposed for the static non-contextualised word embeddings, the discriminative biases in contextualised embeddings have received relatively little attention. We propose a fine-tuning method that can be applied at token- or sentence-levels to debias pre-trained contextualised embeddings. Our proposed method can be applied to any pre-trained contextualised embedding model, without requiring to retrain those models. Using gender bias as an illustrative example, we then conduct a systematic study using several state-of-the-art (SoTA) contextualised representations on multiple benchmark datasets to evaluate the level of biases encoded in different contextualised embeddings before and after debiasing using the proposed method. We find that applying token-level debiasing for all tokens and across all layers of a contextualised embedding model produces the best performance. Interestingly, we observe that there is a trade-off between creating an accurate vs. unbiased contextualised embedding model, and different contextualised embedding models respond differently to this trade-off",
    "checked": true,
    "id": "61ca0040d81c5ed71d3f9b9e5f7b528275048440",
    "semantic_title": "debiasing pre-trained contextualised embeddings",
    "citation_count": 120,
    "authors": [
      "Masahiro Kaneko",
      "Danushka Bollegala"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.108": {
    "title": "Language Models for Lexical Inference in Context",
    "volume": "main",
    "abstract": "Lexical inference in context (LIiC) is the task of recognizing textual entailment between two very similar sentences, i.e., sentences that only differ in one expression. It can therefore be seen as a variant of the natural language inference task that is focused on lexical semantics. We formulate and evaluate the first approaches based on pretrained language models (LMs) for this task: (i) a few-shot NLI classifier, (ii) a relation induction approach based on handcrafted patterns expressing the semantics of lexical inference, and (iii) a variant of (ii) with patterns that were automatically extracted from a corpus. All our approaches outperform the previous state of the art, showing the potential of pretrained LMs for LIiC. In an extensive analysis, we investigate factors of success and failure of our three approaches",
    "checked": true,
    "id": "2bd629ccdc9217ccef813371d3177813a770de88",
    "semantic_title": "language models for lexical inference in context",
    "citation_count": 11,
    "authors": [
      "Martin Schmitt",
      "Hinrich Schütze"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.109": {
    "title": "Few-Shot Semantic Parsing for New Predicates",
    "volume": "main",
    "abstract": "In this work, we investigate the problems of semantic parsing in a few-shot learning setting. In this setting, we are provided with k utterance-logical form pairs per new predicate. The state-of-the-art neural semantic parsers achieve less than 25% accuracy on benchmark datasets when k = 1. To tackle this problem, we proposed to i) apply a designated meta-learning method to train the model; ii) regularize attention scores with alignment statistics; iii) apply a smoothing technique in pretraining. As a result, our method consistently outperforms all the baselines in both one and two-shot settings",
    "checked": true,
    "id": "b35dae83534a9d4500e250a5e9aadf01aaf005c3",
    "semantic_title": "few-shot semantic parsing for new predicates",
    "citation_count": 9,
    "authors": [
      "Zhuang Li",
      "Lizhen Qu",
      "Shuo Huang",
      "Gholamreza Haffari"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.110": {
    "title": "Alternating Recurrent Dialog Model with Large-scale Pre-trained Language Models",
    "volume": "main",
    "abstract": "Existing dialog system models require extensive human annotations and are difficult to generalize to different tasks. The recent success of large pre-trained language models such as BERT and GPT-2 (Devlin et al., 2019; Radford et al., 2019) have suggested the effectiveness of incorporating language priors in down-stream NLP tasks. However, how much pre-trained language models can help dialog response generation is still under exploration. In this paper, we propose a simple, general, and effective framework: Alternating Recurrent Dialog Model (ARDM). ARDM models each speaker separately and takes advantage of the large pre-trained language model. It requires no supervision from human annotations such as belief states or dialog acts to achieve effective conversations. ARDM outperforms or is on par with state-of-the-art methods on two popular task-oriented dialog datasets: CamRest676 and MultiWOZ. Moreover, we can generalize ARDM to more challenging, non-collaborative tasks such as persuasion. In persuasion tasks, ARDM is capable of generating human-like responses to persuade people to donate to a charity",
    "checked": true,
    "id": "ff2541f30507b161316edd879fa65fb80a08d16f",
    "semantic_title": "alternating recurrent dialog model with large-scale pre-trained language models",
    "citation_count": 59,
    "authors": [
      "Qingyang Wu",
      "Yichi Zhang",
      "Yu Li",
      "Zhou Yu"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.111": {
    "title": "On the Evaluation of Vision-and-Language Navigation Instructions",
    "volume": "main",
    "abstract": "Vision-and-Language Navigation wayfinding agents can be enhanced by exploiting automatically generated navigation instructions. However, existing instruction generators have not been comprehensively evaluated, and the automatic evaluation metrics used to develop them have not been validated. Using human wayfinders, we show that these generators perform on par with or only slightly better than a template-based generator and far worse than human instructors. Furthermore, we discover that BLEU, ROUGE, METEOR and CIDEr are ineffective for evaluating grounded navigation instructions. To improve instruction evaluation, we propose an instruction-trajectory compatibility model that operates without reference instructions. Our model shows the highest correlation with human wayfinding outcomes when scoring individual instructions. For ranking instruction generation systems, if reference instructions are available we recommend using SPICE",
    "checked": true,
    "id": "cae06461bfc6978e9f3a3bbb0b0d0840cd762720",
    "semantic_title": "on the evaluation of vision-and-language navigation instructions",
    "citation_count": 45,
    "authors": [
      "Ming Zhao",
      "Peter Anderson",
      "Vihan Jain",
      "Su Wang",
      "Alexander Ku",
      "Jason Baldridge",
      "Eugene Ie"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.112": {
    "title": "Cross-lingual Visual Pre-training for Multimodal Machine Translation",
    "volume": "main",
    "abstract": "Pre-trained language models have been shown to improve performance in many natural language tasks substantially. Although the early focus of such models was single language pre-training, recent advances have resulted in cross-lingual and visual pre-training methods. In this paper, we combine these two approaches to learn visually-grounded cross-lingual representations. Specifically, we extend the translation language modelling (Lample and Conneau, 2019) with masked region classification and perform pre-training with three-way parallel vision & language corpora. We show that when fine-tuned for multimodal machine translation, these models obtain state-of-the-art performance. We also provide qualitative insights into the usefulness of the learned grounded representations",
    "checked": true,
    "id": "56be11090de38a75c599c38638481e8a2e43ad31",
    "semantic_title": "cross-lingual visual pre-training for multimodal machine translation",
    "citation_count": 38,
    "authors": [
      "Ozan Caglayan",
      "Menekse Kuyu",
      "Mustafa Sercan Amac",
      "Pranava Madhyastha",
      "Erkut Erdem",
      "Aykut Erdem",
      "Lucia Specia"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.113": {
    "title": "Memorization vs. Generalization : Quantifying Data Leakage in NLP Performance Evaluation",
    "volume": "main",
    "abstract": "Public datasets are often used to evaluate the efficacy and generalizability of state-of-the-art methods for many tasks in natural language processing (NLP). However, the presence of overlap between the train and test datasets can lead to inflated results, inadvertently evaluating the model's ability to memorize and interpreting it as the ability to generalize. In addition, such data sets may not provide an effective indicator of the performance of these methods in real world scenarios. We identify leakage of training data into test data on several publicly available datasets used to evaluate NLP tasks, including named entity recognition and relation extraction, and study them to assess the impact of that leakage on the model's ability to memorize versus generalize",
    "checked": true,
    "id": "9eea59c34f139f3d2153226c8cf026e975622074",
    "semantic_title": "memorization vs. generalization : quantifying data leakage in nlp performance evaluation",
    "citation_count": 71,
    "authors": [
      "Aparna Elangovan",
      "Jiayuan He",
      "Karin Verspoor"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.114": {
    "title": "An Expert Annotated Dataset for the Detection of Online Misogyny",
    "volume": "main",
    "abstract": "Online misogyny is a pernicious social problem that risks making online platforms toxic and unwelcoming to women. We present a new hierarchical taxonomy for online misogyny, as well as an expert labelled dataset to enable automatic classification of misogynistic content. The dataset consists of 6567 labels for Reddit posts and comments. As previous research has found untrained crowdsourced annotators struggle with identifying misogyny, we hired and trained annotators and provided them with robust annotation guidelines. We report baseline classification performance on the binary classification task, achieving accuracy of 0.93 and F1 of 0.43. The codebook and datasets are made freely available for future researchers",
    "checked": true,
    "id": "2002f59c7ff2fe9a2255b5b13b9fe427d379f4cb",
    "semantic_title": "an expert annotated dataset for the detection of online misogyny",
    "citation_count": 79,
    "authors": [
      "Ella Guest",
      "Bertie Vidgen",
      "Alexandros Mittos",
      "Nishanth Sastry",
      "Gareth Tyson",
      "Helen Margetts"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.115": {
    "title": "WikiMatrix: Mining 135M Parallel Sentences in 1620 Language Pairs from Wikipedia",
    "volume": "main",
    "abstract": "We present an approach based on multilingual sentence embeddings to automatically extract parallel sentences from the content of Wikipedia articles in 96 languages, including several dialects or low-resource languages. We do not limit the extraction process to alignments with English, but we systematically consider all possible language pairs. In total, we are able to extract 135M parallel sentences for 16720 different language pairs, out of which only 34M are aligned with English. This corpus is freely available. To get an indication on the quality of the extracted bitexts, we train neural MT baseline systems on the mined data only for 1886 languages pairs, and evaluate them on the TED corpus, achieving strong BLEU scores for many language pairs. The WikiMatrix bitexts seem to be particularly interesting to train MT systems between distant languages without the need to pivot through English",
    "checked": true,
    "id": "f48f90464d9694e2ea18767f14842c64c9a1e8fb",
    "semantic_title": "wikimatrix: mining 135m parallel sentences in 1620 language pairs from wikipedia",
    "citation_count": 353,
    "authors": [
      "Holger Schwenk",
      "Vishrav Chaudhary",
      "Shuo Sun",
      "Hongyu Gong",
      "Francisco Guzmán"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.116": {
    "title": "ChEMU-Ref: A Corpus for Modeling Anaphora Resolution in the Chemical Domain",
    "volume": "main",
    "abstract": "Chemical patents contain rich coreference and bridging links, which are the target of this research. Specially, we introduce a novel annotation scheme, based on which we create the ChEMU-Ref dataset from reaction description snippets in English-language chemical patents. We propose a neural approach to anaphora resolution, which we show to achieve strong results, especially when jointly trained over coreference and bridging links",
    "checked": true,
    "id": "0c6cc8516eea548e882ea2aa52a558a4da3427e9",
    "semantic_title": "chemu-ref: a corpus for modeling anaphora resolution in the chemical domain",
    "citation_count": 22,
    "authors": [
      "Biaoyan Fang",
      "Christian Druckenbrodt",
      "Saber A Akhondi",
      "Jiayuan He",
      "Timothy Baldwin",
      "Karin Verspoor"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.117": {
    "title": "Syntactic Nuclei in Dependency Parsing – A Multilingual Exploration",
    "volume": "main",
    "abstract": "Standard models for syntactic dependency parsing take words to be the elementary units that enter into dependency relations. In this paper, we investigate whether there are any benefits from enriching these models with the more abstract notion of nucleus proposed by Tesnière. We do this by showing how the concept of nucleus can be defined in the framework of Universal Dependencies and how we can use composition functions to make a transition-based dependency parser aware of this concept. Experiments on 12 languages show that nucleus composition gives small but significant improvements in parsing accuracy. Further analysis reveals that the improvement mainly concerns a small number of dependency relations, including nominal modifiers, relations of coordination, main predicates, and direct objects",
    "checked": true,
    "id": "77ff90deb5d29e57d51a24c56a542db846ec3c10",
    "semantic_title": "syntactic nuclei in dependency parsing – a multilingual exploration",
    "citation_count": 6,
    "authors": [
      "Ali Basirat",
      "Joakim Nivre"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.118": {
    "title": "Searching for Search Errors in Neural Morphological Inflection",
    "volume": "main",
    "abstract": "Neural sequence-to-sequence models are currently the predominant choice for language generation tasks. Yet, on word-level tasks, exact inference of these models reveals the empty string is often the global optimum. Prior works have speculated this phenomenon is a result of the inadequacy of neural models for language generation. However, in the case of morphological inflection, we find that the empty string is almost never the most probable solution under the model. Further, greedy search often finds the global optimum. These observations suggest that the poor calibration of many neural models may stem from characteristics of a specific subset of tasks rather than general ill-suitedness of such models for language generation",
    "checked": true,
    "id": "28d7562a716099a5147509bfd84f76de08b1192c",
    "semantic_title": "searching for search errors in neural morphological inflection",
    "citation_count": 5,
    "authors": [
      "Martina Forster",
      "Clara Meister",
      "Ryan Cotterell"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.119": {
    "title": "Quantifying Appropriateness of Summarization Data for Curriculum Learning",
    "volume": "main",
    "abstract": "Much research has reported the training data of summarization models are noisy; summaries often do not reflect what is written in the source texts. We propose an effective method of curriculum learning to train summarization models from such noisy data. Curriculum learning is used to train sequence-to-sequence models with noisy data. In translation tasks, previous research quantified noise of the training data using two models trained with noisy and clean corpora. Because such corpora do not exist in summarization fields, we propose a model that can quantify noise from a single noisy corpus. We conduct experiments on three summarization models; one pretrained model and two non-pretrained models, and verify our method improves the performance. Furthermore, we analyze how different curricula affect the performance of pretrained and non-pretrained summarization models. Our result on human evaluation also shows our method improves the performance of summarization models",
    "checked": true,
    "id": "f7fa268d6f6005660e341835310ff2fb27b80c64",
    "semantic_title": "quantifying appropriateness of summarization data for curriculum learning",
    "citation_count": 5,
    "authors": [
      "Ryuji Kano",
      "Takumi Takahashi",
      "Toru Nishino",
      "Motoki Taniguchi",
      "Tomoki Taniguchi",
      "Tomoko Ohkuma"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.120": {
    "title": "Evaluating language models for the retrieval and categorization of lexical collocations",
    "volume": "main",
    "abstract": "Lexical collocations are idiosyncratic combinations of two syntactically bound lexical items (e.g., \"heavy rain\" or \"take a step\"). Understanding their degree of compositionality and idiosyncrasy, as well their underlying semantics, is crucial for language learners, lexicographers and downstream NLP applications. In this paper, we perform an exhaustive analysis of current language models for collocation understanding. We first construct a dataset of apparitions of lexical collocations in context, categorized into 17 representative semantic categories. Then, we perform two experiments: (1) unsupervised collocate retrieval using BERT, and (2) supervised collocation classification in context. We find that most models perform well in distinguishing light verb constructions, especially if the collocation's first argument acts as subject, but often fail to distinguish, first, different syntactic structures within the same semantic category, and second, fine-grained semantic categories which restrict the use of small sets of valid collocates for a given base",
    "checked": true,
    "id": "de205f6ad118ac8baaa2b78b05f98299df1cd0b8",
    "semantic_title": "evaluating language models for the retrieval and categorization of lexical collocations",
    "citation_count": 9,
    "authors": [
      "Luis Espinosa Anke",
      "Joan Codina-Filba",
      "Leo Wanner"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.121": {
    "title": "BART-TL: Weakly-Supervised Topic Label Generation",
    "volume": "main",
    "abstract": "We propose a novel solution for assigning labels to topic models by using multiple weak labelers. The method leverages generative transformers to learn accurate representations of the most important topic terms and candidate labels. This is achieved by fine-tuning pre-trained BART models on a large number of potential labels generated by state of the art non-neural models for topic labeling, enriched with different techniques. The proposed BART-TL model is able to generate valuable and novel labels in a weakly-supervised manner and can be improved by adding other weak labelers or distant supervision on similar tasks",
    "checked": true,
    "id": "ae202083ed5759d70f0fffbfde7543f7663eb4c0",
    "semantic_title": "bart-tl: weakly-supervised topic label generation",
    "citation_count": 16,
    "authors": [
      "Cristian Popa",
      "Traian Rebedea"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.122": {
    "title": "Dynamic Graph Transformer for Implicit Tag Recognition",
    "volume": "main",
    "abstract": "Textual information extraction is a typical research topic in the NLP community. Several NLP tasks such as named entity recognition and relation extraction between entities have been well-studied in previous work. However, few works pay their attention to the implicit information. For example, a financial news article mentioned \"Apple Inc.\" may be also related to Samsung, even though Samsung is not explicitly mentioned in this article. This work presents a novel dynamic graph transformer that distills the textual information and the entity relations on the fly. Experimental results confirm the effectiveness of our approach to implicit tag recognition",
    "checked": true,
    "id": "57cc9670e51ed0c0f9a8a4496fd44a18d5e779d3",
    "semantic_title": "dynamic graph transformer for implicit tag recognition",
    "citation_count": 3,
    "authors": [
      "Yi-Ting Liou",
      "Chung-Chi Chen",
      "Hen-Hsen Huang",
      "Hsin-Hsi Chen"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.123": {
    "title": "Implicit Unlikelihood Training: Improving Neural Text Generation with Reinforcement Learning",
    "volume": "main",
    "abstract": "Likelihood training and maximization-based decoding result in dull and repetitive generated texts even when using powerful language models (Holtzman et al., 2019). Adding a loss function for regularization was shown to improve text generation output by helping avoid unwanted properties, such as contradiction or repetition (Li at al., 2020). In this work, we propose fine-tuning a language model by using policy gradient reinforcement learning, directly optimizing for better generation. We apply this approach to minimizing repetition in generated text, and show that, when combined with unlikelihood training (Welleck et al., 2020), our method further reduces repetition without impacting the language model quality. We also evaluate other methods for improving generation at training and decoding time, and compare them using various metrics aimed at control for better text generation output",
    "checked": true,
    "id": "a41ea72917a7440fffb3cd19a10055d48fe3c03d",
    "semantic_title": "implicit unlikelihood training: improving neural text generation with reinforcement learning",
    "citation_count": 17,
    "authors": [
      "Evgeny Lagutin",
      "Daniil Gavrilov",
      "Pavel Kalaidin"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.124": {
    "title": "Civil Rephrases Of Toxic Texts With Self-Supervised Transformers",
    "volume": "main",
    "abstract": "Platforms that support online commentary, from social networks to news sites, are increasingly leveraging machine learning to assist their moderation efforts. But this process does not typically provide feedback to the author that would help them contribute according to the community guidelines. This is prohibitively time-consuming for human moderators to do, and computational approaches are still nascent. This work focuses on models that can help suggest rephrasings of toxic comments in a more civil manner. Inspired by recent progress in unpaired sequence-to-sequence tasks, a self-supervised learning model is introduced, called CAE-T5. CAE-T5 employs a pre-trained text-to-text transformer, which is fine tuned with a denoising and cyclic auto-encoder loss. Experimenting with the largest toxicity detection dataset to date (Civil Comments) our model generates sentences that are more fluent and better at preserving the initial content compared to earlier text style transfer systems which we compare with using several scoring systems and human evaluation",
    "checked": true,
    "id": "31ced335258047c2b6703165887d87048b8acc98",
    "semantic_title": "civil rephrases of toxic texts with self-supervised transformers",
    "citation_count": 44,
    "authors": [
      "Léo Laugier",
      "John Pavlopoulos",
      "Jeffrey Sorensen",
      "Lucas Dixon"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.125": {
    "title": "Generating Weather Comments from Meteorological Simulations",
    "volume": "main",
    "abstract": "The task of generating weather-forecast comments from meteorological simulations has the following requirements: (i) the changes in numerical values for various physical quantities need to be considered, (ii) the weather comments should be dependent on delivery time and area information, and (iii) the comments should provide useful information for users. To meet these requirements, we propose a data-to-text model that incorporates three types of encoders for numerical forecast maps, observation data, and meta-data. We also introduce weather labels representing weather information, such as sunny and rain, for our model to explicitly describe useful information. We conducted automatic and human evaluations. The results indicate that our model performed best against baselines in terms of informativeness. We make our code and data publicly available",
    "checked": true,
    "id": "a0e187727337a196b5af94d70a318bc841ef9b0f",
    "semantic_title": "generating weather comments from meteorological simulations",
    "citation_count": 6,
    "authors": [
      "Soichiro Murakami",
      "Sora Tanaka",
      "Masatsugu Hangyo",
      "Hidetaka Kamigaito",
      "Kotaro Funakoshi",
      "Hiroya Takamura",
      "Manabu Okumura"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.126": {
    "title": "SICK-NL: A Dataset for Dutch Natural Language Inference",
    "volume": "main",
    "abstract": "We present SICK-NL (read: signal), a dataset targeting Natural Language Inference in Dutch. SICK-NL is obtained by translating the SICK dataset of (Marelli et al., 2014) from English into Dutch. Having a parallel inference dataset allows us to compare both monolingual and multilingual NLP models for English and Dutch on the two tasks. In the paper, we motivate and detail the translation process, perform a baseline evaluation on both the original SICK dataset and its Dutch incarnation SICK-NL, taking inspiration from Dutch skipgram embeddings and contextualised embedding models. In addition, we encapsulate two phenomena encountered in the translation to formulate stress tests and verify how well the Dutch models capture syntactic restructurings that do not affect semantics. Our main finding is all models perform worse on SICK-NL than on SICK, indicating that the Dutch dataset is more challenging than the English original. Results on the stress tests show that models don't fully capture word order freedom in Dutch, warranting future systematic studies",
    "checked": true,
    "id": "f69e4f2129c28b84a12b17d893160fcf02576e4e",
    "semantic_title": "sick-nl: a dataset for dutch natural language inference",
    "citation_count": 21,
    "authors": [
      "Gijs Wijnholds",
      "Michael Moortgat"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.127": {
    "title": "A phonetic model of non-native spoken word processing",
    "volume": "main",
    "abstract": "Non-native speakers show difficulties with spoken word processing. Many studies attribute these difficulties to imprecise phonological encoding of words in the lexical memory. We test an alternative hypothesis: that some of these difficulties can arise from the non-native speakers' phonetic perception. We train a computational model of phonetic learning, which has no access to phonology, on either one or two languages. We first show that the model exhibits predictable behaviors on phone-level and word-level discrimination tasks. We then test the model on a spoken word processing task, showing that phonology may not be necessary to explain some of the word processing effects observed in non-native speakers. We run an additional analysis of the model's lexical representation space, showing that the two training languages are not fully separated in that space, similarly to the languages of a bilingual human speaker",
    "checked": true,
    "id": "2d587a98cfb90f82d61854fd8ce6c0e974b99847",
    "semantic_title": "a phonetic model of non-native spoken word processing",
    "citation_count": 7,
    "authors": [
      "Yevgen Matusevych",
      "Herman Kamper",
      "Thomas Schatz",
      "Naomi Feldman",
      "Sharon Goldwater"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.128": {
    "title": "Bootstrapping Relation Extractors using Syntactic Search by Examples",
    "volume": "main",
    "abstract": "The advent of neural-networks in NLP brought with it substantial improvements in supervised relation extraction. However, obtaining a sufficient quantity of training data remains a key challenge. In this work we propose a process for bootstrapping training datasets which can be performed quickly by non-NLP-experts. We take advantage of search engines over syntactic-graphs (Such as Shlain et al. (2020)) which expose a friendly by-example syntax. We use these to obtain positive examples by searching for sentences that are syntactically similar to user input examples. We apply this technique to relations from TACRED and DocRED and show that the resulting models are competitive with models trained on manually annotated data and on data obtained from distant supervision. The models also outperform models trained using NLG data augmentation techniques. Extending the search-based approach with the NLG method further improves the results",
    "checked": true,
    "id": "aa68069cdc4e87afb36f280d411629f6bbc98eb1",
    "semantic_title": "bootstrapping relation extractors using syntactic search by examples",
    "citation_count": 5,
    "authors": [
      "Matan Eyal",
      "Asaf Amrami",
      "Hillel Taub-Tabib",
      "Yoav Goldberg"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.129": {
    "title": "Towards a Decomposable Metric for Explainable Evaluation of Text Generation from AMR",
    "volume": "main",
    "abstract": "Systems that generate natural language text from abstract meaning representations such as AMR are typically evaluated using automatic surface matching metrics that compare the generated texts to reference texts from which the input meaning representations were constructed. We show that besides well-known issues from which such metrics suffer, an additional problem arises when applying these metrics for AMR-to-text evaluation, since an abstract meaning representation allows for numerous surface realizations. In this work we aim to alleviate these issues by proposing ℳℱ𝛽, a decomposable metric that builds on two pillars. The first is the principle of meaning preservation ℳ : it measures to what extent a given AMR can be reconstructed from the generated sentence using SOTA AMR parsers and applying (fine-grained) AMR evaluation metrics to measure the distance between the original and the reconstructed AMR. The second pillar builds on a principle of (grammatical) form ℱ that measures the linguistic quality of the generated text, which we implement using SOTA language models. In two extensive pilot studies we show that fulfillment of both principles offers benefits for AMR-to-text evaluation, including explainability of scores. Since ℳℱ𝛽 does not necessarily rely on gold AMRs, it may extend to other text generation tasks",
    "checked": true,
    "id": "8ba1056fe79b1f15b561f891bc9292ac37ec50b8",
    "semantic_title": "towards a decomposable metric for explainable evaluation of text generation from amr",
    "citation_count": 33,
    "authors": [
      "Juri Opitz",
      "Anette Frank"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.130": {
    "title": "The Source-Target Domain Mismatch Problem in Machine Translation",
    "volume": "main",
    "abstract": "While we live in an increasingly interconnected world, different places still exhibit strikingly different cultures and many events we experience in our every day life pertain only to the specific place we live in. As a result, people often talk about different things in different parts of the world. In this work we study the effect of local context in machine translation and postulate that this causes the domains of the source and target language to greatly mismatch. We first formalize the concept of source-target domain mismatch, propose a metric to quantify it, and provide empirical evidence for its existence. We conclude with an empirical study of how source-target domain mismatch affects training of machine translation systems on low resource languages. While this may severely affect back-translation, the degradation can be alleviated by combining back-translation with self-training and by increasing the amount of target side monolingual data",
    "checked": true,
    "id": "9dc4a5284ecfd37ab8bc8990eddf1b39113e004b",
    "semantic_title": "the source-target domain mismatch problem in machine translation",
    "citation_count": 21,
    "authors": [
      "Jiajun Shen",
      "Peng-Jen Chen",
      "Matthew Le",
      "Junxian He",
      "Jiatao Gu",
      "Myle Ott",
      "Michael Auli",
      "Marc’Aurelio Ranzato"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.131": {
    "title": "Cross-Topic Rumor Detection using Topic-Mixtures",
    "volume": "main",
    "abstract": "There has been much interest in rumor detection using deep learning models in recent years. A well-known limitation of deep learning models is that they tend to learn superficial patterns, which restricts their generalization ability. We find that this is also true for cross-topic rumor detection. In this paper, we propose a method inspired by the \"mixture of experts\" paradigm. We assume that the prediction of the rumor class label given an instance is dependent on the topic distribution of the instance. After deriving a vector representation for each topic, given an instance, we derive a \"topic mixture\" vector for the instance based on its topic distribution. This topic mixture is combined with the vector representation of the instance itself to make rumor predictions. Our experiments show that our proposed method can outperform two baseline debiasing methods in a cross-topic setting. In a synthetic setting when we removed topic-specific words, our method also works better than the baselines, showing that our method does not rely on superficial features",
    "checked": true,
    "id": "cb3767751f64ac159e637d1694f110d811ea6928",
    "semantic_title": "cross-topic rumor detection using topic-mixtures",
    "citation_count": 11,
    "authors": [
      "Xiaoying Ren",
      "Jing Jiang",
      "Ling Min Serena Khoo",
      "Hai Leong Chieu"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.132": {
    "title": "Understanding Pre-Editing for Black-Box Neural Machine Translation",
    "volume": "main",
    "abstract": "Pre-editing is the process of modifying the source text (ST) so that it can be translated by machine translation (MT) in a better quality. Despite the unpredictability of black-box neural MT (NMT), pre-editing has been deployed in various practical MT use cases. Although many studies have demonstrated the effectiveness of pre-editing methods for particular settings, thus far, a deep understanding of what pre-editing is and how it works for black-box NMT is lacking. To elicit such understanding, we extensively investigated human pre-editing practices. We first implemented a protocol to incrementally record the minimum edits for each ST and collected 6,652 instances of pre-editing across three translation directions, two MT systems, and four text domains. We then analysed the instances from three perspectives: the characteristics of the pre-edited ST, the diversity of pre-editing operations, and the impact of the pre-editing operations on NMT outputs. Our findings include the following: (1) enhancing the explicitness of the meaning of an ST and its syntactic structure is more important for obtaining better translations than making the ST shorter and simpler, and (2) although the impact of pre-editing on NMT is generally unpredictable, there are some tendencies of changes in the NMT outputs depending on the editing operation types",
    "checked": true,
    "id": "1990ffbebe3b5e6ec95f0c193ff40c01cd478f4c",
    "semantic_title": "understanding pre-editing for black-box neural machine translation",
    "citation_count": 9,
    "authors": [
      "Rei Miyata",
      "Atsushi Fujita"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.133": {
    "title": "RelWalk - A Latent Variable Model Approach to Knowledge Graph Embedding",
    "volume": "main",
    "abstract": "Embedding entities and relations of a knowledge graph in a low-dimensional space has shown impressive performance in predicting missing links between entities. Although progresses have been achieved, existing methods are heuristically motivated and theoretical understanding of such embeddings is comparatively underdeveloped. This paper extends the random walk model of word embeddings to Knowledge Graph Embeddings (KGEs) to derive a scoring function that evaluates the strength of a relation R between two entities h (head) and t (tail). Moreover, we show that marginal loss minimisation, a popular objective used in much prior work in KGE, follows naturally from the log-likelihood ratio maximisation under the probabilities estimated from the KGEs according to our theoretical relationship. We propose a learning objective motivated by the theoretical analysis to learn KGEs from a given knowledge graph. Using the derived objective, accurate KGEs are learnt from FB15K237 and WN18RR benchmark datasets, providing empirical evidence in support of the theory",
    "checked": true,
    "id": "aaaa736f75c639d810d1998286ef7652934d06f8",
    "semantic_title": "relwalk - a latent variable model approach to knowledge graph embedding",
    "citation_count": 6,
    "authors": [
      "Danushka Bollegala",
      "Huda Hakami",
      "Yuichi Yoshida",
      "Ken-ichi Kawarabayashi"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.134": {
    "title": "Few-shot Learning for Slot Tagging with Attentive Relational Network",
    "volume": "main",
    "abstract": "Metric-based learning is a well-known family of methods for few-shot learning, especially in computer vision. Recently, they have been used in many natural language processing applications but not for slot tagging. In this paper, we explore metric-based learning methods in the slot tagging task and propose a novel metric-based learning architecture - Attentive Relational Network. Our proposed method extends relation networks, making them more suitable for natural language processing applications in general, by leveraging pretrained contextual embeddings such as ELMO and BERT and by using attention mechanism. The results on SNIPS data show that our proposed method outperforms other state of the art metric-based learning methods",
    "checked": true,
    "id": "0364102c8805bd1889467b5a1aaa6916764ffa0e",
    "semantic_title": "few-shot learning for slot tagging with attentive relational network",
    "citation_count": 9,
    "authors": [
      "Cennet Oguz",
      "Ngoc Thang Vu"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.135": {
    "title": "SpanEmo: Casting Multi-label Emotion Classification as Span-prediction",
    "volume": "main",
    "abstract": "Emotion recognition (ER) is an important task in Natural Language Processing (NLP), due to its high impact in real-world applications from health and well-being to author profiling, consumer analysis and security. Current approaches to ER, mainly classify emotions independently without considering that emotions can co-exist. Such approaches overlook potential ambiguities, in which multiple emotions overlap. We propose a new model \"SpanEmo\" casting multi-label emotion classification as span-prediction, which can aid ER models to learn associations between labels and words in a sentence. Furthermore, we introduce a loss function focused on modelling multiple co-existing emotions in the input sentence. Experiments performed on the SemEval2018 multi-label emotion data over three language sets (i.e., English, Arabic and Spanish) demonstrate our method's effectiveness. Finally, we present different analyses that illustrate the benefits of our method in terms of improving the model performance and learning meaningful associations between emotion classes and words in the sentence",
    "checked": true,
    "id": "79a05e01b2eb5b8ab006f57b9a0cb7843c77f719",
    "semantic_title": "spanemo: casting multi-label emotion classification as span-prediction",
    "citation_count": 73,
    "authors": [
      "Hassan Alhuzali",
      "Sophia Ananiadou"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.136": {
    "title": "Exploiting Position and Contextual Word Embeddings for Keyphrase Extraction from Scientific Papers",
    "volume": "main",
    "abstract": "Keyphrases associated with research papers provide an effective way to find useful information in the large and growing scholarly digital collections. In this paper, we present KPRank, an unsupervised graph-based algorithm for keyphrase extraction that exploits both positional information and contextual word embeddings into a biased PageRank. Our experimental results on five benchmark datasets show that KPRank that uses contextual word embeddings with additional position signal outperforms previous approaches and strong baselines for this task",
    "checked": true,
    "id": "b5f79f3618ed777c445019dfa563e07620340aae",
    "semantic_title": "exploiting position and contextual word embeddings for keyphrase extraction from scientific papers",
    "citation_count": 8,
    "authors": [
      "Krutarth Patel",
      "Cornelia Caragea"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.137": {
    "title": "Benchmarking Machine Reading Comprehension: A Psychological Perspective",
    "volume": "main",
    "abstract": "Machine reading comprehension (MRC) has received considerable attention as a benchmark for natural language understanding. However, the conventional task design of MRC lacks explainability beyond the model interpretation, i.e., reading comprehension by a model cannot be explained in human terms. To this end, this position paper provides a theoretical basis for the design of MRC datasets based on psychology as well as psychometrics, and summarizes it in terms of the prerequisites for benchmarking MRC. We conclude that future datasets should (i) evaluate the capability of the model for constructing a coherent and grounded representation to understand context-dependent situations and (ii) ensure substantive validity by shortcut-proof questions and explanation as a part of the task design",
    "checked": true,
    "id": "f9b6d8c666f71bd1c62408234b3ddf31a66bb98f",
    "semantic_title": "benchmarking machine reading comprehension: a psychological perspective",
    "citation_count": 12,
    "authors": [
      "Saku Sugawara",
      "Pontus Stenetorp",
      "Akiko Aizawa"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.138": {
    "title": "Multilingual Neural Machine Translation with Deep Encoder and Multiple Shallow Decoders",
    "volume": "main",
    "abstract": "Recent work in multilingual translation advances translation quality surpassing bilingual baselines using deep transformer models with increased capacity. However, the extra latency and memory costs introduced by this approach may make it unacceptable for efficiency-constrained applications. It has recently been shown for bilingual translation that using a deep encoder and shallow decoder (DESD) can reduce inference latency while maintaining translation quality, so we study similar speed-accuracy trade-offs for multilingual translation. We find that for many-to-one translation we can indeed increase decoder speed without sacrificing quality using this approach, but for one-to-many translation, shallow decoders cause a clear quality drop. To ameliorate this drop, we propose a deep encoder with multiple shallow decoders (DEMSD) where each shallow decoder is responsible for a disjoint subset of target languages. Specifically, the DEMSD model with 2-layer decoders is able to obtain a 1.8x speedup on average compared to a standard transformer model with no drop in translation quality",
    "checked": true,
    "id": "3ca5af81c40fad6d61a4a8bc22c5fb83f4bb3dce",
    "semantic_title": "multilingual neural machine translation with deep encoder and multiple shallow decoders",
    "citation_count": 28,
    "authors": [
      "Xiang Kong",
      "Adithya Renduchintala",
      "James Cross",
      "Yuqing Tang",
      "Jiatao Gu",
      "Xian Li"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.139": {
    "title": "With Measured Words: Simple Sentence Selection for Black-Box Optimization of Sentence Compression Algorithms",
    "volume": "main",
    "abstract": "Sentence Compression is the task of generating a shorter, yet grammatical, version of a given sentence, preserving the essence of the original sentence. This paper proposes a Black-Box Optimizer for Compression (B-BOC): given a black-box compression algorithm and assuming not all sentences need be compressed – find the best candidates for compression in order to maximize both compression rate and quality. Given a required compression ratio, we consider two scenarios: (i) single-sentence compression, and (ii) sentences-sequence compression. In the first scenario our optimizer is trained to predict how well each sentence could be compressed while meeting the specified ratio requirement. In the latter, the desired compression ratio is applied to a sequence of sentences (e.g., a paragraph) as a whole, rather than on each individual sentence. To achieve that we use B-BOC to assign an optimal compression ratio to each sentence, then cast it as a Knapsack problem which we solve using bounded dynamic programming. We evaluate B-BOC on both scenarios on three datasets, demonstrating that our optimizer improves both accuracy and Rouge-F1-score compared to direct application of other compression algorithms",
    "checked": true,
    "id": "1c2a89eca437ce92ebcc1c50b8e0729d611b02ea",
    "semantic_title": "with measured words: simple sentence selection for black-box optimization of sentence compression algorithms",
    "citation_count": 2,
    "authors": [
      "Yotam Shichel",
      "Meir Kalech",
      "Oren Tsur"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.140": {
    "title": "WiC-TSV: An Evaluation Benchmark for Target Sense Verification of Words in Context",
    "volume": "main",
    "abstract": "We present WiC-TSV, a new multi-domain evaluation benchmark for Word Sense Disambiguation. More specifically, we introduce a framework for Target Sense Verification of Words in Context which grounds its uniqueness in the formulation as binary classification task thus being independent of external sense inventories, and the coverage of various domains. This makes the dataset highly flexible for the evaluation of a diverse set of models and systems in and across domains. WiC-TSV provides three different evaluation settings, depending on the input signals provided to the model. We set baseline performance on the dataset using state-of-the-art language models. Experimental results show that even though these models can perform decently on the task, there remains a gap between machine and human performance, especially in out-of-domain settings. WiC-TSV data is available at https://competitions.codalab.org/competitions/23683",
    "checked": true,
    "id": "5cb408fbe7d94ecbff20f07dfcd73a47a184058b",
    "semantic_title": "wic-tsv: an evaluation benchmark for target sense verification of words in context",
    "citation_count": 22,
    "authors": [
      "Anna Breit",
      "Artem Revenko",
      "Kiamehr Rezaee",
      "Mohammad Taher Pilehvar",
      "Jose Camacho-Collados"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.141": {
    "title": "Self-Supervised and Controlled Multi-Document Opinion Summarization",
    "volume": "main",
    "abstract": "We address the problem of unsupervised abstractive summarization of collections of user generated reviews through self-supervision and control. We propose a self-supervised setup that considers an individual document as a target summary for a set of similar documents. This setting makes training simpler than previous approaches by relying only on standard log-likelihood loss and mainstream models. We address the problem of hallucinations through the use of control codes, to steer the generation towards more coherent and relevant summaries",
    "checked": true,
    "id": "383594683e34cf027ef137a2929def0f03eed8a6",
    "semantic_title": "self-supervised and controlled multi-document opinion summarization",
    "citation_count": 44,
    "authors": [
      "Hady Elsahar",
      "Maximin Coavoux",
      "Jos Rozen",
      "Matthias Gallé"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.142": {
    "title": "NewsMTSC: A Dataset for (Multi-)Target-dependent Sentiment Classification in Political News Articles",
    "volume": "main",
    "abstract": "Previous research on target-dependent sentiment classification (TSC) has mostly focused on reviews, social media, and other domains where authors tend to express sentiment explicitly. In this paper, we investigate TSC in news articles, a much less researched TSC domain despite the importance of news as an essential information source in individual and societal decision making. We introduce NewsMTSC, a high-quality dataset for TSC on news articles with key differences compared to established TSC datasets, including, for example, different means to express sentiment, longer texts, and a second test-set to measure the influence of multi-target sentences. We also propose a model that uses a BiGRU to interact with multiple embeddings, e.g., from a language model and external knowledge sources. The proposed model improves the performance of the prior state-of-the-art from F1_m=81.7 to 83.1 (real-world sentiment distribution) and from F1_m=81.2 to 82.5 (multi-target sentences)",
    "checked": true,
    "id": "d20ae75bb7f92eb4597ccc125ee24b58801de6bd",
    "semantic_title": "newsmtsc: a dataset for (multi-)target-dependent sentiment classification in political news articles",
    "citation_count": 39,
    "authors": [
      "Felix Hamborg",
      "Karsten Donnay"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.143": {
    "title": "Cross-lingual Contextualized Topic Models with Zero-shot Learning",
    "volume": "main",
    "abstract": "Many data sets (e.g., reviews, forums, news, etc.) exist parallelly in multiple languages. They all cover the same content, but the linguistic differences make it impossible to use traditional, bag-of-word-based topic models. Models have to be either single-language or suffer from a huge, but extremely sparse vocabulary. Both issues can be addressed by transfer learning. In this paper, we introduce a zero-shot cross-lingual topic model. Our model learns topics on one language (here, English), and predicts them for unseen documents in different languages (here, Italian, French, German, and Portuguese). We evaluate the quality of the topic predictions for the same document in different languages. Our results show that the transferred topics are coherent and stable across languages, which suggests exciting future research directions",
    "checked": true,
    "id": "6ad3a7ec282072ecc4be5c11cd7e307629976af8",
    "semantic_title": "cross-lingual contextualized topic models with zero-shot learning",
    "citation_count": 118,
    "authors": [
      "Federico Bianchi",
      "Silvia Terragni",
      "Dirk Hovy",
      "Debora Nozza",
      "Elisabetta Fersini"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.144": {
    "title": "Dependency parsing with structure preserving embeddings",
    "volume": "main",
    "abstract": "Modern neural approaches to dependency parsing are trained to predict a tree structure by jointly learning a contextual representation for tokens in a sentence, as well as a head–dependent scoring function. Whereas this strategy results in high performance, it is difficult to interpret these representations in relation to the geometry of the underlying tree structure. Our work seeks instead to learn interpretable representations by training a parser to explicitly preserve structural properties of a tree. We do so by casting dependency parsing as a tree embedding problem where we incorporate geometric properties of dependency trees in the form of training losses within a graph-based parser. We provide a thorough evaluation of these geometric losses, showing that a majority of them yield strong tree distance preservation as well as parsing performance on par with a competitive graph-based parser (Qi et al., 2018). Finally, we show where parsing errors lie in terms of tree relationship in order to guide future work",
    "checked": true,
    "id": "dd5d0add6cc2739fc06f8dcb61210408df7c9714",
    "semantic_title": "dependency parsing with structure preserving embeddings",
    "citation_count": 0,
    "authors": [
      "Ákos Kádár",
      "Lan Xiao",
      "Mete Kemertas",
      "Federico Fancellu",
      "Allan Jepson",
      "Afsaneh Fazly"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.145": {
    "title": "Active Learning for Sequence Tagging with Deep Pre-trained Models and Bayesian Uncertainty Estimates",
    "volume": "main",
    "abstract": "Annotating training data for sequence tagging of texts is usually very time-consuming. Recent advances in transfer learning for natural language processing in conjunction with active learning open the possibility to significantly reduce the necessary annotation budget. We are the first to thoroughly investigate this powerful combination for the sequence tagging task. We conduct an extensive empirical study of various Bayesian uncertainty estimation methods and Monte Carlo dropout options for deep pre-trained models in the active learning framework and find the best combinations for different types of models. Besides, we also demonstrate that to acquire instances during active learning, a full-size Transformer can be substituted with a distilled version, which yields better computational performance and reduces obstacles for applying deep active learning in practice",
    "checked": true,
    "id": "194d1089b730e4a1d15a60a0144b50771909c3c5",
    "semantic_title": "active learning for sequence tagging with deep pre-trained models and bayesian uncertainty estimates",
    "citation_count": 44,
    "authors": [
      "Artem Shelmanov",
      "Dmitri Puzyrev",
      "Lyubov Kupriyanova",
      "Denis Belyakov",
      "Daniil Larionov",
      "Nikita Khromov",
      "Olga Kozlova",
      "Ekaterina Artemova",
      "Dmitry V. Dylov",
      "Alexander Panchenko"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.146": {
    "title": "MultiHumES: Multilingual Humanitarian Dataset for Extractive Summarization",
    "volume": "main",
    "abstract": "When responding to a disaster, humanitarian experts must rapidly process large amounts of secondary data sources to derive situational awareness and guide decision-making. While these documents contain valuable information, manually processing them is extremely time-consuming when an expedient response is necessary. To improve this process, effective summarization models are a valuable tool for humanitarian response experts as they provide digestible overviews of essential information in secondary data. This paper focuses on extractive summarization for the humanitarian response domain and describes and makes public a new multilingual data collection for this purpose. The collection – called MultiHumES– provides multilingual documents coupled with informative snippets that have been annotated by humanitarian analysts over the past four years. We report the performance results of a recent neural networks-based summarization model together with other baselines. We hope that the released data collection can further grow the research on multilingual extractive summarization in the humanitarian response domain",
    "checked": true,
    "id": "b1dd23c394b6f802e68146c8ae544d957f703b5c",
    "semantic_title": "multihumes: multilingual humanitarian dataset for extractive summarization",
    "citation_count": 7,
    "authors": [
      "Jenny Paola Yela-Bello",
      "Ewan Oglethorpe",
      "Navid Rekabsaz"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.147": {
    "title": "Learning From Revisions: Quality Assessment of Claims in Argumentation at Scale",
    "volume": "main",
    "abstract": "Assessing the quality of arguments and of the claims the arguments are composed of has become a key task in computational argumentation. However, even if different claims share the same stance on the same topic, their assessment depends on the prior perception and weighting of the different aspects of the topic being discussed. This renders it difficult to learn topic-independent quality indicators. In this paper, we study claim quality assessment irrespective of discussed aspects by comparing different revisions of the same claim. We compile a large-scale corpus with over 377k claim revision pairs of various types from kialo.com, covering diverse topics from politics, ethics, entertainment, and others. We then propose two tasks: (a) assessing which claim of a revision pair is better, and (b) ranking all versions of a claim by quality. Our first experiments with embedding-based logistic regression and transformer-based neural networks show promising results, suggesting that learned indicators generalize well across topics. In a detailed error analysis, we give insights into what quality dimensions of claims can be assessed reliably. We provide the data and scripts needed to reproduce all results",
    "checked": true,
    "id": "031f4e23371e3324cdefc169e755d95eccdcaca2",
    "semantic_title": "learning from revisions: quality assessment of claims in argumentation at scale",
    "citation_count": 19,
    "authors": [
      "Gabriella Skitalinskaya",
      "Jonas Klaff",
      "Henning Wachsmuth"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.148": {
    "title": "Few Shot Dialogue State Tracking using Meta-learning",
    "volume": "main",
    "abstract": "Dialogue State Tracking (DST) forms a core component of automated chatbot based systems designed for specific goals like hotel, taxi reservation, tourist information etc. With the increasing need to deploy such systems in new domains, solving the problem of zero/few-shot DST has become necessary. There has been a rising trend for learning to transfer knowledge from resource-rich domains to unknown domains with minimal need for additional data. In this work, we explore the merits of meta-learning algorithms for this transfer and hence, propose a meta-learner D-REPTILE specific to the DST problem. With extensive experimentation, we provide clear evidence of benefits over conventional approaches across different domains, methods, base models and datasets with significant (5-25%) improvement over the baseline in a low-data setting. Our proposed meta-learner is agnostic of the underlying model and hence any existing state-of-the-art DST system can improve its performance on unknown domains using our training strategy",
    "checked": true,
    "id": "25bc5963b562b48a570afed49ba9f36d208b9b18",
    "semantic_title": "few shot dialogue state tracking using meta-learning",
    "citation_count": 19,
    "authors": [
      "Saket Dingliwal",
      "Shuyang Gao",
      "Sanchit Agarwal",
      "Chien-Wei Lin",
      "Tagyoung Chung",
      "Dilek Hakkani-Tur"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.149": {
    "title": "BERT Prescriptions to Avoid Unwanted Headaches: A Comparison of Transformer Architectures for Adverse Drug Event Detection",
    "volume": "main",
    "abstract": "Pretrained transformer-based models, such as BERT and its variants, have become a common choice to obtain state-of-the-art performances in NLP tasks. In the identification of Adverse Drug Events (ADE) from social media texts, for example, BERT architectures rank first in the leaderboard. However, a systematic comparison between these models has not yet been done. In this paper, we aim at shedding light on the differences between their performance analyzing the results of 12 models, tested on two standard benchmarks. SpanBERT and PubMedBERT emerged as the best models in our evaluation: this result clearly shows that span-based pretraining gives a decisive advantage in the precise recognition of ADEs, and that in-domain language pretraining is particularly useful when the transformer model is trained just on biomedical text from scratch",
    "checked": true,
    "id": "98b1f4049e5b78f0b7237fc4921d00d3f3b6d13f",
    "semantic_title": "bert prescriptions to avoid unwanted headaches: a comparison of transformer architectures for adverse drug event detection",
    "citation_count": 27,
    "authors": [
      "Beatrice Portelli",
      "Edoardo Lenzi",
      "Emmanuele Chersoni",
      "Giuseppe Serra",
      "Enrico Santus"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.150": {
    "title": "Semantic Parsing of Disfluent Speech",
    "volume": "main",
    "abstract": "Speech disfluencies are prevalent in spontaneous speech. The rising popularity of voice assistants presents a growing need to handle naturally occurring disfluencies. Semantic parsing is a key component for understanding user utterances in voice assistants, yet most semantic parsing research to date focuses on written text. In this paper, we investigate semantic parsing of disfluent speech with the ATIS dataset. We find that a state-of-the-art semantic parser does not seamlessly handle disfluencies. We experiment with adding real and synthetic disfluencies at training time and find that adding synthetic disfluencies not only improves model performance by up to 39% but can also outperform adding real disfluencies in the ATIS dataset",
    "checked": true,
    "id": "bbcb01ee54dae0f1eeeae9fcd9fccf14675ea645",
    "semantic_title": "semantic parsing of disfluent speech",
    "citation_count": 3,
    "authors": [
      "Priyanka Sen",
      "Isabel Groves"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.151": {
    "title": "Joint Energy-based Model Training for Better Calibrated Natural Language Understanding Models",
    "volume": "main",
    "abstract": "In this work, we explore joint energy-based model (EBM) training during the finetuning of pretrained text encoders (e.g., Roberta) for natural language understanding (NLU) tasks. Our experiments show that EBM training can help the model reach a better calibration that is competitive to strong baselines, with little or no loss in accuracy. We discuss three variants of energy functions (namely scalar, hidden, and sharp-hidden) that can be defined on top of a text encoder, and compare them in experiments. Due to the discreteness of text data, we adopt noise contrastive estimation (NCE) to train the energy-based model. To make NCE training more effective, we train an auto-regressive noise model with the masked language model (MLM) objective",
    "checked": true,
    "id": "91fed2f5718b9c1dc25b768c8aaf33455f0404c8",
    "semantic_title": "joint energy-based model training for better calibrated natural language understanding models",
    "citation_count": 17,
    "authors": [
      "Tianxing He",
      "Bryan McCann",
      "Caiming Xiong",
      "Ehsan Hosseini-Asl"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.152": {
    "title": "What Sounds \"Right\" to Me? Experiential Factors in the Perception of Political Ideology",
    "volume": "main",
    "abstract": "In this paper, we challenge the assumption that political ideology is inherently built into text by presenting an investigation into the impact of experiential factors on annotator perceptions of political ideology. We construct an annotated corpus of U.S. political discussion, where in addition to ideology labels for texts, annotators provide information about their political affiliation, exposure to political news, and familiarity with the source domain of discussion, Reddit. We investigate the variability in ideology judgments across annotators, finding evidence that these experiential factors may influence the consistency of how political ideologies are perceived. Finally, we present evidence that understanding how humans perceive and interpret ideology from texts remains a challenging task for state-of-the-art language models, pointing towards potential issues when modeling user experiences that may require more contextual knowledge",
    "checked": true,
    "id": "94b8305a6b3469dc85af58c74ea93a0e16fb112f",
    "semantic_title": "what sounds \"right\" to me? experiential factors in the perception of political ideology",
    "citation_count": 10,
    "authors": [
      "Qinlan Shen",
      "Carolyn Rose"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.153": {
    "title": "Language Models as Knowledge Bases: On Entity Representations, Storage Capacity, and Paraphrased Queries",
    "volume": "main",
    "abstract": "Pretrained language models have been suggested as a possible alternative or complement to structured knowledge bases. However, this emerging LM-as-KB paradigm has so far only been considered in a very limited setting, which only allows handling 21k entities whose name is found in common LM vocabularies. Furthermore, a major benefit of this paradigm, i.e., querying the KB using natural language paraphrases, is underexplored. Here we formulate two basic requirements for treating LMs as KBs: (i) the ability to store a large number facts involving a large number of entities and (ii) the ability to query stored facts. We explore three entity representations that allow LMs to handle millions of entities and present a detailed case study on paraphrased querying of facts stored in LMs, thereby providing a proof-of-concept that language models can indeed serve as knowledge bases",
    "checked": true,
    "id": "51ae2c451a1a05293334a509b71c9c9e0377d35c",
    "semantic_title": "language models as knowledge bases: on entity representations, storage capacity, and paraphrased queries",
    "citation_count": 107,
    "authors": [
      "Benjamin Heinzerling",
      "Kentaro Inui"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.154": {
    "title": "Globalizing BERT-based Transformer Architectures for Long Document Summarization",
    "volume": "main",
    "abstract": "Fine-tuning a large language model on downstream tasks has become a commonly adopted process in the Natural Language Processing (NLP) (CITATION). However, such a process, when associated with the current transformer-based (CITATION) architectures, shows several limitations when the target task requires to reason with long documents. In this work, we introduce a novel hierarchical propagation layer that spreads information between multiple transformer windows. We adopt a hierarchical approach where the input is divided in multiple blocks independently processed by the scaled dot-attentions and combined between the successive layers. We validate the effectiveness of our approach on three extractive summarization corpora of long scientific papers and news articles. We compare our approach to standard and pre-trained language-model-based summarizers and report state-of-the-art results for long document summarization and comparable results for smaller document summarization",
    "checked": true,
    "id": "4d244972ed2e0286363bfb054cb269574d21a72c",
    "semantic_title": "globalizing bert-based transformer architectures for long document summarization",
    "citation_count": 59,
    "authors": [
      "Quentin Grail",
      "Julien Perez",
      "Eric Gaussier"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.155": {
    "title": "Through the Looking Glass: Learning to Attribute Synthetic Text Generated by Language Models",
    "volume": "main",
    "abstract": "Given the potential misuse of recent advances in synthetic text generation by language models (LMs), it is important to have the capacity to attribute authorship of synthetic text. While stylometric organic (i.e., human written) authorship attribution has been quite successful, it is unclear whether similar approaches can be used to attribute a synthetic text to its source LM. We address this question with the key insight that synthetic texts carry subtle distinguishing marks inherited from their source LM and that these marks can be leveraged by machine learning (ML) algorithms for attribution. We propose and test several ML-based attribution methods. Our best attributor built using a fine-tuned version of XLNet (XLNet-FT) consistently achieves excellent accuracy scores (91% to near perfect 98%) in terms of attributing the parent pre-trained LM behind a synthetic text. Our experiments show promising results across a range of experiments where the synthetic text may be generated using pre-trained LMs, fine-tuned LMs, or by varying text generation parameters",
    "checked": true,
    "id": "03e6b951e3665d28a7beeef87fcb1c9baecfd396",
    "semantic_title": "through the looking glass: learning to attribute synthetic text generated by language models",
    "citation_count": 23,
    "authors": [
      "Shaoor Munir",
      "Brishna Batool",
      "Zubair Shafiq",
      "Padmini Srinivasan",
      "Fareed Zaffar"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.156": {
    "title": "We Need To Talk About Random Splits",
    "volume": "main",
    "abstract": "(CITATION) argued for using random splits rather than standard splits in NLP experiments. We argue that random splits, like standard splits, lead to overly optimistic performance estimates. We can also split data in biased or adversarial ways, e.g., training on short sentences and evaluating on long ones. Biased sampling has been used in domain adaptation to simulate real-world drift; this is known as the covariate shift assumption. In NLP, however, even worst-case splits, maximizing bias, often under-estimate the error observed on new samples of in-domain data, i.e., the data that models should minimally generalize to at test time. This invalidates the covariate shift assumption. Instead of using multiple random splits, future benchmarks should ideally include multiple, independent test sets instead; if infeasible, we argue that multiple biased splits leads to more realistic performance estimates than multiple random splits",
    "checked": true,
    "id": "d0fcdf47561ff742c9a72495102f16646eca43b7",
    "semantic_title": "we need to talk about random splits",
    "citation_count": 91,
    "authors": [
      "Anders Søgaard",
      "Sebastian Ebert",
      "Jasmijn Bastings",
      "Katja Filippova"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.157": {
    "title": "How Certain is Your Transformer?",
    "volume": "main",
    "abstract": "In this work, we consider the problem of uncertainty estimation for Transformer-based models. We investigate the applicability of uncertainty estimates based on dropout usage at the inference stage (Monte Carlo dropout). The series of experiments on natural language understanding tasks shows that the resulting uncertainty estimates improve the quality of detection of error-prone instances. Special attention is paid to the construction of computationally inexpensive estimates via Monte Carlo dropout and Determinantal Point Processes",
    "checked": true,
    "id": "544cdd96dff6f4744ff3186b1f6f8e6b5d997fa6",
    "semantic_title": "how certain is your transformer?",
    "citation_count": 42,
    "authors": [
      "Artem Shelmanov",
      "Evgenii Tsymbalov",
      "Dmitri Puzyrev",
      "Kirill Fedyanin",
      "Alexander Panchenko",
      "Maxim Panov"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.158": {
    "title": "Alignment verification to improve NMT translation towards highly inflectional languages with limited resources",
    "volume": "main",
    "abstract": "The present article discusses how to improve translation quality when using limited training data to translate towards morphologically rich languages. The starting point is a neural MT system, used to train translation models, using solely publicly available parallel data. An initial analysis of the translation output has shown that quality is sub-optimal, due mainly to an insufficient amount of training data. To improve translation quality, a hybridized solution is proposed, using an ensemble of relatively simple NMT systems trained with different metrics, combined with an open source module, designed for a low-resource MT system. Experimental results of the proposed hybridized method with multiple independent test sets achieve improvements over (i) both the best individual NMT and (ii) the standard ensemble system provided in the Marian-NMT system. Improvements over Marian-NMT are in many cases statistically significant. Finally, a qualitative analysis of translation results indicates a greater robustness for the hybridized method",
    "checked": true,
    "id": "0936df681141abc10aecc2c7ac17983996dfab52",
    "semantic_title": "alignment verification to improve nmt translation towards highly inflectional languages with limited resources",
    "citation_count": 0,
    "authors": [
      "George Tambouratzis"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.159": {
    "title": "Data Augmentation for Voice-Assistant NLU using BERT-based Interchangeable Rephrase",
    "volume": "main",
    "abstract": "We introduce a data augmentation technique based on byte pair encoding and a BERT-like self-attention model to boost performance on spoken language understanding tasks. We compare and evaluate this method with a range of augmentation techniques encompassing generative models such as VAEs and performance-boosting techniques such as synonym replacement and back-translation. We show our method performs strongly on domain and intent classification tasks for a voice assistant and in a user-study focused on utterance naturalness and semantic similarity",
    "checked": true,
    "id": "fe7dab5fff5b46663260c2889e229119f455b2e7",
    "semantic_title": "data augmentation for voice-assistant nlu using bert-based interchangeable rephrase",
    "citation_count": 3,
    "authors": [
      "Akhila Yerukola",
      "Mason Bretan",
      "Hongxia Jin"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.160": {
    "title": "How to Evaluate a Summarizer: Study Design and Statistical Analysis for Manual Linguistic Quality Evaluation",
    "volume": "main",
    "abstract": "Manual evaluation is essential to judge progress on automatic text summarization. However, we conduct a survey on recent summarization system papers that reveals little agreement on how to perform such evaluation studies. We conduct two evaluation experiments on two aspects of summaries' linguistic quality (coherence and repetitiveness) to compare Likert-type and ranking annotations and show that best choice of evaluation method can vary from one aspect to another. In our survey, we also find that study parameters such as the overall number of annotators and distribution of annotators to annotation items are often not fully reported and that subsequent statistical analysis ignores grouping factors arising from one annotator judging multiple summaries. Using our evaluation experiments, we show that the total number of annotators can have a strong impact on study power and that current statistical analysis methods can inflate type I error rates up to eight-fold. In addition, we highlight that for the purpose of system comparison the current practice of eliciting multiple judgements per summary leads to less powerful and reliable annotations given a fixed study budget",
    "checked": true,
    "id": "665172ac8c48a42e4c12405c38400f70c7cabdf4",
    "semantic_title": "how to evaluate a summarizer: study design and statistical analysis for manual linguistic quality evaluation",
    "citation_count": 9,
    "authors": [
      "Julius Steen",
      "Katja Markert"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.161": {
    "title": "Open-Mindedness and Style Coordination in Argumentative Discussions",
    "volume": "main",
    "abstract": "Linguistic accommodation is the process in which speakers adjust their accent, diction, vocabulary, and other aspects of language according to the communication style of one another. Previous research has shown how linguistic accommodation correlates with gaps in the power and status of the speakers and the way it promotes approval and discussion efficiency. In this work, we provide a novel perspective on the phenomena, exploring its correlation with the open-mindedness of a speaker, rather than to her social status. We process thousands of unstructured argumentative discussions that took place in Reddit's Change My View (CMV) subreddit, demonstrating that open-mindedness relates to the assumed role of a speaker in different contexts. On the discussion level, we surprisingly find that discussions that reach agreement present lower levels of accommodation",
    "checked": true,
    "id": "283af65b9b81d3ce8520a26bcafc6cdae83c5bb4",
    "semantic_title": "open-mindedness and style coordination in argumentative discussions",
    "citation_count": 3,
    "authors": [
      "Aviv Ben-Haim",
      "Oren Tsur"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.162": {
    "title": "Error Analysis and the Role of Morphology",
    "volume": "main",
    "abstract": "We evaluate two common conjectures in error analysis of NLP models: (i) Morphology is predictive of errors; and (ii) the importance of morphology increases with the morphological complexity of a language. We show across four different tasks and up to 57 languages that of these conjectures, somewhat surprisingly, only (i) is true. Using morphological features does improve error prediction across tasks; however, this effect is less pronounced with morphologically complex languages. We speculate this is because morphology is more discriminative in morphologically simple languages. Across all four tasks, case and gender are the morphological features most predictive of error",
    "checked": true,
    "id": "8a902a848c3710290f04f2d59030f5670d3433f8",
    "semantic_title": "error analysis and the role of morphology",
    "citation_count": 1,
    "authors": [
      "Marcel Bollmann",
      "Anders Søgaard"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.163": {
    "title": "Applying the Transformer to Character-level Transduction",
    "volume": "main",
    "abstract": "The transformer has been shown to outperform recurrent neural network-based sequence-to-sequence models in various word-level NLP tasks. Yet for character-level transduction tasks, e.g. morphological inflection generation and historical text normalization, there are few works that outperform recurrent models using the transformer. In an empirical study, we uncover that, in contrast to recurrent sequence-to-sequence models, the batch size plays a crucial role in the performance of the transformer on character-level tasks, and we show that with a large enough batch size, the transformer does indeed outperform recurrent models. We also introduce a simple technique to handle feature-guided character-level transduction that further improves performance. With these insights, we achieve state-of-the-art performance on morphological inflection and historical text normalization. We also show that the transformer outperforms a strong baseline on two other character-level transduction tasks: grapheme-to-phoneme conversion and transliteration",
    "checked": true,
    "id": "1da3a9c194a01c0bff7b6ecda79db9d673810bee",
    "semantic_title": "applying the transformer to character-level transduction",
    "citation_count": 92,
    "authors": [
      "Shijie Wu",
      "Ryan Cotterell",
      "Mans Hulden"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.164": {
    "title": "Exploring Supervised and Unsupervised Rewards in Machine Translation",
    "volume": "main",
    "abstract": "Reinforcement Learning (RL) is a powerful framework to address the discrepancy between loss functions used during training and the final evaluation metrics to be used at test time. When applied to neural Machine Translation (MT), it minimises the mismatch between the cross-entropy loss and non-differentiable evaluation metrics like BLEU. However, the suitability of these metrics as reward function at training time is questionable: they tend to be sparse and biased towards the specific words used in the reference texts. We propose to address this problem by making models less reliant on such metrics in two ways: (a) with an entropy-regularised RL method that does not only maximise a reward function but also explore the action space to avoid peaky distributions; (b) with a novel RL method that explores a dynamic unsupervised reward function to balance between exploration and exploitation. We base our proposals on the Soft Actor-Critic (SAC) framework, adapting the off-policy maximum entropy model for language generation applications such as MT. We demonstrate that SAC with BLEU reward tends to overfit less to the training data and performs better on out-of-domain data. We also show that our dynamic unsupervised reward can lead to better translation of ambiguous words",
    "checked": true,
    "id": "a80010909eb6b5cbc3a3aff65093c651c4b37d21",
    "semantic_title": "exploring supervised and unsupervised rewards in machine translation",
    "citation_count": 1,
    "authors": [
      "Julia Ive",
      "Zixu Wang",
      "Marina Fomicheva",
      "Lucia Specia"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.165": {
    "title": "Us vs. Them: A Dataset of Populist Attitudes, News Bias and Emotions",
    "volume": "main",
    "abstract": "Computational modelling of political discourse tasks has become an increasingly important area of research in the field of natural language processing. Populist rhetoric has risen across the political sphere in recent years; however, due to its complex nature, computational approaches to it have been scarce. In this paper, we present the new Us vs. Them dataset, consisting of 6861 Reddit comments annotated for populist attitudes and the first large-scale computational models of this phenomenon. We investigate the relationship between populist mindsets and social groups, as well as a range of emotions typically associated with these. We set a baseline for two tasks associated with populist attitudes and present a set of multi-task learning models that leverage and demonstrate the importance of emotion and group identification as auxiliary tasks",
    "checked": true,
    "id": "33fa38d80ca900dafe708747f062b75f6e4abd96",
    "semantic_title": "us vs. them: a dataset of populist attitudes, news bias and emotions",
    "citation_count": 17,
    "authors": [
      "Pere-Lluís Huguet Cabot",
      "David Abadi",
      "Agneta Fischer",
      "Ekaterina Shutova"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.166": {
    "title": "Multilingual Entity and Relation Extraction Dataset and Model",
    "volume": "main",
    "abstract": "We present a novel dataset and model for a multilingual setting to approach the task of Joint Entity and Relation Extraction. The SMiLER dataset consists of 1.1 M annotated sentences, representing 36 relations, and 14 languages. To the best of our knowledge, this is currently both the largest and the most comprehensive dataset of this type. We introduce HERBERTa, a pipeline that combines two independent BERT models: one for sequence classification, and the other for entity tagging. The model achieves micro F1 81.49 for English on this dataset, which is close to the current SOTA on CoNLL, SpERT",
    "checked": true,
    "id": "2413ec1e8d9dff7fd67151a88ffdfacfa481580e",
    "semantic_title": "multilingual entity and relation extraction dataset and model",
    "citation_count": 20,
    "authors": [
      "Alessandro Seganti",
      "Klaudia Firląg",
      "Helena Skowronska",
      "Michał Satława",
      "Piotr Andruszkiewicz"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.167": {
    "title": "A New View of Multi-modal Language Analysis: Audio and Video Features as Text \"Styles",
    "volume": "main",
    "abstract": "Imposing the style of one image onto another is called style transfer. For example, the style of a Van Gogh painting might be imposed on a photograph to yield an interesting hybrid. This paper applies the adaptive normalization used for image style transfer to language semantics, i.e., the style is the way the words are said (tone of voice and facial expressions) and these are style-transferred onto the text. The goal is to learn richer representations for multi-modal utterances using style-transferred multi-modal features. The proposed Style-Transfer Transformer (STT) grafts a stepped styled adaptive layer-normalization onto a transformer network, the output from which is used in sentiment analysis and emotion recognition problems. In addition to achieving performance on par with the state-of-the art (but using less than a third of the model parameters), we examine the relative contributions of each mode when used in the downstream applications",
    "checked": true,
    "id": "74f6a8b847d36de8a863e3cb592d5b9f6e9535b5",
    "semantic_title": "a new view of multi-modal language analysis: audio and video features as text \"styles",
    "citation_count": 3,
    "authors": [
      "Zhongkai Sun",
      "Prathusha K Sarma",
      "Yingyu Liang",
      "William Sethares"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.168": {
    "title": "Multilingual and cross-lingual document classification: A meta-learning approach",
    "volume": "main",
    "abstract": "The great majority of languages in the world are considered under-resourced for successful application of deep learning methods. In this work, we propose a meta-learning approach to document classification in low-resource languages and demonstrate its effectiveness in two different settings: few-shot, cross-lingual adaptation to previously unseen languages; and multilingual joint-training when limited target-language data is available during trai-ing. We conduct a systematic comparison of several meta-learning methods, investigate multiple settings in terms of data availability, and show that meta-learning thrives in settings with a heterogeneous task distribution. We propose a simple, yet effective adjustment to existing meta-learning methods which allows for better and more stable learning, and set a new state-of-the-art on a number of languages while performing on-par on others, using only a small amount of labeled data",
    "checked": true,
    "id": "35497120715307ac5c4e65fac4874fa334490aa2",
    "semantic_title": "multilingual and cross-lingual document classification: a meta-learning approach",
    "citation_count": 20,
    "authors": [
      "Niels van der Heijden",
      "Helen Yannakoudakis",
      "Pushkar Mishra",
      "Ekaterina Shutova"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.169": {
    "title": "Boosting Low-Resource Biomedical QA via Entity-Aware Masking Strategies",
    "volume": "main",
    "abstract": "Biomedical question-answering (QA) has gained increased attention for its capability to provide users with high-quality information from a vast scientific literature. Although an increasing number of biomedical QA datasets has been recently made available, those resources are still rather limited and expensive to produce; thus, transfer learning via pre-trained language models (LMs) has been shown as a promising approach to leverage existing general-purpose knowledge. However, fine-tuning these large models can be costly and time consuming and often yields limited benefits when adapting to specific themes of specialised domains, such as the COVID-19 literature. Therefore, to bootstrap further their domain adaptation, we propose a simple yet unexplored approach, which we call biomedical entity-aware masking (BEM) strategy, encouraging masked language models to learn entity-centric knowledge based on the pivotal entities characterizing the domain at hand, and employ those entities to drive the LM fine-tuning. The resulting strategy is a downstream process applicable to a wide variety of masked LMs, not requiring additional memory or components in the neural architectures. Experimental results show performance on par with the state-of-the-art models on several biomedical QA datasets",
    "checked": true,
    "id": "04eb972bda6e775d5a15ee49541afca3e3cc93db",
    "semantic_title": "boosting low-resource biomedical qa via entity-aware masking strategies",
    "citation_count": 27,
    "authors": [
      "Gabriele Pergola",
      "Elena Kochkina",
      "Lin Gui",
      "Maria Liakata",
      "Yulan He"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.170": {
    "title": "Attention-based Relational Graph Convolutional Network for Target-Oriented Opinion Words Extraction",
    "volume": "main",
    "abstract": "Target-oriented opinion words extraction (TOWE) is a subtask of aspect-based sentiment analysis (ABSA). It aims to extract the corresponding opinion words for a given opinion target in a review sentence. Intuitively, the relation between an opinion target and an opinion word mostly relies on syntactics. In this study, we design a directed syntactic dependency graph based on a dependency tree to establish a path from the target to candidate opinions. Subsequently, we propose a novel attention-based relational graph convolutional neural network (ARGCN) to exploit syntactic information over dependency graphs. Moreover, to explicitly extract the corresponding opinion words toward the given opinion target, we effectively encode target information in our model with the target-aware representation. Empirical results demonstrate that our model significantly outperforms all of the existing models on four benchmark datasets. Extensive analysis also demonstrates the effectiveness of each component of our models. Our code is available at https://github.com/wcwowwwww/towe-eacl",
    "checked": true,
    "id": "ad536b9dcc412cfc2bed5a969712035fc27ff400",
    "semantic_title": "attention-based relational graph convolutional network for target-oriented opinion words extraction",
    "citation_count": 21,
    "authors": [
      "Junfeng Jiang",
      "An Wang",
      "Akiko Aizawa"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.171": {
    "title": "Laughing at you or with you\": The Role of Sarcasm in Shaping the Disagreement Space",
    "volume": "main",
    "abstract": "Detecting arguments in online interactions is useful to understand how conflicts arise and get resolved. Users often use figurative language, such as sarcasm, either as persuasive devices or to attack the opponent by an ad hominem argument. To further our understanding of the role of sarcasm in shaping the disagreement space, we present a thorough experimental setup using a corpus annotated with both argumentative moves (agree/disagree) and sarcasm. We exploit joint modeling in terms of (a) applying discrete features that are useful in detecting sarcasm to the task of argumentative relation classification (agree/disagree/none), and (b) multitask learning for argumentative relation classification and sarcasm detection using deep learning architectures (e.g., dual Long Short-Term Memory (LSTM) with hierarchical attention and Transformer-based architectures). We demonstrate that modeling sarcasm improves the argumentative relation classification task (agree/disagree/none) in all setups",
    "checked": true,
    "id": "9c554d71d27f4b35c237838101879478528bab77",
    "semantic_title": "laughing at you or with you\": the role of sarcasm in shaping the disagreement space",
    "citation_count": 10,
    "authors": [
      "Debanjan Ghosh",
      "Ritvik Shrivastava",
      "Smaranda Muresan"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.172": {
    "title": "Learning Relatedness between Types with Prototypes for Relation Extraction",
    "volume": "main",
    "abstract": "Relation schemas are often pre-defined for each relation dataset. Relation types can be related from different datasets and have overlapping semantics. We hypothesize we can combine these datasets according to the semantic relatedness between the relation types to overcome the problem of lack of training data. It is often easy to discover the connection between relation types based on relation names or annotation guides, but hard to measure the exact similarity and take advantage of the connection between the relation types from different datasets. We propose to use prototypical examples to represent each relation type and use these examples to augment related types from a different dataset. We obtain further improvement (ACE05) with this type augmentation over a strong baseline which uses multi-task learning between datasets to obtain better feature representation for relations. We make our implementation publicly available: https://github.com/fufrank5/relatedness",
    "checked": true,
    "id": "73b4a1d7c21a8c459f6396d3929cf7cd47ff5a90",
    "semantic_title": "learning relatedness between types with prototypes for relation extraction",
    "citation_count": 3,
    "authors": [
      "Lisheng Fu",
      "Ralph Grishman"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.173": {
    "title": "I Beg to Differ: A study of constructive disagreement in online conversations",
    "volume": "main",
    "abstract": "Disagreements are pervasive in human communication. In this paper we investigate what makes disagreement constructive. To this end, we construct WikiDisputes, a corpus of 7425 Wikipedia Talk page conversations that contain content disputes, and define the task of predicting whether disagreements will be escalated to mediation by a moderator. We evaluate feature-based models with linguistic markers from previous work, and demonstrate that their performance is improved by using features that capture changes in linguistic markers throughout the conversations, as opposed to averaged values. We develop a variety of neural models and show that taking into account the structure of the conversation improves predictive accuracy, exceeding that of feature-based models. We assess our best neural model in terms of both predictive accuracy and uncertainty by evaluating its behaviour when it is only exposed to the beginning of the conversation, finding that model accuracy improves and uncertainty reduces as models are exposed to more information",
    "checked": true,
    "id": "f20ae7cc82be4a8ba23eb4ed8e4b1bda04d93cce",
    "semantic_title": "i beg to differ: a study of constructive disagreement in online conversations",
    "citation_count": 19,
    "authors": [
      "Christine De Kock",
      "Andreas Vlachos"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.174": {
    "title": "Acquiring a Formality-Informed Lexical Resource for Style Analysis",
    "volume": "main",
    "abstract": "To track different levels of formality in written discourse, we introduce a novel type of lexicon for the German language, with entries ordered by their degree of (in)formality. We start with a set of words extracted from traditional lexicographic resources, extend it by sentence-based similarity computations, and let crowdworkers assess the enlarged set of lexical items on a continuous informal-formal scale as a gold standard for evaluation. We submit this lexicon to an intrinsic evaluation related to the best regression models and their effect on predicting formality scores and complement our investigation by an extrinsic evaluation of formality on a German-language email corpus",
    "checked": true,
    "id": "dfaf5a43f77bc25ad54f6f9deb1bff351e42711e",
    "semantic_title": "acquiring a formality-informed lexical resource for style analysis",
    "citation_count": 3,
    "authors": [
      "Elisabeth Eder",
      "Ulrike Krieg-Holz",
      "Udo Hahn"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.175": {
    "title": "Probing into the Root: A Dataset for Reason Extraction of Structural Events from Financial Documents",
    "volume": "main",
    "abstract": "This paper proposes a new task regarding event reason extraction from document-level texts. Unlike the previous causality detection task, we do not assign target events in the text, but only provide structural event descriptions, and such settings accord more with practice scenarios. Moreover, we annotate a large dataset FinReason for evaluation, which provides Reasons annotation for Financial events in company announcements. This task is challenging because the cases of multiple-events, multiple-reasons, and implicit-reasons are included. In total, FinReason contains 8,794 documents, 12,861 financial events and 11,006 reason spans. We also provide the performance of existing canonical methods in event extraction and machine reading comprehension on this task. The results show a 7 percentage point F1 score gap between the best model and human performance, and existing methods are far from resolving this problem",
    "checked": true,
    "id": "afea1ce68c522f5d23c672e786c185d169cac00f",
    "semantic_title": "probing into the root: a dataset for reason extraction of structural events from financial documents",
    "citation_count": 3,
    "authors": [
      "Pei Chen",
      "Kang Liu",
      "Yubo Chen",
      "Taifeng Wang",
      "Jun Zhao"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.176": {
    "title": "Language Modelling as a Multi-Task Problem",
    "volume": "main",
    "abstract": "In this paper, we propose to study language modelling as a multi-task problem, bringing together three strands of research: multi-task learning, linguistics, and interpretability. Based on hypotheses derived from linguistic theory, we investigate whether language models adhere to learning principles of multi-task learning during training. To showcase the idea, we analyse the generalisation behaviour of language models as they learn the linguistic concept of Negative Polarity Items (NPIs). Our experiments demonstrate that a multi-task setting naturally emerges within the objective of the more general task of language modelling. We argue that this insight is valuable for multi-task learning, linguistics and interpretability research and can lead to exciting new findings in all three domains",
    "checked": true,
    "id": "80b0ee8b3f738e535dcf8b8c1223be5f8e3c25ba",
    "semantic_title": "language modelling as a multi-task problem",
    "citation_count": 13,
    "authors": [
      "Lucas Weber",
      "Jaap Jumelet",
      "Elia Bruni",
      "Dieuwke Hupkes"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.177": {
    "title": "ChainCQG: Flow-Aware Conversational Question Generation",
    "volume": "main",
    "abstract": "Conversational systems enable numerous valuable applications, and question-answering is an important component underlying many of these. However, conversational question-answering remains challenging due to the lack of realistic, domain-specific training data. Inspired by this bottleneck, we focus on conversational question generation as a means to generate synthetic conversations for training and evaluation purposes. We present a number of novel strategies to improve conversational flow and accommodate varying question types and overall fluidity. Specifically, we design ChainCQG as a two-stage architecture that learns question-answer representations across multiple dialogue turns using a flow propagation training strategy. ChainCQG significantly outperforms both answer-aware and answer-unaware SOTA baselines (e.g., up to 48% BLEU-1 improvement). Additionally, our model is able to generate different types of questions, with improved fluidity and coreference alignment",
    "checked": true,
    "id": "39bee6f5a509cfa32f89fb2779dbd5c635c02a62",
    "semantic_title": "chaincqg: flow-aware conversational question generation",
    "citation_count": 32,
    "authors": [
      "Jing Gu",
      "Mostafa Mirshekari",
      "Zhou Yu",
      "Aaron Sisto"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.178": {
    "title": "The Interplay of Task Success and Dialogue Quality: An in-depth Evaluation in Task-Oriented Visual Dialogues",
    "volume": "main",
    "abstract": "When training a model on referential dialogue guessing games, the best model is usually chosen based on its task success. We show that in the popular end-to-end approach, this choice prevents the model from learning to generate linguistically richer dialogues, since the acquisition of language proficiency takes longer than learning the guessing task. By comparing models playing different games (GuessWhat, GuessWhich, and Mutual Friends), we show that this discrepancy is model- and task-agnostic. We investigate whether and when better language quality could lead to higher task success. We show that in GuessWhat, models could increase their accuracy if they learn to ground, encode, and decode also words that do not occur frequently in the training set",
    "checked": true,
    "id": "35b0e3dd3245bfcfad37270a543768811c339193",
    "semantic_title": "the interplay of task success and dialogue quality: an in-depth evaluation in task-oriented visual dialogues",
    "citation_count": 4,
    "authors": [
      "Alberto Testoni",
      "Raffaella Bernardi"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.179": {
    "title": "Are you kidding me?\": Detecting Unpalatable Questions on Reddit",
    "volume": "main",
    "abstract": "Abusive language in online discourse negatively affects a large number of social media users. Many computational methods have been proposed to address this issue of online abuse. The existing work, however, tends to focus on detecting the more explicit forms of abuse leaving the subtler forms of abuse largely untouched. Our work addresses this gap by making three core contributions. First, inspired by the theory of impoliteness, we propose a novel task of detecting a subtler form of abuse, namely unpalatable questions. Second, we publish a context-aware dataset for the task using data from a diverse set of Reddit communities. Third, we implement a wide array of learning models and also investigate the benefits of incorporating conversational context into computational models. Our results show that modeling subtle abuse is feasible but difficult due to the language involved being highly nuanced and context-sensitive. We hope that future research in the field will address such subtle forms of abuse since their harm currently passes unnoticed through existing detection systems",
    "checked": true,
    "id": "62c7340ca362ae7e5cfe8258b42641ca292b7670",
    "semantic_title": "are you kidding me?\": detecting unpalatable questions on reddit",
    "citation_count": 5,
    "authors": [
      "Sunyam Bagga",
      "Andrew Piper",
      "Derek Ruths"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.180": {
    "title": "Neural-Driven Search-Based Paraphrase Generation",
    "volume": "main",
    "abstract": "We study a search-based paraphrase generation scheme where candidate paraphrases are generated by iterated transformations from the original sentence and evaluated in terms of syntax quality, semantic distance, and lexical distance. The semantic distance is derived from BERT, and the lexical quality is based on GPT2 perplexity. To solve this multi-objective search problem, we propose two algorithms: Monte-Carlo Tree Search For Paraphrase Generation (MCPG) and Pareto Tree Search (PTS). We provide an extensive set of experiments on 5 datasets with a rigorous reproduction and validation for several state-of-the-art paraphrase generation algorithms. These experiments show that, although being non explicitly supervised, our algorithms perform well against these baselines",
    "checked": true,
    "id": "79470c465916cd253bd9e58fe1248513d2885f13",
    "semantic_title": "neural-driven search-based paraphrase generation",
    "citation_count": 5,
    "authors": [
      "Betty Fabre",
      "Tanguy Urvoy",
      "Jonathan Chevelu",
      "Damien Lolive"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.181": {
    "title": "Word Alignment by Fine-tuning Embeddings on Parallel Corpora",
    "volume": "main",
    "abstract": "Word alignment over parallel corpora has a wide variety of applications, including learning translation lexicons, cross-lingual transfer of language processing tools, and automatic evaluation or analysis of translation outputs. The great majority of past work on word alignment has worked by performing unsupervised learning on parallel text. Recently, however, other work has demonstrated that pre-trained contextualized word embeddings derived from multilingually trained language models (LMs) prove an attractive alternative, achieving competitive results on the word alignment task even in the absence of explicit training on parallel data. In this paper, we examine methods to marry the two approaches: leveraging pre-trained LMs but fine-tuning them on parallel text with objectives designed to improve alignment quality, and proposing methods to effectively extract alignments from these fine-tuned models. We perform experiments on five language pairs and demonstrate that our model can consistently outperform previous state-of-the-art models of all varieties. In addition, we demonstrate that we are able to train multilingual word aligners that can obtain robust performance on different language pairs",
    "checked": true,
    "id": "821532ecef5bc2252823b190c35f1e4c44ddc41c",
    "semantic_title": "word alignment by fine-tuning embeddings on parallel corpora",
    "citation_count": 221,
    "authors": [
      "Zi-Yi Dou",
      "Graham Neubig"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.182": {
    "title": "Paraphrases do not explain word analogies",
    "volume": "main",
    "abstract": "Many types of distributional word embeddings (weakly) encode linguistic regularities as directions (the difference between jump and jumped will be in a similar direction to that of walk and walked, and so on). Several attempts have been made to explain this fact. We respond to Allen and Hospedales' recent (ICML, 2019) theoretical explanation, which claims that word2vec and GloVe will encode linguistic regularities whenever a specific relation of paraphrase holds between the four words involved in the regularity. We demonstrate that the explanation does not go through: the paraphrase relations needed under this explanation do not hold empirically",
    "checked": true,
    "id": "4c032bedffecaa05689d033101cf39cc6f338c5d",
    "semantic_title": "paraphrases do not explain word analogies",
    "citation_count": 4,
    "authors": [
      "Louis Fournier",
      "Ewan Dunbar"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.183": {
    "title": "An Empirical Study on the Generalization Power of Neural Representations Learned via Visual Guessing Games",
    "volume": "main",
    "abstract": "Guessing games are a prototypical instance of the \"learning by interacting\" paradigm. This work investigates how well an artificial agent can benefit from playing guessing games when later asked to perform on novel NLP downstream tasks such as Visual Question Answering (VQA). We propose two ways to exploit playing guessing games: 1) a supervised learning scenario in which the agent learns to mimic successful guessing games and 2) a novel way for an agent to play by itself, called Self-play via Iterated Experience Learning (SPIEL). We evaluate the ability of both procedures to generalise: an in-domain evaluation shows an increased accuracy (+7.79) compared with competitors on the evaluation suite CompGuessWhat?!; a transfer evaluation shows improved performance for VQA on the TDIUC dataset in terms of harmonic average accuracy (+5.31) thanks to more fine-grained object representations learned via SPIEL",
    "checked": true,
    "id": "a67face220a88b6b36f3343a6a017a3536562d5b",
    "semantic_title": "an empirical study on the generalization power of neural representations learned via visual guessing games",
    "citation_count": 8,
    "authors": [
      "Alessandro Suglia",
      "Yonatan Bisk",
      "Ioannis Konstas",
      "Antonio Vergari",
      "Emanuele Bastianelli",
      "Andrea Vanzo",
      "Oliver Lemon"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.184": {
    "title": "A Unified Feature Representation for Lexical Connotations",
    "volume": "main",
    "abstract": "Ideological attitudes and stance are often expressed through subtle meanings of words and phrases. Understanding these connotations is critical to recognizing the cultural and emotional perspectives of the speaker. In this paper, we use distant labeling to create a new lexical resource representing connotation aspects for nouns and adjectives. Our analysis shows that it aligns well with human judgments. Additionally, we present a method for creating lexical representations that capture connotations within the embedding space and show that using the embeddings provides a statistically significant improvement on the task of stance detection when data is limited",
    "checked": true,
    "id": "10bdc0a805f1104d3f7b1d22607c46e0f0be04a2",
    "semantic_title": "a unified feature representation for lexical connotations",
    "citation_count": 6,
    "authors": [
      "Emily Allaway",
      "Kathleen McKeown"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.185": {
    "title": "FAST: Financial News and Tweet Based Time Aware Network for Stock Trading",
    "volume": "main",
    "abstract": "Designing profitable trading strategies is complex as stock movements are highly stochastic; the market is influenced by large volumes of noisy data across diverse information sources like news and social media. Prior work mostly treats stock movement prediction as a regression or classification task and is not directly optimized towards profit-making. Further, they do not model the fine-grain temporal irregularities in the release of vast volumes of text that the market responds to quickly. Building on these limitations, we propose a novel hierarchical, learning to rank approach that uses textual data to make time-aware predictions for ranking stocks based on expected profit. Our approach outperforms state-of-the-art methods by over 8% in terms of cumulative profit and risk-adjusted returns in trading simulations on two benchmarks: English tweets and Chinese financial news spanning two major stock indexes and four global markets. Through ablative and qualitative analyses, we build the case for our method as a tool for daily stock trading",
    "checked": true,
    "id": "9302b13c194a955371c56b977088859c5cc3aeef",
    "semantic_title": "fast: financial news and tweet based time aware network for stock trading",
    "citation_count": 34,
    "authors": [
      "Ramit Sawhney",
      "Arnav Wadhwa",
      "Shivam Agarwal",
      "Rajiv Ratn Shah"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.186": {
    "title": "Building Representative Corpora from Illiterate Communities: A Reviewof Challenges and Mitigation Strategies for Developing Countries",
    "volume": "main",
    "abstract": "Most well-established data collection methods currently adopted in NLP depend on the as- sumption of speaker literacy. Consequently, the collected corpora largely fail to represent swathes of the global population, which tend to be some of the most vulnerable and marginalised people in society, and often live in rural developing areas. Such underrepresented groups are thus not only ignored when making modeling and system design decisions, but also prevented from benefiting from development outcomes achieved through data-driven NLP. This paper aims to address the under-representation of illiterate communities in NLP corpora: we identify potential biases and ethical issues that might arise when collecting data from rural communities with high illiteracy rates in Low-Income Countries, and propose a set of practical mitigation strategies to help future work",
    "checked": true,
    "id": "16b5cd1d84ad0517ec94a75566e27d17b9fc70b7",
    "semantic_title": "building representative corpora from illiterate communities: a reviewof challenges and mitigation strategies for developing countries",
    "citation_count": 9,
    "authors": [
      "Stephanie Hirmer",
      "Alycia Leonard",
      "Josephine Tumwesige",
      "Costanza Conforti"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.187": {
    "title": "Process-Level Representation of Scientific Protocols with Interactive Annotation",
    "volume": "main",
    "abstract": "We develop Process Execution Graphs (PEG), a document-level representation of real-world wet lab biochemistry protocols, addressing challenges such as cross-sentence relations, long-range coreference, grounding, and implicit arguments. We manually annotate PEGs in a corpus of complex lab protocols with a novel interactive textual simulator that keeps track of entity traits and semantic constraints during annotation. We use this data to develop graph-prediction models, finding them to be good at entity identification and local relation extraction, while our corpus facilitates further exploration of challenging long-range relations",
    "checked": true,
    "id": "37a4c6a416e5200e9f6257d9711e54e69f4be754",
    "semantic_title": "process-level representation of scientific protocols with interactive annotation",
    "citation_count": 18,
    "authors": [
      "Ronen Tamari",
      "Fan Bai",
      "Alan Ritter",
      "Gabriel Stanovsky"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.188": {
    "title": "Machine Translationese: Effects of Algorithmic Bias on Linguistic Complexity in Machine Translation",
    "volume": "main",
    "abstract": "Recent studies in the field of Machine Translation (MT) and Natural Language Processing (NLP) have shown that existing models amplify biases observed in the training data. The amplification of biases in language technology has mainly been examined with respect to specific phenomena, such as gender bias. In this work, we go beyond the study of gender in MT and investigate how bias amplification might affect language in a broader sense. We hypothesize that the ‘algorithmic bias', i.e. an exacerbation of frequently observed patterns in combination with a loss of less frequent ones, not only exacerbates societal biases present in current datasets but could also lead to an artificially impoverished language: ‘machine translationese'. We assess the linguistic richness (on a lexical and morphological level) of translations created by different data-driven MT paradigms – phrase-based statistical (PB-SMT) and neural MT (NMT). Our experiments show that there is a loss of lexical and syntactic richness in the translations produced by all investigated MT paradigms for two language pairs (EN-FR and EN-ES)",
    "checked": true,
    "id": "f28208f6847012f986c4ce63b34236284e65407c",
    "semantic_title": "machine translationese: effects of algorithmic bias on linguistic complexity in machine translation",
    "citation_count": 65,
    "authors": [
      "Eva Vanmassenhove",
      "Dimitar Shterionov",
      "Matthew Gwilliam"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.189": {
    "title": "First Align, then Predict: Understanding the Cross-Lingual Ability of Multilingual BERT",
    "volume": "main",
    "abstract": "Multilingual pretrained language models have demonstrated remarkable zero-shot cross-lingual transfer capabilities. Such transfer emerges by fine-tuning on a task of interest in one language and evaluating on a distinct language, not seen during the fine-tuning. Despite promising results, we still lack a proper understanding of the source of this transfer. Using a novel layer ablation technique and analyses of the model's internal representations, we show that multilingual BERT, a popular multilingual language model, can be viewed as the stacking of two sub-networks: a multilingual encoder followed by a task-specific language-agnostic predictor. While the encoder is crucial for cross-lingual transfer and remains mostly unchanged during fine-tuning, the task predictor has little importance on the transfer and can be reinitialized during fine-tuning. We present extensive experiments with three distinct tasks, seventeen typologically diverse languages and multiple domains to support our hypothesis",
    "checked": true,
    "id": "1cfd0f08bb7ab449bbccc3d69a62ffbf43a7eb7f",
    "semantic_title": "first align, then predict: understanding the cross-lingual ability of multilingual bert",
    "citation_count": 59,
    "authors": [
      "Benjamin Muller",
      "Yanai Elazar",
      "Benoît Sagot",
      "Djamé Seddah"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.190": {
    "title": "Stereotype and Skew: Quantifying Gender Bias in Pre-trained and Fine-tuned Language Models",
    "volume": "main",
    "abstract": "This paper proposes two intuitive metrics, skew and stereotype, that quantify and analyse the gender bias present in contextual language models when tackling the WinoBias pronoun resolution task. We find evidence that gender stereotype correlates approximately negatively with gender skew in out-of-the-box models, suggesting that there is a trade-off between these two forms of bias. We investigate two methods to mitigate bias. The first approach is an online method which is effective at removing skew at the expense of stereotype. The second, inspired by previous work on ELMo, involves the fine-tuning of BERT using an augmented gender-balanced dataset. We show that this reduces both skew and stereotype relative to its unaugmented fine-tuned counterpart. However, we find that existing gender bias benchmarks do not fully probe professional bias as pronoun resolution may be obfuscated by cross-correlations from other manifestations of gender prejudice",
    "checked": true,
    "id": "8fa0de4920c8edcb1fea698ff3463a347771d889",
    "semantic_title": "stereotype and skew: quantifying gender bias in pre-trained and fine-tuned language models",
    "citation_count": 66,
    "authors": [
      "Daniel de Vassimon Manela",
      "David Errington",
      "Thomas Fisher",
      "Boris van Breugel",
      "Pasquale Minervini"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.191": {
    "title": "On the evolution of syntactic information encoded by BERT's contextualized representations",
    "volume": "main",
    "abstract": "The adaptation of pretrained language models to solve supervised tasks has become a baseline in NLP, and many recent works have focused on studying how linguistic information is encoded in the pretrained sentence representations. Among other information, it has been shown that entire syntax trees are implicitly embedded in the geometry of such models. As these models are often fine-tuned, it becomes increasingly important to understand how the encoded knowledge evolves along the fine-tuning. In this paper, we analyze the evolution of the embedded syntax trees along the fine-tuning process of BERT for six different tasks, covering all levels of the linguistic structure. Experimental results show that the encoded syntactic information is forgotten (PoS tagging), reinforced (dependency and constituency parsing) or preserved (semantics-related tasks) in different ways along the fine-tuning process depending on the task",
    "checked": true,
    "id": "02b845539f91e3ca526a471285076a200b6472be",
    "semantic_title": "on the evolution of syntactic information encoded by bert's contextualized representations",
    "citation_count": 6,
    "authors": [
      "Laura Pérez-Mayos",
      "Roberto Carlini",
      "Miguel Ballesteros",
      "Leo Wanner"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.192": {
    "title": "Identify, Align, and Integrate: Matching Knowledge Graphs to Commonsense Reasoning Tasks",
    "volume": "main",
    "abstract": "Integrating external knowledge into commonsense reasoning tasks has shown progress in resolving some, but not all, knowledge gaps in these tasks. For knowledge integration to yield peak performance, it is critical to select a knowledge graph (KG) that is well-aligned with the given task's objective. We present an approach to assess how well a candidate KG can correctly identify and accurately fill in gaps of reasoning for a task, which we call KG-to-task match. We show this KG-to-task match in 3 phases: knowledge-task identification, knowledge-task alignment, and knowledge-task integration. We also analyze our transformer-based KG-to-task models via commonsense probes to measure how much knowledge is captured in these models before and after KG integration. Empirically, we investigate KG matches for the SocialIQA (SIQA) (Sap et al., 2019b), Physical IQA (PIQA) (Bisk et al., 2020), and MCScript2.0 (Ostermann et al., 2019) datasets with 3 diverse KGs: ATOMIC (Sap et al., 2019a), ConceptNet (Speer et al., 2017), and an automatically constructed instructional KG based on WikiHow (Koupaee and Wang, 2018). With our methods we are able to demonstrate that ATOMIC, an event-inference focused KG, is the best match for SIQA and MCScript2.0, and that the taxonomic ConceptNet and WikiHow-based KGs are the best match for PIQA across all 3 analysis phases. We verify our methods and findings with human evaluation",
    "checked": true,
    "id": "d3a1e7f060bfe7d97c2e430da9eb6967bbbe358a",
    "semantic_title": "identify, align, and integrate: matching knowledge graphs to commonsense reasoning tasks",
    "citation_count": 16,
    "authors": [
      "Lisa Bauer",
      "Mohit Bansal"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.193": {
    "title": "Calculating the optimal step of arc-eager parsing for non-projective trees",
    "volume": "main",
    "abstract": "It is shown that the optimal next step of an arc-eager parser relative to a non-projective dependency structure can be calculated in cubic time, solving an open problem in parsing theory. Applications are in training of parsers by means of a ‘dynamic oracle'",
    "checked": true,
    "id": "2080b8b7cbb5ad948080d63e4253c896c15bdab4",
    "semantic_title": "calculating the optimal step of arc-eager parsing for non-projective trees",
    "citation_count": 1,
    "authors": [
      "Mark-Jan Nederhof"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.194": {
    "title": "Subword Pooling Makes a Difference",
    "volume": "main",
    "abstract": "Contextual word-representations became a standard in modern natural language processing systems. These models use subword tokenization to handle large vocabularies and unknown words. Word-level usage of such systems requires a way of pooling multiple subwords that correspond to a single word. In this paper we investigate how the choice of subword pooling affects the downstream performance on three tasks: morphological probing, POS tagging and NER, in 9 typologically diverse languages. We compare these in two massively multilingual models, mBERT and XLM-RoBERTa. For morphological tasks, the widely used ‘choose the first subword' is the worst strategy and the best results are obtained by using attention over the subwords. For POS tagging both of these strategies perform poorly and the best choice is to use a small LSTM over the subwords. The same strategy works best for NER and we show that mBERT is better than XLM-RoBERTa in all 9 languages. We publicly release all code, data and the full result tables at https://github.com/juditacs/subword-choice",
    "checked": true,
    "id": "188cd686fb2200f237f688dbda7f64ffc75e67ac",
    "semantic_title": "subword pooling makes a difference",
    "citation_count": 24,
    "authors": [
      "Judit Ács",
      "Ákos Kádár",
      "Andras Kornai"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.195": {
    "title": "Content-based Models of Quotation",
    "volume": "main",
    "abstract": "We explore the task of quotability identification, in which, given a document, we aim to identify which of its passages are the most quotable, i.e. the most likely to be directly quoted by later derived documents. We approach quotability identification as a passage ranking problem and evaluate how well both feature-based and BERT-based (Devlin et al., 2019) models rank the passages in a given document by their predicted quotability. We explore this problem through evaluations on five datasets that span multiple languages (English, Latin) and genres of literature (e.g. poetry, plays, novels) and whose corresponding derived documents are of multiple types (news, journal articles). Our experiments confirm the relatively strong performance of BERT-based models on this task, with the best model, a RoBERTA sequential sentence tagger, achieving an average rho of 0.35 and NDCG@1, 5, 50 of 0.26, 0.31 and 0.40, respectively, across all five datasets",
    "checked": true,
    "id": "6276bbe6cc56234d430725a31a27939eeec88149",
    "semantic_title": "content-based models of quotation",
    "citation_count": 2,
    "authors": [
      "Ansel MacLaughlin",
      "David Smith"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.196": {
    "title": "L2C: Describing Visual Differences Needs Semantic Understanding of Individuals",
    "volume": "main",
    "abstract": "Recent advances in language and vision push forward the research of captioning a single image to describing visual differences between image pairs. Suppose there are two images, I_1 and I_2, and the task is to generate a description W_1,2 comparing them, existing methods directly model I_1, I_2 -> W_1,2 mapping without the semantic understanding of individuals. In this paper, we introduce a Learning-to-Compare (L2C) model, which learns to understand the semantic structures of these two images and compare them while learning to describe each one. We demonstrate that L2C benefits from a comparison between explicit semantic representations and single-image captions, and generalizes better on the new testing image pairs. It outperforms the baseline on both automatic evaluation and human evaluation for the Birds-to-Words dataset",
    "checked": true,
    "id": "3c45ed30b5d9da4d7767a18426ae94b06314db51",
    "semantic_title": "l2c: describing visual differences needs semantic understanding of individuals",
    "citation_count": 10,
    "authors": [
      "An Yan",
      "Xin Wang",
      "Tsu-Jui Fu",
      "William Yang Wang"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.197": {
    "title": "VoiSeR: A New Benchmark for Voice-Based Search Refinement",
    "volume": "main",
    "abstract": "Voice assistants, e.g., Alexa or Google Assistant, have dramatically improved in recent years. Supporting voice-based search, exploration, and refinement are fundamental tasks for voice assistants, and remain an open challenge. For example, when using voice to search an online shopping site, a user often needs to refine their search by some aspect or facet. This common user intent is usually available through a \"filter-by\" interface on online shopping websites, but is challenging to support naturally via voice, as the intent of refinements must be interpreted in the context of the original search, the initial results, and the available product catalogue facets. To our knowledge, no benchmark dataset exists for training or validating such contextual search understanding models. To bridge this gap, we introduce the first large-scale dataset of voice-based search refinements, VoiSeR, consisting of about 10,000 search refinement utterances, collected using a novel crowdsourcing task. These utterances are intended to refine a previous search, with respect to a search facet or attribute (e.g., brand, color, review rating, etc.), and are manually annotated with the specific intent. This paper reports qualitative and empirical insights into the most common and challenging types of refinements that a voice-based conversational search system must support. As we show, VoiSeR can support research in conversational query understanding, contextual user intent prediction, and other conversational search topics to facilitate the development of conversational search systems",
    "checked": true,
    "id": "d32bca1dd41e6af9aa7461b4c0347ea2608388f7",
    "semantic_title": "voiser: a new benchmark for voice-based search refinement",
    "citation_count": 2,
    "authors": [
      "Simone Filice",
      "Giuseppe Castellucci",
      "Marcus Collins",
      "Eugene Agichtein",
      "Oleg Rokhlenko"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.198": {
    "title": "Event-Driven News Stream Clustering using Entity-Aware Contextual Embeddings",
    "volume": "main",
    "abstract": "We propose a method for online news stream clustering that is a variant of the non-parametric streaming K-means algorithm. Our model uses a combination of sparse and dense document representations, aggregates document-cluster similarity along these multiple representations and makes the clustering decision using a neural classifier. The weighted document-cluster similarity model is learned using a novel adaptation of the triplet loss into a linear classification objective. We show that the use of a suitable fine-tuning objective and external knowledge in pre-trained transformer models yields significant improvements in the effectiveness of contextual embeddings for clustering. Our model achieves a new state-of-the-art on a standard stream clustering dataset of English documents",
    "checked": true,
    "id": "c1da7a0759adb5edf5097434896d628bb6bbde4f",
    "semantic_title": "event-driven news stream clustering using entity-aware contextual embeddings",
    "citation_count": 20,
    "authors": [
      "Kailash Karthik Saravanakumar",
      "Miguel Ballesteros",
      "Muthu Kumar Chandrasekaran",
      "Kathleen McKeown"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.199": {
    "title": "Adversarial Learning of Poisson Factorisation Model for Gauging Brand Sentiment in User Reviews",
    "volume": "main",
    "abstract": "In this paper, we propose the Brand-Topic Model (BTM) which aims to detect brand-associated polarity-bearing topics from product reviews. Different from existing models for sentiment-topic extraction which assume topics are grouped under discrete sentiment categories such as ‘positive', ‘negative' and ‘neural', BTM is able to automatically infer real-valued brand-associated sentiment scores and generate fine-grained sentiment-topics in which we can observe continuous changes of words under a certain topic (e.g., ‘shaver' or ‘cream') while its associated sentiment gradually varies from negative to positive. BTM is built on the Poisson factorisation model with the incorporation of adversarial learning. It has been evaluated on a dataset constructed from Amazon reviews. Experimental results show that BTM outperforms a number of competitive baselines in brand ranking, achieving a better balance of topic coherence and unique-ness, and extracting better-separated polarity-bearing topics",
    "checked": true,
    "id": "3434832a02d9e5555605eb8b575eefa5165361b0",
    "semantic_title": "adversarial learning of poisson factorisation model for gauging brand sentiment in user reviews",
    "citation_count": 1,
    "authors": [
      "Runcong Zhao",
      "Lin Gui",
      "Gabriele Pergola",
      "Yulan He"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.200": {
    "title": "Lexical Normalization for Code-switched Data and its Effect on POS Tagging",
    "volume": "main",
    "abstract": "Lexical normalization, the translation of non-canonical data to standard language, has shown to improve the performance of many natural language processing tasks on social media. Yet, using multiple languages in one utterance, also called code-switching (CS), is frequently overlooked by these normalization systems, despite its common use in social media. In this paper, we propose three normalization models specifically designed to handle code-switched data which we evaluate for two language pairs: Indonesian-English and Turkish-German. For the latter, we introduce novel normalization layers and their corresponding language ID and POS tags for the dataset, and evaluate the downstream effect of normalization on POS tagging. Results show that our CS-tailored normalization models significantly outperform monolingual ones, and lead to 5.4% relative performance increase for POS tagging as compared to unnormalized input",
    "checked": true,
    "id": "13cbb4fa75192275dcb4d929c07841ca505c2bda",
    "semantic_title": "lexical normalization for code-switched data and its effect on pos tagging",
    "citation_count": 12,
    "authors": [
      "Rob van der Goot",
      "Özlem Çetinoğlu"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.201": {
    "title": "Structural Encoding and Pre-training Matter: Adapting BERT for Table-Based Fact Verification",
    "volume": "main",
    "abstract": "Growing concern with online misinformation has encouraged NLP research on fact verification. Since writers often base their assertions on structured data, we focus here on verifying textual statements given evidence in tables. Starting from the Table Parsing (TAPAS) model developed for question answering (Herzig et al., 2020), we find that modeling table structure improves a language model pre-trained on unstructured text. Pre-training language models on English Wikipedia table data further improves performance. Pre-training on a question answering task with column-level cell rank information achieves the best performance. With improved pre-training and cell embeddings, this approach outperforms the state-of-the-art Numerically-aware Graph Neural Network table fact verification model (GNN-TabFact), increasing statement classification accuracy from 72.2% to 73.9% even without modeling numerical information. Incorporating numerical information with cell rankings and pre-training on a question-answering task increases accuracy to 76%. We further analyze accuracy on statements implicating single rows or multiple rows and columns of tables, on different numerical reasoning subtasks, and on generalizing to detecting errors in statements derived from the ToTTo table-to-text generation dataset",
    "checked": true,
    "id": "252c116aca5f59a6e357e32b8467e09ded04533d",
    "semantic_title": "structural encoding and pre-training matter: adapting bert for table-based fact verification",
    "citation_count": 9,
    "authors": [
      "Rui Dong",
      "David Smith"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.202": {
    "title": "A Study of Automatic Metrics for the Evaluation of Natural Language Explanations",
    "volume": "main",
    "abstract": "As transparency becomes key for robotics and AI, it will be necessary to evaluate the methods through which transparency is provided, including automatically generated natural language (NL) explanations. Here, we explore parallels between the generation of such explanations and the much-studied field of evaluation of Natural Language Generation (NLG). Specifically, we investigate which of the NLG evaluation measures map well to explanations. We present the ExBAN corpus: a crowd-sourced corpus of NL explanations for Bayesian Networks. We run correlations comparing human subjective ratings with NLG automatic measures. We find that embedding-based automatic NLG evaluation methods, such as BERTScore and BLEURT, have a higher correlation with human ratings, compared to word-overlap metrics, such as BLEU and ROUGE. This work has implications for Explainable AI and transparent robotic and autonomous systems",
    "checked": true,
    "id": "136f161f4c211478553e588f30ed09905c142279",
    "semantic_title": "a study of automatic metrics for the evaluation of natural language explanations",
    "citation_count": 42,
    "authors": [
      "Miruna-Adriana Clinciu",
      "Arash Eshghi",
      "Helen Hastie"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.203": {
    "title": "Adversarial Stylometry in the Wild: Transferable Lexical Substitution Attacks on Author Profiling",
    "volume": "main",
    "abstract": "Written language contains stylistic cues that can be exploited to automatically infer a variety of potentially sensitive author information. Adversarial stylometry intends to attack such models by rewriting an author's text. Our research proposes several components to facilitate deployment of these adversarial attacks in the wild, where neither data nor target models are accessible. We introduce a transformer-based extension of a lexical replacement attack, and show it achieves high transferability when trained on a weakly labeled corpus—decreasing target model performance below chance. While not completely inconspicuous, our more successful attacks also prove notably less detectable by humans. Our framework therefore provides a promising direction for future privacy-preserving adversarial attacks",
    "checked": true,
    "id": "ea2e2d4ae607a35828ec5c51164fac3a557c88b5",
    "semantic_title": "adversarial stylometry in the wild: transferable lexical substitution attacks on author profiling",
    "citation_count": 16,
    "authors": [
      "Chris Emmery",
      "Ákos Kádár",
      "Grzegorz Chrupała"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.204": {
    "title": "Cross-Cultural Similarity Features for Cross-Lingual Transfer Learning of Pragmatically Motivated Tasks",
    "volume": "main",
    "abstract": "Much work in cross-lingual transfer learning explored how to select better transfer languages for multilingual tasks, primarily focusing on typological and genealogical similarities between languages. We hypothesize that these measures of linguistic proximity are not enough when working with pragmatically-motivated tasks, such as sentiment analysis. As an alternative, we introduce three linguistic features that capture cross-cultural similarities that manifest in linguistic patterns and quantify distinct aspects of language pragmatics: language context-level, figurative language, and the lexification of emotion concepts. Our analyses show that the proposed pragmatic features do capture cross-cultural similarities and align well with existing work in sociolinguistics and linguistic anthropology. We further corroborate the effectiveness of pragmatically-driven transfer in the downstream task of choosing transfer languages for cross-lingual sentiment analysis",
    "checked": true,
    "id": "059097ade9658d18846a5c4c95ea4024f74acbf3",
    "semantic_title": "cross-cultural similarity features for cross-lingual transfer learning of pragmatically motivated tasks",
    "citation_count": 20,
    "authors": [
      "Jimin Sun",
      "Hwijeen Ahn",
      "Chan Young Park",
      "Yulia Tsvetkov",
      "David R. Mortensen"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.205": {
    "title": "PHASE: Learning Emotional Phase-aware Representations for Suicide Ideation Detection on Social Media",
    "volume": "main",
    "abstract": "Recent psychological studies indicate that individuals exhibiting suicidal ideation increasingly turn to social media rather than mental health practitioners. Contextualizing the build-up of such ideation is critical for the identification of users at risk. In this work, we focus on identifying suicidal intent in tweets by augmenting linguistic models with emotional phases modeled from users' historical context. We propose PHASE, a time-and phase-aware framework that adaptively learns features from a user's historical emotional spectrum on Twitter for preliminary screening of suicidal risk. Building on clinical studies, PHASE learns phase-like progressions in users' historical Plutchik-wheel-based emotions to contextualize suicidal intent. While outperforming state-of-the-art methods, we show the utility of temporal and phase-based emotional contextual cues for suicide ideation detection. We further discuss practical and ethical considerations",
    "checked": true,
    "id": "0c051ea0ec77c5c27b36dce88b4d4e0109c02e19",
    "semantic_title": "phase: learning emotional phase-aware representations for suicide ideation detection on social media",
    "citation_count": 37,
    "authors": [
      "Ramit Sawhney",
      "Harshit Joshi",
      "Lucie Flek",
      "Rajiv Ratn Shah"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.206": {
    "title": "Exploiting Definitions for Frame Identification",
    "volume": "main",
    "abstract": "Frame identification is one of the key challenges for frame-semantic parsing. The goal of this task is to determine which frame best captures the meaning of a target word or phrase in a sentence. We present a new model for frame identification that uses a pre-trained transformer model to generate representations for frames and lexical units (senses) using their formal definitions in FrameNet. Our frame identification model assesses the suitability of a frame for a target word in a sentence based on the semantic coherence of their meanings. We evaluate our model on three data sets and show that it consistently achieves better performance than previous systems",
    "checked": true,
    "id": "873c52eb0a42f222c43f9c7758130987b1e48180",
    "semantic_title": "exploiting definitions for frame identification",
    "citation_count": 15,
    "authors": [
      "Tianyu Jiang",
      "Ellen Riloff"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.207": {
    "title": "ADePT: Auto-encoder based Differentially Private Text Transformation",
    "volume": "main",
    "abstract": "Privacy is an important concern when building statistical models on data containing personal information. Differential privacy offers a strong definition of privacy and can be used to solve several privacy concerns. Multiple solutions have been proposed for the differentially-private transformation of datasets containing sensitive information. However, such transformation algorithms offer poor utility in Natural Language Processing (NLP) tasks due to noise added in the process. This paper addresses this issue by providing a utility-preserving differentially private text transformation algorithm using auto-encoders. Our algorithm transforms text to offer robustness against attacks and produces transformations with high semantic quality that perform well on downstream NLP tasks. We prove our algorithm's theoretical privacy guarantee and assess its privacy leakage under Membership Inference Attacks (MIA) on models trained with transformed data. Our results show that the proposed model performs better against MIA attacks while offering lower to no degradation in the utility of the underlying transformation process compared to existing baselines",
    "checked": true,
    "id": "37209cdb837e5c2512721a6a838e9a94a90a4aca",
    "semantic_title": "adept: auto-encoder based differentially private text transformation",
    "citation_count": 27,
    "authors": [
      "Satyapriya Krishna",
      "Rahul Gupta",
      "Christophe Dupuy"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.208": {
    "title": "Conceptual Grounding Constraints for Truly Robust Biomedical Name Representations",
    "volume": "main",
    "abstract": "Effective representation of biomedical names for downstream NLP tasks requires the encoding of both lexical as well as domain-specific semantic information. Ideally, the synonymy and semantic relatedness of names should be consistently reflected by their closeness in an embedding space. To achieve such robustness, prior research has considered multi-task objectives when training neural encoders. In this paper, we take a next step towards truly robust representations, which capture more domain-specific semantics while remaining universally applicable across different biomedical corpora and domains. To this end, we use conceptual grounding constraints which more effectively align encoded names to pretrained embeddings of their concept identifiers. These constraints are effective even when using a Deep Averaging Network, a simple feedforward encoding architecture that allows for scaling to large corpora while remaining sufficiently expressive. We empirically validate our approach using multiple tasks and benchmarks, which assess both literal synonymy as well as more general semantic relatedness",
    "checked": true,
    "id": "b8e60fcd138780e5e5a098cbe013b101cb0d1579",
    "semantic_title": "conceptual grounding constraints for truly robust biomedical name representations",
    "citation_count": 4,
    "authors": [
      "Pieter Fivez",
      "Simon Suster",
      "Walter Daelemans"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.209": {
    "title": "Adaptive Mixed Component LDA for Low Resource Topic Modeling",
    "volume": "main",
    "abstract": "Probabilistic topic models in low data resource scenarios are faced with less reliable estimates due to sparsity of discrete word co-occurrence counts, and do not have the luxury of retraining word or topic embeddings using neural methods. In this challenging resource constrained setting, we explore mixture models which interpolate between the discrete and continuous topic-word distributions that utilise pre-trained embeddings to improve topic coherence. We introduce an automatic trade-off between the discrete and continuous representations via an adaptive mixture coefficient, which places greater weight on the discrete representation when the corpus statistics are more reliable. The adaptive mixture coefficient takes into account global corpus statistics, and the uncertainty in each topic's continuous distributions. Our approach outperforms the fully discrete, fully continuous, and static mixture model on topic coherence in low resource settings. We additionally demonstrate the generalisability of our method by extending it to handle multilingual document collections",
    "checked": true,
    "id": "13036fad23e65d4c8f50bd3bd4a5e2f6cfbd6e4e",
    "semantic_title": "adaptive mixed component lda for low resource topic modeling",
    "citation_count": 1,
    "authors": [
      "Suzanna Sia",
      "Kevin Duh"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.210": {
    "title": "Evaluating Neural Model Robustness for Machine Comprehension",
    "volume": "main",
    "abstract": "We evaluate neural model robustness to adversarial attacks using different types of linguistic unit perturbations – character and word, and propose a new method for strategic sentence-level perturbations. We experiment with different amounts of perturbations to examine model confidence and misclassification rate, and contrast model performance with different embeddings BERT and ELMo on two benchmark datasets SQuAD and TriviaQA. We demonstrate how to improve model performance during an adversarial attack by using ensembles. Finally, we analyze factors that effect model behavior under adversarial attack, and develop a new model to predict errors during attacks. Our novel findings reveal that (a) unlike BERT, models that use ELMo embeddings are more susceptible to adversarial attacks, (b) unlike word and paraphrase, character perturbations affect the model the most but are most easily compensated for by adversarial training, (c) word perturbations lead to more high-confidence misclassifications compared to sentence- and character-level perturbations, (d) the type of question and model answer length (the longer the answer the more likely it is to be incorrect) is the most predictive of model errors in adversarial setting, and (e) conclusions about model behavior are dataset-specific",
    "checked": true,
    "id": "5e9faa7316250e6d8ee367a2abce418e9e3269dc",
    "semantic_title": "evaluating neural model robustness for machine comprehension",
    "citation_count": 5,
    "authors": [
      "Winston Wu",
      "Dustin Arendt",
      "Svitlana Volkova"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.211": {
    "title": "Hidden Biases in Unreliable News Detection Datasets",
    "volume": "main",
    "abstract": "Automatic unreliable news detection is a research problem with great potential impact. Recently, several papers have shown promising results on large-scale news datasets with models that only use the article itself without resorting to any fact-checking mechanism or retrieving any supporting evidence. In this work, we take a closer look at these datasets. While they all provide valuable resources for future research, we observe a number of problems that may lead to results that do not generalize in more realistic settings. Specifically, we show that selection bias during data collection leads to undesired artifacts in the datasets. In addition, while most systems train and predict at the level of individual articles, overlapping article sources in the training and evaluation data can provide a strong confounding factor that models can exploit. In the presence of this confounding factor, the models can achieve good performance by directly memorizing the site-label mapping instead of modeling the real task of unreliable news detection. We observed a significant drop (>10%) in accuracy for all models tested in a clean split with no train/test source overlap. Using the observations and experimental results, we provide practical suggestions on how to create more reliable datasets for the unreliable news detection task. We suggest future dataset creation include a simple model as a difficulty/bias probe and future model development use a clean non-overlapping site and date split",
    "checked": true,
    "id": "f4b368e9b8ff32f4bd51080910cbab87c3ce460a",
    "semantic_title": "hidden biases in unreliable news detection datasets",
    "citation_count": 14,
    "authors": [
      "Xiang Zhou",
      "Heba Elfardy",
      "Christos Christodoulopoulos",
      "Thomas Butler",
      "Mohit Bansal"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.212": {
    "title": "Annealing Knowledge Distillation",
    "volume": "main",
    "abstract": "Significant memory and computational requirements of large deep neural networks restricts their application on edge devices. Knowledge distillation (KD) is a prominent model compression technique for deep neural networks in which the knowledge of a trained large teacher model is transferred to a smaller student model. The success of knowledge distillation is mainly attributed to its training objective function, which exploits the soft-target information (also known as \"dark knowledge\") besides the given regular hard labels in a training set. However, it is shown in the literature that the larger the gap between the teacher and the student networks, the more difficult is their training using knowledge distillation. To address this shortcoming, we propose an improved knowledge distillation method (called Annealing-KD) by feeding the rich information provided by teacher's soft-targets incrementally and more efficiently. Our Annealing-KD technique is based on a gradual transition over annealed soft-targets generated by the teacher at different temperatures in an iterative process; and therefore, the student is trained to follow the annealed teacher output in a step-by-step manner. This paper includes theoretical and empirical evidence as well as practical experiments to support the effectiveness of our Annealing-KD method. We did a comprehensive set of experiments on different tasks such as image classification (CIFAR-10 and 100) and NLP language inference with BERT-based models on the GLUE benchmark and consistently got superior results",
    "checked": true,
    "id": "7599401233a40cc8c51b77d0419d2732c35439b2",
    "semantic_title": "annealing knowledge distillation",
    "citation_count": 64,
    "authors": [
      "Aref Jafari",
      "Mehdi Rezagholizadeh",
      "Pranav Sharma",
      "Ali Ghodsi"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.213": {
    "title": "Unsupervised Extractive Summarization using Pointwise Mutual Information",
    "volume": "main",
    "abstract": "Unsupervised approaches to extractive summarization usually rely on a notion of sentence importance defined by the semantic similarity between a sentence and the document. We propose new metrics of relevance and redundancy using pointwise mutual information (PMI) between sentences, which can be easily computed by a pre-trained language model. Intuitively, a relevant sentence allows readers to infer the document content (high PMI with the document), and a redundant sentence can be inferred from the summary (high PMI with the summary). We then develop a greedy sentence selection algorithm to maximize relevance and minimize redundancy of extracted sentences. We show that our method outperforms similarity-based methods on datasets in a range of domains including news, medical journal articles, and personal anecdotes",
    "checked": true,
    "id": "89d0505974747c21d28309b26257bfe56c4eaeda",
    "semantic_title": "unsupervised extractive summarization using pointwise mutual information",
    "citation_count": 34,
    "authors": [
      "Vishakh Padmakumar",
      "He He"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.214": {
    "title": "Context-aware Neural Machine Translation with Mini-batch Embedding",
    "volume": "main",
    "abstract": "It is crucial to provide an inter-sentence context in Neural Machine Translation (NMT) models for higher-quality translation. With the aim of using a simple approach to incorporate inter-sentence information, we propose mini-batch embedding (MBE) as a way to represent the features of sentences in a mini-batch. We construct a mini-batch by choosing sentences from the same document, and thus the MBE is expected to have contextual information across sentences. Here, we incorporate MBE in an NMT model, and our experiments show that the proposed method consistently outperforms the translation capabilities of strong baselines and improves writing style or terminology to fit the document's context",
    "checked": true,
    "id": "04b5fe04fffe7fdddaa47cda3f1aa654d7a999fe",
    "semantic_title": "context-aware neural machine translation with mini-batch embedding",
    "citation_count": 5,
    "authors": [
      "Makoto Morishita",
      "Jun Suzuki",
      "Tomoharu Iwata",
      "Masaaki Nagata"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.215": {
    "title": "Deep Subjecthood: Higher-Order Grammatical Features in Multilingual BERT",
    "volume": "main",
    "abstract": "We investigate how Multilingual BERT (mBERT) encodes grammar by examining how the high-order grammatical feature of morphosyntactic alignment (how different languages define what counts as a \"subject\") is manifested across the embedding spaces of different languages. To understand if and how morphosyntactic alignment affects contextual embedding spaces, we train classifiers to recover the subjecthood of mBERT embeddings in transitive sentences (which do not contain overt information about morphosyntactic alignment) and then evaluate them zero-shot on intransitive sentences (where subjecthood classification depends on alignment), within and across languages. We find that the resulting classifier distributions reflect the morphosyntactic alignment of their training languages. Our results demonstrate that mBERT representations are influenced by high-level grammatical features that are not manifested in any one input sentence, and that this is robust across languages. Further examining the characteristics that our classifiers rely on, we find that features such as passive voice, animacy and case strongly correlate with classification decisions, suggesting that mBERT does not encode subjecthood purely syntactically, but that subjecthood embedding is continuous and dependent on semantic and discourse factors, as is proposed in much of the functional linguistics literature. Together, these results provide insight into how grammatical features manifest in contextual embedding spaces, at a level of abstraction not covered by previous work",
    "checked": true,
    "id": "4548c8e706599f71fdcaa1bb7b278ef07e6e5d69",
    "semantic_title": "deep subjecthood: higher-order grammatical features in multilingual bert",
    "citation_count": 38,
    "authors": [
      "Isabel Papadimitriou",
      "Ethan A. Chi",
      "Richard Futrell",
      "Kyle Mahowald"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.216": {
    "title": "Streaming Models for Joint Speech Recognition and Translation",
    "volume": "main",
    "abstract": "Using end-to-end models for speech translation (ST) has increasingly been the focus of the ST community. These models condense the previously cascaded systems by directly converting sound waves into translated text. However, cascaded models have the advantage of including automatic speech recognition output, useful for a variety of practical ST systems that often display transcripts to the user alongside the translations. To bridge this gap, recent work has shown initial progress into the feasibility for end-to-end models to produce both of these outputs. However, all previous work has only looked at this problem from the consecutive perspective, leaving uncertainty on whether these approaches are effective in the more challenging streaming setting. We develop an end-to-end streaming ST model based on a re-translation approach and compare against standard cascading approaches. We also introduce a novel inference method for the joint case, interleaving both transcript and translation in generation and removing the need to use separate decoders. Our evaluation across a range of metrics capturing accuracy, latency, and consistency shows that our end-to-end models are statistically similar to cascading models, while having half the number of parameters. We also find that both systems provide strong translation quality at low latency, keeping 99% of consecutive quality at a lag of just under a second",
    "checked": true,
    "id": "558240649f7943b9353bea63bd1b81cf712c40f4",
    "semantic_title": "streaming models for joint speech recognition and translation",
    "citation_count": 12,
    "authors": [
      "Orion Weller",
      "Matthias Sperber",
      "Christian Gollan",
      "Joris Kluivers"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.217": {
    "title": "DOCENT: Learning Self-Supervised Entity Representations from Large Document Collections",
    "volume": "main",
    "abstract": "This paper explores learning rich self-supervised entity representations from large amounts of associated text. Once pre-trained, these models become applicable to multiple entity-centric tasks such as ranked retrieval, knowledge base completion, question answering, and more. Unlike other methods that harvest self-supervision signals based merely on a local context within a sentence, we radically expand the notion of context to include any available text related to an entity. This enables a new class of powerful, high-capacity representations that can ultimately distill much of the useful information about an entity from multiple text sources, without any human supervision. We present several training strategies that, unlike prior approaches, learn to jointly predict words and entities – strategies we compare experimentally on downstream tasks in the TV-Movies domain, such as MovieLens tag prediction from user reviews and natural language movie search. As evidenced by results, our models match or outperform competitive baselines, sometimes with little or no fine-tuning, and are also able to scale to very large corpora. Finally, we make our datasets and pre-trained models publicly available. This includes Reviews2Movielens, mapping the ~1B word corpus of Amazon movie reviews (He and McAuley, 2016) to MovieLens tags (Harper and Konstan, 2016), as well as Reddit Movie Suggestions with natural language queries and corresponding community recommendations",
    "checked": true,
    "id": "b0216b8dcd9fcdf2801477573de93ab9a383b42f",
    "semantic_title": "docent: learning self-supervised entity representations from large document collections",
    "citation_count": 10,
    "authors": [
      "Yury Zemlyanskiy",
      "Sudeep Gandhe",
      "Ruining He",
      "Bhargav Kanagal",
      "Anirudh Ravula",
      "Juraj Gottweis",
      "Fei Sha",
      "Ilya Eckstein"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.218": {
    "title": "Scientific Discourse Tagging for Evidence Extraction",
    "volume": "main",
    "abstract": "Evidence plays a crucial role in any biomedical research narrative, providing justification for some claims and refutation for others. We seek to build models of scientific argument using information extraction methods from full-text papers. We present the capability of automatically extracting text fragments from primary research papers that describe the evidence presented in that paper's figures, which arguably provides the raw material of any scientific argument made within the paper. We apply richly contextualized deep representation learning pre-trained on biomedical domain corpus to the analysis of scientific discourse structures and the extraction of \"evidence fragments\" (i.e., the text in the results section describing data presented in a specified subfigure) from a set of biomedical experimental research articles. We first demonstrate our state-of-the-art scientific discourse tagger on two scientific discourse tagging datasets and its transferability to new datasets. We then show the benefit of leveraging scientific discourse tags for downstream tasks such as claim-extraction and evidence fragment detection. Our work demonstrates the potential of using evidence fragments derived from figure spans for improving the quality of scientific claims by cataloging, indexing and reusing evidence fragments as independent documents",
    "checked": true,
    "id": "b37c60c3ce64840ac58bc9ecefca9a2631b087db",
    "semantic_title": "scientific discourse tagging for evidence extraction",
    "citation_count": 18,
    "authors": [
      "Xiangci Li",
      "Gully Burns",
      "Nanyun Peng"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.219": {
    "title": "Incremental Beam Manipulation for Natural Language Generation",
    "volume": "main",
    "abstract": "The performance of natural language generation systems has improved substantially with modern neural networks. At test time they typically employ beam search to avoid locally optimal but globally suboptimal predictions. However, due to model errors, a larger beam size can lead to deteriorating performance according to the evaluation metric. For this reason, it is common to rerank the output of beam search, but this relies on beam search to produce a good set of hypotheses, which limits the potential gains. Other alternatives to beam search require changes to the training of the model, which restricts their applicability compared to beam search. This paper proposes incremental beam manipulation, i.e. reranking the hypotheses in the beam during decoding instead of only at the end. This way, hypotheses that are unlikely to lead to a good final output are discarded, and in their place hypotheses that would have been ignored will be considered instead. Applying incremental beam manipulation leads to an improvement of 1.93 and 5.82 BLEU points over vanilla beam search for the test sets of the E2E and WebNLG challenges respectively. The proposed method also outperformed a strong reranker by 1.04 BLEU points on the E2E challenge, while being on par with it on the WebNLG dataset",
    "checked": true,
    "id": "4dd8d01406664773953daef9109d6cc92b2c552b",
    "semantic_title": "incremental beam manipulation for natural language generation",
    "citation_count": 7,
    "authors": [
      "James Hargreaves",
      "Andreas Vlachos",
      "Guy Emerson"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.220": {
    "title": "StructSum: Summarization via Structured Representations",
    "volume": "main",
    "abstract": "Abstractive text summarization aims at compressing the information of a long source document into a rephrased, condensed summary. Despite advances in modeling techniques, abstractive summarization models still suffer from several key challenges: (i) layout bias: they overfit to the style of training corpora; (ii) limited abstractiveness: they are optimized to copying n-grams from the source rather than generating novel abstractive summaries; (iii) lack of transparency: they are not interpretable. In this work, we propose a framework based on document-level structure induction for summarization to address these challenges. To this end, we propose incorporating latent and explicit dependencies across sentences in the source document into end-to-end single-document summarization models. Our framework complements standard encoder-decoder summarization models by augmenting them with rich structure-aware document representations based on implicitly learned (latent) structures and externally-derived linguistic (explicit) structures. We show that our summarization framework, trained on the CNN/DM dataset, improves the coverage of content in the source documents, generates more abstractive summaries by generating more novel n-grams, and incorporates interpretable sentence-level structures, while performing on par with standard baselines",
    "checked": true,
    "id": "cd4e49a4b3f9ab6c3737a1691bbf7281027932a6",
    "semantic_title": "structsum: summarization via structured representations",
    "citation_count": 19,
    "authors": [
      "Vidhisha Balachandran",
      "Artidoro Pagnoni",
      "Jay Yoon Lee",
      "Dheeraj Rajagopal",
      "Jaime Carbonell",
      "Yulia Tsvetkov"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.221": {
    "title": "Project-then-Transfer: Effective Two-stage Cross-lingual Transfer for Semantic Dependency Parsing",
    "volume": "main",
    "abstract": "This paper describes the first report on cross-lingual transfer for semantic dependency parsing. We present the insight that there are twodifferent kinds of cross-linguality, namely sur-face level and mantic level, and try to cap-ture both kinds of cross-linguality by combin-ing annotation projection and model transferof pre-trained language models. Our exper-iments showed that the performance of our graph-based semantic dependency parser almost achieved the approximated upper bound",
    "checked": true,
    "id": "a6dff2bec85201757ff6bdc3dde663aa7f425713",
    "semantic_title": "project-then-transfer: effective two-stage cross-lingual transfer for semantic dependency parsing",
    "citation_count": 5,
    "authors": [
      "Hiroaki Ozaki",
      "Gaku Morio",
      "Terufumi Morishita",
      "Toshinori Miyoshi"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.222": {
    "title": "LSOIE: A Large-Scale Dataset for Supervised Open Information Extraction",
    "volume": "main",
    "abstract": "Open Information Extraction (OIE) systems seek to compress the factual propositions of a sentence into a series of n-ary tuples. These tuples are useful for downstream tasks in natural language processing like knowledge base creation, textual entailment, and natural language understanding. However, current OIE datasets are limited in both size and diversity. We introduce a new dataset by converting the QA-SRL 2.0 dataset to a large-scale OIE dataset LSOIE. Our LSOIE dataset is 20 times larger than the next largest human-annotated OIE dataset. We construct and evaluate several benchmark OIE models on LSOIE, providing baselines for future improvements on the task. Our LSOIE data, models, and code are made publicly available",
    "checked": true,
    "id": "e01777cd20cd97a94519a2e4c00349d3427ba5fa",
    "semantic_title": "lsoie: a large-scale dataset for supervised open information extraction",
    "citation_count": 27,
    "authors": [
      "Jacob Solawetz",
      "Stefan Larson"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.223": {
    "title": "Changing the Mind of Transformers for Topically-Controllable Language Generation",
    "volume": "main",
    "abstract": "Large Transformer-based language models can aid human authors by suggesting plausible continuations of text written so far. However, current interactive writing assistants do not allow authors to guide text generation in desired topical directions. To address this limitation, we design a framework that displays multiple candidate upcoming topics, of which a user can select a subset to guide the generation. Our framework consists of two components: (1) a method that produces a set of candidate topics by predicting the centers of word clusters in the possible continuations, and (2) a text generation model whose output adheres to the chosen topics. The training of both components is self-supervised, using only unlabeled text. Our experiments demonstrate that our topic options are better than those of standard clustering approaches, and our framework often generates fluent sentences related to the chosen topics, as judged by automated metrics and crowdsourced workers",
    "checked": true,
    "id": "71cdf94d13cc6c497dcc2dcb20893fe64cfaf62e",
    "semantic_title": "changing the mind of transformers for topically-controllable language generation",
    "citation_count": 9,
    "authors": [
      "Haw-Shiuan Chang",
      "Jiaming Yuan",
      "Mohit Iyyer",
      "Andrew McCallum"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.224": {
    "title": "Unsupervised Abstractive Summarization of Bengali Text Documents",
    "volume": "main",
    "abstract": "Abstractive summarization systems generally rely on large collections of document-summary pairs. However, the performance of abstractive systems remains a challenge due to the unavailability of the parallel data for low-resource languages like Bengali. To overcome this problem, we propose a graph-based unsupervised abstractive summarization system in the single-document setting for Bengali text documents, which requires only a Part-Of-Speech (POS) tagger and a pre-trained language model trained on Bengali texts. We also provide a human-annotated dataset with document-summary pairs to evaluate our abstractive model and to support the comparison of future abstractive summarization systems of the Bengali Language. We conduct experiments on this dataset and compare our system with several well-established unsupervised extractive summarization systems. Our unsupervised abstractive summarization model outperforms the baselines without being exposed to any human-annotated reference summaries",
    "checked": true,
    "id": "2b7130e703c5752a00d95673ff31ab638ca9c194",
    "semantic_title": "unsupervised abstractive summarization of bengali text documents",
    "citation_count": 20,
    "authors": [
      "Radia Rayan Chowdhury",
      "Mir Tafseer Nayeem",
      "Tahsin Tasnim Mim",
      "Md. Saifur Rahman Chowdhury",
      "Taufiqul Jannat"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.225": {
    "title": "From Toxicity in Online Comments to Incivility in American News: Proceed with Caution",
    "volume": "main",
    "abstract": "The ability to quantify incivility online, in news and in congressional debates is of great interest to political scientists. Computational tools for detecting online incivility for English are now fairly accessible and potentially could be applied more broadly. We test the Jigsaw Perspective API for its ability to detect the degree of incivility on a corpus that we developed, consisting of manual annotations of civility in American news. We demonstrate that toxicity models, as exemplified by Perspective, are inadequate for the analysis of incivility in news. We carry out error analysis that points to the need to develop methods to remove spurious correlations between words often mentioned in the news, especially identity descriptors and incivility. Without such improvements, applying Perspective or similar models on news is likely to lead to wrong conclusions, that are not aligned with the human perception of incivility",
    "checked": true,
    "id": "89ce680cfa584f0b4dafc2a153b31355dd549e57",
    "semantic_title": "from toxicity in online comments to incivility in american news: proceed with caution",
    "citation_count": 8,
    "authors": [
      "Anushree Hede",
      "Oshin Agarwal",
      "Linda Lu",
      "Diana C. Mutz",
      "Ani Nenkova"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.226": {
    "title": "On the Computational Modelling of Michif Verbal Morphology",
    "volume": "main",
    "abstract": "This paper presents a finite-state computational model of the verbal morphology of Michif. Michif, the official language of the Métis peoples, is a uniquely mixed language with Algonquian and French origins. It is spoken across the Métis homelands in what is now called Canada and the United States, but it is highly endangered with less than 100 speakers. The verbal morphology is remarkably complex, as the already polysynthetic Algonquian patterns are combined with French elements and unique morpho-phonological interactions. The model presented in this paper, LI VERB KAA-OOSHITAHK DI MICHIF handles this complexity by using a series of composed finite-state transducers to model the concatenative morphology and phonological rule alternations that are unique to Michif. Such a rule-based approach is necessary as there is insufficient language data for an approach that uses machine learning. A language model such as LI VERB KAA-OOSHITAHK DI MICHIF furthers the goals of Indigenous computational linguistics in Canada while also supporting the creation of tools for documentation, education, and revitalization that are desired by the Métis community",
    "checked": true,
    "id": "0af4fac5b67847f31aeb84d7d1e9f8f32e0b2bf6",
    "semantic_title": "on the computational modelling of michif verbal morphology",
    "citation_count": 5,
    "authors": [
      "Fineen Davis",
      "Eddie Antonio Santos",
      "Heather Souter"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.227": {
    "title": "A Few Topical Tweets are Enough for Effective User Stance Detection",
    "volume": "main",
    "abstract": "User stance detection entails ascertaining the position of a user towards a target, such as an entity, topic, or claim. Recent work that employs unsupervised classification has shown that performing stance detection on vocal Twitter users, who have many tweets on a target, can be highly accurate (+98%). However, such methods perform poorly or fail completely for less vocal users, who may have authored only a few tweets about a target. In this paper, we tackle stance detection for such users using two approaches. In the first approach, we improve user-level stance detection by representing tweets using contextualized embeddings, which capture latent meanings of words in context. We show that this approach outperforms two strong baselines and achieves 89.6% accuracy and 91.3% macro F-measure on eight controversial topics. In the second approach, we expand the tweets of a given user using their Twitter timeline tweets, which may not be topically relevant, and then we perform unsupervised classification of the user, which entails clustering a user with other users in the training set. This approach achieves 95.6% accuracy and 93.1% macro F-measure",
    "checked": true,
    "id": "42b7a519024a167802a3d6a4711f42b8590a1fcf",
    "semantic_title": "a few topical tweets are enough for effective user stance detection",
    "citation_count": 24,
    "authors": [
      "Younes Samih",
      "Kareem Darwish"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.228": {
    "title": "Do Syntax Trees Help Pre-trained Transformers Extract Information?",
    "volume": "main",
    "abstract": "Much recent work suggests that incorporating syntax information from dependency trees can improve task-specific transformer models. However, the effect of incorporating dependency tree information into pre-trained transformer models (e.g., BERT) remains unclear, especially given recent studies highlighting how these models implicitly encode syntax. In this work, we systematically study the utility of incorporating dependency trees into pre-trained transformers on three representative information extraction tasks: semantic role labeling (SRL), named entity recognition, and relation extraction. We propose and investigate two distinct strategies for incorporating dependency structure: a late fusion approach, which applies a graph neural network on the output of a transformer, and a joint fusion approach, which infuses syntax structure into the transformer attention layers. These strategies are representative of prior work, but we introduce additional model design elements that are necessary for obtaining improved performance. Our empirical analysis demonstrates that these syntax-infused transformers obtain state-of-the-art results on SRL and relation extraction tasks. However, our analysis also reveals a critical shortcoming of these models: we find that their performance gains are highly contingent on the availability of human-annotated dependency parses, which raises important questions regarding the viability of syntax-augmented transformers in real-world applications",
    "checked": true,
    "id": "5bbf9555e9ec9e29ae4cd3c1253b1b74e8ddec20",
    "semantic_title": "do syntax trees help pre-trained transformers extract information?",
    "citation_count": 68,
    "authors": [
      "Devendra Sachan",
      "Yuhao Zhang",
      "Peng Qi",
      "William L. Hamilton"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.229": {
    "title": "Informative and Controllable Opinion Summarization",
    "volume": "main",
    "abstract": "Opinion summarization is the task of automatically generating summaries for a set of reviews about a specific target (e.g., a movie or a product). Since the number of reviews for each target can be prohibitively large, neural network-based methods follow a two-stage approach where an extractive step first pre-selects a subset of salient opinions and an abstractive step creates the summary while conditioning on the extracted subset. However, the extractive model leads to loss of information which may be useful depending on user needs. In this paper we propose a summarization framework that eliminates the need to rely only on pre-selected content and waste possibly useful information, especially when customizing summaries. The framework enables the use of all input reviews by first condensing them into multiple dense vectors which serve as input to an abstractive model. We showcase an effective instantiation of our framework which produces more informative summaries and also allows to take user preferences into account using our zero-shot customization technique. Experimental results demonstrate that our model improves the state of the art on the Rotten Tomatoes dataset and generates customized summaries effectively",
    "checked": true,
    "id": "759521f22de9deb4760ff70440c55dcaf467df46",
    "semantic_title": "informative and controllable opinion summarization",
    "citation_count": 48,
    "authors": [
      "Reinald Kim Amplayo",
      "Mirella Lapata"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.230": {
    "title": "Coloring the Black Box: What Synesthesia Tells Us about Character Embeddings",
    "volume": "main",
    "abstract": "In contrast to their word- or sentence-level counterparts, character embeddings are still poorly understood. We aim at closing this gap with an in-depth study of English character embeddings. For this, we use resources from research on grapheme–color synesthesia – a neuropsychological phenomenon where letters are associated with colors –, which give us insight into which characters are similar for synesthetes and how characters are organized in color space. Comparing 10 different character embeddings, we ask: How similar are character embeddings to a synesthete's perception of characters? And how similar are character embeddings extracted from different models? We find that LSTMs agree with humans more than transformers. Comparing across tasks, grapheme-to-phoneme conversion results in the most human-like character embeddings. Finally, ELMo embeddings differ from both humans and other models",
    "checked": true,
    "id": "f0169c51a88136ef9cf3f57e48575dba3e18c2ff",
    "semantic_title": "coloring the black box: what synesthesia tells us about character embeddings",
    "citation_count": 2,
    "authors": [
      "Katharina Kann",
      "Mauro M. Monsalve-Mercado"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.231": {
    "title": "How Good (really) are Grammatical Error Correction Systems?",
    "volume": "main",
    "abstract": "Standard evaluations of Grammatical Error Correction (GEC) systems make use of a fixed reference text generated relative to the original text; they show, even when using multiple references, that we have a long way to go. This analysis paper studies the performance of GEC systems relative to closest-gold – a gold reference text created relative to the output of a system. Surprisingly, we show that the real performance is 20-40 points better than standard evaluations show. Moreover, the performance remains high even when considering any of the top-10 hypotheses produced by a system. Importantly, the type of mistakes corrected by lower-ranked hypotheses differs in interesting ways from the top one, providing an opportunity to focus on a range of errors – local spelling and grammar edits vs. more complex lexical improvements. Our study shows these results in English and Russian, and thus provides a preliminary proposal for a more realistic evaluation of GEC systems",
    "checked": true,
    "id": "f081254bc814f063dca6bb3df45f02bd1e4c7ac3",
    "semantic_title": "how good (really) are grammatical error correction systems?",
    "citation_count": 12,
    "authors": [
      "Alla Rozovskaya",
      "Dan Roth"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.232": {
    "title": "BERTective: Language Models and Contextual Information for Deception Detection",
    "volume": "main",
    "abstract": "Spotting a lie is challenging but has an enormous potential impact on security as well as private and public safety. Several NLP methods have been proposed to classify texts as truthful or deceptive. In most cases, however, the target texts' preceding context is not considered. This is a severe limitation, as any communication takes place in context, not in a vacuum, and context can help to detect deception. We study a corpus of Italian dialogues containing deceptive statements and implement deep neural models that incorporate various linguistic contexts. We establish a new state-of-the-art identifying deception and find that not all context is equally useful to the task. Only the texts closest to the target, if from the same speaker (rather than questions by an interlocutor), boost performance. We also find that the semantic information in language models such as BERT contributes to the performance. However, BERT alone does not capture the implicit knowledge of deception cues: its contribution is conditional on the concurrent use of attention to learn cues from BERT's representations",
    "checked": true,
    "id": "6d75ad350e2c910d39fba3f401b0e1c32639689d",
    "semantic_title": "bertective: language models and contextual information for deception detection",
    "citation_count": 9,
    "authors": [
      "Tommaso Fornaciari",
      "Federico Bianchi",
      "Massimo Poesio",
      "Dirk Hovy"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.233": {
    "title": "Learning Coupled Policies for Simultaneous Machine Translation using Imitation Learning",
    "volume": "main",
    "abstract": "We present a novel approach to efficiently learn a simultaneous translation model with coupled programmer-interpreter policies. First, we present an algorithmic oracle to produce oracle READ/WRITE actions for training bilingual sentence-pairs using the notion of word alignments. This oracle actions are designed to capture enough information from the partial input before writing the output. Next, we perform a coupled scheduled sampling to effectively mitigate the exposure bias when learning both policies jointly with imitation learning. Experiments on six language-pairs show our method outperforms strong baselines in terms of translation quality quality while keeping the delay low",
    "checked": true,
    "id": "2df9c4b68cb8d006c7d38fd32d2e47f8892010da",
    "semantic_title": "learning coupled policies for simultaneous machine translation using imitation learning",
    "citation_count": 17,
    "authors": [
      "Philip Arthur",
      "Trevor Cohn",
      "Gholamreza Haffari"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.234": {
    "title": "Complementary Evidence Identification in Open-Domain Question Answering",
    "volume": "main",
    "abstract": "This paper proposes a new problem of complementary evidence identification for open-domain question answering (QA). The problem aims to efficiently find a small set of passages that covers full evidence from multiple aspects as to answer a complex question. To this end, we proposes a method that learns vector representations of passages and models the sufficiency and diversity within the selected set, in addition to the relevance between the question and passages. Our experiments demonstrate that our method considers the dependence within the supporting evidence and significantly improves the accuracy of complementary evidence selection in QA domain",
    "checked": true,
    "id": "a6db58de2870e4d024106ea3d1b764070346e2f4",
    "semantic_title": "complementary evidence identification in open-domain question answering",
    "citation_count": 2,
    "authors": [
      "Xiangyang Mou",
      "Mo Yu",
      "Shiyu Chang",
      "Yufei Feng",
      "Li Zhang",
      "Hui Su"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.235": {
    "title": "Entity-level Factual Consistency of Abstractive Text Summarization",
    "volume": "main",
    "abstract": "A key challenge for abstractive summarization is ensuring factual consistency of the generated summary with respect to the original document. For example, state-of-the-art models trained on existing datasets exhibit entity hallucination, generating names of entities that are not present in the source document. We propose a set of new metrics to quantify the entity-level factual consistency of generated summaries and we show that the entity hallucination problem can be alleviated by simply filtering the training data. In addition, we propose a summary-worthy entity classification task to the training process as well as a joint entity and summary generation approach, which yield further improvements in entity level metrics",
    "checked": true,
    "id": "06f6869f8eb90a35148f1dde9a4baff03460d699",
    "semantic_title": "entity-level factual consistency of abstractive text summarization",
    "citation_count": 138,
    "authors": [
      "Feng Nan",
      "Ramesh Nallapati",
      "Zhiguo Wang",
      "Cicero Nogueira dos Santos",
      "Henghui Zhu",
      "Dejiao Zhang",
      "Kathleen McKeown",
      "Bing Xiang"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.236": {
    "title": "On Hallucination and Predictive Uncertainty in Conditional Language Generation",
    "volume": "main",
    "abstract": "Despite improvements in performances on different natural language generation tasks, deep neural models are prone to hallucinating facts that are incorrect or nonexistent. Different hypotheses are proposed and examined separately for different tasks, but no systematic explanations are available across these tasks. In this study, we draw connections between hallucinations and predictive uncertainty in conditional language generation. We investigate their relationship in both image captioning and data-to-text generation and propose a simple extension to beam search to reduce hallucination. Our analysis shows that higher predictive uncertainty corresponds to a higher chance of hallucination. Epistemic uncertainty is more indicative of hallucination than aleatoric or total uncertainties. It helps to achieve better results of trading performance in standard metric for less hallucination with the proposed beam search variant",
    "checked": true,
    "id": "2476832edb11cb3de46587f55e270d6df328b32d",
    "semantic_title": "on hallucination and predictive uncertainty in conditional language generation",
    "citation_count": 127,
    "authors": [
      "Yijun Xiao",
      "William Yang Wang"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.237": {
    "title": "Fine-Grained Event Trigger Detection",
    "volume": "main",
    "abstract": "Most of the previous work on Event Detection (ED) has only considered the datasets with a small number of event types (i.e., up to 38 types). In this work, we present the first study on fine-grained ED (FED) where the evaluation dataset involves much more fine-grained event types (i.e., 449 types). We propose a novel method to transform the Semcor dataset for Word Sense Disambiguation into a large and high-quality dataset for FED. Extensive evaluation of the current ED methods is conducted to demonstrate the challenges of the generated datasets for FED, calling for more research effort in this area",
    "checked": true,
    "id": "b8a9550197e55a43e9778414049975919d8e0eee",
    "semantic_title": "fine-grained event trigger detection",
    "citation_count": 9,
    "authors": [
      "Duong Le",
      "Thien Huu Nguyen"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.238": {
    "title": "Extremely Small BERT Models from Mixed-Vocabulary Training",
    "volume": "main",
    "abstract": "Pretrained language models like BERT have achieved good results on NLP tasks, but are impractical on resource-limited devices due to memory footprint. A large fraction of this footprint comes from the input embeddings with large input vocabulary and embedding dimensions. Existing knowledge distillation methods used for model compression cannot be directly applied to train student models with reduced vocabulary sizes. To this end, we propose a distillation method to align the teacher and student embeddings via mixed-vocabulary training. Our method compresses BERT-LARGE to a task-agnostic model with smaller vocabulary and hidden dimensions, which is an order of magnitude smaller than other distilled BERT models and offers a better size-accuracy trade-off on language understanding benchmarks as well as a practical dialogue task",
    "checked": true,
    "id": "f320fc6b3e184fab254bb92ed3b3f265e6266dfb",
    "semantic_title": "extremely small bert models from mixed-vocabulary training",
    "citation_count": 17,
    "authors": [
      "Sanqiang Zhao",
      "Raghav Gupta",
      "Yang Song",
      "Denny Zhou"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.239": {
    "title": "Diverse Adversaries for Mitigating Bias in Training",
    "volume": "main",
    "abstract": "Adversarial learning can learn fairer and less biased models of language processing than standard training. However, current adversarial techniques only partially mitigate the problem of model bias, added to which their training procedures are often unstable. In this paper, we propose a novel approach to adversarial learning based on the use of multiple diverse discriminators, whereby discriminators are encouraged to learn orthogonal hidden representations from one another. Experimental results show that our method substantially improves over standard adversarial removal methods, in terms of reducing bias and stability of training",
    "checked": true,
    "id": "7ce86586ac7e00e77fe0b679290ad3e8fb96f73e",
    "semantic_title": "diverse adversaries for mitigating bias in training",
    "citation_count": 52,
    "authors": [
      "Xudong Han",
      "Timothy Baldwin",
      "Trevor Cohn"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.240": {
    "title": "‘Just because you are right, doesn't mean I am wrong': Overcoming a bottleneck in development and evaluation of Open-Ended VQA tasks",
    "volume": "main",
    "abstract": "GQA (CITATION) is a dataset for real-world visual reasoning and compositional question answering. We found that many answers predicted by the best vision-language models on the GQA dataset do not match the ground-truth answer but still are semantically meaningful and correct in the given context. In fact, this is the case with most existing visual question answering (VQA) datasets where they assume only one ground-truth answer for each question. We propose Alternative Answer Sets (AAS) of ground-truth answers to address this limitation, which is created automatically using off-the-shelf NLP tools. We introduce a semantic metric based on AAS and modify top VQA solvers to support multiple plausible answers for a question. We implement this approach on the GQA dataset and show the performance improvements",
    "checked": true,
    "id": "d62a479330d0f1b0a1897f1ce761a80863bf1b3b",
    "semantic_title": "‘just because you are right, doesn't mean i am wrong': overcoming a bottleneck in development and evaluation of open-ended vqa tasks",
    "citation_count": 9,
    "authors": [
      "Man Luo",
      "Shailaja Keyur Sampat",
      "Riley Tallman",
      "Yankai Zeng",
      "Manuha Vancha",
      "Akarshan Sajja",
      "Chitta Baral"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.241": {
    "title": "Better Neural Machine Translation by Extracting Linguistic Information from BERT",
    "volume": "main",
    "abstract": "Adding linguistic information (syntax or semantics) to neural machine translation (NMT) have mostly focused on using point estimates from pre-trained models. Directly using the capacity of massive pre-trained contextual word embedding models such as BERT(Devlin et al., 2019) has been marginally useful in NMT because effective fine-tuning is difficult to obtain for NMT without making training brittle and unreliable. We augment NMT by extracting dense fine-tuned vector-based linguistic information from BERT instead of using point estimates. Experimental results show that our method of incorporating linguistic information helps NMT to generalize better in a variety of training contexts and is no more difficult to train than conventional Transformer-based NMT",
    "checked": true,
    "id": "b6f3b9b489051fa3707daa2c57fa55914df853f6",
    "semantic_title": "better neural machine translation by extracting linguistic information from bert",
    "citation_count": 12,
    "authors": [
      "Hassan S. Shavarani",
      "Anoop Sarkar"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.242": {
    "title": "CLiMP: A Benchmark for Chinese Language Model Evaluation",
    "volume": "main",
    "abstract": "Linguistically informed analyses of language models (LMs) contribute to the understanding and improvement of such models. Here, we introduce the corpus of Chinese linguistic minimal pairs (CLiMP) to investigate what knowledge Chinese LMs acquire. CLiMP consists of sets of 1000 minimal pairs (MPs) for 16 syntactic contrasts in Chinese, covering 9 major Chinese linguistic phenomena. The MPs are semi-automatically generated, and human agreement with the labels in CLiMP is 95.8%. We evaluate 11 different LMs on CLiMP, covering n-grams, LSTMs, and Chinese BERT. We find that classifier–noun agreement and verb complement selection are the phenomena that models generally perform best at. However, models struggle the most with the ba construction, binding, and filler-gap dependencies. Overall, Chinese BERT achieves an 81.8% average accuracy, while the performances of LSTMs and 5-grams are only moderately above chance level",
    "checked": true,
    "id": "d21830c081094fbf1b2eaf8513cca0459069cd35",
    "semantic_title": "climp: a benchmark for chinese language model evaluation",
    "citation_count": 27,
    "authors": [
      "Beilei Xiang",
      "Changbing Yang",
      "Yu Li",
      "Alex Warstadt",
      "Katharina Kann"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.243": {
    "title": "Measuring and Improving Faithfulness of Attention in Neural Machine Translation",
    "volume": "main",
    "abstract": "While the attention heatmaps produced by neural machine translation (NMT) models seem insightful, there is little evidence that they reflect a model's true internal reasoning. We provide a measure of faithfulness for NMT based on a variety of stress tests where attention weights which are crucial for prediction are perturbed and the model should alter its predictions if the learned weights are a faithful explanation of the predictions. We show that our proposed faithfulness measure for NMT models can be improved using a novel differentiable objective that rewards faithful behaviour by the model through probability divergence. Our experimental results on multiple language pairs show that our objective function is effective in increasing faithfulness and can lead to a useful analysis of NMT model behaviour and more trustworthy attention heatmaps. Our proposed objective improves faithfulness without reducing the translation quality and has a useful regularization effect on the NMT model and can even improve translation quality in some cases",
    "checked": true,
    "id": "e09f1fc72cbd53db1a06831dda5e22d996c2f3c1",
    "semantic_title": "measuring and improving faithfulness of attention in neural machine translation",
    "citation_count": 13,
    "authors": [
      "Pooya Moradi",
      "Nishant Kambhatla",
      "Anoop Sarkar"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.244": {
    "title": "Progressively Pretrained Dense Corpus Index for Open-Domain Question Answering",
    "volume": "main",
    "abstract": "Commonly used information retrieval methods such as TF-IDF in open-domain question answering (QA) systems are insufficient to capture deep semantic matching that goes beyond lexical overlaps. Some recent studies consider the retrieval process as maximum inner product search (MIPS) using dense question and paragraph representations, achieving promising results on several information-seeking QA datasets. However, the pretraining of the dense vector representations is highly resource-demanding, e.g., requires a very large batch size and lots of training steps. In this work, we propose a sample-efficient method to pretrain the paragraph encoder. First, instead of using heuristically created pseudo question-paragraph pairs for pretraining, we use an existing pretrained sequence-to-sequence model to build a strong question generator that creates high-quality pretraining data. Second, we propose a simple progressive pretraining algorithm to ensure the existence of effective negative samples in each batch. Across three open-domain QA datasets, our method consistently outperforms a strong dense retrieval baseline that uses 6 times more computation for training. On two of the datasets, our method achieves more than 4-point absolute improvement in terms of answer exact match",
    "checked": true,
    "id": "469d92f195aebfa09e9b411ad92b3c879bcd1eba",
    "semantic_title": "progressively pretrained dense corpus index for open-domain question answering",
    "citation_count": 17,
    "authors": [
      "Wenhan Xiong",
      "Hong Wang",
      "William Yang Wang"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.245": {
    "title": "Exploring the Limits of Few-Shot Link Prediction in Knowledge Graphs",
    "volume": "main",
    "abstract": "Real-world knowledge graphs are often characterized by low-frequency relations—a challenge that has prompted an increasing interest in few-shot link prediction methods. These methods perform link prediction for a set of new relations, unseen during training, given only a few example facts of each relation at test time. In this work, we perform a systematic study on a spectrum of models derived by generalizing the current state of the art for few-shot link prediction, with the goal of probing the limits of learning in this few-shot setting. We find that a simple, zero-shot baseline — which ignores any relation-specific information — achieves surprisingly strong performance. Moreover, experiments on carefully crafted synthetic datasets show that having only a few examples of a relation fundamentally limits models from using fine-grained structural information and only allows for exploiting the coarse-grained positional information of entities. Together, our findings challenge the implicit assumptions and inductive biases of prior work and highlight new directions for research in this area",
    "checked": true,
    "id": "123db39bd51ac064f744e6e40450011a04b59de6",
    "semantic_title": "exploring the limits of few-shot link prediction in knowledge graphs",
    "citation_count": 8,
    "authors": [
      "Dora Jambor",
      "Komal Teru",
      "Joelle Pineau",
      "William L. Hamilton"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.246": {
    "title": "ProFormer: Towards On-Device LSH Projection Based Transformers",
    "volume": "main",
    "abstract": "At the heart of text based neural models lay word representations, which are powerful but occupy a lot of memory making it challenging to deploy to devices with memory constraints such as mobile phones, watches and IoT. To surmount these challenges, we introduce ProFormer – a projection based transformer architecture that is faster and lighter making it suitable to deploy to memory constraint devices and preserve user privacy. We use LSH projection layer to dynamically generate word representations on-the-fly without embedding lookup tables leading to significant memory footprint reduction from O(V.d) to O(T), where V is the vocabulary size, d is the embedding dimension size and T is the dimension of the LSH projection representation. We also propose a local projection attention (LPA) layer, which uses self-attention to transform the input sequence of N LSH word projections into a sequence of N/K representations reducing the computations quadratically by O(Kˆ2). We evaluate ProFormer on multiple text classification tasks and observed improvements over prior state-of-the-art on-device approaches for short text classification and comparable performance for long text classification tasks. ProFormer is also competitive with other popular but highly resource-intensive approaches like BERT and even outperforms small-sized BERT variants with significant resource savings – reduces the embedding memory footprint from 92.16 MB to 1.7 KB and requires 16x less computation overhead, which is very impressive making it the fastest and smallest on-device model",
    "checked": true,
    "id": "05a6fa3bb47bd283c95b3d86ba4cbbc471cbd6ac",
    "semantic_title": "proformer: towards on-device lsh projection based transformers",
    "citation_count": 9,
    "authors": [
      "Chinnadhurai Sankar",
      "Sujith Ravi",
      "Zornitsa Kozareva"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.247": {
    "title": "Joint Learning of Hyperbolic Label Embeddings for Hierarchical Multi-label Classification",
    "volume": "main",
    "abstract": "We consider the problem of multi-label classification, where the labels lie on a hierarchy. However, unlike most existing works in hierarchical multi-label classification, we do not assume that the label-hierarchy is known. Encouraged by the recent success of hyperbolic embeddings in capturing hierarchical relations, we propose to jointly learn the classifier parameters as well as the label embeddings. Such a joint learning is expected to provide a twofold advantage: i) the classifier generalises better as it leverages the prior knowledge of existence of a hierarchy over the labels, and ii) in addition to the label co-occurrence information, the label-embedding may benefit from the manifold structure of the input datapoints, leading to embeddings that are more faithful to the label hierarchy. We propose a novel formulation for the joint learning and empirically evaluate its efficacy. The results show that the joint learning improves over the baseline that employs label co-occurrence based pre-trained hyperbolic embeddings. Moreover, the proposed classifiers achieve state-of-the-art generalization on standard benchmarks. We also present evaluation of the hyperbolic embeddings obtained by joint learning and show that they represent the hierarchy more accurately than the other alternatives",
    "checked": true,
    "id": "9220c14828514320c32a57e0a4c68f1df9261c3a",
    "semantic_title": "joint learning of hyperbolic label embeddings for hierarchical multi-label classification",
    "citation_count": 11,
    "authors": [
      "Soumya Chatterjee",
      "Ayush Maheshwari",
      "Ganesh Ramakrishnan",
      "Saketha Nath Jagarlapudi"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.248": {
    "title": "Segmenting Subtitles for Correcting ASR Segmentation Errors",
    "volume": "main",
    "abstract": "Typical ASR systems segment the input audio into utterances using purely acoustic information, which may not resemble the sentence-like units that are expected by conventional machine translation (MT) systems for Spoken Language Translation. In this work, we propose a model for correcting the acoustic segmentation of ASR models for low-resource languages to improve performance on downstream tasks. We propose the use of subtitles as a proxy dataset for correcting ASR acoustic segmentation, creating synthetic acoustic utterances by modeling common error modes. We train a neural tagging model for correcting ASR acoustic segmentation and show that it improves downstream performance on MT and audio-document cross-language information retrieval (CLIR)",
    "checked": true,
    "id": "701f0a4cc4fd5af095ba5c8a30e0aa9df0529941",
    "semantic_title": "segmenting subtitles for correcting asr segmentation errors",
    "citation_count": 4,
    "authors": [
      "David Wan",
      "Chris Kedzie",
      "Faisal Ladhak",
      "Elsbeth Turcan",
      "Petra Galuscakova",
      "Elena Zotkina",
      "Zhengping Jiang",
      "Peter Bell",
      "Kathleen McKeown"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.249": {
    "title": "Crisscrossed Captions: Extended Intramodal and Intermodal Semantic Similarity Judgments for MS-COCO",
    "volume": "main",
    "abstract": "By supporting multi-modal retrieval training and evaluation, image captioning datasets have spurred remarkable progress on representation learning. Unfortunately, datasets have limited cross-modal associations: images are not paired with other images, captions are only paired with other captions of the same image, there are no negative associations and there are missing positive cross-modal associations. This undermines research into how inter-modality learning impacts intra-modality tasks. We address this gap with Crisscrossed Captions (CxC), an extension of the MS-COCO dataset with human semantic similarity judgments for 267,095 intra- and inter-modality pairs. We report baseline results on CxC for strong existing unimodal and multimodal models. We also evaluate a multitask dual encoder trained on both image-caption and caption-caption pairs that crucially demonstrates CxC's value for measuring the influence of intra- and inter-modality learning",
    "checked": true,
    "id": "3fc0cda3bfad305e9da00f3f564f5205e3833c75",
    "semantic_title": "crisscrossed captions: extended intramodal and intermodal semantic similarity judgments for ms-coco",
    "citation_count": 57,
    "authors": [
      "Zarana Parekh",
      "Jason Baldridge",
      "Daniel Cer",
      "Austin Waters",
      "Yinfei Yang"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.250": {
    "title": "On-Device Text Representations Robust To Misspellings via Projections",
    "volume": "main",
    "abstract": "Recently, there has been a strong interest in developing natural language applications that live on personal devices such as mobile phones, watches and IoT with the objective to preserve user privacy and have low memory. Advances in Locality-Sensitive Hashing (LSH)-based projection networks have demonstrated state-of-the-art performance in various classification tasks without explicit word (or word-piece) embedding lookup tables by computing on-the-fly text representations. In this paper, we show that the projection based neural classifiers are inherently robust to misspellings and perturbations of the input text. We empirically demonstrate that the LSH projection based classifiers are more robust to common misspellings compared to BiLSTMs (with both word-piece & word-only tokenization) and fine-tuned BERT based methods. When subject to misspelling attacks, LSH projection based classifiers had a small average accuracy drop of 2.94% across multiple classifications tasks, while the fine-tuned BERT model accuracy had a significant drop of 11.44%",
    "checked": true,
    "id": "1531ffc206bdb310ae08adc02e3df44d1d125d99",
    "semantic_title": "on-device text representations robust to misspellings via projections",
    "citation_count": 4,
    "authors": [
      "Chinnadhurai Sankar",
      "Sujith Ravi",
      "Zornitsa Kozareva"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.251": {
    "title": "ENPAR:Enhancing Entity and Entity Pair Representations for Joint Entity Relation Extraction",
    "volume": "main",
    "abstract": "Current state-of-the-art systems for joint entity relation extraction (Luan et al., 2019; Wad-den et al., 2019) usually adopt the multi-task learning framework. However, annotations for these additional tasks such as coreference resolution and event extraction are always equally hard (or even harder) to obtain. In this work, we propose a pre-training method ENPAR to improve the joint extraction performance. ENPAR requires only the additional entity annotations that are much easier to collect. Unlike most existing works that only consider incorporating entity information into the sentence encoder, we further utilize the entity pair information. Specifically, we devise four novel objectives,i.e., masked entity typing, masked entity prediction, adversarial context discrimination, and permutation prediction, to pre-train an entity encoder and an entity pair encoder. Comprehensive experiments show that the proposed pre-training method achieves significant improvement over BERT on ACE05, SciERC, and NYT, and outperforms current state-of-the-art on ACE05",
    "checked": true,
    "id": "6910538bed71aa00b37f8ee65ffe10cf5e6452c2",
    "semantic_title": "enpar:enhancing entity and entity pair representations for joint entity relation extraction",
    "citation_count": 13,
    "authors": [
      "Yijun Wang",
      "Changzhi Sun",
      "Yuanbin Wu",
      "Hao Zhou",
      "Lei Li",
      "Junchi Yan"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.252": {
    "title": "Text Augmentation in a Multi-Task View",
    "volume": "main",
    "abstract": "Traditional data augmentation aims to increase the coverage of the input distribution by generating augmented examples that strongly resemble original samples in an online fashion where augmented examples dominate training. In this paper, we propose an alternative perspective—a multi-task view (MTV) of data augmentation—in which the primary task trains on original examples and the auxiliary task trains on augmented examples. In MTV data augmentation, both original and augmented samples are weighted substantively during training, relaxing the constraint that augmented examples must resemble original data and thereby allowing us to apply stronger augmentation functions. In empirical experiments using four common data augmentation techniques on three benchmark text classification datasets, we find that using the MTV leads to higher and more robust performance than traditional augmentation",
    "checked": true,
    "id": "bd0a7c8f4d5a7460a44883b8e9fc81e654eaa4b8",
    "semantic_title": "text augmentation in a multi-task view",
    "citation_count": 11,
    "authors": [
      "Jason Wei",
      "Chengyu Huang",
      "Shiqi Xu",
      "Soroush Vosoughi"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.253": {
    "title": "Representations for Question Answering from Documents with Tables and Text",
    "volume": "main",
    "abstract": "Tables in web documents are pervasive and can be directly used to answer many of the queries searched on the web, motivating their integration in question answering. Very often information presented in tables is succinct and hard to interpret with standard language representations. On the other hand, tables often appear within textual context, such as an article describing the table. Using the information from an article as additional context can potentially enrich table representations. In this work we aim to improve question answering from tables by refining table representations based on information from surrounding text. We also present an effective method to combine text and table-based predictions for question answering from full documents, obtaining significant improvements on the Natural Questions dataset (Kwiatkowski et al., 2019)",
    "checked": true,
    "id": "0477ef522fa3ec861a9ff5ba35c96cc08ba3697f",
    "semantic_title": "representations for question answering from documents with tables and text",
    "citation_count": 35,
    "authors": [
      "Vicky Zayats",
      "Kristina Toutanova",
      "Mari Ostendorf"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.254": {
    "title": "PPT: Parsimonious Parser Transfer for Unsupervised Cross-Lingual Adaptation",
    "volume": "main",
    "abstract": "Cross-lingual transfer is a leading technique for parsing low-resource languages in the absence of explicit supervision. Simple ‘direct transfer' of a learned model based on a multilingual input encoding has provided a strong benchmark. This paper presents a method for unsupervised cross-lingual transfer that improves over direct transfer systems by using their output as implicit supervision as part of self-training on unlabelled text in the target language. The method assumes minimal resources and provides maximal flexibility by (a) accepting any pre-trained arc-factored dependency parser; (b) assuming no access to source language data; (c) supporting both projective and non-projective parsing; and (d) supporting multi-source transfer. With English as the source language, we show significant improvements over state-of-the-art transfer models on both distant and nearby languages, despite our conceptually simpler approach. We provide analyses of the choice of source languages for multi-source transfer, and the advantage of non-projective parsing. Our code is available online",
    "checked": true,
    "id": "f9cf62a5abae8118fcf70f52793556c285b958db",
    "semantic_title": "ppt: parsimonious parser transfer for unsupervised cross-lingual adaptation",
    "citation_count": 13,
    "authors": [
      "Kemal Kurniawan",
      "Lea Frermann",
      "Philip Schulz",
      "Trevor Cohn"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.255": {
    "title": "Modelling Context Emotions using Multi-task Learning for Emotion Controlled Dialog Generation",
    "volume": "main",
    "abstract": "A recent topic of research in natural language generation has been the development of automatic response generation modules that can automatically respond to a user's utterance in an empathetic manner. Previous research has tackled this task using neural generative methods by augmenting emotion classes with the input sequences. However, the outputs by these models may be inconsistent. We employ multi-task learning to predict the emotion label and to generate a viable response for a given utterance using a common encoder with multiple decoders. Our proposed encoder-decoder model consists of a self-attention based encoder and a decoder with dot product attention mechanism to generate response with a specified emotion. We use the focal loss to handle imbalanced data distribution, and utilize the consistency loss to allow coherent decoding by the decoders. Human evaluation reveals that our model produces more emotionally pertinent responses. In addition, our model outperforms multiple strong baselines on automatic evaluation measures such as F1 and BLEU scores, thus resulting in more fluent and adequate responses",
    "checked": true,
    "id": "72d9f5c10a0ab2f9a857b161200acc0dbbb2ed95",
    "semantic_title": "modelling context emotions using multi-task learning for emotion controlled dialog generation",
    "citation_count": 11,
    "authors": [
      "Deeksha Varshney",
      "Asif Ekbal",
      "Pushpak Bhattacharyya"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.256": {
    "title": "Gender and Racial Fairness in Depression Research using Social Media",
    "volume": "main",
    "abstract": "Multiple studies have demonstrated that behaviors expressed on online social media platforms can indicate the mental health state of an individual. The widespread availability of such data has spurred interest in mental health research, using several datasets where individuals are labeled with mental health conditions. While previous research has raised concerns about possible biases in models produced from this data, no study has investigated how these biases manifest themselves with regards to demographic groups in data, such as gender and racial/ethnic groups. Here, we analyze the fairness of depression classifiers trained on Twitter data with respect to gender and racial demographic groups. We find that model performance differs for underrepresented groups, and we investigate sources of these biases beyond data representation. Our study results in recommendations on how to avoid these biases in future research",
    "checked": true,
    "id": "59cc744824163b5b8663e26bea0eb0ba2d2be9df",
    "semantic_title": "gender and racial fairness in depression research using social media",
    "citation_count": 33,
    "authors": [
      "Carlos Aguirre",
      "Keith Harrigian",
      "Mark Dredze"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.257": {
    "title": "MTOP: A Comprehensive Multilingual Task-Oriented Semantic Parsing Benchmark",
    "volume": "main",
    "abstract": "Scaling semantic parsing models for task-oriented dialog systems to new languages is often expensive and time-consuming due to the lack of available datasets. Available datasets suffer from several shortcomings: a) they contain few languages b) they contain small amounts of labeled examples per language c) they are based on the simple intent and slot detection paradigm for non-compositional queries. In this paper, we present a new multilingual dataset, called MTOP, comprising of 100k annotated utterances in 6 languages across 11 domains. We use this dataset and other publicly available datasets to conduct a comprehensive benchmarking study on using various state-of-the-art multilingual pre-trained models for task-oriented semantic parsing. We achieve an average improvement of +6.3 points on Slot F1 for the two existing multilingual datasets, over best results reported in their experiments. Furthermore, we demonstrate strong zero-shot performance using pre-trained models combined with automatic translation and alignment, and a proposed distant supervision method to reduce the noise in slot label projection",
    "checked": true,
    "id": "a8a168d53e01b0c35d626cfced103656e22b8343",
    "semantic_title": "mtop: a comprehensive multilingual task-oriented semantic parsing benchmark",
    "citation_count": 151,
    "authors": [
      "Haoran Li",
      "Abhinav Arora",
      "Shuohui Chen",
      "Anchit Gupta",
      "Sonal Gupta",
      "Yashar Mehdad"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.258": {
    "title": "Adapting Event Extractors to Medical Data: Bridging the Covariate Shift",
    "volume": "main",
    "abstract": "We tackle the task of adapting event extractors to new domains without labeled data, by aligning the marginal distributions of source and target domains. As a testbed, we create two new event extraction datasets using English texts from two medical domains: (i) clinical notes, and (ii) doctor-patient conversations. We test the efficacy of three marginal alignment techniques: (i) adversarial domain adaptation (ADA), (ii) domain adaptive fine-tuning (DAFT), and (iii) a new instance weighting technique based on language model likelihood scores (LIW). LIW and DAFT improve over a no-transfer BERT baseline on both domains, but ADA only improves on notes. Deeper analysis of performance under different types of shifts (e.g., lexical shift, semantic shift) explains some of the variations among models. Our best-performing models reach F1 scores of 70.0 and 72.9 on notes and conversations respectively, using no labeled target data",
    "checked": true,
    "id": "bb0f81b4dfef2ccf0e2f92f883e96b952ae986a7",
    "semantic_title": "adapting event extractors to medical data: bridging the covariate shift",
    "citation_count": 3,
    "authors": [
      "Aakanksha Naik",
      "Jill Fain Lehman",
      "Carolyn Rose"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.259": {
    "title": "NoiseQA: Challenge Set Evaluation for User-Centric Question Answering",
    "volume": "main",
    "abstract": "When Question-Answering (QA) systems are deployed in the real world, users query them through a variety of interfaces, such as speaking to voice assistants, typing questions into a search engine, or even translating questions to languages supported by the QA system. While there has been significant community attention devoted to identifying correct answers in passages assuming a perfectly formed question, we show that components in the pipeline that precede an answering engine can introduce varied and considerable sources of error, and performance can degrade substantially based on these upstream noise sources even for powerful pre-trained QA models. We conclude that there is substantial room for progress before QA systems can be effectively deployed, highlight the need for QA evaluation to expand to consider real-world use, and hope that our findings will spur greater community interest in the issues that arise when our systems actually need to be of utility to humans",
    "checked": true,
    "id": "ab1e5a3c5521b6204dc7c6f1fa72b88000bc30ee",
    "semantic_title": "noiseqa: challenge set evaluation for user-centric question answering",
    "citation_count": 31,
    "authors": [
      "Abhilasha Ravichander",
      "Siddharth Dalmia",
      "Maria Ryskina",
      "Florian Metze",
      "Eduard Hovy",
      "Alan W Black"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.260": {
    "title": "Co-evolution of language and agents in referential games",
    "volume": "main",
    "abstract": "Referential games offer a grounded learning environment for neural agents which accounts for the fact that language is functionally used to communicate. However, they do not take into account a second constraint considered to be fundamental for the shape of human language: that it must be learnable by new language learners. Cogswell et al. (2019) introduced cultural transmission within referential games through a changing population of agents to constrain the emerging language to be learnable. However, the resulting languages remain inherently biased by the agents' underlying capabilities. In this work, we introduce Language Transmission Simulator to model both cultural and architectural evolution in a population of agents. As our core contribution, we empirically show that the optimal situation is to take into account also the learning biases of the language learners and thus let language and agents co-evolve. When we allow the agent population to evolve through architectural evolution, we achieve across the board improvements on all considered metrics and surpass the gains made with cultural transmission. These results stress the importance of studying the underlying agent architecture and pave the way to investigate the co-evolution of language and agent in language emergence studies",
    "checked": true,
    "id": "f96ca05a594cca16748092350d4ee591711ab3e6",
    "semantic_title": "co-evolution of language and agents in referential games",
    "citation_count": 34,
    "authors": [
      "Gautier Dagan",
      "Dieuwke Hupkes",
      "Elia Bruni"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.261": {
    "title": "Modeling Context in Answer Sentence Selection Systems on a Latency Budget",
    "volume": "main",
    "abstract": "Answer Sentence Selection (AS2) is an efficient approach for the design of open-domain Question Answering (QA) systems. In order to achieve low latency, traditional AS2 models score question-answer pairs individually, ignoring any information from the document each potential answer was extracted from. In contrast, more computationally expensive models designed for machine reading comprehension tasks typically receive one or more passages as input, which often results in better accuracy. In this work, we present an approach to efficiently incorporate contextual information in AS2 models. For each answer candidate, we first use unsupervised similarity techniques to extract relevant sentences from its source document, which we then feed into an efficient transformer architecture fine-tuned for AS2. Our best approach, which leverages a multi-way attention architecture to efficiently encode context, improves 6% to 11% over non-contextual state of the art in AS2 with minimal impact on system latency. All experiments in this work were conducted in English",
    "checked": true,
    "id": "1e02b4c58e9b2ca6e35ca624de8921bf6f9a9c88",
    "semantic_title": "modeling context in answer sentence selection systems on a latency budget",
    "citation_count": 14,
    "authors": [
      "Rujun Han",
      "Luca Soldaini",
      "Alessandro Moschitti"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.262": {
    "title": "Syntax-BERT: Improving Pre-trained Transformers with Syntax Trees",
    "volume": "main",
    "abstract": "Pre-trained language models like BERT achieve superior performances in various NLP tasks without explicit consideration of syntactic information. Meanwhile, syntactic information has been proved to be crucial for the success of NLP applications. However, how to incorporate the syntax trees effectively and efficiently into pre-trained Transformers is still unsettled. In this paper, we address this problem by proposing a novel framework named Syntax-BERT. This framework works in a plug-and-play mode and is applicable to an arbitrary pre-trained checkpoint based on Transformer architecture. Experiments on various datasets of natural language understanding verify the effectiveness of syntax trees and achieve consistent improvement over multiple pre-trained models, including BERT, RoBERTa, and T5",
    "checked": true,
    "id": "5733233ea20498eba4afbe085038a067d4582f21",
    "semantic_title": "syntax-bert: improving pre-trained transformers with syntax trees",
    "citation_count": 73,
    "authors": [
      "Jiangang Bai",
      "Yujing Wang",
      "Yiren Chen",
      "Yaming Yang",
      "Jing Bai",
      "Jing Yu",
      "Yunhai Tong"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.263": {
    "title": "DISK-CSV: Distilling Interpretable Semantic Knowledge with a Class Semantic Vector",
    "volume": "main",
    "abstract": "Neural networks (NN) applied to natural language processing (NLP) are becoming deeper and more complex, making them increasingly difficult to understand and interpret. Even in applications of limited scope on fixed data, the creation of these complex \"black-boxes\" creates substantial challenges for debugging, understanding, and generalization. But rapid development in this field has now lead to building more straightforward and interpretable models. We propose a new technique (DISK-CSV) to distill knowledge concurrently from any neural network architecture for text classification, captured as a lightweight interpretable/explainable classifier. Across multiple datasets, our approach achieves better performance than the target black-box. In addition, our approach provides better explanations than existing techniques",
    "checked": true,
    "id": "ec0f1549e6e51d741e2037f172f7d1d0c4232455",
    "semantic_title": "disk-csv: distilling interpretable semantic knowledge with a class semantic vector",
    "citation_count": 2,
    "authors": [
      "Housam Khalifa Bashier",
      "Mi-Young Kim",
      "Randy Goebel"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.264": {
    "title": "Attention Can Reflect Syntactic Structure (If You Let It)",
    "volume": "main",
    "abstract": "Since the popularization of the Transformer as a general-purpose feature encoder for NLP, many studies have attempted to decode linguistic structure from its novel multi-head attention mechanism. However, much of such work focused almost exclusively on English — a language with rigid word order and a lack of inflectional morphology. In this study, we present decoding experiments for multilingual BERT across 18 languages in order to test the generalizability of the claim that dependency syntax is reflected in attention patterns. We show that full trees can be decoded above baseline accuracy from single attention heads, and that individual relations are often tracked by the same heads across languages. Furthermore, in an attempt to address recent debates about the status of attention as an explanatory mechanism, we experiment with fine-tuning mBERT on a supervised parsing objective while freezing different series of parameters. Interestingly, in steering the objective to learn explicit linguistic structure, we find much of the same structure represented in the resulting attention patterns, with interesting differences with respect to which parameters are frozen",
    "checked": true,
    "id": "0e6165ec3151e7e758d4ac90e0e009a4e3bbbebd",
    "semantic_title": "attention can reflect syntactic structure (if you let it)",
    "citation_count": 31,
    "authors": [
      "Vinit Ravishankar",
      "Artur Kulmizev",
      "Mostafa Abdou",
      "Anders Søgaard",
      "Joakim Nivre"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.265": {
    "title": "Extractive Summarization Considering Discourse and Coreference Relations based on Heterogeneous Graph",
    "volume": "main",
    "abstract": "Modeling the relations between text spans in a document is a crucial yet challenging problem for extractive summarization. Various kinds of relations exist among text spans of different granularity, such as discourse relations between elementary discourse units and coreference relations between phrase mentions. In this paper, we propose a heterogeneous graph based model for extractive summarization that incorporates both discourse and coreference relations. The heterogeneous graph contains three types of nodes, each corresponds to text spans of different granularity. Experimental results on a benchmark summarization dataset verify the effectiveness of our proposed method",
    "checked": true,
    "id": "471f2a7c86c5128f10afddc394007ffa0f8019fd",
    "semantic_title": "extractive summarization considering discourse and coreference relations based on heterogeneous graph",
    "citation_count": 20,
    "authors": [
      "Yin Jou Huang",
      "Sadao Kurohashi"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.266": {
    "title": "CDA: a Cost Efficient Content-based Multilingual Web Document Aligner",
    "volume": "main",
    "abstract": "We introduce a Content-based Document Alignment approach (CDA), an efficient method to align multilingual web documents based on content in creating parallel training data for machine translation (MT) systems operating at the industrial level. CDA works in two steps: (i) projecting documents of a web domain to a shared multilingual space; then (ii) aligning them based on the similarity of their representations in such space. We leverage lexical translation models to build vector representations using TF×IDF. CDA achieves performance comparable with state-of-the-art systems in the WMT-16 Bilingual Document Alignment Shared Task benchmark while operating in multilingual space. Besides, we created two web-scale datasets to examine the robustness of CDA in an industrial setting involving up to 28 languages and millions of documents. The experiments show that CDA is robust, cost-effective, and is significantly superior in (i) processing large and noisy web data and (ii) scaling to new and low-resourced languages",
    "checked": true,
    "id": "bb77396ce4f0d87c3f83b666d5abf885c113bf6d",
    "semantic_title": "cda: a cost efficient content-based multilingual web document aligner",
    "citation_count": 1,
    "authors": [
      "Thuy Vu",
      "Alessandro Moschitti"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.267": {
    "title": "Metric-Type Identification for Multi-Level Header Numerical Tables in Scientific Papers",
    "volume": "main",
    "abstract": "Numerical tables are widely used to present experimental results in scientific papers. For table understanding, a metric-type is essential to discriminate numbers in the tables. We introduce a new information extraction task, metric-type identification from multi-level header numerical tables, and provide a dataset extracted from scientific papers consisting of header tables, captions, and metric-types. We then propose two joint-learning neural classification and generation schemes featuring pointer-generator-based and BERT-based models. Our results show that the joint models can handle both in-header and out-of-header metric-type identification problems",
    "checked": true,
    "id": "cbfc1def130b8d24e5a7e183af1fb219c38f5e5d",
    "semantic_title": "metric-type identification for multi-level header numerical tables in scientific papers",
    "citation_count": 1,
    "authors": [
      "Lya Hulliyyatus Suadaa",
      "Hidetaka Kamigaito",
      "Manabu Okumura",
      "Hiroya Takamura"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.268": {
    "title": "EmpathBERT: A BERT-based Framework for Demographic-aware Empathy Prediction",
    "volume": "main",
    "abstract": "Affect preferences vary with user demographics, and tapping into demographic information provides important cues about the users' language preferences. In this paper, we utilize the user demographics and propose EmpathBERT, a demographic-aware framework for empathy prediction based on BERT. Through several comparative experiments, we show that EmpathBERT surpasses traditional machine learning and deep learning models, and illustrate the importance of user demographics, for predicting empathy and distress in user responses to stimulative news articles. We also highlight the importance of affect information in the responses by developing affect-aware models to predict user demographic attributes",
    "checked": true,
    "id": "04e556e0dec3886ca29df95931478e91dcbbd4ee",
    "semantic_title": "empathbert: a bert-based framework for demographic-aware empathy prediction",
    "citation_count": 30,
    "authors": [
      "Bhanu Prakash Reddy Guda",
      "Aparna Garimella",
      "Niyati Chhaya"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.269": {
    "title": "Are Neural Networks Extracting Linguistic Properties or Memorizing Training Data? An Observation with a Multilingual Probe for Predicting Tense",
    "volume": "main",
    "abstract": "We evaluate the ability of Bert embeddings to represent tense information, taking French and Chinese as a case study. In French, the tense information is expressed by verb morphology and can be captured by simple surface information. On the contrary, tense interpretation in Chinese is driven by abstract, lexical, syntactic and even pragmatic information. We show that while French tenses can easily be predicted from sentence representations, results drop sharply for Chinese, which suggests that Bert is more likely to memorize shallow patterns from the training data rather than uncover abstract properties",
    "checked": true,
    "id": "a9423f8152e72b964ceadda9f7674a9aaebfea41",
    "semantic_title": "are neural networks extracting linguistic properties or memorizing training data? an observation with a multilingual probe for predicting tense",
    "citation_count": 4,
    "authors": [
      "Bingzhi Li",
      "Guillaume Wisniewski"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.270": {
    "title": "Is Supervised Syntactic Parsing Beneficial for Language Understanding Tasks? An Empirical Investigation",
    "volume": "main",
    "abstract": "Traditional NLP has long held (supervised) syntactic parsing necessary for successful higher-level semantic language understanding (LU). The recent advent of end-to-end neural models, self-supervised via language modeling (LM), and their success on a wide range of LU tasks, however, questions this belief. In this work, we empirically investigate the usefulness of supervised parsing for semantic LU in the context of LM-pretrained transformer networks. Relying on the established fine-tuning paradigm, we first couple a pretrained transformer with a biaffine parsing head, aiming to infuse explicit syntactic knowledge from Universal Dependencies treebanks into the transformer. We then fine-tune the model for LU tasks and measure the effect of the intermediate parsing training (IPT) on downstream LU task performance. Results from both monolingual English and zero-shot language transfer experiments (with intermediate target-language parsing) show that explicit formalized syntax, injected into transformers through IPT, has very limited and inconsistent effect on downstream LU performance. Our results, coupled with our analysis of transformers' representation spaces before and after intermediate parsing, make a significant step towards providing answers to an essential question: how (un)availing is supervised parsing for high-level semantic natural language understanding in the era of large neural models?",
    "checked": true,
    "id": "8656e5a885613f29f0f2f35589d865baeb1317a6",
    "semantic_title": "is supervised syntactic parsing beneficial for language understanding tasks? an empirical investigation",
    "citation_count": 61,
    "authors": [
      "Goran Glavaš",
      "Ivan Vulić"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.271": {
    "title": "Facilitating Terminology Translation with Target Lemma Annotations",
    "volume": "main",
    "abstract": "Most of the recent work on terminology integration in machine translation has assumed that terminology translations are given already inflected in forms that are suitable for the target language sentence. In day-to-day work of professional translators, however, it is seldom the case as translators work with bilingual glossaries where terms are given in their dictionary forms; finding the right target language form is part of the translation process. We argue that the requirement for apriori specified target language forms is unrealistic and impedes the practical applicability of previous work. In this work, we propose to train machine translation systems using a source-side data augmentation method that annotates randomly selected source language words with their target language lemmas. We show that systems trained on such augmented data are readily usable for terminology integration in real-life translation scenarios. Our experiments on terminology translation into the morphologically complex Baltic and Uralic languages show an improvement of up to 7 BLEU points over baseline systems with no means for terminology integration and an average improvement of 4 BLEU points over the previous work. Results of the human evaluation indicate a 47.7% absolute improvement over the previous work in term translation accuracy when translating into Latvian",
    "checked": true,
    "id": "148ca5a3320cd4a5a2550f5b2b656273f83c53b9",
    "semantic_title": "facilitating terminology translation with target lemma annotations",
    "citation_count": 36,
    "authors": [
      "Toms Bergmanis",
      "Mārcis Pinnis"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.272": {
    "title": "Enhancing Sequence-to-Sequence Neural Lemmatization with External Resources",
    "volume": "main",
    "abstract": "We propose a novel hybrid approach to lemmatization that enhances the seq2seq neural model with additional lemmas extracted from an external lexicon or a rule-based system. During training, the enhanced lemmatizer learns both to generate lemmas via a sequential decoder and copy the lemma characters from the external candidates supplied during run-time. Our lemmatizer enhanced with candidates extracted from the Apertium morphological analyzer achieves statistically significant improvements compared to baseline models not utilizing additional lemma information, achieves an average accuracy of 97.25% on a set of 23 UD languages, which is 0.55% higher than obtained with the Stanford Stanza model on the same set of languages. We also compare with other methods of integrating external data into lemmatization and show that our enhanced system performs considerably better than a simple lexicon extension method based on the Stanza system, and it achieves complementary improvements w.r.t. the data augmentation method",
    "checked": true,
    "id": "ad7b5f309287746cbc3b861db7b26ae7264e450c",
    "semantic_title": "enhancing sequence-to-sequence neural lemmatization with external resources",
    "citation_count": 4,
    "authors": [
      "Kirill Milintsevich",
      "Kairit Sirts"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.273": {
    "title": "Summarising Historical Text in Modern Languages",
    "volume": "main",
    "abstract": "We introduce the task of historical text summarisation, where documents in historical forms of a language are summarised in the corresponding modern language. This is a fundamentally important routine to historians and digital humanities researchers but has never been automated. We compile a high-quality gold-standard text summarisation dataset, which consists of historical German and Chinese news from hundreds of years ago summarised in modern German or Chinese. Based on cross-lingual transfer learning techniques, we propose a summarisation model that can be trained even with no cross-lingual (historical to modern) parallel data, and further benchmark it against state-of-the-art algorithms. We report automatic and human evaluations that distinguish the historic to modern language summarisation task from standard cross-lingual summarisation (i.e., modern to modern language), highlight the distinctness and value of our dataset, and demonstrate that our transfer learning approach outperforms standard cross-lingual benchmarks on this task",
    "checked": true,
    "id": "d8d12c922fc571d081bae27c67fcf50cdbb17d90",
    "semantic_title": "summarising historical text in modern languages",
    "citation_count": 10,
    "authors": [
      "Xutan Peng",
      "Yi Zheng",
      "Chenghua Lin",
      "Advaith Siddharthan"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.274": {
    "title": "Challenges in Automated Debiasing for Toxic Language Detection",
    "volume": "main",
    "abstract": "Biased associations have been a challenge in the development of classifiers for detecting toxic language, hindering both fairness and accuracy. As potential solutions, we investigate recently introduced debiasing methods for text classification datasets and models, as applied to toxic language detection. Our focus is on lexical (e.g., swear words, slurs, identity mentions) and dialectal markers (specifically African American English). Our comprehensive experiments establish that existing methods are limited in their ability to prevent biased behavior in current toxicity detectors. We then propose an automatic, dialect-aware data correction method, as a proof-of-concept. Despite the use of synthetic labels, this method reduces dialectal associations with toxicity. Overall, our findings show that debiasing a model trained on biased toxic language data is not as effective as simply relabeling the data to remove existing biases",
    "checked": true,
    "id": "3808e5664541434f1aa0df2bb18ccd06cb20fd73",
    "semantic_title": "challenges in automated debiasing for toxic language detection",
    "citation_count": 125,
    "authors": [
      "Xuhui Zhou",
      "Maarten Sap",
      "Swabha Swayamdipta",
      "Yejin Choi",
      "Noah Smith"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.275": {
    "title": "Adaptive Fusion Techniques for Multimodal Data",
    "volume": "main",
    "abstract": "Effective fusion of data from multiple modalities, such as video, speech, and text, is challenging due to the heterogeneous nature of multimodal data. In this paper, we propose adaptive fusion techniques that aim to model context from different modalities effectively. Instead of defining a deterministic fusion operation, such as concatenation, for the network, we let the network decide \"how\" to combine a given set of multimodal features more effectively. We propose two networks: 1) Auto-Fusion, which learns to compress information from different modalities while preserving the context, and 2) GAN-Fusion, which regularizes the learned latent space given context from complementing modalities. A quantitative evaluation on the tasks of multimodal machine translation and emotion recognition suggests that our lightweight, adaptive networks can better model context from other modalities than existing methods, many of which employ massive transformer-based networks",
    "checked": true,
    "id": "306b8fa6bc423453f76bb6783459e513c2d00021",
    "semantic_title": "adaptive fusion techniques for multimodal data",
    "citation_count": 18,
    "authors": [
      "Gaurav Sahu",
      "Olga Vechtomova"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.276": {
    "title": "Detecting Scenes in Fiction: A new Segmentation Task",
    "volume": "main",
    "abstract": "This paper introduces the novel task of scene segmentation on narrative texts and provides an annotated corpus, a discussion of the linguistic and narrative properties of the task and baseline experiments towards automatic solutions. A scene here is a segment of the text where time and discourse time are more or less equal, the narration focuses on one action and location and character constellations stay the same. The corpus we describe consists of German-language dime novels (550k tokens) that have been annotated in parallel, achieving an inter-annotator agreement of gamma = 0.7. Baseline experiments using BERT achieve an F1 score of 24%, showing that the task is very challenging. An automatic scene segmentation paves the way towards processing longer narrative texts like tales or novels by breaking them down into smaller, coherent and meaningful parts, which is an important stepping stone towards the reconstruction of plot in Computational Literary Studies but also can serve to improve tasks like coreference resolution",
    "checked": true,
    "id": "39d937b8c409c17c2c799521b55b333a52cdb5e4",
    "semantic_title": "detecting scenes in fiction: a new segmentation task",
    "citation_count": 23,
    "authors": [
      "Albin Zehe",
      "Leonard Konle",
      "Lea Katharina Dümpelmann",
      "Evelyn Gius",
      "Andreas Hotho",
      "Fotis Jannidis",
      "Lucas Kaufmann",
      "Markus Krug",
      "Frank Puppe",
      "Nils Reiter",
      "Annekea Schreiber",
      "Nathalie Wiedmer"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.277": {
    "title": "LESA: Linguistic Encapsulation and Semantic Amalgamation Based Generalised Claim Detection from Online Content",
    "volume": "main",
    "abstract": "The conceptualization of a claim lies at the core of argument mining. The segregation of claims is complex, owing to the divergence in textual syntax and context across different distributions. Another pressing issue is the unavailability of labeled unstructured text for experimentation. In this paper, we propose LESA, a framework which aims at advancing headfirst into expunging the former issue by assembling a source-independent generalized model that captures syntactic features through part-of-speech and dependency embeddings, as well as contextual features through a fine-tuned language model. We resolve the latter issue by annotating a Twitter dataset which aims at providing a testing ground on a large unstructured dataset. Experimental results show that LESA improves upon the state-of-the-art performance across six benchmark claim datasets by an average of 3 claim-F1 points for in-domain experiments and by 2 claim-F1 points for general-domain experiments. On our dataset too, LESA outperforms existing baselines by 1 claim-F1 point on the in-domain experiments and 2 claim-F1 points on the general-domain experiments. We also release comprehensive data annotation guidelines compiled during the annotation phase (which was missing in the current literature)",
    "checked": true,
    "id": "226d872898170b324f69c28726be12c669f0b138",
    "semantic_title": "lesa: linguistic encapsulation and semantic amalgamation based generalised claim detection from online content",
    "citation_count": 25,
    "authors": [
      "Shreya Gupta",
      "Parantak Singh",
      "Megha Sundriyal",
      "Md. Shad Akhtar",
      "Tanmoy Chakraborty"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.278": {
    "title": "Interpretability for Morphological Inflection: from Character-level Predictions to Subword-level Rules",
    "volume": "main",
    "abstract": "Neural models for morphological inflection have recently attained very high results. However, their interpretation remains challenging. Towards this goal, we propose a simple linguistically-motivated variant to the encoder-decoder model with attention. In our model, character-level cross-attention mechanism is complemented with a self-attention module over substrings of the input. We design a novel approach for pattern extraction from attention weights to interpret what the model learn. We apply our methodology to analyze the model's decisions on three typologically-different languages and find that a) our pattern extraction method applied to cross-attention weights uncovers variation in form of inflection morphemes, b) pattern extraction from self-attention shows triggers for such variation, c) both types of patterns are closely aligned with grammar inflection classes and class assignment criteria, for all three languages. Additionally, we find that the proposed encoder attention component leads to consistent performance improvements over a strong baseline",
    "checked": true,
    "id": "1087cdac3f0fbf1c9f8e3710b9560ebadd485c67",
    "semantic_title": "interpretability for morphological inflection: from character-level predictions to subword-level rules",
    "citation_count": 3,
    "authors": [
      "Tatyana Ruzsics",
      "Olga Sozinova",
      "Ximena Gutierrez-Vasques",
      "Tanja Samardzic"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.279": {
    "title": "Expanding, Retrieving and Infilling: Diversifying Cross-Domain Question Generation with Flexible Templates",
    "volume": "main",
    "abstract": "Sequence-to-sequence based models have recently shown promising results in generating high-quality questions. However, these models are also known to have main drawbacks such as lack of diversity and bad sentence structures. In this paper, we focus on question generation over SQL database and propose a novel framework by expanding, retrieving, and infilling that first incorporates flexible templates with a neural-based model to generate diverse expressions of questions with sentence structure guidance. Furthermore, a new activation/deactivation mechanism is proposed for template-based sequence-to-sequence generation, which learns to discriminate template patterns and content patterns, thus further improves generation quality. We conduct experiments on two large-scale cross-domain datasets. The experiments show that the superiority of our question generation method in producing more diverse questions while maintaining high quality and consistency under both automatic evaluation and human evaluation",
    "checked": true,
    "id": "b1c2e709a10fcaaa62e81ea0422045dcbb32c2b5",
    "semantic_title": "expanding, retrieving and infilling: diversifying cross-domain question generation with flexible templates",
    "citation_count": 14,
    "authors": [
      "Xiaojing Yu",
      "Anxiao Jiang"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.280": {
    "title": "Handling Out-Of-Vocabulary Problem in Hangeul Word Embeddings",
    "volume": "main",
    "abstract": "Word embedding is considered an essential factor in improving the performance of various Natural Language Processing (NLP) models. However, it is hardly applicable in real-world datasets as word embedding is generally studied with a well-refined corpus. Notably, in Hangeul (Korean writing system), which has a unique writing system, various kinds of Out-Of-Vocabulary (OOV) appear from typos. In this paper, we propose a robust Hangeul word embedding model against typos, while maintaining high performance. The proposed model utilizes a Convolutional Neural Network (CNN) architecture with a channel attention mechanism that learns to infer the original word embeddings. The model train with a dataset that consists of a mix of typos and correct words. To demonstrate the effectiveness of the proposed model, we conduct three kinds of intrinsic and extrinsic tasks. While the existing embedding models fail to maintain stable performance as the noise level increases, the proposed model shows stable performance",
    "checked": true,
    "id": "1babe379f8547a9dc43251e5c5a7bea4a3de5ea1",
    "semantic_title": "handling out-of-vocabulary problem in hangeul word embeddings",
    "citation_count": 7,
    "authors": [
      "Ohjoon Kwon",
      "Dohyun Kim",
      "Soo-Ryeon Lee",
      "Junyoung Choi",
      "SangKeun Lee"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.281": {
    "title": "Exploiting Multimodal Reinforcement Learning for Simultaneous Machine Translation",
    "volume": "main",
    "abstract": "This paper addresses the problem of simultaneous machine translation (SiMT) by exploring two main concepts: (a) adaptive policies to learn a good trade-off between high translation quality and low latency; and (b) visual information to support this process by providing additional (visual) contextual information which may be available before the textual input is produced. For that, we propose a multimodal approach to simultaneous machine translation using reinforcement learning, with strategies to integrate visual and textual information in both the agent and the environment. We provide an exploration on how different types of visual information and integration strategies affect the quality and latency of simultaneous translation models, and demonstrate that visual cues lead to higher quality while keeping the latency low",
    "checked": true,
    "id": "fe0426a9b547df79ef690cffb664a3c0db7d00f1",
    "semantic_title": "exploiting multimodal reinforcement learning for simultaneous machine translation",
    "citation_count": 8,
    "authors": [
      "Julia Ive",
      "Andy Mingren Li",
      "Yishu Miao",
      "Ozan Caglayan",
      "Pranava Madhyastha",
      "Lucia Specia"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.282": {
    "title": "STAR: Cross-modal [STA]tement [R]epresentation for selecting relevant mathematical premises",
    "volume": "main",
    "abstract": "Mathematical statements written in natural language are usually composed of two different modalities: mathematical elements and natural language. These two modalities have several distinct linguistic and semantic properties. State-of-the-art representation techniques have demonstrated an inability in capturing such an entangled style of discourse. In this work, we propose STAR, a model that uses cross-modal attention to learn how to represent mathematical text for the task of Natural Language Premise Selection. This task uses conjectures written in both natural and mathematical language to recommend premises that most likely will be relevant to prove a particular statement. We found that STAR not only outperforms baselines that do not distinguish between natural language and mathematical elements, but it also achieves better performance than state-of-the-art models",
    "checked": true,
    "id": "ec7017debbb9fb25c4aafa946d32bcea7573642c",
    "semantic_title": "star: cross-modal [sta]tement [r]epresentation for selecting relevant mathematical premises",
    "citation_count": 7,
    "authors": [
      "Deborah Ferreira",
      "André Freitas"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.283": {
    "title": "Do Multi-Hop Question Answering Systems Know How to Answer the Single-Hop Sub-Questions?",
    "volume": "main",
    "abstract": "Multi-hop question answering (QA) requires a model to retrieve and integrate information from multiple passages to answer a question. Rapid progress has been made on multi-hop QA systems with regard to standard evaluation metrics, including EM and F1. However, by simply evaluating the correctness of the answers, it is unclear to what extent these systems have learned the ability to perform multi-hop reasoning. In this paper, we propose an additional sub-question evaluation for the multi-hop QA dataset HotpotQA, in order to shed some light on explaining the reasoning process of QA systems in answering complex questions. We adopt a neural decomposition model to generate sub-questions for a multi-hop question, followed by extracting the corresponding sub-answers. Contrary to our expectation, multiple state-of-the-art multi-hop QA models fail to answer a large portion of sub-questions, although the corresponding multi-hop questions are correctly answered. Our work takes a step forward towards building a more explainable multi-hop QA system",
    "checked": true,
    "id": "94f4d2de078455d1335130a0057a93f29c153b59",
    "semantic_title": "do multi-hop question answering systems know how to answer the single-hop sub-questions?",
    "citation_count": 25,
    "authors": [
      "Yixuan Tang",
      "Hwee Tou Ng",
      "Anthony Tung"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.284": {
    "title": "Multilingual LAMA: Investigating Knowledge in Multilingual Pretrained Language Models",
    "volume": "main",
    "abstract": "Recently, it has been found that monolingual English language models can be used as knowledge bases. Instead of structural knowledge base queries, masked sentences such as \"Paris is the capital of [MASK]\" are used as probes. We translate the established benchmarks TREx and GoogleRE into 53 languages. Working with mBERT, we investigate three questions. (i) Can mBERT be used as a multilingual knowledge base? Most prior work only considers English. Extending research to multiple languages is important for diversity and accessibility. (ii) Is mBERT's performance as knowledge base language-independent or does it vary from language to language? (iii) A multilingual model is trained on more text, e.g., mBERT is trained on 104 Wikipedias. Can mBERT leverage this for better performance? We find that using mBERT as a knowledge base yields varying performance across languages and pooling predictions across languages improves performance. Conversely, mBERT exhibits a language bias; e.g., when queried in Italian, it tends to predict Italy as the country of origin",
    "checked": true,
    "id": "fcfc9648561a221750b8085790ad9ba1bebb1800",
    "semantic_title": "multilingual lama: investigating knowledge in multilingual pretrained language models",
    "citation_count": 85,
    "authors": [
      "Nora Kassner",
      "Philipp Dufter",
      "Hinrich Schütze"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.285": {
    "title": "Variational Weakly Supervised Sentiment Analysis with Posterior Regularization",
    "volume": "main",
    "abstract": "Sentiment analysis is an important task in natural language processing (NLP). Most of existing state-of-the-art methods are under the supervised learning paradigm. However, human annotations can be scarce. Thus, we should leverage more weak supervision for sentiment analysis. In this paper, we propose a posterior regularization framework for the variational approach to the weakly supervised sentiment analysis to better control the posterior distribution of the label assignment. The intuition behind the posterior regularization is that if extracted opinion words from two documents are semantically similar, the posterior distributions of two documents should be similar. Our experimental results show that the posterior regularization can improve the original variational approach to the weakly supervised sentiment analysis and the performance is more stable with smaller prediction variance",
    "checked": true,
    "id": "c76274c5b909916f65ed52fce46ae95f13993e93",
    "semantic_title": "variational weakly supervised sentiment analysis with posterior regularization",
    "citation_count": 0,
    "authors": [
      "Ziqian Zeng",
      "Yangqiu Song"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.286": {
    "title": "Framing Word Sense Disambiguation as a Multi-Label Problem for Model-Agnostic Knowledge Integration",
    "volume": "main",
    "abstract": "Recent studies treat Word Sense Disambiguation (WSD) as a single-label classification problem in which one is asked to choose only the best-fitting sense for a target word, given its context. However, gold data labelled by expert annotators suggest that maximizing the probability of a single sense may not be the most suitable training objective for WSD, especially if the sense inventory of choice is fine-grained. In this paper, we approach WSD as a multi-label classification problem in which multiple senses can be assigned to each target word. Not only does our simple method bear a closer resemblance to how human annotators disambiguate text, but it can also be seamlessly extended to exploit structured knowledge from semantic networks to achieve state-of-the-art results in English all-words WSD",
    "checked": true,
    "id": "597547f7a8f2ad53245088acb252cdac9939d8a2",
    "semantic_title": "framing word sense disambiguation as a multi-label problem for model-agnostic knowledge integration",
    "citation_count": 31,
    "authors": [
      "Simone Conia",
      "Roberto Navigli"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.287": {
    "title": "Graph-based Fake News Detection using a Summarization Technique",
    "volume": "main",
    "abstract": "Nowadays, fake news is spreading in various ways, and this fake information is causing a lot of social damages. Thus the need to detect fake information is increasing to prevent the damages caused by fake news. In this paper, we propose a novel graph-based fake news detection method using a summarization technique that uses only the document internal information. Our proposed method represents the relationship between all sentences using a graph and the reflection rate of contextual information among sentences is computed by using an attention mechanism. In addition, we improve the performance of fake news detection by utilizing summary information as an important subject of the document. The experimental results demonstrate that our method achieves high accuracy, 91.04%, that is 8.85%p better than the previous method",
    "checked": true,
    "id": "c4912c224363ae2622ab6a6c2b8a46a80458a7ff",
    "semantic_title": "graph-based fake news detection using a summarization technique",
    "citation_count": 8,
    "authors": [
      "Gihwan Kim",
      "Youngjoong Ko"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.288": {
    "title": "Cognition-aware Cognate Detection",
    "volume": "main",
    "abstract": "Automatic detection of cognates helps downstream NLP tasks of Machine Translation, Cross-lingual Information Retrieval, Computational Phylogenetics and Cross-lingual Named Entity Recognition. Previous approaches for the task of cognate detection use orthographic, phonetic and semantic similarity based features sets. In this paper, we propose a novel method for enriching the feature sets, with cognitive features extracted from human readers' gaze behaviour. We collect gaze behaviour data for a small sample of cognates and show that extracted cognitive features help the task of cognate detection. However, gaze data collection and annotation is a costly task. We use the collected gaze behaviour data to predict cognitive features for a larger sample and show that predicted cognitive features, also, significantly improve the task performance. We report improvements of 10% with the collected gaze features, and 12% using the predicted gaze features, over the previously proposed approaches. Furthermore, we release the collected gaze behaviour data along with our code and cross-lingual models",
    "checked": true,
    "id": "fdd78e80e9473caca19601c8e0c43f15134283cd",
    "semantic_title": "cognition-aware cognate detection",
    "citation_count": 10,
    "authors": [
      "Diptesh Kanojia",
      "Prashant Sharma",
      "Sayali Ghodekar",
      "Pushpak Bhattacharyya",
      "Gholamreza Haffari",
      "Malhar Kulkarni"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.289": {
    "title": "A Simple Three-Step Approach for the Automatic Detection of Exaggerated Statements in Health Science News",
    "volume": "main",
    "abstract": "There is a huge difference between a scientific journal reporting ‘wine consumption might be correlated to cancer', and a media outlet publishing ‘wine causes cancer' citing the journal's results. The above example is a typical case of a scientific statement being exaggerated as an outcome of the rising problem of media manipulation. Given a pair of statements (say one from the source journal article and the other from the news article covering the results published in the journal), is it possible to ascertain with some confidence whether one is an exaggerated version of the other? This paper presents a surprisingly simple yet rational three-step approach that performs best for this task. We solve the task by breaking it into three sub-tasks as follows – (a) given a statement from a scientific paper or press release, we first extract relation phrases (e.g., ‘causes' versus ‘might be correlated to') connecting the dependent (e.g., ‘cancer') and the independent (‘wine') variable, (b) classify the strength of the relationship phrase extracted and (c) compare the strengths of the relation phrases extracted from the statements to identify whether one statement contains an exaggerated version of the other, and to what extent. Through rigorous experiments, we demonstrate that our simple approach by far outperforms baseline models that compare state-of-the-art embedding of the statement pairs through a binary classifier or recast the problem as a textual entailment task, which appears to be a very natural choice in this settings",
    "checked": true,
    "id": "9c674caababd6aa1c4aa92de01a26201f690852c",
    "semantic_title": "a simple three-step approach for the automatic detection of exaggerated statements in health science news",
    "citation_count": 2,
    "authors": [
      "Jasabanta Patro",
      "Sabyasachee Baruah"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.290": {
    "title": "Modeling Coreference Relations in Visual Dialog",
    "volume": "main",
    "abstract": "Visual dialog is a vision-language task where an agent needs to answer a series of questions grounded in an image based on the understanding of the dialog history and the image. The occurrences of coreference relations in the dialog makes it a more challenging task than visual question-answering. Most previous works have focused on learning better multi-modal representations or on exploring different ways of fusing visual and language features, while the coreferences in the dialog are mainly ignored. In this paper, based on linguistic knowledge and discourse features of human dialog we propose two soft constraints that can improve the model's ability of resolving coreferences in dialog in an unsupervised way. Experimental results on the VisDial v1.0 dataset shows that our model, which integrates two novel and linguistically inspired soft constraints in a deep transformer neural architecture, obtains new state-of-the-art performance in terms of recall at 1 and other evaluation metrics compared to current existing models and this without pretraining on other vision language datasets. Our qualitative results also demonstrate the effectiveness of the method that we propose",
    "checked": true,
    "id": "1a347984c84393797d94ff98028e333216931439",
    "semantic_title": "modeling coreference relations in visual dialog",
    "citation_count": 8,
    "authors": [
      "Mingxiao Li",
      "Marie-Francine Moens"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.291": {
    "title": "Increasing Robustness to Spurious Correlations using Forgettable Examples",
    "volume": "main",
    "abstract": "Neural NLP models tend to rely on spurious correlations between labels and input features to perform their tasks. Minority examples, i.e., examples that contradict the spurious correlations present in the majority of data points, have been shown to increase the out-of-distribution generalization of pre-trained language models. In this paper, we first propose using example forgetting to find minority examples without prior knowledge of the spurious correlations present in the dataset. Forgettable examples are instances either learned and then forgotten during training or never learned. We show empirically how these examples are related to minorities in our training sets. Then, we introduce a new approach to robustify models by fine-tuning our models twice, first on the full training data and second on the minorities only. We obtain substantial improvements in out-of-distribution generalization when applying our approach to the MNLI, QQP and FEVER datasets",
    "checked": true,
    "id": "1f0e1657063ea38cf225eaf1c1187ae7b2e4a0e0",
    "semantic_title": "increasing robustness to spurious correlations using forgettable examples",
    "citation_count": 55,
    "authors": [
      "Yadollah Yaghoobzadeh",
      "Soroush Mehri",
      "Remi Tachet des Combes",
      "T. J. Hazen",
      "Alessandro Sordoni"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.292": {
    "title": "On Robustness of Neural Semantic Parsers",
    "volume": "main",
    "abstract": "Semantic parsing maps natural language (NL) utterances into logical forms (LFs), which underpins many advanced NLP problems. Semantic parsers gain performance boosts with deep neural networks, but inherit vulnerabilities against adversarial examples. In this paper, we provide the first empirical study on the robustness of semantic parsers in the presence of adversarial attacks. Formally, adversaries of semantic parsing are considered to be the perturbed utterance-LF pairs, whose utterances have exactly the same meanings as the original ones. A scalable methodology is proposed to construct robustness test sets based on existing benchmark corpora. Our results answered five research questions in measuring the sate-of-the-art parsers' performance on robustness test sets, and evaluating the effect of data augmentation",
    "checked": true,
    "id": "f4cc7fa4f6a638cf79bf52341a05edcb5434fe97",
    "semantic_title": "on robustness of neural semantic parsers",
    "citation_count": 15,
    "authors": [
      "Shuo Huang",
      "Zhuang Li",
      "Lizhen Qu",
      "Lei Pan"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.293": {
    "title": "Benchmarking a transformer-FREE model for ad-hoc retrieval",
    "volume": "main",
    "abstract": "Transformer-based \"behemoths\" have grown in popularity, as well as structurally, shattering multiple NLP benchmarks along the way. However, their real-world usability remains a question. In this work, we empirically assess the feasibility of applying transformer-based models in real-world ad-hoc retrieval applications by comparison to a \"greener and more sustainable\" alternative, comprising only 620 trainable parameters. We present an analysis of their efficacy and efficiency and show that considering limited computational resources, the lighter model running on the CPU achieves a 3 to 20 times speedup in training and 7 to 47 times in inference while maintaining a comparable retrieval performance. Code to reproduce the efficiency experiments is available on \"https://github.com/bioinformatics-ua/EACL2021-reproducibility/\"",
    "checked": true,
    "id": "4cfb26fa6e2da90b6d5832b18247a7e7a74166c8",
    "semantic_title": "benchmarking a transformer-free model for ad-hoc retrieval",
    "citation_count": 1,
    "authors": [
      "Tiago Almeida",
      "Sérgio Matos"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.294": {
    "title": "Reanalyzing the Most Probable Sentence Problem: A Case Study in Explicating the Role of Entropy in Algorithmic Complexity",
    "volume": "main",
    "abstract": "When working with problems in natural language processing, we can find ourselves in situations where the traditional measurements of descriptive complexity are ineffective at describing the behaviour of our algorithms. It is easy to see why — the models we use are often general frameworks into which difficult-to-define tasks can be embedded. These frameworks can have more power than we typically use, and so complexity measures such as worst-case running time can drastically overestimate the cost of running our algorithms. In particular, they can make an apparently tractable problem seem NP-complete. Using empirical studies to evaluate performance is a necessary but incomplete method of dealing with this mismatch, since these studies no longer act as a guarantee of good performance. In this paper we use statistical measures such as entropy to give an updated analysis of the complexity of the NP-complete Most Probable Sentence problem for pCFGs, which can then be applied to word sense disambiguation and inference tasks. We can bound both the running time and the error in a simple search algorithm, allowing for a much faster search than the NP-completeness of this problem would suggest",
    "checked": true,
    "id": "bc046804b3a01b201b4ba7173d5f2fd01295aaa0",
    "semantic_title": "reanalyzing the most probable sentence problem: a case study in explicating the role of entropy in algorithmic complexity",
    "citation_count": 0,
    "authors": [
      "Eric Corlett",
      "Gerald Penn"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.295": {
    "title": "Probing the Probing Paradigm: Does Probing Accuracy Entail Task Relevance?",
    "volume": "main",
    "abstract": "Although neural models have achieved impressive results on several NLP benchmarks, little is understood about the mechanisms they use to perform language tasks. Thus, much recent attention has been devoted to analyzing the sentence representations learned by neural encoders, through the lens of ‘probing' tasks. However, to what extent was the information encoded in sentence representations, as discovered through a probe, actually used by the model to perform its task? In this work, we examine this probing paradigm through a case study in Natural Language Inference, showing that models can learn to encode linguistic properties even if they are not needed for the task on which the model was trained. We further identify that pretrained word embeddings play a considerable role in encoding these properties rather than the training task itself, highlighting the importance of careful controls when designing probing experiments. Finally, through a set of controlled synthetic tasks, we demonstrate models can encode these properties considerably above chance-level, even when distributed in the data as random noise, calling into question the interpretation of absolute claims on probing tasks",
    "checked": true,
    "id": "922535984641f052d2530c8b68f9ebd7de38dfa5",
    "semantic_title": "probing the probing paradigm: does probing accuracy entail task relevance?",
    "citation_count": 101,
    "authors": [
      "Abhilasha Ravichander",
      "Yonatan Belinkov",
      "Eduard Hovy"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.296": {
    "title": "One-class Text Classification with Multi-modal Deep Support Vector Data Description",
    "volume": "main",
    "abstract": "This work presents multi-modal deep SVDD (mSVDD) for one-class text classification. By extending the uni-modal SVDD to a multiple modal one, we build mSVDD with multiple hyperspheres, that enable us to build a much better description for target one-class data. Additionally, the end-to-end architecture of mSVDD can jointly handle neural feature learning and one-class text learning. We also introduce a mechanism for incorporating negative supervision in the absence of real negative data, which can be beneficial to the mSVDD model. We conduct experiments on Reuters and 20 Newsgroup datasets, and the experimental results demonstrate that mSVDD outperforms uni-modal SVDD and mSVDD can get further improvements when negative supervision is incorporated",
    "checked": true,
    "id": "2bed351a6eb9966e510b893a90fb87fc43731c85",
    "semantic_title": "one-class text classification with multi-modal deep support vector data description",
    "citation_count": 8,
    "authors": [
      "Chenlong Hu",
      "Yukun Feng",
      "Hidetaka Kamigaito",
      "Hiroya Takamura",
      "Manabu Okumura"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.297": {
    "title": "Unsupervised Word Polysemy Quantification with Multiresolution Grids of Contextual Embeddings",
    "volume": "main",
    "abstract": "The number of senses of a given word, or polysemy, is a very subjective notion, which varies widely across annotators and resources. We propose a novel method to estimate polysemy based on simple geometry in the contextual embedding space. Our approach is fully unsupervised and purely data-driven. Through rigorous experiments, we show that our rankings are well correlated, with strong statistical significance, with 6 different rankings derived from famous human-constructed resources such as WordNet, OntoNotes, Oxford, Wikipedia, etc., for 6 different standard metrics. We also visualize and analyze the correlation between the human rankings and make interesting observations. A valuable by-product of our method is the ability to sample, at no extra cost, sentences containing different senses of a given word. Finally, the fully unsupervised nature of our approach makes it applicable to any language. Code and data are publicly available https://github.com/ksipos/polysemy-assessment",
    "checked": true,
    "id": "0743a4bc5fdfb931b29b567051700193b05d2ec0",
    "semantic_title": "unsupervised word polysemy quantification with multiresolution grids of contextual embeddings",
    "citation_count": 8,
    "authors": [
      "Christos Xypolopoulos",
      "Antoine Tixier",
      "Michalis Vazirgiannis"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.298": {
    "title": "Mega-COV: A Billion-Scale Dataset of 100+ Languages for COVID-19",
    "volume": "main",
    "abstract": "We describe Mega-COV, a billion-scale dataset from Twitter for studying COVID-19. The dataset is diverse (covers 268 countries), longitudinal (goes as back as 2007), multilingual (comes in 100+ languages), and has a significant number of location-tagged tweets (~169M tweets). We release tweet IDs from the dataset. We also develop two powerful models, one for identifying whether or not a tweet is related to the pandemic (best F1=97%) and another for detecting misinformation about COVID-19 (best F1=92%). A human annotation study reveals the utility of our models on a subset of Mega-COV. Our data and models can be useful for studying a wide host of phenomena related to the pandemic. Mega-COV and our models are publicly available",
    "checked": true,
    "id": "a6000fb674ad02c4d9ae037a0ad2491e44d0615c",
    "semantic_title": "mega-cov: a billion-scale dataset of 100+ languages for covid-19",
    "citation_count": 11,
    "authors": [
      "Muhammad Abdul-Mageed",
      "AbdelRahim Elmadany",
      "El Moatez Billah Nagoudi",
      "Dinesh Pabbi",
      "Kunal Verma",
      "Rannie Lin"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.299": {
    "title": "Disfluency Correction using Unsupervised and Semi-supervised Learning",
    "volume": "main",
    "abstract": "Spoken language is different from the written language in its style and structure. Disfluencies that appear in transcriptions from speech recognition systems generally hamper the performance of downstream NLP tasks. Thus, a disfluency correction system that converts disfluent to fluent text is of great value. This paper introduces a disfluency correction model that translates disfluent to fluent text by drawing inspiration from recent encoder-decoder unsupervised style-transfer models for text. We also show considerable benefits in performance when utilizing a small sample of 500 parallel disfluent-fluent sentences in a semi-supervised way. Our unsupervised approach achieves a BLEU score of 79.39 on the Switchboard corpus test set, with further improvement to a BLEU score of 85.28 with semi-supervision. Both are comparable to two competitive fully-supervised models",
    "checked": true,
    "id": "a25fd3aba1ff7fea9f232210cbe48eff254bff0b",
    "semantic_title": "disfluency correction using unsupervised and semi-supervised learning",
    "citation_count": 4,
    "authors": [
      "Nikhil Saini",
      "Drumil Trivedi",
      "Shreya Khare",
      "Tejas Dhamecha",
      "Preethi Jyothi",
      "Samarth Bharadwaj",
      "Pushpak Bhattacharyya"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.300": {
    "title": "Complex Question Answering on knowledge graphs using machine translation and multi-task learning",
    "volume": "main",
    "abstract": "Question answering (QA) over a knowledge graph (KG) is a task of answering a natural language (NL) query using the information stored in KG. In a real-world industrial setting, this involves addressing multiple challenges including entity linking, multi-hop reasoning over KG, etc. Traditional approaches handle these challenges in a modularized sequential manner where errors in one module lead to the accumulation of errors in downstream modules. Often these challenges are inter-related and the solutions to them can reinforce each other when handled simultaneously in an end-to-end learning setup. To this end, we propose a multi-task BERT based Neural Machine Translation (NMT) model to address these challenges. Through experimental analysis, we demonstrate the efficacy of our proposed approach on one publicly available and one proprietary dataset",
    "checked": true,
    "id": "9c89283076579b2d70cf16dfc7cd67972c7295de",
    "semantic_title": "complex question answering on knowledge graphs using machine translation and multi-task learning",
    "citation_count": 13,
    "authors": [
      "Saurabh Srivastava",
      "Mayur Patidar",
      "Sudip Chowdhury",
      "Puneet Agarwal",
      "Indrajit Bhattacharya",
      "Gautam Shroff"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.301": {
    "title": "Recipes for Adapting Pre-trained Monolingual and Multilingual Models to Machine Translation",
    "volume": "main",
    "abstract": "There has been recent success in pre-training on monolingual data and fine-tuning on Machine Translation (MT), but it remains unclear how to best leverage a pre-trained model for a given MT task. This paper investigates the benefits and drawbacks of freezing parameters, and adding new ones, when fine-tuning a pre-trained model on MT. We focus on 1) Fine-tuning a model trained only on English monolingual data, BART. 2) Fine-tuning a model trained on monolingual data from 25 languages, mBART. For BART we get the best performance by freezing most of the model parameters, and adding extra positional embeddings. For mBART we match or outperform the performance of naive fine-tuning for most language pairs with the encoder, and most of the decoder, frozen. The encoder-decoder attention parameters are most important to fine-tune. When constraining ourselves to an out-of-domain training set for Vietnamese to English we see the largest improvements over the fine-tuning baseline",
    "checked": true,
    "id": "973173fd44df71ca3b5d337cf982cfd11bc7aa4a",
    "semantic_title": "recipes for adapting pre-trained monolingual and multilingual models to machine translation",
    "citation_count": 37,
    "authors": [
      "Asa Cooper Stickland",
      "Xian Li",
      "Marjan Ghazvininejad"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.302": {
    "title": "From characters to words: the turning point of BPE merges",
    "volume": "main",
    "abstract": "The distributions of orthographic word types are very different across languages due to typological characteristics, different writing traditions and potentially other factors. The wide range of cross-linguistic diversity is still a major challenge for NLP and the study of language. We use BPE and information-theoretic measures to investigate if distributions become similar under specific levels of subword tokenization. We perform a cross-linguistic comparison, following incremental merges of BPE (we go from characters to words) for 47 diverse languages. We show that text entropy values (a feature of probability distributions) tend to converge at specific subword levels: relatively few BPE merges (around 350) lead to the most similar distributions across languages. Additionally, we analyze the interaction between subword and word-level distributions and show that our findings can be interpreted in light of the ongoing discussion regarding different types of morphological complexity",
    "checked": true,
    "id": "7771aa7badc3375a31bfac8dc47755ff5d5c7780",
    "semantic_title": "from characters to words: the turning point of bpe merges",
    "citation_count": 19,
    "authors": [
      "Ximena Gutierrez-Vasques",
      "Christian Bentz",
      "Olga Sozinova",
      "Tanja Samardzic"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.303": {
    "title": "A Large-scale Evaluation of Neural Machine Transliteration for Indic Languages",
    "volume": "main",
    "abstract": "We take up the task of large-scale evaluation of neural machine transliteration between English and Indic languages, with a focus on multilingual transliteration to utilize orthographic similarity between Indian languages. We create a corpus of 600K word pairs mined from parallel translation corpora and monolingual corpora, which is the largest transliteration corpora for Indian languages mined from public sources. We perform a detailed analysis of multilingual transliteration and propose an improved multilingual training recipe for Indic languages. We analyze various factors affecting transliteration quality like language family, transliteration direction and word origin",
    "checked": true,
    "id": "912ba9757feb259b02a2b96d6e0e5a0863bc8a53",
    "semantic_title": "a large-scale evaluation of neural machine transliteration for indic languages",
    "citation_count": 6,
    "authors": [
      "Anoop Kunchukuttan",
      "Siddharth Jain",
      "Rahul Kejriwal"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.304": {
    "title": "Communicative-Function-Based Sentence Classification for Construction of an Academic Formulaic Expression Database",
    "volume": "main",
    "abstract": "Formulaic expressions (FEs), such as ‘in this paper, we propose' are frequently used in scientific papers. FEs convey a communicative function (CF), i.e. ‘showing the aim of the paper' in the above-mentioned example. Although CF-labelled FEs are helpful in assisting academic writing, the construction of FE databases requires manual labour for assigning CF labels. In this study, we considered a fully automated construction of a CF-labelled FE database using the top–down approach, in which the CF labels are first assigned to sentences, and then the FEs are extracted. For the CF-label assignment, we created a CF-labelled sentence dataset, on which we trained a SciBERT classifier. We show that the classifier and dataset can be used to construct FE databases of disciplines that are different from the training data. The accuracy of in-disciplinary classification was more than 80%, while cross-disciplinary classification also worked well. We also propose an FE extraction method, which was applied to the CF-labelled sentences. Finally, we constructed and published a new, large CF-labelled FE database. The evaluation of the final CF-labelled FE database showed that approximately 65% of the FEs are correct and useful, which is sufficiently high considering practical use",
    "checked": true,
    "id": "ebc42c06ed899989871ebba497b6b05511ee1394",
    "semantic_title": "communicative-function-based sentence classification for construction of an academic formulaic expression database",
    "citation_count": 1,
    "authors": [
      "Kenichi Iwatsuki",
      "Akiko Aizawa"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.305": {
    "title": "Regulatory Compliance through Doc2Doc Information Retrieval: A case study in EU/UK legislation where text similarity has limitations",
    "volume": "main",
    "abstract": "Major scandals in corporate history have urged the need for regulatory compliance, where organizations need to ensure that their controls (processes) comply with relevant laws, regulations, and policies. However, keeping track of the constantly changing legislation is difficult, thus organizations are increasingly adopting Regulatory Technology (RegTech) to facilitate the process. To this end, we introduce regulatory information retrieval (REG-IR), an application of document-to-document information retrieval (DOC2DOC IR), where the query is an entire document making the task more challenging than traditional IR where the queries are short. Furthermore, we compile and release two datasets based on the relationships between EU directives and UK legislation. We experiment on these datasets using a typical two-step pipeline approach comprising a pre-fetcher and a neural re-ranker. Experimenting with various pre-fetchers from BM25 to k nearest neighbors over representations from several BERT models, we show that fine-tuning a BERT model on an in-domain classification task produces the best representations for IR. We also show that neural re-rankers under-perform due to contradicting supervision, i.e., similar query-document pairs with opposite labels. Thus, they are biased towards the pre-fetcher's score. Interestingly, applying a date filter further improves the performance, showcasing the importance of the time dimension",
    "checked": true,
    "id": "f003191790ff27f4583f767148501f5112e82d8f",
    "semantic_title": "regulatory compliance through doc2doc information retrieval: a case study in eu/uk legislation where text similarity has limitations",
    "citation_count": 16,
    "authors": [
      "Ilias Chalkidis",
      "Manos Fergadiotis",
      "Nikolaos Manginas",
      "Eva Katakalou",
      "Prodromos Malakasiotis"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.306": {
    "title": "The Chinese Remainder Theorem for Compact, Task-Precise, Efficient and Secure Word Embeddings",
    "volume": "main",
    "abstract": "The growing availability of powerful mobile devices and other edge devices, together with increasing regulatory and security concerns about the exchange of personal information across networks of these devices has challenged the Computational Linguistics community to develop methods that are at once fast, space-efficient, accurate and amenable to secure encoding schemes such as homomorphic encryption. Inspired by recent work that restricts floating point precision to speed up neural network training in hardware-based SIMD, we have developed a method for compressing word vector embeddings into integers using the Chinese Reminder Theorem that speeds up addition by up to 48.27% and at the same time compresses GloVe word embedding libraries by up to 25.86%. We explore the practicality of this simple approach by investigating the trade-off between precision and performance in two NLP tasks: compositional semantic relatedness and opinion target sentiment classification. We find that in both tasks, lowering floating point number precision results in negligible changes to performance",
    "checked": true,
    "id": "c272e9a3968d59fa15fe4ea3e09d012da876d571",
    "semantic_title": "the chinese remainder theorem for compact, task-precise, efficient and secure word embeddings",
    "citation_count": 1,
    "authors": [
      "Patricia Thaine",
      "Gerald Penn"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.307": {
    "title": "Don't Change Me! User-Controllable Selective Paraphrase Generation",
    "volume": "main",
    "abstract": "In the paraphrase generation task, source sentences often contain phrases that should not be altered. Which phrases, however, can be context dependent and can vary by application. Our solution to this challenge is to provide the user with explicit tags that can be placed around any arbitrary segment of text to mean \"don't change me!\" when generating a paraphrase; the model learns to explicitly copy these phrases to the output. The contribution of this work is a novel data generation technique using distant supervision that allows us to start with a pretrained sequence-to-sequence model and fine-tune a paraphrase generator that exhibits this behavior, allowing user-controllable paraphrase generation. Additionally, we modify the loss during fine-tuning to explicitly encourage diversity in model output. Our technique is language agnostic, and we report experiments in English and Chinese",
    "checked": true,
    "id": "fa080a3427af8b7d871e05d381d7a322f38dccd9",
    "semantic_title": "don't change me! user-controllable selective paraphrase generation",
    "citation_count": 2,
    "authors": [
      "Mohan Zhang",
      "Luchen Tan",
      "Zihang Fu",
      "Kun Xiong",
      "Jimmy Lin",
      "Ming Li",
      "Zhengkai Tu"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.308": {
    "title": "Rethinking Coherence Modeling: Synthetic vs. Downstream Tasks",
    "volume": "main",
    "abstract": "Although coherence modeling has come a long way in developing novel models, their evaluation on downstream applications for which they are purportedly developed has largely been neglected. With the advancements made by neural approaches in applications such as machine translation (MT), summarization and dialog systems, the need for coherence evaluation of these tasks is now more crucial than ever. However, coherence models are typically evaluated only on synthetic tasks, which may not be representative of their performance in downstream applications. To investigate how representative the synthetic tasks are of downstream use cases, we conduct experiments on benchmarking well-known traditional and neural coherence models on synthetic sentence ordering tasks, and contrast this with their performance on three downstream applications: coherence evaluation for MT and summarization, and next utterance prediction in retrieval-based dialog. Our results demonstrate a weak correlation between the model performances in the synthetic tasks and the downstream applications, motivating alternate training and evaluation methods for coherence models",
    "checked": true,
    "id": "d90a49f8cb0df04a1afdc9c87181cb33f82c4ba2",
    "semantic_title": "rethinking coherence modeling: synthetic vs. downstream tasks",
    "citation_count": 10,
    "authors": [
      "Tasnim Mohiuddin",
      "Prathyusha Jwalapuram",
      "Xiang Lin",
      "Shafiq Joty"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.309": {
    "title": "From the Stage to the Audience: Propaganda on Reddit",
    "volume": "main",
    "abstract": "Political discussions revolve around ideological conflicts that often split the audience into two opposing parties. Both parties try to win the argument by bringing forward information. However, often this information is misleading, and its dissemination employs propaganda techniques. In this work, we analyze the impact of propaganda on six major political forums on Reddit that target a diverse audience in two countries, the US and the UK. We focus on three research questions: who is posting propaganda? how does propaganda differ across the political spectrum? and how is propaganda received on political forums?",
    "checked": true,
    "id": "3649f3cf214d4287f061de8d7139340b15838062",
    "semantic_title": "from the stage to the audience: propaganda on reddit",
    "citation_count": 6,
    "authors": [
      "Oana Balalau",
      "Roxana Horincar"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.310": {
    "title": "Probing for idiomaticity in vector space models",
    "volume": "main",
    "abstract": "Contextualised word representation models have been successfully used for capturing different word usages and they may be an attractive alternative for representing idiomaticity in language. In this paper, we propose probing measures to assess if some of the expected linguistic properties of noun compounds, especially those related to idiomatic meanings, and their dependence on context and sensitivity to lexical choice, are readily available in some standard and widely used representations. For that, we constructed the Noun Compound Senses Dataset, which contains noun compounds and their paraphrases, in context neutral and context informative naturalistic sentences, in two languages: English and Portuguese. Results obtained using four types of probing measures with models like ELMo, BERT and some of its variants, indicate that idiomaticity is not yet accurately represented by contextualised models",
    "checked": true,
    "id": "b8c466002d2e8c26b11681a64775252098a3ae76",
    "semantic_title": "probing for idiomaticity in vector space models",
    "citation_count": 43,
    "authors": [
      "Marcos Garcia",
      "Tiago Kramer Vieira",
      "Carolina Scarton",
      "Marco Idiart",
      "Aline Villavicencio"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.311": {
    "title": "Is the Understanding of Explicit Discourse Relations Required in Machine Reading Comprehension?",
    "volume": "main",
    "abstract": "An in-depth analysis of the level of language understanding required by existing Machine Reading Comprehension (MRC) benchmarks can provide insight into the reading capabilities of machines. In this paper, we propose an ablation-based methodology to assess the extent to which MRC datasets evaluate the understanding of explicit discourse relations. We define seven MRC skills which require the understanding of different discourse relations. We then introduce ablation methods that verify whether these skills are required to succeed on a dataset. By observing the drop in performance of neural MRC models evaluated on the original and the modified dataset, we can measure to what degree the dataset requires these skills, in order to be understood correctly. Experiments on three large-scale datasets with the BERT-base and ALBERT-xxlarge model show that the relative changes for all skills are small (less than 6%). These results imply that most of the answered questions in the examined datasets do not require understanding the discourse structure of the text. To specifically probe for natural language understanding, there is a need to design more challenging benchmarks that can correctly evaluate the intended skills",
    "checked": true,
    "id": "26c36b9468a51a54bc879519c0bb1a3b280e9421",
    "semantic_title": "is the understanding of explicit discourse relations required in machine reading comprehension?",
    "citation_count": 3,
    "authors": [
      "Yulong Wu",
      "Viktor Schlegel",
      "Riza Batista-Navarro"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.312": {
    "title": "Why Is MBTI Personality Detection from Texts a Difficult Task?",
    "volume": "main",
    "abstract": "Automatic detection of the four MBTI personality dimensions from texts has recently attracted noticeable attention from the natural language processing and computational linguistic communities. Despite the large collections of Twitter data for training, the best systems rarely even outperform the majority-class baseline. In this paper, we discuss the theoretical reasons for such low results and present the insights from an annotation study that further shed the light on this issue",
    "checked": true,
    "id": "3a758a7e04f0e5411a452bdb7f581452e09c9f55",
    "semantic_title": "why is mbti personality detection from texts a difficult task?",
    "citation_count": 23,
    "authors": [
      "Sanja Stajner",
      "Seren Yenikent"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.313": {
    "title": "Enconter: Entity Constrained Progressive Sequence Generation via Insertion-based Transformer",
    "volume": "main",
    "abstract": "Pretrained using large amount of data, autoregressive language models are able to generate high quality sequences. However, these models do not perform well under hard lexical constraints as they lack fine control of content generation process. Progressive insertion based transformers can overcome the above limitation and efficiently generate a sequence in parallel given some input tokens as constraint. These transformers however may fail to support hard lexical constraints as their generation process is more likely to terminate prematurely. The paper analyses such early termination problems and proposes the ENtity CONstrained insertion TransformER(ENCONTER), a new insertion transformer that addresses the above pitfall without compromising much generation efficiency. We introduce a new training strategy that considers predefined hard lexical constraints (e.g., entities to be included in the generated sequence). Our experiments show that ENCONTER outperforms other baseline models in several performance metrics rendering it more suitable in practical applications",
    "checked": true,
    "id": "74fcb7f7fbc54b6211c99a0c2c2d74f8ec34cb12",
    "semantic_title": "enconter: entity constrained progressive sequence generation via insertion-based transformer",
    "citation_count": 2,
    "authors": [
      "Lee Hsun Hsieh",
      "Yang-Yin Lee",
      "Ee-Peng Lim"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.314": {
    "title": "Meta-Learning for Effective Multi-task and Multilingual Modelling",
    "volume": "main",
    "abstract": "Natural language processing (NLP) tasks (e.g. question-answering in English) benefit from knowledge of other tasks (e.g., named entity recognition in English) and knowledge of other languages (e.g., question-answering in Spanish). Such shared representations are typically learned in isolation, either across tasks or across languages. In this work, we propose a meta-learning approach to learn the interactions between both tasks and languages. We also investigate the role of different sampling strategies used during meta-learning. We present experiments on five different tasks and six different languages from the XTREME multilingual benchmark dataset. Our meta-learned model clearly improves in performance compared to competitive baseline models that also include multi-task baselines. We also present zero-shot evaluations on unseen target languages to demonstrate the utility of our proposed model",
    "checked": true,
    "id": "5849d6cc9180b6b22ccf5a482f6b5d8f34e065fd",
    "semantic_title": "meta-learning for effective multi-task and multilingual modelling",
    "citation_count": 15,
    "authors": [
      "Ishan Tarunesh",
      "Sushil Khyalia",
      "Vishwajeet Kumar",
      "Ganesh Ramakrishnan",
      "Preethi Jyothi"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.315": {
    "title": "Killing Me\" Is Not a Spoiler: Spoiler Detection Model using Graph Neural Networks with Dependency Relation-Aware Attention Mechanism",
    "volume": "main",
    "abstract": "Several machine learning-based spoiler detection models have been proposed recently to protect users from spoilers on review websites. Although dependency relations between context words are important for detecting spoilers, current attention-based spoiler detection models are insufficient for utilizing dependency relations. To address this problem, we propose a new spoiler detection model called SDGNN that is based on syntax-aware graph neural networks. In the experiments on two real-world benchmark datasets, we show that our SDGNN outperforms the existing spoiler detection models",
    "checked": true,
    "id": "901c2f3e0d333a0e02a6e0b2fa4278e2caabad5c",
    "semantic_title": "killing me\" is not a spoiler: spoiler detection model using graph neural networks with dependency relation-aware attention mechanism",
    "citation_count": 6,
    "authors": [
      "Buru Chang",
      "Inggeol Lee",
      "Hyunjae Kim",
      "Jaewoo Kang"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.316": {
    "title": "BERTese: Learning to Speak to BERT",
    "volume": "main",
    "abstract": "Large pre-trained language models have been shown to encode large amounts of world and commonsense knowledge in their parameters, leading to substantial interest in methods for extracting that knowledge. In past work, knowledge was extracted by taking manually-authored queries and gathering paraphrases for them using a separate pipeline. In this work, we propose a method for automatically rewriting queries into \"BERTese\", a paraphrase query that is directly optimized towards better knowledge extraction. To encourage meaningful rewrites, we add auxiliary loss functions that encourage the query to correspond to actual language tokens. We empirically show our approach outperforms competing baselines, obviating the need for complex pipelines. Moreover, BERTese provides some insight into the type of language that helps language models perform knowledge extraction",
    "checked": true,
    "id": "a49e9a8d29b5838ba392d5d33fb9694f4667c59e",
    "semantic_title": "bertese: learning to speak to bert",
    "citation_count": 98,
    "authors": [
      "Adi Haviv",
      "Jonathan Berant",
      "Amir Globerson"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.317": {
    "title": "Lifelong Knowledge-Enriched Social Event Representation Learning",
    "volume": "main",
    "abstract": "The ability of humans to symbolically represent social events and situations is crucial for various interactions in everyday life. Several studies in cognitive psychology have established the role of mental state attributions in effectively representing variable aspects of these social events. In the past, NLP research on learning event representations often focuses on construing syntactic and semantic information from language. However, they fail to consider the importance of pragmatic aspects and the need to consistently update new social situational information without forgetting the accumulated experiences. In this work, we propose a representation learning framework to directly address these shortcomings by integrating social commonsense knowledge with recent advancements in the space of lifelong language learning. First, we investigate methods to incorporate pragmatic aspects into our social event embeddings by leveraging social commonsense knowledge. Next, we introduce continual learning strategies that allow for incremental consolidation of new knowledge while retaining and promoting efficient usage of prior knowledge. Experimental results on event similarity, reasoning, and paraphrase detection tasks prove the efficacy of our social event embeddings",
    "checked": true,
    "id": "a3e95c1fbc78a6962e8977a5e10beb84f533b11a",
    "semantic_title": "lifelong knowledge-enriched social event representation learning",
    "citation_count": 3,
    "authors": [
      "Prashanth Vijayaraghavan",
      "Deb Roy"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.318": {
    "title": "GLaRA: Graph-based Labeling Rule Augmentation for Weakly Supervised Named Entity Recognition",
    "volume": "main",
    "abstract": "Instead of using expensive manual annotations, researchers have proposed to train named entity recognition (NER) systems using heuristic labeling rules. However, devising labeling rules is challenging because it often requires a considerable amount of manual effort and domain expertise. To alleviate this problem, we propose GLARA, a graph-based labeling rule augmentation framework, to learn new labeling rules from unlabeled data. We first create a graph with nodes representing candidate rules extracted from unlabeled data. Then, we design a new graph neural network to augment labeling rules by exploring the semantic relations between rules. We finally apply the augmented rules on unlabeled data to generate weak labels and train a NER model using the weakly labeled data. We evaluate our method on three NER datasets and find that we can achieve an average improvement of +20% F1 score over the best baseline when given a small set of seed rules",
    "checked": true,
    "id": "53800017056d8ea5883bd63119b76256bffd7e91",
    "semantic_title": "glara: graph-based labeling rule augmentation for weakly supervised named entity recognition",
    "citation_count": 18,
    "authors": [
      "Xinyan Zhao",
      "Haibo Ding",
      "Zhe Feng"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.319": {
    "title": "An End-to-end Model for Entity-level Relation Extraction using Multi-instance Learning",
    "volume": "main",
    "abstract": "We present a joint model for entity-level relation extraction from documents. In contrast to other approaches - which focus on local intra-sentence mention pairs and thus require annotations on mention level - our model operates on entity level. To do so, a multi-task approach is followed that builds upon coreference resolution and gathers relevant signals via multi-instance learning with multi-level representations combining global entity and local mention information. We achieve state-of-the-art relation extraction results on the DocRED dataset and report the first entity-level end-to-end relation extraction results for future reference. Finally, our experimental results suggest that a joint approach is on par with task-specific learning, though more efficient due to shared parameters and training steps",
    "checked": true,
    "id": "17b3e7ce150ed6f0c882bd6b57b204fb831aa8fd",
    "semantic_title": "an end-to-end model for entity-level relation extraction using multi-instance learning",
    "citation_count": 42,
    "authors": [
      "Markus Eberts",
      "Adrian Ulges"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.320": {
    "title": "WER-BERT: Automatic WER Estimation with BERT in a Balanced Ordinal Classification Paradigm",
    "volume": "main",
    "abstract": "Automatic Speech Recognition (ASR) systems are evaluated using Word Error Rate (WER), which is calculated by comparing the number of errors between the ground truth and the transcription of the ASR system. This calculation, however, requires manual transcription of the speech signal to obtain the ground truth. Since transcribing audio signals is a costly process, Automatic WER Evaluation (e-WER) methods have been developed to automatically predict the WER of a speech system by only relying on the transcription and the speech signal features. While WER is a continuous variable, previous works have shown that positing e-WER as a classification problem is more effective than regression. However, while converting to a classification setting, these approaches suffer from heavy class imbalance. In this paper, we propose a new balanced paradigm for e-WER in a classification setting. Within this paradigm, we also propose WER-BERT, a BERT based architecture with speech features for e-WER. Furthermore, we introduce a distance loss function to tackle the ordinal nature of e-WER classification. The proposed approach and paradigm are evaluated on the Librispeech dataset and a commercial (black box) ASR system, Google Cloud's Speech-to-Text API. The results and experiments demonstrate that WER-BERT establishes a new state-of-the-art in automatic WER estimation",
    "checked": true,
    "id": "19938190374bb29d9e313371e5fc87af973be6f6",
    "semantic_title": "wer-bert: automatic wer estimation with bert in a balanced ordinal classification paradigm",
    "citation_count": 7,
    "authors": [
      "Akshay Krishna Sheshadri",
      "Anvesh Rao Vijjini",
      "Sukhdeep Kharbanda"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.321": {
    "title": "Two Training Strategies for Improving Relation Extraction over Universal Graph",
    "volume": "main",
    "abstract": "This paper explores how the Distantly Supervised Relation Extraction (DS-RE) can benefit from the use of a Universal Graph (UG), the combination of a Knowledge Graph (KG) and a large-scale text collection. A straightforward extension of a current state-of-the-art neural model for DS-RE with a UG may lead to degradation in performance. We first report that this degradation is associated with the difficulty in learning a UG and then propose two training strategies: (1) Path Type Adaptive Pretraining, which sequentially trains the model with different types of UG paths so as to prevent the reliance on a single type of UG path; and (2) Complexity Ranking Guided Attention mechanism, which restricts the attention span according to the complexity of a UG path so as to force the model to extract features not only from simple UG paths but also from complex ones. Experimental results on both biomedical and NYT10 datasets prove the robustness of our methods and achieve a new state-of-the-art result on the NYT10 dataset. The code and datasets used in this paper are available at https://github.com/baodaiqin/UGDSRE",
    "checked": true,
    "id": "cb3c7ed9fb5d53864d9f67a1ca3044d5762bfea4",
    "semantic_title": "two training strategies for improving relation extraction over universal graph",
    "citation_count": 1,
    "authors": [
      "Qin Dai",
      "Naoya Inoue",
      "Ryo Takahashi",
      "Kentaro Inui"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.322": {
    "title": "Adaptation of Back-translation to Automatic Post-Editing for Synthetic Data Generation",
    "volume": "main",
    "abstract": "Automatic Post-Editing (APE) aims to correct errors in the output of a given machine translation (MT) system. Although data-driven approaches have become prevalent also in the APE task as in many other NLP tasks, there has been a lack of qualified training data due to the high cost of manual construction. eSCAPE, a synthetic APE corpus, has been widely used to alleviate the data scarcity, but it might not address genuine APE corpora's characteristic that the post-edited sentence should be a minimally edited revision of the given MT output. Therefore, we propose two new methods of synthesizing additional MT outputs by adapting back-translation to the APE task, obtaining robust enlargements of the existing synthetic APE training dataset. Experimental results on the WMT English-German APE benchmarks demonstrate that our enlarged datasets are effective in improving APE performance",
    "checked": true,
    "id": "0701700e45fe372d4c1294bbd343ee737b66ab0e",
    "semantic_title": "adaptation of back-translation to automatic post-editing for synthetic data generation",
    "citation_count": 11,
    "authors": [
      "WonKee Lee",
      "Baikjin Jung",
      "Jaehun Shin",
      "Jong-Hyeok Lee"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.323": {
    "title": "Removing Word-Level Spurious Alignment between Images and Pseudo-Captions in Unsupervised Image Captioning",
    "volume": "main",
    "abstract": "Unsupervised image captioning is a challenging task that aims at generating captions without the supervision of image-sentence pairs, but only with images and sentences drawn from different sources and object labels detected from the images. In previous work, pseudo-captions, i.e., sentences that contain the detected object labels, were assigned to a given image. The focus of the previous work was on the alignment of input images and pseudo-captions at the sentence level. However, pseudo-captions contain many words that are irrelevant to a given image. In this work, we investigate the effect of removing mismatched words from image-sentence alignment to determine how they make this task difficult. We propose a simple gating mechanism that is trained to align image features with only the most reliable words in pseudo-captions: the detected object labels. The experimental results show that our proposed method outperforms the previous methods without introducing complex sentence-level learning objectives. Combined with the sentence-level alignment method of previous work, our method further improves its performance. These results confirm the importance of careful alignment in word-level details",
    "checked": true,
    "id": "c27ad8bcef5123c1f6be3561f9173cce03d1c2ef",
    "semantic_title": "removing word-level spurious alignment between images and pseudo-captions in unsupervised image captioning",
    "citation_count": 14,
    "authors": [
      "Ukyo Honda",
      "Yoshitaka Ushiku",
      "Atsushi Hashimoto",
      "Taro Watanabe",
      "Yuji Matsumoto"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.324": {
    "title": "Towards More Fine-grained and Reliable NLP Performance Prediction",
    "volume": "main",
    "abstract": "Performance prediction, the task of estimating a system's performance without performing experiments, allows us to reduce the experimental burden caused by the combinatorial explosion of different datasets, languages, tasks, and models. In this paper, we make two contributions to improving performance prediction for NLP tasks. First, we examine performance predictors not only for holistic measures of accuracy like F1 or BLEU, but also fine-grained performance measures such as accuracy over individual classes of examples. Second, we propose methods to understand the reliability of a performance prediction model from two angles: confidence intervals and calibration. We perform an analysis of four types of NLP tasks, and both demonstrate the feasibility of fine-grained performance prediction and the necessity to perform reliability analysis for performance prediction methods in the future",
    "checked": true,
    "id": "8512718bafa447f9b433da9e809215dfc28b6b28",
    "semantic_title": "towards more fine-grained and reliable nlp performance prediction",
    "citation_count": 27,
    "authors": [
      "Zihuiwen Ye",
      "Pengfei Liu",
      "Jinlan Fu",
      "Graham Neubig"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.325": {
    "title": "Metrical Tagging in the Wild: Building and Annotating Poetry Corpora with Rhythmic Features",
    "volume": "main",
    "abstract": "A prerequisite for the computational study of literature is the availability of properly digitized texts, ideally with reliable meta-data and ground-truth annotation. Poetry corpora do exist for a number of languages, but larger collections lack consistency and are encoded in various standards, while annotated corpora are typically constrained to a particular genre and/or were designed for the analysis of certain linguistic features (like rhyme). In this work, we provide large poetry corpora for English and German, and annotate prosodic features in smaller corpora to train corpus driven neural models that enable robust large scale analysis. We show that BiLSTM-CRF models with syllable embeddings outperform a CRF baseline and different BERT-based approaches. In a multi-task setup, particular beneficial task relations illustrate the inter-dependence of poetic features. A model learns foot boundaries better when jointly predicting syllable stress, aesthetic emotions and verse measures benefit from each other, and we find that caesuras are quite dependent on syntax and also integral to shaping the overall measure of the line",
    "checked": true,
    "id": "39dbf18f4149ea890a73da36a472b7b713dfeea4",
    "semantic_title": "metrical tagging in the wild: building and annotating poetry corpora with rhythmic features",
    "citation_count": 6,
    "authors": [
      "Thomas Haider"
    ]
  },
  "https://aclanthology.org/2021.eacl-main.326": {
    "title": "Enhancing Aspect-level Sentiment Analysis with Word Dependencies",
    "volume": "main",
    "abstract": "Aspect-level sentiment analysis (ASA) has received much attention in recent years. Most existing approaches tried to leverage syntactic information, such as the dependency parsing results of the input text, to improve sentiment analysis on different aspects. Although these approaches achieved satisfying results, their main focus is to leverage the dependency arcs among words where the dependency type information is omitted; and they model different dependencies equally where the noisy dependency results may hurt model performance. In this paper, we propose an approach to enhance aspect-level sentiment analysis with word dependencies, where the type information is modeled by key-value memory networks and different dependency results are selectively leveraged. Experimental results on five benchmark datasets demonstrate the effectiveness of our approach, where it outperforms baseline models on all datasets and achieves state-of-the-art performance on three of them",
    "checked": true,
    "id": "c4df2a95473b942074d7896ab1805da93a54ccae",
    "semantic_title": "enhancing aspect-level sentiment analysis with word dependencies",
    "citation_count": 37,
    "authors": [
      "Yuanhe Tian",
      "Guimin Chen",
      "Yan Song"
    ]
  },
  "https://aclanthology.org/2021.eacl-demos.1": {
    "title": "Using and comparing Rhetorical Structure Theory parsers with rst-workbench",
    "volume": "demo",
    "abstract": "I present rst-workbench, a software package that simplifies the installation and usage of numerous end-to-end Rhetorical Structure Theory (RST) parsers. The tool offers a web-based interface that allows users to enter text and let multiple RST parsers generate analyses concurrently. The resulting RST trees can be compared visually, manually post-edited (in the browser) and stored for later usage",
    "checked": true,
    "id": "3bd3db6424fbeb4d85138b8135459c209ddade74",
    "semantic_title": "using and comparing rhetorical structure theory parsers with rst-workbench",
    "citation_count": 0,
    "authors": [
      "Arne Neumann"
    ]
  },
  "https://aclanthology.org/2021.eacl-demos.2": {
    "title": "SF-QA: Simple and Fair Evaluation Library for Open-domain Question Answering",
    "volume": "demo",
    "abstract": "Although open-domain question answering (QA) draws great attention in recent years, it requires large amounts of resources for building the full system and it is often difficult to reproduce previous results due to complex configurations. In this paper, we introduce SF-QA: simple and fair evaluation framework for open-domain QA. SF-QA framework modularizes the pipeline open-domain QA system, which makes the task itself easily accessible and reproducible to research groups without enough computing resources. The proposed evaluation framework is publicly available and anyone can contribute to the code and evaluations",
    "checked": true,
    "id": "3661a8d4f2f40efc4d60daf16b0465e9cd55cb4b",
    "semantic_title": "sf-qa: simple and fair evaluation library for open-domain question answering",
    "citation_count": 0,
    "authors": [
      "Xiaopeng Lu",
      "Kyusong Lee",
      "Tiancheng Zhao"
    ]
  },
  "https://aclanthology.org/2021.eacl-demos.3": {
    "title": "Finite-state script normalization and processing utilities: The Nisaba Brahmic library",
    "volume": "demo",
    "abstract": "This paper presents an open-source library for efficient low-level processing of ten major South Asian Brahmic scripts. The library provides a flexible and extensible framework for supporting crucial operations on Brahmic scripts, such as NFC, visual normalization, reversible transliteration, and validity checks, implemented in Python within a finite-state transducer formalism. We survey some common Brahmic script issues that may adversely affect the performance of downstream NLP tasks, and provide the rationale for finite-state design and system implementation details",
    "checked": true,
    "id": "7d1dd7dcf45cedaeab7deb4b475974e89c43cf1c",
    "semantic_title": "finite-state script normalization and processing utilities: the nisaba brahmic library",
    "citation_count": 8,
    "authors": [
      "Cibu Johny",
      "Lawrence Wolf-Sonkin",
      "Alexander Gutkin",
      "Brian Roark"
    ]
  },
  "https://aclanthology.org/2021.eacl-demos.4": {
    "title": "CovRelex: A COVID-19 Retrieval System with Relation Extraction",
    "volume": "demo",
    "abstract": "This paper presents CovRelex, a scientific paper retrieval system targeting entities and relations via relation extraction on COVID-19 scientific papers. This work aims at building a system supporting users efficiently in acquiring knowledge across a huge number of COVID-19 scientific papers published rapidly. Our system can be accessed via https://www.jaist.ac.jp/is/labs/nguyen-lab/systems/covrelex/",
    "checked": true,
    "id": "787933e62cee1e5f55d51faf50c48c92d8021997",
    "semantic_title": "covrelex: a covid-19 retrieval system with relation extraction",
    "citation_count": 12,
    "authors": [
      "Vu Tran",
      "Van-Hien Tran",
      "Phuong Nguyen",
      "Chau Nguyen",
      "Ken Satoh",
      "Yuji Matsumoto",
      "Minh Nguyen"
    ]
  },
  "https://aclanthology.org/2021.eacl-demos.5": {
    "title": "MATILDA - Multi-AnnoTator multi-language InteractiveLight-weight Dialogue Annotator",
    "volume": "demo",
    "abstract": "Dialogue Systems are becoming ubiquitous in various forms and shapes - virtual assistants(Siri, Alexa, etc.), chat-bots, customer sup-port, chit-chat systems just to name a few. The advances in language models and their publication have democratised advanced NLP.However, data remains a crucial bottleneck. Our contribution to this essential pillar isMATILDA, to the best of our knowledge the first multi-annotator, multi-language dialogue annotation tool. MATILDA allows the creation of corpora, the management of users, the annotation of dialogues, the quick adaptation of the user interface to any language and the resolution of inter-annotator disagreement. We evaluate the tool on ease of use, annotation speed and interannotation resolution for both experts and novices and conclude that this tool not only supports the full pipeline for dialogue annotation, but also allows non-technical people to easily use it. We are completely open-sourcing the tool at https://github.com/wluper/matilda and provide a tutorial video1",
    "checked": true,
    "id": "c54e33f376015e19ba9f043e8eddf738f01160bb",
    "semantic_title": "matilda - multi-annotator multi-language interactivelight-weight dialogue annotator",
    "citation_count": 3,
    "authors": [
      "Davide Cucurnia",
      "Nikolai Rozanov",
      "Irene Sucameli",
      "Augusto Ciuffoletti",
      "Maria Simi"
    ]
  },
  "https://aclanthology.org/2021.eacl-demos.6": {
    "title": "AnswerQuest: A System for Generating Question-Answer Items from Multi-Paragraph Documents",
    "volume": "demo",
    "abstract": "One strategy for facilitating reading comprehension is to present information in a question-and-answer format. We demo a system that integrates the tasks of question answering (QA) and question generation (QG) in order to produce Q&A items that convey the content of multi-paragraph documents. We report some experiments for QA and QG that yield improvements on both tasks, and assess how they interact to produce a list of Q&A items for a text. The demo is accessible at qna.sdl.com",
    "checked": true,
    "id": "5bbaa034425d851d9912ce5401a8f77f1ee85b17",
    "semantic_title": "answerquest: a system for generating question-answer items from multi-paragraph documents",
    "citation_count": 6,
    "authors": [
      "Melissa Roemmele",
      "Deep Sidhpura",
      "Steve DeNeefe",
      "Ling Tsou"
    ]
  },
  "https://aclanthology.org/2021.eacl-demos.7": {
    "title": "T-NER: An All-Round Python Library for Transformer-based Named Entity Recognition",
    "volume": "demo",
    "abstract": "Language model (LM) pretraining has led to consistent improvements in many NLP downstream tasks, including named entity recognition (NER). In this paper, we present T-NER (Transformer-based Named Entity Recognition), a Python library for NER LM finetuning. In addition to its practical utility, T-NER facilitates the study and investigation of the cross-domain and cross-lingual generalization ability of LMs finetuned on NER. Our library also provides a web app where users can get model predictions interactively for arbitrary text, which facilitates qualitative model evaluation for non-expert programmers. We show the potential of the library by compiling nine public NER datasets into a unified format and evaluating the cross-domain and cross- lingual performance across the datasets. The results from our initial experiments show that in-domain performance is generally competitive across datasets. However, cross-domain generalization is challenging even with a large pretrained LM, which has nevertheless capacity to learn domain-specific features if fine- tuned on a combined dataset. To facilitate future research, we also release all our LM checkpoints via the Hugging Face model hub",
    "checked": true,
    "id": "6825ca62bde5036fccc06567c3c06093150e14d3",
    "semantic_title": "t-ner: an all-round python library for transformer-based named entity recognition",
    "citation_count": 56,
    "authors": [
      "Asahi Ushio",
      "Jose Camacho-Collados"
    ]
  },
  "https://aclanthology.org/2021.eacl-demos.8": {
    "title": "Forum 4.0: An Open-Source User Comment Analysis Framework",
    "volume": "demo",
    "abstract": "With the increasing number of user comments in diverse domains, including comments on online journalism and e-commerce websites, the manual content analysis of these comments becomes time-consuming and challenging. However, research showed that user comments contain useful information for different domain experts, which is thus worth finding and utilizing. This paper introduces Forum 4.0, an open-source framework to semi-automatically analyze, aggregate, and visualize user comments based on labels defined by domain experts. We demonstrate the applicability of Forum 4.0 with comments analytics scenarios within the domains of online journalism and app stores. We outline the underlying container architecture, including the web-based user interface, the machine learning component, and the task manager for time-consuming tasks. We finally conduct machine learning experiments with simulated annotations and different sampling strategies on existing datasets from both domains to evaluate Forum 4.0's performance. Forum 4.0 achieves promising classification results (ROC-AUC ≥ 0.9 with 100 annotated samples), utilizing transformer-based embeddings with a lightweight logistic regression model. We explain how Forum 4.0's architecture is applicable for millions of user comments in real-time, yet at feasible training and classification costs",
    "checked": true,
    "id": "6a4810f587632ba613e7a6f7dcaae36cd5bb4435",
    "semantic_title": "forum 4.0: an open-source user comment analysis framework",
    "citation_count": 1,
    "authors": [
      "Marlo Haering",
      "Jakob Smedegaard Andersen",
      "Chris Biemann",
      "Wiebke Loosen",
      "Benjamin Milde",
      "Tim Pietz",
      "Christian Stöcker",
      "Gregor Wiedemann",
      "Olaf Zukunft",
      "Walid Maalej"
    ]
  },
  "https://aclanthology.org/2021.eacl-demos.9": {
    "title": "SLTEV: Comprehensive Evaluation of Spoken Language Translation",
    "volume": "demo",
    "abstract": "Automatic evaluation of Machine Translation (MT) quality has been investigated over several decades. Spoken Language Translation (SLT), esp. when simultaneous, needs to consider additional criteria and does not have a standard evaluation procedure and a widely used toolkit. To fill the gap, we develop SLTev, an open-source tool for assessing SLT in a comprehensive way. SLTev reports the quality, latency, and stability of an SLT candidate output based on the time-stamped transcript and reference translation into a target language. For quality, we rely on sacreBLEU which provides MT evaluation measures such as chrF or BLEU. For latency, we propose two new scoring techniques. For stability, we extend the previously defined measures with a normalized Flicker in our work. We also propose a new averaging of older measures. A preliminary version of SLTev was used in the IWSLT 2020 shared task. Moreover, a growing collection of test datasets directly accessible by SLTev are provided for system evaluation comparable across papers",
    "checked": true,
    "id": "c4d48978973ba00cee265f1904e148ff85a4be2f",
    "semantic_title": "sltev: comprehensive evaluation of spoken language translation",
    "citation_count": 13,
    "authors": [
      "Ebrahim Ansari",
      "Ondřej Bojar",
      "Barry Haddow",
      "Mohammad Mahmoudi"
    ]
  },
  "https://aclanthology.org/2021.eacl-demos.10": {
    "title": "Trankit: A Light-Weight Transformer-based Toolkit for Multilingual Natural Language Processing",
    "volume": "demo",
    "abstract": "We introduce Trankit, a light-weight Transformer-based Toolkit for multilingual Natural Language Processing (NLP). It provides a trainable pipeline for fundamental NLP tasks over 100 languages, and 90 pretrained pipelines for 56 languages. Built on a state-of-the-art pretrained language model, Trankit significantly outperforms prior multilingual NLP pipelines over sentence segmentation, part-of-speech tagging, morphological feature tagging, and dependency parsing while maintaining competitive performance for tokenization, multi-word token expansion, and lemmatization over 90 Universal Dependencies treebanks. Despite the use of a large pretrained transformer, our toolkit is still efficient in memory usage and speed. This is achieved by our novel plug-and-play mechanism with Adapters where a multilingual pretrained transformer is shared across pipelines for different languages. Our toolkit along with pretrained models and code are publicly available at: https://github.com/nlp-uoregon/trankit. A demo website for our toolkit is also available at: http://nlp.uoregon.edu/trankit. Finally, we create a demo video for Trankit at: https://youtu.be/q0KGP3zGjGc",
    "checked": true,
    "id": "b53c386b7c65af80905dc05a9b27e98e03324739",
    "semantic_title": "trankit: a light-weight transformer-based toolkit for multilingual natural language processing",
    "citation_count": 101,
    "authors": [
      "Minh Van Nguyen",
      "Viet Dac Lai",
      "Amir Pouran Ben Veyseh",
      "Thien Huu Nguyen"
    ]
  },
  "https://aclanthology.org/2021.eacl-demos.11": {
    "title": "DebIE: A Platform for Implicit and Explicit Debiasing of Word Embedding Spaces",
    "volume": "demo",
    "abstract": "Recent research efforts in NLP have demonstrated that distributional word vector spaces often encode stereotypical human biases, such as racism and sexism. With word representations ubiquitously used in NLP models and pipelines, this raises ethical issues and jeopardizes the fairness of language technologies. While there exists a large body of work on bias measures and debiasing methods, to date, there is no platform that would unify these research efforts and make bias measuring and debiasing of representation spaces widely accessible. In this work, we present DebIE, the first integrated platform for (1) measuring and (2) mitigating bias in word embeddings. Given an (i) embedding space (users can choose between the predefined spaces or upload their own) and (ii) a bias specification (users can choose between existing bias specifications or create their own), DebIE can (1) compute several measures of implicit and explicit bias and modify the embedding space by executing two (mutually composable) debiasing models. DebIE's functionality can be accessed through four different interfaces: (a) a web application, (b) a desktop application, (c) a REST-ful API, and (d) as a command-line application. DebIE is available at: debie.informatik.uni-mannheim.de",
    "checked": true,
    "id": "713ac8f8adbc5a049de2f996ca03b149faec0abd",
    "semantic_title": "debie: a platform for implicit and explicit debiasing of word embedding spaces",
    "citation_count": 6,
    "authors": [
      "Niklas Friedrich",
      "Anne Lauscher",
      "Simone Paolo Ponzetto",
      "Goran Glavaš"
    ]
  },
  "https://aclanthology.org/2021.eacl-demos.12": {
    "title": "A Dashboard for Mitigating the COVID-19 Misinfodemic",
    "volume": "demo",
    "abstract": "This paper describes the current milestones achieved in our ongoing project that aims to understand the surveillance of, impact of and intervention on COVID-19 misinfodemic on Twitter. Specifically, it introduces a public dashboard which, in addition to displaying case counts in an interactive map and a navigational panel, also provides some unique features not found in other places. Particularly, the dashboard uses a curated catalog of COVID-19 related facts and debunks of misinformation, and it displays the most prevalent information from the catalog among Twitter users in user-selected U.S. geographic regions. The paper explains how to use BERT models to match tweets with the facts and misinformation and to detect their stance towards such information. The paper also discusses the results of preliminary experiments on analyzing the spatio-temporal spread of misinformation",
    "checked": true,
    "id": "1fe759eb9f5c78679108950a1e8969f2be21abf7",
    "semantic_title": "a dashboard for mitigating the covid-19 misinfodemic",
    "citation_count": 6,
    "authors": [
      "Zhengyuan Zhu",
      "Kevin Meng",
      "Josue Caraballo",
      "Israa Jaradat",
      "Xiao Shi",
      "Zeyu Zhang",
      "Farahnaz Akrami",
      "Haojin Liao",
      "Fatma Arslan",
      "Damian Jimenez",
      "Mohanmmed Samiul Saeef",
      "Paras Pathak",
      "Chengkai Li"
    ]
  },
  "https://aclanthology.org/2021.eacl-demos.13": {
    "title": "EasyTurk: A User-Friendly Interface for High-Quality Linguistic Annotation with Amazon Mechanical Turk",
    "volume": "demo",
    "abstract": "Amazon Mechanical Turk (AMT) has recently become one of the most popular crowd-sourcing platforms, allowing researchers from all over the world to create linguistic datasets quickly and at a relatively low cost. Amazon provides both a web interface and an API for AMT, but they are not very user-friendly and miss some features that can be useful for NLP researchers. In this paper, we present EasyTurk, a free tool that improves the potential of Amazon Mechanical Turk by adding to it some new features. The tool is free and released under an open source license",
    "checked": true,
    "id": "05f8b77ddd1a4963085d6454aa575b09d239aea4",
    "semantic_title": "easyturk: a user-friendly interface for high-quality linguistic annotation with amazon mechanical turk",
    "citation_count": 1,
    "authors": [
      "Lorenzo Bocchi",
      "Valentino Frasnelli",
      "Alessio Palmero Aprosio"
    ]
  },
  "https://aclanthology.org/2021.eacl-demos.14": {
    "title": "ASAD: Arabic Social media Analytics and unDerstanding",
    "volume": "demo",
    "abstract": "This system demonstration paper describes ASAD: Arabic Social media Analysis and unDerstanding, a suite of seven individual modules that allows users to determine dialects, sentiment, news category, offensiveness, hate speech, adult content, and spam in Arabic tweets. The suite is made available through a web API and a web interface where users can enter text or upload files",
    "checked": true,
    "id": "f277bb4afc0ab2376df8e6f307d448cf4f94d8b3",
    "semantic_title": "asad: arabic social media analytics and understanding",
    "citation_count": 20,
    "authors": [
      "Sabit Hassan",
      "Hamdy Mubarak",
      "Ahmed Abdelali",
      "Kareem Darwish"
    ]
  },
  "https://aclanthology.org/2021.eacl-demos.15": {
    "title": "COCO-EX: A Tool for Linking Concepts from Texts to ConceptNet",
    "volume": "demo",
    "abstract": "In this paper we present COCO-EX, a tool for Extracting Concepts from texts and linking them to the ConceptNet knowledge graph. COCO-EX extracts meaningful concepts from natural language texts and maps them to conjunct concept nodes in ConceptNet, utilizing the maximum of relational information stored in the ConceptNet knowledge graph. COCOEX takes into account the challenging characteristics of ConceptNet, namely that – unlike conventional knowledge graphs – nodes are represented as non-canonicalized, free-form text. This means that i) concepts are not normalized; ii) they often consist of several different, nested phrase types; and iii) many of them are uninformative, over-specific, or misspelled. A commonly used shortcut to circumvent these problems is to apply string matching. We compare COCO-EX to this method and show that COCO-EX enables the extraction of meaningful, important rather than overspecific or uninformative concepts, and allows to assess more relational information stored in the knowledge graph",
    "checked": true,
    "id": "2a702b88ee3ce58a8c5b47a1aa66eef6a77c5a3a",
    "semantic_title": "coco-ex: a tool for linking concepts from texts to conceptnet",
    "citation_count": 10,
    "authors": [
      "Maria Becker",
      "Katharina Korfhage",
      "Anette Frank"
    ]
  },
  "https://aclanthology.org/2021.eacl-demos.16": {
    "title": "A description and demonstration of SAFAR framework",
    "volume": "demo",
    "abstract": "Several tools and resources have been developed to deal with Arabic NLP. However, a homogenous and flexible Arabic environment that gathers these components is rarely available. In this perspective, we introduce SAFAR which is a monolingual framework developed in accordance with software engineering requirements and dedicated to Arabic language, especially, the modern standard Arabic and Moroccan dialect. After one decade of integration and development, SAFAR possesses today more than 50 tools and resources that can be exploited either using its API or using its web interface",
    "checked": true,
    "id": "a04b1739e9ee6e903c5a4300102ab8895e37d7d6",
    "semantic_title": "a description and demonstration of safar framework",
    "citation_count": 2,
    "authors": [
      "Karim Bouzoubaa",
      "Younes Jaafar",
      "Driss Namly",
      "Ridouane Tachicart",
      "Rachida Tajmout",
      "Hakima Khamar",
      "Hamid Jaafar",
      "Lhoussain Aouragh",
      "Abdellah Yousfi"
    ]
  },
  "https://aclanthology.org/2021.eacl-demos.17": {
    "title": "InterpreT: An Interactive Visualization Tool for Interpreting Transformers",
    "volume": "demo",
    "abstract": "With the increasingly widespread use of Transformer-based models for NLU/NLP tasks, there is growing interest in understanding the inner workings of these models, why they are so effective at a wide range of tasks, and how they can be further tuned and improved. To contribute towards this goal of enhanced explainability and comprehension, we present InterpreT, an interactive visualization tool for interpreting Transformer-based models. In addition to providing various mechanisms for investigating general model behaviours, novel contributions made in InterpreT include the ability to track and visualize token embeddings through each layer of a Transformer, highlight distances between certain token embeddings through illustrative plots, and identify task-related functions of attention heads by using new metrics. InterpreT is a task agnostic tool, and its functionalities are demonstrated through the analysis of model behaviours for two disparate tasks: Aspect Based Sentiment Analysis (ABSA) and the Winograd Schema Challenge (WSC)",
    "checked": true,
    "id": "059cd00d47eebfdea4fd74d430c9aa6a86e5ad56",
    "semantic_title": "interpret: an interactive visualization tool for interpreting transformers",
    "citation_count": 14,
    "authors": [
      "Vasudev Lal",
      "Arden Ma",
      "Estelle Aflalo",
      "Phillip Howard",
      "Ana Simoes",
      "Daniel Korat",
      "Oren Pereg",
      "Gadi Singer",
      "Moshe Wasserblat"
    ]
  },
  "https://aclanthology.org/2021.eacl-demos.18": {
    "title": "Representing ELMo embeddings as two-dimensional text online",
    "volume": "demo",
    "abstract": "We describe a new addition to the WebVectors toolkit which is used to serve word embedding models over the Web. The new ELMoViz module adds support for contextualized embedding architectures, in particular for ELMo models. The provided visualizations follow the metaphor of ‘two-dimensional text' by showing lexical substitutes: words which are most semantically similar in context to the words of the input sentence. The system allows the user to change the ELMo layers from which token embeddings are inferred. It also conveys corpus information about the query words and their lexical substitutes (namely their frequency tiers and parts of speech). The module is well integrated into the rest of the WebVectors toolkit, providing lexical hyperlinks to word representations in static embedding models. Two web services have already implemented the new functionality with pre-trained ELMo models for Russian, Norwegian and English",
    "checked": true,
    "id": "425550a0ca406a89b2f25fb67cb1319d84435bab",
    "semantic_title": "representing elmo embeddings as two-dimensional text online",
    "citation_count": 1,
    "authors": [
      "Andrey Kutuzov",
      "Elizaveta Kuzmenko"
    ]
  },
  "https://aclanthology.org/2021.eacl-demos.19": {
    "title": "LOME: Large Ontology Multilingual Extraction",
    "volume": "demo",
    "abstract": "We present LOME, a system for performing multilingual information extraction. Given a text document as input, our core system identifies spans of textual entity and event mentions with a FrameNet (Baker et al., 1998) parser. It subsequently performs coreference resolution, fine-grained entity typing, and temporal relation prediction between events. By doing so, the system constructs an event and entity focused knowledge graph. We can further apply third-party modules for other types of annotation, like relation extraction. Our (multilingual) first-party modules either outperform or are competitive with the (monolingual) state-of-the-art. We achieve this through the use of multilingual encoders like XLM-R (Conneau et al., 2020) and leveraging multilingual training data. LOME is available as a Docker container on Docker Hub. In addition, a lightweight version of the system is accessible as a web demo",
    "checked": true,
    "id": "54bb329be4b557d38e0628b651e7074524d35be2",
    "semantic_title": "lome: large ontology multilingual extraction",
    "citation_count": 34,
    "authors": [
      "Patrick Xia",
      "Guanghui Qin",
      "Siddharth Vashishtha",
      "Yunmo Chen",
      "Tongfei Chen",
      "Chandler May",
      "Craig Harman",
      "Kyle Rawlins",
      "Aaron Steven White",
      "Benjamin Van Durme"
    ]
  },
  "https://aclanthology.org/2021.eacl-demos.20": {
    "title": "MadDog: A Web-based System for Acronym Identification and Disambiguation",
    "volume": "demo",
    "abstract": "Acronyms and abbreviations are the short-form of longer phrases and they are ubiquitously employed in various types of writing. Despite their usefulness to save space in writing and reader's time in reading, they also provide challenges for understanding the text especially if the acronym is not defined in the text or if it is used far from its definition in long texts. To alleviate this issue, there are considerable efforts both from the research community and software developers to build systems for identifying acronyms and finding their correct meanings in the text. However, none of the existing works provide a unified solution capable of processing acronyms in various domains and to be publicly available. Thus, we provide the first web-based acronym identification and disambiguation system which can process acronyms from various domains including scientific, biomedical, and general domains. The web-based system is publicly available at http://iq.cs.uoregon.edu:5000 and a demo video is available at https://youtu.be/IkSh7LqI42M. The system source code is also available at https://github.com/amirveyseh/MadDog",
    "checked": true,
    "id": "e36ac7607346bff2fbcfc2627fc86acd880a247e",
    "semantic_title": "maddog: a web-based system for acronym identification and disambiguation",
    "citation_count": 12,
    "authors": [
      "Amir Pouran Ben Veyseh",
      "Franck Dernoncourt",
      "Walter Chang",
      "Thien Huu Nguyen"
    ]
  },
  "https://aclanthology.org/2021.eacl-demos.21": {
    "title": "Graph Matching and Graph Rewriting: GREW tools for corpus exploration, maintenance and conversion",
    "volume": "demo",
    "abstract": "This article presents a set of tools built around the Graph Rewriting computational framework which can be used to compute complex rule-based transformations on linguistic structures. Application of the graph matching mechanism for corpus exploration, error mining or quantitative typology are also given",
    "checked": true,
    "id": "a7669927094681eb4e2105ae5a6c119e3f9eeac4",
    "semantic_title": "graph matching and graph rewriting: grew tools for corpus exploration, maintenance and conversion",
    "citation_count": 29,
    "authors": [
      "Bruno Guillaume"
    ]
  },
  "https://aclanthology.org/2021.eacl-demos.22": {
    "title": "Massive Choice, Ample Tasks (MaChAmp): A Toolkit for Multi-task Learning in NLP",
    "volume": "demo",
    "abstract": "Transfer learning, particularly approaches that combine multi-task learning with pre-trained contextualized embeddings and fine-tuning, have advanced the field of Natural Language Processing tremendously in recent years. In this paper we present MaChAmp, a toolkit for easy fine-tuning of contextualized embeddings in multi-task settings. The benefits of MaChAmp are its flexible configuration options, and the support of a variety of natural language processing tasks in a uniform toolkit, from text classification and sequence labeling to dependency parsing, masked language modeling, and text generation",
    "checked": true,
    "id": "11ab5cc2f119d3d4f445d14807b2cbc33792bff1",
    "semantic_title": "massive choice, ample tasks (machamp): a toolkit for multi-task learning in nlp",
    "citation_count": 87,
    "authors": [
      "Rob van der Goot",
      "Ahmet Üstün",
      "Alan Ramponi",
      "Ibrahim Sharaf",
      "Barbara Plank"
    ]
  },
  "https://aclanthology.org/2021.eacl-demos.23": {
    "title": "SCoT: Sense Clustering over Time: a tool for the analysis of lexical change",
    "volume": "demo",
    "abstract": "We present Sense Clustering over Time (SCoT), a novel network-based tool for analysing lexical change. SCoT represents the meanings of a word as clusters of similar words. It visualises their formation, change, and demise. There are two main approaches to the exploration of dynamic networks: the discrete one compares a series of clustered graphs from separate points in time. The continuous one analyses the changes of one dynamic network over a time-span. SCoT offers a new hybrid solution. First, it aggregates time-stamped documents into intervals and calculates one sense graph per discrete interval. Then, it merges the static graphs to a new type of dynamic semantic neighbourhood graph over time. The resulting sense clusters offer uniquely detailed insights into lexical change over continuous intervals with model transparency and provenance. SCoT has been successfully used in a European study on the changing meaning of ‘crisis'",
    "checked": true,
    "id": "9d794093dac5425978c3604f1120abb9c1833a0e",
    "semantic_title": "scot: sense clustering over time: a tool for the analysis of lexical change",
    "citation_count": 4,
    "authors": [
      "Christian Haase",
      "Saba Anwar",
      "Seid Muhie Yimam",
      "Alexander Friedrich",
      "Chris Biemann"
    ]
  },
  "https://aclanthology.org/2021.eacl-demos.24": {
    "title": "GCM: A Toolkit for Generating Synthetic Code-mixed Text",
    "volume": "demo",
    "abstract": "Code-mixing is common in multilingual communities around the world, and processing it is challenging due to the lack of labeled and unlabeled data. We describe a tool that can automatically generate code-mixed data given parallel data in two languages. We implement two linguistic theories of code-mixing, the Equivalence Constraint theory and the Matrix Language theory to generate all possible code-mixed sentences in the language-pair, followed by sampling of the generated data to generate natural code-mixed sentences. The toolkit provides three modes: a batch mode, an interactive library mode and a web-interface to address the needs of researchers, linguists and language experts. The toolkit can be used to generate unlabeled text data for pre-trained models, as well as visualize linguistic theories of code-mixing. We plan to release the toolkit as open source and extend it by adding more implementations of linguistic theories, visualization techniques and better sampling techniques. We expect that the release of this toolkit will help facilitate more research in code-mixing in diverse language pairs",
    "checked": true,
    "id": "5718dff1a135c8c4a99ef247522fedd10c64f6fd",
    "semantic_title": "gcm: a toolkit for generating synthetic code-mixed text",
    "citation_count": 29,
    "authors": [
      "Mohd Sanad Zaki Rizvi",
      "Anirudh Srinivasan",
      "Tanuja Ganu",
      "Monojit Choudhury",
      "Sunayana Sitaram"
    ]
  },
  "https://aclanthology.org/2021.eacl-demos.25": {
    "title": "T2NER: Transformers based Transfer Learning Framework for Named Entity Recognition",
    "volume": "demo",
    "abstract": "Recent advances in deep transformer models have achieved state-of-the-art in several natural language processing (NLP) tasks, whereas named entity recognition (NER) has traditionally benefited from long-short term memory (LSTM) networks. In this work, we present a Transformers based Transfer Learning framework for Named Entity Recognition (T2NER) created in PyTorch for the task of NER with deep transformer models. The framework is built upon the Transformers library as the core modeling engine and supports several transfer learning scenarios from sequential transfer to domain adaptation, multi-task learning, and semi-supervised learning. It aims to bridge the gap between the algorithmic advances in these areas by combining them with the state-of-the-art in transformer models to provide a unified platform that is readily extensible and can be used for both the transfer learning research in NER, and for real-world applications. The framework is available at: https://github.com/suamin/t2ner",
    "checked": true,
    "id": "bbadce9131eb08c98bad7d944ce5b366a41abb9b",
    "semantic_title": "t2ner: transformers based transfer learning framework for named entity recognition",
    "citation_count": 6,
    "authors": [
      "Saadullah Amin",
      "Guenter Neumann"
    ]
  },
  "https://aclanthology.org/2021.eacl-demos.26": {
    "title": "European Language Grid: A Joint Platform for the European Language Technology Community",
    "volume": "demo",
    "abstract": "Europe is a multilingual society, in which dozens of languages are spoken. The only option to enable and to benefit from multilingualism is through Language Technologies (LT), i.e., Natural Language Processing and Speech Technologies. We describe the European Language Grid (ELG), which is targeted to evolve into the primary platform and marketplace for LT in Europe by providing one umbrella platform for the European LT landscape, including research and industry, enabling all stakeholders to upload, share and distribute their services, products and resources. At the end of our EU project, which will establish a legal entity in 2022, the ELG will provide access to approx. 1300 services for all European languages as well as thousands of data sets",
    "checked": true,
    "id": "49343df9383e9f4f59e56b307277fc4ff75700da",
    "semantic_title": "european language grid: a joint platform for the european language technology community",
    "citation_count": 13,
    "authors": [
      "Georg Rehm",
      "Stelios Piperidis",
      "Kalina Bontcheva",
      "Jan Hajic",
      "Victoria Arranz",
      "Andrejs Vasiļjevs",
      "Gerhard Backfried",
      "Jose Manuel Gomez-Perez",
      "Ulrich Germann",
      "Rémi Calizzano",
      "Nils Feldhus",
      "Stefanie Hegele",
      "Florian Kintzel",
      "Katrin Marheinecke",
      "Julian Moreno-Schneider",
      "Dimitris Galanis",
      "Penny Labropoulou",
      "Miltos Deligiannis",
      "Katerina Gkirtzou",
      "Athanasia Kolovou",
      "Dimitris Gkoumas",
      "Leon Voukoutis",
      "Ian Roberts",
      "Jana Hamrlova",
      "Dusan Varis",
      "Lukas Kacena",
      "Khalid Choukri",
      "Valérie Mapelli",
      "Mickaël Rigault",
      "Julija Melnika",
      "Miro Janosik",
      "Katja Prinz",
      "Andres Garcia-Silva",
      "Cristian Berrio",
      "Ondrej Klejch",
      "Steve Renals"
    ]
  },
  "https://aclanthology.org/2021.eacl-demos.27": {
    "title": "A New Surprise Measure for Extracting Interesting Relationships between Persons",
    "volume": "demo",
    "abstract": "One way to enhance user engagement in search engines is to suggest interesting facts to the user. Although relationships between persons are important as a target for text mining, there are few effective approaches for extracting the interesting relationships between persons. We therefore propose a method for extracting interesting relationships between persons from natural language texts by focusing on their surprisingness. Our method first extracts all personal relationships from dependency trees for the texts and then calculates surprise scores for distributed representations of the extracted relationships in an unsupervised manner. The unique point of our method is that it does not require any labeled dataset with annotation for the surprising personal relationships. The results of the human evaluation show that the proposed method could extract more interesting relationships between persons from Japanese Wikipedia articles than a popularity-based baseline method. We demonstrate our proposed method as a chrome plugin on google search",
    "checked": true,
    "id": "7ed2a4121ec7d3aec9719b52b2cc3b37680f5b55",
    "semantic_title": "a new surprise measure for extracting interesting relationships between persons",
    "citation_count": 1,
    "authors": [
      "Hidetaka Kamigaito",
      "Jingun Kwon",
      "Young-In Song",
      "Manabu Okumura"
    ]
  },
  "https://aclanthology.org/2021.eacl-demos.28": {
    "title": "Paladin: an annotation tool based on active and proactive learning",
    "volume": "demo",
    "abstract": "In this paper, we present Paladin, an open-source web-based annotation tool for creating high-quality multi-label document-level datasets. By integrating active learning and proactive learning to the annotation task, Paladin makes the task less time-consuming and requiring less human effort. Although Paladin is designed for multi-label settings, the system is flexible and can be adapted to other tasks in single-label settings",
    "checked": true,
    "id": "11541b0754b688e6db0bd7672c9bc2e84c69f55a",
    "semantic_title": "paladin: an annotation tool based on active and proactive learning",
    "citation_count": 14,
    "authors": [
      "Minh-Quoc Nghiem",
      "Paul Baylis",
      "Sophia Ananiadou"
    ]
  },
  "https://aclanthology.org/2021.eacl-demos.29": {
    "title": "Story Centaur: Large Language Model Few Shot Learning as a Creative Writing Tool",
    "volume": "demo",
    "abstract": "Few shot learning with large language models has the potential to give individuals without formal machine learning training the access to a wide range of text to text models. We consider how this applies to creative writers and present Story Centaur, a user interface for prototyping few shot models and a set of recombinable web components that deploy them. Story Centaur's goal is to expose creative writers to few shot learning with a simple but powerful interface that lets them compose their own co-creation tools that further their own unique artistic directions. We build out several examples of such tools, and in the process probe the boundaries and issues surrounding generation with large language models",
    "checked": true,
    "id": "7f7044414101c5e00c8f4f0156083c6925e40886",
    "semantic_title": "story centaur: large language model few shot learning as a creative writing tool",
    "citation_count": 38,
    "authors": [
      "Ben Swanson",
      "Kory Mathewson",
      "Ben Pietrzak",
      "Sherol Chen",
      "Monica Dinalescu"
    ]
  },
  "https://aclanthology.org/2021.eacl-demos.30": {
    "title": "FrameForm: An Open-source Annotation Interface for FrameNet",
    "volume": "demo",
    "abstract": "In this paper, we introduce FrameForm, an open-source annotation tool designed to accommodate predicate annotations based on Frame Semantics. FrameForm is a user-friendly tool for creating, annotating and maintaining computational lexicography projects like FrameNet and has been used while building the Turkish FrameNet. Responsive and open-source, FrameForm can be easily modified to answer the annotation needs of a wide range of different languages",
    "checked": true,
    "id": "b02804c9a6db4e3f8549bc846046b4e2e646e3c0",
    "semantic_title": "frameform: an open-source annotation interface for framenet",
    "citation_count": 0,
    "authors": [
      "Büşra Marşan",
      "Olcay Taner Yıldız"
    ]
  },
  "https://aclanthology.org/2021.eacl-demos.31": {
    "title": "OCTIS: Comparing and Optimizing Topic models is Simple!",
    "volume": "demo",
    "abstract": "In this paper, we present OCTIS, a framework for training, analyzing, and comparing Topic Models, whose optimal hyper-parameters are estimated using a Bayesian Optimization approach. The proposed solution integrates several state-of-the-art topic models and evaluation metrics. These metrics can be targeted as objective by the underlying optimization procedure to determine the best hyper-parameter configuration. OCTIS allows researchers and practitioners to have a fair comparison between topic models of interest, using several benchmark datasets and well-known evaluation metrics, to integrate novel algorithms, and to have an interactive visualization of the results for understanding the behavior of each model. The code is available at the following link: https://github.com/MIND-Lab/OCTIS",
    "checked": true,
    "id": "ac0463750b9ba1ea2d1f9d7d16a00cbaae2e5507",
    "semantic_title": "octis: comparing and optimizing topic models is simple!",
    "citation_count": 64,
    "authors": [
      "Silvia Terragni",
      "Elisabetta Fersini",
      "Bruno Giovanni Galuzzi",
      "Pietro Tropeano",
      "Antonio Candelieri"
    ]
  },
  "https://aclanthology.org/2021.eacl-demos.32": {
    "title": "ELITR Multilingual Live Subtitling: Demo and Strategy",
    "volume": "demo",
    "abstract": "This paper presents an automatic speech translation system aimed at live subtitling of conference presentations. We describe the overall architecture and key processing components. More importantly, we explain our strategy for building a complex system for end-users from numerous individual components, each of which has been tested only in laboratory conditions. The system is a working prototype that is routinely tested in recognizing English, Czech, and German speech and presenting it translated simultaneously into 42 target languages",
    "checked": true,
    "id": "6e45bac68d46d9832721add3c67f74e8ee070889",
    "semantic_title": "elitr multilingual live subtitling: demo and strategy",
    "citation_count": 16,
    "authors": [
      "Ondřej Bojar",
      "Dominik Macháček",
      "Sangeet Sagar",
      "Otakar Smrž",
      "Jonáš Kratochvíl",
      "Peter Polák",
      "Ebrahim Ansari",
      "Mohammad Mahmoudi",
      "Rishu Kumar",
      "Dario Franceschini",
      "Chiara Canton",
      "Ivan Simonini",
      "Thai-Son Nguyen",
      "Felix Schneider",
      "Sebastian Stüker",
      "Alex Waibel",
      "Barry Haddow",
      "Rico Sennrich",
      "Philip Williams"
    ]
  },
  "https://aclanthology.org/2021.eacl-demos.33": {
    "title": "Breaking Writer's Block: Low-cost Fine-tuning of Natural Language Generation Models",
    "volume": "demo",
    "abstract": "It is standard procedure these days to solve Information Extraction task by fine-tuning large pre-trained language models. This is not the case for generation task, which relies on a variety of techniques for controlled language generation. In this paper, we describe a system that fine-tunes a natural language generation model for the problem of solving writer's block. The fine-tuning changes the conditioning to also include the right context in addition to the left context, as well as an optional list of entities, the size, the genre and a summary of the paragraph that the human author wishes to generate. Our proposed fine-tuning obtains excellent results, even with a small number of epochs and a total cost of USD 150. The system can be accessed as a web-service and all the code is released. A video showcasing the interface and the model is also available",
    "checked": true,
    "id": "9fefc1be1867d1bf691ece0d9bef974adfade7ca",
    "semantic_title": "breaking writer's block: low-cost fine-tuning of natural language generation models",
    "citation_count": 6,
    "authors": [
      "Alexandre Duval",
      "Thomas Lamson",
      "Gaël de Léséleuc de Kérouara",
      "Matthias Gallé"
    ]
  },
  "https://aclanthology.org/2021.eacl-demos.34": {
    "title": "OPUS-CAT: Desktop NMT with CAT integration and local fine-tuning",
    "volume": "demo",
    "abstract": "OPUS-CAT is a collection of software which enables translators to use neural machine translation in computer-assisted translation tools without exposing themselves to security and confidentiality risks inherent in online machine translation. OPUS-CAT uses the public OPUS-MT machine translation models, which are available for over a thousand language pairs. The generic OPUS-MT models can be fine-tuned with OPUS-CAT on the desktop using data for a specific client or domain",
    "checked": true,
    "id": "561d08b4fdb2af502bb91b1d8ed80645d8f81c41",
    "semantic_title": "opus-cat: desktop nmt with cat integration and local fine-tuning",
    "citation_count": 4,
    "authors": [
      "Tommi Nieminen"
    ]
  },
  "https://aclanthology.org/2021.eacl-demos.35": {
    "title": "Domain Expert Platform for Goal-Oriented Dialog Collection",
    "volume": "demo",
    "abstract": "Today, most dialogue systems are fully or partly built using neural network architectures. A crucial prerequisite for the creation of a goal-oriented neural network dialogue system is a dataset that represents typical dialogue scenarios and includes various semantic annotations, e.g. intents, slots and dialogue actions, that are necessary for training a particular neural network architecture. In this demonstration paper, we present an easy to use interface and its back-end which is oriented to domain experts for the collection of goal-oriented dialogue samples. The platform not only allows to collect or write sample dialogues in a structured way, but also provides a means for simple annotation and interpretation of the dialogues. The platform itself is language-independent; it depends only on the availability of particular language processing components for a specific language. It is currently being used to collect dialogue samples in Latvian (a highly inflected language) which represent typical communication between students and the student service",
    "checked": true,
    "id": "83acaf32b3004e3f45e5c12ffbc28622703507b7",
    "semantic_title": "domain expert platform for goal-oriented dialog collection",
    "citation_count": 1,
    "authors": [
      "Didzis Goško",
      "Arturs Znotins",
      "Inguna Skadina",
      "Normunds Gruzitis",
      "Gunta Nešpore-Bērzkalne"
    ]
  },
  "https://aclanthology.org/2021.eacl-demos.36": {
    "title": "Which is Better for Deep Learning: Python or MATLAB? Answering Comparative Questions in Natural Language",
    "volume": "demo",
    "abstract": "We present a system for answering comparative questions (Is X better than Y with respect to Z?) in natural language. Answering such questions is important for assisting humans in making informed decisions. The key component of our system is a natural language interface for comparative QA that can be used in personal assistants, chatbots, and similar NLP devices. Comparative QA is a challenging NLP task, since it requires collecting support evidence from many different sources, and direct comparisons of rare objects may be not available even on the entire Web. We take the first step towards a solution for such a task offering a testbed for comparative QA in natural language by probing several methods, making the three best ones available as an online demo",
    "checked": true,
    "id": "45d9d0d9b74605135e3e0dfe3b84661952013760",
    "semantic_title": "which is better for deep learning: python or matlab? answering comparative questions in natural language",
    "citation_count": 7,
    "authors": [
      "Viktoriia Chekalina",
      "Alexander Bondarenko",
      "Chris Biemann",
      "Meriem Beloucif",
      "Varvara Logacheva",
      "Alexander Panchenko"
    ]
  },
  "https://aclanthology.org/2021.eacl-demos.37": {
    "title": "PunKtuator: A Multilingual Punctuation Restoration System for Spoken and Written Text",
    "volume": "demo",
    "abstract": "Text transcripts without punctuation or sentence boundaries are hard to comprehend for both humans and machines. Punctuation marks play a vital role by providing meaning to the sentence and incorrect use or placement of punctuation marks can often alter it. This can impact downstream tasks such as language translation and understanding, pronoun resolution, text summarization, etc. for humans and machines. An automated punctuation restoration (APR) system with minimal human intervention can improve comprehension of text and help users write better. In this paper we describe a multitask modeling approach as a system to restore punctuation in multiple high resource – Germanic (English and German), Romanic (French)– and low resource languages – Indo-Aryan (Hindi) Dravidian (Tamil) – that does not require extensive knowledge of grammar or syntax of a given language for both spoken and written form of text. For German language and the given Indic based languages this is the first towards restoring punctuation and can serve as a baseline for future work",
    "checked": true,
    "id": "b98c13b6be78b728ed4628c66f4dc2619711f01b",
    "semantic_title": "punktuator: a multilingual punctuation restoration system for spoken and written text",
    "citation_count": 9,
    "authors": [
      "Varnith Chordia"
    ]
  },
  "https://aclanthology.org/2021.eacl-demos.38": {
    "title": "Conversational Agent for Daily Living Assessment Coaching Demo",
    "volume": "demo",
    "abstract": "Conversational Agent for Daily Living Assessment Coaching (CADLAC) is a multi-modal conversational agent system designed to impersonate \"individuals\" with various levels of ability in activities of daily living (ADLs: e.g., dressing, bathing, mobility, etc.) for use in training professional assessors how to conduct interviews to determine one's level of functioning. The system is implemented on the MindMeld platform for conversational AI and features a Bidirectional Long Short-Term Memory topic tracker that allows the agent to navigate conversations spanning 18 different ADL domains, a dialogue manager that interfaces with a database of over 10,000 historical ADL assessments, a rule-based Natural Language Generation (NLG) module, and a pre-trained open-domain conversational sub-agent (based on GPT-2) for handling conversation turns outside of the 18 ADL domains. CADLAC is delivered via state-of-the-art web frameworks to handle multiple conversations and users simultaneously and is enabled with voice interface. The paper includes a description of the system design and evaluation of individual components followed by a brief discussion of current limitations and next steps",
    "checked": true,
    "id": "1efa6e84977b8cab834a1c47a6a079a111b147b9",
    "semantic_title": "conversational agent for daily living assessment coaching demo",
    "citation_count": 3,
    "authors": [
      "Raymond Finzel",
      "Aditya Gaydhani",
      "Sheena Dufresne",
      "Maria Gini",
      "Serguei Pakhomov"
    ]
  },
  "https://aclanthology.org/2021.eacl-demos.39": {
    "title": "HULK: An Energy Efficiency Benchmark Platform for Responsible Natural Language Processing",
    "volume": "demo",
    "abstract": "Computation-intensive pretrained models have been taking the lead of many natural language processing benchmarks such as GLUE. However, energy efficiency in the process of model training and inference becomes a critical bottleneck. We introduce HULK, a multi-task energy efficiency benchmarking platform for responsible natural language processing. With HULK, we compare pretrained models' energy efficiency from the perspectives of time and cost. Baseline benchmarking results are provided for further analysis. The fine-tuning efficiency of different pretrained models can differ significantly among different tasks, and fewer parameter number does not necessarily imply better efficiency. We analyzed such a phenomenon and demonstrated the method for comparing the multi-task efficiency of pretrained models. Our platform is available at https://hulkbenchmark.github.io/",
    "checked": true,
    "id": "c26f90d4cfa33ceff373cf49c2a534e2004685da",
    "semantic_title": "hulk: an energy efficiency benchmark platform for responsible natural language processing",
    "citation_count": 28,
    "authors": [
      "Xiyou Zhou",
      "Zhiyu Chen",
      "Xiaoyong Jin",
      "William Yang Wang"
    ]
  },
  "https://aclanthology.org/2021.eacl-srw.1": {
    "title": "Computationally Efficient Wasserstein Loss for Structured Labels",
    "volume": "student",
    "abstract": "The problem of estimating the probability distribution of labels has been widely studied as a label distribution learning (LDL) problem, whose applications include age estimation, emotion analysis, and semantic segmentation. We propose a tree-Wasserstein distance regularized LDL algorithm, focusing on hierarchical text classification tasks. We propose predicting the entire label hierarchy using neural networks, where the similarity between predicted and true labels is measured using the tree-Wasserstein distance. Through experiments using synthetic and real-world datasets, we demonstrate that the proposed method successfully considers the structure of labels during training, and it compares favorably with the Sinkhorn algorithm in terms of computation time and memory usage",
    "checked": true,
    "id": "97098fb32003bb817cc14688595b2ed7d74d53ef",
    "semantic_title": "computationally efficient wasserstein loss for structured labels",
    "citation_count": 2,
    "authors": [
      "Ayato Toyokuni",
      "Sho Yokoi",
      "Hisashi Kashima",
      "Makoto Yamada"
    ]
  },
  "https://aclanthology.org/2021.eacl-srw.2": {
    "title": "Have Attention Heads in BERT Learned Constituency Grammar?",
    "volume": "student",
    "abstract": "With the success of pre-trained language models in recent years, more and more researchers focus on opening the \"black box\" of these models. Following this interest, we carry out a qualitative and quantitative analysis of constituency grammar in attention heads of BERT and RoBERTa. We employ the syntactic distance method to extract implicit constituency grammar from the attention weights of each head. Our results show that there exist heads that can induce some grammar types much better than baselines, suggesting that some heads act as a proxy for constituency grammar. We also analyze how attention heads' constituency grammar inducing (CGI) ability changes after fine-tuning with two kinds of tasks, including sentence meaning similarity (SMS) tasks and natural language inference (NLI) tasks. Our results suggest that SMS tasks decrease the average CGI ability of upper layers, while NLI tasks increase it. Lastly, we investigate the connections between CGI ability and natural language understanding ability on QQP and MNLI tasks",
    "checked": true,
    "id": "a5af4e1b100aa307c81427444d08f8bb25358d3c",
    "semantic_title": "have attention heads in bert learned constituency grammar?",
    "citation_count": 3,
    "authors": [
      "Ziyang Luo"
    ]
  },
  "https://aclanthology.org/2021.eacl-srw.3": {
    "title": "Do we read what we hear? Modeling orthographic influences on spoken word recognition",
    "volume": "student",
    "abstract": "Theories and models of spoken word recognition aim to explain the process of accessing lexical knowledge given an acoustic realization of a word form. There is consensus that phonological and semantic information is crucial for this process. However, there is accumulating evidence that orthographic information could also have an impact on auditory word recognition. This paper presents two models of spoken word recognition that instantiate different hypotheses regarding the influence of orthography on this process. We show that these models reproduce human-like behavior in different ways and provide testable hypotheses for future research on the source of orthographic effects in spoken word recognition",
    "checked": true,
    "id": "bb717b9b4e393992bb680e8fdc4b53706aeae6ac",
    "semantic_title": "do we read what we hear? modeling orthographic influences on spoken word recognition",
    "citation_count": 1,
    "authors": [
      "Nicole Macher",
      "Badr M. Abdullah",
      "Harm Brouwer",
      "Dietrich Klakow"
    ]
  },
  "https://aclanthology.org/2021.eacl-srw.4": {
    "title": "PENELOPIE: Enabling Open Information Extraction for the Greek Language through Machine Translation",
    "volume": "student",
    "abstract": "In this work, we present a methodology that aims at bridging the gap between high and low-resource languages in the context of Open Information Extraction, showcasing it on the Greek language. The goals of this paper are twofold: First, we build Neural Machine Translation (NMT) models for English-to-Greek and Greek-to-English based on the Transformer architecture. Second, we leverage these NMT models to produce English translations of Greek text as input for our NLP pipeline, to which we apply a series of pre-processing and triple extraction tasks. Finally, we back-translate the extracted triples to Greek. We conduct an evaluation of both our NMT and OIE methods on benchmark datasets and demonstrate that our approach outperforms the current state-of-the-art for the Greek natural language",
    "checked": true,
    "id": "4b89088729ea4a989447ebb56caaad18be646e74",
    "semantic_title": "penelopie: enabling open information extraction for the greek language through machine translation",
    "citation_count": 6,
    "authors": [
      "Dimitris Papadopoulos",
      "Nikolaos Papadakis",
      "Nikolaos Matsatsinis"
    ]
  },
  "https://aclanthology.org/2021.eacl-srw.5": {
    "title": "A Computational Analysis of Vagueness in Revisions of Instructional Texts",
    "volume": "student",
    "abstract": "WikiHow is an open-domain repository of instructional articles for a variety of tasks, which can be revised by users. In this paper, we extract pairwise versions of an instruction before and after a revision was made. Starting from a noisy dataset of revision histories, we specifically extract and analyze edits that involve cases of vagueness in instructions. We further investigate the ability of a neural model to distinguish between two versions of an instruction in our data by adopting a pairwise ranking task from previous work and showing improvements over existing baselines",
    "checked": true,
    "id": "eef7d918e7d67cde0a0a28a95afe29983fc5a935",
    "semantic_title": "a computational analysis of vagueness in revisions of instructional texts",
    "citation_count": 9,
    "authors": [
      "Alok Debnath",
      "Michael Roth"
    ]
  },
  "https://aclanthology.org/2021.eacl-srw.6": {
    "title": "A reproduction of Apple's bi-directional LSTM models for language identification in short strings",
    "volume": "student",
    "abstract": "Language Identification is the task of identifying a document's language. For applications like automatic spell checker selection, language identification must use very short strings such as text message fragments. In this work, we reproduce a language identification architecture that Apple briefly sketched in a blog post. We confirm the bi-LSTM model's performance and find that it outperforms current open-source language identifiers. We further find that its language identification mistakes are due to confusion between related languages",
    "checked": true,
    "id": "70ae152b27ce97f621c3ef4ac55a71c60f33a768",
    "semantic_title": "a reproduction of apple's bi-directional lstm models for language identification in short strings",
    "citation_count": 11,
    "authors": [
      "Mads Toftrup",
      "Søren Asger Sørensen",
      "Manuel R. Ciosici",
      "Ira Assent"
    ]
  },
  "https://aclanthology.org/2021.eacl-srw.7": {
    "title": "Automatically Cataloging Scholarly Articles using Library of Congress Subject Headings",
    "volume": "student",
    "abstract": "Institutes are required to catalog their articles with proper subject headings so that the users can easily retrieve relevant articles from the institutional repositories. However, due to the rate of proliferation of the number of articles in these repositories, it is becoming a challenge to manually catalog the newly added articles at the same pace. To address this challenge, we explore the feasibility of automatically annotating articles with Library of Congress Subject Headings (LCSH). We first use web scraping to extract keywords for a collection of articles from the Repository Analytics and Metrics Portal (RAMP). Then, we map these keywords to LCSH names for developing a gold-standard dataset. As a case study, using the subset of Biology-related LCSH concepts, we develop predictive models by formulating this task as a multi-label classification problem. Our experimental results demonstrate the viability of this approach for predicting LCSH for scholarly articles",
    "checked": true,
    "id": "dc240302b7c8577504bcdcbf8a5e3a5441db83c7",
    "semantic_title": "automatically cataloging scholarly articles using library of congress subject headings",
    "citation_count": 2,
    "authors": [
      "Nazmul Kazi",
      "Nathaniel Lane",
      "Indika Kahanda"
    ]
  },
  "https://aclanthology.org/2021.eacl-srw.8": {
    "title": "Model Agnostic Answer Reranking System for Adversarial Question Answering",
    "volume": "student",
    "abstract": "While numerous methods have been proposed as defenses against adversarial examples in question answering (QA), these techniques are often model specific, require retraining of the model, and give only marginal improvements in performance over vanilla models. In this work, we present a simple model-agnostic approach to this problem that can be applied directly to any QA model without any retraining. Our method employs an explicit answer candidate reranking mechanism that scores candidate answers on the basis of their content overlap with the question before making the final prediction. Combined with a strong base QAmodel, our method outperforms state-of-the-art defense techniques, calling into question how well these techniques are actually doing and strong these adversarial testbeds are",
    "checked": true,
    "id": "c624d7fbdf3a16af97fe7393b16b44d19f99a00d",
    "semantic_title": "model agnostic answer reranking system for adversarial question answering",
    "citation_count": 4,
    "authors": [
      "Sagnik Majumder",
      "Chinmoy Samant",
      "Greg Durrett"
    ]
  },
  "https://aclanthology.org/2021.eacl-srw.9": {
    "title": "BERT meets Cranfield: Uncovering the Properties of Full Ranking on Fully Labeled Data",
    "volume": "student",
    "abstract": "Recently, various information retrieval models have been proposed based on pre-trained BERT models, achieving outstanding performance. The majority of such models have been tested on data collections with partial relevance labels, where various potentially relevant documents have not been exposed to the annotators. Therefore, evaluating BERT-based rankers may lead to biased and unfair evaluation results, simply because a relevant document has not been exposed to the annotators while creating the collection. In our work, we aim to better understand a BERT-based ranker's strengths compared to a BERT-based re-ranker and the initial ranker. To this aim, we investigate BERT-based rankers performance on the Cranfield collection, which comes with full relevance judgment on all documents in the collection. Our results demonstrate the BERT-based full ranker's effectiveness, as opposed to the BERT-based re-ranker and BM25. Also, analysis shows that there are documents that the BERT-based full-ranker finds that were not found by the initial ranker",
    "checked": true,
    "id": "1919a7da564a58dca83db286206e266f835f1971",
    "semantic_title": "bert meets cranfield: uncovering the properties of full ranking on fully labeled data",
    "citation_count": 2,
    "authors": [
      "Negin Ghasemi",
      "Djoerd Hiemstra"
    ]
  },
  "https://aclanthology.org/2021.eacl-srw.10": {
    "title": "Siamese Neural Networks for Detecting Complementary Products",
    "volume": "student",
    "abstract": "Recommender systems play an important role in e-commerce websites as they improve the customer journey by helping the users find what they want at the right moment. In this paper, we focus on identifying a complementary relationship between the products of an e-commerce company. We propose a content-based recommender system for detecting complementary products, using Siamese Neural Networks (SNN). To this end, we implement and compare two different models: Siamese Convolutional Neural Network (CNN) and Siamese Long Short-Term Memory (LSTM). Moreover, we propose an extension of the SNN approach to handling millions of products in a matter of seconds, and we reduce the training time complexity by half. In the experiments, we show that Siamese LSTM can predict complementary products with an accuracy of ~85% using only the product titles",
    "checked": true,
    "id": "db6689a61c4bdda44bfb619628b4ae93734c1654",
    "semantic_title": "siamese neural networks for detecting complementary products",
    "citation_count": 12,
    "authors": [
      "Marina Angelovska",
      "Sina Sheikholeslami",
      "Bas Dunn",
      "Amir H. Payberah"
    ]
  },
  "https://aclanthology.org/2021.eacl-srw.11": {
    "title": "Contrasting distinct structured views to learn sentence embeddings",
    "volume": "student",
    "abstract": "We propose a self-supervised method that builds sentence embeddings from the combination of diverse explicit syntactic structures of a sentence. We assume structure is crucial to building consistent representations as we expect sentence meaning to be a function of both syntax and semantic aspects. In this perspective, we hypothesize that some linguistic representations might be better adapted given the considered task or sentence. We, therefore, propose to learn individual representation functions for different syntactic frameworks jointly. Again, by hypothesis, all such functions should encode similar semantic information differently and consequently, be complementary for building better sentential semantic embeddings. To assess such hypothesis, we propose an original contrastive multi-view framework that induces an explicit interaction between models during the training phase. We make experiments combining various structures such as dependency, constituency, or sequential schemes. Our results outperform comparable methods on several tasks from standard sentence embedding benchmarks",
    "checked": true,
    "id": "d1be56207d5bcd29bb6d16d32c48007d20b532c0",
    "semantic_title": "contrasting distinct structured views to learn sentence embeddings",
    "citation_count": 2,
    "authors": [
      "Antoine Simoulin",
      "Benoit Crabbé"
    ]
  },
  "https://aclanthology.org/2021.eacl-srw.12": {
    "title": "Discrete Reasoning Templates for Natural Language Understanding",
    "volume": "student",
    "abstract": "Reasoning about information from multiple parts of a passage to derive an answer is an open challenge for reading-comprehension models. In this paper, we present an approach that reasons about complex questions by decomposing them to simpler subquestions that can take advantage of single-span extraction reading-comprehension models, and derives the final answer according to instructions in a predefined reasoning template. We focus on subtraction based arithmetic questions and evaluate our approach on a subset of the DROP dataset. We show that our approach is competitive with the state of the art while being interpretable and requires little supervision",
    "checked": true,
    "id": "0f08f4458dcf7618263405cbf31e6a48684bc1fa",
    "semantic_title": "discrete reasoning templates for natural language understanding",
    "citation_count": 3,
    "authors": [
      "Hadeel Al-Negheimish",
      "Pranava Madhyastha",
      "Alessandra Russo"
    ]
  },
  "https://aclanthology.org/2021.eacl-srw.13": {
    "title": "Multilingual Email Zoning",
    "volume": "student",
    "abstract": "The segmentation of emails into functional zones (also dubbed email zoning) is a relevant preprocessing step for most NLP tasks that deal with emails. However, despite the multilingual character of emails and their applications, previous literature regarding email zoning corpora and systems was developed essentially for English. In this paper, we analyse the existing email zoning corpora and propose a new multilingual benchmark composed of 625 emails in Portuguese, Spanish and French. Moreover, we introduce OKAPI, the first multilingual email segmentation model based on a language agnostic sentence encoder. Besides generalizing well for unseen languages, our model is competitive with current English benchmarks, and reached new state-of-the-art performances for domain adaptation tasks in English",
    "checked": true,
    "id": "c5b25abba685bd50f1e3d398e2161485a4ded742",
    "semantic_title": "multilingual email zoning",
    "citation_count": 0,
    "authors": [
      "Bruno Jardim",
      "Ricardo Rei",
      "Mariana S. C. Almeida"
    ]
  },
  "https://aclanthology.org/2021.eacl-srw.14": {
    "title": "Familiar words but strange voices: Modelling the influence of speech variability on word recognition",
    "volume": "student",
    "abstract": "We present a deep neural model of spoken word recognition which is trained to retrieve the meaning of a word (in the form of a word embedding) given its spoken form, a task which resembles that faced by a human listener. Furthermore, we investigate the influence of variability in speech signals on the model's performance. To this end, we conduct of set of controlled experiments using word-aligned read speech data in German. Our experiments show that (1) the model is more sensitive to dialectical variation than gender variation, and (2) recognition performance of word cognates from related languages reflect the degree of relatedness between languages in our study. Our work highlights the feasibility of modeling human speech perception using deep neural networks",
    "checked": true,
    "id": "fce61542bbabdbbcb8294d1c4ead11939d22c091",
    "semantic_title": "familiar words but strange voices: modelling the influence of speech variability on word recognition",
    "citation_count": 3,
    "authors": [
      "Alexandra Mayn",
      "Badr M. Abdullah",
      "Dietrich Klakow"
    ]
  },
  "https://aclanthology.org/2021.eacl-srw.15": {
    "title": "Emoji-Based Transfer Learning for Sentiment Tasks",
    "volume": "student",
    "abstract": "Sentiment tasks such as hate speech detection and sentiment analysis, especially when performed on languages other than English, are often low-resource. In this study, we exploit the emotional information encoded in emojis to enhance the performance on a variety of sentiment tasks. This is done using a transfer learning approach, where the parameters learned by an emoji-based source task are transferred to a sentiment target task. We analyse the efficacy of the transfer under three conditions, i.e. i) the emoji content and ii) label distribution of the target task as well as iii) the difference between monolingually and multilingually learned source tasks. We find i.a. that the transfer is most beneficial if the target task is balanced with high emoji content. Monolingually learned source tasks have the benefit of taking into account the culturally specific use of emojis and gain up to F1 +0.280 over the baseline",
    "checked": true,
    "id": "db22ce8f53aa95f8bb00e0319b1d8b66e6895c96",
    "semantic_title": "emoji-based transfer learning for sentiment tasks",
    "citation_count": 2,
    "authors": [
      "Susann Boy",
      "Dana Ruiter",
      "Dietrich Klakow"
    ]
  },
  "https://aclanthology.org/2021.eacl-srw.16": {
    "title": "A Little Pretraining Goes a Long Way: A Case Study on Dependency Parsing Task for Low-resource Morphologically Rich Languages",
    "volume": "student",
    "abstract": "Neural dependency parsing has achieved remarkable performance for many domains and languages. The bottleneck of massive labelled data limits the effectiveness of these approaches for low resource languages. In this work, we focus on dependency parsing for morphological rich languages (MRLs) in a low-resource setting. Although morphological information is essential for the dependency parsing task, the morphological disambiguation and lack of powerful analyzers pose challenges to get this information for MRLs. To address these challenges, we propose simple auxiliary tasks for pretraining. We perform experiments on 10 MRLs in low-resource settings to measure the efficacy of our proposed pretraining method and observe an average absolute gain of 2 points (UAS) and 3.6 points (LAS)",
    "checked": true,
    "id": "98c1d25d4eaec2178bf82486e4cf2484eb1e40de",
    "semantic_title": "a little pretraining goes a long way: a case study on dependency parsing task for low-resource morphologically rich languages",
    "citation_count": 7,
    "authors": [
      "Jivnesh Sandhan",
      "Amrith Krishna",
      "Ashim Gupta",
      "Laxmidhar Behera",
      "Pawan Goyal"
    ]
  },
  "https://aclanthology.org/2021.eacl-srw.17": {
    "title": "Development of Conversational AI for Sleep Coaching Programme",
    "volume": "student",
    "abstract": "Almost 30% of the adult population in the world is experiencing or has experience insomnia. Cognitive Behaviour Therapy for insomnia (CBT-I) is one of the most effective treatment, but it has limitations on accessibility and availability. Utilising technology is one of the possible solutions, but existing methods neglect conversational aspects, which plays a critical role in sleep therapy. To address this issue, we propose a PhD project exploring potentials of developing conversational artificial intelligence (AI) for a sleep coaching programme, which is motivated by CBT-I treatment. This PhD project aims to develop natural language processing (NLP) algorithms to allow the system to interact naturally with a user and provide automated analytic system to support human experts. In this paper, we introduce research questions lying under three phases of the sleep coaching programme: triaging, monitoring the progress, and providing coaching. We expect this research project's outcomes could contribute to the research domains of NLP and AI but also the healthcare field by providing a more accessible and affordable sleep treatment solution and an automated analytic system to lessen the burden of human experts",
    "checked": true,
    "id": "7e31f10ad1f91eb78de10c5cadd661e1cc2e9117",
    "semantic_title": "development of conversational ai for sleep coaching programme",
    "citation_count": 0,
    "authors": [
      "Heereen Shim"
    ]
  },
  "https://aclanthology.org/2021.eacl-srw.18": {
    "title": "Relating Relations: Meta-Relation Extraction from Online Health Forum Posts",
    "volume": "student",
    "abstract": "Relation extraction is a key task in knowledge extraction, and is commonly defined as the task of identifying relations that hold between entities in text. This thesis proposal addresses the specific task of identifying meta-relations, a higher order family of relations naturally construed as holding between other relations which includes temporal, comparative, and causal relations. More specifically, we aim to develop theoretical underpinnings and practical solutions for the challenges of (1) incorporating meta-relations into conceptualisations and annotation schemes for (lower-order) relations and named entities, (2) obtaining annotations for them with tolerable cognitive load on annotators, (3) creating models capable of reliably extracting meta-relations, and related to that (4) addressing the limited-data problem exacerbated by the introduction of meta-relations into the learning task. We explore recent works in relation extraction and discuss our plans to formally conceptualise meta-relations for the domain of user-generated health texts, and create a new dataset, annotation scheme and models for meta-relation extraction",
    "checked": true,
    "id": "d98aba7be55480d73df0675c560d61146cdbbc1d",
    "semantic_title": "relating relations: meta-relation extraction from online health forum posts",
    "citation_count": 0,
    "authors": [
      "Daniel Stickley"
    ]
  },
  "https://aclanthology.org/2021.eacl-srw.19": {
    "title": "Towards Personalised and Document-level Machine Translation of Dialogue",
    "volume": "student",
    "abstract": "State-of-the-art (SOTA) neural machine translation (NMT) systems translate texts at sentence level, ignoring context: intra-textual information, like the previous sentence, and extra-textual information, like the gender of the speaker. As a result, some sentences are translated incorrectly. Personalised NMT (PersNMT) and document-level NMT (DocNMT) incorporate this information into the translation process. Both fields are relatively new and previous work within them is limited. Moreover, there are no readily available robust evaluation metrics for them, which makes it difficult to develop better systems, as well as track global progress and compare different methods. This thesis proposal focuses on PersNMT and DocNMT for the domain of dialogue extracted from TV subtitles in five languages: English, Brazilian Portuguese, German, French and Polish. Three main challenges are addressed: (1) incorporating extra-textual information directly into NMT systems; (2) improving the machine translation of cohesion devices; (3) reliable evaluation for PersNMT and DocNMT",
    "checked": true,
    "id": "b554264a1e49856e775fbcf94091b40effb20992",
    "semantic_title": "towards personalised and document-level machine translation of dialogue",
    "citation_count": 2,
    "authors": [
      "Sebastian Vincent"
    ]
  },
  "https://aclanthology.org/2021.eacl-srw.20": {
    "title": "Semantic-aware transformation of short texts using word embeddings: An application in the Food Computing domain",
    "volume": "student",
    "abstract": "Most works in food computing focus on generating new recipes from scratch. However, there is a large number of new online recipes generated daily with a large number of users reviews, with recommendations to improve the recipe flavor and ideas to modify them. This fact encourages the use of these data for obtaining improved and customized versions. In this thesis, we propose an adaptation engine based on fine-tuning a word embedding model. We will capture, in an unsupervised way, the semantic meaning of the recipe ingredients. We will use their word embedding representations to align them to external databases, thus enriching their data. The adaptation engine will use this food data to modify a recipe into another fitting specific user preferences (e.g., decrease caloric intake or make a recipe). We plan to explore different types of recipe adaptations while preserving recipe essential features such as cuisine style and essence simultaneously. We will also modify the rest of the recipe to the new changes to be reproducible",
    "checked": true,
    "id": "7bf0eedfcbdfeb621569bffc70725a09981de172",
    "semantic_title": "semantic-aware transformation of short texts using word embeddings: an application in the food computing domain",
    "citation_count": 1,
    "authors": [
      "Andrea Morales-Garzón",
      "Juan Gómez-Romero",
      "Maria J. Martin-Bautista"
    ]
  },
  "https://aclanthology.org/2021.eacl-srw.21": {
    "title": "TMR: Evaluating NER Recall on Tough Mentions",
    "volume": "student",
    "abstract": "We propose the Tough Mentions Recall (TMR) metrics to supplement traditional named entity recognition (NER) evaluation by examining recall on specific subsets of \"tough\" mentions: unseen mentions, those whose tokens or token/type combination were not observed in training, and type-confusable mentions, token sequences with multiple entity types in the test data. We demonstrate the usefulness of these metrics by evaluating corpora of English, Spanish, and Dutch using five recent neural architectures. We identify subtle differences between the performance of BERT and Flair on two English NER corpora and identify a weak spot in the performance of current models in Spanish. We conclude that the TMR metrics enable differentiation between otherwise similar-scoring systems and identification of patterns in performance that would go unnoticed from overall precision, recall, and F1",
    "checked": true,
    "id": "d4277b4ea88ae4febe76ea2a58b331410f5ce44a",
    "semantic_title": "tmr: evaluating ner recall on tough mentions",
    "citation_count": 4,
    "authors": [
      "Jingxuan Tu",
      "Constantine Lignos"
    ]
  },
  "https://aclanthology.org/2021.eacl-srw.22": {
    "title": "The Effectiveness of Morphology-aware Segmentation in Low-Resource Neural Machine Translation",
    "volume": "student",
    "abstract": "This paper evaluates the performance of several modern subword segmentation methods in a low-resource neural machine translation setting. We compare segmentations produced by applying BPE at the token or sentence level with morphologically-based segmentations from LMVR and MORSEL. We evaluate translation tasks between English and each of Nepali, Sinhala, and Kazakh, and predict that using morphologically-based segmentation methods would lead to better performance in this setting. However, comparing to BPE, we find that no consistent and reliable differences emerge between the segmentation methods. While morphologically-based methods outperform BPE in a few cases, what performs best tends to vary across tasks, and the performance of segmentation methods is often statistically indistinguishable",
    "checked": true,
    "id": "38e99b3b56d03ad77cc058381bcc90f6cae5cb75",
    "semantic_title": "the effectiveness of morphology-aware segmentation in low-resource neural machine translation",
    "citation_count": 13,
    "authors": [
      "Jonne Saleva",
      "Constantine Lignos"
    ]
  },
  "https://aclanthology.org/2021.eacl-srw.23": {
    "title": "Making Use of Latent Space in Language GANs for Generating Diverse Text without Pre-training",
    "volume": "student",
    "abstract": "Generating diverse texts is an important factor for unsupervised text generation. One approach is to produce the diversity of texts conditioned by the sampled latent code. Although several generative adversarial networks (GANs) have been proposed thus far, these models still suffer from mode-collapsing if the models are not pre-trained. In this paper, we propose a GAN model that aims to improve the approach to generating diverse texts conditioned by the latent space. The generator of our model uses Gumbel-Softmax distribution for the word sampling process. To ensure that the text is generated conditioned upon the sampled latent code, reconstruction loss is introduced in our objective function. The discriminator of our model iteratively inspects incomplete partial texts and learns to distinguish whether they are real or fake by using the standard GAN objective function. Experimental results using the COCO Image Captions dataset show that, although our model is not pre-trained, the performance of our model is quite competitive with the existing baseline models, which requires pre-training",
    "checked": true,
    "id": "ec10fa78e0377b4b448150645922524534141547",
    "semantic_title": "making use of latent space in language gans for generating diverse text without pre-training",
    "citation_count": 1,
    "authors": [
      "Takeshi Kojima",
      "Yusuke Iwasawa",
      "Yutaka Matsuo"
    ]
  },
  "https://aclanthology.org/2021.eacl-srw.24": {
    "title": "Beyond the English Web: Zero-Shot Cross-Lingual and Lightweight Monolingual Classification of Registers",
    "volume": "student",
    "abstract": "We explore cross-lingual transfer of register classification for web documents. Registers, that is, text varieties such as blogs or news are one of the primary predictors of linguistic variation and thus affect the automatic processing of language. We introduce two new register-annotated corpora, FreCORE and SweCORE, for French and Swedish. We demonstrate that deep pre-trained language models perform strongly in these languages and outperform previous state-of-the-art in English and Finnish. Specifically, we show 1) that zero-shot cross-lingual transfer from the large English CORE corpus can match or surpass previously published monolingual models, and 2) that lightweight monolingual classification requiring very little training data can reach or surpass our zero-shot performance. We further analyse classification results finding that certain registers continue to pose challenges in particular for cross-lingual transfer",
    "checked": true,
    "id": "d75333eb9c22f91696b60893ac9d9fa7c57691da",
    "semantic_title": "beyond the english web: zero-shot cross-lingual and lightweight monolingual classification of registers",
    "citation_count": 13,
    "authors": [
      "Liina Repo",
      "Valtteri Skantsi",
      "Samuel Rönnqvist",
      "Saara Hellström",
      "Miika Oinonen",
      "Anna Salmela",
      "Douglas Biber",
      "Jesse Egbert",
      "Sampo Pyysalo",
      "Veronika Laippala"
    ]
  },
  "https://aclanthology.org/2021.eacl-srw.25": {
    "title": "Explaining and Improving BERT Performance on Lexical Semantic Change Detection",
    "volume": "student",
    "abstract": "Type- and token-based embedding architectures are still competing in lexical semantic change detection. The recent success of type-based models in SemEval-2020 Task 1 has raised the question why the success of token-based models on a variety of other NLP tasks does not translate to our field. We investigate the influence of a range of variables on clusterings of BERT vectors and show that its low performance is largely due to orthographic information on the target word, which is encoded even in the higher layers of BERT representations. By reducing the influence of orthography we considerably improve BERT's performance",
    "checked": true,
    "id": "000a54db4a891bd053c3b119468405f799517100",
    "semantic_title": "explaining and improving bert performance on lexical semantic change detection",
    "citation_count": 40,
    "authors": [
      "Severin Laicher",
      "Sinan Kurtyigit",
      "Dominik Schlechtweg",
      "Jonas Kuhn",
      "Sabine Schulte im Walde"
    ]
  },
  "https://aclanthology.org/2021.eacl-srw.26": {
    "title": "Why Find the Right One?",
    "volume": "student",
    "abstract": "The present paper investigates the impact of the anaphoric one words in English on the Neural Machine Translation (NMT) process using English-Hindi as source and target language pair. As expected, the experimental results show that the state-of-the-art Google English-Hindi NMT system achieves significantly poorly on sentences containing anaphoric ones as compared to the sentences containing regular, non-anaphoric ones. But, more importantly, we note that amongst the anaphoric words, the noun class is clearly much harder for NMT than the determinatives. This reaffirms the linguistic disparity of the two phenomenon in recent theoretical syntactic literature, despite the obvious surface similarities",
    "checked": true,
    "id": "4d61b525d9a605fb75c49d94763ab9d23bce2463",
    "semantic_title": "why find the right one?",
    "citation_count": 0,
    "authors": [
      "Payal Khullar"
    ]
  },
  "https://aclanthology.org/2021.eacl-tutorials.1": {
    "title": "Unsupervised Natural Language Parsing (Introductory Tutorial)",
    "volume": "tutorial",
    "abstract": "Unsupervised parsing learns a syntactic parser from training sentences without parse tree annotations. Recently, there has been a resurgence of interest in unsupervised parsing, which can be attributed to the combination of two trends in the NLP community: a general trend towards unsupervised training or pre-training, and an emerging trend towards finding or modeling linguistic structures in neural models. In this tutorial, we will introduce to the general audience what unsupervised parsing does and how it can be useful for and beyond syntactic parsing. We will then provide a systematic overview of major classes of approaches to unsupervised parsing, namely generative and discriminative approaches, and analyze their relative strengths and weaknesses. We will cover both decade-old statistical approaches and more recent neural approaches to give the audience a sense of the historical and recent development of the field. We will also discuss emerging research topics such as BERT-based approaches and visually grounded learning",
    "checked": true,
    "id": "558fd93ded2ae576388425292d574ddd8e6f5821",
    "semantic_title": "unsupervised natural language parsing (introductory tutorial)",
    "citation_count": 0,
    "authors": [
      "Kewei Tu",
      "Yong Jiang",
      "Wenjuan Han",
      "Yanpeng Zhao"
    ]
  },
  "https://aclanthology.org/2021.eacl-tutorials.2": {
    "title": "Aggregating and Learning from Multiple Annotators",
    "volume": "tutorial",
    "abstract": "The success of NLP research is founded on high-quality annotated datasets, which are usually obtained from multiple expert annotators or crowd workers. The standard practice to training machine learning models is to first adjudicate the disagreements and then perform the training. To this end, there has been a lot of work on aggregating annotations, particularly for classification tasks. However, many other tasks, particularly in NLP, have unique characteristics not considered by standard models of annotation, e.g., label interdependencies in sequence labelling tasks, unrestricted labels for anaphoric annotation, or preference labels for ranking texts. In recent years, researchers have picked up on this and are covering the gap. A first objective of this tutorial is to connect NLP researchers with state-of-the-art aggregation models for a diverse set of canonical language annotation tasks. There is also a growing body of recent work arguing that following the convention and training with adjudicated labels ignores any uncertainty the labellers had in their classifications, which results in models with poorer generalisation capabilities. Therefore, a second objective of this tutorial is to teach NLP workers how they can augment their (deep) neural models to learn from data with multiple interpretations",
    "checked": true,
    "id": "7bd94d45c26686d5e679aa669341ba1033fd5cbc",
    "semantic_title": "aggregating and learning from multiple annotators",
    "citation_count": 5,
    "authors": [
      "Silviu Paun",
      "Edwin Simpson"
    ]
  },
  "https://aclanthology.org/2021.eacl-tutorials.3": {
    "title": "Tutorial: End-to-End Speech Translation",
    "volume": "tutorial",
    "abstract": "Speech translation is the translation of speech in one language typically to text in another, traditionally accomplished through a combination of automatic speech recognition and machine translation. Speech translation has attracted interest for many years, but the recent successful applications of deep learning to both individual tasks have enabled new opportunities through joint modeling, in what we today call ‘end-to-end speech translation.' In this tutorial we introduce the techniques used in cutting-edge research on speech translation. Starting from the traditional cascaded approach, we give an overview on data sources and model architectures to achieve state-of-the art performance with end-to-end speech translation for both high- and low-resource languages. In addition, we discuss methods to evaluate analyze the proposed solutions, as well as the challenges faced when applying speech translation models for real-world applications",
    "checked": true,
    "id": "f3dc3e5358d6a82fcc78a44ca5a7448c6484315c",
    "semantic_title": "tutorial proposal: end-to-end speech translation",
    "citation_count": 1,
    "authors": [
      "Jan Niehues",
      "Elizabeth Salesky",
      "Marco Turchi",
      "Matteo Negri"
    ]
  },
  "https://aclanthology.org/2021.eacl-tutorials.4": {
    "title": "Reviewing Natural Language Processing Research",
    "volume": "tutorial",
    "abstract": "The reviewing procedure has been identified as one of the major issues in the current situation of the NLP field. While it is implicitly assumed that junior researcher learn reviewing during their PhD project, this might not always be the case. Additionally, with the growing NLP community and the efforts in the context of widening the NLP community, researchers joining the field might not have the opportunity to practise reviewing. This tutorial fills in this gap by providing an opportunity to learn the basics of reviewing. Also more experienced researchers might find this tutorial interesting to revise their reviewing procedure",
    "checked": true,
    "id": "2fda41d8b554f7689bda63a78a07e29cbb3e1895",
    "semantic_title": "reviewing natural language processing research",
    "citation_count": 3,
    "authors": [
      "Kevin Cohen",
      "Karën Fort",
      "Margot Mieskes",
      "Aurélie Névéol",
      "Anna Rogers"
    ]
  },
  "https://aclanthology.org/2021.eacl-tutorials.5": {
    "title": "Advances and Challenges in Unsupervised Neural Machine Translation",
    "volume": "tutorial",
    "abstract": "Unsupervised cross-lingual language representation initialization methods, together with mechanisms such as denoising and back-translation, have advanced unsupervised neural machine translation (UNMT), which has achieved impressive results. Meanwhile, there are still several challenges for UNMT. This tutorial first introduces the background and the latest progress of UNMT. We then examine a number of challenges to UNMT and give empirical results on how well the technology currently holds up",
    "checked": true,
    "id": "a42381c915aa4230b727d2402907ad922ea4dd37",
    "semantic_title": "advances and challenges in unsupervised neural machine translation",
    "citation_count": 4,
    "authors": [
      "Rui Wang",
      "Hai Zhao"
    ]
  },
  "https://aclanthology.org/2021.adaptnlp-1.1": {
    "title": "Multidomain Pretrained Language Models for Green NLP",
    "volume": "workshop",
    "abstract": "When tackling a task in a given domain, it has been shown that adapting a model to the domain using raw text data before training on the supervised task improves performance versus solely training on the task. The downside is that a lot of domain data is required and if we want to tackle tasks in n domains, we require n models each adapted on domain data before task learning. Storing and using these models separately can be prohibitive for low-end devices. In this paper we show that domain adaptation can be generalised to cover multiple domains. Specifically, a single model can be trained across various domains at the same time with minimal drop in performance, even when we use less data and resources. Thus, instead of training multiple models, we can train a single multidomain model saving on computational resources and training time",
    "checked": true,
    "id": "6df985d08020f468f58d065ea2802904b581ce52",
    "semantic_title": "multidomain pretrained language models for green nlp",
    "citation_count": 12,
    "authors": [
      "Antonis Maronikolakis",
      "Hinrich Schütze"
    ]
  },
  "https://aclanthology.org/2021.adaptnlp-1.2": {
    "title": "Pseudo-Label Guided Unsupervised Domain Adaptation of Contextual Embeddings",
    "volume": "workshop",
    "abstract": "Contextual embedding models such as BERT can be easily fine-tuned on labeled samples to create a state-of-the-art model for many downstream tasks. However, the fine-tuned BERT model suffers considerably from unlabeled data when applied to a different domain. In unsupervised domain adaptation, we aim to train a model that works well on a target domain when provided with labeled source samples and unlabeled target samples. In this paper, we propose a pseudo-label guided method for unsupervised domain adaptation. Two models are fine-tuned on labeled source samples as pseudo labeling models. To learn representations for the target domain, one of those models is adapted by masked language modeling from the target domain. Then those models are used to assign pseudo-labels to target samples. We train the final model with those samples. We evaluate our method on named entity segmentation and sentiment analysis tasks. These experiments show that our approach outperforms baseline methods",
    "checked": true,
    "id": "120da02502c9a04c1a4045316540691050c3d80c",
    "semantic_title": "pseudo-label guided unsupervised domain adaptation of contextual embeddings",
    "citation_count": 2,
    "authors": [
      "Tianyu Chen",
      "Shaohan Huang",
      "Furu Wei",
      "Jianxin Li"
    ]
  },
  "https://aclanthology.org/2021.adaptnlp-1.3": {
    "title": "Conditional Adversarial Networks for Multi-Domain Text Classification",
    "volume": "workshop",
    "abstract": "In this paper, we propose conditional adversarial networks (CANs), a framework that explores the relationship between the shared features and the label predictions to impose stronger discriminability to the learned features, for multi-domain text classification (MDTC). The proposed CAN introduces a conditional domain discriminator to model the domain variance in both the shared feature representations and the class-aware information simultaneously, and adopts entropy conditioning to guarantee the transferability of the shared features. We provide theoretical analysis for the CAN framework, showing that CAN's objective is equivalent to minimizing the total divergence among multiple joint distributions of shared features and label predictions. Therefore, CAN is a theoretically sound adversarial network that discriminates over multiple distributions. Evaluation results on two MDTC benchmarks show that CAN outperforms prior methods. Further experiments demonstrate that CAN has a good ability to generalize learned knowledge to unseen domains",
    "checked": true,
    "id": "27a71682ccfc00650588482048077c3104f08801",
    "semantic_title": "conditional adversarial networks for multi-domain text classification",
    "citation_count": 17,
    "authors": [
      "Yuan Wu",
      "Diana Inkpen",
      "Ahmed El-Roby"
    ]
  },
  "https://aclanthology.org/2021.adaptnlp-1.4": {
    "title": "The impact of domain-specific representations on BERT-based multi-domain spoken language understanding",
    "volume": "workshop",
    "abstract": "This paper provides the first experimental study on the impact of using domain-specific representations on a BERT-based multi-task spoken language understanding (SLU) model for multi-domain applications. Our results on a real-world dataset covering three languages indicate that by using domain-specific representations learned adversarially, model performance can be improved across all of the three SLU subtasks domain classification, intent classification and slot filling. Gains are particularly large for domains with limited training data",
    "checked": true,
    "id": "e318ea77a5ae5c9a70b6f66eaa247b22e2d23bc2",
    "semantic_title": "the impact of domain-specific representations on bert-based multi-domain spoken language understanding",
    "citation_count": 1,
    "authors": [
      "Judith Gaspers",
      "Quynh Do",
      "Tobias Röding",
      "Melanie Bradford"
    ]
  },
  "https://aclanthology.org/2021.adaptnlp-1.5": {
    "title": "Bridging the gap between supervised classification and unsupervised topic modelling for social-media assisted crisis management",
    "volume": "workshop",
    "abstract": "Social media such as Twitter provide valuable information to crisis managers and affected people during natural disasters. Machine learning can help structure and extract information from the large volume of messages shared during a crisis; however, the constantly evolving nature of crises makes effective domain adaptation essential. Supervised classification is limited by unchangeable class labels that may not be relevant to new events, and unsupervised topic modelling by insufficient prior knowledge. In this paper, we bridge the gap between the two and show that BERT embeddings finetuned on crisis-related tweet classification can effectively be used to adapt to a new crisis, discovering novel topics while preserving relevant classes from supervised training, and leveraging bidirectional self-attention to extract topic keywords. We create a dataset of tweets from a snowstorm to evaluate our method's transferability to new crises, and find that it outperforms traditional topic models in both automatic, and human evaluations grounded in the needs of crisis managers. More broadly, our method can be used for textual domain adaptation where the latent classes are unknown but overlap with known classes from other domains",
    "checked": true,
    "id": "273c5c8002fa3fc4341b69337821717125cfc717",
    "semantic_title": "bridging the gap between supervised classification and unsupervised topic modelling for social-media assisted crisis management",
    "citation_count": 0,
    "authors": [
      "Mikael Brunila",
      "Rosie Zhao",
      "Andrei Mircea",
      "Sam Lumley",
      "Renee Sieber"
    ]
  },
  "https://aclanthology.org/2021.adaptnlp-1.6": {
    "title": "Challenges in Annotating and Parsing Spoken, Code-switched, Frisian-Dutch Data",
    "volume": "workshop",
    "abstract": "While high performance have been obtained for high-resource languages, performance on low-resource languages lags behind. In this paper we focus on the parsing of the low-resource language Frisian. We use a sample of code-switched, spontaneously spoken data, which proves to be a challenging setup. We propose to train a parser specifically tailored towards the target domain, by selecting instances from multiple treebanks. Specifically, we use Latent Dirichlet Allocation (LDA), with word and character N-grams. We use a deep biaffine parser initialized with mBERT. The best single source treebank (nl_alpino) resulted in an LAS of 54.7 whereas our data selection outperformed the single best transfer treebank and led to 55.6 LAS on the test data. Additional experiments consisted of removing diacritics from our Frisian data, creating more similar training data by cropping sentences and running our best model using XLM-R. These experiments did not lead to a better performance",
    "checked": true,
    "id": "e90ccd8068d832391e94d0c26cbb7499bc16c90a",
    "semantic_title": "challenges in annotating and parsing spoken, code-switched, frisian-dutch data",
    "citation_count": 12,
    "authors": [
      "Anouck Braggaar",
      "Rob van der Goot"
    ]
  },
  "https://aclanthology.org/2021.adaptnlp-1.7": {
    "title": "Genres, Parsers, and BERT: The Interaction Between Parsers and BERT Models in Cross-Genre Constituency Parsing in English and Swedish",
    "volume": "workshop",
    "abstract": "Genre and domain are often used interchangeably, but are two different properties of a text. Successful parser adaptation requires both cross-domain and cross-genre sensitivity (Rehbein and Bildhauer, 2017). While the impact domain differences have on parser performance degradation is more easily measurable in respect to lexical differences, impact of genre differences can be more nuanced. With the predominance of pre-trained language models (LMs; e.g. BERT (Devlin et al., 2019)), there are now additional complexities in developing cross-genre sensitive models due to the infusion of linguistic characteristics derived from, usually, a third genre. We perform a systematic set of experiments using two neural constituency parsers to examine how different parsers behave in combination with different BERT models with varying source and target genres in English and Swedish. We find that there is extensive difficulty in predicting the best source due to the complex interactions between genres, parsers, and LMs. Additionally, the influence of the data used to derive the underlying BERT model heavily influences how best to create more robust and effective cross-genre parsing models",
    "checked": true,
    "id": "b5cf4e6108050e585747fddd2361d2c10199ef7d",
    "semantic_title": "genres, parsers, and bert: the interaction between parsers and bert models in cross-genre constituency parsing in english and swedish",
    "citation_count": 0,
    "authors": [
      "Daniel Dakota"
    ]
  },
  "https://aclanthology.org/2021.adaptnlp-1.8": {
    "title": "Cross-Lingual Transfer with MAML on Trees",
    "volume": "workshop",
    "abstract": "In meta-learning, the knowledge learned from previous tasks is transferred to new ones, but this transfer only works if tasks are related. Sharing information between unrelated tasks might hurt performance, and it is unclear how to transfer knowledge across tasks that have a hierarchical structure. Our research extends a meta-learning model, MAML, by exploiting hierarchical task relationships. Our algorithm, TreeMAML, adapts the model to each task with a few gradient steps, but the adaptation follows the hierarchical tree structure: in each step, gradients are pooled across tasks clusters and subsequent steps follow down the tree. We also implement a clustering algorithm that generates the tasks tree without previous knowledge of the task structure, allowing us to make use of implicit relationships between the tasks. We show that TreeMAML successfully trains natural language processing models for cross-lingual Natural Language Inference by taking advantage of the language phylogenetic tree. This result is useful since most languages in the world are under-resourced and the improvement on cross-lingual transfer allows the internationalization of NLP models",
    "checked": true,
    "id": "4ea81bd6207415a26dd7bd81cf457256fd83aa8d",
    "semantic_title": "cross-lingual transfer with maml on trees",
    "citation_count": 1,
    "authors": [
      "Jezabel Garcia",
      "Federica Freddi",
      "Jamie McGowan",
      "Tim Nieradzik",
      "Feng-Ting Liao",
      "Ye Tian",
      "Da-shan Shiu",
      "Alberto Bernacchia"
    ]
  },
  "https://aclanthology.org/2021.adaptnlp-1.9": {
    "title": "Addressing Zero-Resource Domains Using Document-Level Context in Neural Machine Translation",
    "volume": "workshop",
    "abstract": "Achieving satisfying performance in machine translation on domains for which there is no training data is challenging. Traditional supervised domain adaptation is not suitable for addressing such zero-resource domains because it relies on in-domain parallel data. We show that when in-domain parallel data is not available, access to document-level context enables better capturing of domain generalities compared to only having access to a single sentence. Having access to more information provides a more reliable domain estimation. We present two document-level Transformer models which are capable of using large context sizes and we compare these models against strong Transformer baselines. We obtain improvements for the two zero-resource domains we study. We additionally provide an analysis where we vary the amount of context and look at the case where in-domain data is available",
    "checked": true,
    "id": "033b55a410b39cacfe75c64160e05f8a7fa172c6",
    "semantic_title": "addressing zero-resource domains using document-level context in neural machine translation",
    "citation_count": 3,
    "authors": [
      "Dario Stojanovski",
      "Alexander Fraser"
    ]
  },
  "https://aclanthology.org/2021.adaptnlp-1.10": {
    "title": "MultiReQA: A Cross-Domain Evaluation forRetrieval Question Answering Models",
    "volume": "workshop",
    "abstract": "Retrieval question answering (ReQA) is the task of retrieving a sentence-level answer to a question from an open corpus (Ahmad et al.,2019).This dataset paper presents MultiReQA, a new multi-domain ReQA evaluation suite composed of eight retrieval QA tasks drawn from publicly available QA datasets. We explore systematic retrieval based evaluation and transfer learning across domains over these datasets using a number of strong base-lines including two supervised neural models, based on fine-tuning BERT and USE-QA models respectively, as well as a surprisingly effective information retrieval baseline, BM25. Five of these tasks contain both training and test data, while three contain test data only. Performing cross training on the five tasks with training data shows that while a general model covering all domains is achievable, the best performance is often obtained by training exclusively on in-domain data",
    "checked": true,
    "id": "fe7ea2dea7d24b5c661ea901c14e82c92c5fbe20",
    "semantic_title": "multireqa: a cross-domain evaluation forretrieval question answering models",
    "citation_count": 40,
    "authors": [
      "Mandy Guo",
      "Yinfei Yang",
      "Daniel Cer",
      "Qinlan Shen",
      "Noah Constant"
    ]
  },
  "https://aclanthology.org/2021.adaptnlp-1.11": {
    "title": "Domain adaptation in practice: Lessons from a real-world information extraction pipeline",
    "volume": "workshop",
    "abstract": "Advances in transfer learning and domain adaptation have raised hopes that once-challenging NLP tasks are ready to be put to use for sophisticated information extraction needs. In this work, we describe an effort to do just that – combining state-of-the-art neural methods for negation detection, document time relation extraction, and aspectual link prediction, with the eventual goal of extracting drug timelines from electronic health record text. We train on the THYME colon cancer corpus and test on both the THYME brain cancer corpus and an internal corpus, and show that performance of the combined systems is unacceptable despite good performance of individual systems. Although domain adaptation shows improvements on each individual system, the model selection problem is a barrier to improving overall pipeline performance",
    "checked": true,
    "id": "f54fe743e456f07b5bf5fab8ac58ea9691468b32",
    "semantic_title": "domain adaptation in practice: lessons from a real-world information extraction pipeline",
    "citation_count": 5,
    "authors": [
      "Timothy Miller",
      "Egoitz Laparra",
      "Steven Bethard"
    ]
  },
  "https://aclanthology.org/2021.adaptnlp-1.12": {
    "title": "BERTologiCoMix: How does Code-Mixing interact with Multilingual BERT?",
    "volume": "workshop",
    "abstract": "Models such as mBERT and XLMR have shown success in solving Code-Mixed NLP tasks even though they were not exposed to such text during pretraining. Code-Mixed NLP models have relied on using synthetically generated data along with naturally occurring data to improve their performance. Finetuning mBERT on such data improves it's code-mixed performance, but the benefits of using the different types of Code-Mixed data aren't clear. In this paper, we study the impact of finetuning with different types of code-mixed data and outline the changes that occur to the model during such finetuning. Our findings suggest that using naturally occurring code-mixed data brings in the best performance improvement after finetuning and that finetuning with any type of code-mixed text improves the responsivity of it's attention heads to code-mixed text inputs",
    "checked": true,
    "id": "2dc0ca9dd9a87ff37f52acb38ffe71077b5c2d16",
    "semantic_title": "bertologicomix: how does code-mixing interact with multilingual bert?",
    "citation_count": 30,
    "authors": [
      "Sebastin Santy",
      "Anirudh Srinivasan",
      "Monojit Choudhury"
    ]
  },
  "https://aclanthology.org/2021.adaptnlp-1.13": {
    "title": "Locality Preserving Loss: Neighbors that Live together, Align together",
    "volume": "workshop",
    "abstract": "We present a locality preserving loss (LPL) that improves the alignment between vector space embeddings while separating uncorrelated representations. Given two pretrained embedding manifolds, LPL optimizes a model to project an embedding and maintain its local neighborhood while aligning one manifold to another. This reduces the overall size of the dataset required to align the two in tasks such as crosslingual word alignment. We show that the LPL-based alignment between input vector spaces acts as a regularizer, leading to better and consistent accuracy than the baseline, especially when the size of the training set is small. We demonstrate the effectiveness of LPL-optimized alignment on semantic text similarity (STS), natural language inference (SNLI), multi-genre language inference (MNLI) and cross-lingual word alignment (CLA) showing consistent improvements, finding up to 16% improvement over our baseline in lower resource settings",
    "checked": true,
    "id": "1962a284d766cc4942bed92eca58154205c26eff",
    "semantic_title": "locality preserving loss: neighbors that live together, align together",
    "citation_count": 0,
    "authors": [
      "Ashwinkumar Ganesan",
      "Francis Ferraro",
      "Tim Oates"
    ]
  },
  "https://aclanthology.org/2021.adaptnlp-1.14": {
    "title": "On the Hidden Negative Transfer in Sequential Transfer Learning for Domain Adaptation from News to Tweets",
    "volume": "workshop",
    "abstract": "Transfer Learning has been shown to be a powerful tool for Natural Language Processing (NLP) and has outperformed the standard supervised learning paradigm, as it takes benefit from the pre-learned knowledge. Nevertheless, when transfer is performed between less related domains, it brings a negative transfer, i.e. hurts the transfer performance. In this research, we shed light on the hidden negative transfer occurring when transferring from the News domain to the Tweets domain, through quantitative and qualitative analysis. Our experiments on three NLP taks: Part-Of-Speech tagging, Chunking and Named Entity recognition, reveal interesting insights",
    "checked": true,
    "id": "2d046314fd340c1928717268039c2694b97ad911",
    "semantic_title": "on the hidden negative transfer in sequential transfer learning for domain adaptation from news to tweets",
    "citation_count": 11,
    "authors": [
      "Sara Meftah",
      "Nasredine Semmar",
      "Youssef Tamaazousti",
      "Hassane Essafi",
      "Fatiha Sadat"
    ]
  },
  "https://aclanthology.org/2021.adaptnlp-1.15": {
    "title": "Trajectory-Based Meta-Learning for Out-Of-Vocabulary Word Embedding Learning",
    "volume": "workshop",
    "abstract": "Word embedding learning methods require a large number of occurrences of a word to accurately learn its embedding. However, out-of-vocabulary (OOV) words which do not appear in the training corpus emerge frequently in the smaller downstream data. Recent work formulated OOV embedding learning as a few-shot regression problem and demonstrated that meta-learning can improve results obtained. However, the algorithm used, model-agnostic meta-learning (MAML) is known to be unstable and perform worse when a large number of gradient steps are used for parameter updates. In this work, we propose the use of Leap, a meta-learning algorithm which leverages the entire trajectory of the learning process instead of just the beginning and the end points, and thus ameliorates these two issues. In our experiments on a benchmark OOV embedding learning dataset and in an extrinsic evaluation, Leap performs comparably or better than MAML. We go on to examine which contexts are most beneficial to learn an OOV embedding from, and propose that the choice of contexts may matter more than the meta-learning employed",
    "checked": true,
    "id": "cab1a9d1f78aa4ac83fd24d85a8a1c458b602742",
    "semantic_title": "trajectory-based meta-learning for out-of-vocabulary word embedding learning",
    "citation_count": 1,
    "authors": [
      "Gordon Buck",
      "Andreas Vlachos"
    ]
  },
  "https://aclanthology.org/2021.adaptnlp-1.16": {
    "title": "Dependency Parsing Evaluation for Low-resource Spontaneous Speech",
    "volume": "workshop",
    "abstract": "How well can a state-of-the-art parsing system, developed for the written domain, perform when applied to spontaneous speech data involving different interlocutors? This study addresses this question in a low-resource setting using child-parent conversations from the CHILDES databse. Specifically, we focus on dependency parsing evaluation for utterances of one specific child (18 - 27 months) and her parents. We first present a semi-automatic adaption of the dependency annotation scheme in CHILDES to that of the Universal Dependencies project, an annotation style that is more commonly applied in dependency parsing. Our evaluation demonstrates that an outof-domain biaffine parser trained only on written texts performs well with parent speech. There is, however, much room for improvement on child utterances, particularly at 18 and 21 months, due to cases of omission and repetition that are prevalent in child speech. By contrast, parsers trained or fine-tuned with in-domain spoken data on a much smaller scale can achieve comparable results for parent speech and improve the weak parsing performance for child speech at these earlier ages",
    "checked": true,
    "id": "ca5e087052e3c004112414d5ff55812d617b9464",
    "semantic_title": "dependency parsing evaluation for low-resource spontaneous speech",
    "citation_count": 5,
    "authors": [
      "Zoey Liu",
      "Emily Prud’hommeaux"
    ]
  },
  "https://aclanthology.org/2021.adaptnlp-1.17": {
    "title": "An Empirical Study of Compound PCFGs",
    "volume": "workshop",
    "abstract": "Compound probabilistic context-free grammars (C-PCFGs) have recently established a new state of the art for phrase-structure grammar induction. However, due to the high time-complexity of chart-based representation and inference, it is difficult to investigate them comprehensively. In this work, we rely on a fast implementation of C-PCFGs to conduct evaluation complementary to that of (CITATION). We highlight three key findings: (1) C-PCFGs are data-efficient, (2) C-PCFGs make the best use of global sentence-level information in preterminal rule probabilities, and (3) the best configurations of C-PCFGs on English do not always generalize to morphology-rich languages",
    "checked": true,
    "id": "6c76e0909d056269c954739fb54e85792329f802",
    "semantic_title": "an empirical study of compound pcfgs",
    "citation_count": 8,
    "authors": [
      "Yanpeng Zhao",
      "Ivan Titov"
    ]
  },
  "https://aclanthology.org/2021.adaptnlp-1.18": {
    "title": "User Factor Adaptation for User Embedding via Multitask Learning",
    "volume": "workshop",
    "abstract": "Language varies across users and their interested fields in social media data: words authored by a user across his/her interests may have different meanings (e.g., cool) or sentiments (e.g., fast). However, most of the existing methods to train user embeddings ignore the variations across user interests, such as product and movie categories (e.g., drama vs. action). In this study, we treat the user interest as domains and empirically examine how the user language can vary across the user factor in three English social media datasets. We then propose a user embedding model to account for the language variability of user interests via a multitask learning framework. The model learns user language and its variations without human supervision. While existing work mainly evaluated the user embedding by extrinsic tasks, we propose an intrinsic evaluation via clustering and evaluate user embeddings by an extrinsic task, text classification. The experiments on the three English-language social media datasets show that our proposed approach can generally outperform baselines via adapting the user factor",
    "checked": true,
    "id": "9cb5c5e7c2fa0a792dd1d008a1236f432006de2f",
    "semantic_title": "user factor adaptation for user embedding via multitask learning",
    "citation_count": 2,
    "authors": [
      "Xiaolei Huang",
      "Michael J. Paul",
      "Franck Dernoncourt",
      "Robin Burke",
      "Mark Dredze"
    ]
  },
  "https://aclanthology.org/2021.adaptnlp-1.19": {
    "title": "On the Effectiveness of Dataset Embeddings in Mono-lingual,Multi-lingual and Zero-shot Conditions",
    "volume": "workshop",
    "abstract": "Recent complementary strands of research have shown that leveraging information on the data source through encoding their properties into embeddings can lead to performance increase when training a single model on heterogeneous data sources. However, it remains unclear in which situations these dataset embeddings are most effective, because they are used in a large variety of settings, languages and tasks. Furthermore, it is usually assumed that gold information on the data source is available, and that the test data is from a distribution seen during training. In this work, we compare the effect of dataset embeddings in mono-lingual settings, multi-lingual settings, and with predicted data source label in a zero-shot setting. We evaluate on three morphosyntactic tasks: morphological tagging, lemmatization, and dependency parsing, and use 104 datasets, 66 languages, and two different dataset grouping strategies. Performance increases are highest when the datasets are of the same language, and we know from which distribution the test-instance is drawn. In contrast, for setups where the data is from an unseen distribution, performance increase vanishes",
    "checked": true,
    "id": "cd6fe6f5de17882d35173960be7809890948f5af",
    "semantic_title": "on the effectiveness of dataset embeddings in mono-lingual,multi-lingual and zero-shot conditions",
    "citation_count": 2,
    "authors": [
      "Rob van der Goot",
      "Ahmet Üstün",
      "Barbara Plank"
    ]
  },
  "https://aclanthology.org/2021.adaptnlp-1.20": {
    "title": "Effective Distant Supervision for Temporal Relation Extraction",
    "volume": "workshop",
    "abstract": "A principal barrier to training temporal relation extraction models in new domains is the lack of varied, high quality examples and the challenge of collecting more. We present a method of automatically collecting distantly-supervised examples of temporal relations. We scrape and automatically label event pairs where the temporal relations are made explicit in text, then mask out those explicit cues, forcing a model trained on this data to learn other signals. We demonstrate that a pre-trained Transformer model is able to transfer from the weakly labeled examples to human-annotated benchmarks in both zero-shot and few-shot settings, and that the masking scheme is important in improving generalization",
    "checked": true,
    "id": "578d361b53fe5a7970bd6f96faf5b6d1d6ca1992",
    "semantic_title": "effective distant supervision for temporal relation extraction",
    "citation_count": 16,
    "authors": [
      "Xinyu Zhao",
      "Shih-Ting Lin",
      "Greg Durrett"
    ]
  },
  "https://aclanthology.org/2021.adaptnlp-1.21": {
    "title": "Zero-Shot Cross-Lingual Dependency Parsing through Contextual Embedding Transformation",
    "volume": "workshop",
    "abstract": "Linear embedding transformation has been shown to be effective for zero-shot cross-lingual transfer tasks and achieve surprisingly promising results. However, cross-lingual embedding space mapping is usually studied in static word-level embeddings, where a space transformation is derived by aligning representations of translation pairs that are referred from dictionaries. We move further from this line and investigate a contextual embedding alignment approach which is sense-level and dictionary-free. To enhance the quality of the mapping, we also provide a deep view of properties of contextual embeddings, i.e., the anisotropy problem and its solution. Experiments on zero-shot dependency parsing through the concept-shared space built by our embedding transformation substantially outperform state-of-the-art methods using multilingual embeddings",
    "checked": true,
    "id": "3daa37a481c66f86538fa8909955005e05df54b8",
    "semantic_title": "zero-shot cross-lingual dependency parsing through contextual embedding transformation",
    "citation_count": 5,
    "authors": [
      "Haoran Xu",
      "Philipp Koehn"
    ]
  },
  "https://aclanthology.org/2021.adaptnlp-1.22": {
    "title": "Gradual Fine-Tuning for Low-Resource Domain Adaptation",
    "volume": "workshop",
    "abstract": "Fine-tuning is known to improve NLP models by adapting an initial model trained on more plentiful but less domain-salient examples to data in a target domain. Such domain adaptation is typically done using one stage of fine-tuning. We demonstrate that gradually fine-tuning in a multi-step process can yield substantial further gains and can be applied without modifying the model or learning objective",
    "checked": true,
    "id": "c041b42cae6bb209b90a7592fa764faea0a97ba0",
    "semantic_title": "gradual fine-tuning for low-resource domain adaptation",
    "citation_count": 29,
    "authors": [
      "Haoran Xu",
      "Seth Ebner",
      "Mahsa Yarmohammadi",
      "Aaron Steven White",
      "Benjamin Van Durme",
      "Kenton Murray"
    ]
  },
  "https://aclanthology.org/2021.adaptnlp-1.23": {
    "title": "Analyzing the Domain Robustness of Pretrained Language Models, Layer by Layer",
    "volume": "workshop",
    "abstract": "The robustness of pretrained language models(PLMs) is generally measured using performance drops on two or more domains. However, we do not yet understand the inherent robustness achieved by contributions from different layers of a PLM. We systematically analyze the robustness of these representations layer by layer from two perspectives. First, we measure the robustness of representations by using domain divergence between two domains. We find that i) Domain variance increases from the lower to the upper layers for vanilla PLMs; ii) Models continuously pretrained on domain-specific data (DAPT)(Gururangan et al., 2020) exhibit more variance than their pretrained PLM counterparts; and that iii) Distilled models (e.g., DistilBERT) also show greater domain variance. Second, we investigate the robustness of representations by analyzing the encoded syntactic and semantic information using diagnostic probes. We find that similar layers have similar amounts of linguistic information for data from an unseen domain",
    "checked": true,
    "id": "33502e2cb9a059920f13daf73210c44269e9016a",
    "semantic_title": "analyzing the domain robustness of pretrained language models, layer by layer",
    "citation_count": 4,
    "authors": [
      "Abhinav Ramesh Kashyap",
      "Laiba Mehnaz",
      "Bhavitvya Malik",
      "Abdul Waheed",
      "Devamanyu Hazarika",
      "Min-Yen Kan",
      "Rajiv Ratn Shah"
    ]
  },
  "https://aclanthology.org/2021.adaptnlp-1.24": {
    "title": "Few-Shot Learning of an Interleaved Text Summarization Model by Pretraining with Synthetic Data",
    "volume": "workshop",
    "abstract": "Interleaved texts, where posts belonging to different threads occur in a sequence, commonly occur in online chat posts, so that it can be time-consuming to quickly obtain an overview of the discussions. Existing systems first disentangle the posts by threads and then extract summaries from those threads. A major issue with such systems is error propagation from the disentanglement component. While end-to-end trainable summarization system could obviate explicit disentanglement, such systems require a large amount of labeled data. To address this, we propose to pretrain an end-to-end trainable hierarchical encoder-decoder system using synthetic interleaved texts. We show that by fine-tuning on a real-world meeting dataset (AMI), such a system out-performs a traditional two-step system by 22%. We also compare against transformer models and observed that pretraining with synthetic data both the encoder and decoder outperforms the BertSumExtAbs transformer model which pretrains only the encoder on a large dataset",
    "checked": true,
    "id": "af3f72c81daf8279885d9c423871b892273fa6bf",
    "semantic_title": "few-shot learning of an interleaved text summarization model by pretraining with synthetic data",
    "citation_count": 5,
    "authors": [
      "Sanjeev Kumar Karn",
      "Francine Chen",
      "Yan-Ying Chen",
      "Ulli Waltinger",
      "Hinrich Schütze"
    ]
  },
  "https://aclanthology.org/2021.adaptnlp-1.25": {
    "title": "Semantic Parsing of Brief and Multi-Intent Natural Language Utterances",
    "volume": "workshop",
    "abstract": "Many military communication domains involve rapidly conveying situation awareness with few words. Converting natural language utterances to logical forms in these domains is challenging, as these utterances are brief and contain multiple intents. In this paper, we present a first effort toward building a weakly-supervised semantic parser to transform brief, multi-intent natural utterances into logical forms. Our findings suggest a new \"projection and reduction\" method that iteratively performs projection from natural to canonical utterances followed by reduction of natural utterances is the most effective. We conduct extensive experiments on two military and a general-domain dataset and provide a new baseline for future research toward accurate parsing of multi-intent utterances",
    "checked": true,
    "id": "c04f293c667870bd52b748c547e3baeeeb2ef958",
    "semantic_title": "semantic parsing of brief and multi-intent natural language utterances",
    "citation_count": 0,
    "authors": [
      "Logan Lebanoff",
      "Charles Newton",
      "Victor Hung",
      "Beth Atkinson",
      "John Killilea",
      "Fei Liu"
    ]
  },
  "https://aclanthology.org/2021.adaptnlp-1.26": {
    "title": "Domain Adaptation for NMT via Filtered Iterative Back-Translation",
    "volume": "workshop",
    "abstract": "Domain-specific Neural Machine Translation (NMT) model can provide improved performance, however, it is difficult to always access a domain-specific parallel corpus. Iterative Back-Translation can be used for fine-tuning an NMT model for a domain even if only a monolingual domain corpus is available. The quality of synthetic parallel corpora in terms of closeness to in-domain sentences can play an important role in the performance of the translation model. Recent works have shown that filtering at different stages of the back translation and weighting the sentences can provide state-of-the-art performance. In comparison, in this work, we observe that a simpler filtering approach based on a domain classifier, applied only to the pseudo-training data can consistently perform better, providing performance gains of 1.40, 1.82 and 0.76 in terms of BLEU score for Medical, Law and IT in one direction, and 1.28, 1.60 and 1.60 in the other direction in low resource scenario over competitive baselines. In the high resource scenario, our approach is at par with competitive baselines",
    "checked": true,
    "id": "77ce17570ef57831df47c6b012d86752cd0783d5",
    "semantic_title": "domain adaptation for nmt via filtered iterative back-translation",
    "citation_count": 9,
    "authors": [
      "Surabhi Kumari",
      "Nikhil Jaiswal",
      "Mayur Patidar",
      "Manasi Patwardhan",
      "Shirish Karande",
      "Puneet Agarwal",
      "Lovekesh Vig"
    ]
  },
  "https://aclanthology.org/2021.bea-1.1": {
    "title": "Negation Scope Resolution for Chinese as a Second Language",
    "volume": "workshop",
    "abstract": "This paper studies Negation Scope Resolution (NSR) for Chinese as a Second Language (CSL), which shows many unique characteristics that distinguish itself from \"standard\" Chinese. We annotate a new moderate-sized corpus that covers two background L1 languages, viz. English and Japanese. We build a neural NSR system, which achieves a new state-of-the-art accuracy on English benchmark data. We leverage this system to gauge how successful NSR for CSL can be. Different native language backgrounds of language learners result in unequal cross-lingual transfer, which has a significant impact on processing second language data. In particular, manual annotation, empirical evaluation and error analysis indicate two non-obvious facts: 1) L2-Chinese, L1-Japanese data are more difficult to analyze and thus annotate than L2-Chinese, L1-English data; 2) computational models trained on L2-Chinese, L1-Japanese data perform better than models trained on L2-Chinese, L1-English data",
    "checked": true,
    "id": "2dd81fd9a7f31f0f98add4d97511a7be1a3441db",
    "semantic_title": "negation scope resolution for chinese as a second language",
    "citation_count": 0,
    "authors": [
      "Mengyu Zhang",
      "Weiqi Wang",
      "Shuqiao Sun",
      "Weiwei Sun"
    ]
  },
  "https://aclanthology.org/2021.bea-1.2": {
    "title": "Text Simplification by Tagging",
    "volume": "workshop",
    "abstract": "Edit-based approaches have recently shown promising results on multiple monolingual sequence transduction tasks. In contrast to conventional sequence-to-sequence (Seq2Seq) models, which learn to generate text from scratch as they are trained on parallel corpora, these methods have proven to be much more effective since they are able to learn to make fast and accurate transformations while leveraging powerful pre-trained language models. Inspired by these ideas, we present TST, a simple and efficient Text Simplification system based on sequence Tagging, leveraging pre-trained Transformer-based encoders. Our system makes simplistic data augmentations and tweaks in training and inference on a pre-existing system, which makes it less reliant on large amounts of parallel training data, provides more control over the outputs and enables faster inference speeds. Our best model achieves near state-of-the-art performance on benchmark test datasets for the task. Since it is fully non-autoregressive, it achieves faster inference speeds by over 11 times than the current state-of-the-art text simplification system",
    "checked": true,
    "id": "870673733d381b9b794bd708438890fee2c0adc1",
    "semantic_title": "text simplification by tagging",
    "citation_count": 37,
    "authors": [
      "Kostiantyn Omelianchuk",
      "Vipul Raheja",
      "Oleksandr Skurzhanskyi"
    ]
  },
  "https://aclanthology.org/2021.bea-1.3": {
    "title": "Employing distributional semantics to organize task-focused vocabulary learning",
    "volume": "workshop",
    "abstract": "How can a learner systematically prepare for reading a book they are interested in? In this paper, we explore how computational linguistic methods such as distributional semantics, morphological clustering, and exercise generation can be combined with graph-based learner models to answer this question both conceptually and in practice. Based on highly structured learner models and concepts from network analysis, the learner is guided to efficiently explore the targeted lexical space. They practice using multi-gap learning activities generated from the book. In sum, the approach combines computational linguistic methods with concepts from network analysis and tutoring systems to support learners in pursuing their individual reading task goals",
    "checked": true,
    "id": "27a72c074d691d4ad9556ca46524b3a47c002d83",
    "semantic_title": "employing distributional semantics to organize task-focused vocabulary learning",
    "citation_count": 1,
    "authors": [
      "Haemanth Santhi Ponnusamy",
      "Detmar Meurers"
    ]
  },
  "https://aclanthology.org/2021.bea-1.4": {
    "title": "Synthetic Data Generation for Grammatical Error Correction with Tagged Corruption Models",
    "volume": "workshop",
    "abstract": "Synthetic data generation is widely known to boost the accuracy of neural grammatical error correction (GEC) systems, but existing methods often lack diversity or are too simplistic to generate the broad range of grammatical errors made by human writers. In this work, we use error type tags from automatic annotation tools such as ERRANT to guide synthetic data generation. We compare several models that can produce an ungrammatical sentence given a clean sentence and an error type tag. We use these models to build a new, large synthetic pre-training data set with error tag frequency distributions matching a given development set. Our synthetic data set yields large and consistent gains, improving the state-of-the-art on the BEA-19 and CoNLL-14 test sets. We also show that our approach is particularly effective in adapting a GEC system, trained on mixed native and non-native English, to a native English test set, even surpassing real training data consisting of high-quality sentence pairs",
    "checked": true,
    "id": "83faba6dc056cf1abd1662c2c650249657abe4ae",
    "semantic_title": "synthetic data generation for grammatical error correction with tagged corruption models",
    "citation_count": 67,
    "authors": [
      "Felix Stahlberg",
      "Shankar Kumar"
    ]
  },
  "https://aclanthology.org/2021.bea-1.5": {
    "title": "Broad Linguistic Complexity Analysis for Greek Readability Classification",
    "volume": "workshop",
    "abstract": "This paper explores the linguistic complexity of Greek textbooks as a readability classification task. We analyze textbook corpora for different school subjects and textbooks for Greek as a Second Language, covering a very wide spectrum of school age groups and proficiency levels. A broad range of quantifiable linguistic complexity features (lexical, morphological and syntactic) are extracted and calculated. Conducting experiments with different feature subsets, we show that the different linguistic dimensions contribute orthogonal information, each contributing towards the highest result achieved using all linguistic feature subsets. A readability classifier trained on this basis reaches a classification accuracy of 88.16% for the Greek as a Second Language corpus. To investigate the generalizability of the classification models, we also perform cross-corpus evaluations. We show that the model trained on the most varied text collection (for Greek as a school subject) generalizes best. In addition to advancing the state of the art for Greek readability analysis, the paper also contributes insights on the role of different feature sets and training setups for generalizable readability classification",
    "checked": true,
    "id": "0960fbdea3cd296cedc762495a74ec6473d82141",
    "semantic_title": "broad linguistic complexity analysis for greek readability classification",
    "citation_count": 11,
    "authors": [
      "Savvas Chatzipanagiotidis",
      "Maria Giagkou",
      "Detmar Meurers"
    ]
  },
  "https://aclanthology.org/2021.bea-1.6": {
    "title": "Character Set Construction for Chinese Language Learning",
    "volume": "workshop",
    "abstract": "To promote efficient learning of Chinese characters, pedagogical materials may present not only a single character, but a set of characters that are related in meaning and in written form. This paper investigates automatic construction of these character sets. The proposed model represents a character as averaged word vectors of common words containing the character. It then identifies sets of characters with high semantic similarity through clustering. Human evaluation shows that this representation outperforms direct use of character embeddings, and that the resulting character sets capture distinct semantic ranges",
    "checked": true,
    "id": "f48cede2e6b118438617d90e692bddd4ab88ab5e",
    "semantic_title": "character set construction for chinese language learning",
    "citation_count": 0,
    "authors": [
      "Chak Yan Yeung",
      "John Lee"
    ]
  },
  "https://aclanthology.org/2021.bea-1.7": {
    "title": "Identifying negative language transfer in learner errors using POS information",
    "volume": "workshop",
    "abstract": "A common mistake made by language learners is the misguided usage of first language rules when communicating in another language. In this paper, n-gram and recurrent neural network language models are used to represent language structures and detect when Chinese native speakers incorrectly transfer rules from their first language (i.e., Chinese) into their English writing. These models make it possible to inform corrective error feedback with error causes, such as negative language transfer. We report the results of our negative language detection experiments with n-gram and recurrent neural network models that were trained using part-of-speech tags. The best performing model achieves an F1-score of 0.51 when tasked with recognizing negative language transfer in English learner data",
    "checked": true,
    "id": "35006d31c39584b35a67397318f15ecd4a080122",
    "semantic_title": "identifying negative language transfer in learner errors using pos information",
    "citation_count": 0,
    "authors": [
      "Leticia Farias Wanderley",
      "Carrie Demmans Epp"
    ]
  },
  "https://aclanthology.org/2021.bea-1.8": {
    "title": "Document-level grammatical error correction",
    "volume": "workshop",
    "abstract": "Document-level context can provide valuable information in grammatical error correction (GEC), which is crucial for correcting certain errors and resolving inconsistencies. In this paper, we investigate context-aware approaches and propose document-level GEC systems. Additionally, we employ a three-step training strategy to benefit from both sentence-level and document-level data. Our system outperforms previous document-level and all other NMT-based single-model systems, achieving state of the art on a common test set",
    "checked": true,
    "id": "fe650346eeecd6cb28d70b67783d11f93c49ccc7",
    "semantic_title": "document-level grammatical error correction",
    "citation_count": 18,
    "authors": [
      "Zheng Yuan",
      "Christopher Bryant"
    ]
  },
  "https://aclanthology.org/2021.bea-1.9": {
    "title": "Essay Quality Signals as Weak Supervision for Source-based Essay Scoring",
    "volume": "workshop",
    "abstract": "Human essay grading is a laborious task that can consume much time and effort. Automated Essay Scoring (AES) has thus been proposed as a fast and effective solution to the problem of grading student writing at scale. However, because AES typically uses supervised machine learning, a human-graded essay corpus is still required to train the AES model. Unfortunately, such a graded corpus often does not exist, so creating a corpus for machine learning can also be a laborious task. This paper presents an investigation of replacing the use of human-labeled essay grades when training an AES system with two automatically available but weaker signals of essay quality: word count and topic distribution similarity. Experiments using two source-based essay scoring (evidence score) corpora show that while weak supervision does not yield a competitive result when training a neural source-based AES model, it can be used to successfully extract Topical Components (TCs) from a source text, which are required by a supervised feature-based AES model. In particular, results show that feature-based AES performance is comparable with either automatically or manually constructed TCs",
    "checked": true,
    "id": "bd0ca5568f0c46b8bc4359a2d9cbdf3430b22a3f",
    "semantic_title": "essay quality signals as weak supervision for source-based essay scoring",
    "citation_count": 4,
    "authors": [
      "Haoran Zhang",
      "Diane Litman"
    ]
  },
  "https://aclanthology.org/2021.bea-1.10": {
    "title": "Parsing Argumentative Structure in English-as-Foreign-Language Essays",
    "volume": "workshop",
    "abstract": "This paper presents a study on parsing the argumentative structure in English-as-foreign-language (EFL) essays, which are inherently noisy. The parsing process consists of two steps, linking related sentences and then labelling their relations. We experiment with several deep learning architectures to address each task independently. In the sentence linking task, a biaffine model performed the best. In the relation labelling task, a fine-tuned BERT model performed the best. Two sentence encoders are employed, and we observed that non-fine-tuning models generally performed better when using Sentence-BERT as opposed to BERT encoder. We trained our models using two types of parallel texts: original noisy EFL essays and those improved by annotators, then evaluate them on the original essays. The experiment shows that an end-to-end in-domain system achieved an accuracy of .341. On the other hand, the cross-domain system achieved 94% performance of the in-domain system. This signals that well-written texts can also be useful to train argument mining system for noisy texts",
    "checked": true,
    "id": "9f7dbeb0e7d035966e3ddda31b18ecee1b110f26",
    "semantic_title": "parsing argumentative structure in english-as-foreign-language essays",
    "citation_count": 10,
    "authors": [
      "Jan Wira Gotama Putra",
      "Simone Teufel",
      "Takenobu Tokunaga"
    ]
  },
  "https://aclanthology.org/2021.bea-1.11": {
    "title": "Training and Domain Adaptation for Supervised Text Segmentation",
    "volume": "workshop",
    "abstract": "Unlike traditional unsupervised text segmentation methods, recent supervised segmentation models rely on Wikipedia as the source of large-scale segmentation supervision. These models have, however, predominantly been evaluated on the in-domain (Wikipedia-based) test sets, preventing conclusions about their general segmentation efficacy. In this work, we focus on the domain transfer performance of supervised neural text segmentation in the educational domain. To this end, we first introduce K12Seg, a new dataset for evaluation of supervised segmentation, created from educational reading material for grade-1 to college-level students. We then benchmark a hierarchical text segmentation model (HITS), based on RoBERTa, in both in-domain and domain-transfer segmentation experiments. While HITS produces state-of-the-art in-domain performance (on three Wikipedia-based test sets), we show that, subject to the standard full-blown fine-tuning, it is susceptible to domain overfitting. We identify adapter-based fine-tuning as a remedy that substantially improves transfer performance",
    "checked": true,
    "id": "abdf8ce5d75312c2e009836b895bee7a5dd064a5",
    "semantic_title": "training and domain adaptation for supervised text segmentation",
    "citation_count": 8,
    "authors": [
      "Goran Glavaš",
      "Ananya Ganesh",
      "Swapna Somasundaran"
    ]
  },
  "https://aclanthology.org/2021.bea-1.12": {
    "title": "Data Strategies for Low-Resource Grammatical Error Correction",
    "volume": "workshop",
    "abstract": "Grammatical Error Correction (GEC) is a task that has been extensively investigated for the English language. However, for low-resource languages the best practices for training GEC systems have not yet been systematically determined. We investigate how best to take advantage of existing data sources for improving GEC systems for languages with limited quantities of high quality training data. We show that methods for generating artificial training data for GEC can benefit from including morphological errors. We also demonstrate that noisy error correction data gathered from Wikipedia revision histories and the language learning website Lang8, are valuable data sources. Finally, we show that GEC systems pre-trained on noisy data sources can be fine-tuned effectively using small amounts of high quality, human-annotated data",
    "checked": true,
    "id": "26e8f8cf8ad32d808eee2c90dbf97620dc24d10d",
    "semantic_title": "data strategies for low-resource grammatical error correction",
    "citation_count": 12,
    "authors": [
      "Simon Flachs",
      "Felix Stahlberg",
      "Shankar Kumar"
    ]
  },
  "https://aclanthology.org/2021.bea-1.13": {
    "title": "Towards a Data Analytics Pipeline for the Visualisation of Complexity Metrics in L2 writings",
    "volume": "workshop",
    "abstract": "We present the design of a tool for the visualisation of linguistic complexity in second language (L2) learner writings. We show how metrics can be exploited to visualise complexity in L2 writings in relation to CEFR levels",
    "checked": true,
    "id": "c2a956b1ae557ca745cd0380ee1569510abab3d1",
    "semantic_title": "towards a data analytics pipeline for the visualisation of complexity metrics in l2 writings",
    "citation_count": 1,
    "authors": [
      "Thomas Gaillat",
      "Anas Knefati",
      "Antoine Lafontaine"
    ]
  },
  "https://aclanthology.org/2021.bea-1.14": {
    "title": "Estonian as a Second Language Teacher's Tools",
    "volume": "workshop",
    "abstract": "The paper presents the results of the project \"Teacher's Tools\" (et Õpetaja tööriistad) published as a subpage of the new language portal Sõnaveeb developed by the Institute of the Estonian Language. The toolbox includes four modules: vocabulary, grammar, communicative language activities and text evaluation. The tools are aimed to help teachers and specialists of Estonian as a second language plan courses, create new educational materials, exercises and tests based on CEFR level descriptions",
    "checked": true,
    "id": "3ab1343d8d1503b2c508b7b9905d185541bfbc0c",
    "semantic_title": "estonian as a second language teacher's tools",
    "citation_count": 3,
    "authors": [
      "Tiiu Üksik",
      "Jelena Kallas",
      "Kristina Koppel",
      "Katrin Tsepelina",
      "Raili Pool"
    ]
  },
  "https://aclanthology.org/2021.bea-1.15": {
    "title": "Assessing Grammatical Correctness in Language Learning",
    "volume": "workshop",
    "abstract": "We present experiments on assessing the grammatical correctness of learners' answers in a language-learning System (references to the System, and the links to the released data and code are withheld for anonymity). In particular, we explore the problem of detecting alternative-correct answers: when more than one inflected form of a lemma fits syntactically and semantically in a given context. We approach the problem with the methods for grammatical error detection (GED), since we hypothesize that models for detecting grammatical mistakes can assess the correctness of potential alternative answers in a learning setting. Due to the paucity of training data, we explore the ability of pre-trained BERT to detect grammatical errors and then fine-tune it using synthetic training data. In this work, we focus on errors in inflection. Our experiments show a. that pre-trained BERT performs worse at detecting grammatical irregularities for Russian than for English; b. that fine-tuned BERT yields promising results on assessing the correctness of grammatical exercises; and c. establish a new benchmark for Russian. To further investigate its performance, we compare fine-tuned BERT with one of the state-of-the-art models for GED (Bell et al., 2019) on our dataset and RULEC-GEC (Rozovskaya and Roth, 2019). We release the manually annotated learner dataset, used for testing, for general use",
    "checked": true,
    "id": "f10b77590bad5005115ef02c6cbffc362056faad",
    "semantic_title": "assessing grammatical correctness in language learning",
    "citation_count": 10,
    "authors": [
      "Anisia Katinskaia",
      "Roman Yangarber"
    ]
  },
  "https://aclanthology.org/2021.bea-1.16": {
    "title": "On the application of Transformers for estimating the difficulty of Multiple-Choice Questions from text",
    "volume": "workshop",
    "abstract": "Classical approaches to question calibration are either subjective or require newly created questions to be deployed before being calibrated. Recent works explored the possibility of estimating question difficulty from text, but did not experiment with the most recent NLP models, in particular Transformers. In this paper, we compare the performance of previous literature with Transformer models experimenting on a public and a private dataset. Our experimental results show that Transformers are capable of outperforming previously proposed models. Moreover, if an additional corpus of related documents is available, Transformers can leverage that information to further improve calibration accuracy. We characterize the dependence of the model performance on some properties of the questions, showing that it performs best on questions ending with a question mark and Multiple-Choice Questions (MCQs) with one correct choice",
    "checked": true,
    "id": "759fc0d8d1b58022c72cf9011eb820988aa1c804",
    "semantic_title": "on the application of transformers for estimating the difficulty of multiple-choice questions from text",
    "citation_count": 10,
    "authors": [
      "Luca Benedetto",
      "Giovanni Aradelli",
      "Paolo Cremonesi",
      "Andrea Cappelli",
      "Andrea Giussani",
      "Roberto Turrin"
    ]
  },
  "https://aclanthology.org/2021.bea-1.17": {
    "title": "Automatically Generating Cause-and-Effect Questions from Passages",
    "volume": "workshop",
    "abstract": "Automated question generation has the potential to greatly aid in education applications, such as online study aids to check understanding of readings. The state-of-the-art in neural question generation has advanced greatly, due in part to the availability of large datasets of question-answer pairs. However, the questions generated are often surface-level and not challenging for a human to answer. To develop more challenging questions, we propose the novel task of cause-and-effect question generation. We build a pipeline that extracts causal relations from passages of input text, and feeds these as input to a state-of-the-art neural question generator. The extractor is based on prior work that classifies causal relations by linguistic category (Cao et al., 2016; Altenberg, 1984). This work results in a new, publicly available collection of cause-and-effect questions. We evaluate via both automatic and manual metrics and find performance improves for both question generation and question answering when we utilize a small auxiliary data source of cause-and-effect questions for fine-tuning. Our approach can be easily applied to generate cause-and-effect questions from other text collections and educational material, allowing for adaptable large-scale generation of cause-and-effect questions",
    "checked": true,
    "id": "44fee4b42af555b29d8f8fb2a8260d079eb4c2a1",
    "semantic_title": "automatically generating cause-and-effect questions from passages",
    "citation_count": 24,
    "authors": [
      "Katherine Stasaski",
      "Manav Rathod",
      "Tony Tu",
      "Yunfang Xiao",
      "Marti A. Hearst"
    ]
  },
  "https://aclanthology.org/2021.bea-1.18": {
    "title": "Interventions Recommendation: Professionals' Observations Analysis in Special Needs Education",
    "volume": "workshop",
    "abstract": "We present a new task in educational NLP, recommend the best interventions to help special needs education professionals to work with students with different disabilities. We use the professionals' observations of the students together with the students diagnosis and other chosen interventions to predict the best interventions for Chilean special needs students",
    "checked": true,
    "id": "5dec15a625886cc5d6a1a0a702aa495ac3eeb109",
    "semantic_title": "interventions recommendation: professionals' observations analysis in special needs education",
    "citation_count": 0,
    "authors": [
      "Javier Muñoz",
      "Felipe Bravo-Marquez"
    ]
  },
  "https://aclanthology.org/2021.bea-1.19": {
    "title": "C-Test Collector: A Proficiency Testing Application to Collect Training Data for C-Tests",
    "volume": "workshop",
    "abstract": "We present the C-Test Collector, a web-based tool that allows language learners to test their proficiency level using c-tests. Our tool collects anonymized data on test performance, which allows teachers to gain insights into common error patterns. At the same time, it allows NLP researchers to collect training data for being able to generate c-test variants at the desired difficulty level",
    "checked": true,
    "id": "79f6e3cadbac744efd0383793cf1ae03e18f062b",
    "semantic_title": "c-test collector: a proficiency testing application to collect training data for c-tests",
    "citation_count": 0,
    "authors": [
      "Christian Haring",
      "Rene Lehmann",
      "Andrea Horbach",
      "Torsten Zesch"
    ]
  },
  "https://aclanthology.org/2021.bea-1.20": {
    "title": "Virtual Pre-Service Teacher Assessment and Feedback via Conversational Agents",
    "volume": "workshop",
    "abstract": "Conversational agents and assistants have been used for decades to facilitate learning. There are many examples of conversational agents used for educational and training purposes in K-12, higher education, healthcare, the military, and private industry settings. The most common forms of conversational agents in education are teaching agents that directly teach and support learning, peer agents that serve as knowledgeable learning companions to guide learners in the learning process, and teachable agents that function as a novice or less-knowledgeable student trained and taught by a learner who learns by teaching. The Instructional Quality Assessment (IQA) provides a robust framework to evaluate reading comprehension and mathematics instruction. We developed a system for pre-service teachers, individuals in a teacher preparation program, to evaluate teaching instruction quality based on a modified interpretation of IQA metrics. Our demonstration and approach take advantage of recent advances in Natural Language Processing (NLP) and deep learning for each dialogue system component. We built an open-source conversational agent system to engage pre-service teachers in a specific mathematical scenario focused on scale factor with the aim to provide feedback on pre-service teachers' questioning strategies. We believe our system is not only practical for teacher education programs but can also enable other researchers to build new educational scenarios with minimal effort",
    "checked": true,
    "id": "3ffd85a5230d4872853739263c300eae8da64bae",
    "semantic_title": "virtual pre-service teacher assessment and feedback via conversational agents",
    "citation_count": 5,
    "authors": [
      "Debajyoti Datta",
      "Maria Phillips",
      "James P. Bywater",
      "Jennifer Chiu",
      "Ginger S. Watson",
      "Laura Barnes",
      "Donald Brown"
    ]
  },
  "https://aclanthology.org/2021.bea-1.21": {
    "title": "Automated Classification of Written Proficiency Levels on the CEFR-Scale through Complexity Contours and RNNs",
    "volume": "workshop",
    "abstract": "Automatically predicting the level of second language (L2) learner proficiency is an emerging topic of interest and research based on machine learning approaches to language learning and development. The key to the present paper is the combined use of what we refer to as ‘complexity contours', a series of measurements of indices of L2 proficiency obtained by a computational tool that implements a sliding window technique, and recurrent neural network (RNN) classifiers that adequately capture the sequential information in those contours. We used the EF-Cambridge Open Language Database (Geertzen et al. 2013) with its labelled Common European Framework of Reference (CEFR) levels (Council of Europe 2018) to predict six classes of L2 proficiency levels (A1, A2, B1, B2, C1, C2) in the assessment of writing skills. Our experiments demonstrate that an RNN classifier trained on complexity contours achieves higher classification accuracy than one trained on text-average complexity scores. In a secondary experiment, we determined the relative importance of features from four distinct categories through a sensitivity-based pruning technique. Our approach makes an important contribution to the field of automated identification of language proficiency levels, more specifically, to the increasing efforts towards the empirical validation of CEFR levels",
    "checked": true,
    "id": "9740eab9d7bca35f71a34f9cd3c05d24c96ec8da",
    "semantic_title": "automated classification of written proficiency levels on the cefr-scale through complexity contours and rnns",
    "citation_count": 6,
    "authors": [
      "Elma Kerz",
      "Daniel Wiechmann",
      "Yu Qiao",
      "Emma Tseng",
      "Marcus Ströbel"
    ]
  },
  "https://aclanthology.org/2021.bea-1.22": {
    "title": "Sharks are not the threat humans are\": Argument Component Segmentation in School Student Essays",
    "volume": "workshop",
    "abstract": "Argument mining is often addressed by a pipeline method where segmentation of text into argumentative units is conducted first and proceeded by an argument component identification task. In this research, we apply a token-level classification to identify claim and premise tokens from a new corpus of argumentative essays written by middle school students. To this end, we compare a variety of state-of-the-art models such as discrete features and deep learning architectures (e.g., BiLSTM networks and BERT-based architectures) to identify the argument components. We demonstrate that a BERT-based multi-task learning architecture (i.e., token and sentence level classification) adaptively pretrained on a relevant unlabeled dataset obtains the best results",
    "checked": true,
    "id": "0c5fd632478abd724a0e37c208b4b71a185bed3a",
    "semantic_title": "sharks are not the threat humans are\": argument component segmentation in school student essays",
    "citation_count": 10,
    "authors": [
      "Tariq Alhindi",
      "Debanjan Ghosh"
    ]
  },
  "https://aclanthology.org/2021.bea-1.23": {
    "title": "Using Linguistic Features to Predict the Response Process Complexity Associated with Answering Clinical MCQs",
    "volume": "workshop",
    "abstract": "This study examines the relationship between the linguistic characteristics of a test item and the complexity of the response process required to answer it correctly. Using data from a large-scale medical licensing exam, clustering methods identified items that were similar with respect to their relative difficulty and relative response-time intensiveness to create low response process complexity and high response process complexity item classes. Interpretable models were used to investigate the linguistic features that best differentiated between these classes from a descriptive and predictive framework. Results suggest that nuanced features such as the number of ambiguous medical terms help explain response process complexity beyond superficial item characteristics such as word count. Yet, although linguistic features carry signal relevant to response process complexity, the classification of individual items remains challenging",
    "checked": true,
    "id": "3e5252a02d430d066b9a564b204306208b0d129f",
    "semantic_title": "using linguistic features to predict the response process complexity associated with answering clinical mcqs",
    "citation_count": 1,
    "authors": [
      "Victoria Yaneva",
      "Daniel Jurich",
      "Le An Ha",
      "Peter Baldwin"
    ]
  },
  "https://aclanthology.org/2021.bsnlp-1.1": {
    "title": "HerBERT: Efficiently Pretrained Transformer-based Language Model for Polish",
    "volume": "workshop",
    "abstract": "BERT-based models are currently used for solving nearly all Natural Language Processing (NLP) tasks and most often achieve state-of-the-art results. Therefore, the NLP community conducts extensive research on understanding these models, but above all on designing effective and efficient training procedures. Several ablation studies investigating how to train BERT-like models have been carried out, but the vast majority of them concerned only the English language. A training procedure designed for English does not have to be universal and applicable to other especially typologically different languages. Therefore, this paper presents the first ablation study focused on Polish, which, unlike the isolating English language, is a fusional language. We design and thoroughly evaluate a pretraining procedure of transferring knowledge from multilingual to monolingual BERT-based models. In addition to multilingual model initialization, other factors that possibly influence pretraining are also explored, i.e. training objective, corpus size, BPE-Dropout, and pretraining length. Based on the proposed procedure, a Polish BERT-based language model – HerBERT – is trained. This model achieves state-of-the-art results on multiple downstream tasks",
    "checked": true,
    "id": "dc70b180329a6ffead5e48093fb5a551955047c4",
    "semantic_title": "herbert: efficiently pretrained transformer-based language model for polish",
    "citation_count": 57,
    "authors": [
      "Robert Mroczkowski",
      "Piotr Rybak",
      "Alina Wróblewska",
      "Ireneusz Gawlik"
    ]
  },
  "https://aclanthology.org/2021.bsnlp-1.2": {
    "title": "Russian Paraphrasers: Paraphrase with Transformers",
    "volume": "workshop",
    "abstract": "This paper studies the generation methods for paraphrasing in the Russian language. There are several transformer-based models (Russian and multilingual) trained on a collected corpus of paraphrases. We compare different models, contrast the quality of paraphrases using different ranking methods and apply paraphrasing methods in the context of augmentation procedure for different tasks. The contributions of the work are the combined paraphrasing dataset, fine-tuned generated models for Russian paraphrasing task and additionally the open source tool for simple usage of the paraphrasers",
    "checked": true,
    "id": "fc42fabab21eab464fef624a106426cb9b681a5c",
    "semantic_title": "russian paraphrasers: paraphrase with transformers",
    "citation_count": 10,
    "authors": [
      "Alena Fenogenova"
    ]
  },
  "https://aclanthology.org/2021.bsnlp-1.3": {
    "title": "Abusive Language Recognition in Russian",
    "volume": "workshop",
    "abstract": "Abusive phenomena are commonplace in language on the web. The scope of recognizing abusive language is broad, covering many behaviors and forms of expression. This work addresses automatic detection of abusive language in Russian. The lexical, grammatical and morphological diversity of Russian language present potential difficulties for this task, which is addressed using a variety of machine learning approaches. Finally, competitive performance is reached over multiple domains for this investigation into automatic detection of abusive language in Russian",
    "checked": true,
    "id": "104507a927fa13b2ffdbdd8bcafa2d40fb322f5f",
    "semantic_title": "abusive language recognition in russian",
    "citation_count": 2,
    "authors": [
      "Kamil Saitov",
      "Leon Derczynski"
    ]
  },
  "https://aclanthology.org/2021.bsnlp-1.4": {
    "title": "Detecting Inappropriate Messages on Sensitive Topics that Could Harm a Company's Reputation",
    "volume": "workshop",
    "abstract": "Not all topics are equally \"flammable\" in terms of toxicity: a calm discussion of turtles or fishing less often fuels inappropriate toxic dialogues than a discussion of politics or sexual minorities. We define a set of sensitive topics that can yield inappropriate and toxic messages and describe the methodology of collecting and labelling a dataset for appropriateness. While toxicity in user-generated data is well-studied, we aim at defining a more fine-grained notion of inappropriateness. The core of inappropriateness is that it can harm the reputation of a speaker. This is different from toxicity in two respects: (i) inappropriateness is topic-related, and (ii) inappropriate message is not toxic but still unacceptable. We collect and release two datasets for Russian: a topic-labelled dataset and an appropriateness-labelled dataset. We also release pre-trained classification models trained on this data",
    "checked": true,
    "id": "6f8a5e7efee7677ca08d2bd8724409bddaa9f98c",
    "semantic_title": "detecting inappropriate messages on sensitive topics that could harm a company's reputation",
    "citation_count": 8,
    "authors": [
      "Nikolay Babakov",
      "Varvara Logacheva",
      "Olga Kozlova",
      "Nikita Semenov",
      "Alexander Panchenko"
    ]
  },
  "https://aclanthology.org/2021.bsnlp-1.5": {
    "title": "BERTić - The Transformer Language Model for Bosnian, Croatian, Montenegrin and Serbian",
    "volume": "workshop",
    "abstract": "In this paper we describe a transformer model pre-trained on 8 billion tokens of crawled text from the Croatian, Bosnian, Serbian and Montenegrin web domains. We evaluate the transformer model on the tasks of part-of-speech tagging, named-entity-recognition, geo-location prediction and commonsense causal reasoning, showing improvements on all tasks over state-of-the-art models. For commonsense reasoning evaluation we introduce COPA-HR - a translation of the Choice of Plausible Alternatives (COPA) dataset into Croatian. The BERTić model is made available for free usage and further task-specific fine-tuning through HuggingFace",
    "checked": true,
    "id": "8c71e90e6cf6a8101e59446faf6f5fbfef4cbdef",
    "semantic_title": "bertić - the transformer language model for bosnian, croatian, montenegrin and serbian",
    "citation_count": 33,
    "authors": [
      "Nikola Ljubešić",
      "Davor Lauc"
    ]
  },
  "https://aclanthology.org/2021.bsnlp-1.6": {
    "title": "RuSentEval: Linguistic Source, Encoder Force!",
    "volume": "workshop",
    "abstract": "The success of pre-trained transformer language models has brought a great deal of interest on how these models work, and what they learn about language. However, prior research in the field is mainly devoted to English, and little is known regarding other languages. To this end, we introduce RuSentEval, an enhanced set of 14 probing tasks for Russian, including ones that have not been explored yet. We apply a combination of complementary probing methods to explore the distribution of various linguistic properties in five multilingual transformers for two typologically contrasting languages – Russian and English. Our results provide intriguing findings that contradict the common understanding of how linguistic knowledge is represented, and demonstrate that some properties are learned in a similar manner despite the language differences",
    "checked": true,
    "id": "16077776529aef23786ea8df109a546dfcb18d90",
    "semantic_title": "rusenteval: linguistic source, encoder force!",
    "citation_count": 3,
    "authors": [
      "Vladislav Mikhailov",
      "Ekaterina Taktasheva",
      "Elina Sigdel",
      "Ekaterina Artemova"
    ]
  },
  "https://aclanthology.org/2021.bsnlp-1.7": {
    "title": "Exploratory Analysis of News Sentiment Using Subgroup Discovery",
    "volume": "workshop",
    "abstract": "In this study, we present an exploratory analysis of a Slovenian news corpus, in which we investigate the association between named entities and sentiment in the news. We propose a methodology that combines Named Entity Recognition and Subgroup Discovery - a descriptive rule learning technique for identifying groups of examples that share the same class label (sentiment) and pattern (features - Named Entities). The approach is used to induce the positive and negative sentiment class rules that reveal interesting patterns related to different Slovenian and international politicians, organizations, and locations",
    "checked": true,
    "id": "6c8767e43cf212c772c290196820c6f7e5054c33",
    "semantic_title": "exploratory analysis of news sentiment using subgroup discovery",
    "citation_count": 0,
    "authors": [
      "Anita Valmarska",
      "Luis Adrián Cabrera-Diego",
      "Elvys Linhares Pontes",
      "Senja Pollak"
    ]
  },
  "https://aclanthology.org/2021.bsnlp-1.8": {
    "title": "Creating an Aligned Russian Text Simplification Dataset from Language Learner Data",
    "volume": "workshop",
    "abstract": "Parallel language corpora where regular texts are aligned with their simplified versions can be used in both natural language processing and theoretical linguistic studies. They are essential for the task of automatic text simplification, but can also provide valuable insights into the characteristics that make texts more accessible and reveal strategies that human experts use to simplify texts. Today, there exist a few parallel datasets for English and Simple English, but many other languages lack such data. In this paper we describe our work on creating an aligned Russian-Simple Russian dataset composed of Russian literature texts adapted for learners of Russian as a foreign language. This will be the first parallel dataset in this domain, and one of the first Simple Russian datasets in general",
    "checked": true,
    "id": "7a77360c0c1966a52ead4277b24753262695eccb",
    "semantic_title": "creating an aligned russian text simplification dataset from language learner data",
    "citation_count": 12,
    "authors": [
      "Anna Dmitrieva",
      "Jörg Tiedemann"
    ]
  },
  "https://aclanthology.org/2021.bsnlp-1.9": {
    "title": "Multilingual Named Entity Recognition and Matching Using BERT and Dedupe for Slavic Languages",
    "volume": "workshop",
    "abstract": "This paper describes the University of Ljubljana (UL FRI) Group's submissions to the shared task at the Balto-Slavic Natural Language Processing (BSNLP) 2021 Workshop. We experiment with multiple BERT-based models, pre-trained in multi-lingual, Croatian-Slovene-English and Slovene-only data. We perform training iteratively and on the concatenated data of previously available NER datasets. For the normalization task we use Stanza lemmatizer, while for entity matching we implemented a baseline using the Dedupe library. The performance of evaluations suggests that multi-source settings outperform less-resourced approaches. The best NER models achieve 0.91 F-score on Slovene training data splits while the best official submission achieved F-scores of 0.84 and 0.78 for relaxed partial matching and strict settings, respectively. In multi-lingual NER setting we achieve F-scores of 0.82 and 0.74",
    "checked": true,
    "id": "6d9119bbe5f5d927652eeda307910d55529d06db",
    "semantic_title": "multilingual named entity recognition and matching using bert and dedupe for slavic languages",
    "citation_count": 4,
    "authors": [
      "Marko Prelevikj",
      "Slavko Zitnik"
    ]
  },
  "https://aclanthology.org/2021.bsnlp-1.10": {
    "title": "Priberam Labs at the 3rd Shared Task on SlavNER",
    "volume": "workshop",
    "abstract": "This document describes our participation at the 3rd Shared Task on SlavNER, part of the 8th Balto-Slavic Natural Language Processing Workshop, where we focused exclusively in the Named Entity Recognition (NER) task. We addressed this task by combining multi-lingual contextual embedding models, such as XLM-R (Conneau et al., 2020), with character- level embeddings and a biaffine classifier (Yu et al., 2020). This allowed us to train downstream models for NER using all the available training data. We are able to show that this approach results in good performance when replicating the scenario of the 2nd Shared Task",
    "checked": true,
    "id": "e9eaaca6c76632ca8f6974bf340fba963871346c",
    "semantic_title": "priberam labs at the 3rd shared task on slavner",
    "citation_count": 3,
    "authors": [
      "Pedro Ferreira",
      "Ruben Cardoso",
      "Afonso Mendes"
    ]
  },
  "https://aclanthology.org/2021.bsnlp-1.11": {
    "title": "Multilingual Slavic Named Entity Recognition",
    "volume": "workshop",
    "abstract": "Named entity recognition, in particular for morphological rich languages, is challenging task due to the richness of inflected forms and ambiguity. This challenge is being addressed by SlavNER Shared Task. In this paper we describe system submitted to this task. Our system uses pre-trained multilingual BERT Language Model and is fine-tuned for six Slavic languages of this task on texts distributed by organizers. In our experiments this multilingual NER model achieved 96 F1 score on in-domain data and an F1 score of 83 on out of domain data. Entity coreference module achieved F1 score of 47.6 as evaluated by bsnlp2021 organizers",
    "checked": true,
    "id": "2f88d1c2804f2aecf6d282cf0ee4acdd34bf4c4a",
    "semantic_title": "multilingual slavic named entity recognition",
    "citation_count": 4,
    "authors": [
      "Rinalds Vīksna",
      "Inguna Skadina"
    ]
  },
  "https://aclanthology.org/2021.bsnlp-1.12": {
    "title": "Using a Frustratingly Easy Domain and Tagset Adaptation for Creating Slavic Named Entity Recognition Systems",
    "volume": "workshop",
    "abstract": "We present a collection of Named Entity Recognition (NER) systems for six Slavic languages: Bulgarian, Czech, Polish, Slovenian, Russian and Ukrainian. These NER systems have been trained using different BERT models and a Frustratingly Easy Domain Adaptation (FEDA). FEDA allow us creating NER systems using multiple datasets without having to worry about whether the tagset (e.g. Location, Event, Miscellaneous, Time) in the source and target domains match, while increasing the amount of data available for training. Moreover, we boosted the prediction on named entities by marking uppercase words and predicting masked words. Participating in the 3rd Shared Task on SlavNER, our NER systems reached a strict match micro F-score of up to 0.908. The results demonstrate good generalization, even in named entities with weak regularity, such as book titles, or entities that were never seen during the training",
    "checked": true,
    "id": "14cab7aa7ce37911c481df3af63ff1ba6fabec8c",
    "semantic_title": "using a frustratingly easy domain and tagset adaptation for creating slavic named entity recognition systems",
    "citation_count": 3,
    "authors": [
      "Luis Adrián Cabrera-Diego",
      "Jose G. Moreno",
      "Antoine Doucet"
    ]
  },
  "https://aclanthology.org/2021.bsnlp-1.13": {
    "title": "Benchmarking Pre-trained Language Models for Multilingual NER: TraSpaS at the BSNLP2021 Shared Task",
    "volume": "workshop",
    "abstract": "In this paper we describe TraSpaS, a submission to the third shared task on named entity recognition hosted as part of the Balto-Slavic Natural Language Processing (BSNLP) Workshop. In it we evaluate various pre-trained language models on the NER task using three open-source NLP toolkits: character level language model with Stanza, language-specific BERT-style models with SpaCy and Adapter-enabled XLM-R with Trankit. Our results show that the Trankit-based models outperformed those based on the other two toolkits, even when trained on smaller amounts of data. Our code is available at https://github.com/NaiveNeuron/slavner-2021",
    "checked": true,
    "id": "abb9dcd74047d7b8963ab7363c5028916d153896",
    "semantic_title": "benchmarking pre-trained language models for multilingual ner: traspas at the bsnlp2021 shared task",
    "citation_count": 6,
    "authors": [
      "Marek Suppa",
      "Ondrej Jariabka"
    ]
  },
  "https://aclanthology.org/2021.bsnlp-1.14": {
    "title": "Named Entity Recognition and Linking Augmented with Large-Scale Structured Data",
    "volume": "workshop",
    "abstract": "In this paper we describe our submissions to the 2nd and 3rd SlavNER Shared Tasks held at BSNLP 2019 and BSNLP 2021, respectively. The tasks focused on the analysis of Named Entities in multilingual Web documents in Slavic languages with rich inflection. Our solution takes advantage of large collections of both unstructured and structured documents. The former serve as data for unsupervised training of language models and embeddings of lexical units. The latter refers to Wikipedia and its structured counterpart - Wikidata, our source of lemmatization rules, and real-world entities. With the aid of those resources, our system could recognize, normalize and link entities, while being trained with only small amounts of labeled data",
    "checked": true,
    "id": "49e1383486c27a6b87e171e9231302a74d83f7e7",
    "semantic_title": "named entity recognition and linking augmented with large-scale structured data",
    "citation_count": 2,
    "authors": [
      "Paweł Rychlikowski",
      "Bartłomiej Najdecki",
      "Adrian Lancucki",
      "Adam Kaczmarek"
    ]
  },
  "https://aclanthology.org/2021.bsnlp-1.15": {
    "title": "Slav-NER: the 3rd Cross-lingual Challenge on Recognition, Normalization, Classification, and Linking of Named Entities across Slavic Languages",
    "volume": "workshop",
    "abstract": "This paper describes Slav-NER: the 3rd Multilingual Named Entity Challenge in Slavic languages. The tasks involve recognizing mentions of named entities in Web documents, normalization of the names, and cross-lingual linking. The Challenge covers six languages and five entity types, and is organized as part of the 8th Balto-Slavic Natural Language Processing Workshop, co-located with the EACL 2021 Conference. Ten teams participated in the competition. Performance for the named entity recognition task reached 90% F-measure, much higher than reported in the first edition of the Challenge. Seven teams covered all six languages, and five teams participated in the cross-lingual entity linking task. Detailed valuation information is available on the shared task web page",
    "checked": true,
    "id": "eeea7ecef365e56aeae13ed20092a396173b63a7",
    "semantic_title": "slav-ner: the 3rd cross-lingual challenge on recognition, normalization, classification, and linking of named entities across slavic languages",
    "citation_count": 16,
    "authors": [
      "Jakub Piskorski",
      "Bogdan Babych",
      "Zara Kancheva",
      "Olga Kanishcheva",
      "Maria Lebedeva",
      "Michał Marcińczuk",
      "Preslav Nakov",
      "Petya Osenova",
      "Lidia Pivovarova",
      "Senja Pollak",
      "Pavel Přibáň",
      "Ivaylo Radev",
      "Marko Robnik-Sikonja",
      "Vasyl Starko",
      "Josef Steinberger",
      "Roman Yangarber"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.1": {
    "title": "Tamil Lyrics Corpus: Analysis and Experiments",
    "volume": "workshop",
    "abstract": "In this paper, we present a new Tamil lyrics corpus extracted from Tamil movies captured across a range of 65 years (1954 to 2019). We present a detailed corpus analysis showing the nature of Tamil lyrics with respect to lyricists and the year which it was written. We also present similar- ity score across different lyricists based on their song lyrics. We present experi- mental results based on the SOTA BERT Tamil models to identify the lyricists of a song. Finally, we present future research directions encouraging researchers to pur- sue Tamil NLP research",
    "checked": true,
    "id": "a88e8d64933850b5a14253ba1ce5b9034f94d2fb",
    "semantic_title": "tamil lyrics corpus: analysis and experiments",
    "citation_count": 3,
    "authors": [
      "Dhivya Chinnappa",
      "Praveenraj Dhandapani"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.2": {
    "title": "DOSA: Dravidian Code-Mixed Offensive Span Identification Dataset",
    "volume": "workshop",
    "abstract": "This paper presents the Dravidian Offensive Span Identification Dataset (DOSA) for under-resourced Tamil-English and Kannada-English code-mixed text. The dataset addresses the lack of code-mixed datasets with annotated offensive spans by extending annotations of existing code-mixed offensive language identification datasets. It provides span annotations for Tamil-English and Kannada-English code-mixed comments posted by users on YouTube social media. Overall the dataset consists of 4786 Tamil-English comments with 6202 annotated spans and 1097 Kannada-English comments with 1641 annotated spans, each annotated by two different annotators. We further present some of our baseline experimental results on the developed dataset, thereby eliciting research in under-resourced languages, leading to an essential step towards semi-automated content moderation in Dravidian languages. The dataset is available in https://github.com/teamdl-mlsg/DOSA",
    "checked": true,
    "id": "89c3f1f316280456b30fd58fb0ef525e824f648a",
    "semantic_title": "dosa: dravidian code-mixed offensive span identification dataset",
    "citation_count": 14,
    "authors": [
      "Manikandan Ravikiran",
      "Subbiah Annamalai"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.3": {
    "title": "Towards Offensive Language Identification for Dravidian Languages",
    "volume": "workshop",
    "abstract": "Offensive speech identification in countries like India poses several challenges due to the usage of code-mixed and romanized variants of multiple languages by the users in their posts on social media. The challenge of offensive language identification on social media for Dravidian languages is harder, considering the low resources available for the same. In this paper, we explored the zero-shot learning and few-shot learning paradigms based on multilingual language models for offensive speech detection in code-mixed and romanized variants of three Dravidian languages - Malayalam, Tamil, and Kannada. We propose a novel and flexible approach of selective translation and transliteration to reap better results from fine-tuning and ensembling multilingual transformer networks like XLMRoBERTa and mBERT. We implemented pretrained, fine-tuned, and ensembled versions of XLM-RoBERTa for offensive speech classification. Further, we experimented with interlanguage, inter-task, and multi-task transfer learning techniques to leverage the rich resources available for offensive speech identification in the English language and to enrich the models with knowledge transfer from related tasks. The proposed models yielded good results and are promising for effective offensive speech identification in low resource settings",
    "checked": true,
    "id": "6d41bb0ec6b1b6283d60c7263eb360286948a407",
    "semantic_title": "towards offensive language identification for dravidian languages",
    "citation_count": 15,
    "authors": [
      "Siva Sai",
      "Yashvardhan Sharma"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.4": {
    "title": "Sentiment Classification of Code-Mixed Tweets using Bi-Directional RNN and Language Tags",
    "volume": "workshop",
    "abstract": "Sentiment analysis tools and models have been developed extensively throughout the years, for European languages. In contrast, similar tools for Indian Languages are scarce. This is because, state-of-the-art pre-processing tools like POS tagger, shallow parsers, etc., are not readily available for Indian languages. Although, such working tools for Indian languages, like Hindi and Bengali, that are spoken by the majority of the population, are available, finding the same for less spoken languages like, Tamil, Telugu, and Malayalam, is difficult. Moreover, due to the advent of social media, the multi-lingual population of India, who are comfortable with both English ad their regional language, prefer to communicate by mixing both languages. This gives rise to massive code-mixed content and automatically annotating them with their respective sentiment labels becomes a challenging task. In this work, we take up a similar challenge of developing a sentiment analysis model that can work with English-Tamil code-mixed data. The proposed work tries to solve this by using bi-directional LSTMs along with language tagging. Other traditional methods, based on classical machine learning algorithms have also been discussed in the literature, and they also act as the baseline systems to which we will compare our Neural Network based model. The performance of the developed algorithm, based on Neural Network architecture, garnered precision, recall, and F1 scores of 0.59, 0.66, and 0.58 respectively",
    "checked": true,
    "id": "ccbdf6fd896be53793f3dda6be5e1c526ba10368",
    "semantic_title": "sentiment classification of code-mixed tweets using bi-directional rnn and language tags",
    "citation_count": 5,
    "authors": [
      "Sainik Mahata",
      "Dipankar Das",
      "Sivaji Bandyopadhyay"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.5": {
    "title": "Offensive language identification in Dravidian code mixed social media text",
    "volume": "workshop",
    "abstract": "Hate speech and offensive language recognition in social media platforms have been an active field of research over recent years. In non-native English spoken countries, social media texts are mostly in code mixed or script mixed/switched form. The current study presents extensive experiments using multiple machine learning, deep learning, and transfer learning models to detect offensive content on Twitter. The data set used for this study are in Tanglish (Tamil and English), Manglish (Malayalam and English) code-mixed, and Malayalam script-mixed. The experimental results showed that 1 to 6-gram character TF-IDF features are better for the said task. The best performing models were naive bayes, logistic regression, and vanilla neural network for the dataset Tamil code-mix, Malayalam code-mixed, and Malayalam script-mixed, respectively instead of more popular transfer learning models such as BERT and ULMFiT and hybrid deep models",
    "checked": true,
    "id": "2f0019e8b9195d34f84a67ee7d1584a92ce9bf4f",
    "semantic_title": "offensive language identification in dravidian code mixed social media text",
    "citation_count": 30,
    "authors": [
      "Sunil Saumya",
      "Abhinav Kumar",
      "Jyoti Prakash Singh"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.6": {
    "title": "Sentiment Analysis of Dravidian Code Mixed Data",
    "volume": "workshop",
    "abstract": "This paper presents the methodologies implemented while classifying Dravidian code-mixed comments according to their polarity. With datasets of code-mixed Tamil and Malayalam available, three methods are proposed - a sub-word level model, a word embedding based model and a machine learning based architecture. The sub-word and word embedding based models utilized Long Short Term Memory (LSTM) network along with language-specific preprocessing while the machine learning model used term frequency–inverse document frequency (TF-IDF) vectorization along with a Logistic Regression model. The sub-word level model was submitted to the the track ‘Sentiment Analysis for Dravidian Languages in Code-Mixed Text' proposed by Forum of Information Retrieval Evaluation in 2020 (FIRE 2020). Although it received a rank of 5 and 12 for the Tamil and Malayalam tasks respectively in the FIRE 2020 track, this paper improves upon the results by a margin to attain final weighted F1-scores of 0.65 for the Tamil task and 0.68 for the Malayalam task. The former score is equivalent to that attained by the highest ranked team of the Tamil track",
    "checked": true,
    "id": "0250a832423351ffe81df381ccb3c99cf8096754",
    "semantic_title": "sentiment analysis of dravidian code mixed data",
    "citation_count": 10,
    "authors": [
      "Asrita Venkata Mandalam",
      "Yashvardhan Sharma"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.7": {
    "title": "Unsupervised Machine Translation On Dravidian Languages",
    "volume": "workshop",
    "abstract": "Unsupervised Neural Machine translation (UNMT) is beneficial especially for under-resourced languages such as from the Dravidian family. They learn to translate between the source and target, relying solely on only monolingual corpora. However, UNMT systems fail in scenarios that occur often when dealing with low resource languages. Recent works have achieved state-of-the-art results by adding auxiliary parallel data with similar languages. In this work, we focus on unsupervised translation between English and Kannada by using limited amounts of auxiliary data between English and other Dravidian languages. We show that transliteration is essential in unsupervised translation between Dravidian languages, as they do not share a common writing system. We explore several model architectures that use the auxiliary data in order to maximize knowledge sharing and enable UNMT for dissimilar language pairs. We show from our experiments it is crucial for Kannada and reference languages to be similar. Further, we propose a method to measure language similarity to choose the most beneficial reference languages",
    "checked": true,
    "id": "585ea8f2b9a4c8b3a707620aaf0e3cfeda3df97f",
    "semantic_title": "unsupervised machine translation on dravidian languages",
    "citation_count": 7,
    "authors": [
      "Sai Koneru",
      "Danni Liu",
      "Jan Niehues"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.8": {
    "title": "Graph Convolutional Networks with Multi-headed Attention for Code-Mixed Sentiment Analysis",
    "volume": "workshop",
    "abstract": "Code-mixing is a frequently observed phenomenon in multilingual communities where a speaker uses multiple languages in an utterance or sentence. Code-mixed texts are abundant, especially in social media, and pose a problem for NLP tools as they are typically trained on monolingual corpora. Recently, finding the sentiment from code-mixed text has been attempted by some researchers in SentiMix SemEval 2020 and Dravidian-CodeMix FIRE 2020 shared tasks. Mostly, the attempts include traditional methods, long short term memory, convolutional neural networks, and transformer models for code-mixed sentiment analysis (CMSA). However, no study has explored graph convolutional neural networks on CMSA. In this paper, we propose the graph convolutional networks (GCN) for sentiment analysis on code-mixed text. We have used the datasets from the Dravidian-CodeMix FIRE 2020. Our experimental results on multiple CMSA datasets demonstrate that the GCN with multi-headed attention model has shown an improvement in classification metrics",
    "checked": true,
    "id": "d6b02b8405144f0ca1c2dac3753b5bdfd4fa703d",
    "semantic_title": "graph convolutional networks with multi-headed attention for code-mixed sentiment analysis",
    "citation_count": 10,
    "authors": [
      "Suman Dowlagar",
      "Radhika Mamidi"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.9": {
    "title": "Task-Specific Pre-Training and Cross Lingual Transfer for Sentiment Analysis in Dravidian Code-Switched Languages",
    "volume": "workshop",
    "abstract": "Sentiment analysis in Code-Mixed languages has garnered a lot of attention in recent years. It is an important task for social media monitoring and has many applications, as a large chunk of social media data is Code-Mixed. In this paper, we work on the problem of sentiment analysis for Dravidian Code-Switched languages - Tamil-Engish and Malayalam-English, using three different BERT based models. We leverage task-specific pre-training and cross-lingual transfer to improve on previously reported results, with significant improvement for the Tamil-Engish dataset. We also present a multilingual sentiment classification model that has competitive performance on both Tamil-English and Malayalam-English datasets",
    "checked": true,
    "id": "ff457336a58f06a20645bb28d5fe8998c3f9c904",
    "semantic_title": "task-specific pre-training and cross lingual transfer for sentiment analysis in dravidian code-switched languages",
    "citation_count": 11,
    "authors": [
      "Akshat Gupta",
      "Sai Krishna Rallabandi",
      "Alan W Black"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.10": {
    "title": "Analysis of Uvama Urubugal in Tamil Sangam Literatures",
    "volume": "workshop",
    "abstract": "Uvama urubugal in Tamil are used to explain a particular context by citing another equivalent context. This is referred to as \"Uvamaiyani\" in Tamil Grammar rules as stated in Tholkappiam. The is called as simile in English. Similes bring out many beautiful poetic contexts. Automatic extraction of such similes can help to build better Natural Language Generation applications such as, story generation systems and lyric suggestion systems. This paper attempts to automatically extract the uvama urubugal from Tamil Sangam Literatures. Natrinai and Mullai Pattu have been used for the analysis. There are 12 uvama urupugal in Tamil as per Nanool and this paper has attempted to analyze the usage of these 12 uvama urubugal in Sangam Literatures and compares their usage distribution in the Tamil Film songs data set comprising of 4215 songs. It was found that only two uvama urubugal were used in the current-day Tamil Film songs. This comparison was done to reveal the diminishing usage of these beautiful uvama urubugal by the current generation and the urge to use them again",
    "checked": true,
    "id": "2d6cd32c1585a7c0af7a42b143f1c8c63fe941b5",
    "semantic_title": "analysis of uvama urubugal in tamil sangam literatures",
    "citation_count": 0,
    "authors": [
      "Subalalitha Cn"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.11": {
    "title": "Task-Oriented Dialog Systems for Dravidian Languages",
    "volume": "workshop",
    "abstract": "Task-oriented dialog systems help a user achieve a particular goal by parsing user requests to execute a particular action. These systems typically require copious amounts of training data to effectively understand the user intent and its corresponding slots. Acquiring large training corpora requires significant manual effort in annotation, rendering its construction infeasible for low-resource languages. In this paper, we present a two-step approach for automatically constructing task-oriented dialogue data in such languages by making use of annotated data from high resource languages. First, we use a machine translation (MT) system to translate the utterance and slot information to the target language. Second, we use token prefix matching and mBERT based semantic matching to align the slot tokens to the corresponding tokens in the utterance. We hand-curate a new test dataset in two low-resource Dravidian languages and show the significance and impact of our training dataset construction using a state-of-the-art mBERT model - achieving a Slot F1 of 81.51 (Kannada) and 78.82 (Tamil) on our test sets",
    "checked": true,
    "id": "6f5545084dbed93ef65a19cfa116f43ddeb3fbea",
    "semantic_title": "task-oriented dialog systems for dravidian languages",
    "citation_count": 4,
    "authors": [
      "Tushar Kanakagiri",
      "Karthik Radhakrishnan"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.12": {
    "title": "A Survey on Paralinguistics in Tamil Speech Processing",
    "volume": "workshop",
    "abstract": "Speech carries not only the semantic content but also the paralinguistic information which captures the speaking style. Speaker traits and emotional states affect how words are being spoken. The research on paralinguistic information is an emerging field in speech and language processing and it has many potential applications including speech recognition, speaker identification and verification, emotion recognition and accent recognition. Among them, there is a significant interest in emotion recognition from speech. A detailed study of paralinguistic information present in speech signal and an overview of research work related to speech emotion for Tamil Language is presented in this paper",
    "checked": true,
    "id": "f63f02446de975a798c57b6505e8e3f843df5920",
    "semantic_title": "a survey on paralinguistics in tamil speech processing",
    "citation_count": 1,
    "authors": [
      "Anosha Ignatius",
      "Uthayasanker Thayasivam"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.13": {
    "title": "Is this Enough?-Evaluation of Malayalam Wordnet",
    "volume": "workshop",
    "abstract": "Quality of a product is the degree to which a product meets the customer's expectation, which must also be valid for the case of lexical semantic resources. Conducting a periodic evaluation of resources is essential to ensure if the resources meet a native speaker's expectations and free from errors. This paper defines the possible mistakes in a lexical semantic resource and explains the steps applied to quantify Malayalam wordnet quality. Malayalam is one of the classical languages of India. We hope to subset the less quality part of the wordnet and perform crowdsourcing to make it better",
    "checked": true,
    "id": "13f0b396fbfa472e7b6f68d3016dbe472de058b9",
    "semantic_title": "is this enough?-evaluation of malayalam wordnet",
    "citation_count": 1,
    "authors": [
      "Nandu Chandran Nair",
      "Maria-chiara Giangregorio",
      "Fausto Giunchiglia"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.14": {
    "title": "LA-SACo: A Study of Learning Approaches for Sentiments Analysis inCode-Mixing Texts",
    "volume": "workshop",
    "abstract": "Substantial amount of text data which is increasingly being generated and shared on the internet and social media every second affect the society positively or negatively almost in any aspect of online world and also business and industries. Sentiments/opinions/reviews' of users posted on social media are the valuable information that have motivated researchers to analyze them to get better insight and feedbacks about any product such as a video in Instagram, a movie in Netflix, or even new brand car introduced by BMW. Sentiments are usually written using a combination of languages such as English which is resource rich and regional languages such as Tamil, Kannada, Malayalam, etc. which are resource poor. However, due to technical constraints, many users prefer to pen their opinions in Roman script. These kinds of texts written in two or more languages using a common language script or different language scripts are called code-mixing texts. Code-mixed texts are increasing day-by-day with the increase in the number of users depending on various online platforms. Analyzing such texts pose a real challenge for the researchers. In view of the challenges posed by the code-mixed texts, this paper describes three proposed models namely, SACo-Ensemble, SACo-Keras, and SACo-ULMFiT using Machine Learning (ML), Deep Learning (DL), and Transfer Learning (TL) approaches respectively for the task of Sentiments Analysis in Tamil-English and Malayalam-English code-mixed texts",
    "checked": true,
    "id": "3dc581979fe1d461f6aa22ee96ab8bbbce2f07b3",
    "semantic_title": "la-saco: a study of learning approaches for sentiments analysis incode-mixing texts",
    "citation_count": 7,
    "authors": [
      "Fazlourrahman Balouchzahi",
      "H L Shashirekha"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.15": {
    "title": "Findings of the Shared Task on Machine Translation in Dravidian languages",
    "volume": "workshop",
    "abstract": "This paper presents an overview of the shared task on machine translation of Dravidian languages. We presented the shared task results at the EACL 2021 workshop on Speech and Language Technologies for Dravidian Languages. This paper describes the datasets used, the methodology used for the evaluation of participants, and the experiments' overall results. As a part of this shared task, we organized four sub-tasks corresponding to machine translation of the following language pairs: English to Tamil, English to Malayalam, English to Telugu and Tamil to Telugu which are available at https://competitions.codalab.org/competitions/27650. We provided the participants with training and development datasets to perform experiments, and the results were evaluated on unseen test data. In total, 46 research groups participated in the shared task and 7 experimental runs were submitted for evaluation. We used BLEU scores for assessment of the translations",
    "checked": true,
    "id": "5ea99d0ed553bbfb2fe421493c999a34e60dadc7",
    "semantic_title": "findings of the shared task on machine translation in dravidian languages",
    "citation_count": 19,
    "authors": [
      "Bharathi Raja Chakravarthi",
      "Ruba Priyadharshini",
      "Shubhanker Banerjee",
      "Richard Saldanha",
      "John P. McCrae",
      "Anand Kumar M",
      "Parameswari Krishnamurthy",
      "Melvin Johnson"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.16": {
    "title": "Findings of the Shared Task on Troll Meme Classification in Tamil",
    "volume": "workshop",
    "abstract": "The internet has facilitated its user-base with a platform to communicate and express their views without any censorship. On the other hand, this freedom of expression or free speech can be abused by its user or a troll to demean an individual or a group. Demeaning people based on their gender, sexual orientation, religious believes or any other characteristics –trolling– could cause great distress in the online community. Hence, the content posted by a troll needs to be identified and dealt with before causing any more damage. Amongst all the forms of troll content, memes are most prevalent due to their popularity and ability to propagate across cultures. A troll uses a meme to demean, attack or offend its targetted audience. In this shared task, we provide a resource (TamilMemes) that could be used to train a system capable of identifying a troll meme in the Tamil language. In our TamilMemes dataset, each meme has been categorized into either a \"troll\" or a \"not_troll\" class. Along with the meme images, we also provided the Latin transcripted text from memes. We received 10 system submissions from the participants which were evaluated using the weighted average F1-score. The system with the weighted average F1-score of 0.55 secured the first rank",
    "checked": true,
    "id": "59f378d05209dfa77d9c15cc210f8b63f64c5b3b",
    "semantic_title": "findings of the shared task on troll meme classification in tamil",
    "citation_count": 56,
    "authors": [
      "Shardul Suryawanshi",
      "Bharathi Raja Chakravarthi"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.17": {
    "title": "Findings of the Shared Task on Offensive Language Identification in Tamil, Malayalam, and Kannada",
    "volume": "workshop",
    "abstract": "Detecting offensive language in social media in local languages is critical for moderating user-generated content. Thus, the field of offensive language identification in under-resourced Tamil, Malayalam and Kannada languages are essential. As the user-generated content is more code-mixed and not well studied for under-resourced languages, it is imperative to create resources and conduct benchmarking studies to encourage research in under-resourced Dravidian languages. We created a shared task on offensive language detection in Dravidian languages. We summarize here the dataset for this challenge which are openly available at https://competitions.codalab.org/competitions/27654, and present an overview of the methods and the results of the competing systems",
    "checked": true,
    "id": "b3cd10556fb8f9abe60e76471ed8a27864e92e82",
    "semantic_title": "findings of the shared task on offensive language identification in tamil, malayalam, and kannada",
    "citation_count": 98,
    "authors": [
      "Bharathi Raja Chakravarthi",
      "Ruba Priyadharshini",
      "Navya Jose",
      "Anand Kumar M",
      "Thomas Mandl",
      "Prasanna Kumar Kumaresan",
      "Rahul Ponnusamy",
      "Hariharan R L",
      "John P. McCrae",
      "Elizabeth Sherly"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.18": {
    "title": "GX@DravidianLangTech-EACL2021: Multilingual Neural Machine Translation and Back-translation",
    "volume": "workshop",
    "abstract": "In this paper, we describe the GX system in the EACL2021 shared task on machine translation in Dravidian languages. Given the low amount of parallel training data, We adopt two methods to improve the overall performance: (1) multilingual translation, we use a shared encoder-decoder multilingual translation model handling multiple languages simultaneously to facilitate the translation performance of these languages; (2) back-translation, we collected other open-source parallel and monolingual data and apply back-translation to benefit from the monolingual data. The experimental results show that we can achieve satisfactory translation results in these Dravidian languages and rank first in English-Telugu and Tamil-Telugu translation",
    "checked": true,
    "id": "d3bc0c6ea3614bc02d67f94fb7dcfcf345a110f4",
    "semantic_title": "gx@dravidianlangtech-eacl2021: multilingual neural machine translation and back-translation",
    "citation_count": 4,
    "authors": [
      "Wanying Xie"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.19": {
    "title": "OFFLangOne@DravidianLangTech-EACL2021: Transformers with the Class Balanced Loss for Offensive Language Identification in Dravidian Code-Mixed text",
    "volume": "workshop",
    "abstract": "The intensity of online abuse has increased in recent years. Automated tools are being developed to prevent the use of hate speech and offensive content. Most of the technologies use natural language and machine learning tools to identify offensive text. In a multilingual society, where code-mixing is a norm, the hate content would be delivered in a code-mixed form in social media, which makes the offensive content identification, further challenging. In this work, we participated in the EACL task to detect offensive content in the code-mixed social media scenario. The methodology uses a transformer model with transliteration and class balancing loss for offensive content identification. In this task, our model has been ranked 2nd in Malayalam-English and 4th in Tamil-English code-mixed languages",
    "checked": true,
    "id": "4be0d324f9c3ae8f925d5dbfca12654f211d95d5",
    "semantic_title": "offlangone@dravidianlangtech-eacl2021: transformers with the class balanced loss for offensive language identification in dravidian code-mixed text",
    "citation_count": 12,
    "authors": [
      "Suman Dowlagar",
      "Radhika Mamidi"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.20": {
    "title": "Simon @ DravidianLangTech-EACL2021: Detecting Offensive Content in Kannada Language",
    "volume": "workshop",
    "abstract": "This article introduces the system for the shared task of Offensive Language Identification in Dravidian Languages-EACL 2021. The world's information technology develops at a high speed. People are used to expressing their views and opinions on social media. This leads to a lot of offensive language on social media. As people become more dependent on social media, the detection of offensive language becomes more and more necessary. This shared task is in three languages: Tamil, Malayalam, and Kannada. Our team takes part in the Kannada language task. To accomplish the task, we use the XLM-Roberta model for pre-training. But the capabilities of the XLM-Roberta model do not satisfy us in terms of statement information collection. So we made some tweaks to the output of this model. In this paper, we describe the models and experiments for accomplishing the task of the Kannada language",
    "checked": true,
    "id": "d968f7ad6bc2d57ad87473872f8884560ca12780",
    "semantic_title": "simon @ dravidianlangtech-eacl2021: detecting offensive content in kannada language",
    "citation_count": 6,
    "authors": [
      "Qinyu Que"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.21": {
    "title": "Codewithzichao@DravidianLangTech-EACL2021: Exploring Multilingual Transformers for Offensive Language Identification on Code Mixing Text",
    "volume": "workshop",
    "abstract": "This paper describes our solution submitted to shared task on Offensive Language Identification in Dravidian Languages. We participated in all three of offensive language identification. In order to address the task, we explored multilingual models based on XLM-RoBERTa and multilingual BERT trained on mixed data of three code-mixed languages. Besides, we solved the class-imbalance problem existed in training data by class combination, class weights and focal loss. Our model achieved weighted average F1 scores of 0.75 (ranked 4th), 0.94 (ranked 4th) and 0.72 (ranked 3rd) in Tamil-English task, Malayalam-English task and Kannada-English task, respectively",
    "checked": true,
    "id": "7ef60ca5d2a5cd449d04d544bf2f2abeed07a791",
    "semantic_title": "codewithzichao@dravidianlangtech-eacl2021: exploring multilingual transformers for offensive language identification on code mixing text",
    "citation_count": 12,
    "authors": [
      "Zichao Li"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.22": {
    "title": "JudithJeyafreedaAndrew@DravidianLangTech-EACL2021:Offensive language detection for Dravidian Code-mixed YouTube comments",
    "volume": "workshop",
    "abstract": "Title: JudithJeyafreedaAndrew@DravidianLangTech-EACL2021:Offensive language detection for Dravidian Code-mixed YouTube comments Author: Judith Jeyafreeda Andrew Messaging online has become one of the major ways of communication. At this level, there are cases of online/digital bullying. These include rants, taunts, and offensive phrases. Thus the identification of offensive language on the internet is a very essential task. In this paper, the task of offensive language detection on YouTube comments from the Dravidian lan- guages of Tamil, Malayalam and Kannada are seen upon as a mutliclass classification prob- lem. After being subjected to language spe- cific pre-processing, several Machine Learn- ing algorithms have been trained for the task at hand. The paper presents the accuracy results on the development datasets for all Machine Learning models that have been used and fi- nally presents the weighted average scores for the test set when using the best performing Ma- chine Learning model",
    "checked": true,
    "id": "6e4c4c9abab2e822be076018d80f13853ae2c369",
    "semantic_title": "judithjeyafreedaandrew@dravidianlangtech-eacl2021:offensive language detection for dravidian code-mixed youtube comments",
    "citation_count": 17,
    "authors": [
      "Judith Jeyafreeda Andrew"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.23": {
    "title": "professionals@DravidianLangTech-EACL2021: Malayalam Offensive Language Identification - A Minimalistic Approach",
    "volume": "workshop",
    "abstract": "The submission is being made as a working note as part of the Offensive Language Identification in Dravidian Languages shared task. The proposed model \"DrOLIC\" uses IndicBERT and a simple 4-layered MLP to do the multiclass classification problem and we achieved an F1 score of 0.85 on the Malayalam dataset",
    "checked": true,
    "id": "99350765e2e03ad69c4f9d378165c97afc8905ad",
    "semantic_title": "professionals@dravidianlangtech-eacl2021: malayalam offensive language identification - a minimalistic approach",
    "citation_count": 4,
    "authors": [
      "Srinath Nair",
      "Dolton Fernandes"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.24": {
    "title": "UVCE-IIITT@DravidianLangTech-EACL2021: Tamil Troll Meme Classification: You need to Pay more Attention",
    "volume": "workshop",
    "abstract": "Tamil is a Dravidian language that is commonly used and spoken in the southern part of Asia. During the 21st century and in the era of social media, memes have been a fun moment during the day to day life of people. Here, we try to analyze the true meaning of Tamil memes by classifying them as troll or non-troll. We present an ingenious model consisting of transformer-transformer architecture that tries to attain state of the art by using attention as its main component. The dataset consists of troll and non-troll images with their captions as texts. The task is a binary classification task. The objective of the model was to pay more and more attention to the extracted features and to ignore the noise in both images and text",
    "checked": true,
    "id": "38da3928da55fdc4ed63e63ef4c3e4afc9839729",
    "semantic_title": "uvce-iiitt@dravidianlangtech-eacl2021: tamil troll meme classification: you need to pay more attention",
    "citation_count": 28,
    "authors": [
      "Siddhanth U Hegde",
      "Adeep Hande",
      "Ruba Priyadharshini",
      "Sajeetha Thavareesan",
      "Bharathi Raja Chakravarthi"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.25": {
    "title": "IIITT@DravidianLangTech-EACL2021: Transfer Learning for Offensive Language Detection in Dravidian Languages",
    "volume": "workshop",
    "abstract": "This paper demonstrates our work for the shared task on Offensive Language Identification in Dravidian Languages-EACL 2021. Offensive language detection in the various social media platforms was identified previously. But with the increase in diversity of users, there is a need to identify the offensive language in multilingual posts that are largely code-mixed or written in a non-native script. We approach this challenge with various transfer learning-based models to classify a given post or comment in Dravidian languages (Malayalam, Tamil, and Kannada) into 6 categories. The source codes for our systems are published",
    "checked": true,
    "id": "0bd615589c399c77ffa7847e45e28d5dc677884f",
    "semantic_title": "iiitt@dravidianlangtech-eacl2021: transfer learning for offensive language detection in dravidian languages",
    "citation_count": 79,
    "authors": [
      "Konthala Yasaswini",
      "Karthik Puranik",
      "Adeep Hande",
      "Ruba Priyadharshini",
      "Sajeetha Thavareesan",
      "Bharathi Raja Chakravarthi"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.26": {
    "title": "Hypers@DravidianLangTech-EACL2021: Offensive language identification in Dravidian code-mixed YouTube Comments and Posts",
    "volume": "workshop",
    "abstract": "Code-Mixed Offensive contents are used pervasively in social media posts in the last few years. Consequently, gained the significant attraction of the research community for identifying the different forms of such content (e.g., hate speech, and sentiments) and contributed to the creation of datasets. Most of the recent studies deal with high-resource languages (e.g., English) due to many publicly available datasets, and by the lack of dataset in low-resource anguages, those studies are slightly involved in these languages. Therefore, this study has the focus on offensive language identification on code-mixed low-resourced Dravidian languages such as Tamil, Kannada, and Malayalam using the bidirectional approach and fine-tuning strategies. According to the leaderboard, the proposed model got a 0.96 F1-score for Malayalam, 0.73 F1-score for Tamil, and 0.70 F1-score for Kannada in the bench-mark. Moreover, in the view of multilingual models, this modal ranked 3rd and achieved favorable results and confirmed the model as the best among all systems submitted to these shared tasks in these three languages",
    "checked": true,
    "id": "b1672099f5b2962ba04c969770beb0a37c7af76e",
    "semantic_title": "hypers@dravidianlangtech-eacl2021: offensive language identification in dravidian code-mixed youtube comments and posts",
    "citation_count": 14,
    "authors": [
      "Charangan Vasantharajan",
      "Uthayasanker Thayasivam"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.27": {
    "title": "HUB@DravidianLangTech-EACL2021: Identify and Classify Offensive Text in Multilingual Code Mixing in Social Media",
    "volume": "workshop",
    "abstract": "This paper introduces the system description of the HUB team participating in DravidianLangTech - EACL2021: Offensive Language Identification in Dravidian Languages. The theme of this shared task is the detection of offensive content in social media. Among the known tasks related to offensive speech detection, this is the first task to detect offensive comments posted in social media comments in the Dravidian language. The task organizer team provided us with the code-mixing task data set mainly composed of three different languages: Malayalam, Kannada, and Tamil. The tasks on the code mixed data in these three different languages can be seen as three different comment/post-level classification tasks. The task on the Malayalam data set is a five-category classification task, and the Kannada and Tamil language data sets are two six-category classification tasks. Based on our analysis of the task description and task data set, we chose to use the multilingual BERT model to complete this task. In this paper, we will discuss our fine-tuning methods, models, experiments, and results",
    "checked": true,
    "id": "3b1b356f58a303650ad49af79a192a9526ce855b",
    "semantic_title": "hub@dravidianlangtech-eacl2021: identify and classify offensive text in multilingual code mixing in social media",
    "citation_count": 6,
    "authors": [
      "Bo Huang",
      "Yang Bai"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.28": {
    "title": "HUB@DravidianLangTech-EACL2021: Meme Classification for Tamil Text-Image Fusion",
    "volume": "workshop",
    "abstract": "This article describes our system for task DravidianLangTech - EACL2021: Meme classification for Tamil. In recent years, we have witnessed the rapid development of the Internet and social media. Compared with traditional TV and radio media platforms, there are not so many restrictions on the use of online social media for individuals and many functions of online social media platforms are free. Based on this feature of social media, it is difficult for people's posts/comments on social media to be strictly and effectively controlled like TV and radio content. Therefore, the detection of negative information in social media has attracted attention from academic and industrial fields in recent years. The task of classifying memes is also driven by offensive posts/comments prevalent on social media. The data of the meme classification task is the fusion data of text and image information. To identify the content expressed by the meme, we develop a system that combines BiGRU and CNN. It can fuse visual features and text features to achieve the purpose of using multi-modal information from memetic data. In this article, we discuss our methods, models, experiments, and results",
    "checked": true,
    "id": "d014820622deeeea5403e73193165650275cf28c",
    "semantic_title": "hub@dravidianlangtech-eacl2021: meme classification for tamil text-image fusion",
    "citation_count": 3,
    "authors": [
      "Bo Huang",
      "Yang Bai"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.29": {
    "title": "ZYJ123@DravidianLangTech-EACL2021: Offensive Language Identification based on XLM-RoBERTa with DPCNN",
    "volume": "workshop",
    "abstract": "The development of online media platforms has given users more opportunities to post and comment freely, but the negative impact of offensive language has become increasingly apparent. It is very necessary for the automatic identification system of offensive language. This paper describes our work on the task of Offensive Language Identification in Dravidian language-EACL 2021. To complete this task, we propose a system based on the multilingual model XLM-Roberta and DPCNN. The test results on the official test data set confirm the effectiveness of our system. The weighted average F1-score of Kannada, Malayalam, and Tami language are 0.69, 0.92, and 0.76 respectively, ranked 6th, 6th, and 3rd",
    "checked": true,
    "id": "15d9c0e078104c16232ed525b1f3b9ed2ceedd1a",
    "semantic_title": "zyj123@dravidianlangtech-eacl2021: offensive language identification based on xlm-roberta with dpcnn",
    "citation_count": 17,
    "authors": [
      "Yingjia Zhao",
      "Xin Tao"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.30": {
    "title": "IIITK@DravidianLangTech-EACL2021: Offensive Language Identification and Meme Classification in Tamil, Malayalam and Kannada",
    "volume": "workshop",
    "abstract": "This paper describes the IIITK team's submissions to the offensive language identification, and troll memes classification shared tasks for Dravidian languages at DravidianLangTech 2021 workshop@EACL 2021. Our best configuration for Tamil troll meme classification achieved 0.55 weighted average F1 score, and for offensive language identification, our system achieved weighted F1 scores of 0.75 for Tamil, 0.95 for Malayalam, and 0.71 for Kannada. Our rank on Tamil troll meme classification is 2, and offensive language identification in Tamil, Malayalam and Kannada are 3, 3 and 4 respectively",
    "checked": true,
    "id": "e36966efc3129d445f7a25888a9dc341d9a1c532",
    "semantic_title": "iiitk@dravidianlangtech-eacl2021: offensive language identification and meme classification in tamil, malayalam and kannada",
    "citation_count": 61,
    "authors": [
      "Nikhil Ghanghor",
      "Parameswari Krishnamurthy",
      "Sajeetha Thavareesan",
      "Ruba Priyadharshini",
      "Bharathi Raja Chakravarthi"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.31": {
    "title": "cs@DravidianLangTech-EACL2021: Offensive Language Identification Based On Multilingual BERT Model",
    "volume": "workshop",
    "abstract": "This paper introduces the related content of the task \"Offensive Language Identification in Dravidian LANGUAGES-EACL 2021\". The task requires us to classify Dravidian languages collected from social media into Not-Offensive, Off-Untargeted, Off-Target-Individual, etc. This data set contains actual annotations in code-mixed text posted by users on Youtube, not from the monolingual text in textbooks. Based on the features of the data set code mixture, we use multilingual BERT and TextCNN for semantic extraction and text classification. In this article, we will show the experiment and result analysis of this task",
    "checked": true,
    "id": "d9c768b019955a4620678b9777b251cffba3ca07",
    "semantic_title": "cs@dravidianlangtech-eacl2021: offensive language identification based on multilingual bert model",
    "citation_count": 9,
    "authors": [
      "Shi Chen",
      "Bing Kong"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.32": {
    "title": "CUSATNLP@DravidianLangTech-EACL2021:Language Agnostic Classification of Offensive Content in Tweets",
    "volume": "workshop",
    "abstract": "Identifying offensive information from tweets is a vital language processing task. This task concentrated more on English and other foreign languages these days. In this shared task on Offensive Language Identification in Dravidian Languages, in the First Workshop of Speech and Language Technologies for Dravidian Languages in EACL 2021, the aim is to identify offensive content from code mixed Dravidian Languages Kannada, Malayalam, and Tamil. Our team used language agnostic BERT (Bidirectional Encoder Representation from Transformers) for sentence embedding and a Softmax classifier. The language-agnostic representation based classification helped obtain good performance for all the three languages, out of which results for the Malayalam language are good enough to obtain a third position among the participating teams",
    "checked": true,
    "id": "1ff4b20904d3c65e6b1df3cfcb7b3a5c8c3f8658",
    "semantic_title": "cusatnlp@dravidianlangtech-eacl2021:language agnostic classification of offensive content in tweets",
    "citation_count": 6,
    "authors": [
      "Sara Renjit",
      "Sumam Mary Idicula"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.33": {
    "title": "IIIT_DWD@EACL2021: Identifying Troll Meme in Tamil using a hybrid deep learning approach",
    "volume": "workshop",
    "abstract": "Social media are an open forum that allows people to share their knowledge, abilities, talents, ideas, or expressions. Simultaneously, it also allows people to post disrespectful, trolling, defamation, or negative content targeting users or the community based on their gender, race, religious beliefs, etc. Such posts are available in the form of text, image, video, and meme. Among them, memes are currently widely used to disseminate offensive material amongst people. It is primarily in the form of pictures and text. In the present paper, troll memes are identified, which is necessary to create a healthy society. To do so, a hybrid deep learning model combining convolutional neural networks and bidirectional long short term memory is proposed to identify trolled memes. The dataset used in the study is a part of the competition EACL 2021: Troll Meme classification in Tamil. The proposed model obtained 10th rank in the competition and reported a precision of 0.52, recall 0.59, and weighted F10.3",
    "checked": true,
    "id": "2bd12f93e583862583b224c3d1e8d0c3e5f18c70",
    "semantic_title": "iiit_dwd@eacl2021: identifying troll meme in tamil using a hybrid deep learning approach",
    "citation_count": 4,
    "authors": [
      "Ankit Kumar Mishra",
      "Sunil Saumya"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.34": {
    "title": "Amrita_CEN_NLP@DravidianLangTech-EACL2021: Deep Learning-based Offensive Language Identification in Malayalam, Tamil and Kannada",
    "volume": "workshop",
    "abstract": "This paper describes the submission of the team Amrita_CEN_NLP to the shared task on Offensive Language Identification in Dravidian Languages at EACL 2021. We implemented three deep neural network architectures such as a hybrid network with a Convolutional layer, a Bidirectional Long Short-Term Memory network (Bi-LSTM) layer and a hidden layer, a network containing a Bi-LSTM and another with a Bidirectional Recurrent Neural Network (Bi-RNN). In addition to that, we incorporated a cost-sensitive learning approach to deal with the problem of class imbalance in the training data. Among the three models, the hybrid network exhibited better training performance, and we submitted the predictions based on the same",
    "checked": true,
    "id": "ed144bf07c8e918df6aeb7b4b0a9673c77e97060",
    "semantic_title": "amrita_cen_nlp@dravidianlangtech-eacl2021: deep learning-based offensive language identification in malayalam, tamil and kannada",
    "citation_count": 16,
    "authors": [
      "Sreelakshmi K",
      "Premjith B",
      "Soman Kp"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.35": {
    "title": "NLP-CUET@DravidianLangTech-EACL2021: Offensive Language Detection from Multilingual Code-Mixed Text using Transformers",
    "volume": "workshop",
    "abstract": "The increasing accessibility of the internet facilitated social media usage and encouraged individuals to express their opinions liberally. Nevertheless, it also creates a place for content polluters to disseminate offensive posts or contents. Most of such offensive posts are written in a cross-lingual manner and can easily evade the online surveillance systems. This paper presents an automated system that can identify offensive text from multilingual code-mixed data. In the task, datasets provided in three languages including Tamil, Malayalam and Kannada code-mixed with English where participants are asked to implement separate models for each language. To accomplish the tasks, we employed two machine learning techniques (LR, SVM), three deep learning (LSTM, LSTM+Attention) techniques and three transformers (m-BERT, Indic-BERT, XLM-R) based methods. Results show that XLM-R outperforms other techniques in Tamil and Malayalam languages while m-BERT achieves the highest score in the Kannada language. The proposed models gained weighted f_1 score of 0.76 (for Tamil), 0.93 (for Malayalam ), and 0.71 (for Kannada) with a rank of 3rd, 5th and 4th respectively",
    "checked": true,
    "id": "d3f0665d4f6a54259f0715facfcb49c29d2bae83",
    "semantic_title": "nlp-cuet@dravidianlangtech-eacl2021: offensive language detection from multilingual code-mixed text using transformers",
    "citation_count": 29,
    "authors": [
      "Omar Sharif",
      "Eftekhar Hossain",
      "Mohammed Moshiul Hoque"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.36": {
    "title": "IRLAB-DAIICT@DravidianLangTech-EACL2021: Neural Machine Translation",
    "volume": "workshop",
    "abstract": "This paper describes our team's submission of the EACL DravidianLangTech-2021's shared task on Machine Translation of Dravidian languages. We submitted our translations for English to Malayalam , Tamil , Telugu and also Tamil-Telugu language pairs. The submissions mainly focus on having adequate amount of data backed up by good preprocessing of it to produce quality translations,which includes some custom made rules to remove unnecessary sentences. We conducted several experiments on these models by tweaking the architecture,Byte Pair Encoding (BPE) and other hyperparameters",
    "checked": true,
    "id": "0bac1f296c13970b3aec86107164c1ce540816cc",
    "semantic_title": "irlab-daiict@dravidianlangtech-eacl2021: neural machine translation",
    "citation_count": 3,
    "authors": [
      "Raj Prajapati",
      "Vedant Vijay Parikh",
      "Prasenjit Majumder"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.37": {
    "title": "IRNLP_DAIICT@DravidianLangTech-EACL2021:Offensive Language identification in Dravidian Languages using TF-IDF Char N-grams and MuRIL",
    "volume": "workshop",
    "abstract": "This paper presents the participation of the IRNLPDAIICT team from Information Retrieval and Natural Language Processing lab at DA-IICT, India in DravidianLangTech-EACL2021 Offensive Language identification in Dravidian Languages. The aim of this shared task is to identify Offensive Language from a code-mixed data-set of YouTube comments. The task is to classify comments into Not Offensive (NO), Offensive Untargetede(OU), Offensive Targeted Individual (OTI), Offensive Targeted Group (OTG), Offensive Targeted Others (OTO), Other Language (OL) for three Dravidian languages: Kannada, Malayalam and Tamil. We use TF-IDF character n-grams and pretrained MuRIL embeddings for text representation and Logistic Regression and Linear SVM for classification. Our best approach achieved Ninth, Third and Eighth with weighted F1 score of 0.64, 0.95 and 0.71in Kannada, Malayalam and Tamil on test dataset respectively",
    "checked": true,
    "id": "75bf08ca8319937163236f110594777cbc42e9c3",
    "semantic_title": "irnlp_daiict@dravidianlangtech-eacl2021:offensive language identification in dravidian languages using tf-idf char n-grams and muril",
    "citation_count": 12,
    "authors": [
      "Bhargav Dave",
      "Shripad Bhat",
      "Prasenjit Majumder"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.38": {
    "title": "Hate-Alert@DravidianLangTech-EACL2021: Ensembling strategies for Transformer-based Offensive language Detection",
    "volume": "workshop",
    "abstract": "Social media often acts as breeding grounds for different forms of offensive content. For low resource languages like Tamil, the situation is more complex due to the poor performance of multilingual or language-specific models and lack of proper benchmark datasets. Based on this shared task \"Offensive Language Identification in Dravidian Languages\" at EACL 2021; we present an exhaustive exploration of different transformer models, We also provide a genetic algorithm technique for ensembling different models. Our ensembled models trained separately for each language secured the first position in Tamil, the second position in Kannada, and the first position in Malayalam sub-tasks. The models and codes are provided",
    "checked": true,
    "id": "db990538aa2279cf8bd2afc702ae9c4903b7630c",
    "semantic_title": "hate-alert@dravidianlangtech-eacl2021: ensembling strategies for transformer-based offensive language detection",
    "citation_count": 29,
    "authors": [
      "Debjoy Saha",
      "Naman Paharia",
      "Debajit Chakraborty",
      "Punyajoy Saha",
      "Animesh Mukherjee"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.39": {
    "title": "TrollMeta@DravidianLangTech-EACL2021: Meme classification using deep learning",
    "volume": "workshop",
    "abstract": "Memes act as a medium to carry one's feelings, cultural ideas, or practices by means of symbols, imitations, or simply images. Whenever social media is involved, hurting the feelings of others and abusing others are always a problem. Here we are proposing a system, that classifies the memes into abusive/offensive memes and neutral ones. The work involved classifying the images into offensive and non-offensive ones. The system implements resnet-50, a deep residual neural network architecture",
    "checked": true,
    "id": "9d7827e3708c718385e510cc59371ea27a6c6cc0",
    "semantic_title": "trollmeta@dravidianlangtech-eacl2021: meme classification using deep learning",
    "citation_count": 5,
    "authors": [
      "Manoj Balaji J",
      "Chinmaya Hs"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.40": {
    "title": "Maoqin @ DravidianLangTech-EACL2021: The Application of Transformer-Based Model",
    "volume": "workshop",
    "abstract": "This paper describes the result of team-Maoqin at DravidianLangTech-EACL2021. The provided task consists of three languages(Tamil, Malayalam, and Kannada), I only participate in one of the language task-Malayalam. The goal of this task is to identify offensive language content of the code-mixed dataset of comments/posts in Dravidian Languages (Tamil-English, Malayalam-English, and Kannada-English) collected from social media. This is a classification task at the comment/post level. Given a Youtube comment, systems have to classify it into Not-offensive, Offensive-untargeted, Offensive-targeted-individual, Offensive-targeted-group, Offensive-targeted-other, or Not-in-indented-language. I use the transformer-based language model with BiGRU-Attention to complete this task. To prove the validity of the model, I also use some other neural network models for comparison. And finally, the team ranks 5th in this task with a weighted average F1 score of 0.93 on the private leader board",
    "checked": true,
    "id": "33345871c229ca783dfb393582408995de4447c8",
    "semantic_title": "maoqin @ dravidianlangtech-eacl2021: the application of transformer-based model",
    "citation_count": 4,
    "authors": [
      "Maoqin Yang"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.41": {
    "title": "Simon @ DravidianLangTech-EACL2021: Meme Classification for Tamil with BERT",
    "volume": "workshop",
    "abstract": "In this paper, we introduce the system for the task of meme classification for Tamil, submitted by our team. In today's society, social media has become an important platform for people to communicate. We use social media to share information about ourselves and express our views on things. It has gradually developed a unique form of emotional expression on social media – meme. The meme is an expression that is often ironic. This also gives the meme a unique sense of humor. But it's not just positive content on social media. There's also a lot of offensive content. Meme's unique expression makes it often used by some users to post offensive content. Therefore, it is very urgent to detect the offensive content of the meme. Our team uses the natural language processing method to classify the offensive content of the meme. Our team combines the BERT model with the CNN to improve the model's ability to collect statement information. Finally, the F1-score of our team in the official test set is 0.49, and our method ranks 5th",
    "checked": true,
    "id": "189fbc407072079ed92ff07e39dbc619163fe182",
    "semantic_title": "simon @ dravidianlangtech-eacl2021: meme classification for tamil with bert",
    "citation_count": 2,
    "authors": [
      "Qinyu Que"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.42": {
    "title": "Bitions@DravidianLangTech-EACL2021: Ensemble of Multilingual Language Models with Pseudo Labeling for offence Detection in Dravidian Languages",
    "volume": "workshop",
    "abstract": "With the advent of social media, we have seen a proliferation of data and public discourse. Unfortunately, this includes offensive content as well. The problem is exacerbated due to the sheer number of languages spoken on these platforms and the multiple other modalities used for sharing offensive content (images, gifs, videos and more). In this paper, we propose a multilingual ensemble-based model that can identify offensive content targeted against an individual (or group) in low resource Dravidian language. Our model is able to handle code-mixed data as well as instances where the script used is mixed (for instance, Tamil and Latin). Our solution ranked number one for the Malayalam dataset and ranked 4th and 5th for Tamil and Kannada, respectively",
    "checked": true,
    "id": "e9bfc1ac3d2be88af977d5c07c3cfc80371b1ced",
    "semantic_title": "bitions@dravidianlangtech-eacl2021: ensemble of multilingual language models with pseudo labeling for offence detection in dravidian languages",
    "citation_count": 16,
    "authors": [
      "Debapriya Tula",
      "Prathyush Potluri",
      "Shreyas Ms",
      "Sumanth Doddapaneni",
      "Pranjal Sahu",
      "Rohan Sukumaran",
      "Parth Patwa"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.43": {
    "title": "NLP-CUET@DravidianLangTech-EACL2021: Investigating Visual and Textual Features to Identify Trolls from Multimodal Social Media Memes",
    "volume": "workshop",
    "abstract": "In the past few years, the meme has become a new way of communication on the Internet. As memes are in images forms with embedded text, it can quickly spread hate, offence and violence. Classifying memes are very challenging because of their multimodal nature and region-specific interpretation. A shared task is organized to develop models that can identify trolls from multimodal social media memes. This work presents a computational model that we developed as part of our participation in the task. Training data comes in two forms: an image with embedded Tamil code-mixed text and an associated caption. We investigated the visual and textual features using CNN, VGG16, Inception, m-BERT, XLM-R, XLNet algorithms. Multimodal features are extracted by combining image (CNN, ResNet50, Inception) and text (Bi-LSTM) features via early fusion approach. Results indicate that the textual approach with XLNet achieved the highest weighted f_1-score of 0.58, which enable our model to secure 3rd rank in this task",
    "checked": true,
    "id": "ded7cc38b301bc08e6c49964c12f8546dbfe1081",
    "semantic_title": "nlp-cuet@dravidianlangtech-eacl2021: investigating visual and textual features to identify trolls from multimodal social media memes",
    "citation_count": 12,
    "authors": [
      "Eftekhar Hossain",
      "Omar Sharif",
      "Mohammed Moshiul Hoque"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.44": {
    "title": "SJ_AJ@DravidianLangTech-EACL2021: Task-Adaptive Pre-Training of Multilingual BERT models for Offensive Language Identification",
    "volume": "workshop",
    "abstract": "In this paper we present our submission for the EACL 2021-Shared Task on Offensive Language Identification in Dravidian languages. Our final system is an ensemble of mBERT and XLM-RoBERTa models which leverage task-adaptive pre-training of multilingual BERT models with a masked language modeling objective. Our system was ranked 1st for Kannada, 2nd for Malayalam and 3rd for Tamil",
    "checked": true,
    "id": "d6a6b7e6d291106982b7c361cc3cab73dc136a4f",
    "semantic_title": "sj_aj@dravidianlangtech-eacl2021: task-adaptive pre-training of multilingual bert models for offensive language identification",
    "citation_count": 29,
    "authors": [
      "Sai Muralidhar Jayanthi",
      "Akshat Gupta"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.45": {
    "title": "SSNCSE_NLP@DravidianLangTech-EACL2021: Offensive Language Identification on Multilingual Code Mixing Text",
    "volume": "workshop",
    "abstract": "Social networks made a huge impact in almost all fields in recent years. Text messaging through the Internet or cellular phones has become a major medium of personal and commercial communication. Everyday we have to deal with texts, emails or different types of messages in which there are a variety of attacks and abusive phrases. It is the moderator's decision which comments to remove from the platform because of violations and which ones to keep but an automatic software for detecting abusive languages would be useful in recent days. In this paper we describe an automatic offensive language identification from Dravidian languages with various machine learning algorithms. This is work is shared task in DravidanLangTech-EACL2021. The goal of this task is to identify offensive language content of the code-mixed dataset of comments/posts in Dravidian Languages ( (Tamil-English, Malayalam-English, and Kannada-English)) collected from social media. This work explains the submissions made by SSNCSE_NLP in DravidanLangTech-EACL2021 Code-mix tasks for Offensive language detection. We achieve F1 scores of 0.95 for Malayalam, 0.7 for Kannada and 0.73 for task2-Tamil on the test-set",
    "checked": true,
    "id": "016864b91e439d49094610f92593d2b55e1976f2",
    "semantic_title": "ssncse_nlp@dravidianlangtech-eacl2021: offensive language identification on multilingual code mixing text",
    "citation_count": 35,
    "authors": [
      "Bharathi B",
      "Agnusimmaculate Silvia A"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.46": {
    "title": "JUNLP@DravidianLangTech-EACL2021: Offensive Language Identification in Dravidian Langauges",
    "volume": "workshop",
    "abstract": "Offensive language identification has been an active area of research in natural language processing. With the emergence of multiple social media platforms offensive language identification has emerged as a need of the hour. Traditional offensive language identification models fail to deliver acceptable results as social media contents are largely in multilingual and are code-mixed in nature. This paper tries to resolve this problem by using IndicBERT and BERT architectures, to facilitate identification of offensive languages for Kannada-English, Malayalam-English, and Tamil-English code-mixed language pairs extracted from social media. The presented approach when evaluated on the test corpus provided precision, recall, and F1 score for language pair Kannada-English as 0.62, 0.71, and 0.66, respectively, for language pair Malayalam-English as 0.77, 0.43, and 0.53, respectively, and for Tamil-English as 0.71, 0.74, and 0.72, respectively",
    "checked": true,
    "id": "f1864283b2bacbd0ac1cb53b1c9d4b26c13ee40b",
    "semantic_title": "junlp@dravidianlangtech-eacl2021: offensive language identification in dravidian langauges",
    "citation_count": 9,
    "authors": [
      "Avishek Garain",
      "Atanu Mandal",
      "Sudip Kumar Naskar"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.47": {
    "title": "MUCS@DravidianLangTech-EACL2021:COOLI-Code-Mixing Offensive Language Identification",
    "volume": "workshop",
    "abstract": "This paper describes the models submitted by the team MUCS for Offensive Language Identification in Dravidian Languages-EACL 2021 shared task that aims at identifying and classifying code-mixed texts of three language pairs namely, Kannada-English (Kn-En), Malayalam-English (Ma-En), and Tamil-English (Ta-En) into six predefined categories (5 categories in Ma-En language pair). Two models, namely, COOLI-Ensemble and COOLI-Keras are trained with the char sequences extracted from the sentences combined with words as features. Out of the two proposed models, COOLI-Ensemble model (best among our models) obtained first rank for Ma-En language pair with 0.97 weighted F1-score and fourth and sixth ranks with 0.75 and 0.69 weighted F1-score for Ta-En and Kn-En language pairs respectively",
    "checked": true,
    "id": "2ec11aaafd6cc98a40ef6908d1bca94c86d67ed0",
    "semantic_title": "mucs@dravidianlangtech-eacl2021:cooli-code-mixing offensive language identification",
    "citation_count": 20,
    "authors": [
      "Fazlourrahman Balouchzahi",
      "Aparna B K",
      "H L Shashirekha"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.48": {
    "title": "indicnlp@kgp at DravidianLangTech-EACL2021: Offensive Language Identification in Dravidian Languages",
    "volume": "workshop",
    "abstract": "The paper aims to classify different offensive content types in 3 code-mixed Dravidian language datasets. The work leverages existing state of the art approaches in text classification by incorporating additional data and transfer learning on pre-trained models. Our final submission is an ensemble of an AWD-LSTM based model along with 2 different transformer model architectures based on BERT and RoBERTa. We achieved weighted-average F1 scores of 0.97, 0.77, and 0.72 in the Malayalam-English, Tamil-English, and Kannada-English datasets ranking 1st, 2nd, and 3rd on the respective shared-task leaderboards",
    "checked": true,
    "id": "bd0eb9f318b9ce69d0f7ab28ba4cf1e0f5a6ff34",
    "semantic_title": "indicnlp@kgp at dravidianlangtech-eacl2021: offensive language identification in dravidian languages",
    "citation_count": 21,
    "authors": [
      "Kushal Kedia",
      "Abhilash Nandy"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.49": {
    "title": "SSNCSE_NLP@DravidianLangTech-EACL2021: Meme classification for Tamil using machine learning approach",
    "volume": "workshop",
    "abstract": "Social media are interactive platforms that facilitate the creation or sharing of information, ideas or other forms of expression among people. This exchange is not free from offensive, trolling or malicious contents targeting users or communities. One way of trolling is by making memes. A meme is an image or video that represents the thoughts and feelings of a specific audience. The challenge of dealing with memes is that they are region-specific and their meaning is often obscured in humour or sarcasm. A meme is a form of media that spreads an idea or emotion across the internet. The multi modal nature of memes, postings of hateful memes or related events like trolling, cyberbullying are increasing day by day. Memes make it even more challenging since they express humour and sarcasm in an implicit way, because of which the meme may not be offensive if we only consider the text or the image. In this paper we proposed a approach for meme classification for Tamil language that considers only the text present in the meme. This work explains the submissions made by SSNCSE NLP in DravidanLangTechEACL2021 task for meme classification in Tamil language. We achieve F1 scores of 0.50 using the proposed approach using the test-set",
    "checked": true,
    "id": "f8225f603f8de55e6c69d3b12563b44e475c7c36",
    "semantic_title": "ssncse_nlp@dravidianlangtech-eacl2021: meme classification for tamil using machine learning approach",
    "citation_count": 12,
    "authors": [
      "Bharathi B",
      "Agnusimmaculate Silvia A"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.50": {
    "title": "MUCS@ - Machine Translation for Dravidian Languages using Stacked Long Short Term Memory",
    "volume": "workshop",
    "abstract": "Dravidian language family is one of the largest language families in the world. In spite of its uniqueness, Dravidian languages have gained very less attention due to scarcity of resources to conduct language technology tasks such as translation, Parts-of-Speech tagging, Word Sense Disambiguation etc,. In this paper, we, team MUCS, describe sequence-to-sequence stacked Long Short Term Memory (LSTM) based Neural Machine Translation (NMT) model submitted to \"Machine Translation in Dravidian languages\", a shared task organized by EACL-2021. The NMT model was applied on translation using English-Tamil, EnglishTelugu, English-Malayalam and Tamil-Telugu corpora provided by the organizers. Standard evaluation metrics namely Bilingual Evaluation Understudy (BLEU) and human evaluations are used to evaluate the model. Our models exhibited good accuracy for all the language pairs and obtained 2nd rank for TamilTelugu language pair",
    "checked": true,
    "id": "6d187ca54ef0a1d081de705a380da36fb4b69303",
    "semantic_title": "mucs@ - machine translation for dravidian languages using stacked long short term memory",
    "citation_count": 5,
    "authors": [
      "Asha Hegde",
      "Ibrahim Gashaw",
      "Shashirekha H.l."
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.51": {
    "title": "OffTamil@DravideanLangTech-EASL2021: Offensive Language Identification in Tamil Text",
    "volume": "workshop",
    "abstract": "In the last few decades, Code-Mixed Offensive texts are used penetratingly in social media posts. Social media platforms and online communities showed much interest on offensive text identification in recent years. Consequently, research community is also interested in identifying such content and also contributed to the development of corpora. Many publicly available corpora are there for research on identifying offensive text written in English language but rare for low resourced languages like Tamil. The first code-mixed offensive text for Dravidian languages are developed by shared task organizers which is used for this study. This study focused on offensive language identification on code-mixed low-resourced Dravidian language Tamil using four classifiers (Support Vector Machine, random forest, k- Nearest Neighbour and Naive Bayes) using chiˆ2 feature selection technique along with BoW and TF-IDF feature representation techniques using different combinations of n-grams. This proposed model achieved an accuracy of 76.96% while using linear SVM with TF-IDF feature representation technique",
    "checked": true,
    "id": "290893c6e81fa6ba21dbc601a2ce85a6f9e7c030",
    "semantic_title": "offtamil@dravideanlangtech-easl2021: offensive language identification in tamil text",
    "citation_count": 4,
    "authors": [
      "Disne Sivalingam",
      "Sajeetha Thavareesan"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.52": {
    "title": "Codewithzichao@DravidianLangTech-EACL2021: Exploring Multimodal Transformers for Meme Classification in Tamil Language",
    "volume": "workshop",
    "abstract": "This paper describes our submission to shared task on Meme Classification for Tamil Language. To address this task, we explore a multimodal transformer for meme classification in Tamil language. According to the characteristics of the image and text, we use different pretrained models to encode the image and text so as to get better representations of the image and text respectively. Besides, we design a multimodal attention layer to make the text and corresponding image interact fully with each other based on cross attention. Our model achieved 0.55 weighted average F1 score and ranked first in this task",
    "checked": true,
    "id": "8688adcb49b3256a454800ca74ac8ddb0e750633",
    "semantic_title": "codewithzichao@dravidianlangtech-eacl2021: exploring multimodal transformers for meme classification in tamil language",
    "citation_count": 8,
    "authors": [
      "Zichao Li"
    ]
  },
  "https://aclanthology.org/2021.dravidianlangtech-1.53": {
    "title": "DLRG@DravidianLangTech-EACL2021: Transformer based approachfor Offensive Language Identification on Code-Mixed Tamil",
    "volume": "workshop",
    "abstract": "Internet advancements have made a huge impact on the communication pattern of people and their life style. People express their opinion on products, politics, movies etc. in social media. Even though, English is predominantly used, nowadays many people prefer to tweet in their native language and some- times by combining it with English. Sentiment analysis on such code-mixed tweets is challenging, due to large vocabulary, grammar and colloquial usage of many words. In this paper, the transformer based language model is applied to analyse the sentiment on Tanglish tweets, which is a combination of Tamil and English. This work has been submitted to the the shared task on DravidianLangTech- EACL2021. From the experimental results, it is shown that an F 1 score of 64% was achieved in detecting the hate speech in code-mixed Tamil-English tweets using bidirectional trans- former model",
    "checked": true,
    "id": "13c3f469bc074a0b1be27ad0305bfe55ad0c96a8",
    "semantic_title": "dlrg@dravidianlangtech-eacl2021: transformer based approachfor offensive language identification on code-mixed tamil",
    "citation_count": 11,
    "authors": [
      "Ratnavel Rajalakshmi",
      "Yashwant Reddy",
      "Lokesh Kumar"
    ]
  },
  "https://aclanthology.org/2021.gwc-1.1": {
    "title": "On Universal Colexifications",
    "volume": "workshop",
    "abstract": "Colexification occurs when two distinct concepts are lexified by the same word. The term covers both polysemy and homonymy. We posit and investigate the hypothesis that no pair of concepts are colexified in every language. We test our hypothesis by analyzing colexification data from BabelNet, Open Multilingual WordNet, and CLICS. The results show that our hypothesis is supported by over 99.9% of colexified concept pairs in these three lexical resources",
    "checked": true,
    "id": "1c4877a21698c895dc4f5840357dc762e1a3a5ad",
    "semantic_title": "on universal colexifications",
    "citation_count": 12,
    "authors": [
      "Hongchang Bao",
      "Bradley Hauer",
      "Grzegorz Kondrak"
    ]
  },
  "https://aclanthology.org/2021.gwc-1.2": {
    "title": "UZWORDNET: A Lexical-Semantic Database for the Uzbek Language",
    "volume": "workshop",
    "abstract": "The results reported in this paper aim to increase the presence of the Uzbek language in the Internet and its usability within IT applications. We describe the initial development of a \"word-net\" for the Uzbek language compatible to Princeton WordNet. We called it UZWORDNET. In the current version, UZWORDNET contains 28140 synsets, 64389 sense and 20683 words; its estimated accuracy is 75.98%. To the best of our knowledge, it is the largest wordnet for Uzbek existing to date, and the second wordnet developed overall",
    "checked": true,
    "id": "f336ecda2618603a71d67c1e08d2861c6e58530b",
    "semantic_title": "uzwordnet: a lexical-semantic database for the uzbek language",
    "citation_count": 5,
    "authors": [
      "Alessandro Agostini",
      "Timur Usmanov",
      "Ulugbek Khamdamov",
      "Nilufar Abdurakhmonova",
      "Mukhammadsaid Mamasaidov"
    ]
  },
  "https://aclanthology.org/2021.gwc-1.3": {
    "title": "Practical Approach on Implementation of WordNets for South African Languages",
    "volume": "workshop",
    "abstract": "This paper proposes the implementation of WordNets for five South African languages, namely, Sepedi, Setswana, Tshivenda, isiZulu and isiXhosa to be added to open multilingual WordNets (OMW) on natural language toolkit (NLTK). The African WordNets are converted from Princeton WordNet (PWN) 2.0 to 3.0 to match the synsets in PWN 3.0. After conversion, there were 7157, 11972, 1288, 6380, and 9460 lemmas for Sepedi, Setswana, Tshivenda, isiZulu and isiX- hosa respectively. Setswana, isiXhosa, Sepedi contains more lemmas compared to 8 languages in OMW and isiZulu contains more lemmas compared to 7 languages in OMW. A library has been published for continuous development of African WordNets in OMW using NLTK",
    "checked": true,
    "id": "c791b11785604c5123a0d72cc9bd74a939546c3e",
    "semantic_title": "practical approach on implementation of wordnets for south african languages",
    "citation_count": 1,
    "authors": [
      "Tshephisho Joseph Sefara",
      "Tumisho Billson Mokgonyane",
      "Vukosi Marivate"
    ]
  },
  "https://aclanthology.org/2021.gwc-1.4": {
    "title": "Homonymy and Polysemy Detection with Multilingual Information",
    "volume": "workshop",
    "abstract": "Deciding whether a semantically ambiguous word is homonymous or polysemous is equivalent to establishing whether it has any pair of senses that are semantically unrelated. We present novel methods for this task that leverage information from multilingual lexical resources. We formally prove the theoretical properties that provide the foundation for our methods. In particular, we show how the One Homonym Per Translation hypothesis of Hauer and Kondrak (2020a) follows from the synset properties formulated by Hauer and Kondrak (2020b). Experimental evaluation shows that our approach sets a new state of the art for homonymy detection",
    "checked": true,
    "id": "26711f78c9049b6e528e748724a29b08675a6788",
    "semantic_title": "homonymy and polysemy detection with multilingual information",
    "citation_count": 4,
    "authors": [
      "Amir Ahmad Habibi",
      "Bradley Hauer",
      "Grzegorz Kondrak"
    ]
  },
  "https://aclanthology.org/2021.gwc-1.5": {
    "title": "Taboo Wordnet",
    "volume": "workshop",
    "abstract": "This paper describes the development of an online lexical resource to help detection systems regulate and curb the use of offensive words online. With the growing prevalence of social media platforms, many conversations are now conducted on- line. The increase of online conversations for leisure, work and socializing has led to an increase in harassment. In particular, we create a specialized sense-based vocabulary of Japanese offensive words for the Open Multilingual Wordnet. This vocabulary expands on an existing list of Japanese offen- sive words and provides categorization and proper linking to synsets within the multilingual wordnet. This paper then discusses the evaluation of the vocabulary as a resource for representing and classifying offensive words and as a possible resource for offensive word use detection in social media",
    "checked": true,
    "id": "13d086822e93fcb6079b68acc71d01bc6f782aff",
    "semantic_title": "taboo wordnet",
    "citation_count": 1,
    "authors": [
      "Francis Bond",
      "Merrick Yeu Herng Choo"
    ]
  },
  "https://aclanthology.org/2021.gwc-1.6": {
    "title": "Ask2Transformers: Zero-Shot Domain labelling with Pretrained Language Models",
    "volume": "workshop",
    "abstract": "In this paper we present a system that exploits different pre-trained Language Models for assigning domain labels to WordNet synsets without any kind of supervision. Furthermore, the system is not restricted to use a particular set of domain labels. We exploit the knowledge encoded within different off-the-shelf pre-trained Language Models and task formulations to infer the domain label of a particular WordNet definition. The proposed zero-shot system achieves a new state-of-the-art on the English dataset used in the evaluation",
    "checked": true,
    "id": "0714e284776d64b2deb1b679f1cdf645653885f9",
    "semantic_title": "ask2transformers: zero-shot domain labelling with pretrained language models",
    "citation_count": 19,
    "authors": [
      "Oscar Sainz",
      "German Rigau"
    ]
  },
  "https://aclanthology.org/2021.gwc-1.7": {
    "title": "Discriminating Homonymy from Polysemy in Wordnets: English, Spanish and Polish Nouns",
    "volume": "workshop",
    "abstract": "We propose a novel method of homonymy-polysemy discrimination for three Indo-European Languages (English, Spanish and Polish). Support vector machines and LASSO logistic regression were successfully used in this task, outperforming baselines. The feature set utilised lemma properties, gloss similarities, graph distances and polysemy patterns. The proposed ML models performed equally well for English and the other two languages (constituting testing data sets). The algorithms not only ruled out most cases of homonymy but also were efficacious in distinguishing between closer and indirect semantic relatedness",
    "checked": true,
    "id": "a9ffcc78141544a54d8964e0280a23ed1b6d778f",
    "semantic_title": "discriminating homonymy from polysemy in wordnets: english, spanish and polish nouns",
    "citation_count": 1,
    "authors": [
      "Arkadiusz Janz",
      "Marek Maziarz"
    ]
  },
  "https://aclanthology.org/2021.gwc-1.8": {
    "title": "Implementing ASLNet V1.0: Progress and Plans",
    "volume": "workshop",
    "abstract": "We report on the development of ASLNet, a wordnet for American Sign Language (ASL). ASLNet V1.0 is currently under construction by mapping easy-to-translate ASL lexical nouns to Princeton WordNet synsets. We describe our data model and mapping approach, which can be extended to any sign language. Analysis of the 390 synsets processed to date indicates the success of our procedure yet also highlights the need to supplement our mapping with the \"merge\" method. We outline our plans for upcoming work to remedy this, which include use of ASL free-association data",
    "checked": true,
    "id": "46584d0dd0831df8b1294fb96ea91bb415cfa4fb",
    "semantic_title": "implementing aslnet v1.0: progress and plans",
    "citation_count": 4,
    "authors": [
      "Colin Lualdi",
      "Elaine Wright",
      "Jack Hudson",
      "Naomi Caselli",
      "Christiane Fellbaum"
    ]
  },
  "https://aclanthology.org/2021.gwc-1.9": {
    "title": "Monolingual Word Sense Alignment as a Classification Problem",
    "volume": "workshop",
    "abstract": "Words are defined based on their meanings in various ways in different resources. Aligning word senses across monolingual lexicographic resources increases domain coverage and enables integration and incorporation of data. In this paper, we explore the application of classification methods using manually-extracted features along with representation learning techniques in the task of word sense alignment and semantic relationship detection. We demonstrate that the performance of classification methods dramatically varies based on the type of semantic relationships due to the nature of the task but outperforms the previous experiments",
    "checked": true,
    "id": "7167e4130488f41229217cad00c2ae8878a6ccad",
    "semantic_title": "monolingual word sense alignment as a classification problem",
    "citation_count": 4,
    "authors": [
      "Sina Ahmadi",
      "John P. McCrae"
    ]
  },
  "https://aclanthology.org/2021.gwc-1.10": {
    "title": "Extraction of Common-Sense Relations from Procedural Task Instructions using BERT",
    "volume": "workshop",
    "abstract": "Manipulation-relevant common-sense knowledge is crucial to support action-planning for complex tasks. In particular, instrumentality information of what can be done with certain tools can be used to limit the search space which is growing exponentially with the number of viable options. Typical sources for such knowledge, structured common-sense knowledge bases such as ConceptNet or WebChild, provide a limited amount of information which also varies drastically across different domains. Considering the recent success of pre-trained language models such as BERT, we investigate whether common-sense information can directly be extracted from semi-structured text with an acceptable annotation effort. Concretely, we compare the common-sense relations obtained from ConceptNet versus those extracted with BERT from large recipe databases. In this context, we propose a scoring function, based on the WordNet taxonomy to match specific terms to more general ones, enabling a rich evaluation against a set of ground-truth relations",
    "checked": true,
    "id": "655db2fd59e4112d0a5596e1695c5765457ca53c",
    "semantic_title": "extraction of common-sense relations from procedural task instructions using bert",
    "citation_count": 10,
    "authors": [
      "Viktor Losing",
      "Lydia Fischer",
      "Jörg Deigmöller"
    ]
  },
  "https://aclanthology.org/2021.gwc-1.11": {
    "title": "The GlobalWordNet Formats: Updates for 2020",
    "volume": "workshop",
    "abstract": "The Global Wordnet Formats have been introduced to enable wordnets to have a common representation that can be integrated through the Global WordNet Grid. As a result of their adoption, a number of shortcomings of the format were identified, and in this paper we describe the extensions to the formats that address these issues. These include: ordering of senses, dependencies between wordnets, pronunciation, syntactic modelling, relations, sense keys, metadata and RDF support. Furthermore, we provide some perspectives on how these changes help in the integration of wordnets",
    "checked": true,
    "id": "e53589a0e48b97d91e392c888dc5fd23fb13f3ab",
    "semantic_title": "the globalwordnet formats: updates for 2020",
    "citation_count": 11,
    "authors": [
      "John P. McCrae",
      "Michael Wayne Goodman",
      "Francis Bond",
      "Alexandre Rademaker",
      "Ewa Rudnicka",
      "Luis Morgado Da Costa"
    ]
  },
  "https://aclanthology.org/2021.gwc-1.12": {
    "title": "Intrinsically Interlingual: The Wn Python Library for Wordnets",
    "volume": "workshop",
    "abstract": "This paper introduces Wn, a new Python library for working with wordnets. Unlike previous libraries, Wn is built from the beginning to accommodate multiple wordnets — for multiple languages or multiple versions of the same wordnet — while retaining the ability to query and traverse them independently. It is also able to download and incorporate wordnets published online. These features are made possible through Wn's adoption of standard formats and methods for interoperability, namely the WN-LMF schema (Vossen et al., 2013; Bond et al., 2020) and the Collaborative Interlingual Index (Bond et al., 2016). Wn is open-source, easily available, and well-documented",
    "checked": true,
    "id": "39f9e349fbb1514bc236bd6d57ce55ce87e40650",
    "semantic_title": "intrinsically interlingual: the wn python library for wordnets",
    "citation_count": 6,
    "authors": [
      "Michael Wayne Goodman",
      "Francis Bond"
    ]
  },
  "https://aclanthology.org/2021.gwc-1.13": {
    "title": "Semantic Analysis of Verb-Noun Derivation in Princeton WordNet",
    "volume": "workshop",
    "abstract": "We present here the results of a morphosemantic analysis of the verb-noun pairs in the Princeton WordNet as reflected in the standoff file containing pairs annotated with a set of 14 semantic relations. We have automatically distinguished between zero-derivation and affixal derivation in the data and identified the affixes and manually checked the results. The data show that for each semantic relation an affix prevails in creating new words, although we cannot talk about their specificity with respect to such a relation. Moreover, certain pairs of verb-noun semantic primes are better represented for each semantic relation, and some semantic clusters (in the form of WordNet subtrees) take shape as a result. We thus employ a large-scale data-driven linguistically motivated analysis afforded by the rich derivational and morphosemantic description in WordNet to the end of capturing finer regularities in the process of derivation as represented in the semantic properties of the words involved and as reflected in the structure of the lexicon",
    "checked": true,
    "id": "2f0af2fdfc1619f5c1163e18fc4ff270b3f742d2",
    "semantic_title": "semantic analysis of verb-noun derivation in princeton wordnet",
    "citation_count": 1,
    "authors": [
      "Verginica Mititelu",
      "Svetlozara Leseva",
      "Ivelina Stoyanova"
    ]
  },
  "https://aclanthology.org/2021.gwc-1.14": {
    "title": "Building the Turkish FrameNet",
    "volume": "workshop",
    "abstract": "FrameNet (Lowe, 1997; Baker et al., 1998; Fillmore and Atkins, 1998; Johnson et al., 2001) is a computational lexicography project that aims to offer insight into the semantic relationships between predicate and arguments. Having uses in many NLP applications, FrameNet has proven itself as a valuable resource. The main goal of this study is laying the foundation for building a comprehensive and cohesive Turkish FrameNet that is compatible with other resources like PropBank (Kara et al., 2020) or WordNet (Bakay et al., 2019; Ehsani, 2018; Ehsani et al., 2018; Parlar et al., 2019; Bakay et al., 2020) in the Turkish language",
    "checked": true,
    "id": "84c223d77441895384ea4687ad154507797b4789",
    "semantic_title": "building the turkish framenet",
    "citation_count": 7,
    "authors": [
      "Büşra Marşan",
      "Neslihan Kara",
      "Merve Özçelik",
      "Bilge Nas Arıcan",
      "Neslihan Cesur",
      "Aslı Kuzgun",
      "Ezgi Sanıyar",
      "Oğuzhan Kuyrukçu",
      "Olcay Taner Yildiz"
    ]
  },
  "https://aclanthology.org/2021.gwc-1.15": {
    "title": "Evaluation of Taxonomy Enrichment on Diachronic WordNet Versions",
    "volume": "workshop",
    "abstract": "The vast majority of the existing approaches for taxonomy enrichment apply word embeddings as they have proven to accumulate contexts (in a broad sense) extracted from texts which are sufficient for attaching orphan words to the taxonomy. On the other hand, apart from being large lexical and semantic resources, taxonomies are graph structures. Combining word embeddings with graph structure of taxonomy could be of use for predicting taxonomic relations. In this paper we compare several approaches for attaching new words to the existing taxonomy which are based on the graph representations with the one that relies on fastText embeddings. We test all methods on Russian and English datasets, but they could be also applied to other wordnets and languages",
    "checked": true,
    "id": "0999b4b20398d179b630b2f0955d606abea6ba13",
    "semantic_title": "evaluation of taxonomy enrichment on diachronic wordnet versions",
    "citation_count": 1,
    "authors": [
      "Irina Nikishina",
      "Natalia Loukachevitch",
      "Varvara Logacheva",
      "Alexander Panchenko"
    ]
  },
  "https://aclanthology.org/2021.gwc-1.16": {
    "title": "A (Non)-Perfect Match: Mapping plWordNet onto PrincetonWordNet",
    "volume": "workshop",
    "abstract": "The paper reports on the methodology and final results of a large-scale synset mapping between plWordNet and Princeton WordNet. Dedicated manual and semi-automatic mapping procedures as well as interlingual relation types for nouns, verbs, adjectives and adverbs are described. The statistics of all types of interlingual relations are also provided",
    "checked": true,
    "id": "fc8d1868f372c8907a333b5e6dcf43d2ebe5007c",
    "semantic_title": "a (non)-perfect match: mapping plwordnet onto princetonwordnet",
    "citation_count": 3,
    "authors": [
      "Ewa Rudnicka",
      "Wojciech Witkowski",
      "Maciej Piasecki"
    ]
  },
  "https://aclanthology.org/2021.gwc-1.17": {
    "title": "Persian SemCor: A Bag of Word Sense Annotated Corpus for the Persian Language",
    "volume": "workshop",
    "abstract": "Supervised approaches usually achieve the best performance in the Word Sense Disambiguation problem. However, the unavailability of large sense annotated corpora for many low-resource languages make these approaches inapplicable for them in practice. In this paper, we mitigate this issue for the Persian language by proposing a fully automatic approach for obtaining Persian SemCor (PerSemCor), as a Persian Bag-of-Word (BoW) sense-annotated corpus. We evaluated PerSemCor both intrinsically and extrinsically and showed that it can be effectively used as training sets for Persian supervised WSD systems. To encourage future research on Persian Word Sense Disambiguation, we release the PerSemCor in http://nlp.sbu.ac.ir",
    "checked": true,
    "id": "b9ef3f09ae10c8302d99f9f8ac5458caccef5827",
    "semantic_title": "persian semcor: a bag of word sense annotated corpus for the persian language",
    "citation_count": 2,
    "authors": [
      "Hossein Rouhizadeh",
      "Mehrnoush Shamsfard",
      "Mahdi Dehghan",
      "Masoud Rouhizadeh"
    ]
  },
  "https://aclanthology.org/2021.gwc-1.18": {
    "title": "HisNet: A Polarity Lexicon based on WordNet for Emotion Analysis",
    "volume": "workshop",
    "abstract": "Dictionary-based methods in sentiment analysis have received scholarly attention recently, the most comprehensive examples of which can be found in English. However, many other languages lack polarity dictionaries, or the existing ones are small in size as in the case of SentiTurkNet, the first and only polarity dictionary in Turkish. Thus, this study aims to extend the content of SentiTurkNet by comparing the two available WordNets in Turkish, namely KeNet and TR-wordnet of BalkaNet. To this end, a current Turkish polarity dictionary has been created relying on 76,825 synsets matching KeNet, where each synset has been annotated with three polarity labels, which are positive, negative and neutral. Meanwhile, the comparison of KeNet and TR-wordnet of BalkaNet has revealed their weaknesses such as the repetition of the same senses, lack of necessary merges of the items belonging to the same synset and the presence of redundant narrower versions of synsets, which are discussed in light of their potential to the improvement of the current lexical databases of Turkish",
    "checked": true,
    "id": "0bb1ba7ee36798eba5dc2a61f2e89ddb0cc9915a",
    "semantic_title": "hisnet: a polarity lexicon based on wordnet for emotion analysis",
    "citation_count": 3,
    "authors": [
      "Merve Özçelik",
      "Bilge Nas Arıcan",
      "Özge Bakay",
      "Elif Sarmış",
      "Özlem Ergelen",
      "Nilgün Güler Bayezit",
      "Olcay Taner Yıldız"
    ]
  },
  "https://aclanthology.org/2021.gwc-1.19": {
    "title": "Turkish WordNet KeNet",
    "volume": "workshop",
    "abstract": "Currently, there are two available wordnets for Turkish: TR-wordnet of BalkaNet and KeNet. As the more comprehensive wordnet for Turkish, KeNet includes 76,757 synsets. KeNet has both intralingual semantic relations and is linked to PWN through interlingual relations. In this paper, we present the procedure adopted in creating KeNet, give details about our approach in annotating semantic relations such as hypernymy and discuss the language-specific problems encountered in these processes",
    "checked": true,
    "id": "f04e28925b235cf0b98465d2f501fdcdea7d56a3",
    "semantic_title": "turkish wordnet kenet",
    "citation_count": 18,
    "authors": [
      "Özge Bakay",
      "Özlem Ergelen",
      "Elif Sarmış",
      "Selin Yıldırım",
      "Bilge Nas Arıcan",
      "Atilla Kocabalcıoğlu",
      "Merve Özçelik",
      "Ezgi Sanıyar",
      "Oğuzhan Kuyrukçu",
      "Begüm Avar",
      "Olcay Taner Yıldız"
    ]
  },
  "https://aclanthology.org/2021.gwc-1.20": {
    "title": "Enriching plWordNet with morphology",
    "volume": "workshop",
    "abstract": "In the paper, we present the process of adding morphological information to the Polish WordNet (plWordNet). We describe the reasons for this connection and the intuitions behind it. We also draw attention to the specificity of the Polish morphology. We show in which tasks the morphological information is important and how the methods can be developed by extending them to include combined morphological information based on WordNet",
    "checked": true,
    "id": "bfe3d68b49b24d4f617b9a7b5af8a7c84ac3f3e7",
    "semantic_title": "enriching plwordnet with morphology",
    "citation_count": 1,
    "authors": [
      "Agnieszka Dziob",
      "Wiktor Walentynowicz"
    ]
  },
  "https://aclanthology.org/2021.gwc-1.21": {
    "title": "Towards Expanding WordNet with Conceptual Frames",
    "volume": "workshop",
    "abstract": "The paper presents the project Semantic Network with a Wide Range of Semantic Relations and its main achievements. The ultimate objective of the project is to expand Princeton WordNet with conceptual frames that define the syntagmatic relations of verb synsets and the semantic classes of nouns felicitous to combine with particular verbs. At this stage of the work: a) over 5,000 WordNet verb synsets have been supplied with manually evaluated FrameNet semantic frames, b) 253 semantic types have been manually mapped to the appropriate WordNet concepts providing detailed ontological representation of the semantic classes of nouns",
    "checked": true,
    "id": "24b39c17f374755768232d9f639ebd9eb9745278",
    "semantic_title": "towards expanding wordnet with conceptual frames",
    "citation_count": 1,
    "authors": [
      "Koeva Svetla"
    ]
  },
  "https://aclanthology.org/2021.gwc-1.22": {
    "title": "OdeNet: Compiling a GermanWordNet from other Resources",
    "volume": "workshop",
    "abstract": "The Princeton WordNet for the English language has been used worldwide in NLP projects for many years. With the OMW initiative, wordnets for different languages of the world are being linked via identifiers. The parallel development and linking allows new multilingual application perspectives. The development of a wordnet for the German language is also in this context. To save development time, existing resources were combined and recompiled. The result was then evaluated and improved. In a relatively short time a resource was created that can be used in projects and continuously improved and extended",
    "checked": true,
    "id": "3e16c44bad1009ea44de08126f239938d2dba464",
    "semantic_title": "odenet: compiling a germanwordnet from other resources",
    "citation_count": 4,
    "authors": [
      "Melanie Siegel",
      "Francis Bond"
    ]
  },
  "https://aclanthology.org/2021.gwc-1.23": {
    "title": "Comparing Similarity of Words Based on Psychosemantic Experiment and RuWordNet",
    "volume": "workshop",
    "abstract": "In the paper we compare the structure of the Russian language thesaurus RuWordNet with the data of a psychosemantic experiment to identify semantically close words. The aim of the study is to find out to what extent the structure of RuWordNet corresponds to the intuitive ideas of native speakers about the semantic proximity of words. The respondents were asked to list synonyms to a given word. As a result of the experiment, we found that the respondents mainly mentioned not only synonyms but words that are in paradigmatic relations with the stimuli. The words of the mental sphere were chosen for the experiment. In 95% of cases, the words characterized in the experiment as semantically close were also close according to the thesaurus. In other cases, additions to the thesaurus were proposed",
    "checked": true,
    "id": "471b78a993fa0d0c1c0d1d27a6364699ad1ab366",
    "semantic_title": "comparing similarity of words based on psychosemantic experiment and ruwordnet",
    "citation_count": 0,
    "authors": [
      "Valery Solovyev",
      "Natalia Loukachevitch"
    ]
  },
  "https://aclanthology.org/2021.gwc-1.24": {
    "title": "Text Document Clustering: Wordnet vs. TF-IDF vs. Word Embeddings",
    "volume": "workshop",
    "abstract": "In the paper, we deal with the problem of unsupervised text document clustering for the Polish language. Our goal is to compare the modern approaches based on language modeling (doc2vec and BERT) with the classical ones, i.e., TF-IDF and wordnet-based. The experiments are conducted on three datasets containing qualification descriptions. The experiments' results showed that wordnet-based similarity measures could compete and even outperform modern embedding-based approaches",
    "checked": true,
    "id": "1e35596e4c54097e248cb414b194c5c773187a65",
    "semantic_title": "text document clustering: wordnet vs. tf-idf vs. word embeddings",
    "citation_count": 13,
    "authors": [
      "Michał Marcińczuk",
      "Mateusz Gniewkowski",
      "Tomasz Walkowiak",
      "Marcin Będkowski"
    ]
  },
  "https://aclanthology.org/2021.gwc-1.25": {
    "title": "Extracting Synonyms from Bilingual Dictionaries",
    "volume": "workshop",
    "abstract": "We present our progress in developing a novel algorithm to extract synonyms from bilingual dictionaries. Identification and usage of synonyms play a significant role in improving the performance of information access applications. The idea is to construct a translation graph from translation pairs, then to extract and consolidate cyclic paths to form bilingual sets of synonyms. The initial evaluation of this algorithm illustrates promising results in extracting Arabic-English bilingual synonyms. In the evaluation, we first converted the synsets in the Arabic WordNet into translation pairs (i.e., losing word-sense memberships). Next, we applied our algorithm to rebuild these synsets. We compared the original and extracted synsets obtaining an F-Measure of 82.3% and 82.1% for Arabic and English synsets extraction, respectively",
    "checked": true,
    "id": "ecb7faf54461d2ee2c805552ea88b9f9819ac9e0",
    "semantic_title": "extracting synonyms from bilingual dictionaries",
    "citation_count": 11,
    "authors": [
      "Mustafa Jarrar",
      "Eman Naser",
      "Muhammad Khalifa",
      "Khaled Shaalan"
    ]
  },
  "https://aclanthology.org/2021.gwc-1.26": {
    "title": "Neural Language Models vs Wordnet-based Semantically Enriched Representation in CST Relation Recognition",
    "volume": "workshop",
    "abstract": "Neural language models, including transformer-based models, that are pre-trained on very large corpora became a common way to represent text in various tasks, including recognition of textual semantic relations, e.g. Cross-document Structure Theory. Pre-trained models are usually fine tuned to downstream tasks and the obtained vectors are used as an input for deep neural classifiers. No linguistic knowledge obtained from resources and tools is utilised. In this paper we compare such universal approaches with a combination of rich graph-based linguistically motivated sentence representation and a typical neural network classifier applied to a task of recognition of CST relation in Polish. The representation describes selected levels of the sentence structure including description of lexical meanings on the basis of the wordnet (plWordNet) synsets and connected SUMO concepts. The obtained results show that in the case of difficult relations and medium size training corpus semantically enriched text representation leads to significantly better results",
    "checked": true,
    "id": "b5f52d7fd2eabb45c9d13008b2b1d91eebd8ea7d",
    "semantic_title": "neural language models vs wordnet-based semantically enriched representation in cst relation recognition",
    "citation_count": 1,
    "authors": [
      "Arkadiusz Janz",
      "Maciej Piasecki",
      "Piotr Wątorski"
    ]
  },
  "https://aclanthology.org/2021.gwc-1.27": {
    "title": "What is on Social Media that is not in WordNet? A Preliminary Analysis on the TwitterAAE Corpus",
    "volume": "workshop",
    "abstract": "Natural Language Processing tools and resources have been so far mainly created and trained for standard varieties of language. Nowadays, with the use of large amounts of data gathered from social media, other varieties and registers need to be processed, which may present other challenges and difficulties. In this work, we focus on English and we present a preliminary analysis by comparing the TwitterAAE corpus, which is annotated for ethnicity, and WordNet by quantifying and explaining the online language that WordNet misses",
    "checked": true,
    "id": "003af7b262e6e3da52839b31eae1e3927c653138",
    "semantic_title": "what is on social media that is not in wordnet? a preliminary analysis on the twitteraae corpus",
    "citation_count": 0,
    "authors": [
      "Cecilia Domingo",
      "Tatiana Gonzalez-Ferrero",
      "Itziar Gonzalez-Dios"
    ]
  },
  "https://aclanthology.org/2021.gwc-1.28": {
    "title": "Creating Domain Dependent Turkish WordNet and SentiNet",
    "volume": "workshop",
    "abstract": "A WordNet is a thesaurus that has a structured list of words organized depending on their meanings. WordNet represents word senses, all meanings a single lemma may have, the relations between these senses, and their definitions. Another study within the domain of Natural Language Processing is sentiment analysis. With sentiment analysis, data sets can be scored according to the emotion they contain. In the sentiment analysis we did with the data we received on the Tourism WordNet, we performed a domain-specific sentiment analysis study by annotating the data. In this paper, we propose a method to facilitate Natural Language Processing tasks such as sentiment analysis performed in specific domains via creating a specific-domain subset of an original Turkish dictionary. As the preliminary study, we have created a WordNet for the tourism domain with 14,000 words and validated it on simple tasks",
    "checked": true,
    "id": "654bad2ff157652d4f62f63c47f6eac91b431914",
    "semantic_title": "creating domain dependent turkish wordnet and sentinet",
    "citation_count": 3,
    "authors": [
      "Bilge Nas Arıcan",
      "Merve Özçelik",
      "Deniz Baran Aslan",
      "Elif Sarmış",
      "Selen Parlar",
      "Olcay Taner Yıldız"
    ]
  },
  "https://aclanthology.org/2021.gwc-1.29": {
    "title": "Towards a Linking between WordNet and Wikidata",
    "volume": "workshop",
    "abstract": "WordNet is the most widely used lexical resource for English, while Wikidata is one of the largest knowledge graphs of entity and concepts available. While, there is a clear difference in the focus of these two resources, there is also a significant overlap and as such a complete linking of these resources would have many uses. We propose the development of such a linking, first by means of the hapax legomenon links and secondly by the use of natural language processing techniques. We show that these can be done with high accuracy but that human validation is still necessary. This has resulted in over 9,000 links being added between these two resources",
    "checked": true,
    "id": "f71cffc672067684f28eb134a2a1407bea754fbf",
    "semantic_title": "towards a linking between wordnet and wikidata",
    "citation_count": 5,
    "authors": [
      "John P. McCrae",
      "David Cillessen"
    ]
  },
  "https://aclanthology.org/2021.gwc-1.30": {
    "title": "Toward the creation of WordNets for ancient Indo-European languages",
    "volume": "workshop",
    "abstract": "This paper presents the work in progress toward the creation of a family of WordNets for Sanskrit, Ancient Greek, and Latin. Building on previous attempts in the field, we elaborate these efforts bridging together WordNet relational semantics with theories of meaning from Cognitive Linguistics. We discuss some of the innovations we have introduced to the WordNet architecture, to better capture the polysemy of words, as well as Indo-European language family-specific features. We conclude the paper framing our work within the larger picture of resources available for ancient languages and showing that WordNet-backed search tools have the potential to re-define the kinds of questions that can be asked of ancient language corpora",
    "checked": true,
    "id": "bdadcbb558af9ad0d464d76bf87017454e4ef96b",
    "semantic_title": "toward the creation of wordnets for ancient indo-european languages",
    "citation_count": 10,
    "authors": [
      "Erica Biagetti",
      "Chiara Zanchi",
      "William Michael Short"
    ]
  },
  "https://aclanthology.org/2021.gwc-1.31": {
    "title": "DanNet2: Extending the coverage of adjectives in DanNet based on thesaurus data (project presentation)",
    "volume": "workshop",
    "abstract": "The paper describes work in progress in the DanNet2 project financed by the Carlsberg Foundation. The project aim is to extend the original Danish wordnet, DanNet, in several ways. Main focus is on extension of the coverage and description of the adjectives, a part of speech that was rather sparsely described in the original wordnet. We describe the methodology and initial work of semi-automatically transferring adjectives from the Danish Thesaurus to the wordnet with the aim of easily enlarging the coverage from 3,000 to approx. 13,000 adjectival synsets. Transfer is performed by manually encoding all missing adjectival subsection headwords from the thesaurus and thereafter employing a semi-automatic procedure where adjectives from the same subsection are transferred to the wordnet as either 1) near synonyms to the section's headword, 2) hyponyms to the section's headword, or 3) as members of the same synset as the headword. We also discuss how to deal with the problem of multiple representations of the same sense in the thesaurus, and present other types of information from the thesaurus that we plan to integrate, such as thematic and sentiment information",
    "checked": true,
    "id": "03c699b83a081842bc5144462fa39733a528f7f1",
    "semantic_title": "dannet2: extending the coverage of adjectives in dannet based on thesaurus data (project presentation)",
    "citation_count": 1,
    "authors": [
      "Sanni Nimb",
      "Bolette Pedersen",
      "Sussi Olsen"
    ]
  },
  "https://aclanthology.org/2021.gwc-1.32": {
    "title": "Teaching Through Tagging — Interactive Lexical Semantics",
    "volume": "workshop",
    "abstract": "In this paper we discuss an ongoing effort to enrich students' learning by involving them in sense tagging. The main goal is to lead students to discover how we can represent meaning and where the limits of our current theories lie. A subsidiary goal is to create sense tagged corpora and an accompanying linked lexicon (in our case wordnets). We present the results of tagging several texts and suggest some ways in which the tagging process could be improved. Two authors of this paper present their own experience as students. Overall, students reported that they found the tagging an enriching experience. The annotated corpora and changes to the wordnet are made available through the NTU multilingual corpus and associated wordnets (NTU-MC)",
    "checked": true,
    "id": "baa62ce792e3efeb81edc86b0b2cba592c8ff4fd",
    "semantic_title": "teaching through tagging — interactive lexical semantics",
    "citation_count": 4,
    "authors": [
      "Francis Bond",
      "Andrew Devadason",
      "Melissa Rui Lin Teo",
      "Luís Morgado da Costa"
    ]
  },
  "https://aclanthology.org/2021.gwc-1.33": {
    "title": "Towards the Addition of Pronunciation Information to Lexical Semantic Resources",
    "volume": "workshop",
    "abstract": "This paper describes ongoing work aiming at adding pronunciation information to lexical semantic resources, with a focus on open wordnets. Our goal is not only to add a new modality to those semantic networks, but also to mark heteronyms listed in them with the pronunciation information associated with their different meanings. This work could contribute in the longer term to the disambiguation of multi-modal resources, which are combining text and speech",
    "checked": true,
    "id": "903c15c06c74d32abd11dd3487b696ee8639199f",
    "semantic_title": "towards the addition of pronunciation information to lexical semantic resources",
    "citation_count": 3,
    "authors": [
      "Thierry Declerck",
      "Lenka Bajčetić"
    ]
  },
  "https://aclanthology.org/2021.gwc-1.34": {
    "title": "Testing agreement between lexicographers: A case of homonymy and polysemy",
    "volume": "workshop",
    "abstract": "In this paper we compare Oxford Lexico and Merriam Webster dictionaries with Princeton WordNet with respect to the description of semantic (dis)similarity between polysemous and homonymous senses that could be inferred from them. WordNet lacks any explicit description of polysemy or homonymy, but as a network of linked senses it may be used to compute semantic distances between word senses. To compare WordNet with the dictionaries, we transformed sample entry microstructures of the latter into graphs and cross-linked them with the equivalent senses of the former. We found that dictionaries are in high agreement with each other, if one considers polysemy and homonymy altogether, and in moderate concordance, if one focuses merely on polysemy descriptions. Measuring the shortest path lengths on WordNet gave results comparable to those on the dictionaries in predicting semantic dissimilarity between polysemous senses, but was less felicitous while recognising homonymy",
    "checked": true,
    "id": "e86db05442e5b150de9c1aee52444ee0a3ed4bc0",
    "semantic_title": "testing agreement between lexicographers: a case of homonymy and polysemy",
    "citation_count": 0,
    "authors": [
      "Marek Maziarz",
      "Francis Bond",
      "Ewa Rudnicka"
    ]
  },
  "https://aclanthology.org/2021.hackashop-1.1": {
    "title": "Adversarial Training for News Stance Detection: Leveraging Signals from a Multi-Genre Corpus",
    "volume": "workshop",
    "abstract": "Cross-target generalization constitutes an important issue for news Stance Detection (SD). In this short paper, we investigate adversarial cross-genre SD, where knowledge from annotated user-generated data is leveraged to improve news SD on targets unseen during training. We implement a BERT-based adversarial network and show experimental performance improvements over a set of strong baselines. Given the abundance of user-generated data, which are considerably less expensive to retrieve and annotate than news articles, this constitutes a promising research direction",
    "checked": true,
    "id": "4c436f1fb70611e3bab1c182b3789c9b02ec28d7",
    "semantic_title": "adversarial training for news stance detection: leveraging signals from a multi-genre corpus",
    "citation_count": 2,
    "authors": [
      "Costanza Conforti",
      "Jakob Berndt",
      "Marco Basaldella",
      "Mohammad Taher Pilehvar",
      "Chryssi Giannitsarou",
      "Flavio Toxvaerd",
      "Nigel Collier"
    ]
  },
  "https://aclanthology.org/2021.hackashop-1.2": {
    "title": "Related Named Entities Classification in the Economic-Financial Context",
    "volume": "workshop",
    "abstract": "The present work uses the Bidirectional Encoder Representations from Transformers (BERT) to process a sentence and its entities and indicate whether two named entities present in a sentence are related or not, constituting a binary classification problem. It was developed for the Portuguese language, considering the financial domain and exploring deep linguistic representations to identify a relation between entities without using other lexical-semantic resources. The results of the experiments show an accuracy of 86% of the predictions",
    "checked": true,
    "id": "374c5b449b7b2be289f4155bc276bb0228ad42cf",
    "semantic_title": "related named entities classification in the economic-financial context",
    "citation_count": 4,
    "authors": [
      "Daniel De Los Reyes",
      "Allan Barcelos",
      "Renata Vieira",
      "Isabel Manssour"
    ]
  },
  "https://aclanthology.org/2021.hackashop-1.3": {
    "title": "BERT meets Shapley: Extending SHAP Explanations to Transformer-based Classifiers",
    "volume": "workshop",
    "abstract": "Transformer-based neural networks offer very good classification performance across a wide range of domains, but do not provide explanations of their predictions. While several explanation methods, including SHAP, address the problem of interpreting deep learning models, they are not adapted to operate on state-of-the-art transformer-based neural networks such as BERT. Another shortcoming of these methods is that their visualization of explanations in the form of lists of most relevant words does not take into account the sequential and structurally dependent nature of text. This paper proposes the TransSHAP method that adapts SHAP to transformer models including BERT-based text classifiers. It advances SHAP visualizations by showing explanations in a sequential manner, assessed by human evaluators as competitive to state-of-the-art solutions",
    "checked": true,
    "id": "7823b7c83ce4b7d739fbd3436ba2c911dfec4d82",
    "semantic_title": "bert meets shapley: extending shap explanations to transformer-based classifiers",
    "citation_count": 29,
    "authors": [
      "Enja Kokalj",
      "Blaž Škrlj",
      "Nada Lavrač",
      "Senja Pollak",
      "Marko Robnik-Šikonja"
    ]
  },
  "https://aclanthology.org/2021.hackashop-1.4": {
    "title": "Extending Neural Keyword Extraction with TF-IDF tagset matching",
    "volume": "workshop",
    "abstract": "Keyword extraction is the task of identifying words (or multi-word expressions) that best describe a given document and serve in news portals to link articles of similar topics. In this work, we develop and evaluate our methods on four novel data sets covering less-represented, morphologically-rich languages in European news media industry (Croatian, Estonian, Latvian, and Russian). First, we perform evaluation of two supervised neural transformer-based methods, Transformer-based Neural Tagger for Keyword Identification (TNT-KID) and Bidirectional Encoder Representations from Transformers (BERT) with an additional Bidirectional Long Short-Term Memory Conditional Random Fields (BiLSTM CRF) classification head, and compare them to a baseline Term Frequency - Inverse Document Frequency (TF-IDF) based unsupervised approach. Next, we show that by combining the keywords retrieved by both neural transformer-based methods and extending the final set of keywords with an unsupervised TF-IDF based technique, we can drastically improve the recall of the system, making it appropriate for usage as a recommendation system in the media house environment",
    "checked": true,
    "id": "deec5c6a00e4e509e1eb4c31733a39e3274bf5c1",
    "semantic_title": "extending neural keyword extraction with tf-idf tagset matching",
    "citation_count": 8,
    "authors": [
      "Boshko Koloski",
      "Senja Pollak",
      "Blaž Škrlj",
      "Matej Martinc"
    ]
  },
  "https://aclanthology.org/2021.hackashop-1.5": {
    "title": "Zero-shot Cross-lingual Content Filtering: Offensive Language and Hate Speech Detection",
    "volume": "workshop",
    "abstract": "We present a system for zero-shot cross-lingual offensive language and hate speech classification. The system was trained on English datasets and tested on a task of detecting hate speech and offensive social media content in a number of languages without any additional training. Experiments show an impressive ability of both models to generalize from English to other languages. There is however an expected gap in performance between the tested cross-lingual models and the monolingual models. The best performing model (offensive content classifier) is available online as a REST API",
    "checked": true,
    "id": "355d3e4bf5430ba16f95496513b69d5f4753bcef",
    "semantic_title": "zero-shot cross-lingual content filtering: offensive language and hate speech detection",
    "citation_count": 16,
    "authors": [
      "Andraž Pelicon",
      "Ravi Shekhar",
      "Matej Martinc",
      "Blaž Škrlj",
      "Matthew Purver",
      "Senja Pollak"
    ]
  },
  "https://aclanthology.org/2021.hackashop-1.6": {
    "title": "Exploring Linguistically-Lightweight Keyword Extraction Techniques for Indexing News Articles in a Multilingual Set-up",
    "volume": "workshop",
    "abstract": "This paper presents a study of state-of-the-art unsupervised and linguistically unsophisticated keyword extraction algorithms, based on statistic-, graph-, and embedding-based approaches, including, i.a., Total Keyword Frequency, TF-IDF, RAKE, KPMiner, YAKE, KeyBERT, and variants of TextRank-based keyword extraction algorithms. The study was motivated by the need to select the most appropriate technique to extract keywords for indexing news articles in a real-world large-scale news analysis engine. The algorithms were evaluated on a corpus of circa 330 news articles in 7 languages. The overall best F1 scores for all languages on average were obtained using a combination of the recently introduced YAKE algorithm and KPMiner (20.1%, 46.6% and 47.2% for exact, partial and fuzzy matching resp.)",
    "checked": true,
    "id": "d6c98f0fd4d7f3b08ceeb04e0f9bc2f906f54e9d",
    "semantic_title": "exploring linguistically-lightweight keyword extraction techniques for indexing news articles in a multilingual set-up",
    "citation_count": 11,
    "authors": [
      "Jakub Piskorski",
      "Nicolas Stefanovitch",
      "Guillaume Jacquet",
      "Aldo Podavini"
    ]
  },
  "https://aclanthology.org/2021.hackashop-1.7": {
    "title": "No NLP Task Should be an Island: Multi-disciplinarity for Diversity in News Recommender Systems",
    "volume": "workshop",
    "abstract": "Natural Language Processing (NLP) is defined by specific, separate tasks, with each their own literature, benchmark datasets, and definitions. In this position paper, we argue that for a complex problem such as the threat to democracy by non-diverse news recommender systems, it is important to take into account a higher-order, normative goal and its implications. Experts in ethics, political science and media studies have suggested that news recommendation systems could be used to support a deliberative democracy. We reflect on the role of NLP in recommendation systems with this specific goal in mind and show that this theory of democracy helps to identify which NLP tasks and techniques can support this goal, and what work still needs to be done. This leads to recommendations for NLP researchers working on this specific problem as well as researchers working on other complex multidisciplinary problems",
    "checked": true,
    "id": "db7dd3d61d0c3fec258466248ddc3c3845dbf037",
    "semantic_title": "no nlp task should be an island: multi-disciplinarity for diversity in news recommender systems",
    "citation_count": 13,
    "authors": [
      "Myrthe Reuver",
      "Antske Fokkens",
      "Suzan Verberne"
    ]
  },
  "https://aclanthology.org/2021.hackashop-1.8": {
    "title": "TeMoTopic: Temporal Mosaic Visualisation of Topic Distribution, Keywords, and Context",
    "volume": "workshop",
    "abstract": "In this paper we present TeMoTopic, a visualization component for temporal exploration of topics in text corpora. TeMoTopic uses the temporal mosaic metaphor to present topics as a timeline of stacked bars along with related keywords for each topic. The visualization serves as an overview of the temporal distribution of topics, along with the keyword contents of the topics, which collectively support detail-on-demand interactions with the source text of the corpora. Through these interactions and the use of keyword highlighting, the content related to each topic and its change over time can be explored",
    "checked": true,
    "id": "068a14b048dfaf0a3f5a0d5a27b6df4368d5b11d",
    "semantic_title": "temotopic: temporal mosaic visualisation of topic distribution, keywords, and context",
    "citation_count": 2,
    "authors": [
      "Shane Sheehan",
      "Saturnino Luz",
      "Masood Masoodian"
    ]
  },
  "https://aclanthology.org/2021.hackashop-1.9": {
    "title": "Using contextual and cross-lingual word embeddings to improve variety in template-based NLG for automated journalism",
    "volume": "workshop",
    "abstract": "In this work, we describe our efforts in improving the variety of language generated from a rule-based NLG system for automated journalism. We present two approaches: one based on inserting completely new words into sentences generated from templates, and another based on replacing words with synonyms. Our initial results from a human evaluation conducted in English indicate that these approaches successfully improve the variety of the language without significantly modifying sentence meaning. We also present variations of the methods applicable to low-resource languages, simulated here using Finnish, where cross-lingual aligned embeddings are harnessed to make use of linguistic resources in a high-resource language. A human evaluation indicates that while proposed methods show potential in the low-resource case, additional work is needed to improve their performance",
    "checked": true,
    "id": "1ee737a77e80fe52cdd2756a0b2233582783fef0",
    "semantic_title": "using contextual and cross-lingual word embeddings to improve variety in template-based nlg for automated journalism",
    "citation_count": 4,
    "authors": [
      "Miia Rämö",
      "Leo Leppänen"
    ]
  },
  "https://aclanthology.org/2021.hackashop-1.10": {
    "title": "Aligning Estonian and Russian news industry keywords with the help of subtitle translations and an environmental thesaurus",
    "volume": "workshop",
    "abstract": "This paper presents the implementation of a bilingual term alignment approach developed by Repar et al. (2019) to a dataset of unaligned Estonian and Russian keywords which were manually assigned by journalists to describe the article topic. We started by separating the dataset into Estonian and Russian tags based on whether they are written in the Latin or Cyrillic script. Then we selected the available language-specific resources necessary for the alignment system to work. Despite the domains of the language-specific resources (subtitles and environment) not matching the domain of the dataset (news articles), we were able to achieve respectable results with manual evaluation indicating that almost 3/4 of the aligned keyword pairs are at least partial matches",
    "checked": true,
    "id": "95b75e9107892ad46b9e0b5f420055c3a45b0547",
    "semantic_title": "aligning estonian and russian news industry keywords with the help of subtitle translations and an environmental thesaurus",
    "citation_count": 0,
    "authors": [
      "Andraž Repar",
      "Andrej Shumakov"
    ]
  },
  "https://aclanthology.org/2021.hackashop-1.11": {
    "title": "Exploring Neural Language Models via Analysis of Local and Global Self-Attention Spaces",
    "volume": "workshop",
    "abstract": "Large pretrained language models using the transformer neural network architecture are becoming a dominant methodology for many natural language processing tasks, such as question answering, text classification, word sense disambiguation, text completion and machine translation. Commonly comprising hundreds of millions of parameters, these models offer state-of-the-art performance, but at the expense of interpretability. The attention mechanism is the main component of transformer networks. We present AttViz, a method for exploration of self-attention in transformer networks, which can help in explanation and debugging of the trained models by showing associations between text tokens in an input sequence. We show that existing deep learning pipelines can be explored with AttViz, which offers novel visualizations of the attention heads and their aggregations. We implemented the proposed methods in an online toolkit and an offline library. Using examples from news analysis, we demonstrate how AttViz can be used to inspect and potentially better understand what a model has learned",
    "checked": true,
    "id": "a9c07a60696a165aed09f0f772cc51ff52574a07",
    "semantic_title": "exploring neural language models via analysis of local and global self-attention spaces",
    "citation_count": 2,
    "authors": [
      "Blaž Škrlj",
      "Shane Sheehan",
      "Nika Eržen",
      "Marko Robnik-Šikonja",
      "Saturnino Luz",
      "Senja Pollak"
    ]
  },
  "https://aclanthology.org/2021.hackashop-1.12": {
    "title": "Comment Section Personalization: Algorithmic, Interface, and Interaction Design",
    "volume": "workshop",
    "abstract": "Comment sections allow users to share their personal experiences, discuss and form different opinions, and build communities out of organic conversations. However, many comment sections present chronological ranking to all users. In this paper, I discuss personalization approaches in comment sections based on different objectives for newsrooms and researchers to consider. I propose algorithmic and interface designs when personalizing the presentation of comments based on different objectives including relevance, diversity, and education/background information. I further explain how transparency, user control, and comment type diversity could help users most benefit from the personalized interacting experience",
    "checked": true,
    "id": "dd809736668809366587f3a6441e0ed0f73bf448",
    "semantic_title": "comment section personalization: algorithmic, interface, and interaction design",
    "citation_count": 0,
    "authors": [
      "Yixue Wang"
    ]
  },
  "https://aclanthology.org/2021.hackashop-1.13": {
    "title": "Unsupervised Approach to Multilingual User Comments Summarization",
    "volume": "workshop",
    "abstract": "User commenting is a valuable feature of many news outlets, enabling them a contact with readers and enabling readers to express their opinion, provide different viewpoints, and even complementary information. Yet, large volumes of user comments are hard to filter, let alone read and extract relevant information. The research on the summarization of user comments is still in its infancy, and human-created summarization datasets are scarce, especially for less-resourced languages. To address this issue, we propose an unsupervised approach to user comments summarization, which uses a modern multilingual representation of sentences together with standard extractive summarization techniques. Our comparison of different sentence representation approaches coupled with different summarization approaches shows that the most successful combinations are the same in news and comment summarization. The empirical results and presented visualisation show usefulness of the proposed methodology for several languages",
    "checked": true,
    "id": "f324c481fbb034ace039ef209338c0dac7ee3855",
    "semantic_title": "unsupervised approach to multilingual user comments summarization",
    "citation_count": 1,
    "authors": [
      "Aleš Žagar",
      "Marko Robnik-Šikonja"
    ]
  },
  "https://aclanthology.org/2021.hackashop-1.14": {
    "title": "EMBEDDIA Tools, Datasets and Challenges: Resources and Hackathon Contributions",
    "volume": "workshop",
    "abstract": "This paper presents tools and data sources collected and released by the EMBEDDIA project, supported by the European Union's Horizon 2020 research and innovation program. The collected resources were offered to participants of a hackathon organized as part of the EACL Hackashop on News Media Content Analysis and Automated Report Generation in February 2021. The hackathon had six participating teams who addressed different challenges, either from the list of proposed challenges or their own news-industry-related tasks. This paper goes beyond the scope of the hackathon, as it brings together in a coherent and compact form most of the resources developed, collected and released by the EMBEDDIA project. Moreover, it constitutes a handy source for news media industry and researchers in the fields of Natural Language Processing and Social Science",
    "checked": true,
    "id": "69254dbe0d17776456dde46cfae1f7cadc80fb74",
    "semantic_title": "embeddia tools, datasets and challenges: resources and hackathon contributions",
    "citation_count": 12,
    "authors": [
      "Senja Pollak",
      "Marko Robnik-Šikonja",
      "Matthew Purver",
      "Michele Boggia",
      "Ravi Shekhar",
      "Marko Pranjić",
      "Salla Salmela",
      "Ivar Krustok",
      "Tarmo Paju",
      "Carl-Gustav Linden",
      "Leo Leppänen",
      "Elaine Zosa",
      "Matej Ulčar",
      "Linda Freienthal",
      "Silver Traat",
      "Luis Adrián Cabrera-Diego",
      "Matej Martinc",
      "Nada Lavrač",
      "Blaž Škrlj",
      "Martin Žnidaršič",
      "Andraž Pelicon",
      "Boshko Koloski",
      "Vid Podpečan",
      "Janez Kranjc",
      "Shane Sheehan",
      "Emanuela Boros",
      "Jose G. Moreno",
      "Antoine Doucet",
      "Hannu Toivonen"
    ]
  },
  "https://aclanthology.org/2021.hackashop-1.15": {
    "title": "A COVID-19 news coverage mood map of Europe",
    "volume": "workshop",
    "abstract": "We present a COVID-19 news dashboard which visualizes sentiment in pandemic news coverage in different languages across Europe. The dashboard shows analyses for positive/neutral/negative sentiment and moral sentiment for news articles across countries and languages. First we extract news articles from news-crawl. Then we use a pre-trained multilingual BERT model for sentiment analysis of news article headlines and a dictionary and word vectors -based method for moral sentiment analysis of news articles. The resulting dashboard gives a unified overview of news events on COVID-19 news overall sentiment, and the region and language of publication from the period starting from the beginning of January 2020 to the end of January 2021",
    "checked": true,
    "id": "1495ab1971d3e41627e612cfce233b108b8c125c",
    "semantic_title": "a covid-19 news coverage mood map of europe",
    "citation_count": 5,
    "authors": [
      "Frankie Robertson",
      "Jarkko Lagus",
      "Kaisla Kajava"
    ]
  },
  "https://aclanthology.org/2021.hackashop-1.16": {
    "title": "Interesting cross-border news discovery using cross-lingual article linking and document similarity",
    "volume": "workshop",
    "abstract": "Team Name: team-8 Embeddia Tool: Cross-Lingual Document Retrieval Zosa et al. Dataset: Estonian and Latvian news datasets abstract: Contemporary news media face increasing amounts of available data that can be of use when prioritizing, selecting and discovering new news. In this work we propose a methodology for retrieving interesting articles in a cross-border news discovery setting. More specifically, we explore how a set of seed documents in Estonian can be projected in Latvian document space and serve as a basis for discovery of novel interesting pieces of Latvian news that would interest Estonian readers. The proposed methodology was evaluated by Estonian journalist who confirmed that in the best setting, from top 10 retrieved Latvian documents, half of them represent news that are potentially interesting to be taken by the Estonian media house and presented to Estonian readers",
    "checked": true,
    "id": "eb7a813ea1469caddef6f9d1bda98bbf8cc3f6a9",
    "semantic_title": "interesting cross-border news discovery using cross-lingual article linking and document similarity",
    "citation_count": 3,
    "authors": [
      "Boshko Koloski",
      "Elaine Zosa",
      "Timen Stepišnik-Perdih",
      "Blaž Škrlj",
      "Tarmo Paju",
      "Senja Pollak"
    ]
  },
  "https://aclanthology.org/2021.hackashop-1.17": {
    "title": "EMBEDDIA hackathon report: Automatic sentiment and viewpoint analysis of Slovenian news corpus on the topic of LGBTIQ+",
    "volume": "workshop",
    "abstract": "We conduct automatic sentiment and viewpoint analysis of the newly created Slovenian news corpus containing articles related to the topic of LGBTIQ+ by employing the state-of-the-art news sentiment classifier and a system for semantic change detection. The focus is on the differences in reporting between quality news media with long tradition and news media with financial and political connections to SDS, a Slovene right-wing political party. The results suggest that political affiliation of the media can affect the sentiment distribution of articles and the framing of specific LGBTIQ+ specific topics, such as same-sex marriage",
    "checked": true,
    "id": "55f0b8af39a833d28c5b35e27a8fa8a31e36b5b2",
    "semantic_title": "embeddia hackathon report: automatic sentiment and viewpoint analysis of slovenian news corpus on the topic of lgbtiq+",
    "citation_count": 2,
    "authors": [
      "Matej Martinc",
      "Nina Perger",
      "Andraž Pelicon",
      "Matej Ulčar",
      "Andreja Vezovnik",
      "Senja Pollak"
    ]
  },
  "https://aclanthology.org/2021.hackashop-1.18": {
    "title": "To Block or not to Block: Experiments with Machine Learning for News Comment Moderation",
    "volume": "workshop",
    "abstract": "Today, news media organizations regularly engage with readers by enabling them to comment on news articles. This creates the need for comment moderation and removal of disallowed comments – a time-consuming task often performed by human moderators. In this paper we approach the problem of automatic news comment moderation as classification of comments into blocked and not blocked categories. We construct a novel dataset of annotated English comments, experiment with cross-lingual transfer of comment labels and evaluate several machine learning models on datasets of Croatian and Estonian news comments. Team name: SuperAdmin; Challenge: Detection of blocked comments; Tools/models: CroSloEn BERT, FinEst BERT, 24Sata comment dataset, Ekspress comment dataset",
    "checked": true,
    "id": "67acbd270fe33d6b4651cc462cdb3be7d845c48e",
    "semantic_title": "to block or not to block: experiments with machine learning for news comment moderation",
    "citation_count": 7,
    "authors": [
      "Damir Korencic",
      "Ipek Baris",
      "Eugenia Fernandez",
      "Katarina Leuschel",
      "Eva Sánchez Salido"
    ]
  },
  "https://aclanthology.org/2021.hackashop-1.19": {
    "title": "Implementing Evaluation Metrics Based on Theories of Democracy in News Comment Recommendation (Hackathon Report)",
    "volume": "workshop",
    "abstract": "Diversity in news recommendation is important for democratic debate. Current recommendation strategies, as well as evaluation metrics for recommender systems, do not explicitly focus on this aspect of news recommendation. In the 2021 Embeddia Hackathon, we implemented one novel, normative theory-based evaluation metric, \"activation\", and use it to compare two recommendation strategies of New York Times comments, one based on user likes and another on editor picks. We found that both comment recommendation strategies lead to recommendations consistently less activating than the available comments in the pool of data, but the editor's picks more so. This might indicate that New York Times editors' support a deliberative democratic model, in which less activation is deemed ideal for democratic debate",
    "checked": true,
    "id": "2a738812857cc3e7629ca872ea5160fbd858fe8c",
    "semantic_title": "implementing evaluation metrics based on theories of democracy in news comment recommendation (hackathon report)",
    "citation_count": 2,
    "authors": [
      "Myrthe Reuver",
      "Nicolas Mattis"
    ]
  },
  "https://aclanthology.org/2021.hcinlp-1.1": {
    "title": "Towards Hybrid Human-Machine Workflow for Natural Language Generation",
    "volume": "workshop",
    "abstract": "In recent years, crowdsourcing has gained much attention from researchers to generate data for the Natural Language Generation (NLG) tools or to evaluate them. However, the quality of crowdsourced data has been questioned repeatedly because of the complexity of NLG tasks and crowd workers' unknown skills. Moreover, crowdsourcing can also be costly and often not feasible for large-scale data generation or evaluation. To overcome these challenges and leverage the complementary strengths of humans and machine tools, we propose a hybrid human-machine workflow designed explicitly for NLG tasks with real-time quality control mechanisms under budget constraints. This hybrid methodology is a powerful tool for achieving high-quality data while preserving efficiency. By combining human and machine intelligence, the proposed workflow decides dynamically on the next step based on the data from previous steps and given constraints. Our goal is to provide not only the theoretical foundations of the hybrid workflow but also to provide its implementation as open-source in future work",
    "checked": true,
    "id": "699b118c371733ddbae85a7f7a48849d8ab9f719",
    "semantic_title": "towards hybrid human-machine workflow for natural language generation",
    "citation_count": 0,
    "authors": [
      "Neslihan Iskender",
      "Tim Polzehl",
      "Sebastian Möller"
    ]
  },
  "https://aclanthology.org/2021.hcinlp-1.2": {
    "title": "Spellchecking for Children in Web Search: a Natural Language Interface Case-study",
    "volume": "workshop",
    "abstract": "Given the more widespread nature of natural language interfaces, it is increasingly important to understand who are accessing those interfaces, and how those interfaces are being used. In this paper, we explore spellchecking in the context of web search with children as the target audience. In particular, via a literature review we show that, while widely used, popular search tools are ill-designed for children. We then use spellcheckers as a case study to highlight the need for an interdisciplinary approach that brings together natural language processing, education, human-computer interaction to address a known information retrieval problem: query misspelling. We conclude that it is imperative that those for whom the interfaces are designed have a voice in the design process",
    "checked": true,
    "id": "7fc769774114994f0efbf0913bac60ca282cb35f",
    "semantic_title": "spellchecking for children in web search: a natural language interface case-study",
    "citation_count": 0,
    "authors": [
      "Casey Kennington",
      "Jerry Alan Fails",
      "Katherine Landau Wright",
      "Maria Soledad Pera"
    ]
  },
  "https://aclanthology.org/2021.hcinlp-1.3": {
    "title": "Capturing Covertly Toxic Speech via Crowdsourcing",
    "volume": "workshop",
    "abstract": "We study the task of labeling covert or veiled toxicity in online conversations. Prior research has highlighted the difficulty in creating language models that recognize nuanced toxicity such as microaggressions. Our investigations further underscore the difficulty in parsing such labels reliably from raters via crowdsourcing. We introduce an initial dataset, COVERTTOXICITY, which aims to identify and categorize such comments from a refined rater template. Finally, we fine-tune a comment-domain BERT model to classify covertly offensive comments and compare against existing baselines",
    "checked": true,
    "id": "69fa4bad8c6e1f43491267d784eebdf737ebec5d",
    "semantic_title": "capturing covertly toxic speech via crowdsourcing",
    "citation_count": 16,
    "authors": [
      "Alyssa Lees",
      "Daniel Borkan",
      "Ian Kivlichan",
      "Jorge Nario",
      "Tesh Goyal"
    ]
  },
  "https://aclanthology.org/2021.hcinlp-1.4": {
    "title": "Towards Human-Centered Summarization: A Case Study on Financial News",
    "volume": "workshop",
    "abstract": "Recent Deep Learning (DL) summarization models greatly outperform traditional summarization methodologies, generating high-quality summaries. Despite their success, there are still important open issues, such as the limited engagement and trust of users in the whole process. In order to overcome these issues, we reconsider the task of summarization from a human-centered perspective. We propose to integrate a user interface with an underlying DL model, instead of tackling summarization as an isolated task from the end user. We present a novel system, where the user can actively participate in the whole summarization process. We also enable the user to gather insights into the causative factors that drive the model's behavior, exploiting the self-attention mechanism. We focus on the financial domain, in order to demonstrate the efficiency of generic DL models for domain-specific applications. Our work takes a first step towards a model-interface co-design approach, where DL models evolve along user needs, paving the way towards human-computer text summarization interfaces",
    "checked": true,
    "id": "24adb1c8cfa0e81921007c9e8c519f11b0cfb058",
    "semantic_title": "towards human-centered summarization: a case study on financial news",
    "citation_count": 13,
    "authors": [
      "Tatiana Passali",
      "Alexios Gidiotis",
      "Efstathios Chatzikyriakidis",
      "Grigorios Tsoumakas"
    ]
  },
  "https://aclanthology.org/2021.hcinlp-1.5": {
    "title": "Methods for the Design and Evaluation of HCI+NLP Systems",
    "volume": "workshop",
    "abstract": "HCI and NLP traditionally focus on different evaluation methods. While HCI involves a small number of people directly and deeply, NLP traditionally relies on standardized benchmark evaluations that involve a larger number of people indirectly. We present five methodological proposals at the intersection of HCI and NLP and situate them in the context of ML-based NLP models. Our goal is to foster interdisciplinary collaboration and progress in both fields by emphasizing what the fields can learn from each other",
    "checked": true,
    "id": "e77ded36e28be70c8e57a8a5d03eb463718d56f9",
    "semantic_title": "methods for the design and evaluation of hci+nlp systems",
    "citation_count": 11,
    "authors": [
      "Hendrik Heuer",
      "Daniel Buschek"
    ]
  },
  "https://aclanthology.org/2021.hcinlp-1.6": {
    "title": "Context-sensitive evaluation of automatic speech recognition: considering user experience & language variation",
    "volume": "workshop",
    "abstract": "Commercial Automatic Speech Recognition (ASR) systems tend to show systemic predictive bias for marginalised speaker/user groups. We highlight the need for an interdisciplinary and context-sensitive approach to documenting this bias incorporating perspectives and methods from sociolinguistics, speech & language technology and human-computer interaction in the context of a case study. We argue evaluation of ASR systems should be disaggregated by speaker group, include qualitative error analysis, and consider user experience in a broader sociolinguistic and social context",
    "checked": true,
    "id": "8ede171076f108fb8cd6b7143ee57c16ad4893a7",
    "semantic_title": "context-sensitive evaluation of automatic speech recognition: considering user experience & language variation",
    "citation_count": 10,
    "authors": [
      "Nina Markl",
      "Catherine Lai"
    ]
  },
  "https://aclanthology.org/2021.hcinlp-1.7": {
    "title": "Embodied Multimodal Agents to Bridge the Understanding Gap",
    "volume": "workshop",
    "abstract": "In this paper we argue that embodied multimodal agents, i.e., avatars, can play an important role in moving natural language processing toward \"deep understanding.\" Fully-featured interactive agents, model encounters between two \"people,\" but a language-only agent has little environmental and situational awareness. Multimodal agents bring new opportunities for interpreting visuals, locational information, gestures, etc., which are more axes along which to communicate. We propose that multimodal agents, by facilitating an embodied form of human-computer interaction, provide additional structure that can be used to train models that move NLP systems closer to genuine \"understanding\" of grounded language, and we discuss ongoing studies using existing systems",
    "checked": true,
    "id": "4c61a99c19b8f5503e0dc8627857c64f056f5732",
    "semantic_title": "embodied multimodal agents to bridge the understanding gap",
    "citation_count": 4,
    "authors": [
      "Nikhil Krishnaswamy",
      "Nada Alalyani"
    ]
  },
  "https://aclanthology.org/2021.hcinlp-1.8": {
    "title": "Putting Humans in the Natural Language Processing Loop: A Survey",
    "volume": "workshop",
    "abstract": "How can we design Natural Language Processing (NLP) systems that learn from human feedback? There is a growing research body of Human-in-the-loop (HITL) NLP frameworks that continuously integrate human feedback to improve the model itself. HITL NLP research is nascent but multifarious—solving various NLP problems, collecting diverse feedback from different people, and applying different methods to learn from human feedback. We present a survey of HITL NLP work from both Machine Learning (ML) and Human-computer Interaction (HCI) communities that highlights its short yet inspiring history, and thoroughly summarize recent frameworks focusing on their tasks, goals, human interactions, and feedback learning methods. Finally, we discuss future studies for integrating human feedback in the NLP development loop",
    "checked": true,
    "id": "c4788d6d19c9c6555264f274d01fd0c34c22c674",
    "semantic_title": "putting humans in the natural language processing loop: a survey",
    "citation_count": 50,
    "authors": [
      "Zijie J. Wang",
      "Dongjin Choi",
      "Shenyu Xu",
      "Diyi Yang"
    ]
  },
  "https://aclanthology.org/2021.hcinlp-1.9": {
    "title": "Can You Distinguish Truthful from Fake Reviews? User Analysis and Assistance Tool for Fake Review Detection",
    "volume": "workshop",
    "abstract": "Customer reviews are useful in providing an indirect, secondhand experience of a product. People often use reviews written by other customers as a guideline prior to purchasing a product. Such behavior signifies the authenticity of reviews in e-commerce platforms. However, fake reviews are increasingly becoming a hassle for both consumers and product owners. To address this issue, we propose You Only Need Gold (YONG), an essential information mining tool for detecting fake reviews and augmenting user discretion. Our experimental results show the poor human performance on fake review detection, substantially improved user capability given our tool, and the ultimate need for user reliance on the tool",
    "checked": true,
    "id": "416eb711345b6dba82922b6e58410b9b8038d642",
    "semantic_title": "can you distinguish truthful from fake reviews? user analysis and assistance tool for fake review detection",
    "citation_count": 6,
    "authors": [
      "Jeonghwan Kim",
      "Junmo Kang",
      "Suwon Shin",
      "Sung-Hyon Myaeng"
    ]
  },
  "https://aclanthology.org/2021.hcinlp-1.10": {
    "title": "Challenges in Designing Games with a Purpose for Abusive Language Annotation",
    "volume": "workshop",
    "abstract": "In this paper we discuss several challenges related to the development of a 3D game, whose goal is to raise awareness on cyberbullying while collecting linguistic annotation on offensive language. The game is meant to be used by teenagers, thus raising a number of issues that need to be tackled during development. For example, the game aesthetics should be appealing for players belonging to this age group, but at the same time all possible solutions should be implemented to meet privacy requirements. Also, the task of linguistic annotation should be possibly hidden, adopting so-called orthogonal game mechanics, without affecting the quality of collected data. While some of these challenges are being tackled in the game development, some others are discussed in this paper but still lack an ultimate solution",
    "checked": true,
    "id": "175bf0e1937c9fee2cced703996f1f38cbe4401c",
    "semantic_title": "challenges in designing games with a purpose for abusive language annotation",
    "citation_count": 4,
    "authors": [
      "Federico Bonetti",
      "Sara Tonelli"
    ]
  },
  "https://aclanthology.org/2021.hcinlp-1.11": {
    "title": "Challenges in Designing Natural Language Interfaces for Complex Visual Models",
    "volume": "workshop",
    "abstract": "Intuitive interaction with visual models becomes an increasingly important task in the field of Visualization (VIS) and verbal interaction represents a significant aspect of it. Vice versa, modeling verbal interaction in visual environments is a major trend in ongoing research in NLP. To date, research on Language & Vision, however, mostly happens at the intersection of NLP and Computer Vision (CV), and much less at the intersection of NLP and Visualization, which is an important area in Human-Computer Interaction (HCI). This paper presents a brief survey of recent work on interactive tasks and set-ups in NLP and Visualization. We discuss the respective methods, show interesting gaps, and conclude by suggesting neural, visually grounded dialogue modeling as a promising potential for NLIs for visual models",
    "checked": true,
    "id": "154a1b2ecd12afc44ed64e859bc746f88d699731",
    "semantic_title": "challenges in designing natural language interfaces for complex visual models",
    "citation_count": 7,
    "authors": [
      "Henrik Voigt",
      "Monique Meuschke",
      "Kai Lawonn",
      "Sina Zarrieß"
    ]
  },
  "https://aclanthology.org/2021.hcinlp-1.12": {
    "title": "AfriKI: Machine-in-the-Loop Afrikaans Poetry Generation",
    "volume": "workshop",
    "abstract": "This paper proposes a generative language model called AfriKI. Our approach is based on an LSTM architecture trained on a small corpus of contemporary fiction. With the aim of promoting human creativity, we use the model as an authoring tool to explore machine-in-the-loop Afrikaans poetry generation. To our knowledge, this is the first study to attempt creative text generation in Afrikaans",
    "checked": true,
    "id": "e9f72084857857c726f57b3b7811946aa8d6287e",
    "semantic_title": "afriki: machine-in-the-loop afrikaans poetry generation",
    "citation_count": 1,
    "authors": [
      "Imke van Heerden",
      "Anil Bas"
    ]
  },
  "https://aclanthology.org/2021.hcinlp-1.13": {
    "title": "Situation-Specific Multimodal Feature Adaptation",
    "volume": "workshop",
    "abstract": "In the next decade, we will see a considerable need for NLP models for situated settings where diversity of situations and also different modalities including eye-movements should be taken into account in order to grasp the intention of the user. However, language comprehension in situated settings can not be handled in isolation, where different multimodal cues are inherently present and essential parts of the situations. In this research proposal, we aim to quantify the influence of each modality in interaction with various referential complexities. We propose to encode the referential complexity of the situated settings in the embeddings during pre-training to implicitly guide the model to the most plausible situation-specific deviations. We summarize the challenges of intention extraction and propose a methodological approach to investigate a situation-specific feature adaptation to improve crossmodal mapping and meaning recovery from noisy communication settings",
    "checked": true,
    "id": "944894f379789295074aeb2e1d90d6640d5fcaeb",
    "semantic_title": "situation-specific multimodal feature adaptation",
    "citation_count": 0,
    "authors": [
      "Özge Alacam"
    ]
  },
  "https://aclanthology.org/2021.hcinlp-1.14": {
    "title": "Machine Translation Believability",
    "volume": "workshop",
    "abstract": "Successful Machine Translation (MT) deployment requires understanding not only the intrinsic qualities of MT output, such as fluency and adequacy, but also user perceptions. Users who do not understand the source language respond to MT output based on their perception of the likelihood that the meaning of the MT output matches the meaning of the source text. We refer to this as believability. Output that is not believable may be off-putting to users, but believable MT output with incorrect meaning may mislead them. In this work, we study the relationship of believability to fluency and adequacy by applying traditional MT direct assessment protocols to annotate all three features on the output of neural MT systems. Quantitative analysis of these annotations shows that believability is closely related to but distinct from fluency, and initial qualitative analysis suggests that semantic features may account for the difference",
    "checked": true,
    "id": "c2f9b231f32a54f0c3e8d0f4888fd024a4e0d91d",
    "semantic_title": "machine translation believability",
    "citation_count": 2,
    "authors": [
      "Marianna Martindale",
      "Kevin Duh",
      "Marine Carpuat"
    ]
  },
  "https://aclanthology.org/2021.hcinlp-1.15": {
    "title": "Provocation: Contestability in Large-Scale Interactive NLP Systems",
    "volume": "workshop",
    "abstract": "",
    "checked": true,
    "id": "1041448ea21bfedac3cfcca70c2ddbed542170bf",
    "semantic_title": "provocation: contestability in large-scale interactive nlp systems",
    "citation_count": 3,
    "authors": [
      "Tanushree Mitra"
    ]
  },
  "https://aclanthology.org/2021.hcinlp-1.16": {
    "title": "An IDR Framework of Opportunities and Barriers between HCI and NLP",
    "volume": "workshop",
    "abstract": "This paper presents a framework of opportunities and barriers/risks between the two research fields Natural Language Processing (NLP) and Human-Computer Interaction (HCI). The framework is constructed by following an interdisciplinary research-model (IDR), combining field-specific knowledge with existing work in the two fields. The resulting framework is intended as a departure point for discussion and inspiration for research collaborations",
    "checked": true,
    "id": "61decd28ec0b2d8a31d68a701ec947c317207564",
    "semantic_title": "an idr framework of opportunities and barriers between hci and nlp",
    "citation_count": 4,
    "authors": [
      "Nanna Inie",
      "Leon Derczynski"
    ]
  },
  "https://aclanthology.org/2021.hcinlp-1.17": {
    "title": "RE-AIMing Predictive Text",
    "volume": "workshop",
    "abstract": "Our increasing reliance on mobile applications means much of our communication is mediated with the support of predictive text systems. How do these systems impact interpersonal communication and broader society? In what ways are predictive text systems harmful, to whom, and why? In this paper, we focus on predictive text systems on mobile devices and attempt to answer these questions. We introduce the concept of a ‘text entry intervention' as a way to evaluate predictive text systems through an interventional lens, and consider the Reach, Effectiveness, Adoption, Implementation, and Maintenance (RE-AIM) of predictive text systems. We finish with a discussion of opportunities for NLP",
    "checked": true,
    "id": "bb6fc10dac56acae9ddeaac84f5e2194d46bf1d0",
    "semantic_title": "re-aiming predictive text",
    "citation_count": 0,
    "authors": [
      "Matthew Higgs",
      "Claire McCallum",
      "Selina Sutton",
      "Mark Warner"
    ]
  },
  "https://aclanthology.org/2021.hcinlp-1.18": {
    "title": "How do people interact with biased text prediction models while writing?",
    "volume": "workshop",
    "abstract": "Recent studies have shown that a bias in thetext suggestions system can percolate in theuser's writing. In this pilot study, we ask thequestion: How do people interact with text pre-diction models, in an inline next phrase sugges-tion interface and how does introducing senti-ment bias in the text prediction model affecttheir writing? We present a pilot study as afirst step to answer this question",
    "checked": true,
    "id": "6a89148ec6f14d7e89d791febabc88537876ce5b",
    "semantic_title": "how do people interact with biased text prediction models while writing?",
    "citation_count": 12,
    "authors": [
      "Advait Bhat",
      "Saaket Agashe",
      "Anirudha Joshi"
    ]
  },
  "https://aclanthology.org/2021.humeval-1.1": {
    "title": "It's Commonsense, isn't it? Demystifying Human Evaluations in Commonsense-Enhanced NLG Systems",
    "volume": "workshop",
    "abstract": "Common sense is an integral part of human cognition which allows us to make sound decisions, communicate effectively with others and interpret situations and utterances. Endowing AI systems with commonsense knowledge capabilities will help us get closer to creating systems that exhibit human intelligence. Recent efforts in Natural Language Generation (NLG) have focused on incorporating commonsense knowledge through large-scale pre-trained language models or by incorporating external knowledge bases. Such systems exhibit reasoning capabilities without common sense being explicitly encoded in the training set. These systems require careful evaluation, as they incorporate additional resources during training which adds additional sources of errors. Additionally, human evaluation of such systems can have significant variation, making it impossible to compare different systems and define baselines. This paper aims to demystify human evaluations of commonsense-enhanced NLG systems by proposing the Commonsense Evaluation Card (CEC), a set of recommendations for evaluation reporting of commonsense-enhanced NLG systems, underpinned by an extensive analysis of human evaluations reported in the recent literature",
    "checked": true,
    "id": "7ca12d8f71774c273fb467d6d779dce973930dd7",
    "semantic_title": "it's commonsense, isn't it? demystifying human evaluations in commonsense-enhanced nlg systems",
    "citation_count": 4,
    "authors": [
      "Miruna-Adriana Clinciu",
      "Dimitra Gkatzia",
      "Saad Mahamood"
    ]
  },
  "https://aclanthology.org/2021.humeval-1.2": {
    "title": "Estimating Subjective Crowd-Evaluations as an Additional Objective to Improve Natural Language Generation",
    "volume": "workshop",
    "abstract": "Human ratings are one of the most prevalent methods to evaluate the performance of NLP (natural language processing) algorithms. Similarly, it is common to measure the quality of sentences generated by a natural language generation model using human raters. In this paper we argue for exploring the use of subjective evaluations within the process of training language generation models in a multi-task learning setting. As a case study, we use a crowd-authored dialogue corpus to fine-tune six different language generation models. Two of these models incorporate multi-task learning and use subjective ratings of lines as part of an explicit learning goal. A human evaluation of the generated dialogue lines reveals that utterances generated by the multi-tasking models were subjectively rated as the most typical, most moving the conversation forward, and least offensive. Based on these promising first results, we discuss future research directions for incorporating subjective human evaluations into language model training and to hence keep the human user in the loop during the development process",
    "checked": true,
    "id": "5eba8724559f97b824e832451e4f832e319ce2cf",
    "semantic_title": "estimating subjective crowd-evaluations as an additional objective to improve natural language generation",
    "citation_count": 0,
    "authors": [
      "Jakob Nyberg",
      "Maike Paetzel",
      "Ramesh Manuvinakurike"
    ]
  },
  "https://aclanthology.org/2021.humeval-1.3": {
    "title": "Trading Off Diversity and Quality in Natural Language Generation",
    "volume": "workshop",
    "abstract": "For open-ended language generation tasks such as storytelling or dialogue, choosing the right decoding algorithm is vital for controlling the tradeoff between generation quality and diversity. However, there presently exists no consensus on which decoding procedure is best or even the criteria by which to compare them. In this paper, we cast decoding as a tradeoff between response quality and diversity, and we perform the first large-scale evaluation of decoding methods along the entire quality-diversity spectrum. Our experiments confirm the existence of the likelihood trap: the counter-intuitive observation that high likelihood sequences are often surprisingly low quality. We also find that when diversity is a priority, all methods perform similarly, but when quality is viewed as more important, nucleus sampling (Holtzman et al., 2019) outperforms all other evaluated decoding algorithms",
    "checked": true,
    "id": "3ed06aca3b25a9af89f08b949753372d29647a10",
    "semantic_title": "trading off diversity and quality in natural language generation",
    "citation_count": 64,
    "authors": [
      "Hugh Zhang",
      "Daniel Duckworth",
      "Daphne Ippolito",
      "Arvind Neelakantan"
    ]
  },
  "https://aclanthology.org/2021.humeval-1.4": {
    "title": "Towards Document-Level Human MT Evaluation: On the Issues of Annotator Agreement, Effort and Misevaluation",
    "volume": "workshop",
    "abstract": "Document-level human evaluation of machine translation (MT) has been raising interest in the community. However, little is known about the issues of using document-level methodologies to assess MT quality. In this article, we compare the inter-annotator agreement (IAA) scores, the effort to assess the quality in different document-level methodologies, and the issue of misevaluation when sentences are evaluated out of context",
    "checked": true,
    "id": "6a7d44ab22c2eea0a977fd425308bb52eca8e36a",
    "semantic_title": "towards document-level human mt evaluation: on the issues of annotator agreement, effort and misevaluation",
    "citation_count": 12,
    "authors": [
      "Sheila Castilho"
    ]
  },
  "https://aclanthology.org/2021.humeval-1.5": {
    "title": "Is This Translation Error Critical?: Classification-Based Human and Automatic Machine Translation Evaluation Focusing on Critical Errors",
    "volume": "workshop",
    "abstract": "This paper discusses a classification-based approach to machine translation evaluation, as opposed to a common regression-based approach in the WMT Metrics task. Recent machine translation usually works well but sometimes makes critical errors due to just a few wrong word choices. Our classification-based approach focuses on such errors using several error type labels, for practical machine translation evaluation in an age of neural machine translation. We made additional annotations on the WMT 2015-2017 Metrics datasets with fluency and adequacy labels to distinguish different types of translation errors from syntactic and semantic viewpoints. We present our human evaluation criteria for the corpus development and automatic evaluation experiments using the corpus. The human evaluation corpus will be publicly available upon publication",
    "checked": true,
    "id": "dbf9d6766d25dfb89470d6ff9a6b1c3f8184dc54",
    "semantic_title": "is this translation error critical?: classification-based human and automatic machine translation evaluation focusing on critical errors",
    "citation_count": 11,
    "authors": [
      "Katsuhito Sudoh",
      "Kosuke Takahashi",
      "Satoshi Nakamura"
    ]
  },
  "https://aclanthology.org/2021.humeval-1.6": {
    "title": "Towards Objectively Evaluating the Quality of Generated Medical Summaries",
    "volume": "workshop",
    "abstract": "We propose a method for evaluating the quality of generated text by asking evaluators to count facts, and computing precision, recall, f-score, and accuracy from the raw counts. We believe this approach leads to a more objective and easier to reproduce evaluation. We apply this to the task of medical report summarisation, where measuring objective quality and accuracy is of paramount importance",
    "checked": true,
    "id": "34e29eaecab43fce7fd0de935fd40609112c47c2",
    "semantic_title": "towards objectively evaluating the quality of generated medical summaries",
    "citation_count": 9,
    "authors": [
      "Francesco Moramarco",
      "Damir Juric",
      "Aleksandar Savkov",
      "Ehud Reiter"
    ]
  },
  "https://aclanthology.org/2021.humeval-1.7": {
    "title": "A Preliminary Study on Evaluating Consultation Notes With Post-Editing",
    "volume": "workshop",
    "abstract": "Automatic summarisation has the potential to aid physicians in streamlining clerical tasks such as note taking. But it is notoriously difficult to evaluate these systems and demonstrate that they are safe to be used in a clinical setting. To circumvent this issue, we propose a semi-automatic approach whereby physicians post-edit generated notes before submitting them. We conduct a preliminary study on the time saving of automatically generated consultation notes with post-editing. Our evaluators are asked to listen to mock consultations and to post-edit three generated notes. We time this and find that it is faster than writing the note from scratch. We present insights and lessons learnt from this experiment",
    "checked": true,
    "id": "11a0f44618c9f0a7d6793e73f40777c516049453",
    "semantic_title": "a preliminary study on evaluating consultation notes with post-editing",
    "citation_count": 8,
    "authors": [
      "Francesco Moramarco",
      "Alex Papadopoulos Korfiatis",
      "Aleksandar Savkov",
      "Ehud Reiter"
    ]
  },
  "https://aclanthology.org/2021.humeval-1.8": {
    "title": "The Great Misalignment Problem in Human Evaluation of NLP Methods",
    "volume": "workshop",
    "abstract": "We outline the Great Misalignment Problem in natural language processing research, this means simply that the problem definition is not in line with the method proposed and the human evaluation is not in line with the definition nor the method. We study this misalignment problem by surveying 10 randomly sampled papers published in ACL 2020 that report results with human evaluation. Our results show that only one paper was fully in line in terms of problem definition, method and evaluation. Only two papers presented a human evaluation that was in line with what was modeled in the method. These results highlight that the Great Misalignment Problem is a major one and it affects the validity and reproducibility of results obtained by a human evaluation",
    "checked": true,
    "id": "2157ae8828bf8c60e954931e68e1a2f97f9d30cc",
    "semantic_title": "the great misalignment problem in human evaluation of nlp methods",
    "citation_count": 10,
    "authors": [
      "Mika Hämäläinen",
      "Khalid Alnajjar"
    ]
  },
  "https://aclanthology.org/2021.humeval-1.9": {
    "title": "A View From the Crowd: Evaluation Challenges for Time-Offset Interaction Applications",
    "volume": "workshop",
    "abstract": "Dialogue systems like chatbots, and tasks like question-answering (QA) have gained traction in recent years; yet evaluating such systems remains difficult. Reasons include the great variety in contexts and use cases for these systems as well as the high cost of human evaluation. In this paper, we focus on a specific type of dialogue systems: Time-Offset Interaction Applications (TOIAs) are intelligent, conversational software that simulates face-to-face conversations between humans and pre-recorded human avatars. Under the constraint that a TOIA is a single output system interacting with users with different expectations, we identify two challenges: first, how do we define a ‘good' answer? and second, what's an appropriate metric to use? We explore both challenges through the creation of a novel dataset that identifies multiple good answers to specific TOIA questions through the help of Amazon Mechanical Turk workers. This ‘view from the crowd' allows us to study the variations of how TOIA interrogators perceive its answers. Our contributions include the annotated dataset that we make publicly available and the proposal of Success Rate @k as an evaluation metric that is more appropriate than the traditional QA's and information retrieval's metrics",
    "checked": true,
    "id": "f4294bf61c1af85293136c71c80bb01dab87f932",
    "semantic_title": "a view from the crowd: evaluation challenges for time-offset interaction applications",
    "citation_count": 1,
    "authors": [
      "Alberto Chierici",
      "Nizar Habash"
    ]
  },
  "https://aclanthology.org/2021.humeval-1.10": {
    "title": "Reliability of Human Evaluation for Text Summarization: Lessons Learned and Challenges Ahead",
    "volume": "workshop",
    "abstract": "Only a small portion of research papers with human evaluation for text summarization provide information about the participant demographics, task design, and experiment protocol. Additionally, many researchers use human evaluation as gold standard without questioning the reliability or investigating the factors that might affect the reliability of the human evaluation. As a result, there is a lack of best practices for reliable human summarization evaluation grounded by empirical evidence. To investigate human evaluation reliability, we conduct a series of human evaluation experiments, provide an overview of participant demographics, task design, experimental set-up and compare the results from different experiments. Based on our empirical analysis, we provide guidelines to ensure the reliability of expert and non-expert evaluations, and we determine the factors that might affect the reliability of the human evaluation",
    "checked": true,
    "id": "968e6596b763cbd48a49af8c83957978296f6dca",
    "semantic_title": "reliability of human evaluation for text summarization: lessons learned and challenges ahead",
    "citation_count": 20,
    "authors": [
      "Neslihan Iskender",
      "Tim Polzehl",
      "Sebastian Möller"
    ]
  },
  "https://aclanthology.org/2021.humeval-1.11": {
    "title": "On User Interfaces for Large-Scale Document-Level Human Evaluation of Machine Translation Outputs",
    "volume": "workshop",
    "abstract": "Recent studies emphasize the need of document context in human evaluation of machine translations, but little research has been done on the impact of user interfaces on annotator productivity and the reliability of assessments. In this work, we compare human assessment data from the last two WMT evaluation campaigns collected via two different methods for document-level evaluation. Our analysis shows that a document-centric approach to evaluation where the annotator is presented with the entire document context on a screen leads to higher quality segment and document level assessments. It improves the correlation between segment and document scores and increases inter-annotator agreement for document scores but is considerably more time consuming for annotators",
    "checked": true,
    "id": "6fb4c2d13812006cc368a0cb0e275037fb146b72",
    "semantic_title": "on user interfaces for large-scale document-level human evaluation of machine translation outputs",
    "citation_count": 2,
    "authors": [
      "Roman Grundkiewicz",
      "Marcin Junczys-Dowmunt",
      "Christian Federmann",
      "Tom Kocmi"
    ]
  },
  "https://aclanthology.org/2021.humeval-1.12": {
    "title": "Eliciting Explicit Knowledge From Domain Experts in Direct Intrinsic Evaluation of Word Embeddings for Specialized Domains",
    "volume": "workshop",
    "abstract": "We evaluate the use of direct intrinsic word embedding evaluation tasks for specialized language. Our case study is philosophical text: human expert judgements on the relatedness of philosophical terms are elicited using a synonym detection task and a coherence task. Uniquely for our task, experts must rely on explicit knowledge and cannot use their linguistic intuition, which may differ from that of the philosopher. We find that inter-rater agreement rates are similar to those of more conventional semantic annotation tasks, suggesting that these tasks can be used to evaluate word embeddings of text types for which implicit knowledge may not suffice",
    "checked": true,
    "id": "76b95c328d0e78ba716a90e0498f4d094cef93d4",
    "semantic_title": "eliciting explicit knowledge from domain experts in direct intrinsic evaluation of word embeddings for specialized domains",
    "citation_count": 1,
    "authors": [
      "Goya van Boven",
      "Jelke Bloem"
    ]
  },
  "https://aclanthology.org/2021.humeval-1.13": {
    "title": "Detecting Post-Edited References and Their Effect on Human Evaluation",
    "volume": "workshop",
    "abstract": "This paper provides a quick overview of possible methods how to detect that reference translations were actually created by post-editing an MT system. Two methods based on automatic metrics are presented: BLEU difference between the suspected MT and some other good MT and BLEU difference using additional references. These two methods revealed a suspicion that the WMT 2020 Czech reference is based on MT. The suspicion was confirmed in a manual analysis by finding concrete proofs of the post-editing procedure in particular sentences. Finally, a typology of post-editing changes is presented where typical errors or changes made by the post-editor or errors adopted from the MT are classified",
    "checked": true,
    "id": "ceaa97f4ee5a84cb321c2789f1b2851e6320ab62",
    "semantic_title": "detecting post-edited references and their effect on human evaluation",
    "citation_count": 3,
    "authors": [
      "Věra Kloudová",
      "Ondřej Bojar",
      "Martin Popel"
    ]
  },
  "https://aclanthology.org/2021.humeval-1.14": {
    "title": "A Case Study of Efficacy and Challenges in Practical Human-in-Loop Evaluation of NLP Systems Using Checklist",
    "volume": "workshop",
    "abstract": "Despite state-of-the-art performance, NLP systems can be fragile in real-world situations. This is often due to insufficient understanding of the capabilities and limitations of models and the heavy reliance on standard evaluation benchmarks. Research into non-standard evaluation to mitigate this brittleness is gaining increasing attention. Notably, the behavioral testing principle ‘Checklist', which decouples testing from implementation revealed significant failures in state-of-the-art models for multiple tasks. In this paper, we present a case study of using Checklist in a practical scenario. We conduct experiments for evaluating an offensive content detection system and use a data augmentation technique for improving the model using insights from Checklist. We lay out the challenges and open questions based on our observations of using Checklist for human-in-loop evaluation and improvement of NLP systems. Disclaimer: The paper contains examples of content with offensive language. The examples do not represent the views of the authors or their employers towards any person(s), group(s), practice(s), or entity/entities",
    "checked": true,
    "id": "da8eb4a68f880d13c2834f1d74b8c92681ae0bb2",
    "semantic_title": "a case study of efficacy and challenges in practical human-in-loop evaluation of nlp systems using checklist",
    "citation_count": 12,
    "authors": [
      "Shaily Bhatt",
      "Rahul Jain",
      "Sandipan Dandapat",
      "Sunayana Sitaram"
    ]
  },
  "https://aclanthology.org/2021.humeval-1.15": {
    "title": "Interrater Disagreement Resolution: A Systematic Procedure to Reach Consensus in Annotation Tasks",
    "volume": "workshop",
    "abstract": "We present a systematic procedure for interrater disagreement resolution. The procedure is general, but of particular use in multiple-annotator tasks geared towards ground truth construction. We motivate our proposal by arguing that, barring cases in which the researchers' goal is to elicit different viewpoints, interrater disagreement is a sign of poor quality in the design or the description of a task. Consensus among annotators, we maintain, should be striven for, through a systematic procedure for disagreement resolution such as the one we describe",
    "checked": true,
    "id": "39b79c6b03619f169d5928fe9bcf46c20dad6a1b",
    "semantic_title": "interrater disagreement resolution: a systematic procedure to reach consensus in annotation tasks",
    "citation_count": 8,
    "authors": [
      "Yvette Oortwijn",
      "Thijs Ossenkoppele",
      "Arianna Betti"
    ]
  },
  "https://aclanthology.org/2021.lantern-1.1": {
    "title": "Reasoning over Vision and Language: Exploring the Benefits of Supplemental Knowledge",
    "volume": "workshop",
    "abstract": "The limits of applicability of vision-and language models are defined by the coverage of their training data. Tasks like vision question answering (VQA) often require commonsense and factual information beyond what can be learned from task-specific datasets. This paper investigates the injection of knowledge from general-purpose knowledge bases (KBs) into vision-and-language transformers. We use an auxiliary training objective that encourages the learned representations to align with graph embeddings of matching entities in a KB. We empirically study the relevance of various KBs to multiple tasks and benchmarks. The technique brings clear benefits to knowledge-demanding question answering tasks (OK-VQA, FVQA) by capturing semantic and relational knowledge absent from existing models. More surprisingly, the technique also benefits visual reasoning tasks (NLVR2, SNLI-VE). We perform probing experiments and show that the injection of additional knowledge regularizes the space of embeddings, which improves the representation of lexical and semantic similarities. The technique is model-agnostic and can expand the applicability of any vision-and-language transformer with minimal computational overhead",
    "checked": true,
    "id": "48847adb786cb8a193818aca8519a887680c2d83",
    "semantic_title": "reasoning over vision and language: exploring the benefits of supplemental knowledge",
    "citation_count": 23,
    "authors": [
      "Violetta Shevchenko",
      "Damien Teney",
      "Anthony Dick",
      "Anton van den Hengel"
    ]
  },
  "https://aclanthology.org/2021.lantern-1.2": {
    "title": "Visual Grounding Strategies for Text-Only Natural Language Processing",
    "volume": "workshop",
    "abstract": "Visual grounding is a promising path toward more robust and accurate Natural Language Processing (NLP) models. Many multimodal extensions of BERT (e.g., VideoBERT, LXMERT, VL-BERT) allow a joint modeling of texts and images that lead to state-of-the-art results on multimodal tasks such as Visual Question Answering. Here, we leverage multimodal modeling for purely textual tasks (language modeling and classification) with the expectation that the multimodal pretraining provides a grounding that can improve text processing accuracy. We propose possible strategies in this respect. A first type of strategy, referred to as transferred grounding consists in applying multimodal models to text-only tasks using a placeholder to replace image input. The second one, which we call associative grounding, harnesses image retrieval to match texts with related images during both pretraining and text-only downstream tasks. We draw further distinctions into both strategies and then compare them according to their impact on language modeling and commonsense-related downstream tasks, showing improvement over text-only baselines",
    "checked": true,
    "id": "7198c33fcbd3561f9491c73529eb19a45ac298cc",
    "semantic_title": "visual grounding strategies for text-only natural language processing",
    "citation_count": 8,
    "authors": [
      "Damien Sileo"
    ]
  },
  "https://aclanthology.org/2021.lantern-1.3": {
    "title": "Exploiting Image–Text Synergy for Contextual Image Captioning",
    "volume": "workshop",
    "abstract": "Modern web content - news articles, blog posts, educational resources, marketing brochures - is predominantly multimodal. A notable trait is the inclusion of media such as images placed at meaningful locations within a textual narrative. Most often, such images are accompanied by captions - either factual or stylistic (humorous, metaphorical, etc.) - making the narrative more engaging to the reader. While standalone image captioning has been extensively studied, captioning an image based on external knowledge such as its surrounding text remains under-explored. In this paper, we study this new task: given an image and an associated unstructured knowledge snippet, the goal is to generate a contextual caption for the image",
    "checked": true,
    "id": "6f40741579bfed3d3ae5c637a32ea52a2c3e5690",
    "semantic_title": "exploiting image–text synergy for contextual image captioning",
    "citation_count": 2,
    "authors": [
      "Sreyasi Nag Chowdhury",
      "Rajarshi Bhowmik",
      "Hareesh Ravi",
      "Gerard de Melo",
      "Simon Razniewski",
      "Gerhard Weikum"
    ]
  },
  "https://aclanthology.org/2021.lantern-1.4": {
    "title": "Large-Scale Zero-Shot Image Classification from Rich and Diverse Textual Descriptions",
    "volume": "workshop",
    "abstract": "We study the impact of using rich and diverse textual descriptions of classes for zero-shot learning (ZSL) on ImageNet. We create a new dataset ImageNet-Wiki that matches each ImageNet class to its corresponding Wikipedia article. We show that merely employing these Wikipedia articles as class descriptions yields much higher ZSL performance than prior works. Even a simple model using this type of auxiliary data outperforms state-of-the-art models that rely on standard features of word embedding encodings of class names. These results highlight the usefulness and importance of textual descriptions for ZSL, as well as the relative importance of auxiliary data type compared to the algorithmic progress. Our experimental results also show that standard zero-shot learning approaches generalize poorly across categories of classes",
    "checked": true,
    "id": "05c605eeafcaf2aa65c7386c8609e05a5c6023c0",
    "semantic_title": "large-scale zero-shot image classification from rich and diverse textual descriptions",
    "citation_count": 19,
    "authors": [
      "Sebastian Bujwid",
      "Josephine Sullivan"
    ]
  },
  "https://aclanthology.org/2021.lantern-1.5": {
    "title": "What Did This Castle Look like before? Exploring Referential Relations in Naturally Occurring Multimodal Texts",
    "volume": "workshop",
    "abstract": "Multi-modal texts are abundant and diverse in structure, yet Language & Vision research of these naturally occurring texts has mostly focused on genres that are comparatively light on text, like tweets. In this paper, we discuss the challenges and potential benefits of a L&V framework that explicitly models referential relations, taking Wikipedia articles about buildings as an example. We briefly survey existing related tasks in L&V and propose multi-modal information extraction as a general direction for future research",
    "checked": true,
    "id": "3bb57a291a7e375af058e91a56d11004891a868e",
    "semantic_title": "what did this castle look like before? exploring referential relations in naturally occurring multimodal texts",
    "citation_count": 2,
    "authors": [
      "Ronja Utescher",
      "Sina Zarrieß"
    ]
  },
  "https://aclanthology.org/2021.louhi-1.1": {
    "title": "ArCorona: Analyzing Arabic Tweets in the Early Days of Coronavirus (COVID-19) Pandemic",
    "volume": "workshop",
    "abstract": "Over the past few months, there were huge numbers of circulating tweets and discussions about Coronavirus (COVID-19) in the Arab region. It is important for policy makers and many people to identify types of shared tweets to better understand public behavior, topics of interest, requests from governments, sources of tweets, etc. It is also crucial to prevent spreading of rumors and misinformation about the virus or bad cures. To this end, we present the largest manually annotated dataset of Arabic tweets related to COVID-19. We describe annotation guidelines, analyze our dataset and build effective machine learning and transformer based models for classification",
    "checked": true,
    "id": "2d6a8e0b3e8f5ad2a39fac0db0e8131c00f9964a",
    "semantic_title": "arcorona: analyzing arabic tweets in the early days of coronavirus (covid-19) pandemic",
    "citation_count": 21,
    "authors": [
      "Hamdy Mubarak",
      "Sabit Hassan"
    ]
  },
  "https://aclanthology.org/2021.louhi-1.2": {
    "title": "Multilingual Negation Scope Resolution for Clinical Text",
    "volume": "workshop",
    "abstract": "Negation scope resolution is key to high-quality information extraction from clinical texts, but so far, efforts to make encoders used for information extraction negation-aware have been limited to English. We present a universal approach to multilingual negation scope resolution, that overcomes the lack of training data by relying on disparate resources in different languages and domains. We evaluate two approaches to learn from these resources, training on combined data and training in a multi-task learning setup. Our experiments show that zero-shot scope resolution in clinical text is possible, and that combining available resources improves performance in most cases",
    "checked": true,
    "id": "ac5b1a285fca3e13213fae8004b6ea35286f2fdb",
    "semantic_title": "multilingual negation scope resolution for clinical text",
    "citation_count": 6,
    "authors": [
      "Mareike Hartmann",
      "Anders Søgaard"
    ]
  },
  "https://aclanthology.org/2021.louhi-1.3": {
    "title": "Understanding Social Support Expressed in a COVID-19 Online Forum",
    "volume": "workshop",
    "abstract": "In online forums focused on health and wellbeing, individuals tend to seek and give the following social support: emotional and informational support. Understanding the expressions of these social supports in an online COVID- 19 forum is important for: (a) the forum and its members to provide the right type of support to individuals and (b) determining the long term effects of the COVID-19 pandemic on the well-being of the public, thereby informing interventions. In this work, we build four machine learning models to measure the extent of the following social supports expressed in each post in a COVID-19 online forum: (a) emotional support given (b) emotional support sought (c) informational support given, and (d) informational support sought. Using these models, we aim to: (i) determine if there is a correlation between the different social supports expressed in posts e.g. when members of the forum give emotional support in posts, do they also tend to give or seek informational support in the same post? (ii) determine how these social supports sought and given changes over time in published posts. We find that (i) there is a positive correlation between the informational support given in posts and the emotional support given and emotional support sought, respectively, in these posts and (ii) over time, users tended to seek more emotional support and give less emotional support",
    "checked": true,
    "id": "efc5de5d54d032d02bbb1cf2fecba2f56add1c9c",
    "semantic_title": "understanding social support expressed in a covid-19 online forum",
    "citation_count": 10,
    "authors": [
      "Anietie Andy",
      "Brian Chu",
      "Ramie Fathy",
      "Barrington Bennett",
      "Daniel Stokes",
      "Sharath Chandra Guntuku"
    ]
  },
  "https://aclanthology.org/2021.louhi-1.4": {
    "title": "Fast and Effective Biomedical Entity Linking Using a Dual Encoder",
    "volume": "workshop",
    "abstract": "Biomedical entity linking is the task of identifying mentions of biomedical concepts in text documents and mapping them to canonical entities in a target thesaurus. Recent advancements in entity linking using BERT-based models follow a retrieve and rerank paradigm, where the candidate entities are first selected using a retriever model, and then the retrieved candidates are ranked by a reranker model. While this paradigm produces state-of-the-art results, they are slow both at training and test time as they can process only one mention at a time. To mitigate these issues, we propose a BERT-based dual encoder model that resolves multiple mentions in a document in one shot. We show that our proposed model is multiple times faster than existing BERT-based models while being competitive in accuracy for biomedical entity linking. Additionally, we modify our dual encoder model for end-to-end biomedical entity linking that performs both mention span detection and entity disambiguation and out-performs two recently proposed models",
    "checked": true,
    "id": "7ea1d2789960d20c3f7d30c7def3bed559df4fda",
    "semantic_title": "fast and effective biomedical entity linking using a dual encoder",
    "citation_count": 23,
    "authors": [
      "Rajarshi Bhowmik",
      "Karl Stratos",
      "Gerard de Melo"
    ]
  },
  "https://aclanthology.org/2021.louhi-1.5": {
    "title": "Leveraging knowledge sources for detecting self-reports of particular health issues on social media",
    "volume": "workshop",
    "abstract": "This paper investigates incorporating quality knowledge sources developed by experts for the medical domain as well as syntactic information for classification of tweets into four different health oriented categories. We claim that resources such as the MeSH hierarchy and currently available parse information are effective extensions of moderately sized training datasets for various fine-grained tweet classification tasks of self-reported health issues",
    "checked": true,
    "id": "6006e4c7712ed39fa206e09c1489d7910a1cffbf",
    "semantic_title": "leveraging knowledge sources for detecting self-reports of particular health issues on social media",
    "citation_count": 5,
    "authors": [
      "Parsa Bagherzadeh",
      "Sabine Bergler"
    ]
  },
  "https://aclanthology.org/2021.louhi-1.6": {
    "title": "Integrating Higher-Level Semantics into Robust Biomedical Name Representations",
    "volume": "workshop",
    "abstract": "Neural encoders of biomedical names are typically considered robust if representations can be effectively exploited for various downstream NLP tasks. To achieve this, encoders need to model domain-specific biomedical semantics while rivaling the universal applicability of pretrained self-supervised representations. Previous work on robust representations has focused on learning low-level distinctions between names of fine-grained biomedical concepts. These fine-grained concepts can also be clustered together to reflect higher-level, more general semantic distinctions, such as grouping the names nettle sting and tick-borne fever together under the description puncture wound of skin. It has not yet been empirically confirmed that training biomedical name encoders on fine-grained distinctions automatically leads to bottom-up encoding of such higher-level semantics. In this paper, we show that this bottom-up effect exists, but that it is still relatively limited. As a solution, we propose a scalable multi-task training regime for biomedical name encoders which can also learn robust representations using only higher-level semantic classes. These representations can generalise both bottom-up as well as top-down among various semantic hierarchies. Moreover, we show how they can be used out-of-the-box for improved unsupervised detection of hypernyms, while retaining robust performance on various semantic relatedness benchmarks",
    "checked": true,
    "id": "2724d88248921ad606b4c4f6e89cd5c9509113df",
    "semantic_title": "integrating higher-level semantics into robust biomedical name representations",
    "citation_count": 1,
    "authors": [
      "Pieter Fivez",
      "Simon Suster",
      "Walter Daelemans"
    ]
  },
  "https://aclanthology.org/2021.louhi-1.7": {
    "title": "Classification of mental illnesses on social media using RoBERTa",
    "volume": "workshop",
    "abstract": "Given the current social distancing regulations across the world, social media has become the primary mode of communication for most people. This has isolated millions suffering from mental illnesses who are unable to receive assistance in person. They have increasingly turned to online platforms to express themselves and to look for guidance in dealing with their illnesses. Keeping this in mind, we propose a solution to classify mental illness posts on social media thereby enabling users to seek appropriate help. In this work, we classify five prominent kinds of mental illnesses- depression, anxiety, bipolar disorder, ADHD and PTSD by analyzing unstructured user data on Reddit. In addition, we share a new high-quality dataset1 to drive research on this topic. The dataset consists of the title and post texts from 17159 posts and 13 subreddits each associated with one of the five mental illnesses listed above or a None class indicating the absence of any mental illness. Our model is trained on Reddit data but is easily extensible to other social media platforms as well as demonstrated in our results. We believe that our work is the first multi-class model that uses a Transformer based architecture such as RoBERTa to analyze people's emotions and psychology. We also demonstrate how we stress test our model using behavioral testing. Our dataset is publicly available and we encourage researchers to utilize this to advance research in this arena. We hope that this work contributes to the public health system by automating some of the detection process and alerting relevant authorities about users that need immediate help",
    "checked": true,
    "id": "90a87763e7ddf91cae91b6733f523723471ee8e2",
    "semantic_title": "classification of mental illnesses on social media using roberta",
    "citation_count": 11,
    "authors": [
      "Ankit Murarka",
      "Balaji Radhakrishnan",
      "Sushma Ravichandran"
    ]
  },
  "https://aclanthology.org/2021.louhi-1.8": {
    "title": "Topic Modeling for Maternal Health Using Reddit",
    "volume": "workshop",
    "abstract": "This paper applies topic modeling to understand maternal health topics, concerns, and questions expressed in online communities on social networking sites. We examine Latent Dirichlet Analysis (LDA) and two state-of-the-art methods: neural topic model with knowledge distillation (KD) and Embedded Topic Model (ETM) on maternal health texts collected from Reddit. The models are evaluated on topic quality and topic inference, using both auto-evaluation metrics and human assessment. We analyze a disconnect between automatic metrics and human evaluations. While LDA performs the best overall with the auto-evaluation metrics NPMI and Coherence, Neural Topic Model with Knowledge Distillation is favorable by expert evaluation. We also create a new partially expert annotated gold-standard maternal health topic",
    "checked": true,
    "id": "7473bdc1a12c1f8ed844cd2f64ec5b583a091180",
    "semantic_title": "topic modeling for maternal health using reddit",
    "citation_count": 3,
    "authors": [
      "Shuang Gao",
      "Shivani Pandya",
      "Smisha Agarwal",
      "João Sedoc"
    ]
  },
  "https://aclanthology.org/2021.louhi-1.9": {
    "title": "FuzzyBIO: A Proposal for Fuzzy Representation of Discontinuous Entities",
    "volume": "workshop",
    "abstract": "Discontinuous entities pose a challenge to named entity recognition (NER). These phenomena occur commonly in the biomedical domain. As a solution, expansions of the BIO representation scheme that can handle these entity types are commonly used (i.e. BIOHD). However, the extra tag types make the NER task more difficult to learn. In this paper we propose an alternative; a fuzzy continuous BIO scheme (FuzzyBIO). We focus on the task of Adverse Drug Response extraction and normalization to compare FuzzyBIO to BIOHD. We find that FuzzyBIO improves recall of NER for two of three data sets and results in a higher percentage of correctly identified disjoint and composite entities for all data sets. Using FuzzyBIO also improves end-to-end performance for continuous and composite entities in two of three data sets. Since FuzzyBIO improves performance for some data sets and the conversion from BIOHD to FuzzyBIO is straightforward, we recommend investigating which is more effective for any data set containing discontinuous entities",
    "checked": true,
    "id": "a9c827932414dd093aa29f7239a7f83da514947a",
    "semantic_title": "fuzzybio: a proposal for fuzzy representation of discontinuous entities",
    "citation_count": 5,
    "authors": [
      "Anne Dirkson",
      "Suzan Verberne",
      "Wessel Kraaij"
    ]
  },
  "https://aclanthology.org/2021.louhi-1.10": {
    "title": "Cluster Analysis of Online Mental Health Discourse using Topic-Infused Deep Contextualized Representations",
    "volume": "workshop",
    "abstract": "With mental health as a problem domain in NLP, the bulk of contemporary literature revolves around building better mental illness prediction models. The research focusing on the identification of discussion clusters in online mental health communities has been relatively limited. Moreover, as the underlying methodologies used in these studies mainly conform to the traditional machine learning models and statistical methods, the scope for introducing contextualized word representations for topic and theme extraction from online mental health communities remains open. Thus, in this research, we propose topic-infused deep contextualized representations, a novel data representation technique that uses autoencoders to combine deep contextual embeddings with topical information, generating robust representations for text clustering. Investigating the Reddit discourse on Post-Traumatic Stress Disorder (PTSD) and Complex Post-Traumatic Stress Disorder (C-PTSD), we elicit the thematic clusters representing the latent topics and themes discussed in the r/ptsd and r/CPTSD subreddits. Furthermore, we also present a qualitative analysis and characterization of each cluster, unraveling the prevalent discourse themes",
    "checked": true,
    "id": "531268aa17b4f6713f7e1774b08b782c3aa8108d",
    "semantic_title": "cluster analysis of online mental health discourse using topic-infused deep contextualized representations",
    "citation_count": 3,
    "authors": [
      "Atharva Kulkarni",
      "Amey Hengle",
      "Pradnya Kulkarni",
      "Manisha Marathe"
    ]
  },
  "https://aclanthology.org/2021.louhi-1.11": {
    "title": "Scientific Claim Verification with VerT5erini",
    "volume": "workshop",
    "abstract": "This work describes the adaptation of a pretrained sequence-to-sequence model to the task of scientific claim verification in the biomedical domain. We propose a system called VerT5erini that exploits T5 for abstract retrieval, sentence selection, and label prediction, which are three critical sub-tasks of claim verification. We evaluate our pipeline on SciFACT, a newly curated dataset that requires models to not just predict the veracity of claims but also provide relevant sentences from a corpus of scientific literature that support the prediction. Empirically, our system outperforms a strong baseline in each of the three sub-tasks. We further show VerT5erini's ability to generalize to two new datasets of COVID-19 claims using evidence from the CORD-19 corpus",
    "checked": true,
    "id": "e4cb6bfe88a8ed729d34d5a9ff74a992932b70ce",
    "semantic_title": "scientific claim verification with vert5erini",
    "citation_count": 48,
    "authors": [
      "Ronak Pradeep",
      "Xueguang Ma",
      "Rodrigo Nogueira",
      "Jimmy Lin"
    ]
  },
  "https://aclanthology.org/2021.ltedi-1.1": {
    "title": "Impact of COVID-19 in Natural Language Processing Publications: a Disaggregated Study in Gender, Contribution and Experience",
    "volume": "workshop",
    "abstract": "This study sheds light on the effects of COVID-19 in the particular field of Computational Linguistics and Natural Language Processing within Artificial Intelligence. We provide an inter-sectional study on gender, contribution, and experience that considers one school year (from August 2019 to August 2020) as a pandemic year. August is included twice for the purpose of an inter-annual comparison. While the trend in publications increased with the crisis, the results show that the ratio between female and male publications decreased. This only helps to reduce the importance of the female role in the scientific contributions of computational linguistics (it is now far below its peak of 0.24). The pandemic has a particularly negative effect on the production of female senior researchers in the first position of authors (maximum work), followed by the female junior researchers in the last position of authors (supervision or collaborative work)",
    "checked": true,
    "id": "97a7b3742b37cbc0c8b94efe63d0310d21db3069",
    "semantic_title": "impact of covid-19 in natural language processing publications: a disaggregated study in gender, contribution and experience",
    "citation_count": 0,
    "authors": [
      "Christine Basta",
      "Marta R. Costa-jussa"
    ]
  },
  "https://aclanthology.org/2021.ltedi-1.2": {
    "title": "Adapting the Portuguese Braille System to Formal Semantics",
    "volume": "workshop",
    "abstract": "Since the seminal work of Richard Montague in the 1970s, mathematical and logic tools have successfully been used to model several aspects of the meaning of natural language. However, visually impaired people continue to face serious difficulties in getting full access to this important instrument. Our paper aims to present a work in progress whose main goal is to provide blind students and researchers with an adequate method to deal with the different resources that are used in formal semantics. In particular, we intend to adapt the Portuguese Braille system in order to accommodate the most common symbols and formulas used in this kind of approach and to develop pedagogical procedures to facilitate its learnability. By making this formalization compatible with the Braille coding (either traditional and electronic), we hope to help blind people to learn and use this notation, essential to acquire a better understanding of a great number of semantic properties displayed by natural language",
    "checked": true,
    "id": "8b0364104eaf0e5805013ebf13e68c0a6962c86f",
    "semantic_title": "adapting the portuguese braille system to formal semantics",
    "citation_count": 0,
    "authors": [
      "Luís Filipe Cunha"
    ]
  },
  "https://aclanthology.org/2021.ltedi-1.3": {
    "title": "Cross-Lingual Transfer Learning for Hate Speech Detection",
    "volume": "workshop",
    "abstract": "We address the task of automatic hate speech detection for low-resource languages. Rather than collecting and annotating new hate speech data, we show how to use cross-lingual transfer learning to leverage already existing data from higher-resource languages. Using bilingual word embeddings based classifiers we achieve good performance on the target language by training only on the source dataset. Using our transferred system we bootstrap on unlabeled target language data, improving the performance of standard cross-lingual transfer approaches. We use English as a high resource language and German as the target language for which only a small amount of annotated corpora are available. Our results indicate that cross-lingual transfer learning together with our approach to leverage additional unlabeled data is an effective way of achieving good performance on low-resource target languages without the need for any target-language annotations",
    "checked": true,
    "id": "43ec6f344c2df857533630abdcc87edfb142cdd9",
    "semantic_title": "cross-lingual transfer learning for hate speech detection",
    "citation_count": 23,
    "authors": [
      "Irina Bigoulaeva",
      "Viktor Hangya",
      "Alexander Fraser"
    ]
  },
  "https://aclanthology.org/2021.ltedi-1.4": {
    "title": "hBERT + BiasCorp - Fighting Racism on the Web",
    "volume": "workshop",
    "abstract": "Subtle and overt racism is still present both in physical and online communities today and has impacted many lives in different segments of the society. In this short piece of work, we present how we're tackling this societal issue with Natural Language Processing. We are releasing BiasCorp, a dataset containing 139,090 comments and news segment from three specific sources - Fox News, BreitbartNews and YouTube. The first batch (45,000 manually annotated) is ready for publication. We are currently in the final phase of manually labeling the remaining dataset using Amazon Mechanical Turk. BERT has been used widely in several downstream tasks. In this work, we present hBERT, where we modify certain layers of the pretrained BERT model with the new Hopfield Layer. hBert generalizes well across different distributions with the added advantage of a reduced model complexity. We are also releasing a JavaScript library 3 and a Chrome Extension Application, to help developers make use of our trained model in web applications (say chat application) and for users to identify and report racially biased contents on the web respectively",
    "checked": true,
    "id": "6a0bfe8188c7be8f7c9845497ee3562a7d67cb82",
    "semantic_title": "hbert + biascorp - fighting racism on the web",
    "citation_count": 6,
    "authors": [
      "Olawale Onabola",
      "Zhuang Ma",
      "Xie Yang",
      "Benjamin Akera",
      "Ibraheem Abdulrahman",
      "Jia Xue",
      "Dianbo Liu",
      "Yoshua Bengio"
    ]
  },
  "https://aclanthology.org/2021.ltedi-1.5": {
    "title": "An Overview of Fairness in Data – Illuminating the Bias in Data Pipeline",
    "volume": "workshop",
    "abstract": "Data in general encodes human biases by default; being aware of this is a good start, and the research around how to handle it is ongoing. The term ‘bias' is extensively used in various contexts in NLP systems. In our research the focus is specific to biases such as gender, racism, religion, demographic and other intersectional views on biases that prevail in text processing systems responsible for systematically discriminating specific population, which is not ethical in NLP. These biases exacerbate the lack of equality, diversity and inclusion of specific population while utilizing the NLP applications. The tools and technology at the intermediate level utilize biased data, and transfer or amplify this bias to the downstream applications. However, it is not enough to be colourblind, gender-neutral alone when designing a unbiased technology – instead, we should take a conscious effort by designing a unified framework to measure and benchmark the bias. In this paper, we recommend six measures and one augment measure based on the observations of the bias in data, annotations, text representations and debiasing techniques",
    "checked": true,
    "id": "559541354eaf3e951433a3c5f71f64541897b604",
    "semantic_title": "an overview of fairness in data – illuminating the bias in data pipeline",
    "citation_count": 13,
    "authors": [
      "Senthil Kumar B",
      "Aravindan Chandrabose",
      "Bharathi Raja Chakravarthi"
    ]
  },
  "https://aclanthology.org/2021.ltedi-1.6": {
    "title": "GEPSA, a tool for monitoring social challenges in digital press",
    "volume": "workshop",
    "abstract": "This papers presents a platform for monitoring press narratives with respect to several social challenges, including gender equality, migrations and minority languages. As narratives are encoded in natural language, we have to use natural processing techniques to automate their analysis. Thus, crawled news are processed by means of several NLP modules, including named entity recognition, keyword extraction,document classification for social challenge detection, and sentiment analysis. A Flask powered interface provides data visualization for a user-based analysis of the data. This paper presents the architecture of the system and describes in detail its different components. Evaluation is provided for the modules related to extraction and classification of information regarding social challenges",
    "checked": true,
    "id": "6e137737ae860c27953f0ad4bbb6a7835c30c135",
    "semantic_title": "gepsa, a tool for monitoring social challenges in digital press",
    "citation_count": 1,
    "authors": [
      "Iñaki San Vicente",
      "Xabier Saralegi",
      "Nerea Zubia"
    ]
  },
  "https://aclanthology.org/2021.ltedi-1.7": {
    "title": "Finding Spoiler Bias in Tweets by Zero-shot Learning and Knowledge Distilling from Neural Text Simplification",
    "volume": "workshop",
    "abstract": "Automatic detection of critical plot information in reviews of media items poses unique challenges to both social computing and computational linguistics. In this paper we propose to cast the problem of discovering spoiler bias in online discourse as a text simplification task. We conjecture that for an item-user pair, the simpler the user review we learn from an item summary the higher its likelihood to present a spoiler. Our neural model incorporates the advanced transformer network to rank the severity of a spoiler in user tweets. We constructed a sustainable high-quality movie dataset scraped from unsolicited review tweets and paired with a title summary and meta-data extracted from a movie specific domain. To a large extent, our quantitative and qualitative results weigh in on the performance impact of named entity presence in plot summaries. Pretrained on a split-and-rephrase corpus with knowledge distilled from English Wikipedia and fine-tuned on our movie dataset, our neural model shows to outperform both a language modeler and monolingual translation baselines",
    "checked": true,
    "id": "cf421fb909bdd867f9ed2cf0697270b12728d9dd",
    "semantic_title": "finding spoiler bias in tweets by zero-shot learning and knowledge distilling from neural text simplification",
    "citation_count": 0,
    "authors": [
      "Avi Bleiweiss"
    ]
  },
  "https://aclanthology.org/2021.ltedi-1.8": {
    "title": "Findings of the Shared Task on Hope Speech Detection for Equality, Diversity, and Inclusion",
    "volume": "workshop",
    "abstract": "Hope is considered significant for the well-being, recuperation and restoration of human life by health professionals. Hope speech reflects the belief that one can discover pathways to their desired objectives and become roused to utilise those pathways. To encourage research in natural language processing towards positive reinforcement approach, we created a hope speech detection dataset. This paper reports on the shared task of hope speech detection for Tamil, English, and Malayalam languages. The shared task was conducted as a part of the EACL 2021 workshop on Language Technology for Equality, Diversity, and Inclusion (LT-EDI-2021). We summarize here the datasets for this challenge which are openly available at https://competitions.codalab.org/competitions/27653, and present an overview of the methods and the results of the competing systems. To the best of our knowledge, this is the first shared task to conduct hope speech detection",
    "checked": true,
    "id": "49ef96f733eaaba532d637555f79c47236589d98",
    "semantic_title": "findings of the shared task on hope speech detection for equality, diversity, and inclusion",
    "citation_count": 125,
    "authors": [
      "Bharathi Raja Chakravarthi",
      "Vigneshwaran Muralidaran"
    ]
  },
  "https://aclanthology.org/2021.ltedi-1.9": {
    "title": "dhivya-hope-detection@LT-EDI-EACL2021: Multilingual Hope Speech Detection for Code-mixed and Transliterated Texts",
    "volume": "workshop",
    "abstract": "In this paper we work with a hope speech detection corpora that includes English, Tamil, and Malayalam datasets. We present a two phase mechanism to detect hope speech. In the first phase we build a classifier to identify the language of the text. In the second phase, we build a classifier to detect hope speech, non hope speech, or not lang labels. Experimental results show that hope speech detection is challenging and there is scope for improvement",
    "checked": true,
    "id": "3e297cc2d3d2224b39de554a189c9926fdf8790b",
    "semantic_title": "dhivya-hope-detection@lt-edi-eacl2021: multilingual hope speech detection for code-mixed and transliterated texts",
    "citation_count": 11,
    "authors": [
      "Dhivya Chinnappa"
    ]
  },
  "https://aclanthology.org/2021.ltedi-1.10": {
    "title": "KU_NLP@LT-EDI-EACL2021: A Multilingual Hope Speech Detection for Equality, Diversity, and Inclusion using Context Aware Embeddings",
    "volume": "workshop",
    "abstract": "Hope speech detection is a new task for finding and highlighting positive comments or supporting content from user-generated social media comments. For this task, we have used a Shared Task multilingual dataset on Hope Speech Detection for Equality, Diversity, and Inclusion (HopeEDI) for three languages English, code-switched Tamil and Malayalam. In this paper, we present deep learning techniques using context-aware string embeddings for word representations and Recurrent Neural Network (RNN) and pooled document embeddings for text representation. We have evaluated and compared the three models for each language with different approaches. Our proposed methodology works fine and achieved higher performance than baselines. The highest weighted average F-scores of 0.93, 0.58, and 0.84 are obtained on the task organisers' final evaluation test set. The proposed models are outperforming the baselines by 3%, 2% and 11% in absolute terms for English, Tamil and Malayalam respectively",
    "checked": true,
    "id": "438eb11e1e63bf309820b8ccf4e35c4dde559eed",
    "semantic_title": "ku_nlp@lt-edi-eacl2021: a multilingual hope speech detection for equality, diversity, and inclusion using context aware embeddings",
    "citation_count": 14,
    "authors": [
      "Junaida M K",
      "Ajees A P"
    ]
  },
  "https://aclanthology.org/2021.ltedi-1.11": {
    "title": "EDIOne@LT-EDI-EACL2021: Pre-trained Transformers with Convolutional Neural Networks for Hope Speech Detection",
    "volume": "workshop",
    "abstract": "Hope is an essential aspect of mental health stability and recovery in every individual in this fast-changing world. Any tools and methods developed for detection, analysis, and generation of hope speech will be beneficial. In this paper, we propose a model on hope-speech detection to automatically detect web content that may play a positive role in diffusing hostility on social media. We perform the experiments by taking advantage of pre-processing and transfer-learning models. We observed that the pre-trained multilingual-BERT model with convolution neural networks gave the best results. Our model ranked first, third, and fourth ranks on English, Malayalam-English, and Tamil-English code-mixed datasets",
    "checked": true,
    "id": "c518a3d299d3a3417cdcab11b4f58839a714a434",
    "semantic_title": "edione@lt-edi-eacl2021: pre-trained transformers with convolutional neural networks for hope speech detection",
    "citation_count": 18,
    "authors": [
      "Suman Dowlagar",
      "Radhika Mamidi"
    ]
  },
  "https://aclanthology.org/2021.ltedi-1.12": {
    "title": "ssn_diBERTsity@LT-EDI-EACL2021:Hope Speech Detection on multilingual YouTube comments via transformer based approach",
    "volume": "workshop",
    "abstract": "In recent times, there exists an abundance of research to classify abusive and offensive texts focusing on negative comments but only minimal research using the positive reinforcement approach. The task was aimed at classifying texts into ‘Hope_speech', ‘Non_hope_speech', and ‘Not in language'. The datasets were provided by the LT-EDI organisers in English, Tamil, and Malayalam language with texts sourced from YouTube comments. We trained our data using transformer models, specifically mBERT for Tamil and Malayalam and BERT for English, and achieved weighted average F1-scores of 0.46, 0.81, 0.92 for Tamil, Malayalam, and English respectively",
    "checked": true,
    "id": "c1b6f97a1361d14081202c14f5bf1a046dd308d2",
    "semantic_title": "ssn_dibertsity@lt-edi-eacl2021:hope speech detection on multilingual youtube comments via transformer based approach",
    "citation_count": 10,
    "authors": [
      "Arunima S",
      "Akshay Ramakrishnan",
      "Avantika Balaji",
      "Thenmozhi D.",
      "Senthil Kumar B"
    ]
  },
  "https://aclanthology.org/2021.ltedi-1.13": {
    "title": "IIITT@LT-EDI-EACL2021-Hope Speech Detection: There is always hope in Transformers",
    "volume": "workshop",
    "abstract": "In a world with serious challenges like climate change, religious and political conflicts, global pandemics, terrorism, and racial discrimination, an internet full of hate speech, abusive and offensive content is the last thing we desire for. In this paper, we work to identify and promote positive and supportive content on these platforms. We work with several transformer-based models to classify social media comments as hope speech or not hope speech in English, Malayalam, and Tamil languages. This paper portrays our work for the Shared Task on Hope Speech Detection for Equality, Diversity, and Inclusion at LT-EDI 2021- EACL 2021. The codes for our best submission can be viewed",
    "checked": true,
    "id": "3ca6eb7b69da71899ff2d20d6a65f3e668b97312",
    "semantic_title": "iiitt@lt-edi-eacl2021-hope speech detection: there is always hope in transformers",
    "citation_count": 52,
    "authors": [
      "Karthik Puranik",
      "Adeep Hande",
      "Ruba Priyadharshini",
      "Sajeetha Thavareesan",
      "Bharathi Raja Chakravarthi"
    ]
  },
  "https://aclanthology.org/2021.ltedi-1.14": {
    "title": "IIIT_DWD@LT-EDI-EACL2021: Hope Speech Detection in YouTube multilingual comments",
    "volume": "workshop",
    "abstract": "Language as a significant part of communication should be inclusive of equality and diversity. The internet user's language has a huge influence on peer users all over the world. People express their views through language on virtual platforms like Facebook, Twitter, YouTube etc. People admire the success of others, pray for their well-being, and encourage on their failure. Such inspirational comments are hope speech comments. At the same time, a group of users promotes discrimination based on gender, racial, sexual orientation, persons with disability, and other minorities. The current paper aims to identify hope speech comments which are very important to move on in life. Various machine learning and deep learning based models (such as support vector machine, logistics regression, convolutional neural network, recurrent neural network) are employed to identify the hope speech in the given YouTube comments. The YouTube comments are available in English, Tamil and Malayalam languages and are part of the task \"EACL-2021:Hope Speech Detection for Equality, Diversity and Inclusion\"",
    "checked": true,
    "id": "f53f9c63c6f54f4c668c590b26520e2202b3f967",
    "semantic_title": "iiit_dwd@lt-edi-eacl2021: hope speech detection in youtube multilingual comments",
    "citation_count": 16,
    "authors": [
      "Sunil Saumya",
      "Ankit Kumar Mishra"
    ]
  },
  "https://aclanthology.org/2021.ltedi-1.15": {
    "title": "IRNLP_DAIICT@LT-EDI-EACL2021: Hope Speech detection in Code Mixed text using TF-IDF Char N-grams and MuRIL",
    "volume": "workshop",
    "abstract": "This paper presents the participation of the IRNLP_DAIICT team from Information Retrieval and Natural Language Processing lab at DA-IICT, India in LT-EDI@EACL2021 Hope Speech Detection task. The aim of this shared task is to identify hope speech from a code-mixed data-set of YouTube comments. The task is to classify comments into Hope Speech, Non Hope speech or Not in language, for three languages: English, Malayalam-English and Tamil-English. We use TF-IDF character n-grams and pretrained MuRIL embeddings for text representation and Logistic Regression and Linear SVM for classification. Our best approach achieved second, eighth and fifth rank with weighted F1 score of 0.92, 0.75 and 0.57 in English, Malayalam-English and Tamil-English on test dataset respectively",
    "checked": true,
    "id": "2a7d099cdde380f98cfd548287533c5621391b6f",
    "semantic_title": "irnlp_daiict@lt-edi-eacl2021: hope speech detection in code mixed text using tf-idf char n-grams and muril",
    "citation_count": 11,
    "authors": [
      "Bhargav Dave",
      "Shripad Bhat",
      "Prasenjit Majumder"
    ]
  },
  "https://aclanthology.org/2021.ltedi-1.16": {
    "title": "ZYJ@LT-EDI-EACL2021:XLM-RoBERTa-Based Model with Attention for Hope Speech Detection",
    "volume": "workshop",
    "abstract": "Due to the development of modern computer technology and the increase in the number of online media users, we can see all kinds of posts and comments everywhere on the internet. Hope speech can not only inspire the creators but also make other viewers pleasant. It is necessary to effectively and automatically detect hope speech. This paper describes the approach of our team in the task of hope speech detection. We use the attention mechanism to adjust the weight of all the output layers of XLM-RoBERTa to make full use of the information extracted from each layer, and use the weighted sum of all the output layers to complete the classification task. And we use the Stratified-K-Fold method to enhance the training data set. We achieve a weighted average F1-score of 0.59, 0.84, and 0.92 for Tamil, Malayalam, and English language, ranked 3rd, 2nd, and 2nd",
    "checked": true,
    "id": "2c39775d7428277a502f22117264f6376e1ea2ab",
    "semantic_title": "zyj@lt-edi-eacl2021:xlm-roberta-based model with attention for hope speech detection",
    "citation_count": 7,
    "authors": [
      "Yingjia Zhao",
      "Xin Tao"
    ]
  },
  "https://aclanthology.org/2021.ltedi-1.17": {
    "title": "TEAM HUB@LT-EDI-EACL2021: Hope Speech Detection Based On Pre-trained Language Model",
    "volume": "workshop",
    "abstract": "This article introduces the system description of TEAM_HUB team participating in LT-EDI 2021: Hope Speech Detection. This shared task is the first task related to the desired voice detection. The data set in the shared task consists of three different languages (English, Tamil, and Malayalam). The task type is text classification. Based on the analysis and understanding of the task description and data set, we designed a system based on a pre-trained language model to complete this shared task. In this system, we use methods and models that combine the XLM-RoBERTa pre-trained language model and the Tf-Idf algorithm. In the final result ranking announced by the task organizer, our system obtained F1 scores of 0.93, 0.84, 0.59 on the English dataset, Malayalam dataset, and Tamil dataset. Our submission results are ranked 1, 2, and 3 respectively",
    "checked": true,
    "id": "f222b42776a4a426d386260dfe9d115a40034781",
    "semantic_title": "team hub@lt-edi-eacl2021: hope speech detection based on pre-trained language model",
    "citation_count": 15,
    "authors": [
      "Bo Huang",
      "Yang Bai"
    ]
  },
  "https://aclanthology.org/2021.ltedi-1.18": {
    "title": "cs_english@LT-EDI-EACL2021: Hope Speech Detection Based On Fine-tuning ALBERT Model",
    "volume": "workshop",
    "abstract": "This paper mainly introduces the relevant content of the task \"Hope Speech Detection for Equality, Diversity, and Inclusion at LT-EDI 2021-EACL 2021\". A total of three language datasets were provided, and we chose the English dataset to complete this task. The specific task objective is to classify the given speech into ‘Hope speech', ‘Not Hope speech', and ‘Not in intended language'. In terms of method, we use fine-tuned ALBERT and K fold cross-validation to accomplish this task. In the end, we achieved a good result in the rank list of the task result, and the final F1 score was 0.93, tying for first place. However, we will continue to try to improve methods to get better results in future work",
    "checked": true,
    "id": "8dea71d78305eb1299510b2bae08f9387d084707",
    "semantic_title": "cs_english@lt-edi-eacl2021: hope speech detection based on fine-tuning albert model",
    "citation_count": 9,
    "authors": [
      "Shi Chen",
      "Bing Kong"
    ]
  },
  "https://aclanthology.org/2021.ltedi-1.19": {
    "title": "GCDH@LT-EDI-EACL2021: XLM-RoBERTa for Hope Speech Detection in English, Malayalam, and Tamil",
    "volume": "workshop",
    "abstract": "This paper describes approaches to identify Hope Speech in short, informal texts in English, Malayalam and Tamil using different machine learning techniques. We demonstrate that even very simple baseline algorithms perform reasonably well on this task if provided with enough training data. However, our best performing algorithm is a cross-lingual transfer learning approach in which we fine-tune XLM-RoBERTa",
    "checked": true,
    "id": "e372972a62f828306796fe36595857828454fbe0",
    "semantic_title": "gcdh@lt-edi-eacl2021: xlm-roberta for hope speech detection in english, malayalam, and tamil",
    "citation_count": 14,
    "authors": [
      "Stefan Ziehe",
      "Franziska Pannach",
      "Aravind Krishnan"
    ]
  },
  "https://aclanthology.org/2021.ltedi-1.20": {
    "title": "TeamUNCC@LT-EDI-EACL2021: Hope Speech Detection using Transfer Learning with Transformers",
    "volume": "workshop",
    "abstract": "In this paper, we describe our approach towards utilizing pre-trained models for the task of hope speech detection. We participated in Task 2: Hope Speech Detection for Equality, Diversity and Inclusion at LT-EDI-2021 @ EACL2021. The goal of this task is to predict the presence of hope speech, along with the presence of samples that do not belong to the same language in the dataset. We describe our approach to fine-tuning RoBERTa for Hope Speech detection in English and our approach to fine-tuning XLM-RoBERTa for Hope Speech detection in Tamil and Malayalam, two low resource Indic languages. We demonstrate the performance of our approach on classifying text into hope-speech, non-hope and not-language. Our approach ranked 1st in English (F1 = 0.93), 1st in Tamil (F1 = 0.61) and 3rd in Malayalam (F1 = 0.83)",
    "checked": true,
    "id": "cd36d39be151c6287c3a5bf99c084bf546a9c2ef",
    "semantic_title": "teamuncc@lt-edi-eacl2021: hope speech detection using transfer learning with transformers",
    "citation_count": 16,
    "authors": [
      "Khyati Mahajan",
      "Erfan Al-Hossami",
      "Samira Shaikh"
    ]
  },
  "https://aclanthology.org/2021.ltedi-1.21": {
    "title": "Autobots@LT-EDI-EACL2021: One World, One Family: Hope Speech Detection with BERT Transformer Model",
    "volume": "workshop",
    "abstract": "The rapid rise of online social networks like YouTube, Facebook, Twitter allows people to express their views more widely online. However, at the same time, it can lead to an increase in conflict and hatred among consumers in the form of freedom of speech. Therefore, it is essential to take a positive strengthening method to research on encouraging, positive, helping, and supportive social media content. In this paper, we describe a Transformer-based BERT model for Hope speech detection for equality, diversity, and inclusion, submitted for LT-EDI-2021 Task 2. Our model achieves a weighted averaged f1-score of 0.93 on the test set",
    "checked": true,
    "id": "7b7ec7c3110a561f0ec50456127c9fcc149486d8",
    "semantic_title": "autobots@lt-edi-eacl2021: one world, one family: hope speech detection with bert transformer model",
    "citation_count": 5,
    "authors": [
      "Sunil Gundapu",
      "Radhika Mamidi"
    ]
  },
  "https://aclanthology.org/2021.ltedi-1.22": {
    "title": "Amrita@LT-EDI-EACL2021: Hope Speech Detection on Multilingual Text",
    "volume": "workshop",
    "abstract": "Analysis and deciphering code-mixed data is imperative in academia and industry, in a multilingual country like India, in order to solve problems apropos Natural Language Processing. This paper proposes a bidirectional long short-term memory (BiLSTM) with the attention-based approach, in solving the hope speech detection problem. Using this approach an F1-score of 0.73 (9thrank) in the Malayalam-English data set was achieved from a total of 31 teams who participated in the competition",
    "checked": true,
    "id": "f9d24cb8edfbbfe29e44ec2be3a1e15e51ef279e",
    "semantic_title": "amrita@lt-edi-eacl2021: hope speech detection on multilingual text",
    "citation_count": 7,
    "authors": [
      "Thara S",
      "Ravi teja Tasubilli",
      "Kothamasu Sai rahul"
    ]
  },
  "https://aclanthology.org/2021.ltedi-1.23": {
    "title": "Hopeful Men@LT-EDI-EACL2021: Hope Speech Detection Using Indic Transliteration and Transformers",
    "volume": "workshop",
    "abstract": "This paper aims to describe the approach we used to detect hope speech in the HopeEDI dataset. We experimented with two approaches. In the first approach, we used contextual embeddings to train classifiers using logistic regression, random forest, SVM, and LSTM based models. The second approach involved using a majority voting ensemble of 11 models which were obtained by fine-tuning pre-trained transformer models (BERT, ALBERT, RoBERTa, IndicBERT) after adding an output layer. We found that the second approach was superior for English, Tamil and Malayalam. Our solution got a weighted F1 score of 0.93, 0.75 and 0.49 for English, Malayalam and Tamil respectively. Our solution ranked 1st in English, 8th in Malayalam and 11th in Tamil",
    "checked": true,
    "id": "59e470aab68583267f8fed50a9dc4306062faa22",
    "semantic_title": "hopeful men@lt-edi-eacl2021: hope speech detection using indic transliteration and transformers",
    "citation_count": 13,
    "authors": [
      "Ishan Sanjeev Upadhyay",
      "Nikhil E",
      "Anshul Wadhawan",
      "Radhika Mamidi"
    ]
  },
  "https://aclanthology.org/2021.ltedi-1.24": {
    "title": "Hopeful NLP@LT-EDI-EACL2021: Finding Hope in YouTube Comment Section",
    "volume": "workshop",
    "abstract": "The proliferation of Hate Speech and misinformation in social media is fast becoming a menace to society. In compliment, the dissemination of hate-diffusing, promising and anti-oppressive messages become a unique alternative. Unfortunately, due to its complex nature as well as the relatively limited manifestation in comparison to hostile and neutral content, the identification of Hope Speech becomes a challenge. This work revolves around the detection of Hope Speech in Youtube comments, for the Shared Task on Hope Speech Detection for Equality, Diversity, and Inclusion. We achieve an f-score of 0.93, ranking 1st on the leaderboard for English comments",
    "checked": true,
    "id": "d5f012faa04415763ebba81716d69f86fc776f5d",
    "semantic_title": "hopeful nlp@lt-edi-eacl2021: finding hope in youtube comment section",
    "citation_count": 5,
    "authors": [
      "Vasudev Awatramani"
    ]
  },
  "https://aclanthology.org/2021.ltedi-1.25": {
    "title": "NLP-CUET@LT-EDI-EACL2021: Multilingual Code-Mixed Hope Speech Detection using Cross-lingual Representation Learner",
    "volume": "workshop",
    "abstract": "In recent years, several systems have been developed to regulate the spread of negativity and eliminate aggressive, offensive or abusive contents from the online platforms. Nevertheless, a limited number of researches carried out to identify positive, encouraging and supportive contents. In this work, our goal is to identify whether a social media post/comment contains hope speech or not. We propose three distinct models to identify hope speech in English, Tamil and Malayalam language to serve this purpose. To attain this goal, we employed various machine learning (SVM, LR, ensemble), deep learning (CNN+BiLSTM) and transformer (m-BERT, Indic-BERT, XLNet, XLM-R) based methods. Results indicate that XLM-R outdoes all other techniques by gaining a weighted f_1-score of 0.93, 0.60 and 0.85 respectively for English, Tamil and Malayalam language. Our team has achieved 1st, 2nd and 1st rank in these three tasks respectively",
    "checked": true,
    "id": "8ee4a388fa854ff9e79775a4e5937d8d7d18b1a2",
    "semantic_title": "nlp-cuet@lt-edi-eacl2021: multilingual code-mixed hope speech detection using cross-lingual representation learner",
    "citation_count": 21,
    "authors": [
      "Eftekhar Hossain",
      "Omar Sharif",
      "Mohammed Moshiul Hoque"
    ]
  },
  "https://aclanthology.org/2021.ltedi-1.26": {
    "title": "Simon @ LT-EDI-EACL2021: Detecting Hope Speech with BERT",
    "volume": "workshop",
    "abstract": "In today's society, the rapid development of communication technology allows us to communicate with people from different parts of the world. In the process of communication, each person treats others differently. Some people are used to using offensive and sarcastic language to express their views. These words cause pain to others and make people feel down. Some people are used to sharing happiness with others and encouraging others. Such people bring joy and hope to others through their words. On social media platforms, these two kinds of language are all over the place. If people want to make the online world a better place, they will have to deal with both. So identifying offensive language and hope language is an essential task. There have been many assignments about offensive language. Shared Task on Hope Speech Detection for Equality, Diversity, and Inclusion at LT-EDI 2021-EACL 2021 uses another unique perspective – to identify the language of Hope to make contributions to society. The XLM-Roberta model is an excellent multilingual model. Our team used a fine-tuned XLM-Roberta model to accomplish this task",
    "checked": true,
    "id": "1ffb4eb6453a5f3419c9dc23da33847a28ad2274",
    "semantic_title": "simon @ lt-edi-eacl2021: detecting hope speech with bert",
    "citation_count": 5,
    "authors": [
      "Qinyu Que"
    ]
  },
  "https://aclanthology.org/2021.ltedi-1.27": {
    "title": "MUCS@LT-EDI-EACL2021:CoHope-Hope Speech Detection for Equality, Diversity, and Inclusion in Code-Mixed Texts",
    "volume": "workshop",
    "abstract": "This paper describes the models submitted by the team MUCS for \"Hope Speech Detection for Equality, Diversity, and Inclusion-EACL 2021\" shared task that aims at classifying a comment / post in English and code-mixed texts in two language pairs, namely, Tamil-English (Ta-En) and Malayalam-English (Ma-En) into one of the three predefined categories, namely, \"Hope_speech\", \"Non_hope_speech\", and \"other_languages\". Three models namely, CoHope-ML, CoHope-NN, and CoHope-TL based on Ensemble of classifiers, Keras Neural Network (NN) and BiLSTM with Conv1d model respectively are proposed for the shared task. CoHope-ML, CoHope-NN models are trained on a feature set comprised of char sequences extracted from sentences combined with words for Ma-En and Ta-En code-mixed texts and a combination of word and char ngrams along with syntactic word ngrams for English text. CoHope-TL model consists of three major parts: training tokenizer, BERT Language Model (LM) training and then using pre-trained BERT LM as weights in BiLSTM-Conv1d model. Out of three proposed models, CoHope-ML model (best among our models) obtained 1st, 2nd, and 3rd ranks with weighted F1-scores of 0.85, 0.92, and 0.59 for Ma-En, English and Ta-En texts respectively",
    "checked": true,
    "id": "1b415dd025ea1fa0834abbeb1f7c21b9594d66da",
    "semantic_title": "mucs@lt-edi-eacl2021:cohope-hope speech detection for equality, diversity, and inclusion in code-mixed texts",
    "citation_count": 21,
    "authors": [
      "Fazlourrahman Balouchzahi",
      "Aparna B K",
      "H L Shashirekha"
    ]
  },
  "https://aclanthology.org/2021.ltedi-1.28": {
    "title": "Spartans@LT-EDI-EACL2021: Inclusive Speech Detection using Pretrained Language Models",
    "volume": "workshop",
    "abstract": "We describe our system that ranked first in Hope Speech Detection (HSD) shared task and fourth in Offensive Language Identification (OLI) shared task, both in Tamil language. The goal of HSD and OLI is to identify if a code-mixed comment or post contains hope speech or offensive content respectively. We pre-train a transformer-based model RoBERTa using synthetically generated code-mixed data and use it in an ensemble along with their pre-trained ULMFiT model available from iNLTK",
    "checked": true,
    "id": "6edf628387a1a1fca6d20b6a62711d807d518ac1",
    "semantic_title": "spartans@lt-edi-eacl2021: inclusive speech detection using pretrained language models",
    "citation_count": 9,
    "authors": [
      "Megha Sharma",
      "Gaurav Arora"
    ]
  },
  "https://aclanthology.org/2021.ltedi-1.29": {
    "title": "CFILT IIT Bombay@LT-EDI-EACL2021: Hope Speech Detection for Equality, Diversity, and Inclusion using Multilingual Representation fromTransformers",
    "volume": "workshop",
    "abstract": "With the internet becoming part and parcel of our lives, engagement in social media has increased a lot. Identifying and eliminating offensive content from social media has become of utmost priority to prevent any kind of violence. However, detecting encouraging, supportive and positive content is equally important to prevent misuse of censorship targeted to attack freedom of speech. This paper presents our system for the shared task Hope Speech Detection for Equality, Diversity, and Inclusion at LT-EDI, EACL 2021. The data for this shared task is provided in English, Tamil, and Malayalam which was collected from YouTube comments. It is a multiclass classification problem where each data instance is categorized into one of the three classes: ‘Hope speech', ‘Not hope speech', and ‘Not in intended language'. We propose a system that employs multilingual transformer models to obtain the representation of text and classifies it into one of the three classes. We explored the use of multilingual models trained specifically for Indian languages along with generic multilingual models. Our system was ranked 2nd for English, 2nd for Malayalam, and 7th for the Tamil language in the final leader board published by organizers and obtained a weighted F1-score of 0.92, 0.84, 0.55 respectively on the hidden test dataset used for the competition. We have made our system publicly available at GitHub",
    "checked": true,
    "id": "84cc24fbcf0169f43df9056d496f9cf9993b43da",
    "semantic_title": "cfilt iit bombay@lt-edi-eacl2021: hope speech detection for equality, diversity, and inclusion using multilingual representation fromtransformers",
    "citation_count": 1,
    "authors": [
      "Pankaj Singh",
      "Prince Kumar",
      "Pushpak Bhattacharyya"
    ]
  },
  "https://aclanthology.org/2021.ltedi-1.30": {
    "title": "IIITK@LT-EDI-EACL2021: Hope Speech Detection for Equality, Diversity, and Inclusion in Tamil , Malayalam and English",
    "volume": "workshop",
    "abstract": "This paper describes the IIITK's team submissions to the hope speech detection for equality, diversity and inclusion in Dravidian languages shared task organized by LT-EDI 2021 workshop@EACL 2021. Our best configurations for the shared tasks achieve weighted F1 scores of 0.60 for Tamil, 0.83 for Malayalam, and 0.93 for English. We have secured ranks of 4, 3, 2 in Tamil, Malayalam and English respectively",
    "checked": true,
    "id": "de93db5c8e180b023a7a8ba0774ca7be7601e3f7",
    "semantic_title": "iiitk@lt-edi-eacl2021: hope speech detection for equality, diversity, and inclusion in tamil , malayalam and english",
    "citation_count": 43,
    "authors": [
      "Nikhil Ghanghor",
      "Rahul Ponnusamy",
      "Prasanna Kumar Kumaresan",
      "Ruba Priyadharshini",
      "Sajeetha Thavareesan",
      "Bharathi Raja Chakravarthi"
    ]
  },
  "https://aclanthology.org/2021.vardial-1.1": {
    "title": "Findings of the VarDial Evaluation Campaign 2021",
    "volume": "workshop",
    "abstract": "This paper describes the results of the shared tasks organized as part of the VarDial Evaluation Campaign 2021. The campaign was part of the eighth workshop on Natural Language Processing (NLP) for Similar Languages, Varieties and Dialects (VarDial), co-located with EACL 2021. Four separate shared tasks were included this year: Dravidian Language Identification (DLI), Romanian Dialect Identification (RDI), Social Media Variety Geolocation (SMG), and Uralic Language Identification (ULI). DLI was organized for the first time and the other three continued a series of tasks from previous evaluation campaigns",
    "checked": true,
    "id": "9f8a7aa5fc4b5546a3ef21b07df5a0dfdaba96e0",
    "semantic_title": "findings of the vardial evaluation campaign 2021",
    "citation_count": 42,
    "authors": [
      "Bharathi Raja Chakravarthi",
      "Gaman Mihaela",
      "Radu Tudor Ionescu",
      "Heidi Jauhiainen",
      "Tommi Jauhiainen",
      "Krister Lindén",
      "Nikola Ljubešić",
      "Niko Partanen",
      "Ruba Priyadharshini",
      "Christoph Purschke",
      "Eswari Rajagopal",
      "Yves Scherrer",
      "Marcos Zampieri"
    ]
  },
  "https://aclanthology.org/2021.vardial-1.2": {
    "title": "Hierarchical Transformer for Multilingual Machine Translation",
    "volume": "workshop",
    "abstract": "The choice of parameter sharing strategy in multilingual machine translation models determines how optimally parameter space is used and hence, directly influences ultimate translation quality. Inspired by linguistic trees that show the degree of relatedness between different languages, the new general approach to parameter sharing in multilingual machine translation was suggested recently. The main idea is to use these expert language hierarchies as a basis for multilingual architecture: the closer two languages are, the more parameters they share. In this work, we test this idea using the Transformer architecture and show that despite the success in previous work there are problems inherent to training such hierarchical models. We demonstrate that in case of carefully chosen training strategy the hierarchical architecture can outperform bilingual models and multilingual models with full parameter sharing",
    "checked": true,
    "id": "821fac667c95b426a64ecf9763a5a077bdf7643c",
    "semantic_title": "hierarchical transformer for multilingual machine translation",
    "citation_count": 1,
    "authors": [
      "Albina Khusainova",
      "Adil Khan",
      "Adín Ramírez Rivera",
      "Vitaly Romanov"
    ]
  },
  "https://aclanthology.org/2021.vardial-1.3": {
    "title": "Regression Analysis of Lexical and Morpho-Syntactic Properties of Kiezdeutsch",
    "volume": "workshop",
    "abstract": "Kiezdeutsch is a variety of German predominantly spoken by teenagers from multi-ethnic urban neighborhoods in casual conversations with their peers. In recent years, the popularity of Kiezdeutsch has increased among young people, independently of their socio-economic origin, and has spread in social media, too. While previous studies have extensively investigated this language variety from a linguistic and qualitative perspective, not much has been done from a quantitative point of view. We perform the first large-scale data-driven analysis of the lexical and morpho-syntactic properties of Kiezdeutsch in comparison with standard German. At the level of results, we confirm predictions of previous qualitative analyses and integrate them with further observations on specific linguistic phenomena such as slang and self-centered speaker attitude. At the methodological level, we provide logistic regression as a framework to perform bottom-up feature selection in order to quantify differences across language varieties",
    "checked": true,
    "id": "ba2543ad6d06afbb227ba5dbb247c57155686542",
    "semantic_title": "regression analysis of lexical and morpho-syntactic properties of kiezdeutsch",
    "citation_count": 2,
    "authors": [
      "Diego Frassinelli",
      "Gabriella Lapesa",
      "Reem Alatrash",
      "Dominik Schlechtweg",
      "Sabine Schulte im Walde"
    ]
  },
  "https://aclanthology.org/2021.vardial-1.4": {
    "title": "Representations of Language Varieties Are Reliable Given Corpus Similarity Measures",
    "volume": "workshop",
    "abstract": "This paper measures similarity both within and between 84 language varieties across nine languages. These corpora are drawn from digital sources (the web and tweets), allowing us to evaluate whether such geo-referenced corpora are reliable for modelling linguistic variation. The basic idea is that, if each source adequately represents a single underlying language variety, then the similarity between these sources should be stable across all languages and countries. The paper shows that there is a consistent agreement between these sources using frequency-based corpus similarity measures. This provides further evidence that digital geo-referenced corpora consistently represent local language varieties",
    "checked": true,
    "id": "69b95bae048c2d8d18153d2874a86965c4c5510a",
    "semantic_title": "representations of language varieties are reliable given corpus similarity measures",
    "citation_count": 10,
    "authors": [
      "Jonathan Dunn"
    ]
  },
  "https://aclanthology.org/2021.vardial-1.5": {
    "title": "Whit's the Richt Pairt o Speech: PoS tagging for Scots",
    "volume": "workshop",
    "abstract": "In this paper we explore PoS tagging for the Scots language. Scots is spoken in Scotland and Northern Ireland, and is closely related to English. As no linguistically annotated Scots data were available, we manually PoS tagged a small set that is used for evaluation and training. We use English as a transfer language to examine zero-shot transfer and transfer learning methods. We find that training on a very small amount of Scots data was superior to zero-shot transfer from English. Combining the Scots and English data led to further improvements, with a concatenation method giving the best results. We also compared the use of two different English treebanks and found that a treebank containing web data was superior in the zero-shot setting, while it was outperformed by a treebank containing a mix of genres when combined with Scots data",
    "checked": true,
    "id": "d37e58874c491a6834973c545e685a43150f46e9",
    "semantic_title": "whit's the richt pairt o speech: pos tagging for scots",
    "citation_count": 2,
    "authors": [
      "Harm Lameris",
      "Sara Stymne"
    ]
  },
  "https://aclanthology.org/2021.vardial-1.6": {
    "title": "Efficient Unsupervised NMT for Related Languages with Cross-Lingual Language Models and Fidelity Objectives",
    "volume": "workshop",
    "abstract": "The most successful approach to Neural Machine Translation (NMT) when only monolingual training data is available, called unsupervised machine translation, is based on back-translation where noisy translations are generated to turn the task into a supervised one. However, back-translation is computationally very expensive and inefficient. This work explores a novel, efficient approach to unsupervised NMT. A transformer, initialized with cross-lingual language model weights, is fine-tuned exclusively on monolingual data of the target language by jointly learning on a paraphrasing and denoising autoencoder objective. Experiments are conducted on WMT datasets for German-English, French-English, and Romanian-English. Results are competitive to strong baseline unsupervised NMT models, especially for closely related source languages (German) compared to more distant ones (Romanian, French), while requiring about a magnitude less training time",
    "checked": true,
    "id": "58784f4ed20ef1920b5bbffbf3818e57bca2ae24",
    "semantic_title": "efficient unsupervised nmt for related languages with cross-lingual language models and fidelity objectives",
    "citation_count": 0,
    "authors": [
      "Rami Aly",
      "Andrew Caines",
      "Paula Buttery"
    ]
  },
  "https://aclanthology.org/2021.vardial-1.7": {
    "title": "Fine-tuning Distributional Semantic Models for Closely-Related Languages",
    "volume": "workshop",
    "abstract": "In this paper we compare the performance of three models: SGNS (skip-gram negative sampling) and augmented versions of SVD (singular value decomposition) and PPMI (Positive Pointwise Mutual Information) on a word similarity task. We particularly focus on the role of hyperparameter tuning for Hindi based on recommendations made in previous work (on English). Our results show that there are language specific preferences for these hyperparameters. We extend the best settings for Hindi to a set of related languages: Punjabi, Gujarati and Marathi with favourable results. We also find that a suitably tuned SVD model outperforms SGNS for most of our languages and is also more robust in a low-resource setting",
    "checked": true,
    "id": "56f09209b154931fbb3085e580e0cab24718c8d5",
    "semantic_title": "fine-tuning distributional semantic models for closely-related languages",
    "citation_count": 2,
    "authors": [
      "Kushagra Bhatia",
      "Divyanshu Aggarwal",
      "Ashwini Vaidya"
    ]
  },
  "https://aclanthology.org/2021.vardial-1.8": {
    "title": "Discriminating Between Similar Nordic Languages",
    "volume": "workshop",
    "abstract": "Automatic language identification is a challenging problem. Discriminating between closely related languages is especially difficult. This paper presents a machine learning approach for automatic language identification for the Nordic languages, which often suffer miscategorisation by existing state-of-the-art tools. Concretely we will focus on discrimination between six Nordic languages: Danish, Swedish, Norwegian (Nynorsk), Norwegian (Bokmål), Faroese and Icelandic",
    "checked": true,
    "id": "1a860b6411495ae29080548c1e30f4f78121bc52",
    "semantic_title": "discriminating between similar nordic languages",
    "citation_count": 8,
    "authors": [
      "René Haas",
      "Leon Derczynski"
    ]
  },
  "https://aclanthology.org/2021.vardial-1.9": {
    "title": "Naive Bayes-based Experiments in Romanian Dialect Identification",
    "volume": "workshop",
    "abstract": "This article describes the experiments and systems developed by the SUKI team for the second edition of the Romanian Dialect Identification (RDI) shared task which was organized as part of the 2021 VarDial Evaluation Campaign. We submitted two runs to the shared task and our second submission was the overall best submission by a noticeable margin. Our best submission used a character n-gram based naive Bayes classifier with adaptive language models. We describe our experiments on the development set leading to both submissions",
    "checked": true,
    "id": "41a6f9792cb0bcfbe15c76a26dd3e8c1fb61f413",
    "semantic_title": "naive bayes-based experiments in romanian dialect identification",
    "citation_count": 16,
    "authors": [
      "Tommi Jauhiainen",
      "Heidi Jauhiainen",
      "Krister Lindén"
    ]
  },
  "https://aclanthology.org/2021.vardial-1.10": {
    "title": "UnibucKernel: Geolocating Swiss German Jodels Using Ensemble Learning",
    "volume": "workshop",
    "abstract": "In this work, we describe our approach addressing the Social Media Variety Geolocation task featured in the 2021 VarDial Evaluation Campaign. We focus on the second subtask, which is based on a data set formed of approximately 30 thousand Swiss German Jodels. The dialect identification task is about accurately predicting the latitude and longitude of test samples. We frame the task as a double regression problem, employing an XGBoost meta-learner with the combined power of a variety of machine learning approaches to predict both latitude and longitude. The models included in our ensemble range from simple regression techniques, such as Support Vector Regression, to deep neural models, such as a hybrid neural network and a neural transformer. To minimize the prediction error, we approach the problem from a few different perspectives and consider various types of features, from low-level character n-grams to high-level BERT embeddings. The XGBoost ensemble resulted from combining the power of the aforementioned methods achieves a median distance of 23.6 km on the test data, which places us on the third place in the ranking, at a difference of 6.05 km and 2.9 km from the submissions on the first and second places, respectively",
    "checked": true,
    "id": "0bdae7be8e966b104a07122403b242b08e079d38",
    "semantic_title": "unibuckernel: geolocating swiss german jodels using ensemble learning",
    "citation_count": 4,
    "authors": [
      "Gaman Mihaela",
      "Sebastian Cojocariu",
      "Radu Tudor Ionescu"
    ]
  },
  "https://aclanthology.org/2021.vardial-1.11": {
    "title": "Optimizing a Supervised Classifier for a Difficult Language Identification Problem",
    "volume": "workshop",
    "abstract": "This paper describes the system developed by the Laboratoire d'analyse statistique des textes for the Dravidian Language Identification (DLI) shared task of VarDial 2021. This task is particularly difficult because the materials consists of short YouTube comments, written in Roman script, from three closely related Dravidian languages, and a fourth category consisting of several other languages in varying proportions, all mixed with English. The proposed system is made up of a logistic regression model which uses as only features n-grams of characters with a maximum length of 5. After its optimization both in terms of the feature weighting and the classifier parameters, it ranked first in the challenge. The additional analyses carried out underline the importance of optimization, especially when the measure of effectiveness is the Macro-F1",
    "checked": true,
    "id": "f694ac8c857a2e4f963c26148bba7ba07f1d95b0",
    "semantic_title": "optimizing a supervised classifier for a difficult language identification problem",
    "citation_count": 8,
    "authors": [
      "Yves Bestgen"
    ]
  },
  "https://aclanthology.org/2021.vardial-1.12": {
    "title": "Comparing the Performance of CNNs and Shallow Models for Language Identification",
    "volume": "workshop",
    "abstract": "In this work we compare the performance of convolutional neural networks and shallow models on three out of the four language identification shared tasks proposed in the VarDial Evaluation Campaign 2021. In our experiments, convolutional neural networks and shallow models yielded comparable performance in the Romanian Dialect Identification (RDI) and the Dravidian Language Identification (DLI) shared tasks, after the training data was augmented, while an ensemble of support vector machines and Naïve Bayes models was the best performing model in the Uralic Language Identification (ULI) task. While the deep learning models did not achieve state-of-the-art performance at the tasks and tended to overfit the data, the ensemble method was one of two methods that beat the existing baseline for the first track of the ULI shared task",
    "checked": true,
    "id": "6a2089b6b65cc775cdc55f9617d89411bf401efa",
    "semantic_title": "comparing the performance of cnns and shallow models for language identification",
    "citation_count": 12,
    "authors": [
      "Andrea Ceolin"
    ]
  },
  "https://aclanthology.org/2021.vardial-1.13": {
    "title": "Dialect Identification through Adversarial Learning and Knowledge Distillation on Romanian BERT",
    "volume": "workshop",
    "abstract": "Dialect identification is a task with applicability in a vast array of domains, ranging from automatic speech recognition to opinion mining. This work presents our architectures used for the VarDial 2021 Romanian Dialect Identification subtask. We introduced a series of solutions based on Romanian or multilingual Transformers, as well as adversarial training techniques. At the same time, we experimented with a knowledge distillation tool in order to check whether a smaller model can maintain the performance of our best approach. Our best solution managed to obtain a weighted F1-score of 0.7324, allowing us to obtain the 2nd place on the leaderboard",
    "checked": true,
    "id": "6b43becda4f3c6d79571dcfad9619f30c6ff3846",
    "semantic_title": "dialect identification through adversarial learning and knowledge distillation on romanian bert",
    "citation_count": 13,
    "authors": [
      "George-Eduard Zaharia",
      "Andrei-Marius Avram",
      "Dumitru-Clementin Cercel",
      "Traian Rebedea"
    ]
  },
  "https://aclanthology.org/2021.vardial-1.14": {
    "title": "Comparing Approaches to Dravidian Language Identification",
    "volume": "workshop",
    "abstract": "This paper describes the submissions by team HWR to the Dravidian Language Identification (DLI) shared task organized at VarDial 2021 workshop. The DLI training set includes 16,674 YouTube comments written in Roman script containing code-mixed text with English and one of the three South Dravidian languages: Kannada, Malayalam, and Tamil. We submitted results generated using two models, a Naive Bayes classifier with adaptive language models, which has shown to obtain competitive performance in many language and dialect identification tasks, and a transformer-based model which is widely regarded as the state-of-the-art in a number of NLP tasks. Our first submission was sent in the closed submission track using only the training set provided by the shared task organisers, whereas the second submission is considered to be open as it used a pretrained model trained with external data. Our team attained shared second position in the shared task with the submission based on Naive Bayes. Our results reinforce the idea that deep learning methods are not as competitive in language identification related tasks as they are in many other text classification tasks",
    "checked": true,
    "id": "522744e66d36b981b5fd5b72f9a497051589695e",
    "semantic_title": "comparing approaches to dravidian language identification",
    "citation_count": 15,
    "authors": [
      "Tommi Jauhiainen",
      "Tharindu Ranasinghe",
      "Marcos Zampieri"
    ]
  },
  "https://aclanthology.org/2021.vardial-1.15": {
    "title": "N-gram and Neural Models for Uralic Language Identification: NRC at VarDial 2021",
    "volume": "workshop",
    "abstract": "We describe the systems developed by the National Research Council Canada for the Uralic language identification shared task at the 2021 VarDial evaluation campaign. We evaluated two different approaches to this task: a probabilistic classifier exploiting only character 5-grams as features, and a character-based neural network pre-trained through self-supervision, then fine-tuned on the language identification task. The former method turned out to perform better, which casts doubt on the usefulness of deep learning methods for language identification, where they have yet to convincingly and consistently outperform simpler and less costly classification algorithms exploiting n-gram features",
    "checked": true,
    "id": "e20af91d769b23969562d9d31ef8dd4280dc2ced",
    "semantic_title": "n-gram and neural models for uralic language identification: nrc at vardial 2021",
    "citation_count": 7,
    "authors": [
      "Gabriel Bernier-Colborne",
      "Serge Leger",
      "Cyril Goutte"
    ]
  },
  "https://aclanthology.org/2021.vardial-1.16": {
    "title": "Social Media Variety Geolocation with geoBERT",
    "volume": "workshop",
    "abstract": "This paper describes the Helsinki–Ljubljana contribution to the VarDial 2021 shared task on social media variety geolocation. Following our successful participation at VarDial 2020, we again propose constrained and unconstrained systems based on the BERT architecture. In this paper, we report experiments with different tokenization settings and different pre-trained models, and we contrast our parameter-free regression approach with various classification schemes proposed by other participants at VarDial 2020. Both the code and the best-performing pre-trained models are made freely available",
    "checked": true,
    "id": "92933250302d68a2f7c52bb83712c558e1da4d7c",
    "semantic_title": "social media variety geolocation with geobert",
    "citation_count": 9,
    "authors": [
      "Yves Scherrer",
      "Nikola Ljubešić"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.1": {
    "title": "QADI: Arabic Dialect Identification in the Wild",
    "volume": "workshop",
    "abstract": "Proper dialect identification is important for a variety of Arabic NLP applications. In this paper, we present a method for rapidly constructing a tweet dataset containing a wide range of country-level Arabic dialects —covering 18 different countries in the Middle East and North Africa region. Our method relies on applying multiple filters to identify users who belong to different countries based on their account descriptions and to eliminate tweets that either write mainly in Modern Standard Arabic or mostly use vulgar language. The resultant dataset contains 540k tweets from 2,525 users who are evenly distributed across 18 Arab countries. Using intrinsic evaluation, we show that the labels of a set of randomly selected tweets are 91.5% accurate. For extrinsic evaluation, we are able to build effective country level dialect identification on tweets with a macro-averaged F1-score of 60.6% across 18 classes",
    "checked": true,
    "id": "ce55b13f1006a1f1076e5f6ca0bb99e59dcfc631",
    "semantic_title": "qadi: arabic dialect identification in the wild",
    "citation_count": 53,
    "authors": [
      "Ahmed Abdelali",
      "Hamdy Mubarak",
      "Younes Samih",
      "Sabit Hassan",
      "Kareem Darwish"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.2": {
    "title": "DiaLex: A Benchmark for Evaluating Multidialectal Arabic Word Embeddings",
    "volume": "workshop",
    "abstract": "Word embeddings are a core component of modern natural language processing systems, making the ability to thoroughly evaluate them a vital task. We describe DiaLex, a benchmark for intrinsic evaluation of dialectal Arabic word embeddings. DiaLex covers five important Arabic dialects: Algerian, Egyptian, Lebanese, Syrian, and Tunisian. Across these dialects, DiaLex provides a testbank for six syntactic and semantic relations, namely male to female, singular to dual, singular to plural, antonym, comparative, and genitive to past tense. DiaLex thus consists of a collection of word pairs representing each of the six relations in each of the five dialects. To demonstrate the utility of DiaLex, we use it to evaluate a set of existing and new Arabic word embeddings that we developed. Beyond evaluation of word embeddings, DiaLex supports efforts to integrate dialects into the Arabic language curriculum. It can be easily translated into Modern Standard Arabic and English, which can be useful for evaluating word translation. Our benchmark, evaluation code, and new word embedding models will be publicly available",
    "checked": true,
    "id": "9bdb6f01a9cb5c91ccb503e75ab7f884adf13600",
    "semantic_title": "dialex: a benchmark for evaluating multidialectal arabic word embeddings",
    "citation_count": 1,
    "authors": [
      "Muhammad Abdul-Mageed",
      "Shady Elbassuoni",
      "Jad Doughman",
      "AbdelRahim Elmadany",
      "El Moatez Billah Nagoudi",
      "Yorgo Zoughby",
      "Ahmad Shaher",
      "Iskander Gaba",
      "Ahmed Helal",
      "Mohammed El-Razzaz"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.3": {
    "title": "Benchmarking Transformer-based Language Models for Arabic Sentiment and Sarcasm Detection",
    "volume": "workshop",
    "abstract": "The introduction of transformer-based language models has been a revolutionary step for natural language processing (NLP) research. These models, such as BERT, GPT and ELECTRA, led to state-of-the-art performance in many NLP tasks. Most of these models were initially developed for English and other languages followed later. Recently, several Arabic-specific models started emerging. However, there are limited direct comparisons between these models. In this paper, we evaluate the performance of 24 of these models on Arabic sentiment and sarcasm detection. Our results show that the models achieving the best performance are those that are trained on only Arabic data, including dialectal Arabic, and use a larger number of parameters, such as the recently released MARBERT. However, we noticed that AraELECTRA is one of the top performing models while being much more efficient in its computational cost. Finally, the experiments on AraGPT2 variants showed low performance compared to BERT models, which indicates that it might not be suitable for classification tasks",
    "checked": true,
    "id": "61186b6c692c738215f7aab6f2417e7db57cb6d5",
    "semantic_title": "benchmarking transformer-based language models for arabic sentiment and sarcasm detection",
    "citation_count": 49,
    "authors": [
      "Ibrahim Abu Farha",
      "Walid Magdy"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.4": {
    "title": "What does BERT Learn from Arabic Machine Reading Comprehension Datasets?",
    "volume": "workshop",
    "abstract": "In machine reading comprehension tasks, a model must extract an answer from the available context given a question and a passage. Recently, transformer-based pre-trained language models have achieved state-of-the-art performance in several natural language processing tasks. However, it is unclear whether such performance reflects true language understanding. In this paper, we propose adversarial examples to probe an Arabic pre-trained language model (AraBERT), leading to a significant performance drop over four Arabic machine reading comprehension datasets. We present a layer-wise analysis for the transformer's hidden states to offer insights into how AraBERT reasons to derive an answer. The experiments indicate that AraBERT relies on superficial cues and keyword matching rather than text understanding. Furthermore, hidden state visualization demonstrates that prediction errors can be recognized from vector representations in earlier layers",
    "checked": true,
    "id": "3285368076437486d9c47fa490bcd273b7a4d591",
    "semantic_title": "what does bert learn from arabic machine reading comprehension datasets?",
    "citation_count": 2,
    "authors": [
      "Eman Albilali",
      "Nora Altwairesh",
      "Manar Hosny"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.5": {
    "title": "Kawarith: an Arabic Twitter Corpus for Crisis Events",
    "volume": "workshop",
    "abstract": "Social media (SM) platforms such as Twitter provide large quantities of real-time data that can be leveraged during mass emergencies. Developing tools to support crisis-affected communities requires available datasets, which often do not exist for low resource languages. This paper introduces Kawarith a multi-dialect Arabic Twitter corpus for crisis events, comprising more than a million Arabic tweets collected during 22 crises that occurred between 2018 and 2020 and involved several types of hazard. Exploration of this content revealed the most discussed topics and information types, and the paper presents a labelled dataset from seven emergency events that serves as a gold standard for several tasks in crisis informatics research. Using annotated data from the same event, a BERT model is fine-tuned to classify tweets into different categories in the multi- label setting. Results show that BERT-based models yield good performance on this task even with small amounts of task-specific training data",
    "checked": true,
    "id": "e51ddd6d9a633fe0f410971860457cf5bb9dc28c",
    "semantic_title": "kawarith: an arabic twitter corpus for crisis events",
    "citation_count": 15,
    "authors": [
      "Alaa Alharbi",
      "Mark Lee"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.6": {
    "title": "Arabic Compact Language Modelling for Resource Limited Devices",
    "volume": "workshop",
    "abstract": "Natural language modelling has gained a lot of interest recently. The current state-of-the-art results are achieved by first training a very large language model and then fine-tuning it on multiple tasks. However, there is little work on smaller more compact language models for resource-limited devices or applications. Not to mention, how to efficiently train such models for a low-resource language like Arabic. In this paper, we investigate how such models can be trained in a compact way for Arabic. We also show how distillation and quantization can be applied to create even smaller models. Our experiments show that our largest model which is 2x smaller than the baseline can achieve better results on multiple tasks with 2x less data for pretraining",
    "checked": true,
    "id": "5599a7b4cbb0690e25ba0d3774f1bd7bfec5fadb",
    "semantic_title": "arabic compact language modelling for resource limited devices",
    "citation_count": 2,
    "authors": [
      "Zaid Alyafeai",
      "Irfan Ahmad"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.7": {
    "title": "Arabic Emoji Sentiment Lexicon (Arab-ESL): A Comparison between Arabic and European Emoji Sentiment Lexicons",
    "volume": "workshop",
    "abstract": "Emoji (the popular digital pictograms) are sometimes seen as a new kind of artificial and universally usable and consistent writing code. In spite of their assumed universality, there is some evidence that the sense of an emoji, specifically in regard to sentiment, may change from language to language and culture to culture. This paper investigates whether contextual emoji sentiment analysis is consistent across Arabic and European languages. To conduct this investigation, we, first, created the Arabic emoji sentiment lexicon (Arab-ESL). Then, we exploited an existing European emoji sentiment lexicon to compare the sentiment conveyed in each of the two families of language and culture (Arabic and European). The results show that the pairwise correlation between the two lexicons is consistent for emoji that represent, for instance, hearts, facial expressions, and body language. However, for a subset of emoji (those that represent objects, nature, symbols, and some human activities), there are large differences in the sentiment conveyed. More interestingly, an extremely high level of inconsistency has been shown with food emoji",
    "checked": true,
    "id": "cb0c004981c75250bfa8d474ad7e73d2ba64bfa6",
    "semantic_title": "arabic emoji sentiment lexicon (arab-esl): a comparison between arabic and european emoji sentiment lexicons",
    "citation_count": 8,
    "authors": [
      "Shatha Ali A. Hakami",
      "Robert Hendley",
      "Phillip Smith"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.8": {
    "title": "ArCOV19-Rumors: Arabic COVID-19 Twitter Dataset for Misinformation Detection",
    "volume": "workshop",
    "abstract": "In this paper we introduce ArCOV19-Rumors, an Arabic COVID-19 Twitter dataset for misinformation detection composed of tweets containing claims from 27th January till the end of April 2020. We collected 138 verified claims, mostly from popular fact-checking websites, and identified 9.4K relevant tweets to those claims. Tweets were manually-annotated by veracity to support research on misinformation detection, which is one of the major problems faced during a pandemic. ArCOV19-Rumors supports two levels of misinformation detection over Twitter: verifying free-text claims (called claim-level verification) and verifying claims expressed in tweets (called tweet-level verification). Our dataset covers, in addition to health, claims related to other topical categories that were influenced by COVID-19, namely, social, politics, sports, entertainment, and religious. Moreover, we present benchmarking results for tweet-level verification on the dataset. We experimented with SOTA models of versatile approaches that either exploit content, user profiles features, temporal features and propagation structure of the conversational threads for tweet verification",
    "checked": true,
    "id": "ad2714552d323a24d05986a452a879e3303b5c60",
    "semantic_title": "arcov19-rumors: arabic covid-19 twitter dataset for misinformation detection",
    "citation_count": 46,
    "authors": [
      "Fatima Haouari",
      "Maram Hasanain",
      "Reem Suwaileh",
      "Tamer Elsayed"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.9": {
    "title": "ArCOV-19: The First Arabic COVID-19 Twitter Dataset with Propagation Networks",
    "volume": "workshop",
    "abstract": "In this paper, we present ArCOV-19, an Arabic COVID-19 Twitter dataset that spans one year, covering the period from 27th of January 2020 till 31st of January 2021. ArCOV-19 is the first publicly-available Arabic Twitter dataset covering COVID-19 pandemic that includes about 2.7M tweets alongside the propagation networks of the most-popular subset of them (i.e., most-retweeted and -liked). The propagation networks include both retweetsand conversational threads (i.e., threads of replies). ArCOV-19 is designed to enable research under several domains including natural language processing, information retrieval, and social computing. Preliminary analysis shows that ArCOV-19 captures rising discussions associated with the first reported cases of the disease as they appeared in the Arab world. In addition to the source tweets and the propagation networks, we also release the search queries and the language-independent crawler used to collect the tweets to encourage the curation of similar datasets",
    "checked": true,
    "id": "b50671177e3368e6ef9854d5a8984f30b9572456",
    "semantic_title": "arcov-19: the first arabic covid-19 twitter dataset with propagation networks",
    "citation_count": 62,
    "authors": [
      "Fatima Haouari",
      "Maram Hasanain",
      "Reem Suwaileh",
      "Tamer Elsayed"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.10": {
    "title": "The Interplay of Variant, Size, and Task Type in Arabic Pre-trained Language Models",
    "volume": "workshop",
    "abstract": "In this paper, we explore the effects of language variants, data sizes, and fine-tuning task types in Arabic pre-trained language models. To do so, we build three pre-trained language models across three variants of Arabic: Modern Standard Arabic (MSA), dialectal Arabic, and classical Arabic, in addition to a fourth language model which is pre-trained on a mix of the three. We also examine the importance of pre-training data size by building additional models that are pre-trained on a scaled-down set of the MSA variant. We compare our different models to each other, as well as to eight publicly available models by fine-tuning them on five NLP tasks spanning 12 datasets. Our results suggest that the variant proximity of pre-training data to fine-tuning data is more important than the pre-training data size. We exploit this insight in defining an optimized system selection model for the studied tasks",
    "checked": true,
    "id": "a4ab1bd1501668e932c986725b33d065e4f0a233",
    "semantic_title": "the interplay of variant, size, and task type in arabic pre-trained language models",
    "citation_count": 140,
    "authors": [
      "Go Inoue",
      "Bashar Alhafni",
      "Nurpeiis Baimukan",
      "Houda Bouamor",
      "Nizar Habash"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.11": {
    "title": "Automatic Difficulty Classification of Arabic Sentences",
    "volume": "workshop",
    "abstract": "In this paper, we present a Modern Standard Arabic (MSA) Sentence difficulty classifier, which predicts the difficulty of sentences for language learners using either the CEFR proficiency levels or the binary classification as simple or complex. We compare the use of sentence embeddings of different kinds (fastText, mBERT , XLM-R and Arabic-BERT), as well as traditional language features such as POS tags, dependency trees, readability scores and frequency lists for language learners. Our best results have been achieved using fined-tuned Arabic-BERT. The accuracy of our 3-way CEFR classification is F-1 of 0.80 and 0.75 for Arabic-Bert and XLM-R classification respectively and 0.71 Spearman correlation for regression. Our binary difficulty classifier reaches F-1 0.94 and F-1 0.98 for sentence-pair semantic similarity classifier",
    "checked": true,
    "id": "1413990db540d8437f01b9b1e56df02e4bacb28f",
    "semantic_title": "automatic difficulty classification of arabic sentences",
    "citation_count": 7,
    "authors": [
      "Nouran Khallaf",
      "Serge Sharoff"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.12": {
    "title": "Dynamic Ensembles in Named Entity Recognition for Historical Arabic Texts",
    "volume": "workshop",
    "abstract": "The use of Named Entity Recognition (NER) over archaic Arabic texts is steadily increasing. However, most tools have been either developed for modern English or trained over English language documents and are limited over historical Arabic text. Even Arabic NER tools are often trained on modern web-sourced text, making their fit for a historical task questionable. To mitigate historic Arabic NER resource scarcity, we propose a dynamic ensemble model utilizing several learners. The dynamic aspect is achieved by utilizing predictors and features over NER algorithm results that identify which have performed better on a specific task in real-time. We evaluate our approach against state-of-the-art Arabic NER and static ensemble methods over a novel historical Arabic NER task we have created. Our results show that our approach improves upon the state-of-the-art and reaches a 0.8 F-score on this challenging task",
    "checked": true,
    "id": "695912b0c7bbe8b50beb6e897699aa3c11c112cb",
    "semantic_title": "dynamic ensembles in named entity recognition for historical arabic texts",
    "citation_count": 1,
    "authors": [
      "Muhammad Majadly",
      "Tomer Sagi"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.13": {
    "title": "Arabic Offensive Language on Twitter: Analysis and Experiments",
    "volume": "workshop",
    "abstract": "Detecting offensive language on Twitter has many applications ranging from detecting/predicting bullying to measuring polarization. In this paper, we focus on building a large Arabic offensive tweet dataset. We introduce a method for building a dataset that is not biased by topic, dialect, or target. We produce the largest Arabic dataset to date with special tags for vulgarity and hate speech. We thoroughly analyze the dataset to determine which topics, dialects, and gender are most associated with offensive tweets and how Arabic speakers useoffensive language. Lastly, we conduct many experiments to produce strong results (F1 =83.2) on the dataset using SOTA techniques",
    "checked": true,
    "id": "1e3170311bf21a70b6f974e40b8932fbc0f3052b",
    "semantic_title": "arabic offensive language on twitter: analysis and experiments",
    "citation_count": 125,
    "authors": [
      "Hamdy Mubarak",
      "Ammar Rashed",
      "Kareem Darwish",
      "Younes Samih",
      "Ahmed Abdelali"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.14": {
    "title": "Adult Content Detection on Arabic Twitter: Analysis and Experiments",
    "volume": "workshop",
    "abstract": "With Twitter being one of the most popular social media platforms in the Arab region, it is not surprising to find accounts that post adult content in Arabic tweets; despite the fact that these platforms dissuade users from such content. In this paper, we present a dataset of Twitter accounts that post adult content. We perform an in-depth analysis of the nature of this data and contrast it with normal tweet content. Additionally, we present extensive experiments with traditional machine learning models, deep neural networks and contextual embeddings to identify such accounts. We show that from user information alone, we can identify such accounts with F1 score of 94.7% (macro average). With the addition of only one tweet as input, the F1 score rises to 96.8%",
    "checked": true,
    "id": "7a10b20961c847e2fd40ca845849476910067c67",
    "semantic_title": "adult content detection on arabic twitter: analysis and experiments",
    "citation_count": 18,
    "authors": [
      "Hamdy Mubarak",
      "Sabit Hassan",
      "Ahmed Abdelali"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.15": {
    "title": "UL2C: Mapping User Locations to Countries on Arabic Twitter",
    "volume": "workshop",
    "abstract": "Mapping user locations to countries can be useful for many applications such as dialect identification, author profiling, recommendation system, etc. Twitter allows users to declare their locations as free text, and these user-declared locations are often noisy and hard to decipher automatically. In this paper, we present the largest manually labeled dataset for mapping user locations on Arabic Twitter to their corresponding countries. We build effective machine learning models that can automate this mapping with significantly better efficiency compared to libraries such as geopy. We also show that our dataset is more effective than data extracted from GeoNames geographical database in this task as the latter covers only locations written in formal ways",
    "checked": true,
    "id": "0c069221f8f7678eb2db505eddba453a993b0ab1",
    "semantic_title": "ul2c: mapping user locations to countries on arabic twitter",
    "citation_count": 3,
    "authors": [
      "Hamdy Mubarak",
      "Sabit Hassan"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.16": {
    "title": "Let-Mi: An Arabic Levantine Twitter Dataset for Misogynistic Language",
    "volume": "workshop",
    "abstract": "Online misogyny has become an increasing worry for Arab women who experience gender-based online abuse on a daily basis. Misogyny automatic detection systems can assist in the prohibition of anti-women Arabic toxic content. Developing such systems is hindered by the lack of the Arabic misogyny benchmark datasets. In this paper, we introduce an Arabic Levantine Twitter dataset for Misogynistic language (LeT-Mi) to be the first benchmark dataset for Arabic misogyny. We further provide a detailed review of the dataset creation and annotation phases. The consistency of the annotations for the proposed dataset was emphasized through inter-rater agreement evaluation measures. Moreover, Let-Mi was used as an evaluation dataset through binary/multi-/target classification tasks conducted by several state-of-the-art machine learning systems along with Multi-Task Learning (MTL) configuration. The obtained results indicated that the performances achieved by the used systems are consistent with state-of-the-art results for languages other than Arabic, while employing MTL improved the performance of the misogyny/target classification tasks",
    "checked": true,
    "id": "1473c05708bcb37cca42cfe38bc110a647326acb",
    "semantic_title": "let-mi: an arabic levantine twitter dataset for misogynistic language",
    "citation_count": 33,
    "authors": [
      "Hala Mulki",
      "Bilal Ghanem"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.17": {
    "title": "Empathetic BERT2BERT Conversational Model: Learning Arabic Language Generation with Little Data",
    "volume": "workshop",
    "abstract": "Enabling empathetic behavior in Arabic dialogue agents is an important aspect of building human-like conversational models. While Arabic Natural Language Processing has seen significant advances in Natural Language Understanding (NLU) with language models such as AraBERT, Natural Language Generation (NLG) remains a challenge. The shortcomings of NLG encoder-decoder models are primarily due to the lack of Arabic datasets suitable to train NLG models such as conversational agents. To overcome this issue, we propose a transformer-based encoder-decoder initialized with AraBERT parameters. By initializing the weights of the encoder and decoder with AraBERT pre-trained weights, our model was able to leverage knowledge transfer and boost performance in response generation. To enable empathy in our conversational model, we train it using the ArabicEmpatheticDialogues dataset and achieve high performance in empathetic response generation. Specifically, our model achieved a low perplexity value of 17.0 and an increase in 5 BLEU points compared to the previous state-of-the-art model. Also, our proposed model was rated highly by 85 human evaluators, validating its high capability in exhibiting empathy while generating relevant and fluent responses in open-domain settings",
    "checked": true,
    "id": "983921bd0ccaee71df7580ce13dd0d53dba5f368",
    "semantic_title": "empathetic bert2bert conversational model: learning arabic language generation with little data",
    "citation_count": 13,
    "authors": [
      "Tarek Naous",
      "Wissam Antoun",
      "Reem Mahmoud",
      "Hazem Hajj"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.18": {
    "title": "ALUE: Arabic Language Understanding Evaluation",
    "volume": "workshop",
    "abstract": "The emergence of Multi-task learning (MTL)models in recent years has helped push thestate of the art in Natural Language Un-derstanding (NLU). We strongly believe thatmany NLU problems in Arabic are especiallypoised to reap the benefits of such models. Tothis end we propose the Arabic Language Un-derstanding Evaluation Benchmark (ALUE),based on 8 carefully selected and previouslypublished tasks. For five of these, we providenew privately held evaluation datasets to en-sure the fairness and validity of our benchmark. We also provide a diagnostic dataset to helpresearchers probe the inner workings of theirmodels.Our initial experiments show thatMTL models outperform their singly trainedcounterparts on most tasks. But in order to en-tice participation from the wider community,we stick to publishing singly trained baselinesonly. Nonetheless, our analysis reveals thatthere is plenty of room for improvement inArabic NLU. We hope that ALUE will playa part in helping our community realize someof these improvements. Interested researchersare invited to submit their results to our online,and publicly accessible leaderboard",
    "checked": true,
    "id": "a458ea049f87db9d2c856f3554f1819a27d4e676",
    "semantic_title": "alue: arabic language understanding evaluation",
    "citation_count": 30,
    "authors": [
      "Haitham Seelawi",
      "Ibraheem Tuffaha",
      "Mahmoud Gzawi",
      "Wael Farhan",
      "Bashar Talafha",
      "Riham Badawi",
      "Zyad Sober",
      "Oday Al-Dweik",
      "Abed Alhakim Freihat",
      "Hussein Al-Natsheh"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.19": {
    "title": "Quranic Verses Semantic Relatedness Using AraBERT",
    "volume": "workshop",
    "abstract": "Bidirectional Encoder Representations from Transformers (BERT) has gained popularity in recent years producing state-of-the-art performances across Natural Language Processing tasks. In this paper, we used AraBERT language model to classify pairs of verses provided by the QurSim dataset to either be semantically related or not. We have pre-processed The QurSim dataset and formed three datasets for comparisons. Also, we have used both versions of AraBERT, which are AraBERTv02 and AraBERTv2, to recognise which version performs the best with the given datasets. The best results was AraBERTv02 with 92% accuracy score using a dataset comprised of label ‘2' and label '-1', the latter was generated outside of QurSim dataset",
    "checked": true,
    "id": "639b0d3f0933649e79bad41469a2b2941ecfa70c",
    "semantic_title": "quranic verses semantic relatedness using arabert",
    "citation_count": 6,
    "authors": [
      "Abdullah Alsaleh",
      "Eric Atwell",
      "Abdulrahman Altahhan"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.20": {
    "title": "AraELECTRA: Pre-Training Text Discriminators for Arabic Language Understanding",
    "volume": "workshop",
    "abstract": "Advances in English language representation enabled a more sample-efficient pre-training task by Efficiently Learning an Encoder that Classifies Token Replacements Accurately (ELECTRA). Which, instead of training a model to recover masked tokens, it trains a discriminator model to distinguish true input tokens from corrupted tokens that were replaced by a generator network. On the other hand, current Arabic language representation approaches rely only on pretraining via masked language modeling. In this paper, we develop an Arabic language representation model, which we name AraELECTRA. Our model is pretrained using the replaced token detection objective on large Arabic text corpora. We evaluate our model on multiple Arabic NLP tasks, including reading comprehension, sentiment analysis, and named-entity recognition and we show that AraELECTRA outperforms current state-of-the-art Arabic language representation models, given the same pretraining data and with even a smaller model size",
    "checked": true,
    "id": "aea9f18d70a78d39ca927f2baa143e084c486086",
    "semantic_title": "araelectra: pre-training text discriminators for arabic language understanding",
    "citation_count": 78,
    "authors": [
      "Wissam Antoun",
      "Fady Baly",
      "Hazem Hajj"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.21": {
    "title": "AraGPT2: Pre-Trained Transformer for Arabic Language Generation",
    "volume": "workshop",
    "abstract": "Recently, pre-trained transformer-based architectures have proven to be very efficient at language modeling and understanding, given that they are trained on a large enough corpus. Applications in language generation for Arabic are still lagging in comparison to other NLP advances primarily due to the lack of advanced Arabic language generation models. In this paper, we develop the first advanced Arabic language generation model, AraGPT2, trained from scratch on a large Arabic corpus of internet text and news articles. Our largest model, AraGPT2-mega, has 1.46 billion parameters, which makes it the largest Arabic language model available. The mega model was evaluated and showed success on different tasks including synthetic news generation, and zero-shot question answering. For text generation, our best model achieves a perplexity of 29.8 on held-out Wikipedia articles. A study conducted with human evaluators showed the significant success of AraGPT2-mega in generating news articles that are difficult to distinguish from articles written by humans. We thus develop and release an automatic discriminator model with a 98% percent accuracy in detecting model-generated text. The models are also publicly available, hoping to encourage new research directions and applications for Arabic NLP",
    "checked": true,
    "id": "c342798bafc1eaaa60c652fc90fd738941542133",
    "semantic_title": "aragpt2: pre-trained transformer for arabic language generation",
    "citation_count": 67,
    "authors": [
      "Wissam Antoun",
      "Fady Baly",
      "Hazem Hajj"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.22": {
    "title": "QuranTree.jl: A Julia Package for Quranic Arabic Corpus",
    "volume": "workshop",
    "abstract": "QuranTree.jl is an open-source package for working with the Quranic Arabic Corpus (Dukes and Habash, 2010). It aims to provide Julia APIs as an alternative to the Java APIs of JQuranTree. QuranTree.jl currently offers functionalities for intuitive indexing of chapters, verses, words and parts of words of the Qur'an; for creating custom transliteration; for character dediacritization and normalization; and, for handling the morphological features. Lastly, it can work well with Julia's TextAnalysis.jl and Python's CAMeL Tools",
    "checked": true,
    "id": "81c850f8c9af695f7ee0f9e7090c25fbc0c5c723",
    "semantic_title": "qurantree.jl: a julia package for quranic arabic corpus",
    "citation_count": 0,
    "authors": [
      "Al-Ahmadgaid Asaad"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.23": {
    "title": "Automatic Romanization of Arabic Bibliographic Records",
    "volume": "workshop",
    "abstract": "International library standards require cataloguers to tediously input Romanization of their catalogue records for the benefit of library users without specific language expertise. In this paper, we present the first reported results on the task of automatic Romanization of undiacritized Arabic bibliographic entries. This complex task requires the modeling of Arabic phonology, morphology, and even semantics. We collected a 2.5M word corpus of parallel Arabic and Romanized bibliographic entries, and benchmarked a number of models that vary in terms of complexity and resource dependence. Our best system reaches 89.3% exact word Romanization on a blind test set. We make our data and code publicly available",
    "checked": true,
    "id": "925cfcd5077f55dc1770ec0b7480090e8f997212",
    "semantic_title": "automatic romanization of arabic bibliographic records",
    "citation_count": 0,
    "authors": [
      "Fadhl Eryani",
      "Nizar Habash"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.24": {
    "title": "SERAG: Semantic Entity Retrieval from Arabic Knowledge Graphs",
    "volume": "workshop",
    "abstract": "Knowledge graphs (KGs) are widely used to store and access information about entities and their relationships. Given a query, the task of entity retrieval from a KG aims at presenting a ranked list of entities relevant to the query. Lately, an increasing number of models for entity retrieval have shown a significant improvement over traditional methods. These models, however, were developed for English KGs. In this work, we build on one such system, named KEWER, to propose SERAG (Semantic Entity Retrieval from Arabic knowledge Graphs). Like KEWER, SERAG uses random walks to generate entity embeddings. DBpedia-Entity v2 is considered the standard test collection for entity retrieval. We discuss the challenges of using it for non-English languages in general and Arabic in particular. We provide an Arabic version of this standard collection, and use it to evaluate SERAG. SERAG is shown to significantly outperform the popular BM25 model thanks to its multi-hop reasoning",
    "checked": true,
    "id": "5ab01df052881d6a24b3d1aa214043d6ab7378b1",
    "semantic_title": "serag: semantic entity retrieval from arabic knowledge graphs",
    "citation_count": 3,
    "authors": [
      "Saher Esmeir"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.25": {
    "title": "Introducing A large Tunisian Arabizi Dialectal Dataset for Sentiment Analysis",
    "volume": "workshop",
    "abstract": "On various Social Media platforms, people, tend to use the informal way to communicate, or write posts and comments: their local dialects. In Africa, more than 1500 dialects and languages exist. Particularly, Tunisians talk and write informally using Latin letters and numbers rather than Arabic ones. In this paper, we introduce a large common-crawl-based Tunisian Arabizi dialectal dataset dedicated for Sentiment Analysis. The dataset consists of a total of 100k comments (about movies, politic, sport, etc.) annotated manually by Tunisian native speakers as Positive, negative and Neutral. We evaluate our dataset on sentiment analysis task using the Bidirectional Encoder Representations from Transformers (BERT) as a contextual language model in its multilingual version (mBERT) as an embedding technique then combining mBERT with Convolutional Neural Network (CNN) as classifier. The dataset is publicly available",
    "checked": true,
    "id": "297d4d84d7d5abdeb4cd6c2958eab51b05786ea7",
    "semantic_title": "introducing a large tunisian arabizi dialectal dataset for sentiment analysis",
    "citation_count": 6,
    "authors": [
      "Chayma Fourati",
      "Hatem Haddad",
      "Abir Messaoudi",
      "Moez BenHajhmida",
      "Aymen Ben Elhaj Mabrouk",
      "Malek Naski"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.26": {
    "title": "AraFacts: The First Large Arabic Dataset of Naturally Occurring Claims",
    "volume": "workshop",
    "abstract": "We introduce AraFacts, the first large Arabic dataset of naturally occurring claims collected from 5 Arabic fact-checking websites, e.g., Fatabyyano and Misbar, and covering claims since 2016. Our dataset consists of 6,121 claims along with their factual labels and additional metadata, such as fact-checking article content, topical category, and links to posts or Web pages spreading the claim. Since the data is obtained from various fact-checking websites, we standardize the original claim labels to provide a unified label rating for all claims. Moreover, we provide revealing dataset statistics and motivate its use by suggesting possible research applications. The dataset is made publicly available for the research community",
    "checked": true,
    "id": "98ccb4cb22684ca25c8cf483c18ed5553b2a61c1",
    "semantic_title": "arafacts: the first large arabic dataset of naturally occurring claims",
    "citation_count": 18,
    "authors": [
      "Zien Sheikh Ali",
      "Watheq Mansour",
      "Tamer Elsayed",
      "Abdulaziz Al‐Ali"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.27": {
    "title": "Improving Cross-Lingual Transfer for Event Argument Extraction with Language-Universal Sentence Structures",
    "volume": "workshop",
    "abstract": "We study the problem of Cross-lingual Event Argument Extraction (CEAE). The task aims to predict argument roles of entity mentions for events in text, whose language is different from the language that a predictive model has been trained on. Previous work on CEAE has shown the cross-lingual benefits of universal dependency trees in capturing shared syntactic structures of sentences across languages. In particular, this work exploits the existence of the syntactic connections between the words in the dependency trees as the anchor knowledge to transfer the representation learning across languages for CEAE models (i.e., via graph convolutional neural networks – GCNs). In this paper, we introduce two novel sources of language-independent information for CEAE models based on the semantic similarity and the universal dependency relations of the word pairs in different languages. We propose to use the two sources of information to produce shared sentence structures to bridge the gap between languages and improve the cross-lingual performance of the CEAE models. Extensive experiments are conducted with Arabic, Chinese, and English to demonstrate the effectiveness of the proposed method for CEAE",
    "checked": true,
    "id": "f851a5aa8e6a203bc9de23b1a845e8653556ddd5",
    "semantic_title": "improving cross-lingual transfer for event argument extraction with language-universal sentence structures",
    "citation_count": 10,
    "authors": [
      "Minh Van Nguyen",
      "Thien Huu Nguyen"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.28": {
    "title": "NADI 2021: The Second Nuanced Arabic Dialect Identification Shared Task",
    "volume": "workshop",
    "abstract": "We present the findings and results of theSecond Nuanced Arabic Dialect IdentificationShared Task (NADI 2021). This Shared Taskincludes four subtasks: country-level ModernStandard Arabic (MSA) identification (Subtask1.1), country-level dialect identification (Subtask1.2), province-level MSA identification (Subtask2.1), and province-level sub-dialect identifica-tion (Subtask 2.2). The shared task dataset cov-ers a total of 100 provinces from 21 Arab coun-tries, collected from the Twitter domain. A totalof 53 teams from 23 countries registered to par-ticipate in the tasks, thus reflecting the interestof the community in this area. We received 16submissions for Subtask 1.1 from five teams, 27submissions for Subtask 1.2 from eight teams,12 submissions for Subtask 2.1 from four teams,and 13 Submissions for subtask 2.2 from fourteams",
    "checked": true,
    "id": "56ba945a373b8e45ab77c3eb5aa6cdf670af8b39",
    "semantic_title": "nadi 2021: the second nuanced arabic dialect identification shared task",
    "citation_count": 61,
    "authors": [
      "Muhammad Abdul-Mageed",
      "Chiyu Zhang",
      "AbdelRahim Elmadany",
      "Houda Bouamor",
      "Nizar Habash"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.29": {
    "title": "Adapting MARBERT for Improved Arabic Dialect Identification: Submission to the NADI 2021 Shared Task",
    "volume": "workshop",
    "abstract": "In this paper, we tackle the Nuanced Arabic Dialect Identification (NADI) shared task (Abdul-Mageed et al., 2021) and demonstrate state-of-the-art results on all of its four subtasks. Tasks are to identify the geographic origin of short Dialectal (DA) and Modern Standard Arabic (MSA) utterances at the levels of both country and province. Our final model is an ensemble of variants built on top of MARBERT that achieves an F1-score of 34.03% for DA at the country-level development set—an improvement of 7.63% from previous work",
    "checked": true,
    "id": "54712ea90be0bf41bb39745240a371f2de41eca6",
    "semantic_title": "adapting marbert for improved arabic dialect identification: submission to the nadi 2021 shared task",
    "citation_count": 15,
    "authors": [
      "Badr AlKhamissi",
      "Mohamed Gabr",
      "Muhammad ElNokrashy",
      "Khaled Essam"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.30": {
    "title": "Country-level Arabic Dialect Identification Using Small Datasets with Integrated Machine Learning Techniques and Deep Learning Models",
    "volume": "workshop",
    "abstract": "Arabic is characterised by a considerable number of varieties including spoken dialects. In this paper, we presented our models developed to participate in the NADI subtask 1.2 that requires building a system to distinguish between 21 country-level dialects. We investigated several classical machine learning approaches and deep learning models using small datasets. We examined an integration technique between two machine learning approaches. Additionally, we created dictionaries automatically based on Pointwise Mutual Information and labelled datasets, which enriched the feature space when training models. A semi-supervised learning approach was also examined and compared to other methods that exploit large unlabelled datasets, such as building pre-trained word embeddings. Our winning model was the Support Vector Machine with dictionary-based features and Pointwise Mutual Information values, achieving an 18.94% macros-average F1-score",
    "checked": true,
    "id": "fa43c021e37d3703a3b3d11dc9b95880fe6ebafc",
    "semantic_title": "country-level arabic dialect identification using small datasets with integrated machine learning techniques and deep learning models",
    "citation_count": 3,
    "authors": [
      "Maha J. Althobaiti"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.31": {
    "title": "BERT-based Multi-Task Model for Country and Province Level MSA and Dialectal Arabic Identification",
    "volume": "workshop",
    "abstract": "Dialect and standard language identification are crucial tasks for many Arabic natural language processing applications. In this paper, we present our deep learning-based system, submitted to the second NADI shared task for country-level and province-level identification of Modern Standard Arabic (MSA) and Dialectal Arabic (DA). The system is based on an end-to-end deep Multi-Task Learning (MTL) model to tackle both country-level and province-level MSA/DA identification. The latter MTL model consists of a shared Bidirectional Encoder Representation Transformers (BERT) encoder, two task-specific attention layers, and two classifiers. Our key idea is to leverage both the task-discriminative and the inter-task shared features for country and province MSA/DA identification. The obtained results show that our MTL model outperforms single-task models on most subtasks",
    "checked": true,
    "id": "2dee2b9361c6cb59282971e249b53ae9fe67f796",
    "semantic_title": "bert-based multi-task model for country and province level msa and dialectal arabic identification",
    "citation_count": 10,
    "authors": [
      "Abdellah El Mekki",
      "Abdelkader El Mahdaouy",
      "Kabil Essefar",
      "Nabil El Mamoun",
      "Ismail Berrada",
      "Ahmed Khoumsi"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.32": {
    "title": "Country-level Arabic Dialect Identification using RNNs with and without Linguistic Features",
    "volume": "workshop",
    "abstract": "This work investigates the value of augmenting recurrent neural networks with feature engineering for the Second Nuanced Arabic Dialect Identification (NADI) Subtask 1.2: Country-level DA identification. We compare the performance of a simple word-level LSTM using pretrained embeddings with one enhanced using feature embeddings for engineered linguistic features. Our results show that the addition of explicit features to the LSTM is detrimental to performance. We attribute this performance loss to the bivalency of some linguistic items in some text, ubiquity of topics, and participant mobility",
    "checked": true,
    "id": "ae5822094b6902900696a0dc4b06b39582b3e48c",
    "semantic_title": "country-level arabic dialect identification using rnns with and without linguistic features",
    "citation_count": 5,
    "authors": [
      "Elsayed Issa",
      "Mohammed AlShakhori1",
      "Reda Al-Bahrani",
      "Gus Hahn-Powell"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.33": {
    "title": "Arabic Dialect Identification based on a Weighted Concatenation of TF-IDF Features",
    "volume": "workshop",
    "abstract": "In this paper, we analyze the impact of the weighted concatenation of TF-IDF features for the Arabic Dialect Identification task while we participated in the NADI2021 shared task. This study is performed for two subtasks: subtask 1.1 (country-level MSA) and subtask 1.2 (country-level DA) identification. The classifiers supporting our comparative study are Linear Support Vector Classification (LSVC), Linear Regression (LR), Perceptron, Stochastic Gradient Descent (SGD), Passive Aggressive (PA), Complement Naive Bayes (CNB), MutliLayer Perceptron (MLP), and RidgeClassifier. In the evaluation phase, our system gives F1 scores of 14.87% and 21.49%, for country-level MSA and DA identification respectively, which is very close to the average F1 scores achieved by the submitted systems and recorded for both subtasks (18.70% and 24.23%)",
    "checked": true,
    "id": "1fe0d9c4e6b209c4cc796626d8979a7c3988b412",
    "semantic_title": "arabic dialect identification based on a weighted concatenation of tf-idf features",
    "citation_count": 4,
    "authors": [
      "Mohamed Lichouri",
      "Mourad Abbas",
      "Khaled Lounnas",
      "Besma Benaziz",
      "Aicha Zitouni"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.34": {
    "title": "Machine Learning-Based Approach for Arabic Dialect Identification",
    "volume": "workshop",
    "abstract": "This paper describes our systems submitted to the Second Nuanced Arabic Dialect Identification Shared Task (NADI 2021). Dialect identification is the task of automatically detecting the source variety of a given text or speech segment. There are four subtasks, two subtasks for country-level identification and the other two subtasks for province-level identification. The data in this task covers a total of 100 provinces from all 21 Arab countries and come from the Twitter domain. The proposed systems depend on five machine-learning approaches namely Complement Naïve Bayes, Support Vector Machine, Decision Tree, Logistic Regression and Random Forest Classifiers. F1 macro-averaged score of Naïve Bayes classifier outperformed all other classifiers for development and test data",
    "checked": true,
    "id": "e636ddb8a51a89693551cd7c4d47358d15df0746",
    "semantic_title": "machine learning-based approach for arabic dialect identification",
    "citation_count": 8,
    "authors": [
      "Hamada Nayel",
      "Ahmed Hassan",
      "Mahmoud Sobhi",
      "Ahmed El-Sawy"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.35": {
    "title": "Dialect Identification in Nuanced Arabic Tweets Using Farasa Segmentation and AraBERT",
    "volume": "workshop",
    "abstract": "This paper presents our approach to address the EACL WANLP-2021 Shared Task 1: Nuanced Arabic Dialect Identification (NADI). The task is aimed at developing a system that identifies the geographical location(country/province) from where an Arabic tweet in the form of modern standard Arabic or dialect comes from. We solve the task in two parts. The first part involves pre-processing the provided dataset by cleaning, adding and segmenting various parts of the text. This is followed by carrying out experiments with different versions of two Transformer based models, AraBERT and AraELECTRA. Our final approach achieved macro F1-scores of 0.216, 0.235, 0.054, and 0.043 in the four subtasks, and we were ranked second in MSA identification subtasks and fourth in DA identification subtasks",
    "checked": true,
    "id": "e694ab89eee01db100ad52dff6a58eda4a92585c",
    "semantic_title": "dialect identification in nuanced arabic tweets using farasa segmentation and arabert",
    "citation_count": 6,
    "authors": [
      "Anshul Wadhawan"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.36": {
    "title": "Overview of the WANLP 2021 Shared Task on Sarcasm and Sentiment Detection in Arabic",
    "volume": "workshop",
    "abstract": "This paper provides an overview of the WANLP 2021 shared task on sarcasm and sentiment detection in Arabic. The shared task has two subtasks: sarcasm detection (subtask 1) and sentiment analysis (subtask 2). This shared task aims to promote and bring attention to Arabic sarcasm detection, which is crucial to improve the performance in other tasks such as sentiment analysis. The dataset used in this shared task, namely ArSarcasm-v2, consists of 15,548 tweets labelled for sarcasm, sentiment and dialect. We received 27 and 22 submissions for subtasks 1 and 2 respectively. Most of the approaches relied on using and fine-tuning pre-trained language models such as AraBERT and MARBERT. The top achieved results for the sarcasm detection and sentiment analysis tasks were 0.6225 F1-score and 0.748 F1-PN respectively",
    "checked": true,
    "id": "aa7c5d2ff7ac71cca78987fd1bb1d963cd53c407",
    "semantic_title": "overview of the wanlp 2021 shared task on sarcasm and sentiment detection in arabic",
    "citation_count": 62,
    "authors": [
      "Ibrahim Abu Farha",
      "Wajdi Zaghouani",
      "Walid Magdy"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.37": {
    "title": "WANLP 2021 Shared-Task: Towards Irony and Sentiment Detection in Arabic Tweets using Multi-headed-LSTM-CNN-GRU and MaRBERT",
    "volume": "workshop",
    "abstract": "Irony and Sentiment detection is important to understand people's behavior and thoughts. Thus it has become a popular task in natural language processing (NLP). This paper presents results and main findings in WANLP 2021 shared tasks one and two. The task was based on the ArSarcasm-v2 dataset (Abu Farha et al., 2021). In this paper, we describe our system Multi-headed-LSTM-CNN-GRU and also MARBERT (Abdul-Mageed et al., 2021) submitted for the shared task, ranked 10 out of 27 in shared task one achieving 0.5662 F1-Sarcasm and ranked 3 out of 22 in shared task two achieving 0.7321 F1-PN under CodaLab username \"rematchka\". We experimented with various models and the two best performing models are a Multi-headed CNN-LSTM-GRU in which we used prepossessed text and emoji presented from tweets and MARBERT",
    "checked": true,
    "id": "d3420fbef4506746e4af40e380c5b66833baaeb1",
    "semantic_title": "wanlp 2021 shared-task: towards irony and sentiment detection in arabic tweets using multi-headed-lstm-cnn-gru and marbert",
    "citation_count": 11,
    "authors": [
      "Reem Abdel-Salam"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.38": {
    "title": "Sarcasm and Sentiment Detection In Arabic Tweets Using BERT-based Models and Data Augmentation",
    "volume": "workshop",
    "abstract": "In this paper, we describe our efforts on the shared task of sarcasm and sentiment detection in Arabic (Abu Farha et al., 2021). The shared task consists of two sub-tasks: Sarcasm Detection (Subtask 1) and Sentiment Analysis (Subtask 2). Our experiments were based on fine-tuning seven BERT-based models with data augmentation to solve the imbalanced data problem. For both tasks, the MARBERT BERT-based model with data augmentation outperformed other models with an increase of the F-score by 15% for both tasks which shows the effectiveness of our approach",
    "checked": true,
    "id": "d774a1ff5c12158728dcf45898914f67e2a91ffc",
    "semantic_title": "sarcasm and sentiment detection in arabic tweets using bert-based models and data augmentation",
    "citation_count": 30,
    "authors": [
      "Abeer Abuzayed",
      "Hend Al-Khalifa"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.39": {
    "title": "Multi-task Learning Using a Combination of Contextualised and Static Word Embeddings for Arabic Sarcasm Detection and Sentiment Analysis",
    "volume": "workshop",
    "abstract": "Sarcasm detection and sentiment analysis are important tasks in Natural Language Understanding. Sarcasm is a type of expression where the sentiment polarity is flipped by an interfering factor. In this study, we exploited this relationship to enhance both tasks by proposing a multi-task learning approach using a combination of static and contextualised embeddings. Our proposed system achieved the best result in the sarcasm detection subtask",
    "checked": true,
    "id": "cec0e64398bd9d6e682953d17a90c4d36dd66d79",
    "semantic_title": "multi-task learning using a combination of contextualised and static word embeddings for arabic sarcasm detection and sentiment analysis",
    "citation_count": 16,
    "authors": [
      "Abdullah I. Alharbi",
      "Mark Lee"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.40": {
    "title": "ArSarcasm Shared Task: An Ensemble BERT Model for SarcasmDetection in Arabic Tweets",
    "volume": "workshop",
    "abstract": "Detecting Sarcasm has never been easy for machines to process. In this work, we present our submission of the sub-task1 of the shared task on sarcasm and sentiment detection in Arabic organized by the 6th Workshop for Arabic Natural Language Processing. In this work, we explored different approaches based on BERT models. First, we fine-tuned the AraBERTv02 model for the sarcasm detection task. Then, we used the Sentence-BERT model trained with contrastive learning to extract representative tweet embeddings. Finally, inspired by how the human brain comprehends the surface and the implicit meanings of sarcastic tweets, we combined the sentence embedding with the fine-tuned AraBERTv02 to further boost the performance of the model. Through the ensemble of the two models, our team ranked 5th out of 27 teams on the shared task of sarcasm detection in Arabic, with an F1-score of %59.89 on the official test data. The obtained result is %2.36 lower than the 1st place which confirms the capabilities of the employed combined model in detecting sarcasm",
    "checked": true,
    "id": "bdf9123f72ab738a8083b29748fce5d4835381d7",
    "semantic_title": "arsarcasm shared task: an ensemble bert model for sarcasmdetection in arabic tweets",
    "citation_count": 12,
    "authors": [
      "Laila Bashmal",
      "Daliyah AlZeer"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.41": {
    "title": "Sarcasm and Sentiment Detection in Arabic: investigating the interest of character-level features",
    "volume": "workshop",
    "abstract": "We present three methods developed for the Shared Task on Sarcasm and Sentiment Detection in Arabic. We present a baseline that uses character n-gram features. We also propose two more sophisticated methods: a recurrent neural network with a word level representation and an ensemble classifier relying on word and character-level features. We chose to present results from an ensemble classifier but it was not very successful as compared to the best systems : 22th/37 on sarcasm detection and 15th/22 on sentiment detection. It finally appeared that our baseline could have been improved and beat those results",
    "checked": true,
    "id": "8ae39c0b31c47ba097888df35fea350df691c7c5",
    "semantic_title": "sarcasm and sentiment detection in arabic: investigating the interest of character-level features",
    "citation_count": 2,
    "authors": [
      "Dhaou Ghoul",
      "Gaël Lejeune"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.42": {
    "title": "Deep Multi-Task Model for Sarcasm Detection and Sentiment Analysis in Arabic Language",
    "volume": "workshop",
    "abstract": "The prominence of figurative language devices, such as sarcasm and irony, poses serious challenges for Arabic Sentiment Analysis (SA). While previous research works tackle SA and sarcasm detection separately, this paper introduces an end-to-end deep Multi-Task Learning (MTL) model, allowing knowledge interaction between the two tasks. Our MTL model's architecture consists of a Bidirectional Encoder Representation from Transformers (BERT) model, a multi-task attention interaction module, and two task classifiers. The overall obtained results show that our proposed model outperforms its single-task and MTL counterparts on both sarcasm and sentiment detection subtasks",
    "checked": true,
    "id": "b91260d212e04753bf283e75cf9802b2808acdae",
    "semantic_title": "deep multi-task model for sarcasm detection and sentiment analysis in arabic language",
    "citation_count": 26,
    "authors": [
      "Abdelkader El Mahdaouy",
      "Abdellah El Mekki",
      "Kabil Essefar",
      "Nabil El Mamoun",
      "Ismail Berrada",
      "Ahmed Khoumsi"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.43": {
    "title": "A Contextual Word Embedding for Arabic Sarcasm Detection with Random Forests",
    "volume": "workshop",
    "abstract": "Sarcasm detection is of great importance in understanding people's true sentiments and opinions. Many online feedbacks, reviews, social media comments, etc. are sarcastic. Several researches have already been done in this field, but most researchers studied the English sarcasm analysis compared to the researches are done in Arabic sarcasm analysis because of the Arabic language challenges. In this paper, we propose a new approach for improving Arabic sarcasm detection. Our approach is using data augmentation, contextual word embedding and random forests model to get the best results. Our accuracy in the shared task on sarcasm and sentiment detection in Arabic was 0.5189 for F1-sarcastic as the official metric using the shared dataset ArSarcasmV2 (Abu Farha, et al., 2021)",
    "checked": true,
    "id": "b223b62c535d7637a6cc70470d94c3b251199d8c",
    "semantic_title": "a contextual word embedding for arabic sarcasm detection with random forests",
    "citation_count": 14,
    "authors": [
      "Hazem Elgabry",
      "Shimaa Attia",
      "Ahmed Abdel-Rahman",
      "Ahmed Abdel-Ate",
      "Sandra Girgis"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.44": {
    "title": "SarcasmDet at Sarcasm Detection Task 2021 in Arabic using AraBERT Pretrained Model",
    "volume": "workshop",
    "abstract": "This paper presents one of the top five winning solutions for the Shared Task on Sarcasm and Sentiment Detection in Arabic (Subtask-1 Sarcasm Detection). The goal of the task is to identify whether a tweet is sarcastic or not. Our solution has been developed using ensemble technique with AraBERT pre-trained model. We describe the architecture of the submitted solution in the shared task. We also provide the experiments and the hyperparameter tuning that lead to this result. Besides, we discuss and analyze the results by comparing all the models that we trained or tested to achieve a better score in a table design. Our model is ranked fifth out of 27 teams with an F1 score of 0.5985. It is worth mentioning that our model achieved the highest accuracy score of 0.7830",
    "checked": true,
    "id": "b68d86cbd9baae774213ae524b2646ca599f9876",
    "semantic_title": "sarcasmdet at sarcasm detection task 2021 in arabic using arabert pretrained model",
    "citation_count": 21,
    "authors": [
      "Dalya Faraj",
      "Dalya Faraj",
      "Malak Abdullah"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.45": {
    "title": "Sarcasm and Sentiment Detection in Arabic language A Hybrid Approach Combining Embeddings and Rule-based Features",
    "volume": "workshop",
    "abstract": "This paper presents the ArabicProcessors team's system designed for sarcasm (subtask 1) and sentiment (subtask 2) detection shared task. We created a hybrid system by combining rule-based features and both static and dynamic embeddings using transformers and deep learning. The system's architecture is an ensemble of Naive bayes, MarBERT and Mazajak embedding. This process scored an F1-score of 51% on sarcasm and 71% for sentiment detection",
    "checked": true,
    "id": "c11e291ee2600f867c2cfc3adbb261c57d2474eb",
    "semantic_title": "sarcasm and sentiment detection in arabic language a hybrid approach combining embeddings and rule-based features",
    "citation_count": 11,
    "authors": [
      "Kamel Gaanoun",
      "Imade Benelallam"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.46": {
    "title": "Combining Context-Free and Contextualized Representations for Arabic Sarcasm Detection and Sentiment Identification",
    "volume": "workshop",
    "abstract": "Since their inception, transformer-based language models have led to impressive performance gains across multiple natural language processing tasks. For Arabic, the current state-of-the-art results on most datasets are achieved by the AraBERT language model. Notwithstanding these recent advancements, sarcasm and sentiment detection persist to be challenging tasks in Arabic, given the language's rich morphology, linguistic disparity and dialectal variations. This paper proffers team SPPU-AASM's submission for the WANLP ArSarcasm shared-task 2021, which centers around the sarcasm and sentiment polarity detection of Arabic tweets. The study proposes a hybrid model, combining sentence representations from AraBERT with static word vectors trained on Arabic social media corpora. The proposed system achieves a F1-sarcastic score of 0.62 and a F-PN score of 0.715 for the sarcasm and sentiment detection tasks, respectively. Simulation results show that the proposed system outperforms multiple existing approaches for both the tasks, suggesting that the amalgamation of context-free and context-dependent text representations can help capture complementary facets of word meaning in Arabic. The system ranked second and tenth in the respective sub-tasks of sarcasm detection and sentiment identification",
    "checked": true,
    "id": "724cc225a0171bcde03e06e83b64bc21d9aa5460",
    "semantic_title": "combining context-free and contextualized representations for arabic sarcasm detection and sentiment identification",
    "citation_count": 11,
    "authors": [
      "Amey Hengle",
      "Atharva Kshirsagar",
      "Shaily Desai",
      "Manisha Marathe"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.47": {
    "title": "Leveraging Offensive Language for Sarcasm and Sentiment Detection in Arabic",
    "volume": "workshop",
    "abstract": "Sarcasm detection is one of the top challenging tasks in text classification, particularly for informal Arabic with high syntactic and semantic ambiguity. We propose two systems that harness knowledge from multiple tasks to improve the performance of the classifier. This paper presents the systems used in our participation to the two sub-tasks of the Sixth Arabic Natural Language Processing Workshop (WANLP); Sarcasm Detection and Sentiment Analysis. Our methodology is driven by the hypothesis that tweets with negative sentiment and tweets with sarcasm content are more likely to have offensive content, thus, fine-tuning the classification model using large corpus of offensive language, supports the learning process of the model to effectively detect sentiment and sarcasm contents. Results demonstrate the effectiveness of our approach for sarcasm detection task over sentiment analysis task",
    "checked": true,
    "id": "68159fcf3f95044c99d90522475f808a4958be8c",
    "semantic_title": "leveraging offensive language for sarcasm and sentiment detection in arabic",
    "citation_count": 20,
    "authors": [
      "Fatemah Husain",
      "Ozlem Uzuner"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.48": {
    "title": "The IDC System for Sentiment Classification and Sarcasm Detection in Arabic",
    "volume": "workshop",
    "abstract": "Sentiment classification and sarcasm detection attract a lot of attention by the NLP research community. However, solving these two problems in Arabic and on the basis of social network data (i.e., Twitter) is still of lower interest. In this paper we present designated solutions for sentiment classification and sarcasm detection tasks that were introduced as part of a shared task by Abu Farha et al. (2021). We adjust the existing state-of-the-art transformer pretrained models for our needs. In addition, we use a variety of machine-learning techniques such as down-sampling, augmentation, bagging, and usage of meta-features to improve the models performance. We achieve an F1-score of 0.75 over the sentiment classification problem where the F1-score is calculated over the positive and negative classes (the neutral class is not taken into account). We achieve an F1-score of 0.66 over the sarcasm detection problem where the F1-score is calculated over the sarcastic class only. In both cases, the above reported results are evaluated over the ArSarcasm-v2–an extended dataset of the ArSarcasm (Farha and Magdy, 2020) that was introduced as part of the shared task. This reflects an improvement to the state-of-the-art results in both tasks",
    "checked": true,
    "id": "55186b28155dd0ac7213e19a905776ab632cf3c6",
    "semantic_title": "the idc system for sentiment classification and sarcasm detection in arabic",
    "citation_count": 12,
    "authors": [
      "Abraham Israeli",
      "Yotam Nahum",
      "Shai Fine",
      "Kfir Bar"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.49": {
    "title": "Preprocessing Solutions for Detection of Sarcasm and Sentiment for Arabic",
    "volume": "workshop",
    "abstract": "This paper describes our approach to detecting Sentiment and Sarcasm for Arabic in the ArSarcasm 2021 shared task. Data preprocessing is a crucial task for a successful learning, that is why we applied a set of preprocessing steps to the dataset before training two classifiers, namely Linear Support Vector Classifier (LSVC) and Bidirectional Long Short Term Memory (BiLSTM). The findings show that despite the simplicity of the proposed approach, using the LSVC model with a normalizing Arabic (NA) preprocessing and the BiLSTM architecture with an Embedding layer as input have yielded an encouraging F1score of 33.71% and 57.80% for sarcasm and sentiment detection, respectively",
    "checked": true,
    "id": "c7d7b1b0d8a0fd234261b5633b61f5907fec129f",
    "semantic_title": "preprocessing solutions for detection of sarcasm and sentiment for arabic",
    "citation_count": 10,
    "authors": [
      "Mohamed Lichouri",
      "Mourad Abbas",
      "Besma Benaziz",
      "Aicha Zitouni",
      "Khaled Lounnas"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.50": {
    "title": "iCompass at Shared Task on Sarcasm and Sentiment Detection in Arabic",
    "volume": "workshop",
    "abstract": "We describe our submitted system to the 2021 Shared Task on Sarcasm and Sentiment Detection in Arabic (Abu Farha et al., 2021). We tackled both subtasks, namely Sarcasm Detection (Subtask 1) and Sentiment Analysis (Subtask 2). We used state-of-the-art pretrained contextualized text representation models and fine-tuned them according to the downstream task in hand. As a first approach, we used Google's multilingual BERT and then other Arabic variants: AraBERT, ARBERT and MARBERT. The results found show that MARBERT outperforms all of the previously mentioned models overall, either on Subtask 1 or Subtask 2",
    "checked": true,
    "id": "d9f4a53d29bc075b3ee78ab030eb745d7e389b49",
    "semantic_title": "icompass at shared task on sarcasm and sentiment detection in arabic",
    "citation_count": 9,
    "authors": [
      "Malek Naski",
      "Abir Messaoudi",
      "Hatem Haddad",
      "Moez BenHajhmida",
      "Chayma Fourati",
      "Aymen Ben Elhaj Mabrouk"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.51": {
    "title": "Machine Learning-Based Model for Sentiment and Sarcasm Detection",
    "volume": "workshop",
    "abstract": "Within the last few years, the number of Arabic internet users and Arabic online content is in exponential growth. Dealing with Arabic datasets and the usage of non-explicit sentences to express an opinion are considered to be the major challenges in the field of natural language processing. Hence, sarcasm and sentiment analysis has gained a major interest from the research community, especially in this language. Automatic sarcasm detection and sentiment analysis can be applied using three approaches, namely supervised, unsupervised and hybrid approach. In this paper, a model based on a supervised machine learning algorithm called Support Vector Machine (SVM) has been used for this process. The proposed model has been evaluated using ArSarcasm-v2 dataset. The performance of the proposed model has been compared with other models submitted to sentiment analysis and sarcasm detection shared task",
    "checked": true,
    "id": "a880e772bf5cf8a6ad9f36d6a97bab5a8ed8107c",
    "semantic_title": "machine learning-based model for sentiment and sarcasm detection",
    "citation_count": 24,
    "authors": [
      "Hamada Nayel",
      "Eslam Amer",
      "Aya Allam",
      "Hanya Abdallah"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.52": {
    "title": "DeepBlueAI at WANLP-EACL2021 task 2: A Deep Ensemble-based Method for Sarcasm and Sentiment Detection in Arabic",
    "volume": "workshop",
    "abstract": "Sarcasm is one of the main challenges for sentiment analysis systems due to using implicit indirect phrasing for expressing opinions, especially in Arabic. This paper presents the system we submitted to the Sarcasm and Sentiment Detection task of WANLP-2021 that is capable of dealing with both two subtasks. We first perform fine-tuning on two kinds of pre-trained language models (PLMs) with different training strategies. Then an effective stacking mechanism is applied on top of the fine-tuned PLMs to obtain the final prediction. Experimental results on ArSarcasm-v2 dataset show the effectiveness of our method and we rank third and second for subtask 1 and 2",
    "checked": true,
    "id": "a09f7fab7cb9eccc13f10960d2b52b45c825973d",
    "semantic_title": "deepblueai at wanlp-eacl2021 task 2: a deep ensemble-based method for sarcasm and sentiment detection in arabic",
    "citation_count": 4,
    "authors": [
      "Bingyan Song",
      "Chunguang Pan",
      "Shengguang Wang",
      "Zhipeng Luo"
    ]
  },
  "https://aclanthology.org/2021.wanlp-1.53": {
    "title": "AraBERT and Farasa Segmentation Based Approach For Sarcasm and Sentiment Detection in Arabic Tweets",
    "volume": "workshop",
    "abstract": "This paper presents our strategy to tackle the EACL WANLP-2021 Shared Task 2: Sarcasm and Sentiment Detection. One of the subtasks aims at developing a system that identifies whether a given Arabic tweet is sarcastic in nature or not, while the other aims to identify the sentiment of the Arabic tweet. We approach the task in two steps. The first step involves pre processing the provided dataset by performing insertions, deletions and segmentation operations on various parts of the text. The second step involves experimenting with multiple variants of two transformer based models, AraELECTRA and AraBERT. Our final approach was ranked seventh and fourth in the Sarcasm and Sentiment Detection subtasks respectively",
    "checked": true,
    "id": "6e48f92b52f0541b3cf243a1e424d79a53fd8c85",
    "semantic_title": "arabert and farasa segmentation based approach for sarcasm and sentiment detection in arabic tweets",
    "citation_count": 20,
    "authors": [
      "Anshul Wadhawan"
    ]
  },
  "https://aclanthology.org/2021.wassa-1.1": {
    "title": "ToxCCIn: Toxic Content Classification with Interpretability",
    "volume": "workshop",
    "abstract": "Despite the recent successes of transformer-based models in terms of effectiveness on a variety of tasks, their decisions often remain opaque to humans. Explanations are particularly important for tasks like offensive language or toxicity detection on social media because a manual appeal process is often in place to dispute automatically flagged content. In this work, we propose a technique to improve the interpretability of these models, based on a simple and powerful assumption: a post is at least as toxic as its most toxic span. We incorporate this assumption into transformer models by scoring a post based on the maximum toxicity of its spans and augmenting the training process to identify correct spans. We find this approach effective and can produce explanations that exceed the quality of those provided by Logistic Regression analysis (often regarded as a highly-interpretable model), according to a human study",
    "checked": true,
    "id": "4d874a47870dfed68a64acb3dabcc8b113cb1836",
    "semantic_title": "toxccin: toxic content classification with interpretability",
    "citation_count": 10,
    "authors": [
      "Tong Xiang",
      "Sean MacAvaney",
      "Eugene Yang",
      "Nazli Goharian"
    ]
  },
  "https://aclanthology.org/2021.wassa-1.2": {
    "title": "Language that Captivates the Audience: Predicting Affective Ratings of TED Talks in a Multi-Label Classification Task",
    "volume": "workshop",
    "abstract": "The aim of the paper is twofold: (1) to automatically predict the ratings assigned by viewers to 14 categories available for TED talks in a multi-label classification task and (2) to determine what types of features drive classification accuracy for each of the categories. The focus is on features of language usage from five groups pertaining to syntactic complexity, lexical richness, register-based n-gram measures, information-theoretic measures and LIWC-style measures. We show that a Recurrent Neural Network classifier trained exclusively on within-text distributions of such features can reach relatively high levels of overall accuracy (69%) across the 14 categories. We find that features from two groups are strong predictors of the affective ratings across all categories and that there are distinct patterns of language usage for each rating category",
    "checked": true,
    "id": "dc1b668812ba22c45464ba73757e04b3e7accbe8",
    "semantic_title": "language that captivates the audience: predicting affective ratings of ted talks in a multi-label classification task",
    "citation_count": 7,
    "authors": [
      "Elma Kerz",
      "Yu Qiao",
      "Daniel Wiechmann"
    ]
  },
  "https://aclanthology.org/2021.wassa-1.3": {
    "title": "Partisanship and Fear are Associated with Resistance to COVID-19 Directives",
    "volume": "workshop",
    "abstract": "Ideological differences have had a large impact on individual and community response to the COVID-19 pandemic in the United States. Early behavioral research during the pandemic showed that conservatives were less likely to adhere to health directives, which contradicts a body of work suggesting that conservative ideology emphasizes a rule abiding, loss aversion, and prevention focus. We reconcile this contradiction by analyzing semantic content of local press releases, federal press releases, and localized tweets during the first month of the government response to COVID-19 in the United States. Controlling for factors such as COVID-19 confirmed cases and deaths, local economic indicators, and more, we find that online expressions of fear in conservative areas lead to an increase in adherence to public health recommendations concerning COVID-19, and that expressions of fear in government press releases are a significant predictor of expressed fear on Twitter",
    "checked": true,
    "id": "eb73f867fff629d98d3cf282e75b886418b945ba",
    "semantic_title": "partisanship and fear are associated with resistance to covid-19 directives",
    "citation_count": 2,
    "authors": [
      "Mike Lindow",
      "David DeFranza",
      "Arul Mishra",
      "Himanshu Mishra"
    ]
  },
  "https://aclanthology.org/2021.wassa-1.4": {
    "title": "Explainable Detection of Sarcasm in Social Media",
    "volume": "workshop",
    "abstract": "Sarcasm is a linguistic expression often used to communicate the opposite of what is said, usually something that is very unpleasant with an intention to insult or ridicule. Inherent ambiguity in sarcastic expressions makes sarcasm detection very difficult. In this work, we focus on detecting sarcasm in textual conversations, written in English, from various social networking platforms and online media. To this end, we develop an interpretable deep learning model using multi-head self-attention and gated recurrent units. We show the effectiveness and interpretability of our approach by achieving state-of-the-art results on datasets from social networking platforms, online discussion forums, and political dialogues",
    "checked": true,
    "id": "ff4a68ccecf73f73d86c4553c7c0fbc8645c699f",
    "semantic_title": "explainable detection of sarcasm in social media",
    "citation_count": 6,
    "authors": [
      "Ramya Akula",
      "Ivan Garibay"
    ]
  },
  "https://aclanthology.org/2021.wassa-1.5": {
    "title": "Emotion Ratings: How Intensity, Annotation Confidence and Agreements are Entangled",
    "volume": "workshop",
    "abstract": "When humans judge the affective content of texts, they also implicitly assess the correctness of such judgment, that is, their confidence. We hypothesize that people's (in)confidence that they performed well in an annotation task leads to (dis)agreements among each other. If this is true, confidence may serve as a diagnostic tool for systematic differences in annotations. To probe our assumption, we conduct a study on a subset of the Corpus of Contemporary American English, in which we ask raters to distinguish neutral sentences from emotion-bearing ones, while scoring the confidence of their answers. Confidence turns out to approximate inter-annotator disagreements. Further, we find that confidence is correlated to emotion intensity: perceiving stronger affect in text prompts annotators to more certain classification performances. This insight is relevant for modelling studies of intensity, as it opens the question wether automatic regressors or classifiers actually predict intensity, or rather human's self-perceived confidence",
    "checked": true,
    "id": "4945d9c088fed4d3e36a75e45a2747e5a452d630",
    "semantic_title": "emotion ratings: how intensity, annotation confidence and agreements are entangled",
    "citation_count": 11,
    "authors": [
      "Enrica Troiano",
      "Sebastian Padó",
      "Roman Klinger"
    ]
  },
  "https://aclanthology.org/2021.wassa-1.6": {
    "title": "Disentangling Document Topic and Author Gender in Multiple Languages: Lessons for Adversarial Debiasing",
    "volume": "workshop",
    "abstract": "Text classification is a central tool in NLP. However, when the target classes are strongly correlated with other textual attributes, text classification models can pick up \"wrong\" features, leading to bad generalization and biases. In social media analysis, this problem surfaces for demographic user classes such as language, topic, or gender, which influence the generate text to a substantial extent. Adversarial training has been claimed to mitigate this problem, but thorough evaluation is missing. In this paper, we experiment with text classification of the correlated attributes of document topic and author gender, using a novel multilingual parallel corpus of TED talk transcripts. Our findings are: (a) individual classifiers for topic and author gender are indeed biased; (b) debiasing with adversarial training works for topic, but breaks down for author gender; (c) gender debiasing results differ across languages. We interpret the result in terms of feature space overlap, highlighting the role of linguistic surface realization of the target classes",
    "checked": true,
    "id": "1abe884af880135c80753dd3086234566bf0b8b9",
    "semantic_title": "disentangling document topic and author gender in multiple languages: lessons for adversarial debiasing",
    "citation_count": 7,
    "authors": [
      "Erenay Dayanik",
      "Sebastian Padó"
    ]
  },
  "https://aclanthology.org/2021.wassa-1.7": {
    "title": "Universal Joy A Data Set and Results for Classifying Emotions Across Languages",
    "volume": "workshop",
    "abstract": "While emotions are universal aspects of human psychology, they are expressed differently across different languages and cultures. We introduce a new data set of over 530k anonymized public Facebook posts across 18 languages, labeled with five different emotions. Using multilingual BERT embeddings, we show that emotions can be reliably inferred both within and across languages. Zero-shot learning produces promising results for low-resource languages. Following established theories of basic emotions, we provide a detailed analysis of the possibilities and limits of cross-lingual emotion classification. We find that structural and typological similarity between languages facilitates cross-lingual learning, as well as linguistic diversity of training data. Our results suggest that there are commonalities underlying the expression of emotion in different languages. We publicly release the anonymized data for future research",
    "checked": true,
    "id": "08b88c4d13f98bcf474f453b352920a2463a6ff5",
    "semantic_title": "universal joy a data set and results for classifying emotions across languages",
    "citation_count": 23,
    "authors": [
      "Sotiris Lamprinidis",
      "Federico Bianchi",
      "Daniel Hardt",
      "Dirk Hovy"
    ]
  },
  "https://aclanthology.org/2021.wassa-1.8": {
    "title": "FEEL-IT: Emotion and Sentiment Classification for the Italian Language",
    "volume": "workshop",
    "abstract": "While sentiment analysis is a popular task to understand people's reactions online, we often need more nuanced information: is the post negative because the user is angry or sad? An abundance of approaches have been introduced for tackling these tasks, also for Italian, but they all treat only one of the tasks. We introduce FEEL-IT, a novel benchmark corpus of Italian Twitter posts annotated with four basic emotions: anger, fear, joy, sadness. By collapsing them, we can also do sentiment analysis. We evaluate our corpus on benchmark datasets for both emotion and sentiment classification, obtaining competitive results. We release an open-source Python library, so researchers can use a model trained on FEEL-IT for inferring both sentiments and emotions from Italian text",
    "checked": true,
    "id": "279bb6a990e37a4478ce640b7f0c7dba3f034b3f",
    "semantic_title": "feel-it: emotion and sentiment classification for the italian language",
    "citation_count": 31,
    "authors": [
      "Federico Bianchi",
      "Debora Nozza",
      "Dirk Hovy"
    ]
  },
  "https://aclanthology.org/2021.wassa-1.9": {
    "title": "An End-to-End Network for Emotion-Cause Pair Extraction",
    "volume": "workshop",
    "abstract": "The task of Emotion-Cause Pair Extraction (ECPE) aims to extract all potential clause-pairs of emotions and their corresponding causes in a document. Unlike the more well-studied task of Emotion Cause Extraction (ECE), ECPE does not require the emotion clauses to be provided as annotations. Previous works on ECPE have either followed a multi-stage approach where emotion extraction, cause extraction, and pairing are done independently or use complex architectures to resolve its limitations. In this paper, we propose an end-to-end model for the ECPE task. Due to the unavailability of an English language ECPE corpus, we adapt the NTCIR-13 ECE corpus and establish a baseline for the ECPE task on this dataset. On this dataset, the proposed method produces significant performance improvements (∼ 6.5% increase in F1 score) over the multi-stage approach and achieves comparable performance to the state-of-the-art methods",
    "checked": true,
    "id": "3cbce50dcd213b10190a9d27ddafa700752380e5",
    "semantic_title": "an end-to-end network for emotion-cause pair extraction",
    "citation_count": 32,
    "authors": [
      "Aaditya Singh",
      "Shreeshail Hingane",
      "Saim Wani",
      "Ashutosh Modi"
    ]
  },
  "https://aclanthology.org/2021.wassa-1.10": {
    "title": "WASSA 2021 Shared Task: Predicting Empathy and Emotion in Reaction to News Stories",
    "volume": "workshop",
    "abstract": "This paper presents the results that were obtained from the WASSA 2021 shared task on predicting empathy and emotions. The participants were given access to a dataset comprising empathic reactions to news stories where harm is done to a person, group, or other. These reactions consist of essays, Batson empathic concern, and personal distress scores, and the dataset was further extended with news articles, person-level demographic information (age, gender, ethnicity, income, education level), and personality information. Additionally, emotion labels, namely Ekman's six basic emotions, were added to the essays at both the document and sentence level. Participation was encouraged in two tracks: predicting empathy and predicting emotion categories. In total five teams participated in the shared task. We summarize the methods and resources used by the participating teams",
    "checked": true,
    "id": "c5d1cc128d0f31511683b2f5c3c1080717eda8a7",
    "semantic_title": "wassa 2021 shared task: predicting empathy and emotion in reaction to news stories",
    "citation_count": 27,
    "authors": [
      "Shabnam Tafreshi",
      "Orphee De Clercq",
      "Valentin Barriere",
      "Sven Buechel",
      "João Sedoc",
      "Alexandra Balahur"
    ]
  },
  "https://aclanthology.org/2021.wassa-1.11": {
    "title": "PVG at WASSA 2021: A Multi-Input, Multi-Task, Transformer-Based Architecture for Empathy and Distress Prediction",
    "volume": "workshop",
    "abstract": "Active research pertaining to the affective phenomenon of empathy and distress is invaluable for improving human-machine interaction. Predicting intensities of such complex emotions from textual data is difficult, as these constructs are deeply rooted in the psychological theory. Consequently, for better prediction, it becomes imperative to take into account ancillary factors such as the psychological test scores, demographic features, underlying latent primitive emotions, along with the text's undertone and its psychological complexity. This paper proffers team PVG's solution to the WASSA 2021 Shared Task on Predicting Empathy and Emotion in Reaction to News Stories. Leveraging the textual data, demographic features, psychological test score, and the intrinsic interdependencies of primitive emotions and empathy, we propose a multi-input, multi-task framework for the task of empathy score prediction. Here, the empathy score prediction is considered the primary task, while emotion and empathy classification are considered secondary auxiliary tasks. For the distress score prediction task, the system is further boosted by the addition of lexical features. Our submission ranked 1st based on the average correlation (0.545) as well as the distress correlation (0.574), and 2nd for the empathy Pearson correlation (0.517)",
    "checked": true,
    "id": "53383dc5840c7dca1bc4ae4924ab41535aaa0196",
    "semantic_title": "pvg at wassa 2021: a multi-input, multi-task, transformer-based architecture for empathy and distress prediction",
    "citation_count": 8,
    "authors": [
      "Atharva Kulkarni",
      "Sunanda Somwase",
      "Shivam Rajput",
      "Manisha Marathe"
    ]
  },
  "https://aclanthology.org/2021.wassa-1.12": {
    "title": "WASSA@IITK at WASSA 2021: Multi-task Learning and Transformer Finetuning for Emotion Classification and Empathy Prediction",
    "volume": "workshop",
    "abstract": "This paper describes our contribution to the WASSA 2021 shared task on Empathy Prediction and Emotion Classification. The broad goal of this task was to model an empathy score, a distress score and the overall level of emotion of an essay written in response to a newspaper article associated with harm to someone. We have used the ELECTRA model abundantly and also advanced deep learning approaches like multi-task learning. Additionally, we also leveraged standard machine learning techniques like ensembling. Our system achieves a Pearson Correlation Coefficient of 0.533 on sub-task I and a macro F1 score of 0.5528 on sub-task II. We ranked 1st in Emotion Classification sub-task and 3rd in Empathy Prediction sub-task",
    "checked": true,
    "id": "7e0882e0a6eb4321c42908c034d371ee1f4f3c43",
    "semantic_title": "wassa@iitk at wassa 2021: multi-task learning and transformer finetuning for emotion classification and empathy prediction",
    "citation_count": 10,
    "authors": [
      "Jay Mundra",
      "Rohan Gupta",
      "Sagnik Mukherjee"
    ]
  },
  "https://aclanthology.org/2021.wassa-1.13": {
    "title": "Analyzing Curriculum Learning for Sentiment Analysis along Task Difficulty, Pacing and Visualization Axes",
    "volume": "workshop",
    "abstract": "While Curriculum Learning (CL) has recently gained traction in Natural language Processing Tasks, it is still not adequately analyzed. Previous works only show their effectiveness but fail short to explain and interpret the internal workings fully. In this paper, we analyze curriculum learning in sentiment analysis along multiple axes. Some of these axes have been proposed by earlier works that need more in-depth study. Such analysis requires understanding where curriculum learning works and where it does not. Our axes of analysis include Task difficulty on CL, comparing CL pacing techniques, and qualitative analysis by visualizing the movement of attention scores in the model as curriculum phases progress. We find that curriculum learning works best for difficult tasks and may even lead to a decrement in performance for tasks with higher performance without curriculum learning. We see that One-Pass curriculum strategies suffer from catastrophic forgetting and attention movement visualization within curriculum pacing. This shows that curriculum learning breaks down the challenging main task into easier sub-tasks solved sequentially",
    "checked": true,
    "id": "89d5fafc1112abb23b72328bfe0e37e945331f25",
    "semantic_title": "analyzing curriculum learning for sentiment analysis along task difficulty, pacing and visualization axes",
    "citation_count": 2,
    "authors": [
      "Anvesh Rao Vijjini",
      "Kaveri Anuranjana",
      "Radhika Mamidi"
    ]
  },
  "https://aclanthology.org/2021.wassa-1.14": {
    "title": "Lightweight Models for Multimodal Sequential Data",
    "volume": "workshop",
    "abstract": "Human language encompasses more than just text; it also conveys emotions through tone and gestures. We present a case study of three simple and efficient Transformer-based architectures for predicting sentiment and emotion in multimodal data. The Late Fusion model merges unimodal features to create a multimodal feature sequence, the Round Robin model iteratively combines bimodal features using cross-modal attention, and the Hybrid Fusion model combines trimodal and unimodal features together to form a final feature sequence for predicting sentiment. Our experiments show that our small models are effective and outperform the publicly released versions of much larger, state-of-the-art multimodal sentiment analysis systems",
    "checked": true,
    "id": "ecd85a2c3b95ae04ff7dd28b7a0ee4ec864319da",
    "semantic_title": "lightweight models for multimodal sequential data",
    "citation_count": 2,
    "authors": [
      "Soumya Sourav",
      "Jessica Ouyang"
    ]
  },
  "https://aclanthology.org/2021.wassa-1.15": {
    "title": "Exploring Implicit Sentiment Evoked by Fine-grained News Events",
    "volume": "workshop",
    "abstract": "We investigate the feasibility of defining sentiment evoked by fine-grained news events. Our research question is based on the premise that methods for detecting implicit sentiment in news can be a key driver of content diversity, which is one way to mitigate the detrimental effects of filter bubbles that recommenders based on collaborative filtering may produce. Our experiments are based on 1,735 news articles from major Flemish newspapers that were manually annotated, with high agreement, for implicit sentiment. While lexical resources prove insufficient for sentiment analysis in this data genre, our results demonstrate that machine learning models based on SVM and BERT are able to automatically infer the implicit sentiment evoked by news events",
    "checked": true,
    "id": "3368b18ca8c6e82ec07540c3002fc388e2a68473",
    "semantic_title": "exploring implicit sentiment evoked by fine-grained news events",
    "citation_count": 5,
    "authors": [
      "Cynthia Van Hee",
      "Orphee De Clercq",
      "Veronique Hoste"
    ]
  },
  "https://aclanthology.org/2021.wassa-1.16": {
    "title": "Exploring Stylometric and Emotion-Based Features for Multilingual Cross-Domain Hate Speech Detection",
    "volume": "workshop",
    "abstract": "In this paper, we describe experiments designed to evaluate the impact of stylometric and emotion-based features on hate speech detection: the task of classifying textual content into hate or non-hate speech classes. Our experiments are conducted for three languages – English, Slovene, and Dutch – both in in-domain and cross-domain setups, and aim to investigate hate speech using features that model two linguistic phenomena: the writing style of hateful social media content operationalized as function word usage on the one hand, and emotion expression in hateful messages on the other hand. The results of experiments with features that model different combinations of these phenomena support our hypothesis that stylometric and emotion-based features are robust indicators of hate speech. Their contribution remains persistent with respect to domain and language variation. We show that the combination of features that model the targeted phenomena outperforms words and character n-gram features under cross-domain conditions, and provides a significant boost to deep learning models, which currently obtain the best results, when combined with them in an ensemble",
    "checked": true,
    "id": "6644f1a4573c3b9042b3f0b9ca9d72fd7b668fdd",
    "semantic_title": "exploring stylometric and emotion-based features for multilingual cross-domain hate speech detection",
    "citation_count": 36,
    "authors": [
      "Ilia Markov",
      "Nikola Ljubešić",
      "Darja Fišer",
      "Walter Daelemans"
    ]
  },
  "https://aclanthology.org/2021.wassa-1.17": {
    "title": "Emotion-Aware, Emotion-Agnostic, or Automatic: Corpus Creation Strategies to Obtain Cognitive Event Appraisal Annotations",
    "volume": "workshop",
    "abstract": "Appraisal theories explain how the cognitive evaluation of an event leads to a particular emotion. In contrast to theories of basic emotions or affect (valence/arousal), this theory has not received a lot of attention in natural language processing. Yet, in psychology it has been proven powerful: Smith and Ellsworth (1985) showed that the appraisal dimensions attention, certainty, anticipated effort, pleasantness, responsibility/control and situational control discriminate between (at least) 15 emotion classes. We study different annotation strategies for these dimensions, based on the event-focused enISEAR corpus (Troiano et al., 2019). We analyze two manual annotation settings: (1) showing the text to annotate while masking the experienced emotion label; (2) revealing the emotion associated with the text. Setting 2 enables the annotators to develop a more realistic intuition of the described event, while Setting 1 is a more standard annotation procedure, purely relying on text. We evaluate these strategies in two ways: by measuring inter-annotator agreement and by fine- tuning RoBERTa to predict appraisal variables. Our results show that knowledge of the emotion increases annotators' reliability. Further, we evaluate a purely automatic rule-based labeling strategy (inferring appraisal from annotated emotion classes). Training on automatically assigned labels leads to a competitive performance of our classifier, even when tested on manual annotations. This is an indicator that it might be possible to automatically create appraisal corpora for every domain for which emotion corpora already exist",
    "checked": true,
    "id": "5f90193d4767a3dd73f3463fa001c044798f87f5",
    "semantic_title": "emotion-aware, emotion-agnostic, or automatic: corpus creation strategies to obtain cognitive event appraisal annotations",
    "citation_count": 7,
    "authors": [
      "Jan Hofmann",
      "Enrica Troiano",
      "Roman Klinger"
    ]
  },
  "https://aclanthology.org/2021.wassa-1.18": {
    "title": "Hate Towards the Political Opponent: A Twitter Corpus Study of the 2020 US Elections on the Basis of Offensive Speech and Stance Detection",
    "volume": "workshop",
    "abstract": "The 2020 US Elections have been, more than ever before, characterized by social media campaigns and mutual accusations. We investigate in this paper if this manifests also in online communication of the supporters of the candidates Biden and Trump, by uttering hateful and offensive communication. We formulate an annotation task, in which we join the tasks of hateful/offensive speech detection and stance detection, and annotate 3000 Tweets from the campaign period, if they express a particular stance towards a candidate. Next to the established classes of favorable and against, we add mixed and neutral stances and also annotate if a candidate is mentioned with- out an opinion expression. Further, we an- notate if the tweet is written in an offensive style. This enables us to analyze if supporters of Joe Biden and the Democratic Party communicate differently than supporters of Donald Trump and the Republican Party. A BERT baseline classifier shows that the detection if somebody is a supporter of a candidate can be performed with high quality (.89 F1 for Trump and .91 F1 for Biden), while the detection that somebody expresses to be against a candidate is more challenging (.79 F1 and .64 F1, respectively). The automatic detection of hate/offensive speech remains challenging (with .53 F1). Our corpus is publicly available and constitutes a novel resource for computational modelling of offensive language under consideration of stances",
    "checked": true,
    "id": "9ba86bd0e837334ac8a2b51ed7e6a90709f16c4c",
    "semantic_title": "hate towards the political opponent: a twitter corpus study of the 2020 us elections on the basis of offensive speech and stance detection",
    "citation_count": 45,
    "authors": [
      "Lara Grimminger",
      "Roman Klinger"
    ]
  },
  "https://aclanthology.org/2021.wassa-1.19": {
    "title": "Synthetic Examples Improve Cross-Target Generalization: A Study on Stance Detection on a Twitter corpus",
    "volume": "workshop",
    "abstract": "Cross-target generalization is a known problem in stance detection (SD), where systems tend to perform poorly when exposed to targets unseen during training. Given that data annotation is expensive and time-consuming, finding ways to leverage abundant unlabeled in-domain data can offer great benefits. In this paper, we apply a weakly supervised framework to enhance cross-target generalization through synthetically annotated data. We focus on Twitter SD and show experimentally that integrating synthetic data is helpful for cross-target generalization, leading to significant improvements in performance, with gains in F1 scores ranging from +3.4 to +5.1",
    "checked": true,
    "id": "07d9bab660fc42aafc2681539ef6362d23a843ff",
    "semantic_title": "synthetic examples improve cross-target generalization: a study on stance detection on a twitter corpus",
    "citation_count": 9,
    "authors": [
      "Costanza Conforti",
      "Jakob Berndt",
      "Mohammad Taher Pilehvar",
      "Chryssi Giannitsarou",
      "Flavio Toxvaerd",
      "Nigel Collier"
    ]
  },
  "https://aclanthology.org/2021.wassa-1.20": {
    "title": "Creating and Evaluating Resources for Sentiment Analysis in the Low-resource Language: Sindhi",
    "volume": "workshop",
    "abstract": "In this paper, we develop Sindhi subjective lexicon using a merger of existing English resources: NRC lexicon, list of opinion words, SentiWordNet, Sindhi-English bilingual dictionary, and collection of Sindhi modifiers. The positive or negative sentiment score is assigned to each Sindhi opinion word. Afterwards, we determine the coverage of the proposed lexicon with subjectivity analysis. Moreover, we crawl multi-domain tweet corpus of news, sports, and finance. The crawled corpus is annotated by experienced annotators using the Doccano text annotation tool. The sentiment annotated corpus is evaluated by employing support vector machine (SVM), recurrent neural network (RNN) variants, and convolutional neural network (CNN)",
    "checked": true,
    "id": "ff9d2c6fa90e51c6795d7ed56bb5bdadcee82085",
    "semantic_title": "creating and evaluating resources for sentiment analysis in the low-resource language: sindhi",
    "citation_count": 7,
    "authors": [
      "Wazir Ali",
      "Naveed Ali",
      "Yong Dai",
      "Jay Kumar",
      "Saifullah Tumrani",
      "Zenglin Xu"
    ]
  },
  "https://aclanthology.org/2021.wassa-1.21": {
    "title": "Towards Emotion Recognition in Hindi-English Code-Mixed Data: A Transformer Based Approach",
    "volume": "workshop",
    "abstract": "In the last few years, emotion detection in social-media text has become a popular problem due to its wide ranging application in better understanding the consumers, in psychology, in aiding human interaction with computers, designing smart systems etc. Because of the availability of huge amounts of data from social-media, which is regularly used for expressing sentiments and opinions, this problem has garnered great attention. In this paper, we present a Hinglish dataset labelled for emotion detection. We highlight a deep learning based approach for detecting emotions using bilingual word embeddings derived from FastText and Word2Vec approaches in Hindi-English code mixed tweets. We experiment with various deep learning models, including CNNs, LSTMs, Bi-directional LSTMs (with and without attention), along with transformers like BERT, RoBERTa, and ALBERT. The transformer based BERT model outperforms all current state-of-the-art models giving the best performance with an accuracy of 71.43%",
    "checked": true,
    "id": "35ed5d218e317fed0b2333ddcb1c8b81bfa980fe",
    "semantic_title": "towards emotion recognition in hindi-english code-mixed data: a transformer based approach",
    "citation_count": 17,
    "authors": [
      "Anshul Wadhawan",
      "Akshita Aggarwal"
    ]
  },
  "https://aclanthology.org/2021.wassa-1.22": {
    "title": "Nearest neighbour approaches for Emotion Detection in Tweets",
    "volume": "workshop",
    "abstract": "Emotion detection is an important task that can be applied to social media data to discover new knowledge. While the use of deep learning methods for this task has been prevalent, they are black-box models, making their decisions hard to interpret for a human operator. Therefore, in this paper, we propose an approach using weighted k Nearest Neighbours (kNN), a simple, easy to implement, and explainable machine learning model. These qualities can help to enhance results' reliability and guide error analysis. In particular, we apply the weighted kNN model to the shared emotion detection task in tweets from SemEval-2018. Tweets are represented using different text embedding methods and emotion lexicon vocabulary scores, and classification is done by an ensemble of weighted kNN models. Our best approaches obtain results competitive with state-of-the-art solutions and open up a promising alternative path to neural network methods",
    "checked": true,
    "id": "6aa29218fa39c62f898ca493daf629ce3b0b5170",
    "semantic_title": "fuzzy-rough nearest neighbour approaches for emotion detection in tweets",
    "citation_count": 8,
    "authors": [
      "Olha Kaminska",
      "Chris Cornelis",
      "Veronique Hoste"
    ]
  },
  "https://aclanthology.org/2021.wassa-1.23": {
    "title": "L3CubeMahaSent: A Marathi Tweet-based Sentiment Analysis Dataset",
    "volume": "workshop",
    "abstract": "Sentiment analysis is one of the most fundamental tasks in Natural Language Processing. Popular languages like English, Arabic, Russian, Mandarin, and also Indian languages such as Hindi, Bengali, Tamil have seen a significant amount of work in this area. However, the Marathi language which is the third most popular language in India still lags behind due to the absence of proper datasets. In this paper, we present the first major publicly available Marathi Sentiment Analysis Dataset - L3CubeMahaSent. It is curated using tweets extracted from various Maharashtrian personalities' Twitter accounts. Our dataset consists of ~16,000 distinct tweets classified in three broad classes viz. positive, negative, and neutral. We also present the guidelines using which we annotated the tweets. Finally, we present the statistics of our dataset and baseline classification results using CNN, LSTM, ULMFiT, and BERT based models",
    "checked": true,
    "id": "bb8d2aa9ef06ecc830828dee92bfdeb46e14e25f",
    "semantic_title": "l3cubemahasent: a marathi tweet-based sentiment analysis dataset",
    "citation_count": 32,
    "authors": [
      "Atharva Kulkarni",
      "Meet Mandhane",
      "Manali Likhitkar",
      "Gayatri Kshirsagar",
      "Raviraj Joshi"
    ]
  },
  "https://aclanthology.org/2021.wassa-1.24": {
    "title": "Multi-Emotion Classification for Song Lyrics",
    "volume": "workshop",
    "abstract": "Song lyrics convey a multitude of emotions to the listener and powerfully portray the emotional state of the writer or singer. This paper examines a variety of modeling approaches to the multi-emotion classification problem for songs. We introduce the Edmonds Dance dataset, a novel emotion-annotated lyrics dataset from the reader's perspective, and annotate the dataset of Mihalcea and Strapparava (2012) at the song level. We find that models trained on relatively small song datasets achieve marginally better performance than BERT (Devlin et al., 2018) fine-tuned on large social media or dialog datasets",
    "checked": true,
    "id": "30466a455bc6038f543f68113d0c7c3cd17e1ac2",
    "semantic_title": "multi-emotion classification for song lyrics",
    "citation_count": 10,
    "authors": [
      "Darren Edmonds",
      "João Sedoc"
    ]
  },
  "https://aclanthology.org/2021.wassa-1.25": {
    "title": "ONE: Toward ONE model, ONE algorithm, ONE corpus dedicated to sentiment analysis of Arabic/Arabizi and its dialects",
    "volume": "workshop",
    "abstract": "Arabic is the official language of 22 countries, spoken by more than 400 million speakers. Each one of this country use at least on dialect for daily life conversation. Then, Arabic has at least 22 dialects. Each dialect can be written in Arabic or Arabizi Scripts. The most recent researches focus on constructing a language model and a training corpus for each dialect, in each script. Following this technique means constructing 46 different resources (by including the Modern Standard Arabic, MSA) for handling only one language. In this paper, we extract ONE corpus, and we propose ONE algorithm to automatically construct ONE training corpus using ONE classification model architecture for sentiment analysis MSA and different dialects. After manually reviewing the training corpus, the obtained results outperform all the research literature results for the targeted test corpora",
    "checked": true,
    "id": "8b58d6a6ffdc78c6e4e7c8e90065f206b7f99f86",
    "semantic_title": "one: toward one model, one algorithm, one corpus dedicated to sentiment analysis of arabic/arabizi and its dialects",
    "citation_count": 4,
    "authors": [
      "Imane Guellil",
      "Faical Azouaou",
      "Fodil Benali",
      "Hachani Ala-Eddine"
    ]
  },
  "https://aclanthology.org/2021.wassa-1.26": {
    "title": "Me, myself, and ire: Effects of automatic transcription quality on emotion, sarcasm, and personality detection",
    "volume": "workshop",
    "abstract": "In deployment, systems that use speech as input must make use of automated transcriptions. Yet, typically when these systems are evaluated, gold transcriptions are assumed. We explicitly examine the impact of transcription errors on the downstream performance of a multi-modal system on three related tasks from three datasets: emotion, sarcasm, and personality detection. We include three separate transcription tools and show that while all automated transcriptions propagate errors that substantially impact downstream performance, the open-source tools fair worse than the paid tool, though not always straightforwardly, and word error rates do not correlate well with downstream performance. We further find that the inclusion of audio features partially mitigates transcription errors, but that a naive usage of a multi-task setup does not",
    "checked": true,
    "id": "6bb895454b5aba2bf9406b9a01dd04d07e611d74",
    "semantic_title": "me, myself, and ire: effects of automatic transcription quality on emotion, sarcasm, and personality detection",
    "citation_count": 1,
    "authors": [
      "John Culnan",
      "Seongjin Park",
      "Meghavarshini Krishnaswamy",
      "Rebecca Sharp"
    ]
  },
  "https://aclanthology.org/2021.wassa-1.27": {
    "title": "Emotional RobBERT and Insensitive BERTje: Combining Transformers and Affect Lexica for Dutch Emotion Detection",
    "volume": "workshop",
    "abstract": "In a first step towards improving Dutch emotion detection, we try to combine the Dutch transformer models BERTje and RobBERT with lexicon-based methods. We propose two architectures: one in which lexicon information is directly injected into the transformer model and a meta-learning approach where predictions from transformers are combined with lexicon features. The models are tested on 1,000 Dutch tweets and 1,000 captions from TV-shows which have been manually annotated with emotion categories and dimensions. We find that RobBERT clearly outperforms BERTje, but that directly adding lexicon information to transformers does not improve performance. In the meta-learning approach, lexicon information does have a positive effect on BERTje, but not on RobBERT. This suggests that more emotional information is already contained within this latter language model",
    "checked": true,
    "id": "5c0481fca835b696c8e1c8c5173f10890fbd18ff",
    "semantic_title": "emotional robbert and insensitive bertje: combining transformers and affect lexica for dutch emotion detection",
    "citation_count": 10,
    "authors": [
      "Luna De Bruyne",
      "Orphee De Clercq",
      "Veronique Hoste"
    ]
  },
  "https://aclanthology.org/2021.wassa-1.28": {
    "title": "EmpNa at WASSA 2021: A Lightweight Model for the Prediction of Empathy, Distress and Emotions from Reactions to News Stories",
    "volume": "workshop",
    "abstract": "This paper describes our submission for the WASSA 2021 shared task regarding the prediction of empathy, distress and emotions from news stories. The solution is based on combining the frequency of words, lexicon-based information, demographics of the annotators and personality of the annotators into a linear model. The prediction of empathy and distress is performed using Linear Regression while the prediction of emotions is performed using Logistic Regression. Both tasks are performed using the same features. Our models rank 4th for the prediction of emotions and 2nd for the prediction of empathy and distress. These results are particularly interesting when considered that the computational requirements of the solution are minimal",
    "checked": true,
    "id": "74d5b2372abde05370e9d55040a381a0ed95ce71",
    "semantic_title": "empna at wassa 2021: a lightweight model for the prediction of empathy, distress and emotions from reactions to news stories",
    "citation_count": 5,
    "authors": [
      "Giuseppe Vettigli",
      "Antonio Sorgente"
    ]
  },
  "https://aclanthology.org/2021.wassa-1.29": {
    "title": "MilaNLP @ WASSA: Does BERT Feel Sad When You Cry?",
    "volume": "workshop",
    "abstract": "The paper describes the MilaNLP team's submission (Bocconi University, Milan) in the WASSA 2021 Shared Task on Empathy Detection and Emotion Classification. We focus on Track 2 - Emotion Classification - which consists of predicting the emotion of reactions to English news stories at the essay-level. We test different models based on multi-task and multi-input frameworks. The goal was to better exploit all the correlated information given in the data set. We find, though, that empathy as an auxiliary task in multi-task learning and demographic attributes as additional input provide worse performance with respect to single-task learning. While the result is competitive in terms of the competition, our results suggest that emotion and empathy are not related tasks - at least for the purpose of prediction",
    "checked": true,
    "id": "9874b2e480e97232672744666b98ebabc5802750",
    "semantic_title": "milanlp @ wassa: does bert feel sad when you cry?",
    "citation_count": 5,
    "authors": [
      "Tommaso Fornaciari",
      "Federico Bianchi",
      "Debora Nozza",
      "Dirk Hovy"
    ]
  },
  "https://aclanthology.org/2021.wassa-1.30": {
    "title": "Team Phoenix at WASSA 2021: Emotion Analysis on News Stories with Pre-Trained Language Models",
    "volume": "workshop",
    "abstract": "Emotion is fundamental to humanity. The ability to perceive, understand and respond to social interactions in a human-like manner is one of the most desired capabilities in artificial agents, particularly in social-media bots. Over the past few years, computational understanding and detection of emotional aspects in language have been vital in advancing human-computer interaction. The WASSA Shared Task 2021 released a dataset of news-stories across two tracks, Track-1 for Empathy and Distress Prediction and Track-2 for Multi-Dimension Emotion prediction at the essay-level. We describe our system entry for the WASSA 2021 Shared Task (for both Track-1 and Track-2), where we leveraged the information from Pre-trained language models for Track-specific Tasks. Our proposed models achieved an Average Pearson Score of 0.417, and a Macro-F1 Score of 0.502 in Track 1 and Track 2, respectively. In the Shared Task leaderboard, we secured the fourth rank in Track 1 and the second rank in Track 2",
    "checked": true,
    "id": "212fbb40d042ddcdaad0b87931ad077270148e3d",
    "semantic_title": "team phoenix at wassa 2021: emotion analysis on news stories with pre-trained language models",
    "citation_count": 8,
    "authors": [
      "Yash Butala",
      "Kanishk Singh",
      "Adarsh Kumar",
      "Shrey Shrivastava"
    ]
  }
}