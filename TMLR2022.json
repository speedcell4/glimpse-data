{
  "https://openreview.net/forum?id=QaDevCcmcg": {
    "title": "Uncertainty-Based Active Learning for Reading Comprehension",
    "volume": "main",
    "abstract": "Recent years have witnessed a surge of successful applications of machine reading comprehension. Of central importance to these tasks is the availability of massive amount of labeled data, which facilitates training of large-scale neural networks. However, in many real-world problems, annotated data are expensive to gather not only because of time cost and budget, but also of certain domain-specific restrictions such as privacy for healthcare data. In this regard, we propose an uncertainty-based active learning algorithm for reading comprehension, which interleaves data annotation and model updating to mitigate the demand of labeling. Our key techniques are two-fold: 1) an unsupervised uncertainty-based sampling scheme that queries the labels of the most informative instances with respect to the currently learned model; and 2) an adaptive loss minimization paradigm that simultaneously fits the data and controls the degree of model updating. We demonstrate on benchmark datasets that 25% less labeled samples suffice to guarantee similar, or even improved performance. Our results show strong evidence that for label-demanding scenarios, the proposed approach offers a practical guide on data collection and model training",
    "checked": true,
    "id": "5350fa166ee3bd093bb031fc3b0aae2b97e18b72",
    "semantic_title": "uncertainty-based active learning for reading comprehension",
    "citation_count": 2,
    "authors": [
      "Jing Wang",
      "Jie Shen",
      "Xiaofei Ma",
      "Andrew Arnold"
    ]
  },
  "https://openreview.net/forum?id=p8gncJbMit": {
    "title": "A geometrical connection between sparse and low-rank matrices and its application to manifold learning",
    "volume": "main",
    "abstract": "We consider when a sparse nonnegative matrix $\\mathbf{S}$ can be recovered, via an elementwise nonlinearity, from a real-valued matrix~$\\mathbf{L}$ of significantly lower rank. Of particular interest is the setting where the positive elements of $\\mathbf{S}$ encode the similarities of nearby points on a low dimensional manifold. The recovery can then be posed as a problem in manifold learning---in this case, how to learn a norm-preserving and neighborhood-preserving mapping of high dimensional inputs into a lower dimensional space. We describe an algorithm for this problem based on a generalized low-rank decomposition of sparse matrices. This decomposition has the interesting property that it can be encoded by a neural network with one layer of rectified linear units; since the algorithm discovers this encoding, it can also be viewed as a layerwise primitive for deep learning. The algorithm regards the inputs $\\mathbf{x}_i$ and $\\mathbf{x}_j$ as similar whenever the cosine of the angle between them exceeds some threshold $\\tau\\in(0,1)$. Given this threshold, the algorithm attempts to discover a mapping $\\mathbf{x}_i\\mapsto\\mathbf{y}_i$ by matching the elements of two sparse matrices; in particular, it seeks a mapping for which $\\mathbf{S}=\\max(0,\\mathbf{L})$, where $S_{ij} = \\max(0,\\mathbf{x}_i\\!\\cdot\\!\\mathbf{x}_j\\! -\\! \\tau\\|\\mathbf{x}_i\\|\\|\\mathbf{x}_j\\|)$ and $L_{ij} = \\mathbf{y}_i\\!\\cdot\\!\\mathbf{y}_j\\! -\\! \\tau\\|\\mathbf{y}_i\\|\\|\\mathbf{y}_j\\|$. We apply the algorithm to data sets where vector magnitudes and small cosine distances have interpretable meanings (e.g., the brightness of an image, the similarity to other words). On these data sets, the algorithm is able to discover much lower dimensional representations that preserve these meanings",
    "checked": true,
    "id": "feba15635b24fb16630fec20c7bd1b7ee82533be",
    "semantic_title": "a geometrical connection between sparse and low-rank matrices and its application to manifold learning",
    "citation_count": 8,
    "authors": [
      "Lawrence K. Saul"
    ]
  },
  "https://openreview.net/forum?id=VipljNfZSZ": {
    "title": "Collaborative Algorithms for Online Personalized Mean Estimation",
    "volume": "main",
    "abstract": "We consider an online estimation problem involving a set of agents. Each agent has access to a (personal) process that generates samples from a real-valued distribution and seeks to estimate its mean. We study the case where some of the distributions have the same mean, and the agents are allowed to actively query information from other agents. The goal is to design an algorithm that enables each agent to improve its mean estimate thanks to communication with other agents. The means as well as the number of distributions with same mean are unknown, which makes the task nontrivial. We introduce a novel collaborative strategy to solve this online personalized mean estimation problem. We analyze its time complexity and introduce variants that enjoy good performance in numerical experiments. We also extend our approach to the setting where clusters of agents with similar means seek to estimate the mean of their cluster",
    "checked": true,
    "id": "ee32f3994f162e928e510843df2cd014672c4f12",
    "semantic_title": "collaborative algorithms for online personalized mean estimation",
    "citation_count": 4,
    "authors": [
      "Mahsa Asadi",
      "Aur√©lien Bellet",
      "Odalric-Ambrym Maillard",
      "Marc Tommasi"
    ]
  },
  "https://openreview.net/forum?id=x4hmIsWu7e": {
    "title": "Indiscriminate Data Poisoning Attacks on Neural Networks",
    "volume": "main",
    "abstract": "Data poisoning attacks, in which a malicious adversary aims to influence a model by injecting ``poisoned'' data into the training process, have attracted significant recent attention. In this work, we take a closer look at existing poisoning attacks and connect them with old and new algorithms for solving sequential Stackelberg games. By choosing an appropriate loss function for the attacker and optimizing with algorithms that exploit second-order information, we design poisoning attacks that are effective on neural networks. We present efficient implementations by parameterizing the attacker and allowing simultaneous and coordinated generation of tens of thousands of poisoned points, in contrast to most existing methods that generate poisoned points one by one. We further perform extensive experiments that empirically explore the effect of data poisoning attacks on deep neural networks. Our paper sets a new benchmark on the possibility of performing indiscriminate data poisoning attacks on modern neural networks",
    "checked": true,
    "id": "3473619be2b2a481c0ac459ec2bf5449119fceb1",
    "semantic_title": "indiscriminate data poisoning attacks on neural networks",
    "citation_count": 26,
    "authors": [
      "Yiwei Lu",
      "Gautam Kamath",
      "Yaoliang Yu"
    ]
  },
  "https://openreview.net/forum?id=HFfJWx60IT": {
    "title": "An empirical study of implicit regularization in deep offline RL",
    "volume": "main",
    "abstract": "Deep neural networks are the most commonly used function approximators in offline reinforcement learning. Prior works have shown that neural nets trained with TD-learning and gradient descent can exhibit implicit regularization that can be characterized by under-parameterization of these networks. Specifically, the rank of the penultimate feature layer, also called effective rank, has been observed to drastically collapse during the training. In turn, this collapse has been argued to reduce the model's ability to further adapt in later stages of learning, leading to the diminished final performance. Such an association between the effective rank and performance makes effective rank compelling for offline RL, primarily for offline policy evaluation. In this work, we conduct a careful empirical study on the relation between effective rank and performance on three offline RL datasets : bsuite, Atari, and DeepMind lab. We observe that a direct association exists only in restricted settings and disappears in the more extensive hyperparameter sweeps. Also, we empirically identify three phases of learning that explain the impact of implicit regularization on the learning dynamics and found that bootstrapping alone is insufficient to explain the collapse of the effective rank. Further, we show that several other factors could confound the relationship between effective rank and performance and conclude that studying this association under simplistic assumptions could be highly misleading",
    "checked": true,
    "id": "8856b6e9a0d79d674d772c175e95fb03e9e4a5f5",
    "semantic_title": "an empirical study of implicit regularization in deep offline rl",
    "citation_count": 17,
    "authors": [
      "Caglar Gulcehre",
      "Srivatsan Srinivasan",
      "Jakub Sygnowski",
      "Georg Ostrovski",
      "Mehrdad Farajtabar",
      "Matthew Hoffman",
      "Razvan Pascanu",
      "Arnaud Doucet"
    ]
  },
  "https://openreview.net/forum?id=sRgvmXjrmg": {
    "title": "Unsupervised Network Embedding Beyond Homophily",
    "volume": "main",
    "abstract": "Network embedding (NE) approaches have emerged as a predominant technique to represent complex networks and have benefited numerous tasks. However, most NE approaches rely on a homophily assumption to learn embeddings with the guidance of supervisory signals, leaving the unsupervised heterophilous scenario relatively unexplored. This problem becomes especially relevant in fields where a scarcity of labels exists. Here, we formulate the unsupervised NE task as an r-ego network discrimination problem and develop the SELENE framework for learning on networks with homophily and heterophily. Specifically, we design a dual-channel feature embedding pipeline to discriminate r-ego networks using node attributes and structural information separately. We employ heterophily adapted self-supervised learning objective functions to optimise the framework to learn intrinsic node embeddings. We show that SELENE's components improve the quality of node embeddings, facilitating the discrimination of connected heterophilous nodes. Comprehensive empirical evaluations on both synthetic and real-world datasets with varying homophily ratios validate the effectiveness of SELENE in homophilous and heterophilous settings showing an up to 12.52% clustering accuracy gain",
    "checked": true,
    "id": "b95ec59a0fd7641f4dc7eaade0014ce89950c415",
    "semantic_title": "unsupervised network embedding beyond homophily",
    "citation_count": 6,
    "authors": [
      "Zhiqiang Zhong",
      "Guadalupe Gonzalez",
      "Daniele Grattarola",
      "Jun Pang"
    ]
  },
  "https://openreview.net/forum?id=eWvBEMTlRq": {
    "title": "Unsupervised Learning of Neurosymbolic Encoders",
    "volume": "main",
    "abstract": "We present a framework for the unsupervised learning of neurosymbolic encoders, which are encoders obtained by composing neural networks with symbolic programs from a domain-specific language. Our framework naturally incorporates symbolic expert knowledge into the learning process, which leads to more interpretable and factorized latent representations compared to fully neural encoders. We integrate modern program synthesis techniques with the variational autoencoding (VAE) framework, in order to learn a neurosymbolic encoder in conjunction with a standard decoder. The programmatic descriptions from our encoders can benefit many analysis workflows, such as in behavior modeling where interpreting agent actions and movements is important. We evaluate our method on learning latent representations for real-world trajectory data from animal biology and sports analytics. We show that our approach offers significantly better separation of meaningful categories than standard VAEs and leads to practical gains on downstream analysis tasks, such as for behavior classification",
    "checked": true,
    "id": "d90d3975f9ee7fb4ffb886ffe1f09dff90b2f951",
    "semantic_title": "unsupervised learning of neurosymbolic encoders",
    "citation_count": 14,
    "authors": [
      "Eric Zhan",
      "Jennifer J. Sun",
      "Ann Kennedy",
      "Yisong Yue",
      "Swarat Chaudhuri"
    ]
  },
  "https://openreview.net/forum?id=4pCjIGIjrt": {
    "title": "Sequentially learning the topological ordering of directed acyclic graphs with likelihood ratio scores",
    "volume": "main",
    "abstract": "Causal discovery, the learning of causality in a data mining scenario, has been of strong scientific and theoretical interest as a starting point to identify \"what causes what?'' Contingent on assumptions and a proper learning algorithm, it is sometimes possible to identify and accurately estimate an underlying directed acyclic graph (DAG), as opposed to a Markov equivalence class of graphs that gives ambiguity of causal directions. The focus of this paper is in highlighting the identifiability and estimation of DAGs through a sequential sorting procedure that orders variables one at a time, starting at root nodes, followed by children of the root nodes, and so on until completion. We demonstrate a novel application of this general sequential approach to estimate the topological ordering of the DAG corresponding to a linear structural equation model with a non-Gaussian error distribution family. At each step of the procedure, only simple likelihood ratio scores are calculated on regression residuals to decide the next node to append to the current partial ordering. The computational complexity of our algorithm on a $p$-node problem is $\\mathcal{O}(pd)$, where $d$ is the maximum neighborhood size. Under mild assumptions, the population version of our procedure provably identifies a true ordering of the underlying DAG. We provide extensive numerical evidence to demonstrate that this sequential procedure scales to possibly thousands of nodes and works well for high-dimensional data. We accompany these numerical experiments with an application to a single-cell gene expression dataset. Our $\\texttt{R}$ package with examples and installation instructions can be found at https://gabriel-ruiz.github.io/scorelingam/",
    "checked": true,
    "id": "8db5a8c6f346177a8caaec22ff0d5144e47fad72",
    "semantic_title": "sequentially learning the topological ordering of directed acyclic graphs with likelihood ratio scores",
    "citation_count": 2,
    "authors": [
      "Gabriel Ruiz",
      "OSCAR HERNAN MADRID PADILLA",
      "Qing Zhou"
    ]
  },
  "https://openreview.net/forum?id=lukVf4VrfP": {
    "title": "Lazy vs hasty: linearization in deep networks impacts learning schedule based on example difficulty",
    "volume": "main",
    "abstract": "Among attempts at giving a theoretical account of the success of deep neural networks, a recent line of work has identified a so-called `lazy' training regime in which the network can be well approximated by its linearization around initialization. Here we investigate the comparative effect of the lazy (linear) and feature learning (non-linear) regimes on subgroups of examples based on their difficulty. Specifically, we show that easier examples are given more weight in feature learning mode, resulting in faster training compared to more difficult ones. In other words, the non-linear dynamics tends to sequentialize the learning of examples of increasing difficulty. We illustrate this phenomenon across different ways to quantify example difficulty, including c-score, label noise, and in the presence of easy-to-learn spurious correlations. Our results reveal a new understanding of how deep networks prioritize resources across example difficulty",
    "checked": true,
    "id": "74692ff84d28e8ef013e08714552dd0f32711152",
    "semantic_title": "lazy vs hasty: linearization in deep networks impacts learning schedule based on example difficulty",
    "citation_count": 6,
    "authors": [
      "Thomas George",
      "Guillaume Lajoie",
      "Aristide Baratin"
    ]
  },
  "https://openreview.net/forum?id=VmTYgjYloM": {
    "title": "Fourier Sensitivity and Regularization of Computer Vision Models",
    "volume": "main",
    "abstract": "Recent work has empirically shown that deep neural networks latch on to the Fourier statistics of training data and show increased sensitivity to Fourier-basis directions in the input. Understanding and modifying this Fourier-sensitivity of computer vision models may help improve their robustness, hence, in this paper we study the frequency sensitivity characteristics of deep neural networks using a principled approach. We first propose a $\\textbf{\\textit{basis trick}}$, proving that unitary transformations of the input-gradient of a function can be used to compute its gradient in the basis induced by the transformation. Using this result, we propose a general measure of any differentiable computer vision model's $\\textit{\\textbf{Fourier-sensitivity}}$ using the unitary Fourier-transform of its input-gradient. When applied to deep neural networks, we find that computer vision models are consistently sensitive to particular frequencies dependent on the dataset, training method and architecture. Based on this measure, we further propose a $\\textit{\\textbf{Fourier-regularization}}$ framework to modify the Fourier-sensitivities and frequency bias of models. Using our proposed regularizer-family, we demonstrate that deep neural networks obtain improved classification accuracy on robustness evaluations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kiran Krishnamachari",
      "See-Kiong Ng",
      "Chuan-Sheng Foo"
    ]
  },
  "https://openreview.net/forum?id=2VWR6JfwNo": {
    "title": "MVSFormer: Multi-View Stereo by Learning Robust Image Features and Temperature-based Depth",
    "volume": "main",
    "abstract": "Feature representation learning is the key recipe for learning-based Multi-View Stereo (MVS). As the common feature extractor of learning-based MVS, vanilla Feature Pyramid Networks (FPNs) suffer from discouraged feature representations for reflection and texture-less areas, which limits the generalization of MVS. Even FPNs worked with pre-trained Convolutional Neural Networks (CNNs) fail to tackle these issues. On the other hand, Vision Transformers (ViTs) have achieved prominent success in many 2D vision tasks. Thus we ask whether ViTs can facilitate feature learning in MVS? In this paper, we propose a pre-trained ViT enhanced MVS network called MVSFormer, which can learn more reliable feature representations benefited by informative priors from ViT. The finetuned MVSFormer with hierarchical ViTs of efficient attention mechanisms can achieve prominent improvement based on FPNs. Besides, the alternative MVSFormer with frozen ViT weights is further proposed. This largely alleviates the training cost with competitive performance strengthened by the attention map from the self-distillation pre-training. MVSFormer can be generalized to various input resolutions with efficient multi-scale training strengthened by gradient accumulation. Moreover, we discuss the merits and drawbacks of classification and regression-based MVS methods, and further propose to unify them with a temperature-based strategy. MVSFormer achieves state-of-the-art performance on the DTU dataset. Particularly, MVSFormer ranks as Top-1 on both intermediate and advanced sets of the highly competitive Tanks-and-Temples leaderboard. Codes and models are released in https://github.com/ewrfcas/MVSFormer",
    "checked": true,
    "id": "0475f5c612a4eb5324a23179eede6680ac0ecdb3",
    "semantic_title": "mvsformer: multi-view stereo by learning robust image features and temperature-based depth",
    "citation_count": 54,
    "authors": [
      "Chenjie Cao",
      "Xinlin Ren",
      "Yanwei Fu"
    ]
  },
  "https://openreview.net/forum?id=Z44YAcLaGw": {
    "title": "Controllable Generative Modeling via Causal Reasoning",
    "volume": "main",
    "abstract": "Deep latent variable generative models excel at generating complex, high-dimensional data, often exhibiting impressive generalization beyond the training distribution. However, many such models in use today are black-boxes trained on large unlabelled datasets with statistical objectives and lack an interpretable understanding of the latent space required for controlling the generative process. We propose CAGE, a framework for controllable generation in latent variable models based on causal reasoning. Given a pair of attributes, CAGE infers the implicit cause-effect relationships between these attributes as induced by a deep generative model. This is achieved by defining and estimating a novel notion of unit-level causal effects in the latent space of the generative model. Thereafter, we use the inferred cause-effect relationships to design a novel strategy for controllable generation based on counterfactual sampling. Through a series of large-scale synthetic and human evaluations, we demonstrate that generating counterfactual samples which respect the underlying causal relationships inferred via CAGE leads to subjectively more realistic images",
    "checked": true,
    "id": "e2bc83185fbac80d61d2ad7c242ebe7425a40d23",
    "semantic_title": "controllable generative modeling via causal reasoning",
    "citation_count": 5,
    "authors": [
      "Joey Bose",
      "Ricardo Pio Monti",
      "Aditya Grover"
    ]
  },
  "https://openreview.net/forum?id=DY1pMrmDkm": {
    "title": "Modeling Bounded Rationality in Multi-Agent Simulations Using Rationally Inattentive Reinforcement Learning",
    "volume": "main",
    "abstract": "Multi-agent reinforcement learning (MARL) is a powerful framework for studying emergent behavior in complex agent-based simulations. However, RL agents are often assumed to be rational and behave optimally, which does not fully reflect human behavior. In this work, we propose a new, more human-like RL agent, which incorporates an established model of human-irrationality, the Rational Inattention (RI) model. RI models the cost of cognitive information processing using mutual information. Our RIRL framework generalizes and is more flexible than prior work by allowing for multi-timestep dynamics and information channels with heterogeneous processing costs. We demonstrate the flexibility of RIRL in versions of a classic economic setting (Principal-Agent setting) with varying complexity. In simple settings, we show using RIRL can lead to optimal agent behavior policy with approximately the same functional form as what is expected from the analysis of prior work, which utilizes theoretical methods. We additionally demonstrate that using RIRL to analyze complex, theoretically intractable settings, yields a rich spectrum of new equilibrium behaviors that differ from those found under rationality assumptions. For example, increasing the cognitive cost experienced by a manager agent results in the other agents increasing the magnitude of their action to compensate. These results suggest RIRL is a powerful tool towards building AI agents that can mimic real human behavior",
    "checked": true,
    "id": "a29c571dfa2af621147adf039c0eaffb32eec49b",
    "semantic_title": "modeling bounded rationality in multi-agent simulations using rationally inattentive reinforcement learning",
    "citation_count": 2,
    "authors": [
      "Tong Mu",
      "Stephan Zheng",
      "Alexander R Trott"
    ]
  },
  "https://openreview.net/forum?id=zFhNBs8GaV": {
    "title": "Calibrated Selective Classification",
    "volume": "main",
    "abstract": "Selective classification allows models to abstain from making predictions (e.g., say ``I don't know'') when in doubt in order to obtain better effective accuracy. While typical selective models can succeed at producing more accurate predictions on average, they may still allow for wrong predictions that have high confidence, or skip correct predictions that have low confidence. Providing calibrated uncertainty estimates alongside predictions---probabilities that correspond to true frequencies---can be as important as having predictions that are simply accurate on average. Uncertainty estimates, however, can sometimes be unreliable. In this paper, we develop a new approach to selective classification in which we propose a method for rejecting examples with ``uncertain'' uncertainties. By doing so, we aim to make predictions with well-calibrated uncertainty estimates over the distribution of accepted examples, a property we call selective calibration. We present a framework for learning selectively calibrated models, where a separate selector network is trained to improve the selective calibration error of a given base model. In particular, our work focuses on achieving robust calibration, where the model is intentionally designed to be tested on out-of-domain data. We achieve this through a training strategy inspired by distributionally robust optimization, in which we apply simulated input perturbations to the known, in-domain training data. We demonstrate the empirical effectiveness of our approach on multiple image classification and lung cancer risk assessment tasks",
    "checked": true,
    "id": "9f6d0f47f4dc0dabd2da6634def89f6d8d5b31b2",
    "semantic_title": "calibrated selective classification",
    "citation_count": 17,
    "authors": [
      "Adam Fisch",
      "Tommi S. Jaakkola",
      "Regina Barzilay"
    ]
  },
  "https://openreview.net/forum?id=29V0xo7jKp": {
    "title": "Unsupervised Mismatch Localization in Cross-Modal Sequential Data with Application to Mispronunciations Localization",
    "volume": "main",
    "abstract": "Content mismatch usually occurs when data from one modality is translated to another, e.g. language learners producing mispronunciations (errors in speech) when reading a sentence (target text) aloud. However, most existing alignment algorithms assume that the content involved in the two modalities is perfectly matched, thus leading to difficulty in locating such mismatch between speech and text. In this work, we develop an unsupervised learning algorithm that can infer the relationship between content-mismatched cross-modal sequential data, especially for speech-text sequences. More specifically, we propose a hierarchical Bayesian deep learning model, dubbed mismatch localization variational autoencoder (ML-VAE), which decomposes the generative process of the speech into hierarchically structured latent variables, indicating the relationship between the two modalities. Training such a model is very challenging due to the discrete latent variables with complex dependencies involved. To address this challenge, we propose a novel and effective training procedure that alternates between estimating the hard assignments of the discrete latent variables over a specifically designed mismatch localization finite-state acceptor (ML-FSA) and updating the parameters of neural networks. In this work, we focus on the mismatch localization problem for speech and text, and our experimental results show that ML-VAE successfully locates the mismatch between text and speech, without the need for human annotations for model training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Wei",
      "Hengguan Huang",
      "Xiangming Gu",
      "Hao Wang",
      "Ye Wang"
    ]
  },
  "https://openreview.net/forum?id=b4tMhpN0JC": {
    "title": "GIT: A Generative Image-to-text Transformer for Vision and Language",
    "volume": "main",
    "abstract": "In this paper, we design and train a Generative Image-to-text Transformer, GIT, to unify vision-language tasks such as image/video captioning and question answering. While generative models provide a consistent network architecture between pre-training and fine-tuning, existing work typically contains complex structures (uni/multi-modal encoder/decoder) and depends on external modules such as object detectors/taggers and optical character recognition (OCR). In GIT, we simplify the architecture as one image encoder and one text decoder under a single language modeling task. We also scale up the pre-training data and the model size to boost the model performance. Without bells and whistles, our GIT establishes new state of the arts on numerous challenging benchmarks with a large margin. For instance, our model surpasses the human performance for the first time on TextCaps (138.2 vs. 125.5 in CIDEr). Furthermore, we present a new scheme of generation-based image classification and scene text recognition, achieving decent performance on standard benchmarks",
    "checked": true,
    "id": "60ee030773ba1b68eb222a265b052ca028353362",
    "semantic_title": "git: a generative image-to-text transformer for vision and language",
    "citation_count": 564,
    "authors": [
      "Jianfeng Wang",
      "Zhengyuan Yang",
      "Xiaowei Hu",
      "Linjie Li",
      "Kevin Lin",
      "Zhe Gan",
      "Zicheng Liu",
      "Ce Liu",
      "Lijuan Wang"
    ]
  },
  "https://openreview.net/forum?id=WXVkgkPXRk": {
    "title": "Concave Utility Reinforcement Learning with Zero-Constraint Violations",
    "volume": "main",
    "abstract": "We consider the problem of tabular infinite horizon concave utility reinforcement learning (CURL) with convex constraints. For this, we propose a model-based learning algorithm that also achieves zero constraint violations. Assuming that the concave objective and the convex constraints have a solution interior to the set of feasible occupation measures, we solve a tighter optimization problem to ensure that the constraints are never violated despite the imprecise model knowledge and model stochasticity. We use Bellman error-based analysis for tabular infinite-horizon setups which allows analyzing stochastic policies. Combining the Bellman error-based analysis and tighter optimization equation, for $T$ interactions with the environment, we obtain a high-probability regret guarantee for objective which grows as $\\Tilde{O}(1/\\sqrt{T})$, excluding other factors. The proposed method can be applied for optimistic algorithms to obtain high-probability regret bounds and also be used for posterior sampling algorithms to obtain a loose Bayesian regret bounds but with significant improvement in computational complexity",
    "checked": true,
    "id": "85d44bb076fa060345132ee5149cd73d30dbe417",
    "semantic_title": "concave utility reinforcement learning with zero-constraint violations",
    "citation_count": 13,
    "authors": [
      "Mridul Agarwal",
      "Qinbo Bai",
      "Vaneet Aggarwal"
    ]
  },
  "https://openreview.net/forum?id=k4iWTEdUSF": {
    "title": "Fast and Accurate Spreading Process Temporal Scale Estimation",
    "volume": "main",
    "abstract": "Spreading processes on graphs arise in a host of application domains, from the study of online social networks to viral marketing to epidemiology. Various discrete-time probabilistic models for spreading processes have been proposed. These are used for downstream statistical estimation and prediction problems, often involving messages or other information that is transmitted along with infections caused by the process. These models generally model cascade behavior at a small time scale but are insufficiently flexible to model cascades that exhibit intermittent behavior governed by multiple scales. We argue that the presence of such time scales that are unaccounted for by a cascade model can result in degradation of performance of models on downstream statistical and time-sensitive optimization tasks. To address these issues, we formulate a model that incorporates multiple temporal scales of cascade behavior. This model is parameterized by a \\emph{clock}, which encodes the times at which sessions of cascade activity start. These sessions are themselves governed by a small-scale cascade model, such as the discretized independent cascade (IC) model. Estimation of the multiscale cascade model parameters leads to the problem of \\emph{clock estimation} in terms of a natural distortion measure that we formulate. Our framework is inspired by the optimization problem posed by DiTursi et al, 2017, which can be seen as providing one possible estimator (a maximum-proxy-likelihood estimator) for the parameters of our generative model. We give a clock estimation algorithm, which we call FastClock, that runs in linear time in the size of its input and is provably statistically accurate for a broad range of model parameters when cascades are generated from any spreading process model with well-concentrated session infection set sizes and when the underlying graph is at least in the semi-sparse regime. We exemplify our algorithm for the case where the small-scale model is the discretized independent cascade process and extend substantially to processes whose infection set sizes satisfy a general martingale difference property. We further evaluate the performance of FastClock empirically in comparison to the state of the art estimator from DiTursi et al, 2017. We find that in a broad parameter range on synthetic networks and on a real network, our algorithm substantially outperforms that algorithm in terms of both running time and accuracy. In all cases, our algorithm's running time is asymptotically lower than that of the baseline",
    "checked": true,
    "id": "e616a197c685b9c0114090e1517501b70f7fda24",
    "semantic_title": "fast and accurate spreading process temporal scale estimation",
    "citation_count": 0,
    "authors": [
      "Abram Magner",
      "Carolyn S Kaminski",
      "Petko Bogdanov"
    ]
  },
  "https://openreview.net/forum?id=RP6G787uD8": {
    "title": "Extracting Local Reasoning Chains of Deep Neural Networks",
    "volume": "main",
    "abstract": "We study how to explain the main steps of inference that a pre-trained deep neural net (DNN) relies on to produce predictions for a (sub)task and its data. This problem is related to network pruning and interpretable machine learning with the following highlighted differences: (1) fine-tuning of any neurons/filters is forbidden; (2) we target a very high pruning rate, e.g., ‚â• 95%, for better interpretability; (3) the interpretation is for the whole inference process on a few data of a task rather than for individual neurons/filters or a single sample. In this paper, we introduce NeuroChains to extract the local inference chains by optimizing differentiable sparse scores for the filters and layers, which reflects their importance in preserving the outputs on a few data drawn from a given (sub)task. Thereby, NeuroChains can extract an extremely small sub-network composed of critical filters exactly copied from the original pre-trained DNN by removing the filters/layers with small scores. For samples from the same class, we can then visualize the inference pathway in the pre-trained DNN by applying existing interpretation techniques to the retained filters and layers. It reveals how the inference process stitches and integrates the information layer by layer and filter by filter. We provide detailed and insightful case studies together with several quantitative analyses over thousands of trials to demonstrate the quality, sparsity, fidelity and accuracy of the interpretation. In extensive empirical studies on VGG, ResNet, and ViT, NeuroChains significantly enriches the interpretation and makes the inner mechanism of DNNs more transparent",
    "checked": true,
    "id": "156070205d9b7f0030c5a394d56ef64ebef4eb25",
    "semantic_title": "extracting local reasoning chains of deep neural networks",
    "citation_count": 0,
    "authors": [
      "Haiyan Zhao",
      "Tianyi Zhou",
      "Guodong Long",
      "Jing Jiang",
      "Chengqi Zhang"
    ]
  },
  "https://openreview.net/forum?id=LdEm0umNcv": {
    "title": "On Sample Complexity of Offline Reinforcement Learning with Deep ReLU Networks in Besov Spaces",
    "volume": "main",
    "abstract": "Offline reinforcement learning (RL) leverages previously collected data for policy optimization without any further active exploration. Despite the recent interest in this problem, its theoretical results in neural network function approximation settings remain elusive. In this paper, we study the statistical theory of offline RL with deep ReLU network function approximation. In particular, we establish the sample complexity of $n = \\tilde{\\mathcal{O}}( H^{4 + 4 \\frac{d}{\\alpha}} \\kappa_{\\mu}^{1 + \\frac{d}{\\alpha}} \\epsilon^{-2 - 2\\frac{d}{\\alpha}} )$ for offline RL with deep ReLU networks, where $\\kappa_{\\mu}$ is a measure of distributional shift, $H = (1-\\gamma)^{-1}$ is the effective horizon length, $d$ is the dimension of the state-action space, $\\alpha$ is a (possibly fractional) smoothness parameter of the underlying Markov decision process (MDP), and $\\epsilon$ is a user-specified error. Notably, our sample complexity holds under two novel considerations: the Besov dynamic closure and the correlated structure. While the Besov dynamic closure subsumes the dynamic conditions for offline RL in the prior works, the correlated structure renders the prior works of offline RL with general/neural network function approximation improper or inefficient in long (effective) horizon problems. To the best of our knowledge, this is the first theoretical characterization of the sample complexity of offline RL with deep neural network function approximation under the general Besov regularity condition that goes beyond the linearity regime in the traditional Reproducing Hilbert kernel spaces and Neural Tangent Kernels",
    "checked": true,
    "id": "f8d5ca8f8140a7475dacdc3ba0c27c4fa2c7c1de",
    "semantic_title": "on sample complexity of offline reinforcement learning with deep relu networks in besov spaces",
    "citation_count": 7,
    "authors": [
      "Thanh Nguyen-Tang",
      "Sunil Gupta",
      "Hung Tran-The",
      "Svetha Venkatesh"
    ]
  },
  "https://openreview.net/forum?id=F2rG2CXsgO": {
    "title": "Distribution Embedding Networks for Generalization from a Diverse Set of Classification Tasks",
    "volume": "main",
    "abstract": "We propose Distribution Embedding Networks (DEN) for classification with small data. In the same spirit of meta-learning, DEN learns from a diverse set of training tasks with the goal to generalize to unseen target tasks. Unlike existing approaches which require the inputs of training and target tasks to have the same dimension with possibly similar distributions, DEN allows training and target tasks to live in heterogeneous input spaces. This is especially useful for tabular-data tasks where labeled data from related tasks are scarce. DEN uses a three-block architecture: a covariate transformation block followed by a distribution embedding block and then a classification block. We provide theoretical insights to show that this architecture allows the embedding and classification blocks to be fixed after pre-training on a diverse set of tasks; only the covariate transformation block with relatively few parameters needs to be fine-tuned for each new task. To facilitate training, we also propose an approach to synthesize binary classification tasks, and demonstrate that DEN outperforms existing methods in a number of synthetic and real tasks in numerical studies",
    "checked": true,
    "id": "6cfcf483bf9468f800cb3fba854f39a60b4fae35",
    "semantic_title": "distribution embedding networks for generalization from a diverse set of classification tasks",
    "citation_count": 5,
    "authors": [
      "Lang Liu",
      "Mahdi Milani Fard",
      "Sen Zhao"
    ]
  },
  "https://openreview.net/forum?id=NXB0rEM2Tq": {
    "title": "COIN++: Neural Compression Across Modalities",
    "volume": "main",
    "abstract": "Neural compression algorithms are typically based on autoencoders that require specialized encoder and decoder architectures for different data modalities. In this paper, we propose COIN++, a neural compression framework that seamlessly handles a wide range of data modalities. Our approach is based on converting data to implicit neural representations, i.e. neural functions that map coordinates (such as pixel locations) to features (such as RGB values). Then, instead of storing the weights of the implicit neural representation directly, we store modulations applied to a meta-learned base network as a compressed code for the data. We further quantize and entropy code these modulations, leading to large compression gains while reducing encoding time by two orders of magnitude compared to baselines. We empirically demonstrate the feasibility of our method by compressing various data modalities, from images and audio to medical and climate data",
    "checked": true,
    "id": "4664d3b1050c246ed0a723f5b9f2afb78c865f1a",
    "semantic_title": "coin++: neural compression across modalities",
    "citation_count": 90,
    "authors": [
      "Emilien Dupont",
      "Hrushikesh Loya",
      "Milad Alizadeh",
      "Adam Golinski",
      "Yee Whye Teh",
      "Arnaud Doucet"
    ]
  },
  "https://openreview.net/forum?id=FTtFAg3pek": {
    "title": "Systematically and efficiently improving $k$-means initialization by pairwise-nearest-neighbor smoothing",
    "volume": "main",
    "abstract": "We present a meta-method for initializing (seeding) the $k$-means clustering algorithm called PNN-smoothing. It consists in splitting a given dataset into $J$ random subsets, clustering each of them individually, and merging the resulting clusterings with the pairwise-nearest-neighbor (PNN) method. It is a meta-method in the sense that when clustering the individual subsets any seeding algorithm can be used. If the computational complexity of that seeding algorithm is linear in the size of the data $N$ and the number of clusters $k$, PNN-smoothing is also almost linear with an appropriate choice of $J$, and quite competitive in practice. We show empirically, using several existing seeding methods and testing on several synthetic and real datasets, that this procedure results in systematically better costs. In particular, our method of enhancing $k$-means++ seeding proves superior in both effectiveness and speed compared to the popular ``greedy'' $k$-means++ variant. Our implementation is publicly available at \\href{https://github.com/carlobaldassi/KMeansPNNSmoothing.jl}{https://github.com/carlobaldassi/KMeansPNNSmoothing.jl}",
    "checked": false,
    "id": "7d03f6867d4e087eedafef2332ec24d6adfce334",
    "semantic_title": "systematically and efficiently improving k-means initialization by pairwise-nearest-neighbor smoothing",
    "citation_count": 0,
    "authors": [
      "Carlo Baldassi"
    ]
  },
  "https://openreview.net/forum?id=tbd9f3HwPy": {
    "title": "GhostSR: Learning Ghost Features for Efficient Image Super-Resolution",
    "volume": "main",
    "abstract": "Modern single image super-resolution (SISR) systems based on convolutional neural networks (CNNs) have achieved impressive performance but require huge computational costs. The problem on feature redundancy has been well studied in visual recognition task, but rarely discussed in SISR. Based on the observation that many features in SISR models are also similar to each other, we propose to use shift operation for generating the redundant features (i.e. ghost features). Compared with depth-wise convolution which is time-consuming on GPU-like devices, shift operation can bring a real inference acceleration for CNNs on common hardware. We analyze the benefits of shift operation in SISR and make the shift orientation learnable based on the Gumbel-Softmax trick. Besides, a clustering procedure is explored based on pre-trained models to identify the intrinsic filters for generating corresponding intrinsic features. The ghost features will be generated by moving these intrinsic features along a certain orientation. Finally, the complete output features are constructed by concatenating the intrinsic and ghost features together. Extensive experiments on several benchmark models and datasets demonstrate that both the non-compact and lightweight SISR CNN models embedded with the proposed method can achieve a comparable performance to the baseline models with a large reduction of parameters, FLOPs and GPU inference latency. For example, we reduce the parameters by 46%, FLOPs by 46% and GPU inference latency by 42% of x2 EDSR model with almost lossless performance. Code will be available at https://gitee.com/mindspore/models/tree/master/research/cv/GhostSR",
    "checked": true,
    "id": "95042006075799ce0f7734e80584f9974b621c57",
    "semantic_title": "ghostsr: learning ghost features for efficient image super-resolution",
    "citation_count": 24,
    "authors": [
      "Ying Nie",
      "Kai Han",
      "Zhenhua Liu",
      "Chuanjian Liu",
      "Yunhe Wang"
    ]
  },
  "https://openreview.net/forum?id=ygoNPRiLxw": {
    "title": "DiffuseVAE: Efficient, Controllable and High-Fidelity Generation from Low-Dimensional Latents",
    "volume": "main",
    "abstract": "Diffusion probabilistic models have been shown to generate state-of-the-art results on several competitive image synthesis benchmarks but lack a low-dimensional, interpretable latent space, and are slow at generation. On the other hand, standard Variational Autoencoders (VAEs) typically have access to a low-dimensional latent space but exhibit poor sample quality. We present DiffuseVAE, a novel generative framework that integrates VAE within a diffusion model framework, and leverage this to design novel conditional parameterizations for diffusion models. We show that the resulting model equips diffusion models with a low-dimensional VAE inferred latent code which can be used for downstream tasks like controllable synthesis. The proposed method also improves upon the speed vs quality tradeoff exhibited in standard unconditional DDPM/DDIM models (for instance, \\textbf{FID of 16.47 vs 34.36} using a standard DDIM on the CelebA-HQ-128 benchmark using \\textbf{T=10} reverse process steps) without having explicitly trained for such an objective. Furthermore, the proposed model exhibits synthesis quality comparable to state-of-the-art models on standard image synthesis benchmarks like CIFAR-10 and CelebA-64 while outperforming most existing VAE-based methods. Lastly, we show that the proposed method exhibits inherent generalization to different types of noise in the conditioning signal. For reproducibility, our source code is publicly available at \\url{https://github.com/kpandey008/DiffuseVAE}",
    "checked": true,
    "id": "ce8e3fa6fa6d45b8b92169a2e181dafb20749a2f",
    "semantic_title": "diffusevae: efficient, controllable and high-fidelity generation from low-dimensional latents",
    "citation_count": 121,
    "authors": [
      "Kushagra Pandey",
      "Avideep Mukherjee",
      "Piyush Rai",
      "Abhishek Kumar"
    ]
  },
  "https://openreview.net/forum?id=9tl6zjLYVS": {
    "title": "On the Origins of the Block Structure Phenomenon in Neural Network Representations",
    "volume": "main",
    "abstract": "Recent work by Nguyen et al. (2021) has uncovered a striking phenomenon in large-capacity neural networks: they contain blocks of contiguous hidden layers with highly similar representations. This block structure has two seemingly contradictory properties: on the one hand, its constituent layers exhibit highly similar dominant first principal components (PCs), but on the other hand, their representations, and their common first PC, are highly dissimilar across different random seeds. Our work seeks to reconcile these discrepant properties by investigating the origin of the block structure in relation to the data and training methods. By analyzing properties of the dominant PCs, we find that the block structure arises from dominant datapoints ‚Äî a small group of examples that share similar image statistics (e.g. background color). However, the set of dominant datapoints, and the precise shared image statistic, can vary across random seeds. Thus, the block structure reflects meaningful dataset statistics, but is simultaneously unique to each model. Through studying hidden layer activations and creating synthetic datapoints, we demonstrate that these simple image statistics dominate the representational geometry of the layers inside the block structure. We explore how the phenomenon evolves through training, finding that the block structure takes shape early in training, but the underlying representations and the corresponding dominant datapoints continue to change substantially. Finally, we study the interplay between the block structure and different training mechanisms, introducing a targeted intervention to eliminate the block structure, as well as examining the effects of pre-training and Shake-Shake regularization",
    "checked": true,
    "id": "5fe4f6fbe26f94ff65290c58007185ec71669921",
    "semantic_title": "on the origins of the block structure phenomenon in neural network representations",
    "citation_count": 13,
    "authors": [
      "Thao Nguyen",
      "Maithra Raghu",
      "Simon Kornblith"
    ]
  },
  "https://openreview.net/forum?id=AZIfC91hjM": {
    "title": "Interpretable Node Representation with Attribute Decoding",
    "volume": "main",
    "abstract": "Variational Graph Autoencoders (VGAEs) are powerful models for unsupervised learning of node representations from graph data. In this work, we make a systematic analysis of modeling node attributes in VGAEs and show that attribute decoding is important for node representation learning. We further propose a new learning model, interpretable NOde Representation with Attribute Decoding (NORAD). The model encodes node representations in an interpretable approach: node representations capture community structures in the graph and the relationship between communities and node attributes. We further propose a rectifying procedure to refine node representations of isolated notes, which improves the quality of the representations of these nodes. Our empirical results demonstrate the advantage of the proposed model when learning graph data in an interpretable approach",
    "checked": true,
    "id": "177492be91e39479d2ac7010f6c1b8a7b85162f5",
    "semantic_title": "interpretable node representation with attribute decoding",
    "citation_count": 4,
    "authors": [
      "Xiaohui Chen",
      "Xi Chen",
      "Liping Liu"
    ]
  },
  "https://openreview.net/forum?id=yeT9cBq8Cn": {
    "title": "A Unified Domain Adaptation Framework with Distinctive Divergence Analysis",
    "volume": "main",
    "abstract": "Unsupervised domain adaptation enables knowledge transfer from a labeled source domain to an unlabeled target domain by aligning the learnt features of both domains. The idea is theoretically supported by the generalization bound analysis in Ben-David et al. (2007), which specifies the applicable task (binary classification) and designates a specific distribution divergence measure. Although most distribution-aligning domain adaptation models seek theoretical grounds from this particular bound analysis, they do not actually fit into the stringent conditions. In this paper, we bridge the long-standing theoretical gap in literature by providing a unified generalization bound. Our analysis can well accommodate the classification/regression tasks and most commonly-used divergence measures, and more importantly, it can theoretically recover a large amount of previous models. In addition, we identify the key difference in the distribution divergence measures underlying the diverse models and commit a comprehensive in-depth comparison of the commonly-used divergence measures. Based on the unified generalization bound, we propose new domain adaptation models that achieve transferability through domain-invariant representations and conduct experiments on real-world datasets that corroborate our theoretical findings. We believe these insights are helpful in guiding the future design of distribution-aligning domain adaptation algorithms",
    "checked": true,
    "id": "a97e222f208fd9fda55c57a046d1837610a7dd20",
    "semantic_title": "a unified domain adaptation framework with distinctive divergence analysis",
    "citation_count": 3,
    "authors": [
      "Zhiri YUAN",
      "Xixu HU",
      "Qi WU",
      "Shumin MA",
      "Cheuk Hang LEUNG",
      "Xin Shen",
      "Yiyan HUANG"
    ]
  },
  "https://openreview.net/forum?id=A5tIluhDW6": {
    "title": "Infinitely wide limits for deep Stable neural networks: sub-linear, linear and super-linear activation functions",
    "volume": "main",
    "abstract": "There is a growing literature on the study of large-width properties of deep Gaussian neural networks (NNs), i.e. deep NNs with Gaussian-distributed parameters or weights, and Gaussian stochastic processes. Motivated by some empirical and theoretical studies showing the potential of replacing Gaussian distributions with Stable distributions, namely distributions with heavy tails, in this paper we investigate large-width properties of deep Stable NNs, i.e. deep NNs with Stable-distributed parameters. For sub-linear activation functions, a recent work has characterized the infinitely wide limit of a suitable rescaled deep Stable NN in terms of a Stable stochastic process, both under the assumption of a ``joint growth\" and under the assumption of a ``sequential growth\" of the width over the NN's layers. Here, assuming a ``sequential growth\" of the width, we extend such a characterization to a general class of activation functions, which includes sub-linear, asymptotically linear and super-linear functions. As a novelty with respect to previous works, our results rely on the use of a generalized central limit theorem for heavy tails distributions, which allows for an interesting unified treatment of infinitely wide limits for deep Stable NNs. Our study shows that the scaling of Stable NNs and the stability of their infinitely wide limits may depend on the choice of the activation function, bringing out a critical difference with respect to the Gaussian setting",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alberto Bordino",
      "Stefano Favaro",
      "Sandra Fortini"
    ]
  },
  "https://openreview.net/forum?id=iGREAJdULX": {
    "title": "Counterfactual Learning with Multioutput Deep Kernels",
    "volume": "main",
    "abstract": "In this paper, we address the challenge of performing counterfactual inference with observational data via Bayesian nonparametric regression adjustment, with a focus on high-dimensional settings featuring multiple actions and multiple correlated outcomes. We present a general class of counterfactual multi-task deep kernels models that estimate causal effects and learn policies proficiently thanks to their sample efficiency gains, while scaling well with high dimensions. In the first part of the work, we rely on Structural Causal Models (SCM) to formally introduce the setup and the problem of identifying counterfactual quantities under observed confounding. We then discuss the benefits of tackling the task of causal effects estimation via stacked coregionalized Gaussian Processes and Deep Kernels. Finally, we demonstrate the use of the proposed methods on simulated experiments that span individual causal effects estimation, off-policy evaluation and optimization",
    "checked": true,
    "id": "c5a6b1f0e4317afa89b070609810dafa01ae05de",
    "semantic_title": "counterfactual learning with multioutput deep kernels",
    "citation_count": 1,
    "authors": [
      "Alberto Caron",
      "Ioanna Manolopoulou",
      "Gianluca Baio"
    ]
  },
  "https://openreview.net/forum?id=gzu4ZbBY7S": {
    "title": "Incorporating Sum Constraints into Multitask Gaussian Processes",
    "volume": "main",
    "abstract": "Machine learning models can be improved by adapting them to respect existing background knowledge. In this paper we consider multitask Gaussian processes, with background knowledge in the form of constraints that require a specific sum of the outputs to be constant. This is achieved by conditioning the prior distribution on the constraint fulfillment. The approach allows for both linear and nonlinear constraints. We demonstrate that the constraints are fulfilled with high precision and that the construction can improve the overall prediction accuracy as compared to the standard Gaussian process",
    "checked": true,
    "id": "876ddef3ba445f7c66918fa30a79e7ca01786f77",
    "semantic_title": "incorporating sum constraints into multitask gaussian processes",
    "citation_count": 3,
    "authors": [
      "Philipp Pilar",
      "Carl Jidling",
      "Thomas B. Sch√∂n",
      "Niklas Wahlstr√∂m"
    ]
  },
  "https://openreview.net/forum?id=P0XO5ZE98j": {
    "title": "Degradation Attacks on Certifiably Robust Neural Networks",
    "volume": "main",
    "abstract": "Certifiably robust neural networks protect against adversarial examples by employing run-time defenses that check if the model is certifiably locally robust at the input under evaluation. We show through examples and experiments that any defense (whether complete or incomplete) based on checking local robustness is inherently over-cautious. Specifically, such defenses flag inputs for which local robustness checks fail, but yet that are not adversarial; i.e., they are classified consistently with all valid inputs within a distance of $\\epsilon$. As a result, while a norm-bounded adversary cannot change the classification of an input, it can use norm-bounded changes to degrade the utility of certifiably robust networks by forcing them to reject otherwise correctly classifiable inputs. We empirically demonstrate the efficacy of such attacks against state-of-the-art certifiable defenses. Our code is available at https://github.com/ravimangal/degradation-attacks",
    "checked": true,
    "id": "4acf1bd971b093ddfb323b70f604a1def5d02d48",
    "semantic_title": "degradation attacks on certifiably robust neural networks",
    "citation_count": 2,
    "authors": [
      "Klas Leino",
      "Chi Zhang",
      "Ravi Mangal",
      "Matt Fredrikson",
      "Bryan Parno",
      "Corina Pasareanu"
    ]
  },
  "https://openreview.net/forum?id=VW4IrC0n0M": {
    "title": "An approximate sampler for energy-based models with divergence diagnostics",
    "volume": "main",
    "abstract": "Energy-based models (EBMs) allow flexible specifications of probability distributions. However, sampling from EBMs is non-trivial, usually requiring approximate techniques such as Markov chain Monte Carlo (MCMC). A major downside of MCMC sampling is that it is often impossible to compute the divergence of the sampling distribution from the target distribution: therefore, the quality of the samples cannot be guaranteed. Here, we introduce quasi-rejection sampling (QRS), a simple extension of rejection sampling that performs approximate sampling, but, crucially, does provide divergence diagnostics (in terms of f-divergences, such as KL divergence and total variation distance). We apply QRS to sampling from discrete EBMs over text for controlled generation. We show that we can sample from such EBMs with arbitrary precision in exchange for sampling efficiency and quantify the trade-off between the two by means of the aforementioned diagnostics",
    "checked": true,
    "id": "ab23ff73e450f6b7e8d639e68320c7952c003694",
    "semantic_title": "an approximate sampler for energy-based models with divergence diagnostics",
    "citation_count": 9,
    "authors": [
      "Bryan Eikema",
      "Germ√°n Kruszewski",
      "Christopher R Dance",
      "Hady Elsahar",
      "Marc Dymetman"
    ]
  },
  "https://openreview.net/forum?id=aRtjVZvbpK": {
    "title": "A Unified Survey on Anomaly, Novelty, Open-Set, and Out of-Distribution Detection: Solutions and Future Challenges",
    "volume": "main",
    "abstract": "Machine learning models often encounter samples that are diverged from the training distribution. Failure to recognize an out-of-distribution (OOD) sample, and consequently assign that sample to an in-class label, significantly compromises the reliability of a model. The problem has gained significant attention due to its importance for safety deploying models in open-world settings. Detecting OOD samples is challenging due to the intractability of modeling all possible unknown distributions. To date, several research domains tackle the problem of detecting unfamiliar samples, including anomaly detection, novelty detection, one-class learning, open set recognition, and out-of-distribution detection. Despite having similar and shared concepts, out-of-distribution, open-set, and anomaly detection have been investigated independently. Accordingly, these research avenues have not crosspollinated, creating research barriers. While some surveys intend to provide an overview of these approaches, they seem to only focus on a specific domain without examining the relationship between different domains. This survey aims to provide a cross-domain and comprehensive review of numerous eminent works in respective areas while identifying their commonalities. Researchers can benefit from the overview of research advances in different fields and develop future methodology synergistically. Furthermore, to the best of our knowledge, while there are surveys in anomaly detection or one-class learning, there is no comprehensive or up-to-date survey on out-of-distribution detection, which this survey covers extensively. Finally, having a unified cross-domain perspective, this study discusses and sheds light on future lines of research, intending to bring these fields closer together",
    "checked": false,
    "id": "8b153cc2c7f5ea9f307f12ea945a5e9196ee5c52",
    "semantic_title": "a unified survey on anomaly, novelty, open-set, and out-of-distribution detection: solutions and future challenges",
    "citation_count": 199,
    "authors": [
      "Mohammadreza Salehi",
      "Hossein Mirzaei",
      "Dan Hendrycks",
      "Yixuan Li",
      "Mohammad Hossein Rohban",
      "Mohammad Sabokrou"
    ]
  },
  "https://openreview.net/forum?id=oRjk5V9eDp": {
    "title": "Bayesian Methods for Constraint Inference in Reinforcement Learning",
    "volume": "main",
    "abstract": "Learning constraints from demonstrations provides a natural and efficient way to improve the safety of AI systems; however, prior work only considers learning a single, point-estimate of the constraints. By contrast, we consider the problem of inferring constraints from demonstrations using a Bayesian perspective. We propose Bayesian Inverse Constraint Reinforcement Learning (BICRL), a novel approach that infers a posterior probability distribution over constraints from demonstrated trajectories. The main advantages of BICRL, compared to prior constraint inference algorithms, are (1) the freedom to infer constraints from partial trajectories and even from disjoint state-action pairs, (2) the ability to infer constraints from suboptimal demonstrations and in stochastic environments, and (3) the opportunity to use the posterior distribution over constraints in order to implement active learning and robust policy optimization techniques. We show that BICRL outperforms pre-existing constraint learning approaches, leading to more accurate constraint inference and consequently safer policies. We further propose Hierarchical BICRL that infers constraints locally in sub-spaces of the entire domain and then composes global constraint estimates leading to accurate and computationally efficient constraint estimation",
    "checked": true,
    "id": "ef4bdf801ce6475b1a2f4f148f4e964aa6ca6d43",
    "semantic_title": "bayesian methods for constraint inference in reinforcement learning",
    "citation_count": 9,
    "authors": [
      "Dimitris Papadimitriou",
      "Usman Anwar",
      "Daniel S. Brown"
    ]
  },
  "https://openreview.net/forum?id=LHAbHkt6Aq": {
    "title": "A Crisis In Simulation-Based Inference? Beware, Your Posterior Approximations Can Be Unfaithful",
    "volume": "main",
    "abstract": "We present extensive empirical evidence showing that current Bayesian simulation-based inference algorithms can produce computationally unfaithful posterior approximations. Our results show that all benchmarked algorithms -- (S)NPE, (S)NRE, SNL and variants of ABC -- can yield overconfident posterior approximations, which makes them unreliable for scientific use cases and falsificationist inquiry. Failing to address this issue may reduce the range of applicability of simulation-based inference. For this reason, we argue that research efforts should be made towards theoretical and methodological developments of conservative approximate inference algorithms and present research directions towards this objective. In this regard, we show empirical evidence that ensembling posterior surrogates provides more reliable approximations and mitigates the issue",
    "checked": true,
    "id": "d386cb044775dfb6f7f6fccbbaa3952b69a3066e",
    "semantic_title": "a crisis in simulation-based inference? beware, your posterior approximations can be unfaithful",
    "citation_count": 25,
    "authors": [
      "Joeri Hermans",
      "Arnaud Delaunoy",
      "Fran√ßois Rozet",
      "Antoine Wehenkel",
      "Volodimir Begy",
      "Gilles Louppe"
    ]
  },
  "https://openreview.net/forum?id=tLG26QxoD8": {
    "title": "On Pseudo-Labeling for Class-Mismatch Semi-Supervised Learning",
    "volume": "main",
    "abstract": "When there are unlabeled Out-Of-Distribution (OOD) data from other classes, Semi-Supervised Learning (SSL) methods suffer from severe performance degradation and even get worse than merely training on labeled data. In this paper, we empirically analyze Pseudo-Labeling (PL) in class-mismatched SSL. PL is a simple and representative SSL method that transforms SSL problems into supervised learning by creating pseudo-labels for unlabeled data according to the model's prediction. We aim to answer two main questions: (1) How do OOD data influence PL? (2) What is the proper usage of OOD data with PL? First, we show that the major problem of PL is imbalanced pseudo-labels on OOD data. Second, we find that OOD data can help classify In-Distribution (ID) data given their OOD ground truth labels. Based on the findings, we propose to improve PL in class-mismatched SSL with two components -- Re-balanced Pseudo-Labeling (RPL) and Semantic Exploration Clustering (SEC). RPL re-balances pseudo-labels of high-confidence data, which simultaneously filters out OOD data and addresses the imbalance problem. SEC uses balanced clustering on low-confidence data to create pseudo-labels on extra classes, simulating the process of training with ground truth. Experiments show that our method achieves steady improvement over supervised baseline and state-of-the-art performance under all class mismatch ratios on different benchmarks",
    "checked": false,
    "id": "d8530a37603e0dbcbfdf49d2726fd8dbfe7c47e8",
    "semantic_title": "distribution-aware semantics-oriented pseudo-label for imbalanced semi-supervised learning",
    "citation_count": 24,
    "authors": [
      "Lu Han",
      "Han-Jia Ye",
      "De-Chuan Zhan"
    ]
  },
  "https://openreview.net/forum?id=bN2vWLTh0P": {
    "title": "Reinventing Policy Iteration under Time Inconsistency",
    "volume": "main",
    "abstract": "Policy iteration (PI) is a fundamental policy search algorithm in standard reinforcement learning (RL) setting, which can be shown to converge to an optimal policy by policy improvement theorems. However, the standard PI relies on Bellman's Principle of Optimality, which might be violated by some specifications of objectives (also known as time-inconsistent (TIC) objectives), such as non-exponentially discounted reward functions. The use of standard PI under TIC objectives has thus been marked with questions regarding the convergence of its policy improvement scheme and the optimality of its termination policy, often leading to its avoidance. In this paper, we consider an infinite-horizon TIC RL setting and formally present an alternative type of optimality drawn from game theory, i.e., subgame perfect equilibrium (SPE), that attempts to resolve the aforementioned questions. We first analyze standard PI under the SPE type of optimality, revealing its merits and insufficiencies. Drawing on these observations, we propose backward Q-learning (bwdQ), a new algorithm in the approximate PI family that targets SPE policy under non-exponentially discounted reward functions. Finally, with two TIC gridworld environments, we demonstrate the implications of our theoretical findings on the behavior of bwdQ and other approximate PI variants",
    "checked": true,
    "id": "1491714962b285fab35e041ebfd861e483b761e3",
    "semantic_title": "reinventing policy iteration under time inconsistency",
    "citation_count": 4,
    "authors": [
      "Nixie S Lesmana",
      "Huangyuan Su",
      "Chi Seng Pun"
    ]
  },
  "https://openreview.net/forum?id=YiOI0vqJ0n": {
    "title": "Nonparametric Learning of Two-Layer ReLU Residual Units",
    "volume": "main",
    "abstract": "We describe an algorithm that learns two-layer residual units using rectified linear unit (ReLU) activation: suppose the input $\\mathbf{x}$ is from a distribution with support space $\\mathbb{R}^d$ and the ground-truth generative model is a residual unit of this type, given by $\\mathbf{y} = \\boldsymbol{B}^\\ast\\left[\\left(\\boldsymbol{A}^\\ast\\mathbf{x}\\right)^+ + \\mathbf{x}\\right]$, where ground-truth network parameters $\\boldsymbol{A}^\\ast \\in \\mathbb{R}^{d\\times d}$ represent a full-rank matrix with nonnegative entries and $\\boldsymbol{B}^\\ast \\in \\mathbb{R}^{m\\times d}$ is full-rank with $m \\geq d$ and for $\\boldsymbol{c} \\in \\mathbb{R}^d$, $[\\boldsymbol{c}^{+}]_i = \\max\\{0, c_i\\}$. We design layer-wise objectives as functionals whose analytic minimizers express the exact ground-truth network in terms of its parameters and nonlinearities. Following this objective landscape, learning residual units from finite samples can be formulated using convex optimization of a nonparametric function: for each layer, we first formulate the corresponding empirical risk minimization (ERM) as a positive semi-definite quadratic program (QP), then we show the solution space of the QP can be equivalently determined by a set of linear inequalities, which can then be efficiently solved by linear programming (LP). We further prove the strong statistical consistency of our algorithm, and demonstrate its robustness and sample efficiency through experimental results on synthetic data and a set of benchmark regression datasets",
    "checked": true,
    "id": "0f6f91b57e5e4e957fd175e55471c84232fdbda9",
    "semantic_title": "nonparametric learning of two-layer relu residual units",
    "citation_count": 1,
    "authors": [
      "Zhunxuan Wang",
      "Linyun He",
      "Chunchuan Lyu",
      "Shay B Cohen"
    ]
  },
  "https://openreview.net/forum?id=uvDD9rN6Zz": {
    "title": "Stochastic Douglas-Rachford Splitting for Regularized Empirical Risk Minimization: Convergence, Mini-batch, and Implementation",
    "volume": "main",
    "abstract": "In this paper, we study the stochastic Douglas-Rachford splitting (SDRS) for general empirical risk minimization (ERM) problems with regularization. Our first contribution is to prove its convergence for both convex and strongly convex problems; the convergence rates are $O(1/\\sqrt{t})$ and $O(1/t)$, respectively. Since SDRS reduces to the stochastic proximal point algorithm (SPPA) when there is no regularization, it is pleasing to see the result matches that of SPPA, under the same mild conditions. We also propose the mini-batch version of SDRS that handles multiple samples simultaneously while maintaining the same efficiency as that of a single one, which is not a straight-forward extension in the context of stochastic proximal algorithms. We show that the mini-batch SDRS again enjoys the same convergence rate. Furthermore, we demonstrate that, for some of the canonical regularized ERM problems, each iteration of SDRS can be efficiently calculated either in closed form or in close to closed form via bisection---the resulting complexity is identical to, for example, the stochastic (sub)gradient method. Experiments on real data demonstrate its effectiveness in terms of convergence compared to SGD and its variants",
    "checked": true,
    "id": "1bbda32a7549d84237d4ba9fd55920f150de5f72",
    "semantic_title": "stochastic douglas-rachford splitting for regularized empirical risk minimization: convergence, mini-batch, and implementation",
    "citation_count": 0,
    "authors": [
      "Aysegul Bumin",
      "Kejun Huang"
    ]
  },
  "https://openreview.net/forum?id=9nhmKwLAWV": {
    "title": "Does Entity Abstraction Help Generative Transformers Reason?",
    "volume": "main",
    "abstract": "We study the utility of incorporating entity type abstractions into pre-trained Transformers and test these methods on four NLP tasks requiring different forms of logical reasoning: (1) compositional language understanding with text-based relational reasoning (CLUTRR), (2) abductive reasoning (ProofWriter), (3) multi-hop question answering (HotpotQA), and (4) conversational question answering (CoQA). We propose and empirically explore three ways to add such abstraction: (i) as additional input embeddings, (ii) as a separate sequence to encode, and (iii) as an auxiliary prediction task for the model. Overall, our analysis demonstrates that models with abstract entity knowledge performs better than without it. The best abstraction aware models achieved an overall accuracy of 88.8% and 91.8% compared to the baseline model achieving 62.9% and 89.8% on CLUTRR and ProofWriter respectively. However, for HotpotQA and CoQA, we find that F1 scores improve by only 0.5% on average. Our results suggest that the benefit of explicit abstraction is significant in formally defined logical reasoning settings requiring many reasoning hops, but point to the notion that it is less beneficial for NLP tasks having less formal logical structure",
    "checked": true,
    "id": "a85c6a003450ef1e6caed8a6494301ad581957ee",
    "semantic_title": "does entity abstraction help generative transformers reason?",
    "citation_count": 5,
    "authors": [
      "Nicolas Gontier",
      "Siva Reddy",
      "Christopher Pal"
    ]
  },
  "https://openreview.net/forum?id=1PfcmFTXoa": {
    "title": "Complex-Valued Autoencoders for Object Discovery",
    "volume": "main",
    "abstract": "Object-centric representations form the basis of human perception, and enable us to reason about the world and to systematically generalize to new settings. Currently, most works on unsupervised object discovery focus on slot-based approaches, which explicitly separate the latent representations of individual objects. While the result is easily interpretable, it usually requires the design of involved architectures. In contrast to this, we propose a comparatively simple approach ‚Äì the Complex AutoEncoder (CAE) ‚Äì that creates distributed object-centric representations. Following a coding scheme theorized to underlie object representations in biological neurons, its complex-valued activations represent two messages: their magnitudes express the presence of a feature, while the relative phase differences between neurons express which features should be bound together to create joint object representations. In contrast to previous approaches using complex-valued activations for object discovery, we present a fully unsupervised approach that is trained end-to-end ‚Äì resulting in significant improvements in performance and efficiency. Further, we show that the CAE achieves competitive or better unsupervised object discovery performance on simple multi-object datasets compared to a state-of-the-art slot-based approach while being up to 100 times faster to train",
    "checked": true,
    "id": "0bc95645df1845050e642b34dc7593ba9a16072f",
    "semantic_title": "complex-valued autoencoders for object discovery",
    "citation_count": 39,
    "authors": [
      "Sindy L√∂we",
      "Phillip Lippe",
      "Maja Rudolph",
      "Max Welling"
    ]
  },
  "https://openreview.net/forum?id=Sh3RF9JowK": {
    "title": "Learning Algorithms for Markovian Bandits:\\\\Is Posterior Sampling more Scalable than Optimism?",
    "volume": "main",
    "abstract": "In this paper, we study the scalability of model-based algorithms learning the optimal policy of a discounted \\blue{rested} Markovian bandit problem with $n$ arms. There are two categories of model-based reinforcement learning algorithms: Bayesian algorithms (like PSRL), and optimistic algorithms (like UCRL2 or UCBVI). A naive application of these algorithms is not scalable because the state-space is exponential in $n$. In this paper, we construct variants of these algorithms specially tailored to Markovian bandits (MB) that we call MB-PSRL, MB-UCRL2, and MB-UCBVI. \\blue{We consider an episodic setting with geometrically distributed episode length, and measure the performance of the algorithm in terms of regret (Bayesian regret for MB-PSRL and expected regret for MB-UCRL2 and MB-UCBVI)}. We prove that, for this setting, all algorithms have a low regret in $\\tilde{O}(S\\sqrt{nK})$ -- where $K$ is the number of episodes, $n$ is the number of arms and $S$ is the number of states of each arm. Up to a factor $\\sqrt{S}$, these regrets match the \\blue{Bayesian minimax regret} lower bound of $\\Omega(\\sqrt{SnK})$ that we also derive. Even if their theoretical regrets are comparable, the {\\it time complexities} of these algorithms vary greatly: We show that MB-UCRL2, as well as all algorithms that use bonuses on transition matrices have a { time} complexity that grows exponentially in $n$. In contrast, MB-UCBVI does not use bonuses on transition matrices and we show that it can be implemented efficiently, with a time complexity linear in $n$. Our numerical experiments show, however, that its empirical regret is large. Our Bayesian algorithm, MB-PSRL, enjoys the best of both worlds: its running time is linear in the number of arms and its empirical regret is the smallest of all algorithms. This is a new addition in the understanding of the power of Bayesian algorithms, that can often be tailored to the structure of the problems to learn",
    "checked": false,
    "id": "e9770e1fc0f9977a0e890546f4d9062e6a934a28",
    "semantic_title": "learning algorithms for markovian bandits: is posterior sampling more scalable than optimism?",
    "citation_count": 2,
    "authors": [
      "Nicolas Gast",
      "Bruno Gaujal",
      "Kimang Khun"
    ]
  },
  "https://openreview.net/forum?id=NmTMc3uD1G": {
    "title": "Modeling Object Dissimilarity for Deep Saliency Prediction",
    "volume": "main",
    "abstract": "Saliency prediction has made great strides over the past two decades, with current techniques modeling low-level information, such as color, intensity and size contrasts, and high-level ones, such as attention and gaze direction for entire objects. Despite this, these methods fail to account for the dissimilarity between objects, which affects human visual attention. In this paper, we introduce a detection-guided saliency prediction network that explicitly models the differences between multiple objects, such as their appearance and size dissimilarities. Our approach allows us to fuse our object dissimilarities with features extracted by any deep saliency prediction network. As evidenced by our experiments, this consistently boosts the accuracy of the baseline networks, enabling us to outperform the state-of-the-art models on three saliency benchmarks, namely SALICON, MIT300 and CAT2000. Our project page is at https://github.com/IVRL/DisSal",
    "checked": true,
    "id": "5d3828cce5ccce25783aa4486637dc22985aa0f6",
    "semantic_title": "modeling object dissimilarity for deep saliency prediction",
    "citation_count": 3,
    "authors": [
      "Bahar Aydemir",
      "Deblina Bhattacharjee",
      "Tong Zhang",
      "Seungryong Kim",
      "Mathieu Salzmann",
      "Sabine S√ºsstrunk"
    ]
  },
  "https://openreview.net/forum?id=YAVE6jfeJb": {
    "title": "Optimizing Intermediate Representations of Generative Models for Phase Retrieval",
    "volume": "main",
    "abstract": "Phase retrieval is the problem of reconstructing images from magnitude-only measurements. In many real-world applications the problem is underdetermined. When training data is available, generative models allow optimization in a lower-dimensional latent space, hereby constraining the solution set to those images that can be synthesized by the generative model. However, not all possible solutions are within the range of the generator. Instead, they are represented with some error. To reduce this representation error in the context of phase retrieval, we first leverage a novel variation of intermediate layer optimization (ILO) to extend the range of the generator while still producing images consistent with the training data. Second, we introduce new initialization schemes that further improve the quality of the reconstruction. With extensive experiments on the Fourier phase retrieval problem and thorough ablation studies, we can show the benefits of our modified ILO and the new initialization schemes. Additionally, we analyze the performance of our approach on the Gaussian phase retrieval problem",
    "checked": true,
    "id": "a4f718b776b0fc8deea07b6c9df96a7628571094",
    "semantic_title": "optimizing intermediate representations of generative models for phase retrieval",
    "citation_count": 1,
    "authors": [
      "Tobias Uelwer",
      "Sebastian Konietzny",
      "Stefan Harmeling"
    ]
  },
  "https://openreview.net/forum?id=35y5hv9fbb": {
    "title": "Algorithms and Theory for Supervised Gradual Domain Adaptation",
    "volume": "main",
    "abstract": "The phenomenon of data distribution evolving over time has been observed in a range of applications, calling the needs of adaptive learning algorithms. We thus study the problem of supervised gradual domain adaptation, where labeled data from shifting distributions are available to the learner along the trajectory, and we aim to learn a classifier on a target data distribution of interest. Under this setting, we provide the first generalization upper bound on the learning error under mild assumptions. Our results are algorithm agnostic, general for a range of loss functions, and only depend linearly on the averaged learning error across the trajectory. This shows significant improvement compared to the previous upper bound for unsupervised gradual domain adaptation, where the learning error on the target domain depends exponentially on the initial error on the source domain. Compared with the offline setting of learning from multiple domains, our results also suggest the potential benefits of the temporal structure among different domains in adapting to the target one. Empirically, our theoretical results imply that learning proper representations across the domains will effectively mitigate the learning errors. Motivated by these theoretical insights, we propose a min-max learning objective to learn the representation and classifier simultaneously. Experimental results on both semi-synthetic and large-scale real datasets corroborate our findings and demonstrate the effectiveness of our objectives",
    "checked": true,
    "id": "0b0126adcef98dedd96bd9f253dd3bb3569d1d3f",
    "semantic_title": "algorithms and theory for supervised gradual domain adaptation",
    "citation_count": 7,
    "authors": [
      "Jing Dong",
      "Shiji Zhou",
      "Baoxiang Wang",
      "Han Zhao"
    ]
  },
  "https://openreview.net/forum?id=ph3AYXpwEb": {
    "title": "Teacher's pet: understanding and mitigating biases in distillation",
    "volume": "main",
    "abstract": "Knowledge distillation is widely used as a means of improving the performance of a relatively simple ``student'' model using the predictions from a complex ``teacher'' model. Several works have shown that distillation significantly boosts the student's \\emph{overall} performance; however, are these gains uniform across all data subgroups? In this paper, we show that distillation can \\emph{harm} performance on certain subgroups, {e.g., classes with few associated samples}, compared to the vanilla student trained using the one-hot labels. We trace this behaviour to errors made by the teacher distribution being transferred to and \\emph{amplified} by the student model, and formally prove that distillation can indeed harm underrepresented subgroups in certain regression settings. To mitigate this problem, we present techniques which soften the teacher influence for subgroups where it is less reliable. Experiments on several image classification benchmarks show that these modifications of distillation maintain boost in overall accuracy, while additionally ensuring improvement in subgroup performance",
    "checked": true,
    "id": "b057d01576177dcf055dcc3601471b68190658f6",
    "semantic_title": "teacher's pet: understanding and mitigating biases in distillation",
    "citation_count": 25,
    "authors": [
      "Michal Lukasik",
      "Srinadh Bhojanapalli",
      "Aditya Krishna Menon",
      "Sanjiv Kumar"
    ]
  },
  "https://openreview.net/forum?id=LFkRUCalFt": {
    "title": "An Efficient One-Class SVM for Novelty Detection in IoT",
    "volume": "main",
    "abstract": "One-Class Support Vector Machines (OCSVM) are a common approach for novelty detection, due to their flexibility in fitting complex nonlinear boundaries between {normal} and {novel} data. Novelty detection is important in the Internet of Things (``IoT'') due to the threats these devices can present, and OCSVM often performs well in these environments due to the variety of devices, traffic patterns, and anomalies that IoT devices present. Unfortunately, conventional OCSVMs can introduce prohibitive memory and computational overhead at detection time. This work designs, implements and evaluates an efficient OCSVM for such practical settings. We extend Nystr\\\"om and (Gaussian) Sketching approaches to OCSVM, combining these methods with clustering and Gaussian mixture models to achieve 15-30x speedup in prediction time and 30-40x reduction in memory requirements without sacrificing detection accuracy. Here, the very nature of IoT devices is crucial: they tend to admit few modes of \\emph{normal} operation, allowing for efficient pattern compression",
    "checked": true,
    "id": "e9cf168e193075dc9bce9274ff4285e7091eb0d6",
    "semantic_title": "an efficient one-class svm for novelty detection in iot",
    "citation_count": 2,
    "authors": [
      "Kun Yang",
      "Samory Kpotufe",
      "Nick Feamster"
    ]
  },
  "https://openreview.net/forum?id=63sJsCmq6Q": {
    "title": "Competition over data: how does data purchase affect users?",
    "volume": "main",
    "abstract": "As the competition among machine learning (ML) predictors is widespread in practice, it becomes increasingly important to understand the impact and biases arising from such competition. One critical aspect of ML competition is that ML predictors are constantly updated by acquiring additional data during the competition. Although this active data acquisition can largely affect the overall competition environment, it has not been well-studied before. In this paper, we study what happens when ML predictors can purchase additional data during the competition. We introduce a new environment in which ML predictors use active learning algorithms to effectively acquire labeled data within their budgets while competing against each other. We empirically show that the overall performance of an ML predictor improves when predictors can purchase additional labeled data. Surprisingly, however, the quality that users experience---i.e., the accuracy of the predictor selected by each user---can decrease even as the individual predictors get better. We demonstrate that this phenomenon naturally arises due to a trade-off whereby competition pushes each predictor to specialize in a subset of the population while data purchase has the effect of making predictors more uniform. With comprehensive experiments, we show that our findings are robust against different modeling assumptions",
    "checked": true,
    "id": "a993c8842917c1d0e661fc368a3c3a690167772a",
    "semantic_title": "competition over data: how does data purchase affect users?",
    "citation_count": 5,
    "authors": [
      "Yongchan Kwon",
      "Tony A Ginart",
      "James Zou"
    ]
  },
  "https://openreview.net/forum?id=lf0lr4AYM6": {
    "title": "Diffusion Models for Video Prediction and Infilling",
    "volume": "main",
    "abstract": "Predicting and anticipating future outcomes or reasoning about missing information in a sequence are critical skills for agents to be able to make intelligent decisions. This requires strong, temporally coherent generative capabilities. Diffusion models have shown remarkable success in several generative tasks, but have not been extensively explored in the video domain. We present Random-Mask Video Diffusion (RaMViD), which extends image diffusion models to videos using 3D convolutions, and introduces a new conditioning technique during training. By varying the mask we condition on, the model is able to perform video prediction, infilling, and upsampling. Due to our simple conditioning scheme, we can utilize the same architecture as used for unconditional training, which allows us to train the model in a conditional and unconditional fashion at the same time. We evaluate RaMViD on two benchmark datasets for video prediction, on which we achieve state-of-the-art results, and one for video generation. High-resolution videos are provided at https://sites.google.com/view/video-diffusion-prediction",
    "checked": true,
    "id": "4175003efc9cbf2781c0fd8ef8e6bcb756316296",
    "semantic_title": "diffusion models for video prediction and infilling",
    "citation_count": 137,
    "authors": [
      "Tobias H√∂ppe",
      "Arash Mehrjou",
      "Stefan Bauer",
      "Didrik Nielsen",
      "Andrea Dittadi"
    ]
  },
  "https://openreview.net/forum?id=Au1LNKmRvh": {
    "title": "Efficient Gradient Flows in Sliced-Wasserstein Space",
    "volume": "main",
    "abstract": "Minimizing functionals in the space of probability distributions can be done with Wasser- stein gradient flows. To solve them numerically, a possible approach is to rely on the Jordan‚ÄìKinderlehrer‚ÄìOtto (JKO) scheme which is analogous to the proximal scheme in Euclidean spaces. However, it requires solving a nested optimization problem at each it- eration, and is known for its computational challenges, especially in high dimension. To alleviate it, very recent works propose to approximate the JKO scheme leveraging Brenier's theorem, and using gradients of Input Convex Neural Networks to parameterize the density (JKO-ICNN). However, this method comes with a high computational cost and stability is- sues. Instead, this work proposes to use gradient flows in the space of probability measures endowed with the sliced-Wasserstein (SW) distance. We argue that this method is more flex- ible than JKO-ICNN, since SW enjoys a closed-form differentiable approximation. Thus, the density at each step can be parameterized by any generative model which alleviates the computational burden and makes it tractable in higher dimensions",
    "checked": true,
    "id": "bd278f7f557857f3938b3f722a52aec2c799d36e",
    "semantic_title": "efficient gradient flows in sliced-wasserstein space",
    "citation_count": 21,
    "authors": [
      "Cl√©ment Bonet",
      "Nicolas Courty",
      "Fran√ßois Septier",
      "Lucas Drumetz"
    ]
  },
  "https://openreview.net/forum?id=Ii7UeHc0mO": {
    "title": "Approximate Policy Iteration with Bisimulation Metrics",
    "volume": "main",
    "abstract": "Bisimulation metrics define a distance measure between states of a Markov decision process (MDP) based on a comparison of reward sequences. Due to this property they provide theoretical guarantees in value function approximation (VFA). In this work we first prove that bisimulation and $\\pi$-bisimulation metrics can be defined via a more general class of Sinkhorn distances, which unifies various state similarity metrics used in recent work. Then we describe an approximate policy iteration (API) procedure that uses a bisimulation-based discretization of the state space for VFA and prove asymptotic performance bounds. Next, we bound the difference between $\\pi$-bisimulation metrics in terms of the change in the policies themselves. Based on these results, we design an API($\\alpha$) procedure that employs conservative policy updates and enjoys better performance bounds than the naive API approach. We discuss how such API procedures map onto practical actor-critic methods that use bisimulation metrics for state representation learning. Lastly, we validate our theoretical results and investigate their practical implications via a controlled empirical analysis based on an implementation of bisimulation-based API for finite MDPs",
    "checked": true,
    "id": "2cc36e4d0b6bf6913ce68f49a80a9c098e3039c7",
    "semantic_title": "approximate policy iteration with bisimulation metrics",
    "citation_count": 8,
    "authors": [
      "Mete Kemertas",
      "Allan Douglas Jepson"
    ]
  },
  "https://openreview.net/forum?id=3v78awEzyB": {
    "title": "Exposing Outlier Exposure: What Can Be Learned From Few, One, and Zero Outlier Images",
    "volume": "main",
    "abstract": "Due to the intractability of characterizing everything that looks unlike the normal data, anomaly detection (AD) is traditionally treated as an unsupervised problem utilizing only normal samples. However, it has recently been found that unsupervised image AD can be drastically improved through the utilization of huge corpora of random images to represent anomalousness; a technique which is known as Outlier Exposure. In this paper we show that specialized AD learning methods seem unnecessary for state-of-the-art performance, and furthermore one can achieve strong performance with just a small collection of Outlier Exposure data, contradicting common assumptions in the field of AD. We find that standard classifiers and semi-supervised one-class methods trained to discern between normal samples and relatively few random natural images are able to outperform the current state of the art on an established AD benchmark with ImageNet. Further experiments reveal that even one well-chosen outlier sample is sufficient to achieve decent performance on this benchmark (79.3% AUC). We investigate this phenomenon and find that one-class methods are more robust to the choice of training outliers, indicating that there are scenarios where these are still more useful than standard classifiers. Additionally, we include experiments that delineate the scenarios where our results hold. Lastly, no training samples are necessary when one uses the representations learned by CLIP, a recent foundation model, which achieves state-of-the-art AD results on CIFAR-10 and ImageNet in a zero-shot setting",
    "checked": true,
    "id": "5b9f294a37799a454543c4ce3d9cb40bdb2ad9a4",
    "semantic_title": "exposing outlier exposure: what can be learned from few, one, and zero outlier images",
    "citation_count": 46,
    "authors": [
      "Philipp Liznerski",
      "Lukas Ruff",
      "Robert A. Vandermeulen",
      "Billy Joe Franks",
      "Klaus Robert Muller",
      "Marius Kloft"
    ]
  },
  "https://openreview.net/forum?id=15SoThZmtU": {
    "title": "Mitigating Catastrophic Forgetting in Spiking Neural Networks through Threshold Modulation",
    "volume": "main",
    "abstract": "Artificial Neural Networks (ANNs) trained with Backpropagation and Stochastic Gradient Descent (SGD) suffer from the problem of Catastrophic Forgetting; when learning tasks sequentially, the ANN tends to abruptly forget previous knowledge upon being trained on a new task. On the other hand, biological neural networks do not suffer from this problem. Spiking Neural Networks (SNNs) are a class of Neural Networks that are closer to biological networks than ANNs and their intrinsic properties inspired from biology could alleviate the problem of Catastrophic Forgetting. In this paper, we investigate if the firing threshold mechanism of SNNs can be used to gate the activity of the network in order to reduce catastrophic forgetting. To this end, we evolve a Neuromodulatory Network that adapts the thresholds of an SNN depending on the spiking activity of the previous layer. Our experiments on different datasets show that the neurmodulated SNN can mitigate forgetting significantly with respect to a fixed threshold SNN. We also show that the evolved Neuromodulatory Network can generalize to multiple new scenarios and analyze its behavior",
    "checked": true,
    "id": "f0327031895f127d66b72e20d2200f648bbec1c5",
    "semantic_title": "mitigating catastrophic forgetting in spiking neural networks through threshold modulation",
    "citation_count": 9,
    "authors": [
      "Ilyass Hammouamri",
      "Timoth√©e Masquelier",
      "Dennis George Wilson"
    ]
  },
  "https://openreview.net/forum?id=1AxQpKmiTc": {
    "title": "ZerO Initialization: Initializing Neural Networks with only Zeros and Ones",
    "volume": "main",
    "abstract": "Deep neural networks are usually initialized with random weights, with adequately selected initial variance to ensure stable signal propagation during training. However, selecting the appropriate variance becomes challenging especially as the number of layers grows. In this work, we replace random weight initialization with a fully deterministic initialization scheme, viz., ZerO, which initializes the weights of networks with only zeros and ones (up to a normalization factor), based on identity and Hadamard transforms. Through both theoretical and empirical studies, we demonstrate that ZerO is able to train networks without damaging their expressivity. Applying ZerO on ResNet achieves state-of-the-art performance on various datasets, including ImageNet, which suggests random weights may be unnecessary for network initialization. In addition, ZerO has many benefits, such as training ultra deep networks (without batch-normalization), exhibiting low-rank learning trajectories that result in low-rank and sparse solutions, and improving training reproducibility",
    "checked": true,
    "id": "ad1aec71ff9e65bab89f3c7ad168b3c91ffceb4c",
    "semantic_title": "zero initialization: initializing neural networks with only zeros and ones",
    "citation_count": 26,
    "authors": [
      "Jiawei Zhao",
      "Florian Tobias Schaefer",
      "Anima Anandkumar"
    ]
  },
  "https://openreview.net/forum?id=Y4mgmw9OgV": {
    "title": "A Rigorous Study Of The Deep Taylor Decomposition",
    "volume": "main",
    "abstract": "Saliency methods attempt to explain deep neural networks by highlighting the most salient features of a sample. Some widely used methods are based on a theoretical framework called Deep Taylor Decomposition (DTD), which formalizes the recursive application of the Taylor Theorem to the network's layers. However, recent work has found these methods to be independent of the network's deeper layers and appear to respond only to lower-level image structure. Here, we investigate DTD theory to better understand this perplexing behavior and found that the Deep Taylor Decomposition is equivalent to the basic gradient$\\times$input method when the Taylor root points (an important parameter of the algorithm chosen by the user) are locally constant. If the root points are locally input-dependent, then one can justify any explanation. In this case, the theory is under-constrained. In an empirical evaluation, we find that DTD roots do not lie the same linear regions as the input -- contrary to a fundamental assumption of the Taylor Theorem. The theoretical foundations of DTD were cited as a source of reliability for the explanations. However, our findings urge caution in making such claims",
    "checked": true,
    "id": "c25159007564e22e63d4926c940de75e104b823b",
    "semantic_title": "a rigorous study of the deep taylor decomposition",
    "citation_count": 4,
    "authors": [
      "Leon Sixt",
      "Tim Landgraf"
    ]
  },
  "https://openreview.net/forum?id=e4Bb0b3QgJ": {
    "title": "Fail-Safe Adversarial Generative Imitation Learning",
    "volume": "main",
    "abstract": "For flexible yet safe imitation learning (IL), we propose theory and a modular method, with a safety layer that enables a closed-form probability density/gradient of the safe generative continuous policy, end-to-end generative adversarial training, and worst-case safety guarantees. The safety layer maps all actions into a set of safe actions, and uses the change-of-variables formula plus additivity of measures for the density. The set of safe actions is inferred by first checking safety of a finite sample of actions via adversarial reachability analysis of fallback maneuvers, and then concluding on the safety of these actions' neighborhoods using, e.g., Lipschitz continuity. We provide theoretical analysis showing the robustness advantage of using the safety layer already during training (imitation error linear in the horizon) compared to only using it at test time (up to quadratic error). In an experiment on real-world driver interaction data, we empirically demonstrate tractability, safety and imitation performance of our approach",
    "checked": true,
    "id": "1015c935649824664c206fabe546f13995d156aa",
    "semantic_title": "fail-safe adversarial generative imitation learning",
    "citation_count": 2,
    "authors": [
      "Philipp Geiger",
      "Christoph-Nikolas Straehle"
    ]
  },
  "https://openreview.net/forum?id=MHOAEiTlen": {
    "title": "DHA: End-to-End Joint Optimization of Data Augmentation Policy, Hyper-parameter and Architecture",
    "volume": "main",
    "abstract": "Automated machine learning (AutoML) usually involves several crucial components, such as Data Augmentation (DA) policy, Hyper-Parameter Optimization (HPO), and Neural Architecture Search (NAS). Although many strategies have been developed for automating these components in separation, joint optimization of these components remains challenging due to the largely increased search dimension and the variant input types of each component. In parallel to this, the common practice of searching for the optimal architecture first and then retraining it before deployment in NAS often suffers from the low-performance correlation between the searching and retraining stages. An end-to-end solution that integrates the AutoML components and returns a ready-to-use model at the end of the search is desirable. In view of these, we propose DHA, which achieves joint optimization of Data augmentation policy, Hyper-parameter, and Architecture. Specifically, end-to-end NAS is achieved in a differentiable manner by optimizing a compressed lower-dimensional feature space, while DA policy and HPO are regarded as dynamic schedulers, which adapt themselves to the update of network parameters and network architecture at the same time. Experiments show that DHA achieves state-of-the-art (SOTA) results on various datasets and search spaces. To the best of our knowledge, we are the first to efficiently and jointly optimize DA policy, NAS, and HPO in an end-to-end manner without retraining",
    "checked": true,
    "id": "e62b102e072bb83506f858e15ebd8be5030024be",
    "semantic_title": "dha: end-to-end joint optimization of data augmentation policy, hyper-parameter and architecture",
    "citation_count": 10,
    "authors": [
      "kaichen zhou",
      "Lanqing HONG",
      "Shoukang Hu",
      "Fengwei Zhou",
      "Binxin Ru",
      "Jiashi Feng",
      "Zhenguo Li"
    ]
  },
  "https://openreview.net/forum?id=e7A0B99zJf": {
    "title": "Data Leakage in Federated Averaging",
    "volume": "main",
    "abstract": "Recent attacks have shown that user data can be recovered from FedSGD updates, thus breaking privacy. However, these attacks are of limited practical relevance as federated learning typically uses the FedAvg algorithm. Compared to FedSGD, recovering data from FedAvg updates is much harder as: (i) the updates are computed at unobserved intermediate network weights, (ii) a large number of batches are used, and (iii) labels and network weights vary simultaneously across client steps. In this work, we propose a new optimization-based attack which successfully attacks FedAvg by addressing the above challenges. First, we solve the optimization problem using automatic differentiation that forces a simulation of the client's update that generates the unobserved parameters for the recovered labels and inputs to match the received client update. Second, we address the large number of batches by relating images from different epochs with a permutation invariant prior. Third, we recover the labels by estimating the parameters of existing FedSGD attacks at every FedAvg step. On the popular FEMNIST dataset, we demonstrate that on average we successfully recover >45% of the client's images from realistic FedAvg updates computed on 10 local epochs of 10 batches each with 5 images, compared to only <10% using the baseline. Our findings show many real-world federated learning implementations based on FedAvg are vulnerable",
    "checked": true,
    "id": "4fe85cbbde2185d8515fd9f396d97fe475843e80",
    "semantic_title": "data leakage in federated averaging",
    "citation_count": 32,
    "authors": [
      "Dimitar Iliev Dimitrov",
      "Mislav Balunovic",
      "Nikola Konstantinov",
      "Martin Vechev"
    ]
  },
  "https://openreview.net/forum?id=lE7K4n1Esk": {
    "title": "On the Adversarial Robustness of Vision Transformers",
    "volume": "main",
    "abstract": "Following the success in advancing natural language processing and understanding, transformers are expected to bring revolutionary changes to computer vision. This work provides a comprehensive study on the robustness of vision transformers (ViTs) against adversarial perturbations. Tested on various white-box and transfer attack settings, we find that ViTs possess better adversarial robustness when compared with MLP-Mixer and convolutional neural networks (CNNs) including ConvNeXt, and this observation also holds for certified robustness. Through frequency analysis and feature visualization, we summarize the following main observations contributing to the improved robustness of ViTs: 1) Features learned by ViTs contain less high-frequency patterns that have spurious correlation, which helps explain why ViTs are less sensitive to high-frequency perturbations than CNNs and MLP-Mixer, and there is a high correlation between how much the model learns high-frequency features and its robustness against different frequency-based perturbations. 2) Introducing convolutional or tokens-to-token blocks for learning high-frequency features in ViTs can improve classification accuracy but at the cost of adversarial robustness. 3) Modern CNN designs that borrow techniques from ViTs including activation function, layer norm, larger kernel size to imitate the global attention, and patchify the images as inputs, etc., could help bridge the performance gap between ViTs and CNNs not only in terms of performance, but also certified and empirical adversarial robustness. Moreover, we show adversarial training is also applicable to ViT for training robust models, and sharpness-aware minimization can also help improve robustness, while pre-training with clean images on larger datasets does not significantly improve adversarial robustness",
    "checked": true,
    "id": "0def290ae38abb4a04e35e0bcdc86b71d237f494",
    "semantic_title": "on the adversarial robustness of vision transformers",
    "citation_count": 146,
    "authors": [
      "Rulin Shao",
      "Zhouxing Shi",
      "Jinfeng Yi",
      "Pin-Yu Chen",
      "Cho-Jui Hsieh"
    ]
  },
  "https://openreview.net/forum?id=7iSYW1FRWA": {
    "title": "Behind the Machine's Gaze: Neural Networks with Biologically-inspired Constraints Exhibit Human-like Visual Attention",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "3b5361986d79be91e6d490e09fed49d9ee5abab2",
    "semantic_title": "behind the machine's gaze: neural networks with biologically-inspired constraints exhibit human-like visual attention",
    "citation_count": 7,
    "authors": [
      "Leo Schwinn",
      "Doina Precup",
      "Bjoern Eskofier",
      "Dario Zanca"
    ]
  },
  "https://openreview.net/forum?id=cxp7n9q5c4": {
    "title": "Structured Uncertainty in the Observation Space of Variational Autoencoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "James Langley",
      "Miguel Monteiro",
      "Charles Jones",
      "Nick Pawlowski",
      "Ben Glocker"
    ]
  },
  "https://openreview.net/forum?id=CExeD0jpB6": {
    "title": "Distributed Stochastic Algorithms for High-rate Streaming Principal Component Analysis",
    "volume": "main",
    "abstract": "This paper considers the problem of estimating the principal eigenvector of a covariance matrix from independent and identically distributed data samples in streaming settings. The streaming rate of data in many contemporary applications can be high enough that a single processor cannot finish an iteration of existing methods for eigenvector estimation before a new sample arrives. This paper formulates and analyzes a distributed variant of the classical Krasulina's method (D-Krasulina) that can keep up with the high streaming rate of data by distributing the computational load across multiple processing nodes. The analysis improves upon the one in (Balsubramani et al., 2013) for the original Krasulina's method and shows that---under appropriate conditions---D-Krasulina converges to the principal eigenvector in an order-wise optimal manner; i.e., after receiving $M$ samples across all nodes, its estimation error can be $O(1/M)$. In order to reduce the network communication overhead, the paper also develops and analyzes a mini-batch extension of D-Krasulina, which is termed DM-Krasulina. The analysis of DM-Krasulina shows that it can also achieve order-optimal estimation error rates under appropriate conditions, even when some samples have to be discarded within the network due to communication latency. Finally, experiments are performed over synthetic and real-world data to validate the convergence behaviors of D-Krasulina and DM-Krasulina in high-rate streaming settings",
    "checked": true,
    "id": "1c3eaeb7a67d3c53f0badc408dede57f2bf52dd7",
    "semantic_title": "distributed stochastic algorithms for high-rate streaming principal component analysis",
    "citation_count": 11,
    "authors": [
      "Haroon Raja",
      "Waheed Bajwa"
    ]
  },
  "https://openreview.net/forum?id=9NjqD9i48M": {
    "title": "Benchmarking Progress to Infant-Level Physical Reasoning in AI",
    "volume": "main",
    "abstract": "To what extent do modern AI systems comprehend the physical world? We introduce the open-access Infant-Level Physical Reasoning Benchmark (InfLevel) to gain insight into this question. We evaluate ten neural-network architectures developed for video understanding on tasks designed to test these models' ability to reason about three essential physical principles which researchers have shown to guide human infants' physical understanding. We explore the sensitivity of each AI system to the continuity of objects as they travel through space and time, to the solidity of objects, and to gravity. We find strikingly consistent results across 60 experiments with multiple systems, training regimes, and evaluation metrics: current popular visual-understanding systems are at or near chance on all three principles of physical reasoning. We close by suggesting some potential ways forward",
    "checked": true,
    "id": "414a476f83634e3b452b243ed7460c9ef3d1aaa4",
    "semantic_title": "benchmarking progress to infant-level physical reasoning in ai",
    "citation_count": 16,
    "authors": [
      "Luca Weihs",
      "Amanda Yuile",
      "Ren√©e Baillargeon",
      "Cynthia Fisher",
      "Gary Marcus",
      "Roozbeh Mottaghi",
      "Aniruddha Kembhavi"
    ]
  },
  "https://openreview.net/forum?id=Hp4g7FAXXG": {
    "title": "Linear algebra with transformers",
    "volume": "main",
    "abstract": "Transformers can learn to perform numerical computations from examples only. I study nine problems of linear algebra, from basic matrix operations to eigenvalue decomposition and inversion, and introduce and discuss four encoding schemes to represent real numbers. On all problems, transformers trained on sets of random matrices achieve high accuracies (over 90\\%). The models are robust to noise, and can generalize out of their training distribution. In particular, models trained to predict Laplace-distributed eigenvalues generalize to different classes of matrices: Wigner matrices or matrices with positive eigenvalues. The reverse is not true",
    "checked": true,
    "id": "45ece6f3b0a319dba60c20b3013b5161dd49c58b",
    "semantic_title": "linear algebra with transformers",
    "citation_count": 59,
    "authors": [
      "Francois Charton"
    ]
  },
  "https://openreview.net/forum?id=aIoEkwc2oB": {
    "title": "INR-V: A Continuous Representation Space for Video-based Generative Tasks",
    "volume": "main",
    "abstract": "Generating videos is a complex task that is accomplished by generating a set of temporally coherent images frame-by-frame. This limits the expressivity of videos to only image-based operations on the individual video frames needing network designs to obtain temporally coherent trajectories in the underlying image space. We propose INR-V, a video representation network that learns a continuous space for video-based generative tasks. INR-V parameterizes videos using implicit neural representations (INRs), a multi-layered perceptron that predicts an RGB value for each input pixel location of the video. The INR is predicted using a meta-network which is a hypernetwork trained on neural representations of multiple video instances. Later, the meta-network can be sampled to generate diverse novel videos enabling many downstream video-based generative tasks. Interestingly, we find that conditional regularization and progressive weight initialization play a crucial role in obtaining INR-V. The representation space learned by INR-V is more expressive than an image space showcasing many interesting properties not possible with the existing works. For instance, INR-V can smoothly interpolate intermediate videos between known video instances (such as intermediate identities, expressions, and poses in face videos). It can also in-paint missing portions in videos to recover temporally coherent full videos. In this work, we evaluate the space learned by INR-V on diverse generative tasks such as video interpolation, novel video generation, video inversion, and video inpainting against the existing baselines. INR-V significantly outperforms the baselines on several of these demonstrated tasks, clearly showing the potential of the proposed representation space",
    "checked": true,
    "id": "6310ac90d660b3359df2c375670799d08d5db454",
    "semantic_title": "inr-v: a continuous representation space for video-based generative tasks",
    "citation_count": 7,
    "authors": [
      "Bipasha Sen",
      "Aditya Agarwal",
      "Vinay P Namboodiri",
      "C.V. Jawahar"
    ]
  },
  "https://openreview.net/forum?id=ZPQhzTSWA7": {
    "title": "A Simple Convergence Proof of Adam and Adagrad",
    "volume": "main",
    "abstract": "We provide a simple proof of convergence covering both the Adam and Adagrad adaptive optimization algorithms when applied to smooth (possibly non-convex) objective functions with bounded gradients. We show that in expectation, the squared norm of the objective gradient averaged over the trajectory has an upper-bound which is explicit in the constants of the problem, parameters of the optimizer, the dimension $d$, and the total number of iterations $N$. This bound can be made arbitrarily small, and with the right hyper-parameters, Adam can be shown to converge with the same rate of convergence $O(d\\ln(N)/\\sqrt{N})$. When used with the default parameters, Adam doesn't converge, however, and just like constant step-size SGD, it moves away from the initialization point faster than Adagrad, which might explain its practical success. Finally, we obtain the tightest dependency on the heavy ball momentum decay rate $\\beta_1$ among all previous convergence bounds for non-convex Adam and Adagrad, improving from $O((1-\\beta_1)^{-3})$ to $O((1-\\beta_1)^{-1})$",
    "checked": true,
    "id": "05b4436d504d5615801639a120a2c8eca7cbaabd",
    "semantic_title": "a simple convergence proof of adam and adagrad",
    "citation_count": 159,
    "authors": [
      "Alexandre D√©fossez",
      "Leon Bottou",
      "Francis Bach",
      "Nicolas Usunier"
    ]
  },
  "https://openreview.net/forum?id=atJHLVyBi8": {
    "title": "On the Paradox of Certified Training",
    "volume": "main",
    "abstract": "Certified defenses based on convex relaxations are an established technique for training provably robust models. The key component is the choice of relaxation, varying from simple intervals to tight polyhedra. Counterintuitively, loose interval-based training often leads to higher certified robustness than what can be achieved with tighter relaxations, which is a well-known but poorly understood paradox. While recent works introduced various improvements aiming to circumvent this issue in practice, the fundamental problem of training models with high certified robustness remains unsolved. In this work, we investigate the underlying reasons behind the paradox and identify two key properties of relaxations, beyond tightness, that impact certified training dynamics: continuity and sensitivity. Our extensive experimental evaluation with a number of popular convex relaxations provides strong evidence that these factors can explain the drop in certified robustness observed for tighter relaxations. We also systematically explore modifications of existing relaxations and discover that improving unfavorable properties is challenging, as such attempts often harm other properties, revealing a complex tradeoff. Our findings represent an important first step towards understanding the intricate optimization challenges involved in certified training",
    "checked": true,
    "id": "1b226f93f4ba9cdf2bcd5191015e6588d097b35f",
    "semantic_title": "on the paradox of certified training",
    "citation_count": 13,
    "authors": [
      "Nikola Jovanoviƒá",
      "Mislav Balunovic",
      "Maximilian Baader",
      "Martin Vechev"
    ]
  },
  "https://openreview.net/forum?id=JXCH5N4Ujy": {
    "title": "Time Series Alignment with Global Invariances",
    "volume": "main",
    "abstract": "Multivariate time series are ubiquitous objects in signal processing. Measuring a distance or similarity between two such objects is of prime interest in a variety of applications, including machine learning, but can be very difficult as soon as the temporal dynamics and the representation of the time series, i.e. the nature of the observed quantities, differ from one another. In this work, we propose a novel distance accounting both feature space and temporal variabilities by learning a latent global transformation of the feature space together with a temporal alignment, cast as a joint optimization problem. The versatility of our framework allows for several variants depending on the invariance class at stake. Among other contributions, we define a differentiable loss for time series and present two algorithms for the computation of time series barycenters under this new geometry. We illustrate the interest of our approach on both simulated and real world data and show the robustness of our approach compared to state-of-the-art methods",
    "checked": true,
    "id": "b5a48960a1cb45ab8811dfe92904e3fb46c5231e",
    "semantic_title": "time series alignment with global invariances",
    "citation_count": 17,
    "authors": [
      "Titouan Vayer",
      "Romain Tavenard",
      "Laetitia Chapel",
      "R√©mi Flamary",
      "Nicolas Courty",
      "Yann Soullard"
    ]
  },
  "https://openreview.net/forum?id=jIrOeWjdpc": {
    "title": "Explicit Group Sparse Projection with Applications to Deep Learning and NMF",
    "volume": "main",
    "abstract": "We design a new sparse projection method for a set of vectors that guarantees a desired average sparsity level measured leveraging the popular Hoyer measure (an affine function of the ratio of the $\\ell_1$ and $\\ell_2$ norms). Existing approaches either project each vector individually or require the use of a regularization parameter which implicitly maps to the average $\\ell_0$-measure of sparsity. Instead, in our approach we set the sparsity level for the whole set explicitly and simultaneously project a group of vectors with the sparsity level of each vector tuned automatically. We show that the computational complexity of our projection operator is linear in the size of the problem. Additionally, we propose a generalization of this projection by replacing the $\\ell_1$ norm by its weighted version. We showcase the efficacy of our approach in both supervised and unsupervised learning tasks on image datasets including CIFAR10 and ImageNet. In deep neural network pruning, the sparse models produced by our method on ResNet50 have significantly higher accuracies at corresponding sparsity values compared to existing competitors. In nonnegative matrix factorization, our approach yields competitive reconstruction errors against state-of-the-art algorithms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Riyasat Ohib",
      "Nicolas Gillis",
      "Niccolo Dalmasso",
      "Sameena Shah",
      "Vamsi K. Potluru",
      "Sergey Plis"
    ]
  },
  "https://openreview.net/forum?id=jjtFD8A1Wx": {
    "title": "Reasonable Effectiveness of Random Weighting: A Litmus Test for Multi-Task Learning",
    "volume": "main",
    "abstract": "Multi-Task Learning (MTL) has achieved success in various fields. However, training with equal weights for all tasks may cause unsatisfactory performance for part of tasks. To address this problem, there are many works to carefully design dynamical loss/gradient weighting strategies but the basic random experiments are ignored to examine their effectiveness. In this paper, we propose the Random Weighting (RW) methods, including Random Loss Weighting (RLW) and Random Gradient Weighting (RGW), where an MTL model is trained with random loss/gradient weights sampled from a distribution. To show the effectiveness and necessity of RW methods, theoretically, we analyze the convergence of RW and reveal that RW has a higher probability to escape local minima, resulting in better generalization ability. Empirically, we extensively evaluate the proposed RW methods to compare with twelve state-of-the-art methods on five image datasets and two multilingual problems from the XTREME benchmark to show that RW methods can achieve comparable performance with state-of-the-art baselines. Therefore, we think the RW methods are important baselines for MTL and should attract more attention",
    "checked": true,
    "id": "36f30a2d5d3d0b47fa6caa19b3ac772ef68bb71e",
    "semantic_title": "reasonable effectiveness of random weighting: a litmus test for multi-task learning",
    "citation_count": 100,
    "authors": [
      "Baijiong Lin",
      "Feiyang Ye",
      "Yu Zhang",
      "Ivor Tsang"
    ]
  },
  "https://openreview.net/forum?id=lCPOHiztuw": {
    "title": "Direct Molecular Conformation Generation",
    "volume": "main",
    "abstract": "Molecular conformation generation aims to generate three-dimensional coordinates of all the atoms in a molecule and is an important task in bioinformatics and pharmacology. Previous methods usually first predict the interatomic distances, the gradients of interatomic distances or the local structures (e.g., torsion angles) of a molecule, and then reconstruct its 3D conformation. How to directly generate the conformation without the above intermediate values is not fully explored. In this work, we propose a method that directly predicts the coordinates of atoms: (1) the loss function is invariant to roto-translation of coordinates and permutation of symmetric atoms; (2) the newly proposed model adaptively aggregates the bond and atom information and iteratively refines the coordinates of the generated conformation. Our method achieves the best results on GEOM-QM9 and GEOM-Drugs datasets. Further analysis shows that our generated conformations have closer properties (e.g., HOMO-LUMO gap) with the groundtruth conformations. In addition, our method improves molecular docking by providing better initial conformations. All the results demonstrate the effectiveness of our method and the great potential of the direct approach. The code is released at \\url{https://github.com/DirectMolecularConfGen/DMCG}",
    "checked": true,
    "id": "636aae3d8028ec550c087573238ef0a1f480383d",
    "semantic_title": "direct molecular conformation generation",
    "citation_count": 42,
    "authors": [
      "Jinhua Zhu",
      "Yingce Xia",
      "Chang Liu",
      "Lijun Wu",
      "Shufang Xie",
      "Yusong Wang",
      "Tong Wang",
      "Tao Qin",
      "Wengang Zhou",
      "Houqiang Li",
      "Haiguang Liu",
      "Tie-Yan Liu"
    ]
  },
  "https://openreview.net/forum?id=LTiaPxqe2e": {
    "title": "Symbolic Regression is NP-hard",
    "volume": "main",
    "abstract": "Symbolic regression (SR) is the task of learning a model of data in the form of a mathematical expression. By their nature, SR models have the potential to be accurate and human-interpretable at the same time. Unfortunately, finding such models, i.e., performing SR, appears to be a computationally intensive task. Historically, SR has been tackled with heuristics such as greedy or genetic algorithms and, while some works have hinted at the possible hardness of SR, no proof has yet been given that SR is, in fact, NP-hard. This begs the question: Is there an exact polynomial-time algorithm to compute SR models? We provide evidence suggesting that the answer is probably negative by showing that SR is NP-hard",
    "checked": true,
    "id": "c703a193a48befe47b4c62cfbec8ae9d1a243fdf",
    "semantic_title": "symbolic regression is np-hard",
    "citation_count": 64,
    "authors": [
      "Marco Virgolin",
      "Solon P Pissis"
    ]
  },
  "https://openreview.net/forum?id=e5ILb2Nqst": {
    "title": "Differentially Private Stochastic Expectation Propagation",
    "volume": "main",
    "abstract": "We are interested in privatizing an approximate posterior inference algorithm, called Expectation Propagation (EP). EP approximates the posterior distribution by iteratively refining approximations to the local likelihood terms. By doing so, EP typically provides better posterior uncertainties than variational inference (VI) which globally approximates the likelihood term. However, EP needs a large memory to maintain all local approximations associated with each datapoint in the training data. To overcome this challenge, stochastic expectation propagation (SEP) considers a single unique local factor that captures the average effect of each likelihood term to the posterior and refines it in a way analogous to EP. In terms of privatization, SEP is more tractable than EP. It is because at each factor's refining step we fix the remaining factors, where these factors are independent of other datapoints, which is different from EP. This independence makes the sensitivity analysis straightforward. We provide a theoretical analysis of the privacy-accuracy trade-off in the posterior distributions under our method, which we call differentially private stochastic expectation propagation (DP-SEP). Furthermore, we test the DP-SEP algorithm on both synthetic and real-world datasets and evaluate the quality of posterior estimates at different levels of guaranteed privacy",
    "checked": true,
    "id": "d83b00cc251eedc1fe9dc9ea0fb9c7f07e2c8afe",
    "semantic_title": "differentially private stochastic expectation propagation",
    "citation_count": 1,
    "authors": [
      "Margarita Vinaroz",
      "Mijung Park"
    ]
  },
  "https://openreview.net/forum?id=4FU8Jz1Oyj": {
    "title": "On Noise Abduction for Answering Counterfactual Queries: A Practical Outlook",
    "volume": "main",
    "abstract": "A crucial step in counterfactual inference is abduction - inference of the exogenous noise variables. Deep Learning approaches model an exogenous noise variable as a latent variable. Our ability to infer a latent variable comes at a computational cost as well as a statistical cost. In this paper, we show that it may not be necessary to abduct all the noise variables in a structural causal model (SCM) to answer a counterfactual query. In a fully specified causal model with no unobserved confounding, we also identify exogenous noises that must be abducted for a counterfactual query. We introduce a graphical condition for noise identification from an action consisting of an arbitrary combination of hard and soft interventions. We report experimental results on both synthetic and real-world German Credit Dataset showcasing the promise and usefulness of the proposed exogenous noise identification",
    "checked": true,
    "id": "0fb1b3d51a0556b6b4268fd9c2f2b2ce238af3c7",
    "semantic_title": "on noise abduction for answering counterfactual queries: a practical outlook",
    "citation_count": 6,
    "authors": [
      "Saptarshi Saha",
      "Utpal Garain"
    ]
  },
  "https://openreview.net/forum?id=VBHuLfnOMf": {
    "title": "Failure Detection in Medical Image Classification: A Reality Check and Benchmarking Testbed",
    "volume": "main",
    "abstract": "Failure detection in automated image classification is a critical safeguard for clinical deployment. Detected failure cases can be referred to human assessment, ensuring patient safety in computer-aided clinical decision making. Despite its paramount importance, there is insufficient evidence about the ability of state-of-the-art confidence scoring methods to detect test-time failures of classification models in the context of medical imaging. This paper provides a reality check, establishing the performance of in-domain misclassification detection methods, benchmarking 9 widely used confidence scores on 6 medical imaging datasets with different imaging modalities, in multiclass and binary classification settings. Our experiments show that the problem of failure detection is far from being solved. We found that none of the benchmarked advanced methods proposed in the computer vision and machine learning literature can consistently outperform a simple softmax baseline, demonstrating that improved out-of-distribution detection or model calibration do not necessarily translate to improved in-domain misclassification detection. Our developed testbed facilitates future work in this important area",
    "checked": true,
    "id": "8b1b7f372f6e5f6649801249e4bdcaf6c812ee3f",
    "semantic_title": "failure detection in medical image classification: a reality check and benchmarking testbed",
    "citation_count": 10,
    "authors": [
      "M√©lanie Bernhardt",
      "Fabio De Sousa Ribeiro",
      "Ben Glocker"
    ]
  },
  "https://openreview.net/forum?id=XX8CEN815d": {
    "title": "Bridging Offline and Online Experimentation: Constraint Active Search for Deployed Performance Optimization",
    "volume": "main",
    "abstract": "A common challenge in machine learning model development is that models perform differently between the offline development phase and the eventual deployment phase. Fundamentally, the goal of such a model is to maximize performance during deployment, but such performance cannot be measured offline. As such, we propose to augment the standard offline sample efficient hyperparameter optimization to instead search offline for a diverse set of models which can have potentially superior online performance. To this end, we utilize Constraint Active Search to identify such a diverse set of models, and we study their online performance using a variant of Best Arm Identification to select the best model for deployment. The key contribution of this article is the theoretical analysis of the two-phase development strategy, both in analyzing the probability of improvement over the baseline as well as the number of viable treatments for online testing. We demonstrate the viability of this strategy on synthetic examples, as well as a recommendation system benchmark",
    "checked": true,
    "id": "5f655bbb12f01f5994dc97c50af100cf506c9b9d",
    "semantic_title": "bridging offline and online experimentation: constraint active search for deployed performance optimization",
    "citation_count": 2,
    "authors": [
      "Junpei Komiyama",
      "Gustavo Malkomes",
      "Bolong Cheng",
      "Michael McCourt"
    ]
  },
  "https://openreview.net/forum?id=CrimIjBa64": {
    "title": "Multi-Source Causal Inference Using Control Variates under Outcome Selection Bias",
    "volume": "main",
    "abstract": "While many areas of machine learning have benefited from the increasing availability of large and varied datasets, the benefit to causal inference has been limited given the strong assumptions needed to ensure the identifiability of causal effects -- which are often not satisfied in real-world datasets. For example, many large observational datasets (e.g., case-control studies in epidemiology, click-through data in recommender systems) suffer from selection bias on the outcome, which makes the average treatment effect (ATE) non-identifiable. We propose an algorithm to estimate causal effects from multiple data sources, where the ATE may be identifiable only in some datasets but not others. The idea is to construct control variates across the datasets in which the ATE may not be identifiable, which provably reduces the variance of the ATE estimate. We focus on a setting where the observational datasets suffer from outcome selection bias, assuming access to an auxiliary small dataset from which we can obtain a consistent estimate of the ATE. We propose a construction of control variate by taking the difference of the conditional odds ratio estimates from the two datasets. Across simulations and two case studies with real data, we show that the control variate-based ATE estimator has consistently and significantly reduced variance against different baselines",
    "checked": false,
    "id": "7a8d1aaf5873d43707dc75871cdb9e9e0bb36bd8",
    "semantic_title": "multi-source causal inference using control variates",
    "citation_count": 19,
    "authors": [
      "Wenshuo Guo",
      "Serena Lutong Wang",
      "Peng Ding",
      "Yixin Wang",
      "Michael Jordan"
    ]
  },
  "https://openreview.net/forum?id=vd0onGWZbE": {
    "title": "Identifiable Deep Generative Models via Sparse Decoding",
    "volume": "main",
    "abstract": "We develop the sparse VAE for unsupervised representation learning on high-dimensional data. The sparse VAE learns a set of latent factors (representations) which summarize the associations in the observed data features. The underlying model is sparse in that each observed feature (i.e. each dimension of the data) depends on a small subset of the latent factors. As examples, in ratings data each movie is only described by a few genres; in text data each word is only applicable to a few topics; in genomics, each gene is active in only a few biological processes. We prove such sparse deep generative models are identifiable: with infinite data, the true model parameters can be learned. (In contrast, most deep generative models are not identifiable.) We empirically study the sparse VAE with both simulated and real data. We find that it recovers meaningful latent factors and has smaller heldout reconstruction error than related methods",
    "checked": true,
    "id": "0bf88192d02c08661b9185b2b16399306694c4a4",
    "semantic_title": "identifiable deep generative models via sparse decoding",
    "citation_count": 49,
    "authors": [
      "Gemma Elyse Moran",
      "Dhanya Sridhar",
      "Yixin Wang",
      "David Blei"
    ]
  },
  "https://openreview.net/forum?id=QFJ3gtbwHR": {
    "title": "Using unsupervised learning to detect broken symmetries, with relevance to searches for parity violation in nature",
    "volume": "main",
    "abstract": "Testing whether data breaks symmetries of interest can be important to many fields. This paper describes a simple way that machine learning algorithms (whose outputs have been appropriately symmetrised) can be used to detect symmetry breaking. The original motivation for the paper was an important question in Particle Physics: \"Is parity violated at the LHC in some way that no-one has anticipated?\" and so we illustrate the main idea with an example strongly related to that question. However, in order that the key ideas be accessible to readers who are not particle physicists but who are interesting in symmetry breaking, we choose to illustrate the method/approach with a 'toy' example which places a simple discrete source of symmetry breaking (the handedness of human handwriting) within a idealised particle-physics-like context. Readers interested in seeing extensions to continuous symmetries, non-ideal environments or more realistic particle-physics contexts are provided with links to separate papers which delve into such details",
    "checked": true,
    "id": "0c9d931c1ea19ab6a0a407e3d8049f3dfc0794e9",
    "semantic_title": "using unsupervised learning to detect broken symmetries, with relevance to searches for parity violation in nature",
    "citation_count": 7,
    "authors": [
      "Christopher Gorham Lester"
    ]
  },
  "https://openreview.net/forum?id=Kb1lb0vSLa": {
    "title": "Integrating Rankings into Quantized Scores in Peer Review",
    "volume": "main",
    "abstract": "In peer review, reviewers are usually asked to provide scores for the papers. The scores are then used by Area Chairs or Program Chairs in various ways in the decision-making process. The scores are usually elicited in a quantized form to accommodate the limited cognitive ability of humans to describe their opinions in numerical values. It has been found that the quantized scores suffer from a large number of ties, thereby leading to a significant loss of information. To mitigate this issue, conferences have started to ask reviewers to additionally provide a ranking of the papers they have reviewed. There are however two key challenges. First, there is no standard procedure for using this ranking information and Area Chairs may use it in different ways (including simply ignoring them), thereby leading to arbitrariness in the peer-review process. Second, there are no suitable interfaces for judicious use of this data nor methods to incorporate it in existing workflows, thereby leading to inefficiencies. We take a principled approach to integrate the ranking information into the scores. The output of our method is an updated score pertaining to each review that also incorporates the rankings. Our approach addresses the two aforementioned challenges by: (i) ensuring that rankings are incorporated into the updated scores in the same manner for all papers, thereby mitigating arbitrariness, and (ii) allowing to seamlessly use existing interfaces and workflows designed for scores. We empirically evaluate our method on synthetic datasets as well as on peer reviews from the ICLR 2017 conference, and find that it reduces the error by approximately 30% as compared to the best performing baseline on the ICLR 2017 data",
    "checked": true,
    "id": "552321788ea6f61ecb9ac56c870a556b3df51bdd",
    "semantic_title": "integrating rankings into quantized scores in peer review",
    "citation_count": 7,
    "authors": [
      "Yusha Liu",
      "Yichong Xu",
      "Nihar B Shah",
      "Aarti Singh"
    ]
  },
  "https://openreview.net/forum?id=CfzIsWWBlo": {
    "title": "Towards Accurate Subgraph Similarity Computation via Neural Graph Pruning",
    "volume": "main",
    "abstract": "Subgraph similarity search, one of the core problems in graph search, concerns whether a target graph approximately contains a query graph. The problem is recently touched by neural methods. However, current neural methods do not consider pruning the target graph, though pruning is critically important in traditional calculations of subgraph similarities. One obstacle to applying pruning in neural methods is the discrete property of pruning. In this work, we convert graph pruning to a problem of node relabeling and then relax it to a differentiable problem. Based on this idea, we further design a novel neural network to approximate a type of subgraph distance: the subgraph edit distance (SED). In particular, we construct the pruning component using a neural structure, and the entire model can be optimized end-to-end. In the design of the model, we propose an attention mechanism to leverage the information about the query graph and guide the pruning of the target graph. Moreover, we develop a multi-head pruning strategy such that the model can better explore multiple ways of pruning the target graph. The proposed model establishes new state-ofthe-art results across seven benchmark datasets. Extensive analysis of the model indicates that the proposed model can reasonably prune the target graph for SED computation",
    "checked": true,
    "id": "4ff9b0f36d37f37e3985f4608f3037f506b3c825",
    "semantic_title": "towards accurate subgraph similarity computation via neural graph pruning",
    "citation_count": 5,
    "authors": [
      "Linfeng Liu",
      "XU HAN",
      "Dawei Zhou",
      "Liping Liu"
    ]
  },
  "https://openreview.net/forum?id=Ox5tmhFBrc": {
    "title": "Learning Two-Step Hybrid Policy for Graph-Based Interpretable Reinforcement Learning",
    "volume": "main",
    "abstract": "We present a two-step hybrid reinforcement learning (RL) policy that is designed to generate interpretable and robust hierarchical policies on the RL problem with graph-based input. Unlike prior deep reinforcement learning policies parameterized by an end-to-end black-box graph neural network, our approach disentangles the decision-making process into two steps. The first step is a simplified classification problem that maps the graph input to an action group where all actions share a similar semantic meaning. The second step implements a sophisticated rule-miner that conducts explicit one-hop reasoning over the graph and identifies decisive edges in the graph input without the necessity of heavy domain knowledge. This two-step hybrid policy presents human-friendly interpretations and achieves better performance in terms of generalization and robustness. Extensive experimental studies on four levels of complex text-based games have demonstrated the superiority of the proposed method compared to the state-of-the-art",
    "checked": true,
    "id": "5b97c0c6e075d8f6e8c61bdc20e03ef048e5c762",
    "semantic_title": "learning two-step hybrid policy for graph-based interpretable reinforcement learning",
    "citation_count": 0,
    "authors": [
      "Tongzhou Mu",
      "Kaixiang Lin",
      "Feiyang Niu",
      "Govind Thattai"
    ]
  },
  "https://openreview.net/forum?id=UDmH3HxxPp": {
    "title": "Secure Domain Adaptation with Multiple Sources",
    "volume": "main",
    "abstract": "Multi-source unsupervised domain adaptation (MUDA) is a framework to address the challenge of annotated data scarcity in a target domain via transferring knowledge from multiple annotated source domains. When the source domains are distributed, data privacy and security can become significant concerns and protocols may limit data sharing, yet existing MUDA methods overlook these constraints. We develop an algorithm to address MUDA when source domain data cannot be shared with the target or across the source domains. Our method is based on aligning the distributions of source and target domains indirectly via estimating the source feature embeddings and predicting over a confidence based combination of domain specific model predictions. We provide theoretical analysis to support our approach and conduct empirical experiments to demonstrate that our algorithm is effective",
    "checked": true,
    "id": "6ae637ba0c56da24584a2d28f522c65a45e6a460",
    "semantic_title": "secure domain adaptation with multiple sources",
    "citation_count": 13,
    "authors": [
      "Serban Stan",
      "Mohammad Rostami"
    ]
  },
  "https://openreview.net/forum?id=8s8K2UZGTZ": {
    "title": "Teaching Models to Express Their Uncertainty in Words",
    "volume": "main",
    "abstract": "We show that a GPT-3 model can learn to express uncertainty about its own answers in natural language -- without use of model logits. When given a question, the model generates both an answer and a level of confidence (e.g. \"90% confidence\" or \"high confidence\"). These levels map to probabilities that are well calibrated. The model also remains moderately calibrated under distribution shift, and is sensitive to uncertainty in its own answers, rather than imitating human examples. For testing calibration, we introduce the CalibratedMath suite of tasks. We compare the calibration of uncertainty expressed in words (\"verbalized probability\") to uncertainty extracted from model logits. Both kinds of uncertainty are capable of generalizing calibration under distribution shift. We also provide evidence that GPT-3's ability to generalize calibration depends on pre-trained latent representations that correlate with epistemic uncertainty over its answers",
    "checked": true,
    "id": "374dd173491a59a10bbb2b3519ebcfe3649f529d",
    "semantic_title": "teaching models to express their uncertainty in words",
    "citation_count": 425,
    "authors": [
      "Stephanie Lin",
      "Jacob Hilton",
      "Owain Evans"
    ]
  },
  "https://openreview.net/forum?id=Qs3EfpieOh": {
    "title": "The Evolution of Out-of-Distribution Robustness Throughout Fine-Tuning",
    "volume": "main",
    "abstract": "Although machine learning models typically experience a drop in performance on out-of-distribution data, accuracies on in- versus out-of-distribution data are widely observed to follow a single linear trend when evaluated across a testbed of models. Models that are more accurate on the out-of-distribution data relative to this baseline exhibit \"effective robustness\" and are exceedingly rare. Identifying such models, and understanding their properties, is key to improving out-of-distribution performance. We conduct a thorough empirical investigation of effective robustness during fine-tuning and surprisingly find that models pre-trained on larger datasets exhibit effective robustness during training that vanishes at convergence. We study how properties of the data influence effective robustness, and we show that it increases with the larger size, more diversity, and higher example difficulty of the dataset. We also find that models that display effective robustness are able to correctly classify 10\\% of the examples that no other current testbed model gets correct. Finally, we discuss several strategies for scaling effective robustness to the high-accuracy regime to improve the out-of-distribution accuracy of state-of-the-art models",
    "checked": true,
    "id": "4b1db6ebbdfcfe8ef67c5db511b6ad169fcc8f7f",
    "semantic_title": "the evolution of out-of-distribution robustness throughout fine-tuning",
    "citation_count": 82,
    "authors": [
      "Anders Johan Andreassen",
      "Yasaman Bahri",
      "Behnam Neyshabur",
      "Rebecca Roelofs"
    ]
  },
  "https://openreview.net/forum?id=JBuCfkmKYu": {
    "title": "Simplifying Node Classification on Heterophilous Graphs with Compatible Label Propagation",
    "volume": "main",
    "abstract": "Graph Neural Networks (GNNs) have been predominant for graph learning tasks; however, recent studies showed that a well-known graph algorithm, Label Propagation (LP), combined with a shallow neural network can achieve comparable performance to GNNs in semi-supervised node classification on graphs with high homophily. In this paper, we show that this approach falls short on graphs with low homophily, where nodes often connect to the nodes of the opposite classes. To overcome this, we carefully design a combination of a base predictor with LP algorithm that enjoys a closed-form solution as well as convergence guarantees. Our algorithm first learns the class compatibility matrix and then aggregates label predictions using LP algorithm weighted by class compatibilities. On a wide variety of benchmarks, we show that our approach achieves the leading performance on graphs with various levels of homophily. Meanwhile, it has orders of magnitude fewer parameters and requires less execution time",
    "checked": true,
    "id": "2cc70191b6f2fd047ab1ac0ed41d48186e66662d",
    "semantic_title": "simplifying node classification on heterophilous graphs with compatible label propagation",
    "citation_count": 9,
    "authors": [
      "Zhiqiang Zhong",
      "Sergei Ivanov",
      "Jun Pang"
    ]
  },
  "https://openreview.net/forum?id=7gzQltQSwr": {
    "title": "Centroids Matching: an efficient Continual Learning approach operating in the embedding space",
    "volume": "main",
    "abstract": "Catastrophic forgetting (CF) occurs when a neural network loses the information previously learned while training on a set of samples from a different distribution, i.e., a new task. Existing approaches have achieved remarkable results in mitigating CF, especially in a scenario called task incremental learning. However, this scenario is not realistic, and limited work has been done to achieve good results on more realistic scenarios. In this paper, we propose a novel regularization method called Centroids Matching, that, inspired by meta-learning approaches, fights CF by operating in the feature space produced by the neural network, achieving good results while requiring a small memory footprint. Specifically, the approach classifies the samples directly using the feature vectors produced by the neural network, by matching those vectors with the centroids representing the classes from the current task, or all the tasks up to that point. Centroids Matching is faster than competing baselines, and it can be exploited to efficiently mitigate CF, by preserving the distances between the embedding space produced by the model when past tasks were over, and the one currently produced, leading to a method that achieves high accuracy on all the tasks, without using an external memory when operating on easy scenarios, or using a small one for more realistic ones. Extensive experiments demonstrate that Centroids Matching achieves accuracy gains on multiple datasets and scenarios",
    "checked": true,
    "id": "6b1efd7b87fb370b52e67df6ecf3a43a4e027bee",
    "semantic_title": "centroids matching: an efficient continual learning approach operating in the embedding space",
    "citation_count": 1,
    "authors": [
      "Jary Pomponi",
      "Simone Scardapane",
      "Aurelio Uncini"
    ]
  },
  "https://openreview.net/forum?id=nS8A9nOrqp": {
    "title": "Nonstationary Reinforcement Learning with Linear Function Approximation",
    "volume": "main",
    "abstract": "We consider reinforcement learning (RL) in episodic Markov decision processes (MDPs) with linear function approximation under drifting environment. Specifically, both the reward and state transition functions can evolve over time but their total variations do not exceed a \\textit{variation budget}. We first develop $\\texttt{LSVI-UCB-Restart}$ algorithm, an optimistic modification of least-squares value iteration with periodic restart, and bound its dynamic regret when variation budgets are known. Then we propose a parameter-free algorithm \\texttt{Ada-LSVI-UCB-Restart} that extends to unknown variation budgets. We also derive the first minimax dynamic regret lower bound for nonstationary linear MDPs and as a byproduct establish a minimax regret lower bound for linear MDPs unsolved by Jin et al. (2020). Finally, we provide numerical experiments to demonstrate the effectiveness of our proposed algorithms",
    "checked": true,
    "id": "750228d26cb6a33bd7393372983edfde1e09733b",
    "semantic_title": "nonstationary reinforcement learning with linear function approximation",
    "citation_count": 30,
    "authors": [
      "Huozhi Zhou",
      "Jinglin Chen",
      "Lav R. Varshney",
      "Ashish Jagmohan"
    ]
  },
  "https://openreview.net/forum?id=j2Mid5hFUJ": {
    "title": "Enhanced gradient-based MCMC in discrete spaces",
    "volume": "main",
    "abstract": "The recent introduction of gradient-based Markov chain Monte Carlo (MCMC) for discrete spaces holds great promise, and comes with the tantalising possibility of new discrete counterparts to celebrated continuous methods such as the Metropolis-adjusted Langevin algorithm (MALA). Towards this goal, we introduce several discrete Metropolis-Hastings samplers that are conceptually inspired by MALA, and demonstrate their strong empirical performance across a range of challenging sampling problems in Bayesian inference and energy-based modelling. Methodologically, we identify why discrete analogues to \\emph{preconditioned} MALA are generally intractable, motivating us to introduce a new kind of preconditioning based on auxiliary variables and the `Gaussian integral trick'",
    "checked": true,
    "id": "b9bbebe9f719e33229dd0801275f984c8f4f21ae",
    "semantic_title": "enhanced gradient-based mcmc in discrete spaces",
    "citation_count": 17,
    "authors": [
      "Benjamin Rhodes",
      "Michael U. Gutmann"
    ]
  },
  "https://openreview.net/forum?id=w3x20YEcQK": {
    "title": "Flipped Classroom: Effective Teaching for Time Series Forecasting",
    "volume": "main",
    "abstract": "Sequence-to-sequence models based on LSTM and GRU are a most popular choice for forecasting time series data reaching state-of-the-art performance. Training such models can be delicate though. The two most common training strategies within this context are teacher forcing (TF) and free running (FR). TF can be used to help the model to converge faster but may provoke an exposure bias issue due to a discrepancy between training and inference phase. FR helps to avoid this but does not necessarily lead to better results, since it tends to make the training slow and unstable instead. Scheduled sampling was the first approach tackling these issues by picking the best from both worlds and combining it into a curriculum learning (CL) strategy. Although scheduled sampling seems to be a convincing alternative to FR and TF, we found that, even if parametrized carefully, scheduled sampling may lead to premature termination of the training when applied for time series forecasting. To mitigate the problems of the above approaches we formalize CL strategies along the training as well as the training iteration scale. We propose several new curricula, and systematically evaluate their performance in two experimental sets. For our experiments, we utilize six datasets generated from prominent chaotic systems. We found that the newly proposed increasing training scale curricula with a probabilistic iteration scale curriculum consistently outperforms previous training strategies yielding an NRMSE improvement of up to 81% over FR or TF training. For some datasets we additionally observe a reduced number of training iterations. We observed that all models trained with the new curricula yield higher prediction stability allowing for longer prediction horizons",
    "checked": true,
    "id": "304718bd00445304f0834f5462fb131c955b0ad7",
    "semantic_title": "flipped classroom: effective teaching for time series forecasting",
    "citation_count": 8,
    "authors": [
      "Philipp Teutsch",
      "Patrick M√§der"
    ]
  },
  "https://openreview.net/forum?id=mbwm7NdkpO": {
    "title": "Deep Policies for Online Bipartite Matching: A Reinforcement Learning Approach",
    "volume": "main",
    "abstract": "The challenge in the widely applicable online matching problem lies in making irrevocable assignments while there is uncertainty about future inputs. Most theoretically-grounded policies are myopic or greedy in nature. In real-world applications where the matching process is repeated on a regular basis, the underlying data distribution can be leveraged for better decision-making. We present an end-to-end Reinforcement Learning framework for deriving better matching policies based on trial-and-error on historical data. We devise a set of neural network architectures, design feature representations, and empirically evaluate them across two online matching problems: Edge-Weighted Online Bipartite Matching and Online Submodular Bipartite Matching. We show that most of the learning approaches perform consistently better than classical baseline algorithms on four synthetic and real-world datasets. On average, our proposed models improve the matching quality by 3-10% on a variety of synthetic and real-world datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Ali Alomrani",
      "Reza Moravej",
      "Elias Boutros Khalil"
    ]
  },
  "https://openreview.net/forum?id=X1VzbBU6xZ": {
    "title": "Generative Adversarial Neural Operators",
    "volume": "main",
    "abstract": "We propose the generative adversarial neural operator (GANO), a generative model paradigm for learning probabilities on infinite-dimensional function spaces. The natural sciences and engineering are known to have many types of data that are sampled from infinite- dimensional function spaces, where classical finite-dimensional deep generative adversarial networks (GANs) may not be directly applicable. GANO generalizes the GAN framework and allows for the sampling of functions by learning push-forward operator maps in infinite-dimensional spaces. GANO consists of two main components, a generator neural operator and a discriminator neural functional. The inputs to the generator are samples of functions from a user-specified probability measure, e.g., Gaussian random field (GRF), and the generator outputs are synthetic data functions. The input to the discriminator is either a real or synthetic data function. In this work, we instantiate GANO using the Wasserstein criterion and show how the Wasserstein loss can be computed in infinite-dimensional spaces. We empirically study GANO in controlled cases where both input and output functions are samples from GRFs and compare its performance to the finite-dimensional counterpart GAN. We empirically study the efficacy of GANO on real-world function data of volcanic activities and show its superior performance over GAN",
    "checked": true,
    "id": "010dbeb1bb09073fff8aafa3a0b7b78921e6b83a",
    "semantic_title": "generative adversarial neural operators",
    "citation_count": 41,
    "authors": [
      "Md Ashiqur Rahman",
      "Manuel A Florez",
      "Anima Anandkumar",
      "Zachary E Ross",
      "Kamyar Azizzadenesheli"
    ]
  },
  "https://openreview.net/forum?id=mW6nD3567x": {
    "title": "From Optimization Dynamics to Generalization Bounds via ≈Åojasiewicz Gradient Inequality",
    "volume": "main",
    "abstract": "Optimization and generalization are two essential aspects of statistical machine learning. In this paper, we propose a framework to connect optimization with generalization by analyz- ing the generalization error based on the optimization trajectory under the gradient flow algorithm. The key ingredient of this framework is the Uniform-LGI, a property that is generally satisfied when training machine learning models. Leveraging the Uniform-LGI, we first derive convergence rates for gradient flow algorithm, then we give generalization bounds for a large class of machine learning models. We further apply our framework to three distinct machine learning models: linear regression, kernel regression, and two-layer neural networks. Through our approach, we obtain generalization estimates that match or extend previous results",
    "checked": true,
    "id": "54353089a925504a0082e813a19baede4d9be606",
    "semantic_title": "from optimization dynamics to generalization bounds via ≈Çojasiewicz gradient inequality",
    "citation_count": 2,
    "authors": [
      "Fusheng Liu",
      "Haizhao Yang",
      "Soufiane Hayou",
      "Qianxiao Li"
    ]
  },
  "https://openreview.net/forum?id=1l0sClLiPc": {
    "title": "Unimodal Likelihood Models for Ordinal Data",
    "volume": "main",
    "abstract": "Ordinal regression (OR) is the classification of ordinal data, in which the underlying target variable is categorical and considered to have a natural ordinal relation for the explanatory variables. In this study, we suppose the unimodality of the conditional probability distribution of the target variable given a value of the explanatory variables as a natural ordinal relation of the ordinal data. Under this supposition, unimodal likelihood models are considered to be promising for achieving good generalization performance in OR tasks. Demonstrating that previous unimodal likelihood models have a weak representation ability, we thus develop more representable unimodal likelihood models, including the most representable one. OR experiments in this study showed that the developed more representable unimodal likelihood models could yield better generalization performance for real-world ordinal data compared with previous unimodal likelihood models and popular statistical OR models having no unimodality guarantee",
    "checked": true,
    "id": "d0d343b8733c537dbb7bab945d6ebbc558bbeacd",
    "semantic_title": "unimodal likelihood models for ordinal data",
    "citation_count": 4,
    "authors": [
      "Ryoya Yamasaki"
    ]
  },
  "https://openreview.net/forum?id=DijnKziche": {
    "title": "Differentiable Model Compression via Pseudo Quantization Noise",
    "volume": "main",
    "abstract": "We propose DiffQ a differentiable method for model compression for quantizing model parameters without gradient approximations (e.g., Straight Through Estimator). We suggest adding independent pseudo quantization noise to model parameters during training to approximate the effect of a quantization operator. DiffQ is differentiable both with respect to the unquantized weights and the number of bits used. Given a single hyper-parameter balancing between the quantized model size and accuracy, DiffQ optimizes the number of bits used per individual weight or groups of weights, in end-to-end training. We experimentally verify that our method is competitive with STE based quantization techniques on several benchmarks and architectures for image classification, language modeling, and audio source separation. For instance, on the ImageNet dataset, DiffQ compresses a 12 layers transformer-based model by more than a factor of 8, (lower than 4 bits precision per weight on average), with a loss of 0.3\\% in model accuracy. Code is available at github.com/facebookresearch/diffq",
    "checked": true,
    "id": "138d7ee8cc618e7b365bda6d8a8f4e6cb85a0c37",
    "semantic_title": "differentiable model compression via pseudo quantization noise",
    "citation_count": 50,
    "authors": [
      "Alexandre D√©fossez",
      "Yossi Adi",
      "Gabriel Synnaeve"
    ]
  },
  "https://openreview.net/forum?id=Zxm0kNe3u7": {
    "title": "Mace: A flexible framework for membership privacy estimation in generative models",
    "volume": "main",
    "abstract": "Generative machine learning models are being increasingly viewed as a way to share sensitive data between institutions. While there has been work on developing differentially private generative modeling approaches, these approaches generally lead to sub-par sample quality, limiting their use in real world applications. Another line of work has focused on developing generative models which lead to higher quality samples but currently lack any formal privacy guarantees. In this work, we propose the first formal framework for membership privacy estimation in generative models. We formulate the membership privacy risk as a statistical divergence between training samples and hold-out samples, and propose sample-based methods to estimate this divergence. Compared to previous works, our framework makes more realistic and flexible assumptions. First, we offer a generalizable metric as an alternative to the accuracy metric (Yeom et al., 2018; Hayes et al., 2019) especially for imbalanced datasets. Second, we loosen the assumption of having full access to the underlying distribution from previous studies (Yeom et al., 2018; Jayaraman et al., 2020), and propose sample-based estimations with theoretical guarantees. Third, along with the population-level membership privacy risk estimation via the optimal membership advantage, we offer the individual-level estimation via the individual privacy risk. Fourth, our framework allows adversaries to access the trained model via a customized query, while prior works require specific attributes (Hayes et al., 2019; Chen et al., 2019; Hilprecht et al., 2019)",
    "checked": true,
    "id": "00fa213314d8e7ee20201a051c3293c16ff549f9",
    "semantic_title": "mace: a flexible framework for membership privacy estimation in generative models",
    "citation_count": 11,
    "authors": [
      "Yixi Xu",
      "Sumit Mukherjee",
      "Xiyang Liu",
      "Shruti Tople",
      "Rahul M Dodhia",
      "Juan M Lavista Ferres"
    ]
  },
  "https://openreview.net/forum?id=Jj0qSbtwdb": {
    "title": "Fingerprints of Super Resolution Networks",
    "volume": "main",
    "abstract": "Several recent studies have demonstrated that deep-learning based image generation models, such as GANs, can be uniquely identified, and possibly even reverse-engineered, by the fingerprints they leave on their output images. We extend this research to single image super-resolution (SISR) networks. Compared to previously studied models, SISR networks are a uniquely challenging class of image generation model from which to extract and analyze fingerprints, as they can often generate images that closely match the corresponding ground truth and thus likely leave little flexibility to embed signatures. We take SISR models as examples to investigate if the findings from the previous work on fingerprints of GAN-based networks are valid for general image generation models. We show that SISR networks with a high upscaling factor or trained using adversarial loss leave highly distinctive fingerprints, and that under certain conditions, some SISR network hyperparameters can be reverse-engineered from these fingerprints",
    "checked": false,
    "id": "4495d0cac49a1c86eff7b7ea3b2898bf2aa19846",
    "semantic_title": "a novel fingerprint image enhancement based on super resolution",
    "citation_count": 8,
    "authors": [
      "Jeremy Vonderfecht",
      "Feng Liu"
    ]
  },
  "https://openreview.net/forum?id=rrMK6hYNSx": {
    "title": "Online Double Oracle",
    "volume": "main",
    "abstract": "Solving strategic games with huge action spaces is a critical yet under-explored topic in economics, operations research and artificial intelligence. This paper proposes new learning algorithms for solving two-player zero-sum normal-form games where the number of pure strategies is prohibitively large. Specifically, we combine no-regret analysis from online learning with Double Oracle (DO) from game theory. Our method---\\emph{Online Double Oracle (ODO)}---is provably convergent to a Nash equilibrium (NE). Most importantly, unlike normal DO, ODO is \\emph{rational} in the sense that each agent in ODO can exploit a strategic adversary with a regret bound of $\\mathcal{O}(\\sqrt{ k \\log(k)/T})$, where $k$ is not the total number of pure strategies, but rather the size of \\emph{effective strategy set}. In many applications, we empirically show that $k$ is linearly dependent on the support size of the NE. On tens of different real-world matrix games, ODO outperforms DO, PSRO, and no-regret algorithms such as Multiplicative Weights Update by a significant margin, both in terms of convergence rate to a NE, and average payoff against strategic adversaries",
    "checked": true,
    "id": "0578a754cc0066347817071c2ce4cfb8695c673a",
    "semantic_title": "online double oracle",
    "citation_count": 32,
    "authors": [
      "Le Cong Dinh",
      "Stephen Marcus McAleer",
      "Zheng Tian",
      "Nicolas Perez-Nieves",
      "Oliver Slumbers",
      "David Henry Mguni",
      "Jun Wang",
      "Haitham Bou Ammar",
      "Yaodong Yang"
    ]
  },
  "https://openreview.net/forum?id=nmFczdJtc2": {
    "title": "Attribute Prediction as Multiple Instance Learning",
    "volume": "main",
    "abstract": "Attribute-based representations help machine learning models perform tasks based on human understandable concepts, allowing a closer human-machine collaboration. However, learning attributes that accurately reflect the content of an image is not always straightforward, as per-image ground truth attributes are often not available. We propose applying the Multiple Instance Learning (MIL) paradigm to attribute learning (AMIL) while only using class-level labels. We allow the model to under-predict the positive attributes, which may be missing in a particular image due to occlusions or unfavorable pose, but not to over-predict the negative ones, which are almost certainly not present. We evaluate it in the zero-shot learning (ZSL) setting, where training and test classes are disjoint, and show that this also allows to profit from knowledge about the semantic relatedness of attributes. In addition, we apply the MIL assumption to ZSL classification and propose MIL-DAP, an attribute-based zero-shot classification method, based on Direct Attribute Prediction (DAP), to evaluate attribute prediction methods when no image-level data is available for evaluation. Experiments on CUB-200-2011, SUN Attributes and AwA2 show improvements on attribute detection, attribute-based zero-shot classification and weakly supervised part localization",
    "checked": true,
    "id": "a941749e380ebc689fd1744ea563cddb6cb07d3f",
    "semantic_title": "attribute prediction as multiple instance learning",
    "citation_count": 2,
    "authors": [
      "Diego Marcos",
      "Aike Potze",
      "Wenjia Xu",
      "Devis Tuia",
      "Zeynep Akata"
    ]
  },
  "https://openreview.net/forum?id=4N6T6Rop6k": {
    "title": "Completeness and Coherence Learning for Fast Arbitrary Style Transfer",
    "volume": "main",
    "abstract": "Style transfer methods put a premium on two objectives: (1) completeness which encourages the encoding of a complete set of style patterns; (2) coherence which discourages the production of spurious artifacts not found in input styles. While existing methods pursue the two objectives either partially or implicitly, we present the Completeness and Coherence Network (CCNet) which jointly learns completeness and coherence components and rejects their incompatibility, both in an explicit manner. Specifically, we develop an attention mechanism integrated with bi-directional softmax operations for explicit imposition of the two objectives and for their collaborative modelling. We also propose CCLoss as a quantitative measure for evaluating the quality of a stylized image in terms of completeness and coherence. Through an empirical evaluation, we demonstrate that compared with existing methods, our method strikes a better tradeoff between computation costs, generalization ability and stylization quality",
    "checked": true,
    "id": "77c9e6e427f8e7ee9fb3c780c13466390c74a952",
    "semantic_title": "completeness and coherence learning for fast arbitrary style transfer",
    "citation_count": 2,
    "authors": [
      "Zhijie Wu",
      "Chunjin Song",
      "Guanxiong Chen",
      "Sheng Guo",
      "Weilin Huang"
    ]
  },
  "https://openreview.net/forum?id=gvSHaaD2wQ": {
    "title": "sigmoidF1: A Smooth F1 Score Surrogate Loss for Multilabel Classification",
    "volume": "main",
    "abstract": "Multilabel classification is the task of attributing multiple labels to examples via predictions. Current models formulate a reduction of the multilabel setting into either multiple binary classifications or multiclass classification, allowing for the use of existing loss functions (sigmoid, cross-entropy, logistic, etc.). These multilabel classification reductions do not accommodate for the prediction of varying numbers of labels per example. Moreover, the loss functions are distant estimates of the performance metrics. We propose sigmoidF1, a loss function that is an approximation of the F1 score that (i) is smooth and tractable for stochastic gradient descent, (ii) naturally approximates a multilabel metric, and (iii) estimates both label suitability and label counts. We show that any confusion matrix metric can be formulated with a smooth surrogate. We evaluate the proposed loss function on text and image datasets, and with a variety of metrics, to account for the complexity of multilabel classification evaluation. sigmoidF1 outperforms other loss functions on one text and two image datasets over several metrics. These results show the effectiveness of using inference-time metrics as loss functions for non-trivial classification problems like multilabel classification",
    "checked": true,
    "id": "29848114ee52606094de50d2634909cfe15adaec",
    "semantic_title": "sigmoidf1: a smooth f1 score surrogate loss for multilabel classification",
    "citation_count": 33,
    "authors": [
      "Gabriel B√©n√©dict",
      "Hendrik Vincent Koops",
      "Daan Odijk",
      "Maarten de Rijke"
    ]
  },
  "https://openreview.net/forum?id=XsPopigZXV": {
    "title": "FLEA: Provably Robust Fair Multisource Learning from Unreliable Training Data",
    "volume": "main",
    "abstract": "Fairness-aware learning aims at constructing classifiers that not only make accurate predictions, but also do not discriminate against specific groups. It is a fast-growing area of machine learning with far-reaching societal impact. However, existing fair learning methods are vulnerable to accidental or malicious artifacts in the training data, which can cause them to unknowingly produce unfair classifiers. In this work we address the problem of fair learning from unreliable training data in the robust multisource setting, where the available training data comes from multiple sources, a fraction of which might not be representative of the true data distribution. We introduce FLEA, a filtering-based algorithm that identifies and suppresses those data sources that would have a negative impact on fairness or accuracy if they were used for training. As such, FLEA is not a replacement of prior fairness-aware learning methods but rather an augmentation that makes any of them robust against unreliable training data. We show the effectiveness of our approach by a diverse range of experiments on multiple datasets. Additionally, we prove formally that ‚Äìgiven enough data‚Äì FLEA protects the learner against corruptions as long as the fraction of affected data sources is less than half. Our source code and documentation are available at https://github.com/ISTAustria-CVML/FLEA",
    "checked": true,
    "id": "ec5735d77d486201bf68b566861c7b4bf6d4ee7a",
    "semantic_title": "flea: provably robust fair multisource learning from unreliable training data",
    "citation_count": 0,
    "authors": [
      "Eugenia Iofinova",
      "Nikola Konstantinov",
      "Christoph H Lampert"
    ]
  },
  "https://openreview.net/forum?id=u8tvSxm4Bs": {
    "title": "GemNet-OC: Developing Graph Neural Networks for Large and Diverse Molecular Simulation Datasets",
    "volume": "main",
    "abstract": "Recent years have seen the advent of molecular simulation datasets that are orders of magnitude larger and more diverse. These new datasets differ substantially in four aspects of complexity: 1. Chemical diversity (number of different elements), 2. system size (number of atoms per sample), 3. dataset size (number of data samples), and 4. domain shift (similarity of the training and test set). Despite these large differences, benchmarks on small and narrow datasets remain the predominant method of demonstrating progress in graph neural networks (GNNs) for molecular simulation, likely due to cheaper training compute requirements. This raises the question -- does GNN progress on small and narrow datasets translate to these more complex datasets? This work investigates this question by first developing the GemNet-OC model based on the large Open Catalyst 2020 (OC20) dataset. GemNet-OC outperforms the previous state-of-the-art on OC20 by 16% while reducing training time by a factor of 10. We then compare the impact of 18 model components and hyperparameter choices on performance in multiple datasets. We find that the resulting model would be drastically different depending on the dataset used for making model choices. To isolate the source of this discrepancy we study six subsets of the OC20 dataset that individually test each of the above-mentioned four dataset aspects. We find that results on the OC-2M subset correlate well with the full OC20 dataset while being substantially cheaper to train on. Our findings challenge the common practice of developing GNNs solely on small datasets, but highlight ways of achieving fast development cycles and generalizable results via moderately-sized, representative datasets such as OC-2M and efficient models such as GemNet-OC. Our code and pretrained model weights are open-sourced",
    "checked": true,
    "id": "aed45120bd674b9c801def63c25577de76348285",
    "semantic_title": "gemnet-oc: developing graph neural networks for large and diverse molecular simulation datasets",
    "citation_count": 70,
    "authors": [
      "Johannes Gasteiger",
      "Muhammed Shuaibi",
      "Anuroop Sriram",
      "Stephan G√ºnnemann",
      "Zachary Ward Ulissi",
      "C. Lawrence Zitnick",
      "Abhishek Das"
    ]
  },
  "https://openreview.net/forum?id=tqDhrbKJLS": {
    "title": "MixTailor: Mixed Gradient Aggregation for Robust Learning Against Tailored Attacks",
    "volume": "main",
    "abstract": "Implementations of SGD on distributed and multi-GPU systems creates new vulnerabilities, which can be identified and misused by one or more adversarial agents. Recently, it has been shown that well-known Byzantine-resilient gradient aggregation schemes are indeed vulnerable to informed attackers that can tailor the attacks (Fang et al., 2020; Xie et al., 2020b). We introduce MixTailor, a scheme based on randomization of the aggregation strategies that makes it impossible for the attacker to be fully informed. Deterministic schemes can be integrated into MixTailor on the fly without introducing any additional hyperparameters. Randomization decreases the capability of a powerful adversary to tailor its attacks, while the resulting randomized aggregation scheme is still competitive in terms of performance. For both iid and non-iid settings, we establish almost sure convergence guarantees that are both stronger and more general than those available in the literature. Our empirical studies across various datasets, attacks, and settings, validate our hypothesis and show that MixTailor successfully defends when well-known Byzantine-tolerant schemes fail",
    "checked": true,
    "id": "d5ac48b67e111766864cff438dc7fce4923d0f71",
    "semantic_title": "mixtailor: mixed gradient aggregation for robust learning against tailored attacks",
    "citation_count": 6,
    "authors": [
      "Ali Ramezani-Kebrya",
      "Iman Tabrizian",
      "Fartash Faghri",
      "Petar Popovski"
    ]
  },
  "https://openreview.net/forum?id=S8eABAy8P3": {
    "title": "LIMIS: Locally Interpretable Modeling using Instance-wise Subsampling",
    "volume": "main",
    "abstract": "Understanding black-box machine learning models is crucial for their widespread adoption. Learning globally interpretable models is one approach, but achieving high performance with them is challenging. An alternative approach is to explain individual predictions using locally interpretable models. For locally interpretable modeling, various methods have been proposed and indeed commonly used, but they suffer from low fidelity, i.e. their explanations do not approximate the predictions well. In this paper, our goal is to push the state-of-the-art in high-fidelity locally interpretable modeling. We propose a novel framework, Locally Interpretable Modeling using Instance-wise Subsampling (LIMIS). LIMIS utilizes a policy gradient to select a small number of instances and distills the black-box model into a low-capacity locally interpretable model using those selected instances. Training is guided with a reward obtained directly by measuring the fidelity of the locally interpretable models. We show on multiple tabular datasets that LIMIS near-matches the prediction accuracy of black-box models, significantly outperforming state-of-the-art locally interpretable models in terms of fidelity and prediction accuracy",
    "checked": false,
    "id": "d19ed42fd63f6d09a7d9dd8ab9f025943fd0c823",
    "semantic_title": "select wisely and explain: active learning and probabilistic local post-hoc explainability",
    "citation_count": 13,
    "authors": [
      "Jinsung Yoon",
      "Sercan O Arik",
      "Tomas Pfister"
    ]
  },
  "https://openreview.net/forum?id=Lgs5pQ1v30": {
    "title": "FedShuffle: Recipes for Better Use of Local Work in Federated Learning",
    "volume": "main",
    "abstract": "The practice of applying several local updates before aggregation across clients has been empirically shown to be a successful approach to overcoming the communication bottleneck in Federated Learning (FL). Such methods are usually implemented by having clients perform one or more epochs of local training per round while randomly reshuffling their finite dataset in each epoch. Data imbalance, where clients have different numbers of local training samples, is ubiquitous in FL applications, resulting in different clients performing different numbers of local updates in each round. In this work, we propose a general recipe, FedShuffle, that better utilizes the local updates in FL, especially in this regime encompassing random reshuffling and heterogeneity. FedShuffle is the first local update method with theoretical convergence guarantees that incorporates random reshuffling, data imbalance, and client sampling ‚Äî features that are essential in large-scale cross-device FL. We present a comprehensive theoretical analysis of FedShuffle and show, both theoretically and empirically, that it does not suffer from the objective function mismatch that is present in FL methods that assume homogeneous updates in heterogeneous FL setups, such as FedAvg (McMahan et al., 2017). In addition, by combining the ingredients above, FedShuffle improves upon FedNova (Wang et al., 2020), which was previously proposed to solve this mismatch. Similar to Mime (Karimireddy et al., 2020), we show that FedShuffle with momentum variance reduction (Cutkosky & Orabona, 2019) improves upon non-local methods under a Hessian similarity assumption",
    "checked": true,
    "id": "1f571c8b972540370b6fb9b48564fd2d8069901f",
    "semantic_title": "fedshuffle: recipes for better use of local work in federated learning",
    "citation_count": 21,
    "authors": [
      "Samuel Horv√°th",
      "Maziar Sanjabi",
      "Lin Xiao",
      "Peter Richt√°rik",
      "Michael Rabbat"
    ]
  },
  "https://openreview.net/forum?id=OslAMMF4ZP": {
    "title": "Faking Interpolation Until You Make It",
    "volume": "main",
    "abstract": "Deep over-parameterized neural networks exhibit the interpolation property on many data sets. Specifically, these models can achieve approximately zero loss on all training samples simultaneously. This property has been exploited to develop optimisation algorithms for this setting. These algorithms use the fact that the optimal loss value is known to employ a variation of a Polyak step size calculated on each stochastic batch of data. We introduce a novel extension of this idea to tasks where the interpolation property does not hold. As we no longer have access to the optimal loss values a priori, we instead estimate them for each sample online. To realise this, we introduce a simple but highly effective heuristic for approximating the optimal value based on previous loss evaluations. We provide rigorous experimentation on a range of problems. From our empirical analysis we demonstrate the effectiveness of our approach, which outperforms other single hyperparameter optimisation methods",
    "checked": true,
    "id": "987deac4428d901f806748a23b7a356b2a3e7f33",
    "semantic_title": "faking interpolation until you make it",
    "citation_count": 0,
    "authors": [
      "Alasdair Paren",
      "Rudra P. K. Poudel",
      "M. Pawan Kumar"
    ]
  },
  "https://openreview.net/forum?id=lIOQFVncY9": {
    "title": "Ensembles of Classifiers: a Bias-Variance Perspective",
    "volume": "main",
    "abstract": "Ensembles are a straightforward, remarkably effective method for improving the accuracy, calibration, and robustness of neural networks on classification tasks. Yet, the reasons underlying their success remain an active area of research. Building upon (Pfau, 2013), we turn to the bias-variance decomposition of Bregman divergences in order to gain insight into the behavior of ensembles under classification losses. Introducing a dual reparameterization of the bias-variance decomposition, we first derive generalized laws of total expectation and variance, then discuss how bias and variance terms can be estimated empirically. Next, we show that the dual reparameterization naturally introduces a way of constructing ensembles which reduces the variance and leaves the bias unchanged. Conversely, we show that ensembles that directly average model outputs can arbitrarily increase or decrease the bias. Empirically, we see that such ensembles of neural networks may reduce the bias. We conclude with an empirical analysis of ensembles over neural network architecture hyperparameters, revealing that these techniques allow for more efficient bias reduction than standard ensembles",
    "checked": true,
    "id": "852a9f09e898c79a9f1b5c8323a5931aa5e9f2dd",
    "semantic_title": "ensembles of classifiers: a bias-variance perspective",
    "citation_count": 10,
    "authors": [
      "Neha Gupta",
      "Jamie Smith",
      "Ben Adlam",
      "Zelda E Mariet"
    ]
  },
  "https://openreview.net/forum?id=LSAAlS7Yts": {
    "title": "GFNet: Geometric Flow Network for 3D Point Cloud Semantic Segmentation",
    "volume": "main",
    "abstract": "Point cloud semantic segmentation from projected views, such as range-view (RV) and bird's-eye-view (BEV), has been intensively investigated. Different views capture different information of point clouds and thus are complementary to each other. However, recent projection-based methods for point cloud semantic segmentation usually utilize a vanilla late fusion strategy for the predictions of different views, failing to explore the complementary information from a geometric perspective during the representation learning. In this paper, we introduce a geometric flow network (GFNet) to explore the geometric correspondence between different views in an align-before-fuse manner. Specifically, we devise a novel geometric flow module (GFM) to bidirectionally align and propagate the complementary information across different views according to geometric relationships under the end-to-end learning scheme. We perform extensive experiments on two widely used benchmark datasets, SemanticKITTI and nuScenes, to demonstrate the effectiveness of our GFNet for project-based point cloud semantic segmentation. Concretely, GFNet not only significantly boosts the performance of each individual view but also achieves state-of-the-art results over all existing projection-based models. Code is available at \\url{https://github.com/haibo-qiu/GFNet}",
    "checked": true,
    "id": "0323d48ee0f1194e34a115bac15cc75db4c1e7ca",
    "semantic_title": "gfnet: geometric flow network for 3d point cloud semantic segmentation",
    "citation_count": 38,
    "authors": [
      "Haibo Qiu",
      "Baosheng Yu",
      "Dacheng Tao"
    ]
  },
  "https://openreview.net/forum?id=tPMQ6Je2rB": {
    "title": "Deep Learning for Bayesian Optimization of Scientific Problems with High-Dimensional Structure",
    "volume": "main",
    "abstract": "Bayesian optimization (BO) is a popular paradigm for global optimization of expensive black-box functions, but there are many domains where the function is not completely a black-box. The data may have some known structure (e.g.\\ symmetries) and/or the data generation process may be a composite process that yields useful intermediate or auxiliary information in addition to the value of the optimization objective. However, surrogate models traditionally employed in BO, such as Gaussian Processes (GPs), scale poorly with dataset size and do not easily accommodate known structure. Instead, we use Bayesian neural networks, a class of scalable and flexible surrogate models with inductive biases, to extend BO to complex, structured problems with high dimensionality. We demonstrate BO on a number of realistic problems in physics and chemistry, including topology optimization of photonic crystal materials using convolutional neural networks, and chemical property optimization of molecules using graph neural networks. On these complex tasks, we show that neural networks often outperform GPs as surrogate models for BO in terms of both sampling efficiency and computational cost",
    "checked": true,
    "id": "9b437a86c4cd410b035754741b48ee7ad42730f4",
    "semantic_title": "deep learning for bayesian optimization of scientific problems with high-dimensional structure",
    "citation_count": 17,
    "authors": [
      "Samuel Kim",
      "Peter Y Lu",
      "Charlotte Loh",
      "Jamie Smith",
      "Jasper Snoek",
      "Marin Soljacic"
    ]
  },
  "https://openreview.net/forum?id=6qMKztPn0n": {
    "title": "Evolving Decomposed Plasticity Rules for Information-Bottlenecked Meta-Learning",
    "volume": "main",
    "abstract": "Artificial neural networks (ANNs) are typically confined to accomplishing pre-defined tasks by learning a set of static parameters. In contrast, biological neural networks (BNNs) can adapt to various new tasks by continually updating the neural connections based on the inputs, which is aligned with the paradigm of learning effective learning rules in addition to static parameters, \\textit{e.g.}, meta-learning. Among various biologically inspired learning rules, Hebbian plasticity updates the neural network weights using local signals without the guide of an explicit target function, thus enabling an agent to learn automatically without human efforts. However, typical plastic ANNs using a large amount of meta-parameters violate the nature of the genomics bottleneck and potentially deteriorate the generalization capacity. This work proposes a new learning paradigm decomposing those connection-dependent plasticity rules into neuron-dependent rules thus accommodating $\\Theta(n^2)$ learnable parameters with only $\\Theta(n)$ meta-parameters. We also thoroughly study the effect of different neural modulation on plasticity. Our algorithms are tested in challenging random 2D maze environments, where the agents have to use their past experiences to shape the neural connections and improve their performances for the future. The results of our experiment validate the following: 1. Plasticity can be adopted to continually update a randomly initialized RNN to surpass pre-trained, more sophisticated recurrent models, especially when coming to long-term memorization. 2. Following the genomics bottleneck, the proposed decomposed plasticity can be comparable to or even more effective than canonical plasticity rules in some instances",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan Wang",
      "Hao Tian",
      "Haoyi Xiong",
      "Hua Wu",
      "Jie Fu",
      "Yang Cao",
      "Yu Kang",
      "Haifeng Wang"
    ]
  },
  "https://openreview.net/forum?id=zwRX9kkKzj": {
    "title": "Multitask Online Mirror Descent",
    "volume": "main",
    "abstract": "We introduce and analyze MT-OMD, a multitask generalization of Online Mirror Descent (OMD) which operates by sharing updates between tasks. We prove that the regret of MT-OMD is of order $\\sqrt{1 + \\sigma^2(N-1)}\\sqrt{T}$, where $\\sigma^2$ is the task variance according to the geometry induced by the regularizer, $N$ is the number of tasks, and $T$ is the time horizon. Whenever tasks are similar, that is $\\sigma^2 \\le 1$, our method improves upon the $\\sqrt{NT}$ bound obtained by running independent OMDs on each task. We further provide a matching lower bound, and show that our multitask extensions of Online Gradient Descent and Exponentiated Gradient, two major instances of OMD, enjoy closed-form updates, making them easy to use in practice. Finally, we present experiments which support our theoretical findings",
    "checked": true,
    "id": "29433d72936f44b1114ceb98e9a6d5c724102eda",
    "semantic_title": "multitask online mirror descent",
    "citation_count": 6,
    "authors": [
      "Nicol√≤ Cesa-Bianchi",
      "Pierre Laforgue",
      "Andrea Paudice",
      "Massimiliano Pontil"
    ]
  },
  "https://openreview.net/forum?id=Ig82l87ZVU": {
    "title": "Approximating 1-Wasserstein Distance with Trees",
    "volume": "main",
    "abstract": "The Wasserstein distance, which measures the discrepancy between distributions, shows efficacy in various types of natural language processing and computer vision applications. One of the challenges in estimating the Wasserstein distance is that it is computationally expensive and does not scale well for many distribution-comparison tasks. In this study, we aim to approximate the 1-Wasserstein distance by the tree-Wasserstein distance (TWD), where the TWD is a 1-Wasserstein distance with tree-based embedding that can be computed in linear time with respect to the number of nodes on a tree. More specifically, we propose a simple yet efficient L1-regularized approach for learning the weights of edges in a tree. To this end, we first demonstrate that the 1-Wasserstein approximation problem can be formulated as a distance approximation problem using the shortest path distance on a tree. We then show that the shortest path distance can be represented by a linear model and formulated as a Lasso-based regression problem. Owing to the convex formulation, we can efficiently obtain a globally optimal solution. We also propose a tree-sliced variant of these methods. Through experiments, we demonstrate that the TWD can accurately approximate the original 1-Wasserstein distance by using the weight estimation technique. Our code can be found in the GitHub repository",
    "checked": true,
    "id": "95d7b8ad43bce4a75988a9f1645f2eb39acf7253",
    "semantic_title": "approximating 1-wasserstein distance with trees",
    "citation_count": 9,
    "authors": [
      "Makoto Yamada",
      "Yuki Takezawa",
      "Ryoma Sato",
      "Han Bao",
      "Zornitsa Kozareva",
      "Sujith Ravi"
    ]
  },
  "https://openreview.net/forum?id=u0n1chY0b6": {
    "title": "Learning Accurate Decision Trees with Bandit Feedback via Quantized Gradient Descent",
    "volume": "main",
    "abstract": "Decision trees provide a rich family of highly non-linear but efficient models, due to which they continue to be the go-to family of predictive models by practitioners across domains. But learning trees is challenging due to their discrete decision boundaries. The state-of-the-art (SOTA) techniques resort to (a) learning soft trees thereby losing logarithmic inference time; or (b) using methods tailored to specific supervised learning settings, requiring access to labeled examples and loss function. In this work, by leveraging techniques like overparameterization and straight-through estimators, we propose a unified method that enables accurate end-to-end gradient based tree training and can be deployed in a variety of settings like offline supervised learning and online learning with bandit feedback. Using extensive validation on standard benchmarks, we demonstrate that our method provides best of both worlds, i.e., it is competitive to, and in some cases more accurate than methods designed specifically for the supervised settings; and in bandit settings, where most existing tree learning techniques are not applicable, our models are still accurate and significantly outperform the applicable SOTA methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ajaykrishna Karthikeyan",
      "Naman Jain",
      "Nagarajan Natarajan",
      "Prateek Jain"
    ]
  },
  "https://openreview.net/forum?id=AEoYjvjKVA": {
    "title": "Probabilistic Autoencoder",
    "volume": "main",
    "abstract": "Principal Component Analysis (PCA) minimizes the reconstruction error given a class of linear models of fixed component dimensionality. Probabilistic PCA adds a probabilistic structure by learning the probability distribution of the PCA latent space weights, thus creating a generative model. Autoencoders (AE) minimize the reconstruction error in a class of nonlinear models of fixed latent space dimensionality and outperform PCA at fixed dimensionality. Here, we introduce the Probabilistic Autoencoder (PAE) that learns the probability distribution of the AE latent space weights using a normalizing flow (NF). The PAE is fast and easy to train and achieves small reconstruction errors, high sample quality, and good performance in downstream tasks. We compare the PAE to Variational AE (VAE), showing that the PAE trains faster, reaches a lower reconstruction error, and produces good sample quality without requiring special tuning parameters or training procedures. We further demonstrate that the PAE is a powerful model for performing the downstream tasks of probabilistic image reconstruction in the context of Bayesian inference of inverse problems for inpainting and denoising applications. Finally, we identify latent space density from NF as a promising outlier detection metric",
    "checked": false,
    "id": "033032b0d5a36561f0a804bbb3051537745642d6",
    "semantic_title": "vanilla probabilistic autoencoder",
    "citation_count": 1,
    "authors": [
      "Vanessa M Boehm",
      "Uros Seljak"
    ]
  },
  "https://openreview.net/forum?id=D3WI0QG7dC": {
    "title": "Decoder Denoising Pretraining for Semantic Segmentation",
    "volume": "main",
    "abstract": "Semantic segmentation labels are expensive and time consuming to acquire. Hence, pretraining is commonly used to improve the label-efficiency of segmentation models. Typically, the encoder of a segmentation model is pretrained as a classifier and the decoder is randomly initialized. Here, we argue that random initialization of the decoder can be suboptimal, especially when few labeled examples are available. We propose a decoder pretraining approach based on denoising, which can be combined with supervised pretraining of the encoder. We find that decoder denoising pretraining on the ImageNet dataset strongly outperforms encoder-only supervised pretraining. Despite its simplicity, decoder denoising pretraining achieves state-of-the-art results on label-efficient semantic segmentation and offers considerable gains on the Cityscapes, Pascal Context, and ADE20K datasets",
    "checked": true,
    "id": "868f052bf099692d4d188675488b21dce35fdd07",
    "semantic_title": "decoder denoising pretraining for semantic segmentation",
    "citation_count": 27,
    "authors": [
      "Emmanuel Asiedu Brempong",
      "Simon Kornblith",
      "Ting Chen",
      "Niki Parmar",
      "Matthias Minderer",
      "Mohammad Norouzi"
    ]
  },
  "https://openreview.net/forum?id=JL6MU9XFzW": {
    "title": "Can You Win Everything with A Lottery Ticket?",
    "volume": "main",
    "abstract": "$\\textit{Lottery ticket hypothesis}$ (LTH) has demonstrated to yield independently trainable and highly sparse neural networks (a.k.a. $\\textit{winning tickets}$), whose test set accuracies can be surprisingly on par or even better than dense models. However, accuracy is far from the only evaluation metric, and perhaps not always the most important one. Hence it might be myopic to conclude that a sparse subnetwork can replace its dense counterpart, even if the accuracy is preserved. Spurred by that, we perform the first comprehensive assessment of lottery tickets from diverse aspects beyond test accuracy, including $\\textit{(i)}$ generalization to distribution shifts, $\\textit{(ii)}$ prediction uncertainty, $\\textit{(iii)}$ interpretability, and $\\textit{(iv)}$ geometry of loss landscapes. With extensive experiments across datasets {CIFAR-10, CIFAR-100, and ImageNet}, model architectures, as well as tens of sparsification methods, we thoroughly characterize the trade-off between model sparsity and the all-dimension model capabilities. We find that an appropriate sparsity (e.g., $20\\%\\sim99.08\\%$) can yield the winning ticket to perform comparably or even better $\\textbf{in all above four aspects}$, although some aspects (generalization to certain distribution shifts, and uncertainty) appear more sensitive to the sparsification than others. We term it as a $\\texttt{LTH-PASS}$. Overall, our results endorse choosing a good sparse subnetwork of a larger dense model, over directly training a small dense model of similar parameter counts. We hope that our study can offer more in-depth insights on pruning, for researchers and engineers who seek to incorporate sparse neural networks for user-facing deployments. Codes are available in: https://github.com/VITA-Group/LTH-Pass",
    "checked": true,
    "id": "7c0b26c232b47b8567be1cdf675154d271209cae",
    "semantic_title": "can you win everything with a lottery ticket?",
    "citation_count": 13,
    "authors": [
      "Tianlong Chen",
      "Zhenyu Zhang",
      "Jun Wu",
      "Randy Huang",
      "Sijia Liu",
      "Shiyu Chang",
      "Zhangyang Wang"
    ]
  },
  "https://openreview.net/forum?id=gCmQK6McbR": {
    "title": "HEAT: Hyperedge Attention Networks",
    "volume": "main",
    "abstract": "Learning from structured data is a core machine learning task. Commonly, such data is represented as graphs, which normally only consider (typed) binary relationships between pairs of nodes. This is a substantial limitation for many domains with highly-structured data. One important such domain is source code, where hypergraph-based representations can better capture the semantically rich and structured nature of code. In this work, we present HEAT, a neural model capable of representing typed and qualified hypergraphs, where each hyperedge explicitly qualifies how participating nodes contribute. It can be viewed as a generalization of both message passing neural networks and Transformers. We evaluate HEAT on knowledge base completion and on bug detection and repair using a novel hypergraph representation of programs. In both settings, it outperforms strong baselines, indicating its power and generality",
    "checked": true,
    "id": "7428f9b16a82839e2cb6e6c7a77c1ffeab898813",
    "semantic_title": "heat: hyperedge attention networks",
    "citation_count": 17,
    "authors": [
      "Dobrik Georgiev Georgiev",
      "Marc Brockschmidt",
      "Miltiadis Allamanis"
    ]
  },
  "https://openreview.net/forum?id=t5HkgbxZp1": {
    "title": "On the Near-Optimality of Local Policies in Large Cooperative Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "We show that in a cooperative $N$-agent network, one can design locally executable policies for the agents such that the resulting discounted sum of average rewards (value) well approximates the optimal value computed over all (including non-local) policies. Specifically, we prove that, if $|\\mathcal{X}|, |\\mathcal{U}|$ denote the size of state, and action spaces of individual agents, then for sufficiently small discount factor, the approximation error is given by $\\mathcal{O}(e)$ where $e\\triangleq \\frac{1}{\\sqrt{N}}\\left[\\sqrt{|\\mathcal{X}|}+\\sqrt{|\\mathcal{U}|}\\right]$. Moreover, in a special case where the reward and state transition functions are independent of the action distribution of the population, the error improves to $\\mathcal{O}(e)$ where $e\\triangleq \\frac{1}{\\sqrt{N}}\\sqrt{|\\mathcal{X}|}$. Finally, we also devise an algorithm to explicitly construct a local policy. With the help of our approximation results, we further establish that the constructed local policy is within $\\mathcal{O}(\\max\\{e,\\epsilon\\})$ distance of the optimal policy, and the sample complexity to achieve such a local policy is $\\mathcal{O}(\\epsilon^{-3})$, for any $\\epsilon>0$",
    "checked": true,
    "id": "7744326ff1fde9c769b8d6cc23449e4c44f6b6f4",
    "semantic_title": "on the near-optimality of local policies in large cooperative multi-agent reinforcement learning",
    "citation_count": 5,
    "authors": [
      "Washim Uddin Mondal",
      "Vaneet Aggarwal",
      "Satish Ukkusuri"
    ]
  },
  "https://openreview.net/forum?id=n3qLz4eL1l": {
    "title": "Exploring Efficient Few-shot Adaptation for Vision Transformers",
    "volume": "main",
    "abstract": "The task of Few-shot Learning (FSL) aims to do the inference on novel categories containing only few labeled examples, with the help of knowledge learned from base categories containing abundant labeled training samples. While there are numerous works into FSL task, Vision Transformers (ViTs) have rarely been taken as the backbone to FSL with few trials focusing on naive finetuning of whole backbone or classification layer. Essentially, despite ViTs have been shown to enjoy comparable or even better performance on other vision tasks, it is still very nontrivial to efficiently finetune the ViTs in real-world FSL scenarios. To this end, we propose a novel efficient Transformer Tuning (eTT) method that facilitates finetuning ViTs in the FSL tasks. The key novelties come from the newly presented Attentive Prefix Tuning (APT) and Domain Residual Adapter (DRA) for the task and backbone finetuning, individually. Specifically, in APT, the prefix is projected to new key and value pairs that are attached to each self-attention layer to provide the model with task-specific information. Moreover, we design the DRA in the form of learnable offset vectors to handle the potential domain gaps between base and novel data. To ensure the APT would not deviate from the initial task-specific information much, we further propose a novel prototypical regularization, which minimizes the similarity between the projected distribution of prefix and initial prototypes, regularizing the update procedure. Our method receives outstanding performance on the challenging Meta-Dataset. We conduct extensive experiments to show the efficacy of our model. Our model and codes will be released",
    "checked": false,
    "id": "af593c53a9221bd12211f78d4f1ebd6b59cc4e7c",
    "semantic_title": "parameter-efficient model adaptation for vision transformers",
    "citation_count": 91,
    "authors": [
      "Chengming Xu",
      "Siqian Yang",
      "Yabiao Wang",
      "Zhanxiong Wang",
      "Yanwei Fu",
      "Xiangyang Xue"
    ]
  },
  "https://openreview.net/forum?id=w3z3sN1b04": {
    "title": "Weight Expansion: A New Perspective on Dropout and Generalization",
    "volume": "main",
    "abstract": "While dropout is known to be a successful regularization technique, insights into the mechanisms that lead to this success are still lacking. We introduce the concept of weight expansion, an increase in the signed volume of a parallelotope spanned by the column or row vectors of the weight covariance matrix, and show that weight expansion is an effective means of increasing the generalization in a PAC-Bayesian setting. We provide a theoretical argument that dropout leads to weight expansion and extensive empirical support for the correlation between dropout and weight expansion. To support our hypothesis that weight expansion can be regarded as an indicator of the enhanced generalization capability endowed by dropout, and not just as a mere by-product, we have studied other methods that achieve weight expansion (resp.\\ contraction), and found that they generally lead to an increased (resp.\\ decreased) generalization ability. This suggests that dropout is an attractive regularizer, because it is a computationally cheap method for obtaining weight expansion. This insight justifies the role of dropout as a regularizer, while paving the way for identifying regularizers that promise improved generalization through weight expansion",
    "checked": true,
    "id": "6d9c42e0a653a13b0dc4881f3e5f874d12a5a0c0",
    "semantic_title": "weight expansion: a new perspective on dropout and generalization",
    "citation_count": 5,
    "authors": [
      "Gaojie Jin",
      "Xinping Yi",
      "Pengfei Yang",
      "Lijun Zhang",
      "Sven Schewe",
      "Xiaowei Huang"
    ]
  },
  "https://openreview.net/forum?id=HjelcW6wio": {
    "title": "Exploring the Learning Mechanisms of Neural Division Modules",
    "volume": "main",
    "abstract": "Of the four fundamental arithmetic operations (+, -, $\\times$, $\\div$), division is considered the most difficult for both humans and computers. In this paper, we show that robustly learning division in a systematic manner remains a challenge even at the simplest level of dividing two numbers. We propose two novel approaches for division which we call the Neural Reciprocal Unit (NRU) and the Neural Multiplicative Reciprocal Unit (NMRU), and present improvements for an existing division module, the Real Neural Power Unit (Real NPU). In total we measure robustness over 475 different training sets for setups with and without input redundancy. We discover robustness is greatly affected by the input sign for the Real NPU and NRU, input magnitude for the NMRU and input distribution for every module. Despite this issue, we show that the modules can learn as part of larger end-to-end networks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bhumika Mistry",
      "Katayoun Farrahi",
      "Jonathon Hare"
    ]
  },
  "https://openreview.net/forum?id=U8uJAUMzj9": {
    "title": "Domain Invariant Adversarial Learning",
    "volume": "main",
    "abstract": "The phenomenon of adversarial examples illustrates one of the most basic vulnerabilities of deep neural networks. Among the variety of techniques introduced to surmount this inherent weakness, adversarial training has emerged as the most effective strategy for learning robust models. Typically, this is achieved by balancing robust and natural objectives. In this work, we aim to further optimize the trade-off between robust and standard accuracy by enforcing a domain-invariant feature representation. We present a new adversarial training method, Domain Invariant Adversarial Learning (DIAL), which learns a feature representation that is both robust and domain invariant. DIAL uses a variant of Domain Adversarial Neural Network (DANN) on the natural domain and its corresponding adversarial domain. In the case where the source domain consists of natural examples and the target domain is the adversarially perturbed examples, our method learns a feature representation constrained not to discriminate between the natural and adversarial examples, and can therefore achieve a more robust representation. DIAL is a generic and modular technique that can be easily incorporated into any adversarial training method. Our experiments indicate that incorporating DIAL in the adversarial training process improves both robustness and standard accuracy",
    "checked": true,
    "id": "3ce0b4d9787cfb91dffca9126b5724f42115c920",
    "semantic_title": "domain invariant adversarial learning",
    "citation_count": 11,
    "authors": [
      "Matan Levi",
      "Idan Attias",
      "Aryeh Kontorovich"
    ]
  },
  "https://openreview.net/forum?id=Su290sknyQ": {
    "title": "Momentum Capsule Networks",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "635e491b570937bc244f0c5796d53cf72844011c",
    "semantic_title": "momentum capsule networks",
    "citation_count": 1,
    "authors": [
      "Josef Gugglberger",
      "Antonio Rodriguez-sanchez",
      "David Peer"
    ]
  },
  "https://openreview.net/forum?id=7j0GI6tPYi": {
    "title": "ANCER: Anisotropic Certification via Sample-wise Volume Maximization",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "ccae570e9a47227c4141dbc1469a555e5895137b",
    "semantic_title": "ancer: anisotropic certification via sample-wise volume maximization",
    "citation_count": 32,
    "authors": [
      "Francisco Eiras",
      "Motasem Alfarra",
      "Philip Torr",
      "M. Pawan Kumar",
      "Puneet K. Dokania",
      "Bernard Ghanem",
      "Adel Bibi"
    ]
  },
  "https://openreview.net/forum?id=caRBFhxXIG": {
    "title": "On the Choice of Interpolation Scheme for Neural CDEs",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "23e26bf6a6219caa23609c460aa2a3bfcb0b3d7d",
    "semantic_title": "on the choice of interpolation scheme for neural cdes",
    "citation_count": 17,
    "authors": [
      "James Morrill",
      "Patrick Kidger",
      "Lingyi Yang",
      "Terry Lyons"
    ]
  },
  "https://openreview.net/forum?id=8QoxXTDcsH": {
    "title": "Conformal Prediction Intervals with Temporal Dependence",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "fa1dfdac997c62692d670c3cd9169c7871ed6a01",
    "semantic_title": "conformal prediction intervals with temporal dependence",
    "citation_count": 12,
    "authors": [
      "Zhen Lin",
      "Shubhendu Trivedi",
      "Jimeng Sun"
    ]
  },
  "https://openreview.net/forum?id=Cct7kqbHK6": {
    "title": "Meta-Learning Sparse Compression Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonathan Schwarz",
      "Yee Whye Teh"
    ]
  },
  "https://openreview.net/forum?id=q1Fey9feu7": {
    "title": "Estimating Potential Outcome Distributions with Collaborating Causal Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianhui Zhou",
      "William E Carson IV",
      "David Carlson"
    ]
  },
  "https://openreview.net/forum?id=4GuIi1jJ74": {
    "title": "Sparse Coding with Multi-layer Decoders using Variance Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Katrina Evtimova",
      "Yann LeCun"
    ]
  },
  "https://openreview.net/forum?id=LnjclqBl8R": {
    "title": "Efficient CDF Approximations for Normalizing Flows",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chandramouli Shama Sastry",
      "Andreas Lehrmann",
      "Marcus A Brubaker",
      "Alexander Radovic"
    ]
  },
  "https://openreview.net/forum?id=jKN1pXi7b0": {
    "title": "Unsupervised Dense Information Retrieval with Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gautier Izacard",
      "Mathilde Caron",
      "Lucas Hosseini",
      "Sebastian Riedel",
      "Piotr Bojanowski",
      "Armand Joulin",
      "Edouard Grave"
    ]
  },
  "https://openreview.net/forum?id=e7mYYMSyZH": {
    "title": "On the Convergence of Shallow Neural Network Training with Randomly Masked Neurons",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangshuo Liao",
      "Anastasios Kyrillidis"
    ]
  },
  "https://openreview.net/forum?id=Ee277P3AYC": {
    "title": "CoCa: Contrastive Captioners are Image-Text Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahui Yu",
      "Zirui Wang",
      "Vijay Vasudevan",
      "Legg Yeung",
      "Mojtaba Seyedhosseini",
      "Yonghui Wu"
    ]
  },
  "https://openreview.net/forum?id=TWSTyYd2Rl": {
    "title": "Attentive Walk-Aggregating Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mehmet F Demirel",
      "Shengchao Liu",
      "Siddhant Garg",
      "Zhenmei Shi",
      "Yingyu Liang"
    ]
  },
  "https://openreview.net/forum?id=p5V8P2J61u": {
    "title": "Birds of a Feather Trust Together: Knowing When to Trust a Classifier via Adaptive Neighborhood Aggregation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miao Xiong",
      "Shen Li",
      "Wenjie Feng",
      "Ailin Deng",
      "Jihai Zhang",
      "Bryan Hooi"
    ]
  },
  "https://openreview.net/forum?id=8GvRCWKHIL": {
    "title": "Optimal Client Sampling for Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenlin Chen",
      "Samuel Horv√°th",
      "Peter Richt√°rik"
    ]
  },
  "https://openreview.net/forum?id=VcXNAr5Rur": {
    "title": "DR-DSGD: A Distributionally Robust Decentralized Learning Algorithm over Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaouki Ben Issaid",
      "Anis Elgabli",
      "Mehdi Bennis"
    ]
  },
  "https://openreview.net/forum?id=qzM1Tw5i7N": {
    "title": "SemiNLL: A Framework of Noisy-Label Learning by Semi-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "ZHUOWEI WANG",
      "Jing Jiang",
      "Bo Han",
      "Lei Feng",
      "Bo An",
      "Gang Niu",
      "Guodong Long"
    ]
  },
  "https://openreview.net/forum?id=qYNfwFCX9a": {
    "title": "SFP: State-free Priors for Exploration in Off-Policy Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marco Bagatella",
      "Sammy Joe Christen",
      "Otmar Hilliges"
    ]
  },
  "https://openreview.net/forum?id=h1zuM6cXpH": {
    "title": "Zero-Shot Learning with Common Sense Knowledge Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nihal V. Nayak",
      "Stephen Bach"
    ]
  },
  "https://openreview.net/forum?id=Q54jBjc896": {
    "title": "Causal Feature Selection via Orthogonal Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashkan Soleymani",
      "Anant Raj",
      "Stefan Bauer",
      "Bernhard Sch√∂lkopf",
      "Michel Besserve"
    ]
  },
  "https://openreview.net/forum?id=urfWb7VjmL": {
    "title": "High Fidelity Visualization of What Your Self-Supervised Representation Knows About",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Florian Bordes",
      "Randall Balestriero",
      "Pascal Vincent"
    ]
  },
  "https://openreview.net/forum?id=fsacLLU35V": {
    "title": "The Fundamental Limits of Neural Networks for Interval Certified Robustness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthew B Mirman",
      "Maximilian Baader",
      "Martin Vechev"
    ]
  },
  "https://openreview.net/forum?id=M8D5iZsnrO": {
    "title": "TITRATED: Learned Human Driving Behavior without Infractions via Amortized Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vasileios Lioutas",
      "Adam Scibior",
      "Frank Wood"
    ]
  },
  "https://openreview.net/forum?id=AiOUi3440V": {
    "title": "No More Pesky Hyperparameters: Offline Hyperparameter Tuning for RL",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Wang",
      "Archit Sakhadeo",
      "Adam M White",
      "James M Bell",
      "Vincent Liu",
      "Xutong Zhao",
      "Puer Liu",
      "Tadashi Kozuno",
      "Alona Fyshe",
      "Martha White"
    ]
  },
  "https://openreview.net/forum?id=BDqzLH1gEm": {
    "title": "Mean-Field Langevin Dynamics : Exponential Convergence and Annealing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "L√©na√Øc Chizat"
    ]
  },
  "https://openreview.net/forum?id=fudOtITMIZ": {
    "title": "Variational Disentanglement for Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufei Wang",
      "Haoliang Li",
      "Hao Cheng",
      "Bihan Wen",
      "Lap-Pui Chau",
      "Alex Kot"
    ]
  },
  "https://openreview.net/forum?id=fXorxxbDvO": {
    "title": "On Robustness to Missing Video for Audiovisual Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oscar Chang",
      "Otavio Braga",
      "Hank Liao",
      "Dmitriy Serdyuk",
      "Olivier Siohan"
    ]
  },
  "https://openreview.net/forum?id=X2BodlyLvT": {
    "title": "Identifying Causal Structure in Dynamical Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dominik Baumann",
      "Friedrich Solowjow",
      "Karl Henrik Johansson",
      "Sebastian Trimpe"
    ]
  },
  "https://openreview.net/forum?id=sNxNi54B8b": {
    "title": "Do ReLU Networks Have An Edge When Approximating Compactly-Supported Functions?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anastasis Kratsios",
      "Behnoosh Zamanlooy"
    ]
  },
  "https://openreview.net/forum?id=dkHfV3wB2l": {
    "title": "Recurrent networks, hidden states and beliefs in partially observable environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gaspard Lambrechts",
      "Adrien Bolland",
      "Damien Ernst"
    ]
  },
  "https://openreview.net/forum?id=b3v1UrtF6G": {
    "title": "Self-supervise, Refine, Repeat: Improving Unsupervised Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinsung Yoon",
      "Kihyuk Sohn",
      "Chun-Liang Li",
      "Sercan O Arik",
      "Chen-Yu Lee",
      "Tomas Pfister"
    ]
  },
  "https://openreview.net/forum?id=0xENE7HiYm": {
    "title": "Domain-invariant Feature Exploration for Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wang Lu",
      "Jindong Wang",
      "Haoliang Li",
      "Yiqiang Chen",
      "Xing Xie"
    ]
  },
  "https://openreview.net/forum?id=e3S0Bl2RO8": {
    "title": "Stable and Interpretable Unrolled Dictionary Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bahareh Tolooshams",
      "Demba E. Ba"
    ]
  },
  "https://openreview.net/forum?id=NPfS5N3jbL": {
    "title": "Exploring Generative Neural Temporal Point Process",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haitao Lin",
      "Lirong Wu",
      "Guojiang Zhao",
      "Liu Pai",
      "Stan Z. Li"
    ]
  },
  "https://openreview.net/forum?id=LJohl5DnZf": {
    "title": "Improving the Trainability of Deep Neural Networks through Layerwise Batch-Entropy Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Peer",
      "Bart Keulen",
      "Sebastian Stabinger",
      "Justus Piater",
      "Antonio Rodriguez-sanchez"
    ]
  },
  "https://openreview.net/forum?id=lAv8fShACA": {
    "title": "Online Coresets for Parameteric and Non-Parametric Bregman Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Supratim Shit",
      "Anirban Dasgupta",
      "Rachit Chhaya",
      "Jayesh Choudhari"
    ]
  },
  "https://openreview.net/forum?id=bMar2OkxVu": {
    "title": "Max-Affine Spline Insights Into Deep Network Pruning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoran You",
      "Randall Balestriero",
      "Zhihan Lu",
      "Yutong Kou",
      "Huihong Shi",
      "Shunyao Zhang",
      "Shang Wu",
      "Yingyan Lin",
      "Richard Baraniuk"
    ]
  },
  "https://openreview.net/forum?id=NL2L3XjVFx": {
    "title": "Did I do that? Blame as a means to identify controlled effects in reinforcement learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oriol Corcoll",
      "Youssef Sherif Mansour Mohamed",
      "Raul Vicente"
    ]
  },
  "https://openreview.net/forum?id=xwWsiFmUEs": {
    "title": "QuaRL: Quantization for Fast and Environmentally Sustainable Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Srivatsan Krishnan",
      "Max Lam",
      "Sharad Chitlangia",
      "Zishen Wan",
      "Gabriel Barth-maron",
      "Aleksandra Faust",
      "Vijay Janapa Reddi"
    ]
  },
  "https://openreview.net/forum?id=ggPhsYCsm9": {
    "title": "NeSF: Neural Semantic Fields for Generalizable Semantic Segmentation of 3D Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suhani Vora",
      "Noha Radwan",
      "Klaus Greff",
      "Henning Meyer",
      "Kyle Genova",
      "Mehdi S. M. Sajjadi",
      "Etienne Pot",
      "Andrea Tagliasacchi",
      "Daniel Duckworth"
    ]
  },
  "https://openreview.net/forum?id=NT9zgedd3I": {
    "title": "Learning to Switch Among Agents in a Team via 2-Layer Markov Decision Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vahid Balazadeh",
      "Abir De",
      "Adish Singla",
      "Manuel Gomez Rodriguez"
    ]
  },
  "https://openreview.net/forum?id=ILPFasEaHA": {
    "title": "A Self-Supervised Framework for Function Learning and Extrapolation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simon Segert",
      "Jonathan Cohen"
    ]
  },
  "https://openreview.net/forum?id=2EOVIvRXlv": {
    "title": "Ranking Recovery under Privacy Considerations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minoh Jeong",
      "Alex Dytso",
      "Martina Cardone"
    ]
  },
  "https://openreview.net/forum?id=tLIBAEYjcv": {
    "title": "Learning the Transformer Kernel",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sankalan Pal Chowdhury",
      "Adamos Solomou",
      "Kumar Avinava Dubey",
      "Mrinmaya Sachan"
    ]
  },
  "https://openreview.net/forum?id=dpOYN7o8Jm": {
    "title": "Optimizing Functionals on the Space of Probabilities with Input Convex Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Alvarez-Melis",
      "Yair Schiff",
      "Youssef Mroueh"
    ]
  },
  "https://openreview.net/forum?id=yVkpxs77cD": {
    "title": "Deformation Robust Roto-Scale-Translation Equivariant CNNs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liyao Gao",
      "Guang Lin",
      "Wei Zhu"
    ]
  },
  "https://openreview.net/forum?id=2VEUIq9Yff": {
    "title": "Adversarial Feature Augmentation and Normalization for Visual Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianlong Chen",
      "Yu Cheng",
      "Zhe Gan",
      "Jianfeng Wang",
      "Lijuan Wang",
      "Jingjing Liu",
      "Zhangyang Wang"
    ]
  },
  "https://openreview.net/forum?id=aRsLetumx1": {
    "title": "How Expressive are Transformers in Spectral Domain for Graphs?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anson Bastos",
      "Abhishek Nadgeri",
      "Kuldeep Singh",
      "Hiroki Kanezashi",
      "Toyotaro Suzumura",
      "Isaiah Onando Mulang'"
    ]
  },
  "https://openreview.net/forum?id=ak6Bds2DcI": {
    "title": "Robust and Data-efficient Q-learning by Composite Value-estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gabriel Kalweit",
      "Maria Kalweit",
      "Joschka Boedecker"
    ]
  },
  "https://openreview.net/forum?id=xyt4wfdo4J": {
    "title": "Iterative State Estimation in Non-linear Dynamical Systems Using Approximate Expectation Propagation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanket Kamthe",
      "So Takao",
      "Shakir Mohamed",
      "Marc Peter Deisenroth"
    ]
  },
  "https://openreview.net/forum?id=SEUGkraMPi": {
    "title": "The Graph Cut Kernel for Ranked Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michelangelo Conserva",
      "Marc Peter Deisenroth",
      "K S Sesh Kumar"
    ]
  },
  "https://openreview.net/forum?id=zlQXV7xtZs": {
    "title": "NoiLin: Improving adversarial training and correcting stereotype of noisy labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingfeng Zhang",
      "Xilie Xu",
      "Bo Han",
      "Tongliang Liu",
      "Lizhen Cui",
      "Gang Niu",
      "Masashi Sugiyama"
    ]
  },
  "https://openreview.net/forum?id=tnPjQpYk7D": {
    "title": "Multi-Agent Off-Policy TDC with Near-Optimal Sample and Communication Complexities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyi Chen",
      "Yi Zhou",
      "Rong-Rong Chen"
    ]
  },
  "https://openreview.net/forum?id=GvF9ktXI1V": {
    "title": "Benchmarking and Analyzing Unsupervised Network Representation Learning and the Illusion of Progress",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saket Gurukar",
      "Priyesh Vijayan",
      "srinivasan parthasarathy",
      "Balaraman Ravindran",
      "Aakash Srinivasan",
      "Goonmeet Bajaj",
      "Chen Cai",
      "Moniba Keymanesh",
      "Saravana Kumar",
      "Pranav Maneriker",
      "Anasua Mitra",
      "Vedang Patel"
    ]
  },
  "https://openreview.net/forum?id=ZPBJPGX3Bz": {
    "title": "Decoding EEG With Spiking Neural Networks on Neuromorphic Hardware",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Neelesh Kumar",
      "Guangzhi Tang",
      "Raymond Yoo",
      "Konstantinos P. Michmizos"
    ]
  },
  "https://openreview.net/forum?id=8HuyXvbvqX": {
    "title": "Understanding Linearity of Cross-Lingual Word Embedding Mappings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xutan Peng",
      "Mark Stevenson",
      "Chenghua Lin",
      "Chen Li"
    ]
  },
  "https://openreview.net/forum?id=Euf7KofunK": {
    "title": "Clustering units in neural networks: upstream vs downstream information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Richard D Lange",
      "David Rolnick",
      "Konrad Kording"
    ]
  },
  "https://openreview.net/forum?id=0ZbPmmB61g": {
    "title": "Boosting Search Engines with Interactive Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leonard Adolphs",
      "Benjamin B√∂rschinger",
      "Christian Buck",
      "Michelle Chen Huebscher",
      "Massimiliano Ciaramita",
      "Lasse Espeholt",
      "Thomas Hofmann",
      "Yannic Kilcher",
      "Sascha Rothe",
      "Pier Giuseppe Sessa",
      "Lierni Sestorain"
    ]
  },
  "https://openreview.net/forum?id=P1DuPJzVTN": {
    "title": "Greedy Bayesian Posterior Approximation with Deep Ensembles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aleksei Tiulpin",
      "Matthew B. Blaschko"
    ]
  },
  "https://openreview.net/forum?id=KKeCMim5VN": {
    "title": "Auto-Lambda: Disentangling Dynamic Task Relationships",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shikun Liu",
      "Stephen James",
      "Andrew Davison",
      "Edward Johns"
    ]
  },
  "https://openreview.net/forum?id=4nPswr1KcP": {
    "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andreas Peter Steiner",
      "Alexander Kolesnikov",
      "Xiaohua Zhai",
      "Ross Wightman",
      "Jakob Uszkoreit",
      "Lucas Beyer"
    ]
  },
  "https://openreview.net/forum?id=3gfpBR1ncr": {
    "title": "On Characterizing the Trade-off in Invariant Representation Learning",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bashir Sadeghi",
      "Sepehr Dehdashtian",
      "Vishnu Boddeti"
    ]
  },
  "https://openreview.net/forum?id=1ikK0kHjvj": {
    "title": "A Generalist Agent",
    "volume": "outstanding",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Scott Reed",
      "Konrad Zolna",
      "Emilio Parisotto",
      "Sergio G√≥mez Colmenarejo",
      "Alexander Novikov",
      "Gabriel Barth-maron",
      "Mai Gim√©nez",
      "Yury Sulsky",
      "Jackie Kay",
      "Jost Tobias Springenberg",
      "Tom Eccles",
      "Jake Bruce",
      "Ali Razavi",
      "Ashley Edwards",
      "Nicolas Heess",
      "Yutian Chen",
      "Raia Hadsell",
      "Oriol Vinyals",
      "Mahyar Bordbar",
      "Nando de Freitas"
    ]
  },
  "https://openreview.net/forum?id=AFDcYJKhND": {
    "title": "Scaling Autoregressive Models for Content-Rich Text-to-Image Generation",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahui Yu",
      "Yuanzhong Xu",
      "Jing Yu Koh",
      "Thang Luong",
      "Gunjan Baid",
      "Zirui Wang",
      "Vijay Vasudevan",
      "Alexander Ku",
      "Yinfei Yang",
      "Burcu Karagol Ayan",
      "Ben Hutchinson",
      "Wei Han",
      "Zarana Parekh",
      "Xin Li",
      "Han Zhang",
      "Jason Baldridge",
      "Yonghui Wu"
    ]
  },
  "https://openreview.net/forum?id=oLvlPJheCD": {
    "title": "Queried Unlabeled Data Improves and Robustifies Class-Incremental Learning",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianlong Chen",
      "Sijia Liu",
      "Shiyu Chang",
      "Lisa Amini",
      "Zhangyang Wang"
    ]
  },
  "https://openreview.net/forum?id=GFK1FheE7F": {
    "title": "Deconstructing Self-Supervised Monocular Reconstruction: The Design Decisions that Matter",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaime Spencer",
      "Chris Russell",
      "Simon Hadfield",
      "Richard Bowden"
    ]
  },
  "https://openreview.net/forum?id=sX9d3gfwtE": {
    "title": "Non-Deterministic Behavior of Thompson Sampling with Linear Payoffs and How to Avoid It",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Doruk Kilitcioglu",
      "Serdar Kadioglu"
    ]
  },
  "https://openreview.net/forum?id=vwOKBldzFu": {
    "title": "A Snapshot of the Frontiers of Client Selection in Federated Learning",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gergely D√°niel N√©meth",
      "Miguel Angel Lozano",
      "Novi Quadrianto",
      "Nuria M Oliver"
    ]
  },
  "https://openreview.net/forum?id=NljBlZ6hmG": {
    "title": "Action Noise in Off-Policy Deep Reinforcement Learning: Impact on Exploration and Performance",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jakob Hollenstein",
      "Sayantan Auddy",
      "Matteo Saveriano",
      "Erwan Renaudo",
      "Justus Piater"
    ]
  },
  "https://openreview.net/forum?id=yzkSU5zdwD": {
    "title": "Emergent Abilities of Large Language Models",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jason Wei",
      "Yi Tay",
      "Rishi Bommasani",
      "Colin Raffel",
      "Barret Zoph",
      "Sebastian Borgeaud",
      "Dani Yogatama",
      "Maarten Bosma",
      "Denny Zhou",
      "Donald Metzler",
      "Ed H. Chi",
      "Tatsunori Hashimoto",
      "Oriol Vinyals",
      "Percy Liang",
      "Jeff Dean",
      "William Fedus"
    ]
  },
  "https://openreview.net/forum?id=ywr5sWqQt4": {
    "title": "A Comprehensive Study of Real-Time Object Detection Networks Across Multiple Domains: A Survey",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elahe Arani",
      "Shruthi Gowda",
      "Ratnajit Mukherjee",
      "Omar Magdy",
      "Senthilkumar Sockalingam Kathiresan",
      "Bahram Zonooz"
    ]
  },
  "https://openreview.net/forum?id=LTyqvLEv5b": {
    "title": "On the link between conscious function and general intelligence in humans and machines",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arthur Juliani",
      "Kai Arulkumaran",
      "Shuntaro Sasai",
      "Ryota Kanai"
    ]
  },
  "https://openreview.net/forum?id=gzhEGhcsnN": {
    "title": "Structural Learning in Artificial Neural Networks: A Neural Operator Perspective",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaitlin Maile",
      "Luga Herv√©",
      "Dennis George Wilson"
    ]
  },
  "https://openreview.net/forum?id=P9Cj6RJmN2": {
    "title": "A Stochastic Optimization Framework for Fair Risk Minimization",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrew Lowy",
      "Sina Baharlouei",
      "Rakesh Pavan",
      "Meisam Razaviyayn",
      "Ahmad Beirami"
    ]
  },
  "https://openreview.net/forum?id=vqRzLv6POg": {
    "title": "If your data distribution shifts, use self-learning",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Evgenia Rusak",
      "Steffen Schneider",
      "George Pachitariu",
      "Luisa Eck",
      "Peter Vincent Gehler",
      "Oliver Bringmann",
      "Wieland Brendel",
      "Matthias Bethge"
    ]
  },
  "https://openreview.net/forum?id=UVDAKQANOW": {
    "title": "Unifying Approaches in Active Learning and Active Sampling via Fisher Information and Information-Theoretic Quantities",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andreas Kirsch",
      "Yarin Gal"
    ]
  },
  "https://openreview.net/forum?id=MhK5aXo3gB": {
    "title": "Convergence of denoising diffusion models under the manifold hypothesis",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Valentin De Bortoli"
    ]
  },
  "https://openreview.net/forum?id=oRP8urZ8Fx": {
    "title": "A Note on \"Assessing Generalization of SGD via Disagreement",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andreas Kirsch",
      "Yarin Gal"
    ]
  },
  "https://openreview.net/forum?id=vUuHPRrWs2": {
    "title": "Practicality of generalization guarantees for unsupervised domain adaptation with neural networks",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adam Breitholtz",
      "Fredrik Daniel Johansson"
    ]
  },
  "https://openreview.net/forum?id=EQpGkw5rvL": {
    "title": "Lookback for Learning to Branch",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prateek Gupta",
      "Elias Boutros Khalil",
      "Didier Ch√©telat",
      "Maxime Gasse",
      "Andrea Lodi",
      "Yoshua Bengio",
      "M. Pawan Kumar"
    ]
  },
  "https://openreview.net/forum?id=EDAk6F8yMM": {
    "title": "Local Kernel Ridge Regression for Scalable, Interpolating, Continuous Regression",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingxuan Han",
      "Chenglong Ye",
      "Jeff Phillips"
    ]
  },
  "https://openreview.net/forum?id=i0ZM36d2qU": {
    "title": "Sparse MoEs meet Efficient Ensembles",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "James Urquhart Allingham",
      "Florian Wenzel",
      "Zelda E Mariet",
      "Basil Mustafa",
      "Joan Puigcerver",
      "Neil Houlsby",
      "Ghassen Jerfel",
      "Vincent Fortuin",
      "Balaji Lakshminarayanan",
      "Jasper Snoek",
      "Dustin Tran",
      "Carlos Riquelme Ruiz",
      "Rodolphe Jenatton"
    ]
  },
  "https://openreview.net/forum?id=qrGKGZZvH0": {
    "title": "Do better ImageNet classifiers assess perceptual similarity better?",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manoj Kumar",
      "Neil Houlsby",
      "Nal Kalchbrenner",
      "Ekin Dogus Cubuk"
    ]
  },
  "https://openreview.net/forum?id=3IqqJh2Ycy": {
    "title": "Equivariant Mesh Attention Networks",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sourya Basu",
      "Jose Gallego-Posada",
      "Francesco Vigan√≤",
      "James Rowbottom",
      "Taco Cohen"
    ]
  },
  "https://openreview.net/forum?id=whJPugmP5I": {
    "title": "Finding and Fixing Spurious Patterns with Explanations",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gregory Plumb",
      "Marco Tulio Ribeiro",
      "Ameet Talwalkar"
    ]
  },
  "https://openreview.net/forum?id=IKhEPWGdwK": {
    "title": "Understanding AdamW through Proximal Methods and Scale-Freeness",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenxun Zhuang",
      "Mingrui Liu",
      "Ashok Cutkosky",
      "Francesco Orabona"
    ]
  },
  "https://openreview.net/forum?id=0nEZCVshxS": {
    "title": "Diagnosing and Fixing Manifold Overfitting in Deep Generative Models",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gabriel Loaiza-Ganem",
      "Brendan Leigh Ross",
      "Jesse C Cresswell",
      "Anthony L. Caterini"
    ]
  },
  "https://openreview.net/forum?id=Id7hTt78FV": {
    "title": "Deep Classifiers with Label Noise Modeling and Distance Awareness",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vincent Fortuin",
      "Mark Collier",
      "Florian Wenzel",
      "James Urquhart Allingham",
      "Jeremiah Zhe Liu",
      "Dustin Tran",
      "Balaji Lakshminarayanan",
      "Jesse Berent",
      "Rodolphe Jenatton",
      "Effrosyni Kokiopoulou"
    ]
  },
  "https://openreview.net/forum?id=berNQMTYWZ": {
    "title": "Your Policy Regularizer is Secretly an Adversary",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rob Brekelmans",
      "Tim Genewein",
      "Jordi Grau-Moya",
      "Gregoire Detetang",
      "Markus Kunesch",
      "Shane Legg",
      "Pedro A Ortega"
    ]
  },
  "https://openreview.net/forum?id=86fhqdBUbx": {
    "title": "TLDR: Twin Learning for Dimensionality Reduction",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yannis Kalantidis",
      "Carlos Eduardo Rosar Kos Lassance",
      "Jon Almaz√°n",
      "Diane Larlus"
    ]
  }
}