{
  "https://openreview.net/forum?id=QaDevCcmcg": {
    "title": "Uncertainty-Based Active Learning for Reading Comprehension",
    "volume": "main",
    "abstract": "Recent years have witnessed a surge of successful applications of machine reading comprehension. Of central importance to these tasks is the availability of massive amount of labeled data, which facilitates training of large-scale neural networks. However, in many real-world problems, annotated data are expensive to gather not only because of time cost and budget, but also of certain domain-specific restrictions such as privacy for healthcare data. In this regard, we propose an uncertainty-based active learning algorithm for reading comprehension, which interleaves data annotation and model updating to mitigate the demand of labeling. Our key techniques are two-fold: 1) an unsupervised uncertainty-based sampling scheme that queries the labels of the most informative instances with respect to the currently learned model; and 2) an adaptive loss minimization paradigm that simultaneously fits the data and controls the degree of model updating. We demonstrate on benchmark datasets that 25% less labeled samples suffice to guarantee similar, or even improved performance. Our results show strong evidence that for label-demanding scenarios, the proposed approach offers a practical guide on data collection and model training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Wang",
      "Jie Shen",
      "Xiaofei Ma",
      "Andrew Arnold"
    ]
  },
  "https://openreview.net/forum?id=p8gncJbMit": {
    "title": "A geometrical connection between sparse and low-rank matrices and its application to manifold learning",
    "volume": "main",
    "abstract": "We consider when a sparse nonnegative matrix $\\mathbf{S}$ can be recovered, via an elementwise nonlinearity, from a real-valued matrix~$\\mathbf{L}$ of significantly lower rank. Of particular interest is the setting where the positive elements of $\\mathbf{S}$ encode the similarities of nearby points on a low dimensional manifold. The recovery can then be posed as a problem in manifold learning---in this case, how to learn a norm-preserving and neighborhood-preserving mapping of high dimensional inputs into a lower dimensional space. We describe an algorithm for this problem based on a generalized low-rank decomposition of sparse matrices. This decomposition has the interesting property that it can be encoded by a neural network with one layer of rectified linear units; since the algorithm discovers this encoding, it can also be viewed as a layerwise primitive for deep learning. The algorithm regards the inputs $\\mathbf{x}_i$ and $\\mathbf{x}_j$ as similar whenever the cosine of the angle between them exceeds some threshold $\\tau\\in(0,1)$. Given this threshold, the algorithm attempts to discover a mapping $\\mathbf{x}_i\\mapsto\\mathbf{y}_i$ by matching the elements of two sparse matrices; in particular, it seeks a mapping for which $\\mathbf{S}=\\max(0,\\mathbf{L})$, where $S_{ij} = \\max(0,\\mathbf{x}_i\\!\\cdot\\!\\mathbf{x}_j\\! -\\! \\tau\\|\\mathbf{x}_i\\|\\|\\mathbf{x}_j\\|)$ and $L_{ij} = \\mathbf{y}_i\\!\\cdot\\!\\mathbf{y}_j\\! -\\! \\tau\\|\\mathbf{y}_i\\|\\|\\mathbf{y}_j\\|$. We apply the algorithm to data sets where vector magnitudes and small cosine distances have interpretable meanings (e.g., the brightness of an image, the similarity to other words). On these data sets, the algorithm is able to discover much lower dimensional representations that preserve these meanings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lawrence K. Saul"
    ]
  },
  "https://openreview.net/forum?id=VipljNfZSZ": {
    "title": "Collaborative Algorithms for Online Personalized Mean Estimation",
    "volume": "main",
    "abstract": "We consider an online estimation problem involving a set of agents. Each agent has access to a (personal) process that generates samples from a real-valued distribution and seeks to estimate its mean. We study the case where some of the distributions have the same mean, and the agents are allowed to actively query information from other agents. The goal is to design an algorithm that enables each agent to improve its mean estimate thanks to communication with other agents. The means as well as the number of distributions with same mean are unknown, which makes the task nontrivial. We introduce a novel collaborative strategy to solve this online personalized mean estimation problem. We analyze its time complexity and introduce variants that enjoy good performance in numerical experiments. We also extend our approach to the setting where clusters of agents with similar means seek to estimate the mean of their cluster",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mahsa Asadi",
      "Aur√©lien Bellet",
      "Odalric-Ambrym Maillard",
      "Marc Tommasi"
    ]
  },
  "https://openreview.net/forum?id=x4hmIsWu7e": {
    "title": "Indiscriminate Data Poisoning Attacks on Neural Networks",
    "volume": "main",
    "abstract": "Data poisoning attacks, in which a malicious adversary aims to influence a model by injecting ``poisoned'' data into the training process, have attracted significant recent attention. In this work, we take a closer look at existing poisoning attacks and connect them with old and new algorithms for solving sequential Stackelberg games. By choosing an appropriate loss function for the attacker and optimizing with algorithms that exploit second-order information, we design poisoning attacks that are effective on neural networks. We present efficient implementations by parameterizing the attacker and allowing simultaneous and coordinated generation of tens of thousands of poisoned points, in contrast to most existing methods that generate poisoned points one by one. We further perform extensive experiments that empirically explore the effect of data poisoning attacks on deep neural networks. Our paper sets a new benchmark on the possibility of performing indiscriminate data poisoning attacks on modern neural networks",
    "checked": true,
    "id": "3473619be2b2a481c0ac459ec2bf5449119fceb1",
    "semantic_title": "indiscriminate data poisoning attacks on neural networks",
    "citation_count": 26,
    "authors": [
      "Yiwei Lu",
      "Gautam Kamath",
      "Yaoliang Yu"
    ]
  },
  "https://openreview.net/forum?id=HFfJWx60IT": {
    "title": "An empirical study of implicit regularization in deep offline RL",
    "volume": "main",
    "abstract": "Deep neural networks are the most commonly used function approximators in offline reinforcement learning. Prior works have shown that neural nets trained with TD-learning and gradient descent can exhibit implicit regularization that can be characterized by under-parameterization of these networks. Specifically, the rank of the penultimate feature layer, also called effective rank, has been observed to drastically collapse during the training. In turn, this collapse has been argued to reduce the model's ability to further adapt in later stages of learning, leading to the diminished final performance. Such an association between the effective rank and performance makes effective rank compelling for offline RL, primarily for offline policy evaluation. In this work, we conduct a careful empirical study on the relation between effective rank and performance on three offline RL datasets : bsuite, Atari, and DeepMind lab. We observe that a direct association exists only in restricted settings and disappears in the more extensive hyperparameter sweeps. Also, we empirically identify three phases of learning that explain the impact of implicit regularization on the learning dynamics and found that bootstrapping alone is insufficient to explain the collapse of the effective rank. Further, we show that several other factors could confound the relationship between effective rank and performance and conclude that studying this association under simplistic assumptions could be highly misleading",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Caglar Gulcehre",
      "Srivatsan Srinivasan",
      "Jakub Sygnowski",
      "Georg Ostrovski",
      "Mehrdad Farajtabar",
      "Matthew Hoffman",
      "Razvan Pascanu",
      "Arnaud Doucet"
    ]
  },
  "https://openreview.net/forum?id=sRgvmXjrmg": {
    "title": "Unsupervised Network Embedding Beyond Homophily",
    "volume": "main",
    "abstract": "Network embedding (NE) approaches have emerged as a predominant technique to represent complex networks and have benefited numerous tasks. However, most NE approaches rely on a homophily assumption to learn embeddings with the guidance of supervisory signals, leaving the unsupervised heterophilous scenario relatively unexplored. This problem becomes especially relevant in fields where a scarcity of labels exists. Here, we formulate the unsupervised NE task as an r-ego network discrimination problem and develop the SELENE framework for learning on networks with homophily and heterophily. Specifically, we design a dual-channel feature embedding pipeline to discriminate r-ego networks using node attributes and structural information separately. We employ heterophily adapted self-supervised learning objective functions to optimise the framework to learn intrinsic node embeddings. We show that SELENE's components improve the quality of node embeddings, facilitating the discrimination of connected heterophilous nodes. Comprehensive empirical evaluations on both synthetic and real-world datasets with varying homophily ratios validate the effectiveness of SELENE in homophilous and heterophilous settings showing an up to 12.52% clustering accuracy gain",
    "checked": true,
    "id": "b95ec59a0fd7641f4dc7eaade0014ce89950c415",
    "semantic_title": "unsupervised network embedding beyond homophily",
    "citation_count": 6,
    "authors": [
      "Zhiqiang Zhong",
      "Guadalupe Gonzalez",
      "Daniele Grattarola",
      "Jun Pang"
    ]
  },
  "https://openreview.net/forum?id=eWvBEMTlRq": {
    "title": "Unsupervised Learning of Neurosymbolic Encoders",
    "volume": "main",
    "abstract": "We present a framework for the unsupervised learning of neurosymbolic encoders, which are encoders obtained by composing neural networks with symbolic programs from a domain-specific language. Our framework naturally incorporates symbolic expert knowledge into the learning process, which leads to more interpretable and factorized latent representations compared to fully neural encoders. We integrate modern program synthesis techniques with the variational autoencoding (VAE) framework, in order to learn a neurosymbolic encoder in conjunction with a standard decoder. The programmatic descriptions from our encoders can benefit many analysis workflows, such as in behavior modeling where interpreting agent actions and movements is important. We evaluate our method on learning latent representations for real-world trajectory data from animal biology and sports analytics. We show that our approach offers significantly better separation of meaningful categories than standard VAEs and leads to practical gains on downstream analysis tasks, such as for behavior classification",
    "checked": true,
    "id": "d90d3975f9ee7fb4ffb886ffe1f09dff90b2f951",
    "semantic_title": "unsupervised learning of neurosymbolic encoders",
    "citation_count": 14,
    "authors": [
      "Eric Zhan",
      "Jennifer J. Sun",
      "Ann Kennedy",
      "Yisong Yue",
      "Swarat Chaudhuri"
    ]
  },
  "https://openreview.net/forum?id=4pCjIGIjrt": {
    "title": "Sequentially learning the topological ordering of directed acyclic graphs with likelihood ratio scores",
    "volume": "main",
    "abstract": "Causal discovery, the learning of causality in a data mining scenario, has been of strong scientific and theoretical interest as a starting point to identify \"what causes what?'' Contingent on assumptions and a proper learning algorithm, it is sometimes possible to identify and accurately estimate an underlying directed acyclic graph (DAG), as opposed to a Markov equivalence class of graphs that gives ambiguity of causal directions. The focus of this paper is in highlighting the identifiability and estimation of DAGs through a sequential sorting procedure that orders variables one at a time, starting at root nodes, followed by children of the root nodes, and so on until completion. We demonstrate a novel application of this general sequential approach to estimate the topological ordering of the DAG corresponding to a linear structural equation model with a non-Gaussian error distribution family. At each step of the procedure, only simple likelihood ratio scores are calculated on regression residuals to decide the next node to append to the current partial ordering. The computational complexity of our algorithm on a $p$-node problem is $\\mathcal{O}(pd)$, where $d$ is the maximum neighborhood size. Under mild assumptions, the population version of our procedure provably identifies a true ordering of the underlying DAG. We provide extensive numerical evidence to demonstrate that this sequential procedure scales to possibly thousands of nodes and works well for high-dimensional data. We accompany these numerical experiments with an application to a single-cell gene expression dataset. Our $\\texttt{R}$ package with examples and installation instructions can be found at https://gabriel-ruiz.github.io/scorelingam/",
    "checked": true,
    "id": "8db5a8c6f346177a8caaec22ff0d5144e47fad72",
    "semantic_title": "sequentially learning the topological ordering of directed acyclic graphs with likelihood ratio scores",
    "citation_count": 2,
    "authors": [
      "Gabriel Ruiz",
      "OSCAR HERNAN MADRID PADILLA",
      "Qing Zhou"
    ]
  },
  "https://openreview.net/forum?id=lukVf4VrfP": {
    "title": "Lazy vs hasty: linearization in deep networks impacts learning schedule based on example difficulty",
    "volume": "main",
    "abstract": "Among attempts at giving a theoretical account of the success of deep neural networks, a recent line of work has identified a so-called `lazy' training regime in which the network can be well approximated by its linearization around initialization. Here we investigate the comparative effect of the lazy (linear) and feature learning (non-linear) regimes on subgroups of examples based on their difficulty. Specifically, we show that easier examples are given more weight in feature learning mode, resulting in faster training compared to more difficult ones. In other words, the non-linear dynamics tends to sequentialize the learning of examples of increasing difficulty. We illustrate this phenomenon across different ways to quantify example difficulty, including c-score, label noise, and in the presence of easy-to-learn spurious correlations. Our results reveal a new understanding of how deep networks prioritize resources across example difficulty",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas George",
      "Guillaume Lajoie",
      "Aristide Baratin"
    ]
  },
  "https://openreview.net/forum?id=VmTYgjYloM": {
    "title": "Fourier Sensitivity and Regularization of Computer Vision Models",
    "volume": "main",
    "abstract": "Recent work has empirically shown that deep neural networks latch on to the Fourier statistics of training data and show increased sensitivity to Fourier-basis directions in the input. Understanding and modifying this Fourier-sensitivity of computer vision models may help improve their robustness, hence, in this paper we study the frequency sensitivity characteristics of deep neural networks using a principled approach. We first propose a $\\textbf{\\textit{basis trick}}$, proving that unitary transformations of the input-gradient of a function can be used to compute its gradient in the basis induced by the transformation. Using this result, we propose a general measure of any differentiable computer vision model's $\\textit{\\textbf{Fourier-sensitivity}}$ using the unitary Fourier-transform of its input-gradient. When applied to deep neural networks, we find that computer vision models are consistently sensitive to particular frequencies dependent on the dataset, training method and architecture. Based on this measure, we further propose a $\\textit{\\textbf{Fourier-regularization}}$ framework to modify the Fourier-sensitivities and frequency bias of models. Using our proposed regularizer-family, we demonstrate that deep neural networks obtain improved classification accuracy on robustness evaluations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kiran Krishnamachari",
      "See-Kiong Ng",
      "Chuan-Sheng Foo"
    ]
  },
  "https://openreview.net/forum?id=2VWR6JfwNo": {
    "title": "MVSFormer: Multi-View Stereo by Learning Robust Image Features and Temperature-based Depth",
    "volume": "main",
    "abstract": "Feature representation learning is the key recipe for learning-based Multi-View Stereo (MVS). As the common feature extractor of learning-based MVS, vanilla Feature Pyramid Networks (FPNs) suffer from discouraged feature representations for reflection and texture-less areas, which limits the generalization of MVS. Even FPNs worked with pre-trained Convolutional Neural Networks (CNNs) fail to tackle these issues. On the other hand, Vision Transformers (ViTs) have achieved prominent success in many 2D vision tasks. Thus we ask whether ViTs can facilitate feature learning in MVS? In this paper, we propose a pre-trained ViT enhanced MVS network called MVSFormer, which can learn more reliable feature representations benefited by informative priors from ViT. The finetuned MVSFormer with hierarchical ViTs of efficient attention mechanisms can achieve prominent improvement based on FPNs. Besides, the alternative MVSFormer with frozen ViT weights is further proposed. This largely alleviates the training cost with competitive performance strengthened by the attention map from the self-distillation pre-training. MVSFormer can be generalized to various input resolutions with efficient multi-scale training strengthened by gradient accumulation. Moreover, we discuss the merits and drawbacks of classification and regression-based MVS methods, and further propose to unify them with a temperature-based strategy. MVSFormer achieves state-of-the-art performance on the DTU dataset. Particularly, MVSFormer ranks as Top-1 on both intermediate and advanced sets of the highly competitive Tanks-and-Temples leaderboard. Codes and models are released in https://github.com/ewrfcas/MVSFormer",
    "checked": true,
    "id": "0475f5c612a4eb5324a23179eede6680ac0ecdb3",
    "semantic_title": "mvsformer: multi-view stereo by learning robust image features and temperature-based depth",
    "citation_count": 54,
    "authors": [
      "Chenjie Cao",
      "Xinlin Ren",
      "Yanwei Fu"
    ]
  },
  "https://openreview.net/forum?id=Z44YAcLaGw": {
    "title": "Controllable Generative Modeling via Causal Reasoning",
    "volume": "main",
    "abstract": "Deep latent variable generative models excel at generating complex, high-dimensional data, often exhibiting impressive generalization beyond the training distribution. However, many such models in use today are black-boxes trained on large unlabelled datasets with statistical objectives and lack an interpretable understanding of the latent space required for controlling the generative process. We propose CAGE, a framework for controllable generation in latent variable models based on causal reasoning. Given a pair of attributes, CAGE infers the implicit cause-effect relationships between these attributes as induced by a deep generative model. This is achieved by defining and estimating a novel notion of unit-level causal effects in the latent space of the generative model. Thereafter, we use the inferred cause-effect relationships to design a novel strategy for controllable generation based on counterfactual sampling. Through a series of large-scale synthetic and human evaluations, we demonstrate that generating counterfactual samples which respect the underlying causal relationships inferred via CAGE leads to subjectively more realistic images",
    "checked": true,
    "id": "e2bc83185fbac80d61d2ad7c242ebe7425a40d23",
    "semantic_title": "controllable generative modeling via causal reasoning",
    "citation_count": 5,
    "authors": [
      "Joey Bose",
      "Ricardo Pio Monti",
      "Aditya Grover"
    ]
  },
  "https://openreview.net/forum?id=DY1pMrmDkm": {
    "title": "Modeling Bounded Rationality in Multi-Agent Simulations Using Rationally Inattentive Reinforcement Learning",
    "volume": "main",
    "abstract": "Multi-agent reinforcement learning (MARL) is a powerful framework for studying emergent behavior in complex agent-based simulations. However, RL agents are often assumed to be rational and behave optimally, which does not fully reflect human behavior. In this work, we propose a new, more human-like RL agent, which incorporates an established model of human-irrationality, the Rational Inattention (RI) model. RI models the cost of cognitive information processing using mutual information. Our RIRL framework generalizes and is more flexible than prior work by allowing for multi-timestep dynamics and information channels with heterogeneous processing costs. We demonstrate the flexibility of RIRL in versions of a classic economic setting (Principal-Agent setting) with varying complexity. In simple settings, we show using RIRL can lead to optimal agent behavior policy with approximately the same functional form as what is expected from the analysis of prior work, which utilizes theoretical methods. We additionally demonstrate that using RIRL to analyze complex, theoretically intractable settings, yields a rich spectrum of new equilibrium behaviors that differ from those found under rationality assumptions. For example, increasing the cognitive cost experienced by a manager agent results in the other agents increasing the magnitude of their action to compensate. These results suggest RIRL is a powerful tool towards building AI agents that can mimic real human behavior",
    "checked": true,
    "id": "a29c571dfa2af621147adf039c0eaffb32eec49b",
    "semantic_title": "modeling bounded rationality in multi-agent simulations using rationally inattentive reinforcement learning",
    "citation_count": 2,
    "authors": [
      "Tong Mu",
      "Stephan Zheng",
      "Alexander R Trott"
    ]
  },
  "https://openreview.net/forum?id=zFhNBs8GaV": {
    "title": "Calibrated Selective Classification",
    "volume": "main",
    "abstract": "Selective classification allows models to abstain from making predictions (e.g., say ``I don't know'') when in doubt in order to obtain better effective accuracy. While typical selective models can succeed at producing more accurate predictions on average, they may still allow for wrong predictions that have high confidence, or skip correct predictions that have low confidence. Providing calibrated uncertainty estimates alongside predictions---probabilities that correspond to true frequencies---can be as important as having predictions that are simply accurate on average. Uncertainty estimates, however, can sometimes be unreliable. In this paper, we develop a new approach to selective classification in which we propose a method for rejecting examples with ``uncertain'' uncertainties. By doing so, we aim to make predictions with well-calibrated uncertainty estimates over the distribution of accepted examples, a property we call selective calibration. We present a framework for learning selectively calibrated models, where a separate selector network is trained to improve the selective calibration error of a given base model. In particular, our work focuses on achieving robust calibration, where the model is intentionally designed to be tested on out-of-domain data. We achieve this through a training strategy inspired by distributionally robust optimization, in which we apply simulated input perturbations to the known, in-domain training data. We demonstrate the empirical effectiveness of our approach on multiple image classification and lung cancer risk assessment tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adam Fisch",
      "Tommi S. Jaakkola",
      "Regina Barzilay"
    ]
  },
  "https://openreview.net/forum?id=29V0xo7jKp": {
    "title": "Unsupervised Mismatch Localization in Cross-Modal Sequential Data with Application to Mispronunciations Localization",
    "volume": "main",
    "abstract": "Content mismatch usually occurs when data from one modality is translated to another, e.g. language learners producing mispronunciations (errors in speech) when reading a sentence (target text) aloud. However, most existing alignment algorithms assume that the content involved in the two modalities is perfectly matched, thus leading to difficulty in locating such mismatch between speech and text. In this work, we develop an unsupervised learning algorithm that can infer the relationship between content-mismatched cross-modal sequential data, especially for speech-text sequences. More specifically, we propose a hierarchical Bayesian deep learning model, dubbed mismatch localization variational autoencoder (ML-VAE), which decomposes the generative process of the speech into hierarchically structured latent variables, indicating the relationship between the two modalities. Training such a model is very challenging due to the discrete latent variables with complex dependencies involved. To address this challenge, we propose a novel and effective training procedure that alternates between estimating the hard assignments of the discrete latent variables over a specifically designed mismatch localization finite-state acceptor (ML-FSA) and updating the parameters of neural networks. In this work, we focus on the mismatch localization problem for speech and text, and our experimental results show that ML-VAE successfully locates the mismatch between text and speech, without the need for human annotations for model training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Wei",
      "Hengguan Huang",
      "Xiangming Gu",
      "Hao Wang",
      "Ye Wang"
    ]
  },
  "https://openreview.net/forum?id=b4tMhpN0JC": {
    "title": "GIT: A Generative Image-to-text Transformer for Vision and Language",
    "volume": "main",
    "abstract": "In this paper, we design and train a Generative Image-to-text Transformer, GIT, to unify vision-language tasks such as image/video captioning and question answering. While generative models provide a consistent network architecture between pre-training and fine-tuning, existing work typically contains complex structures (uni/multi-modal encoder/decoder) and depends on external modules such as object detectors/taggers and optical character recognition (OCR). In GIT, we simplify the architecture as one image encoder and one text decoder under a single language modeling task. We also scale up the pre-training data and the model size to boost the model performance. Without bells and whistles, our GIT establishes new state of the arts on numerous challenging benchmarks with a large margin. For instance, our model surpasses the human performance for the first time on TextCaps (138.2 vs. 125.5 in CIDEr). Furthermore, we present a new scheme of generation-based image classification and scene text recognition, achieving decent performance on standard benchmarks",
    "checked": true,
    "id": "60ee030773ba1b68eb222a265b052ca028353362",
    "semantic_title": "git: a generative image-to-text transformer for vision and language",
    "citation_count": 564,
    "authors": [
      "Jianfeng Wang",
      "Zhengyuan Yang",
      "Xiaowei Hu",
      "Linjie Li",
      "Kevin Lin",
      "Zhe Gan",
      "Zicheng Liu",
      "Ce Liu",
      "Lijuan Wang"
    ]
  },
  "https://openreview.net/forum?id=WXVkgkPXRk": {
    "title": "Concave Utility Reinforcement Learning with Zero-Constraint Violations",
    "volume": "main",
    "abstract": "We consider the problem of tabular infinite horizon concave utility reinforcement learning (CURL) with convex constraints. For this, we propose a model-based learning algorithm that also achieves zero constraint violations. Assuming that the concave objective and the convex constraints have a solution interior to the set of feasible occupation measures, we solve a tighter optimization problem to ensure that the constraints are never violated despite the imprecise model knowledge and model stochasticity. We use Bellman error-based analysis for tabular infinite-horizon setups which allows analyzing stochastic policies. Combining the Bellman error-based analysis and tighter optimization equation, for $T$ interactions with the environment, we obtain a high-probability regret guarantee for objective which grows as $\\Tilde{O}(1/\\sqrt{T})$, excluding other factors. The proposed method can be applied for optimistic algorithms to obtain high-probability regret bounds and also be used for posterior sampling algorithms to obtain a loose Bayesian regret bounds but with significant improvement in computational complexity",
    "checked": true,
    "id": "85d44bb076fa060345132ee5149cd73d30dbe417",
    "semantic_title": "concave utility reinforcement learning with zero-constraint violations",
    "citation_count": 13,
    "authors": [
      "Mridul Agarwal",
      "Qinbo Bai",
      "Vaneet Aggarwal"
    ]
  },
  "https://openreview.net/forum?id=k4iWTEdUSF": {
    "title": "Fast and Accurate Spreading Process Temporal Scale Estimation",
    "volume": "main",
    "abstract": "Spreading processes on graphs arise in a host of application domains, from the study of online social networks to viral marketing to epidemiology. Various discrete-time probabilistic models for spreading processes have been proposed. These are used for downstream statistical estimation and prediction problems, often involving messages or other information that is transmitted along with infections caused by the process. These models generally model cascade behavior at a small time scale but are insufficiently flexible to model cascades that exhibit intermittent behavior governed by multiple scales. We argue that the presence of such time scales that are unaccounted for by a cascade model can result in degradation of performance of models on downstream statistical and time-sensitive optimization tasks. To address these issues, we formulate a model that incorporates multiple temporal scales of cascade behavior. This model is parameterized by a \\emph{clock}, which encodes the times at which sessions of cascade activity start. These sessions are themselves governed by a small-scale cascade model, such as the discretized independent cascade (IC) model. Estimation of the multiscale cascade model parameters leads to the problem of \\emph{clock estimation} in terms of a natural distortion measure that we formulate. Our framework is inspired by the optimization problem posed by DiTursi et al, 2017, which can be seen as providing one possible estimator (a maximum-proxy-likelihood estimator) for the parameters of our generative model. We give a clock estimation algorithm, which we call FastClock, that runs in linear time in the size of its input and is provably statistically accurate for a broad range of model parameters when cascades are generated from any spreading process model with well-concentrated session infection set sizes and when the underlying graph is at least in the semi-sparse regime. We exemplify our algorithm for the case where the small-scale model is the discretized independent cascade process and extend substantially to processes whose infection set sizes satisfy a general martingale difference property. We further evaluate the performance of FastClock empirically in comparison to the state of the art estimator from DiTursi et al, 2017. We find that in a broad parameter range on synthetic networks and on a real network, our algorithm substantially outperforms that algorithm in terms of both running time and accuracy. In all cases, our algorithm's running time is asymptotically lower than that of the baseline",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abram Magner",
      "Carolyn S Kaminski",
      "Petko Bogdanov"
    ]
  },
  "https://openreview.net/forum?id=RP6G787uD8": {
    "title": "Extracting Local Reasoning Chains of Deep Neural Networks",
    "volume": "main",
    "abstract": "We study how to explain the main steps of inference that a pre-trained deep neural net (DNN) relies on to produce predictions for a (sub)task and its data. This problem is related to network pruning and interpretable machine learning with the following highlighted differences: (1) fine-tuning of any neurons/filters is forbidden; (2) we target a very high pruning rate, e.g., ‚â• 95%, for better interpretability; (3) the interpretation is for the whole inference process on a few data of a task rather than for individual neurons/filters or a single sample. In this paper, we introduce NeuroChains to extract the local inference chains by optimizing differentiable sparse scores for the filters and layers, which reflects their importance in preserving the outputs on a few data drawn from a given (sub)task. Thereby, NeuroChains can extract an extremely small sub-network composed of critical filters exactly copied from the original pre-trained DNN by removing the filters/layers with small scores. For samples from the same class, we can then visualize the inference pathway in the pre-trained DNN by applying existing interpretation techniques to the retained filters and layers. It reveals how the inference process stitches and integrates the information layer by layer and filter by filter. We provide detailed and insightful case studies together with several quantitative analyses over thousands of trials to demonstrate the quality, sparsity, fidelity and accuracy of the interpretation. In extensive empirical studies on VGG, ResNet, and ViT, NeuroChains significantly enriches the interpretation and makes the inner mechanism of DNNs more transparent",
    "checked": true,
    "id": "156070205d9b7f0030c5a394d56ef64ebef4eb25",
    "semantic_title": "extracting local reasoning chains of deep neural networks",
    "citation_count": 0,
    "authors": [
      "Haiyan Zhao",
      "Tianyi Zhou",
      "Guodong Long",
      "Jing Jiang",
      "Chengqi Zhang"
    ]
  },
  "https://openreview.net/forum?id=LdEm0umNcv": {
    "title": "On Sample Complexity of Offline Reinforcement Learning with Deep ReLU Networks in Besov Spaces",
    "volume": "main",
    "abstract": "Offline reinforcement learning (RL) leverages previously collected data for policy optimization without any further active exploration. Despite the recent interest in this problem, its theoretical results in neural network function approximation settings remain elusive. In this paper, we study the statistical theory of offline RL with deep ReLU network function approximation. In particular, we establish the sample complexity of $n = \\tilde{\\mathcal{O}}( H^{4 + 4 \\frac{d}{\\alpha}} \\kappa_{\\mu}^{1 + \\frac{d}{\\alpha}} \\epsilon^{-2 - 2\\frac{d}{\\alpha}} )$ for offline RL with deep ReLU networks, where $\\kappa_{\\mu}$ is a measure of distributional shift, $H = (1-\\gamma)^{-1}$ is the effective horizon length, $d$ is the dimension of the state-action space, $\\alpha$ is a (possibly fractional) smoothness parameter of the underlying Markov decision process (MDP), and $\\epsilon$ is a user-specified error. Notably, our sample complexity holds under two novel considerations: the Besov dynamic closure and the correlated structure. While the Besov dynamic closure subsumes the dynamic conditions for offline RL in the prior works, the correlated structure renders the prior works of offline RL with general/neural network function approximation improper or inefficient in long (effective) horizon problems. To the best of our knowledge, this is the first theoretical characterization of the sample complexity of offline RL with deep neural network function approximation under the general Besov regularity condition that goes beyond the linearity regime in the traditional Reproducing Hilbert kernel spaces and Neural Tangent Kernels",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thanh Nguyen-Tang",
      "Sunil Gupta",
      "Hung Tran-The",
      "Svetha Venkatesh"
    ]
  },
  "https://openreview.net/forum?id=F2rG2CXsgO": {
    "title": "Distribution Embedding Networks for Generalization from a Diverse Set of Classification Tasks",
    "volume": "main",
    "abstract": "We propose Distribution Embedding Networks (DEN) for classification with small data. In the same spirit of meta-learning, DEN learns from a diverse set of training tasks with the goal to generalize to unseen target tasks. Unlike existing approaches which require the inputs of training and target tasks to have the same dimension with possibly similar distributions, DEN allows training and target tasks to live in heterogeneous input spaces. This is especially useful for tabular-data tasks where labeled data from related tasks are scarce. DEN uses a three-block architecture: a covariate transformation block followed by a distribution embedding block and then a classification block. We provide theoretical insights to show that this architecture allows the embedding and classification blocks to be fixed after pre-training on a diverse set of tasks; only the covariate transformation block with relatively few parameters needs to be fine-tuned for each new task. To facilitate training, we also propose an approach to synthesize binary classification tasks, and demonstrate that DEN outperforms existing methods in a number of synthetic and real tasks in numerical studies",
    "checked": true,
    "id": "6cfcf483bf9468f800cb3fba854f39a60b4fae35",
    "semantic_title": "distribution embedding networks for generalization from a diverse set of classification tasks",
    "citation_count": 5,
    "authors": [
      "Lang Liu",
      "Mahdi Milani Fard",
      "Sen Zhao"
    ]
  },
  "https://openreview.net/forum?id=NXB0rEM2Tq": {
    "title": "COIN++: Neural Compression Across Modalities",
    "volume": "main",
    "abstract": "Neural compression algorithms are typically based on autoencoders that require specialized encoder and decoder architectures for different data modalities. In this paper, we propose COIN++, a neural compression framework that seamlessly handles a wide range of data modalities. Our approach is based on converting data to implicit neural representations, i.e. neural functions that map coordinates (such as pixel locations) to features (such as RGB values). Then, instead of storing the weights of the implicit neural representation directly, we store modulations applied to a meta-learned base network as a compressed code for the data. We further quantize and entropy code these modulations, leading to large compression gains while reducing encoding time by two orders of magnitude compared to baselines. We empirically demonstrate the feasibility of our method by compressing various data modalities, from images and audio to medical and climate data",
    "checked": true,
    "id": "4664d3b1050c246ed0a723f5b9f2afb78c865f1a",
    "semantic_title": "coin++: neural compression across modalities",
    "citation_count": 90,
    "authors": [
      "Emilien Dupont",
      "Hrushikesh Loya",
      "Milad Alizadeh",
      "Adam Golinski",
      "Yee Whye Teh",
      "Arnaud Doucet"
    ]
  },
  "https://openreview.net/forum?id=FTtFAg3pek": {
    "title": "Systematically and efficiently improving $k$-means initialization by pairwise-nearest-neighbor smoothing",
    "volume": "main",
    "abstract": "We present a meta-method for initializing (seeding) the $k$-means clustering algorithm called PNN-smoothing. It consists in splitting a given dataset into $J$ random subsets, clustering each of them individually, and merging the resulting clusterings with the pairwise-nearest-neighbor (PNN) method. It is a meta-method in the sense that when clustering the individual subsets any seeding algorithm can be used. If the computational complexity of that seeding algorithm is linear in the size of the data $N$ and the number of clusters $k$, PNN-smoothing is also almost linear with an appropriate choice of $J$, and quite competitive in practice. We show empirically, using several existing seeding methods and testing on several synthetic and real datasets, that this procedure results in systematically better costs. In particular, our method of enhancing $k$-means++ seeding proves superior in both effectiveness and speed compared to the popular ``greedy'' $k$-means++ variant. Our implementation is publicly available at \\href{https://github.com/carlobaldassi/KMeansPNNSmoothing.jl}{https://github.com/carlobaldassi/KMeansPNNSmoothing.jl}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Carlo Baldassi"
    ]
  },
  "https://openreview.net/forum?id=tbd9f3HwPy": {
    "title": "GhostSR: Learning Ghost Features for Efficient Image Super-Resolution",
    "volume": "main",
    "abstract": "Modern single image super-resolution (SISR) systems based on convolutional neural networks (CNNs) have achieved impressive performance but require huge computational costs. The problem on feature redundancy has been well studied in visual recognition task, but rarely discussed in SISR. Based on the observation that many features in SISR models are also similar to each other, we propose to use shift operation for generating the redundant features (i.e. ghost features). Compared with depth-wise convolution which is time-consuming on GPU-like devices, shift operation can bring a real inference acceleration for CNNs on common hardware. We analyze the benefits of shift operation in SISR and make the shift orientation learnable based on the Gumbel-Softmax trick. Besides, a clustering procedure is explored based on pre-trained models to identify the intrinsic filters for generating corresponding intrinsic features. The ghost features will be generated by moving these intrinsic features along a certain orientation. Finally, the complete output features are constructed by concatenating the intrinsic and ghost features together. Extensive experiments on several benchmark models and datasets demonstrate that both the non-compact and lightweight SISR CNN models embedded with the proposed method can achieve a comparable performance to the baseline models with a large reduction of parameters, FLOPs and GPU inference latency. For example, we reduce the parameters by 46%, FLOPs by 46% and GPU inference latency by 42% of x2 EDSR model with almost lossless performance. Code will be available at https://gitee.com/mindspore/models/tree/master/research/cv/GhostSR",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ying Nie",
      "Kai Han",
      "Zhenhua Liu",
      "Chuanjian Liu",
      "Yunhe Wang"
    ]
  },
  "https://openreview.net/forum?id=ygoNPRiLxw": {
    "title": "DiffuseVAE: Efficient, Controllable and High-Fidelity Generation from Low-Dimensional Latents",
    "volume": "main",
    "abstract": "Diffusion probabilistic models have been shown to generate state-of-the-art results on several competitive image synthesis benchmarks but lack a low-dimensional, interpretable latent space, and are slow at generation. On the other hand, standard Variational Autoencoders (VAEs) typically have access to a low-dimensional latent space but exhibit poor sample quality. We present DiffuseVAE, a novel generative framework that integrates VAE within a diffusion model framework, and leverage this to design novel conditional parameterizations for diffusion models. We show that the resulting model equips diffusion models with a low-dimensional VAE inferred latent code which can be used for downstream tasks like controllable synthesis. The proposed method also improves upon the speed vs quality tradeoff exhibited in standard unconditional DDPM/DDIM models (for instance, \\textbf{FID of 16.47 vs 34.36} using a standard DDIM on the CelebA-HQ-128 benchmark using \\textbf{T=10} reverse process steps) without having explicitly trained for such an objective. Furthermore, the proposed model exhibits synthesis quality comparable to state-of-the-art models on standard image synthesis benchmarks like CIFAR-10 and CelebA-64 while outperforming most existing VAE-based methods. Lastly, we show that the proposed method exhibits inherent generalization to different types of noise in the conditioning signal. For reproducibility, our source code is publicly available at \\url{https://github.com/kpandey008/DiffuseVAE}",
    "checked": true,
    "id": "ce8e3fa6fa6d45b8b92169a2e181dafb20749a2f",
    "semantic_title": "diffusevae: efficient, controllable and high-fidelity generation from low-dimensional latents",
    "citation_count": 121,
    "authors": [
      "Kushagra Pandey",
      "Avideep Mukherjee",
      "Piyush Rai",
      "Abhishek Kumar"
    ]
  },
  "https://openreview.net/forum?id=9tl6zjLYVS": {
    "title": "On the Origins of the Block Structure Phenomenon in Neural Network Representations",
    "volume": "main",
    "abstract": "Recent work by Nguyen et al. (2021) has uncovered a striking phenomenon in large-capacity neural networks: they contain blocks of contiguous hidden layers with highly similar representations. This block structure has two seemingly contradictory properties: on the one hand, its constituent layers exhibit highly similar dominant first principal components (PCs), but on the other hand, their representations, and their common first PC, are highly dissimilar across different random seeds. Our work seeks to reconcile these discrepant properties by investigating the origin of the block structure in relation to the data and training methods. By analyzing properties of the dominant PCs, we find that the block structure arises from dominant datapoints ‚Äî a small group of examples that share similar image statistics (e.g. background color). However, the set of dominant datapoints, and the precise shared image statistic, can vary across random seeds. Thus, the block structure reflects meaningful dataset statistics, but is simultaneously unique to each model. Through studying hidden layer activations and creating synthetic datapoints, we demonstrate that these simple image statistics dominate the representational geometry of the layers inside the block structure. We explore how the phenomenon evolves through training, finding that the block structure takes shape early in training, but the underlying representations and the corresponding dominant datapoints continue to change substantially. Finally, we study the interplay between the block structure and different training mechanisms, introducing a targeted intervention to eliminate the block structure, as well as examining the effects of pre-training and Shake-Shake regularization",
    "checked": true,
    "id": "5fe4f6fbe26f94ff65290c58007185ec71669921",
    "semantic_title": "on the origins of the block structure phenomenon in neural network representations",
    "citation_count": 13,
    "authors": [
      "Thao Nguyen",
      "Maithra Raghu",
      "Simon Kornblith"
    ]
  },
  "https://openreview.net/forum?id=AZIfC91hjM": {
    "title": "Interpretable Node Representation with Attribute Decoding",
    "volume": "main",
    "abstract": "Variational Graph Autoencoders (VGAEs) are powerful models for unsupervised learning of node representations from graph data. In this work, we make a systematic analysis of modeling node attributes in VGAEs and show that attribute decoding is important for node representation learning. We further propose a new learning model, interpretable NOde Representation with Attribute Decoding (NORAD). The model encodes node representations in an interpretable approach: node representations capture community structures in the graph and the relationship between communities and node attributes. We further propose a rectifying procedure to refine node representations of isolated notes, which improves the quality of the representations of these nodes. Our empirical results demonstrate the advantage of the proposed model when learning graph data in an interpretable approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaohui Chen",
      "Xi Chen",
      "Liping Liu"
    ]
  },
  "https://openreview.net/forum?id=yeT9cBq8Cn": {
    "title": "A Unified Domain Adaptation Framework with Distinctive Divergence Analysis",
    "volume": "main",
    "abstract": "Unsupervised domain adaptation enables knowledge transfer from a labeled source domain to an unlabeled target domain by aligning the learnt features of both domains. The idea is theoretically supported by the generalization bound analysis in Ben-David et al. (2007), which specifies the applicable task (binary classification) and designates a specific distribution divergence measure. Although most distribution-aligning domain adaptation models seek theoretical grounds from this particular bound analysis, they do not actually fit into the stringent conditions. In this paper, we bridge the long-standing theoretical gap in literature by providing a unified generalization bound. Our analysis can well accommodate the classification/regression tasks and most commonly-used divergence measures, and more importantly, it can theoretically recover a large amount of previous models. In addition, we identify the key difference in the distribution divergence measures underlying the diverse models and commit a comprehensive in-depth comparison of the commonly-used divergence measures. Based on the unified generalization bound, we propose new domain adaptation models that achieve transferability through domain-invariant representations and conduct experiments on real-world datasets that corroborate our theoretical findings. We believe these insights are helpful in guiding the future design of distribution-aligning domain adaptation algorithms",
    "checked": true,
    "id": "a97e222f208fd9fda55c57a046d1837610a7dd20",
    "semantic_title": "a unified domain adaptation framework with distinctive divergence analysis",
    "citation_count": 3,
    "authors": [
      "Zhiri YUAN",
      "Xixu HU",
      "Qi WU",
      "Shumin MA",
      "Cheuk Hang LEUNG",
      "Xin Shen",
      "Yiyan HUANG"
    ]
  },
  "https://openreview.net/forum?id=A5tIluhDW6": {
    "title": "Infinitely wide limits for deep Stable neural networks: sub-linear, linear and super-linear activation functions",
    "volume": "main",
    "abstract": "There is a growing literature on the study of large-width properties of deep Gaussian neural networks (NNs), i.e. deep NNs with Gaussian-distributed parameters or weights, and Gaussian stochastic processes. Motivated by some empirical and theoretical studies showing the potential of replacing Gaussian distributions with Stable distributions, namely distributions with heavy tails, in this paper we investigate large-width properties of deep Stable NNs, i.e. deep NNs with Stable-distributed parameters. For sub-linear activation functions, a recent work has characterized the infinitely wide limit of a suitable rescaled deep Stable NN in terms of a Stable stochastic process, both under the assumption of a ``joint growth\" and under the assumption of a ``sequential growth\" of the width over the NN's layers. Here, assuming a ``sequential growth\" of the width, we extend such a characterization to a general class of activation functions, which includes sub-linear, asymptotically linear and super-linear functions. As a novelty with respect to previous works, our results rely on the use of a generalized central limit theorem for heavy tails distributions, which allows for an interesting unified treatment of infinitely wide limits for deep Stable NNs. Our study shows that the scaling of Stable NNs and the stability of their infinitely wide limits may depend on the choice of the activation function, bringing out a critical difference with respect to the Gaussian setting",
    "checked": false,
    "id": "4b27c00b279ff90b7072342c065218915438714a",
    "semantic_title": "large-width asymptotics for relu neural networks with $\\alpha$-stable initializations",
    "citation_count": 2,
    "authors": [
      "Alberto Bordino",
      "Stefano Favaro",
      "Sandra Fortini"
    ]
  },
  "https://openreview.net/forum?id=iGREAJdULX": {
    "title": "Counterfactual Learning with Multioutput Deep Kernels",
    "volume": "main",
    "abstract": "In this paper, we address the challenge of performing counterfactual inference with observational data via Bayesian nonparametric regression adjustment, with a focus on high-dimensional settings featuring multiple actions and multiple correlated outcomes. We present a general class of counterfactual multi-task deep kernels models that estimate causal effects and learn policies proficiently thanks to their sample efficiency gains, while scaling well with high dimensions. In the first part of the work, we rely on Structural Causal Models (SCM) to formally introduce the setup and the problem of identifying counterfactual quantities under observed confounding. We then discuss the benefits of tackling the task of causal effects estimation via stacked coregionalized Gaussian Processes and Deep Kernels. Finally, we demonstrate the use of the proposed methods on simulated experiments that span individual causal effects estimation, off-policy evaluation and optimization",
    "checked": true,
    "id": "c5a6b1f0e4317afa89b070609810dafa01ae05de",
    "semantic_title": "counterfactual learning with multioutput deep kernels",
    "citation_count": 1,
    "authors": [
      "Alberto Caron",
      "Ioanna Manolopoulou",
      "Gianluca Baio"
    ]
  },
  "https://openreview.net/forum?id=gzu4ZbBY7S": {
    "title": "Incorporating Sum Constraints into Multitask Gaussian Processes",
    "volume": "main",
    "abstract": "Machine learning models can be improved by adapting them to respect existing background knowledge. In this paper we consider multitask Gaussian processes, with background knowledge in the form of constraints that require a specific sum of the outputs to be constant. This is achieved by conditioning the prior distribution on the constraint fulfillment. The approach allows for both linear and nonlinear constraints. We demonstrate that the constraints are fulfilled with high precision and that the construction can improve the overall prediction accuracy as compared to the standard Gaussian process",
    "checked": true,
    "id": "876ddef3ba445f7c66918fa30a79e7ca01786f77",
    "semantic_title": "incorporating sum constraints into multitask gaussian processes",
    "citation_count": 3,
    "authors": [
      "Philipp Pilar",
      "Carl Jidling",
      "Thomas B. Sch√∂n",
      "Niklas Wahlstr√∂m"
    ]
  },
  "https://openreview.net/forum?id=P0XO5ZE98j": {
    "title": "Degradation Attacks on Certifiably Robust Neural Networks",
    "volume": "main",
    "abstract": "Certifiably robust neural networks protect against adversarial examples by employing run-time defenses that check if the model is certifiably locally robust at the input under evaluation. We show through examples and experiments that any defense (whether complete or incomplete) based on checking local robustness is inherently over-cautious. Specifically, such defenses flag inputs for which local robustness checks fail, but yet that are not adversarial; i.e., they are classified consistently with all valid inputs within a distance of $\\epsilon$. As a result, while a norm-bounded adversary cannot change the classification of an input, it can use norm-bounded changes to degrade the utility of certifiably robust networks by forcing them to reject otherwise correctly classifiable inputs. We empirically demonstrate the efficacy of such attacks against state-of-the-art certifiable defenses. Our code is available at https://github.com/ravimangal/degradation-attacks",
    "checked": true,
    "id": "4acf1bd971b093ddfb323b70f604a1def5d02d48",
    "semantic_title": "degradation attacks on certifiably robust neural networks",
    "citation_count": 2,
    "authors": [
      "Klas Leino",
      "Chi Zhang",
      "Ravi Mangal",
      "Matt Fredrikson",
      "Bryan Parno",
      "Corina Pasareanu"
    ]
  },
  "https://openreview.net/forum?id=VW4IrC0n0M": {
    "title": "An approximate sampler for energy-based models with divergence diagnostics",
    "volume": "main",
    "abstract": "Energy-based models (EBMs) allow flexible specifications of probability distributions. However, sampling from EBMs is non-trivial, usually requiring approximate techniques such as Markov chain Monte Carlo (MCMC). A major downside of MCMC sampling is that it is often impossible to compute the divergence of the sampling distribution from the target distribution: therefore, the quality of the samples cannot be guaranteed. Here, we introduce quasi-rejection sampling (QRS), a simple extension of rejection sampling that performs approximate sampling, but, crucially, does provide divergence diagnostics (in terms of f-divergences, such as KL divergence and total variation distance). We apply QRS to sampling from discrete EBMs over text for controlled generation. We show that we can sample from such EBMs with arbitrary precision in exchange for sampling efficiency and quantify the trade-off between the two by means of the aforementioned diagnostics",
    "checked": true,
    "id": "ab23ff73e450f6b7e8d639e68320c7952c003694",
    "semantic_title": "an approximate sampler for energy-based models with divergence diagnostics",
    "citation_count": 9,
    "authors": [
      "Bryan Eikema",
      "Germ√°n Kruszewski",
      "Christopher R Dance",
      "Hady Elsahar",
      "Marc Dymetman"
    ]
  },
  "https://openreview.net/forum?id=aRtjVZvbpK": {
    "title": "A Unified Survey on Anomaly, Novelty, Open-Set, and Out of-Distribution Detection: Solutions and Future Challenges",
    "volume": "main",
    "abstract": "Machine learning models often encounter samples that are diverged from the training distribution. Failure to recognize an out-of-distribution (OOD) sample, and consequently assign that sample to an in-class label, significantly compromises the reliability of a model. The problem has gained significant attention due to its importance for safety deploying models in open-world settings. Detecting OOD samples is challenging due to the intractability of modeling all possible unknown distributions. To date, several research domains tackle the problem of detecting unfamiliar samples, including anomaly detection, novelty detection, one-class learning, open set recognition, and out-of-distribution detection. Despite having similar and shared concepts, out-of-distribution, open-set, and anomaly detection have been investigated independently. Accordingly, these research avenues have not crosspollinated, creating research barriers. While some surveys intend to provide an overview of these approaches, they seem to only focus on a specific domain without examining the relationship between different domains. This survey aims to provide a cross-domain and comprehensive review of numerous eminent works in respective areas while identifying their commonalities. Researchers can benefit from the overview of research advances in different fields and develop future methodology synergistically. Furthermore, to the best of our knowledge, while there are surveys in anomaly detection or one-class learning, there is no comprehensive or up-to-date survey on out-of-distribution detection, which this survey covers extensively. Finally, having a unified cross-domain perspective, this study discusses and sheds light on future lines of research, intending to bring these fields closer together",
    "checked": false,
    "id": "8b153cc2c7f5ea9f307f12ea945a5e9196ee5c52",
    "semantic_title": "a unified survey on anomaly, novelty, open-set, and out-of-distribution detection: solutions and future challenges",
    "citation_count": 199,
    "authors": [
      "Mohammadreza Salehi",
      "Hossein Mirzaei",
      "Dan Hendrycks",
      "Yixuan Li",
      "Mohammad Hossein Rohban",
      "Mohammad Sabokrou"
    ]
  },
  "https://openreview.net/forum?id=oRjk5V9eDp": {
    "title": "Bayesian Methods for Constraint Inference in Reinforcement Learning",
    "volume": "main",
    "abstract": "Learning constraints from demonstrations provides a natural and efficient way to improve the safety of AI systems; however, prior work only considers learning a single, point-estimate of the constraints. By contrast, we consider the problem of inferring constraints from demonstrations using a Bayesian perspective. We propose Bayesian Inverse Constraint Reinforcement Learning (BICRL), a novel approach that infers a posterior probability distribution over constraints from demonstrated trajectories. The main advantages of BICRL, compared to prior constraint inference algorithms, are (1) the freedom to infer constraints from partial trajectories and even from disjoint state-action pairs, (2) the ability to infer constraints from suboptimal demonstrations and in stochastic environments, and (3) the opportunity to use the posterior distribution over constraints in order to implement active learning and robust policy optimization techniques. We show that BICRL outperforms pre-existing constraint learning approaches, leading to more accurate constraint inference and consequently safer policies. We further propose Hierarchical BICRL that infers constraints locally in sub-spaces of the entire domain and then composes global constraint estimates leading to accurate and computationally efficient constraint estimation",
    "checked": true,
    "id": "ef4bdf801ce6475b1a2f4f148f4e964aa6ca6d43",
    "semantic_title": "bayesian methods for constraint inference in reinforcement learning",
    "citation_count": 9,
    "authors": [
      "Dimitris Papadimitriou",
      "Usman Anwar",
      "Daniel S. Brown"
    ]
  },
  "https://openreview.net/forum?id=LHAbHkt6Aq": {
    "title": "A Crisis In Simulation-Based Inference? Beware, Your Posterior Approximations Can Be Unfaithful",
    "volume": "main",
    "abstract": "We present extensive empirical evidence showing that current Bayesian simulation-based inference algorithms can produce computationally unfaithful posterior approximations. Our results show that all benchmarked algorithms -- (S)NPE, (S)NRE, SNL and variants of ABC -- can yield overconfident posterior approximations, which makes them unreliable for scientific use cases and falsificationist inquiry. Failing to address this issue may reduce the range of applicability of simulation-based inference. For this reason, we argue that research efforts should be made towards theoretical and methodological developments of conservative approximate inference algorithms and present research directions towards this objective. In this regard, we show empirical evidence that ensembling posterior surrogates provides more reliable approximations and mitigates the issue",
    "checked": true,
    "id": "d386cb044775dfb6f7f6fccbbaa3952b69a3066e",
    "semantic_title": "a crisis in simulation-based inference? beware, your posterior approximations can be unfaithful",
    "citation_count": 25,
    "authors": [
      "Joeri Hermans",
      "Arnaud Delaunoy",
      "Fran√ßois Rozet",
      "Antoine Wehenkel",
      "Volodimir Begy",
      "Gilles Louppe"
    ]
  },
  "https://openreview.net/forum?id=tLG26QxoD8": {
    "title": "On Pseudo-Labeling for Class-Mismatch Semi-Supervised Learning",
    "volume": "main",
    "abstract": "When there are unlabeled Out-Of-Distribution (OOD) data from other classes, Semi-Supervised Learning (SSL) methods suffer from severe performance degradation and even get worse than merely training on labeled data. In this paper, we empirically analyze Pseudo-Labeling (PL) in class-mismatched SSL. PL is a simple and representative SSL method that transforms SSL problems into supervised learning by creating pseudo-labels for unlabeled data according to the model's prediction. We aim to answer two main questions: (1) How do OOD data influence PL? (2) What is the proper usage of OOD data with PL? First, we show that the major problem of PL is imbalanced pseudo-labels on OOD data. Second, we find that OOD data can help classify In-Distribution (ID) data given their OOD ground truth labels. Based on the findings, we propose to improve PL in class-mismatched SSL with two components -- Re-balanced Pseudo-Labeling (RPL) and Semantic Exploration Clustering (SEC). RPL re-balances pseudo-labels of high-confidence data, which simultaneously filters out OOD data and addresses the imbalance problem. SEC uses balanced clustering on low-confidence data to create pseudo-labels on extra classes, simulating the process of training with ground truth. Experiments show that our method achieves steady improvement over supervised baseline and state-of-the-art performance under all class mismatch ratios on different benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lu Han",
      "Han-Jia Ye",
      "De-Chuan Zhan"
    ]
  },
  "https://openreview.net/forum?id=bN2vWLTh0P": {
    "title": "Reinventing Policy Iteration under Time Inconsistency",
    "volume": "main",
    "abstract": "Policy iteration (PI) is a fundamental policy search algorithm in standard reinforcement learning (RL) setting, which can be shown to converge to an optimal policy by policy improvement theorems. However, the standard PI relies on Bellman's Principle of Optimality, which might be violated by some specifications of objectives (also known as time-inconsistent (TIC) objectives), such as non-exponentially discounted reward functions. The use of standard PI under TIC objectives has thus been marked with questions regarding the convergence of its policy improvement scheme and the optimality of its termination policy, often leading to its avoidance. In this paper, we consider an infinite-horizon TIC RL setting and formally present an alternative type of optimality drawn from game theory, i.e., subgame perfect equilibrium (SPE), that attempts to resolve the aforementioned questions. We first analyze standard PI under the SPE type of optimality, revealing its merits and insufficiencies. Drawing on these observations, we propose backward Q-learning (bwdQ), a new algorithm in the approximate PI family that targets SPE policy under non-exponentially discounted reward functions. Finally, with two TIC gridworld environments, we demonstrate the implications of our theoretical findings on the behavior of bwdQ and other approximate PI variants",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nixie S Lesmana",
      "Huangyuan Su",
      "Chi Seng Pun"
    ]
  },
  "https://openreview.net/forum?id=YiOI0vqJ0n": {
    "title": "Nonparametric Learning of Two-Layer ReLU Residual Units",
    "volume": "main",
    "abstract": "We describe an algorithm that learns two-layer residual units using rectified linear unit (ReLU) activation: suppose the input $\\mathbf{x}$ is from a distribution with support space $\\mathbb{R}^d$ and the ground-truth generative model is a residual unit of this type, given by $\\mathbf{y} = \\boldsymbol{B}^\\ast\\left[\\left(\\boldsymbol{A}^\\ast\\mathbf{x}\\right)^+ + \\mathbf{x}\\right]$, where ground-truth network parameters $\\boldsymbol{A}^\\ast \\in \\mathbb{R}^{d\\times d}$ represent a full-rank matrix with nonnegative entries and $\\boldsymbol{B}^\\ast \\in \\mathbb{R}^{m\\times d}$ is full-rank with $m \\geq d$ and for $\\boldsymbol{c} \\in \\mathbb{R}^d$, $[\\boldsymbol{c}^{+}]_i = \\max\\{0, c_i\\}$. We design layer-wise objectives as functionals whose analytic minimizers express the exact ground-truth network in terms of its parameters and nonlinearities. Following this objective landscape, learning residual units from finite samples can be formulated using convex optimization of a nonparametric function: for each layer, we first formulate the corresponding empirical risk minimization (ERM) as a positive semi-definite quadratic program (QP), then we show the solution space of the QP can be equivalently determined by a set of linear inequalities, which can then be efficiently solved by linear programming (LP). We further prove the strong statistical consistency of our algorithm, and demonstrate its robustness and sample efficiency through experimental results on synthetic data and a set of benchmark regression datasets",
    "checked": true,
    "id": "0f6f91b57e5e4e957fd175e55471c84232fdbda9",
    "semantic_title": "nonparametric learning of two-layer relu residual units",
    "citation_count": 1,
    "authors": [
      "Zhunxuan Wang",
      "Linyun He",
      "Chunchuan Lyu",
      "Shay B Cohen"
    ]
  },
  "https://openreview.net/forum?id=uvDD9rN6Zz": {
    "title": "Stochastic Douglas-Rachford Splitting for Regularized Empirical Risk Minimization: Convergence, Mini-batch, and Implementation",
    "volume": "main",
    "abstract": "In this paper, we study the stochastic Douglas-Rachford splitting (SDRS) for general empirical risk minimization (ERM) problems with regularization. Our first contribution is to prove its convergence for both convex and strongly convex problems; the convergence rates are $O(1/\\sqrt{t})$ and $O(1/t)$, respectively. Since SDRS reduces to the stochastic proximal point algorithm (SPPA) when there is no regularization, it is pleasing to see the result matches that of SPPA, under the same mild conditions. We also propose the mini-batch version of SDRS that handles multiple samples simultaneously while maintaining the same efficiency as that of a single one, which is not a straight-forward extension in the context of stochastic proximal algorithms. We show that the mini-batch SDRS again enjoys the same convergence rate. Furthermore, we demonstrate that, for some of the canonical regularized ERM problems, each iteration of SDRS can be efficiently calculated either in closed form or in close to closed form via bisection---the resulting complexity is identical to, for example, the stochastic (sub)gradient method. Experiments on real data demonstrate its effectiveness in terms of convergence compared to SGD and its variants",
    "checked": true,
    "id": "1bbda32a7549d84237d4ba9fd55920f150de5f72",
    "semantic_title": "stochastic douglas-rachford splitting for regularized empirical risk minimization: convergence, mini-batch, and implementation",
    "citation_count": 0,
    "authors": [
      "Aysegul Bumin",
      "Kejun Huang"
    ]
  },
  "https://openreview.net/forum?id=9nhmKwLAWV": {
    "title": "Does Entity Abstraction Help Generative Transformers Reason?",
    "volume": "main",
    "abstract": "We study the utility of incorporating entity type abstractions into pre-trained Transformers and test these methods on four NLP tasks requiring different forms of logical reasoning: (1) compositional language understanding with text-based relational reasoning (CLUTRR), (2) abductive reasoning (ProofWriter), (3) multi-hop question answering (HotpotQA), and (4) conversational question answering (CoQA). We propose and empirically explore three ways to add such abstraction: (i) as additional input embeddings, (ii) as a separate sequence to encode, and (iii) as an auxiliary prediction task for the model. Overall, our analysis demonstrates that models with abstract entity knowledge performs better than without it. The best abstraction aware models achieved an overall accuracy of 88.8% and 91.8% compared to the baseline model achieving 62.9% and 89.8% on CLUTRR and ProofWriter respectively. However, for HotpotQA and CoQA, we find that F1 scores improve by only 0.5% on average. Our results suggest that the benefit of explicit abstraction is significant in formally defined logical reasoning settings requiring many reasoning hops, but point to the notion that it is less beneficial for NLP tasks having less formal logical structure",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicolas Gontier",
      "Siva Reddy",
      "Christopher Pal"
    ]
  },
  "https://openreview.net/forum?id=1PfcmFTXoa": {
    "title": "Complex-Valued Autoencoders for Object Discovery",
    "volume": "main",
    "abstract": "Object-centric representations form the basis of human perception, and enable us to reason about the world and to systematically generalize to new settings. Currently, most works on unsupervised object discovery focus on slot-based approaches, which explicitly separate the latent representations of individual objects. While the result is easily interpretable, it usually requires the design of involved architectures. In contrast to this, we propose a comparatively simple approach ‚Äì the Complex AutoEncoder (CAE) ‚Äì that creates distributed object-centric representations. Following a coding scheme theorized to underlie object representations in biological neurons, its complex-valued activations represent two messages: their magnitudes express the presence of a feature, while the relative phase differences between neurons express which features should be bound together to create joint object representations. In contrast to previous approaches using complex-valued activations for object discovery, we present a fully unsupervised approach that is trained end-to-end ‚Äì resulting in significant improvements in performance and efficiency. Further, we show that the CAE achieves competitive or better unsupervised object discovery performance on simple multi-object datasets compared to a state-of-the-art slot-based approach while being up to 100 times faster to train",
    "checked": true,
    "id": "0bc95645df1845050e642b34dc7593ba9a16072f",
    "semantic_title": "complex-valued autoencoders for object discovery",
    "citation_count": 39,
    "authors": [
      "Sindy L√∂we",
      "Phillip Lippe",
      "Maja Rudolph",
      "Max Welling"
    ]
  },
  "https://openreview.net/forum?id=Sh3RF9JowK": {
    "title": "Learning Algorithms for Markovian Bandits:\\\\Is Posterior Sampling more Scalable than Optimism?",
    "volume": "main",
    "abstract": "In this paper, we study the scalability of model-based algorithms learning the optimal policy of a discounted \\blue{rested} Markovian bandit problem with $n$ arms. There are two categories of model-based reinforcement learning algorithms: Bayesian algorithms (like PSRL), and optimistic algorithms (like UCRL2 or UCBVI). A naive application of these algorithms is not scalable because the state-space is exponential in $n$. In this paper, we construct variants of these algorithms specially tailored to Markovian bandits (MB) that we call MB-PSRL, MB-UCRL2, and MB-UCBVI. \\blue{We consider an episodic setting with geometrically distributed episode length, and measure the performance of the algorithm in terms of regret (Bayesian regret for MB-PSRL and expected regret for MB-UCRL2 and MB-UCBVI)}. We prove that, for this setting, all algorithms have a low regret in $\\tilde{O}(S\\sqrt{nK})$ -- where $K$ is the number of episodes, $n$ is the number of arms and $S$ is the number of states of each arm. Up to a factor $\\sqrt{S}$, these regrets match the \\blue{Bayesian minimax regret} lower bound of $\\Omega(\\sqrt{SnK})$ that we also derive. Even if their theoretical regrets are comparable, the {\\it time complexities} of these algorithms vary greatly: We show that MB-UCRL2, as well as all algorithms that use bonuses on transition matrices have a { time} complexity that grows exponentially in $n$. In contrast, MB-UCBVI does not use bonuses on transition matrices and we show that it can be implemented efficiently, with a time complexity linear in $n$. Our numerical experiments show, however, that its empirical regret is large. Our Bayesian algorithm, MB-PSRL, enjoys the best of both worlds: its running time is linear in the number of arms and its empirical regret is the smallest of all algorithms. This is a new addition in the understanding of the power of Bayesian algorithms, that can often be tailored to the structure of the problems to learn",
    "checked": false,
    "id": "e9770e1fc0f9977a0e890546f4d9062e6a934a28",
    "semantic_title": "learning algorithms for markovian bandits: is posterior sampling more scalable than optimism?",
    "citation_count": 2,
    "authors": [
      "Nicolas Gast",
      "Bruno Gaujal",
      "Kimang Khun"
    ]
  },
  "https://openreview.net/forum?id=NmTMc3uD1G": {
    "title": "Modeling Object Dissimilarity for Deep Saliency Prediction",
    "volume": "main",
    "abstract": "Saliency prediction has made great strides over the past two decades, with current techniques modeling low-level information, such as color, intensity and size contrasts, and high-level ones, such as attention and gaze direction for entire objects. Despite this, these methods fail to account for the dissimilarity between objects, which affects human visual attention. In this paper, we introduce a detection-guided saliency prediction network that explicitly models the differences between multiple objects, such as their appearance and size dissimilarities. Our approach allows us to fuse our object dissimilarities with features extracted by any deep saliency prediction network. As evidenced by our experiments, this consistently boosts the accuracy of the baseline networks, enabling us to outperform the state-of-the-art models on three saliency benchmarks, namely SALICON, MIT300 and CAT2000. Our project page is at https://github.com/IVRL/DisSal",
    "checked": true,
    "id": "5d3828cce5ccce25783aa4486637dc22985aa0f6",
    "semantic_title": "modeling object dissimilarity for deep saliency prediction",
    "citation_count": 3,
    "authors": [
      "Bahar Aydemir",
      "Deblina Bhattacharjee",
      "Tong Zhang",
      "Seungryong Kim",
      "Mathieu Salzmann",
      "Sabine S√ºsstrunk"
    ]
  },
  "https://openreview.net/forum?id=YAVE6jfeJb": {
    "title": "Optimizing Intermediate Representations of Generative Models for Phase Retrieval",
    "volume": "main",
    "abstract": "Phase retrieval is the problem of reconstructing images from magnitude-only measurements. In many real-world applications the problem is underdetermined. When training data is available, generative models allow optimization in a lower-dimensional latent space, hereby constraining the solution set to those images that can be synthesized by the generative model. However, not all possible solutions are within the range of the generator. Instead, they are represented with some error. To reduce this representation error in the context of phase retrieval, we first leverage a novel variation of intermediate layer optimization (ILO) to extend the range of the generator while still producing images consistent with the training data. Second, we introduce new initialization schemes that further improve the quality of the reconstruction. With extensive experiments on the Fourier phase retrieval problem and thorough ablation studies, we can show the benefits of our modified ILO and the new initialization schemes. Additionally, we analyze the performance of our approach on the Gaussian phase retrieval problem",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tobias Uelwer",
      "Sebastian Konietzny",
      "Stefan Harmeling"
    ]
  },
  "https://openreview.net/forum?id=35y5hv9fbb": {
    "title": "Algorithms and Theory for Supervised Gradual Domain Adaptation",
    "volume": "main",
    "abstract": "The phenomenon of data distribution evolving over time has been observed in a range of applications, calling the needs of adaptive learning algorithms. We thus study the problem of supervised gradual domain adaptation, where labeled data from shifting distributions are available to the learner along the trajectory, and we aim to learn a classifier on a target data distribution of interest. Under this setting, we provide the first generalization upper bound on the learning error under mild assumptions. Our results are algorithm agnostic, general for a range of loss functions, and only depend linearly on the averaged learning error across the trajectory. This shows significant improvement compared to the previous upper bound for unsupervised gradual domain adaptation, where the learning error on the target domain depends exponentially on the initial error on the source domain. Compared with the offline setting of learning from multiple domains, our results also suggest the potential benefits of the temporal structure among different domains in adapting to the target one. Empirically, our theoretical results imply that learning proper representations across the domains will effectively mitigate the learning errors. Motivated by these theoretical insights, we propose a min-max learning objective to learn the representation and classifier simultaneously. Experimental results on both semi-synthetic and large-scale real datasets corroborate our findings and demonstrate the effectiveness of our objectives",
    "checked": true,
    "id": "0b0126adcef98dedd96bd9f253dd3bb3569d1d3f",
    "semantic_title": "algorithms and theory for supervised gradual domain adaptation",
    "citation_count": 7,
    "authors": [
      "Jing Dong",
      "Shiji Zhou",
      "Baoxiang Wang",
      "Han Zhao"
    ]
  },
  "https://openreview.net/forum?id=ph3AYXpwEb": {
    "title": "Teacher's pet: understanding and mitigating biases in distillation",
    "volume": "main",
    "abstract": "Knowledge distillation is widely used as a means of improving the performance of a relatively simple ``student'' model using the predictions from a complex ``teacher'' model. Several works have shown that distillation significantly boosts the student's \\emph{overall} performance; however, are these gains uniform across all data subgroups? In this paper, we show that distillation can \\emph{harm} performance on certain subgroups, {e.g., classes with few associated samples}, compared to the vanilla student trained using the one-hot labels. We trace this behaviour to errors made by the teacher distribution being transferred to and \\emph{amplified} by the student model, and formally prove that distillation can indeed harm underrepresented subgroups in certain regression settings. To mitigate this problem, we present techniques which soften the teacher influence for subgroups where it is less reliable. Experiments on several image classification benchmarks show that these modifications of distillation maintain boost in overall accuracy, while additionally ensuring improvement in subgroup performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michal Lukasik",
      "Srinadh Bhojanapalli",
      "Aditya Krishna Menon",
      "Sanjiv Kumar"
    ]
  },
  "https://openreview.net/forum?id=LFkRUCalFt": {
    "title": "An Efficient One-Class SVM for Novelty Detection in IoT",
    "volume": "main",
    "abstract": "One-Class Support Vector Machines (OCSVM) are a common approach for novelty detection, due to their flexibility in fitting complex nonlinear boundaries between {normal} and {novel} data. Novelty detection is important in the Internet of Things (``IoT'') due to the threats these devices can present, and OCSVM often performs well in these environments due to the variety of devices, traffic patterns, and anomalies that IoT devices present. Unfortunately, conventional OCSVMs can introduce prohibitive memory and computational overhead at detection time. This work designs, implements and evaluates an efficient OCSVM for such practical settings. We extend Nystr\\\"om and (Gaussian) Sketching approaches to OCSVM, combining these methods with clustering and Gaussian mixture models to achieve 15-30x speedup in prediction time and 30-40x reduction in memory requirements without sacrificing detection accuracy. Here, the very nature of IoT devices is crucial: they tend to admit few modes of \\emph{normal} operation, allowing for efficient pattern compression",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kun Yang",
      "Samory Kpotufe",
      "Nick Feamster"
    ]
  },
  "https://openreview.net/forum?id=63sJsCmq6Q": {
    "title": "Competition over data: how does data purchase affect users?",
    "volume": "main",
    "abstract": "As the competition among machine learning (ML) predictors is widespread in practice, it becomes increasingly important to understand the impact and biases arising from such competition. One critical aspect of ML competition is that ML predictors are constantly updated by acquiring additional data during the competition. Although this active data acquisition can largely affect the overall competition environment, it has not been well-studied before. In this paper, we study what happens when ML predictors can purchase additional data during the competition. We introduce a new environment in which ML predictors use active learning algorithms to effectively acquire labeled data within their budgets while competing against each other. We empirically show that the overall performance of an ML predictor improves when predictors can purchase additional labeled data. Surprisingly, however, the quality that users experience---i.e., the accuracy of the predictor selected by each user---can decrease even as the individual predictors get better. We demonstrate that this phenomenon naturally arises due to a trade-off whereby competition pushes each predictor to specialize in a subset of the population while data purchase has the effect of making predictors more uniform. With comprehensive experiments, we show that our findings are robust against different modeling assumptions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongchan Kwon",
      "Tony A Ginart",
      "James Zou"
    ]
  },
  "https://openreview.net/forum?id=lf0lr4AYM6": {
    "title": "Diffusion Models for Video Prediction and Infilling",
    "volume": "main",
    "abstract": "Predicting and anticipating future outcomes or reasoning about missing information in a sequence are critical skills for agents to be able to make intelligent decisions. This requires strong, temporally coherent generative capabilities. Diffusion models have shown remarkable success in several generative tasks, but have not been extensively explored in the video domain. We present Random-Mask Video Diffusion (RaMViD), which extends image diffusion models to videos using 3D convolutions, and introduces a new conditioning technique during training. By varying the mask we condition on, the model is able to perform video prediction, infilling, and upsampling. Due to our simple conditioning scheme, we can utilize the same architecture as used for unconditional training, which allows us to train the model in a conditional and unconditional fashion at the same time. We evaluate RaMViD on two benchmark datasets for video prediction, on which we achieve state-of-the-art results, and one for video generation. High-resolution videos are provided at https://sites.google.com/view/video-diffusion-prediction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tobias H√∂ppe",
      "Arash Mehrjou",
      "Stefan Bauer",
      "Didrik Nielsen",
      "Andrea Dittadi"
    ]
  },
  "https://openreview.net/forum?id=Au1LNKmRvh": {
    "title": "Efficient Gradient Flows in Sliced-Wasserstein Space",
    "volume": "main",
    "abstract": "Minimizing functionals in the space of probability distributions can be done with Wasser- stein gradient flows. To solve them numerically, a possible approach is to rely on the Jordan‚ÄìKinderlehrer‚ÄìOtto (JKO) scheme which is analogous to the proximal scheme in Euclidean spaces. However, it requires solving a nested optimization problem at each it- eration, and is known for its computational challenges, especially in high dimension. To alleviate it, very recent works propose to approximate the JKO scheme leveraging Brenier's theorem, and using gradients of Input Convex Neural Networks to parameterize the density (JKO-ICNN). However, this method comes with a high computational cost and stability is- sues. Instead, this work proposes to use gradient flows in the space of probability measures endowed with the sliced-Wasserstein (SW) distance. We argue that this method is more flex- ible than JKO-ICNN, since SW enjoys a closed-form differentiable approximation. Thus, the density at each step can be parameterized by any generative model which alleviates the computational burden and makes it tractable in higher dimensions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cl√©ment Bonet",
      "Nicolas Courty",
      "Fran√ßois Septier",
      "Lucas Drumetz"
    ]
  },
  "https://openreview.net/forum?id=Ii7UeHc0mO": {
    "title": "Approximate Policy Iteration with Bisimulation Metrics",
    "volume": "main",
    "abstract": "Bisimulation metrics define a distance measure between states of a Markov decision process (MDP) based on a comparison of reward sequences. Due to this property they provide theoretical guarantees in value function approximation (VFA). In this work we first prove that bisimulation and $\\pi$-bisimulation metrics can be defined via a more general class of Sinkhorn distances, which unifies various state similarity metrics used in recent work. Then we describe an approximate policy iteration (API) procedure that uses a bisimulation-based discretization of the state space for VFA and prove asymptotic performance bounds. Next, we bound the difference between $\\pi$-bisimulation metrics in terms of the change in the policies themselves. Based on these results, we design an API($\\alpha$) procedure that employs conservative policy updates and enjoys better performance bounds than the naive API approach. We discuss how such API procedures map onto practical actor-critic methods that use bisimulation metrics for state representation learning. Lastly, we validate our theoretical results and investigate their practical implications via a controlled empirical analysis based on an implementation of bisimulation-based API for finite MDPs",
    "checked": true,
    "id": "2cc36e4d0b6bf6913ce68f49a80a9c098e3039c7",
    "semantic_title": "approximate policy iteration with bisimulation metrics",
    "citation_count": 8,
    "authors": [
      "Mete Kemertas",
      "Allan Douglas Jepson"
    ]
  },
  "https://openreview.net/forum?id=3v78awEzyB": {
    "title": "Exposing Outlier Exposure: What Can Be Learned From Few, One, and Zero Outlier Images",
    "volume": "main",
    "abstract": "Due to the intractability of characterizing everything that looks unlike the normal data, anomaly detection (AD) is traditionally treated as an unsupervised problem utilizing only normal samples. However, it has recently been found that unsupervised image AD can be drastically improved through the utilization of huge corpora of random images to represent anomalousness; a technique which is known as Outlier Exposure. In this paper we show that specialized AD learning methods seem unnecessary for state-of-the-art performance, and furthermore one can achieve strong performance with just a small collection of Outlier Exposure data, contradicting common assumptions in the field of AD. We find that standard classifiers and semi-supervised one-class methods trained to discern between normal samples and relatively few random natural images are able to outperform the current state of the art on an established AD benchmark with ImageNet. Further experiments reveal that even one well-chosen outlier sample is sufficient to achieve decent performance on this benchmark (79.3% AUC). We investigate this phenomenon and find that one-class methods are more robust to the choice of training outliers, indicating that there are scenarios where these are still more useful than standard classifiers. Additionally, we include experiments that delineate the scenarios where our results hold. Lastly, no training samples are necessary when one uses the representations learned by CLIP, a recent foundation model, which achieves state-of-the-art AD results on CIFAR-10 and ImageNet in a zero-shot setting",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Philipp Liznerski",
      "Lukas Ruff",
      "Robert A. Vandermeulen",
      "Billy Joe Franks",
      "Klaus Robert Muller",
      "Marius Kloft"
    ]
  },
  "https://openreview.net/forum?id=15SoThZmtU": {
    "title": "Mitigating Catastrophic Forgetting in Spiking Neural Networks through Threshold Modulation",
    "volume": "main",
    "abstract": "Artificial Neural Networks (ANNs) trained with Backpropagation and Stochastic Gradient Descent (SGD) suffer from the problem of Catastrophic Forgetting; when learning tasks sequentially, the ANN tends to abruptly forget previous knowledge upon being trained on a new task. On the other hand, biological neural networks do not suffer from this problem. Spiking Neural Networks (SNNs) are a class of Neural Networks that are closer to biological networks than ANNs and their intrinsic properties inspired from biology could alleviate the problem of Catastrophic Forgetting. In this paper, we investigate if the firing threshold mechanism of SNNs can be used to gate the activity of the network in order to reduce catastrophic forgetting. To this end, we evolve a Neuromodulatory Network that adapts the thresholds of an SNN depending on the spiking activity of the previous layer. Our experiments on different datasets show that the neurmodulated SNN can mitigate forgetting significantly with respect to a fixed threshold SNN. We also show that the evolved Neuromodulatory Network can generalize to multiple new scenarios and analyze its behavior",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ilyass Hammouamri",
      "Timoth√©e Masquelier",
      "Dennis George Wilson"
    ]
  },
  "https://openreview.net/forum?id=1AxQpKmiTc": {
    "title": "ZerO Initialization: Initializing Neural Networks with only Zeros and Ones",
    "volume": "main",
    "abstract": "Deep neural networks are usually initialized with random weights, with adequately selected initial variance to ensure stable signal propagation during training. However, selecting the appropriate variance becomes challenging especially as the number of layers grows. In this work, we replace random weight initialization with a fully deterministic initialization scheme, viz., ZerO, which initializes the weights of networks with only zeros and ones (up to a normalization factor), based on identity and Hadamard transforms. Through both theoretical and empirical studies, we demonstrate that ZerO is able to train networks without damaging their expressivity. Applying ZerO on ResNet achieves state-of-the-art performance on various datasets, including ImageNet, which suggests random weights may be unnecessary for network initialization. In addition, ZerO has many benefits, such as training ultra deep networks (without batch-normalization), exhibiting low-rank learning trajectories that result in low-rank and sparse solutions, and improving training reproducibility",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawei Zhao",
      "Florian Tobias Schaefer",
      "Anima Anandkumar"
    ]
  },
  "https://openreview.net/forum?id=Y4mgmw9OgV": {
    "title": "A Rigorous Study Of The Deep Taylor Decomposition",
    "volume": "main",
    "abstract": "Saliency methods attempt to explain deep neural networks by highlighting the most salient features of a sample. Some widely used methods are based on a theoretical framework called Deep Taylor Decomposition (DTD), which formalizes the recursive application of the Taylor Theorem to the network's layers. However, recent work has found these methods to be independent of the network's deeper layers and appear to respond only to lower-level image structure. Here, we investigate DTD theory to better understand this perplexing behavior and found that the Deep Taylor Decomposition is equivalent to the basic gradient$\\times$input method when the Taylor root points (an important parameter of the algorithm chosen by the user) are locally constant. If the root points are locally input-dependent, then one can justify any explanation. In this case, the theory is under-constrained. In an empirical evaluation, we find that DTD roots do not lie the same linear regions as the input -- contrary to a fundamental assumption of the Taylor Theorem. The theoretical foundations of DTD were cited as a source of reliability for the explanations. However, our findings urge caution in making such claims",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leon Sixt",
      "Tim Landgraf"
    ]
  },
  "https://openreview.net/forum?id=e4Bb0b3QgJ": {
    "title": "Fail-Safe Adversarial Generative Imitation Learning",
    "volume": "main",
    "abstract": "For flexible yet safe imitation learning (IL), we propose theory and a modular method, with a safety layer that enables a closed-form probability density/gradient of the safe generative continuous policy, end-to-end generative adversarial training, and worst-case safety guarantees. The safety layer maps all actions into a set of safe actions, and uses the change-of-variables formula plus additivity of measures for the density. The set of safe actions is inferred by first checking safety of a finite sample of actions via adversarial reachability analysis of fallback maneuvers, and then concluding on the safety of these actions' neighborhoods using, e.g., Lipschitz continuity. We provide theoretical analysis showing the robustness advantage of using the safety layer already during training (imitation error linear in the horizon) compared to only using it at test time (up to quadratic error). In an experiment on real-world driver interaction data, we empirically demonstrate tractability, safety and imitation performance of our approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Philipp Geiger",
      "Christoph-Nikolas Straehle"
    ]
  },
  "https://openreview.net/forum?id=MHOAEiTlen": {
    "title": "DHA: End-to-End Joint Optimization of Data Augmentation Policy, Hyper-parameter and Architecture",
    "volume": "main",
    "abstract": "Automated machine learning (AutoML) usually involves several crucial components, such as Data Augmentation (DA) policy, Hyper-Parameter Optimization (HPO), and Neural Architecture Search (NAS). Although many strategies have been developed for automating these components in separation, joint optimization of these components remains challenging due to the largely increased search dimension and the variant input types of each component. In parallel to this, the common practice of searching for the optimal architecture first and then retraining it before deployment in NAS often suffers from the low-performance correlation between the searching and retraining stages. An end-to-end solution that integrates the AutoML components and returns a ready-to-use model at the end of the search is desirable. In view of these, we propose DHA, which achieves joint optimization of Data augmentation policy, Hyper-parameter, and Architecture. Specifically, end-to-end NAS is achieved in a differentiable manner by optimizing a compressed lower-dimensional feature space, while DA policy and HPO are regarded as dynamic schedulers, which adapt themselves to the update of network parameters and network architecture at the same time. Experiments show that DHA achieves state-of-the-art (SOTA) results on various datasets and search spaces. To the best of our knowledge, we are the first to efficiently and jointly optimize DA policy, NAS, and HPO in an end-to-end manner without retraining",
    "checked": true,
    "id": "e62b102e072bb83506f858e15ebd8be5030024be",
    "semantic_title": "dha: end-to-end joint optimization of data augmentation policy, hyper-parameter and architecture",
    "citation_count": 10,
    "authors": [
      "kaichen zhou",
      "Lanqing HONG",
      "Shoukang Hu",
      "Fengwei Zhou",
      "Binxin Ru",
      "Jiashi Feng",
      "Zhenguo Li"
    ]
  },
  "https://openreview.net/forum?id=e7A0B99zJf": {
    "title": "Data Leakage in Federated Averaging",
    "volume": "main",
    "abstract": "Recent attacks have shown that user data can be recovered from FedSGD updates, thus breaking privacy. However, these attacks are of limited practical relevance as federated learning typically uses the FedAvg algorithm. Compared to FedSGD, recovering data from FedAvg updates is much harder as: (i) the updates are computed at unobserved intermediate network weights, (ii) a large number of batches are used, and (iii) labels and network weights vary simultaneously across client steps. In this work, we propose a new optimization-based attack which successfully attacks FedAvg by addressing the above challenges. First, we solve the optimization problem using automatic differentiation that forces a simulation of the client's update that generates the unobserved parameters for the recovered labels and inputs to match the received client update. Second, we address the large number of batches by relating images from different epochs with a permutation invariant prior. Third, we recover the labels by estimating the parameters of existing FedSGD attacks at every FedAvg step. On the popular FEMNIST dataset, we demonstrate that on average we successfully recover >45% of the client's images from realistic FedAvg updates computed on 10 local epochs of 10 batches each with 5 images, compared to only <10% using the baseline. Our findings show many real-world federated learning implementations based on FedAvg are vulnerable",
    "checked": true,
    "id": "4fe85cbbde2185d8515fd9f396d97fe475843e80",
    "semantic_title": "data leakage in federated averaging",
    "citation_count": 32,
    "authors": [
      "Dimitar Iliev Dimitrov",
      "Mislav Balunovic",
      "Nikola Konstantinov",
      "Martin Vechev"
    ]
  },
  "https://openreview.net/forum?id=lE7K4n1Esk": {
    "title": "On the Adversarial Robustness of Vision Transformers",
    "volume": "main",
    "abstract": "Following the success in advancing natural language processing and understanding, transformers are expected to bring revolutionary changes to computer vision. This work provides a comprehensive study on the robustness of vision transformers (ViTs) against adversarial perturbations. Tested on various white-box and transfer attack settings, we find that ViTs possess better adversarial robustness when compared with MLP-Mixer and convolutional neural networks (CNNs) including ConvNeXt, and this observation also holds for certified robustness. Through frequency analysis and feature visualization, we summarize the following main observations contributing to the improved robustness of ViTs: 1) Features learned by ViTs contain less high-frequency patterns that have spurious correlation, which helps explain why ViTs are less sensitive to high-frequency perturbations than CNNs and MLP-Mixer, and there is a high correlation between how much the model learns high-frequency features and its robustness against different frequency-based perturbations. 2) Introducing convolutional or tokens-to-token blocks for learning high-frequency features in ViTs can improve classification accuracy but at the cost of adversarial robustness. 3) Modern CNN designs that borrow techniques from ViTs including activation function, layer norm, larger kernel size to imitate the global attention, and patchify the images as inputs, etc., could help bridge the performance gap between ViTs and CNNs not only in terms of performance, but also certified and empirical adversarial robustness. Moreover, we show adversarial training is also applicable to ViT for training robust models, and sharpness-aware minimization can also help improve robustness, while pre-training with clean images on larger datasets does not significantly improve adversarial robustness",
    "checked": true,
    "id": "0def290ae38abb4a04e35e0bcdc86b71d237f494",
    "semantic_title": "on the adversarial robustness of vision transformers",
    "citation_count": 146,
    "authors": [
      "Rulin Shao",
      "Zhouxing Shi",
      "Jinfeng Yi",
      "Pin-Yu Chen",
      "Cho-Jui Hsieh"
    ]
  },
  "https://openreview.net/forum?id=7iSYW1FRWA": {
    "title": "Behind the Machine's Gaze: Neural Networks with Biologically-inspired Constraints Exhibit Human-like Visual Attention",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "3b5361986d79be91e6d490e09fed49d9ee5abab2",
    "semantic_title": "behind the machine's gaze: neural networks with biologically-inspired constraints exhibit human-like visual attention",
    "citation_count": 7,
    "authors": [
      "Leo Schwinn",
      "Doina Precup",
      "Bjoern Eskofier",
      "Dario Zanca"
    ]
  },
  "https://openreview.net/forum?id=cxp7n9q5c4": {
    "title": "Structured Uncertainty in the Observation Space of Variational Autoencoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "James Langley",
      "Miguel Monteiro",
      "Charles Jones",
      "Nick Pawlowski",
      "Ben Glocker"
    ]
  },
  "https://openreview.net/forum?id=CExeD0jpB6": {
    "title": "Distributed Stochastic Algorithms for High-rate Streaming Principal Component Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haroon Raja",
      "Waheed Bajwa"
    ]
  },
  "https://openreview.net/forum?id=9NjqD9i48M": {
    "title": "Benchmarking Progress to Infant-Level Physical Reasoning in AI",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "414a476f83634e3b452b243ed7460c9ef3d1aaa4",
    "semantic_title": "benchmarking progress to infant-level physical reasoning in ai",
    "citation_count": 16,
    "authors": [
      "Luca Weihs",
      "Amanda Yuile",
      "Ren√©e Baillargeon",
      "Cynthia Fisher",
      "Gary Marcus",
      "Roozbeh Mottaghi",
      "Aniruddha Kembhavi"
    ]
  },
  "https://openreview.net/forum?id=Hp4g7FAXXG": {
    "title": "Linear algebra with transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francois Charton"
    ]
  },
  "https://openreview.net/forum?id=aIoEkwc2oB": {
    "title": "INR-V: A Continuous Representation Space for Video-based Generative Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bipasha Sen",
      "Aditya Agarwal",
      "Vinay P Namboodiri",
      "C.V. Jawahar"
    ]
  },
  "https://openreview.net/forum?id=ZPQhzTSWA7": {
    "title": "A Simple Convergence Proof of Adam and Adagrad",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexandre D√©fossez",
      "Leon Bottou",
      "Francis Bach",
      "Nicolas Usunier"
    ]
  },
  "https://openreview.net/forum?id=atJHLVyBi8": {
    "title": "On the Paradox of Certified Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikola Jovanoviƒá",
      "Mislav Balunovic",
      "Maximilian Baader",
      "Martin Vechev"
    ]
  },
  "https://openreview.net/forum?id=JXCH5N4Ujy": {
    "title": "Time Series Alignment with Global Invariances",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Titouan Vayer",
      "Romain Tavenard",
      "Laetitia Chapel",
      "R√©mi Flamary",
      "Nicolas Courty",
      "Yann Soullard"
    ]
  },
  "https://openreview.net/forum?id=jIrOeWjdpc": {
    "title": "Explicit Group Sparse Projection with Applications to Deep Learning and NMF",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Riyasat Ohib",
      "Nicolas Gillis",
      "Niccolo Dalmasso",
      "Sameena Shah",
      "Vamsi K. Potluru",
      "Sergey Plis"
    ]
  },
  "https://openreview.net/forum?id=jjtFD8A1Wx": {
    "title": "Reasonable Effectiveness of Random Weighting: A Litmus Test for Multi-Task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baijiong Lin",
      "Feiyang Ye",
      "Yu Zhang",
      "Ivor Tsang"
    ]
  },
  "https://openreview.net/forum?id=lCPOHiztuw": {
    "title": "Direct Molecular Conformation Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinhua Zhu",
      "Yingce Xia",
      "Chang Liu",
      "Lijun Wu",
      "Shufang Xie",
      "Yusong Wang",
      "Tong Wang",
      "Tao Qin",
      "Wengang Zhou",
      "Houqiang Li",
      "Haiguang Liu",
      "Tie-Yan Liu"
    ]
  },
  "https://openreview.net/forum?id=LTiaPxqe2e": {
    "title": "Symbolic Regression is NP-hard",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marco Virgolin",
      "Solon P Pissis"
    ]
  },
  "https://openreview.net/forum?id=e5ILb2Nqst": {
    "title": "Differentially Private Stochastic Expectation Propagation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Margarita Vinaroz",
      "Mijung Park"
    ]
  },
  "https://openreview.net/forum?id=4FU8Jz1Oyj": {
    "title": "On Noise Abduction for Answering Counterfactual Queries: A Practical Outlook",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saptarshi Saha",
      "Utpal Garain"
    ]
  },
  "https://openreview.net/forum?id=VBHuLfnOMf": {
    "title": "Failure Detection in Medical Image Classification: A Reality Check and Benchmarking Testbed",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "M√©lanie Bernhardt",
      "Fabio De Sousa Ribeiro",
      "Ben Glocker"
    ]
  },
  "https://openreview.net/forum?id=XX8CEN815d": {
    "title": "Bridging Offline and Online Experimentation: Constraint Active Search for Deployed Performance Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junpei Komiyama",
      "Gustavo Malkomes",
      "Bolong Cheng",
      "Michael McCourt"
    ]
  },
  "https://openreview.net/forum?id=CrimIjBa64": {
    "title": "Multi-Source Causal Inference Using Control Variates under Outcome Selection Bias",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenshuo Guo",
      "Serena Lutong Wang",
      "Peng Ding",
      "Yixin Wang",
      "Michael Jordan"
    ]
  },
  "https://openreview.net/forum?id=vd0onGWZbE": {
    "title": "Identifiable Deep Generative Models via Sparse Decoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gemma Elyse Moran",
      "Dhanya Sridhar",
      "Yixin Wang",
      "David Blei"
    ]
  },
  "https://openreview.net/forum?id=QFJ3gtbwHR": {
    "title": "Using unsupervised learning to detect broken symmetries, with relevance to searches for parity violation in nature",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christopher Gorham Lester"
    ]
  },
  "https://openreview.net/forum?id=Kb1lb0vSLa": {
    "title": "Integrating Rankings into Quantized Scores in Peer Review",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yusha Liu",
      "Yichong Xu",
      "Nihar B Shah",
      "Aarti Singh"
    ]
  },
  "https://openreview.net/forum?id=CfzIsWWBlo": {
    "title": "Towards Accurate Subgraph Similarity Computation via Neural Graph Pruning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linfeng Liu",
      "XU HAN",
      "Dawei Zhou",
      "Liping Liu"
    ]
  },
  "https://openreview.net/forum?id=Ox5tmhFBrc": {
    "title": "Learning Two-Step Hybrid Policy for Graph-Based Interpretable Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tongzhou Mu",
      "Kaixiang Lin",
      "Feiyang Niu",
      "Govind Thattai"
    ]
  },
  "https://openreview.net/forum?id=UDmH3HxxPp": {
    "title": "Secure Domain Adaptation with Multiple Sources",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Serban Stan",
      "Mohammad Rostami"
    ]
  },
  "https://openreview.net/forum?id=8s8K2UZGTZ": {
    "title": "Teaching Models to Express Their Uncertainty in Words",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stephanie Lin",
      "Jacob Hilton",
      "Owain Evans"
    ]
  },
  "https://openreview.net/forum?id=Qs3EfpieOh": {
    "title": "The Evolution of Out-of-Distribution Robustness Throughout Fine-Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anders Johan Andreassen",
      "Yasaman Bahri",
      "Behnam Neyshabur",
      "Rebecca Roelofs"
    ]
  },
  "https://openreview.net/forum?id=JBuCfkmKYu": {
    "title": "Simplifying Node Classification on Heterophilous Graphs with Compatible Label Propagation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiqiang Zhong",
      "Sergei Ivanov",
      "Jun Pang"
    ]
  },
  "https://openreview.net/forum?id=7gzQltQSwr": {
    "title": "Centroids Matching: an efficient Continual Learning approach operating in the embedding space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jary Pomponi",
      "Simone Scardapane",
      "Aurelio Uncini"
    ]
  },
  "https://openreview.net/forum?id=nS8A9nOrqp": {
    "title": "Nonstationary Reinforcement Learning with Linear Function Approximation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huozhi Zhou",
      "Jinglin Chen",
      "Lav R. Varshney",
      "Ashish Jagmohan"
    ]
  },
  "https://openreview.net/forum?id=j2Mid5hFUJ": {
    "title": "Enhanced gradient-based MCMC in discrete spaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Rhodes",
      "Michael U. Gutmann"
    ]
  },
  "https://openreview.net/forum?id=w3x20YEcQK": {
    "title": "Flipped Classroom: Effective Teaching for Time Series Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Philipp Teutsch",
      "Patrick M√§der"
    ]
  },
  "https://openreview.net/forum?id=mbwm7NdkpO": {
    "title": "Deep Policies for Online Bipartite Matching: A Reinforcement Learning Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Ali Alomrani",
      "Reza Moravej",
      "Elias Boutros Khalil"
    ]
  },
  "https://openreview.net/forum?id=X1VzbBU6xZ": {
    "title": "Generative Adversarial Neural Operators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md Ashiqur Rahman",
      "Manuel A Florez",
      "Anima Anandkumar",
      "Zachary E Ross",
      "Kamyar Azizzadenesheli"
    ]
  },
  "https://openreview.net/forum?id=mW6nD3567x": {
    "title": "From Optimization Dynamics to Generalization Bounds via ≈Åojasiewicz Gradient Inequality",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fusheng Liu",
      "Haizhao Yang",
      "Soufiane Hayou",
      "Qianxiao Li"
    ]
  },
  "https://openreview.net/forum?id=1l0sClLiPc": {
    "title": "Unimodal Likelihood Models for Ordinal Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryoya Yamasaki"
    ]
  },
  "https://openreview.net/forum?id=DijnKziche": {
    "title": "Differentiable Model Compression via Pseudo Quantization Noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexandre D√©fossez",
      "Yossi Adi",
      "Gabriel Synnaeve"
    ]
  },
  "https://openreview.net/forum?id=Zxm0kNe3u7": {
    "title": "Mace: A flexible framework for membership privacy estimation in generative models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixi Xu",
      "Sumit Mukherjee",
      "Xiyang Liu",
      "Shruti Tople",
      "Rahul M Dodhia",
      "Juan M Lavista Ferres"
    ]
  },
  "https://openreview.net/forum?id=Jj0qSbtwdb": {
    "title": "Fingerprints of Super Resolution Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeremy Vonderfecht",
      "Feng Liu"
    ]
  },
  "https://openreview.net/forum?id=rrMK6hYNSx": {
    "title": "Online Double Oracle",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Le Cong Dinh",
      "Stephen Marcus McAleer",
      "Zheng Tian",
      "Nicolas Perez-Nieves",
      "Oliver Slumbers",
      "David Henry Mguni",
      "Jun Wang",
      "Haitham Bou Ammar",
      "Yaodong Yang"
    ]
  },
  "https://openreview.net/forum?id=nmFczdJtc2": {
    "title": "Attribute Prediction as Multiple Instance Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Diego Marcos",
      "Aike Potze",
      "Wenjia Xu",
      "Devis Tuia",
      "Zeynep Akata"
    ]
  },
  "https://openreview.net/forum?id=4N6T6Rop6k": {
    "title": "Completeness and Coherence Learning for Fast Arbitrary Style Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhijie Wu",
      "Chunjin Song",
      "Guanxiong Chen",
      "Sheng Guo",
      "Weilin Huang"
    ]
  },
  "https://openreview.net/forum?id=gvSHaaD2wQ": {
    "title": "sigmoidF1: A Smooth F1 Score Surrogate Loss for Multilabel Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gabriel B√©n√©dict",
      "Hendrik Vincent Koops",
      "Daan Odijk",
      "Maarten de Rijke"
    ]
  },
  "https://openreview.net/forum?id=XsPopigZXV": {
    "title": "FLEA: Provably Robust Fair Multisource Learning from Unreliable Training Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eugenia Iofinova",
      "Nikola Konstantinov",
      "Christoph H Lampert"
    ]
  },
  "https://openreview.net/forum?id=u8tvSxm4Bs": {
    "title": "GemNet-OC: Developing Graph Neural Networks for Large and Diverse Molecular Simulation Datasets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Johannes Gasteiger",
      "Muhammed Shuaibi",
      "Anuroop Sriram",
      "Stephan G√ºnnemann",
      "Zachary Ward Ulissi",
      "C. Lawrence Zitnick",
      "Abhishek Das"
    ]
  },
  "https://openreview.net/forum?id=tqDhrbKJLS": {
    "title": "MixTailor: Mixed Gradient Aggregation for Robust Learning Against Tailored Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali Ramezani-Kebrya",
      "Iman Tabrizian",
      "Fartash Faghri",
      "Petar Popovski"
    ]
  },
  "https://openreview.net/forum?id=S8eABAy8P3": {
    "title": "LIMIS: Locally Interpretable Modeling using Instance-wise Subsampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinsung Yoon",
      "Sercan O Arik",
      "Tomas Pfister"
    ]
  },
  "https://openreview.net/forum?id=Lgs5pQ1v30": {
    "title": "FedShuffle: Recipes for Better Use of Local Work in Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuel Horv√°th",
      "Maziar Sanjabi",
      "Lin Xiao",
      "Peter Richt√°rik",
      "Michael Rabbat"
    ]
  },
  "https://openreview.net/forum?id=OslAMMF4ZP": {
    "title": "Faking Interpolation Until You Make It",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alasdair Paren",
      "Rudra P. K. Poudel",
      "M. Pawan Kumar"
    ]
  },
  "https://openreview.net/forum?id=lIOQFVncY9": {
    "title": "Ensembles of Classifiers: a Bias-Variance Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Neha Gupta",
      "Jamie Smith",
      "Ben Adlam",
      "Zelda E Mariet"
    ]
  },
  "https://openreview.net/forum?id=LSAAlS7Yts": {
    "title": "GFNet: Geometric Flow Network for 3D Point Cloud Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haibo Qiu",
      "Baosheng Yu",
      "Dacheng Tao"
    ]
  },
  "https://openreview.net/forum?id=tPMQ6Je2rB": {
    "title": "Deep Learning for Bayesian Optimization of Scientific Problems with High-Dimensional Structure",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuel Kim",
      "Peter Y Lu",
      "Charlotte Loh",
      "Jamie Smith",
      "Jasper Snoek",
      "Marin Soljacic"
    ]
  },
  "https://openreview.net/forum?id=6qMKztPn0n": {
    "title": "Evolving Decomposed Plasticity Rules for Information-Bottlenecked Meta-Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan Wang",
      "Hao Tian",
      "Haoyi Xiong",
      "Hua Wu",
      "Jie Fu",
      "Yang Cao",
      "Yu Kang",
      "Haifeng Wang"
    ]
  },
  "https://openreview.net/forum?id=zwRX9kkKzj": {
    "title": "Multitask Online Mirror Descent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicol√≤ Cesa-Bianchi",
      "Pierre Laforgue",
      "Andrea Paudice",
      "Massimiliano Pontil"
    ]
  },
  "https://openreview.net/forum?id=Ig82l87ZVU": {
    "title": "Approximating 1-Wasserstein Distance with Trees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Makoto Yamada",
      "Yuki Takezawa",
      "Ryoma Sato",
      "Han Bao",
      "Zornitsa Kozareva",
      "Sujith Ravi"
    ]
  },
  "https://openreview.net/forum?id=u0n1chY0b6": {
    "title": "Learning Accurate Decision Trees with Bandit Feedback via Quantized Gradient Descent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ajaykrishna Karthikeyan",
      "Naman Jain",
      "Nagarajan Natarajan",
      "Prateek Jain"
    ]
  },
  "https://openreview.net/forum?id=AEoYjvjKVA": {
    "title": "Probabilistic Autoencoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vanessa M Boehm",
      "Uros Seljak"
    ]
  },
  "https://openreview.net/forum?id=D3WI0QG7dC": {
    "title": "Decoder Denoising Pretraining for Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emmanuel Asiedu Brempong",
      "Simon Kornblith",
      "Ting Chen",
      "Niki Parmar",
      "Matthias Minderer",
      "Mohammad Norouzi"
    ]
  },
  "https://openreview.net/forum?id=JL6MU9XFzW": {
    "title": "Can You Win Everything with A Lottery Ticket?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianlong Chen",
      "Zhenyu Zhang",
      "Jun Wu",
      "Randy Huang",
      "Sijia Liu",
      "Shiyu Chang",
      "Zhangyang Wang"
    ]
  },
  "https://openreview.net/forum?id=gCmQK6McbR": {
    "title": "HEAT: Hyperedge Attention Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dobrik Georgiev Georgiev",
      "Marc Brockschmidt",
      "Miltiadis Allamanis"
    ]
  },
  "https://openreview.net/forum?id=t5HkgbxZp1": {
    "title": "On the Near-Optimality of Local Policies in Large Cooperative Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Washim Uddin Mondal",
      "Vaneet Aggarwal",
      "Satish Ukkusuri"
    ]
  },
  "https://openreview.net/forum?id=n3qLz4eL1l": {
    "title": "Exploring Efficient Few-shot Adaptation for Vision Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengming Xu",
      "Siqian Yang",
      "Yabiao Wang",
      "Zhanxiong Wang",
      "Yanwei Fu",
      "Xiangyang Xue"
    ]
  },
  "https://openreview.net/forum?id=w3z3sN1b04": {
    "title": "Weight Expansion: A New Perspective on Dropout and Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gaojie Jin",
      "Xinping Yi",
      "Pengfei Yang",
      "Lijun Zhang",
      "Sven Schewe",
      "Xiaowei Huang"
    ]
  },
  "https://openreview.net/forum?id=HjelcW6wio": {
    "title": "Exploring the Learning Mechanisms of Neural Division Modules",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bhumika Mistry",
      "Katayoun Farrahi",
      "Jonathon Hare"
    ]
  },
  "https://openreview.net/forum?id=U8uJAUMzj9": {
    "title": "Domain Invariant Adversarial Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matan Levi",
      "Idan Attias",
      "Aryeh Kontorovich"
    ]
  },
  "https://openreview.net/forum?id=Su290sknyQ": {
    "title": "Momentum Capsule Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Josef Gugglberger",
      "Antonio Rodriguez-sanchez",
      "David Peer"
    ]
  },
  "https://openreview.net/forum?id=7j0GI6tPYi": {
    "title": "ANCER: Anisotropic Certification via Sample-wise Volume Maximization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francisco Eiras",
      "Motasem Alfarra",
      "Philip Torr",
      "M. Pawan Kumar",
      "Puneet K. Dokania",
      "Bernard Ghanem",
      "Adel Bibi"
    ]
  },
  "https://openreview.net/forum?id=caRBFhxXIG": {
    "title": "On the Choice of Interpolation Scheme for Neural CDEs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "James Morrill",
      "Patrick Kidger",
      "Lingyi Yang",
      "Terry Lyons"
    ]
  },
  "https://openreview.net/forum?id=8QoxXTDcsH": {
    "title": "Conformal Prediction Intervals with Temporal Dependence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Lin",
      "Shubhendu Trivedi",
      "Jimeng Sun"
    ]
  },
  "https://openreview.net/forum?id=Cct7kqbHK6": {
    "title": "Meta-Learning Sparse Compression Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonathan Schwarz",
      "Yee Whye Teh"
    ]
  },
  "https://openreview.net/forum?id=q1Fey9feu7": {
    "title": "Estimating Potential Outcome Distributions with Collaborating Causal Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianhui Zhou",
      "William E Carson IV",
      "David Carlson"
    ]
  },
  "https://openreview.net/forum?id=4GuIi1jJ74": {
    "title": "Sparse Coding with Multi-layer Decoders using Variance Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Katrina Evtimova",
      "Yann LeCun"
    ]
  },
  "https://openreview.net/forum?id=LnjclqBl8R": {
    "title": "Efficient CDF Approximations for Normalizing Flows",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chandramouli Shama Sastry",
      "Andreas Lehrmann",
      "Marcus A Brubaker",
      "Alexander Radovic"
    ]
  },
  "https://openreview.net/forum?id=jKN1pXi7b0": {
    "title": "Unsupervised Dense Information Retrieval with Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gautier Izacard",
      "Mathilde Caron",
      "Lucas Hosseini",
      "Sebastian Riedel",
      "Piotr Bojanowski",
      "Armand Joulin",
      "Edouard Grave"
    ]
  },
  "https://openreview.net/forum?id=e7mYYMSyZH": {
    "title": "On the Convergence of Shallow Neural Network Training with Randomly Masked Neurons",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangshuo Liao",
      "Anastasios Kyrillidis"
    ]
  },
  "https://openreview.net/forum?id=Ee277P3AYC": {
    "title": "CoCa: Contrastive Captioners are Image-Text Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahui Yu",
      "Zirui Wang",
      "Vijay Vasudevan",
      "Legg Yeung",
      "Mojtaba Seyedhosseini",
      "Yonghui Wu"
    ]
  },
  "https://openreview.net/forum?id=TWSTyYd2Rl": {
    "title": "Attentive Walk-Aggregating Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mehmet F Demirel",
      "Shengchao Liu",
      "Siddhant Garg",
      "Zhenmei Shi",
      "Yingyu Liang"
    ]
  },
  "https://openreview.net/forum?id=p5V8P2J61u": {
    "title": "Birds of a Feather Trust Together: Knowing When to Trust a Classifier via Adaptive Neighborhood Aggregation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miao Xiong",
      "Shen Li",
      "Wenjie Feng",
      "Ailin Deng",
      "Jihai Zhang",
      "Bryan Hooi"
    ]
  },
  "https://openreview.net/forum?id=8GvRCWKHIL": {
    "title": "Optimal Client Sampling for Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenlin Chen",
      "Samuel Horv√°th",
      "Peter Richt√°rik"
    ]
  },
  "https://openreview.net/forum?id=VcXNAr5Rur": {
    "title": "DR-DSGD: A Distributionally Robust Decentralized Learning Algorithm over Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaouki Ben Issaid",
      "Anis Elgabli",
      "Mehdi Bennis"
    ]
  },
  "https://openreview.net/forum?id=qzM1Tw5i7N": {
    "title": "SemiNLL: A Framework of Noisy-Label Learning by Semi-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "ZHUOWEI WANG",
      "Jing Jiang",
      "Bo Han",
      "Lei Feng",
      "Bo An",
      "Gang Niu",
      "Guodong Long"
    ]
  },
  "https://openreview.net/forum?id=qYNfwFCX9a": {
    "title": "SFP: State-free Priors for Exploration in Off-Policy Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marco Bagatella",
      "Sammy Joe Christen",
      "Otmar Hilliges"
    ]
  },
  "https://openreview.net/forum?id=h1zuM6cXpH": {
    "title": "Zero-Shot Learning with Common Sense Knowledge Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nihal V. Nayak",
      "Stephen Bach"
    ]
  },
  "https://openreview.net/forum?id=Q54jBjc896": {
    "title": "Causal Feature Selection via Orthogonal Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashkan Soleymani",
      "Anant Raj",
      "Stefan Bauer",
      "Bernhard Sch√∂lkopf",
      "Michel Besserve"
    ]
  },
  "https://openreview.net/forum?id=urfWb7VjmL": {
    "title": "High Fidelity Visualization of What Your Self-Supervised Representation Knows About",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Florian Bordes",
      "Randall Balestriero",
      "Pascal Vincent"
    ]
  },
  "https://openreview.net/forum?id=fsacLLU35V": {
    "title": "The Fundamental Limits of Neural Networks for Interval Certified Robustness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthew B Mirman",
      "Maximilian Baader",
      "Martin Vechev"
    ]
  },
  "https://openreview.net/forum?id=M8D5iZsnrO": {
    "title": "TITRATED: Learned Human Driving Behavior without Infractions via Amortized Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vasileios Lioutas",
      "Adam Scibior",
      "Frank Wood"
    ]
  },
  "https://openreview.net/forum?id=AiOUi3440V": {
    "title": "No More Pesky Hyperparameters: Offline Hyperparameter Tuning for RL",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Wang",
      "Archit Sakhadeo",
      "Adam M White",
      "James M Bell",
      "Vincent Liu",
      "Xutong Zhao",
      "Puer Liu",
      "Tadashi Kozuno",
      "Alona Fyshe",
      "Martha White"
    ]
  },
  "https://openreview.net/forum?id=BDqzLH1gEm": {
    "title": "Mean-Field Langevin Dynamics : Exponential Convergence and Annealing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "L√©na√Øc Chizat"
    ]
  },
  "https://openreview.net/forum?id=fudOtITMIZ": {
    "title": "Variational Disentanglement for Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufei Wang",
      "Haoliang Li",
      "Hao Cheng",
      "Bihan Wen",
      "Lap-Pui Chau",
      "Alex Kot"
    ]
  },
  "https://openreview.net/forum?id=fXorxxbDvO": {
    "title": "On Robustness to Missing Video for Audiovisual Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oscar Chang",
      "Otavio Braga",
      "Hank Liao",
      "Dmitriy Serdyuk",
      "Olivier Siohan"
    ]
  },
  "https://openreview.net/forum?id=X2BodlyLvT": {
    "title": "Identifying Causal Structure in Dynamical Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dominik Baumann",
      "Friedrich Solowjow",
      "Karl Henrik Johansson",
      "Sebastian Trimpe"
    ]
  },
  "https://openreview.net/forum?id=sNxNi54B8b": {
    "title": "Do ReLU Networks Have An Edge When Approximating Compactly-Supported Functions?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anastasis Kratsios",
      "Behnoosh Zamanlooy"
    ]
  },
  "https://openreview.net/forum?id=dkHfV3wB2l": {
    "title": "Recurrent networks, hidden states and beliefs in partially observable environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gaspard Lambrechts",
      "Adrien Bolland",
      "Damien Ernst"
    ]
  },
  "https://openreview.net/forum?id=b3v1UrtF6G": {
    "title": "Self-supervise, Refine, Repeat: Improving Unsupervised Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinsung Yoon",
      "Kihyuk Sohn",
      "Chun-Liang Li",
      "Sercan O Arik",
      "Chen-Yu Lee",
      "Tomas Pfister"
    ]
  },
  "https://openreview.net/forum?id=0xENE7HiYm": {
    "title": "Domain-invariant Feature Exploration for Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wang Lu",
      "Jindong Wang",
      "Haoliang Li",
      "Yiqiang Chen",
      "Xing Xie"
    ]
  },
  "https://openreview.net/forum?id=e3S0Bl2RO8": {
    "title": "Stable and Interpretable Unrolled Dictionary Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bahareh Tolooshams",
      "Demba E. Ba"
    ]
  },
  "https://openreview.net/forum?id=NPfS5N3jbL": {
    "title": "Exploring Generative Neural Temporal Point Process",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haitao Lin",
      "Lirong Wu",
      "Guojiang Zhao",
      "Liu Pai",
      "Stan Z. Li"
    ]
  },
  "https://openreview.net/forum?id=LJohl5DnZf": {
    "title": "Improving the Trainability of Deep Neural Networks through Layerwise Batch-Entropy Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Peer",
      "Bart Keulen",
      "Sebastian Stabinger",
      "Justus Piater",
      "Antonio Rodriguez-sanchez"
    ]
  },
  "https://openreview.net/forum?id=lAv8fShACA": {
    "title": "Online Coresets for Parameteric and Non-Parametric Bregman Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Supratim Shit",
      "Anirban Dasgupta",
      "Rachit Chhaya",
      "Jayesh Choudhari"
    ]
  },
  "https://openreview.net/forum?id=bMar2OkxVu": {
    "title": "Max-Affine Spline Insights Into Deep Network Pruning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoran You",
      "Randall Balestriero",
      "Zhihan Lu",
      "Yutong Kou",
      "Huihong Shi",
      "Shunyao Zhang",
      "Shang Wu",
      "Yingyan Lin",
      "Richard Baraniuk"
    ]
  },
  "https://openreview.net/forum?id=NL2L3XjVFx": {
    "title": "Did I do that? Blame as a means to identify controlled effects in reinforcement learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oriol Corcoll",
      "Youssef Sherif Mansour Mohamed",
      "Raul Vicente"
    ]
  },
  "https://openreview.net/forum?id=xwWsiFmUEs": {
    "title": "QuaRL: Quantization for Fast and Environmentally Sustainable Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Srivatsan Krishnan",
      "Max Lam",
      "Sharad Chitlangia",
      "Zishen Wan",
      "Gabriel Barth-maron",
      "Aleksandra Faust",
      "Vijay Janapa Reddi"
    ]
  },
  "https://openreview.net/forum?id=ggPhsYCsm9": {
    "title": "NeSF: Neural Semantic Fields for Generalizable Semantic Segmentation of 3D Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suhani Vora",
      "Noha Radwan",
      "Klaus Greff",
      "Henning Meyer",
      "Kyle Genova",
      "Mehdi S. M. Sajjadi",
      "Etienne Pot",
      "Andrea Tagliasacchi",
      "Daniel Duckworth"
    ]
  },
  "https://openreview.net/forum?id=NT9zgedd3I": {
    "title": "Learning to Switch Among Agents in a Team via 2-Layer Markov Decision Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vahid Balazadeh",
      "Abir De",
      "Adish Singla",
      "Manuel Gomez Rodriguez"
    ]
  },
  "https://openreview.net/forum?id=ILPFasEaHA": {
    "title": "A Self-Supervised Framework for Function Learning and Extrapolation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simon Segert",
      "Jonathan Cohen"
    ]
  },
  "https://openreview.net/forum?id=2EOVIvRXlv": {
    "title": "Ranking Recovery under Privacy Considerations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minoh Jeong",
      "Alex Dytso",
      "Martina Cardone"
    ]
  },
  "https://openreview.net/forum?id=tLIBAEYjcv": {
    "title": "Learning the Transformer Kernel",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sankalan Pal Chowdhury",
      "Adamos Solomou",
      "Kumar Avinava Dubey",
      "Mrinmaya Sachan"
    ]
  },
  "https://openreview.net/forum?id=dpOYN7o8Jm": {
    "title": "Optimizing Functionals on the Space of Probabilities with Input Convex Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Alvarez-Melis",
      "Yair Schiff",
      "Youssef Mroueh"
    ]
  },
  "https://openreview.net/forum?id=yVkpxs77cD": {
    "title": "Deformation Robust Roto-Scale-Translation Equivariant CNNs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liyao Gao",
      "Guang Lin",
      "Wei Zhu"
    ]
  },
  "https://openreview.net/forum?id=2VEUIq9Yff": {
    "title": "Adversarial Feature Augmentation and Normalization for Visual Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianlong Chen",
      "Yu Cheng",
      "Zhe Gan",
      "Jianfeng Wang",
      "Lijuan Wang",
      "Jingjing Liu",
      "Zhangyang Wang"
    ]
  },
  "https://openreview.net/forum?id=aRsLetumx1": {
    "title": "How Expressive are Transformers in Spectral Domain for Graphs?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anson Bastos",
      "Abhishek Nadgeri",
      "Kuldeep Singh",
      "Hiroki Kanezashi",
      "Toyotaro Suzumura",
      "Isaiah Onando Mulang'"
    ]
  },
  "https://openreview.net/forum?id=ak6Bds2DcI": {
    "title": "Robust and Data-efficient Q-learning by Composite Value-estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gabriel Kalweit",
      "Maria Kalweit",
      "Joschka Boedecker"
    ]
  },
  "https://openreview.net/forum?id=xyt4wfdo4J": {
    "title": "Iterative State Estimation in Non-linear Dynamical Systems Using Approximate Expectation Propagation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanket Kamthe",
      "So Takao",
      "Shakir Mohamed",
      "Marc Peter Deisenroth"
    ]
  },
  "https://openreview.net/forum?id=SEUGkraMPi": {
    "title": "The Graph Cut Kernel for Ranked Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michelangelo Conserva",
      "Marc Peter Deisenroth",
      "K S Sesh Kumar"
    ]
  },
  "https://openreview.net/forum?id=zlQXV7xtZs": {
    "title": "NoiLin: Improving adversarial training and correcting stereotype of noisy labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingfeng Zhang",
      "Xilie Xu",
      "Bo Han",
      "Tongliang Liu",
      "Lizhen Cui",
      "Gang Niu",
      "Masashi Sugiyama"
    ]
  },
  "https://openreview.net/forum?id=tnPjQpYk7D": {
    "title": "Multi-Agent Off-Policy TDC with Near-Optimal Sample and Communication Complexities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyi Chen",
      "Yi Zhou",
      "Rong-Rong Chen"
    ]
  },
  "https://openreview.net/forum?id=GvF9ktXI1V": {
    "title": "Benchmarking and Analyzing Unsupervised Network Representation Learning and the Illusion of Progress",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saket Gurukar",
      "Priyesh Vijayan",
      "srinivasan parthasarathy",
      "Balaraman Ravindran",
      "Aakash Srinivasan",
      "Goonmeet Bajaj",
      "Chen Cai",
      "Moniba Keymanesh",
      "Saravana Kumar",
      "Pranav Maneriker",
      "Anasua Mitra",
      "Vedang Patel"
    ]
  },
  "https://openreview.net/forum?id=ZPBJPGX3Bz": {
    "title": "Decoding EEG With Spiking Neural Networks on Neuromorphic Hardware",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Neelesh Kumar",
      "Guangzhi Tang",
      "Raymond Yoo",
      "Konstantinos P. Michmizos"
    ]
  },
  "https://openreview.net/forum?id=8HuyXvbvqX": {
    "title": "Understanding Linearity of Cross-Lingual Word Embedding Mappings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xutan Peng",
      "Mark Stevenson",
      "Chenghua Lin",
      "Chen Li"
    ]
  },
  "https://openreview.net/forum?id=Euf7KofunK": {
    "title": "Clustering units in neural networks: upstream vs downstream information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Richard D Lange",
      "David Rolnick",
      "Konrad Kording"
    ]
  },
  "https://openreview.net/forum?id=0ZbPmmB61g": {
    "title": "Boosting Search Engines with Interactive Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leonard Adolphs",
      "Benjamin B√∂rschinger",
      "Christian Buck",
      "Michelle Chen Huebscher",
      "Massimiliano Ciaramita",
      "Lasse Espeholt",
      "Thomas Hofmann",
      "Yannic Kilcher",
      "Sascha Rothe",
      "Pier Giuseppe Sessa",
      "Lierni Sestorain"
    ]
  },
  "https://openreview.net/forum?id=P1DuPJzVTN": {
    "title": "Greedy Bayesian Posterior Approximation with Deep Ensembles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aleksei Tiulpin",
      "Matthew B. Blaschko"
    ]
  },
  "https://openreview.net/forum?id=KKeCMim5VN": {
    "title": "Auto-Lambda: Disentangling Dynamic Task Relationships",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shikun Liu",
      "Stephen James",
      "Andrew Davison",
      "Edward Johns"
    ]
  },
  "https://openreview.net/forum?id=4nPswr1KcP": {
    "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andreas Peter Steiner",
      "Alexander Kolesnikov",
      "Xiaohua Zhai",
      "Ross Wightman",
      "Jakob Uszkoreit",
      "Lucas Beyer"
    ]
  },
  "https://openreview.net/forum?id=3gfpBR1ncr": {
    "title": "On Characterizing the Trade-off in Invariant Representation Learning",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bashir Sadeghi",
      "Sepehr Dehdashtian",
      "Vishnu Boddeti"
    ]
  },
  "https://openreview.net/forum?id=1ikK0kHjvj": {
    "title": "A Generalist Agent",
    "volume": "outstanding",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Scott Reed",
      "Konrad Zolna",
      "Emilio Parisotto",
      "Sergio G√≥mez Colmenarejo",
      "Alexander Novikov",
      "Gabriel Barth-maron",
      "Mai Gim√©nez",
      "Yury Sulsky",
      "Jackie Kay",
      "Jost Tobias Springenberg",
      "Tom Eccles",
      "Jake Bruce",
      "Ali Razavi",
      "Ashley Edwards",
      "Nicolas Heess",
      "Yutian Chen",
      "Raia Hadsell",
      "Oriol Vinyals",
      "Mahyar Bordbar",
      "Nando de Freitas"
    ]
  },
  "https://openreview.net/forum?id=AFDcYJKhND": {
    "title": "Scaling Autoregressive Models for Content-Rich Text-to-Image Generation",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahui Yu",
      "Yuanzhong Xu",
      "Jing Yu Koh",
      "Thang Luong",
      "Gunjan Baid",
      "Zirui Wang",
      "Vijay Vasudevan",
      "Alexander Ku",
      "Yinfei Yang",
      "Burcu Karagol Ayan",
      "Ben Hutchinson",
      "Wei Han",
      "Zarana Parekh",
      "Xin Li",
      "Han Zhang",
      "Jason Baldridge",
      "Yonghui Wu"
    ]
  },
  "https://openreview.net/forum?id=oLvlPJheCD": {
    "title": "Queried Unlabeled Data Improves and Robustifies Class-Incremental Learning",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianlong Chen",
      "Sijia Liu",
      "Shiyu Chang",
      "Lisa Amini",
      "Zhangyang Wang"
    ]
  },
  "https://openreview.net/forum?id=GFK1FheE7F": {
    "title": "Deconstructing Self-Supervised Monocular Reconstruction: The Design Decisions that Matter",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaime Spencer",
      "Chris Russell",
      "Simon Hadfield",
      "Richard Bowden"
    ]
  },
  "https://openreview.net/forum?id=sX9d3gfwtE": {
    "title": "Non-Deterministic Behavior of Thompson Sampling with Linear Payoffs and How to Avoid It",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Doruk Kilitcioglu",
      "Serdar Kadioglu"
    ]
  },
  "https://openreview.net/forum?id=vwOKBldzFu": {
    "title": "A Snapshot of the Frontiers of Client Selection in Federated Learning",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gergely D√°niel N√©meth",
      "Miguel Angel Lozano",
      "Novi Quadrianto",
      "Nuria M Oliver"
    ]
  },
  "https://openreview.net/forum?id=NljBlZ6hmG": {
    "title": "Action Noise in Off-Policy Deep Reinforcement Learning: Impact on Exploration and Performance",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jakob Hollenstein",
      "Sayantan Auddy",
      "Matteo Saveriano",
      "Erwan Renaudo",
      "Justus Piater"
    ]
  },
  "https://openreview.net/forum?id=yzkSU5zdwD": {
    "title": "Emergent Abilities of Large Language Models",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jason Wei",
      "Yi Tay",
      "Rishi Bommasani",
      "Colin Raffel",
      "Barret Zoph",
      "Sebastian Borgeaud",
      "Dani Yogatama",
      "Maarten Bosma",
      "Denny Zhou",
      "Donald Metzler",
      "Ed H. Chi",
      "Tatsunori Hashimoto",
      "Oriol Vinyals",
      "Percy Liang",
      "Jeff Dean",
      "William Fedus"
    ]
  },
  "https://openreview.net/forum?id=ywr5sWqQt4": {
    "title": "A Comprehensive Study of Real-Time Object Detection Networks Across Multiple Domains: A Survey",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elahe Arani",
      "Shruthi Gowda",
      "Ratnajit Mukherjee",
      "Omar Magdy",
      "Senthilkumar Sockalingam Kathiresan",
      "Bahram Zonooz"
    ]
  },
  "https://openreview.net/forum?id=LTyqvLEv5b": {
    "title": "On the link between conscious function and general intelligence in humans and machines",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arthur Juliani",
      "Kai Arulkumaran",
      "Shuntaro Sasai",
      "Ryota Kanai"
    ]
  },
  "https://openreview.net/forum?id=gzhEGhcsnN": {
    "title": "Structural Learning in Artificial Neural Networks: A Neural Operator Perspective",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaitlin Maile",
      "Luga Herv√©",
      "Dennis George Wilson"
    ]
  },
  "https://openreview.net/forum?id=P9Cj6RJmN2": {
    "title": "A Stochastic Optimization Framework for Fair Risk Minimization",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrew Lowy",
      "Sina Baharlouei",
      "Rakesh Pavan",
      "Meisam Razaviyayn",
      "Ahmad Beirami"
    ]
  },
  "https://openreview.net/forum?id=vqRzLv6POg": {
    "title": "If your data distribution shifts, use self-learning",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Evgenia Rusak",
      "Steffen Schneider",
      "George Pachitariu",
      "Luisa Eck",
      "Peter Vincent Gehler",
      "Oliver Bringmann",
      "Wieland Brendel",
      "Matthias Bethge"
    ]
  },
  "https://openreview.net/forum?id=UVDAKQANOW": {
    "title": "Unifying Approaches in Active Learning and Active Sampling via Fisher Information and Information-Theoretic Quantities",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andreas Kirsch",
      "Yarin Gal"
    ]
  },
  "https://openreview.net/forum?id=MhK5aXo3gB": {
    "title": "Convergence of denoising diffusion models under the manifold hypothesis",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Valentin De Bortoli"
    ]
  },
  "https://openreview.net/forum?id=oRP8urZ8Fx": {
    "title": "A Note on \"Assessing Generalization of SGD via Disagreement",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andreas Kirsch",
      "Yarin Gal"
    ]
  },
  "https://openreview.net/forum?id=vUuHPRrWs2": {
    "title": "Practicality of generalization guarantees for unsupervised domain adaptation with neural networks",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adam Breitholtz",
      "Fredrik Daniel Johansson"
    ]
  },
  "https://openreview.net/forum?id=EQpGkw5rvL": {
    "title": "Lookback for Learning to Branch",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prateek Gupta",
      "Elias Boutros Khalil",
      "Didier Ch√©telat",
      "Maxime Gasse",
      "Andrea Lodi",
      "Yoshua Bengio",
      "M. Pawan Kumar"
    ]
  },
  "https://openreview.net/forum?id=EDAk6F8yMM": {
    "title": "Local Kernel Ridge Regression for Scalable, Interpolating, Continuous Regression",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingxuan Han",
      "Chenglong Ye",
      "Jeff Phillips"
    ]
  },
  "https://openreview.net/forum?id=i0ZM36d2qU": {
    "title": "Sparse MoEs meet Efficient Ensembles",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "James Urquhart Allingham",
      "Florian Wenzel",
      "Zelda E Mariet",
      "Basil Mustafa",
      "Joan Puigcerver",
      "Neil Houlsby",
      "Ghassen Jerfel",
      "Vincent Fortuin",
      "Balaji Lakshminarayanan",
      "Jasper Snoek",
      "Dustin Tran",
      "Carlos Riquelme Ruiz",
      "Rodolphe Jenatton"
    ]
  },
  "https://openreview.net/forum?id=qrGKGZZvH0": {
    "title": "Do better ImageNet classifiers assess perceptual similarity better?",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manoj Kumar",
      "Neil Houlsby",
      "Nal Kalchbrenner",
      "Ekin Dogus Cubuk"
    ]
  },
  "https://openreview.net/forum?id=3IqqJh2Ycy": {
    "title": "Equivariant Mesh Attention Networks",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sourya Basu",
      "Jose Gallego-Posada",
      "Francesco Vigan√≤",
      "James Rowbottom",
      "Taco Cohen"
    ]
  },
  "https://openreview.net/forum?id=whJPugmP5I": {
    "title": "Finding and Fixing Spurious Patterns with Explanations",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gregory Plumb",
      "Marco Tulio Ribeiro",
      "Ameet Talwalkar"
    ]
  },
  "https://openreview.net/forum?id=IKhEPWGdwK": {
    "title": "Understanding AdamW through Proximal Methods and Scale-Freeness",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenxun Zhuang",
      "Mingrui Liu",
      "Ashok Cutkosky",
      "Francesco Orabona"
    ]
  },
  "https://openreview.net/forum?id=0nEZCVshxS": {
    "title": "Diagnosing and Fixing Manifold Overfitting in Deep Generative Models",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gabriel Loaiza-Ganem",
      "Brendan Leigh Ross",
      "Jesse C Cresswell",
      "Anthony L. Caterini"
    ]
  },
  "https://openreview.net/forum?id=Id7hTt78FV": {
    "title": "Deep Classifiers with Label Noise Modeling and Distance Awareness",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vincent Fortuin",
      "Mark Collier",
      "Florian Wenzel",
      "James Urquhart Allingham",
      "Jeremiah Zhe Liu",
      "Dustin Tran",
      "Balaji Lakshminarayanan",
      "Jesse Berent",
      "Rodolphe Jenatton",
      "Effrosyni Kokiopoulou"
    ]
  },
  "https://openreview.net/forum?id=berNQMTYWZ": {
    "title": "Your Policy Regularizer is Secretly an Adversary",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rob Brekelmans",
      "Tim Genewein",
      "Jordi Grau-Moya",
      "Gregoire Detetang",
      "Markus Kunesch",
      "Shane Legg",
      "Pedro A Ortega"
    ]
  },
  "https://openreview.net/forum?id=86fhqdBUbx": {
    "title": "TLDR: Twin Learning for Dimensionality Reduction",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yannis Kalantidis",
      "Carlos Eduardo Rosar Kos Lassance",
      "Jon Almaz√°n",
      "Diane Larlus"
    ]
  }
}