{
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4_ECCV_2024_paper.php": {
    "title": "Is Retain Set All You Need in Machine Unlearning? Restoring Performance of Unlearned Models with Out-Of-Distribution Images",
    "volume": "main",
    "abstract": "In this paper, we introduce Selective-distillation for Class and Architecture-agnostic unleaRning (SCAR), a novel approximate unlearning method. SCAR efficiently eliminates specific information while preserving the model's test accuracy without using a retain set, which is a key component in state-of-the-art approximate unlearning algorithms. Our approach utilizes a modified Mahalanobis distance to guide the unlearning of the feature vectors of the instances to be forgotten, aligning them to the nearest wrong class distribution. Moreover, we propose a distillation-trick mechanism that distills the knowledge of the original model into the unlearning model with out-of-distribution images for retaining the original model's test performance without using any retain set. Importantly, we propose a self-forget version of SCAR that unlearns without having access to the forget set. We experimentally verified the effectiveness of our method, on three public datasets, comparing it with state-of-the-art methods. Our method obtains performance higher than methods that operate without the retain set and comparable w.r.t the best methods that rely on the retain set",
    "checked": true,
    "id": "6dddeeeee9927a63ef7996e4a35b768d8da0282d",
    "semantic_title": "is retain set all you need in machine unlearning? restoring performance of unlearned models with out-of-distribution images",
    "citation_count": 1,
    "authors": [
      "Jacopo Bonato*",
      "Marco Cotogni",
      "Luigi Sabetta*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6_ECCV_2024_paper.php": {
    "title": "Octopus: Embodied Vision-Language Programmer from Environmental Feedback",
    "volume": "main",
    "abstract": "Large vision-language models (VLMs) have achieved substantial progress in multimodal perception and reasoning. When integrated into an embodied agent, existing embodied VLM works either output detailed action sequences at the manipulation level or only provide plans at an abstract level, leaving a gap between high-level planning and real-world manipulation. To bridge this gap, we introduce Octopus, an embodied vision-language programmer that uses executable code generation as a medium to connect planning and manipulation. Octopus is designed to 1) proficiently comprehend an agent's visual and textual task objectives, 2) formulate intricate action sequences, and 3) generate executable code. To facilitate Octopus model development, we introduce OctoVerse: a suite of environments tailored for benchmarking vision-based code generators on a wide spectrum of tasks, ranging from mundane daily chores in simulators to sophisticated interactions in complex video games such as Grand Theft Auto (GTA) and Minecraft. To train Octopus, we leverage GPT-4 to control an explorative agent that generates training data, i.e., action blueprints and corresponding executable code. We also collect feedback that enables an enhanced training scheme called Reinforcement Learning with Environmental Feedback (RLEF). Through a series of experiments, we demonstrate Octopus's functionality and present compelling results, showing that the proposed RLEF refines the agent's decision-making. By open-sourcing our simulation environments, dataset, and model architecture, we aspire to ignite further innovation and foster collaborative applications within the broader embodied AI community. The project page is available at https://choiszt.github.io/Octopus/",
    "checked": true,
    "id": "b6c8c1745a18d6e59c7a8a99f0df7aa4c18a1e73",
    "semantic_title": "octopus: embodied vision-language programmer from environmental feedback",
    "citation_count": 29,
    "authors": [
      "Jingkang Yang",
      "Yuhao Dong",
      "Shuai Liu",
      "Bo Li",
      "Ziyue Wang",
      "ChenCheng Jiang",
      "Haoran Tan",
      "Jiamu Kang",
      "Yuanhan Zhang",
      "Kaiyang Zhou",
      "Ziwei Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10_ECCV_2024_paper.php": {
    "title": "FunQA: Towards Surprising Video Comprehension",
    "volume": "main",
    "abstract": "Surprising videos, e.g., funny clips, creative performances, or visual illusions, attract significant attention. Enjoyment of these videos is not simply a response to visual stimuli; rather, it hinges on the human capacity to understand (and appreciate) commonsense violations depicted in these videos. We introduce FunQA, a challenging video question answering (QA) dataset specifically designed to evaluate and enhance the depth of video reasoning based on counter-intuitive and fun videos. Unlike most video QA benchmarks which focus on less surprising contexts, e.g., cooking or instructional videos, FunQA covers three previously unexplored types of surprising videos: 1) HumorQA, 2) CreativeQA, and 3) MagicQA. For each subset, we establish rigorous QA tasks designed to assess the model's capability in counter-intuitive timestamp localization, detailed video description, and reasoning around counter-intuitiveness. We also pose higher-level tasks, such as attributing a fitting and vivid title to the video, and scoring the video creativity. In total, the FunQA benchmark consists of 312K free-text QA pairs derived from 4.3K video clips, spanning a total of 24 video hours. Moreover, we propose FunMentor, an agent designed for Vision-Language Models (VLMs) that uses multi-turn dialogues to enhance models' understanding of counter-intuitiveness. Extensive experiments with existing VLMs demonstrate the effectiveness of FunMentor and reveal significant performance gaps for the FunQA videos across spatial-temporal reasoning, visual-centered reasoning, and free-text generation",
    "checked": true,
    "id": "cbf052125c90f78d8750f7d447121193ef53d2f8",
    "semantic_title": "funqa: towards surprising video comprehension",
    "citation_count": 11,
    "authors": [
      "Binzhu Xie",
      "Sicheng Zhang",
      "Zitang Zhou",
      "Bo Li",
      "Yuanhan Zhang",
      "Jack Hessel",
      "Jingkang Yang",
      "Ziwei Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/19_ECCV_2024_paper.php": {
    "title": "4D Contrastive Superflows are Dense 3D Representation Learners",
    "volume": "main",
    "abstract": "In the realm of autonomous driving, accurate 3D perception is the foundation. However, developing such models relies on extensive human annotations – a process that is both costly and labor-intensive. To address this challenge from a data representation learning perspective, we introduce SuperFlow, a novel framework designed to harness consecutive LiDAR-camera pairs for establishing spatiotemporal pretraining objectives. SuperFlow stands out by integrating two key designs: 1) a dense-to-sparse consistency regularization, which promotes insensitivity to point cloud density variations during feature learning, and 2) a flow-based contrastive learning module, carefully crafted to extract meaningful temporal cues from readily available sensor calibrations. To further boost learning efficiency, we incorporate a plug-and-play view consistency module that enhances the alignment of the knowledge distilled from camera views. Extensive comparative and ablation studies across 11 heterogeneous LiDAR datasets validate our effectiveness and superiority. Additionally, we observe several interesting emerging properties by scaling up the 2D and 3D backbones during pretraining, shedding light on the future research of 3D foundation models for LiDAR-based perception. Code is publicly available at https: //github.com/Xiangxu-0103/SuperFlow",
    "checked": true,
    "id": "7b1e7b2495d4a6ccba363dfc9031249bc3ead699",
    "semantic_title": "4d contrastive superflows are dense 3d representation learners",
    "citation_count": 2,
    "authors": [
      "Xiang Xu*",
      "Lingdong Kong",
      "Hui Shuai",
      "Wenwei Zhang",
      "Liang Pan",
      "Kai Chen",
      "Ziwei Liu",
      "Qingshan Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/22_ECCV_2024_paper.php": {
    "title": "ItTakesTwo: Leveraging Peer Representations for Semi-supervised LiDAR Semantic Segmentation",
    "volume": "main",
    "abstract": "The costly and time-consuming annotation process to produce large training sets for modelling semantic LiDAR segmentation methods has motivated the development of semi-supervised learning (SSL) methods. However, such SSL approaches often concentrate on employing consistency learning only for individual LiDAR representations. This narrow focus results in limited perturbations that generally fail to enable effective consistency learning. Additionally, these SSL approaches employ contrastive learning based on the sampling from a limited set of positive and negative embedding samples. This paper introduces a novel semi-supervised LiDAR semantic segmentation framework called ItTakesTwo (IT2). IT2 is designed to ensure consistent predictions from peer LiDAR representations, thereby improving the perturbation effectiveness in consistency learning. Furthermore, our contrastive learning employs informative samples drawn from a distribution of positive and negative embeddings learned from the entire training set. Results on public benchmarks show that our approach achieves remarkable improvements over the previous state-of-the-art (SOTA) methods in the field. redThe code is available at: https://github.com/yyliu01/IT2",
    "checked": true,
    "id": "5d5158de7def0df3b739af9df7e4c7181ac8e30f",
    "semantic_title": "ittakestwo: leveraging peer representations for semi-supervised lidar semantic segmentation",
    "citation_count": 0,
    "authors": [
      "Yuyuan Liu*",
      "Yuanhong Chen",
      "Hu Wang",
      "Vasileios Belagiannis",
      "Ian Reid",
      "Gustavo Carneiro"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/29_ECCV_2024_paper.php": {
    "title": "Ponymation: Learning Articulated 3D Animal Motions from Unlabeled Online Videos",
    "volume": "main",
    "abstract": "We introduce a new method for learning a generative model of articulated 3D animal motions from raw, unlabeled online videos. Unlike existing approaches for 3D motion synthesis, our model requires no pose annotations or parametric shape models for training; it learns purely from a collection of unlabeled web video clips, leveraging semantic correspondences distilled from self-supervised image features. At the core of our method is a video Photo-Geometric Auto-Encoding framework that decomposes each training video clip into a set of explicit geometric and photometric representations, including a rest-pose 3D shape, an articulated pose sequence, and texture, with the objective of re-rendering the input video via a differentiable renderer. This decomposition allows us to learn a generative model over the underlying articulated pose sequences akin to a Variational Auto-Encoding (VAE) formulation, but without requiring any external pose annotations. At inference time, we can generate new motion sequences by sampling from the learned motion VAE, and create plausible 4D animations of an animal automatically within seconds given a single input image",
    "checked": true,
    "id": "a595fbb4d760022f96cd531ff95e9a814362ee22",
    "semantic_title": "ponymation: learning articulated 3d animal motions from unlabeled online videos",
    "citation_count": 0,
    "authors": [
      "Keqiang Sun",
      "Dor Litvak",
      "Yunzhi Zhang",
      "Hongsheng Li",
      "Jiajun Wu*",
      "Shangzhe Wu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/37_ECCV_2024_paper.php": {
    "title": "Robust Fitting on a Gate Quantum Computer",
    "volume": "main",
    "abstract": "Gate quantum computers generate significant interest due to their potential to solve certain difficult problems such as prime factorization in polynomial time. Computer vision researchers have long been attracted to the power of quantum computers. Robust fitting, which is fundamentally important to many computer vision pipelines, has recently been shown to be amenable to gate quantum computing. The previous proposed solution was to compute Boolean influence as a measure of outlyingness using the Bernstein-Vazirani quantum circuit. However, the method assumed a quantum implementation of an ℓ∞ feasibility test, which has not been demonstrated. In this paper, we take a big stride towards quantum robust fitting: we propose a quantum circuit to solve the ℓ∞ feasibility test in the 1D case, which allows to demonstrate for the first time quantum robust fitting on a real gate quantum computer, the IonQ Aria. We also show how 1D Boolean influences can be accumulated to compute Boolean influences for higher-dimensional non-linear models, which we experimentally validate on real benchmark datasets",
    "checked": true,
    "id": "b6f21d350de6268d1eaa82913d673cde3adeb5e3",
    "semantic_title": "robust fitting on a gate quantum computer",
    "citation_count": 0,
    "authors": [
      "Frances F Yang*",
      "Michele Sasdelli",
      "Tat-Jun Chin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/41_ECCV_2024_paper.php": {
    "title": "H-V2X: A Large Scale Highway Dataset for BEV Perception",
    "volume": "main",
    "abstract": "Vehicle-to-everything (V2X) technology has become an area of interest in research due to the availability of roadside infrastructure perception datasets. However, these datasets primarily focus on urban intersections and lack data on highway scenarios. Additionally, the perception tasks in the datasets are mainly MONO 3D due to limited synchronized data across multiple sensors. To bridge this gap, we propose Highway-V2X (H-V2X), the first large-scale highway Bird's-Eye-View (BEV) perception dataset captured by sensors in the real world. The dataset covers over 100 kilometers of highway, with a diverse range of road and weather conditions. H-V2X consists of over 1.9 million fine-grained categorized samples in BEV space, captured by multiple synchronized cameras, with vector map provided. We performed joint 2D-3D calibrations to ensure correct projection and human labor was involved to ensure data quality. Furthermore, we propose three highly relevant tasks to the highway scenario: BEV detection, BEV tracking, and trajectory prediction. We conducted benchmarks for each task, and innovative methods incorporating vector map information were proposed. We hope that H-V2X and benchmark methods will facilitate highway BEV perception research direction. The dataset is available at https://pan.quark.cn/s/86d19da10d18",
    "checked": true,
    "id": "868ad3a6b78fe11f97ff3569b22a55db65c51d45",
    "semantic_title": "h-v2x: a large scale highway dataset for bev perception",
    "citation_count": 0,
    "authors": [
      "Chang Liu*",
      "MingXu zhu",
      "Cong Ma"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/51_ECCV_2024_paper.php": {
    "title": "Learning Camouflaged Object Detection from Noisy Pseudo Label",
    "volume": "main",
    "abstract": "Existing Camouflaged Object Detection (COD) methods rely heavily on large-scale pixel-annotated training sets, which are both time-consuming and labor-intensive. Although weakly supervised methods offer higher annotation efficiency, their performance is far behind due to the unclear visual demarcations between foreground and background in camouflaged images. In this paper, we explore the potential of using boxes as prompts in camouflaged scenes and introduce the first weakly semi-supervised COD method, aiming for budget-efficient and high-precision camouflaged object segmentation with an extremely limited number of fully labeled images. Critically, learning from such limited set inevitably generates pseudo labels with serious noisy pixels. To address this, we propose a noise correction loss that facilitates the model's learning of correct pixels in the early learning stage, and corrects the error risk gradients dominated by noisy pixels in the memorization stage, ultimately achieving accurate segmentation of camouflaged objects from noisy labels. When using only 20% of fully labeled data, our method shows superior performance over the state-of-the-art methods",
    "checked": true,
    "id": "1c351a5b5489ef8556f7bad69dff45991c0247d9",
    "semantic_title": "learning camouflaged object detection from noisy pseudo label",
    "citation_count": 1,
    "authors": [
      "Jin Zhang*",
      "Ruiheng Zhang*",
      "Yanjiao Shi",
      "Zhe Cao",
      "Nian Liu",
      "Fahad Shahbaz Khan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/55_ECCV_2024_paper.php": {
    "title": "Weakly Supervised 3D Object Detection via Multi-Level Visual Guidance",
    "volume": "main",
    "abstract": "Weakly supervised 3D object detection aims to learn a 3D detector with lower annotation cost, e.g., 2D labels. Unlike prior work which still relies on few accurate 3D annotations, we propose a framework to study how to leverage constraints between 2D and 3D domains without requiring any 3D labels. Specifically, we employ visual data from three perspectives to establish connections between 2D and 3D domains. First, we design a feature-level constraint to align LiDAR and image features based on object-aware regions. Second, the output-level constraint is developed to enforce the overlap between 2D and projected 3D box estimations. Finally, the training-level constraint is utilized by producing accurate and consistent 3D pseudo-labels that align with the visual data. We conduct extensive experiments on the KITTI dataset to validate the effectiveness of the proposed three constraints. Without using any 3D labels, our method achieves favorable performance against state-of-the-art approaches and is competitive with the method that uses 500-frame 3D annotations. Code and models will be made publicly available",
    "checked": true,
    "id": "7aa02182b817cba2e7037a79a411d11b1a346cc2",
    "semantic_title": "weakly supervised 3d object detection via multi-level visual guidance",
    "citation_count": 2,
    "authors": [
      "Kuan-Chih Huang*",
      "Yi-Hsuan Tsai",
      "Ming-Hsuan Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/69_ECCV_2024_paper.php": {
    "title": "Deblur e-NeRF: NeRF from Motion-Blurred Events under High-speed or Low-light Conditions",
    "volume": "main",
    "abstract": "The distinctive design philosophy of event cameras makes them ideal for high-speed, high dynamic range & low-light environments, where standard cameras underperform. However, event cameras also suffer from motion blur, especially under these challenging conditions, contrary to what most think. This is due to the limited bandwidth of the event sensor pixel, which is mostly proportional to the light intensity. Thus, to ensure event cameras can truly excel in such conditions where it has an edge over standard cameras, event motion blur must be accounted for in downstream tasks, especially reconstruction. However, no prior work on reconstructing Neural Radiance Fields (NeRFs) from events, nor event simulators, have considered the full effects of event motion blur. To this end, we propose, Deblur e-NeRF, a novel method to directly and effectively reconstruct blur-minimal NeRFs from motion-blurred events, generated under high-speed or low-light conditions. The core component of this work is a physically-accurate pixel bandwidth model that accounts for event motion blur. We also introduce a threshold-normalized total variation loss to better regularize large textureless patches. Experiments on real & novel realistically simulated sequences verify our effectiveness. Our code, event simulator and synthetic event dataset are open-sourced",
    "checked": true,
    "id": "c3e950220f47dcdaebd596f4258f1e6042958f02",
    "semantic_title": "deblur e-nerf: nerf from motion-blurred events under high-speed or low-light conditions",
    "citation_count": 0,
    "authors": [
      "Weng Fei Low*",
      "Gim Hee Lee"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/72_ECCV_2024_paper.php": {
    "title": "CLR-GAN: Improving GANs Stability and Quality via Consistent Latent Representation and Reconstruction",
    "volume": "main",
    "abstract": "Generative Adversarial Networks(GANs) have received considerable attention due to its outstanding ability to generate images. However, training a GAN is hard since the game between the Generator(G) and the Discriminator(D) is unfair. Towards making the competition fairer, we propose a new perspective of training GANs, named Consistent Latent Representation and Reconstruction(CLR-GAN). In this paradigm, we treat the G and D as an inverse process, the discriminator has an additional task to restore the pre-defined latent code while the generator also needs to reconstruct the real input, thus obtaining a relationship between the latent space of G and the out-features of D. Based on this prior, we can put D and G on an equal position during training using a new criterion. Experimental results on various datasets and architectures prove our paradigm can make GANs more stable and generate better quality images(31.22% gain of FID on CIFAR10 and 39.5% on AFHQ-Cat, respectively). We hope that the proposed perspective can inspire researchers to explore different ways of viewing GANs training, rather than being limited to a two-player game. The code is publicly available at https://github.com/Petecheco/CLR-GAN",
    "checked": true,
    "id": "ce3156cb13297b7f5728d67a76048bca134b360f",
    "semantic_title": "clr-gan: improving gans stability and quality via consistent latent representation and reconstruction",
    "citation_count": 0,
    "authors": [
      "Shengke Sun",
      "Ziqian Luan",
      "Zhanshan Zhao*",
      "Shijie Luo",
      "Shuzhen Han*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/73_ECCV_2024_paper.php": {
    "title": "Learn from the Learnt: Source-Free Active Domain Adaptation via Contrastive Sampling and Visual Persistence",
    "volume": "main",
    "abstract": "Domain Adaptation (DA) facilitates knowledge transfer from a source domain to a related target domain. This paper investigates a practical DA paradigm, namely Source data-Free Active Domain Adaptation (SFADA), where source data becomes inaccessible during adaptation, and a minimum amount of annotation budget is available in the target domain. Without referencing the source data, new challenges emerge in identifying the most informative target samples for labeling, establishing cross-domain alignment during adaptation, and ensuring continuous performance improvements through the iterative query-and-adaptation process. In response, we present learn from the learnt (LFTL), a novel paradigm for SFADA to leverage the learnt knowledge from the source pretrained model and actively iterated models without extra overhead. We propose Contrastive Active Sampling to learn from the hypotheses of the preceding model, thereby querying target samples that are both informative to the current model and persistently challenging throughout active learning. During adaptation, we learn from features of actively selected anchors obtained from previous intermediate models, so that the Visual Persistence-guided Adaptation can facilitate feature distribution alignment and active sample exploitation. Extensive experiments on three widely-used benchmarks show that our LFTL achieves state-of-the-art performance, superior computational efficiency and continuous improvements as the annotation budget increases. Our code is available at https://github.com/lyumengyao/lftl",
    "checked": true,
    "id": "ea2c010f24f955d63489408f1ba951b1b0cfdbd8",
    "semantic_title": "learn from the learnt: source-free active domain adaptation via contrastive sampling and visual persistence",
    "citation_count": 3,
    "authors": [
      "Mengyao Lyu",
      "Tianxiang Hao",
      "Xinhao Xu",
      "Hui Chen*",
      "Zijia Lin",
      "Jungong Han",
      "Guiguang Ding*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/75_ECCV_2024_paper.php": {
    "title": "PromptIQA: Boosting the Performance and Generalization for No-Reference Image Quality Assessment via Prompts",
    "volume": "main",
    "abstract": "Due to the diversity of assessment requirements in various application scenarios for the IQA task, existing IQA methods struggle to directly adapt to these varied requirements after training. Thus, when facing new requirements, a typical approach is fine-tuning these models on datasets specifically created for those requirements. However, it is time-consuming to establish IQA datasets. In this work, we propose a Prompt-based IQA (PromptIQA) that can fast adapt to new requirements without fine-tuning after training. On one hand, it utilizes a short sequence of Image-Score Pairs (ISP) as prompts for targeted predictions, which significantly reduces the dependency on the data requirements. On the other hand, PromptIQA is trained on a mixed dataset with two proposed data augmentation strategies to learn diverse requirements, thus enabling it to fast adapt to new requirements. Experiments indicate that the PromptIQA outperforms SOTA methods with higher performance and better generalization. The code is available at the link",
    "checked": true,
    "id": "5f686a772ae233e0acea26298191251df4022617",
    "semantic_title": "promptiqa: boosting the performance and generalization for no-reference image quality assessment via prompts",
    "citation_count": 4,
    "authors": [
      "Zewen Chen",
      "Haina Qin",
      "Juan Wang",
      "Chunfeng Yuan",
      "Bing Li*",
      "Weiming Hu",
      "Leon Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/100_ECCV_2024_paper.php": {
    "title": "Motion Mamba: Efficient and Long Sequence Motion Generation",
    "volume": "main",
    "abstract": "Human motion generation stands as a significant pursuit in generative computer vision, while achieving long-sequence and efficient motion generation remains challenging. Recent advancements in state space models (SSMs), notably Mamba, have showcased considerable promise in long sequence modeling with an efficient hardware-aware design, which appears to be a significant direction upon building motion generation model. Nevertheless, adapting SSMs to motion generation faces hurdles since the lack of a specialized design architecture to model motion sequence. To address these challenges, we propose Motion Mamba, a simple yet efficient approach that presents the pioneering motion generation model utilized SSMs. Specifically, we design a Hierarchical Temporal Mamba (HTM) block to process temporal data by ensemble varying numbers of isolated SSM modules across a symmetric U-Net architecture aimed at preserving motion consistency between frames. We also design a Bidirectional Spatial Mamba (BSM) block to bidirectionally process latent poses, to enhance accurate motion generation within a temporal frame. Our proposed method achieves up to 50% FID improvement and up to 4 times faster on the HumanML3D and KIT-ML datasets compared to the previous best diffusion-based method, which demonstrates strong capabilities of high-quality long sequence motion modeling and real-time human motion generation",
    "checked": false,
    "id": "b9646f057887825d7471ec01664494b0b7ca5a83",
    "semantic_title": "motion mamba: efficient and long sequence motion generation with hierarchical and bidirectional selective ssm",
    "citation_count": 26,
    "authors": [
      "Zeyu Zhang",
      "Akide Liu",
      "Ian Reid",
      "RICHARD HARTLEY",
      "Bohan Zhuang",
      "Hao Tang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/111_ECCV_2024_paper.php": {
    "title": "Radiative Gaussian Splatting for Efficient X-ray Novel View Synthesis",
    "volume": "main",
    "abstract": "X-ray is widely applied for transmission imaging due to its stronger penetration than natural light. When rendering novel view X-ray projections, existing methods mainly based on NeRF suffer from long training time and slow inference speed. In this paper, we propose a 3D Gaussian splatting-based method, namely X-Gaussian, for X-ray novel view synthesis. Firstly, we redesign a radiative Gaussian point cloud model inspired by the isotropic nature of X-ray imaging. Our model excludes the influence of view direction when learning to predict the radiation intensity of 3D points. Based on this model, we develop a Differentiable Radiative Rasterization (DRR) with CUDA implementation. Secondly, we customize an Angle-pose Cuboid Uniform Initialization (ACUI) strategy that directly uses the parameters of the X-ray scanner to compute the camera information and then uniformly samples point positions within a cuboid enclosing the scanned object. Experiments show that our X-Gaussian outperforms state-of-the-art methods by 6.5 dB while enjoying less than 15% training time and over 73× inference speed. The application on CT reconstruction also reveals the practical values of our method. Code is at https://github.com/caiyuanhao1998/X-Gaussian",
    "checked": true,
    "id": "e00fa03ef1ec8f082a63a50ed2d6741377eb3771",
    "semantic_title": "radiative gaussian splatting for efficient x-ray novel view synthesis",
    "citation_count": 9,
    "authors": [
      "Yuanhao Cai*",
      "Yixun Liang",
      "Jiahao Wang",
      "Angtian Wang",
      "Yulun Zhang",
      "Xiaokang Yang",
      "Zongwei Zhou",
      "Alan Yuille"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/113_ECCV_2024_paper.php": {
    "title": "Tracking Meets LoRA: Faster Training, Larger Model, Stronger Performance",
    "volume": "main",
    "abstract": "Motivated by the Parameter-Efficient Fine-Tuning (PEFT) in large language models, we propose LoRAT, a method that unveils the power of larger Vision Transformers (ViT) for tracking within laboratory-level resources. The essence of our work lies in adapting LoRA, a technique that fine-tunes a small subset of model parameters without adding inference latency, to the domain of visual tracking. However, unique challenges and potential domain gaps make this transfer not as easy as the first intuition. Firstly, a transformer-based tracker constructs unshared position embedding for template and search image. This poses a challenge for the transfer of LoRA, usually requiring consistency in the design when applied to the pre-trained backbone, to downstream tasks. Secondly, the inductive bias inherent in convolutional heads diminishes the effectiveness of parameter-efficient fine-tuning in tracking models. To overcome these limitations, we first decouple the position embeddings in transformer-based trackers into shared spatial ones and independent type ones. The shared embeddings, which describe the absolute coordinates of multi-resolution images (namely, the template and search images), are inherited from the pre-trained backbones. In contrast, the independent embeddings indicate the sources of each token and are learned from scratch. Furthermore, we design an anchor-free head solely based on a multilayer perceptron (MLP) to adapt PETR, enabling better performance with less computational overhead. With our design, 1) it becomes practical to train trackers with the ViT-g backbone on GPUs with only memory of 25.8GB (batch size of 16); 2) we reduce the training time of the L-224 variant from 35.0 to 10.8 GPU hours; 3) we improve the LaSOT SUC score from 0.703 to 0.742 with the L-224 variant; 4) we fast the inference speed of the L-224 variant from 52 to 119 FPS. Code and models are available at https://github.com/LitingLin/LoRAT",
    "checked": true,
    "id": "eb2c30897dd1cf01f5a1d31ae402eb641a9123a1",
    "semantic_title": "tracking meets lora: faster training, larger model, stronger performance",
    "citation_count": 3,
    "authors": [
      "Liting Lin",
      "Heng Fan",
      "Zhipeng Zhang",
      "Yaowei Wang*",
      "Yong Xu",
      "Haibin Ling*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/126_ECCV_2024_paper.php": {
    "title": "A Direct Approach to Viewing Graph Solvability",
    "volume": "main",
    "abstract": "The viewing graph is a useful way to represent uncalibrated cameras and their geometric relationships: nodes correspond to cameras and edges represent fundamental matrices. By analyzing this graph, it is possible to establish if the problem is \"solvable\" in the sense that there exists a unique (up to a single projective transformation) set of cameras that are compliant with the given fundamental matrices. In this paper, we take several steps forward in the study of viewing graph solvability: we propose a new formulation of the problem that is more direct than previous literature, based on a formula that explicitly links pairs of cameras via their fundamental matrix; we introduce the new concept of \"\", demonstrating its usefulness in understanding real structure from motion graphs; we propose an algorithm for testing and extracting components of unsolvable cases, that is more efficient than previous work; we set up an open question on the connection between and solvability",
    "checked": true,
    "id": "856cc53455bb9d843c8859cdaf56b794809a9798",
    "semantic_title": "a direct approach to viewing graph solvability",
    "citation_count": 0,
    "authors": [
      "Federica Arrigoni*",
      "Andrea Fusiello",
      "Tomas Pajdla"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/139_ECCV_2024_paper.php": {
    "title": "CoR-GS: Sparse-View 3D Gaussian Splatting via Co-Regularization",
    "volume": "main",
    "abstract": "3D Gaussian Splatting (3DGS) creates a radiance field consisting of 3D Gaussians to represent a scene. With sparse training views, 3DGS easily suffers from overfitting, negatively impacting rendering. This paper introduces a new co-regularization perspective for improving sparse-view 3DGS. When training two 3D Gaussian radiance fields, we observe that the two radiance fields exhibit point disagreement and rendering disagreement that can unsupervisedly predict reconstruction quality, stemming from the randomness of densification implementation. We further quantify the two disagreements and demonstrate the negative correlation between them and accurate reconstruction, which allows us to identify inaccurate reconstruction without accessing ground-truth information. Based on the study, we propose CoR-GS, which identifies and suppresses inaccurate reconstruction based on the two disagreements: (i) Co-pruning considers Gaussians that exhibit high point disagreement in inaccurate positions and prunes them. (ii) Pseudo-view co-regularization considers pixels that exhibit high rendering disagreement are inaccurate and suppress the disagreement. Results on LLFF, Mip-NeRF360, DTU, and Blender demonstrate that CoR-GS effectively regularizes the scene geometry, reconstructs the compact representations, and achieves state-of-the-art novel view synthesis quality under sparse training views. Project page: https: //jiaw-z.github.io/CoR-GS",
    "checked": true,
    "id": "58d7c088a9cc9697cffb9397258ac9e457b08410",
    "semantic_title": "cor-gs: sparse-view 3d gaussian splatting via co-regularization",
    "citation_count": 7,
    "authors": [
      "Jiawei Zhang",
      "Jiahe Li",
      "Xiaohan Yu",
      "Lei Huang",
      "Lin Gu",
      "Jin Zheng*",
      "Xiao Bai*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/143_ECCV_2024_paper.php": {
    "title": "SeFlow: A Self-Supervised Scene Flow Method in Autonomous Driving",
    "volume": "main",
    "abstract": "Scene flow estimation predicts the 3D motion at each point in successive LiDAR scans. This detailed, point-level, information can help autonomous vehicles to accurately predict and understand dynamic changes in their surroundings. Current state-of-the-art methods require annotated data to train scene flow networks and the expense of labeling inherently limits their scalability. Self-supervised approaches can overcome the above limitations, yet face two principal challenges that hinder optimal performance: point distribution imbalance and disregard for object-level motion constraints. In this paper, we propose SeFlow, a self-supervised method that integrates efficient dynamic classification into a learning-based scene flow pipeline. We demonstrate that classifying static and dynamic points helps design targeted objective functions for different motion patterns. We also emphasize the importance of internal cluster consistency and correct object point association to refine the scene flow estimation, in particular on object details. Our real-time capable method achieves state-of-the-art performance on the self-supervised scene flow task on Argoverse 2 and Waymo datasets. The code is open-sourced at https://github.com/KTH-RPL/SeFlow",
    "checked": true,
    "id": "143a44051a8542585339f1f7ae08c2c1b457e745",
    "semantic_title": "seflow: a self-supervised scene flow method in autonomous driving",
    "citation_count": 4,
    "authors": [
      "Qingwen Zhang*",
      "Yi Yang",
      "Peizheng Li",
      "Olov Andersson",
      "Patric Jensfelt"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/144_ECCV_2024_paper.php": {
    "title": "ZeST: Zero-Shot Material Transfer from a Single Image",
    "volume": "main",
    "abstract": "We propose , a method for zero-shot material transfer to an object in the input image given a material exemplar image. leverages existing diffusion adapters to extract implicit material representation from the exemplar image. This representation is used to transfer the material using pre-trained inpainting diffusion model on the object in the input image using depth estimates as geometry cue and grayscale object shading as illumination cues. The method works on real images without any training resulting a zero-shot approach. Both qualitative and quantitative results on real and synthetic datasets demonstrate that outputs photorealistic images with transferred materials. We also show the application of to perform multiple edits and robust material assignment under different illuminations. Project Page: https://ttchengab.github.io/zest",
    "checked": true,
    "id": "8d36accc2237a5800de8b2f6af438beb6de1c207",
    "semantic_title": "zest: zero-shot material transfer from a single image",
    "citation_count": 1,
    "authors": [
      "Ta-Ying Cheng",
      "Prafull Sharma",
      "Andrew Markham",
      "Niki Trigoni",
      "Varun Jampani*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/145_ECCV_2024_paper.php": {
    "title": "3D Congealing: 3D-Aware Image Alignment in the Wild",
    "volume": "main",
    "abstract": "We propose , a novel problem of 3D-aware alignment for 2D images capturing semantically similar objects. Given a collection of unlabeled Internet images, our goal is to associate the shared semantic parts from the inputs and aggregate the knowledge from 2D images to a shared 3D canonical space. We introduce a general framework that tackles the task without assuming shape templates, poses, or any camera parameters. At its core is a canonical 3D representation that encapsulates geometric and semantic information. The framework optimizes for the canonical representation together with the pose for each input image, and a per-image coordinate map that warps 2D pixel coordinates to the 3D canonical frame to account for the shape matching. The optimization procedure fuses prior knowledge from a pre-trained image generative model and semantic information from input images. The former provides strong knowledge guidance for this under-constraint task, while the latter provides the necessary information to mitigate the training data bias from the pre-trained model. Our framework can be used for various tasks such as pose estimation and image editing, achieving strong results on real-world image datasets under challenging illumination conditions and on in-the-wild online image collections. Project page at https://ai.stanford. edu/~yzzhang/projects/3d-congealing/",
    "checked": true,
    "id": "c9385ded43b296ae78bceb05be6462293c4e4fc6",
    "semantic_title": "3d congealing: 3d-aware image alignment in the wild",
    "citation_count": 0,
    "authors": [
      "Yunzhi Zhang*",
      "Zizhang Li",
      "Amit Raj",
      "Andreas Engelhardt",
      "Yuanzhen Li",
      "Tingbo Hou",
      "Jiajun Wu",
      "Varun Jampani"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/147_ECCV_2024_paper.php": {
    "title": "SMooDi: Stylized Motion Diffusion Model",
    "volume": "main",
    "abstract": "We introduce a novel Stylized Motion Diffusion model, dubbed , to generate stylized motion driven by content texts and style motion sequences. Unlike existing methods that either generate motion of various content or transfer style from one sequence to another, can rapidly generate motion across a broad range of content and diverse styles. To this end, we tailor a pre-trained text-to-motion model for stylization. Specifically, we propose style guidance to ensure that the generated motion closely matches the reference style, alongside a lightweight style adaptor that directs the motion towards the desired style while ensuring realism. Experiments across various applications demonstrate that our proposed framework outperforms existing methods in stylized motion generation. Project Page: https://neu-vi.github.io/SMooDi/",
    "checked": true,
    "id": "42b3728c96b2ad1ffe702ed5c692f35037d61690",
    "semantic_title": "smoodi: stylized motion diffusion model",
    "citation_count": 4,
    "authors": [
      "Lei Zhong",
      "Yiming Xie",
      "Varun Jampani",
      "Deqing Sun",
      "Huaizu Jiang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/148_ECCV_2024_paper.php": {
    "title": "ZipLoRA: Any Subject in Any Style by Effectively Merging LoRAs",
    "volume": "main",
    "abstract": "Methods for finetuning generative models for concept-driven personalization generally achieve strong results for subject-driven or style-driven generation. Recently, low-rank adaptations () have been proposed as a parameter-efficient way of achieving concept-driven personalization. While recent work explores the combination of separate LoRAs to achieve joint generation of learned styles and subjects, existing techniques do not reliably address the problem, so that either subject fidelity or style fidelity are compromised. We propose , a method to cheaply and effectively merge independently trained style and subject LoRAs in order to achieve generation of any user-provided subject in any user-provided style. Experiments on a wide range of subject and style combinations show that can generate compelling results with meaningful improvements over baselines in subject and style fidelity while preserving the ability to recontextualize",
    "checked": true,
    "id": "185e88645dfc07d6ca81a55dfc66bd3452400276",
    "semantic_title": "ziplora: any subject in any style by effectively merging loras",
    "citation_count": 55,
    "authors": [
      "Viraj Shah",
      "Nataniel Ruiz",
      "Forrester Cole",
      "Erika Lu",
      "Svetlana Lazebnik",
      "Yuanzhen Li",
      "Varun Jampani*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/150_ECCV_2024_paper.php": {
    "title": "SV3D: Novel Multi-view Synthesis and 3D Generation from a Single Image using Latent Video Diffusion",
    "volume": "main",
    "abstract": "We present Stable Video 3D (SV3D) — a latent video diffusion model for high-resolution, image-to-multi-view generation of orbital videos around a 3D object. Recent works propose to adapt 2D generative models for novel view synthesis (NVS) and 3D optimization. However, these methods have several disadvantages due to limited views or inconsistent NVS, affecting the performance of 3D object generation. In this work, we propose SV3D that adapts image-to-video diffusion model for novel multi-view synthesis and 3D generation, thereby leveraging the generalization and multi-view consistency of the video models, while further adding explicit camera control for NVS. We also propose improved 3D optimization techniques for image-to-3D generation using SV3D and its NVS outputs. Extensive experiments on multiple datasets with 2D and 3D metrics and user study demonstrate SV3D's state-of-the-art performance on NVS as well as 3D reconstruction compared to prior works",
    "checked": true,
    "id": "03e2cfa44b64489fb98f09dfbd940043fbef90ad",
    "semantic_title": "sv3d: novel multi-view synthesis and 3d generation from a single image using latent video diffusion",
    "citation_count": 67,
    "authors": [
      "Vikram Voleti*",
      "Chun-Han Yao",
      "Mark Boss",
      "Adam Letts",
      "David Pankratz",
      "Dmitrii Tochilkin",
      "Christian Laforte",
      "Robin Rombach",
      "Varun Jampani*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/151_ECCV_2024_paper.php": {
    "title": "WordRobe: Text-Guided Generation of Textured 3D Garments",
    "volume": "main",
    "abstract": "In this paper, we tackle a new and challenging problem of text-driven generation of 3D garments with high-quality textures. We propose, WordRobe, a novel framework for the generation of unposed & textured 3D garment meshes from user-friendly text prompts. We achieve this by first learning a latent representation of 3D garments using a novel coarse-to-fine training strategy and a loss for latent disentanglement, promoting better latent interpolation. Subsequently, we align the garment latent space to the CLIP embedding space in a weakly supervised manner, enabling text-driven 3D garment generation and editing. For appearance modeling, we leverage the zero-shot generation capability of ControlNet to synthesize view-consistent texture maps in a single feed-forward inference step, thereby drastically decreasing the generation time as compared to existing methods. We demonstrate superior performance over current SOTAs for learning 3D garment latent space, garment interpolation, and text-driven texture synthesis, supported by quantitative evaluation and qualitative user study. The unposed 3D garment meshes generated using WordRobe can be directly fed to standard cloth simulation & animation pipelines without any post-processing",
    "checked": true,
    "id": "a19f249557e1a0b2a8b6388645fb88892f0c3887",
    "semantic_title": "wordrobe: text-guided generation of textured 3d garments",
    "citation_count": 4,
    "authors": [
      "Astitva Srivastava*",
      "Pranav Manu",
      "Amit Raj",
      "Varun Jampani",
      "Avinash Sharma"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/159_ECCV_2024_paper.php": {
    "title": "Learning to Generate Conditional Tri-plane for 3D-aware Expression Controllable Portrait Animation",
    "volume": "main",
    "abstract": "In this paper, we present , a one-shot 3D-aware portrait animation method that is able to control the facial expression and camera view of a given portrait image. To achieve this, we introduce a tri-plane generator with an effective expression conditioning method, which directly generates a tri-plane of 3D prior by transferring the expression parameter of 3DMM into the source image. The tri-plane is then decoded into the image of different view through a differentiable volume rendering. Existing portrait animation methods heavily rely on image warping to transfer the expression in the motion space, challenging on disentanglement of appearance and expression. In contrast, we propose a contrastive pre-training framework for appearance-free expression parameter, eliminating undesirable appearance swap when transferring a cross-identity expression. Extensive experiments show that our pre-training framework can learn the appearance-free expression representation hidden in 3DMM, and our model can generate 3D-aware expression controllable portrait images without appearance swap in the cross-identity manner",
    "checked": true,
    "id": "87dae5e837789163fe5202c939adf226b5ce5a73",
    "semantic_title": "learning to generate conditional tri-plane for 3d-aware expression controllable portrait animation",
    "citation_count": 2,
    "authors": [
      "Taekyung Ki*",
      "Dongchan Min",
      "Gyeongsu Chae*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/161_ECCV_2024_paper.php": {
    "title": "SimPB: A Single Model for 2D and 3D Object Detection from Multiple Cameras",
    "volume": "main",
    "abstract": "The field of autonomous driving has attracted considerable interest in approaches that directly infer 3D objects in the Bird's Eye View (BEV) from multiple cameras. Some attempts have also explored utilizing 2D detectors from single images to enhance the performance of 3D detection. However, these approaches rely on a two-stage process with separate detectors, where the 2D detection results are utilized only once for token selection or query initialization. In this paper, we present a single model termed SimPB, which Simultaneously detects 2D objects in the Perspective view and 3D objects in the BEV space from multiple cameras. To achieve this, we introduce a hybrid decoder consisting of several multi-view 2D decoder layers and several 3D decoder layers, specifically designed for their respective detection tasks. A Dynamic Query Allocation module and an Adaptive Query Aggregation module are proposed to continuously update and refine the interaction between 2D and 3D results, in a cyclic 3D-2D-3D manner. Additionally, Query-group Attention is utilized to strengthen the interaction among 2D queries within each camera group. In the experiments, we evaluate our method on the nuScenes dataset and demonstrate promising results for both 2D and 3D detection tasks. Our code is available at: https: //github.com/nullmax-vision/SimPB",
    "checked": true,
    "id": "88880b6ca5b975ef124cb0b2e05e1bbed7d45f3b",
    "semantic_title": "simpb: a single model for 2d and 3d object detection from multiple cameras",
    "citation_count": 0,
    "authors": [
      "Yingqi Tang",
      "Zhaotie Meng",
      "Guoliang Chen",
      "Erkang Cheng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/168_ECCV_2024_paper.php": {
    "title": "EMDM: Efficient Motion Diffusion Model for Fast, High-Quality Human Motion Generation",
    "volume": "main",
    "abstract": "We introduce Efficient Motion Diffusion Model (EMDM) for fast and high-quality human motion generation. Current state-of-the-art generative diffusion models have produced impressive results but struggle to achieve fast generation without sacrificing quality. On the one hand, previous works, like motion latent diffusion, conduct diffusion within a latent space for efficiency, but learning such a latent space can be a non-trivial effort. On the other hand, accelerating generation by naively increasing the sampling step size, e.g., DDIM, often leads to quality degradation as it fails to approximate the complex denoising distribution. To address these issues, we propose EMDM, which captures the complex distribution during multiple sampling steps in the diffusion model, allowing for much fewer sampling steps and significant acceleration in generation. This is achieved by a conditional denoising diffusion GAN to capture multimodal data distributions among arbitrary (and potentially larger) step sizes conditioned on control signals, enabling fewer-step motion sampling with high fidelity and diversity. To minimize undesired motion artifacts, geometric losses are imposed during network learning. As a result, EMDM achieves real-time motion generation and significantly improves the efficiency of motion diffusion models compared to existing methods while achieving high-quality motion generation. Our code is available at https: //github.com/Frank-ZY-Dou/EMDM",
    "checked": false,
    "id": "24b7abcd5a876146d613cbca0ad5e09d9851b259",
    "semantic_title": "emdm: efficient motion diffusion model for fast, high-quality motion generation",
    "citation_count": 19,
    "authors": [
      "Wenyang Zhou",
      "Zhiyang Dou*",
      "Zeyu Cao",
      "Zhouyingcheng Liao",
      "Jingbo Wang",
      "Wenjia Wang",
      "Yuan Liu",
      "Taku Komura",
      "Wenping Wang",
      "Lingjie Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/175_ECCV_2024_paper.php": {
    "title": "Editable Image Elements for Controllable Synthesis",
    "volume": "main",
    "abstract": "Diffusion models have made significant advances in text-guided synthesis tasks. However, editing user-provided images remains challenging, as the high dimensional noise input space of diffusion models is not naturally suited for image inversion or spatial editing. In this work, we propose an image representation that promotes spatial editing of input images using a diffusion model. Concretely, we learn to encode an input into \"image elements\" that can faithfully reconstruct an input image. These elements can be intuitively edited by a user, and are decoded by a diffusion model into realistic images. We show the effectiveness of our representation on various image editing tasks, such as object resizing, rearrangement, dragging, de-occlusion, removal, variation, and image composition",
    "checked": true,
    "id": "a9e3061b0948084d8af6197b38f9442d0c1882ed",
    "semantic_title": "editable image elements for controllable synthesis",
    "citation_count": 3,
    "authors": [
      "Jiteng Mu*",
      "Michaël Gharbi",
      "Richard Zhang",
      "Eli Shechtman",
      "Nuno Vasconcelos",
      "Xiaolong Wang",
      "Taesung Park*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/176_ECCV_2024_paper.php": {
    "title": "Improving 2D Feature Representations by 3D-Aware Fine-Tuning",
    "volume": "main",
    "abstract": "Current visual foundation models are trained purely on unstructured 2D data, limiting their understanding of 3D structure of objects and scenes. In this work, we show that fine-tuning on 3D-aware data improves the quality of emerging semantic features. We design a method to lift semantic 2D features into an efficient 3D Gaussian representation, which allows us to re-render them for arbitrary views. Using the rendered 3D-aware features, we design a fine-tuning strategy to transfer such 3D awareness into a 2D foundation model. We demonstrate that models fine-tuned in that way produce features that readily improve downstream task performance in semantic segmentation and depth estimation through simple linear probing. Notably, though fined-tuned on a single indoor dataset, the improvement is transferable to a variety of indoor datasets and out-of-domain datasets. We hope our study encourages the community to consider injecting 3D awareness when training 2D foundation models. Project page: https://ywyue.github.io/FiT3D",
    "checked": true,
    "id": "270eff2aac446af35c64ed0efbb7e57fc8f859f0",
    "semantic_title": "improving 2d feature representations by 3d-aware fine-tuning",
    "citation_count": 1,
    "authors": [
      "Yuanwen Yue*",
      "Anurag Das",
      "Francis Engelmann",
      "Siyu Tang",
      "Jan Eric Lenssen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/180_ECCV_2024_paper.php": {
    "title": "Self-supervised Feature Adaptation for 3D Industrial Anomaly Detection",
    "volume": "main",
    "abstract": "Industrial anomaly detection is generally addressed as an unsupervised task that aims at locating defects with only normal training samples. Recently, numerous 2D anomaly detection methods have been proposed and have achieved promising results, however, using only the 2D RGB data as input is not sufficient to identify imperceptible geometric surface anomalies. Hence, in this work, we focus on multi-modal anomaly detection. Specifically, we investigate early multi-modal approaches that attempted to utilize models pre-trained on large-scale visual datasets, i.e., ImageNet, to construct feature databases. And we empirically find that directly using these pre-trained models is not optimal, it can either fail to detect subtle defects or mistake abnormal features as normal ones. This may be attributed to the domain gap between target industrial data and source data. Towards this problem, we propose a Local-to-global Self-supervised Feature Adaptation (LSFA) method to finetune the adaptors and learn task-oriented representation toward anomaly detection. Both intra-modal adaptation and cross-modal alignment are optimized from a local-to-global perspective in LSFA to ensure the representation quality and consistency in the inference stage. Extensive experiments demonstrate that our method not only brings a significant performance boost to feature embedding based approaches, but also outperforms previous State-of-The-Art (SoTA) methods prominently on both MVTec-3D AD and Eyecandies datasets, e.g., LSFA achieves 97.1% I-AUROC on MVTec-3D, surpass previous SoTA by +3.4%. Code is available at https://github.com/ yuanpengtu/LSFA",
    "checked": true,
    "id": "023d614a25a0bfa48cfe6574a9fa6b29f68b09df",
    "semantic_title": "self-supervised feature adaptation for 3d industrial anomaly detection",
    "citation_count": 2,
    "authors": [
      "Yuanpeng Tu",
      "Boshen Zhang",
      "Liang Liu",
      "YUXI LI",
      "Jiangning Zhang",
      "Yabiao Wang*",
      "Chengjie Wang",
      "cairong zhao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/187_ECCV_2024_paper.php": {
    "title": "PCF-Lift: Panoptic Lifting by Probabilistic Contrastive Fusion",
    "volume": "main",
    "abstract": "Panoptic lifting is an effective technique to address the 3D panoptic segmentation task by unprojecting 2D panoptic segmentations from multi-views to 3D scene. However, the quality of its results largely depends on the 2D segmentations, which could be noisy and error-prone, so its performance often drops significantly for complex scenes. In this work, we design a new pipeline coined PCF-Lift based on our Probabilis-tic Contrastive Fusion (PCF) to learn and embed probabilistic features throughout our pipeline to actively consider inaccurate segmentations and inconsistent instance IDs. Technical-wise, we first model the probabilistic feature embeddings through multivariate Gaussian distributions. To fuse the probabilistic features, we incorporate the probability product kernel into the contrastive loss formulation and design a cross-view constraint to enhance the feature consistency across different views. For the inference, we introduce a new probabilistic clustering method to effectively associate prototype features with the underlying 3D object instances for the generation of consistent panoptic segmentation results. Further, we provide a theoretical analysis to justify the superiority of the proposed probabilistic solution. By conducting extensive experiments, our PCF-lift not only significantly outperforms the state-of-the-art methods on widely used benchmarks including the ScanNet dataset and the challenging Messy Room dataset (4.4% improvement of scene-level PQ), but also demonstrates strong robustness when incorporating various 2D segmentation models or different levels of hand-crafted noise",
    "checked": true,
    "id": "fe9705d25869a12ee0bd6d89ca61ea09e4fc909f",
    "semantic_title": "pcf-lift: panoptic lifting by probabilistic contrastive fusion",
    "citation_count": 0,
    "authors": [
      "Runsong Zhu*",
      "Shi Qiu*",
      "Qianyi Wu",
      "Ka-Hei Hui",
      "Pheng-Ann Heng",
      "Chi-Wing Fu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/193_ECCV_2024_paper.php": {
    "title": "SemGrasp: Semantic Grasp Generation via Language Aligned Discretization",
    "volume": "main",
    "abstract": "Generating natural human grasps necessitates consideration of not just object geometry but also semantic information. Solely depending on object shape for grasp generation confines the applications of prior methods in downstream tasks. This paper presents a novel semantic-based grasp generation method, termed , which generates a static human grasp pose by incorporating semantic information into the grasp representation. We introduce a discrete representation that aligns the grasp space with semantic space, enabling the generation of grasp postures in accordance with language instructions. A Multimodal Large Language Model (MLLM) is subsequently fine-tuned, integrating object, grasp, and language within a unified semantic space. To facilitate the training of , we compile a large-scale, grasp-text-aligned dataset named , featuring over 300k detailed captions and 50k diverse grasps. Experimental findings demonstrate that efficiently generates natural human grasps in alignment with linguistic intentions. Our code, models, and dataset will be made publicly available.Our code, models, and dataset are available publicly at:",
    "checked": true,
    "id": "dd7b2817d54c20e6c0bf854b3463c17bb18014a0",
    "semantic_title": "semgrasp: semantic grasp generation via language aligned discretization",
    "citation_count": 6,
    "authors": [
      "Kailin Li*",
      "Jingbo Wang",
      "Lixin Yang",
      "Cewu Lu*",
      "Bo Dai"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/194_ECCV_2024_paper.php": {
    "title": "MANIKIN: Biomechanically Accurate Neural Inverse Kinematics for Human Motion Estimation",
    "volume": "main",
    "abstract": "Mixed Reality systems aim to estimate a user's full-body joint configurations from just the pose of the end effectors, primarily head and hand poses. Existing methods often involve solving inverse kinematics (IK) to obtain the full skeleton from just these sparse observations, usually directly optimizing the joint angle parameters of a human skeleton. Since this accumulates error through the kinematic tree, predicted end effector poses fail to align with the provided input pose. This leads to discrepancies between the predicted and the actual hand positions or feet that penetrate the ground. In this paper, we first refine the commonly used SMPL parametric model by embedding anatomical constraints that reduce the degrees of freedom for specific parameters to more closely mirror human biomechanics. This ensures that our model produces physically plausible pose predictions. We then propose a biomechanically accurate neural inverse kinematics solver () for full-body motion tracking. is based on swivel angle prediction and perfectly matches input poses while avoiding ground penetration. We evaluate in extensive experiments on motion capture datasets and demonstrate that our method surpasses the state of the art in quantitative and qualitative results at fast inference speed",
    "checked": true,
    "id": "5bc6dd6f25e5635b77a609e5a63b70384c6318da",
    "semantic_title": "manikin: biomechanically accurate neural inverse kinematics for human motion estimation",
    "citation_count": 1,
    "authors": [
      "Jiaxi Jiang*",
      "Paul Streli",
      "Xuejing Luo",
      "Christoph Gebhardt",
      "Christian Holz"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/195_ECCV_2024_paper.php": {
    "title": "Simple Unsupervised Knowledge Distillation With Space Similarity",
    "volume": "main",
    "abstract": "As per recent studies, Self-supervised learning (SSL) does not readily extend to smaller architectures. One direction to mitigate this shortcoming while simultaneously training a smaller network without labels is to adopt unsupervised knowledge distillation (UKD). Existing UKD approaches handcraft preservation worthy inter/intra sample relationships between the teacher and its student. However, this may overlook/ignore other key relationships present in the mapping of a teacher. In this paper, instead of heuristically constructing preservation worthy relationships between samples, we directly motivate the student to model the teacher's embedding manifold. If the mapped manifold is similar, all inter/intra sample relationships are indirectly conserved. We first demonstrate that prior methods cannot preserve teacher's latent manifold due to their sole reliance on L2 normalised embedding features. Subsequently, we propose a simple objective to capture the lost information due to normalisation. Our proposed loss component, termed space similarity, motivates each dimension of a student's feature space to be similar to the corresponding dimension of its teacher. We perform extensive experiments demonstrating strong performance of our proposed approach on various benchmarks",
    "checked": true,
    "id": "91e72a77b65449fde2baacbaff566117eceb5208",
    "semantic_title": "simple unsupervised knowledge distillation with space similarity",
    "citation_count": 0,
    "authors": [
      "Aditya Singh*",
      "Haohan Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/201_ECCV_2024_paper.php": {
    "title": "DragAPart: Learning a Part-Level Motion Prior for Articulated Objects",
    "volume": "main",
    "abstract": "We introduce , a method that, given an image and a set of drags as input, generates a new image of the same object that responds to the action of the drags. Differently from prior works that focused on repositioning objects, predicts part-level interactions, such as opening and closing a drawer. We study this problem as a proxy for learning a generalist motion model, not restricted to a specific kinematic structure or object category. We start from a pre-trained image generator and fine-tune it on a new synthetic dataset, , which we introduce. Combined with a new encoding for the drags and dataset randomization, the model generalizes well to real images and different categories. Compared to prior motion-controlled generators, we demonstrate much better part-level motion understanding",
    "checked": true,
    "id": "c91b2c65da5e50df129b877da57ec07bb7f9c363",
    "semantic_title": "dragapart: learning a part-level motion prior for articulated objects",
    "citation_count": 7,
    "authors": [
      "Ruining Li*",
      "Chuanxia Zheng",
      "Christian Rupprecht",
      "Andrea Vedaldi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/203_ECCV_2024_paper.php": {
    "title": "Diffusion Bridges for 3D Point Cloud Denoising",
    "volume": "main",
    "abstract": "In this work, we address the task of point cloud denoising using a novel framework adapting Diffusion Schrödinger bridges to unstructured data like point sets. Unlike previous works that predict point-wise displacements from point features or learned noise distributions, our method learns an optimal transport plan between paired point clouds. In experiments on object datasets such as the PU-Net dataset and real-world datasets like ScanNet++ and ARKitScenes, improves by a notable margin over existing methods. Although our method demonstrates promising results utilizing solely point coordinates, we demonstrate that incorporating additional features like RGB information and point-wise DINOV2 features further improves the results.Code and pretrained networks are available at https://github.com/matvogel/P2P-Bridge",
    "checked": false,
    "id": "6e872cdad76830c2dbc67696cd82563010d8f09b",
    "semantic_title": "p2p-bridge: diffusion bridges for 3d point cloud denoising",
    "citation_count": 0,
    "authors": [
      "Mathias Vogel Hüni",
      "Keisuke Tateno",
      "Marc Pollefeys",
      "Federico Tombari",
      "Marie-Julie Rakotosaona",
      "Francis Engelmann*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/206_ECCV_2024_paper.php": {
    "title": "Optimizing Illuminant Estimation in Dual-Exposure HDR Imaging",
    "volume": "main",
    "abstract": "High dynamic range (HDR) imaging involves capturing a series of frames of the same scene, each with different exposure settings, to broaden the dynamic range of light. This can be achieved through burst capturing or using staggered HDR sensors that capture long and short exposures simultaneously in the camera image signal processor (ISP). Within camera ISP pipeline, illuminant estimation is a crucial step aiming to estimate the color of the global illuminant in the scene. This estimation is used in camera ISP white-balance module to remove undesirable color cast in the final image. Despite the multiple frames captured in the HDR pipeline, conventional illuminant estimation methods often rely only on a single frame of the scene. In this paper, we explore leveraging information from frames captured with different exposure times. Specifically, we introduce a simple feature extracted from dual-exposure images to guide illuminant estimators, referred to as the dual-exposure feature (DEF). To validate the efficiency of DEF, we employed two illuminant estimators using the proposed DEF: 1) a multilayer perceptron network (MLP), referred to as exposure-based MLP (EMLP), and 2) a modified version of the convolutional color constancy (CCC) to integrate our DEF, that we call ECCC. Both EMLP and ECCC achieve promising results, in some cases surpassing prior methods that require hundreds of thousands or millions of parameters, with only a few hundred parameters for EMLP and a few thousand parameters for ECCC",
    "checked": true,
    "id": "7416437f4b15f4ee6ec175074eb5954b6895f48a",
    "semantic_title": "optimizing illuminant estimation in dual-exposure hdr imaging",
    "citation_count": 0,
    "authors": [
      "Mahmoud Afifi*",
      "Zhenhua Hu",
      "Liang Liang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/212_ECCV_2024_paper.php": {
    "title": "BAM-DETR: Boundary-Aligned Moment Detection Transformer for Temporal Sentence Grounding in Videos",
    "volume": "main",
    "abstract": "Temporal sentence grounding aims to localize moments relevant to a language description. Recently, DETR-like approaches achieved notable progress by predicting the center and length of a target moment. However, they suffer from the issue of center misalignment raised by the inherent ambiguity of moment centers, leading to inaccurate predictions. To remedy this problem, we propose a novel boundary-oriented moment formulation. In our paradigm, the model no longer needs to find the precise center but instead suffices to predict any anchor point within the interval, from which the boundaries are directly estimated. Based on this idea, we design a boundary-aligned moment detection transformer, equipped with a dual-pathway decoding process. Specifically, it refines the anchor and boundaries within parallel pathways using global and boundary-focused attention, respectively. This separate design allows the model to focus on desirable regions, enabling precise refinement of moment predictions. Further, we propose a quality-based ranking method, ensuring that proposals with high localization qualities are prioritized over incomplete ones. Experiments on three benchmarks validate the effectiveness of the proposed methods. The code is available here",
    "checked": true,
    "id": "04f791d5d4676d096fafe9ea952aa7f2ab1c5d15",
    "semantic_title": "bam-detr: boundary-aligned moment detection transformer for temporal sentence grounding in videos",
    "citation_count": 2,
    "authors": [
      "Pilhyeon Lee*",
      "Hyeran Byun"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/223_ECCV_2024_paper.php": {
    "title": "MarineInst: A Foundation Model for Marine Image Analysis with Instance Visual Description",
    "volume": "main",
    "abstract": "Recent foundation models trained on a tremendous scale of data have shown great promise in a wide range of computer vision tasks and application domains. However, less attention has been paid to the marine realms, which in contrast cover the majority of our blue planet. The scarcity of labeled data is the most hindering issue, and marine photographs illustrate significantly different appearances and contents from general in-air images. Using existing foundation models for marine visual analysis does not yield satisfactory performance, due to not only the data distribution shift, but also the intrinsic limitations of the existing foundation models (, lacking semantics, redundant mask generation, or restricted to image-level scene understanding). In this work, we emphasize both model and data approaches for understanding marine ecosystems. We introduce MarineInst, a foundation model for the analysis of the marine realms with instance visual description, which outputs instance masks and captions for marine object instances. To train MarineInst, we acquire MarineInst20M, the largest marine image dataset to date, which contains a wide spectrum of marine images with high-quality semantic instance masks constructed by a mixture of human-annotated instance masks and model-generated instance masks from our automatic procedure of binary instance filtering. To generate informative and detailed semantic instance captions, we use vision-language models to produce semantic richness with various granularities. Our model and dataset support a wide range of marine visual analysis tasks, from image-level scene understanding to regional mask-level instance understanding. More significantly, MarineInst exhibits strong generalization ability and flexibility to support a wide range of downstream tasks with state-of-the-art performance as demonstrated in fig:teaser. Project website: https://marineinst.hkustvgd.com",
    "checked": true,
    "id": "1b007dd47051ec83bc20a09809b85c79061bd2d8",
    "semantic_title": "marineinst: a foundation model for marine image analysis with instance visual description",
    "citation_count": 1,
    "authors": [
      "Ziqiang Zheng*",
      "Yiwei Chen",
      "Huimin Zeng",
      "Tuan-Anh Vu",
      "Binh-Son Hua",
      "Sai-Kit Yeung"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/234_ECCV_2024_paper.php": {
    "title": "Superpixel-informed Implicit Neural Representation for Multi-Dimensional Data",
    "volume": "main",
    "abstract": "Recently, implicit neural representations (INRs) have attracted increasing attention for multi-dimensional data recovery. However, INRs simply map coordinates via a multi-layer perceptron (MLP) to corresponding values, ignoring the inherent semantic information of the data. To leverage semantic priors from the data, we propose a novel Superpixel-informed INR (S-INR). Specifically, we suggest utilizing generalized superpixel instead of pixel as an alternative basic unit of INR for multi-dimensional data (e.g., images and weather data). The coordinates of generalized superpixels are first fed into exclusive attention-based MLPs, and then the intermediate results interact with a shared dictionary matrix. The elaborately designed modules in S-INR allow us to ingenuously exploit the semantic information within and across generalized superpixels. Extensive experiments on various applications validate the effectiveness and efficacy of our S-INR compared to state-of-the-art INR methods",
    "checked": true,
    "id": "414e28f1c17c83f62dffa3e7313caee9835fd182",
    "semantic_title": "superpixel-informed implicit neural representation for multi-dimensional data",
    "citation_count": 0,
    "authors": [
      "Jia-Yi Li",
      "Xi-Le Zhao*",
      "Jian-Li Wang",
      "Chao Wang",
      "Min Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/248_ECCV_2024_paper.php": {
    "title": "EgoPoser: Robust Real-Time Egocentric Pose Estimation from Sparse and Intermittent Observations Everywhere",
    "volume": "main",
    "abstract": "Full-body egocentric pose estimation from head and hand poses alone has become an active area of research to power articulate avatar representations on headset-based platforms. However, existing methods over-rely on the indoor motion-capture spaces in which datasets were recorded, while simultaneously assuming continuous joint motion capture and uniform body dimensions. We propose to overcome these limitations with four main contributions. 1) robustly models body pose from intermittent hand position and orientation tracking only when inside a headset's field of view. 2) We rethink input representations for headset-based ego-pose estimation and introduce a novel global motion decomposition method that predicts full-body pose independent of global positions. 3) We enhance pose estimation by capturing longer motion time series through an efficient SlowFast module design that maintains computational efficiency. 4) generalizes across various body shapes for different users. We experimentally evaluate our method and show that it outperforms state-of-the-art methods both qualitatively and quantitatively while maintaining a high inference speed of over 600 fps. establishes a robust baseline for future work where full-body pose estimation no longer needs to rely on outside-in capture and can scale to large-scale and unseen environments",
    "checked": true,
    "id": "914865b7b5ac4edd99bc15a8e8184204837b0342",
    "semantic_title": "egoposer: robust real-time egocentric pose estimation from sparse and intermittent observations everywhere",
    "citation_count": 3,
    "authors": [
      "Jiaxi Jiang*",
      "Paul Streli",
      "Manuel Meier",
      "Christian Holz"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/252_ECCV_2024_paper.php": {
    "title": "Physics-Free Spectrally Multiplexed Photometric Stereo under Unknown Spectral Composition",
    "volume": "main",
    "abstract": "In this paper, we present a groundbreaking spectrally multiplexed photometric stereo approach for recovering surface normals of dynamic surfaces without the need for calibrated lighting or sensors, a notable advancement in the field traditionally hindered by stringent prerequisites and spectral ambiguity. By embracing spectral ambiguity as an advantage, our technique enables the generation of training data without specialized multispectral rendering frameworks. We introduce a unique, physics-free network architecture, SpectraM-PS, that effectively processes multiplexed images to determine surface normals across a wide range of conditions and material types, without relying on specific physically-based knowledge. Additionally, we establish the first benchmark dataset, SpectraM14, for spectrally multiplexed photometric stereo, facilitating comprehensive evaluations against existing calibrated methods. Our contributions significantly enhance the capabilities for dynamic surface recovery, particularly in uncalibrated setups, marking a pivotal step forward in the application of photometric stereo across various domains",
    "checked": true,
    "id": "a6baaa842038d662297ba56fd23ca5da138b452e",
    "semantic_title": "physics-free spectrally multiplexed photometric stereo under unknown spectral composition",
    "citation_count": 0,
    "authors": [
      "Satoshi Ikehata*",
      "Yuta Asano"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/254_ECCV_2024_paper.php": {
    "title": "SplatFields: Neural Gaussian Splats for Sparse 3D and 4D Reconstruction",
    "volume": "main",
    "abstract": "Digitizing 3D static scenes and 4D dynamic events from multi-view images has long been a challenge in computer vision and graphics. Recently, 3D Gaussian Splatting (3DGS) has emerged as a practical and scalable reconstruction method, gaining popularity due to its impressive reconstruction quality, real-time rendering capabilities, and compatibility with widely used visualization tools. However, the method requires a substantial number of input views to achieve high-quality scene reconstruction, introducing a significant practical bottleneck. This challenge is especially severe in capturing dynamic scenes, where deploying an extensive camera array can be prohibitively costly. In this work, we identify the lack of spatial autocorrelation of splat features as one of the factors contributing to the suboptimal performance of the 3DGS technique in sparse reconstruction settings. To address the issue, we propose an optimization strategy that effectively regularizes splat features by modeling them as the outputs of a corresponding implicit neural field. This results in a consistent enhancement of reconstruction quality across various scenarios. Our approach effectively handles static and dynamic cases, as demonstrated by extensive testing across different setups and scene complexities",
    "checked": true,
    "id": "dc16c52e7f78ad6bb5e12467c64e0d2c9b49d6ad",
    "semantic_title": "splatfields: neural gaussian splats for sparse 3d and 4d reconstruction",
    "citation_count": 1,
    "authors": [
      "Marko Mihajlovic*",
      "Sergey Prokudin",
      "Siyu Tang",
      "Robert Maier",
      "Federica Bogo",
      "Tony Tung",
      "Edmond Boyer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/255_ECCV_2024_paper.php": {
    "title": "VFusion3D: Learning Scalable 3D Generative Models from Video Diffusion Models",
    "volume": "main",
    "abstract": "This paper presents a novel method for building scalable 3D generative models utilizing pre-trained video diffusion models. The primary obstacle in developing foundation 3D generative models is the limited availability of 3D data. Unlike images, texts, or videos, 3D data are not readily accessible and are difficult to acquire. This results in a significant disparity in scale compared to the vast quantities of other types of data. To address this issue, we propose using a video diffusion model, trained with extensive volumes of text, images, and videos, as a knowledge source for 3D data. By unlocking its multi-view generative capabilities through fine-tuning, we generate a large-scale synthetic multi-view dataset to train a feed-forward 3D generative model. The proposed model, VFusion3D, trained on nearly 3M synthetic multi-view data, can generate a 3D asset from a single image in seconds and achieves superior performance when compared to current SOTA feed-forward 3D generative models, with users preferring our results over 90% of the time",
    "checked": true,
    "id": "77b3af0bc3b3967529ecea7f72c53941ae4ba357",
    "semantic_title": "vfusion3d: learning scalable 3d generative models from video diffusion models",
    "citation_count": 22,
    "authors": [
      "Junlin Han*",
      "Filippos Kokkinos",
      "Philip Torr"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/259_ECCV_2024_paper.php": {
    "title": "Alignist: CAD-Informed Orientation Distribution Estimation by Fusing Shape and Correspondences",
    "volume": "main",
    "abstract": "Object pose distribution estimation is crucial in robotics for better path planning and handling of symmetric objects. Recent distribution estimation approaches employ contrastive learning-based approaches by maximizing the likelihood of a single pose estimate in the absence of a CAD model. We propose a pose distribution estimation method leveraging symmetry respecting correspondence distributions and shape information obtained using a CAD model. Contrastive learning-based approaches require an exhaustive amount of training images from different viewpoints to learn the distribution properly, which is not possible in realistic scenarios. Instead, we propose a pipeline that can leverage correspondence distributions and shape information from the CAD model, which are later used to learn pose distributions. Besides, having access to pose distribution based on correspondences before learning pose distributions conditioned on images, can help formulate the loss between distributions. The prior knowledge of distribution also helps the network to focus on getting sharper modes instead. With the CAD prior, our approach converges much faster and learns distribution better by focusing on learning sharper distribution near all the valid modes, unlike contrastive approaches, which focus on a single mode at a time. We achieve benchmark results on SYMSOL-I and T-Less datasets",
    "checked": true,
    "id": "63dbed46eba424c8f1d4325ecccedd9613601195",
    "semantic_title": "alignist: cad-informed orientation distribution estimation by fusing shape and correspondences",
    "citation_count": 0,
    "authors": [
      "Shishir Reddy Vutukur*",
      "Junwen Huang",
      "Rasmus Laurvig Haugaard",
      "Benjamin Busam",
      "Tolga Birdal"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/261_ECCV_2024_paper.php": {
    "title": "Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs",
    "volume": "main",
    "abstract": "Prompt ensembling of Large Language Model (LLM) generated category-specific prompts has emerged as an effective method to enhance zero-shot recognition ability of Vision-Language Models (VLMs). To obtain these category-specific prompts, the present methods rely on hand-crafting the prompts to the LLMs for generating VLM prompts for the downstream tasks. However, this requires manually composing these task-specific prompts and still, they might not cover the diverse set of visual concepts and task-specific styles associated with the categories of interest. To effectively take humans out of the loop and completely automate the prompt generation process for zero-shot recognition, we propose Meta-Prompting for Visual Recognition (). Taking as input only minimal information about the target task, in the form of its short natural language description, and a list of associated class labels, automatically produces a diverse set of category-specific prompts resulting in a strong zero-shot classifier. generalizes effectively across various popular zero-shot image recognition benchmarks belonging to widely different domains when tested with multiple LLMs and VLMs. For example, obtains a zero-shot recognition improvement over CLIP by up to 19.8% and 18.2% (5.0% and 4.5% on average over 20 datasets) leveraging GPT and Mixtral LLMs, respectively",
    "checked": true,
    "id": "08f11c8f4ba6f10b4df26ed4376df4f2f3106da9",
    "semantic_title": "meta-prompting for automating zero-shot visual recognition with llms",
    "citation_count": 6,
    "authors": [
      "Muhammad Jehanzeb Mirza*",
      "Leonid Karlinsky",
      "Wei Lin",
      "Sivan Doveh",
      "Jakub Micorek",
      "Mateusz Kozinski",
      "Hilde Kuehne",
      "Horst Possegger"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/270_ECCV_2024_paper.php": {
    "title": "Physics-Based Interaction with 3D Objects via Video Generation",
    "volume": "main",
    "abstract": "Realistic object interactions are crucial for creating immersive virtual experiences, yet synthesizing realistic 3D object dynamics in response to novel interactions remains a significant challenge. Unlike unconditional or text-conditioned dynamics generation, action-conditioned dynamics requires perceiving the physical material properties of objects and grounding the 3D motion prediction on these properties, such as object stiffness. However, estimating physical material properties is an open problem due to the lack of material ground-truth data, as measuring these properties for real objects is highly difficult. We present , a physics-based approach that endows static 3D objects with interactive dynamics by leveraging the object dynamics priors learned by video generation models. By distilling these priors, enables the synthesis of realistic object responses to novel interactions, such as external forces or agent manipulations. We demonstrate our approach on diverse examples of elastic objects and evaluate the realism of the synthesized interactions through a user study. takes a step towards more engaging and realistic virtual experiences by enabling static 3D objects to dynamically respond to interactive stimuli in a physically plausible manner. See our project page at https://physdreamer.github.io/",
    "checked": false,
    "id": "280f4625876c1d32d803e2e5d73660665740048c",
    "semantic_title": "physdreamer: physics-based interaction with 3d objects via video generation",
    "citation_count": 15,
    "authors": [
      "Tianyuan Zhang*",
      "Hong-Xing Yu",
      "Rundi Wu",
      "Brandon Y Feng",
      "Changxi Zheng",
      "Noah Snavely",
      "Jiajun Wu",
      "William T. Freeman"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/271_ECCV_2024_paper.php": {
    "title": "Reconstruction and Simulation of Elastic Objects with Spring-Mass 3D Gaussians",
    "volume": "main",
    "abstract": "Reconstructing and simulating elastic objects from visual observations is crucial for applications in computer vision and robotics. Existing methods, such as 3D Gaussians, model 3D appearance and geometry, but lack the ability to estimate physical properties for objects and simulate them. The core challenge lies in integrating an expressive yet efficient physical dynamics model. We propose , a 3D physical object representation for reconstructing and simulating elastic objects from videos of the object from multiple viewpoints. In particular, we develop and integrate a 3D Spring-Mass model into 3D Gaussian kernels, enabling the reconstruction of the visual appearance, shape, and physical dynamics of the object. Our approach enables future prediction and simulation under various initial states and environmental properties. We evaluate on both synthetic and real-world datasets, demonstrating accurate reconstruction and simulation of elastic objects. Project page:",
    "checked": true,
    "id": "8056aa94116869410459b813169a57e50f04ff8b",
    "semantic_title": "reconstruction and simulation of elastic objects with spring-mass 3d gaussians",
    "citation_count": 8,
    "authors": [
      "Licheng Zhong",
      "Hong-Xing Yu",
      "Jiajun Wu",
      "Yunzhu Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/272_ECCV_2024_paper.php": {
    "title": "Deep Patch Visual SLAM",
    "volume": "main",
    "abstract": "Recent work in Visual Odometry and SLAM has shown the effectiveness of using deep network backbones. Despite excellent accuracy, such approaches are often expensive to run or do not generalize well zero-shot. To address this problem, we introduce Deep Patch Visual-SLAM, a new system for monocular visual SLAM based on the DPVO visual odometry system. We introduce two loop closure mechanisms which significantly improve the accuracy with minimal runtime and memory overhead. On real-world datasets, DPV-SLAM runs at 1x-3x real-time framerates. We achieve comparable accuracy to DROID-SLAM on EuRoC and TartanAir while running twice as fast using a third of the VRAM. We also outperform DROID-SLAM by large margins on KITTI. As DPV-SLAM is an extension to DPVO, its code can be found in the same repository: https: //github.com/princeton-vl/DPVO",
    "checked": true,
    "id": "f55f8b26e95e9a1847b3c144cfa334d955bf1774",
    "semantic_title": "deep patch visual slam",
    "citation_count": 1,
    "authors": [
      "Lahav Lipson*",
      "Zachary Teed",
      "Jia Deng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/274_ECCV_2024_paper.php": {
    "title": "Surface Reconstruction for 3D Gaussian Splatting via Local Structural Hints",
    "volume": "main",
    "abstract": "This paper presents a novel approach for surface mesh reconstruction from 3D Gaussian Splatting (3DGS) [?], a technique renowned for its efficiency in novel view synthesis but challenged for surface reconstruction. The key obstacle is the lack of geometry hints to regulate the optimization of millions of unorganized Gaussian blobs to align to the true surface. This paper introduces local structural hints during training to address the challenge. We first leverage the prior knowledge from monocular normal and depth estimations to refine the covariance and mean of Gaussian primitives, enhancing their organization and providing crucial normal information for surface extraction. However, due to the highly discrete nature of Gaussian primitives, such geometry guidance remains insufficient for the alignment with the true surface. We then propose to construct a signed distance field by a moving least square (MLS) function over the Gaussians in each local region. More importantly, we further propose to jointly learn a neural implicit network to mimic and regularize the MLS function. The joint optimization helps the optimization of Gaussian Splatting towards accurate surface alignment. Extensive experimental results demonstrate the effectiveness of our method in achieving superior mesh quality compared with the SoTA surface reconstruction for 3DGS. More resources can be found on our project page: https://qianyiwu.github.io/gsrec",
    "checked": false,
    "id": "059b5ecd5d7b0a93854b1472b8acb78bd5aa36f8",
    "semantic_title": "surface reconstruction from 3d gaussian splatting via local structural hints",
    "citation_count": 0,
    "authors": [
      "Qianyi Wu*",
      "Jianmin Zheng",
      "Jianfei Cai"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/280_ECCV_2024_paper.php": {
    "title": "HeadGaS: Real-Time Animatable Head Avatars via 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "3D head animation has seen major quality and runtime improvements over the last few years, particularly empowered by the advances in differentiable rendering and neural radiance fields. Real-time rendering is a highly desirable goal for real-world applications. We propose HeadGaS, a model that uses 3D Gaussian Splats (3DGS) for 3D head reconstruction and animation. In this paper we introduce a hybrid model that extends the explicit 3DGS representation with a base of learnable latent features, which can be linearly blended with low-dimensional parameters from parametric head models to obtain expression-dependent color and opacity values. We demonstrate that HeadGaS delivers state-of-the-art results in real-time inference frame rates, surpassing baselines by up to 2 dB, while accelerating rendering speed by over ×10",
    "checked": true,
    "id": "22492ecd9881bcd190f7500e1ccabb9ee2584afe",
    "semantic_title": "headgas: real-time animatable head avatars via 3d gaussian splatting",
    "citation_count": 18,
    "authors": [
      "Helisa Dhamo*",
      "Yinyu Nie",
      "Arthur Moreau",
      "Jifei Song",
      "Richard Shaw",
      "Yiren Zhou",
      "Eduardo Pérez-Pellitero*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/289_ECCV_2024_paper.php": {
    "title": "LayeredFlow: A Real-World Benchmark for Non-Lambertian Multi-Layer Optical Flow",
    "volume": "main",
    "abstract": "Achieving 3D understanding of non-Lambertian objects is an important task with many useful applications, but most existing algorithms struggle to deal with such objects. One major obstacle towards progress in this field is the lack of holistic non-Lambertian benchmarks—most benchmarks have low scene and object diversity, and none provide multi-layer 3D annotations for objects occluded by transparent surfaces. In this paper, we introduce , a real world benchmark containing multi-layer ground truth annotation for optical flow of non-Lambertian objects. Compared to previous benchmarks, our benchmark exhibits greater scene and object diversity, with 150k high quality optical flow and stereo pairs taken over 185 indoor and outdoor scenes and 360 unique objects. Using as evaluation data, we propose a new task called multi-layer optical flow. To provide training data for this task, we introduce a large-scale densely-annotated synthetic dataset containing 60k images within 30 scenes tailored for non-Lambertian objects. Training on our synthetic dataset enables model to predict multi-layer optical flow, while fine-tuning existing optical flow methods on the dataset notably boosts their performance on non-Lambertian objects without compromising the performance on diffuse objects",
    "checked": true,
    "id": "15d6dff07cccefd7fb3246d5dab55d2f7c8715f8",
    "semantic_title": "layeredflow: a real-world benchmark for non-lambertian multi-layer optical flow",
    "citation_count": 0,
    "authors": [
      "Hongyu Wen*",
      "Erich Liang",
      "Jia Deng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/294_ECCV_2024_paper.php": {
    "title": "Learning 3D Geometry and Feature Consistent Gaussian Splatting for Object Removal",
    "volume": "main",
    "abstract": "This paper tackles the intricate challenge of object removal to update the radiance field using the 3D Gaussian Splatting. The main challenges of this task lie in the preservation of geometric consistency and the maintenance of texture coherence in the presence of the substantial discrete nature of Gaussian primitives. We introduce a robust framework specifically designed to overcome these obstacles. The key insight of our approach is the enhancement of information exchange among visible and invisible areas, facilitating content restoration in terms of both geometry and texture. Our methodology begins with optimizing the positioning of Gaussian primitives to improve geometric consistency across both removed and visible areas, guided by an online registration process informed by monocular depth estimation. Following this, we employ a novel feature propagation mechanism to bolster texture coherence, leveraging a cross-attention design that bridges sampling Gaussians from both uncertain and certain areas. This innovative approach significantly refines the texture coherence within the final radiance field. Extensive experiments validate that our method not only elevates the quality of novel view synthesis for scenes undergoing object removal but also showcases notable efficiency gains in training and rendering speeds. Project Page: https://w-ted.github.io/publications/gscream",
    "checked": false,
    "id": "07a056ab69ac80d9ac35b7d4004a052592a30c06",
    "semantic_title": "gscream: learning 3d geometry and feature consistent gaussian splatting for object removal",
    "citation_count": 3,
    "authors": [
      "Yuxin Wang",
      "Qianyi Wu",
      "Guofeng Zhang",
      "Dan Xu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/304_ECCV_2024_paper.php": {
    "title": "Motion-prior Contrast Maximization for Dense Continuous-Time Motion Estimation",
    "volume": "main",
    "abstract": "Current optical flow and point-tracking methods rely heavily on synthetic datasets. Event cameras are novel vision sensors with advantages in challenging visual conditions, but state-of-the-art frame-based methods cannot be easily adapted to event data due to the limitations of current event simulators. We introduce a novel self-supervised loss combining the Contrast Maximization framework with a non-linear motion prior in the form of pixel-level trajectories and propose an efficient solution to solve the high-dimensional assignment problem between non-linear trajectories and events. Their effectiveness is demonstrated in two scenarios: In dense continuous-time motion estimation, our method improves the zero-shot performance of a synthetically trained model on the real-world dataset EVIMO2 by 29%. In optical flow estimation, our method elevates a simple UNet to achieve state-of-the-art performance among self-supervised methods on the DSEC optical flow benchmark. Our code is available at https: //github.com/tub-rip/MotionPriorCMax",
    "checked": true,
    "id": "283f25f2d7437cb9749025402c3d57ceff9ddbe0",
    "semantic_title": "motion-prior contrast maximization for dense continuous-time motion estimation",
    "citation_count": 1,
    "authors": [
      "Friedhelm Hamann*",
      "Ziyun Wang",
      "Ioannis Asmanis",
      "Kenneth Chaney",
      "Guillermo Gallego",
      "Kostas Daniilidis"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/305_ECCV_2024_paper.php": {
    "title": "Efficient Few-Shot Action Recognition via Multi-Level Post-Reasoning",
    "volume": "main",
    "abstract": "The integration with CLIP (Contrastive Vision-Language Pre-training) has significantly refreshed the accuracy leaderboard of FSAR (Few-Shot Action Recognition). However, the trainable overhead of ensuring that the domain alignment of CLIP and FSAR is often unbearable. To mitigate this issue, we present an Efficient Multi-Level Post-Reasoning Network, namely EMP-Net. By design, a post-reasoning mechanism is proposed for domain adaptation, which avoids most gradient backpropagation, improving the efficiency; meanwhile, a multi-level representation is utilised during the reasoning and matching processes to improve the discriminability, ensuring effectiveness. Specifically, the proposed EMP-Net starts with a skip-fusion involving cached multi-stage features extracted by CLIP. After that, the fused feature is decoupled into multi-level representations, including global-level, patch-level, and frame-level. The ensuing spatiotemporal reasoning module operates on multi-level representations to generate discriminative features. As for matching, the contrasts between text-visual and support-query are integrated to provide comprehensive guidance. The experimental results demonstrate that EMP-Net can unlock the potential performance of CLIP in a more efficient manner. The code and supplementary material can be found at https://github.com/cong-wu/EMP-Net",
    "checked": false,
    "id": "514f746b92c2b038906806f94cc6a0e4f11e278f",
    "semantic_title": "hierarchical reasoning network with contrastive learning for few-shot human-object interaction recognition",
    "citation_count": 2,
    "authors": [
      "Cong Wu",
      "Xiao-Jun Wu*",
      "Linze Li",
      "Tianyang Xu",
      "Zhenhua Feng",
      "Josef Kittler"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/308_ECCV_2024_paper.php": {
    "title": "Text2Place: Affordance-aware Text Guided Human Placement",
    "volume": "main",
    "abstract": "For a given scene, humans can easily reason for the locations and pose to place objects. Designing a computational model to reason about these affordances poses a significant challenge, mirroring the intuitive reasoning abilities of humans. This work tackles the problem of realistic human insertion in a given background scene termed as Semantic Human Placement. This task is extremely challenging given the diverse backgrounds, scale, and pose of the generated person and, finally, the identity preservation of the person. We divide the problem into the following two stages i) learning semantic masks using text guidance for localizing regions in the image to place humans and ii) subject-conditioned inpainting to place a given subject adhering to the scene affordance within the semantic masks. For learning semantic masks, we leverage rich object-scene priors learned from the text-to-image generative models and optimize a novel parameterization of the semantic mask, eliminating the need for large-scale training. To the best of our knowledge, we are the first ones to provide an effective solution for realistic human placements in diverse real-world scenes. The proposed method can generate highly realistic scene compositions while preserving the background and subject identity. Further, we present results for several downstream tasks - scene hallucination from a single or multiple generated persons and text-based attribute editing. With extensive comparisons against strong baselines, we show the superiority of our method in realistic human placement",
    "checked": true,
    "id": "09cb8b8800749c266d596b21f089aa9df52a0adf",
    "semantic_title": "text2place: affordance-aware text guided human placement",
    "citation_count": 0,
    "authors": [
      "Rishubh Parihar*",
      "Harsh Gupta",
      "Sachidanand VS",
      "Venkatesh Babu RADHAKRISHNAN"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/319_ECCV_2024_paper.php": {
    "title": "OGNI-DC: Robust Depth Completion with Optimization-Guided Neural Iterations",
    "volume": "main",
    "abstract": "Depth completion is the task of generating a dense depth map given an image and a sparse depth map as inputs. In this paper, we present OGNI-DC, a novel framework for depth completion. The key to our method is \"Optimization-Guided Neural Iterations\" (OGNI). It consists of a recurrent unit that refines a depth gradient field and a differentiable depth integrator that integrates the depth gradients into a depth map. OGNI-DC exhibits strong generalization, outperforming baselines by a large margin on unseen datasets and across various sparsity levels. Moreover, OGNI-DC has high accuracy, achieving state-of-the-art performance on the NYUv2 and the KITTI benchmarks. Code is available at https:// github.com/princeton-vl/OGNI-DC",
    "checked": true,
    "id": "19260f01e6db7f4d50fae9a08b02c3c9c9e748a3",
    "semantic_title": "ogni-dc: robust depth completion with optimization-guided neural iterations",
    "citation_count": 1,
    "authors": [
      "Yiming Zuo*",
      "Jia Deng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/324_ECCV_2024_paper.php": {
    "title": "Zero-Shot Multi-Object Scene Completion",
    "volume": "main",
    "abstract": "We present a 3D scene completion method that recovers the complete geometry of multiple unseen objects in complex scenes from a single RGB-D image. Despite notable advancements in single-object 3D shape completion, high-quality reconstructions in highly cluttered real-world multi-object scenes remains a challenge. To address this issue, we propose OctMAE, an architecture that leverages an Octree U-Net and a latent 3D MAE to achieve high-quality and near real-time multi-object scene completion through both local and global geometric reasoning. Because a naive 3D MAE can be computationally intractable and memory intensive even in the latent space, we introduce a novel occlusion masking strategy and adopt 3D rotary embeddings, which significantly improve the runtime and scene completion quality. To generalize to a wide range of objects in diverse scenes, we create a large-scale photorealistic dataset, featuring a diverse set of 12K 3D object models from the Objaverse dataset that are rendered in multi-object scenes with physics-based positioning. Our method outperforms the current state-of-the-art on both synthetic and real-world datasets and demonstrates a strong zero-shot capability. https://sh8.io/#/oct_mae",
    "checked": true,
    "id": "6f8f62c3034c41a4f92173cddb31db19964f224b",
    "semantic_title": "zero-shot multi-object scene completion",
    "citation_count": 0,
    "authors": [
      "Shun Iwase*",
      "Katherine Liu",
      "Vitor Guizilini",
      "Adrien Gaidon",
      "Kris Kitani",
      "Rareș A Ambruș",
      "Sergey Zakharov"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/328_ECCV_2024_paper.php": {
    "title": "Beta-Tuned Timestep Diffusion Model",
    "volume": "main",
    "abstract": "Diffusion models have received a lot of attention in the field of generation due to their ability to produce high-quality samples. However, several recent studies indicate that treating all distributions equally in diffusion model training is sub-optimal. In this paper, we conduct an in-depth theoretical analysis of the forward process of diffusion models. Our findings reveal that the distribution variations are non-uniform throughout the diffusion process and the most drastic variations in distribution occur in the initial stages. Consequently, simple uniform timestep sampling strategy fail to align with these properties, potentially leading to sub-optimal training of diffusion models. To address this, we propose the Beta-Tuned Timestep Diffusion Model (B-TTDM), which devises a timestep sampling strategy based on the beta distribution. By choosing the correct parameters, B-TTDM aligns the timestep sampling distribution with the properties of the forward diffusion process. Extensive experiments on different benchmark datasets validate the effectiveness of B-TTDM",
    "checked": false,
    "id": "f4cdc3d3651878de407d95aff4a530ab3bb390f4",
    "semantic_title": "memory-efficient fine-tuning for quantized diffusion model",
    "citation_count": 2,
    "authors": [
      "Tianyi Zheng*",
      "Peng-Tao Jiang",
      "Ben Wan",
      "Hao Zhang",
      "Jinwei Chen",
      "Jia Wang*",
      "Bo Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/333_ECCV_2024_paper.php": {
    "title": "POA: Pre-training Once for Models of All Sizes",
    "volume": "main",
    "abstract": "Large-scale self-supervised pre-training has paved the way for one foundation model to handle many different vision tasks. Most pre-training methodologies train a single model of a certain size at one time. Nevertheless, various computation or storage constraints in real-world scenarios require substantial efforts to develop a series of models with different sizes to deploy. Thus, in this study, we propose a novel tri-branch self-supervised training framework, termed as POA (Pre-training Once for All), to tackle this aforementioned issue. Our approach introduces an innovative elastic student branch into a modern self-distillation paradigm. At each pre-training step, we randomly sample a sub-network from the original student to form the elastic student and train all branches in a self-distilling fashion. Once pre-trained, POA allows the extraction of pre-trained models of diverse sizes for downstream tasks. Remarkably, the elastic student facilitates the simultaneous pre-training of multiple models with different sizes, which also acts as an additional ensemble of models of various sizes to enhance representation learning. Extensive experiments, including k-nearest neighbors, linear probing evaluation and assessments on multiple downstream tasks demonstrate the effectiveness and advantages of our POA. It achieves state-of-the-art performance using ViT, Swin Transformer and ResNet backbones, producing around a hundred models with different sizes through a single pre-training session. The code is available at: https://github.com/Qichuzyy/POA",
    "checked": true,
    "id": "cf61b64c37ed553e10000c06e4a9e16863dd0a25",
    "semantic_title": "poa: pre-training once for models of all sizes",
    "citation_count": 0,
    "authors": [
      "Yingying Zhang*",
      "Xin Guo",
      "Jiangwei Lao",
      "Lei Yu",
      "Lixiang Ru",
      "Jian Wang",
      "Guo Ye",
      "HUIMEI HE",
      "Jingdong Chen",
      "Ming Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/354_ECCV_2024_paper.php": {
    "title": "Taming Latent Diffusion Model for Neural Radiance Field Inpainting",
    "volume": "main",
    "abstract": "Neural Radiance Field (NeRF) is a representation for 3D reconstruction from multi-view images. Despite some recent work showing preliminary success in editing a reconstructed NeRF with diffusion prior, they remain struggling to synthesize reasonable geometry in completely uncovered regions. One major reason is the high diversity of synthetic contents from the diffusion model, which hinders the radiance field from converging to a crisp and deterministic geometry. Moreover, applying latent diffusion models on real data often yields a textural shift incoherent to the image condition due to auto-encoding errors. These two problems are further reinforced with the use of pixel-distance losses. To address these issues, we propose tempering the diffusion model's stochasticity with per-scene customization and mitigating the textural shift with masked adversarial training. During the analyses, we also found the commonly used pixel and perceptual losses are harmful in the NeRF inpainting task. Through rigorous experiments, our framework yields state-of-the-art NeRF inpainting results on various real-world scenes",
    "checked": true,
    "id": "6f866b79c54da19f7573cce186831796df2e0953",
    "semantic_title": "taming latent diffusion model for neural radiance field inpainting",
    "citation_count": 2,
    "authors": [
      "Chieh Hubert Lin*",
      "Changil Kim",
      "Jia-Bin Huang",
      "Qinbo Li",
      "Chih-Yao Ma",
      "Johannes Kopf",
      "Ming-Hsuan Yang",
      "Hung-Yu Tseng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/358_ECCV_2024_paper.php": {
    "title": "MapDistill: Boosting Efficient Camera-based HD Map Construction via Camera-LiDAR Fusion Model Distillation",
    "volume": "main",
    "abstract": "Online high-definition (HD) map construction is an important and challenging task in autonomous driving. Recently, there has been a growing interest in cost-effective multi-view camera-based methods without relying on other sensors like LiDAR. However, these methods suffer from a lack of explicit depth information, necessitating the use of large models to achieve satisfactory performance. To address this, we employ the Knowledge Distillation (KD) idea for efficient HD map construction for the first time and introduce a novel KD-based approach called MapDistill to transfer knowledge from a high-performance camera-LiDAR fusion model to a lightweight camera-only model. Specifically, we adopt the teacher-student architecture, , a camera-LiDAR fusion model as the teacher and a lightweight camera model as the student, and devise a dual BEV transform module to facilitate cross-modal knowledge distillation while maintaining cost-effective camera-only deployment. Additionally, we present a comprehensive distillation scheme encompassing cross-modal relation distillation, dual-level feature distillation, and map head distillation. This approach alleviates knowledge transfer challenges between modalities, enabling the student model to learn improved feature representations for HD map construction. Experimental results on the challenging nuScenes dataset demonstrate the effectiveness of MapDistill, surpassing existing competitors by over 7.7 mAP or 4.5× speedup",
    "checked": true,
    "id": "986c0c00583bb724a265bd107e0fc54704bf3982",
    "semantic_title": "mapdistill: boosting efficient camera-based hd map construction via camera-lidar fusion model distillation",
    "citation_count": 2,
    "authors": [
      "Xiaoshuai Hao*",
      "Ruikai Li",
      "Hui Zhang",
      "Rong Yin",
      "Dingzhe Li",
      "Sangil Jung",
      "Seung-In Park",
      "ByungIn Yoo",
      "Haimei Zhao",
      "Jing Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/359_ECCV_2024_paper.php": {
    "title": "ByteEdit: Boost, Comply and Accelerate Generative Image Editing",
    "volume": "main",
    "abstract": "Recent advancements in diffusion-based generative image editing have sparked a profound revolution, reshaping the landscape of image outpainting and inpainting tasks. Despite these strides, the field grapples with inherent challenges, including: i) inferior quality; ii) poor consistency; iii) insufficient instrcution adherence; iv) suboptimal generation efficiency. To address these obstacles, we present ByteEdit, an innovative feedback learning framework meticulously designed to Boost, Comply, and Accelerate Generative Image Editing tasks. ByteEdit seamlessly integrates image reward models dedicated to enhancing aesthetics and image-text alignment, while also introducing a dense, pixel-level reward model tailored to foster coherence in the output. Furthermore, we propose a pioneering adversarial and progressive feedback learning strategy to expedite the model's inference speed. Through extensive large-scale user evaluations, we demonstrate that ByteEdit surpasses leading generative image editing products, including Adobe, Canva, and MeiTu, in both generation quality and consistency. ByteEdit-Outpainting exhibits a remarkable enhancement of 388% and 135% in quality and consistency, respectively, when compared to the baseline model. Experiments also verfied that our acceleration models maintains excellent performance results in terms of quality and consistency",
    "checked": true,
    "id": "63cba94e9d5988736f4b58a01877748fb0ee97cc",
    "semantic_title": "byteedit: boost, comply and accelerate generative image editing",
    "citation_count": 1,
    "authors": [
      "Yuxi Ren",
      "Jie Wu*",
      "Yanzuo Lu",
      "Huafeng Kuang",
      "Xin Xia",
      "Xionghui Wang",
      "Qianqian Wang",
      "Yixing Zhu",
      "Pan Xie",
      "Shiyin Wang",
      "Xuefeng Xiao",
      "Yitong Wang",
      "Min Zheng",
      "Lean FU"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/367_ECCV_2024_paper.php": {
    "title": "ProDepth: Boosting Self-Supervised Multi-Frame Monocular Depth with Probabilistic Fusion",
    "volume": "main",
    "abstract": "Self-supervised multi-frame monocular depth estimation relies on the geometric consistency between successive frames under the assumption of a static scene. However, the presence of moving objects in dynamic scenes introduces inevitable inconsistencies, causing misaligned multi-frame feature matching and misleading self-supervision during training. In this paper, we propose a novel framework called ProDepth, which effectively addresses the mismatch problem caused by dynamic objects using a probabilistic approach. We initially deduce the uncertainty associated with static scene assumption by adopting an auxiliary decoder. This decoder analyzes inconsistencies embedded in the cost volume, inferring the probability of areas being dynamic. We then directly rectify the erroneous cost volume for dynamic areas through a Probabilistic Cost Volume Modulation (PCVM) module. Specifically, we derive probability distributions of depth candidates from both single-frame and multi-frame cues, modulating the cost volume by adaptively fusing those distributions based on the inferred uncertainty. Additionally, we present a self-supervision loss reweighting strategy that not only masks out incorrect supervision with high uncertainty but also mitigates the risks in remaining possible dynamic areas in accordance with the probability. Our proposed method excels over state-of-the-art approaches in all metrics on both Cityscapes and KITTI datasets, and demonstrates superior generalization ability on the Waymo Open dataset",
    "checked": true,
    "id": "734b59d44e30144bfa42807e05ad46ea04313982",
    "semantic_title": "prodepth: boosting self-supervised multi-frame monocular depth with probabilistic fusion",
    "citation_count": 1,
    "authors": [
      "Sungmin Woo*",
      "Wonjoon Lee",
      "Woo Jin Kim",
      "Dogyoon Lee",
      "Sangyoun Lee*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/368_ECCV_2024_paper.php": {
    "title": "High-Resolution and Few-shot View Synthesis from Asymmetric Dual-lens Inputs",
    "volume": "main",
    "abstract": "Novel view synthesis has achieved remarkable quality and efficiency by the paradigm of 3D Gaussian Splatting (3D-GS), but still faces two challenges: 1) significant performance degradation when trained with only few-shot samples due to a lack of geometry constraint, and 2) incapability of rendering at a higher resolution that is beyond the input resolution of training samples. In this paper, we propose Dual-Lens 3D-GS (DL-GS) to achieve high-resolution (HR) and few-shot view synthesis, by leveraging the characteristics of the asymmetric dual-lens system commonly equipped on mobile devices. This kind of system captures the same scene with different focal lengths (i.e., wide-angle and telephoto) under an asymmetric stereo configuration, which naturally provides geometric hints for few-shot training and HR guidance for resolution improvement. Nevertheless, there remain two major technical problems to achieving this goal. First, how to effectively exploit the geometry information from the asymmetric stereo configuration? To this end, we propose a consistency-aware training strategy, which integrates a dual-lens-consistent loss to regularize the 3D-GS optimization. Second, how to make the best use of the dual-lens training samples to effectively improve the resolution of newly synthesized views? To this end, we design a multi-reference-guided refinement module to select proper telephoto and wide-angle guided images from training samples based on the camera pose distances, and then exploit their information for high-frequency detail enhancement. Extensive experiments on simulated and real-captured datasets validate the distinct superiority of our DL-GS over various competitors on the task of HR and few-shot view synthesis. The implementation code is available at https://github.com/XrKang/ DL-GS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruikang Xu",
      "Mingde Yao",
      "Yue Li",
      "Yueyi Zhang",
      "Zhiwei Xiong*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/370_ECCV_2024_paper.php": {
    "title": "Accelerating Image Super-Resolution Networks with Pixel-Level Classification",
    "volume": "main",
    "abstract": "In recent times, the need for effective super-resolution (SR) techniques has surged, especially for large-scale images ranging 2K to 8K resolutions. For DNN-based SISR, decomposing images into overlapping patches is typically necessary due to computational constraints. In such patch-decomposing scheme, one can allocate computational resources differently based on each patch's difficulty to further improve efficiency while maintaining SR performance. However, this approach has a limitation: computational resources is uniformly allocated within a patch, leading to lower efficiency when the patch contain pixels with varying levels of restoration difficulty. To address the issue, we propose the Pixel-level Classifier for Single Image Super-Resolution (PCSR), a novel method designed to distribute computational resources adaptively at the pixel level. A PCSR model comprises a backbone, a pixel-level classifier, and a set of pixel-level upsamplers with varying capacities. The pixel-level classifier assigns each pixel to an appropriate upsampler based on its restoration difficulty, thereby optimizing computational resource usage. Our method allows for performance and computational cost balance during inference without re-training. Our experiments demonstrate PCSR's advantage over existing patch-distributing methods in PSNR-FLOP trade-offs across different backbone models and benchmarks. The code will be available at https://github.com/3587jjh/PCSR",
    "checked": true,
    "id": "dd364a44227f7d17af413f16570d7e257c1a8da2",
    "semantic_title": "accelerating image super-resolution networks with pixel-level classification",
    "citation_count": 0,
    "authors": [
      "Jinho Jeong",
      "Jinwoo Kim",
      "Younghyun Jo",
      "Seon Joo Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/378_ECCV_2024_paper.php": {
    "title": "LASS3D: Language-Assisted Semi-Supervised 3D Semantic Segmentation with Progressive Unreliable Data Exploitation",
    "volume": "main",
    "abstract": "Precisely annotating large-scale 3D datasets for point cloud segmentation is laborious. To alleviate the annotation burden, several semi-supervised 3D segmentation methods have been proposed in literature. However, two issues remain to be tackled: 1) The utilization of large language-vision models (LVM) in semi-supervised 3D semantic segmentation remains under-explored. 2) The unlabeled points with low-confidence predictions are directly discarded by existing methods. Taking these two issues into consideration, we propose a language-assisted semi-supervised 3D semantic segmentation method named LASS3D, which is built upon the commonly used MeanTeacher framework. In LASS3D, we use two off-the-shelf LVM to generate multi-level captions and leverage the images as the bridge to connect the text data and point clouds. Then, a semantic-aware adaptive fusion module is explored in the student branch, where the semantic information encoded in the embeddings of multi-level captions is injected into 3D features by adaptive fusion and then the semantic information in the text-enhanced 3D features is transferred to the teacher branch by knowledge distillation. In addition, a progressive exploitation strategy is explored for the unreliable points in the teacher branch, which can effectively exploit the information encapsulated in unreliable points via negative learning. Experimental results on both outdoor and indoor datasets demonstrate that LASS3D outperforms the comparative methods in most cases",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianan Li*",
      "Qiulei Dong*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/391_ECCV_2024_paper.php": {
    "title": "Contourlet Residual for Prompt Learning Enhanced Infrared Image Super-Resolution",
    "volume": "main",
    "abstract": "Image super-resolution (SR) is a critical technique for enhancing image quality, playing a vital role in image enhancement. While recent advancements, notably transformer-based methods, have advanced the field, infrared image SR remains a formidable challenge. Due to the inherent characteristics of infrared sensors, such as limited resolution, temperature sensitivity, high noise levels, and environmental impacts, existing deep learning methods result in suboptimal enhancement outcomes when applied to infrared images. To address these challenges, we propose a specialized Contourlet residual framework tailored for infrared images to restore and enhance the critical details from the multi-scale and multi-directional infrared spectra decomposition. It precisely captures and amplifies the high-pass subbands of infrared images, such as edge details and texture nuances, which are vital for achieving superior reconstruction quality. Moreover, recognizing the limitations of traditional learning techniques in capturing the inherent characteristics of infrared images, we incorporate a prompt-based learning paradigm. This approach facilitates a more nuanced understanding and targeted optimization process for infrared images by leveraging the semantic comprehension offered by the visual language model. Our approach not only addresses the common pitfalls associated with infrared imaging but also sets a new paradigm for infrared image SR. Extensive experiments demonstrate that our approach obtains superior results, attaining state-of-the-art performance. Project page: https://github.com/hey-it-s-me/CoRPLE",
    "checked": false,
    "id": "e19018549f0216a6d3ff02bd119fba2663815143",
    "semantic_title": "human tooth crack image analysis with multiple deep learning approaches",
    "citation_count": 0,
    "authors": [
      "Xingyuan Li",
      "Jinyuan Liu*",
      "ZHIXIN CHEN",
      "Yang Zou",
      "Long Ma",
      "Xin Fan",
      "Risheng Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/406_ECCV_2024_paper.php": {
    "title": "Click-Gaussian: Interactive Segmentation to Any 3D Gaussians",
    "volume": "main",
    "abstract": "Interactive segmentation of 3D Gaussians opens a great opportunity for real-time manipulation of 3D scenes thanks to the real-time rendering capability of 3D Gaussian Splatting. However, the current methods suffer from time-consuming post-processing to deal with noisy segmentation output. Also, they struggle to provide detailed segmentation, which is important for fine-grained manipulation of 3D scenes. In this study, we propose Click-Gaussian, which learns distinguishable feature fields of two-level granularity, facilitating segmentation without time-consuming post-processing. We delve into challenges stemming from inconsistently learned feature fields resulting from 2D segmentation obtained independently from a 3D scene. 3D segmentation accuracy deteriorates when 2D segmentation results across the views, primary cues for 3D segmentation, are in conflict. To overcome these issues, we propose Global Feature-guided Learning (GFL). GFL constructs the clusters of global feature candidates from noisy 2D segments across the views, which smooths out noises when training the features of 3D Gaussians. Our method runs in 10 ms per click, 15 to 130 times as fast as the previous methods, while also significantly improving segmentation accuracy",
    "checked": true,
    "id": "2d81e6af252b16238b10cf6009514a61ed60d77a",
    "semantic_title": "click-gaussian: interactive segmentation to any 3d gaussians",
    "citation_count": 2,
    "authors": [
      "Seokhun Choi",
      "Hyeonseop Song",
      "Jaechul Kim",
      "Taehyeong Kim*",
      "Hoseok Do*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/412_ECCV_2024_paper.php": {
    "title": "Random Walk on Pixel Manifolds for Anomaly Segmentation of Complex Driving Scenes",
    "volume": "main",
    "abstract": "In anomaly segmentation for complex driving scenes, state-of-the-art approaches utilize anomaly scoring functions to calculate anomaly scores. For these functions, accurately predicting the logits of inlier classes for each pixel is crucial for precisely inferring the anomaly score. However, in real-world driving scenarios, the diversity of scenes often results in distorted manifolds of pixel embeddings in the space. This effect is not conducive to directly using the pixel embeddings for the logit prediction during inference, a concern overlooked by existing methods. To address this problem, we propose a novel method called Random Walk on Pixel Manifolds (RWPM). RWPM utilizes random walks to reveal the intrinsic relationships among pixels to refine the pixel embeddings. The refined pixel embeddings alleviate the distortion of manifolds, improving the accuracy of anomaly scores. Our extensive experiments show that RWPM consistently improve the performance of the existing anomaly segmentation methods and achieve the best results 1 . 1 Code is available at: https://github.com/ZelongZeng/RWPM",
    "checked": true,
    "id": "3d3d5cf8103e04b3235f8ab2b81ac0839ee7bd4f",
    "semantic_title": "random walk on pixel manifolds for anomaly segmentation of complex driving scenes",
    "citation_count": 0,
    "authors": [
      "Zelong Zeng*",
      "Kaname Tomite"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/414_ECCV_2024_paper.php": {
    "title": "DySeT: a Dynamic Masked Self-distillation Approach for Robust Trajectory Prediction",
    "volume": "main",
    "abstract": "The lack of generalization capability of behavior prediction models for autonomous vehicles is a crucial concern for safe motion planning. One way to address this is via self-supervised pre-training through masked trajectory prediction. However, the existing models rely on uniform random sampling of tokens, which is sub-optimal because it implies that all components of driving scenes are equally informative. In this paper, to enable more robust representation learning, we introduce a dynamic masked self-distillation approach to identify and utilize informative aspects of the scenes, particularly those corresponding to complex driving behaviors, such as overtaking. Specifically, for targeted sampling, we propose a dynamic method that prioritizes tokens, such as trajectory or lane segments, based on their informativeness. The latter is determined via an auxiliary network that estimates token distributions. Through sampler optimization, more informative tokens are rewarded and selected as visible based on the policy gradient algorithm adopted from reinforcement learning. In addition, we propose a masked self-distillation approach to transfer knowledge from fully visible to masked scene representations. The distillation process not only enriches the semantic information within the visible token set but also progressively refines the sampling process. Further, we use an integrated training regime to enhance the model's ability to learn meaningful representations from informative tokens. Our extensive evaluation on two large-scale trajectory prediction datasets demonstrates the superior performance of the proposed method and its improved prediction robustness across different scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mozghan Pourkeshavarz*",
      "Arielle Zhang",
      "Amir Rasouli"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/418_ECCV_2024_paper.php": {
    "title": "Track Everything Everywhere Fast and Robustly",
    "volume": "main",
    "abstract": "We propose a novel test-time optimization approach for efficiently and robustly tracking any pixel at any time in a video. The latest state-of-the-art optimization-based tracking technique, OmniMotion, requires a prohibitively long optimization time, rendering it impractical for downstream applications. OmniMotion is sensitive to the choice of random seeds, leading to unstable convergence. To improve efficiency and robustness, we introduce a novel invertible deformation network, CaDeX++, which factorizes the function representation into a local spatial-temporal feature grid and enhances the expressivity of the coupling blocks with non-linear functions. While CaDeX++ incorporates a stronger geometric bias within its architectural design, it also takes advantage of the inductive bias provided by the vision foundation models. Our system utilizes monocular depth estimation to represent scene geometry and enhances the objective by incorporating DINOv2 long-term semantics to regulate the optimization process. Our experiments demonstrate a substantial improvement in training speed (more than 10 times faster), robustness, and accuracy in tracking over the SoTA optimization-based method OmniMotion",
    "checked": true,
    "id": "3153a00a59a4c91c946b05b9ff3ef7b56eb330f8",
    "semantic_title": "track everything everywhere fast and robustly",
    "citation_count": 2,
    "authors": [
      "Yunzhou Song",
      "Jiahui Lei*",
      "Ziyun Wang",
      "Lingjie Liu",
      "Kostas Daniilidis"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/422_ECCV_2024_paper.php": {
    "title": "Towards Open-ended Visual Quality Comparison",
    "volume": "main",
    "abstract": "Comparative settings (pairwise choice, listwise ranking) have been adopted by a wide range of subjective studies for image quality assessment (IQA), as it inherently standardizes the evaluation criteria across different observers and offer more clear-cut responses. In this work, we extend the edge of emerging large multi-modality models (LMMs) to further advance visual quality comparison into open-ended settings, that 1) can respond to deepgreenopen-range questions on quality comparison; 2) can provide deepgreendetailed reasonings beyond direct answers. To this end, we propose the . To train this first-of-its-kind open-source open-ended visual quality comparer, we collect the Co-Instruct-562K dataset, from two sources: (a) LLM-merged single image quality description, (b) GPT-4V \"teacher\" responses on unlabeled data. Furthermore, to better evaluate this setting, we propose the , the first benchmark on multi-image comparison for LMMs. We demonstrate that not only achieves in average 30% higher accuracy than state-of-the-art open-source LMMs, but also outperforms GPT-4V (its teacher ), on both existing related benchmarks and the proposed . Our code, model and data are released on https://github.com/Q-Future/ Co-Instruct",
    "checked": true,
    "id": "3d54adf4e688602548f81386fcfeab23728fd441",
    "semantic_title": "towards open-ended visual quality comparison",
    "citation_count": 26,
    "authors": [
      "Haoning Wu",
      "Hanwei Zhu",
      "Zicheng Zhang",
      "Erli Zhang",
      "Chaofeng Chen",
      "Liang Liao",
      "Chunyi Li",
      "Annan Wang",
      "Wenxiu Sun",
      "Qiong Yan",
      "Xiaohong Liu",
      "Guangtao Zhai",
      "Shiqi Wang",
      "Weisi Lin*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/423_ECCV_2024_paper.php": {
    "title": "FreeInit: Bridging Initialization Gap in Video Diffusion Models",
    "volume": "main",
    "abstract": "Though diffusion-based video generation has witnessed rapid progress, the inference results of existing models still exhibit unsatisfactory temporal consistency and unnatural dynamics. In this paper, we delve deep into the noise initialization of video diffusion models, and discover an implicit training-inference gap that attributes to the unsatisfactory inference quality. Our key findings are: 1) the spatial-temporal frequency distribution of the initial noise at inference is intrinsically different from that for training, and 2) the denoising process is significantly influenced by the low-frequency components of the initial noise. Motivated by these observations, we propose a concise yet effective inference sampling strategy, FreeInit, which significantly improves temporal consistency of videos generated by diffusion models. Through iteratively refining the spatial-temporal low-frequency components of the initial latent during inference, FreeInit is able to compensate the initialization gap between training and inference, thus effectively improving the subject appearance and temporal consistency of generation results. Extensive experiments demonstrate that FreeInit consistently enhances the generation results of various text-to-video generation models without additional training",
    "checked": true,
    "id": "08e2fa11bb6af1ed39a459210665fb89104933c2",
    "semantic_title": "freeinit: bridging initialization gap in video diffusion models",
    "citation_count": 28,
    "authors": [
      "Tianxing Wu*",
      "Chenyang Si",
      "Yuming Jiang",
      "Ziqi Huang",
      "Ziwei Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/430_ECCV_2024_paper.php": {
    "title": "DenseNets Reloaded: Paradigm Shift Beyond ResNets and ViTs",
    "volume": "main",
    "abstract": "This paper revives Densely Connected Convolutional Networks (DenseNets) and reveals the underrated effectiveness over predominant ResNet-style architectures. We believe DenseNets' potential was overlooked due to untouched training methods and traditional design elements not fully revealing their capabilities. Our pilot study shows dense connections through concatenation are strong, demonstrating that DenseNets can be revitalized to compete with modern architectures. We methodically refine suboptimal components - architectural adjustments, block redesign, and improved training recipes towards widening DenseNets and boosting memory efficiency while keeping concatenation shortcuts. Our models, employing simple architectural elements, ultimately surpass Swin Transformer, ConvNeXt, and DeiT-III — key architectures in the residual learning lineage. Furthermore, our models exhibit near state-of-the-art performance on ImageNet-1K, competing with the very recent models and downstream tasks, ADE20k semantic segmentation, and COCO object detection/instance segmentation. Finally, we provide empirical analyses that uncover the merits of the concatenation over additive shortcuts, steering a renewed preference towards DenseNet-style designs. Our code is available at https://github.com/naver-ai/rdnet",
    "checked": true,
    "id": "bd703f339cd704522707efa977483f7bd3ea3858",
    "semantic_title": "densenets reloaded: paradigm shift beyond resnets and vits",
    "citation_count": 0,
    "authors": [
      "DongHyun Kim",
      "Byeongho Heo",
      "Dongyoon Han*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/450_ECCV_2024_paper.php": {
    "title": "Eliminating Feature Ambiguity for Few-Shot Segmentation",
    "volume": "main",
    "abstract": "Recent advancements in few-shot segmentation (FSS) have exploited pixel-by-pixel matching between query and support features, typically based on cross attention, which selectively activate query foreground (FG) features that correspond to the same-class support FG features. However, due to the large receptive fields in deep layers of the backbone, the extracted query and support FG features are inevitably mingled with background (BG) features, impeding the FG-FG matching in cross attention. Hence, the query FG features are fused with less support FG features, , the support information is not well utilized. This paper presents a novel plug-in termed ambiguity elimination network (AENet), which can be plugged into any existing cross attention-based FSS methods. The main idea is to mine discriminative query FG regions to rectify the ambiguous FG features, increasing the proportion of FG information, so as to suppress the negative impacts of the doped BG features. In this way, the FG-FG matching is naturally enhanced. We plug AENet into three baselines CyCTR, SCCAN and HDMNet for evaluation, and their scores are improved by large margins, , the 1-shot performance of SCCAN can be improved by 3.0%+ on both PASCAL-5i and COCO-20i . The code is available at https://github.com/Sam1224/AENet",
    "checked": true,
    "id": "876e3cb192c177a59396e0509b74f7303cb77d2e",
    "semantic_title": "eliminating feature ambiguity for few-shot segmentation",
    "citation_count": 1,
    "authors": [
      "Qianxiong Xu*",
      "Guosheng Lin",
      "Chen Change Loy",
      "Cheng Long",
      "Ziyue Li",
      "Rui Zhao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/454_ECCV_2024_paper.php": {
    "title": "Soft Prompt Generation for Domain Generalization",
    "volume": "main",
    "abstract": "Large pre-trained vision language models (VLMs) have shown impressive zero-shot ability on downstream tasks with manually designed prompt. To further adapt VLMs to downstream tasks, soft prompt is proposed to replace manually designed prompt, which undergoes fine-tuning based on specific domain data. Prior prompt learning methods primarily learn a fixed prompt or residuled prompt from training samples. However, the learned prompts lack diversity and ignore information about unseen domains. In this paper, we reframe the prompt learning framework from a generative perspective and propose a simple yet efficient method for the Domain Generalization (DG) task, namely Soft Prompt Generation (SPG). Specifically, SPG consists of a two-stage training phase and an inference phase. During the training phase, we introduce soft prompt label for each domain, aiming to incorporate the generative model domain knowledge. During the inference phase, the generator of the generative model is employed to obtain instance-specific soft prompts for the unseen target domain. Extensive experiments on five domain generalization benchmarks of three DG tasks demonstrate that SPG achieves state-of-the-art performance. The code is available at https://github.com/renytek13/Soft-Prompt-Generation-with-CGAN",
    "checked": true,
    "id": "c36cb4a41369369d837ea170397f7818d02150dd",
    "semantic_title": "soft prompt generation for domain generalization",
    "citation_count": 1,
    "authors": [
      "Shuanghao Bai*",
      "Yuedi Zhang",
      "Wanqi Zhou",
      "Zhirong Luan",
      "Badong Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/457_ECCV_2024_paper.php": {
    "title": "Shedding More Light on Robust Classifiers under the lens of Energy-based Models",
    "volume": "main",
    "abstract": "By reinterpreting a robust discriminative classifier as Energy-based Model (EBM), we offer a new take on the dynamics of adversarial training (AT). Our analysis of the energy landscape during AT reveals that untargeted attacks generate adversarial images much more in-distribution (lower energy) than the original data from the point of view of the model. Conversely, we observe the opposite for targeted attacks. On the ground of our thorough analysis, we present new theoretical and practical results that show how interpreting AT energy dynamics unlocks a better understanding: (1) AT dynamic is governed by three phases and robust overfitting occurs in the third phase with a drastic divergence between natural and adversarial energies (2) by rewriting TRADES loss in terms of energies, we show that TRADES implicitly alleviates overfitting by means of aligning the natural energy with the adversarial one (3) we empirically show that all recent state-of-the-art robust classifiers are smoothing the energy landscape and we reconcile a variety of studies about understanding AT and weighting the loss function under the umbrella of EBMs. Motivated by rigorous evidence, we propose Weighted Energy Adversarial Training (WEAT), a novel sample weighting scheme that yields robust accuracy matching the state-of-the-art on multiple benchmarks such as CIFAR-10 and SVHN and going beyond in CIFAR-100 and Tiny-ImageNet. We further show that robust classifiers vary in the intensity and quality of their generative capabilities, and offer a simple method to push this capability, reaching a remarkable Inception Score (IS) and FID using a robust classifier without training for generative modeling. The code to reproduce our results is available at github.com/OmnAI-Lab/Robust-Classifiers-under-the-lens-of-EBM",
    "checked": true,
    "id": "4ebc229abb15553f5c54465da222cc73b0ab5eac",
    "semantic_title": "shedding more light on robust classifiers under the lens of energy-based models",
    "citation_count": 1,
    "authors": [
      "Mujtaba Hussain Mirza*",
      "Maria Rosaria Briglia*",
      "Senad Beadini*",
      "Iacopo Masi*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/465_ECCV_2024_paper.php": {
    "title": "LGM: Large Multi-View Gaussian Model for High-Resolution 3D Content Creation",
    "volume": "main",
    "abstract": "3D content creation has achieved significant progress in terms of both quality and speed. Although current feed-forward models can produce 3D objects in seconds, their resolution is constrained by the intensive computation required during training. In this paper, we introduce Large Multi-View Gaussian Model (LGM), a novel framework designed to generate high-resolution 3D models from text prompts or single-view images. Our key insights are two-fold: 1) 3D Representation: We propose multi-view Gaussian features as an efficient yet powerful representation, which can then be fused together for differentiable rendering. 2) 3D Backbone: We present an asymmetric U-Net as a high-throughput backbone operating on multi-view images, which can be produced from text or single-view image input by leveraging multi-view diffusion models. Extensive experiments demonstrate the high fidelity and efficiency of our approach. Notably, we maintain the fast speed to generate 3D objects within 5 seconds while boosting the training resolution to 512, thereby achieving high-resolution 3D content generation. Our project page is available at https://me.kiui.moe/ lgm/",
    "checked": true,
    "id": "11665dbecb17ef4d3d71b75b8666ce0e61bd43fa",
    "semantic_title": "lgm: large multi-view gaussian model for high-resolution 3d content creation",
    "citation_count": 158,
    "authors": [
      "Jiaxiang Tang*",
      "Zhaoxi Chen",
      "Xiaokang Chen",
      "Tengfei Wang",
      "Gang Zeng",
      "Ziwei Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/471_ECCV_2024_paper.php": {
    "title": "Mahalanobis Distance-based Multi-view Optimal Transport for Multi-view Crowd Localization",
    "volume": "main",
    "abstract": "Multi-view crowd localization predicts the ground locations of all people in the scene. Typical methods usually estimate the crowd density maps on the ground plane first, and then obtain the crowd locations. However, existing methods' performances are limited by the ambiguity of the density maps in crowded areas, where local peaks can be smoothed away. To mitigate the weakness of density map supervision, optimal transport-based point supervision methods have been proposed in the single-image crowd localization tasks, but have not been explored for multi-view crowd localization yet. Thus, in this paper, we propose a novel Mahalanobis distance-based multi-view optimal transport (M-MVOT) loss specifically designed for multi-view crowd localization. First, we replace the Euclidean-based transport cost with the Mahalanobis distance, which defines elliptical iso-contours in the cost function whose long-axis and short-axis directions are guided by the view ray direction. Second, the object-to-camera distance in each view is used to adjust the optimal transport cost of each location further, where the wrong predictions far away from the camera are more heavily penalized. Finally, we propose a strategy to consider all the input camera views in the model loss (M-MVOT) by computing the optimal transport cost for each ground-truth point based on its closest camera. Experiments demonstrate the advantage of the proposed method over density map-based or common Euclidean distance-based optimal transport loss on several multi-view crowd localization datasets",
    "checked": true,
    "id": "dc15011f8547879f863d6da9a769621b2c6f8213",
    "semantic_title": "mahalanobis distance-based multi-view optimal transport for multi-view crowd localization",
    "citation_count": 0,
    "authors": [
      "Qi Zhang",
      "Kaiyi Zhang",
      "Antoni B. Chan",
      "Hui Huang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/484_ECCV_2024_paper.php": {
    "title": "RAW-Adapter: Adapting Pretrained Visual Model to Camera RAW Images",
    "volume": "main",
    "abstract": "sRGB images are now the predominant choice for pre-training visual models in computer vision research, owing to their ease of acquisition and efficient storage. Meanwhile, the advantage of RAW images lies in their rich physical information under variable real-world challenging lighting conditions. For computer vision tasks directly based on camera RAW data, most existing studies adopt methods of integrating image signal processor (ISP) with backend networks, yet often overlook the interaction capabilities between the ISP stages and subsequent networks. Drawing inspiration from ongoing adapter research in NLP and CV areas, we introduce RAW-Adapter, a novel approach aimed at adapting sRGB pre-trained models to camera RAW data. RAW-Adapter comprises input-level adapters that employ learnable ISP stages to adjust RAW inputs, as well as model-level adapters to build connections between ISP stages and subsequent high-level networks. Additionally, RAW-Adapter is a general framework that could be used in various computer vision frameworks. Abundant experiments under different lighting conditions have shown our algorithm's state-of-the-art (SOTA) performance, demonstrating its effectiveness and efficiency across a range of real-world and synthetic datasets. Code is available at this url",
    "checked": false,
    "id": "189e4c59782255f06f126af82cc1a9b00ab9f365",
    "semantic_title": "raw-adapter: adapting pre-trained visual model to camera raw images",
    "citation_count": 0,
    "authors": [
      "Ziteng Cui*",
      "Tatsuya Harada"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/490_ECCV_2024_paper.php": {
    "title": "SLEDGE: Synthesizing Driving Environments with Generative Models and Rule-Based Traffic",
    "volume": "main",
    "abstract": "SLEDGE is the first generative simulator for vehicle motion planning trained on real-world driving logs. Its core component is a learned model that is able to generate agent bounding boxes and lane graphs. The model's outputs serve as an initial state for rule-based traffic simulation. The unique properties of the entities to be generated for SLEDGE, such as their connectivity and variable count per scene, render the naive application of most modern generative models to this task non-trivial. Therefore, together with a systematic study of existing lane graph representations, we introduce a novel raster-to-vector autoencoder. It encodes agents and the lane graph into distinct channels in a rasterized latent map. This facilitates both lane-conditioned agent generation and combined generation of lanes and agents with a Diffusion Transformer. Using generated entities in SLEDGE enables greater control over the simulation, e.g. upsampling turns or increasing traffic density. Further, SLEDGE can support 500m long routes, a capability not found in existing data-driven simulators like nuPlan. It presents new challenges for planning algorithms, evidenced by failure rates of over 40% for PDM, the winner of the 2023 nuPlan challenge, when tested on hard routes and dense traffic generated by our model. Compared to nuPlan, SLEDGE requires 500× less storage to set up (¡4 GB), making it a more accessible option and helping with democratizing future research in this field",
    "checked": true,
    "id": "a0fba82ec86a08f4ea7ea4fc3b555bd26a345397",
    "semantic_title": "sledge: synthesizing driving environments with generative models and rule-based traffic",
    "citation_count": 1,
    "authors": [
      "Kashyap Chitta*",
      "Daniel Dauner",
      "Andreas Geiger"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/497_ECCV_2024_paper.php": {
    "title": "AFreeCA: Annotation-Free Counting for All",
    "volume": "main",
    "abstract": "Object counting methods typically rely on manually annotated datasets. The cost of creating such datasets has restricted the versatility of these networks to count objects from specific classes (such as humans or penguins), and counting objects from diverse categories remains a challenge. The availability of robust text-to-image latent diffusion models (LDMs) raises the question of whether these models can be utilized to generate counting datasets. However, LDMs struggle to create images with an exact number of objects based solely on text prompts but they can be used to offer a dependable sorting signal by adding and removing objects within an image. Leveraging this data, we initially introduce an unsupervised sorting methodology to learn object-related features that are subsequently refined and anchored for counting purposes using counting data generated by LDMs. Further, we present a density classifier-guided method for dividing an image into patches containing objects that can be reliably counted. Consequently, we can generate counting data for any type of object and count them in an unsupervised manner. Our approach outperforms unsupervised and few-shot alternatives and is not restricted to specific object classes for which counting data is available. Code available at: github.com/adrian-dalessandro/AFreeCA",
    "checked": true,
    "id": "8661397c2520acd06fff2ddbbc48359c3227d819",
    "semantic_title": "afreeca: annotation-free counting for all",
    "citation_count": 0,
    "authors": [
      "Adriano D'Alessandro*",
      "Ali Mahdavi-Amiri",
      "Ghassan Hamarneh"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/499_ECCV_2024_paper.php": {
    "title": "Adversarially Robust Distillation by Reducing the Student-Teacher Variance Gap",
    "volume": "main",
    "abstract": "Adversarial robustness generally relies on large-scale architectures and datasets, hindering resource-efficient deployment. For scalable solutions, adversarially robust knowledge distillation has emerged as a principle strategy, facilitating the transfer of robustness from a large-scale teacher model to a lightweight student model. However, existing works focus solely on sample-to-sample alignment of features or predictions between the teacher and student models, overlooking the vital role of their statistical alignment. Thus, we propose a novel adversarially robust knowledge distillation method that integrates the alignment of feature distributions between the teacher and student backbones under adversarial and clean sample sets. To motivate our idea, for an adversarially trained model (, student or teacher), we show that the robust accuracy (evaluated on testing adversarial samples under an increasing perturbation radius) correlates negatively with the gap between the feature variance evaluated on testing adversarial samples and testing clean samples. Such a negative correlation exhibits a strong linear trend, suggesting that aligning the feature covariance of the student model toward the feature covariance of the teacher model should improve the adversarial robustness of the student model by reducing the variance gap. A similar trend is observed by reducing the variance gap between the gram matrices of the student and teacher models. Extensive evaluations highlight the state-of-the-art adversarial robustness and natural performance of our method across diverse datasets and distillation scenarios",
    "checked": true,
    "id": "ddcc8743009d9da0306f6f372bed8f19ba96c705",
    "semantic_title": "adversarially robust distillation by reducing the student-teacher variance gap",
    "citation_count": 0,
    "authors": [
      "Junhao Dong",
      "Piotr Koniusz*",
      "Junxi Chen",
      "Yew-Soon Ong*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/501_ECCV_2024_paper.php": {
    "title": "LN3Diff: Scalable Latent Neural Fields Diffusion for Speedy 3D Generation",
    "volume": "main",
    "abstract": "The field of neural rendering has witnessed significant progress with advancements in generative models and differentiable rendering techniques. Though 2D diffusion has achieved success, a unified 3D diffusion pipeline remains unsettled. This paper introduces a novel framework called to address this gap and enable fast, high-quality, and generic conditional 3D generation. Our approach harnesses a 3D-aware architecture and variational autoencoder (VAE) to encode the input image(s) into a structured, compact, and 3D latent space. The latent is decoded by a transformer-based decoder into a high-capacity 3D neural field. Through training a diffusion model on this 3D-aware latent space, our method achieves superior performance on Objaverse, ShapeNet and FFHQ for conditional 3D generation. Moreover, it surpasses existing 3D diffusion methods in terms of inference speed, requiring no per-instance optimization. Video demos can be found on our project webpage: https://nirvanalan.github.io/projects/ ln3diff",
    "checked": true,
    "id": "c115fe03ce13e8cd7a1809e4325f95c36c06ef23",
    "semantic_title": "ln3diff: scalable latent neural fields diffusion for speedy 3d generation",
    "citation_count": 5,
    "authors": [
      "Yushi Lan",
      "Fangzhou Hong",
      "Shuai Yang",
      "Shangchen Zhou",
      "Xuyi Meng",
      "Bo Dai",
      "Xingang Pan",
      "Chen Change Loy*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/502_ECCV_2024_paper.php": {
    "title": "Hierarchical Temporal Context Learning for Camera-based Semantic Scene Completion",
    "volume": "main",
    "abstract": "Camera-based 3D semantic scene completion (SSC) is pivotal for predicting complicated 3D layouts with limited 2D image observations. The existing mainstream solutions generally leverage temporal information by roughly stacking history frames to supplement the current frame, such straightforward temporal modeling inevitably diminishes valid clues and increases learning difficulty. To address this problem, we present HTCL, a novel Hierarchical Temporal Context Learning paradigm for improving camera-based semantic scene completion. The primary innovation of this work involves decomposing temporal context learning into two hierarchical steps: (a) cross-frame affinity measurement and (b) affinity-based dynamic refinement. Firstly, to separate critical relevant context from redundant information, we introduce the pattern affinity with scale-aware isolation and multiple independent learners for fine-grained contextual correspondence modeling. Subsequently, to dynamically compensate for incomplete observations, we adaptively refine the feature sampling locations based on initially identified locations with high affinity and their neighboring relevant regions. Our method ranks 1st on the SemanticKITTI benchmark and even surpasses LiDAR-based methods in terms of mIoU on the OpenOccupancy benchmark. Our code is available on https://github.com/Arlo0o/HTCL",
    "checked": true,
    "id": "fcf89076b1e8f4b2850e6784fc23c88fae5aac52",
    "semantic_title": "hierarchical temporal context learning for camera-based semantic scene completion",
    "citation_count": 1,
    "authors": [
      "Bohan Li*",
      "Jiajun Deng",
      "Wenyao Zhang",
      "Zhujin Liang",
      "Dalong Du",
      "Xin Jin",
      "Wenjun Zeng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/521_ECCV_2024_paper.php": {
    "title": "Equi-GSPR: Equivariant SE(3) Graph Network Model for Sparse Point Cloud Registration",
    "volume": "main",
    "abstract": "Point cloud registration is a foundational task for 3D alignment and reconstruction applications. While both traditional and learning-based registration approaches have succeeded, leveraging the intrinsic symmetry of point cloud data, including rotation equivariance, has received insufficient attention. This prohibits the model from learning effectively, resulting in a requirement for more training data and increased model complexity. To address these challenges, we propose a graph neural network model embedded with a local Spherical Euclidean 3D equivariance property through SE(3) message passing based propagation. Our model is composed mainly of a descriptor module, equivariant graph layers, match similarity, and the final regression layers. Such modular design enables us to utilize sparsely sampled input points and initialize the descriptor by self-trained or pre-trained geometric feature descriptors easily. Experiments conducted on the 3DMatch and KITTI datasets exhibit the compelling and robust performance of our model compared to state-of-the-art approaches, while the model complexity remains relatively low at the same time",
    "checked": true,
    "id": "ea350944d25562cfcf1832007b31c040c0e6715a",
    "semantic_title": "equi-gspr: equivariant se(3) graph network model for sparse point cloud registration",
    "citation_count": 0,
    "authors": [
      "Xueyang Kang*",
      "Zhaoliang Luan",
      "Kourosh Khoshelham",
      "Bing WANG*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/523_ECCV_2024_paper.php": {
    "title": "GTP-4o: Modality-prompted Heterogeneous Graph Learning for Omni-modal Biomedical Representation",
    "volume": "main",
    "abstract": "Recent advances in learning multi-modal representation have witnessed the success in biomedical domains. While established techniques enable handling multi-modal information, the challenges are posed when extended to various clinical modalities and practical modality-missing setting due to the inherent modality gaps. To tackle these, we propose an innovative Modality-prompted Heterogeneous Graph for Omni-modal Learning (GTP-4o), which embeds the numerous disparate clinical modalities into a unified representation, completes the deficient embedding of missing modality and reformulates the cross-modal learning with a graph-based aggregation. Specially, we establish a heterogeneous graph embedding to explicitly capture the diverse semantic properties on both the modality-specific features (nodes) and the cross-modal relations (edges). Then, we design a modality-prompted completion that enables completing the inadequate graph representation of missing modality through a graph prompting mechanism, which generates hallucination graphic topologies to steer the missing embedding towards the intact representation. Through the completed graph, we meticulously develop a knowledge-guided hierarchical cross-modal aggregation consisting of a global meta-path neighbouring to uncover the potential heterogeneous neighbors along the pathways driven by domain knowledge, and a local multi-relation aggregation module for the comprehensive cross-modal interaction across various heterogeneous relations. We assess the efficacy of our methodology on rigorous benchmarking experiments against prior state-of-the-arts. In a nutshell, GTP-4o presents an initial foray into the intriguing realm of embedding, relating and perceiving the heterogeneous patterns from various clinical modalities holistically via a graph theory. Project page: https://gtp-4-o.github.io/",
    "checked": true,
    "id": "8967a871d24eaa982539fdfb39b2738e7f7a00c5",
    "semantic_title": "gtp-4o: modality-prompted heterogeneous graph learning for omni-modal biomedical representation",
    "citation_count": 5,
    "authors": [
      "Chenxin Li*",
      "Xinyu Liu",
      "Cheng Wang",
      "Yifan Liu",
      "Weihao Yu",
      "Jing Shao",
      "Yixuan Yuan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/524_ECCV_2024_paper.php": {
    "title": "PromptCCD: Learning Gaussian Mixture Prompt Pool for Continual Category Discovery",
    "volume": "main",
    "abstract": "We tackle the problem of Continual Category Discovery (CCD), which aims to automatically discover novel categories in a continuous stream of unlabeled data while mitigating the challenge of catastrophic forgetting – an open problem that persists even in conventional, fully supervised continual learning. To address this challenge, we propose PromptCCD, a simple yet effective framework that utilizes a Gaussian Mixture Model (GMM) as a prompting method for CCD. At the core of PromptCCD lies the Gaussian Mixture Prompting (GMP) module, which acts as a dynamic pool that updates over time to facilitate representation learning and prevent forgetting during category discovery. Moreover, GMP enables on-the-fly estimation of category numbers, allowing PromptCCD to discover categories in unlabeled data without prior knowledge of the category numbers. We extend the standard evaluation metric for Generalized Category Discovery (GCD) to CCD and benchmark state-of-the-art methods on diverse public datasets. PromptCCD significantly outperforms existing methods, demonstrating its effectiveness. Project page: https://visual-ai.github.io/promptccd",
    "checked": true,
    "id": "2c9a49d66422afccb69928a66a3aae0265a4e1e7",
    "semantic_title": "promptccd: learning gaussian mixture prompt pool for continual category discovery",
    "citation_count": 1,
    "authors": [
      "Fernando Julio Cendra",
      "Bingchen Zhao",
      "Kai Han*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/529_ECCV_2024_paper.php": {
    "title": "Sapiens: Foundation for Human Vision Models",
    "volume": "main",
    "abstract": "We present Sapiens, a family of models for four fundamental human-centric vision tasks – 2D pose estimation, body-part segmentation, depth estimation, and surface normal prediction. Our models natively support 1K high-resolution inference and are extremely easy to adapt for individual tasks by simply fine-tuning foundation models pretrained on over 300 million in-the-wild human images. We observe that, given the same computational budget, self-supervised pretraining on a curated dataset of human images significantly boosts the performance for a diverse set of human-centric tasks. The resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. Our simple model design also brings scalability – model performance across tasks significantly improves as we scale the number of parameters from 0.3 to 2 billion. Sapiens consistently surpasses existing complex baselines across various human-centric benchmarks. Specifically, we achieve significant improvements over the prior state-of-the-art on Humans-5K (pose) by 7.6 mAP, Humans-2K (part-seg) by 17.1 mIoU, Hi4D (depth) by 22.4% relative RMSE, and THuman2 (normal) by 53.5% relative angular error",
    "checked": true,
    "id": "19de22fc50c5f8609b2c31854f5e568fa58c6041",
    "semantic_title": "sapiens: foundation for human vision models",
    "citation_count": 0,
    "authors": [
      "Rawal Khirodkar*",
      "Timur Bagautdinov",
      "Julieta Martinez",
      "Zhaoen Su",
      "Austin T James",
      "Peter Selednik",
      "Stuart Anderson",
      "Shunsuke Saito"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/540_ECCV_2024_paper.php": {
    "title": "Linearly Controllable GAN: Unsupervised Feature Categorization and Decomposition for Image Generation and Manipulation",
    "volume": "main",
    "abstract": "This paper introduces an approach to linearly controllable generative adversarial networks (LC-GAN) driven by unsupervised learning. Departing from traditional methods relying on supervision signals or post-processing for latent feature disentanglement, our proposed technique enables unsupervised learning using only image data through contrastive feature categorization and spectral regularization. In our framework, the discriminator constructs geometry- and appearance-related feature spaces using a combination of image augmentation and contrastive representation learning. Leveraging these feature spaces, the generator autonomously categorizes input latent codes into geometry- and appearance-related features. Subsequently, the categorized features undergo projection into a subspace via our proposed spectral regularization, with each component controlling a distinct aspect of the generated image. Beyond providing fine-grained control over the generative model, our approach achieves state-of-the-art image generation quality on benchmark datasets, including FFHQ, CelebA-HQ, and AFHQ-V2",
    "checked": true,
    "id": "984621a4a6ba241dd828d88eefc05f59adcc76f2",
    "semantic_title": "linearly controllable gan: unsupervised feature categorization and decomposition for image generation and manipulation",
    "citation_count": 0,
    "authors": [
      "sehyung lee*",
      "Mijung Kim",
      "Yeongnam Chae",
      "Bjorn Stenger"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/549_ECCV_2024_paper.php": {
    "title": "Generating Human Interaction Motions in Scenes with Text Control",
    "volume": "main",
    "abstract": "We present , a text-controlled scene-aware motion generation method based on denoising diffusion models. Previous text-to-motion methods focus on characters in isolation without considering scenes due to the limited availability of datasets that include motion, text descriptions, and interactive scenes. Our approach begins with pre-training a scene-agnostic text-to-motion diffusion model, emphasizing goal-reaching constraints on large-scale motion-capture datasets. We then enhance this model with a scene-aware component, fine-tuned using data augmented with detailed scene information, including ground plane and object shapes. To facilitate training, we embed annotated navigation and interaction motions within scenes. The proposed method produces realistic and diverse human-object interactions, such as navigation and sitting, in different scenes with various object shapes, orientations, initial body positions, and poses. Extensive experiments demonstrate that our approach surpasses prior techniques in terms of the plausibility of human-scene interactions and the realism and variety of the generated motions. Code and data are available at",
    "checked": true,
    "id": "422fe25c07c7b622339960e74c59af586054db1e",
    "semantic_title": "generating human interaction motions in scenes with text control",
    "citation_count": 8,
    "authors": [
      "Hongwei Yi*",
      "Justus Thies",
      "Michael J. Black",
      "Xue Bin Peng",
      "Davis Rempe*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/553_ECCV_2024_paper.php": {
    "title": "NOVUM: Neural Object Volumes for Robust Object Classification",
    "volume": "main",
    "abstract": "Discriminative models for object classification typically learn image-based representations that do not capture the compositional and 3D nature of objects. In this work, we show that explicitly integrating 3D compositional object representations into deep networks for image classification leads to a largely enhanced generalization in out-of-distribution scenarios. In particular, we introduce a novel architecture, referred to as , that consists of a feature extractor and a neural object volume for every target object class. Each neural object volume is a composition of 3D Gaussians that emit feature vectors. This compositional object representation allows for a highly robust and fast estimation of the object class by independently matching the features of the 3D Gaussians of each category to features extracted from an input image. Additionally, the object pose can be estimated via inverse rendering of the corresponding neural object volume. To enable the classification of objects, the neural features at each 3D Gaussian are trained discriminatively to be distinct from (i) the features of 3D Gaussians in other categories, (ii) features of other 3D Gaussians of the same object, and (iii) the background features. Our experiments show that offers intriguing advantages over standard architectures due to the 3D compositional structure of the object representation, namely: (1) An exceptional robustness across a spectrum of real-world and synthetic out-of-distribution shifts and (2) an enhanced human interpretability compared to standard models, all while maintaining real-time inference and a competitive accuracy on in-distribution data. Code and model can be found at /GenIntel/NOVUM",
    "checked": true,
    "id": "7f6b686b1a9ae3983dd4facfb23038d49f16dcc4",
    "semantic_title": "novum: neural object volumes for robust object classification",
    "citation_count": 5,
    "authors": [
      "Artur Jesslen*",
      "Guofeng Zhang",
      "Angtian Wang",
      "Wufei Ma",
      "Alan Yuille",
      "Adam Kortylewski"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/560_ECCV_2024_paper.php": {
    "title": "Align before Collaborate: Mitigating Feature Misalignment for Robust Multi-Agent Perception",
    "volume": "main",
    "abstract": "Collaborative perception has received widespread attention recently since it enhances the perception ability of autonomous vehicles via inter-agent information sharing. However, the performance of existing systems is hindered by the unavoidable collaboration noises, which induce feature-level spatial misalignment over the collaborator-shared information. In this paper, we propose a model-agnostic and lightweight plugin to mitigate the feature-level misalignment issue, called dynamic feature alignment (NEAT). The merits of the NEAT plugin are threefold. First, we introduce an importance-guided query proposal to predict potential foreground regions with space-channel semantics and exclude environmental redundancies. On this basis, a deformable feature alignment is presented to explicitly align the collaborator-shared features through query-aware spatial associations, aggregating multi-grained visual clues with corrective mismatch properties. Ultimately, we perform a region cross-attention reinforcement to facilitate aligned representation diffusion and achieve global feature semantic enhancement. NEAT can be readily inserted into existing collaborative perception procedures and significantly improves the robustness of vanilla baselines against pose errors and transmission delay. Extensive experiments on four collaborative 3D object detection datasets under noisy settings confirm that NEAT provides consistent gains for most methods with distinct structures",
    "checked": true,
    "id": "8ad27f3a29907cc44e30d414460d2310957f6804",
    "semantic_title": "align before collaborate: mitigating feature misalignment for robust multi-agent perception",
    "citation_count": 0,
    "authors": [
      "Dingkang Yang",
      "Dingkang Yang",
      "Ke Li",
      "Dongling Xiao",
      "Zedian Shao",
      "Peng Sun",
      "Liang Song*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/564_ECCV_2024_paper.php": {
    "title": "HIMO: A New Benchmark for Full-Body Human Interacting with Multiple Objects",
    "volume": "main",
    "abstract": "Generating human-object interactions (HOIs) is critical with the tremendous advances of digital avatars. Existing datasets are typically limited to humans interacting with a single object while neglecting the ubiquitous manipulation of multiple objects. Thus, we propose HIMO, a large-scale MoCap dataset of full-body human interacting with multiple objects, containing 3.3K 4D HOI sequences and 4.08M 3D HOI frames. We also annotate HIMO with detailed textual descriptions and temporal segments, benchmarking two novel tasks of HOI synthesis conditioned on either the whole text prompt or the segmented text prompts as fine-grained timeline control. To address these novel tasks, we propose a dual-branch conditional diffusion model with a mutual interaction module for HOI synthesis. Besides, an auto-regressive generation pipeline is also designed to obtain smooth transitions between HOI segments. Experimental results demonstrate the generalization ability to unseen object geometries and temporal compositions. Our data, codes, and models will be publicly available for research purposes",
    "checked": true,
    "id": "d46bf8b9f4388f017f70a3f2c82356fd0ca1b537",
    "semantic_title": "himo: a new benchmark for full-body human interacting with multiple objects",
    "citation_count": 0,
    "authors": [
      "Xintao Lv",
      "Liang Xu",
      "Yichao Yan*",
      "Xin Jin",
      "Congsheng Xu",
      "Wu Shuwen",
      "Yifan Liu",
      "Lincheng Li",
      "Mengxiao Bi",
      "Wenjun Zeng",
      "Xiaokang Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/565_ECCV_2024_paper.php": {
    "title": "SAIR: Learning Semantic-aware Implicit Representation",
    "volume": "main",
    "abstract": "Implicit representation of an image can map arbitrary coordinates in the continuous domain to their corresponding color values, presenting a powerful capability for image reconstruction. Nevertheless, existing implicit representation approaches only focus on building continuous appearance mapping, ignoring the continuities of the semantic information across pixels. Consequently, achieving the desired reconstruction results becomes challenging when the semantic information within input image is corrupted, such as when a large region is missing. To address the issue, we suggest learning semantic-aware implicit representation ( SAIR), that is, we make the implicit representation of each pixel rely on both its appearance and semantic information (, which object does the pixel belong to). To this end, we propose a framework with two modules: (1) a semantic implicit representation (SIR) for a corrupted image. Given an arbitrary coordinate in the continuous domain, we can obtain its respective text-aligned embedding indicating the object the pixel belongs. (2) an appearance implicit representation (AIR) based on the SIR. Given an arbitrary coordinate in the continuous domain, we can reconstruct its color whether or not the pixel is missed in the input. We validate the novel semantic-aware implicit representation method on the image inpainting task, and the extensive experiments demonstrate that our method surpasses state-of-the-art approaches by a significant margin",
    "checked": true,
    "id": "cbd3736a82ab1dfe25ef37c27cfec4da34de9be8",
    "semantic_title": "sair: learning semantic-aware implicit representation",
    "citation_count": 2,
    "authors": [
      "Canyu Zhang*",
      "Xiaoguang Li*",
      "Qing Guo*",
      "Song Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/578_ECCV_2024_paper.php": {
    "title": "ColorMNet: A Memory-based Deep Spatial-Temporal Feature Propagation Network for Video Colorization",
    "volume": "main",
    "abstract": "How to effectively explore spatial-temporal features is important for video colorization. Instead of stacking multiple frames along the temporal dimension or recurrently propagating estimated features that will accumulate errors or cannot explore information from far-apart frames, we develop a memory-based feature propagation module that can establish reliable connections with features from far-apart frames and alleviate the influence of inaccurately estimated features. To extract better features from each frame for the above-mentioned feature propagation, we explore the features from large-pretrained visual models to guide the feature estimation of each frame so that the estimated features can model complex scenarios. In addition, we note that adjacent frames usually contain similar contents. To explore this property for better spatial and temporal feature utilization, we develop a local attention module to aggregate the features from adjacent frames in a spatial-temporal neighborhood. We formulate our memory-based feature propagation module, large-pretrained visual model guided feature estimation module, and local attention module into an end-to-end trainable network (named ColorMNet) and show that it performs favorably against state-of-the-art methods on both the benchmark datasets and real-world scenarios. Our source codes and pre-trained models are available at: https://github.com/yyang181/colormnet",
    "checked": true,
    "id": "c41d4b8cfe61d546ce7288a23a6a9b890ff9a632",
    "semantic_title": "colormnet: a memory-based deep spatial-temporal feature propagation network for video colorization",
    "citation_count": 0,
    "authors": [
      "Yixin Yang",
      "Jiangxin Dong",
      "Jinhui Tang",
      "Jinshan Pan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/581_ECCV_2024_paper.php": {
    "title": "UNIC: Universal Classification Models via Multi-teacher Distillation",
    "volume": "main",
    "abstract": "Pretrained models have become a commodity and offer strong results on a broad range of tasks. In this work, we focus on classification and seek to learn a unique encoder able to take from several complementary pretrained models. We aim at even stronger generalization across a variety of classification tasks. We propose to learn such an encoder via multi-teacher distillation. We first thoroughly analyze standard distillation when driven by multiple strong teachers with complementary strengths. Guided by this analysis, we gradually propose improvements to the basic distillation setup. Among those, we enrich the architecture of the encoder with a ladder of expendable projectors, which increases the impact of intermediate features during distillation, and we introduce teacher dropping, a regularization mechanism that better balances the teachers' influence. Our final distillation strategy leads to student models of the same capacity as any of the teachers, while retaining or improving upon the performance of the best teacher for each task",
    "checked": true,
    "id": "4d1a04561776455bc2b11d0deae7ad99f5e251b2",
    "semantic_title": "unic: universal classification models via multi-teacher distillation",
    "citation_count": 1,
    "authors": [
      "Yannis Kalantidis",
      "Diane Larlus",
      "Mert Bulent Sariyildiz*",
      "Philippe Weinzaepfel",
      "Thomas LUCAS"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/589_ECCV_2024_paper.php": {
    "title": "Instance-dependent Noisy-label Learning with Graphical Model Based Noise-rate Estimation",
    "volume": "main",
    "abstract": "Deep learning faces a formidable challenge when handling noisy labels, as models tend to overfit samples affected by label noise. This challenge is further compounded by the presence of instance-dependent noise (IDN), a realistic form of label noise arising from ambiguous sample information. To address IDN, Label Noise Learning (LNL) incorporates a sample selection stage to differentiate clean and noisy-label samples. This stage uses an arbitrary criterion and a pre-defined curriculum that initially selects most samples as noisy and gradually decreases this selection rate during training. Such curriculum is sub-optimal since it does not consider the actual label noise rate in the training set. This paper addresses this issue with a new noise-rate estimation method that is easily integrated with most state-of-the-art (SOTA) LNL methods to produce a more effective curriculum. Synthetic and real-world benchmarks' results demonstrate that integrating our approach with SOTA LNL methods improves accuracy in most cases.1 1 Code is available at https://github.com/arpit2412/NoiseRateLearning. Supported by the Engineering and Physical Sciences Research Council (EPSRC) through grant EP/Y018036/1 and the Australian Research Council (ARC) through grant FT190100525",
    "checked": true,
    "id": "c6da4c98b00ddd8b2d85b31b2bda3ed7cc164690",
    "semantic_title": "instance-dependent noisy-label learning with graphical model based noise-rate estimation",
    "citation_count": 1,
    "authors": [
      "Arpit Garg*",
      "Cuong Cao Nguyen",
      "RAFAEL FELIX",
      "Thanh-Toan Do",
      "Gustavo Carneiro"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/608_ECCV_2024_paper.php": {
    "title": "Eliminating Warping Shakes for Unsupervised Online Video Stitching",
    "volume": "main",
    "abstract": "In this paper, we retarget video stitching to an emerging issue, named warping shake, when extending image stitching to video stitching. It unveils the temporal instability of warped content in non-overlapping regions, despite image stitching having endeavored to preserve the natural structures. Therefore, in most cases, even if the input videos to be stitched are stable, the stitched video will inevitably cause undesired warping shakes and affect the visual experience. To eliminate the shakes, we propose StabStitch to simultaneously realize video stitching and video stabilization in a unified unsupervised learning framework. Starting from the camera paths in video stabilization, we first derive the expression of stitching trajectories in video stitching by elaborately integrating spatial and temporal warps. Then a warp smoothing model is presented to optimize them with a comprehensive consideration regarding content alignment, trajectory smoothness, spatial consistency, and online collaboration. To establish an evaluation benchmark and train the learning framework, we build a video stitching dataset with a rich diversity in camera motions and scenes. Compared with existing stitching solutions, StabStitch exhibits significant superiority in scene robustness and inference speed in addition to stitching and stabilization performance, contributing to a robust and real-time online video stitching system. The codes and dataset are available at https://github.com/nie-lang/StabStitch",
    "checked": true,
    "id": "03dc0dd9636120f668d5a470dc083721038f5b1e",
    "semantic_title": "eliminating warping shakes for unsupervised online video stitching",
    "citation_count": 0,
    "authors": [
      "Lang Nie",
      "Chunyu Lin*",
      "Kang Liao",
      "Yun Zhang",
      "Shuaicheng Liu",
      "Rui Ai",
      "Yao Zhao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/610_ECCV_2024_paper.php": {
    "title": "Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models",
    "volume": "main",
    "abstract": "Most Large Vision-Language Models (LVLMs) enjoy the same vision vocabulary, i.e., CLIP, for common vision tasks. However, for some special task that needs dense and fine-grained perception, the CLIP-style vocabulary may encounter low efficiency in tokenizing corresponding vision knowledge and even suffer out-of-vocabulary problems. Accordingly, we propose Vary, an efficient and productive method to scale up the Vision vocabulary of LVLMs. The procedures of Vary are naturally divided into two folds: the generation and integration of a new vision vocabulary. In the first phase, we devise a vocabulary network along with a tiny decoder-only transformer to compress rich vision signals. Next, we scale up the vanilla vision vocabulary by merging the new with the original one (CLIP), enabling the LVLMs to garner new features effectively. We present frameworks with two sizes: Vary-base (7B) and Vary-toy (1.8B), both of which enjoy excellent fine-grained perception performance while maintaining great general ability",
    "checked": true,
    "id": "b240a1d8ec2860bdd7370daa3144268ce46ac018",
    "semantic_title": "vary: scaling up the vision vocabulary for large vision-language models",
    "citation_count": 54,
    "authors": [
      "Haoran Wei*",
      "Lingyu Kong",
      "Jinyue Chen",
      "Liang Zhao",
      "Zheng Ge",
      "Jinrong Yang",
      "Jianjian Sun",
      "Chunrui Han",
      "Xiangyu Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/613_ECCV_2024_paper.php": {
    "title": "Merlin: Empowering Multimodal LLMs with Foresight Minds",
    "volume": "main",
    "abstract": "Humans can foresee the future based on present observations, a skill we term as foresight minds. However, this capability remains under-explored within existing MLLMs, hindering their capacity to understand intentions behind subjects. To address this, we integrate the future modeling into MLLMs. By utilizing the trajectory, a highly structured representation, as a learning objective, we aim to equip the model to understand spatiotemporal dynamics. Inspired by the learning paradigm of LLMs, we first propose Foresight Pre-Training (FPT) that jointly learns various tasks centered on trajectories, enabling MLLMs to predict entire trajectories from a given initial observation. Then, we propose Foresight Instruction-Tuning (FIT) that requires MLLMs to reason about potential future events based on predicted trajectories. Aided by FPT and FIT, we build an unified MLLM named Merlin that supports complex future reasoning. Experiments show Merlin's foresight minds with impressive performance on both future reasoning and visual comprehension tasks. Project page: https: //ahnsun.github.io/merlin",
    "checked": true,
    "id": "40de3296157b9d7a7882b61f967e37b3cc93f197",
    "semantic_title": "merlin: empowering multimodal llms with foresight minds",
    "citation_count": 13,
    "authors": [
      "En Yu",
      "Liang Zhao",
      "YANA WEI",
      "Jinrong Yang",
      "Dongming Wu",
      "Lingyu Kong",
      "Haoran Wei",
      "Tiancai Wang",
      "Zheng Ge",
      "Xiangyu Zhang",
      "Wenbing Tao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/629_ECCV_2024_paper.php": {
    "title": "ViC-MAE: Self-Supervised Representation Learning from Images and Video with Contrastive Masked Autoencoders",
    "volume": "main",
    "abstract": "We propose , a model that combines both Masked AutoEncoders (MAE) and contrastive learning. is trained using a global representation obtained by pooling the local features learned under an MAE reconstruction loss and using this representation under a contrastive objective across images and video frames. We show that visual representations learned under generalize well to video and image classification tasks. Particularly, obtains state-of-the-art transfer learning performance from video to images on Imagenet-1k compared to the recently proposed OmniMAE by achieving a top-1 accuracy of 86% (+1.3% absolute improvement) when trained on the same data and 87.1% (+2.4% absolute improvement) when training on extra data. At the same time, outperforms most other methods on video benchmarks by obtaining 75.9% top-1 accuracy on the challenging Something something-v2 video benchmark. When training on videos and images from diverse datasets, our method maintains a balanced transfer-learning performance between video and image classification benchmarks, coming only as a close second to the best-supervised method",
    "checked": true,
    "id": "e7085ec3f5e0abd496342feec7c23c4ca0b629fb",
    "semantic_title": "vic-mae: self-supervised representation learning from images and video with contrastive masked autoencoders",
    "citation_count": 0,
    "authors": [
      "Jefferson Hernandez*",
      "Ruben Villegas",
      "Vicente Ordonez"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/632_ECCV_2024_paper.php": {
    "title": "E.T. the Exceptional Trajectory: Text-to-camera-trajectory generation with character awareness",
    "volume": "main",
    "abstract": "Stories and emotions in movies emerge through the effect of well-thought-out directing decisions, in particular camera placement and movement over time. Crafting compelling camera trajectories remains a complex iterative process, even for skilful artists. To tackle this, in this paper, we propose a dataset called the Exceptional Trajectories (E.T.) with camera trajectories along with character information and textual captions encompassing descriptions of both camera and character. To our knowledge, this is the first dataset of its kind. To show the potential applications of the E.T. dataset, we propose a diffusion-based approach, named Director, which generates complex camera trajectories from textual captions that describe the relation and synchronisation between the camera and characters. To ensure robust and accurate evaluations, we train on the E.T. dataset CLaTr, a Contrastive Language-Trajectory embedding for evaluation metrics. We posit that our proposed dataset and method significantly advance the democratization of cinematography, making it more accessible to common users",
    "checked": false,
    "id": "93a7e0defa0fc675ffb7fd6ec9f1b21cbeb1cd41",
    "semantic_title": "e.t. the exceptional trajectories: text-to-camera-trajectory generation with character awareness",
    "citation_count": 0,
    "authors": [
      "Robin Courant*",
      "Nicolas Dufour",
      "Xi WANG",
      "Marc Christie",
      "Vicky Kalogeiton"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/633_ECCV_2024_paper.php": {
    "title": "OphNet: A Large-Scale Video Benchmark for Ophthalmic Surgical Workflow Understanding",
    "volume": "main",
    "abstract": "Surgical scene perception via videos is critical for advancing robotic surgery, telesurgery, and AI-assisted surgery, particularly in ophthalmology. However, the scarcity of diverse and richly annotated video datasets has hindered the development of intelligent systems for surgical workflow analysis. Existing datasets face challenges such as small scale, lack of diversity in surgery and phase categories, and absence of time-localized annotations. These limitations impede action understanding and model generalization validation in complex and diverse real-world surgical scenarios. To address this gap, we introduce OphNet, a large-scale, expert-annotated video benchmark for ophthalmic surgical workflow understanding. OphNet features: 1) A diverse collection of 2,278 surgical videos spanning 66 types of cataract, glaucoma, and corneal surgeries, with detailed annotations for 102 unique surgical phases and 150 fine-grained operations. 2) Sequential and hierarchical annotations for each surgery, phase, and operation, enabling comprehensive understanding and improved interpretability. 3) Time-localized annotations, facilitating temporal localization and prediction tasks within surgical workflows. With approximately 285 hours of surgical videos, OphNet is about 20 times larger than the largest existing surgical workflow analysis benchmark. Code and dataset are available at: https: //minghu0830.github.io/OphNet-benchmark/",
    "checked": true,
    "id": "80ae23fba0fd41a42934107664d63e8a63362f61",
    "semantic_title": "ophnet: a large-scale video benchmark for ophthalmic surgical workflow understanding",
    "citation_count": 6,
    "authors": [
      "Ming Hu*",
      "Peng Xia",
      "Lin Wang",
      "Siyuan Yan",
      "Feilong Tang",
      "zhongxing xu",
      "Yimin Luo",
      "Kaimin Song",
      "Jurgen Leitner",
      "Xuelian Cheng",
      "Jun Cheng",
      "Chi Liu",
      "Kaijing Zhou*",
      "Zongyuan Ge*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/653_ECCV_2024_paper.php": {
    "title": "SignAvatars: A Large-scale 3D Sign Language Holistic Motion Dataset and Benchmark",
    "volume": "main",
    "abstract": "We present SignAvatars1 , the first large-scale, multi-prompt 3D sign language (SL) motion dataset designed to bridge the communication gap for Deaf and hard-of-hearing individuals. While there has been an exponentially growing number of research regarding digital communication, the majority of existing communication technologies primarily cater to spoken or written languages, instead of SL, the essential communication method for Deaf and hard-of-hearing communities. Existing SL datasets, dictionaries, and sign language production (SLP) methods are typically limited to 2D as annotating 3D models and avatars for SL is usually an entirely manual and labor-intensive process conducted by SL experts, often resulting in unnatural avatars. In response to these challenges, we compile and curate the SignAvatars dataset, which comprises 70,000 videos from 153 signers, totaling 8.34 million frames, covering both isolated signs and continuous, co-articulated signs, with multiple prompts including HamNoSys, spoken language, and words. To yield 3D holistic annotations, including meshes and biomechanically-valid poses of body, hands, and face, as well as 2D and 3D keypoints, we introduce an automated annotation pipeline operating on our large corpus of SL videos. SignAvatars facilitates various tasks such as 3D sign language recognition (SLR) and the novel 3D SL production (SLP) from diverse inputs like text scripts, individual words, and HamNoSys notation. Hence, to evaluate the potential of SignAvatars, we further propose a unified benchmark of 3D SL holistic motion production. We believe that this work is a significant step forward towards bringing the digital world to the Deaf and hard-of-hearing communities as well as people interacting with them. 1 https://signavatars.github.io/",
    "checked": true,
    "id": "33aa057a8268a5965e1682904f4148fbd16cf1ee",
    "semantic_title": "signavatars: a large-scale 3d sign language holistic motion dataset and benchmark",
    "citation_count": 0,
    "authors": [
      "Zhengdi Yu",
      "Shaoli Huang*",
      "yongkang cheng",
      "Tolga Birdal"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/666_ECCV_2024_paper.php": {
    "title": "AttnZero: Efficient Attention Discovery for Vision Transformers",
    "volume": "main",
    "abstract": "In this paper, we present AttnZero, the first framework for automatically discovering efficient attention modules tailored for Vision Transformers (ViTs). While traditional self-attention in ViTs suffers from quadratic computation complexity, linear attention offers a more efficient alternative with linear complexity approximation. However, existing hand-crafted linear attention suffers from performance degradation. To address these issues, our AttnZero constructs search spaces and employs evolutionary algorithms to discover potential linear attention formulations. Specifically, our search space consists of six kinds of computation graphs and advanced activation, normalize, and binary operators. To enhance generality, we derive results of candidate attention applied to multiple advanced ViTs as the multi-objective for the evolutionary search. To expedite the search process, we utilize program checking and rejection protocols to filter out unpromising candidates swiftly. Additionally, we develop Attn-Bench-101, which provides precomputed performance of 2,000 attentions in the search spaces, enabling us to summarize attention design insights. Experimental results demonstrate that the discovered AttnZero module generalizes well to different tasks and consistently achieves improved performance across various ViTs. For instance, the tiny model of DeiT—PVT—Swin—CSwin trained with AttnZero on ImageNet reaches 74.9%—78.1%—82.1%—82.9% top-1 accuracy. Codes at: https://github.com/lliai/AttnZero",
    "checked": false,
    "id": "cd6734fd52338aad94ad8c369ed41ad201f229b3",
    "semantic_title": "abstract 6183: exploring the role of microenvironment context on diagnostic accuracy in tissue microarray core selection",
    "citation_count": 0,
    "authors": [
      "Lujun Li",
      "Zimian Wei*",
      "Peijie Dong",
      "Wenhan Luo",
      "Wei Xue",
      "Qifeng Liu*",
      "Yike Guo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/668_ECCV_2024_paper.php": {
    "title": "Auto-GAS: Automated Proxy Discovery for Training-free Generative Architecture Search",
    "volume": "main",
    "abstract": "In this paper, we introduce Auto-GAS, the first training-free Generative Architecture Search (GAS) framework enabled by an auto-discovered proxy. Generative models like Generative Adversarial Networks (GANs) are now widely used in many real-time applications. Previous GAS methods use differentiable or evolutionary search to find optimal GAN generators for fast inference and memory efficiency. However, the high computational overhead of these training-based GAS techniques limits their adoption. To improve search efficiency, we explore training-free GAS but find existing zero-cost proxies designed for classification tasks underperform on generation benchmarks. To address this challenge, we develop a custom proxy search framework tailored for GAS tasks to enhance predictive power. Specifically, we construct an information-aware proxy that takes feature statistics as inputs and utilizes advanced transform, encoding, reduction, and augment operations to represent candidate proxies. Then, we employ an evolutionary algorithm to perform crossover and mutation on superior candidates within the population based on correlation evaluation. Finally, we perform generator search without training using the optimized proxy. Thus, Auto-GAS enables automated proxy discovery for GAS while significantly accelerating the search before training stage. Extensive experiments on image generation and image-to-image translation tasks demonstrate that Auto-GAS strikes superior accuracy-speed tradeoffs over state-of-the-art methods. Remarkably, Auto-GAS achieves competitive scores with 110× faster search than GAN Compression. Code at: https://github.com/lliai/Auto-GAS",
    "checked": false,
    "id": "4cbcd2d7d89878502a28a72ecaf6391265c40ba9",
    "semantic_title": "saswot: real-time semantic segmentation architecture search without training",
    "citation_count": 6,
    "authors": [
      "Lujun Li",
      "Haosen Sun",
      "Shiwen Li",
      "Peijie Dong",
      "Wenhan Luo",
      "Wei Xue",
      "Qifeng Liu*",
      "Yike Guo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/676_ECCV_2024_paper.php": {
    "title": "Auto-DAS: Automated Proxy Discovery for Training-free Distillation-aware Architecture Search",
    "volume": "main",
    "abstract": "Distillation-aware Architecture Search (DAS) seeks to discover the ideal student architecture that delivers superior performance by distilling knowledge from a given teacher model. Previous DAS methods involve time-consuming training-based search processes. Recently, the training-free DAS method (, DisWOT) proposes KD-based proxies and achieves significant search acceleration. However, we observe that DisWOT suffers from limitations such as the need for manual design and poor generalization to diverse architectures, such as the Vision Transformer (ViT). To address these issues, we present Auto-DAS, an automatic proxy discovery framework using an Evolutionary Algorithm (EA) for training-free DAS. Specifically, we empirically find that proxies conditioned on student instinct statistics and teacher-student interaction statistics can effectively predict distillation accuracy. Then, we represent the proxy with computation graphs and construct the proxy search space using instinct and interaction statistics as inputs. To identify promising proxies, our search space incorporates various types of basic transformations and network distance operators inspired by previous proxy and KD-loss designs. Next, our EA initializes populations, evaluates, performs crossover and mutation operations, and selects the best correlation candidate with distillation accuracy. We introduce an adaptive-elite selection strategy to enhance search efficiency and strive for a balance between exploitation and exploration. Finally, we conduct training-free DAS with discovered proxy before the optimal student distillation phase. In this way, our auto-discovery framework eliminates the need for manual design and tuning, while also adapting to different search spaces through direct correlation optimization. Extensive experiments demonstrate that Auto-DAS generalizes well to various architectures and search spaces (, ResNet, ViT, NAS-Bench-101, and NAS-Bench-201), achieving state-of-the-art results in both ranking correlation and final searched accuracy. Code at: https://github.com/lliai/Auto-DAS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haosen Sun",
      "Lujun Li*",
      "Peijie Dong",
      "Zimian Wei",
      "Shitong Shao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/698_ECCV_2024_paper.php": {
    "title": "UniDream: Unifying Diffusion Priors for Relightable Text-to-3D Generation",
    "volume": "main",
    "abstract": "Recent advancements in text-to-3D generation technology have significantly advanced the conversion of textual descriptions into imaginative well-geometrical and finely textured 3D objects. Despite these developments, a prevalent limitation arises from the use of RGB data in diffusion or reconstruction models, which often results in models with inherent lighting and shadows effects that detract from their realism, thereby limiting their usability in applications that demand accurate relighting capabilities. To bridge this gap, we present UniDream, a text-to-3D generation framework by incorporating unified diffusion priors. Our approach consists of three main components: (1) a dual-phase training process to get albedo-normal aligned multi-view diffusion and reconstruction models, (2) a progressive generation procedure for geometry and albedo-textures based on Score Distillation Sample (SDS) using the trained reconstruction and diffusion models, and (3) an innovative application of SDS for finalizing PBR generation while keeping a fixed albedo based on Stable Diffusion model. Extensive evaluations demonstrate that UniDream surpasses existing methods in generating 3D objects with clearer albedo textures, smoother surfaces, enhanced realism, and superior relighting capabilities. The project homepage is at: https://UniDream.github.io",
    "checked": true,
    "id": "210e63599d49abdb848a4440d4244cdcdedeadff",
    "semantic_title": "unidream: unifying diffusion priors for relightable text-to-3d generation",
    "citation_count": 25,
    "authors": [
      "Zexiang Liu",
      "Yangguang Li",
      "Youtian Lin",
      "Xin Yu",
      "Sida Peng",
      "Yan-Pei Cao",
      "Xiaojuan Qi",
      "Xiaoshui Huang",
      "Ding Liang*",
      "Wanli Ouyang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/720_ECCV_2024_paper.php": {
    "title": "TimeCraft: Navigate Weakly-Supervised Temporal Grounded Video Question Answering via Bi-directional Reasoning",
    "volume": "main",
    "abstract": "Video reasoning typically operates within the Video Question-Answering (VQA) paradigm, which demands that the models understand and reason about video content from temporal and causal perspectives. Traditional supervised VQA methods gain this capability through meticulously annotated QA datasets, while advanced visual-language models exhibit remarkable performance due to large-scale visual-text pretraining data. Nevertheless, due to potential language bias and spurious visual-text correlations in cross-modal learning, concerns about the reliability of their answers persist in real-world applications. In this paper, we focus on the grounded VQA task, which necessitates models to provide answers along with explicit visual evidence, i.e., certain video segments. As temporal annotation is not available during training, we propose a novel bi-directional reasoning framework to perform grounded VQA in a weakly-supervised setting. Specifically, our framework consists of two parallel but dual reasoning paths. They conduct temporal grounding and answering based on the video content, approaching it from two dual directions that are symmetrical in terms of temporal order or causal relationships. By constructing a cycle-consistency relationship between these two branches, the model is prompted to provide self-guidance supervision for both temporal grounding and answering. Experiments conducted on the Next-GQA and Env-QA datasets demonstrate that our framework achieves superior performance in grounded VQA and can provide reasonable temporal locations that validate the answers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huabin Liu",
      "Xiao Ma",
      "Cheng Zhong",
      "Yang Zhang",
      "Weiyao Lin*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/725_ECCV_2024_paper.php": {
    "title": "Spectral Subsurface Scattering for Material Classification",
    "volume": "main",
    "abstract": "This study advances material classification using Spectral Sub-Surface Scattering (4) measurements. While spectrum and subsurface scattering measurements have individually been used in material classification, we argue that the strong spectral dependence of subsurface scattering lends itself to highly discriminative features. However, obtaining 4 measurements requires a time-consuming hyperspectral scan. We avoid this by showing that a carefully chosen 2D projection of the 4 point spread function is sufficient for material estimation. We also design and implement a novel imaging setup, consisting of a point illumination and a spectrally-dispersing camera, to make the desired 2D projections. Finally, through comprehensive experiments, we demonstrate the superiority of 4 imaging over spectral and sub-surface scattering measurements for the task of material classification",
    "checked": true,
    "id": "a35db7a027be591fa77f8848d6275b06f78d0106",
    "semantic_title": "spectral subsurface scattering for material classification",
    "citation_count": 0,
    "authors": [
      "Haejoon Lee*",
      "Aswin Sankaranarayanan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/730_ECCV_2024_paper.php": {
    "title": "nuCraft: Crafting High Resolution 3D Semantic Occupancy for Unified 3D Scene Understanding",
    "volume": "main",
    "abstract": "Existing benchmarks for 3D semantic occupancy prediction in autonomous driving are limited by low resolution (up to [512×512×40] with 0.2m voxel size) and inaccurate annotations, hindering the unification of 3D scene understanding through the occupancy representation. Moreover, previous methods can only generate occupancy predictions at 0.4m resolution or lower, requiring post-upsampling to reach their full resolution (0.2m). The root of these limitations lies in the sparsity, noise, and even errors present in the raw data. In this paper, we overcome these challenges by introducing nuCraft, a high-resolution and accurate semantic occupancy dataset derived from nuScenes. nuCraft offers an 8× increase in resolution ([1024 × 1024 × 80] with voxel size of 0.1m) and more precise semantic annotations compared to previous benchmarks. To address the high memory cost of high-resolution occupancy prediction, we propose VQ-Occ, a novel method that encodes occupancy data into a compact latent feature space using a VQ-VAE. This approach simplifies semantic occupancy prediction into feature simulation in the VQ latent space, making it easier and more memory-efficient. Our method enables direct generation of semantic occupancy fields at high resolution without post-upsampling, facilitating a more unified approach to 3D scene understanding. We validate the superior quality of nuCraft and the effectiveness of VQ-Occ through extensive experiments, demonstrating significant advancements over existing benchmarks and methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjin Zhu*",
      "zhe wang",
      "Hongsheng Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/742_ECCV_2024_paper.php": {
    "title": "Dynamic Neural Radiance Field From Defocused Monocular Video",
    "volume": "main",
    "abstract": "Dynamic Neural Radiance Field (NeRF) from monocular videos has recently been explored for space-time novel view synthesis and achieved excellent results. However, defocus blur caused by depth variation often occurs in video capture, compromising the quality of dynamic reconstruction because the lack of sharp details interferes with modeling temporal consistency between input views. To tackle this issue, we propose , the first dynamic NeRF method designed to restore sharp novel views from defocused monocular videos. We introduce layered Depth-of-Field (DoF) volume rendering to model the defocus blur and reconstruct a sharp NeRF supervised by defocused views. The blur model is inspired by the connection between DoF rendering and volume rendering. The opacity in volume rendering aligns with the layer visibility in DoF rendering. To execute the blurring, we modify the layered blur kernel to the ray-based kernel and employ an optimized sparse kernel to gather the input rays efficiently and render the optimized rays with our layered DoF volume rendering. We synthesize a dataset with defocused dynamic scenes for our task, and extensive experiments on our dataset show that our method outperforms existing approaches in synthesizing all-in-focus novel views from defocus blur while maintaining spatial-temporal consistency in the scene",
    "checked": true,
    "id": "0bc467097e4c45dd0302bea75b9d45ce9bc1dbbb",
    "semantic_title": "dynamic neural radiance field from defocused monocular video",
    "citation_count": 0,
    "authors": [
      "Xianrui Luo",
      "Huiqiang Sun",
      "Juewen Peng",
      "Zhiguo Cao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/744_ECCV_2024_paper.php": {
    "title": "PiTe: Pixel-Temporal Alignment for Large Video-Language Model",
    "volume": "main",
    "abstract": "Fueled by the Large Language Models (LLMs) wave, Large Visual-Language Models (LVLMs) have emerged as a pivotal advancement, bridging the gap between image and text. However, video making it challenging for LVLMs to perform adequately due to the complexity of the relationship between language and spatial-temporal data structure. Recent Large Video-Language Models (LVidLMs) align feature of static visual data like image into latent space of language feature, by general multi-modal tasks to leverage abilities of LLMs sufficiently. In this paper, we explore fine-grained alignment approach via object trajectory for different modalities across both spatial and temporal dimensions simultaneously. Thus, we propose a novel LVidLM by trajectory-guided Pixel-Temporal Alignment, dubbed PiTe, that exhibits promising applicable model property. To achieve fine-grained video-language alignment, we curate a multi-modal pre-training dataset PiTe-143k, the dataset provision of moving trajectories in pixel level for all individual objects, that appear and mention in the video and caption both, by our automatic annotation pipeline. Meanwhile, PiTe demonstrates astounding capabilities on myriad video-related multi-modal tasks through beat the state-of-the-art methods by a large margin",
    "checked": true,
    "id": "13533fd66425486da20df73851e190c6ceddfc0c",
    "semantic_title": "pite: pixel-temporal alignment for large video-language model",
    "citation_count": 0,
    "authors": [
      "Yang Liu*",
      "Pengxiang Ding",
      "Siteng Huang",
      "Min Zhang",
      "Han Zhao",
      "Donglin Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/753_ECCV_2024_paper.php": {
    "title": "CarFormer: Self-Driving with Learned Object-Centric Representations",
    "volume": "main",
    "abstract": "The choice of representation plays a key role in self-driving. Bird's eye view (BEV) representations have shown remarkable performance in recent years. In this paper, we propose to learn object-centric representations in BEV to distill a complex scene into more actionable information for self-driving. We first learn to place objects into slots with a slot attention model on BEV sequences. Based on these object-centric representations, we then train a transformer to learn to drive as well as reason about the future of other vehicles. We found that object-centric slot representations outperform both scene-level and object-level approaches that use the exact attributes of objects. Slot representations naturally incorporate information about objects from their spatial and temporal context such as position, heading, and speed without explicitly providing it. Our model with slots achieves an increased completion rate of the provided routes and, consequently, a higher driving score, with a lower variance across multiple runs, affirming slots as a reliable alternative in object-centric approaches. Additionally, we validate our model's performance as a world model through forecasting experiments, demonstrating its capability to predict future slot representations accurately. The code and the pre-trained models can be found at https://kuis-ai.github.io/CarFormer/",
    "checked": true,
    "id": "bdee94697937b6fea59e64f262888e9c80ed519a",
    "semantic_title": "carformer: self-driving with learned object-centric representations",
    "citation_count": 1,
    "authors": [
      "Shadi Hamdan*",
      "Fatma Guney"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/759_ECCV_2024_paper.php": {
    "title": "FreeDiff: Progressive Frequency Truncation for Image Editing with Diffusion Models",
    "volume": "main",
    "abstract": "Precise image editing with text-to-image models has attracted increasing interest due to their remarkable generative capabilities and user-friendly nature. However, such attempts face the pivotal challenge of misalignment between the intended precise editing target regions and the broader area impacted by the guidance in practice. Despite excellent methods leveraging attention mechanisms that have been developed to refine the editing guidance, these approaches necessitate modifications through complex network architecture and are limited to specific editing tasks. In this work, we re-examine the diffusion process and misalignment problem from a frequency perspective, revealing that, due to the power law of natural images and the decaying noise schedule, the denoising network primarily recovers low-frequency image components during the earlier timesteps and thus brings excessive low-frequency signals for editing. Leveraging this insight, we introduce a novel fine-tuning free approach that employs progressive Frequency truncation to refine the guidance of Diff usion models for universal editing tasks (FreeDiff ). Our method achieves comparable results with state-of-the-art methods across a variety of editing tasks and on a diverse set of images, highlighting its potential as a versatile tool in image editing applications",
    "checked": true,
    "id": "c1ec1123e69e14db895cdf426eedd22942b2d231",
    "semantic_title": "freediff: progressive frequency truncation for image editing with diffusion models",
    "citation_count": 0,
    "authors": [
      "Wei WU*",
      "Qingnan Fan",
      "Shuai Qin",
      "Hong Gu",
      "Ruoyu Zhao",
      "Antoni Chan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/763_ECCV_2024_paper.php": {
    "title": "Plain-Det: A Plain Multi-Dataset Object Detector",
    "volume": "main",
    "abstract": "Recent advancements in large-scale foundational models have sparked widespread interest in training highly proficient large vision models. A common consensus revolves around the necessity of aggregating extensive, high-quality annotated data. However, given the inherent challenges in annotating dense tasks in computer vision, such as object detection and segmentation, a practical strategy is to combine and leverage all available data for training purposes. In this work, we propose Plain-Det, which offers flexibility to accommodate new datasets, robustness in performance across diverse datasets, training efficiency, and compatibility with various detection architectures. We utilize Def-DETR, with the assistance of Plain-Det, to achieve a mAP of 51.9 on COCO, matching the current state-of-the-art detectors. We conduct extensive experiments on 13 downstream datasets and Plain-Det demonstrates strong generalization capability. Code is release at https://github.com/ChengShiest/Plain-Det",
    "checked": true,
    "id": "b5ccbfa9380682f1078a185630cfd4dd1a593167",
    "semantic_title": "plain-det: a plain multi-dataset object detector",
    "citation_count": 0,
    "authors": [
      "Cheng Shi",
      "Yuchen Zhu",
      "Sibei Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/764_ECCV_2024_paper.php": {
    "title": "Alternate Diverse Teaching for Semi-supervised Medical Image Segmentation",
    "volume": "main",
    "abstract": "Semi-supervised medical image segmentation has shown promise in training models with limited labeled data. However, current dominant teacher-student based approaches can suffer from the confirmation bias. To address this challenge, we propose AD-MT, an alternate diverse teaching approach in a teacher-student framework. It involves a single student model and two non-trainable teacher models that are momentum-updated periodically and randomly in an alternate fashion. To mitigate the confirmation bias via the diverse supervision, the core of AD-MT lies in two proposed modules: the Random Periodic Alternate (RPA) Updating Module and the Conflict-Combating Module (CCM). The RPA schedules an alternating diverse updating process with complementary unlabeled data batches, distinct data augmentation, and random switching periods to encourage diverse reasoning from different teaching perspectives. The CCM employs an entropy-based ensembling strategy to encourage the model to learn from both the consistent and conflicting predictions between the teachers. Experimental results demonstrate the effectiveness and superiority of AD-MT on the 2D and 3D medical segmentation benchmarks across various semi-supervised settings",
    "checked": true,
    "id": "a03020c47a5b3daf35781d4e5b5d1ac46116f584",
    "semantic_title": "alternate diverse teaching for semi-supervised medical image segmentation",
    "citation_count": 3,
    "authors": [
      "Zhen Zhao*",
      "Zicheng Wang",
      "Dian Yu",
      "Longyue Wang*",
      "Yixuan Yuan",
      "Luping Zhou"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/780_ECCV_2024_paper.php": {
    "title": "Cs2K: Class-specific and Class-shared Knowledge Guidance for Incremental Semantic Segmentation",
    "volume": "main",
    "abstract": "Incremental semantic segmentation endeavors to segment newly encountered classes while maintaining knowledge of old classes. However, existing methods either 1) lack guidance from class-specific knowledge (i.e., old class prototypes), leading to a bias towards new classes, or 2) constrain class-shared knowledge (i.e., old model weights) excessively without discrimination, resulting in a preference for old classes. In this paper, to trade off model performance, we propose the Class-specific and Class-shared Knowledge (Cs2 K) guidance for incremental semantic segmentation. Specifically, from the class-specific knowledge aspect, we design a prototype-guided pseudo labeling that exploits feature proximity from prototypes to correct pseudo labels, thereby overcoming catastrophic forgetting. Meanwhile, we develop a prototype-guided class adaptation that aligns class distribution across datasets via learning old augmented prototypes. Moreover, from the class-shared knowledge aspect, we propose a weight-guided selective consolidation to strengthen old memory while maintaining new memory by integrating old and new model weights based on weight importance relative to old classes. Experiments on public datasets demonstrate that our proposed Cs2 K significantly improves segmentation performance and is plug-and-play",
    "checked": true,
    "id": "017c7d81093ae136c340e36115578add194c6678",
    "semantic_title": "cs2k: class-specific and class-shared knowledge guidance for incremental semantic segmentation",
    "citation_count": 0,
    "authors": [
      "Wei Cong*",
      "Yang Cong",
      "Yuyang Liu",
      "Gan Sun"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/786_ECCV_2024_paper.php": {
    "title": "Synchronous Diffusion for Unsupervised Smooth Non-Rigid 3D Shape Matching",
    "volume": "main",
    "abstract": "Most recent unsupervised non-rigid 3D shape matching methods are based on the functional map framework due to its efficiency and superior performance. Nevertheless, respective methods struggle to obtain spatially smooth pointwise correspondences due to the lack of proper regularisation. In this work, inspired by the success of message passing on graphs, we propose a synchronous diffusion process which we use as regularisation to achieve smoothness in non-rigid 3D shape matching problems. The intuition of synchronous diffusion is that diffusing the same input function on two different shapes results in consistent outputs. Using different challenging datasets, we demonstrate that our novel regularisation can substantially improve the state-of-the-art in shape matching, especially in the presence of topological noise",
    "checked": true,
    "id": "24e8741b8ec40aff686daf9efea18b817869f77c",
    "semantic_title": "synchronous diffusion for unsupervised smooth non-rigid 3d shape matching",
    "citation_count": 0,
    "authors": [
      "Dongliang Cao*",
      "Zorah Laehner",
      "Florian Bernard"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/790_ECCV_2024_paper.php": {
    "title": "Text-Guided Video Masked Autoencoder",
    "volume": "main",
    "abstract": "Recent video masked autoencoder (MAE) works have designed improved masking algorithms focused on saliency. These works leverage visual cues such as motion to mask the most salient regions. However, the robustness of such visual cues depends on how often input videos match underlying assumptions. On the other hand, natural language description is an information dense representation of video that implicitly captures saliency without requiring modality-specific assumptions, and has not been explored yet for video MAE. To this end, we introduce a novel text-guided masking algorithm (TGM) that masks the video regions with highest correspondence to paired captions. Without leveraging any explicit visual cues for saliency, our TGM is competitive with state-of-the-art masking algorithms such as motion-guided masking. To further benefit from the semantics of natural language for masked reconstruction, we next introduce a unified framework for joint MAE and masked video-text contrastive learning. We show that across existing masking algorithms, unifying MAE and masked video-text contrastive learning improves downstream performance compared to pure MAE on a variety of video recognition tasks, especially for linear probe. Within this unified framework, our TGM achieves the best relative performance on five action recognition and one egocentric datasets, highlighting the complementary nature of natural language for masked video modeling",
    "checked": true,
    "id": "f631ff4f13b6b85f67d06f5c050c5ef270f89267",
    "semantic_title": "text-guided video masked autoencoder",
    "citation_count": 0,
    "authors": [
      "David Fan*",
      "Jue Wang",
      "Shuai Liao",
      "Zhikang Zhang",
      "Vimal Bhat",
      "Xinyu Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/794_ECCV_2024_paper.php": {
    "title": "Diffusion Models for Open-Vocabulary Segmentation",
    "volume": "main",
    "abstract": "Open-vocabulary segmentation is the task of segmenting anything that can be named in an image. Recently, large-scale vision-language modelling has led to significant advances in open-vocabulary segmentation, but at the cost of gargantuan and increasing training and annotation efforts. Hence, we ask if it is possible to use existing foundation models to synthesise on-demand efficient segmentation algorithms for specific class sets, making them applicable in an open-vocabulary setting without the need to collect further data, annotations or perform training. To that end, we present , a novel method that leverages generative text-to-image diffusion models for unsupervised open-vocabulary segmentation. synthesises support image sets for arbitrary textual categories, creating for each a set of prototypes representative of both the category and its surrounding context (background). It relies solely on pre-trained components and outputs the synthesised segmenter directly, without training. Our approach shows strong performance on a range of benchmarks, obtaining a lead of more than 5% over prior work on PASCAL VOC",
    "checked": true,
    "id": "fd6d5e39d4e6641f3a1b7bdebd9f649c2c3705a8",
    "semantic_title": "diffusion models for open-vocabulary segmentation",
    "citation_count": 52,
    "authors": [
      "Laurynas Karazija*",
      "Iro Laina",
      "Andrea Vedaldi",
      "Christian Rupprecht"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/796_ECCV_2024_paper.php": {
    "title": "Textual-Visual Logic Challenge: Understanding and Reasoning in Text-to-Image Generation",
    "volume": "main",
    "abstract": "Text-to-image generation plays a pivotal role in computer vision and natural language processing by translating textual descriptions into visual representations. However, understanding complex relations in detailed text prompts filled with rich relational content remains a significant challenge. To address this, we introduce a novel task: Logic-Rich Text-to-Image generation. Unlike conventional image generation tasks that rely on short and structurally simple natural language inputs, our task focuses on intricate text inputs abundant in relational information. To tackle these complexities, we collect the Textual-Visual Logic dataset, designed to evaluate the performance of text-to-image generation models across diverse and complex scenarios. Furthermore, we propose a baseline model as a benchmark for this task. Our model comprises three key components: a relation understanding module, a multimodality fusion module, and a negative pair discriminator. These components enhance the model's ability to handle disturbances in informative tokens and prioritize relational elements during image generation. https:// github.com/IntelLabs/Textual-Visual-Logic-Challenge",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peixi Xiong*",
      "Michael A Kozuch",
      "Nilesh Jain"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/799_ECCV_2024_paper.php": {
    "title": "EvSign: Sign Language Recognition and Translation with Streaming Events",
    "volume": "main",
    "abstract": "Sign language is one of the most effective communication tools for people with hearing difficulties. Most existing works focus on improving the performance of sign language tasks on RGB videos, which may suffer from degraded recording conditions, such as fast movement of hands with motion blur and textured signer's appearance. The bio-inspired event camera, which asynchronously captures brightness change with high speed, could naturally perceive dynamic hand movements, providing rich manual clues for sign language tasks. In this work, we aim at exploring the potential of event camera in continuous sign language recognition (CSLR) and sign language translation (SLT). To promote the research, we first collect an event-based benchmark EvSign for those tasks with both gloss and spoken language annotations. EvSign dataset offers a substantial amount of high-quality event streams and an extensive vocabulary of glosses and words, thereby facilitating the development of sign language tasks. In addition, we propose an efficient transformer-based framework for event-based SLR and SLT tasks, which fully leverages the advantages of streaming events. The sparse backbone is employed to extract visual features from sparse events. Then, the temporal coherence is effectively utilized through the proposed local token fusion and gloss-aware temporal aggregation modules. Extensive experimental results are reported on both simulated (PHOENIX14T) and EvSign datasets. Our method performs favorably against existing state-of-the-art approaches with only 0.34% computational cost (0.84G FLOPS per video) and 44.2% network parameters. The project is available at https://zhang-pengyu.github.io/EVSign",
    "checked": true,
    "id": "5fadb52eeebcf45283a87972ffb28b56b22a8cb6",
    "semantic_title": "evsign: sign language recognition and translation with streaming events",
    "citation_count": 1,
    "authors": [
      "Pengyu Zhang*",
      "Hao Yin",
      "Zeren Wang",
      "Wenyue Chen",
      "Sheng Ming Li",
      "Dong Wang",
      "Huchuan Lu",
      "Xu Jia"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/808_ECCV_2024_paper.php": {
    "title": "QUAR-VLA: Vision-Language-Action Model for Quadruped Robots",
    "volume": "main",
    "abstract": "The important manifestation of robot intelligence is the ability to naturally interact and autonomously make decisions. Traditional quadruped robot learning typically handles language interaction and visual autonomous perception separately, which, while simplifying system design, also limits the synergy between different information streams. This separation poses challenges in achieving seamless autonomous reasoning, decision-making, and action execution. To address these limitations, a novel paradigm, named Vision-Language-Action tasks for QUAdruped Robots (QUAR-VLA), has been introduced in this paper. This approach tightly integrates visual information and instructions to generate executable actions, effectively merging perception, planning, and decision-making. The central idea is to elevate the overall intelligence of the robot. Within this framework, a notable challenge lies in aligning fine-grained instructions with visual perception information. This emphasizes the complexity involved in ensuring that the robot accurately interprets and acts upon detailed instructions in harmony with its visual observations. Consequently, we propose QUAdruped Robotic Transformer (QUART), a VLA model to integrate visual information and instructions from diverse modalities as input and generates executable actions for real-world robots and present QUAdruped Robot Dataset (QUARD), a large-scale multi-task dataset including perception, navigation and advanced capability like whole-body manipulation tasks for training QUART model. Our extensive evaluation shows that our approach leads to performant robotic policies and enables QUART to obtain a range of generalization capabilities",
    "checked": true,
    "id": "c0b3a44ff6b482a073138eb243da9b762eb1cd1f",
    "semantic_title": "quar-vla: vision-language-action model for quadruped robots",
    "citation_count": 4,
    "authors": [
      "Pengxiang Ding",
      "Han Zhao",
      "Wenjie Zhang",
      "Wenxuan Song",
      "Min Zhang",
      "Siteng Huang",
      "Ningxi Yang",
      "Donglin Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/812_ECCV_2024_paper.php": {
    "title": "Zero-shot Object Counting with Good Exemplars",
    "volume": "main",
    "abstract": "Zero-shot object counting (ZOC) aims to enumerate objects in images using only the names of object classes during testing, without the need for manual annotations. However, a critical challenge in current ZOC methods lies in their inability to identify high-quality exemplars effectively. This deficiency hampers scalability across diverse classes and undermines the development of strong visual associations between the identified classes and image content. To this end, we propose the Visual Association-based Zero-shot Object Counting (VA-Count) framework. VA-Count consists of an Exemplar Enhancement Module (EEM) and a Noise Suppression Module (NSM) that synergistically refine the process of class exemplar identification while minimizing the consequences of incorrect object identification. The EEM utilizes advanced vision-language pretaining models to discover potential exemplars, ensuring the framework's adaptability to various classes. Meanwhile, the NSM employs contrastive learning to differentiate between optimal and suboptimal exemplar pairs, reducing the negative effects of erroneous exemplars. VA-Count demonstrates its effectiveness and scalability in zero-shot contexts with superior performance on two object counting datasets",
    "checked": true,
    "id": "8f769e9a659e0f5ffa6ca5d0f465827bf6d8905f",
    "semantic_title": "zero-shot object counting with good exemplars",
    "citation_count": 0,
    "authors": [
      "Huilin Zhu",
      "Jingling Yuan",
      "Zhengwei Yang",
      "Yu Guo",
      "Xian Zhong*",
      "Zheng Wang",
      "Shengfeng He*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/813_ECCV_2024_paper.php": {
    "title": "TextDiffuser-2: Unleashing the Power of Language Models for Text Rendering",
    "volume": "main",
    "abstract": "The diffusion model has been proven a powerful generative model in recent years, yet it remains a challenge in generating visual text. Although existing work has endeavored to enhance the accuracy of text rendering, these methods still suffer from several drawbacks, such as (1) limited flexibility and automation, (2) constrained capability of layout prediction, and (3) restricted diversity. In this paper, we present TextDiffuser-2, aiming to unleash the power of language models for text rendering while taking these three aspects into account. Firstly, we fine-tune a large language model for layout planning. The large language model is capable of automatically generating keywords and placing the text in optimal positions for text rendering. Secondly, we utilize the language model within the diffusion model to encode the position and content of keywords at the line level. Unlike previous methods that employed tight character-level guidance, our approach generates more diverse text images. We conduct extensive experiments and incorporate user studies involving human participants and GPT-4V, validating TextDiffuser-2's capacity to achieve a more rational text layout and generation with enhanced diversity. Furthermore, the proposed methods are compatible with existing text rendering techniques, such as TextDiffuser and GlyphControl, serving to enhance automation and diversity, as well as augment the rendering accuracy. For instance, by using the proposed layout planner, TextDiffuser is capable of rendering text with more aesthetically pleasing line breaks and alignment, meanwhile obviating the need for explicit keyword specification. Furthermore, GlyphControl can leverage the layout planner to achieve diverse layouts without the necessity for user-specified glyph images, and the rendering F-measure can be boosted by 6.51% when using the proposed layout encoding training technique. The code and model are available at https: //aka.ms/textdiffuser-2",
    "checked": true,
    "id": "1c6e2a4da1ead685a95c079751bf4d7a727d8180",
    "semantic_title": "textdiffuser-2: unleashing the power of language models for text rendering",
    "citation_count": 31,
    "authors": [
      "Jingye Chen*",
      "Yupan Huang",
      "Tengchao Lv",
      "Lei Cui",
      "Qifeng Chen",
      "Furu Wei"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/815_ECCV_2024_paper.php": {
    "title": "SFPNet: Sparse Focal Point Network for Semantic Segmentation on General LiDAR Point Clouds",
    "volume": "main",
    "abstract": "Although LiDAR semantic segmentation advances rapidly, state-of-the-art methods often incorporate specifically designed inductive bias derived from benchmarks originating from mechanical spinning LiDAR. This can limit model generalizability to other kinds of LiDAR technologies and make hyperparameter tuning more complex. To tackle these issues, we propose a generalized framework to accommodate various types of LiDAR prevalent in the market by replacing window-attention with our sparse focal point modulation. Our SFPNet is capable of extracting multi-level contexts and dynamically aggregating them using a gate mechanism. By implementing a channel-wise information query, features that incorporate both local and global contexts are encoded. We also introduce a novel large-scale hybrid-solid LiDAR semantic segmentation dataset for robotic applications. SFPNet demonstrates competitive performance on conventional benchmarks derived from mechanical spinning LiDAR, while achieving state-of-the-art results on benchmark derived from solid-state LiDAR. Additionally, it outperforms existing methods on our novel dataset sourced from hybrid-solid LiDAR. Code and dataset are available at https://github.com/Cavendish518/SFPNet and https://www.semanticindustry.top",
    "checked": true,
    "id": "3fff2afffcf18086cfdf8f43c65fe899821ffca7",
    "semantic_title": "sfpnet: sparse focal point network for semantic segmentation on general lidar point clouds",
    "citation_count": 2,
    "authors": [
      "Yanbo Wang*",
      "Wentao Zhao",
      "Cao Chuan",
      "Tianchen Deng",
      "Jingchuan Wang",
      "Weidong Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/817_ECCV_2024_paper.php": {
    "title": "PartSTAD: 2D-to-3D Part Segmentation Task Adaptation",
    "volume": "main",
    "abstract": "We introduce , a method designed for the task adaptation of 2D-to-3D segmentation lifting. Recent studies have highlighted the advantages of utilizing 2D segmentation models to achieve high-quality 3D segmentation through few-shot adaptation. However, previous approaches have focused on adapting 2D segmentation models for domain shift to rendered images and synthetic text descriptions, rather than optimizing the model specifically for 3D segmentation. Our proposed task adaptation method finetunes a 2D bounding box prediction model with an objective function for 3D segmentation. We introduce weights for 2D bounding boxes for adaptive merging and learn the weights using a small additional neural network. Additionally, we incorporate SAM, a foreground segmentation model on a bounding box, to improve the boundaries of 2D segments and consequently those of 3D segmentation. Our experiments on the PartNet-Mobility dataset show significant improvements with our task adaptation approach, achieving a 7.0%p increase in mIoU and a 5.2%p improvement in mAP50 for semantic and instance segmentation compared to the SotA few-shot 3D segmentation model. The code is available at https://github.com/KAIST-Visual-AI-Group/PartSTAD",
    "checked": true,
    "id": "be3992ba496b493295699d647553fecf6482bfd8",
    "semantic_title": "partstad: 2d-to-3d part segmentation task adaptation",
    "citation_count": 2,
    "authors": [
      "Hyunjin Kim",
      "Minhyuk Sung*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/828_ECCV_2024_paper.php": {
    "title": "FutureDepth: Learning to Predict the Future Improves Video Depth Estimation",
    "volume": "main",
    "abstract": "In this paper, we propose a novel video depth estimation approach, , which enables the model to implicitly leverage multi-frame and motion cues to improve depth estimation by making it learn to predict the future at training. More specifically, we propose a future prediction network, F-Net, which takes the features of multiple consecutive frames and is trained to predict multi-frame features one time step ahead iteratively. In this way, F-Net learns the underlying motion and correspondence information, and we incorporate its features into the depth decoding process. Additionally, to enrich the learning of multi-frame correspondence cues, we further leverage a reconstruction network, R-Net, which is trained via adaptively masked auto-encoding of multi-frame feature volumes. At inference time, both F-Net and R-Net are used to produce queries to work with the depth decoder, as well as a final refinement network. Through extensive experiments on several benchmarks, i.e., NYUDv2, KITTI, DDAD, and Sintel, which cover indoor, driving, and open-domain scenarios, we show that significantly improves upon baseline models, outperforms existing video depth estimation methods, and sets new state-of-the-art (SOTA) accuracy. Furthermore, is more efficient than existing SOTA video depth estimation models and has similar latencies when comparing to monocular models",
    "checked": true,
    "id": "a09e2608b6036d8b7f5de575e0aeb30b4c81a002",
    "semantic_title": "futuredepth: learning to predict the future improves video depth estimation",
    "citation_count": 3,
    "authors": [
      "Rajeev Yasarla*",
      "Manish Kumar Singh",
      "Hong Cai",
      "Yunxiao Shi",
      "Jisoo Jeong",
      "Yinhao Zhu",
      "Shizhong Han",
      "Risheek Garrepalli",
      "Fatih Porikli"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/833_ECCV_2024_paper.php": {
    "title": "LLM as Copilot for Coarse-grained Vision-and-Language Navigation",
    "volume": "main",
    "abstract": "Vision-and-Language Navigation (VLN) involves guiding an agent through indoor environments using human-provided textual instructions. Coarse-grained VLN, with short and high-level instructions, has gained popularity as it closely mirrors real-world scenarios. However, a significant challenge is these instructions are often too concise for agents to comprehend and act upon. Previous studies have explored allowing agents to seek assistance during navigation, but typically offer rigid support from pre-existing datasets or simulators. The advent of Large Language Models (LLMs) presents a novel avenue for aiding VLN agents. This paper introduces VLN-Copilot, a framework enabling agents to actively seek assistance when encountering confusion, with the LLM serving as a copilot to facilitate navigation. Our approach includes the introduction of a confusion score, quantifying the level of uncertainty in an agent's action decisions, while the LLM offers real-time detailed guidance for navigation. Experimental results on two coarse-grained VLN datasets show the efficacy of our method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanyuan Qiao*",
      "Qianyi Liu",
      "Jiajun Liu",
      "Jing Liu",
      "Qi Wu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/841_ECCV_2024_paper.php": {
    "title": "Raindrop Clarity: A Dual-Focused Dataset for Day and Night Raindrop Removal",
    "volume": "main",
    "abstract": "Existing raindrop removal datasets have two shortcomings. First, they consist of images captured by cameras with a focus on the background, leading to the presence of blurry raindrops. To our knowledge, none of these datasets include images where the focus is specifically on raindrops, which results in a blurry background. Second, these datasets predominantly consist of daytime images, thereby lacking nighttime raindrop scenarios. Consequently, algorithms trained on these datasets may struggle to perform effectively in raindrop-focused or nighttime scenarios. The absence of datasets specifically designed for raindrop-focused and nighttime raindrops constrains research in this area. In this paper, we introduce a large-scale, real-world raindrop removal dataset called Raindrop Clarity. Raindrop Clarity comprises 15,186 high-quality pairs/triplets (raindrops, blur, and background) of images with raindrops and the corresponding clear background images. There are 5,442 daytime raindrop images and 9,744 nighttime raindrop images. Specifically, the 5,442 daytime images include 3,606 raindropand 1,836 background-focused images. While the 9,744 nighttime images contain 4,834 raindrop- and 4,906 background-focused images. Our dataset will enable the community to explore background-focused and raindrop-focused images, including challenges unique to daytime and nighttime conditions. 1 1 Our data and code are available at: https://github.com/jinyeying/RaindropClarity",
    "checked": true,
    "id": "a977ffc91624b93634ff1f5c1a09bc5292d93d88",
    "semantic_title": "raindrop clarity: a dual-focused dataset for day and night raindrop removal",
    "citation_count": 0,
    "authors": [
      "Yeying Jin*",
      "Xin Li",
      "Jiadong Wang",
      "Yan Zhan",
      "Malu Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/851_ECCV_2024_paper.php": {
    "title": "Unsupervised Moving Object Segmentation with Atmospheric Turbulence",
    "volume": "main",
    "abstract": "Moving object segmentation in the presence of atmospheric turbulence is highly challenging due to turbulence-induced irregular and time-varying distortions. In this paper, we present an unsupervised approach for segmenting moving objects in videos downgraded by atmospheric turbulence. Our key approach is a detect-then-grow scheme: we first identify a small set of moving object pixels with high confidence, then gradually grow a foreground mask from those seeds to segment all moving objects. This method leverages rigid geometric consistency among video frames to disentangle different types of motions, and then uses the Sampson distance to initialize the seedling pixels. After growing per-frame foreground masks, we use spatial grouping loss and temporal consistency loss to further refine the masks in order to ensure their spatio-temporal consistency. Our method is unsupervised and does not require training on labeled data. For validation, we collect and release the first real-captured long-range turbulent video dataset with ground truth masks for moving objects. Results show that our method achieves good accuracy in segmenting moving objects and is robust for long-range videos with various turbulence strengths",
    "checked": true,
    "id": "94f454011ab17da4b07450cdd84f7004d5b8fbe2",
    "semantic_title": "unsupervised moving object segmentation with atmospheric turbulence",
    "citation_count": 0,
    "authors": [
      "Dehao Qin*",
      "Ripon k Saha",
      "Woojeh Chung",
      "Suren Jayasuriya",
      "Jinwei Ye",
      "Nianyi Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/857_ECCV_2024_paper.php": {
    "title": "AccDiffusion: An Accurate Method for Higher-Resolution Image Generation",
    "volume": "main",
    "abstract": "This paper attempts to address the object repetition issue in patch-wise higher-resolution image generation. We propose AccDiffusion, an accurate method for patch-wise higher-resolution image generation without training. An in-depth analysis in this paper reveals an identical text prompt for different patches causes repeated object generation, while no prompt compromises the image details. Therefore, our AccDiffusion, for the first time, proposes to decouple the vanilla image-content-aware prompt into a set of patch-content-aware prompts, each of which serves as a more precise description of an image patch. Besides, AccDiffusion also introduces dilated sampling with window interaction for better global consistency in higher-resolution image generation. Experimental comparison with existing methods demonstrates that our AccDiffusion effectively addresses the issue of repeated object generation and leads to better performance in higher-resolution image generation. Our code is released at https://github. com/lzhxmu/AccDiffusion",
    "checked": true,
    "id": "1006a18cce8fb5326b097a21b24b1e208060fe93",
    "semantic_title": "accdiffusion: an accurate method for higher-resolution image generation",
    "citation_count": 7,
    "authors": [
      "Zhihang Lin",
      "Mingbao Lin",
      "Meng Zhao",
      "Rongrong Ji*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/861_ECCV_2024_paper.php": {
    "title": "Uncertainty-Driven Spectral Compressive Imaging with Spatial-Frequency Transformer",
    "volume": "main",
    "abstract": "Recently, learning-based Hyperspectral image (HSI) reconstruction methods have demonstrated promising performance. However, existing learning-based methods still face two issues. 1) They rarely consider both the spatial sparsity and inter-spectral similarity priors of HSI. 2) They treat all image regions equally, ignoring that texture-rich and edge regions are more difficult to reconstruct than smooth regions. To address these issues, we propose an uncertainty-driven HSI reconstruction method termed Specformer. Specifically, we first introduce a frequency-wise self-attention (FWSA) module, and combine it with a spatial-wise local-window self-attention (LWSA) module in parallel to form a Spatial-Frequency (SF) block. LWSA can guide the network to focus on the regions with dense spectral information, and FWSA can capture the inter-spectral similarity. Parallel design helps the network to model cross-window connections, and expand its receptive fields while maintaining linear complexity. We use SF-block as the main building block in a multi-scale U-shape network to form our Specformer. In addition, we introduce an uncertainty-driven loss function, which can reinforce the network's attention to the challenging regions with rich textures and edges. Experiments on simulated and real HSI datasets show that our Specformer outperforms state-of-the-art methods with lower computational and memory costs. The code is available at https://github.com/bianlab/Specformer",
    "checked": true,
    "id": "3ba9f0ae519f98bbdb991c45f48df984bbd54fc9",
    "semantic_title": "uncertainty-driven spectral compressive imaging with spatial-frequency transformer",
    "citation_count": 0,
    "authors": [
      "Lintao Peng",
      "Siyu Xie",
      "Liheng Bian*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/869_ECCV_2024_paper.php": {
    "title": "CaesarNeRF: Calibrated Semantic Representation for Few-Shot Generalizable Neural Rendering",
    "volume": "main",
    "abstract": "Generalizability and few-shot learning are key challenges in Neural Radiance Fields (NeRF), often due to the lack of a holistic understanding in pixel-level rendering. We introduce CaesarNeRF, an end-to-end approach that leverages scene-level CAlibratEd SemAntic Representation along with pixel-level representations to advance few-shot, generalizable neural rendering, facilitating a holistic understanding without compromising high-quality details. CaesarNeRF explicitly models pose differences of reference views to combine scene-level semantic representations, providing a calibrated holistic understanding. This calibration process aligns various viewpoints with precise location and is further enhanced by sequential refinement to capture varying details. Extensive experiments on public datasets, including LLFF, Shiny, mip-NeRF 360, and MVImgNet, show that CaesarNeRF delivers state-of-the-art performance across varying numbers of reference views, † proving effective even with a single reference image. ∗ Equal contribution. Corresponding author. This work was done when Haidong Zhu was an intern at Microsoft",
    "checked": true,
    "id": "23c3144a1f1447adfdda92f8cda08a1b2e7b3d69",
    "semantic_title": "caesarnerf: calibrated semantic representation for few-shot generalizable neural rendering",
    "citation_count": 3,
    "authors": [
      "Haidong Zhu",
      "Tianyu Ding*",
      "Tianyi Chen",
      "Ilya Zharkov",
      "Ram Nevatia",
      "Luming Liang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/926_ECCV_2024_paper.php": {
    "title": "MapTracker: Tracking with Strided Memory Fusion for Consistent Vector HD Mapping",
    "volume": "main",
    "abstract": "This paper presents a vector HD-mapping algorithm that formulates the mapping as a tracking task and uses a history of memory latents to ensure consistent reconstructions over time. Our method, , accumulates a sensor stream into memory buffers of two latent representations: 1) Raster latents in the bird's-eye-view (BEV) space and 2) Vector latents over the road elements (i.e., pedestrian-crossings, lane-dividers, and road-boundaries). The approach borrows the query propagation paradigm from the tracking literature that explicitly associates tracked road elements from the previous frame to the current, while fusing a subset of memory latents selected with distance strides to further enhance temporal consistency. A vector latent is decoded to reconstruct the geometry of a road element. The paper further makes benchmark contributions by 1) Improving processing code for existing datasets to produce consistent ground truth with temporal alignments and 2) Augmenting existing mAP metrics with consistency checks. significantly outperforms existing methods on both nuScenes and Agroverse2 datasets by over 8% and 19% on the conventional and the new consistency-aware metrics, respectively. The code and models are available on our project page: https://map-tracker.github.io",
    "checked": true,
    "id": "68e0eb0041be94fd527c1e3907a352a49583fa64",
    "semantic_title": "maptracker: tracking with strided memory fusion for consistent vector hd mapping",
    "citation_count": 11,
    "authors": [
      "Jiacheng Chen*",
      "Yuefan Wu",
      "Jiaqi Tan",
      "Hang Ma",
      "Yasutaka Furukawa*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/929_ECCV_2024_paper.php": {
    "title": "Image Demoireing in RAW and sRGB Domains",
    "volume": "main",
    "abstract": "Moiré patterns frequently appear when capturing screens with smartphones or cameras, potentially compromising image quality. Previous studies suggest that moiré pattern elimination in the RAW domain offers greater effectiveness compared to demoiréing in the sRGB domain. Nevertheless, relying solely on RAW data for image demoiréing is insufficient in mitigating the color cast due to the absence of essential information required for the color correction by the image signal processor (ISP). In this paper, we propose to jointly utilize both RAW and sRGB data for image demoiréing (RRID), which are readily accessible in modern smartphones and DSLR cameras. We develop Skip-Connection-based Demoiréing Module (SCDM) with Gated Feedback Module (GFM) and Frequency Selection Module (FSM) embedded in skip-connections for the efficient and effective demoiréing of RAW and sRGB features, respectively. Subsequently, we design a RGB Guided ISP (RGISP) to learn a device-dependent ISP, assisting the process of color recovery. Extensive experiments demonstrate that our RRID outperforms state-of-the-art approaches, in terms of the performance in moiré pattern removal and color cast correction by 0.62dB in PSNR and 0.003 in SSIM. Code is available at https://github.com/rebeccaeexu/RRID",
    "checked": true,
    "id": "6c4c8868c08e0c17b57d5d727d8fe8a02e098c74",
    "semantic_title": "image demoireing in raw and srgb domains",
    "citation_count": 1,
    "authors": [
      "Shuning Xu",
      "Binbin Song",
      "Xiangyu Chen",
      "Xina Liu",
      "Jiantao Zhou*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/932_ECCV_2024_paper.php": {
    "title": "LiDAR-Event Stereo Fusion with Hallucinations",
    "volume": "main",
    "abstract": "Event stereo matching is an emerging technique to estimate depth from neuromorphic cameras; however, events are unlikely to trigger in the absence of motion or the presence of large, untextured regions, making the correspondence problem extremely challenging. Purposely, we propose integrating a stereo event camera with a fixed-frequency active sensor – e.g., a LiDAR – collecting sparse depth measurements, overcoming the aforementioned limitations. Such depth hints are used by hallucinating – i.e., inserting fictitious events – the stacks or raw input streams, compensating for the lack of information in the absence of brightness changes. Our techniques are general, can be adapted to any structured representation to stack events and outperform state-of-the-art fusion methods applied to event-based stereo",
    "checked": true,
    "id": "7736b2cc87b8ef769e7cbefac0865ad2e04acb0a",
    "semantic_title": "lidar-event stereo fusion with hallucinations",
    "citation_count": 1,
    "authors": [
      "Luca Bartolomei*",
      "Matteo Poggi",
      "Andrea Conti",
      "Stefano Mattoccia*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/935_ECCV_2024_paper.php": {
    "title": "X-Former: Unifying Contrastive and Reconstruction Learning for MLLMs",
    "volume": "main",
    "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have revolutionized the field of vision-language understanding by integrating visual perception capabilities into Large Language Models (LLMs). The prevailing trend in this field involves the utilization of a vision encoder derived from vision-language contrastive learning (CL), showing expertise in capturing overall representations while facing difficulties in capturing detailed local patterns. In this work, we focus on enhancing the visual representations for MLLMs by combining high-frequency and detailed visual representations, obtained through masked image modeling (MIM), with semantically-enriched low-frequency representations captured by CL. To achieve this goal, we introduce X-Former which is a lightweight transformer module designed to exploit the complementary strengths of CL and MIM through an innovative interaction mechanism. Specifically, X-Former first bootstraps vision-language representation learning and multimodal-to-multimodal generative learning from two frozen vision encoders, i.e., CLIP-ViT (CL-based) and MAE-ViT (MIM-based). It further bootstraps vision-to-language generative learning from a frozen LLM to ensure visual features from X-Former can be interpreted by the LLM. To demonstrate the effectiveness of our approach, we assess its performance on tasks demanding detailed visual understanding. Extensive evaluations indicate that X-Former excels in visual reasoning tasks involving both structural and semantic categories in the GQA dataset. Assessment on fine-grained visual perception benchmark further confirms its superior capabilities in visual understanding",
    "checked": true,
    "id": "bd6c3f2fe1d0ae1bf9a842e923a052cddef586f9",
    "semantic_title": "x-former: unifying contrastive and reconstruction learning for mllms",
    "citation_count": 0,
    "authors": [
      "Sirnam Swetha*",
      "Jinyu Yang",
      "Tal Neiman",
      "Mamshad Nayeem Rizve",
      "Son Tran",
      "Benjamin Yao",
      "Trishul A Chilimbi",
      "Mubarak Shah"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/941_ECCV_2024_paper.php": {
    "title": "Learning Anomalies with Normality Prior for Unsupervised Video Anomaly Detection",
    "volume": "main",
    "abstract": "Unsupervised video anomaly detection (UVAD) aims to detect abnormal events in videos without any annotations. It remains challenging because anomalies are rare, diverse, and usually not well-defined. Existing UVAD methods are purely data-driven and perform unsupervised learning by identifying various abnormal patterns in videos. Since these methods largely rely on the feature representation and data distribution, they can only learn salient anomalies that are substantially different from normal events but ignore the less distinct ones. To address this challenge, this paper pursues a different approach that leverages data-irrelevant prior knowledge about normal and abnormal events for UVAD. We first propose a new normality prior for UVAD, suggesting that the start and end of a video are predominantly normal. We then propose normality propagation, which propagates normal knowledge based on relationships between video snippets to estimate the normal magnitudes of unlabeled snippets. Finally, unsupervised learning of abnormal detection is performed based on the propagated labels and a new loss re-weighting method. These components are complementary to normality propagation and mitigate the negative impact of incorrectly propagated labels. Extensive experiments on the ShanghaiTech and UCF-Crime benchmarks demonstrate the superior performance of our method. The code is available at https://github.com/shyern/LANP-UVAD.git",
    "checked": true,
    "id": "4ff90ddf809566ab99b6737d6eac05228efb5809",
    "semantic_title": "learning anomalies with normality prior for unsupervised video anomaly detection",
    "citation_count": 0,
    "authors": [
      "Haoyue Shi",
      "Le Wang*",
      "Sanping Zhou",
      "Gang Hua",
      "Wei Tang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/944_ECCV_2024_paper.php": {
    "title": "Revisiting Supervision for Continual Representation Learning",
    "volume": "main",
    "abstract": "In the field of continual learning, models are designed to learn tasks one after the other. While most research has centered on supervised continual learning, there is a growing interest in unsupervised continual learning, which makes use of the vast amounts of unlabeled data. Recent studies have highlighted the strengths of unsupervised methods, particularly self-supervised learning, in providing robust representations. The improved transferability of those representations built with self-supervised methods is often associated with the role played by the multi-layer perceptron projector. In this work, we depart from this observation and reexamine the role of supervision in continual representation learning. We reckon that additional information, such as human annotations, should not deteriorate the quality of representations. Our findings show that supervised models when enhanced with a multi-layer perceptron head, can outperform self-supervised models in continual representation learning. This highlights the importance of the multi-layer perceptron projector in shaping feature transferability across a sequence of tasks in continual learning. The code is available on github",
    "checked": true,
    "id": "c843cd3e0d83dc364ec458f79fb53539f2b5e5dd",
    "semantic_title": "revisiting supervision for continual representation learning",
    "citation_count": 0,
    "authors": [
      "Daniel Marczak*",
      "Sebastian Cygert*",
      "Tomasz Trzcinski*",
      "Bartlomiej Twardowski*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/951_ECCV_2024_paper.php": {
    "title": "FLAT: Flux-aware Imperceptible Adversarial Attacks on 3D Point Clouds",
    "volume": "main",
    "abstract": "Adversarial attacks on point clouds play a vital role in assessing and enhancing the adversarial robustness of 3D deep learning models. While employing a variety of geometric constraints, existing adversarial attack solutions often display unsatisfactory imperceptibility due to inadequate consideration of uniformity changes. In this paper, we propose , a novel framework designed to generate imperceptible adversarial point clouds by addressing the issue from a flux perspective. Specifically, during adversarial attacks, we assess the extent of uniformity alterations by calculating the flux of the local perturbation vector field. Upon identifying a high flux, which signals potential disruption in uniformity, the directions of the perturbation vectors are adjusted to minimize these alterations, thereby improving imperceptibility. Extensive experiments validate the effectiveness of in generating imperceptible adversarial point clouds, and its superiority to the state-of-the-art methods",
    "checked": true,
    "id": "4df3bdf0e604b155e8315d04bb7c87ac06968dc3",
    "semantic_title": "flat: flux-aware imperceptible adversarial attacks on 3d point clouds",
    "citation_count": 0,
    "authors": [
      "Keke Tang",
      "Lujie Huang",
      "Weilong Peng*",
      "Daizong Liu",
      "Xiaofei Wang",
      "Yang Ma",
      "Ligang Liu",
      "Zhihong Tian"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/959_ECCV_2024_paper.php": {
    "title": "MMBENCH: Is Your Multi-Modal Model an All-around Player?",
    "volume": "main",
    "abstract": "Large vision-language models (VLMs) have recently achieved remarkable progress, exhibiting impressive multimodal perception and reasoning abilities. However, effectively evaluating these large VLMs remains a major challenge, hindering future development in this domain. Traditional benchmarks like VQAv2 or COCO Caption provide quantitative performance measurements but lack fine-grained ability assessment and robust evaluation metrics. Meanwhile, subjective benchmarks, such as OwlEval, offer comprehensive evaluations of a model's abilities by incorporating human labor, which is not scalable and may display significant bias. In response to these challenges, we propose MMBench, a bilingual benchmark for assessing the multi-modal capabilities of VLMs. MMBench methodically develops a comprehensive evaluation pipeline, primarily comprised of the following key features: 1. MMBench is meticulously curated with well-designed quality control schemes, surpassing existing similar benchmarks in terms of the number and variety of evaluation questions and abilities; 2. MMBench introduces a rigorous CircularEval strategy and incorporates large language models to convert free-form predictions into pre-defined choices, which helps to yield accurate evaluation results for models with limited instruction-following capabilities. 3. MMBench incorporates multiple-choice questions in both English and Chinese versions, enabling an apples-to-apples comparison of VLMs' performance under a bilingual context. To summarize, MMBench is a systematically designed objective benchmark for a robust and holistic evaluation of vision-language models. We hope MMBench will assist the research community in better evaluating their models and facilitate future progress in this area. MMBench has been supported in VLMEvalKit1 . 1 https://github.com/open-compass/VLMEvalKit",
    "checked": true,
    "id": "b37b1dc72b1882858f5120f2cd6883134089a6ed",
    "semantic_title": "mmbench: is your multi-modal model an all-around player?",
    "citation_count": 514,
    "authors": [
      "Yuan Liu*",
      "Haodong Duan*",
      "Yuanhan Zhang",
      "Bo Li",
      "Songyang Zhang",
      "Wangbo Zhao",
      "Yike Yuan",
      "Jiaqi Wang",
      "Conghui He",
      "Ziwei Liu",
      "Kai Chen",
      "Dahua Lin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/960_ECCV_2024_paper.php": {
    "title": "Implicit Filtering for Learning Neural Signed Distance Functions from 3D Point Clouds",
    "volume": "main",
    "abstract": "Neural signed distance functions (SDFs) have shown powerful ability in fitting the shape geometry. However, inferring continuous signed distance fields from discrete unoriented point clouds still remains a challenge. The neural network typically fits the shape with a rough surface and omits fine-grained geometric details such as shape edges and corners. In this paper, we propose a novel non-linear implicit filter to smooth the implicit field while preserving high-frequency geometry details. Our novelty lies in that we can filter the surface (zero level set) by the neighbor input points with gradients of the signed distance field. By moving the input raw point clouds along the gradient, our proposed implicit filtering can be extended to non-zero level sets to keep the promise consistency between different level sets, which consequently results in a better regularization of the zero level set. We conduct comprehensive experiments in surface reconstruction from objects and complex scene point clouds, the numerical and visual comparisons demonstrate our improvements over the state-of-the-art methods under the widely used benchmarks. Project page: https://list17.github.io/ImplicitFilter",
    "checked": true,
    "id": "8a0dc936ad7aba7e4766f5f08438d2e70ae8cc12",
    "semantic_title": "implicit filtering for learning neural signed distance functions from 3d point clouds",
    "citation_count": 1,
    "authors": [
      "Shengtao Li*",
      "Ge Gao",
      "Yudong Liu",
      "Ming Gu",
      "Yu-Shen Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/986_ECCV_2024_paper.php": {
    "title": "Unsupervised Exposure Correction",
    "volume": "main",
    "abstract": "Current exposure correction methods have three challenges, labor-intensive paired data annotation, limited generalizability, and performance degradation in low-level computer vision tasks. In this work, we introduce an innovative Unsupervised Exposure Correction (UEC) method that eliminates the need for manual annotations, offers improved generalizability, and enhances performance in low-level downstream tasks. Our model is trained using freely available paired data from an emulated Image Signal Processing (ISP) pipeline. This approach does not need expensive manual annotations, thereby minimizing individual style biases from the annotation and consequently improving its generalizability. Furthermore, we present a large-scale Radiometry Correction Dataset, specifically designed to emphasize exposure variations, to facilitate unsupervised learning. In addition, we develop a transformation function that preserves image details and outperforms state-of-the-art supervised methods[?], while utilizing only 0.01% of their parameters. Our work further investigates the broader impact of exposure correction on downstream tasks, including edge detection, demonstrating its effectiveness in mitigating the adverse effects of poor exposure on low-level features. The source code and dataset are publicly available at https://github.com/BeyondHeaven/uec_code",
    "checked": true,
    "id": "9410b47d851c3a792a1ab6d035f310cc43236820",
    "semantic_title": "unsupervised exposure correction",
    "citation_count": 0,
    "authors": [
      "Ruodai Cui*",
      "Li Niu",
      "Guosheng Hu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/990_ECCV_2024_paper.php": {
    "title": "Anytime Continual Learning for Open Vocabulary Classification",
    "volume": "main",
    "abstract": "We propose an approach for anytime continual learning (AnytimeCL) for open vocabulary image classification. The AnytimeCL problem aims to break away from batch training and rigid models by requiring that a system can predict any set of labels at any time and efficiently update and improve when receiving one or more training samples at any time. Despite the challenging goal, we achieve substantial improvements over recent methods. We propose a dynamic weighting between predictions of a partially fine-tuned model and a fixed open vocabulary model that enables continual improvement when training samples are available for a subset of a task's labels. We also propose an attention-weighted PCA compression of training features that reduces storage and computation with little impact to model accuracy. Our methods are validated with experiments that test flexibility of learning and inference",
    "checked": true,
    "id": "2b762bd591e54217e1905279f285c30d7bd4abe2",
    "semantic_title": "anytime continual learning for open vocabulary classification",
    "citation_count": 0,
    "authors": [
      "Zhen Zhu*",
      "Yiming Gong",
      "Derek Hoiem*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/991_ECCV_2024_paper.php": {
    "title": "External Knowledge Enhanced 3D Scene Generation from Sketch",
    "volume": "main",
    "abstract": "Generating realistic 3D scenes is challenging due to the complexity of room layouts and object geometries. We propose a sketch based knowledge enhanced diffusion architecture (SEK) for generating customized, diverse, and plausible 3D scenes. SEK conditions the denoising process with a hand-drawn sketch of the target scene and cues from an object relationship knowledge base. We first construct an external knowledge base containing object relationships and then leverage knowledge enhanced graph reasoning to assist our model in understanding hand-drawn sketches. A scene is represented as a combination of 3D objects and their relationships, and then incrementally diffused to reach a Gaussian distribution. We propose a 3D denoising scene transformer that learns to reverse the diffusion process, conditioned by a hand-drawn sketch along with knowledge cues, to regressively generate the scene including the 3D object instances as well as their layout. Experiments on the 3D-FRONT dataset show that our model improves FID, CKL by 17.41%, 37.18% in 3D scene generation and FID, KID by 19.12%, 20.06% in 3D scene completion compared to the nearest competitor DiffuScene",
    "checked": true,
    "id": "2ed081f094af37fa45b90994d9408b3fd48db46e",
    "semantic_title": "external knowledge enhanced 3d scene generation from sketch",
    "citation_count": 1,
    "authors": [
      "Zijie Wu",
      "Mingtao Feng*",
      "Yaonan Wang",
      "He Xie",
      "Weisheng Dong",
      "Bo Miao",
      "Ajmal Mian"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/992_ECCV_2024_paper.php": {
    "title": "G3R: Gradient Guided Generalizable Reconstruction",
    "volume": "main",
    "abstract": "Large scale 3D scene reconstruction is important for applications such as virtual reality and simulation. Existing neural rendering approaches (, NeRF, 3DGS) have achieved realistic reconstructions on large scenes, but optimize per scene, which is expensive and slow, and exhibit noticeable artifacts under large view changes due to overfitting. Generalizable approaches, or large reconstruction models, are fast, but primarily work for small scenes/objects and often produce lower quality rendering results. In this work, we introduce , a generalizable reconstruction approach that can efficiently predict high-quality 3D scene representations for large scenes. We propose to learn a reconstruction network that takes the gradient feedback signals from differentiable rendering to iteratively update a 3D scene representation, combining the benefits of high photorealism from per-scene optimization with data-driven priors from fast feed-forward prediction methods. Experiments on urban-driving and drone datasets show that generalizes across diverse large scenes and accelerates the reconstruction process by at least 10× while achieving comparable or better realism compared to 3DGS, and also being more robust to large view changes. Please visit our project page for more results: https://waabi.ai/g3r",
    "checked": true,
    "id": "485a016b60c3fc4d580036ce0d517cbc59eda9be",
    "semantic_title": "g3r: gradient guided generalizable reconstruction",
    "citation_count": 1,
    "authors": [
      "Yun Chen*",
      "Jingkang Wang",
      "Ze Yang",
      "Sivabalan Manivasagam*",
      "Raquel Urtasun*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/996_ECCV_2024_paper.php": {
    "title": "DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic Gaussian Splatting",
    "volume": "main",
    "abstract": "The increasing demand for virtual reality applications has highlighted the significance of crafting immersive 3D assets. We present a text-to-3D 360◦ scene generation pipeline that facilitates the creation of comprehensive 360◦ scenes for in-the-wild environments in a matter of minutes. Our approach utilizes the generative power of a 2D diffusion model and prompt self-refinement to create a high-quality and globally coherent panoramic image. This image acts as a preliminary \"flat\" (2D) scene representation. Subsequently, it is lifted into 3D Gaussians, employing splatting techniques to enable real-time exploration. To produce consistent 3D geometry, our pipeline constructs a spatially coherent structure by aligning the 2D monocular depth into a globally optimized point cloud. This point cloud serves as the initial state for the centroids of 3D Gaussians. In order to address invisible issues inherent in single-view inputs, we impose semantic and geometric constraints on both synthesized and input camera views as regularizations. These guide the optimization of Gaussians, aiding in the reconstruction of unseen regions. In summary, our method offers a globally consistent 3D scene within a 360◦ perspective, providing an enhanced immersive experience over existing techniques. Project website at: http:// dreamscene360.github.io/",
    "checked": true,
    "id": "4a0baaa32e955093d36746d9b3569d95ceee58c5",
    "semantic_title": "dreamscene360: unconstrained text-to-3d scene generation with panoramic gaussian splatting",
    "citation_count": 13,
    "authors": [
      "Shijie Zhou*",
      "Zhiwen Fan",
      "Dejia Xu",
      "Haoran Chang",
      "Pradyumna Chari",
      "Tejas K Bharadwaj",
      "Suya You",
      "Zhangyang Wang",
      "Achuta Kadambi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1001_ECCV_2024_paper.php": {
    "title": "Frequency-Spatial Entanglement Learning for Camouflaged Object Detection",
    "volume": "main",
    "abstract": "Camouflaged object detection has attracted a lot of attention in computer vision. The main challenge lies in the high degree of similarity between camouflaged objects and their surroundings in the spatial domain, making identification difficult. Existing methods attempt to reduce the impact of pixel similarity by maximizing the distinguishing ability of spatial features with complicated design, but often ignore the sensitivity and locality of features in the spatial domain, leading to sub-optimal results. In this paper, we propose a new approach to address this issue by jointly exploring the representation in the frequency and spatial domains, introducing the Frequency-Spatial Entanglement Learning (FSEL) method. This method consists of a series of well-designed Entanglement Transformer Blocks (ETB) for representation learning, a Joint Domain Perception Module for semantic enhancement, and a Dual-domain Reverse Parser for feature integration in the frequency and spatial domains. Specifically, the ETB utilizes frequency self-attention to effectively characterize the relationship between different frequency bands, while the entanglement feed-forward network facilitates information interaction between features of different domains through entanglement learning. Our extensive experiments demonstrate the superiority of our FSEL over 21 state-of-the-art methods, through comprehensive quantitative and qualitative comparisons in three widely-used datasets. The source code is available at: bluehttps://github.com/CSYSI/FSEL",
    "checked": true,
    "id": "df087f3b8cce3575662e10147c0242bf0d48d10a",
    "semantic_title": "frequency-spatial entanglement learning for camouflaged object detection",
    "citation_count": 1,
    "authors": [
      "Yanguang Sun",
      "Chunyan Xu",
      "Jian Yang",
      "Hanyu Xuan*",
      "Lei Luo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1005_ECCV_2024_paper.php": {
    "title": "VisionTrap: Vision-Augmented Trajectory Prediction Guided by Textual Descriptions",
    "volume": "main",
    "abstract": "Predicting future trajectories for other road agents is an essential task for autonomous vehicles. Established trajectory prediction methods primarily use agent tracks generated by a detection and tracking system and HD map as inputs. In this work, we propose a novel method that also incorporates visual input from surround-view cameras, allowing the model to utilize visual cues such as human gazes and gestures, road conditions, vehicle turn signals, etc, which are typically hidden from the model in prior methods. Furthermore, we use textual descriptions generated by a Vision-Language Model (VLM) and refined by a Large Language Model (LLM) as supervision during training to guide the model on what to learn from the input data. Despite using these extra inputs, our method achieves a latency of 53 ms, making it feasible for real-time processing, which is significantly faster than that of previous single-agent prediction methods with similar performance. Our experiments show that both the visual inputs and the textual descriptions contribute to improvements in trajectory prediction performance, and our qualitative analysis highlights how the model is able to exploit these additional inputs. Lastly, in this work we create and release the nuScenes-Text dataset, which augments the established nuScenes dataset with rich textual annotations for every scene, demonstrating the positive impact of utilizing VLM on trajectory prediction. Our project page is at https://moonseokha.github.io/VisionTrap",
    "checked": true,
    "id": "7ed3f3624eb358faa1bfeaa9a50fdd5bc3b74033",
    "semantic_title": "visiontrap: vision-augmented trajectory prediction guided by textual descriptions",
    "citation_count": 0,
    "authors": [
      "Seokha Moon",
      "Hyun Woo",
      "Hongbeen Park",
      "Haeji Jung",
      "Reza Mahjourian",
      "Hyung-gun Chi",
      "Hyerin Lim",
      "Sangpil Kim",
      "Jinkyu Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1016_ECCV_2024_paper.php": {
    "title": "Occluded Gait Recognition with Mixture of Experts: An Action Detection Perspective",
    "volume": "main",
    "abstract": "Extensive occlusions in real-world scenarios pose challenges to gait recognition due to missing and noisy information, as well as body misalignment in position and scale. We argue that rich dynamic contextual information within a gait sequence inherently possesses occlusion-solving traits: 1) Adjacent frames with gait continuity allow holistic body regions to infer occluded body regions; 2) Gait cycles allow information integration between holistic actions and occluded actions. Therefore, we introduce an action detection perspective where a gait sequence is regarded as a composition of actions. To detect accurate actions under complex occlusion scenarios, we propose an Action Detection Based Mixture of Experts (GaitMoE), consisting of Mixture of Temporal Experts (MTE) and Mixture of Action Experts (MAE). MTE adaptively constructs action anchors by temporal experts and MAE adaptively constructs action proposals from action anchors by action experts. Especially, action detection as a proxy task with gait recognition is an end-to-end joint training only with ID labels. In addition, due to the lack of a unified occluded benchmark, we construct a pioneering Occluded Gait database (OccGait), containing rich occlusion scenarios and annotations of occlusion types. Extensive experiments on OccGait, OccCASIA-B, Gait3D and GREW demonstrate the superior performance of GaitMoE. OccGait is available at https://github.com/BNU-IVC/OccGait",
    "checked": true,
    "id": "acb7d265c6ee4de392f7677194764320d36ab4c6",
    "semantic_title": "occluded gait recognition with mixture of experts: an action detection perspective",
    "citation_count": 0,
    "authors": [
      "Panjian Huang",
      "Yunjie Peng",
      "Saihui Hou*",
      "Chunshui Cao",
      "Xu Liu",
      "Zhiqiang He",
      "Yongzhen Huang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1019_ECCV_2024_paper.php": {
    "title": "EDTalk: Efficient Disentanglement for Emotional Talking Head Synthesis",
    "volume": "main",
    "abstract": "Achieving disentangled control over multiple facial motions and accommodating diverse input modalities greatly enhances the application and entertainment of the talking head generation. This necessitates a deep exploration of the decoupling space for facial features, ensuring that they a) operate independently without mutual interference and b) can be preserved to share with different modal inputs—both aspects often neglected in existing methods. To address this gap, this paper proposes a novel Efficient Disentanglement framework for Talking head generation (EDTalk). Our framework enables individual manipulation of mouth shape, head pose, and emotional expression, conditioned on video or audio inputs. Specifically, we employ three lightweight modules to decompose the facial dynamics into three distinct latent spaces representing mouth, pose, and expression, respectively. Each space is characterized by a set of learnable bases whose linear combinations define specific motions. To ensure independence and accelerate training, we enforce orthogonality among bases and devise an efficient training strategy to allocate motion responsibilities to each space without relying on external knowledge. The learned bases are then stored in corresponding banks, enabling shared visual priors with audio input. Furthermore, considering the properties of each space, we propose an Audio-to-Motion module for audio-driven talking head synthesis. Experiments are conducted to demonstrate the effectiveness of EDTalk. The code and pretrained models are released at: https://tanshuai0219.github.io/EDTalk/",
    "checked": true,
    "id": "f9511d7409f72ecaa2634af02f4be3a8643c4037",
    "semantic_title": "edtalk: efficient disentanglement for emotional talking head synthesis",
    "citation_count": 7,
    "authors": [
      "Shuai Tan*",
      "Bin Ji",
      "Mengxiao Bi",
      "ye pan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1023_ECCV_2024_paper.php": {
    "title": "Groma: Localized Visual Tokenization for Grounding Multimodal Large Language Models",
    "volume": "main",
    "abstract": "We introduce Groma, a Multimodal Large Language Model (MLLM) with grounded and fine-grained visual perception ability. Beyond holistic image understanding, Groma is adept at region-level tasks such as region captioning and visual grounding. Such capabilities are built upon a localized visual tokenization mechanism, where an image input is decomposed into regions of interest and subsequently encoded into region tokens. By integrating region tokens into user instructions and model responses, we seamlessly enable Groma to understand user-specified region inputs and ground its textual output to images. Besides, to enhance the grounded chat ability of Groma, we curate a visually grounded instruction dataset by leveraging the powerful GPT-4V and visual prompting techniques. Compared with MLLMs that rely on the language model or external module for localization, Groma consistently demonstrates superior performances in standard referring and grounding benchmarks, highlighting the advantages of embedding localization into image tokenization. Project page: https://groma-mllm.github.io/",
    "checked": true,
    "id": "0be923cad65522c921dfaa04e517b051593121ce",
    "semantic_title": "groma: localized visual tokenization for grounding multimodal large language models",
    "citation_count": 15,
    "authors": [
      "Chuofan Ma*",
      "Yi Jiang*",
      "Jiannan Wu",
      "Zehuan Yuan",
      "Xiaojuan Qi*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1025_ECCV_2024_paper.php": {
    "title": "On the Utility of 3D Hand Poses for Action Recognition",
    "volume": "main",
    "abstract": "3D hand pose is an underexplored modality for action recognition. Poses are compact yet informative and can greatly benefit applications with limited compute budgets. However, poses alone offer an incomplete understanding of actions, as they cannot fully capture objects and environments with which humans interact. We propose HandFormer, a novel multimodal transformer, to efficiently model hand-object interactions. HandFormer combines 3D hand poses at a high temporal resolution for fine-grained motion modeling with sparsely sampled RGB frames for encoding scene semantics. Observing the unique characteristics of hand poses, we temporally factorize hand modeling and represent each joint by its short-term trajectories. This factorized pose representation combined with sparse RGB samples is remarkably efficient and highly accurate. Unimodal HandFormer with only hand poses outperforms existing skeleton-based methods at 5× fewer FLOPs. With RGB, we achieve new state-of-the-art performance on Assembly101 and H2O with significant improvements in egocentric action recognition",
    "checked": true,
    "id": "241ecc8392ef3dda094119992681fde070edad31",
    "semantic_title": "on the utility of 3d hand poses for action recognition",
    "citation_count": 2,
    "authors": [
      "Md Salman Shamil*",
      "Dibyadip Chatterjee",
      "Fadime Sener",
      "Shugao Ma",
      "Angela Yao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1036_ECCV_2024_paper.php": {
    "title": "DG-PIC: Domain Generalized Point-In-Context Learning for Point Cloud Understanding",
    "volume": "main",
    "abstract": "Recent point cloud understanding research suffers from performance drops on unseen data, due to the distribution shifts across different domains. While recent studies use Domain Generalization (DG) techniques to mitigate this by learning domain-invariant features, most are designed for a single task and neglect the potential of testing data. Despite In-Context Learning (ICL) showcasing multi-task learning capability, it usually relies on high-quality context-rich data and considers a single dataset, and has rarely been studied in point cloud understanding. In this paper, we introduce a novel, practical, multi-domain multi-task setting, handling multiple domains and multiple tasks within one unified model for domain generalized point cloud understanding. To this end, we propose Domain Generalized Point-In-Context Learning (DG-PIC) that boosts the generalizability across various tasks and domains at testing time. In particular, we develop dual-level source prototype estimation that considers both global-level shape contextual and local-level geometrical structures for representing source domains and a dual-level test-time feature shifting mechanism that leverages both macro-level domain semantic information and micro-level patch positional relationships to pull the target data closer to the source ones during the testing. Our DG-PIC does not require any model updates during the testing and can handle unseen domains and multiple tasks, i.e., point cloud reconstruction, denoising, and registration, within one unified model. We also introduce a benchmark for this new setting. Comprehensive experiments demonstrate that DG-PIC outperforms state-of-the-art techniques significantly",
    "checked": true,
    "id": "fd3915245c765aa063351b865aa147014d3a093a",
    "semantic_title": "dg-pic: domain generalized point-in-context learning for point cloud understanding",
    "citation_count": 5,
    "authors": [
      "Jincen Jiang",
      "Qianyu Zhou",
      "Yuhang Li",
      "Xuequan Lu*",
      "Meili Wang*",
      "Lizhuang Ma",
      "Jian Chang",
      "Jian Jun Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1043_ECCV_2024_paper.php": {
    "title": "Operational Open-Set Recognition and PostMax Refinement",
    "volume": "main",
    "abstract": "Open-Set Recognition (OSR) is a problem with mainly practical applications. However, recent evaluations have largely focused on small-scale data and tuning thresholds over the test set, which disregard the real-world operational needs of parameter selection. Thus, we revisit the original goals of OSR and propose a new evaluation metric, Operational Open-Set Accuracy (OOSA), which requires predicting an operationally relevant threshold from a validation set with known and a surrogate set with unknown samples, and then applying this threshold during testing. With this new measure in mind, we develop a large-scale evaluation protocol suited for operational scenarios. Additionally, we introduce the novel PostMax algorithm that performs post-processing refinement of the logit of the maximal class. This refinement involves normalizing logits by deep feature magnitudes and utilizing an extreme-value-based generalized Pareto distribution to map them into proper probabilities. We evaluate multiple pre-trained deep networks, including leading transformer and convolution-based architectures, on different selections of large-scale surrogate and test sets. Our experiments demonstrate that PostMax advances the state of the art in open-set recognition, showing statistically significant improvements in our novel OOSA metric as well as in previously used metrics such as AUROC, FPR95, and others",
    "checked": true,
    "id": "fb22a09669fefa4d7bb040eae61440c5ea623e9e",
    "semantic_title": "operational open-set recognition and postmax refinement",
    "citation_count": 0,
    "authors": [
      "Steve Cruz*",
      "Ryan Rabinowitz",
      "Manuel Günther",
      "Terrance E. Boult"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1046_ECCV_2024_paper.php": {
    "title": "ScaleDreamer: Scalable Text-to-3D Synthesis with Asynchronous Score Distillation",
    "volume": "main",
    "abstract": "By leveraging the text-to-image diffusion prior, score distillation can synthesize 3D contents without paired text-3D training data. Instead of spending hours of online optimization per text prompt, recent studies have been focused on learning a text-to-3D generative network for amortizing multiple text-3D relations, which can synthesize 3D contents in seconds. However, existing score distillation methods are hard to scale up to a large amount of text prompts due to the difficulties in aligning pretrained diffusion prior with the distribution of rendered images from various text prompts. Current state-of-the-arts such as Variational Score Distillation finetune the pretrained diffusion model to minimize the noise prediction error so as to align the distributions, which are however unstable to train and will impair the model's comprehension capability to numerous text prompts. Based on the observation that the diffusion models tend to have lower noise prediction errors at earlier timesteps, we propose Asynchronous Score Distillation (ASD), which minimizes the noise prediction error by shifting the diffusion timestep to earlier ones. ASD is stable to train and can scale up to 100k prompts. It reduces the noise prediction error without changing the weights of pre-trained diffusion model, thus keeping its strong comprehension capability to prompts. We conduct extensive experiments using different text-to-3D architectures, including Hyper-iNGP and 3DConv-Net. The results demonstrate ASD's effectiveness in stable 3D generator training, high-quality 3D content synthesis, and its superior prompt-consistency, especially under large prompt corpus. Code is available at https://github.com/theEricMa/ScaleDreamer",
    "checked": true,
    "id": "84dba30ad62b37212d1f3f709afdcffbd028800b",
    "semantic_title": "scaledreamer: scalable text-to-3d synthesis with asynchronous score distillation",
    "citation_count": 2,
    "authors": [
      "Zhiyuan Ma*",
      "Yuxiang Wei",
      "Yabin Zhang",
      "Xiangyu Zhu",
      "Zhen Lei",
      "Lei Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1055_ECCV_2024_paper.php": {
    "title": "SINDER: Repairing the Singular Defects of DINOv2",
    "volume": "main",
    "abstract": "Vision Transformer models trained on large-scale datasets, although effective, often exhibit artifacts in the patch token they extract. While such defects can be alleviated by re-training the entire model with additional classification tokens, the underlying reasons for the presence of these tokens remain unclear. In this paper, we conduct a thorough investigation of this phenomenon, combining theoretical analysis with empirical observations. Our findings reveal that these artifacts originate from the pre-trained network itself, specifically stemming from the leading left singular vector of the network's weights. Furthermore, to mitigate these defects, we propose a novel fine-tuning smooth regularization that rectifies structural deficiencies using only a small dataset, thereby avoiding the need for complete re-training. We validate our method on various downstream tasks, including unsupervised segmentation, classification, supervised segmentation, and depth estimation, demonstrating its effectiveness in improving model performance. Codes and checkpoints are available at https://github.com/haoqiwang/sinder",
    "checked": true,
    "id": "d0a6fb583971a5b3ffc89780ff914d8beae02a9c",
    "semantic_title": "sinder: repairing the singular defects of dinov2",
    "citation_count": 0,
    "authors": [
      "Haoqi Wang",
      "Tong Zhang",
      "Mathieu Salzmann*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1065_ECCV_2024_paper.php": {
    "title": "SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow",
    "volume": "main",
    "abstract": "We introduce SEA-RAFT, a more simple, efficient, and accurate RAFT for optical flow. Compared with RAFT, SEA-RAFT is trained with a new loss (mixture of Laplace). It directly regresses an initial flow for faster convergence in iterative refinements and introduces rigid-motion pre-training to improve generalization. SEA-RAFT achieves state-of-the-art accuracy on the Spring benchmark with a 3.69 endpoint-error (EPE) and a 0.36 1-pixel outlier rate (1px), representing 22.9% and 17.8% error reduction from best published results. In addition, SEA-RAFT obtains the best cross-dataset generalization on KITTI and Spring. With its high efficiency, SEA-RAFT operates at least 2.3× faster than existing methods while maintaining competitive performance. The code is publicly available at https://github.com/princeton-vl/SEA-RAFT",
    "checked": true,
    "id": "561ecdcac76f52d8c74df916c6d49d08f10522b4",
    "semantic_title": "sea-raft: simple, efficient, accurate raft for optical flow",
    "citation_count": 5,
    "authors": [
      "Yihan Wang*",
      "Lahav O Lipson",
      "Jia Deng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1066_ECCV_2024_paper.php": {
    "title": "Learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation",
    "volume": "main",
    "abstract": "While the success of deep learning relies on large amounts of training datasets, data is often limited in privacy-sensitive domains. To address this challenge, generative model learning with differential privacy has emerged as a solution to train private generative models for desensitized data generation. However, the quality of the images generated by existing methods is limited due to the complexity of modeling data distribution. We build on the success of diffusion models and introduce DP-SAD, which trains a private diffusion model by a stochastic adversarial distillation method. Specifically, we first train a diffusion model as a teacher and then train a student by distillation, in which we achieve differential privacy by adding noise to the gradients from other models to the student. For better generation quality, we introduce a discriminator to distinguish whether an image is from the teacher or the student, which forms the adversarial training. Extensive experiments and analysis clearly demonstrate the effectiveness of our proposed method",
    "checked": true,
    "id": "1f35a841e42722d3554784d66c8c9ccb12f4b1be",
    "semantic_title": "learning differentially private diffusion models via stochastic adversarial distillation",
    "citation_count": 1,
    "authors": [
      "Bochao Liu",
      "Pengju Wang",
      "Shiming Ge*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1070_ECCV_2024_paper.php": {
    "title": "General and Task-Oriented Video Segmentation",
    "volume": "main",
    "abstract": "We present GvSeg, a general video segmentation framework for addressing four different video segmentation tasks (, instance, semantic, panoptic, and exemplar-guided) while maintaining an identical architectural design. Currently, there is a trend towards developing general video segmentation solutions that can be applied across multiple tasks. This streamlines research endeavors and simplifies deployment. However, such a highly homogenized framework in current design, where each element maintains uniformity, could overlook the inherent diversity among different tasks and lead to suboptimal performance. To tackle this, GvSeg: i) provides a holistic disentanglement and modeling for segment targets, thoroughly examining them from the perspective of appearance, position, and shape, and on this basis, ii) reformulates the query initialization, matching and sampling strategies in alignment with the task-specific requirement. These architecture-agnostic innovations empower GvSeg to effectively address each unique task by accommodating the specific properties that characterize them. Extensive experiments on seven gold-standard benchmark datasets demonstrate that GvSeg surpasses all existing specialized/general solutions by a significant margin on four different video segmentation tasks",
    "checked": true,
    "id": "761aa8a28485281fe4a10562bd6af51fc75a7183",
    "semantic_title": "general and task-oriented video segmentation",
    "citation_count": 3,
    "authors": [
      "Mu Chen",
      "Liulei Li",
      "Wenguan Wang",
      "Ruijie Quan",
      "Yi Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1071_ECCV_2024_paper.php": {
    "title": "VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement",
    "volume": "main",
    "abstract": "In recent years, online Video Instance Segmentation (VIS) methods have shown remarkable advancement with their powerful query-based detectors. Utilizing the output queries of the detector at the frame-level, these methods achieve high accuracy on challenging benchmarks. However, our observations demonstrate that these methods heavily rely on location information, which often causes incorrect associations between objects. This paper presents that a key axis of object matching in trackers is appearance information, which becomes greatly instructive under conditions where positional cues are insufficient for distinguishing their identities. Therefore, we suggest a simple yet powerful extension to object decoders that explicitly extract embeddings from backbone features and drive queries to capture the appearances of objects, which greatly enhances instance association accuracy. Furthermore, recognizing the limitations of existing benchmarks in fully evaluating appearance awareness, we have constructed a synthetic dataset to rigorously validate our method. By effectively resolving the over-reliance on location information, we achieve state-of-the-art results on YouTube-VIS 2019/2021 and Occluded VIS (OVIS). Code is available at https://github.com/KimHanjung/VISAGE",
    "checked": true,
    "id": "7785b8a68e540b7685f83d86c6db037a82da59c9",
    "semantic_title": "visage: video instance segmentation with appearance-guided enhancement",
    "citation_count": 0,
    "authors": [
      "Hanjung Kim",
      "Jaehyun Kang",
      "Miran Heo",
      "Sukjun Hwang",
      "Seoung Wug Oh",
      "Seon Joo Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1086_ECCV_2024_paper.php": {
    "title": "LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors",
    "volume": "main",
    "abstract": "We present a simple self-supervised method to enhance the performance of ViT features for dense downstream tasks. Our Lightweight Feature Transform (LiFT) is a straightforward and compact postprocessing network that can be applied to enhance the features of any pre-trained ViT backbone. LiFT is fast and easy to train with a self-supervised objective, and it boosts the density of ViT features for minimal extra inference cost. Furthermore, we demonstrate that LiFT can be applied with approaches that use additional task-specific downstream modules, as we integrate LiFT with ViTDet for COCO detection and segmentation. Despite the simplicity of LiFT, we find that it is not simply learning a more complex version of bilinear interpolation. Instead, our LiFT training protocol leads to several desirable emergent properties that benefit ViT features in dense downstream tasks. This includes greater scale invariance for features, and better object boundary maps. By simply training LiFT for a few epochs, we show improved performance on keypoint correspondence, detection, segmentation, and object discovery tasks. Overall, LiFT provides an easy way to unlock the benefits of denser feature arrays for a fraction of the computational cost. For more details, refer to our magentaproject page",
    "checked": true,
    "id": "dab3e9c381b1aa2e471e65d0acc2db4cdbb005d8",
    "semantic_title": "lift: a surprisingly simple lightweight feature transform for dense vit descriptors",
    "citation_count": 0,
    "authors": [
      "Saksham Suri*",
      "Matthew Walmer",
      "Kamal Gupta",
      "Abhinav Shrivastava"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1091_ECCV_2024_paper.php": {
    "title": "ControlNet++: Improving Conditional Controls with Efficient Consistency Feedback",
    "volume": "main",
    "abstract": "To enhance the controllability of text-to-image diffusion models, existing efforts like ControlNet incorporated image-based conditional controls. In this paper, we reveal that existing methods still face significant challenges in generating images that align with the image conditional controls. To this end, we propose ControlNet++, a novel approach that improves controllable generation by explicitly optimizing pixel-level cycle consistency between generated images and conditional controls. Specifically, for an input conditional control, we use a pre-trained discriminative reward model to extract the corresponding condition of the generated images, and then optimize the consistency loss between the input conditional control and extracted condition. A straightforward implementation would be generating images from random noises and then calculating the consistency loss, but such an approach requires storing gradients for multiple sampling timesteps, leading to considerable time and memory costs. To address this, we introduce an efficient reward strategy that deliberately disturbs the input images by adding noise, and then uses the single-step denoised images for reward fine-tuning. This avoids the extensive costs associated with image sampling, allowing for more efficient reward fine-tuning. Extensive experiments show that ControlNet++ significantly improves controllability under various conditional controls. For example, it achieves improvements over ControlNet by 11.1% mIoU, 13.4% SSIM, and 7.6% RMSE, respectively, for segmentation mask, line-art edge, and depth conditions. All the code, models, demo and organized data have been open sourced on our magentaGithub Repo",
    "checked": true,
    "id": "502e7c8921de673f79d957e602cef356cb9d7956",
    "semantic_title": "controlnet++: improving conditional controls with efficient consistency feedback",
    "citation_count": 10,
    "authors": [
      "Ming Li*",
      "Taojiannan Yang",
      "Huafeng Kuang",
      "Jie Wu",
      "Zhaoning Wang",
      "Xuefeng Xiao",
      "Chen Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1098_ECCV_2024_paper.php": {
    "title": "TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-Spoofing",
    "volume": "main",
    "abstract": "Generalizable Face anti-spoofing (FAS) approaches have recently garnered considerable attention due to their robustness in unseen scenarios. Some recent methods incorporate vision-language models into FAS, leveraging their impressive pre-trained performance to improve the generalization. However, these methods only utilize coarse-grained or single-element prompts for fine-tuning FAS tasks, without fully exploring the potential of language supervision, leading to unsatisfactory generalization ability. To address these concerns, we propose a novel framework called TF-FAS, which aims to thoroughly explore and harness twofold-element fine-grained semantic guidance to enhance generalization. Specifically, the Content Element Decoupling Module (CEDM) is proposed to comprehensively explore the semantic elements related to content. It is subsequently employed to supervise the decoupling of categorical features from content-related features, thereby enhancing the generalization abilities. Moreover, recognizing the subtle differences within the data of each class in FAS, we present a Fine-Grained Categorical Element Module (FCEM) to explore fine-grained categorical element guidance, then adaptively integrate them to facilitate the distribution modeling for each class. Comprehensive experiments and analysis demonstrate the superiority of our method over state-of-the-art competitors. Code:https://github.com/xudongww/TF-FAS",
    "checked": true,
    "id": "2752e9a88de87426027c9a6d60b5e655229b5677",
    "semantic_title": "tf-fas: twofold-element fine-grained semantic guidance for generalizable face anti-spoofing",
    "citation_count": 1,
    "authors": [
      "Xudong Wang",
      "Ke-Yue Zhang",
      "Taiping Yao*",
      "Qianyu Zhou",
      "Shouhong Ding",
      "Pingyang Dai*",
      "Rongrong Ji"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1102_ECCV_2024_paper.php": {
    "title": "Prompting Future Driven Diffusion Model for Hand Motion Prediction",
    "volume": "main",
    "abstract": "Hand motion prediction from both first- and third-person perspectives is vital for enhancing user experience in AR/VR and ensuring safe remote robotic arm control. Previous works typically focus on predicting hand motion trajectories or human body motion, with direct hand motion prediction remaining largely unexplored - despite the additional challenges posed by compact skeleton size. To address this, we propose a prompt-based Future Driven Diffusion Model (PromptFDDM) for predicting hand motion with guidance and prompts. Specifically, we develop a Spatial-Temporal Extractor Network (STEN) to predict hand motion with guidance, a Ground Truth Extractor Network (GTEN), and a Reference Data Generator Network (RDGN), which extract ground truth and substitute future data with generated reference data, respectively, to guide STEN. Additionally, interactive prompts generated from observed motions further enhance model performance. Experimental results on the FPHA and HO3D datasets demonstrate that the proposed PromptFDDM achieves state-of-the-art performance in both first- and third-person perspectives",
    "checked": true,
    "id": "4d21fd54779700481721d952882eef8c96b4bae0",
    "semantic_title": "prompting future driven diffusion model for hand motion prediction",
    "citation_count": 0,
    "authors": [
      "Bowen Tang*",
      "Kaihao Zhang*",
      "Wenhan Luo*",
      "Wei Liu",
      "HONGDONG LI"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1105_ECCV_2024_paper.php": {
    "title": "Defect Spectrum: A Granular Look of Large-scale Defect Datasets with Rich Semantics",
    "volume": "main",
    "abstract": "Defect inspection is paramount within the closed-loop manufacturing system. However, existing datasets for defect inspection often lack the precision and semantic granularity required for practical applications. In this paper, we introduce the Defect Spectrum, a comprehensive benchmark that offers precise, semantic-abundant, and large-scale annotations for a wide range of industrial defects. Building on four key industrial benchmarks, our dataset refines existing annotations and introduces rich semantic details, distinguishing multiple defect types within a single image. With our dataset, we were able to achieve an increase of 10.74% in the Recall rate, and a decrease of 33.10% in the False Positive Rate (FPR) from the industrial simulation experiment. Furthermore, we introduce Defect-Gen, a two-stage diffusion-based generator designed to create high-quality and diverse defective images, even when working with limited defective data. The synthetic images generated by Defect-Gen significantly enhance the performance of defect segmentation models, achieving an improvement in mIoU scores up to 9.85 on Defect-Spectrum subsets. Overall, The Defect Spectrum dataset demonstrates its potential in defect inspection research, offering a solid platform for testing and refining advanced models. Our project page is in https://envision-research.github.io/Defect_Spectrum/",
    "checked": true,
    "id": "24d9542bffc4ce0f9928757895f93f2b7ceeeebc",
    "semantic_title": "defect spectrum: a granular look of large-scale defect datasets with rich semantics",
    "citation_count": 4,
    "authors": [
      "Shuai Yang",
      "ZhiFei Chen",
      "Pengguang Chen",
      "Xi Fang",
      "Yixun Liang",
      "Shu Liu*",
      "Yingcong Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1108_ECCV_2024_paper.php": {
    "title": "Unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement",
    "volume": "main",
    "abstract": "Previous low-light image enhancement (LLIE) approaches, while employing frequency decomposition techniques to address the intertwined challenges of low frequency (e.g., illumination recovery) and high frequency (e.g., noise reduction), primarily focused on the development of dedicated and complex networks to achieve improved performance. In contrast, we reveal that an advanced disentanglement paradigm is sufficient to consistently enhance state-of-the-art methods with minimal computational overhead. Leveraging the image Laplace decomposition scheme, we propose a novel low-frequency consistency method, facilitating improved frequency disentanglement optimization. Our method, seamlessly integrating with various models such as CNNs, Transformers, and flow-based and diffusion models, demonstrates remarkable adaptability. Noteworthy improvements are showcased across five popular benchmarks, with up to 7.68dB gains on PSNR achieved for six state-of-the-art models. Impressively, our approach maintains efficiency with only 88K extra parameters, setting a new standard in the challenging realm of low-light image enhancement. https://github.com/redrock303/ ADF-LLIE",
    "checked": true,
    "id": "213b10f79180f96ee99b9a91f5e42bd773d827eb",
    "semantic_title": "unveiling advanced frequency disentanglement paradigm for low-light image enhancement",
    "citation_count": 0,
    "authors": [
      "Kun Zhou*",
      "Xinyu Lin",
      "Wenbo Li",
      "Xiaogang Xu",
      "Yuanhao Cai",
      "Zhonghang Liu",
      "Xiaoguang Han",
      "Jiangbo Lu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1129_ECCV_2024_paper.php": {
    "title": "RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation",
    "volume": "main",
    "abstract": "[-4] 3D point clouds play a pivotal role in outdoor scene perception, especially in the context of autonomous driving. Recent advancements in 3D LiDAR segmentation often focus intensely on the spatial positioning and distribution of points for accurate segmentation. However, these methods, while robust in variable conditions, encounter challenges due to sole reliance on coordinates and point intensity, leading to poor isometric invariance and suboptimal segmentation. To tackle this challenge, our work introduces Range-Aware Pointwise Distance Distribution () features and the associated architecture. Our features exhibit rigid transformation invariance and effectively adapt to variations in point density, with a design focus on capturing the localized geometry of neighboring structures. They utilize inherent LiDAR isotropic radiation and semantic categorization for enhanced local representation and computational efficiency, while incorporating a 4D distance metric that integrates geometric and surface material reflectivity for improved semantic segmentation. To effectively embed high-dimensional features, we propose a double-nested autoencoder structure with a novel class-aware embedding objective to encode high-dimensional features into manageable voxel-wise embeddings. Additionally, we propose which incorporates a channel-wise attention fusion and two effective -Seg variants, further optimizing the embedding for enhanced performance and generalization. Our method outperforms contemporary LiDAR segmentation work in terms of mIoU on SemanticKITTI (76.1) and nuScenes (83.6) datasets",
    "checked": true,
    "id": "b573e5cfb40c23cee6466c800ed2ec8f4ef66d46",
    "semantic_title": "rapid-seg: range-aware pointwise distance distribution networks for 3d lidar segmentation",
    "citation_count": 2,
    "authors": [
      "Li Li*",
      "Hubert P. H. Shum",
      "Toby P Breckon"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1133_ECCV_2024_paper.php": {
    "title": "UMBRAE: Unified Multimodal Brain Decoding",
    "volume": "main",
    "abstract": "We address prevailing challenges of the brain-powered research, departing from the observation that the literature hardly recover accurate spatial information and require subject-specific models. To address these challenges, we propose UMBRAE, a unified multimodal decoding of brain signals. First, to extract instance-level conceptual and spatial details from neural signals, we introduce an efficient universal brain encoder for multimodal-brain alignment and recover object descriptions at multiple levels of granularity from subsequent multimodal large language model (MLLM). Second, we introduce a cross-subject training strategy mapping subject-specific features to a common feature space. This allows a model to be trained on multiple subjects without extra resources, even yielding superior results compared to subject-specific models. Further, we demonstrate this supports weakly-supervised adaptation to new subjects, with only a fraction of the total training data. Experiments demonstrate that not only achieves superior results in the newly introduced tasks but also outperforms methods in well established tasks. To assess our method, we construct and share with the community a comprehensive brain understanding benchmark . Our code and benchmark are available at https://weihaox.github.io/UMBRAE",
    "checked": true,
    "id": "1698bf38d144749e46cc9c3604ddfa5fcd647c0e",
    "semantic_title": "umbrae: unified multimodal brain decoding",
    "citation_count": 0,
    "authors": [
      "Weihao Xia*",
      "Raoul de Charette",
      "A. Cengiz Oztireli",
      "Jing-Hao Xue"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1143_ECCV_2024_paper.php": {
    "title": "NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models",
    "volume": "main",
    "abstract": "Capitalizing on the remarkable advancements in Large Language Models (LLMs), there is a burgeoning initiative to harness LLMs for instruction following robotic navigation. Such a trend underscores the potential of LLMs to generalize navigational reasoning and diverse language understanding. However, a significant discrepancy in agent performance is observed when integrating LLMs in the Vision-and-Language navigation (VLN) tasks compared to previous downstream specialist models. Furthermore, the inherent capacity of language to interpret and facilitate communication in agent interactions is often underutilized in these integrations. In this work, we strive to bridge the divide between VLN-specialized models and LLM-based navigation paradigms, while maintaining the interpretative prowess of LLMs in generating linguistic navigational reasoning. By aligning visual content in a frozen LLM, we encompass visual observation comprehension for LLMs and exploit a way to incorporate LLMs and navigation policy networks for effective action predictions and navigational reasoning. We demonstrate the data efficiency of the proposed methods and eliminate the gap between LM-based agents and state-of-the-art VLN specialists. The source code is available at https://github.com/GengzeZhou/NavGPT-2",
    "checked": true,
    "id": "c2d858527df96e269f072e3dee37ec7bae281c0f",
    "semantic_title": "navgpt-2: unleashing navigational reasoning capability for large vision-language models",
    "citation_count": 2,
    "authors": [
      "Gengze Zhou*",
      "Yicong Hong",
      "Zun Wang",
      "Xin Eric Wang",
      "Qi Wu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1145_ECCV_2024_paper.php": {
    "title": "3D Single-object Tracking in Point Clouds with High Temporal Variation",
    "volume": "main",
    "abstract": "The high temporal variation of the point clouds is the key challenge of 3D single-object tracking (3D SOT). Existing approaches rely on the assumption that the shape variation of the point clouds and the motion of the objects across neighboring frames are smooth, failing to cope with high temporal variation data. In this paper, we present a novel framework for 3D SOT in point clouds with high temporal variation, called HVTrack. HVTrack proposes three novel components to tackle the challenges in the high temporal variation scenario: 1) A Relative-Pose-Aware Memory module to handle temporal point cloud shape variations; 2) a Base-Expansion Feature Cross-Attention module to deal with similar object distractions in expanded search areas; 3) a Contextual Point Guided Self-Attention module for suppressing heavy background noise. We construct a dataset with high temporal variation (KITTI-HV) by setting different frame intervals for sampling in the KITTI dataset. On the KITTI-HV with 5 frame intervals, our HVTrack surpasses the state-of-the-art tracker CXTracker by 11.3%/15.7% in Success/Precision",
    "checked": true,
    "id": "11e74c0ed42851810e6db4e6d02e18e4a581c53d",
    "semantic_title": "3d single-object tracking in point clouds with high temporal variation",
    "citation_count": 0,
    "authors": [
      "Qiao Wu",
      "Kun Sun",
      "Pei An",
      "Mathieu Salzmann",
      "Yanning Zhang",
      "Jiaqi Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1149_ECCV_2024_paper.php": {
    "title": "Adaptive Multi-task Learning for Few-shot Object Detection",
    "volume": "main",
    "abstract": "The majority of few-shot object detection methods use a shared feature map for both classification and localization, despite the conflicting requirements of these two tasks. Localization needs scale and positional sensitive features, whereas classification requires features that are robust to scale and positional variations. Although few methods have recognized this challenge and attempted to address it, they may not provide a comprehensive resolution to the issue. To overcome the contradictory preferences between classification and localization in few-shot object detection, an adaptive multi-task learning method, featuring a novel precision-driven gradient balancer, is proposed. This balancer effectively mitigates the conflicts by dynamically adjusting the backward gradient ratios for both tasks. Furthermore, a knowledge distillation and classification refinement scheme based on CLIP is introduced, aiming to enhance individual tasks by leveraging the capabilities of large vision-language models. Experimental results of the proposed method consistently show improvements over strong few-shot detection baselines on benchmark datasets. https://github.com/RY-Paper/MTL-FSOD",
    "checked": true,
    "id": "532b967ed725e52eca11d84adcc5c658289e0233",
    "semantic_title": "adaptive multi-task learning for few-shot object detection",
    "citation_count": 0,
    "authors": [
      "Yan Ren*",
      "Yanling Li",
      "Adams Wai-Kin Kong"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1155_ECCV_2024_paper.php": {
    "title": "Event Trojan: Asynchronous Event-based Backdoor Attacks",
    "volume": "main",
    "abstract": "As asynchronous event data is more frequently engaged in various vision tasks, the risk of backdoor attacks becomes more evident. However, research into the potential risk associated with backdoor attacks in asynchronous event data has been scarce, leaving related tasks vulnerable to potential threats. This paper has uncovered the possibility of directly poisoning event data streams by proposing Event Trojan framework, including two kinds of triggers, , immutable and mutable triggers. Specifically, our two types of event triggers are based on a sequence of simulated event spikes, which can be easily incorporated into any event stream to initiate backdoor attacks. Additionally, for the mutable trigger, we design an adaptive learning mechanism to maximize its aggressiveness. To improve the stealthiness, we introduce a novel loss function that constrains the generated contents of mutable triggers, minimizing the difference between triggers and original events while maintaining effectiveness. Extensive experiments on public event datasets show the effectiveness of the proposed backdoor triggers. We hope that this paper can draw greater attention to the potential threats posed by backdoor attacks on event-based tasks",
    "checked": true,
    "id": "84a050644e4c9030fad4505d6d1aa534f0bd838c",
    "semantic_title": "event trojan: asynchronous event-based backdoor attacks",
    "citation_count": 2,
    "authors": [
      "Ruofei Wang*",
      "Qing Guo",
      "Haoliang Li",
      "Renjie Wan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1159_ECCV_2024_paper.php": {
    "title": "Stepwise Multi-grained Boundary Detector for Point-supervised Temporal Action Localization",
    "volume": "main",
    "abstract": "Point-supervised temporal action localization pursues high-accuracy action detection under low-cost data annotation. Despite recent advances, a significant challenge remains: sparse labeling of individual frames leads to semantic ambiguity in determining action boundaries due to the lack of continuity in the highly sparse point-supervision scheme. We propose a Stepwise Multi-grained Boundary Detector (SMBD), which is comprised of a Background Anchor Generator (BAG) and a Dual Boundary Detector (DBD) to provide fine-grained supervision. Specifically, for each epoch in the training process, BAG computes the optimal background snippet between each pair of adjacent action labels, which we term Background Anchor. Subsequently, DBD leverages the background anchor and the action labels to locate the action boundaries from the perspectives of detecting action changes and scene changes. Then, the corresponding labels can be assigned to each side of the boundaries, with the boundaries continuously updated throughout the training process. Consequently, the proposed SMBD could ensure that more snippets contribute to the training process. Extensive experiments on the THUMOS'14, GTEA and BEOID datasets demonstrate that the proposed method outperforms existing state-of-the-art methods",
    "checked": true,
    "id": "e7363c72acb1fb58309d96162771a573dd20622e",
    "semantic_title": "stepwise multi-grained boundary detector for point-supervised temporal action localization",
    "citation_count": 0,
    "authors": [
      "Mengnan Liu",
      "Le Wang*",
      "Sanping Zhou",
      "Kun Xia",
      "Qi Wu",
      "Qilin Zhang",
      "Gang Hua"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1161_ECCV_2024_paper.php": {
    "title": "Imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems",
    "volume": "main",
    "abstract": "Electromagnetic Inverse Scattering Problems (EISP) have gained wide applications in computational imaging. By solving EISP, the internal relative permittivity of the scatterer can be non-invasively determined based on the scattered electromagnetic fields. Despite previous efforts to address EISP, achieving better solutions to this problem has remained elusive, due to the challenges posed by inversion and discretization. This paper tackles those challenges in EISP via an implicit approach. By representing the scatterer's relative permittivity as a continuous implicit representation, our method is able to address the low-resolution problems arising from discretization. Further, optimizing this implicit representation within a forward framework allows us to conveniently circumvent the challenges posed by inverse estimation. Our approach outperforms existing methods on standard benchmark datasets. Project page: https://luo-ziyuan.github.io/Imaging-Interiors",
    "checked": true,
    "id": "eef8bb955ea7a4df54a47d4e5b2c7727846b7e6f",
    "semantic_title": "imaging interiors: an implicit solution to electromagnetic inverse scattering problems",
    "citation_count": 2,
    "authors": [
      "Ziyuan Luo",
      "Boxin Shi",
      "Haoliang Li",
      "Renjie Wan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1163_ECCV_2024_paper.php": {
    "title": "Dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning",
    "volume": "main",
    "abstract": "Parameter-efficient fine-tuning methods adjust a small subset of parameters in large models, achieving performance comparable to or even surpassing that of models fine-tuned with the full parameter set, and significantly reducing the time and computational costs associated with the fine-tuning process. Despite the developments of parameter-efficient fine-tuning methods for large models, we observe significant performance disparities across different vision tasks. We attribute this pronounced performance variability to the insufficient robustness of current parameter-efficient fine-tuning methods. In this paper, we propose a robust reparameterization framework for parameter-efficient fine-tuning. This framework has a dynamic training structure and introduces no additional computational overhead during the inference stage. Specifically, we propose Dropout-Mixture Low-Rank Adaptation (DMLoRA), which incorporates multiple up and down branches, to provide the model with a more robust gradient descent path. As training proceeds, DMLoRA gradually drops out branches to achieve a balance between accuracy and regularization. Additionally, we employ a 2-Stage Learning Scalar (LS) strategy to optimize the scale factor for each layer's DMLoRA module. Experimental results demonstrate that our method achieves state-of-the-art performance on the benchmark VTAB-1k and FGVC datasets for parameter-efficient fine-tuning. Paramter-Efficient Fine-Tuning Dropout-Mixture Low-Rank Adaptation Gradual Branch Dropout 2-Stage Learning Scalar",
    "checked": true,
    "id": "7ba9047ac7c9e87d9291589db087dc8f4253d351",
    "semantic_title": "dropout mixture low-rank adaptation for visual parameters-efficient fine-tuning",
    "citation_count": 0,
    "authors": [
      "Zhengyi Fang",
      "Yue Wang",
      "Ran Yi*",
      "Lizhuang Ma"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1174_ECCV_2024_paper.php": {
    "title": "OneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers",
    "volume": "main",
    "abstract": "Existing end-to-end trackers for vision-based 3D perception suffer from performance degradation due to the conflict between detection and tracking tasks. In this work, we get to the bottom of this conflict, which was vaguely attributed to incompatible task-specific object features previously. We find the conflict between the two tasks lies in their partially conflicted classification gradients, which stems from their subtle difference in positive sample assignments. Based on this observation, we propose to coordinate those conflicted gradients from object queries with contradicted polarity in the two tasks. We also dynamically split all object queries into four groups based on their polarity in the two tasks. Attention between query sets with conflicted positive sample assignments is masked. The tracking classification loss is modified to suppress inaccurate predictions. To this end, we propose , the first one-stage joint detection and tracking model that bridges the gap between detection and tracking under a unified object feature representation. On the nuScenes camera-based object tracking benchmark, outperforms previous works by 6.9% AMOTA on the validation set and by 3.1% AMOTA on the test set",
    "checked": true,
    "id": "5a122fc35b8856202c93cddfdbb7576e945eecff",
    "semantic_title": "onetrack: demystifying the conflict between detection and tracking in end-to-end 3d trackers",
    "citation_count": 0,
    "authors": [
      "Qitai Wang",
      "Jiawei He",
      "Yuntao Chen",
      "Zhaoxiang Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1177_ECCV_2024_paper.php": {
    "title": "LoA-Trans: Enhancing Visual Grounding by Location-Aware Transformers",
    "volume": "main",
    "abstract": "Given an image and text description, visual grounding will find target region in the image explained by the text. It has two task settings: referring expression comprehension (REC) to estimate bounding-box and referring expression segmentation (RES) to predict segmentation mask. Currently the most promising visual grounding approaches are to learn REC and RES jointly by giving rich ground truth of both bounding-box and segmentation mask of the target object. However, we argue that a very simple but strong constraint has been overlooked by the existing approaches: given an image and a text description, REC and RES refer to the same object. We propose Location Aware Transformer (LoA-Trans) making this constraint explicit by a center prompt, where the system first predicts the center of the target object by Location-Aware Network, and feeds it as a common prompt to both REC and RES. In this way, the system constrains that REC and RES refer to the same object. To mitigate possible inaccuracies in center estimation, we introduce a query selection mechanism. Instead of random initialization queries for bounding-box and segmentation mask decoding, the query selection mechanism generates possible object locations other than the estimated center and use them as location-aware queries as a remedy for possible inaccurate center estimation. We also introduce a TaskSyn Network in the decoder to better coordination between REC and RES. Our method achieved state-of-the-art performance on three commonly used datasets: Refcoco, Refcoco+, and Refcocog. Extensive ablation studies demonstrated the validity of each of the proposed components",
    "checked": true,
    "id": "3e9675d4d98ddacd3c1e8d79eb9531bc27a02e25",
    "semantic_title": "loa-trans: enhancing visual grounding by location-aware transformers",
    "citation_count": 0,
    "authors": [
      "Ziling Huang*",
      "Shin'ichi Satoh"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1178_ECCV_2024_paper.php": {
    "title": "HAC: Hash-grid Assisted Context for 3D Gaussian Splatting Compression",
    "volume": "main",
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a promising framework for novel view synthesis, boasting rapid rendering speed with high fidelity. However, the substantial Gaussians and their associated attributes necessitate effective compression techniques. Nevertheless, the sparse and unorganized nature of the point cloud of Gaussians (or anchors in our paper) presents challenges for compression. To address this, we make use of the relations between the unorganized anchors and the structured hash grid, leveraging their mutual information for context modeling, and propose a Hash-grid Assisted Context (HAC) framework for highly compact 3DGS representation. Our approach introduces a binary hash grid to establish continuous spatial consistencies, allowing us to unveil the inherent spatial relations of anchors through a carefully designed context model. To facilitate entropy coding, we utilize Gaussian distributions to accurately estimate the probability of each quantized attribute, where an adaptive quantization module is proposed to enable high-precision quantization of these attributes for improved fidelity restoration. Additionally, we incorporate an adaptive masking strategy to eliminate invalid Gaussians and anchors. Importantly, our work is the pioneer to explore context-based compression for 3DGS representation, resulting in a remarkable size reduction of over 75× compared to vanilla 3DGS, while simultaneously improving fidelity, and achieving over 11× size reduction over SoTA 3DGS compression approach Scaffold-GS. Our code is available redhere",
    "checked": true,
    "id": "1be2584d082822e2ba9d6d8c47313a4ad3e653a5",
    "semantic_title": "hac: hash-grid assisted context for 3d gaussian splatting compression",
    "citation_count": 22,
    "authors": [
      "Yihang Chen*",
      "Qianyi Wu",
      "Weiyao Lin*",
      "Mehrtash Harandi",
      "Jianfei Cai"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1183_ECCV_2024_paper.php": {
    "title": "Energy-induced Explicit quantification for Multi-modality MRI fusion",
    "volume": "main",
    "abstract": "Multi-modality magnetic resonance imaging (MRI) is crucial for accurate disease diagnosis and surgical planning by comprehensively analyzing multi-modality information fusion. This fusion is characterized by unique patterns of information aggregation for each disease across modalities, influenced by distinct inter-dependencies and shifts in information flow. Existing fusion methods implicitly identify distinct aggregation patterns for various tasks, indicating the potential for developing a unified and explicit aggregation pattern. In this study, we propose a novel aggregation pattern, Energy-induced Explicit Propagation and Alignment (E2 PA), to explicitly quantify and optimize the properties of multi-modality MRI fusion to adapt to different scenarios. In E2 PA, (1) An energy-guided hierarchical fusion (EHF) uncovers the quantification and optimization of inter-dependencies propagation among multi-modalities by hierarchical same energy among patients. (2) An energy-regularized space alignment (ESA) measures the consistency of information flow in multi-modality aggregation by the alignment on space factorization and energy minimization. Through the extensive experiments on three public multi-modality MRI datasets (with different modality combinations and tasks), the superiority of E2 PA can be demonstrated from the comparison with state-of-the-art methods. Our code is available at https://github.com/JerryQseu/EEPA",
    "checked": true,
    "id": "bffe789c088d43ba14fc34817a62cb6782ef2fa2",
    "semantic_title": "energy-induced explicit quantification for multi-modality mri fusion",
    "citation_count": 0,
    "authors": [
      "Xiaoming Qi*",
      "Yuan Zhang",
      "Tong Wang",
      "Guanyu Yang*",
      "Yueming Jin*",
      "Shuo Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1191_ECCV_2024_paper.php": {
    "title": "ColorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement",
    "volume": "main",
    "abstract": "Text-to-Image (T2I) generation has made significant advancements with the advent of diffusion models. These models exhibit remarkable abilities to produce images based on textual prompts. Current T2I models allow users to specify object colors using linguistic color names. However, these labels encompass broad color ranges, making it difficult to achieve precise color matching. To tackle this challenging task, named color prompt learning, we propose to learn specific color prompts tailored to user-selected colors. Existing T2I personalization methods tend to result in color-shape entanglement. To overcome this, we generate several basic geometric objects in the target color, allowing for color and shape disentanglement during the color prompt learning. Our method, denoted as , successfully assists the T2I models to peel off the novel color prompts from these colored shapes. In the experiments, we demonstrate the efficacy of in achieving precise color generation with T2I models. Furthermore, we generalize to effectively learn abstract attribute concepts, including textures, materials, etc. Our findings represent a significant step towards improving precision and versatility of T2I models, offering new opportunities for creative applications and design tasks. Our project is available at https://moatifbutt.github.io/colorpeel/",
    "checked": true,
    "id": "70ee19452224c1a97fc06cd672ba6d5bb9eb6f4e",
    "semantic_title": "colorpeel: color prompt learning with diffusion models via color and shape disentanglement",
    "citation_count": 1,
    "authors": [
      "Muhammad Atif Butt*",
      "Kai Wang",
      "Javier Vazquez-Corral",
      "Joost van de Weijer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1192_ECCV_2024_paper.php": {
    "title": "Exemplar-free Continual Representation Learning via Learnable Drift Compensation",
    "volume": "main",
    "abstract": "Exemplar-free class-incremental learning using a backbone trained from scratch and starting from a small first task presents a significant challenge for continual representation learning. Prototype-based approaches, when continually updated, face the critical issue of semantic drift due to which the old class prototypes drift to different positions in the new feature space. Through an analysis of prototype-based continual learning, we show that forgetting is not due to diminished discriminative power of the feature extractor, and can potentially be corrected by drift compensation. To address this, we propose Learnable Drift Compensation (LDC), which can effectively mitigate drift in any moving backbone, whether supervised or unsupervised. LDC is fast and straightforward to integrate on top of existing continual learning approaches. Furthermore, we showcase how LDC can be applied in combination with self-supervised CL methods, resulting in the first exemplar-free semi-supervised continual learning approach. We achieve state-of-the-art performance in both supervised and semi-supervised settings across multiple datasets. Code is available at https: //github.com/alviur/ldc",
    "checked": true,
    "id": "3ff5d43ca2e7f74bf41b24eff510e80f60a42a0d",
    "semantic_title": "exemplar-free continual representation learning via learnable drift compensation",
    "citation_count": 0,
    "authors": [
      "Alex Gomez-Villa*",
      "Dipam Goswami",
      "Kai Wang",
      "Andy Bagdanov",
      "Bartlomiej Twardowski",
      "Joost van de Weijer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1205_ECCV_2024_paper.php": {
    "title": "Walker: Self-supervised Multiple Object Tracking by Walking on Temporal Object Appearance Graphs",
    "volume": "main",
    "abstract": "The supervision of state-of-the-art multiple object tracking (MOT) methods requires enormous annotation efforts to provide bounding boxes for all frames of all videos, and instance IDs to associate them through time. To this end, we introduce Walker, the first self-supervised tracker that learns from videos with sparse bounding box annotations, and no tracking labels. First, we design a quasi-dense temporal object appearance graph, and propose a novel multi-positive contrastive objective to optimize random walks on the graph and learn instance similarities. Then, we introduce an algorithm to enforce mutually-exclusive connective properties across instances in the graph, optimizing the learned topology for MOT. At inference time, we propose to associate detected instances to tracklets based on the max-likelihood transition state under motion-constrained bi-directional walks. Walker is the first self-supervised tracker to achieve competitive performance on MOT17, DanceTrack, and BDD100K. Remarkably, our proposal outperforms the previous self-supervised trackers even when drastically reducing the annotation requirements by up to 400x",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mattia Segù*",
      "Luigi Piccinelli",
      "Siyuan Li",
      "Luc Van Gool",
      "Fisher Yu",
      "Bernt Schiele"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1223_ECCV_2024_paper.php": {
    "title": "Spatio-Temporal Proximity-Aware Dual-Path Model for Panoramic Activity Recognition",
    "volume": "main",
    "abstract": "Panoramic Activity Recognition (PAR) seeks to identify diverse human activities across different scales, from individual actions to social group and global activities in crowded panoramic scenes. PAR presents two major challenges: 1) recognizing the nuanced interactions among numerous individuals and 2) understanding multi-granular human activities. To address these, we propose Social Proximity-aware Dual-Path Network (SPDP-Net) based on two key design principles. First, while previous works often focus on spatial distance among individuals within an image, we argue to consider the spatio-temporal proximity. It is crucial for individual relation encoding to correctly understand social dynamics. Secondly, deviating from existing hierarchical approaches (individual-to-social-to-global activity), we introduce a dual-path architecture for multi-granular activity recognition. This architecture comprises individual-to-global and individual-to-social paths, mutually reinforcing each other's task with global-local context through multiple layers. Through extensive experiments, we validate the effectiveness of the spatio-temporal proximity among individuals and the dual-path architecture in PAR. Furthermore, SPDP-Net achieves new state-of-the-art performance with 46.5% of overall F1 score on JRDB-PAR dataset",
    "checked": true,
    "id": "cbf4604e52e063b8ec06c21b01f863e39a535820",
    "semantic_title": "spatio-temporal proximity-aware dual-path model for panoramic activity recognition",
    "citation_count": 0,
    "authors": [
      "Sumin Lee*",
      "Yooseung Wang",
      "Sangmin Woo",
      "Changick Kim"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1231_ECCV_2024_paper.php": {
    "title": "DiffiT: Diffusion Vision Transformers for Image Generation",
    "volume": "main",
    "abstract": "Diffusion models with their powerful expressivity and high sample quality have achieved State-Of-The-Art (SOTA) performance in the generative domain. The pioneering Vision Transformer (ViT) has also demonstrated strong modeling capabilities and scalability, especially for recognition tasks. In this paper, we study the effectiveness of ViTs in diffusion-based generative learning and propose a new model denoted as Diffusion Vision Transformers (DiffiT). Specifically, we propose a methodology for finegrained control of the denoising process and introduce the Time-dependant Multihead Self Attention (TMSA) mechanism. DiffiT is surprisingly effective in generating high-fidelity images with significantly better parameter efficiency. We also propose latent and image space DiffiT models and show SOTA performance on a variety of class-conditional and unconditional synthesis tasks at different resolutions. The Latent DiffiT model achieves a new SOTA FID score of 1.73 on ImageNet-256 dataset while having 19.85%, 16.88% less parameters than other Transformer-based diffusion models such as MDT and DiT, respectively",
    "checked": true,
    "id": "6ce3a7c0c0035a39420d27c2ce01f48f5fab79c3",
    "semantic_title": "diffit: diffusion vision transformers for image generation",
    "citation_count": 34,
    "authors": [
      "Ali Hatamizadeh*",
      "Jiaming Song",
      "Guilin Liu",
      "Jan Kautz",
      "Arash Vahdat"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1237_ECCV_2024_paper.php": {
    "title": "WebRPG: Automatic Web Rendering Parameters Generation for Visual Presentation",
    "volume": "main",
    "abstract": "In the era of content creation revolution propelled by advancements in generative models, the field of web design remains unexplored despite its critical role in modern digital communication. The web design process is complex and often time-consuming, especially for those with limited expertise. In this paper, we introduce Web Rendering Parameters Generation (WebRPG), a new task that aims at automating the generation for visual presentation of web pages based on their HTML code. WebRPG would contribute to a faster web development workflow. Since there is no existing benchmark available, we develop a new dataset for WebRPG through an automated pipeline. Moreover, we present baseline models, utilizing VAE to manage numerous elements and rendering parameters, along with custom HTML embedding for capturing essential semantic and hierarchical information from HTML. Extensive experiments, including customized quantitative evaluations for this specific task, are conducted to evaluate the quality of the generated results. The dataset and code can be accessed at GitHub1 . 1 https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/ DocumentUnderstanding/WebRPG",
    "checked": true,
    "id": "a6a52671751e9216c32bcb08e469fb222fb8bb6f",
    "semantic_title": "webrpg: automatic web rendering parameters generation for visual presentation",
    "citation_count": 1,
    "authors": [
      "Zirui Shao",
      "Feiyu Gao",
      "Hangdi Xing",
      "Zepeng Zhu",
      "Zhi Yu*",
      "Jiajun Bu",
      "Qi Zheng",
      "Cong Yao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1247_ECCV_2024_paper.php": {
    "title": "GPSFormer: A Global Perception and Local Structure Fitting-based Transformer for Point Cloud Understanding",
    "volume": "main",
    "abstract": "Despite the significant advancements in pre-training methods for point cloud understanding, directly capturing intricate shape information from irregular point clouds without reliance on external data remains a formidable challenge. To address this problem, we propose GPSFormer, an innovative Global Perception and Local Structure Fitting-based Transformer, which learns detailed shape information from point clouds with remarkable precision. The core of GPSFormer is the Global Perception Module (GPM) and the Local Structure Fitting Convolution (LSFConv). Specifically, GPM utilizes Adaptive Deformable Graph Convolution (ADGConv) to identify short-range dependencies among similar features in the feature space and employs Multi-Head Attention (MHA) to learn long-range dependencies across all positions within the feature space, ultimately enabling flexible learning of contextual representations. Inspired by Taylor series, we design LSFConv, which learns both low-order fundamental and high-order refinement information from explicitly encoded local geometric structures. Integrating the GPM and LSFConv as fundamental components, we construct GPSFormer, a cutting-edge Transformer that effectively captures global and local structures of point clouds. Extensive experiments validate GPSFormer's effectiveness in three point cloud tasks: shape classification, part segmentation, and few-shot learning. The code of GPSFormer is available at https://github.com/changshuowang/ GPSFormer",
    "checked": true,
    "id": "978d8bd9f8ffe4ecda863bd50234b4181145e351",
    "semantic_title": "gpsformer: a global perception and local structure fitting-based transformer for point cloud understanding",
    "citation_count": 3,
    "authors": [
      "Changshuo Wang*",
      "Meiqing Wu",
      "Siew-Kei Lam",
      "Xin Ning",
      "Shangshu Yu",
      "Ruiping Wang",
      "Weijun Li",
      "Thambipillai Srikanthan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1249_ECCV_2024_paper.php": {
    "title": "FreeMotion: A Unified Framework for Number-free Text-to-Motion Synthesis",
    "volume": "main",
    "abstract": "Text-to-motion synthesis is a crucial task in computer vision. Existing methods are limited in their universality, as they are tailored for single-person or two-person scenarios and can not be applied to generate motions for more individuals. To achieve the number-free motion synthesis, this paper reconsiders motion generation and proposes to unify the single and multi-person motion by the conditional motion distribution. Furthermore, a generation module and an interaction module are designed for our FreeMotion framework to decouple the process of conditional motion generation and finally support the number-free motion synthesis. Besides, based on our framework, the current single-person motion spatial control method could be seamlessly integrated, achieving precise control of multi-person motion. Extensive experiments demonstrate the superior performance of our method and our capability to infer single and multi-human motions simultaneously",
    "checked": true,
    "id": "f9dfeadca81f89700d8f1a2e7eb67ed1395f85a1",
    "semantic_title": "freemotion: a unified framework for number-free text-to-motion synthesis",
    "citation_count": 4,
    "authors": [
      "Ke Fan",
      "Junshu Tang",
      "Weijian Cao",
      "Ran Yi*",
      "Moran Li",
      "Jingyu Gong",
      "Jiangning Zhang",
      "Yabiao Wang",
      "Chengjie Wang",
      "Lizhuang Ma*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1250_ECCV_2024_paper.php": {
    "title": "FSD-BEV: Foreground Self-Distillation for Multi-view 3D Object Detection",
    "volume": "main",
    "abstract": "Although multi-view 3D object detection based on the Bird's-Eye-View (BEV) paradigm has garnered widespread attention as an economical and deployment-friendly perception solution for autonomous driving, there is still a performance gap compared to LiDAR-based methods. In recent years, several cross-modal distillation methods have been proposed to transfer beneficial information from teacher models to student models, with the aim of enhancing performance. However, these methods face challenges due to discrepancies in feature distribution originating from different data modalities and network structures, making knowledge transfer exceptionally challenging. In this paper, we propose a Foreground Self-Distillation (FSD) scheme that effectively avoids the issue of distribution discrepancies, maintaining remarkable distillation effects without the need for pre-trained teacher models or cumbersome distillation strategies. Additionally, we design two Point Cloud Intensification (PCI) strategies to compensate for the sparsity of point clouds by frame combination and pseudo point assignment. Finally, we develop a Multi-Scale Foreground Enhancement (MSFE) module to extract and fuse multi-scale foreground features by predicted elliptical Gaussian heatmap, further improving the model's performance. We integrate all the above innovations into a unified framework named FSD-BEV. Extensive experiments on the nuScenes dataset exhibit that FSD-BEV achieves state-of-the-art performance, highlighting its effectiveness. The code and models are available at: https: // github. com/ CocoBoom/ fsd-bev",
    "checked": true,
    "id": "640916e72e859278f18eed902ffa5fb706c7186a",
    "semantic_title": "fsd-bev: foreground self-distillation for multi-view 3d object detection",
    "citation_count": 0,
    "authors": [
      "Zheng Jiang",
      "Jinqing Zhang",
      "Yanan Zhang",
      "Qingjie Liu*",
      "Zhenghui HU*",
      "Baohui Wang",
      "Yunhong Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1255_ECCV_2024_paper.php": {
    "title": "SceneGraphLoc: Cross-Modal Coarse Visual Localization on 3D Scene Graphs",
    "volume": "main",
    "abstract": "We introduce the task of localizing an input image within a multi-modal reference map represented by a collection of 3D scene graphs. These scene graphs comprise multiple modalities, including object-level point clouds, images, attributes, and relationships between objects, offering a lightweight and efficient alternative to conventional methods that rely on extensive image databases. Given these modalities, the proposed method learns a fixed-sized embedding for each node (, representing object instances) in the scene graph, enabling effective matching with the objects visible in the input query image. This strategy significantly outperforms other cross-modal methods, even without incorporating images into the map representation. With images, achieves performance close to that of state-of-the-art techniques depending on large image databases, while requiring three orders-of-magnitude less storage and operating orders-of-magnitude faster. Code and models are available at https://scenegraphloc.github.io",
    "checked": true,
    "id": "4f8e0b5ca78f1e3925089ea5ba0623db65e9a51f",
    "semantic_title": "scenegraphloc: cross-modal coarse visual localization on 3d scene graphs",
    "citation_count": 0,
    "authors": [
      "Yang Miao",
      "Francis Engelmann",
      "Olga Vysotska",
      "Federico Tombari",
      "Marc Pollefeys",
      "Daniel Barath*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1267_ECCV_2024_paper.php": {
    "title": "ScanReason: Empowering 3D Visual Grounding with Reasoning Capabilities",
    "volume": "main",
    "abstract": "Although great progress has been made in 3D visual grounding, current models still rely on explicit textual descriptions for grounding and lack the ability to reason human intentions from implicit instructions. We propose a new task called and introduce a new benchmark ScanReason which provides over 10K question-answer-location pairs from five reasoning types that require the synerization of reasoning and grounding. We further design our approach, , composed of the visual-centric reasoning module empowered by Multi-modal Large Language Model (MLLM) and the 3D grounding module to obtain accurate object locations by looking back to the enhanced geometry and fine-grained details from the 3D scenes. A chain-of-grounding mechanism is proposed to further boost the performance with interleaved reasoning and grounding steps during inference. Extensive experiments on the proposed benchmark validate the effectiveness of our proposed approach",
    "checked": true,
    "id": "fd36c54ae4f81f1fca80c52125a14bee3a0b6711",
    "semantic_title": "scanreason: empowering 3d visual grounding with reasoning capabilities",
    "citation_count": 4,
    "authors": [
      "Chenming Zhu",
      "Tai Wang",
      "Wenwei Zhang",
      "Kai Chen",
      "Xihui Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1270_ECCV_2024_paper.php": {
    "title": "MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?",
    "volume": "main",
    "abstract": "The remarkable progress of Multi-modal Large Language Models (MLLMs) has gained unparalleled attention. However, their capabilities in visual math problem-solving remain insufficiently evaluated and understood. We investigate current benchmarks to incorporate excessive visual content within textual questions, which potentially assist MLLMs in deducing answers without truly interpreting the input diagrams. To this end, we introduce , an all-around visual math benchmark designed for an equitable and in-depth evaluation of MLLMs. We meticulously collect 2,612 high-quality, multi-subject math problems with diagrams from publicly available sources. Each problem is then transformed by human annotators into six distinct versions, each offering varying degrees of information content in multi-modality, contributing to 15K test samples in total. This approach allows to comprehensively assess whether and how much MLLMs can truly understand the visual diagrams for mathematical reasoning. In addition, we propose a Chain-of-Thought (CoT) evaluation strategy for a fine-grained assessment of the output answers. Rather than naively judging true or false, we employ GPT-4(V) to adaptively assess each step with error analysis to derive a total score, which can reveal the inner CoT reasoning quality by MLLMs. With , we unveil that, most existing MLLMs struggle to understand math diagrams, relying heavily on textual questions. Surprisingly, some of them even achieve 5%+ higher accuracy without the visual input. Besides, GPT-4V and MAVIS-7B achieve the best overall performance within closed-source and open-source models, respectively. We hope the benchmark may provide unique insights to guide the future development of MLLMs. Project page: https://mathverse-cuhk.github.io. ∗ Equal contribution ‡ Project lead † Corresponding author",
    "checked": true,
    "id": "6d017adda6b2b1ea627dde2f0e85401ebb9fe566",
    "semantic_title": "mathverse: does your multi-modal llm truly see the diagrams in visual math problems?",
    "citation_count": 63,
    "authors": [
      "Renrui Zhang",
      "Dongzhi Jiang",
      "Yichi Zhang",
      "Haokun Lin",
      "Ziyu Guo",
      "Pengshuo Qiu",
      "Aojun Zhou",
      "Pan Lu",
      "Kai-Wei Chang",
      "Peng Gao",
      "Hongsheng Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1280_ECCV_2024_paper.php": {
    "title": "See and Think: Embodied Agent in Virtual Environment",
    "volume": "main",
    "abstract": "Large language models (LLMs) have achieved impressive pro-gress on several open-world tasks. Recently, using LLMs to build embodied agents has been a hotspot. This paper proposes STEVE, a comprehensive and visionary embodied agent in the Minecraft virtual environment. STEVE comprises three key components: vision perception, language instruction, and code action. Vision perception involves interpreting visual information in the environment, which is then integrated into the LLMs component with agent state and task instruction. Language instruction is responsible for iterative reasoning and decomposing complex tasks into manageable guidelines. Code action generates executable skill actions based on retrieval in skill database, enabling the agent to interact effectively within the Minecraft environment. We also collect STEVE-21K dataset, which includes 600+ vision-environment pairs, 20K knowledge question-answering pairs, and 200+ skill-code pairs. We conduct continuous block search, knowledge question and answering, and tech tree mastery to evaluate the performance. Extensive experiments show that STEVE achieves at most 1.5× faster unlocking key tech trees and 2.5× quicker in block search tasks",
    "checked": true,
    "id": "a756b584f8f8b4307e52895ae2120bc339580ad8",
    "semantic_title": "see and think: embodied agent in virtual environment",
    "citation_count": 14,
    "authors": [
      "Zhonghan Zhao",
      "Xuan Wang",
      "Wenhao Chai",
      "Boyi Li",
      "Shengyu Hao",
      "Shidong Cao",
      "Tian Ye",
      "Gaoang Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1284_ECCV_2024_paper.php": {
    "title": "PISR: Polarimetric Neural Implicit Surface Reconstruction for Textureless and Specular Objects",
    "volume": "main",
    "abstract": "Neural implicit surface reconstruction has achieved remarkable progress recently. Despite resorting to complex radiance modeling, state-of-the-art methods still struggle with textureless and specular surfaces. Different from RGB images, polarization images can provide direct constraints on the azimuth angles of the surface normals. In this paper, we present PISR, a novel method that utilizes a geometrically accurate polarimetric loss to refine shape independently of appearance. In addition, PISR smooths surface normals in image space to eliminate severe shape distortions and leverages the hash-grid-based neural signed distance function to accelerate the reconstruction. Experimental results demonstrate that PISR achieves higher accuracy and robustness, with an L1 Chamfer distance of 0.5 mm and an F-score of 99.5% at 1 mm, while converging 4 ∼ 30× faster than previous polarimetric surface reconstruction methods. The source code is available at https://github.com/GCChen97/PISR",
    "checked": true,
    "id": "bc0e117f7b5f027e346083d4c47788749915c80f",
    "semantic_title": "pisr: polarimetric neural implicit surface reconstruction for textureless and specular objects",
    "citation_count": 0,
    "authors": [
      "Guangcheng Chen*",
      "Yicheng He",
      "Li He",
      "Hong Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1286_ECCV_2024_paper.php": {
    "title": "Bridging the Gap Between Human Motion and Action Semantics via Kinematics Phrases",
    "volume": "main",
    "abstract": "Motion understanding aims to establish a reliable mapping between motion and action semantics, while it is a challenging many-to-many problem. An abstract action semantic (i.e., walk forwards) could be conveyed by perceptually diverse motions (walking with arms up or swinging). In contrast, a motion could carry different semantics w.r.t. its context and intention. This makes an elegant mapping between them difficult. Previous attempts adopted direct-mapping paradigms with limited reliability. Also, current automatic metrics fail to provide reliable assessments of the consistency between motions and action semantics. We identify the source of these problems as the significant gap between the two modalities. To alleviate this gap, we propose Kinematic Phrases (KP) that take the objective kinematic facts of human motion with proper abstraction, interpretability, and generality. Based on KP, we can unify a motion knowledge base and build a motion understanding system. Meanwhile, KP can be automatically converted from motions to text descriptions with no subjective bias, inspiring Kinematic Prompt Generation (KPG) as a novel white-box motion generation benchmark. In extensive experiments, our approach shows superiority over other methods. Our project is available at https://foruck.github.io/KP/",
    "checked": false,
    "id": "6eb922eb6ce09d1a7f853a17000c8dec99243514",
    "semantic_title": "bridging the gap between human motion and action semantics via kinematic phrases",
    "citation_count": 3,
    "authors": [
      "Xinpeng Liu",
      "Yong-Lu Li*",
      "Ailing Zeng",
      "Zizheng Zhou",
      "Yang You",
      "Cewu Lu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1287_ECCV_2024_paper.php": {
    "title": "VisFocus: Prompt-Guided Vision Encoders for OCR-Free Dense Document Understanding",
    "volume": "main",
    "abstract": "In recent years, notable advancements have been made in the domain of visual document understanding, with the prevailing architecture comprising a cascade of vision and language models. The text component can either be extracted explicitly with the use of external OCR models in OCR-based approaches, or alternatively, the vision model can be endowed with reading capabilities in OCR-free approaches. Typically, the queries to the model are input exclusively to the language component, necessitating the visual features to encompass the entire document. In this paper, we present VisFocus, an OCR-free method designed to better exploit the vision encoder's capacity by coupling it directly with the language prompt. To do so, we replace the down-sampling layers with layers that receive the input prompt and allow highlighting relevant parts of the document, while disregarding others. We pair the architecture enhancements with a novel pre-training task, using language masking on a snippet of the document text fed to the visual encoder in place of the prompt, to empower the model with focusing capabilities. Consequently, VisFocus learns to allocate its attention to text patches pertinent to the provided prompt. Our experiments demonstrate that this prompt-guided visual encoding approach significantly improves performance, achieving state-of-the-art results on various benchmarks. *Work done during an internshipemployment at Amazon† Corresponding author: nivnay@amazon.com",
    "checked": true,
    "id": "1ab3c15e56a91c6a7b3c22c7004f1f79e7f4ed8c",
    "semantic_title": "visfocus: prompt-guided vision encoders for ocr-free dense document understanding",
    "citation_count": 1,
    "authors": [
      "Ofir Abramovich*",
      "Niv Nayman*",
      "Sharon Fogel",
      "Inbal Lavi",
      "Ron Litman",
      "Shahar Tsiper",
      "Royee Tichauer",
      "Srikar Appalaraju",
      "Shai Mazor",
      "R. Manmatha"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1288_ECCV_2024_paper.php": {
    "title": "Masked Angle-Aware Autoencoder for Remote Sensing Images",
    "volume": "main",
    "abstract": "To overcome the inherent domain gap between remote sensing (RS) images and natural images, some self-supervised representation learning methods have made promising progress. However, they have overlooked the diverse angles present in RS objects. This paper proposes the Masked Angle-Aware Autoencoder (MA3E) to perceive and learn angles during pre-training. We design a scaling center crop operation to create the rotated crop with random orientation on each original image, introducing the explicit angle variation. MA3E inputs this composite image while reconstruct the original image, aiming to effectively learn rotation-invariant representations by restoring the angle variation introduced on the rotated crop. To avoid biases caused by directly reconstructing the rotated crop, we propose an Optimal Transport (OT) loss that automatically assigns similar original image patches to each rotated crop patch for reconstruction. MA3E1 demonstrates more competitive performance than existing pre-training methods on seven different RS image datasets in three downstream tasks. 1 Our code will be released at: https://github.com/benesakitam/MA3E",
    "checked": true,
    "id": "1598b8e202e6a358ec13193d8a14850d88205dff",
    "semantic_title": "masked angle-aware autoencoder for remote sensing images",
    "citation_count": 1,
    "authors": [
      "Zhihao Li*",
      "Biao Hou",
      "Siteng Ma",
      "zitong wu",
      "Xianpeng Guo",
      "bo ren",
      "Licheng Jiao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1291_ECCV_2024_paper.php": {
    "title": "Infinite-ID: Identity-preserved Personalization via ID-semantics Decoupling Paradigm",
    "volume": "main",
    "abstract": "Drawing on recent advancements in diffusion models for text-to-image generation, identity-preserved personalization has made significant progress in accurately capturing specific identities with just a single reference image. However, existing methods primarily integrate reference images within the text embedding space, leading to a complex entanglement of image and text information, which poses challenges for preserving both identity fidelity and semantic consistency. To tackle this challenge, we propose Infinite-ID, an ID-semantics decoupling paradigm for identity-preserved personalization. Specifically, we introduce identity-enhanced training, incorporating an additional image cross-attention module to capture sufficient ID information while deactivating the original text cross-attention module of the diffusion model. This ensures that the image stream faithfully represents the identity provided by the reference image while mitigating interference from textual input. Additionally, we introduce a feature interaction mechanism that combines a mixed attention module with an AdaIN-mean operation to seamlessly merge the two streams. This mechanism not only enhances the fidelity of identity and semantic consistency but also enables convenient control over the styles of the generated images. Extensive experimental results on both raw photo generation and style image generation demonstrate the superior performance of our proposed method",
    "checked": true,
    "id": "7d86baec4e76b6ae20439380596453180865b03d",
    "semantic_title": "infinite-id: identity-preserved personalization via id-semantics decoupling paradigm",
    "citation_count": 7,
    "authors": [
      "Yi Wu",
      "Ziqiang Li",
      "Heliang Zheng",
      "Chaoyue Wang*",
      "Bin Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1296_ECCV_2024_paper.php": {
    "title": "MultiGen: Zero-shot Image Generation from Multi-modal Prompts",
    "volume": "main",
    "abstract": "The field of text-to-image generation has witnessed substantial advancements in the preceding years, allowing the generation of high-quality images based solely on text prompts. However, accurately describing objects through text alone is challenging, necessitating the integration of additional modalities like coordinates and images for more precise image generation. Existing methods often require fine-tuning or only support using single object as the constraint, leaving the zero-shot image generation from multi-object multi-modal prompts as an unresolved challenge. In this paper, we propose MultiGen, a novel method designed to address this problem. Given an image-text pair, we obtain object-level text, coordinates and images, and integrate the information into an \"augmented token\" for each object. The augmented tokens serve as additional conditions and are trained alongside text prompts in the diffusion model, enabling our model to handle multi-object multi-modal prompts. To manage the absence of modalities during inference, we leverage a coordinate model and a feature model to generate object-level coordinates and features based on text prompts. Consequently, our method can generate images from text prompts alone or from various combinations of multi-modal prompts. Through extensive qualitative and quantitative experiments, we demonstrate that our method not only outperforms existing methods but also enables a wide range of tasks",
    "checked": false,
    "id": "ef4b604fca0c62dcd0d5caf7ca24ad74e285632d",
    "semantic_title": "multiqg-ti: towards question generation from multi-modal sources",
    "citation_count": 5,
    "authors": [
      "Zhi-Fan Wu*",
      "Lianghua Huang",
      "Wei Wang",
      "Yanheng Wei",
      "Yu Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1308_ECCV_2024_paper.php": {
    "title": "GazeXplain: Learning to Predict Natural Language Explanations of Visual Scanpaths",
    "volume": "main",
    "abstract": "While exploring visual scenes, humans' scanpaths are driven by their underlying attention processes. Understanding visual scanpaths is essential for various applications. Traditional scanpath models predict the where and when of gaze shifts without providing explanations, creating a gap in understanding the rationale behind fixations. To bridge this gap, we introduce GazeXplain, a novel study of visual scanpath prediction and explanation. This involves annotating natural-language explanations for fixations across eye-tracking datasets and proposing a general model with an attention-language decoder that jointly predicts scanpaths and generates explanations. It integrates a unique semantic alignment mechanism to enhance the consistency between fixations and explanations, alongside a cross-dataset co-training approach for generalization. These novelties present a comprehensive and adaptable solution for explainable human visual scanpath prediction. Extensive experiments on diverse eye-tracking datasets demonstrate the effectiveness of GazeXplain in both scanpath prediction and explanation, offering valuable insights into human visual attention and cognitive processes",
    "checked": true,
    "id": "f50e886a981fdb7f2c7b0756dab37d83a6230580",
    "semantic_title": "gazexplain: learning to predict natural language explanations of visual scanpaths",
    "citation_count": 1,
    "authors": [
      "Xianyu Chen*",
      "Ming Jiang",
      "Qi Zhao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1309_ECCV_2024_paper.php": {
    "title": "Learning Chain of Counterfactual Thought for Bias-Robust Vision-Language Reasoning",
    "volume": "main",
    "abstract": "Despite the remarkable success of large vision-language models (LVLMs) on various tasks, their susceptibility to knowledge bias inherited from training data hinders their ability to generalize to new scenarios and limits their real-world applicability. To address this challenge, we propose the Counterfactual Bias-Robust Reasoning (CoBRa) dataset that tackles knowledge bias by offering a novel collection of VQA examples designed to evaluate and mitigate bias in LVLMs. These examples encourage counterfactual thinking by providing edited knowledge graphs and image contents, with detailed annotations of reasoning processes to facilitate a comprehensive understanding of the examples. Based on the dataset, we introduce a Chain of Counterfactual Thought (CoCT) method that learns the bias-robust reasoning processes and provides in-context examples demonstrating how existing reasoning generalizes to counterfactual scenarios. This enables LVLMs to explicitly reason step-by-step rather than relying on biased knowledge, leading to more generalizable solutions. Our extensive evaluation demonstrates that CoCT outperforms existing approaches on tasks requiring reasoning under knowledge bias. Our work is available at https://github. com/SuperJohnZhang/CoBRa",
    "checked": true,
    "id": "feac15fe9b4f84af64ee8545cfdb62259d042148",
    "semantic_title": "learning chain of counterfactual thought for bias-robust vision-language reasoning",
    "citation_count": 0,
    "authors": [
      "Yifeng Zhang",
      "Ming Jiang",
      "Qi Zhao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1317_ECCV_2024_paper.php": {
    "title": "SegGen: Supercharging Segmentation Models with Text2Mask and Mask2Img Synthesis",
    "volume": "main",
    "abstract": "We present , a new data generation approach that pushes the performance boundaries of state-of-the-art image segmentation models. One major bottleneck of previous data synthesis methods for segmentation is the design of \"segmentation labeler module\", which is used to synthesize segmentation masks for images [?]. The segmentation labeler modules, which are segmentation models by themselves, bound the performance of downstream segmentation models trained on the synthetic masks. These methods encounter a \"chicken or egg dilemma\" and thus fail to outperform existing segmentation models. To address this issue, we propose a novel method that reverses the traditional data generation process: we first (i) generate highly diverse segmentation masks that match real-world distribution from text prompts, and then (ii) synthesize realistic images conditioned on the segmentation masks. In this way, we avoid the need for any segmentation labeler module. integrates two data generation strategies, namely MaskSyn and ImgSyn, to largely improve data diversity in synthetic masks and images. Notably, the high quality of our synthetic data enables our method to outperform the previous data synthesis method [?] by +25.2 mIoU on ADE20K when trained with pure synthetic data. On the highly competitive ADE20K and COCO benchmarks, our data generation method markedly improves the performance of state-of-the-art segmentation models in semantic segmentation, panoptic segmentation, and instance segmentation. Moreover, experiments show that training with our synthetic data makes the segmentation models more robust towards unseen data domains, including real-world and AI-generated images",
    "checked": true,
    "id": "80cc3f4aad61277286b26520599db8a6aa58e8c2",
    "semantic_title": "seggen: supercharging segmentation models with text2mask and mask2img synthesis",
    "citation_count": 7,
    "authors": [
      "Hanrong Ye*",
      "Jason Kuen",
      "Qing Liu",
      "Zhe Lin",
      "Brian Price",
      "Dan Xu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1318_ECCV_2024_paper.php": {
    "title": "Sync from the Sea: Retrieving Alignable Videos from Large-Scale Datasets",
    "volume": "main",
    "abstract": "Temporal video alignment aims to synchronize the key events like object interactions or action phase transitions in two videos. Such methods could benefit various video editing, processing, and understanding tasks. However, existing approaches operate under the restrictive assumption that a suitable video pair for alignment is given, significantly limiting their broader applicability. To address this, we re-pose temporal alignment as a search problem and introduce the task of Alignable Video Retrieval (AVR). Given a query video, our approach can identify well-alignable videos from a large collection of clips and temporally synchronize them to the query. To achieve this, we make three key contributions: 1) we introduce DRAQ, a video alignability indicator to identify and re-rank the best alignable video from a set of candidates; 2) we propose an effective and generalizable frame-level video feature design to improve the alignment performance of several off-the-shelf feature representations, and 3) we propose a novel benchmark and evaluation protocol for AVR using cycle-consistency metrics. Our experiments on 3 datasets, including large-scale Kinetics700, demonstrate the effectiveness of our approach in identifying alignable video pairs from diverse datasets",
    "checked": true,
    "id": "421d6206c1d6d4eead488f24aa29c611f2e0a740",
    "semantic_title": "sync from the sea: retrieving alignable videos from large-scale datasets",
    "citation_count": 1,
    "authors": [
      "Ishan Rajendrakumar Dave*",
      "Fabian Caba",
      "Mubarak Shah",
      "Simon Jenni*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1319_ECCV_2024_paper.php": {
    "title": "FinePseudo: Improving Pseudo-Labelling through Temporal-Alignablity for Semi-Supervised Fine-Grained Action Recognition",
    "volume": "main",
    "abstract": "Real-life applications of action recognition often require a fine-grained understanding of subtle movements, e.g., in sports analytics, user interactions in AR/VR, and surgical videos. Although fine-grained actions are more costly to annotate, existing semi-supervised action recognition has mainly focused on coarse-grained action recognition. Since fine-grained actions are more challenging due to the absence of scene bias, classifying these actions requires an understanding of action-phases. Hence, existing coarse-grained semi-supervised methods do not work effectively. In this work, we for the first time thoroughly investigate semi-supervised fine-grained action recognition (FGAR). We observe that alignment distances like dynamic time warping (DTW) provide a suitable action-phase-aware measure for comparing fine-grained actions, a concept previously unexploited in FGAR. However, since regular DTW distance is pairwise and assumes strict alignment between pairs, it is not directly suitable for classifying fine-grained actions. To utilize such alignment distances in a limited-label setting, we propose an Alignability-Verification-based Metric learning technique to effectively discriminate between fine-grained action pairs. Our learnable alignability score provides a better phase-aware measure, which we use to refine the pseudo-labels of the primary video encoder. Our collaborative pseudo-labeling-based framework ‘FinePseudo' significantly outperforms prior methods on four fine-grained action recognition datasets: Diving48, FineGym99, FineGym288, and FineDiving, and shows improvement on existing coarse-grained datasets: Kinetics400 and Something-SomethingV2. We also demonstrate the robustness of our collaborative pseudo-labeling in handling novel unlabeled classes in open-world semi-supervised setups",
    "checked": true,
    "id": "49b18c86140bfafb3f99742146f98d0af17e129f",
    "semantic_title": "finepseudo: improving pseudo-labelling through temporal-alignablity for semi-supervised fine-grained action recognition",
    "citation_count": 1,
    "authors": [
      "Ishan Rajendrakumar Dave*",
      "Mamshad Nayeem Rizve*",
      "Mubarak Shah"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1329_ECCV_2024_paper.php": {
    "title": "Elegantly Written: Disentangling Writer and Character Styles for Enhancing Online Chinese Handwriting",
    "volume": "main",
    "abstract": "The electronic writing tools, while enhancing convenience, sacrifice the readability and efficiency of handwritten content. Balancing high efficiency with readable handwriting poses a challenging research task. In this paper, we propose a method sequence-based models to beautify user handwritten traces. Unlike most existing methods that treat Chinese handwriting as images and cannot reflect the human writing process, we capture individual writing characteristics from a small amount of user handwriting trajectories and beautify the user's traces by mimicking their writing style and process. We fully consider the style of radicals and components between the content and reference glyphs, assigning appropriate fine-grained styles to strokes in the content glyphs through a cross-attention mechanism module. Additionally, we find that many style features contribute minimally to the final stylized results. Therefore, we decompose the style features into the Cartesian product of single-dimensional variable sets, effectively removing redundant features with limited impact on the stylization effect while preserving key style information. Qualitative and quantitative experiments both demonstrate the superiority of our approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Liu",
      "Fatimah binti Khalid",
      "Lei Wang",
      "Youxi Zhang",
      "Cunrui Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1331_ECCV_2024_paper.php": {
    "title": "UniCode : Learning a Unified Codebook for Multimodal Large Language Models",
    "volume": "main",
    "abstract": "In this paper, we propose UniCode, a novel approach within the domain of multimodal large language models (MLLMs) that learns a unified codebook to efficiently tokenize visual, text, and potentially other types of signals. This innovation addresses a critical limitation in existing MLLMs: their reliance on a text-only codebook, which restricts MLLMs' ability to generate images and texts in a multimodal context. Towards this end, we propose a language-driven iterative training paradigm, coupled with an in-context pre-training task we term \"image decompression\", enabling our model to interpret compressed visual data and generate high-quality images. The unified codebook empowers our model to extend visual instruction tuning to non-linguistic generation tasks. Moreover, UniCode is adaptable to diverse stacked quantization approaches in order to compress visual signals into a more compact token representation. Despite using significantly fewer parameters and less data during training, UniCode demonstrates promising capabilities in visual reconstruction and generation. It also achieves performance comparable to leading MLLMs across a spectrum of VQA benchmarks",
    "checked": false,
    "id": "835424063bf3c4035f96374a60147d821d474c2a",
    "semantic_title": "unicode: learning a unified codebook for multimodal large language models",
    "citation_count": 5,
    "authors": [
      "Sipeng Zheng*",
      "Bohan Zhou",
      "Yicheng Feng",
      "Ye Wang",
      "Zongqing Lu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1334_ECCV_2024_paper.php": {
    "title": "When Do We Not Need Larger Vision Models?",
    "volume": "main",
    "abstract": "Scaling up the size of vision models has been the de facto standard to obtain more powerful visual representations. In this work, we discuss the point beyond which larger vision models are not necessary. First, we demonstrate the power of Scaling on Scales (), whereby a pre-trained and frozen smaller vision model (, ViT-B or ViT-L), run over multiple image scales, can outperform larger models (, ViT-H or ViT-G) on classification, segmentation, depth estimation, Multimodal LLM (MLLM) benchmarks, and robotic manipulation. Notably, achieves state-of-the-art performance in detailed understanding of MLLM on the V∗ benchmark, surpassing models such as GPT-4V. We examine the conditions under which is a preferred scaling approach compared to scaling on model size. While larger models have the advantage of better generalization on hard examples, we show that features of larger vision models can be well approximated by those of multi-scale smaller models. This suggests most, if not all, of the representations learned by current large pre-trained models can also be obtained from multi-scale smaller models. Our results show that a multi-scale smaller model has comparable learning capacity to a larger model, and pre-training smaller models with can match or even exceed the advantage of larger models. We release a Python package that can apply on any vision model with one line of code: https://github.com/bfshi/scaling_on_scales",
    "checked": true,
    "id": "dbdfe71fdf641bebac5a052a60de75342871e3df",
    "semantic_title": "when do we not need larger vision models?",
    "citation_count": 26,
    "authors": [
      "Baifeng Shi*",
      "Ziyang Wu",
      "Maolin Mao",
      "Xin Wang",
      "Trevor Darrell"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1340_ECCV_2024_paper.php": {
    "title": "GVGEN: Text-to-3D Generation with Volumetric Representation",
    "volume": "main",
    "abstract": "In recent years, 3D Gaussian splatting has emerged as a powerful technique for 3D reconstruction and generation, known for its fast and high-quality rendering capabilities. Nevertheless, these methods often come with limitations, either lacking the ability to produce diverse samples or requiring prolonged inference times. To address these shortcomings, this paper introduces a novel diffusion-based framework, GVGEN, designed to efficiently generate 3D Gaussian representations from text input. We propose two innovative techniques: (1) Structured Volumetric Representation. We first arrange disorganized 3D Gaussian points as a structured form GaussianVolume. This transformation allows the capture of intricate texture details within a volume composed of a fixed number of Gaussians. To better optimize the representation of these details, we propose a unique pruning and densifying method named the Candidate Pool Strategy, enhancing detail fidelity through selective optimization. (2) Coarse-to-fine Generation Pipeline. To simplify the generation of GaussianVolume and empower the model to generate instances with detailed 3D geometry, we propose a coarse-to-fine pipeline. It initially constructs a basic geometric structure, followed by the prediction of complete Gaussian attributes. Our framework, GVGEN, demonstrates superior performance in qualitative and quantitative assessments compared to existing 3D generation methods. Simultaneously, it maintains a fast generation speed (∼7 seconds), effectively striking a balance between quality and efficiency. Our project page is https://gvgen.github.io/. ∗ Equal Contribution. † Corresponding Authors",
    "checked": true,
    "id": "0ecb340140fb81b4a114a6cbc7fac00129ff3231",
    "semantic_title": "gvgen: text-to-3d generation with volumetric representation",
    "citation_count": 9,
    "authors": [
      "Xianglong He",
      "Junyi Chen",
      "Sida Peng",
      "Di Huang",
      "Yangguang Li",
      "Xiaoshui Huang",
      "Chun Yuan*",
      "Wanli Ouyang",
      "Tong He*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1341_ECCV_2024_paper.php": {
    "title": "Bidirectional Stereo Image Compression with Cross-Dimensional Entropy Model",
    "volume": "main",
    "abstract": "With the rapid advancement of stereo vision technologies, stereo image compression has emerged as a crucial field that continues to draw significant attention. Previous approaches have primarily employed a unidirectional paradigm, where the compression of one view is dependent on the other, resulting in imbalanced compression. To address this issue, we introduce a symmetric bidirectional stereo image compression architecture, named BiSIC. Specifically, we propose a 3D convolution based codec backbone to capture local features and incorporate bidirectional attention blocks to exploit global features. Moreover, we design a novel cross-dimensional entropy model that integrates various conditioning factors, including the spatial context, channel context, and stereo dependency, to effectively estimate the distribution of latent representations for entropy coding. Extensive experiments demonstrate that our proposed BiSIC outperforms conventional image/video compression standards, as well as state-of-the-art learning-based methods, in terms of both PSNR and MS-SSIM",
    "checked": true,
    "id": "1fd9e5ff4e4471c061ebdb97d302fbf977b1d08e",
    "semantic_title": "bidirectional stereo image compression with cross-dimensional entropy model",
    "citation_count": 1,
    "authors": [
      "Zhening Liu",
      "Xinjie Zhang",
      "Jiawei Shao",
      "Zehong Lin*",
      "Jun Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1346_ECCV_2024_paper.php": {
    "title": "UniINR: Event-guided Unified Rolling Shutter Correction, Deblurring, and Interpolation",
    "volume": "main",
    "abstract": "Video frames captured by rolling shutter (RS) cameras during fast camera movement frequently exhibit RS distortion and blur simultaneously. Naturally, recovering high-frame-rate global shutter (GS) sharp frames from an RS blur frame must simultaneously consider RS correction, deblur, and frame interpolation. A naive way is to decompose the whole process into separate tasks and cascade existing methods; however, this results in cumulative errors and noticeable artifacts. Event cameras enjoy many advantages, , high temporal resolution, making them potential for our problem. To this end, we propose the first and novel approach, named UniINR, to recover arbitrary frame-rate sharp GS frames from an RS blur frame and paired events. Our key idea is unifying spatial-temporal implicit neural representation (INR) to directly map the position and time coordinates to color values to address the interlocking degradations. Specifically, we introduce spatial-temporal implicit encoding (STE) to convert an RS blur image and events into a spatial-temporal representation (STR). To query a specific sharp frame (GS or RS), we embed the exposure time into STR and decode the embedded features pixel-by-pixel to recover a sharp frame. Our method features a lightweight model with only 0.38M parameters, and it also enjoys high inference efficiency, achieving 2.83ms/f rame in 31× frame interpolation of an RS blur frame. Extensive experiments show that our method significantly outperforms prior methods. Code is available at https: //github.com/yunfanLu/UniINR",
    "checked": true,
    "id": "0655201b70109a862e87de06088fcbe9cf29f133",
    "semantic_title": "uniinr: event-guided unified rolling shutter correction, deblurring, and interpolation",
    "citation_count": 1,
    "authors": [
      "Yunfan Lu*",
      "Guoqiang Liang",
      "Yusheng Wang",
      "Lin Wang",
      "Hui Xiong*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1355_ECCV_2024_paper.php": {
    "title": "ReLoo: Reconstructing Humans Dressed in Loose Garments from Monocular Video in the Wild",
    "volume": "main",
    "abstract": "While previous years have seen great progress in the 3D reconstruction of humans from monocular videos, few of the state-of-the-art methods are able to handle loose garments that exhibit large non-rigid surface deformations during articulation. This limits the application of such methods to humans that are dressed in standard pants or T-shirts. Our method, , overcomes this limitation and reconstructs high-quality 3D models of humans dressed in loose garments from monocular in-the-wild videos. To tackle this problem, we first establish a layered neural human representation that decomposes clothed humans into a neural inner body and outer clothing. On top of the layered neural representation, we further introduce a non-hierarchical virtual bone deformation module for the clothing layer that can freely move, which allows the accurate recovery of non-rigidly deforming loose clothing. A global optimization jointly optimizes the shape, appearance, and deformations of the human body and clothing via multi-layer differentiable volume rendering. To evaluate , we record subjects with dynamically deforming garments in a multi-view capture studio. This evaluation, both on existing and our novel dataset, demonstrates 's clear superiority over prior art on both indoor datasets and in-the-wild videos",
    "checked": true,
    "id": "a28b49291045de0140b42c6e46d2d4fd2430231b",
    "semantic_title": "reloo: reconstructing humans dressed in loose garments from monocular video in the wild",
    "citation_count": 2,
    "authors": [
      "Chen Guo*",
      "Tianjian Jiang",
      "Manuel Kaufmann",
      "Chengwei Zheng",
      "Julien Valentin",
      "Jie Song*",
      "Otmar Hilliges"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1358_ECCV_2024_paper.php": {
    "title": "Weakly-supervised Camera Localization by Ground-to-satellite Image Registration",
    "volume": "main",
    "abstract": "The ground-to-satellite image matching/retrieval was initially proposed for city-scale ground camera localization. This work addresses the problem of improving camera pose accuracy by ground-to-satellite image matching after a coarse location and orientation have been obtained, either from the city-scale retrieval or from consumer-level GPS and compass sensors. Existing learning-based methods for solving this task require accurate GPS labels of ground images for network training. However, obtaining such accurate GPS labels is difficult, often requiring an expensive blackReal Time Kinematics (RTK) setup and suffering from signal occlusion, multi-path signal disruptions, . To alleviate this issue, this paper proposes a weakly supervised learning strategy for ground-to-satellite image registration when only noisy pose labels for ground images are available for network training. It derives positive and negative satellite images for each ground image and leverages contrastive learning to learn feature representations for ground and satellite images useful for translation estimation. We also propose a self-supervision strategy for cross-view image relative rotation estimation, which trains the network by creating pseudo query and reference image pairs. Experimental results show that our weakly supervised learning strategy achieves the best performance on cross-area evaluation compared to recent state-of-the-art methods that are reliant on accurate pose labels for supervision",
    "checked": true,
    "id": "ac56fdc7236db09715e020ce5872fcce2d8e2d47",
    "semantic_title": "weakly-supervised camera localization by ground-to-satellite image registration",
    "citation_count": 0,
    "authors": [
      "Yujiao Shi*",
      "HONGDONG LI",
      "Akhil Perincherry",
      "Ankit Vora"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1370_ECCV_2024_paper.php": {
    "title": "Dataset Growth",
    "volume": "main",
    "abstract": "Deep learning benefits from the growing abundance of available data. Meanwhile, efficiently dealing with the growing data scale has become a challenge. Data publicly available are from different sources with various qualities, and it is impractical to do manual cleaning against noise and redundancy given today's data scale. There are existing techniques for cleaning/selecting the collected data. However, these methods are mainly proposed for offline settings that target one of the cleanness and redundancy problems. In practice, data are growing exponentially with both problems. This leads to repeated data curation with sub-optimal efficiency. To tackle this challenge, we propose InfoGrowth, an efficient online algorithm for data cleaning and selection, resulting in a growing dataset that keeps up to date with awareness of cleanliness and diversity. InfoGrowth can improve data quality/efficiency on both single-modal and multi-modal tasks, with an efficient and scalable design. Its framework makes it practical for real-world data engines",
    "checked": true,
    "id": "1ccc1ec26f3b35f2e719e8ea1494d31ffc81a10b",
    "semantic_title": "dataset growth",
    "citation_count": 0,
    "authors": [
      "Ziheng Qin*",
      "zhaopan xu",
      "YuKun Zhou",
      "Kai Wang*",
      "Zangwei Zheng",
      "Zebang Cheng",
      "Hao Tang",
      "Lei Shang",
      "Baigui Sun",
      "Radu Timofte",
      "Xiaojiang Peng",
      "Hongxun Yao*",
      "Yang You*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1372_ECCV_2024_paper.php": {
    "title": "MaRINeR: Enhancing Novel Views by Matching Rendered Images with Nearby References",
    "volume": "main",
    "abstract": "Rendering realistic images from 3D reconstruction is an essential task of many Computer Vision and Robotics pipelines, notably for mixed-reality applications as well as training autonomous agents in simulated environments. However, the quality of novel views heavily depends of the source reconstruction which is often imperfect due to noisy or missing geometry and appearance. Inspired by the recent success of reference-based super-resolution networks, we propose MaRINeR, a refinement method that leverages information of a nearby mapping image to improve the rendering of a target viewpoint. We first establish matches between the raw rendered image of the scene geometry from the target viewpoint and the nearby reference based on deep features, followed by hierarchical detail transfer. We show improved renderings in quantitative metrics and qualitative examples from both explicit and implicit scene representations. We further employ our method on the downstream tasks of pseudo-ground-truth validation, synthetic data enhancement and detail recovery for renderings of reduced 3D reconstructions",
    "checked": true,
    "id": "91271535eccac967542a6c464fe901e7b844a498",
    "semantic_title": "mariner: enhancing novel views by matching rendered images with nearby references",
    "citation_count": 0,
    "authors": [
      "Lukas Bösiger*",
      "Mihai Dusmanu",
      "Marc Pollefeys",
      "Zuria Bauer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1377_ECCV_2024_paper.php": {
    "title": "Teaching Tailored to Talent: Adverse Weather Restoration via Prompt Pool and Depth-Anything Constraint",
    "volume": "main",
    "abstract": "Recent advancements in adverse weather restoration have shown potential, yet the unpredictable and varied combinations of weather degradations in the real world pose significant challenges. Previous methods typically struggle with dynamically handling intricate degradation combinations and carrying on background reconstruction precisely, leading to performance and generalization limitations. Drawing inspiration from prompt learning and the \"Teaching Tailored to Talent\" concept, we introduce a novel pipeline, T3 -DiffWeather. Specifically, we employ a prompt pool that allows the network to autonomously combine sub-prompts to construct weather-prompts, harnessing the necessary attributes to adaptively tackle unforeseen weather input. Moreover, from a scene modeling perspective, we incorporate general prompts constrained by Depth-Anything feature to provide the scene-specific condition for the diffusion process. Furthermore, by incorporating contrastive prompt loss, we ensures distinctive representations for both types of prompts by a mutual pushing strategy. Experimental results demonstrate that our method achieves state-of-the-art performance across various synthetic and real-world datasets, markedly outperforming existing diffusion techniques in terms of computational efficiency",
    "checked": true,
    "id": "4a596d19d8a513422629a551253c62121c4c2c2d",
    "semantic_title": "teaching tailored to talent: adverse weather restoration via prompt pool and depth-anything constraint",
    "citation_count": 0,
    "authors": [
      "Sixiang Chen",
      "Tian Ye",
      "Kai Zhang",
      "Zhaohu Xing",
      "Yunlong Lin",
      "Lei Zhu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1381_ECCV_2024_paper.php": {
    "title": "MoE-DiffIR: Task-customized Diffusion Priors for Universal Compressed Image Restoration",
    "volume": "main",
    "abstract": "We present MoE-DiffIR, an innovative universal compressed image restoration (CIR) method with task-customized diffusion priors. This intends to handle two pivotal challenges in the existing CIR methods: (i) lacking adaptability and universality for different image codecs, , JPEG and WebP; (ii) poor texture generation capability, particularly at low bitrates. Specifically, our MoE-DiffIR develops the powerful mixture-of-experts (MoE) prompt module, where some basic prompts cooperate to excavate the task-customized diffusion priors from Stable Diffusion (SD) for each compression task. Moreover, the degradation-aware routing mechanism is proposed to enable the flexible assignment of basic prompts. To activate and reuse the cross-modality generation prior of SD, we design the visual-to-text adapter for MoE-DiffIR, which aims to adapt the embedding of low-quality images from the visual domain to the textual domain as the textual guidance for SD, enabling more consistent and reasonable texture generation. We also construct one comprehensive benchmark dataset for universal CIR, covering 21 types of degradations from 7 popular traditional and learned codecs. Extensive experiments on universal CIR have demonstrated the excellent robustness and texture restoration capability of our proposed MoE-DiffIR. The project can be found atmagenta https://renyulin-f.github.io/MoE-DiffIR.github.io/",
    "checked": true,
    "id": "3e681b0b942dcddca457ca20b95dfa4314fe7ec9",
    "semantic_title": "moe-diffir: task-customized diffusion priors for universal compressed image restoration",
    "citation_count": 1,
    "authors": [
      "Yulin Ren",
      "Xin Li*",
      "Bingchen Li",
      "Xingrui Wang",
      "Mengxi China Guo",
      "Shijie Zhao",
      "Li Zhang",
      "Zhibo Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1383_ECCV_2024_paper.php": {
    "title": "LEGO: Learning EGOcentric Action Frame Generation via Visual Instruction Tuning",
    "volume": "main",
    "abstract": "Generating instructional images of human daily actions from an egocentric viewpoint serves as a key step towards efficient skill transfer. In this paper, we introduce a novel problem – egocentric action frame generation. The goal is to synthesize an image depicting an action in the user's context (, action frame) by conditioning on a user prompt and an input egocentric image. Notably, existing egocentric action datasets lack the detailed annotations that describe the execution of actions. Additionally, existing diffusion-based image manipulation models are sub-optimal in controlling the state transition of an action in egocentric image pixel space because of the domain gap. To this end, we propose to Learn EGOcentric (LEGO) action frame generation via visual instruction tuning. First, we introduce a prompt enhancement scheme to generate enriched action descriptions from a visual large language model (VLLM) by visual instruction tuning. Then we propose a novel method to leverage image and text embeddings from the VLLM as additional conditioning to improve the performance of a diffusion model. We validate our model on two egocentric datasets – Ego4D and Epic-Kitchens. Our experiments show substantial improvement over prior image manipulation models in both quantitative and qualitative evaluation. We also conduct detailed ablation studies and analysis to provide insights in our method. More details of the dataset and code are available on the website (https://bolinlai.github.io/Lego_EgoActGen/)",
    "checked": true,
    "id": "b92289123a94f6076505487adfb4513bd3495c1d",
    "semantic_title": "lego: learning egocentric action frame generation via visual instruction tuning",
    "citation_count": 4,
    "authors": [
      "Bolin Lai*",
      "Xiaoliang Dai",
      "Lawrence Chen",
      "Guan Pang",
      "James M Rehg",
      "Miao Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1393_ECCV_2024_paper.php": {
    "title": "SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant",
    "volume": "main",
    "abstract": "Recent advances in vision-language models have shown notable generalization in broad tasks through visual instruction tuning. However, bridging the gap between the pre-trained vision encoder and the large language models (LLMs) becomes the whole network's bottleneck. To improve cross-modality alignment, existing works usually consider more visual instruction data covering a broader range of vision tasks to fine-tune the model for question-answering, which, however, is costly to obtain and has not thoroughly explored the rich contextual information contained in images. This paper first attempts to harness the overlooked context within visual instruction data, training the model to self-supervised \"learning\" how to ask high-quality questions. In this way, we introduce a novel framework named SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant. SQ-LLaVA exhibits proficiency in generating flexible and meaningful image-related questions while analyzing the visual clue and prior language knowledge, signifying an advanced level of generalized visual understanding. Moreover, fine-tuning SQ-LLaVA on higher-quality instruction data shows a performance improvement compared with traditional visual-instruction tuning methods. This improvement highlights the efficacy of self-questioning techniques in achieving a deeper and more nuanced comprehension of visual content across various contexts. Our code is available at https://github.com/heliossun/SQ-LLaVA",
    "checked": true,
    "id": "0a7aad85e06dd46ce96bf0e0d20979678b5d4cd3",
    "semantic_title": "sq-llava: self-questioning for large vision-language assistant",
    "citation_count": 2,
    "authors": [
      "Guohao Sun*",
      "Can Qin",
      "JIAMINAN WANG",
      "Zeyuan Chen",
      "Ran Xu",
      "Zhiqiang Tao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1394_ECCV_2024_paper.php": {
    "title": "Mesh2NeRF: Direct Mesh Supervision for Neural Radiance Field Representation and Generation",
    "volume": "main",
    "abstract": "We present , an approach to derive ground-truth radiance fields from textured meshes for 3D generation tasks. Many 3D generative approaches represent 3D scenes as radiance fields for training. Their ground-truth radiance fields are usually fitted from multi-view renderings from a large-scale synthetic 3D dataset, which often results in artifacts due to occlusions or under-fitting issues. In , we propose an analytic solution to directly obtain ground-truth radiance fields from 3D meshes, characterizing the density field with an occupancy function featuring a defined surface thickness, and determining view-dependent color through a reflection function considering both the mesh and environment lighting. extracts accurate radiance fields which provides direct supervision for training generative NeRFs and single scene representation. We validate the effectiveness of Mesh2NeRF across various tasks, achieving a noteworthy 3.12dB improvement in PSNR for view synthesis in single scene representation on the ABO dataset, a 0.69 PSNR enhancement in the single-view conditional generation of ShapeNet Cars, and notably improved mesh extraction from NeRF in the unconditional generation of Objaverse Mugs",
    "checked": true,
    "id": "5ad600caeb6427025e5d6922d068b806357931c8",
    "semantic_title": "mesh2nerf: direct mesh supervision for neural radiance field representation and generation",
    "citation_count": 0,
    "authors": [
      "Yujin Chen*",
      "Yinyu Nie",
      "Benjamin Ummenhofer",
      "Reiner Birkl",
      "Michael Paulitsch",
      "Matthias Müller",
      "Matthias Niessner"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1396_ECCV_2024_paper.php": {
    "title": "Listen to Look into the Future: Audio-Visual Egocentric Gaze Anticipation",
    "volume": "main",
    "abstract": "Egocentric gaze anticipation serves as a key building block for the emerging capability of Augmented Reality. Notably, gaze behavior is driven by both visual cues and audio signals during daily activities. Motivated by this observation, we introduce the first model that leverages both the video and audio modalities for egocentric gaze anticipation. Specifically, we propose a Contrastive Spatial-Temporal Separable (CSTS) fusion approach that adopts two modules to separately capture audio-visual correlations in spatial and temporal dimensions, and applies a contrastive loss on the re-weighted audio-visual features from fusion modules for representation learning. We conduct extensive ablation studies and thorough analysis using two egocentric video datasets: Ego4D and Aria, to validate our model design. We demonstrate that audio improves the performance by +2.5% and +2.4% on the two datasets. Our model also outperforms the prior state-of-the-art methods by at least +1.9% and +1.6%. Moreover, we provide visualizations to show the gaze anticipation results and share additional insights into audio-visual representation learning. The code and data split are available on our website (https://bolinlai.github.io/CSTS-EgoGazeAnticipation/)",
    "checked": true,
    "id": "004c8f4f37ecbef5d3a347a7e6ba00ecaac733e0",
    "semantic_title": "listen to look into the future: audio-visual egocentric gaze anticipation",
    "citation_count": 6,
    "authors": [
      "Bolin Lai*",
      "Fiona Ryan",
      "Wenqi Jia",
      "Miao Liu",
      "James M Rehg"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1397_ECCV_2024_paper.php": {
    "title": "R^2-Bench: Benchmarking the Robustness of Referring Perception Models under Perturbations",
    "volume": "main",
    "abstract": "Referring perception, which aims at grounding visual objects with multimodal referring guidance, is essential for bridging the gap between humans, who provide instructions, and the environment where intelligent systems perceive. Despite progress in this field, the robustness of referring perception models (RPMs) against disruptive perturbations is not well explored. This work thoroughly assesses the resilience of RPMs against various perturbations in both general and specific contexts. Recognizing the complex nature of referring perception tasks, we present a comprehensive taxonomy of perturbations, and then develop a versatile toolbox for synthesizing and evaluating the effects of composite disturbances. Employing this toolbox, we construct R2 -Bench, a benchmark for assessing the Robustness of Referring perception models under noisy conditions across five key tasks. Moreover, we propose the R2 -Agent, an LLM-based agent that simplifies and automates model evaluation via natural language instructions. Our investigation uncovers the vulnerabilities of current RPMs to various perturbations and provides tools for assessing model robustness, potentially promoting the safe and resilient integration of intelligent systems into complex real-world scenarios",
    "checked": false,
    "id": "911fa15f466548eab6fba0f592bc6c9908a386ae",
    "semantic_title": "r2-bench: benchmarking the robustness of referring perception models under perturbations",
    "citation_count": 0,
    "authors": [
      "Xiang Li*",
      "Kai Qiu",
      "Jinglu Wang",
      "Xiaohao Xu",
      "Kashu Yamazaki",
      "Hao Chen",
      "Rita Singh",
      "Xiaonan Huang",
      "Bhiksha Raj"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1402_ECCV_2024_paper.php": {
    "title": "Self-supervised co-salient object detection via feature correspondences at multiple scales",
    "volume": "main",
    "abstract": "Our paper introduces a novel two-stage self-supervised approach for detecting co-occurring salient objects (CoSOD) in image groups without requiring segmentation annotations. Unlike existing unsupervised methods that rely solely on patch-level information (clustering patch descriptors) or on computation heavy off-the-shelf components for CoSOD, our lightweight model leverages feature correspondences at both patch and region levels, significantly improving prediction performance. In the first stage, we train a self-supervised network that detects co-salient regions by computing local patch-level feature correspondences across images. We obtain the segmentation predictions using confidence-based adaptive thresholding. In the next stage, we refine these intermediate segmentations by eliminating the detected regions (within each image) whose averaged feature representations are dissimilar to the foreground feature representation averaged across all the thresholded cross-attention maps (from the previous stage). Extensive experiments on three CoSOD benchmark datasets show that our self-supervised model outperforms the corresponding state-of-the-art models by a huge margin (, on the CoCA dataset, our model has a 13.7% F-measure gain over the SOTA unsupervised CoSOD model). Notably, our self-supervised model also outperforms several recent fully supervised CoSOD models on the three test datasets (, on the CoCA dataset, our model has a 4.6% F-measure gain over a recent supervised CoSOD model). Our code is available at: https://github.com/sourachakra/ SCoSPARC",
    "checked": false,
    "id": "e6012265697eacf941d56f22c4eb34232a351ffa",
    "semantic_title": "self-supervised co-salient object detection via feature correspondence at multiple scales",
    "citation_count": 0,
    "authors": [
      "Souradeep Chakraborty*",
      "Dimitris Samaras"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1403_ECCV_2024_paper.php": {
    "title": "Differentiable Convex Polyhedra Optimization from Multi-view Images",
    "volume": "main",
    "abstract": "This paper presents a novel approach for the differentiable rendering of convex polyhedra, addressing the limitations of recent methods that rely on implicit field supervision. Our technique introduces a strategy that combines non-differentiable computation of hyperplane intersection through duality transform with differentiable optimization for vertex positioning with three-plane intersection, enabling gradient-based optimization without the need for 3D implicit fields. This allows for efficient shape representation across a range of applications, from shape parsing to compact mesh reconstruction. This work not only overcomes the challenges of previous approaches but also sets a new standard for representing shapes with convex polyhedra",
    "checked": true,
    "id": "e408eca420d56159ea61fc05b82a8e8e03c96640",
    "semantic_title": "differentiable convex polyhedra optimization from multi-view images",
    "citation_count": 0,
    "authors": [
      "Daxuan Ren*",
      "Haiyi Mei",
      "Hezi Shi",
      "Jianmin Zheng",
      "Jianfei Cai",
      "Lei Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1406_ECCV_2024_paper.php": {
    "title": "SlotLifter: Slot-guided Feature Lifting for Learning Object-Centric Radiance Fields",
    "volume": "main",
    "abstract": "The ability to distill object-centric abstractions from intricate visual scenes underpins human-level generalization. Despite the significant progress in object-centric learning methods, learning object-centric representations in the 3D physical world remains a crucial challenge. In this work, we propose , a novel object-centric radiance model addressing scene reconstruction and decomposition jointly via slot-guided feature lifting. Such a design unites object-centric learning representations and image-based rendering methods, offering performance in scene decomposition and novel-view synthesis on four challenging synthetic and four complex real-world datasets, outperforming existing 3D object-centric learning methods by a large margin. Through extensive ablative studies, we showcase the efficacy of designs in , revealing key insights for potential future directions",
    "checked": true,
    "id": "7037816e0a4628d5dff9c316f18ad267215df535",
    "semantic_title": "slotlifter: slot-guided feature lifting for learning object-centric radiance fields",
    "citation_count": 0,
    "authors": [
      "Yu Liu",
      "Baoxiong Jia*",
      "Yixin Chen",
      "Siyuan Huang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1407_ECCV_2024_paper.php": {
    "title": "SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding",
    "volume": "main",
    "abstract": "3D vision-language (3dvl) grounding, which aims to align language with 3D physical environments, stands as a cornerstone in developing embodied agents. In comparison to recent advancements in the 2D domain, grounding language in 3D scenes faces two significant challenges: (i) the scarcity of paired 3dvl data to support grounded learning of 3D scenes, especially considering complexities within diverse object configurations, rich attributes, and intricate relationships; and (ii) the absence of a unified learning framework to distill knowledge from grounded 3D data. In this work, we aim to address these major challenges in 3D-VL by examining the potential of systematically upscaling 3D-VL learning in indoor scenes. We introduce the first million-scale 3D-VL dataset, , encompassing indoor scenes and comprising vision-language pairs collected from both human annotations and our scalable scene-graph-based generation approach. We demonstrate that this scaling allows for a unified pre-training framework, Grounded Pre-training for Scenes (), for 3D-VL learning. Through extensive experiments, we showcase the effectiveness of by achieving performance on existing 3D visual grounding and question-answering benchmarks. We also show that the data scaling effect is not limited to , but is generally beneficial for models on tasks like 3D semantic segmentation. The vast potential of and is unveiled through zero-shot transfer experiments in challenging 3dvl tasks",
    "checked": true,
    "id": "b01e8f939c685dbf11cb38d3af7604ef1e77306f",
    "semantic_title": "sceneverse: scaling 3d vision-language learning for grounded scene understanding",
    "citation_count": 21,
    "authors": [
      "Baoxiong Jia*",
      "Yixin Chen",
      "Huangyue Yu",
      "Yan Wang",
      "Xuesong Niu",
      "Tengyu Liu",
      "Qing Li",
      "Siyuan Huang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1418_ECCV_2024_paper.php": {
    "title": "ADMap: Anti-disturbance Framework for Vectorized HD Map Construction",
    "volume": "main",
    "abstract": "In the field of autonomous driving, online High-definition (HD) map construction is crucial for planning tasks. Recent studies have developed several high-performance HD map construction models to meet the demand. However, the point sequences generated by recent HD map construction models are jittery or jagged due to prediction bias and impact subsequent tasks. To mitigate this jitter issue, we propose the Anti-Disturbance Map construction framework (ADMap), which contains Multi-scale Perception Neck (MPN), Instance Interactive Attention (IIA), and Vector Direction Difference Loss (VDDL). By exploring the point sequence relations between and within instances in a cascading manner, our proposed ADMap effectively monitors the point sequence prediction process, and achieves state-of-the-art performance on the nuScenes and Argoverse2 datasets. Extensive results demonstrate its ability to produce stable and reliable map elements in complex and changing driving scenarios",
    "checked": true,
    "id": "570ddb3d25392939bffae6b7dae2cf7367cc9581",
    "semantic_title": "admap: anti-disturbance framework for vectorized hd map construction",
    "citation_count": 0,
    "authors": [
      "Haotian Hu",
      "Fanyi Wang*",
      "Yaonong Wang",
      "Laifeng Hu",
      "Jingwei Xu",
      "Zhiwang Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1421_ECCV_2024_paper.php": {
    "title": "GaussianImage: 1000 FPS Image Representation and Compression by 2D Gaussian Splatting",
    "volume": "main",
    "abstract": "Implicit neural representations (INRs) recently achieved great success in image representation and compression, offering high visual quality and fast rendering speeds with 10-1000 FPS, assuming sufficient GPU resources are available. However, this requirement often hinders their use on low-end devices with limited memory. In response, we propose a groundbreaking paradigm of image representation and compression by 2D Gaussian Splatting, named GaussianImage. We first introduce 2D Gaussian to represent the image, where each Gaussian has 8 parameters including position, covariance and color. Subsequently, we unveil a novel rendering algorithm based on accumulated summation. Remarkably, our method with a minimum of 3× lower GPU memory usage and 5× faster fitting time not only rivals INRs (e.g., WIRE, I-NGP) in representation performance, but also delivers a faster rendering speed of 1500-2000 FPS regardless of parameter size. Furthermore, we integrate existing vector quantization technique to build an image codec. Experimental results demonstrate that our codec attains rate-distortion performance comparable to compression-based INRs such as COIN and COIN++, while facilitating decoding speeds of approximately 2000 FPS. Additionally, preliminary proof of concept shows that our codec surpasses COIN and COIN++ in performance when using partial bits-back coding. Code is available at https://github.com/Xinjie-Q/GaussianImage",
    "checked": true,
    "id": "b955b789019a39a847779abbf35e42975be89ed6",
    "semantic_title": "gaussianimage: 1000 fps image representation and compression by 2d gaussian splatting",
    "citation_count": 5,
    "authors": [
      "Xinjie Zhang",
      "Xingtong Ge",
      "Tongda Xu",
      "Dailan He",
      "Yan Wang",
      "Hongwei Qin",
      "Guo Lu",
      "Jing Geng*",
      "Jun Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1431_ECCV_2024_paper.php": {
    "title": "PanoVOS: Bridging Non-panoramic and Panoramic Views with Transformer for Video Segmentation",
    "volume": "main",
    "abstract": "Panoramic videos contain richer spatial information and have attracted tremendous amounts of attention due to their exceptional experience in some fields such as autonomous driving and virtual reality. However, existing datasets for video segmentation only focus on conventional planar images. To address the challenge, in this paper, we present a panoramic video dataset, i.e., PanoVOS. The dataset provides 150 videos with high video resolutions and diverse motions. To quantify the domain gap between 2D planar videos and panoramic videos, we evaluate 15 off-the-shelf video object segmentation (VOS) models on PanoVOS. Through error analysis, we found that all of them fail to tackle pixel-level content discontinues of panoramic videos. Thus, we present a Panoramic Space Consistency Transformer (PSCFormer), which can effectively utilize the semantic boundary information of the previous frame for pixel-level matching with the current frame. Extensive experiments demonstrate that compared with the previous SOTA models, our PSCFormer network exhibits a great advantage in terms of segmentation results under the panoramic setting. Our dataset poses new challenges in panoramic VOS and we hope that our PanoVOS can advance the development of panoramic segmentation/tracking. The dataset, codes, and pre-train models will be published at https://github.com/shilinyan99/PanoVOS",
    "checked": true,
    "id": "1cf8bc57014f76d3636b86e13712e5332053a8a5",
    "semantic_title": "panovos: bridging non-panoramic and panoramic views with transformer for video segmentation",
    "citation_count": 3,
    "authors": [
      "Shilin Yan*",
      "Xiaohao Xu",
      "Renrui Zhang",
      "Lingyi Hong",
      "wenchao chen",
      "Wenqiang Zhang",
      "Wei Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1435_ECCV_2024_paper.php": {
    "title": "Evaluating Text-to-Visual Generation with Image-to-Text Generation",
    "volume": "main",
    "abstract": "Despite significant progress in generative AI, comprehensive evaluation remains challenging because of the lack of effective metrics and standardized benchmarks. For instance, the widely-used CLIPScore measures the alignment between a (generated) image and text prompt, but it fails to produce reliable scores for complex prompts involving compositions of objects, attributes, and relations. One reason is that text encoders of CLIP can notoriously act as a \"bag of words\", conflating prompts such as the horse is eating the grass with the grass is eating the horse [?, ?, ?]. To address this, we introduce the VQAScore, which uses a visual-question-answering (VQA) model to produce an alignment score by computing the probability of a Yes answer to a simple Does this figure show {text}? question. Though simpler than prior art, VQAScore computed with off-the-shelf models produces state-of-the-art results across many (8) image-text alignment benchmarks. We also compute VQAScore with an in-house model that follows best practices in the literature. For example, we use a bidirectional image-question encoder that allows image embeddings to depend on the question being asked (and vice versa). Our in-house model, CLIP-FlanT5, outperforms even the strongest baselines that make use of the proprietary GPT-4V. Interestingly, although we train with only images, VQAScore can also align text with video and 3D models. VQAScore allows researchers to benchmark text-to-visual generation using complex texts that capture the compositional structure of real-world prompts. Towards this end, we introduce GenAI-Bench, a more challenging benchmark with 1,600 compositional text prompts that require parsing scenes, objects, attributes, relationships, and high-order reasoning such as comparison and logic. GenAI-Bench also collects over 15,000 human ratings for leading image and video models such as Stable Diffusion, DALL-E 3, Midjourney, and Gen2. We open-source our data, model, and code at link",
    "checked": true,
    "id": "ec45c3f0f88c8ce1deb5baa71c2c0e14ad64d249",
    "semantic_title": "evaluating text-to-visual generation with image-to-text generation",
    "citation_count": 32,
    "authors": [
      "Zhiqiu Lin*",
      "Deepak Pathak",
      "Baiqi Li",
      "Jiayao Li",
      "Xide Xia",
      "Graham Neubig",
      "Pengchuan Zhang",
      "Deva Ramanan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1436_ECCV_2024_paper.php": {
    "title": "SENC: Handling Self-collision in Neural Cloth Simulation",
    "volume": "main",
    "abstract": "We present SENC, a novel self-supervised neural cloth simulator that addresses the challenge of cloth self-collision. This problem has remained unresolved due to the gap in simulation setup between recent collision detection and response approaches and self-supervised neural simulators. The former requires collision-free initial setups, while the latter necessitates random cloth instantiation during training. To tackle this issue, we propose a novel loss based on Global Intersection Analysis (GIA). This loss extracts the volume surrounded by the cloth region that forms the penetration. By constructing an energy based on this volume, our self-supervised neural simulator can effectively address cloth self-collisions. Moreover, we develop a self-collision-aware graph neural network capable of learning to handle self-collisions, even for parts that are topologically distant from one another. Additionally, we introduce an effective external force scheme that enables the simulation to learn the cloth's behavior in response to random external forces. We validate the efficacy of SENC through extensive quantitative and qualitative experiments, demonstrating that it effectively reduces cloth self-collision while maintaining high-quality animation results",
    "checked": true,
    "id": "6e27f41cc9a106e991352a5880567d4a46711156",
    "semantic_title": "senc: handling self-collision in neural cloth simulation",
    "citation_count": 2,
    "authors": [
      "Zhouyingcheng Liao*",
      "Sinan Wang",
      "Taku Komura"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1439_ECCV_2024_paper.php": {
    "title": "HybridBooth: Hybrid Prompt Inversion for Efficient Subject-Driven Generation",
    "volume": "main",
    "abstract": "Recent advancements in text-to-image diffusion models have shown remarkable creative capabilities with textual prompts, but generating personalized instances based on specific subjects, known as subject-driven generation, remains challenging. To tackle this issue, we present a new hybrid framework called , which merges the benefits of optimization-based and direct-regression methods. operates in two stages: the Word Embedding Probe, which generates a robust initial word embedding using a fine-tuned encoder, and the Word Embedding Refinement, which further adapts the encoder to specific subject images by optimizing key parameters. This approach allows for effective and fast inversion of visual concepts into textual embedding, even from a single image, while maintaining the model's generalization capabilities",
    "checked": true,
    "id": "80559dd7fd5e6444a4951b5c71862b6c35986bd9",
    "semantic_title": "hybridbooth: hybrid prompt inversion for efficient subject-driven generation",
    "citation_count": 0,
    "authors": [
      "Shanyan Guan",
      "Yanhao Ge",
      "Ying Tai*",
      "Jian Yang",
      "Wei Li",
      "Mingyu You*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1451_ECCV_2024_paper.php": {
    "title": "PartCraft: Crafting Creative Objects by Parts",
    "volume": "main",
    "abstract": "This paper propels creative control in generative visual AI by allowing users to \"select\". Departing from traditional text or sketch-based methods, we for the first time allow users to choose visual concepts by parts for their creative endeavors. The outcome is fine-grained generation that precisely captures selected visual concepts, ensuring a holistically faithful and plausible result. To achieve this, we first parse objects into parts through unsupervised feature clustering. Then, we encode parts into text tokens and introduce an entropy-based normalized attention loss that operates on them. This loss design enables our model to learn generic prior topology knowledge about object's part composition, and further generalize to novel part compositions to ensure the generation looks holistically faithful. Lastly, we employ a bottleneck encoder to project the part tokens. This not only enhances fidelity but also accelerates learning, by leveraging shared knowledge and facilitating information exchange among instances. Visual results in the paper and supplementary material showcase the compelling power of in crafting highly customized, innovative creations, exemplified by the \"charming\" and creative birds in Fig. ??. Code is released at https://github.com/kamwoh/partcraft",
    "checked": true,
    "id": "f42f1a767bc1265408f3cb41497bc410f886181f",
    "semantic_title": "partcraft: crafting creative objects by parts",
    "citation_count": 1,
    "authors": [
      "Kam Woh Ng*",
      "Xiatian Zhu",
      "Yi-Zhe Song",
      "Tao Xiang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1453_ECCV_2024_paper.php": {
    "title": "GeometrySticker: Enabling Ownership Claim of Recolorized Neural Radiance Fields",
    "volume": "main",
    "abstract": "Remarkable advancements in the recolorization of Neural Radiance Fields (NeRF) have simplified the process of modifying NeRF's color attributes. Yet, with the potential of NeRF to serve as shareable digital assets, there's a concern that malicious users might alter the color of NeRF models and falsely claim the recolorized version as their own. To safeguard against such breaches of ownership, enabling original NeRF creators to establish rights over recolorized NeRF is crucial. While approaches like CopyRNeRF have been introduced to embed binary messages into NeRF models as digital signatures for copyright protection, the process of recolorization can remove these binary messages. In our paper, we present GeometrySticker, a method for seamlessly integrating binary messages into the geometry components of radiance fields, akin to applying a sticker. GeometrySticker can embed binary messages into NeRF models while preserving the effectiveness of these messages against recolorization. Our comprehensive studies demonstrate that GeometrySticker is adaptable to prevalent NeRF architectures and maintains a commendable level of robustness against various distortions. Project page: https://kevinhuangxf.github.io/GeometrySticker",
    "checked": true,
    "id": "13d4e1c2f09828526d7e5daa31f7d381fac4ac6a",
    "semantic_title": "geometrysticker: enabling ownership claim of recolorized neural radiance fields",
    "citation_count": 2,
    "authors": [
      "Xiufeng HUANG*",
      "Ka Chun Cheung",
      "Simon See",
      "Renjie Wan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1465_ECCV_2024_paper.php": {
    "title": "PYRA: Parallel Yielding Re-Activation for Training-Inference Efficient Task Adaptation",
    "volume": "main",
    "abstract": "Recently, the scale of transformers has grown rapidly, which introduces considerable challenges in terms of training overhead and inference efficiency in the scope of task adaptation. Existing works, namely Parameter-Efficient Fine-Tuning (PEFT) and model compression, have separately investigated the challenges. However, PEFT cannot guarantee the inference efficiency of the original backbone, especially for large-scale models. Model compression requires significant training costs for structure searching and re-training. Consequently, a simple combination of them cannot guarantee accomplishing both training efficiency and inference efficiency with minimal costs. In this paper, we propose a novel Parallel Yielding Re-Activation (PYRA) method for such a challenge of training-inference efficient task adaptation. PYRA first utilizes parallel yielding adaptive weights to comprehensively perceive the data distribution in downstream tasks. A re-activation strategy for token modulation is then applied for tokens to be merged, leading to calibrated token features. Extensive experiments demonstrate that PYRA outperforms all competing methods under both low compression rate and high compression rate, demonstrating its effectiveness and superiority in maintaining both training efficiency and inference efficiency for large-scale foundation models. Our code is available at https://github.com/ THU-MIG/PYRA",
    "checked": true,
    "id": "438825debb5f7c129b0bbc7e3b5c95606545f5f0",
    "semantic_title": "pyra: parallel yielding re-activation for training-inference efficient task adaptation",
    "citation_count": 8,
    "authors": [
      "Yizhe Xiong",
      "Hui Chen*",
      "Tianxiang Hao",
      "Zijia Lin",
      "Jungong Han",
      "Yuesong Zhang",
      "Guoxin Wang",
      "Yongjun Bao",
      "Guiguang Ding"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1467_ECCV_2024_paper.php": {
    "title": "FineMatch: Aspect-based Fine-grained Image and Text Mismatch Detection and Correction",
    "volume": "main",
    "abstract": "Recent progress in large-scale pre-training has led to the development of advanced vision-language models (VLMs) with remarkable proficiency in comprehending and generating multimodal content. Despite the impressive ability to perform complex reasoning for VLMs, current models often struggle to effectively and precisely capture the compositional information on both the image and text sides. To address this, we propose , a new aspect-based fine-grained text and image matching benchmark, focusing on text and image mismatch detection and correction. This benchmark introduces a novel task for boosting and evaluating the VLMs' compositionality for aspect-based fine-grained text and image matching. In this task, models are required to identify mismatched aspect phrases within a caption, determine the aspect's class, and propose corrections for an image-text pair that may contain between 0 and 3 mismatches. To evaluate the models' performance on this new task, we propose a new evaluation metric named ITM-IoU for which our experiments show a high correlation to human evaluation. In addition, we also provide a comprehensive experimental analysis of existing mainstream VLMs, including fully supervised learning and in-context learning settings. We have found that models trained on demonstrate enhanced proficiency in detecting fine-grained text and image mismatches. Moreover, models (e.g., GPT-4V, Gemini Pro Vision) with strong abilities to perform multimodal in-context learning are not as skilled at fine-grained compositional image and text matching analysis. With , we are able to build a system for text-to-image generation hallucination detection and correction. Resources are available at https://hanghuacs.github.io/finematch/",
    "checked": true,
    "id": "55df271485a4b620f5a2b83af46e4ce49fd1a405",
    "semantic_title": "finematch: aspect-based fine-grained image and text mismatch detection and correction",
    "citation_count": 6,
    "authors": [
      "Hang Hua*",
      "Jing Shi",
      "Kushal Kafle",
      "Simon Jenni",
      "Daoan Zhang",
      "John Collomosse",
      "Scott Cohen",
      "Jiebo Luo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1470_ECCV_2024_paper.php": {
    "title": "CrossScore: A Multi-View Approach to Image Evaluation and Scoring",
    "volume": "main",
    "abstract": "We introduce a novel cross-reference image quality assessment method that effectively fills the gap in the image assessment landscape, complementing the array of established evaluation schemes – ranging from full-reference metrics like SSIM [?], no-reference metrics such as NIQE [?], to general-reference metrics including FID [?], and Multi-modal-reference metrics, CLIPScore [?]. Utilising a neural network with the cross-attention mechanism and a unique data collection pipeline from NVS optimisation, our method enables accurate image quality assessment without requiring ground truth references. By comparing a query image against multiple views of the same scene, our method addresses the limitations of existing metrics in novel view synthesis (NVS) and similar tasks where direct reference images are unavailable. Experimental results show that our method is closely correlated to the full-reference metric SSIM, while not requiring ground truth references",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zirui Wang*",
      "Wenjing Bian",
      "Victor Adrian Prisacariu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1481_ECCV_2024_paper.php": {
    "title": "Modeling and Driving Human Body Soundfields through Acoustic Primitives",
    "volume": "main",
    "abstract": "While rendering and animation of photorealistic 3D human body models have matured and reached an impressive quality over the past years, modeling the spatial audio associated with such full body models has been largely ignored so far. In this work, we present a framework that allows for high-quality spatial audio generation, capable of rendering the full 3D soundfield generated by a human body, including speech, footsteps, hand-body interactions, and others. Given a basic audio-visual representation of the body in form of 3D body pose and audio from a head-mounted microphone, we demonstrate that we can render the full acoustic scene at any point in 3D space efficiently and accurately. To enable near-field and realtime rendering of sound, we borrow the idea of volumetric primitives from graphical neural rendering and transfer them into the acoustic domain. Our acoustic primitives result in an order of magnitude smaller soundfield representations and overcome deficiencies in near-field rendering compared to previous approaches. Our project page: https: //wikichao.github.io/Acoustic-Primitives/",
    "checked": true,
    "id": "2ee8de540f890887f024b9cf046e89d1082f4100",
    "semantic_title": "modeling and driving human body soundfields through acoustic primitives",
    "citation_count": 1,
    "authors": [
      "Chao Huang*",
      "Dejan Markovic*",
      "Chenliang Xu*",
      "Alexander Richard*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1486_ECCV_2024_paper.php": {
    "title": "m&m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks",
    "volume": "main",
    "abstract": "Real-world multi-modal problems are rarely solved by a single machine learning model, and often require multi-step computational plans that involve stitching several models. Tool-augmented LLMs hold tremendous promise for automating the generation of such computational plans. However, the lack of standardized benchmarks for evaluating LLMs as planners for multi-step multi-modal tasks has prevented a systematic study of planner design decisions. Should LLMs generate a full plan in a single shot or step-by-step? Should they invoke tools directly with Python code or through structured data formats like JSON? Does feedback improve planning? To answer these questions and more, we introduce : a benchmark containing 4K+ multi-step multi-modal tasks involving 33 tools that include multi-modal models, (free) public APIs, and image processing modules. For each of these task queries, we provide automatically generated plans using this realistic toolset. We further provide a high-quality subset of 1,565 task plans that are human-verified and correctly executable. With , we evaluate popular LLMs with 2 planning strategies (multi-step vs. step-by-step planning), 2 plan formats (JSON vs. code), and 3 types of feedback (parsing/verification/execution). Finally, we summarize takeaways from our extensive experiments and provide practical recommendations for designing planners for multi-step multi-modal tasks. Our dataset and evaluation code are available on HuggingFace1 and Github2 respectively. 1 https://huggingface.co/datasets/zixianma/mms 2 https://github.com/RAIVNLab/mms",
    "checked": true,
    "id": "ab12d8c5e607d702f2fc02ea711960509f9ec1a9",
    "semantic_title": "m&m's: a benchmark to evaluate tool-use for multi-step multi-modal tasks",
    "citation_count": 9,
    "authors": [
      "Zixian Ma*",
      "Weikai Huang",
      "Jieyu Zhang",
      "Tanmay Gupta",
      "Ranjay Krishna"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1492_ECCV_2024_paper.php": {
    "title": "Label-anticipated Event Disentanglement for Audio-Visual Video Parsing",
    "volume": "main",
    "abstract": "Audio-Visual Video Parsing (AVVP) task aims to detect and temporally locate events within audio and visual modalities. Multiple events can overlap in the timeline, making identification challenging. While traditional methods usually focus on improving the early audio-visual encoders to embed more effective features, the decoding phase – crucial for final event classification, often receives less attention. We aim to advance the decoding phase and improve its interpretability. Specifically, we introduce a new decoding paradigm, label semantic-based projection (LEAP), that employs labels texts of event categories, each bearing distinct and explicit semantics, for parsing potentially overlapping events. LEAP works by iteratively projecting encoded latent features of audio/visual segments onto semantically independent label embeddings. This process, enriched by modeling cross-modal (audio/visual-label) interactions, gradually disentangles event semantics within video segments to refine relevant label embeddings, guaranteeing a more discriminative and interpretable decoding process. To facilitate the LEAP paradigm, we propose a semantic-aware optimization strategy, which includes a novel audio-visual semantic similarity loss function. This function leverages the Intersection over Union of audio and visual events (EIoU) as a novel metric to calibrate audio-visual similarities at the feature level, accommodating the varied event densities across modalities. Extensive experiments demonstrate the superiority of our method, achieving new state-of-the-art performance for AVVP and also enhancing the relevant audio-visual event localization task.: Corresponding authors ({guodan,wangmeng}@hfut.edu.cn)",
    "checked": true,
    "id": "78c51e8c493d990054e8602b71dfc33080e29f57",
    "semantic_title": "label-anticipated event disentanglement for audio-visual video parsing",
    "citation_count": 1,
    "authors": [
      "Jinxing Zhou*",
      "Dan Guo*",
      "Yuxin Mao",
      "Yiran Zhong",
      "Xiaojun Chang",
      "Meng Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1495_ECCV_2024_paper.php": {
    "title": "High-Fidelity 3D Textured Shapes Generation by Sparse Encoding and Adversarial Decoding",
    "volume": "main",
    "abstract": "3D vision is inherently characterized by sparse spatial structures, which propels the necessity for an efficient paradigm tailored to 3D generation. Another discrepancy is the amount of training data, which undeniably affects generalization if we only use limited 3D data. To solve these, we design a 3D generation framework that maintains most of the building blocks of StableDiffusion with minimal adaptations for textured shape generation. We design a Sparse Encoding Module for details preservation and an Adversarial Decoding Module for better shape recovery. Moreover, we clean up data and build a benchmark on the biggest 3D dataset (Objaverse). We drop the concept of ‘specific class' and treat the 3D Textured Shapes Generation as an open-vocabulary problem. We first validate our network design on ShapeNetV2 with 55K samples on single-class unconditional generation and multi-class conditional generation tasks. Then we report metrics on processed G-Objaverse with 200K samples on the image conditional generation task. Extensive experiments demonstrate our proposal outperforms SOTA methods and takes a further step towards open-vocabulary 3D generation. We release the processed data at https://aigc3d.github.io/gobjaverse/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Zuo*",
      "Xiaodong Gu",
      "Yuan Dong",
      "Zhengyi Zhao",
      "Weihao Yuan",
      "Qiu Lingteng",
      "Liefeng Bo",
      "Zilong Dong"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1500_ECCV_2024_paper.php": {
    "title": "Semi-Supervised Video Desnowing Network via Temporal Decoupling Experts and Distribution-Driven Contrastive Regularization",
    "volume": "main",
    "abstract": "Snow degradations present formidable challenges to the advancement of computer vision tasks by the undesirable corruption in outdoor scenarios. While current deep learning-based desnowing approaches achieve success on synthetic benchmark datasets, they struggle to restore out-of-distribution real-world snowy videos due to the deficiency of paired real-world training data. To address this bottleneck, we devise a new paradigm for video desnowing in a semi-supervised spirit to involve unlabeled real data for the generalizable snow removal. Specifically, we construct a real-world dataset with 85 snowy videos, and then present a Semi-supervised Video Desnowing Network (SemiVDN) equipped by a novel Distribution-driven Contrastive Regularization. The elaborated contrastive regularization mitigates the distribution gap between the synthetic and real data, and consequently maintains the desired snow-invariant background details. Furthermore, based on the atmospheric scattering model, we introduce a Prior-guided Temporal Decoupling Experts module to decompose the physical components that make up a snowy video in a frame-correlated manner. We evaluate our SemiVDN on benchmark datasets and the collected real snowy data. The experimental results demonstrate the superiority of our approach against state-of-the-art imageand video-level desnowing methods. Our code and the dataset are available at https://github.com/TonyHongtaoWu/SemiVDN",
    "checked": true,
    "id": "644c9a9951b4541dd1c7e9895e2fd6eb60649bac",
    "semantic_title": "semi-supervised video desnowing network via temporal decoupling experts and distribution-driven contrastive regularization",
    "citation_count": 0,
    "authors": [
      "Hongtao Wu",
      "Angelica I Aviles-Rivero",
      "Yijun Yang",
      "Jingjing Ren",
      "Sixiang Chen",
      "Haoyu Chen",
      "Lei Zhu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1503_ECCV_2024_paper.php": {
    "title": "I-MedSAM: Implicit Medical Image Segmentation with Segment Anything",
    "volume": "main",
    "abstract": "With the development of Deep Neural Networks (DNNs), many efforts have been made to handle medical image segmentation. Traditional methods such as nnUNet train specific segmentation models on the individual datasets. Plenty of recent methods have been proposed to adapt the foundational Segment Anything Model (SAM) to medical image segmentation. However, they still focus on discrete representations to generate pixel-wise predictions, which are spatially inflexible and scale poorly to higher resolution. In contrast, implicit methods learn continuous representations for segmentation, which is crucial for medical image segmentation. In this paper, we propose I-MedSAM, which leverages the benefits of both continuous representations and SAM, to obtain better cross-domain ability and accurate boundary delineation. Since medical image segmentation needs to predict detailed segmentation boundaries, we designed a novel adapter to enhance the SAM features with high-frequency information during Parameter-Efficient Fine-Tuning (PEFT). To convert the SAM features and coordinates into continuous segmentation output, we utilize Implicit Neural Representation (INR) to learn an implicit segmentation decoder. We also propose an uncertainty-guided sampling strategy for efficient learning of INR. Extensive evaluations on 2D medical image segmentation tasks have shown that our proposed method with only 1.6M trainable parameters outperforms existing methods including discrete and implicit methods. The code will be available at: https://github.com/ucwxb/I-MedSAM",
    "checked": true,
    "id": "03262d71228e57d58939f2ecea85f462fe459a73",
    "semantic_title": "i-medsam: implicit medical image segmentation with segment anything",
    "citation_count": 5,
    "authors": [
      "Xiaobao Wei",
      "Jiajun Cao",
      "Yizhu Jin",
      "Ming Lu",
      "Guangyu Wang",
      "Shanghang Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1522_ECCV_2024_paper.php": {
    "title": "ReMamber: Referring Image Segmentation with Mamba Twister",
    "volume": "main",
    "abstract": "Referring Image Segmentation (RIS) leveraging transformers has achieved great success on the interpretation of complex visual-language tasks. However, the quadratic computation cost makes it resource-consuming in capturing long-range visual-language dependencies. Fortunately, Mamba addresses this with efficient linear complexity in processing. However, directly applying Mamba to multi-modal interactions presents challenges, primarily due to inadequate channel interactions for the effective fusion of multi-modal data. In this paper, we propose , a novel RIS architecture that integrates the power of Mamba with a multi-modal Mamba Twister block. The Mamba Twister explicitly models image-text interaction, and fuses textual and visual features through its unique channel and spatial twisting mechanism. We achieve competitive results on three challenging benchmarks with a simple and efficient architecture. Moreover, we conduct thorough analyses of and discuss other fusion designs using Mamba. These provide valuable perspectives for future research. The code has been released at: https:// github.com/yyh-rain-song/ReMamber",
    "checked": true,
    "id": "9bd60a0b1b5e70c9e6ccfde513f8fdea61d8b503",
    "semantic_title": "remamber: referring image segmentation with mamba twister",
    "citation_count": 14,
    "authors": [
      "Yuhuan Yang",
      "Chaofan Ma",
      "Jiangchao Yao",
      "Zhun Zhong*",
      "Ya Zhang",
      "Yanfeng Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1539_ECCV_2024_paper.php": {
    "title": "TalkingGaussian: Structure-Persistent 3D Talking Head Synthesis via Gaussian Splatting",
    "volume": "main",
    "abstract": "Radiance fields have demonstrated impressive performance in synthesizing lifelike 3D talking heads. However, due to the difficulty in fitting steep appearance changes, the prevailing paradigm that presents facial motions by directly modifying point appearance may lead to distortions in dynamic regions. To tackle this challenge, we introduce TalkingGaussian, a deformation-based radiance fields framework for high-fidelity talking head synthesis. Leveraging the point-based Gaussian Splatting, facial motions can be represented in our method by applying smooth and continuous deformations to persistent Gaussian primitives, without requiring to learn the difficult appearance change like previous methods. Due to this simplification, precise facial motions can be synthesized while keeping a highly intact facial feature. Under such a deformation paradigm, we further identify a face-mouth motion inconsistency that would affect the learning of detailed speaking motions. To address this conflict, we decompose the model into two branches separately for the face and inside mouth areas, therefore simplifying the learning tasks to help reconstruct more accurate motion and structure of the mouth region. Extensive experiments demonstrate that our method renders high-quality lip-synchronized talking head videos, with better facial fidelity and higher efficiency compared with previous methods. Code is available at: https://github.com/Fictionarry/TalkingGaussian",
    "checked": true,
    "id": "17a7117f0f24dead58eee434da920edefa94c000",
    "semantic_title": "talkinggaussian: structure-persistent 3d talking head synthesis via gaussian splatting",
    "citation_count": 4,
    "authors": [
      "Jiahe Li",
      "Jiawei Zhang",
      "Xiao Bai*",
      "Jin Zheng*",
      "Xin Ning",
      "Jun Zhou",
      "Lin Gu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1541_ECCV_2024_paper.php": {
    "title": "CAT: Enhancing Multimodal Large Language Model to Answer Questions in Dynamic Audio-Visual Scenarios",
    "volume": "main",
    "abstract": "This paper focuses on the challenge of answering questions in scenarios that are composed of rich and complex dynamic audio-visual components. Although existing Multimodal Large Language Models (MLLMs) can respond to audio-visual content, these responses are sometimes ambiguous and fail to describe specific audio-visual events. To overcome this limitation, we introduce the CAT, which enhances MLLM in three ways: 1) besides straightforwardly bridging audio and video, we design a clue aggregator that aggregates question-related clues in dynamic audio-visual scenarios to enrich the detailed knowledge required for large language models. 2) CAT is trained on a mixed multimodal dataset, allowing direct application in audio-visual scenarios. Notably, we collect an audio-visual joint instruction dataset named AVinstruct, to further enhance the capacity of CAT to model cross-semantic correlations. 3) we propose AI-assisted ambiguity-aware direct preference optimization, a strategy specialized in retraining the model to favor the non-ambiguity response and improve the ability to localize specific audio-visual objects. Extensive experimental results demonstrate that CAT outperforms existing methods on multimodal tasks, especially in Audio-Visual Question Answering (AVQA) tasks. The codes and the collected instructions will be released soon",
    "checked": true,
    "id": "eb6b054789ff8c9edf7c1d50667be5bdd95e019b",
    "semantic_title": "cat: enhancing multimodal large language model to answer questions in dynamic audio-visual scenarios",
    "citation_count": 6,
    "authors": [
      "Qilang Ye",
      "Zitong Yu*",
      "Rui Shao",
      "Xinyu Xie",
      "Philip Torr",
      "Xiaochun Cao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1546_ECCV_2024_paper.php": {
    "title": "Segmentation-guided Layer-wise Image Vectorization with Gradient Fills",
    "volume": "main",
    "abstract": "The widespread use of vector graphics creates a significant demand for vectorization methods. While recent learning-based techniques have shown their capability to create vector images of clear topology, filling these primitives with gradients remains a challenge. In this paper, we propose a segmentation-guided vectorization framework to convert raster images into concise vector graphics with radial gradient fills. With the guidance of an embedded gradient-aware segmentation subroutine, our approach progressively appends gradient-filled Bézier paths to the output, where primitive parameters are initiated with our newly designed initialization technique and are optimized to minimize our novel loss function. We build our method on a differentiable renderer with traditional segmentation algorithms to develop it as a model-free tool for raster-to-vector conversion. It is tested on various inputs to demonstrate its feasibility, independent of datasets, to synthesize vector graphics with improved visual quality and layer-wise topology compared to prior work",
    "checked": true,
    "id": "4dca1992bd5d8ce1228b578d7d58bfb2391112d6",
    "semantic_title": "segmentation-guided layer-wise image vectorization with gradient fills",
    "citation_count": 0,
    "authors": [
      "Hengyu Zhou",
      "Hui Zhang*",
      "Bin Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1549_ECCV_2024_paper.php": {
    "title": "Implicit Style-Content Separation using B-LoRA",
    "volume": "main",
    "abstract": "Image stylization involves manipulating the visual appearance and texture (style) of an image while preserving its underlying objects, structures, and concepts (content). The separation of style and content is essential for manipulating the image's style independently from its content, ensuring a harmonious and visually pleasing result. Achieving this separation requires a deep understanding of both the visual and semantic characteristics of images, often necessitating the training of specialized models or employing heavy optimization. In this paper, we introduce B-LoRA, a method that leverages LoRA (Low-Rank Adaptation) to implicitly separate the style and content components of a single image, facilitating various image stylization tasks. By analyzing the architecture of SDXL combined with LoRA, we find that jointly learning the LoRA weights of two specific blocks (referred to as B-LoRAs) achieves style-content separation that cannot be achieved by training each B-LoRA independently. Consolidating the training into only two blocks and separating style and content allows for significantly improving style manipulation and overcoming overfitting issues often associated with model fine-tuning. Once trained, the two B-LoRAs can be used as independent components to allow various image stylization tasks, including image style transfer, text-based image stylization, consistent style generation, and style-content mixing",
    "checked": true,
    "id": "e04d2413139ac947ce9ebe44510424964206f6ad",
    "semantic_title": "implicit style-content separation using b-lora",
    "citation_count": 9,
    "authors": [
      "Yarden Frenkel*",
      "Yael Vinker",
      "Ariel Shamir",
      "Danny Cohen-Or"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1552_ECCV_2024_paper.php": {
    "title": "OpenPSG: Open-set Panoptic Scene Graph Generation via Large Multimodal Models",
    "volume": "main",
    "abstract": "Panoptic Scene Graph Generation (PSG) aims to segment objects and recognize their relations, enabling the structured understanding of an image. Previous methods focus on predicting predefined object and relation categories, hence limiting their applications in the open world scenarios. With the rapid development of large multimodal models (LMMs), significant progress has been made in open-set object detection and segmentation, yet open-set relation prediction in PSG remains unexplored. In this paper, we focus on the task of open-set relation prediction integrated with a pretrained open-set panoptic segmentation model to achieve true open-set panoptic scene graph generation (OpenPSG). Our OpenPSG leverages LMMs to achieve open-set relation prediction in an autoregressive manner. We introduce a relation query transformer to efficiently extract visual features of object pairs and estimate the existence of relations between them. The latter can enhance the prediction efficiency by filtering irrelevant pairs. Finally, we design the generation and judgement instructions to perform open-set relation prediction in PSG autoregressively. To our knowledge, we are the first to propose the open-set PSG task. Extensive experiments demonstrate that our method achieves state-of-the-art performance in open-set relation prediction and panoptic scene graph generation. Code is available at https://github.com/franciszzj/OpenPSG",
    "checked": true,
    "id": "2e3912b56db90b2df913b58e3b78c0679af3b35f",
    "semantic_title": "openpsg: open-set panoptic scene graph generation via large multimodal models",
    "citation_count": 0,
    "authors": [
      "Zijian Zhou*",
      "Zheng Zhu",
      "Holger Caesar",
      "Miaojing Shi*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1553_ECCV_2024_paper.php": {
    "title": "ActionVOS: Actions as Prompts for Video Object Segmentation",
    "volume": "main",
    "abstract": "Delving into the realm of egocentric vision, the advancement of referring video object segmentation (RVOS) stands as pivotal in understanding human activities. However, existing RVOS task primarily relies on static attributes such as object names to segment target objects, posing challenges in distinguishing target objects from background objects and in identifying objects undergoing state changes. To address these problems, this work proposes a novel action-aware RVOS setting called , aiming at segmenting only active objects in egocentric videos using human actions as a key language prompt. This is because human actions precisely describe the behavior of humans, thereby helping to identify the objects truly involved in the interaction and to understand possible state changes. We also build a method tailored to work under this specific setting. Specifically, we develop an action-aware labeling module with an efficient action-guided focal loss. Such designs enable ActionVOS model to prioritize active objects with existing readily-available annotations. Experimental results on the VISOR dataset reveal that significantly reduces the mis-segmentation of inactive objects, confirming that actions help the model understand objects' involvement. Further evaluations on VOST and VSCOS datasets show that the novel ActionVOS setting enhances segmentation performance when encountering challenging circumstances involving object state changes. We will make our implementation available at https://github.com/ut-vision/ActionVOS",
    "checked": true,
    "id": "073074655acca7c5e3a63f35f02a777830eb2b6e",
    "semantic_title": "actionvos: actions as prompts for video object segmentation",
    "citation_count": 0,
    "authors": [
      "Liangyang Ouyang*",
      "Ruicong Liu",
      "Yifei Huang*",
      "Ryosuke Furuta",
      "Yoichi Sato*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1558_ECCV_2024_paper.php": {
    "title": "FALIP: Visual Prompt as Foveal Attention Boosts CLIP Zero-Shot Performance",
    "volume": "main",
    "abstract": "CLIP has achieved impressive zero-shot performance after pretraining on a large-scale dataset consisting of paired image-text data. Previous works have utilized CLIP by incorporating manually designed visual prompts like colored circles and blur masks into the images to guide the model's attention, showing enhanced zero-shot performance in downstream tasks. Although these methods have achieved promising results, they inevitably alter the original information of the images, which can lead to failure in specific tasks. We propose a train-free method Foveal-Attention CLIP (FALIP), which adjusts the CLIP's attention by inserting foveal attention masks into the multi-head self-attention module. We demonstrate FALIP effectively boosts CLIP zero-shot performance in tasks such as referring expressions comprehension, image classification, and 3D point cloud recognition. Experimental results further show that FALIP outperforms existing methods on most metrics and can augment current methods to enhance their performance. Our project page is link to https://pumpkin805.github.io/FALIP/",
    "checked": true,
    "id": "588a832668175bd71cff327089b091dd45082b7c",
    "semantic_title": "falip: visual prompt as foveal attention boosts clip zero-shot performance",
    "citation_count": 0,
    "authors": [
      "Jiedong Zhuang",
      "Jiaqi Hu",
      "Lianrui Mu",
      "Rui Hu",
      "Xiaoyu Liang",
      "Jiangnan Ye",
      "Haoji Hu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1566_ECCV_2024_paper.php": {
    "title": "U-COPE: Taking a Further Step to Universal 9D Category-level Object Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "li zhang*",
      "Weiqing Meng",
      "Yan Zhong",
      "Bin Kong",
      "Mingliang Xu",
      "Jianming Du",
      "Xue Wang",
      "Rujing Wang",
      "Liu Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1578_ECCV_2024_paper.php": {
    "title": "Integrating Markov Blanket Discovery into Causal Representation Learning for Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Naiyu Yin*",
      "Hanjing Wang",
      "Yue Yu",
      "Tian Gao",
      "Amit Dhurandhar",
      "Qiang Ji"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1584_ECCV_2024_paper.php": {
    "title": "Rotary Position Embedding for Vision Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Byeongho Heo*",
      "Song Park",
      "Dongyoon Han",
      "Sangdoo Yun"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1595_ECCV_2024_paper.php": {
    "title": "Local All-Pair Correspondence for Point Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seokju Cho",
      "Jiahui Huang",
      "Jisu Nam",
      "Honggyu An",
      "Seungryong Kim*",
      "Joon-Young Lee*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1600_ECCV_2024_paper.php": {
    "title": "MonoWAD: Weather-Adaptive Diffusion Model for Robust Monocular 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youngmin Oh",
      "Hyung-Il Kim",
      "Seong Tae Kim*",
      "Jung Uk Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1610_ECCV_2024_paper.php": {
    "title": "ReALFRED: An Embodied Instruction Following Benchmark in Photo-Realistic Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taewoong Kim",
      "Cheolhong Min",
      "Byeonghwi Kim",
      "Jinyeon Kim",
      "Wonje Jeung",
      "Jonghyun Choi*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1618_ECCV_2024_paper.php": {
    "title": "S^3D-NeRF: Single-Shot Speech-Driven Neural Radiance Field for High Fidelity Talking Head Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongze Li*",
      "Kang Zhao*",
      "Wei Wang*",
      "Yifeng Ma",
      "Bo Peng",
      "Yingya Zhang",
      "Jing Dong"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1621_ECCV_2024_paper.php": {
    "title": "ActionSwitch: Class-agnostic Detection of Simultaneous Actions in Streaming Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyolim Kang",
      "Jeongseok Hyun",
      "Joungbin An",
      "Youngjae Yu",
      "Seon Joo Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1622_ECCV_2024_paper.php": {
    "title": "Hierarchically Structured Neural Bones for Reconstructing Animatable Objects from Casual Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Subin Jeon",
      "In Cho",
      "Minsu Kim",
      "Woong Oh Cho",
      "Seon Joo Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1627_ECCV_2024_paper.php": {
    "title": "PQ-SAM: Post-training Quantization for Segment Anything Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyu Liu*",
      "Xin Ding",
      "Lei Yu",
      "Yuanyuan Xi",
      "Wei Li",
      "Zhijun Tu",
      "jie hu",
      "Hanting Chen",
      "Baoqun YIN",
      "Zhiwei Xiong*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1634_ECCV_2024_paper.php": {
    "title": "CPM: Class-conditional Prompting Machine for Audio-visual Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanhong Chen*",
      "Chong Wang",
      "Yuyuan Liu",
      "Hu Wang",
      "Gustavo Carneiro"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1635_ECCV_2024_paper.php": {
    "title": "Optimizing Factorized Encoder Models: Time and Memory Reduction for Scalable and Efficient Action Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shreyank N Gowda*",
      "Anurag Arnab",
      "Jonathan Huang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1652_ECCV_2024_paper.php": {
    "title": "DVLO: Deep Visual-LiDAR Odometry with Local-to-Global Feature Fusion and Bi-Directional Structure Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiuming Liu",
      "Dong Zhuo",
      "Zhiheng Feng",
      "Siting Zhu",
      "Chensheng Peng",
      "Zhe Liu",
      "Hesheng Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1653_ECCV_2024_paper.php": {
    "title": "CoLeaF: A Contrastive-Collaborative Learning Framework for Weakly Supervised Audio-Visual Video Parsing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Faegheh Sardari*",
      "Armin Mustafa",
      "Philip JB Jackson",
      "Adrian Hilton"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1688_ECCV_2024_paper.php": {
    "title": "Noise-assisted Prompt Learning for Image Forgery Detection and Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dong Li",
      "Jiaying Zhu",
      "Xueyang Fu*",
      "Xun Guo",
      "Yidi Liu",
      "Gang Yang",
      "Jiawei Liu",
      "Zheng-Jun Zha"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1689_ECCV_2024_paper.php": {
    "title": "Data Collection-free Masked Video Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchi Ishikawa*",
      "Masayoshi Kondo",
      "Yoshimitsu Aoki"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1691_ECCV_2024_paper.php": {
    "title": "Protecting NeRFs' Copyright via Plug-And-Play Watermarking Base Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Song*",
      "Ziyuan Luo",
      "Ka Chun Cheung",
      "Simon See",
      "Renjie Wan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1705_ECCV_2024_paper.php": {
    "title": "Pixel-Aware Stable Diffusion for Realistic Image Super-Resolution and Personalized Stylization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Yang*",
      "Rongyuan Wu",
      "Peiran Ren",
      "Xuansong Xie",
      "Lei Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1706_ECCV_2024_paper.php": {
    "title": "AnyControl: Create Your Artwork with Versatile Control on Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanan Sun*",
      "Yanchen Liu",
      "Yinhao Tang",
      "Wenjie Pei",
      "Kai Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1713_ECCV_2024_paper.php": {
    "title": "SEED: A Simple and Effective 3D DETR in Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhe Liu",
      "Jinghua Hou",
      "Xiaoqing Ye",
      "Tong Wang",
      "Jingdong Wang",
      "Xiang Bai*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1714_ECCV_2024_paper.php": {
    "title": "AEDNet: Adaptive Embedding and Multiview-Aware Disentanglement for Point Cloud Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiheng Fu",
      "Longguang Wang",
      "Lian Xu",
      "Zhiyong Wang",
      "Hamid Laga",
      "Yulan Guo*",
      "Farid Boussaid",
      "Mohammed Bennamoun"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1721_ECCV_2024_paper.php": {
    "title": "Synergy of Sight and Semantics: Visual Intention Understanding with CLIP",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qu Yang",
      "Mang Ye*",
      "Dacheng Tao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1723_ECCV_2024_paper.php": {
    "title": "Intrinsic Single-Image HDR Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Dille*",
      "Chris Careaga*",
      "Yagiz Aksoy"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1724_ECCV_2024_paper.php": {
    "title": "T-MAE: Temporal Masked Autoencoders for Point Cloud Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weijie Wei*",
      "Fatemeh Karimi Nejadasl",
      "Theo Gevers",
      "Martin R. Oswald*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1736_ECCV_2024_paper.php": {
    "title": "Pathology-knowledge Enhanced Multi-instance Prompt Learning for Few-shot Whole Slide Image Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linhao Qu*",
      "Dingkang Yang",
      "Dan Huang",
      "Qinhao Guo",
      "rongkui luo",
      "Shaoting Zhang",
      "Xiaosong Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1738_ECCV_2024_paper.php": {
    "title": "Towards Natural Language-Guided Drones: GeoText-1652 Benchmark with Spatial Relation Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meng Chu",
      "Zhedong Zheng*",
      "Wei Ji",
      "Tingyu Wang",
      "Tat-Seng Chua"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1750_ECCV_2024_paper.php": {
    "title": "BEAF: Observing BEfore-AFter Changes to Evaluate Hallucination in Vision-language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Moon Ye-Bin",
      "Nam Hyeon-Woo",
      "Wonseok Choi",
      "Tae-Hyun Oh*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1751_ECCV_2024_paper.php": {
    "title": "Approaching Outside: Scaling Unsupervised 3D Object Detection from 2D Scene",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruiyang Zhang*",
      "Hu Zhang",
      "Hang Yu",
      "Zhedong Zheng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1752_ECCV_2024_paper.php": {
    "title": "DATENeRF: Depth-Aware Text-based Editing of NeRFs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sara Rojas Martinez*",
      "Julien Philip",
      "Kai Zhang",
      "Sai Bi",
      "Fujun Luan",
      "Bernard Ghanem",
      "Kalyan Sunkavalli"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1755_ECCV_2024_paper.php": {
    "title": "XPSR: Cross-modal Priors for Diffusion-based Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qu Yunpeng*",
      "Kun Yuan",
      "Kai Zhao",
      "Qizhi Xie",
      "Jinhua Hao",
      "Ming Sun",
      "Chao Zhou"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1761_ECCV_2024_paper.php": {
    "title": "ABC Easy as 123: A Blind Counter for Exemplar-Free Multi-Class Class-agnostic Counting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael A Hobley*",
      "Victor Adrian Prisacariu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1766_ECCV_2024_paper.php": {
    "title": "Category Adaptation Meets Projected Distillation in Generalized Continual Category Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Grzegorz Rypeść*",
      "Daniel Marczak",
      "Sebastian Cygert",
      "Tomasz Trzcinski",
      "Bartlomiej Twardowski"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1769_ECCV_2024_paper.php": {
    "title": "LaRa: Efficient Large-Baseline Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anpei Chen*",
      "Haofei Xu",
      "Stefano Esposito",
      "Siyu Tang",
      "Andreas Geiger"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1774_ECCV_2024_paper.php": {
    "title": "Bi-TTA: Bidirectional Test-Time Adapter for Remote Physiological Measurement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haodong LI*",
      "Hao LU",
      "Yingcong Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1775_ECCV_2024_paper.php": {
    "title": "MAGR: Manifold-Aligned Graph Regularization for Continual Action Quality Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kanglei Zhou",
      "Liyuan Wang",
      "Xingxing Zhang",
      "Hubert P. H. Shum",
      "Frederick W. B. Li",
      "Jianguo Li",
      "Xiaohui Liang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1783_ECCV_2024_paper.php": {
    "title": "Grounding Language Models for Visual Entity Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zilin Xiao*",
      "Ming Gong",
      "Paola Cascante-Bonilla",
      "Xingyao Zhang",
      "Jie Wu",
      "Vicente Ordonez*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1791_ECCV_2024_paper.php": {
    "title": "ELSE: Efficient Deep Neural Network Inference through Line-based Sparsity Exploration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeqi Zhu*",
      "Alberto Garcia-Ortiz",
      "Luc Waeijen",
      "Egor Bondarev",
      "Arash Pourtaherian",
      "Orlando Moreira"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1793_ECCV_2024_paper.php": {
    "title": "DiffusionDepth: Diffusion Denoising Approach for Monocular Depth Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiqun Duan*",
      "Xianda Guo*",
      "Zheng Zhu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1795_ECCV_2024_paper.php": {
    "title": "DC-Solver: Improving Predictor-Corrector Diffusion Sampler via Dynamic Compensation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenliang Zhao",
      "Haolin Wang",
      "Jie Zhou",
      "Jiwen Lu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1796_ECCV_2024_paper.php": {
    "title": "TRAM: Global Trajectory and Motion of 3D Humans from in-the-wild Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufu Wang*",
      "Ziyun Wang",
      "Lingjie Liu",
      "Kostas Daniilidis"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1812_ECCV_2024_paper.php": {
    "title": "MutDet: Mutually Optimizing Pre-training for Remote Sensing Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyue Huang",
      "Yongchao Feng",
      "Qingjie Liu*",
      "Yunhong Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1818_ECCV_2024_paper.php": {
    "title": "Self-Supervised Video Copy Localization with Regional Token Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minlong Lu*",
      "Yichen Lu",
      "Siwei Nie",
      "Xudong Yang",
      "Xiaobo Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1824_ECCV_2024_paper.php": {
    "title": "Enhancing Perceptual Quality in Video Super-Resolution through Temporally-Consistent Detail Synthesis using Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Claudio Rota*",
      "Marco Buzzelli",
      "Joost van de Weijer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1833_ECCV_2024_paper.php": {
    "title": "RoGUENeRF: A Robust Geometry-Consistent Universal Enhancer for NeRF",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sibi Catley-Chandar*",
      "Richard Shaw",
      "Gregory Slabaugh",
      "Eduardo Pérez Pellitero"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1834_ECCV_2024_paper.php": {
    "title": "Bridging the Gap: Studio-like Avatar Creation from a Monocular Phone Capture",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "ShahRukh Athar*",
      "Shunsuke Saito",
      "Stanislav Pidhorskyi",
      "Zhengyu Yang",
      "Chen Cao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1838_ECCV_2024_paper.php": {
    "title": "ControlLLM: Augment Language Models with Tools by Searching on Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoyang Liu",
      "Zeqiang Lai",
      "Zhangwei Gao",
      "erfei cui",
      "Ziheng Li",
      "Xizhou Zhu",
      "Lewei Lu",
      "Qifeng Chen*",
      "Yu Qiao",
      "Jifeng Dai",
      "Wenhai Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1843_ECCV_2024_paper.php": {
    "title": "UniTraj: A Unified Framework for Scalable Vehicle Trajectory Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lan Feng",
      "Mohammadhossein Bahari*",
      "Kaouther Messaoud",
      "Eloi Zablocki",
      "Matthieu Cord",
      "Alexandre Alahi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1847_ECCV_2024_paper.php": {
    "title": "DreamDissector: Learning Disentangled Text-to-3D Generation from 2D Diffusion Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zizheng Yan*",
      "Jiapeng Zhou",
      "Fanpeng Meng",
      "Yushuang Wu",
      "Lingteng Qiu",
      "Zisheng Ye",
      "Shuguang Cui",
      "Guanying CHEN",
      "Xiaoguang Han*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1860_ECCV_2024_paper.php": {
    "title": "Vamos: Versatile Action Models for Video Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shijie Wang*",
      "Qi Zhao",
      "Minh Quan Do",
      "Nakul Agarwal",
      "Kwonjoon Lee",
      "Chen Sun"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1872_ECCV_2024_paper.php": {
    "title": "Prioritized Semantic Learning for Zero-shot Instance Navigation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "xinyu sun*",
      "Lizhao Liu",
      "Hongyan Zhi",
      "Ronghe Qiu",
      "Junwei Liang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1878_ECCV_2024_paper.php": {
    "title": "RoadPainter: Points Are Ideal Navigators for Topology transformER",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongxing Ma",
      "Liang Shuang",
      "Yongkun Wen",
      "Weixin Lu",
      "Guowei Wan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1879_ECCV_2024_paper.php": {
    "title": "FouriScale: A Frequency Perspective on Training-Free High-Resolution Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linjiang Huang*",
      "Rongyao Fang",
      "Aiping Zhang",
      "Guanglu Song",
      "Si Liu",
      "Yu Liu",
      "Hongsheng Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1889_ECCV_2024_paper.php": {
    "title": "Can OOD Object Detectors Learn from Foundation Models?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahui Liu*",
      "Xin Wen",
      "Shizhen Zhao",
      "Yingxian Chen",
      "Xiaojuan Qi*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1890_ECCV_2024_paper.php": {
    "title": "Videoshop: Localized Semantic Video Editing with Noise-Extrapolated Diffusion Inversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Fan*",
      "Anand Bhattad",
      "Ranjay Krishna"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1894_ECCV_2024_paper.php": {
    "title": "MERLiN: Single-Shot Material Estimation and Relighting for Photometric Stereo",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashish Tiwari*",
      "Satoshi Ikehata",
      "Shanmuganathan Raman"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1900_ECCV_2024_paper.php": {
    "title": "Boosting 3D Single Object Tracking with 2D Matching Distillation and 3D Pre-training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiangqiang Wu",
      "Yan Xia*",
      "Jia Wan",
      "Antoni Chan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1909_ECCV_2024_paper.php": {
    "title": "Diffusion-Based Image-to-Image Translation by Noise Correction via Prompt Interpolation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junsung Lee",
      "Minsoo Kang",
      "Bohyung Han*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1910_ECCV_2024_paper.php": {
    "title": "Real-data-driven 2000 FPS Color Video from Mosaicked Chromatic Spikes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siqi Yang*",
      "Zhaojun Huang",
      "Yakun Chang",
      "Bin Fan",
      "Zhaofei Yu",
      "Boxin Shi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1918_ECCV_2024_paper.php": {
    "title": "Brain-ID: Learning Contrast-agnostic Anatomical Representations for Brain Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peirong Liu*",
      "Oula Puonti",
      "Xiaoling Hu",
      "Daniel C. Alexander",
      "Juan E. Iglesias"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1921_ECCV_2024_paper.php": {
    "title": "TTT-MIM: Test-Time Training with Masked Image Modeling for Denoising Distribution Shifts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youssef Mansour*",
      "Xuyang Zhong",
      "Serdar Caglar",
      "Reinhard Heckel"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1923_ECCV_2024_paper.php": {
    "title": "RadEdit: stress-testing biomedical vision models via diffusion image editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fernando Pérez-García",
      "Sam Bond-Taylor",
      "Pedro Sanchez",
      "Boris van Breugel",
      "Daniel Coelho de Castro",
      "Harshita Sharma",
      "Valentina Salvatelli",
      "Maria Teodora A Wetscherek",
      "Hannah CM Richardson",
      "Lungren Matthew",
      "Aditya Nori",
      "Javier Alvarez-Valle",
      "Ozan Oktay",
      "Maximilian Ilse*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1927_ECCV_2024_paper.php": {
    "title": "SPAMming Labels: Efficient Annotations for the Trackers of Tomorrow",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Orcun Cetintas*",
      "Tim Meinhardt",
      "Guillem Brasó",
      "Laura Leal-Taixé"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1944_ECCV_2024_paper.php": {
    "title": "AdaDiffSR: Adaptive Region-aware Dynamic acceleration Diffusion Model for Real-World Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanting Fan",
      "Chengxu Liu",
      "Nengzhong Yin",
      "Changlong Gao",
      "Xueming Qian*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1946_ECCV_2024_paper.php": {
    "title": "Explicitly Guided Information Interaction Network for Cross-modal Point Cloud Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xu Hang",
      "Chen Long",
      "Wenxiao Zhang*",
      "Yuan Liu",
      "Zhen Cao",
      "Zhen Dong",
      "Bisheng Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1982_ECCV_2024_paper.php": {
    "title": "Towards Real-world Event-guided Low-light Video Enhancement and Deblurring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taewoo Kim",
      "Jaeseok Jeong",
      "Hoonhee Cho",
      "Yuhwan Jeong",
      "Kuk-Jin Yoon*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1985_ECCV_2024_paper.php": {
    "title": "Exploring Pre-trained Text-to-Video Diffusion Models for Referring Video Object Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuelu Feng",
      "Dongdong Chen",
      "Junsong Yuan",
      "Chunming Qiao",
      "Gang Hua",
      "Zixin Zhu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1990_ECCV_2024_paper.php": {
    "title": "TrackNeRF: Bundle Adjusting NeRF from Sparse and Noisy Views via Feature Tracks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinjie Mai*",
      "Wenxuan Zhu",
      "Sara Rojas",
      "Jesus Zarzar",
      "Abdullah Hamdi",
      "Guocheng Qian",
      "Bing Li",
      "Silvio Giancola",
      "Bernard Ghanem"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2007_ECCV_2024_paper.php": {
    "title": "COHO: Context-Sensitive City-Scale Hierarchical Urban Layout Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liu He*",
      "Daniel Aliaga"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2012_ECCV_2024_paper.php": {
    "title": "Joint RGB-Spectral Decomposition Model Guided Image Enhancement in Mobile Photography",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kailai Zhou*",
      "Lijing Cai",
      "Yibo Wang",
      "Mengya Zhang",
      "Bihan Wen",
      "Qiu Shen*",
      "Xun Cao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2019_ECCV_2024_paper.php": {
    "title": "SpatialFormer: Towards Generalizable Vision Transformers with Explicit Spatial Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Xiao",
      "Wenzhao Zheng",
      "Sicheng Zuo",
      "Peng Gao",
      "Jie Zhou",
      "Jiwen Lu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2024_ECCV_2024_paper.php": {
    "title": "OccWorld: Learning a 3D Occupancy World Model for Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenzhao Zheng",
      "Weiliang Chen",
      "Yuanhui Huang",
      "Borui Zhang",
      "Yueqi Duan",
      "Jiwen Lu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2028_ECCV_2024_paper.php": {
    "title": "MyVLM: Personalizing VLMs for User-Specific Queries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuval Alaluf*",
      "Elad Richardson",
      "Sergey Tulyakov",
      "Kfir Aberman",
      "Danny Cohen-Or"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2032_ECCV_2024_paper.php": {
    "title": "AMEGO: Active Memory from long EGOcentric videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gabriele Goletto*",
      "Tushar Nagarajan",
      "Giuseppe Averta",
      "Dima Damen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2034_ECCV_2024_paper.php": {
    "title": "Power Variable Projection for Initialization-Free Large-Scale Bundle Adjustment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simon Weber*",
      "Je Hyeong Hong",
      "Daniel Cremers"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2035_ECCV_2024_paper.php": {
    "title": "Collaborative Control for Geometry-Conditioned PBR Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shimon Vainer",
      "Mark Boss",
      "Mathias Parger",
      "Konstantin Kutsy",
      "Dante De Nigris",
      "Ciara Rowles",
      "Nicolas Perony",
      "Simon Donné*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2037_ECCV_2024_paper.php": {
    "title": "Co-synthesis of Histopathology Nuclei Image-Label Pairs using a Context-Conditioned Joint Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seonghui Min",
      "Hyun-Jic Oh",
      "Won-Ki Jeong*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2053_ECCV_2024_paper.php": {
    "title": "One-stage Prompt-based Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youngeun Kim*",
      "Yuhang Li",
      "Priyadarshini Panda"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2054_ECCV_2024_paper.php": {
    "title": "SpaceJAM: a Lightweight and Regularization-free Method for Fast Joint Alignment of Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nir Barel*",
      "Ron A Shapira Weber*",
      "Nir Mualem",
      "Shahaf E Finder",
      "Oren Freifeld*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2055_ECCV_2024_paper.php": {
    "title": "APL: Anchor-based Prompt Learning for One-stage Weakly Supervised Referring Expression Comprehension",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaxin Luo",
      "Jiayi Ji",
      "Xiaofu Chen",
      "Yuxin Zhang",
      "Tianhe Ren",
      "Gen Luo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2058_ECCV_2024_paper.php": {
    "title": "GenQ: Quantization in Low Data Regimes with Generative Synthetic Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhang Li*",
      "Youngeun Kim",
      "Donghyun Lee",
      "Souvik Kundu",
      "Priyadarshini Panda"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2081_ECCV_2024_paper.php": {
    "title": "MVDD: Multi-View Depth Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Wang*",
      "Qiangeng Xu",
      "Feitong Tan",
      "Menglei Chai",
      "Shichen Liu",
      "Rohit Pandey",
      "Sean Fanello",
      "Achuta Kadambi",
      "Yinda Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2083_ECCV_2024_paper.php": {
    "title": "Rethinking Video-Text Understanding: Retrieval from Counterfactually Augmented Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wufei Ma*",
      "Kai Li",
      "Zhongshi Jiang",
      "Moustafa Meshry",
      "Qihao Liu",
      "Huiyu Wang",
      "Christian Haene",
      "Alan Yuille"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2087_ECCV_2024_paper.php": {
    "title": "Risk-Aware Self-Consistent Imitation Learning for Trajectory Planning in Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixuan Fan*",
      "Ya-Li Li",
      "Shengjin Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2090_ECCV_2024_paper.php": {
    "title": "Dual-level Adaptive Self-Labeling for Novel Class Discovery in Point Cloud Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruijie Xu*",
      "CHUYU ZHANG",
      "Hui Ren",
      "Xuming He"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2096_ECCV_2024_paper.php": {
    "title": "EBDM: Exemplar-guided Image Translation with Brownian-bridge Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eungbean Lee",
      "Somi Jeong",
      "Kwanghoon Sohn*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2100_ECCV_2024_paper.php": {
    "title": "DreamDrone: Text-to-Image Diffusion Models are Zero-shot Perpetual View Generators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanyang Kong*",
      "Dongze Lian",
      "Michael Bi Mi",
      "Xinchao Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2103_ECCV_2024_paper.php": {
    "title": "Harnessing Text-to-Image Diffusion Models for Category-Agnostic Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Duo Peng",
      "Zhengbo Zhang",
      "Ping Hu",
      "Qiuhong Ke",
      "David Yau",
      "Jun Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2109_ECCV_2024_paper.php": {
    "title": "SC4D: Sparse-Controlled Video-to-4D Generation and Motion Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijie Wu*",
      "Chaohui Yu",
      "Yanqin Jiang",
      "Chenjie Cao",
      "Fan Wang",
      "Xiang Bai*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2121_ECCV_2024_paper.php": {
    "title": "Overcoming Distribution Mismatch in Quantizing Image Super-Resolution Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheeun Hong",
      "Kyoung Mu Lee*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2125_ECCV_2024_paper.php": {
    "title": "Large Motion Model for Unified Multi-Modal Motion Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingyuan Zhang*",
      "Daisheng Jin",
      "Chenyang Gu",
      "Fangzhou Hong",
      "Zhongang Cai",
      "Jingfang Huang",
      "Chongzhi Zhang",
      "Xinying Guo",
      "Lei Yang",
      "Ying He",
      "Ziwei Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2130_ECCV_2024_paper.php": {
    "title": "FisherRF: Active View Selection and Mapping with Radiance Fields using Fisher Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wen Jiang*",
      "BOSHU LEI",
      "Kostas Daniilidis*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2132_ECCV_2024_paper.php": {
    "title": "Occlusion Handling in 3D Human Pose Estimation with Perturbed Positional Encoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Niloofar Azizi*",
      "Mohsen Fayyaz",
      "Horst Bischof"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2138_ECCV_2024_paper.php": {
    "title": "Gradient-based Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taha Entesari*",
      "Sina Sharifi*",
      "Bardia Safaei*",
      "Vishal Patel",
      "Mahyar Fazlyab"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2142_ECCV_2024_paper.php": {
    "title": "Event-based Mosaicing Bundle Adjustment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuang Guo*",
      "Guillermo Gallego"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2146_ECCV_2024_paper.php": {
    "title": "ProMerge: Prompt and Merge for Unsupervised Instance Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dylan J Li",
      "Gyungin Shin*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2148_ECCV_2024_paper.php": {
    "title": "M2D2M: Multi-Motion Generation from Text with Discrete Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seunggeun Chi*",
      "Hyung-gun Chi",
      "Hengbo Ma",
      "Nakul Agarwal",
      "Faizan Siddiqui",
      "Karthik Ramani*",
      "Kwonjoon Lee*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2149_ECCV_2024_paper.php": {
    "title": "The Hard Positive Truth about Vision-Language Compositionality",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amita Kamath*",
      "Cheng-Yu Hsieh",
      "Kai-Wei Chang",
      "Ranjay Krishna"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2153_ECCV_2024_paper.php": {
    "title": "GaussCtrl: Multi-View Consistent Text-Driven 3D Gaussian Splatting Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Wu*",
      "Jia-Wang Bian",
      "Xinghui Li",
      "Guangrun Wang",
      "Ian Reid",
      "Philip Torr",
      "Victor Adrian Prisacariu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2155_ECCV_2024_paper.php": {
    "title": "Shapefusion: 3D localized human diffusion models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rolandos Alexandros Potamias*",
      "Michael Tarasiou",
      "Stylianos Ploumpis",
      "Stefanos Zafeiriou"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2157_ECCV_2024_paper.php": {
    "title": "Eta Inversion: Designing an Optimal Eta Function for Diffusion-based Real Image Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wonjun Kang",
      "Kevin Galim",
      "Hyung Il Koo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2160_ECCV_2024_paper.php": {
    "title": "Prompting Language-Informed Distribution for Compositional Zero-Shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wentao Bao*",
      "Lichang Chen",
      "Heng Huang",
      "Yu Kong"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2167_ECCV_2024_paper.php": {
    "title": "Wear-Any-Way: Manipulable Virtual Try-on via Sparse Correspondence Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengting Chen*",
      "Xi Chen",
      "Zhonghua Zhai",
      "Chen Ju",
      "Xuewen Hong",
      "Jinsong Lan",
      "Shuai Xiao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2170_ECCV_2024_paper.php": {
    "title": "3iGS: Factorised Tensorial Illumination for 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhe Jun Tang*",
      "Tat-Jen Cham"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2177_ECCV_2024_paper.php": {
    "title": "Distribution-Aware Robust Learning from Long-Tailed Data with Noisy Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jae Soon Baik*",
      "In Young Yoon",
      "Kun Hoon Kim",
      "Jun Won Choi*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2178_ECCV_2024_paper.php": {
    "title": "Free-Viewpoint Video of Outdoor Sports Using a Drone",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengdong Hong*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2182_ECCV_2024_paper.php": {
    "title": "Wavelength-Embedding-guided Filter-Array Transformer for Spectral Demosaicing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haijin Zeng*",
      "Hiep Luong",
      "Wilfried Philips"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2187_ECCV_2024_paper.php": {
    "title": "ConGeo: Robust Cross-view Geo-localization across Ground View Variations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li Mi",
      "Chang Xu*",
      "Javiera Castillo Navarro",
      "SYRIELLE MONTARIOL",
      "Wen Yang",
      "Antoine Bosselut",
      "Devis Tuia"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2196_ECCV_2024_paper.php": {
    "title": "Generalizable Facial Expression Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhang Zhang",
      "Xiuqi Zheng",
      "Chenyi Liang",
      "Jiani Hu*",
      "Weihong Deng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2197_ECCV_2024_paper.php": {
    "title": "GAURA: Generalizable Approach for Unified Restoration and Rendering of Arbitrary Views",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vinayak Gupta*",
      "Rongali Simhachala Venkata Girish",
      "Mukund Varma T",
      "Ayush Tewari",
      "Kaushik Mitra"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2209_ECCV_2024_paper.php": {
    "title": "Self-Supervised Any-Point Tracking by Contrastive Random Walks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ayush Shrivastava*",
      "Andrew Owens"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2212_ECCV_2024_paper.php": {
    "title": "MixDQ: Memory-Efficient Few-Step Text-to-Image Diffusion Models with Metric-Decoupled Mixed Precision Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianchen Zhao*",
      "Xuefei Ning",
      "Tongcheng Fang",
      "Enshu Liu",
      "Guyue Huang",
      "Zinan Lin",
      "Shengen Yan",
      "Guohao Dai",
      "Yu Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2220_ECCV_2024_paper.php": {
    "title": "Siamese Vision Transformers are Scalable Audio-visual Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan-Bo Lin*",
      "Gedas Bertasius"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2228_ECCV_2024_paper.php": {
    "title": "LCM-Lookahead for Encoder-based Text-to-Image Personalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rinon Gal*",
      "Or Lichter",
      "Elad Richardson",
      "Or Patashnik",
      "Amit Bermano",
      "Gal Chechik",
      "Danny Cohen-Or"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2230_ECCV_2024_paper.php": {
    "title": "Towards Architecture-Agnostic Untrained Networks Priors for Image Reconstruction with Frequency Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yilin Liu",
      "Yunkui Pang",
      "Jiang Li",
      "Yong Chen",
      "Pew-Thian Yap*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2233_ECCV_2024_paper.php": {
    "title": "Towards Open-Ended Visual Recognition with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qihang Yu*",
      "Xiaohui Shen",
      "Liang-Chieh Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2234_ECCV_2024_paper.php": {
    "title": "Ray-Distance Volume Rendering for Neural Scene Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruihong Yin*",
      "Yunlu Chen",
      "Sezer Karaoglu",
      "Theo Gevers"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2236_ECCV_2024_paper.php": {
    "title": "ReNoise: Real Image Inversion Through Iterative Noising",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Garibi*",
      "Or Patashnik",
      "Andrey Voynov",
      "Hadar Averbuch-Elor",
      "Danny Cohen-Or"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2238_ECCV_2024_paper.php": {
    "title": "Attention Decomposition for Cross-Domain Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liqiang He*",
      "Sinisa Todorovic"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2245_ECCV_2024_paper.php": {
    "title": "Be Yourself: Bounded Attention for Multi-Subject Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Omer Dahary*",
      "Or Patashnik",
      "Kfir Aberman",
      "Danny Cohen-Or"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2249_ECCV_2024_paper.php": {
    "title": "Handling The Non-Smooth Challenge in Tensor SVD: A Multi-Objective Tensor Recovery Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingjing Zheng",
      "Wanglong Lu",
      "Wenzhe Wang",
      "Yankai Cao*",
      "Xiaoqin Zhang",
      "Xianta Jiang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2255_ECCV_2024_paper.php": {
    "title": "RodinHD: High-Fidelity 3D Avatar Generation with Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowen Zhang",
      "Yiji Cheng",
      "Chunyu Wang*",
      "Ting Zhang",
      "Jiaolong Yang",
      "Yansong Tang",
      "Feng Zhao",
      "Dong Chen",
      "Baining Guo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2257_ECCV_2024_paper.php": {
    "title": "GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction and Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinghao Xu*",
      "Zifan Shi",
      "Wang Yifan",
      "Hansheng Chen",
      "Ceyuan Yang",
      "Sida Peng",
      "Yujun Shen",
      "Gordon Wetzstein"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2265_ECCV_2024_paper.php": {
    "title": "IRGen: Generative Modeling for Image Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yidan Zhang*",
      "Ting Zhang*",
      "Dong Chen",
      "Yujing Wang",
      "Qi Chen",
      "Xing Xie",
      "Hao Sun",
      "Weiwei Deng",
      "Qi Zhang",
      "Fan Yang",
      "Mao Yang",
      "Qingmin Liao",
      "Jingdong Wang",
      "Baining Guo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2295_ECCV_2024_paper.php": {
    "title": "Learning Trimodal Relation for Audio-Visual Question Answering with Missing Modality",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kyu Ri Park",
      "Hong Joo Lee*",
      "Jung Uk Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2302_ECCV_2024_paper.php": {
    "title": "FastCAD: Real-Time CAD Retrieval and Alignment from Scans and Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Florian Maximilian Langer*",
      "Jihong Ju",
      "Georgi Dikov",
      "Gerhard Reitmayr",
      "Mohsen Ghafoorian"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2304_ECCV_2024_paper.php": {
    "title": "A Simple Latent Diffusion Approach for Panoptic Segmentation and Mask Inpainting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wouter Van Gansbeke*",
      "Bert De Brabandere"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2307_ECCV_2024_paper.php": {
    "title": "VISA: Reasoning Video Object Segmentation via Large Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cilin Yan",
      "Haochen Wang",
      "Shilin Yan",
      "Xiaolong Jiang",
      "Yao Hu",
      "Guoliang Kang*",
      "Weidi Xie",
      "Efstratios Gavves"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2309_ECCV_2024_paper.php": {
    "title": "Lego: Learning to Disentangle and Invert Personalized Concepts Beyond Object Appearance in Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saman Motamed*",
      "Danda Pani Paudel",
      "Luc Van Gool"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2312_ECCV_2024_paper.php": {
    "title": "IDOL: Unified Dual-Modal Latent Diffusion for Human-Centric Joint Video-Depth Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanhao Zhai*",
      "Kevin Lin",
      "Linjie Li",
      "Chung-Ching Lin",
      "Jianfeng Wang",
      "Zhengyuan Yang",
      "David Doermann",
      "Junsong Yuan",
      "Zicheng Liu",
      "Lijuan Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2315_ECCV_2024_paper.php": {
    "title": "Scaling Backwards: Minimal Synthetic Pre-training?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryo Nakamura*",
      "Ryu Tadokoro*",
      "Ryosuke Yamada*",
      "Yuki M Asano*",
      "Iro Laina*",
      "Christian Rupprecht*",
      "Nakamasa Inoue*",
      "Rio Yokota*",
      "Hirokatsu Kataoka*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2316_ECCV_2024_paper.php": {
    "title": "BAMM: Bidirectional Autoregressive Motion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ekkasit Pinyoanuntapong*",
      "Muhammad Usama Saleem",
      "Pu Wang",
      "Minwoo Lee",
      "Srijan Das",
      "Chen Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2323_ECCV_2024_paper.php": {
    "title": "Event-based Head Pose Estimation: Benchmark and Method",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahui Yuan*",
      "Hebei Li",
      "Yansong Peng",
      "Jin Wang",
      "Yuheng Jiang",
      "Yueyi Zhang*",
      "Xiaoyan Sun"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2330_ECCV_2024_paper.php": {
    "title": "Avatar Fingerprinting for Authorized Use of Synthetic Talking-Head Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ekta Prashnani*",
      "Koki Nagano",
      "Shalini De Mello",
      "David P Luebke",
      "Orazio Gallo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2339_ECCV_2024_paper.php": {
    "title": "Towards Multi-modal Transformers in Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangyu Sun*",
      "Matias Mendieta",
      "Aritra Dutta",
      "Xin Li",
      "Chen Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2353_ECCV_2024_paper.php": {
    "title": "Fisher Calibration for Backdoor-Robust Heterogeneous Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenke Huang",
      "Mang Ye*",
      "zekun shi",
      "Bo Du*",
      "Dacheng Tao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2355_ECCV_2024_paper.php": {
    "title": "QueryCDR: Query-based Controllable Distortion Rectification Network for Fisheye Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengbo Guo",
      "Chengxu Liu",
      "Xingsong Hou*",
      "Xueming Qian"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2356_ECCV_2024_paper.php": {
    "title": "Latent-INR: A Flexible Framework for Implicit Representations of Videos with Discriminative Semantics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shishira R Maiya*",
      "Anubhav Gupta",
      "Matthew A Gwilliam",
      "Max Ehrlich",
      "Abhinav Shrivastava"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2357_ECCV_2024_paper.php": {
    "title": "DCDM: Diffusion-Conditioned-Diffusion Model for Scene Text Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shrey Singh*",
      "Prateek Keserwani",
      "Masakazu Iwamura*",
      "Partha Pratim Roy"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2361_ECCV_2024_paper.php": {
    "title": "Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeongmin Bae",
      "Seoha Kim",
      "Youngsik Yun",
      "Hahyun Lee",
      "Gun Bang",
      "Youngjung Uh*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2366_ECCV_2024_paper.php": {
    "title": "DreamMover: Leveraging the Prior of Diffusion Models for Image Interpolation with Large Motion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liao Shen",
      "Tianqi Liu",
      "Huiqiang Sun",
      "Xinyi Ye",
      "Baopu Li",
      "Jianming Zhang",
      "Zhiguo Cao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2371_ECCV_2024_paper.php": {
    "title": "CoLA: Conditional Dropout and Language-driven Robust Dual-modal Salient Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuang Hao",
      "Chunlin Zhong",
      "He Tang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2375_ECCV_2024_paper.php": {
    "title": "Image-Feature Weak-to-Strong Consistency: An Enhanced Paradigm for Semi-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyu Wu*",
      "Jinshi Cui*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2379_ECCV_2024_paper.php": {
    "title": "RPBG: Towards Robust Neural Point-based Graphics in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingtian Zhu",
      "Zizhuang Wei",
      "Zhongtian Zheng",
      "Yifan Zhan",
      "Zhuyu Yao",
      "Jiawang Zhang",
      "Kejian Wu",
      "Yinqiang Zheng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2380_ECCV_2024_paper.php": {
    "title": "GaussReg: Fast 3D Registration with Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahao Chang*",
      "Yinglin Xu",
      "Yihao Li",
      "Yuantao Chen",
      "Wensen Feng",
      "Xiaoguang Han"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2391_ECCV_2024_paper.php": {
    "title": "Efficient Diffusion Transformer with Step-wise Dynamic Attention Mediators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Pu*",
      "Zhuofan Xia",
      "Jiayi Guo",
      "Dongchen Han",
      "Qixiu Li",
      "Duo Li",
      "Yuhui Yuan",
      "Ji Li",
      "Yizeng Han",
      "Shiji Song",
      "Gao Huang*",
      "Xiu Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2396_ECCV_2024_paper.php": {
    "title": "Open Vocabulary 3D Scene Understanding via Geometry Guided Self-Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengfei Wang*",
      "Yuxi Wang",
      "Shuai Li",
      "Zhaoxiang Zhang",
      "Zhen Lei",
      "Lei Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2398_ECCV_2024_paper.php": {
    "title": "IAM-VFI : Interpolate Any Motion for Video Frame Interpolation with motion complexity map",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kihwan Yoon*",
      "Yong Han Kim",
      "Sungjei Kim*",
      "Jinwoo Jeong*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2403_ECCV_2024_paper.php": {
    "title": "TIP: Tabular-Image Pre-training for Multimodal Classification with Incomplete Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyi Du*",
      "Shaoming Zheng",
      "Yinsong Wang",
      "Wenjia Bai",
      "Declan P. O'Regan",
      "Chen Qin*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2408_ECCV_2024_paper.php": {
    "title": "Diffusion Model is a Good Pose Estimator from 3D RF-Vision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junqiao Fan",
      "Jianfei Yang*",
      "Yuecong Xu",
      "Lihua Xie"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2418_ECCV_2024_paper.php": {
    "title": "UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vandad Davoodnia*",
      "Saeed Ghorbani",
      "Marc-André Carbonneau",
      "Alexandre Messier",
      "Ali Etemad"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2419_ECCV_2024_paper.php": {
    "title": "Learning 3D-aware GANs from Unposed Images with Template Feature Field",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinya Chen",
      "Hanlei Guo",
      "Yanrui Bin",
      "Shangzhan Zhang",
      "Yuanbo Yang",
      "Yujun Shen",
      "Yue Wang",
      "Yiyi Liao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2422_ECCV_2024_paper.php": {
    "title": "TAPTR: Tracking Any Point with Transformers as Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyang Li*",
      "Hao Zhang",
      "Shilong Liu",
      "Zhaoyang Zeng",
      "Tianhe Ren",
      "Feng Li",
      "Lei Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2429_ECCV_2024_paper.php": {
    "title": "Token Compensator: Altering Inference Cost of Vision Transformer without Re-Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shibo Jie",
      "Yehui Tang",
      "Jianyuan Guo",
      "Zhi-Hong Deng*",
      "Kai Han*",
      "Yunhe Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2430_ECCV_2024_paper.php": {
    "title": "Point-supervised Panoptic Segmentation via Estimating Pseudo Labels from Learnable Distance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Li",
      "Junsong Fan*",
      "Zhaoxiang Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2433_ECCV_2024_paper.php": {
    "title": "BRAVE: Broadening the visual encoding of vision-language models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oğuzhan Fatih Kar*",
      "Alessio Tonioni*",
      "Petra Poklukar",
      "Achin Kulshrestha",
      "Amir Zamir",
      "Federico Tombari"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2441_ECCV_2024_paper.php": {
    "title": "HUMOS: Human Motion Model Conditioned on Body Shape",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shashank Tripathi*",
      "Omid Taheri",
      "Christoph Lassner*",
      "Michael J. Black*",
      "Daniel Holden*",
      "Carsten Stoll*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2444_ECCV_2024_paper.php": {
    "title": "Omni-Recon: Harnessing Image-based Rendering for General-Purpose Neural Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yonggan Fu",
      "Huaizhi Qu",
      "Zhifan Ye",
      "Chaojian Li",
      "Kevin Zhao",
      "Yingyan (Celine) Lin*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2446_ECCV_2024_paper.php": {
    "title": "MVDiffHD: A Dense High-resolution Multi-view Diffusion Model for Single or Sparse-view 3D Object Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shitao Tang*",
      "Jiacheng Chen",
      "Dilin Wang",
      "Chengzhou Tang",
      "Fuyang Zhang",
      "Yuchen Fan",
      "Vikas Chandra",
      "Yasutaka Furukawa",
      "Rakesh Ranjan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2447_ECCV_2024_paper.php": {
    "title": "FlowCon: Out-of-Distribution Detection using Flow-based Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saandeep Aathreya*",
      "Shaun Canavan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2455_ECCV_2024_paper.php": {
    "title": "LEIA: Latent View-invariant Embeddings for Implicit 3D Articulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Archana Swaminathan*",
      "Anubhav Gupta",
      "Kamal Gupta",
      "Shishira R Maiya",
      "Vatsal Agarwal",
      "Abhinav Shrivastava"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2459_ECCV_2024_paper.php": {
    "title": "Un-EVIMO: Unsupervised Event-based Independent Motion Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyun Wang*",
      "Jinyuan Guo",
      "Kostas Daniilidis"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2469_ECCV_2024_paper.php": {
    "title": "Seeing the Unseen: A Frequency Prompt Guided Transformer for Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shihao Zhou",
      "Jinshan Pan",
      "Jinglei Shi*",
      "Duosheng Chen",
      "Lishen Qu",
      "Jufeng Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2472_ECCV_2024_paper.php": {
    "title": "CityGaussian: Real-time High-quality Large-Scale Scene Rendering with Gaussians",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Liu",
      "Chuanchen Luo",
      "Lue Fan",
      "Naiyan Wang",
      "Junran Peng*",
      "Zhaoxiang Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2475_ECCV_2024_paper.php": {
    "title": "Bayesian Evidential Deep Learning for Online Action Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongji Guo",
      "Hanjing Wang",
      "Qiang Ji*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2478_ECCV_2024_paper.php": {
    "title": "AdaNAT: Exploring Adaptive Policy for Token-Based Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zanlin Ni",
      "Yulin Wang",
      "Renping Zhou",
      "Rui Lu",
      "Jiayi Guo",
      "Jinyi Hu",
      "Zhiyuan Liu",
      "Yuan Yao*",
      "Gao Huang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2480_ECCV_2024_paper.php": {
    "title": "Rethinking Data Augmentation for Robust LiDAR Semantic Segmentation in Adverse Weather",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junsung Park",
      "Kyungmin Kim",
      "Hyunjung Shim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2488_ECCV_2024_paper.php": {
    "title": "Diffusion-Generated Pseudo-Observations for High-Quality Sparse-View Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinhang Liu*",
      "Jiaben Chen",
      "Shiu-Hong Kao",
      "Yu-Wing Tai",
      "Chi-Keung Tang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2494_ECCV_2024_paper.php": {
    "title": "Memory-Efficient Fine-Tuning for Quantized Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyogon Ryu",
      "Seohyun Lim",
      "Hyunjung Shim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2497_ECCV_2024_paper.php": {
    "title": "VCD-Texture: Variance Alignment based 3D-2D Co-Denoising for Text-Guided Texturing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shang Liu*",
      "Chaohui Yu",
      "Chenjie Cao",
      "Wen Qian",
      "Fan Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2504_ECCV_2024_paper.php": {
    "title": "MotionLCM: Real-time Controllable Motion Generation via Latent Consistency Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenxun Dai",
      "Ling-Hao Chen",
      "Jingbo Wang*",
      "Jinpeng Liu",
      "Bo Dai*",
      "Yansong Tang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2505_ECCV_2024_paper.php": {
    "title": "Human Hair Reconstruction with Strand-Aligned 3D Gaussians",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Egor Zakharov*",
      "Vanessa Sklyarova",
      "Michael J. Black",
      "Giljoo Nam",
      "Justus Thies",
      "Otmar Hilliges"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2507_ECCV_2024_paper.php": {
    "title": "COIN: Control-Inpainting Diffusion Prior for Human and Camera Motion Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiefeng Li*",
      "Ye Yuan",
      "Davis Rempe",
      "Haotian Zhang",
      "Pavlo Molchanov",
      "Cewu Lu",
      "Jan Kautz",
      "Umar Iqbal*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2508_ECCV_2024_paper.php": {
    "title": "SA-DVAE: Improving Zero-Shot Skeleton-Based Action Recognition by Disentangled Variational Autoencoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheng-Wei Li",
      "Zi-Xiang Wei",
      "Wei-Jie Chen",
      "Yi-Hsin Yu",
      "Chih-Yuan Yang*",
      "Jane Yung-jen Hsu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2511_ECCV_2024_paper.php": {
    "title": "Bridge Past and Future: Overcoming Information Asymmetry in Incremental Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qijie Mo",
      "Yipeng Gao",
      "Shenghao Fu",
      "Junkai Yan",
      "Ancong Wu*",
      "Wei-Shi Zheng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2513_ECCV_2024_paper.php": {
    "title": "Global-to-Pixel Regression for Human Mesh Recovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yabo Xiao",
      "Mingshu HE*",
      "Dongdong Yu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2515_ECCV_2024_paper.php": {
    "title": "Visible and Clear: Finding Tiny Objects in Difference Map",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bing Cao",
      "Haiyu Yao",
      "Pengfei Zhu*",
      "Qinghua Hu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2525_ECCV_2024_paper.php": {
    "title": "Rethinking Image Super Resolution from Training Data Perspectives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Go Ohtani*",
      "Ryu Tadokoro",
      "Ryosuke Yamada",
      "Yuki M Asano",
      "Iro Laina",
      "Christian Rupprecht",
      "Nakamasa Inoue",
      "Rio Yokota",
      "Hirokatsu Kataoka",
      "Yoshimitsu Aoki"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2526_ECCV_2024_paper.php": {
    "title": "BlazeBVD: Make Scale-Time Equalization Great Again for Blind Video Deflickering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinmin Qiu",
      "Congying Han",
      "Zicheng Zhang",
      "Bonan Li*",
      "Tiande Guo",
      "Pingyu Wang",
      "Xuecheng Nie"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2528_ECCV_2024_paper.php": {
    "title": "Efficient Inference of Vision Instruction-Following Models with Elastic Cache",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zuyan Liu",
      "Benlin Liu",
      "Jiahui Wang",
      "Yuhao Dong",
      "Guangyi Chen",
      "Yongming Rao",
      "Ranjay Krishna",
      "Jiwen Lu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2529_ECCV_2024_paper.php": {
    "title": "FreeCompose: Generic Zero-Shot Image Composition with Diffusion Prior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhekai Chen",
      "Wen Wang",
      "Zhen Yang",
      "Zeqing Yuan",
      "Hao Chen*",
      "Chunhua Shen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2547_ECCV_2024_paper.php": {
    "title": "Learning to Robustly Reconstruct Dynamic Scenes from Low-light Spike Streams",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liwen Hu*",
      "Ziluo Ding",
      "Mianzhi Liu",
      "Lei Ma*",
      "Tiejun Huang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2551_ECCV_2024_paper.php": {
    "title": "MarvelOVD: Marrying Object Recognition and Vision-Language Models for Robust Open-Vocabulary Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kuo Wang",
      "Lechao Cheng*",
      "Weikai Chen",
      "Pingping Zhang",
      "Liang Lin",
      "Fan Zhou",
      "Guanbin Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2554_ECCV_2024_paper.php": {
    "title": "WildVidFit: Video Virtual Try-On in the Wild via Image-Based Controlled Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijian He",
      "Peixin Chen",
      "Guangrun Wang",
      "Guanbin Li*",
      "Philip Torr",
      "Liang Lin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2556_ECCV_2024_paper.php": {
    "title": "Interactive 3D Object Detection with Prompts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruifei Zhang",
      "Xiangru Lin",
      "Wei Zhang",
      "Jincheng Lu",
      "Xuekuan Wang",
      "Xiao Tan",
      "Yingying Li",
      "Errui Ding",
      "Jingdong Wang",
      "Guanbin Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2562_ECCV_2024_paper.php": {
    "title": "How Video Meetings Change Your Expression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sumit Sarin*",
      "Utkarsh Mall",
      "Purva Tendulkar",
      "Carl Vondrick"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2569_ECCV_2024_paper.php": {
    "title": "GRACE: Graph-Based Contextual Debiasing for Fair Visual Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifeng Zhang",
      "Ming Jiang",
      "Qi Zhao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2571_ECCV_2024_paper.php": {
    "title": "Neural Volumetric World Models for Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zanming Huang*",
      "Jimuyang Zhang*",
      "Eshed Ohn-Bar*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2577_ECCV_2024_paper.php": {
    "title": "IVTP: Instruction-guided Visual Token Pruning for Large Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Huang*",
      "Hao Zou",
      "Ye Xi",
      "Bochen Wang",
      "Zhen Xie",
      "Liang Yu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2585_ECCV_2024_paper.php": {
    "title": "RegionDrag: Fast Region-Based Image Editing with Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyi Lu",
      "Xinghui Li",
      "Kai Han*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2588_ECCV_2024_paper.php": {
    "title": "On the Error Analysis of 3D Gaussian Splatting and an Optimal Projection Strategy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Letian Huang",
      "Jiayang Bai",
      "Jie Guo*",
      "Yuanqi Li",
      "Yanwen Guo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2596_ECCV_2024_paper.php": {
    "title": "Bad Students Make Great Teachers: Active Learning Accelerates Large-Scale Visual Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Talfan Evans*",
      "Shreya Pathak",
      "Hamza Merzic",
      "Jonathan Richard Schwarz",
      "Ryutaro Tanno",
      "Olivier Henaff*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2597_ECCV_2024_paper.php": {
    "title": "Analytic-Splatting: Anti-Aliased 3D Gaussian Splatting via Analytic Integration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihao Liang*",
      "Qi Zhang*",
      "Wenbo Hu",
      "Ying Feng",
      "Lei ZHU",
      "Kui Jia*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2600_ECCV_2024_paper.php": {
    "title": "GRA: Detecting Oriented Objects through Group-wise Rotating and Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiangshan Wang*",
      "Yifan Pu",
      "Yizeng Han",
      "Jiayi Guo",
      "Yiru Wang",
      "Xiu Li*",
      "Gao Huang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2615_ECCV_2024_paper.php": {
    "title": "Portrait4D-v2: Pseudo Multi-View Data Creates Better 4D Head Synthesizer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Deng*",
      "Duomin Wang",
      "Baoyuan Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2623_ECCV_2024_paper.php": {
    "title": "CSOT: Cross-Scan Object Transfer for Semi-Supervised LiDAR Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinglin Zhan",
      "Tiejun Liu",
      "Rengang Li",
      "Zhaoxiang Zhang",
      "Yuntao Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2627_ECCV_2024_paper.php": {
    "title": "Learning from the Web: Language Drives Weakly-Supervised Incremental Learning for Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chang Liu",
      "Giulia Rizzoli",
      "Pietro Zanuttigh",
      "Fu Li",
      "Yi Niu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2629_ECCV_2024_paper.php": {
    "title": "ShareGPT4V: Improving Large Multi-Modal Models with Better Captions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lin Chen*",
      "Jinsong Li",
      "Xiaoyi Dong",
      "Pan Zhang",
      "Conghui He",
      "Jiaqi Wang",
      "Feng Zhao*",
      "Dahua Lin*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2634_ECCV_2024_paper.php": {
    "title": "Eyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text Transformation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunhao Gou*",
      "Kai Chen",
      "Zhili LIU",
      "Lanqing Hong",
      "Hang Xu",
      "Zhenguo Li",
      "Dit-Yan Yeung",
      "James Kwok",
      "Yu Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2637_ECCV_2024_paper.php": {
    "title": "Invertible Neural Warp for NeRF",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shin-Fang Chng*",
      "Ravi Garg",
      "Hemanth Saratchandran",
      "Simon Lucey"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2641_ECCV_2024_paper.php": {
    "title": "Enhancing Vectorized Map Perception with Historical Rasterized Maps",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyu Zhang",
      "Guangwei Liu",
      "Zihao Liu",
      "Ningyi Xu",
      "Yunhui Liu*",
      "Ji Zhao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2642_ECCV_2024_paper.php": {
    "title": "Efficient and Versatile Robust Fine-Tuning of Zero-shot Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sungyeon Kim*",
      "Boseung Jeong",
      "Donghyun Kim",
      "Suha Kwak*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2657_ECCV_2024_paper.php": {
    "title": "Part2Object: Hierarchical Unsupervised 3D Instance Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Shi",
      "Yulin Zhang",
      "Bin Yang",
      "Jiajin Tang",
      "Yuexin Ma",
      "Sibei Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2660_ECCV_2024_paper.php": {
    "title": "PetFace: A Large-Scale Dataset and Benchmark for Animal Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Risa Shinoda*",
      "Kaede Shiohara"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2662_ECCV_2024_paper.php": {
    "title": "MVSGaussian: Fast Generalizable Gaussian Splatting Reconstruction from Multi-View Stereo",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianqi Liu",
      "Guangcong Wang",
      "Shoukang Hu",
      "Liao Shen",
      "Xinyi Ye",
      "Yuhang Zang",
      "Zhiguo Cao*",
      "Wei Li",
      "Ziwei Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2665_ECCV_2024_paper.php": {
    "title": "Zero-Shot Detection of AI-Generated Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Davide Cozzolino",
      "GIovanni Poggi",
      "Matthias Niessner",
      "Luisa Verdoliva*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2666_ECCV_2024_paper.php": {
    "title": "Language-Image Pre-training with Long Captions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kecheng Zheng*",
      "Yifei Zhang",
      "Wei Wu",
      "Fan Lu",
      "Shuailei Ma",
      "Xin Jin",
      "Wei Chen",
      "Yujun Shen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2675_ECCV_2024_paper.php": {
    "title": "GKGNet: Group K-Nearest Neighbor based Graph Convolutional Network for Multi-Label Image Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruijie Yao",
      "Sheng Jin",
      "Lumin Xu",
      "Wang Zeng",
      "Wentao Liu",
      "Chen Qian*",
      "Ping Luo",
      "Ji Wu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2686_ECCV_2024_paper.php": {
    "title": "DISCO: Embodied Navigation and Interaction via Differentiable Scene Semantics and Dual-level Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyu Xu*",
      "Shengcheng Luo",
      "Yanchao Yang",
      "Yong-Lu Li*",
      "Cewu Lu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2688_ECCV_2024_paper.php": {
    "title": "You Only Learn One Query: Learning Unified Human Query for Single-Stage Multi-Person Multi-Task Human-Centric Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheng Jin",
      "Shuhuai Li",
      "Tong Li",
      "Wentao Liu*",
      "Chen Qian",
      "Ping Luo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2698_ECCV_2024_paper.php": {
    "title": "Towards Real-World Adverse Weather Image Restoration: Enhancing Clearness and Semantics with Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqi Xu*",
      "Mengyang Wu",
      "Xiaowei Hu*",
      "Chi-Wing Fu",
      "Qi Dou",
      "Pheng-Ann Heng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2709_ECCV_2024_paper.php": {
    "title": "Facial Affective Behavior Analysis with Instruction Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Li*",
      "Anh Dao",
      "Wentao Bao",
      "Zhen Tan",
      "Tianlong Chen",
      "Huan Liu",
      "Yu Kong"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2711_ECCV_2024_paper.php": {
    "title": "CoReS: Orchestrating the Dance of Reasoning and Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyi Bao",
      "Siyang Sun",
      "Shuailei Ma",
      "Kecheng Zheng",
      "Yuxin Guo",
      "Guosheng Zhao",
      "Yun Zheng",
      "Xingang Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2738_ECCV_2024_paper.php": {
    "title": "MagDiff: Multi-Alignment Diffusion for High-Fidelity Video Generation and Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu Zhao",
      "Tianyi Lu",
      "Jiaxi Gu",
      "Xing Zhang",
      "Qingping Zheng",
      "Zuxuan Wu*",
      "Hang Xu",
      "Yu-Gang Jiang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2740_ECCV_2024_paper.php": {
    "title": "MambaIR: A Simple Baseline for Image Restoration with State-Space Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hang Guo*",
      "Jinmin Li",
      "Tao Dai*",
      "Zhihao Ouyang",
      "Xudong Ren",
      "Shu-Tao Xia"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2757_ECCV_2024_paper.php": {
    "title": "I Can't Believe It's Not Scene Flow!",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ishan Khatri*",
      "Kyle Vedder*",
      "Neehar Peri",
      "Deva Ramanan",
      "James Hays"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2763_ECCV_2024_paper.php": {
    "title": "Rethinking Unsupervised Outlier Detection via Multiple Thresholding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhonghang Liu*",
      "Panzhong Lu",
      "Guoyang Xie",
      "Zhichao Lu",
      "Wen-Yan Lin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2764_ECCV_2024_paper.php": {
    "title": "Compress3D: a Compressed Latent Space for 3D Generation from a Single Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowen Zhang*",
      "Tianyu Yang*",
      "Yu Li",
      "Lei Zhang",
      "Xi Zhao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2766_ECCV_2024_paper.php": {
    "title": "Scalable Group Choreography via Variational Phase Manifold Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nhat Le",
      "Khoa Do",
      "Xuan Bui",
      "Tuong Do",
      "Erman Tjiputra",
      "Quang D.Tran",
      "Anh Nguyen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2781_ECCV_2024_paper.php": {
    "title": "Masked Video and Body-worn IMU Autoencoder for Egocentric Action Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingfang Zhang",
      "Yifei Huang*",
      "Ruicong Liu",
      "Yoichi Sato"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2790_ECCV_2024_paper.php": {
    "title": "Mutual Learning for Acoustic Matching and Dereverberation via Visual Scene-driven Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Ma",
      "Wenguan Wang*",
      "Yi Yang",
      "Feng Zheng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2792_ECCV_2024_paper.php": {
    "title": "PoseSOR: Human Pose Can Guide Our Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huankang Guan",
      "Rynson W.H. Lau*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2800_ECCV_2024_paper.php": {
    "title": "TOD3Cap: Towards 3D Dense Captioning in Outdoor Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bu Jin",
      "Yupeng Zheng*",
      "Pengfei Li",
      "Weize Li",
      "Yuhang Zheng",
      "Sujie Hu",
      "Xinyu Liu",
      "Jinwei Zhu",
      "Zhijie Yan",
      "Haiyang Sun",
      "Kun Zhan",
      "Peng Jia",
      "Xiaoxiao Long",
      "Yilun Chen",
      "Hao Zhao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2804_ECCV_2024_paper.php": {
    "title": "Bi-directional Contextual Attention for 3D Dense Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minjung Kim*",
      "Hyung Suk Lim",
      "Soonyoung Lee",
      "Bumsoo Kim*",
      "Gunhee Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2808_ECCV_2024_paper.php": {
    "title": "Multi-Person Pose Forecasting with Individual Interaction Perceptron and Prior Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peng Xiao",
      "Yi Xie",
      "Xuemiao Xu*",
      "Weihong Chen",
      "Huaidong Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2810_ECCV_2024_paper.php": {
    "title": "InfMAE: A Foundation Model in The Infrared Modality",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangcen Liu",
      "Chenqiang Gao*",
      "Yaming Zhang",
      "Junjie Guo",
      "Jinghao Wang",
      "Deyu Meng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2818_ECCV_2024_paper.php": {
    "title": "TPA3D: Triplane Attention for Fast Text-to-3D Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bin-Shih Wu*",
      "Hong-En Chen*",
      "Sheng-Yu Huang",
      "Yu-Chiang Frank Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2819_ECCV_2024_paper.php": {
    "title": "Multi-Memory Matching for Unsupervised Visible-Infrared Person Re-Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiangming Shi",
      "Xiangbo Yin",
      "Yeyun Chen",
      "Yachao Zhang",
      "Zhizhong Zhang",
      "Yuan Xie*",
      "Yanyun Qu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2821_ECCV_2024_paper.php": {
    "title": "LivePhoto: Real Image Animation with Text-guided Motion Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xi Chen",
      "Zhiheng Liu",
      "Mengting Chen",
      "Yutong Feng",
      "Yu Liu",
      "Yujun Shen",
      "Hengshuang Zhao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2823_ECCV_2024_paper.php": {
    "title": "NeuSDFusion: A Spatial-Aware Generative Model for 3D Shape Completion, Reconstruction, and Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruikai Cui",
      "Weizhe Liu*",
      "Weixuan Sun",
      "Senbo Wang",
      "Taizhang Shang",
      "Yang Li",
      "Xibin Song",
      "Han Yan",
      "ZHENNAN WU",
      "Shenzhou Chen",
      "HONGDONG LI",
      "Pan Ji"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2824_ECCV_2024_paper.php": {
    "title": "AID-AppEAL: Automatic Image Dataset and Algorithm for Content Appeal Enhancement and Assessment Labeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sherry X. Chen*",
      "Yaron Vaxman",
      "Elad Ben Baruch",
      "David Asulin",
      "Aviad Moreshet",
      "Misha Sra",
      "Pradeep Sen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2829_ECCV_2024_paper.php": {
    "title": "SEDiff: Structure Extraction for Domain Adaptive Depth Estimation via Denoising Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongseok Shim*",
      "Hyoun Jin Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2833_ECCV_2024_paper.php": {
    "title": "Quantized Prompt for Efficient Generalization of Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianxiang Hao",
      "Xiaohan Ding*",
      "Juexiao Feng",
      "Yuhong Yang",
      "Hui Chen",
      "Guiguang Ding*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2834_ECCV_2024_paper.php": {
    "title": "Online Temporal Action Localization with Memory-Augmented Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youngkil Song",
      "Dongkeun Kim",
      "Minsu Cho",
      "Suha Kwak*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2840_ECCV_2024_paper.php": {
    "title": "Efficient Cascaded Multiscale Adaptive Network for Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yichen Zhou*",
      "Pan Zhou*",
      "Teck Khim Ng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2842_ECCV_2024_paper.php": {
    "title": "MOFA-Video: Controllable Image Animation via Generative Motion Field Adaptions in Frozen Image-to-Video Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muyao Niu",
      "Xiaodong Cun*",
      "Xintao Wang",
      "Yong Zhang",
      "Ying Shan",
      "Yinqiang Zheng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2865_ECCV_2024_paper.php": {
    "title": "Occlusion-Aware Seamless Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihong Cao",
      "Jiaming Zhang",
      "Hao Shi",
      "Kunyu Peng",
      "Yuhongxuan Zhang",
      "Hui Zhang*",
      "Rainer Stiefelhagen",
      "Kailun Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2866_ECCV_2024_paper.php": {
    "title": "OpenKD: Opening Prompt Diversity for Zero- and Few-shot Keypoint Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changsheng Lu*",
      "Zheyuan Liu",
      "Piotr Koniusz*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2873_ECCV_2024_paper.php": {
    "title": "Referring Atomic Video Action Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kunyu Peng*",
      "Jia Fu",
      "Kailun Yang",
      "Di Wen",
      "Yufan Chen",
      "Ruiping Liu",
      "Junwei Zheng",
      "Jiaming Zhang",
      "Saquib Sarfraz",
      "Rainer Stiefelhagen",
      "Alina Roitberg"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2877_ECCV_2024_paper.php": {
    "title": "Agent3D-Zero: An Agent for Zero-shot 3D Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "sha zhang",
      "Di Huang",
      "Jiajun Deng*",
      "Shixiang Tang",
      "Wanli Ouyang",
      "Tong He*",
      "Yanyong Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2879_ECCV_2024_paper.php": {
    "title": "Stream Query Denoising for Vectorized HD-Map Construction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuo Wang*",
      "Fan Jia",
      "Weixin Mao",
      "Yingfei Liu",
      "Yucheng Zhao",
      "Zehui Chen",
      "Tiancai Wang",
      "Chi Zhang",
      "Xiangyu Zhang",
      "Feng Zhao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2887_ECCV_2024_paper.php": {
    "title": "SAGS: Structure-Aware 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Evangelos Ververas",
      "Rolandos Alexandros Potamias*",
      "Jifei Song",
      "Jiankang Deng",
      "Stefanos Zafeiriou"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2904_ECCV_2024_paper.php": {
    "title": "Spherical Linear Interpolation and Text-Anchoring for Zero-shot Composed Image Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Young Kyun Jang*",
      "Dat B Huynh",
      "Ashish Shah",
      "Wen-Kai Chen",
      "Ser-Nam Lim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2906_ECCV_2024_paper.php": {
    "title": "OneRestore: A Universal Restoration Framework for Composite Degradation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Guo*",
      "Yuan Gao",
      "Yuxu Lu",
      "Huilin Zhu",
      "Wen Liu",
      "Shengfeng He"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2909_ECCV_2024_paper.php": {
    "title": "Beat-It: Beat-Synchronized Multi-Condition 3D Dance Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zikai Huang",
      "Xuemiao Xu*",
      "Cheng Xu*",
      "Huaidong Zhang",
      "Chenxi Zheng",
      "Jing Qin",
      "Shengfeng He"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2910_ECCV_2024_paper.php": {
    "title": "SkyMask: Attack-agnostic Robust Federated Learning with Fine-grained Learnable Masks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peishen Yan",
      "Hao Wang",
      "Tao Song*",
      "Yang Hua",
      "Ruhui Ma",
      "Ningxin Hu",
      "Mohammad Reza Haghighat",
      "Haibing Guan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2925_ECCV_2024_paper.php": {
    "title": "RePOSE: 3D Human Pose Estimation via Spatio-Temporal Depth Relational Consistency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziming Sun",
      "Yuan Liang",
      "Zejun Ma",
      "Tianle Zhang",
      "Linchao Bao",
      "Guiqing Li",
      "Shengfeng He*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2926_ECCV_2024_paper.php": {
    "title": "Pixel-GS Density Control with Pixel-aware Gradient for 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheng Zhang",
      "Wenbo Hu*",
      "Yixing Lao",
      "Tong He",
      "Hengshuang Zhao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2927_ECCV_2024_paper.php": {
    "title": "WorldPose: A World Cup Dataset for Global 3D Human Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianjian Jiang*",
      "Johsan Billingham",
      "Sebastian Müksch",
      "Juan J Zarate",
      "Nicolas Evans",
      "Martin R. Oswald",
      "Marc Pollefeys",
      "Otmar Hilliges",
      "Manuel Kaufmann",
      "Jie Song"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2933_ECCV_2024_paper.php": {
    "title": "Language-Driven 6-DoF Grasp Detection Using Negative Prompt Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Toan Nguyen",
      "Minh Nhat Nhat Vu",
      "Baoru Huang",
      "An Dinh Vuong",
      "Quan Vuong",
      "Ngan Le",
      "Thieu Vo",
      "Anh Nguyen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2943_ECCV_2024_paper.php": {
    "title": "COIN-Matting: Confounder Intervention for Image Matting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaohe Liao",
      "Jiangtong Li",
      "Jun Lan",
      "Huijia Zhu",
      "Weiqiang Wang",
      "Li Niu*",
      "Liqing Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2948_ECCV_2024_paper.php": {
    "title": "SHINE: Saliency-aware HIerarchical NEgative Ranking for Compositional Temporal Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixu Cheng*",
      "Yujiang Pu*",
      "Shaogang Gong",
      "Parisa Kordjamshidi",
      "Yu Kong"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2950_ECCV_2024_paper.php": {
    "title": "Audio-driven Talking Face Generation with Stabilized Synchronization Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dogucan Yaman*",
      "Fevziye Irem Eyiokur",
      "Leonard Bärmann",
      "HAZIM KEMAL EKENEL",
      "Alexander Waibel"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2965_ECCV_2024_paper.php": {
    "title": "Propose, Assess, Search: Harnessing LLMs for Goal-Oriented Planning in Instructional Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md Mohaiminul Islam*",
      "Tushar Nagarajan",
      "Huiyu Wang",
      "FU-JEN CHU",
      "Kris Kitani",
      "Gedas Bertasius",
      "Xitong Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2968_ECCV_2024_paper.php": {
    "title": "Train Till You Drop: Towards Stable and Robust Source-free Unsupervised 3D Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Björn Michele*",
      "Alexandre Boulch",
      "Tuan-Hung VU",
      "Gilles Puy",
      "Renaud Marlet",
      "Nicolas Courty"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2971_ECCV_2024_paper.php": {
    "title": "Learning to Obstruct Few-Shot Image Classification over Restricted Classes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amber Yijia Zheng*",
      "Chiao-An Yang*",
      "Raymond A. Yeh"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2977_ECCV_2024_paper.php": {
    "title": "RoofDiffusion: Constructing Roofs from Severely Corrupted Point Data via Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kyle Shih-Huang Lo*",
      "Jorg Peters",
      "Eric Spellman"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2988_ECCV_2024_paper.php": {
    "title": "L-DiffER: Single Image Reflection Removal with Language-based Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchen Hong*",
      "Haofeng Zhong*",
      "Shuchen Weng",
      "Jinxiu S Liang",
      "Boxin Shi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2990_ECCV_2024_paper.php": {
    "title": "AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Wang*",
      "Xiaogeng Liu*",
      "Yu Li*",
      "Muhao Chen",
      "Chaowei Xiao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3004_ECCV_2024_paper.php": {
    "title": "OccGen: Generative Multi-modal 3D Occupancy Prediction for Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guoqing Wang",
      "Zhongdao Wang",
      "Pin Tang",
      "Jilai Zheng",
      "Xiangxuan Ren",
      "Bailan Feng",
      "Chao Ma*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3008_ECCV_2024_paper.php": {
    "title": "CrossGLG: LLM Guides One-shot Skeleton-based 3D Action Recognition in a Cross-level Manner",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tingbing Yan",
      "Wenzheng Zeng*",
      "Yang Xiao*",
      "Xingyu Tong",
      "Bo Tan",
      "Zhiwen Fang",
      "Zhiguo Cao",
      "Joey Tianyi Zhou"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3012_ECCV_2024_paper.php": {
    "title": "HYDRA: A Hyper Agent for Dynamic Compositional Visual Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fucai Ke*",
      "Zhixi Cai",
      "Simindokht Jahangard",
      "Weiqing Wang",
      "Pari Delir Haghighi",
      "Hamid Rezatofighi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3014_ECCV_2024_paper.php": {
    "title": "BrushNet: A Plug-and-Play Image Inpainting Model with Decomposed Dual-Branch Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan Ju*",
      "Xian Liu",
      "Xintao Wang*",
      "Yuxuan Bian",
      "Ying Shan",
      "Qiang Xu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3022_ECCV_2024_paper.php": {
    "title": "LayoutDETR: Detection Transformer Is a Good Multimodal Layout Designer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ning Yu*",
      "Chia-chih Chen",
      "Zeyuan Chen",
      "Rui Meng",
      "Gang Wu",
      "Paul W Josel",
      "Juan Carlos Niebles",
      "Caiming Xiong",
      "Ran Xu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3024_ECCV_2024_paper.php": {
    "title": "Blind image deblurring with noise-robust kernel estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chanseok Lee*",
      "Jeongsol Kim",
      "Seungmin Lee",
      "Jaehwang Jung",
      "Yunje Cho",
      "Taejoong Kim",
      "Taeyong Jo",
      "Myungjun Lee",
      "Mooseok Jang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3025_ECCV_2024_paper.php": {
    "title": "Binomial Self-compensation for Motion Error in Dynamic 3D Scanning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Geyou Zhang",
      "Ce Zhu*",
      "Kai Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3028_ECCV_2024_paper.php": {
    "title": "AddMe: Zero-shot Group-photo Synthesis by Inserting People into Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongxu Yue",
      "Maomao Li",
      "Yunfei Liu",
      "Ailing Zeng",
      "Tianyu Yang",
      "Qin Guo",
      "Yu Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3030_ECCV_2024_paper.php": {
    "title": "Distill Gold from Massive Ores: Bi-level Data Pruning towards Efficient Dataset Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Xu",
      "Yong-Lu Li*",
      "Kaitong Cui",
      "Ziyu Wang",
      "Cewu Lu",
      "Yu-Wing Tai",
      "Chi-Keung Tang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3032_ECCV_2024_paper.php": {
    "title": "VersatileGaussian: Real-time Neural Rendering for Versatile Tasks using Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Renjie Li",
      "Zhiwen Fan*",
      "Bohua Wang",
      "Peihao Wang",
      "Zhangyang Wang",
      "Xi Wu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3037_ECCV_2024_paper.php": {
    "title": "Momentum Auxiliary Network for Supervised Local Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junhao Su",
      "Changpeng Cai",
      "Feiyu Zhu",
      "Chenghao He",
      "Xiaojie Xu",
      "Dongzhi Guan*",
      "Chenyang Si*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3039_ECCV_2024_paper.php": {
    "title": "HPFF: Hierarchical Locally Supervised Learning with Patch Feature Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junhao Su",
      "Chenghao He",
      "Feiyu Zhu",
      "Xiaojie Xu",
      "Dongzhi Guan",
      "Chenyang Si*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3042_ECCV_2024_paper.php": {
    "title": "Rethinking LiDAR Domain Generalization: Single Source as Multiple Density Domains",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaeyeul Kim",
      "Jungwan Woo",
      "Jeonghoon Kim",
      "Sunghoon Im*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3044_ECCV_2024_paper.php": {
    "title": "Improving Zero-Shot Generalization for CLIP with Variational Adapter",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziqian Lu",
      "Fengli Shen",
      "Mushui Liu",
      "Yunlong Yu*",
      "Xi Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3045_ECCV_2024_paper.php": {
    "title": "Realistic Human Motion Generation with Cross-Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeping Ren",
      "Shaoli Huang*",
      "Xiu Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3057_ECCV_2024_paper.php": {
    "title": "EgoExo-Fitness: Towards Egocentric and Exocentric Full-Body Action Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan-Ming Li",
      "Wei-Jin Huang",
      "An-Lan Wang",
      "Ling-An Zeng",
      "Jing-Ke Meng*",
      "Wei-Shi Zheng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3058_ECCV_2024_paper.php": {
    "title": "Any Target Can be Offense: Adversarial Example Generation via Generalized Latent Infection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youheng Sun",
      "Shengming Yuan",
      "Xuanhan Wang*",
      "Lianli Gao",
      "Jingkuan Song"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3062_ECCV_2024_paper.php": {
    "title": "Towards Reliable Advertising Image Generation Using Human Feedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenbang Du*",
      "Wei Feng",
      "Haohan Wang",
      "Yaoyu Li",
      "Jingsen Wang",
      "Jian Li",
      "Zheng Zhang",
      "Jingjing Lv",
      "Xin Zhu",
      "Junsheng Jin",
      "Junjie Shen",
      "Zhangang Lin",
      "Jingping Shao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3067_ECCV_2024_paper.php": {
    "title": "Topology-Preserving Downsampling of Binary Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chia-Chia Chen*",
      "Chi-Han Peng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3072_ECCV_2024_paper.php": {
    "title": "ColorMAE: Exploring data-independent masking strategies in Masked AutoEncoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Carlos Hinojosa*",
      "Shuming Liu",
      "Bernard Ghanem"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3079_ECCV_2024_paper.php": {
    "title": "Classification Matters: Improving Video Action Detection with Class-Specific Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinsung Lee",
      "Taeoh Kim",
      "Inwoong Lee",
      "Minho Shim",
      "Dongyoon Wee",
      "Minsu Cho",
      "Suha Kwak*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3080_ECCV_2024_paper.php": {
    "title": "Improving Medical Multi-modal Contrastive Learning with Expert Annotations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yogesh Kumar*",
      "Pekka Marttinen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3084_ECCV_2024_paper.php": {
    "title": "Rethinking Data Bias: Dataset Copyright Protection via Embedding Class-wise Hidden Bias",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinhyeok Jang*",
      "ByungOk Han",
      "Jaehong Kim",
      "Chan-Hyun Youn"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3092_ECCV_2024_paper.php": {
    "title": "Pose-Aware Self-Supervised Learning with Viewpoint Trajectory Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayun Wang*",
      "Yubei Chen",
      "Stella X. Yu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3093_ECCV_2024_paper.php": {
    "title": "SILC: Improving Vision Language Pretraining with Self-Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Ferjad Naeem*",
      "Yongqin Xian",
      "Xiaohua Zhai",
      "Lukas Hoyer",
      "Luc Van Gool",
      "Federico Tombari"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3098_ECCV_2024_paper.php": {
    "title": "Learning Semantic Latent Directions for Accurate and Controllable Human Motion Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guowei Xu",
      "Jiale Tao",
      "Wen Li*",
      "Lixin Duan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3099_ECCV_2024_paper.php": {
    "title": "Leveraging temporal contextualization for video action recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minji Kim",
      "Dongyoon Han",
      "Taekyung Kim*",
      "Bohyung Han*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3114_ECCV_2024_paper.php": {
    "title": "ChEX: Interactive Localization and Region Description in Chest X-rays",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Philip Müller*",
      "Georgios Kaissis",
      "Daniel Rueckert"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3128_ECCV_2024_paper.php": {
    "title": "AdaGlimpse: Active Visual Exploration with Arbitrary Glimpse Position and Scale",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adam Pardyl*",
      "Michał Wronka",
      "Maciej Wołczyk",
      "Kamil Adamczewski",
      "Tomasz Trzcinski",
      "Bartosz Zieliński*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3131_ECCV_2024_paper.php": {
    "title": "CLAP: Isolating Content from Style through Contrastive Learning with Augmented Prompts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yichao Cai*",
      "Yuhang Liu",
      "Zhen Zhang",
      "Javen Qinfeng Shi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3141_ECCV_2024_paper.php": {
    "title": "ZigMa: A DiT-style Zigzag Mamba Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vincent Tao Hu*",
      "Stefan A Baumann",
      "Ming Gui",
      "Olga Grebenkova",
      "Pingchuan Ma",
      "Johannes S Fischer",
      "Bjorn Ommer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3146_ECCV_2024_paper.php": {
    "title": "EchoScene: Indoor Scene Generation via Information Echo over Scene Graph Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangyao Zhai*",
      "Evin Pınar Örnek",
      "Dave Zhenyu Chen",
      "Ruotong Liao",
      "Yan Di",
      "Nassir Navab",
      "Federico Tombari",
      "Benjamin Busam"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3148_ECCV_2024_paper.php": {
    "title": "On Calibration of Object Detectors: Pitfalls, Evaluation and Baselines",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Selim Kuzucu*",
      "Kemal Oksuz*",
      "Jonathan Sadeghi",
      "Puneet Dokania"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3153_ECCV_2024_paper.php": {
    "title": "HAT: History-Augmented Anchor Transformer for Online Temporal Action Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sakib Reza",
      "Yuexi Zhang",
      "Mohsen Moghaddam",
      "Octavia Camps*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3155_ECCV_2024_paper.php": {
    "title": "Deep Nets with Subsampling Layers Unwittingly Discard Useful Activations at Test-Time",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chiao-An Yang*",
      "Ziwei Liu",
      "Raymond Yeh"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3157_ECCV_2024_paper.php": {
    "title": "Safe-Sim: Safety-Critical Closed-Loop Traffic Simulation with Diffusion-Controllable Adversaries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei-Jer Chang*",
      "Francesco Pittaluga",
      "Masayoshi Tomizuka",
      "Wei Zhan",
      "Manmohan Chandraker"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3170_ECCV_2024_paper.php": {
    "title": "Analysis-by-Synthesis Transformer for Single-View 3D Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dian Jia",
      "Xiaoqian Ruan",
      "Kun Xia",
      "Zhiming Zou",
      "Le Wang",
      "Wei Tang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3171_ECCV_2024_paper.php": {
    "title": "Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine Unlearning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chongyu Fan",
      "Jiancheng Liu*",
      "Alfred Hero",
      "Sijia Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3174_ECCV_2024_paper.php": {
    "title": "WaSt-3D: Wasserstein-2 Distance for Scene-to-Scene Stylization on 3D Gaussians",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dmytro Kotovenko*",
      "Olga Grebenkova*",
      "Nikolaos Sarafianos",
      "Avinash Paliwal",
      "Pingchuan Ma",
      "Omid Poursaeed",
      "Sreyas Mohan",
      "Yuchen Fan",
      "Yilei Li",
      "Rakesh Ranjan",
      "Bjorn Ommer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3178_ECCV_2024_paper.php": {
    "title": "SCLIP: Rethinking Self-Attention for Dense Vision-Language Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feng Wang*",
      "Jieru Mei",
      "Alan Yuille"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3184_ECCV_2024_paper.php": {
    "title": "Flying with Photons: Rendering Novel Views of Propagating Light",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anagh Malik*",
      "Noah Juravsky",
      "Ryan Po",
      "Gordon Wetzstein",
      "Kiriakos N. Kutulakos",
      "David B. Lindell"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3186_ECCV_2024_paper.php": {
    "title": "RGNet: A Unified Clip Retrieval and Grounding Network for Long Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tanveer Hannan*",
      "Md Mohaiminul Islam",
      "Thomas Seidl",
      "Gedas Bertasius"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3187_ECCV_2024_paper.php": {
    "title": "MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuedong Chen*",
      "Haofei Xu",
      "Chuanxia Zheng",
      "Bohan Zhuang",
      "Marc Pollefeys",
      "Andreas Geiger",
      "Tat-Jen Cham",
      "Jianfei Cai"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3191_ECCV_2024_paper.php": {
    "title": "3DGazeNet: Generalizing Gaze Estimation with Weak Supervision from Synthetic Views",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Evangelos Ververas*",
      "Polydefkis Gkagkos",
      "Jiankang Deng",
      "Michail C Doukas",
      "Jia Guo",
      "Stefanos Zafeiriou"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3192_ECCV_2024_paper.php": {
    "title": "Removing Distributional Discrepancies in Captions Improves Image-Text Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mu Cai",
      "Haotian Liu",
      "Yuheng Li*",
      "Yijun Li",
      "Eli Shechtman",
      "Zhe Lin",
      "Yong Jae Lee",
      "Krishna Kumar Singh"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3193_ECCV_2024_paper.php": {
    "title": "Resilience of Entropy Model in Distributed Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Milin Zhang*",
      "Mohammad Abdi",
      "Shahriar Rifat",
      "Francesco Restuccia"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3195_ECCV_2024_paper.php": {
    "title": "Rejection Sampling IMLE: Designing Priors for Better Few-Shot Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chirag Vashist*",
      "Shichong Peng",
      "Ke Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3200_ECCV_2024_paper.php": {
    "title": "Implicit Concept Removal of Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhili Liu*",
      "Kai Chen",
      "Yifan Zhang",
      "Jianhua Han",
      "Lanqing Hong",
      "Hang Xu",
      "Zhenguo Li",
      "Dit-Yan Yeung",
      "James Kwok"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3209_ECCV_2024_paper.php": {
    "title": "PLOT: Text-based Person Search with Part Slot Attention for Corresponding Part Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jicheol Park",
      "Dongwon Kim",
      "Boseung Jeong",
      "Suha Kwak*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3212_ECCV_2024_paper.php": {
    "title": "GS-LRM: Large Reconstruction Model for 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Zhang*",
      "Sai Bi",
      "Hao Tan",
      "Yuanbo Xiangli",
      "Nanxuan Zhao",
      "Kalyan Sunkavalli",
      "Zexiang Xu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3220_ECCV_2024_paper.php": {
    "title": "Robust-Wide: Robust Watermarking against Instruction-driven Image Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runyi Hu",
      "Jie Zhang*",
      "Ting Xu",
      "Jiwei Li",
      "Tianwei Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3228_ECCV_2024_paper.php": {
    "title": "OAPT: Offset-Aware Partition Transformer for Double JPEG Artifacts Removal",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiao Mo",
      "Yukang Ding",
      "Jinhua Hao*",
      "Qiang Zhu",
      "Ming Sun",
      "Chao Zhou",
      "Feiyu Chen",
      "Shuyuan Zhu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3233_ECCV_2024_paper.php": {
    "title": "Formula-Supervised Visual-Geometric Pre-training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryosuke Yamada*",
      "Kensho Hara*",
      "Hirokatsu Kataoka",
      "Koshi Makihara",
      "Nakamasa Inoue",
      "Rio Yokota",
      "Yutaka Satoh"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3241_ECCV_2024_paper.php": {
    "title": "VideoAgent: A Memory-augmented Multimodal Agent for Video Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Fan",
      "Xiaojian Ma*",
      "Rujie Wu",
      "yuntao du",
      "Jiaqi Li",
      "Zhi Gao",
      "Qing Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3248_ECCV_2024_paper.php": {
    "title": "Towards Unified Representation of Invariant-Specific Features in Missing Modality Face Anti-Spoofing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guanghao Zheng",
      "Yuchen Liu",
      "Wenrui Dai*",
      "Chenglin Li",
      "Junni Zou",
      "Hongkai Xiong"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3249_ECCV_2024_paper.php": {
    "title": "Restoring Images in Adverse Weather Conditions via Histogram Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shangquan Sun",
      "Wenqi Ren*",
      "Xinwei Gao",
      "Rui Wang",
      "Xiaochun Cao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3251_ECCV_2024_paper.php": {
    "title": "PosFormer: Recognizing Complex Handwritten Mathematical Expression with Position Forest Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tongkun Guan",
      "Chengyu Lin",
      "Wei Shen*",
      "Xiaokang Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3255_ECCV_2024_paper.php": {
    "title": "NGP-RT: Fusing Multi-Level Hash Features with Lightweight Attention for Real-Time Novel View Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yubin Hu",
      "Xiaoyang Guo",
      "Yang Xiao",
      "Jingwei Huang",
      "Yong-Jin Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3256_ECCV_2024_paper.php": {
    "title": "Elysium: Exploring Object-level Perception in Videos through Semantic Integration Using MLLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Wang*",
      "Yanjie Wang",
      "Ye Yongjie",
      "Yuxiang Nie",
      "Can Huang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3259_ECCV_2024_paper.php": {
    "title": "G2fR: Frequency Regularization in Grid-based Feature Encoding Neural Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuxiang Xie*",
      "Shuyi Zhou",
      "Ken Sakurada",
      "Ryoichi Ishikawa",
      "Masaki Onishi",
      "Takeshi Oishi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3260_ECCV_2024_paper.php": {
    "title": "Getting it Right: Improving Spatial Consistency in Text-to-Image Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Agneet Chatterjee*",
      "Gabriela Ben Melech Stan",
      "Estelle Guez Aflalo",
      "Sayak Paul",
      "Dhruba Ghosh",
      "Tejas Gokhale",
      "Ludwig Schmidt",
      "Hanna Hajishirzi",
      "Vasudev Lal",
      "Chitta R Baral",
      "Yezhou Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3261_ECCV_2024_paper.php": {
    "title": "Generating 3D House Wireframes with Semantics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xueqi Ma",
      "Yilin Liu",
      "Wenjun Zhou",
      "Ruowei Wang",
      "Hui Huang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3265_ECCV_2024_paper.php": {
    "title": "GeoWizard: Unleashing the Diffusion Priors for 3D Geometry Estimation from a Single Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Fu*",
      "Wei Yin",
      "Mu Hu",
      "Kaixuan Wang",
      "Yuexin Ma",
      "Ping Tan",
      "Shaojie Shen",
      "Dahua Lin",
      "Xiaoxiao Long"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3268_ECCV_2024_paper.php": {
    "title": "Shape-guided Configuration-aware Learning for Endoscopic-image-based Pose Estimation of Flexible Robotic Instruments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiyao Ma*",
      "Kai Chen*",
      "Hon-Sing Tong",
      "Ruofeng Wei",
      "Yui-Lun Ng",
      "Ka-Wai Kwok*",
      "Qi Dou*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3272_ECCV_2024_paper.php": {
    "title": "Nonverbal Interaction Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianan Wei",
      "Tianfei Zhou",
      "Yi Yang",
      "Wenguan Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3274_ECCV_2024_paper.php": {
    "title": "UniM2AE: Multi-modal Masked Autoencoders with Unified 3D Representation for 3D Perception in Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Zou",
      "Tianyu Huang",
      "Guanglei Yang*",
      "Zhenhua Guo",
      "Tao Luo*",
      "Chun-Mei Feng",
      "Wangmeng Zuo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3276_ECCV_2024_paper.php": {
    "title": "Responsible Visual Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minheng Ni",
      "Yeli Shen",
      "Lei Zhang*",
      "Wangmeng Zuo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3277_ECCV_2024_paper.php": {
    "title": "Drag Anything: Motion Control for Anything using Entity Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weijia Wu ",
      "Zhuang Li",
      "Yuchao Gu",
      "Rui Zhao",
      "Yefei He",
      "David Junhao Zhang",
      "Mike Zheng Shou*",
      "Yan Li",
      "Tingting Gao",
      "Zhang Di"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3280_ECCV_2024_paper.php": {
    "title": "SegPoint: Segment Any Point Cloud via Large Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuting He",
      "Henghui Ding",
      "Xudong Jiang",
      "Bihan Wen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3285_ECCV_2024_paper.php": {
    "title": "Navigation Instruction Generation with BEV Perception and Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheng Fan",
      "Rui Liu",
      "Wenguan Wang*",
      "Yi Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3287_ECCV_2024_paper.php": {
    "title": "Rebalancing Using Estimated Class Distribution for Imbalanced Semi-Supervised Learning under Class Distribution Mismatch",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taemin Park",
      "Hyuck Lee",
      "Heeyoung Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3293_ECCV_2024_paper.php": {
    "title": "Vista3D: unravel the 3d darkside of a single image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiuhong Shen",
      "Xingyi Yang",
      "Michael Bi Mi",
      "Xinchao Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3294_ECCV_2024_paper.php": {
    "title": "The Fabrication of Reality and Fantasy: Scene Generation with LLM-Assisted Prompt Interpretation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Yao",
      "Chan-Feng Hsu*",
      "Jhe-Hao Lin",
      "Hongxia Xie",
      "Terence Lin",
      "Yi-Ning Huang",
      "Hong-Han Shuai*",
      "Wen-Huang Cheng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3298_ECCV_2024_paper.php": {
    "title": "Detecting As Labeling: Rethinking LiDAR-camera Fusion in 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junjie Huang*",
      "Yun Ye",
      "Zhujin Liang",
      "Yi Shan",
      "Dalong Du"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3300_ECCV_2024_paper.php": {
    "title": "FlashSplat: 2D to 3D Gaussian Splatting Segmentation Solved Optimally",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiuhong Shen",
      "Xingyi Yang",
      "Xinchao Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3302_ECCV_2024_paper.php": {
    "title": "Exploiting Dual-Correlation for Multi-frame Time-of-Flight Denoising",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guanting Dong*",
      "Yueyi Zhang*",
      "Xiaoyan Sun",
      "Zhiwei Xiong"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3307_ECCV_2024_paper.php": {
    "title": "Weak-to-Strong Compositional Learning from Generative Models for Language-based Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kwanyong Park",
      "Kuniaki Saito",
      "Donghyun Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3309_ECCV_2024_paper.php": {
    "title": "Domesticating SAM for Breast Ultrasound Image Segmentation via Spatial-frequency Fusion and Uncertainty Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wanting Zhang",
      "Huisi Wu*",
      "Jing Qin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3322_ECCV_2024_paper.php": {
    "title": "CanonicalFusion: Generating Drivable 3D Human Avatars from Multiple Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jisu Shin",
      "Junmyeong Lee",
      "Seongmin Lee",
      "Min-Gyu Park",
      "Jumi Kang",
      "Ju Hong Yoon",
      "Hae-Gon Jeon*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3328_ECCV_2024_paper.php": {
    "title": "Camera Height Doesn't Change: Unsupervised Training for Metric Monocular Road-Scene Depth Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Genki Kinoshita*",
      "Ko Nishino"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3330_ECCV_2024_paper.php": {
    "title": "Uni3DL: A Unified Model for 3D Vision-Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Li*",
      "Jian Ding",
      "Zhaoyang Chen",
      "Mohamed Elhoseiny"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3338_ECCV_2024_paper.php": {
    "title": "Object-Aware NIR-to-Visible Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunyi Gao",
      "Lin Gu",
      "Qiankun Liu",
      "Ying Fu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3341_ECCV_2024_paper.php": {
    "title": "PaPr: Training-Free One-Step Patch Pruning with Lightweight ConvNets for Faster Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tanvir Mahmud*",
      "Burhaneddin Yaman",
      "Chun-Hao Liu",
      "Diana Marculescu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3355_ECCV_2024_paper.php": {
    "title": "GENIXER: Empowering Multimodal Large Language Models as a Powerful Data Generator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Henry Hengyuan Zhao*",
      "Pan Zhou*",
      "Mike Zheng Shou*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3356_ECCV_2024_paper.php": {
    "title": "BLINK: Multimodal Large Language Models Can See but Not Perceive",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyu Fu*",
      "Yushi Hu*",
      "Bangzheng Li",
      "Yu Feng",
      "Haoyu Wang",
      "Xudong Lin",
      "Dan Roth",
      "Noah A Smith",
      "Wei-Chiu Ma",
      "Ranjay Krishna"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3361_ECCV_2024_paper.php": {
    "title": "AFF-ttention! Affordances and Attention models for Short-Term Object Interaction Anticipation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lorenzo Mur-Labadia*",
      "Ruben Martinez-Cantin",
      "Jose J Guerrero",
      "Giovanni Maria Farinella",
      "Antonino Furnari"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3363_ECCV_2024_paper.php": {
    "title": "PreLAR: World Model Pre-training with Learnable Action Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lixuan Zhang",
      "Meina Kan",
      "Shiguang Shan",
      "Xilin Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3364_ECCV_2024_paper.php": {
    "title": "Multi-HMR: Multi-Person Whole-Body Human Mesh Recovery in a Single Shot",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fabien Baradel*",
      "Thomas LUCAS",
      "Matthieu Armando",
      "Salma Galaaoui",
      "Romain Brégier",
      "Philippe Weinzaepfel",
      "Gregory Rogez"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3367_ECCV_2024_paper.php": {
    "title": "De-confounded Gaze Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyang Liang",
      "Yiwei Bao",
      "Feng Lu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3381_ECCV_2024_paper.php": {
    "title": "Diffusion Models for Monocular Depth Estimation: Overcoming Challenging Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fabio Tosi",
      "Pierluigi Zama Ramirez",
      "Matteo Poggi*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3384_ECCV_2024_paper.php": {
    "title": "FreestyleRet: Retrieving Images from Style-Diversified Queries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Li*",
      "Yanhao Jia",
      "Peng Jin",
      "Zesen Cheng",
      "Kehan Li",
      "Jialu Sui",
      "Chang Liu",
      "Li Yuan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3388_ECCV_2024_paper.php": {
    "title": "ReGround: Improving Textual and Spatial Grounding at No Cost",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Phillip Y. Lee",
      "Minhyuk Sung*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3391_ECCV_2024_paper.php": {
    "title": "CardiacNet: Learning to Reconstruct Abnormalities for Cardiac Disease Assessment from Echocardiogram Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiewen Yang*",
      "Yiqun Lin",
      "Bin Pu",
      "Jiarong GUO",
      "Xiaowei Xu*",
      "Xiaomeng Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3396_ECCV_2024_paper.php": {
    "title": "LaMI-DETR: Open-Vocabulary Detection with Language Model Instruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Penghui Du",
      "Yu Wang",
      "Yifan Sun",
      "Luting Wang",
      "Yue Liao",
      "gang zhang",
      "Errui Ding",
      "Yan Wang*",
      "Jingdong Wang",
      "Si Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3398_ECCV_2024_paper.php": {
    "title": "Unrolled Decomposed Unpaired Learning for Controllable Low-Light Video Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingyu Zhu",
      "Wenhan Yang",
      "Baoliang Chen",
      "Hanwei Zhu",
      "Zhangkai Ni",
      "Qi Mao",
      "Shiqi Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3399_ECCV_2024_paper.php": {
    "title": "Efficient Image Pre-Training with Siamese Cropped Masked Autoencoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexandre Eymaël",
      "Renaud Vandeghen*",
      "Anthony Cioppa",
      "Silvio Giancola",
      "Bernard Ghanem",
      "Marc Van Droogenbroeck"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3403_ECCV_2024_paper.php": {
    "title": "VP-SAM: Taming Segment Anything Model for Video Polyp Segmentation via Disentanglement and Spatio-temporal Side Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhixue Fang",
      "Yuzhi Liu",
      "Huisi Wu*",
      "Jing Qin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3404_ECCV_2024_paper.php": {
    "title": "Dataset Enhancement with Instance-Level Augmentations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Orest Kupyn*",
      "Christian Rupprecht"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3408_ECCV_2024_paper.php": {
    "title": "FreeMotion: MoCap-Free Human Motion Synthesis with Multimodal Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhikai Zhang",
      "Yitang Li",
      "Haofeng Huang",
      "Mingxian Lin",
      "Li Yi*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3409_ECCV_2024_paper.php": {
    "title": "Chameleon: A Data-Efficient Generalist for Dense Visual Prediction in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Donggyun Kim",
      "Seongwoong Cho",
      "Semin Kim",
      "Chong Luo",
      "Seunghoon Hong*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3410_ECCV_2024_paper.php": {
    "title": "Reliability in Semantic Segmentation: Can We Use Synthetic Data?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thibaut Loiseau",
      "Tuan-Hung Vu*",
      "Mickael Chen",
      "Patrick Pérez",
      "Matthieu Cord"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3420_ECCV_2024_paper.php": {
    "title": "SCPNet: Unsupervised Cross-modal Homography Estimation via Intra-modal Self-supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runmin Zhang*",
      "Jun Ma",
      "Lun Luo",
      "Beinan Yu",
      "Shu-Jie Chen",
      "Junwei Li",
      "Hui-Liang Shen",
      "Si-Yuan Cao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3423_ECCV_2024_paper.php": {
    "title": "SCAPE: A Simple and Strong Category-Agnostic Pose Estimator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujia Liang",
      "Zixuan Ye",
      "Wenze Liu",
      "Hao Lu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3426_ECCV_2024_paper.php": {
    "title": "Elevating All Zero-Shot Sketch-Based Image Retrieval Through Multimodal Prompt Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mainak Singha*",
      "Ankit Jha",
      "Divyam Gupta",
      "Pranav Singla",
      "Biplab Banerjee"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3432_ECCV_2024_paper.php": {
    "title": "Improving Knowledge Distillation via Regularizing Feature Direction and Norm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuzhu Wang",
      "Lechao Cheng*",
      "Manni Duan",
      "Yongheng Wang",
      "Zunlei Feng",
      "Shu Kong"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3433_ECCV_2024_paper.php": {
    "title": "3DFG-PIFu: 3D Feature Grids for Human Digitization from Sparse Views",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kennard Yanting Chan*",
      "Fayao Liu",
      "Guosheng Lin",
      "Chuan Sheng Foo",
      "Weisi Lin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3436_ECCV_2024_paper.php": {
    "title": "Lazy Diffusion Transformer for Interactive Image Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yotam Nitzan*",
      "Zongze Wu",
      "Richard Zhang",
      "Eli Shechtman",
      "Danny Cohen-Or",
      "Taesung Park",
      "Michaël Gharbi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3438_ECCV_2024_paper.php": {
    "title": "Non-parametric Sensor Noise Modeling and Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali Mosleh*",
      "Luxi Zhao",
      "Atin Vikram Singh",
      "Jaeduk Han",
      "Abhijith Punnappurath",
      "Marcus A Brubaker",
      "Jihwan Choe",
      "Michael S Brown"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3451_ECCV_2024_paper.php": {
    "title": "Stripe Observation Guided Inference Cost-free Attention Mechanism",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongzhan Huang*",
      "Shanshan Zhong",
      "Wushao Wen",
      "Jinghui Qin",
      "Liang Lin*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3456_ECCV_2024_paper.php": {
    "title": "The Nerfect Match: Exploring NeRF Features for Visual Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qunjie Zhou*",
      "Maxim Maximov",
      "Or Litany",
      "Laura Leal-Taixé"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3457_ECCV_2024_paper.php": {
    "title": "ComboVerse: Compositional 3D Assets Creation Using Spatially-Aware Diffusion Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongwei Chen",
      "Tengfei Wang",
      "Tong Wu",
      "Xingang Pan",
      "Kui Jia*",
      "Ziwei Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3461_ECCV_2024_paper.php": {
    "title": "Robust Calibration of Large Vision-Language Adapters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Balamurali Murugesan*",
      "Julio Silva-Rodríguez",
      "Ismail Ben Ayed",
      "Jose Dolz"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3463_ECCV_2024_paper.php": {
    "title": "Leveraging Hierarchical Feature Sharing for Efficient Dataset Condensation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haizhong Zheng*",
      "Jiachen Sun",
      "Shutong Wu",
      "Bhavya Kailkhura",
      "Zhuoqing Morley Mao",
      "Chaowei Xiao*",
      "Atul Prakash*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3467_ECCV_2024_paper.php": {
    "title": "Improving Domain Generalization in Self-Supervised Monocular Depth Estimation via Stabilized Adversarial Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanqi Yao*",
      "Gang Wu",
      "Kui Jiang",
      "Siao Liu",
      "Jian Kuai",
      "Xianming Liu",
      "Junjun Jiang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3470_ECCV_2024_paper.php": {
    "title": "milliFlow: Scene Flow Estimation on mmWave Radar Point Cloud for Human Motion Sensing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangqiang Ding*",
      "Zhen Luo",
      "Peijun Zhao",
      "Chris Xiaoxuan Lu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3482_ECCV_2024_paper.php": {
    "title": "denoiSplit: a method for joint microscopy image splitting and unsupervised denoising",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashesh Ashesh*",
      "Florian Jug*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3484_ECCV_2024_paper.php": {
    "title": "AugDETR: Improving Multi-scale Learning for Detection Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinpeng Dong",
      "Yutong Lin",
      "Chen Li",
      "Sanping Zhou",
      "Nanning Zheng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3489_ECCV_2024_paper.php": {
    "title": "Spherical World-Locking for Audio-Visual Localization in Egocentric Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heeseung Yun*",
      "Ruohan Gao",
      "Ishwarya Ananthabhotla",
      "Anurag Kumar",
      "Jacob Donley",
      "Chao Li",
      "Gunhee Kim",
      "Vamsi Krishna Ithapu",
      "Calvin Murdock*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3498_ECCV_2024_paper.php": {
    "title": "SPIN: Hierarchical Segmentation with Subpart Granularity in Natural Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Josh David Myers-Dean*",
      "Jarek T Reynolds",
      "Brian Price",
      "Yifei Fan",
      "Danna Gurari"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3506_ECCV_2024_paper.php": {
    "title": "SIGMA: Sinkhorn-Guided Masked Video Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammadreza Salehi*",
      "Michael Dorkenwald*",
      "Fida Mohammad Thoker",
      "Efstratios Gavves",
      "Cees Snoek",
      "Yuki M Asano"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3510_ECCV_2024_paper.php": {
    "title": "Generative Camera Dolly: Extreme Monocular Dynamic Novel View Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Basile Van Hoorick*",
      "Rundi Wu",
      "Ege Ozguroglu",
      "Kyle Sargent",
      "Ruoshi Liu",
      "Pavel Tokmakov",
      "Achal Dave",
      "Changxi Zheng",
      "Carl Vondrick"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3521_ECCV_2024_paper.php": {
    "title": "Distribution Alignment for Fully Test-Time Adaptation with Dynamic Online Data Streams",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziqiang Wang*",
      "Zhixiang Chi",
      "Yanan Wu",
      "Li Gu",
      "Zhi Liu*",
      "Konstantinos N Plataniotis*",
      "Yang Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3522_ECCV_2024_paper.php": {
    "title": "Divide and Fuse: Body Part Mesh Recovery from Partially Visible Human Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyu Luan",
      "Zhongpai Gao",
      "Luyuan Xie",
      "Abhishek Sharma",
      "Hao Ding",
      "Benjamin Planche",
      "Meng Zheng",
      "Ange Lou",
      "Terrence Chen",
      "Junsong Yuan",
      "Ziyan Wu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3523_ECCV_2024_paper.php": {
    "title": "Understanding Physical Dynamics with Counterfactual World Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rahul Venkatesh*",
      "Honglin Chen*",
      "Kevin Feigelis",
      "Daniel M Bear",
      "Khaled Jedoui",
      "Klemen Kotar",
      "Felix J Binder",
      "Wanhee Lee",
      "Sherry Liu",
      "Kevin Smith",
      "Judith E. Fan",
      "Daniel Yamins"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3525_ECCV_2024_paper.php": {
    "title": "MIGS: Multi-Identity Gaussian Splatting via Tensor Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aggelina Chatziagapi*",
      "Grigorios Chrysos",
      "Dimitris Samaras"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3536_ECCV_2024_paper.php": {
    "title": "4Diff: 3D-Aware Diffusion Model for Third-to-First Viewpoint Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feng Cheng*",
      "Mi Luo*",
      "Huiyu Wang",
      "Alex Dimakis",
      "Lorenzo Torresani",
      "Gedas Bertasius",
      "Kristen Grauman"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3537_ECCV_2024_paper.php": {
    "title": "Improving Point-based Crowd Counting and Localization Based on Auxiliary Point Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "I-HSIANG CHEN",
      "Wei-Ting Chen",
      "Yu-Wei Liu",
      "Ming-Hsuan Yang",
      "Sy-Yen Kuo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3541_ECCV_2024_paper.php": {
    "title": "Nymeria: A Massive Collection of Egocentric Multi-modal Human Motion in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingni Ma*",
      "Yuting Ye",
      "Rowan Postyeni",
      "Alexander J Gamino",
      "Vijay Baiyya",
      "Luis Pesqueira",
      "Kevin M Bailey",
      "David Soriano Fosas",
      "Fangzhou Hong",
      "Vladimir Guzov",
      "Yifeng Jiang",
      "Hyo Jin Kim",
      "Jakob Engel",
      "Karen Liu",
      "Ziwei Liu",
      "Renzo De Nardi",
      "Richard Newcombe"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3543_ECCV_2024_paper.php": {
    "title": "DreamStruct: Understanding Slides and User Interfaces via Synthetic Data Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi-Hao Peng*",
      "Faria Huq",
      "Yue Jiang",
      "Jason Wu",
      "Xin Yue Li",
      "Jeffrey Bigham",
      "Amy Pavel"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3555_ECCV_2024_paper.php": {
    "title": "SemTrack: A Large-scale Dataset for Semantic Tracking in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengfei Wang",
      "Xiaofei Hui",
      "Jing Wu",
      "Zile Yang",
      "Kian Eng Ong",
      "Xinge Zhao",
      "Beijia Lu",
      "Dezhao Huang",
      "Evan Ling",
      "Weiling Chen",
      "Keng Teck Ma",
      "Minhoe Hur",
      "Jun Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3565_ECCV_2024_paper.php": {
    "title": "VideoMamba: Spatio-Temporal Selective State Space Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinyoung Park*",
      "Hee-Seon Kim",
      "Kangwook Ko",
      "Minbeom Kim",
      "Changick Kim"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3566_ECCV_2024_paper.php": {
    "title": "Text to Layer-wise 3D Clothed Human Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junting Dong*",
      "Qi Fang",
      "Zehuan Huang",
      "Xudong XU",
      "Jingbo Wang",
      "Sida Peng",
      "Bo Dai"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3567_ECCV_2024_paper.php": {
    "title": "Texture-GS: Disentangle the Geometry and Texture for 3D Gaussian Splatting Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianxing Xu*",
      "Wenbo Hu",
      "Yu-Kun Lai",
      "Ying Shan",
      "Song-Hai Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3570_ECCV_2024_paper.php": {
    "title": "Fully Sparse 3D Occupancy Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haisong Liu",
      "Yang Chen",
      "Haiguang Wang",
      "Zetong Yang",
      "Tianyu Li",
      "Jia Zeng",
      "Li Chen",
      "Hongyang Li",
      "Limin Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3572_ECCV_2024_paper.php": {
    "title": "Is user feedback always informative? Retrieval Latent Defending for Semi-Supervised Domain Adaptation without Source Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junha Song*",
      "Tae Soo Kim",
      "Junha Kim",
      "Gunhee Nam",
      "Thijs Kooi",
      "Jaegul Choo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3580_ECCV_2024_paper.php": {
    "title": "CG-SLAM: Efficient Dense RGB-D SLAM in a Consistent Uncertainty-aware 3D Gaussian Field",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiarui Hu",
      "Xianhao Chen",
      "Boyin Feng",
      "Guanglin Li",
      "Liangjing Yang",
      "Hujun Bao",
      "Guofeng Zhang",
      "Zhaopeng Cui*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3594_ECCV_2024_paper.php": {
    "title": "Shifted Autoencoders for Point Annotation Restoration in Object Counting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuda Zou",
      "Xin Xiao",
      "Peilin Zhou",
      "Zhichao Sun",
      "Bo Du",
      "Yongchao Xu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3601_ECCV_2024_paper.php": {
    "title": "PointLLM: Empowering Large Language Models to Understand Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runsen Xu*",
      "Xiaolong Wang",
      "Tai Wang*",
      "Yilun Chen",
      "Jiangmiao Pang*",
      "Dahua Lin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3622_ECCV_2024_paper.php": {
    "title": "GarmentAligner: Text-to-Garment Generation via Retrieval-augmented Multi-level Corrections",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiyue Zhang",
      "Zheng Chong",
      "Xujie Zhang",
      "Hanhui Li",
      "Yuhao Cheng",
      "yiqiang yan",
      "Xiaodan Liang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3623_ECCV_2024_paper.php": {
    "title": "Improving Agent Behaviors with RL Fine-tuning for Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenghao Peng",
      "Wenjie Luo",
      "Yiren Lu*",
      "Tianyi Shen",
      "Cole Gulino",
      "Ari Seff",
      "Justin Fu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3625_ECCV_2024_paper.php": {
    "title": "Enhancing Diffusion Models with Text-Encoder Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaofeng Chen*",
      "Annan Wang",
      "Haoning Wu",
      "Liang Liao",
      "Wenxiu Sun",
      "Qiong Yan",
      "Weisi Lin*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3627_ECCV_2024_paper.php": {
    "title": "Asymmetric Mask Scheme for Self-Supervised Real Image Denoising",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangyu Liao*",
      "Tianheng Zheng",
      "Jiayu Zhong",
      "Pingping Zhang",
      "Chao Ren*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3632_ECCV_2024_paper.php": {
    "title": "Omni6D: Large-Vocabulary 3D Object Dataset for Category-Level 6D Object Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengchen Zhang*",
      "Tong Wu",
      "Tai Wang",
      "Tengfei Wang",
      "Ziwei Liu",
      "Dahua Lin*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3633_ECCV_2024_paper.php": {
    "title": "BAD-Gaussians: Bundle Adjusted Deblur Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingzhe Zhao",
      "Peng Wang",
      "Peidong Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3638_ECCV_2024_paper.php": {
    "title": "Forest2Seq: Revitalizing Order Prior for Sequential Indoor Scene Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Sun*",
      "Hang Zhou",
      "Wengang Zhou",
      "Li Li",
      "Houqiang Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3640_ECCV_2024_paper.php": {
    "title": "BaSIC: BayesNet Structure Learning for Computational Scalable Neural Image Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufeng Zhang",
      "Hang Yu",
      "Shizhan Liu",
      "Wenrui Dai",
      "Weiyao Lin*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3642_ECCV_2024_paper.php": {
    "title": "FlexAttention for Efficient High-Resolution Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyan Li*",
      "Delin Chen",
      "Tianle Cai",
      "Peihao Chen",
      "Yining Hong",
      "Zhenfang Chen",
      "Yikang Shen",
      "Chuang Gan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3643_ECCV_2024_paper.php": {
    "title": "Repaint123: Fast and High-quality One Image to 3D Generation with Progressive Controllable Repainting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junwu Zhang*",
      "Zhenyu Tang",
      "Yatian Pang",
      "Xinhua Cheng",
      "Peng Jin",
      "Yida Wei",
      "xing zhou",
      "munan ning",
      "Li Yuan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3644_ECCV_2024_paper.php": {
    "title": "AnimatableDreamer: Text-Guided Non-rigid 3D Model Generation and Reconstruction with Canonical Score Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinzhou Wang",
      "Yikai Wang*",
      "Junliang Ye",
      "Fuchun Sun*",
      "Zhengyi Wang",
      "Ling Wang",
      "Pengkun Liu",
      "Kai Sun",
      "Xintong Wang",
      "Xie wende",
      "Fangfu Liu",
      "Bin He"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3655_ECCV_2024_paper.php": {
    "title": "Spatially-Variant Degradation Model for Dataset-free Super-resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "SHAOJIE GUO",
      "Haofei Song",
      "Qingli Li",
      "Yan Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3660_ECCV_2024_paper.php": {
    "title": "DreamView: Injecting View-specific Text Guidance into Text-to-3D Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junkai Yan",
      "Yipeng Gao",
      "Qize Yang",
      "Xihan Wei",
      "Xuansong Xie",
      "Ancong Wu*",
      "WEI-SHI ZHENG*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3662_ECCV_2024_paper.php": {
    "title": "Learning Exhaustive Correlation for Spectral Super-Resolution: Where Spatial-Spectral Attention Meets Linear Dependence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyuan Wang",
      "Lizhi Wang*",
      "Jiang Xu",
      "Chang Chen",
      "Xue Hu",
      "Fenglong Song",
      "Youliang Yan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3669_ECCV_2024_paper.php": {
    "title": "Local Action-Guided Motion Diffusion Model for Text-to-Motion Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peng Jin*",
      "Hao Li",
      "Zesen Cheng",
      "Kehan Li",
      "Runyi Yu",
      "Chang Liu*",
      "Xiangyang Ji",
      "Li Yuan*",
      "Jie Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3678_ECCV_2024_paper.php": {
    "title": "EAFormer: Scene Text Segmentation with Edge-Aware Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiyang Yu",
      "Teng Fu",
      "Bin Li*",
      "Xiangyang Xue"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3682_ECCV_2024_paper.php": {
    "title": "Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zicong Fan",
      "Takehiko Ohkawa*",
      "Linlin Yang",
      "Nie Lin",
      "Zhishan Zhou",
      "Shihao Zhou",
      "Jiajun Liang",
      "Zhong Gao",
      "Xuanyang Zhang",
      "Xue Zhang",
      "Fei Li",
      "Liu Zheng",
      "Feng Lu",
      "Karim Abou Zeid",
      "Bastian Leibe",
      "Jeongwan On",
      "Seungryul Baek",
      "Aditya Prakash",
      "Saurabh Gupta",
      "Kun He",
      "Yoichi Sato",
      "Otmar Hilliges",
      "Hyung Jin Chang",
      "Angela Yao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3694_ECCV_2024_paper.php": {
    "title": "DetailSemNet: Elevating Signature Verification through Detail-Semantic Integration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meng-Cheng Shih*",
      "Tsai-Ling Huang",
      "Yu-Heng Shih",
      "Hong-Han Shuai",
      "Hsuan-Tung Liu",
      "Yi-Ren Yeh",
      "Ching-Chun Huang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3696_ECCV_2024_paper.php": {
    "title": "LaPose: Laplacian Mixture Shape Modeling for RGB-Based Category-Level Object Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruida Zhang",
      "Ziqin Huang",
      "Gu Wang",
      "Chenyangguang Zhang",
      "Yan Di",
      "Xingxing Zuo",
      "Jiwen Tang",
      "Xiangyang Ji*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3697_ECCV_2024_paper.php": {
    "title": "Upper-body Hierarchical Graph for Skeleton Based Emotion Recognition in Assistive Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiehui Wu",
      "Jiansheng Chen*",
      "Qifeng Luo",
      "Siqi Liu",
      "Youze Xue",
      "Huimin Ma"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3698_ECCV_2024_paper.php": {
    "title": "Fine-Grained Scene Graph Generation via Sample-Level Bias Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yansheng Li",
      "Tingzhu Wang*",
      "Kang Wu",
      "Linlin Wang",
      "Xin Guo",
      "Wenbin Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3700_ECCV_2024_paper.php": {
    "title": "Exploring Guided Sampling of Conditional GANs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei Zhang*",
      "Mengfei Xia",
      "Yujun Shen",
      "Jiapeng Zhu",
      "Ceyuan Yang",
      "Kecheng Zheng",
      "Lianghua Huang",
      "Yu Liu",
      "Fan Cheng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3701_ECCV_2024_paper.php": {
    "title": "MotionChain: Conversational Motion Controllers via Multimodal Prompts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Biao Jiang",
      "Xin Chen",
      "Chi Zhang",
      "Fukun Yin",
      "Zhuoyuan Li",
      "Gang Yu",
      "Jiayuan Fan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3717_ECCV_2024_paper.php": {
    "title": "Idempotent Unsupervised Representation Learning for Skeleton-Based Action Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lilang Lin",
      "Lehong Wu",
      "Jiahang Zhang",
      "Jiaying Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3726_ECCV_2024_paper.php": {
    "title": "Latent Guard: a Safety Framework for Text-to-image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runtao Liu*",
      "Ashkan Khakzar",
      "Jindong Gu",
      "Qifeng Chen*",
      "Philip Torr",
      "Fabio Pizzati*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3727_ECCV_2024_paper.php": {
    "title": "MacDiff: Unified Skeleton Modeling with Masked Conditional Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lehong Wu*",
      "Lilang Lin",
      "Jiahang Zhang",
      "Yiyang Ma",
      "Jiaying Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3732_ECCV_2024_paper.php": {
    "title": "TCC-Det: Temporarily consistent cues for weakly-supervised 3D detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jan Skvrna*",
      "Lukáš Neumann"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3741_ECCV_2024_paper.php": {
    "title": "OPEN: Object-wise Position Embedding for Multi-view 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinghua Hou",
      "Tong Wang",
      "Xiaoqing Ye",
      "Zhe Liu",
      "Shi Gong",
      "Xiao Tan",
      "Errui Ding",
      "Jingdong Wang",
      "Xiang Bai*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3742_ECCV_2024_paper.php": {
    "title": "FoundPose: Unseen Object Pose Estimation with Foundation Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Evin Pınar Örnek*",
      "Yann Labbé",
      "Bugra Tekin",
      "Lingni Ma",
      "Cem Keskin",
      "Christian Forster",
      "Tomas Hodan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3745_ECCV_2024_paper.php": {
    "title": "Early Preparation Pays Off: New Classifier Pre-tuning for Class Incremental Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengyuan Xie",
      "Haiquan Lu",
      "Jia-wen Xiao",
      "Enguang Wang",
      "Le Zhang",
      "Xialei Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3752_ECCV_2024_paper.php": {
    "title": "Kalman-Inspired Feature Propagation for Video Face Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruicheng Feng",
      "Chongyi Li",
      "Chen Change Loy*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3759_ECCV_2024_paper.php": {
    "title": "Select and Distill: Selective Dual-Teacher Knowledge Transfer for Continual Learning on Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu-Chu Yu*",
      "Chi-Pin Huang",
      "Jr-Jen Chen",
      "Kai-Po Chang",
      "Yung-Hsuan Lai",
      "Fu-En Yang",
      "Yu-Chiang Frank Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3773_ECCV_2024_paper.php": {
    "title": "VideoMamba: State Space Model for Efficient Video Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kunchang Li*",
      "Xinhao Li",
      "Yi Wang*",
      "Yinan He",
      "Yali Wang*",
      "Limin Wang*",
      "Yu Qiao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3782_ECCV_2024_paper.php": {
    "title": "SAFNet: Selective Alignment Fusion Network for Efficient HDR Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingtong Kong*",
      "Bo Li",
      "Yike Xiong",
      "Hao Zhang",
      "Hong Gu",
      "Jinwei Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3785_ECCV_2024_paper.php": {
    "title": "Heterogeneous Graph Learning for Scene Graph Prediction in 3D Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanni Ma",
      "Hao Liu",
      "Yun Pei",
      "Yulan Guo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3786_ECCV_2024_paper.php": {
    "title": "Reason2Drive: Towards Interpretable and Chain-based Reasoning for Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ming Nie",
      "Renyuan Peng",
      "Chunwei Wang",
      "Xinyue Cai",
      "Jianhua Han",
      "Hang Xu*",
      "Li Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3791_ECCV_2024_paper.php": {
    "title": "Omniview-Tuning: Boosting Viewpoint Invariance of Vision-Language Pre-training Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shouwei Ruan*",
      "Yinpeng Dong",
      "Liu Hanqing",
      "Yao Huang",
      "Hang Su",
      "Xingxing Wei*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3794_ECCV_2024_paper.php": {
    "title": "Deep Cost Ray Fusion for Sparse Depth Video Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jungeon Kim",
      "Soongjin Kim",
      "Jaesik Park",
      "Seungyong Lee*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3796_ECCV_2024_paper.php": {
    "title": "GraphBEV: Towards Robust BEV Feature Alignment for Multi-Modal 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziying Song",
      "Lei Yang",
      "Shaoqing Xu",
      "Lin Liu",
      "Dongyang Xu",
      "Caiyan Jia*",
      "Feiyang Jia",
      "Li Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3799_ECCV_2024_paper.php": {
    "title": "DINO-Tracker: Taming DINO for Self-Supervised Point Tracking in a Single Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Narek Tumanyan*",
      "Assaf Singer",
      "Shai Bagon",
      "Tali Dekel"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3801_ECCV_2024_paper.php": {
    "title": "GraspXL: Generating Grasping Motions for Diverse Objects at Scale",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hui Zhang*",
      "Sammy Christen",
      "Zicong Fan",
      "Otmar Hilliges",
      "Jie Song"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3804_ECCV_2024_paper.php": {
    "title": "Source Prompt Disentangled Inversion for Boosting Image Editability with Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruibin Li*",
      "Ruihuang Li",
      "Song Guo",
      "Lei Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3805_ECCV_2024_paper.php": {
    "title": "Improving Intervention Efficacy via Concept Realignment in Concept Bottleneck Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nishad Singhi*",
      "Jae Myung Kim",
      "Karsten Roth",
      "Zeynep Akata"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3809_ECCV_2024_paper.php": {
    "title": "JointDreamer: Ensuring Geometry Consistency and Text Congruence in Text-to-3D Generation via Joint Score Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "ChenHan Jiang*",
      "Yihan Zeng",
      "Tianyang Hu",
      "Songcen Xu",
      "Wei Zhang",
      "Hang Xu",
      "Dit-Yan Yeung"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3816_ECCV_2024_paper.php": {
    "title": "Brain Netflix: Scaling Data to Reconstruct Videos from Brain Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Camilo L Fosco*",
      "Benjamin Lahner",
      "Bowen Pan",
      "Alex Andonian",
      "Emilie L Josephs",
      "Alex Lascelles",
      "Aude Oliva"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3831_ECCV_2024_paper.php": {
    "title": "Equivariant Spatio-Temporal Self-Supervision for LiDAR Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Deepti Hegde",
      "Suhas Lohit*",
      "Kuan-Chuan Peng*",
      "Michael J. Jones",
      "Vishal M. Patel"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3832_ECCV_2024_paper.php": {
    "title": "SLAck: Semantic, Location, and Appearance Aware Open-Vocabulary Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyuan Li*",
      "Lei Ke",
      "Yung-Hsu Yang",
      "Luigi Piccinelli",
      "Mattia Segù",
      "Martin Danelljan",
      "Luc Van Gool"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3843_ECCV_2024_paper.php": {
    "title": "Tensorial template matching for fast cross-correlation with rotations and its application for tomography",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antonio Martinez-Sanchez*",
      "Ulrike Homberg",
      "J. M. Almira",
      "Harold Phelippeau"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3848_ECCV_2024_paper.php": {
    "title": "FreeAugment: Data Augmentation Search Across All Degrees of Freedom",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tom Bekor*",
      "Niv Nayman",
      "Lihi Zelnik-Manor"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3849_ECCV_2024_paper.php": {
    "title": "Learning Representations of Satellite Images From Metadata Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jules Bourcier*",
      "Gohar Dashyan",
      "Karteek Alahari",
      "Jocelyn Chanussot"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3857_ECCV_2024_paper.php": {
    "title": "I2-SLAM: Inverting Imaging Process for Robust Photorealistic Dense SLAM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gwangtak Bae",
      "Changwoon Choi",
      "Hyeongjun Heo",
      "Sang Min Kim",
      "Young Min Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3874_ECCV_2024_paper.php": {
    "title": "FlashTex: Fast Relightable Mesh Texturing with LightControlNet",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kangle Deng*",
      "Timothy Omernick",
      "Alexander B Weiss",
      "Deva Ramanan",
      "Jun-Yan Zhu",
      "Tinghui Zhou",
      "Maneesh Agrawala"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3878_ECCV_2024_paper.php": {
    "title": "GS-Pose: Category-Level Object Pose Estimation via Geometric and Semantic Correspondence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengyuan Wang*",
      "Takuya Ikeda",
      "Robert Lee",
      "Koichi Nishiwaki"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3880_ECCV_2024_paper.php": {
    "title": "ArtVLM: Attribute Recognition Through Vision-Based Prefix Language Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "William Yicheng Zhu*",
      "Keren Ye*",
      "Junjie Ke",
      "Jiahui Yu",
      "Leonidas Guibas",
      "Peyman Milanfar",
      "Feng Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3887_ECCV_2024_paper.php": {
    "title": "PanoFree: Tuning-Free Holistic Multi-view Image Generation with Cross-view Self-Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aoming Liu*",
      "Zhong Li*",
      "Zhang Chen*",
      "Nannan Li",
      "Yi Xu",
      "Bryan Plummer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3895_ECCV_2024_paper.php": {
    "title": "SOS: Segment Object System for Open-World Instance Segmentation With Object Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christian Wilms*",
      "Tim Rolff",
      "Maris N Hillemann",
      "Robert Johanson",
      "Simone Frintrop"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3896_ECCV_2024_paper.php": {
    "title": "Lagrangian Hashing for Compressed Neural Field Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shrisudhan Govindarajan*",
      "Zeno Sambugaro",
      "Akhmedkhan Shabanov",
      "Towaki Takikawa",
      "Weiwei Sun",
      "Daniel Rebain",
      "Nicola Conci",
      "Kwang Moo Yi",
      "Andrea Tagliasacchi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3905_ECCV_2024_paper.php": {
    "title": "EDformer: Transformer-Based Event Denoising Across Varied Noise Levels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bin Jiang",
      "Bo Xiong",
      "Bohan Qu",
      "M. Salman Asif",
      "You Zhou*",
      "Zhan Ma*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3908_ECCV_2024_paper.php": {
    "title": "Foster Adaptivity and Balance in Learning with Noisy Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengmeng Sheng",
      "Zeren Sun*",
      "Tao Chen",
      "Shuchao Pang",
      "yucheng wang",
      "Yazhou Yao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3914_ECCV_2024_paper.php": {
    "title": "MetaAug: Meta-Data Augmentation for Post-Training Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cuong Van Pham*",
      "Hoang Anh Dung",
      "Cuong Cao Nguyen",
      "Trung Le",
      "Dinh Phung",
      "Gustavo Carneiro",
      "Thanh-Toan Do"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3916_ECCV_2024_paper.php": {
    "title": "Thermal3D-GS: Physics-induced 3D Gaussians for Thermal Infrared Novel-view Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qian Chen",
      "Shihao Shu",
      "Xiangzhi Bai*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3918_ECCV_2024_paper.php": {
    "title": "Cross-Platform Video Person ReID: A New Benchmark Dataset and Adaptation Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shizhou Zhang",
      "Wenlong Luo",
      "De Cheng*",
      "Qingchun Yang",
      "Lingyan Ran",
      "Yinghui Xing",
      "Yanning Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3925_ECCV_2024_paper.php": {
    "title": "Unleashing the Power of Prompt-driven Nucleus Instance Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongyi Shui*",
      "Yunlong Zhang",
      "Kai Yao",
      "Chenglu Zhu",
      "Sunyi Zheng",
      "Jingxiong Li",
      "Honglin Li",
      "YUXUAN SUN",
      "Ruizhe Guo",
      "Lin Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3933_ECCV_2024_paper.php": {
    "title": "Gaze Target Detection Based on Head-Local-Global Coordination",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaokun Yang",
      "Feng Lu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3937_ECCV_2024_paper.php": {
    "title": "3DSA:Multi-View 3D Human Pose Estimation With 3D Space Attention Mechanisms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Po Han Chen",
      "Chia-Chi Tsai*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3942_ECCV_2024_paper.php": {
    "title": "Toward Tiny and High-quality Facial Makeup with Data Amplify Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiaoqiao Jin",
      "Xuanhong Chen",
      "Meiguang Jin",
      "Ying Chen",
      "Rui Shi",
      "Yucheng Zheng",
      "Yupeng Zhu",
      "Bingbing Ni*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3946_ECCV_2024_paper.php": {
    "title": "An Economic Framework for 6-DoF Grasp Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao-Ming Wu*",
      "Jia-Feng Cai",
      "Jian-Jian Jiang",
      "Dian Zheng",
      "Yi-Lin Wei",
      "Wei-Shi Zheng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3958_ECCV_2024_paper.php": {
    "title": "GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanhui Huang",
      "Wenzhao Zheng",
      "Yunpeng Zhang",
      "Jie Zhou",
      "Jiwen Lu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3967_ECCV_2024_paper.php": {
    "title": "Powerful and Flexible: Personalized Text-to-Image Generation via Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fanyue Wei",
      "Wei Zeng",
      "Zhenyang Li",
      "Dawei Yin",
      "Lixin Duan",
      "Wen Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3969_ECCV_2024_paper.php": {
    "title": "AdaLog: Post-Training Quantization for Vision Transformers with Adaptive Logarithm Quantizer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuguanyu Wu",
      "Jiaxin Chen*",
      "Hanwen Zhong",
      "Di Huang",
      "Yunhong Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3972_ECCV_2024_paper.php": {
    "title": "Multi-Label Cluster Discrimination for Visual Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang An",
      "Kaicheng Yang",
      "Xiangzi Dai",
      "Ziyong Feng",
      "Jiankang Deng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3983_ECCV_2024_paper.php": {
    "title": "Plan, Posture and Go: Towards Open-vocabulary Text-to-Motion Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinpeng Liu",
      "Wenxun Dai",
      "Chunyu Wang",
      "Yiji Cheng",
      "Yansong Tang*",
      "Xin Tong"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3984_ECCV_2024_paper.php": {
    "title": "DAMSDet: Dynamic Adaptive Multispectral Detection Transformer with Competitive Query Selection and Adaptive Feature Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junjie Guo*",
      "Chenqiang Gao*",
      "Fangcen Liu",
      "Deyu Meng",
      "Xinbo Gao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3990_ECCV_2024_paper.php": {
    "title": "CLIP-Guided Generative Networks for Transferable Targeted Adversarial Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Fang",
      "Jiawei Kong",
      "Bin Chen*",
      "Tao Dai",
      "Hao Wu",
      "Shu-Tao Xia"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3991_ECCV_2024_paper.php": {
    "title": "Flash Cache: Reducing Bias in Radiance Cache Based Inverse Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Attal*",
      "Dor Verbin",
      "Ben Mildenhall",
      "Peter Hedman",
      "Jonathan T Barron",
      "Matthew O'Toole",
      "Pratul Srinivasan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4012_ECCV_2024_paper.php": {
    "title": "Progressive Classifier and Feature Extractor Adaptation for Unsupervised Domain Adaptation on Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zicheng Wang",
      "Zhen Zhao",
      "Yiming Wu",
      "Luping Zhou*",
      "Dong Xu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4014_ECCV_2024_paper.php": {
    "title": "A New Dataset and Framework for Real-World Blurred Images Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Qin",
      "Ming Sun",
      "Chao Zhou",
      "Bin Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4016_ECCV_2024_paper.php": {
    "title": "AddressCLIP: Empowering Vision-Language Models for City-wide Image Address Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shixiong Xu",
      "Chenghao Zhang",
      "Lubin Fan*",
      "Gaofeng Meng*",
      "SHIMING XIANG",
      "Jieping Ye"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4024_ECCV_2024_paper.php": {
    "title": "RISurConv: Rotation Invariant Surface Attention-Augmented Convolutions for 3D Point Cloud Classification and Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyuan Zhang*",
      "Licheng Yang",
      "Zhiyu Xiang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4025_ECCV_2024_paper.php": {
    "title": "StyleTokenizer: Defining Image Style by a Single Instance for Controlling Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wen Li*",
      "Muyuan Fang",
      "Cheng Zou",
      "Biao Gong",
      "Ruobing Zheng",
      "Meng Wang",
      "Jingdong Chen",
      "Ming Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4026_ECCV_2024_paper.php": {
    "title": "Bidirectional Uncertainty-Based Active Learning for Open-Set Annotation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen-Chen Zong",
      "Ye-Wen Wang",
      "Kun-Peng Ning",
      "Hai-Bo Ye",
      "Sheng-Jun Huang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4040_ECCV_2024_paper.php": {
    "title": "Preventing Catastrophic Overfitting in Fast Adversarial Training: A Bi-level Optimization Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoxin Wang*",
      "Handing Wang*",
      "Cong Tian",
      "Yaochu Jin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4051_ECCV_2024_paper.php": {
    "title": "Projecting Points to Axes: Oriented Object Detection via Point-Axis Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyang Zhao",
      "Qilong Xue",
      "Yifan Bai",
      "Yuhang He",
      "Xing Wei*",
      "Yihong Gong"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4067_ECCV_2024_paper.php": {
    "title": "SeiT++: Masked Token Modeling Improves Storage-efficient Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minhyun Lee",
      "Song Park",
      "Byeongho Heo",
      "Dongyoon Han",
      "Hyunjung Shim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4069_ECCV_2024_paper.php": {
    "title": "Rectify the Regression Bias in Long-Tailed Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ke Zhu",
      "Minghao Fu",
      "Jie Shao",
      "Tianyu Liu",
      "Jianxin Wu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4070_ECCV_2024_paper.php": {
    "title": "MagicEraser: Erasing Any Objects via Semantics-Aware Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan Li*",
      "Zixiao Zhang",
      "Yi Huang",
      "Jianzhuang Liu",
      "Renjing Pei",
      "Bin Shao",
      "Songcen Xu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4076_ECCV_2024_paper.php": {
    "title": "Reliable Spatial-Temporal Voxels For Multi-Modal Test-Time Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haozhi Cao",
      "Yuecong Xu",
      "Jianfei Yang*",
      "Pengyu Yin",
      "Xingyu Ji",
      "Shenghai Yuan",
      "Lihua Xie"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4077_ECCV_2024_paper.php": {
    "title": "Stable Preference: Redefining training paradigm of human preference model for Text-to-Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanting Li",
      "Hongjing Niu",
      "Feng Zhao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4088_ECCV_2024_paper.php": {
    "title": "SparseSSP: 3D Subcellular Structure Prediction from Sparse-View Transmitted Light Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jintu Zheng",
      "Yi Ding",
      "Qizhe Liu",
      "Yuehui Chen",
      "Yi Cao",
      "Ying Hu",
      "Zenan Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4090_ECCV_2024_paper.php": {
    "title": "NL2Contact: Natural Language Guided 3D Hand-Object Contact Modeling with Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongqun Zhang*",
      "Hengfei Wang",
      "Ziwei Yu",
      "Yihua Cheng*",
      "Angela Yao",
      "Hyung Jin Chang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4094_ECCV_2024_paper.php": {
    "title": "Self-Adapting Large Visual-Language Models to Edge Devices across Visual Modalities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaiwen Cai",
      "ZheKai Duan",
      "Gaowen Liu",
      "Charles Fleming",
      "Chris Xiaoxuan Lu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4096_ECCV_2024_paper.php": {
    "title": "Diff-Tracker: Text-to-Image Diffusion Models are Unsupervised Trackers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengbo Zhang*",
      "Li Xu",
      "Duo Peng",
      "Hossein Rahmani",
      "Jun Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4104_ECCV_2024_paper.php": {
    "title": "Rethinking Tree-Ring Watermarking for Enhanced Multi-Key Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hai Ci*",
      "Pei Yang",
      "Yiren Song",
      "Mike Zheng Shou*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4105_ECCV_2024_paper.php": {
    "title": "3D Small Object Detection with Dynamic Spatial Pruning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihao Sun",
      "Ziwei Wang",
      "Hongmin Liu",
      "Jie Zhou",
      "Jiwen Lu*",
      "Xiuwei Xu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4106_ECCV_2024_paper.php": {
    "title": "STSP: Spatial-Temporal Subspace Projection for Video Class-incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Cheng",
      "SIYUAN YANG",
      "Chong Wang",
      "Joey Tianyi Zhou",
      "Alex Kot",
      "Bihan Wen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4117_ECCV_2024_paper.php": {
    "title": "Transferable 3D Adversarial Shape Completion using Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuelong Dai*",
      "Bin Xiao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4127_ECCV_2024_paper.php": {
    "title": "OmniSat: Self-Supervised Modality Fusion for Earth Observation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guillaume Astruc*",
      "Nicolas Gonthier",
      "Clement Mallet",
      "Loic Landrieu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4129_ECCV_2024_paper.php": {
    "title": "Distilling Diffusion Models into Conditional GANs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "MinGuk Kang*",
      "Richard Zhang",
      "Connelly Barnes",
      "Sylvain Paris",
      "Suha Kwak",
      "Jaesik Park",
      "Eli Shechtman",
      "Jun-Yan Zhu",
      "Taesung Park*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4140_ECCV_2024_paper.php": {
    "title": "Semantically Guided Representation Learning For Action Anticipation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anxhelo Diko*",
      "Danilo Avola",
      "Bardh Prenkaj",
      "Federico Fontana",
      "Luigi Cinque"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4143_ECCV_2024_paper.php": {
    "title": "MemBN: Robust Test-Time Adaptation via Batch Norm with Statistics Memory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juwon Kang*",
      "Nayeong Kim",
      "Jungseul Ok",
      "Suha Kwak*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4145_ECCV_2024_paper.php": {
    "title": "FREST: Feature RESToration for Semantic Segmentation under Multiple Adverse Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sohyun Lee",
      "Namyup Kim",
      "Sungyeon Kim",
      "Suha Kwak*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4147_ECCV_2024_paper.php": {
    "title": "ScanTalk: 3D Talking Heads from Unregistered Scans",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Federico Nocentini*",
      "Thomas Besnier",
      "Claudio Ferrari",
      "Sylvain Arguillere",
      "Stefano Berretti",
      "Mohamed Daoudi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4155_ECCV_2024_paper.php": {
    "title": "Controllable Navigation Instruction Generation with Chain of Thought Prompting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xianghao Kong",
      "Jinyu Chen",
      "Wenguan Wang*",
      "Hang Su",
      "Xiaolin Hu",
      "Yi Yang",
      "Si Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4158_ECCV_2024_paper.php": {
    "title": "GiT: Towards Generalist Vision Transformer through Universal Language Interface",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiyang Wang*",
      "Hao Tang",
      "Li Jiang",
      "Shaoshuai Shi",
      "Muhammad Ferjad Naeem",
      "Hongsheng Li",
      "Bernt Schiele",
      "Liwei Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4159_ECCV_2024_paper.php": {
    "title": "ScatterFormer: Efficient Voxel Transformer with Scattered Linear Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenhang He*",
      "Ruihuang Li",
      "Guowen Zhang",
      "Lei Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4167_ECCV_2024_paper.php": {
    "title": "A Cephalometric Landmark Regression Method based on Dual-encoder for High-resolution X-ray Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Dai",
      "yang wang*",
      "Chaolin Huang",
      "zhou jiakai",
      "Qilin Xu",
      "Minpeng Xu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4168_ECCV_2024_paper.php": {
    "title": "Exploring the Feature Extraction and Relation Modeling For Light-Weight Transformer Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jikai Zheng",
      "Mingjiang Liang",
      "Shaoli Huang",
      "Jifeng Ning*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4179_ECCV_2024_paper.php": {
    "title": "LiveHPS++: Robust and Coherent Motion Capture in Dynamic Free Environment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiming Ren",
      "Xiao Han",
      "Yichen Yao",
      "Xiaoxiao Long",
      "Yujing Sun*",
      "Yuexin Ma*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4181_ECCV_2024_paper.php": {
    "title": "You Only Need One Step: Fast Super-Resolution with Stable Diffusion via Scale Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mehdi Noroozi*",
      "Isma Hadji*",
      "Brais Martinez*",
      "Adrian Bulat*",
      "Georgios Tzimiropoulos*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4195_ECCV_2024_paper.php": {
    "title": "Gaussian Grouping: Segment and Edit Anything in 3D Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingqiao Ye",
      "Martin Danelljan",
      "Fisher Yu",
      "Lei Ke*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4203_ECCV_2024_paper.php": {
    "title": "CoMo: Controllable Motion Generation through Language Guided Pose Code Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiming Huang*",
      "Weilin Wan",
      "Yue Yang",
      "Chris Callison-Burch",
      "Mark Yatskar",
      "Lingjie Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4207_ECCV_2024_paper.php": {
    "title": "MegaScenes: Scene-Level View Synthesis at Scale",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joseph Tung",
      "Gene Chou*",
      "Ruojin Cai",
      "Guandao Yang",
      "Kai Zhang",
      "Gordon Wetzstein",
      "Bharath Hariharan",
      "Noah Snavely"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4210_ECCV_2024_paper.php": {
    "title": "SuperGaussian: Repurposing Video Models for 3D Super Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Shen*",
      "Duygu Ceylan*",
      "Paul Guerrero",
      "Zexiang Xu",
      "Niloy J. Mitra",
      "Shenlong Wang",
      "Anna Fruehstueck*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4212_ECCV_2024_paper.php": {
    "title": "Towards Model-Agnostic Dataset Condensation by Heterogeneous Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun-Yeong Moon",
      "Jung Uk Kim*",
      "Gyeong-Moon Park*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4213_ECCV_2024_paper.php": {
    "title": "Goldfish: Vision-Language Understanding of Arbitrarily Long Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kirolos Ataallah*",
      "Xiaoqian shen",
      "Eslam mohamed abdelrahman*",
      "Essam Sleiman",
      "Mingchen Zhuge",
      "Jian Ding",
      "Deyao Zhu",
      "Jürgen Schmidhuber",
      "Mohamed Elhoseiny"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4230_ECCV_2024_paper.php": {
    "title": "MeshFeat: Multi-Resolution Features for Neural Fields on Meshes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mihir Mahajan*",
      "Florian Hofherr*",
      "Daniel Cremers"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4236_ECCV_2024_paper.php": {
    "title": "Decoupling Common and Unique Representations for Multimodal Self-supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Wang*",
      "Conrad M Albrecht",
      "Nassim Ait Ali Braham",
      "Chenying Liu",
      "Zhitong Xiong",
      "Xiao Xiang Zhu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4237_ECCV_2024_paper.php": {
    "title": "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brandon McKinzie",
      "Zhe Gan",
      "Jean-Philippe Fauconnier",
      "Samuel Dodge",
      "Bowen Zhang",
      "Philipp Dufter",
      "Dhruti Shah",
      "Futang Peng",
      "Anton Belyi",
      "Max A Schwarzer",
      "Hongyu Hè",
      "Xianzhi Du",
      "Haotian Zhang",
      "Karanjeet Singh",
      "Doug Kang",
      "Tom Gunter",
      "Xiang Kong",
      "Aonan Zhang",
      "Jianyu Wang",
      "Chong Wang",
      "Nan Du",
      "Tao Lei",
      "Sam Wiseman",
      "Mark Lee",
      "Zirui Wang",
      "Ruoming Pang",
      "Peter Grasch",
      "Alexander Toshev*",
      "Yinfei Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4239_ECCV_2024_paper.php": {
    "title": "Optimizing Diffusion Models for Joint Trajectory Prediction and Controllable Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixiao Wang*",
      "Chen Tang",
      "Lingfeng Sun",
      "Simone Rossi",
      "Yichen Xie",
      "Chensheng Peng",
      "Thomas Hannagan",
      "Stefano Sabatini",
      "Nicola Poerio",
      "Masayoshi TOMIZUKA",
      "Wei Zhan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4249_ECCV_2024_paper.php": {
    "title": "2S-ODIS: Two-Stage Omni-Directional Image Synthesis by Geometric Distortion Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Atsuya Nakata*",
      "Takao Yamanaka*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4252_ECCV_2024_paper.php": {
    "title": "Open-Vocabulary 3D Semantic Segmentation with Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyu Zhu*",
      "Hao Zhou",
      "Pengfei Xing",
      "Long Zhao",
      "Hao Xu",
      "Junwei Liang",
      "Alexander G. Hauptmann",
      "Ting Liu",
      "Andrew Gallagher"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4261_ECCV_2024_paper.php": {
    "title": "D-SCo: Dual-Stream Conditional Diffusion for Monocular Hand-Held Object Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowen Fu*",
      "Gu Wang*",
      "Chenyangguang Zhang",
      "Yan Di",
      "Ziqin Huang",
      "Zhiying Leng",
      "Fabian Manhardt",
      "Xiangyang Ji*",
      "Federico Tombari*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4272_ECCV_2024_paper.php": {
    "title": "Combining Generative and Geometry Priors for Wide-Angle Portrait Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lan Yao",
      "Chaofeng Chen",
      "Xiaoming Li*",
      "Zifei Yan",
      "Wangmeng Zuo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4277_ECCV_2024_paper.php": {
    "title": "RealViformer: Investigating Attention for Real-World Video Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuehan Zhang*",
      "Angela Yao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4278_ECCV_2024_paper.php": {
    "title": "Pairwise Distance Distillation for Unsupervised Real-World Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuehan Zhang*",
      "Seungjun Lee",
      "Angela Yao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4294_ECCV_2024_paper.php": {
    "title": "Decomposed Vector-Quantized Variational Autoencoder for Human Grasp Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "zhao zhe*",
      "Mengshi Qi",
      "Huadong Ma"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4298_ECCV_2024_paper.php": {
    "title": "UniFS: Universal Few-shot Instance Perception with Point Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheng Jin*",
      "Ruijie Yao",
      "Lumin Xu",
      "Wentao Liu*",
      "Chen Qian",
      "Ji Wu",
      "Ping Luo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4301_ECCV_2024_paper.php": {
    "title": "SemanticHuman-HD: High Resolution Semantic disentangled 3D Human Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peng Zheng",
      "Tao Liu",
      "Zili Yi",
      "Rui Ma*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4306_ECCV_2024_paper.php": {
    "title": "CoherentGS: Sparse Novel View Synthesis with Coherent 3D Gaussians",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Avinash Paliwal*",
      "Wei Ye",
      "Jinhui Xiong",
      "Dmytro Kotovenko",
      "Rakesh Ranjan",
      "Vikas Chandra",
      "Nima Khademi Kalantari"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4310_ECCV_2024_paper.php": {
    "title": "Monocular Occupancy Prediction for Scalable Indoor Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongxiao Yu",
      "Yuqi Wang",
      "Yuntao Chen",
      "Zhaoxiang Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4314_ECCV_2024_paper.php": {
    "title": "Visual Grounding for Object-Level Generalization in Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haobin Jiang",
      "Zongqing Lu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4320_ECCV_2024_paper.php": {
    "title": "3DEgo: 3D Editing on the Go!",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Umar Khalid*",
      "Hasan Iqbal*",
      "Azib Farooq",
      "Jing Hua",
      "Chen Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4325_ECCV_2024_paper.php": {
    "title": "Efficient Depth-Guided Urban View Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "sheng miao*",
      "Jiaxin Huang",
      "Dongfeng Bai",
      "Weichao Qiu",
      "Liu Bingbing",
      "Andreas Geiger",
      "Yiyi Liao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4326_ECCV_2024_paper.php": {
    "title": "Probabilistic Weather Forecasting with Deterministic Guidance-based Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Donggeun Yoon",
      "Minseok Seo",
      "Doyi Kim",
      "Yeji Choi",
      "Donghyeon Cho*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4329_ECCV_2024_paper.php": {
    "title": "Domain-adaptive Video Deblurring via Test-time Blurring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jin-Ting He*",
      "Fu-Jen Tsai",
      "Jia-Hao Wu",
      "Yan-Tsung Peng",
      "Chung-Chi Tsai",
      "Chia-Wen Lin",
      "Yen-Yu Lin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4334_ECCV_2024_paper.php": {
    "title": "Representing Topological Self-Similarity Using Fractal Feature Maps for Accurate Segmentation of Tubular Structures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxing Huang",
      "Yanfeng Zhou",
      "Yaoru Luo",
      "Guole Liu",
      "Heng Guo",
      "Ge Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4335_ECCV_2024_paper.php": {
    "title": "NeuroNCAP: Photorealistic Closed-loop Safety Testing for Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "William Ljungbergh*",
      "Adam Tonderski",
      "Joakim Johnander",
      "Holger Caesar",
      "Kalle Åström",
      "Michael Felsberg",
      "Christoffer Petersson"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4338_ECCV_2024_paper.php": {
    "title": "OLAF: A Plug-and-Play Framework for Enhanced Multi-object Multi-part Scene Parsing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pranav Gupta*",
      "Rishubh Singh",
      "Pradeep Shenoy",
      "Ravi Kiran Sarvadevabhatla*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4345_ECCV_2024_paper.php": {
    "title": "Progressive Pretext Task Learning for Human Trajectory Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaotong Lin",
      "Tianming Liang",
      "Jianhuang Lai",
      "Jian-Fang Hu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4347_ECCV_2024_paper.php": {
    "title": "Hyperion – A fast, versatile symbolic Gaussian Belief Propagation framework for Continuous-Time SLAM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Hug*",
      "Ignacio Alzugaray",
      "Margarita Chli"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4364_ECCV_2024_paper.php": {
    "title": "Isomorphic Pruning for Vision Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gongfan Fang*",
      "Xinyin Ma",
      "Michael Bi Mi",
      "Xinchao Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4374_ECCV_2024_paper.php": {
    "title": "Attention Prompting on Image for Large Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runpeng Yu*",
      "Weihao Yu*",
      "Xinchao Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4377_ECCV_2024_paper.php": {
    "title": "Learning Cross-hand Policies of High-DOF Reaching and Grasping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qijin She",
      "Shishun Zhang",
      "Yunfan Ye",
      "Ruizhen Hu",
      "Kai Xu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4383_ECCV_2024_paper.php": {
    "title": "Reprojection Errors as Prompts for Efficient Scene Coordinate Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ting-Ru Liu*",
      "Hsuan-Kung Yang",
      "Jou-Min Liu",
      "Chun-Wei Huang",
      "Tsung-Chih Chiang",
      "Quan Kong",
      "Norimasa Kobori",
      "Chun-Yi Lee"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4384_ECCV_2024_paper.php": {
    "title": "Diffusion-Driven Data Replay: A Novel Approach to Combat Forgetting in Federated Class Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinglin Liang",
      "Jin Zhong",
      "Hanlin Gu",
      "Zhongqi Lu",
      "Xingxing Tang",
      "Gang Dai",
      "Shuangping Huang*",
      "Lixin Fan",
      "Qiang Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4389_ECCV_2024_paper.php": {
    "title": "Long-Tail Temporal Action Segmentation with Group-wise Temporal Logit Adjustment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhanzhong Pang*",
      "Fadime Sener",
      "Shrinivas Ramasubramanian",
      "Angela Yao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4416_ECCV_2024_paper.php": {
    "title": "REVISION: Rendering Tools Enable Spatial Fidelity in Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Agneet Chatterjee*",
      "Yiran Luo",
      "Tejas Gokhale",
      "Yezhou Yang",
      "Chitta R Baral"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4429_ECCV_2024_paper.php": {
    "title": "DreamMotion: Space-Time Self-Similar Score Distillation for Zero-Shot Video Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyeonho Jeong",
      "Jinho Chang",
      "Geon Yeong Park",
      "Jong Chul Ye*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4432_ECCV_2024_paper.php": {
    "title": "VideoClusterNet: Self-Supervised and Adaptive Face Clustering for Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Devesh Walawalkar*",
      "Pablo Garrido"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4439_ECCV_2024_paper.php": {
    "title": "Unveiling Privacy Risks in Stochastic Neural Networks Training: Effective Image Reconstruction from Gradients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiming Chen*",
      "Xiangyu Yang",
      "Nikos Deligiannis"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4442_ECCV_2024_paper.php": {
    "title": "Controlling the World by Sleight of Hand",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sruthi Sudhakar*",
      "Ruoshi Liu",
      "Basile Van Hoorick",
      "Carl Vondrick",
      "Richard Zemel"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4444_ECCV_2024_paper.php": {
    "title": "Hiding Imperceptible Noise in Curvature-Aware Patches for 3D Point Cloud Attack",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingyu Yang*",
      "Daizong Liu",
      "Keke Tang",
      "Pan Zhou",
      "Lixing Chen",
      "Junyang Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4450_ECCV_2024_paper.php": {
    "title": "Interleaving One-Class and Weakly-Supervised Models with Adaptive Thresholding for Unsupervised Video Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongwei Nie",
      "Hao Huang",
      "Chengjiang Long",
      "Qing Zhang",
      "Pradipta Maji",
      "Hongmin Cai*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4459_ECCV_2024_paper.php": {
    "title": "Cross-Domain Learning for Video Anomaly Detection with Limited Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yashika Jain",
      "Ali Dabouei*",
      "Min Xu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4462_ECCV_2024_paper.php": {
    "title": "YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chien-Yao Wang*",
      "I-Hau Yeh",
      "Hong-Yuan Mark Liao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4467_ECCV_2024_paper.php": {
    "title": "Unsupervised Multi-modal Medical Image Registration via Invertible Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengjie Guo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4468_ECCV_2024_paper.php": {
    "title": "Functional Transform-Based Low-Rank Tensor Factorization for Multi-Dimensional Data Recovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian-Li Wang",
      "Xi-Le Zhao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4484_ECCV_2024_paper.php": {
    "title": "CRM: Single Image to 3D Textured Mesh with Convolutional Reconstruction Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengyi Wang*",
      "Yikai Wang",
      "Yifei Chen",
      "Chendong Xiang",
      "Shuo Chen",
      "Dajiang Yu",
      "Chongxuan Li",
      "Hang Su",
      "Jun Zhu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4491_ECCV_2024_paper.php": {
    "title": "Domain Reduction Strategy for Non-Line-of-Sight Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyunbo Shim",
      "In Cho",
      "Daekyu Kwon",
      "Seon Joo Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4496_ECCV_2024_paper.php": {
    "title": "HPE-Li: WiFi-enabled Lightweight Dual Selective Kernel Convolution for Human Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Toan D. Gian",
      "Tien Dac Lai",
      "Thien Van Luong",
      "Kok-Seng Wong",
      "Van-Dinh Nguyen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4501_ECCV_2024_paper.php": {
    "title": "Cut out the Middleman: Revisiting Pose-based Gait Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Fu",
      "Saihui Hou*",
      "Shibei Meng",
      "Xuecai Hu*",
      "Chunshui Cao",
      "Xu Liu",
      "Yongzhen Huang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4514_ECCV_2024_paper.php": {
    "title": "HiEI: A Universal Framework for Generating High-quality Emerging Images from Natural Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingmeng Li",
      "Lukang Fu",
      "Surun Yang",
      "Hui Wei*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4515_ECCV_2024_paper.php": {
    "title": "High-Precision Self-Supervised Monocular Depth Estimation with Rich-Resource Prior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianbing Shen*",
      "Wencheng Han"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4516_ECCV_2024_paper.php": {
    "title": "SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingrui Li",
      "Shuhong Liu",
      "Heng Zhou",
      "Guohao Zhu",
      "Na Cheng",
      "Tianchen Deng",
      "Hongyu Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4528_ECCV_2024_paper.php": {
    "title": "View Selection for 3D Captioning via Diffusion Ranking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tiange Luo*",
      "Justin Johnson",
      "Honglak Lee"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4529_ECCV_2024_paper.php": {
    "title": "OmniSSR: Zero-shot Omnidirectional Image Super-Resolution using Stable Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runyi Li*",
      "Xuhan Sheng",
      "Weiqi Li",
      "Jian Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4534_ECCV_2024_paper.php": {
    "title": "UDiffText: A Unified Framework for High-quality Text Synthesis in Arbitrary Images via Character-aware Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiming Zhao*",
      "Zhouhui Lian*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4537_ECCV_2024_paper.php": {
    "title": "Confidence Self-Calibration for Multi-Label Class-Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaile Du*",
      "Yifan Zhou",
      "Fan Lyu",
      "Yuyang Li",
      "Chen Lu",
      "Guangcan Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4539_ECCV_2024_paper.php": {
    "title": "OMG: Occlusion-friendly Personalized Multi-concept Generation in Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhe Kong*",
      "Yong Zhang*",
      "Tianyu Yang",
      "Tao Wang",
      "Kaihao Zhang",
      "Bizhu Wu",
      "Guanying Chen",
      "Wei Liu",
      "Wenhan Luo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4550_ECCV_2024_paper.php": {
    "title": "Versatile Incremental Learning: Towards Class and Domain-Agnostic Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Min-Yeong Park",
      "Jae-Ho Lee",
      "Gyeong-Moon Park*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4553_ECCV_2024_paper.php": {
    "title": "WeCromCL: Weakly Supervised Cross-Modality Contrastive Learning for Transcription-only Supervised Text Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingjing Wu",
      "Zhengyao Fang",
      "Pengyuan Lyu",
      "Chengquan Zhang",
      "Fanglin Chen",
      "Guangming Lu",
      "Wenjie Pei*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4569_ECCV_2024_paper.php": {
    "title": "An Incremental Unified Framework for Small Defect Inspection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqi Tang",
      "Hao Lu",
      "Xiaogang Xu",
      "Ruizheng Wu",
      "Sixing Hu",
      "Tong Zhang",
      "Tsz Wa Cheng",
      "Ming Ge",
      "Ying-Cong Chen*",
      "Fugee Tsung"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4573_ECCV_2024_paper.php": {
    "title": "Enhancing Optimization Robustness in 1-bit Neural Networks through Stochastic Sign Descent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "NianHui Guo*",
      "Hong Guo",
      "Christoph Meinel",
      "Haojin Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4579_ECCV_2024_paper.php": {
    "title": "Temporally Consistent Stereo Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxi Zeng*",
      "Chengtang Yao",
      "Yuwei Wu*",
      "Yunde Jia"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4582_ECCV_2024_paper.php": {
    "title": "A Rotation-invariant Texture ViT for Fine-Grained Recognition of Esophageal Cancer Endoscopic Ultrasound Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyi Liu",
      "Shuaishuai S Zhuang",
      "Jiacheng Nie",
      "Geng Chen ",
      "Yusheng Guo",
      "Guangquan Zhou*",
      "Jean-Louis Coatrieux",
      "Yang Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4591_ECCV_2024_paper.php": {
    "title": "BI-MDRG: Bridging Image History in Multimodal Dialogue Response Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hee Suk Yoon",
      "Eunseop Yoon",
      "Joshua Tian Jin Tee",
      "Kang Zhang",
      "Yu-Jung Heo",
      "Du-Seong Chang",
      "Chang D. Yoo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4592_ECCV_2024_paper.php": {
    "title": "Adapting Fine-Grained Cross-View Localization to Areas without Fine Ground Truth",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zimin Xia*",
      "Yujiao Shi",
      "Hongdong Li",
      "Julian F. P. Kooij"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4596_ECCV_2024_paper.php": {
    "title": "BeNeRF:Neural Radiance Fields from a Single Blurry Image and Event Stream",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenpu Li",
      "Pian Wan",
      "Peng Wang",
      "Jinghang Li",
      "Yi Zhou",
      "Peidong Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4599_ECCV_2024_paper.php": {
    "title": "Human Motion Forecasting in Dynamic Domain Shifts: A Homeostatic Continual Test-time Adaptation Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiongjie Cui*",
      "Huaijiang Sun",
      "Bin Li",
      "Jianfeng Lu",
      "Weiqing Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4603_ECCV_2024_paper.php": {
    "title": "CloudFixer: Test-Time Adaptation for 3D Point Clouds via Diffusion-Guided Geometric Transformation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hajin Shim",
      "Changhun Kim",
      "Eunho Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4605_ECCV_2024_paper.php": {
    "title": "DreamDiffusion: High-Quality EEG-to-Image Generation with Temporal Masked Signal Modeling and CLIP Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunpeng Bai*",
      "Xintao Wang",
      "Yan-Pei Cao",
      "Yixiao Ge",
      "Chun Yuan",
      "Ying Shan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4606_ECCV_2024_paper.php": {
    "title": "FRI-Net: Floorplan Reconstruction via Room-wise Implicit Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Honghao Xu",
      "Juzhan Xu",
      "Zeyu Huang",
      "Pengfei Xu",
      "Hui Huang",
      "Ruizhen Hu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4613_ECCV_2024_paper.php": {
    "title": "BugNIST - a Large Volumetric Dataset for Detection under Domain Shift",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Patrick M Jensen",
      "Vedrana A Dahl",
      "Rebecca Engberg",
      "Carsten Gundlach",
      "Hans Martin Kjer",
      "Anders B Dahl*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4623_ECCV_2024_paper.php": {
    "title": "SCP-Diff: Spatial-Categorical Joint Prior for Diffusion Based Semantic Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huan-ang Gao",
      "Mingju Gao",
      "Jiaju Li",
      "Wenyi Li",
      "Rong Zhi",
      "Hao Tang",
      "Hao Zhao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4630_ECCV_2024_paper.php": {
    "title": "PoseAugment: Generative Human Pose Data Augmentation with Physical Plausibility for IMU-based Motion Capture",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuojun Li*",
      "Chun Yu*",
      "Chen Liang",
      "Yuanchun Shi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4633_ECCV_2024_paper.php": {
    "title": "PixArt-Sigma: Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junsong Chen",
      "Chongjian GE",
      "Enze Xie*",
      "Yue Wu",
      "Lewei Yao",
      "Xiaozhe Ren",
      "Zhongdao Wang",
      "Ping Luo",
      "Huchuan Lu",
      "Zhenguo Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4634_ECCV_2024_paper.php": {
    "title": "Hierarchical Gaussian Mixture Normalizing Flow Modeling for Unified Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xincheng Yao*",
      "Ruoqi Li",
      "Zefeng Qian",
      "lu wang",
      "Chongyang Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4642_ECCV_2024_paper.php": {
    "title": "A Closer Look at GAN Priors: Exploiting Intermediate Features for Enhanced Model Inversion Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixiang Qiu*",
      "Hao Fang",
      "Hongyao Yu",
      "Bin Chen*",
      "Meikang Qiu",
      "Shu-Tao Xia"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4650_ECCV_2024_paper.php": {
    "title": "Improving Unsupervised Domain Adaptation: A Pseudo-Candidate Set Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aveen Dayal*",
      "Rishabh Lalla",
      "Linga Reddy Cenkeramaddi",
      "C. Krishna Mohan",
      "Abhinav Kumar",
      "Vineeth N Balasubramanian"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4681_ECCV_2024_paper.php": {
    "title": "HeadStudio: Text to Animatable Head Avatars with 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenglin Zhou*",
      "Fan Ma",
      "Hehe Fan",
      "Zongxin Yang",
      "Yi Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4682_ECCV_2024_paper.php": {
    "title": "DetToolChain: A New Prompting Paradigm to Unleash Detection Ability of MLLM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixuan Wu*",
      "Yizhou Wang",
      "Shixiang Tang",
      "Wenhao Wu",
      "Tong He",
      "Wanli Ouyang",
      "Philip Torr",
      "Jian Wu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4691_ECCV_2024_paper.php": {
    "title": "Surface-Centric Modeling for High-Fidelity Generalizable Neural Surface Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Peng",
      "Shihe Shen",
      "Kaiqiang Xiong",
      "Huachen Gao",
      "Jianbo Jiao",
      "Xiaodong Gu",
      "Ronggang Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4696_ECCV_2024_paper.php": {
    "title": "HumanRefiner: Benchmarking Abnormal Human Generation and Refining with Coarse-to-fine Pose-Reversible Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guian Fang*",
      "Wenbiao Yan",
      "Yuanfan Guo",
      "Jianhua Han",
      "Zutao Jiang",
      "Hang Xu",
      "Shengcai Liao",
      "Xiaodan Liang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4702_ECCV_2024_paper.php": {
    "title": "Multiscale Graph Texture Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ravishankar Evani*",
      "Deepu Rajan",
      "Shangbo Mao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4703_ECCV_2024_paper.php": {
    "title": "HyTAS: A Hyperspectral Image Transformer Architecture Search Benchmark and Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangqin Zhou*",
      "Mert Kilickaya",
      "Joaquin Vanschoren",
      "Ran Piao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4704_ECCV_2024_paper.php": {
    "title": "Integer-Valued Training and Spike-driven Inference Spiking Neural Network for High-performance and Energy-efficient Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinhao Luo",
      "Man Yao",
      "Yuhong Chou",
      "Bo Xu",
      "Guoqi Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4706_ECCV_2024_paper.php": {
    "title": "RepVF: A Unified Vector Fields Representation for Multi-task 3D Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianbing Shen",
      "Chunliang Li",
      "Wencheng Han",
      "Junbo Yin",
      "Sanyuan Zhao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4729_ECCV_2024_paper.php": {
    "title": "Phase Concentration and Shortcut Suppression for Weakly Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hoyong Kwon",
      "Jaeseok Jeong",
      "Sung-Hoon Yoon",
      "Kuk-Jin Yoon*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4734_ECCV_2024_paper.php": {
    "title": "Group Testing for Accurate and Efficient Range-Based Near Neighbor Search for Plagiarism Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Harsh Shah*",
      "Kashish Mittal",
      "Ajit Rajwade*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4735_ECCV_2024_paper.php": {
    "title": "CompGS: Smaller and Faster Gaussian Splatting with Vector Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "K L Navaneet*",
      "Kossar Pourahmadi Meibodi",
      "Soroush Abbasi Koohpayegani",
      "Hamed Pirsiavash"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4743_ECCV_2024_paper.php": {
    "title": "SMILe: Leveraging Submodular Mutual Information For Robust Few-Shot Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anay Majee*",
      "Ryan X Sharp",
      "Rishabh Iyer*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4746_ECCV_2024_paper.php": {
    "title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixuan Ren*",
      "Yang Zhou",
      "Jimei Yang",
      "Jing Shi",
      "Difan Liu",
      "Feng Liu",
      "Mingi Kwon",
      "Abhinav Shrivastava"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4755_ECCV_2024_paper.php": {
    "title": "S-JEPA: A Joint Embedding Predictive Architecture for Skeletal Action Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohamed Abdelfattah*",
      "Alexandre Alahi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4761_ECCV_2024_paper.php": {
    "title": "∞-Brush: Controllable Large Image Synthesis with Diffusion Models in Infinite Dimensions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minh-Quan Le*",
      "Alexandros Graikos",
      "Srikar Yellapragada",
      "Rajarsi Gupta",
      "Joel Saltz",
      "Dimitris Samaras"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4768_ECCV_2024_paper.php": {
    "title": "SwapAnything: Enabling Arbitrary Object Swapping in Personalized Image Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Gu*",
      "Nanxuan Zhao",
      "Wei Xiong",
      "Qing Liu",
      "Zhifei Zhang",
      "He Zhang",
      "Jianming Zhang",
      "HyunJoon Jung",
      "Yilin Wang*",
      "Xin Eric Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4769_ECCV_2024_paper.php": {
    "title": "Interaction-centric Spatio-Temporal Context Reasoning for Multi-Person Video HOI Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yisong Wang",
      "Nan Xi*",
      "Jingjing Meng",
      "Junsong Yuan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4780_ECCV_2024_paper.php": {
    "title": "Efficient Unsupervised Visual Representation Learning with Explicit Cluster Balancing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ioannis Maniadis Metaxas*",
      "Georgios Tzimiropoulos",
      "Ioannis Patras"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4783_ECCV_2024_paper.php": {
    "title": "ProTIP: Probabilistic Robustness Verification on Text-to-Image Diffusion Models against Stochastic Perturbation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Zhang",
      "Yun Tang",
      "Wenjie Ruan",
      "Xiaowei Huang",
      "Siddartha Khastgir",
      "Paul A Jennings",
      "Xingyu Zhao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4784_ECCV_2024_paper.php": {
    "title": "Leveraging Near-Field Lighting for Monocular Depth Estimation from Endoscopy Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akshay Paruchuri*",
      "Samuel Ehrenstein",
      "Shuxian Wang",
      "Inbar Fried",
      "Stephen Pizer",
      "Marc Niethammer",
      "Roni Sengupta"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4787_ECCV_2024_paper.php": {
    "title": "OvSW: Overcoming Silent Weights for Accurate Binary Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "jingyang xiang*",
      "Zuohui Chen",
      "Siqi Li",
      "Qing Wu",
      "Yong Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4788_ECCV_2024_paper.php": {
    "title": "Multistain Pretraining for Slide Representation Learning in Pathology",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guillaume Jaume*",
      "Anurag J Vaidya*",
      "Andrew Zhang",
      "Andrew Song",
      "Richard J Chen",
      "Sharifa Sahai",
      "Dandan Mo",
      "Emilio Madrigal",
      "Long P Le",
      "Faisal Mahmood*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4804_ECCV_2024_paper.php": {
    "title": "T-Rex2: Towards Generic Object Detection via Text-Visual Prompt Synergy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qing Jiang*",
      "Feng Li",
      "Zhaoyang Zeng",
      "Shilong Liu",
      "Tianhe Ren",
      "Lei Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4805_ECCV_2024_paper.php": {
    "title": "Harmonizing knowledge Transfer in Neural Network with Unified Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "yaomin huang",
      "Faming Fang",
      "Zaoming Yan",
      "Chaomin Shen",
      "Guixu Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4811_ECCV_2024_paper.php": {
    "title": "Mamba-ND: Selective State Space Modeling for Multi-Dimensional Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shufan Li*",
      "Aditya Grover",
      "Harkanwar Singh"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4816_ECCV_2024_paper.php": {
    "title": "Click Prompt Learning with Optimal Transport for Interactive Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Liu*",
      "Haochen wang",
      "Wenzhe Yin",
      "Jan-Jakob Sonke",
      "Efstratios Gavves"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4820_ECCV_2024_paper.php": {
    "title": "3D Human Pose Estimation via Non-Causal Retentive Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaili Zheng",
      "Feixiang Lu",
      "Yihao Lv",
      "Liangjun Zhang",
      "Chenyi Guo*",
      "Ji Wu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4822_ECCV_2024_paper.php": {
    "title": "OMR: Occlusion-Aware Memory-Based Refinement for Video Lane Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongkwon Jin",
      "Chang-Su Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4824_ECCV_2024_paper.php": {
    "title": "6DoF Head Pose Estimation through Explicit Bidirectional Interaction with Face Geometry",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sungho Chun",
      "Ju Yong Chang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4825_ECCV_2024_paper.php": {
    "title": "Latent Diffusion Prior Enhanced Deep Unfolding for Snapshot Spectral Compressive Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zongliang Wu*",
      "Ruiying Lu",
      "Ying Fu",
      "Xin Yuan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4830_ECCV_2024_paper.php": {
    "title": "Multimodal Cross-Domain Few-Shot Learning for Egocentric Action Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Masashi Hatano*",
      "Ryo Hachiuma",
      "Ryo Fujii",
      "Hideo Saito"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4834_ECCV_2024_paper.php": {
    "title": "Enhancing Tampered Text Detection through Frequency Feature Fusion and Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongxi Chen",
      "Shen Chen",
      "Taiping Yao*",
      "Ke Sun",
      "Shouhong Ding",
      "Xianming Lin*",
      "Liujuan Cao",
      "Rongrong Ji"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4855_ECCV_2024_paper.php": {
    "title": "Modeling Label Correlations with Latent Context for Multi-Label Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaomin Chen*",
      "Quan Cui",
      "Ruoxi Deng",
      "Jie Hu",
      "Guodao Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4858_ECCV_2024_paper.php": {
    "title": "LLM as Dataset Analyst: Subpopulation Structure Discovery with Large Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yulin Luo",
      "Ruichuan An",
      "Bocheng Zou",
      "Yiming Tang",
      "Jiaming Liu",
      "Shanghang Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4872_ECCV_2024_paper.php": {
    "title": "Finding a needle in a haystack: A Black-Box Approach to Invisible Watermark Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minzhou Pan*",
      "Zhenting Wang",
      "Xin Dong",
      "Vikash Sehwag",
      "Lingjuan Lyu",
      "Xue Lin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4886_ECCV_2024_paper.php": {
    "title": "DynoSurf: Neural Deformation-based Temporally Consistent Dynamic Surface Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxin Yao",
      "Siyu Ren",
      "Junhui Hou*",
      "Zhi Deng",
      "Juyong Zhang",
      "Wenping Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4888_ECCV_2024_paper.php": {
    "title": "MOD-UV: Learning Mobile Object Detectors from Unlabeled Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihong Sun*",
      "Bharath Hariharan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4890_ECCV_2024_paper.php": {
    "title": "ARoFace: Alignment Robustness to Improve Low-quality Face Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Saeed Ebrahimi Saadabadi*",
      "Sahar Rahimi Malakshan",
      "Ali Dabouei",
      "Nasser Nasrabadi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4907_ECCV_2024_paper.php": {
    "title": "Learning Diffusion Models for Multi-View Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chieh Liu*",
      "Yu-Min Chu*",
      "Ting-I Hsieh*",
      "Hwann-Tzong Chen*",
      "Tyng-Luh Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4908_ECCV_2024_paper.php": {
    "title": "Clearer Frames, Anytime: Resolving Velocity Ambiguity in Video Frame Interpolation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihang Zhong",
      "Gurunandan Krishnan",
      "Xiao Sun",
      "Yu Qiao",
      "Sizhuo Ma*",
      "Jian Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4910_ECCV_2024_paper.php": {
    "title": "Multi-modal Relation Distillation for Unified 3D Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huiqun Wang",
      "Yiping Bao",
      "Panwang Pan",
      "Zeming Li",
      "Xiao Liu",
      "Ruijie Yang",
      "Di Huang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4914_ECCV_2024_paper.php": {
    "title": "Strengthening Multimodal Large Language Model with Bootstrapped Preference Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Renjie Pi*",
      "Tianyang Han",
      "Wei Xiong",
      "Jipeng ZHANG",
      "Runtao Liu",
      "Rui Pan",
      "Tong Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4917_ECCV_2024_paper.php": {
    "title": "Collaborative Vision-Text Representation Optimizing for Open-Vocabulary Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyu Jiao*",
      "hongguang Zhu",
      "Yunchao Wei",
      "Yao Zhao*",
      "Jiannan Huang",
      "Humphrey Shi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4926_ECCV_2024_paper.php": {
    "title": "Distributionally Robust Loss for Long-Tailed Multi-Label Image Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dekun Lin*",
      "Zhe Cui",
      "Rui Chen",
      "Tailai Peng",
      "xinran xie",
      "Xiaolin Qin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4927_ECCV_2024_paper.php": {
    "title": "MesonGS: Post-training Compression of 3D Gaussians via Efficient Attribute Transformation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuzhao Xie*",
      "Weixiang Zhang",
      "Chen Tang",
      "Yunpeng Bai",
      "Rongwei Lu",
      "Shjia Ge",
      "Zhi Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4936_ECCV_2024_paper.php": {
    "title": "LongVLM: Efficient Long Video Understanding via Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuetian Weng",
      "Mingfei Han",
      "Haoyu He",
      "Xiaojun Chang",
      "Bohan Zhuang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4939_ECCV_2024_paper.php": {
    "title": "The All-Seeing Project V2: Towards General Relation Comprehension of the Open World",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiyun Wang",
      "yiming ren",
      "Haowen Luo",
      "Tiantong Li",
      "Chenxiang Yan",
      "Zhe Chen",
      "Wenhai Wang",
      "Qingyun Li",
      "Lewei Lu",
      "Xizhou Zhu",
      "Yu Qiao",
      "Jifeng Dai*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4943_ECCV_2024_paper.php": {
    "title": "Neural Metamorphosis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyi Yang*",
      "Xinchao Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4945_ECCV_2024_paper.php": {
    "title": "WHAC: World-grounded Humans and Cameras",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wanqi Yin",
      "Zhongang Cai",
      "Chen Wei",
      "Fanzhou Wang",
      "Ruisi Wang",
      "Haiyi Mei",
      "Weiye Xiao",
      "Zhitao Yang",
      "Qingping Sun",
      "Atsushi Yamashita",
      "Ziwei Liu",
      "Lei Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4952_ECCV_2024_paper.php": {
    "title": "Federated Learning with Local Openset Noisy Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zonglin Di*",
      "Zhaowei Zhu",
      "Xiaoxiao Li",
      "Yang Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4960_ECCV_2024_paper.php": {
    "title": "Diff3DETR: Agent-based Diffusion Model for Semi-supervised 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiacheng Deng*",
      "Jiahao Lu",
      "Tianzhu Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4964_ECCV_2024_paper.php": {
    "title": "PSALM: Pixelwise Segmentation with Large Multi-modal Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheng Zhang",
      "yeyao ma",
      "Enming Zhang",
      "Xiang Bai*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4969_ECCV_2024_paper.php": {
    "title": "Layout-Corrector: Alleviating Layout Sticking Phenomenon in Discrete Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shoma Iwai*",
      "Atsuki Osanai",
      "Shunsuke Kitada",
      "Shinichiro Omachi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4970_ECCV_2024_paper.php": {
    "title": "Active Coarse-to-Fine Segmentation of Moveable Parts from Real Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruiqi Wang*",
      "Akshay Gadi Patil",
      "Fenggen Yu",
      "Hao Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4975_ECCV_2024_paper.php": {
    "title": "Topo4D: Topology-Preserving Gaussian Splatting for High-Fidelity 4D Head Capture",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuanchen Li",
      "Yuhao Cheng",
      "Xingyu Ren",
      "Haozhe Jia",
      "Di Xu",
      "Wenhan Zhu",
      "Yichao Yan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4978_ECCV_2024_paper.php": {
    "title": "Learning Modality-agnostic Representation for Semantic Segmentation from Any Modalities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xu Zheng*",
      "Yuanhuiyi Lyu",
      "Lin Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5000_ECCV_2024_paper.php": {
    "title": "Kinetic Typography Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seonmi Park",
      "Inhwan Bae",
      "Seunghyun Shin",
      "Hae-Gon Jeon*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5008_ECCV_2024_paper.php": {
    "title": "Refine, Discriminate and Align: Stealing Encoders via Sample-Wise Prototypes and Multi-Relational Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuchi Wu*",
      "Chuan Ma*",
      "Kang Wei*",
      "Xiaogang XU",
      "Ming Ding",
      "Yuwen Qian",
      "Di Xiao",
      "Tao Xiang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5009_ECCV_2024_paper.php": {
    "title": "Light-in-Flight for a World-in-Motion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jongho Lee*",
      "Ryan J Suess",
      "Mohit Gupta"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5013_ECCV_2024_paper.php": {
    "title": "GroupDiff: Diffusion-based Group Portrait Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuming Jiang",
      "Nanxuan Zhao*",
      "Qing Liu",
      "Krishna Kumar Singh",
      "Shuai Yang",
      "Chen Change Loy",
      "Ziwei Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5019_ECCV_2024_paper.php": {
    "title": "Faceptor: A Generalist Model for Face Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lixiong Qin*",
      "Mei Wang",
      "Xuannan Liu",
      "Yuhang Zhang",
      "Wei Deng",
      "Xiaoshuai Song",
      "Weiran Xu*",
      "Weihong Deng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5026_ECCV_2024_paper.php": {
    "title": "Inter-Class Topology Alignment for Efficient Black-Box Substitute Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingzhuang Meng",
      "Mingwen Shao*",
      "Yuanjian Qiao",
      "Wenjie Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5027_ECCV_2024_paper.php": {
    "title": "Segment3D: Learning Fine-Grained Class-Agnostic 3D Segmentation without Manual Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Huang",
      "Songyou Peng",
      "Ayca Takmaz",
      "Federico Tombari",
      "Marc Pollefeys",
      "Shiji Song",
      "Gao Huang*",
      "Francis Engelmann"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5030_ECCV_2024_paper.php": {
    "title": "InsMapper: Exploring Inner-instance Information for Vectorized HD Mapping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "zhenhua xu*",
      "Kwan-Yee K. Wong",
      "Hengshuang Zhao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5033_ECCV_2024_paper.php": {
    "title": "KDProR: A Knowledge-Decoupling Probabilistic Framework for Video-Text Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xianwei Zhuang*",
      "Hongxiang Li",
      "Xuxin Cheng",
      "Zhihong Zhu",
      "Yuxin Xie",
      "Yuexian Zou"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5039_ECCV_2024_paper.php": {
    "title": "Category-level Object Detection, Pose Estimation and Reconstruction from Stereo Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuanrui Zhang*",
      "Yonggen Ling*",
      "Minglei Lu",
      "Minghan Qin",
      "Haoqian Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5042_ECCV_2024_paper.php": {
    "title": "Learning with Unmasked Tokens Drives Stronger Vision Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taekyung Kim*",
      "Sanghyuk Chun",
      "Byeongho Heo",
      "Dongyoon Han*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5047_ECCV_2024_paper.php": {
    "title": "Dual-stage Hyperspectral Image Classification Model with Spectral Supertoken",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peifu Liu",
      "Tingfa Xu*",
      "Jie Wang",
      "Huan Chen",
      "Huiyan Bai",
      "Jianan Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5059_ECCV_2024_paper.php": {
    "title": "Multi-Task Domain Adaptation for Language Grounding with 3D Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Penglei Sun",
      "Yaoxian Song",
      "Xinglin Pan",
      "Peijie Dong",
      "Xiaofei Yang",
      "Qiang Wang*",
      "Zhixu Li",
      "Tiefeng Li",
      "Xiaowen Chu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5060_ECCV_2024_paper.php": {
    "title": "Efficient Active Domain Adaptation for Semantic Segmentation by Selecting Information-rich Superpixels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Gao",
      "Zilei Wang*",
      "Yixin Zhang",
      "Bohai Tu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5068_ECCV_2024_paper.php": {
    "title": "Efficient Training of Spiking Neural Networks with Multi-Parallel Implicit Stream Architecture",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhigao Cao",
      "Meng Li",
      "Xiashuang Wang",
      "Haoyu Wang",
      "Fan Wang",
      "Youjun Li",
      "Zigang Huang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5072_ECCV_2024_paper.php": {
    "title": "Camera-LiDAR Cross-modality Gait Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenxuan Guo*",
      "Yingping Liang",
      "Zhiyu Pan",
      "Ziheng Xi",
      "Jianjiang Feng",
      "Jie Zhou"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5077_ECCV_2024_paper.php": {
    "title": "LiteSAM is Actually what you Need for segment Everything",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianhai Fu",
      "Yuanjie Yu",
      "Ningchuan Li*",
      "Yi Zhang",
      "Qichao Chen",
      "Jianping Xiong",
      "Jun Yin",
      "Zhiyu Xiang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5081_ECCV_2024_paper.php": {
    "title": "IGNORE: Information Gap-based False Negative Loss Rejection for Single Positive Multi-Label Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gyeong Ryeol Song",
      "Noo-ri Kim",
      "Jin-Seop Lee",
      "Jee-Hyong Lee*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5082_ECCV_2024_paper.php": {
    "title": "Visual Prompting via Partial Optimal Transport",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengyu Zheng*",
      "Zhiwei Hao",
      "Yehui Tang",
      "Chang Xu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5085_ECCV_2024_paper.php": {
    "title": "Modelling Competitive Behaviors in Autonomous Driving Under Generative World Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guanren Qiao",
      "Guiliang Liu*",
      "Guorui Quan",
      "Rongxiao Qu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5086_ECCV_2024_paper.php": {
    "title": "Tendency-driven Mutual Exclusivity for Weakly Supervised Incremental Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chongjie Si",
      "Xuehui Wang",
      "Xiaokang Yang",
      "Wei Shen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5096_ECCV_2024_paper.php": {
    "title": "AdaCLIP: Adapting CLIP with Hybrid Learnable Prompts for Zero-Shot Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunkang Cao*",
      "Jiangning Zhang",
      "Luca Frittoli",
      "Yuqi Cheng",
      "Weiming Shen*",
      "Giacomo Boracchi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5104_ECCV_2024_paper.php": {
    "title": "Pathformer3D: A 3D Scanpath Transformer for 360° Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rong Quan",
      "yantao Lai",
      "Mengyu Qiu",
      "Dong Liang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5117_ECCV_2024_paper.php": {
    "title": "TransFusion -- A Transparency-Based Diffusion Model for Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matic Fučka*",
      "Vitjan Zavrtanik",
      "Danijel Skočaj"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5130_ECCV_2024_paper.php": {
    "title": "SparseLIF: High-Performance Sparse LiDAR-Camera Fusion for 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongcheng Zhang",
      "Liu Liang",
      "Pengxin Zeng*",
      "Xiao Song",
      "Zhe Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5133_ECCV_2024_paper.php": {
    "title": "3D Gaussian Parametric Head Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuelang Xu",
      "Lizhen Wang",
      "Zerong Zheng",
      "Zhaoqi Su",
      "Yebin Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5141_ECCV_2024_paper.php": {
    "title": "RING-NeRF : Rethinking Inductive Biases for Versatile and Efficient Neural Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Doriand Petit*",
      "Steve Bourgeois",
      "Dumitru Pavel",
      "Vincent Gay-Bellile",
      "Florian Chabot",
      "Loïc Barthe"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5143_ECCV_2024_paper.php": {
    "title": "Platypus: A Generalized Specialist Model for Reading Text in Various Forms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peng Wang",
      "Zhaohai Li",
      "Jun Tang",
      "Humen Zhong",
      "Fei Huang",
      "Zhibo Yang*",
      "Cong Yao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5154_ECCV_2024_paper.php": {
    "title": "Structured-NeRF: Hierarchical Scene Graph with Neural Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhide Zhong",
      "Jiakai Cao",
      "songen gu",
      "Sirui Xie",
      "Liyi Luo",
      "Hao Zhao",
      "Guyue Zhou",
      "Haoang Li",
      "Zike Yan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5155_ECCV_2024_paper.php": {
    "title": "EGIC: Enhanced Low-Bit-Rate Generative Image Compression Guided by Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikolai Körber*",
      "Eduard Kromer",
      "Andreas Siebert",
      "Sascha Hauke",
      "Daniel Mueller-Gritschneder",
      "Björn Schuller"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5164_ECCV_2024_paper.php": {
    "title": "Plug-and-Play Learned Proximal Trajectory for 3D Sparse-View X-Ray Computed Tomography",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Romain Vo*",
      "Julie Escoda",
      "Caroline Vienne",
      "Etienne Decenciere"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5165_ECCV_2024_paper.php": {
    "title": "PPAD: Iterative Interactions of Prediction and Planning for End-to-end Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhili Chen",
      "Maosheng Ye",
      "Shuangjie Xu",
      "Tongyi Cao",
      "Qifeng Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5175_ECCV_2024_paper.php": {
    "title": "Test-Time Stain Adaptation with Diffusion Models for Histopathology Image Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng-Chang Tsai*",
      "Yuan-Chih Chen",
      "Chun-Shien Lu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5182_ECCV_2024_paper.php": {
    "title": "Beyond MOT: Semantic Multi-Object Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunhao Li",
      "Qin Li",
      "Hao Wang",
      "Xue Ma",
      "Jiali Yao",
      "Shaohua Dong",
      "Heng Fan",
      "Libo Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5185_ECCV_2024_paper.php": {
    "title": "Temporal Event Stereo via Joint Learning with Stereoscopic Flow",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hoonhee Cho",
      "Jae-Young Kang",
      "Kuk-Jin Yoon*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5187_ECCV_2024_paper.php": {
    "title": "SAM-COD: SAM-guided Unified Framework for Weakly-Supervised Camouflaged Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huafeng Chen",
      "Pengxu Wei",
      "Guangqian Guo",
      "Shan Gao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5190_ECCV_2024_paper.php": {
    "title": "Just a Hint: Point-Supervised Camouflaged Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huafeng Chen",
      "Dian SHAO*",
      "Guangqian Guo",
      "shan gao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5194_ECCV_2024_paper.php": {
    "title": "ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guanxing Lu",
      "Shiyi Zhang",
      "Ziwei Wang*",
      "Changliu Liu",
      "Jiwen Lu",
      "Yansong Tang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5197_ECCV_2024_paper.php": {
    "title": "Global-Local Collaborative Inference with LLM for Lidar-Based Open-Vocabulary Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyu Peng",
      "Yan Bai",
      "Chen Gao",
      "Lirong Yang",
      "Fei Xia",
      "Beipeng Mu",
      "Xiaofei Wang",
      "Si Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5200_ECCV_2024_paper.php": {
    "title": "Learning High-resolution Vector Representation from Multi-Camera Images for 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhili Chen",
      "Shuangjie Xu",
      "Maosheng Ye",
      "Zian Qian",
      "Xiaoyi Zou",
      "Dit-Yan Yeung",
      "Qifeng Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5210_ECCV_2024_paper.php": {
    "title": "View-Consistent 3D Editing with Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Wang*",
      "Xuanyu Yi",
      "Zike Wu",
      "Na Zhao",
      "Long Chen",
      "Hanwang Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5216_ECCV_2024_paper.php": {
    "title": "E3V-K5: An Authentic Benchmark for Redefining Video-Based Energy Expenditure Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengxuming Zhang",
      "Lei Jin",
      "Yifan Wang",
      "Xinyu Wang",
      "Xu Wen",
      "Zunlei Feng*",
      "Mingli Song"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5218_ECCV_2024_paper.php": {
    "title": "GeoGaussian: Geometry-aware Gaussian Splatting for Scene Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanyan Li*",
      "Chenyu Lyu",
      "Yan Di",
      "Guangyao Zhai",
      "Gim Hee Lee",
      "Federico Tombari"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5236_ECCV_2024_paper.php": {
    "title": "URS-NeRF: Unordered Rolling Shutter Bundle Adjustment for Neural Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Xu*",
      "Liu Ziao",
      "Mengqi Guo",
      "jiancheng Li",
      "Gim Hee Lee"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5238_ECCV_2024_paper.php": {
    "title": "InstructIR: High-Quality Image Restoration Following Human Instructions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marcos V. Conde*",
      "Gregor Geigle",
      "Radu Timofte"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5239_ECCV_2024_paper.php": {
    "title": "Asynchronous Large Language Model Enhanced Planner for Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Chen",
      "Zi-han Ding",
      "Ziqin Wang",
      "Yan Wang*",
      "Lijun Zhang",
      "Si Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5243_ECCV_2024_paper.php": {
    "title": "Make a Cheap Scaling: A Self-Cascade Diffusion Model for Higher-Resolution Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lanqing Guo",
      "Yingqing HE",
      "Haoxin Chen",
      "Menghan Xia",
      "Xiaodong Cun",
      "Yufei Wang",
      "Siyu Huang",
      "Yong Zhang",
      "Xintao Wang",
      "Qifeng Chen",
      "Ying Shan",
      "Bihan Wen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5246_ECCV_2024_paper.php": {
    "title": "LayoutFlow: Flow Matching for Layout Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julian Jorge Andrade Guerreiro*",
      "Naoto Inoue*",
      "Kento Masui",
      "Mayu Otani",
      "Hideki Nakayama"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5259_ECCV_2024_paper.php": {
    "title": "Making Large Language Models Better Planners with Reasoning-Decision Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhijian Huang",
      "Tao Tang",
      "Shaoxiang Chen",
      "Sihao Lin",
      "Zequn Jie",
      "Lin Ma",
      "Guangrun Wang",
      "Xiaodan Liang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5272_ECCV_2024_paper.php": {
    "title": "R3D-AD: Reconstruction via Diffusion for 3D Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheyuan Zhou",
      "Le Wang",
      "Naiyu Fang",
      "Zili Wang",
      "Lemiao Qiu*",
      "Shuyou Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5278_ECCV_2024_paper.php": {
    "title": "Representation Enhancement-Stabilization: Reducing Bias-Variance of Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Huang*",
      "Yilei Shi",
      "Zhitong Xiong",
      "Xiao Xiang Zhu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5279_ECCV_2024_paper.php": {
    "title": "Continual Learning for Remote Physiological Measurement: Minimize Forgetting and Simplify Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qian Liang",
      "Yan Chen",
      "Yang Hu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5282_ECCV_2024_paper.php": {
    "title": "An Optimization Framework to Enforce Multi-View Consistency for Texturing 3D Meshes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengyi Zhao",
      "Chen Song",
      "Xiaodong Gu",
      "Yuan Dong",
      "Qi Zuo",
      "Weihao Yuan",
      "Zilong Dong*",
      "Liefeng Bo",
      "Qixing Huang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5288_ECCV_2024_paper.php": {
    "title": "STAG4D: Spatial-Temporal Anchored Generative 4D Gaussians",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei Zeng",
      "Yanqin Jiang",
      "Siyu Zhu",
      "Yuanxun Lu",
      "Youtian Lin",
      "Hao Zhu",
      "Weiming Hu",
      "Xun Cao",
      "Yao Yao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5296_ECCV_2024_paper.php": {
    "title": "RGBD GS-ICP SLAM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seongbo Ha",
      "Jiung Yeon",
      "Hyeonwoo Yu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5300_ECCV_2024_paper.php": {
    "title": "Efficient NeRF Optimization - Not All Samples Remain Equally Hard",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juuso Korhonen*",
      "Goutham Rangu",
      "Hamed Rezazadegan Tavakoli",
      "Juho Kannala"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5302_ECCV_2024_paper.php": {
    "title": "Revisiting Calibration of Wide-Angle Radially Symmetric Cameras",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrea Porfiri Dal Cin*",
      "Francesco Azzoni",
      "Giacomo Boracchi",
      "Luca Magri*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5305_ECCV_2024_paper.php": {
    "title": "Rawformer: Unpaired Raw-to-Raw Translation for Learnable Camera ISPs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Georgy Perevozchikov*",
      "Nancy Mehta*",
      "Mahmoud Afifi*",
      "Radu Timofte*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5311_ECCV_2024_paper.php": {
    "title": "Robust Incremental Structure-from-Motion with Hybrid Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaohui Liu*",
      "Yidan Gao",
      "Tianyi Zhang",
      "Rémi Pautrat",
      "Johannes L Schönberger",
      "Viktor Larsson",
      "Marc Pollefeys"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5312_ECCV_2024_paper.php": {
    "title": "Revisiting Domain-Adaptive Object Detection in Adverse Weather by the Generation and Composition of High-Quality Pseudo-Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Zhao",
      "Huibin Yan",
      "Shuoyao Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5316_ECCV_2024_paper.php": {
    "title": "Prediction Exposes Your Face: Black-box Model Inversion via Prediction Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufan Liu*",
      "Wanqian Zhang",
      "Dayan Wu",
      "Zheng Lin",
      "jingzi Gu",
      "Weiping Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5324_ECCV_2024_paper.php": {
    "title": "Noise Calibration: Plug-and-play Content-Preserving Video Enhancement using Pre-trained Video Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qinyu Yang",
      "Haoxin Chen",
      "Yong Zhang*",
      "Menghan Xia",
      "Xiaodong Cun",
      "Zhixun Su*",
      "Ying Shan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5325_ECCV_2024_paper.php": {
    "title": "UniCal: Unified Neural Sensor Calibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ze Yang*",
      "George G Chen",
      "Haowei Zhang",
      "Kevin Ta",
      "Ioan Andrei Bârsan",
      "Daniel Murphy",
      "Sivabalan Manivasagam*",
      "Raquel Urtasun*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5329_ECCV_2024_paper.php": {
    "title": "Mind the Interference: Retaining Pre-trained Knowledge in Parameter Efficient Continual Learning of Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Longxiang Tang*",
      "Zhuotao Tian",
      "Kai Li",
      "Chunming He",
      "Hantao Zhou",
      "Hengshuang Zhao",
      "Xiu Li",
      "Jiaya Jia"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5333_ECCV_2024_paper.php": {
    "title": "Urban Waterlogging Detection: A Challenging Benchmark and Large-Small Model Co-Adapter",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suqi Song",
      "Chenxu Zhang",
      "Peng Zhang",
      "Pengkun Li",
      "Fenglong Song",
      "Lei Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5346_ECCV_2024_paper.php": {
    "title": "Pseudo-Embedding for Generalized Few-Shot Point Cloud Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chih-Jung Tsai",
      "Hwann-Tzong Chen*",
      "Tyng-Luh Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5355_ECCV_2024_paper.php": {
    "title": "WSI-VQA: Interpreting Whole Slide Images by Generative Visual Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pingyi Chen*",
      "Chenglu Zhu",
      "Sunyi Zheng",
      "Honglin Li",
      "Lin Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5358_ECCV_2024_paper.php": {
    "title": "ReMoS: 3D Motion-Conditioned Reaction Synthesis for Two-Person Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anindita Ghosh*",
      "Rishabh Dabral",
      "Vladislav Golyanik",
      "Christian Theobalt",
      "Philipp Slusallek"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5359_ECCV_2024_paper.php": {
    "title": "Statewide Visual Geolocalization in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Florian Fervers*",
      "Sebastian Bullinger",
      "Christoph Bodensteiner",
      "Michael Arens",
      "Rainer Stiefelhagen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5360_ECCV_2024_paper.php": {
    "title": "Any2Point: Empowering Any-modality Transformers for Efficient 3D Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiwen Tang",
      "Ray Zhang",
      "Jiaming Liu",
      "Zoey Guo",
      "Bin Zhao*",
      "Zhigang Wang",
      "Dong Wang*",
      "Peng Gao",
      "Hongsheng Li",
      "Xuelong Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5362_ECCV_2024_paper.php": {
    "title": "Trajectory-aligned Space-time Tokens for Few-shot Action Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pulkit Kumar*",
      "Namitha Padmanabhan",
      "Luke Luo",
      "Sai Saketh Rambhatla",
      "Abhinav Shrivastava"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5363_ECCV_2024_paper.php": {
    "title": "EgoCVR: An Egocentric Benchmark for Fine-Grained Composed Video Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Hummel*",
      "Shyamgopal Karthik",
      "Mariana-Iuliana Georgescu",
      "Zeynep Akata"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5369_ECCV_2024_paper.php": {
    "title": "Synchronization of Projective Transformations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rakshith Madhavan*",
      "Andrea Fusiello",
      "Federica Arrigoni"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5370_ECCV_2024_paper.php": {
    "title": "TLControl: Trajectory and Language Control for Human Motion Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weilin Wan*",
      "Zhiyang Dou",
      "Taku Komura",
      "Wenping Wang",
      "Dinesh Jayaraman",
      "Lingjie Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5373_ECCV_2024_paper.php": {
    "title": "Insect Identification in the Wild: The AMI Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditya Jain*",
      "Fagner Cunha",
      "Michael J Bunsen",
      "Juan Sebastián Cañas",
      "Léonard Pasi",
      "Nathan Pinoy",
      "Flemming Helsing",
      "JoAnne Russo",
      "Marc S Botham",
      "Michael Sabourin",
      "Jonathan Fréchette",
      "Alexandre Anctil",
      "Yacksecari Lopez",
      "Eduardo Navarro",
      "Filonila Pérez",
      "Ana C Zamora",
      "Jose Alejandro Ramirez-Silva",
      "Jonathan Gagnon",
      "Tom A August",
      "Kim Bjerge",
      "Alba Gomez Segura",
      "Marc Belisle",
      "Yves Basset",
      "Kent P McFarland",
      "David B Roy",
      "Toke T Høye",
      "Maxim Larrivee",
      "David Rolnick"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5379_ECCV_2024_paper.php": {
    "title": "Cross-view image geo-localization with Panorama-BEV Co-Retrieval Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyan Ye",
      "Zhutao Lv",
      "Weijia Li*",
      "Jinhua Yu",
      "Haote Yang",
      "Huaping Zhong",
      "Conghui He*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5386_ECCV_2024_paper.php": {
    "title": "F-HOI: Toward Fine-grained Semantic-Aligned 3D Human-Object Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Yang",
      "Xuesong Niu",
      "Nan Jiang",
      "Ruimao Zhang*",
      "Siyuan Huang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5391_ECCV_2024_paper.php": {
    "title": "Test-time Model Adaptation for Image Reconstruction Using Self-supervised Adaptive Layers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yutian Zhao",
      "Tianjing Zhang",
      "Hui Ji*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5400_ECCV_2024_paper.php": {
    "title": "SHIC: Shape-Image Correspondences with no Keypoint Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aleksandar Shtedritski*",
      "Christian Rupprecht",
      "Andrea Vedaldi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5406_ECCV_2024_paper.php": {
    "title": "GenRC: Generative 3D Room Completion from Sparse Image Collections",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ming-Feng Li*",
      "Yueh-Feng Ku",
      "Hong-Xuan Yen",
      "Chi Liu",
      "Yu-Lun Liu",
      "Albert Y Chen",
      "Cheng-Hao Kuo",
      "Min Sun"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5407_ECCV_2024_paper.php": {
    "title": "A Probability-guided Sampler for Neural Implicit Surface Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gonçalo José Dias Pais",
      "Valter André Piedade",
      "Moitreya Chatterjee",
      "Marcus Greiff",
      "Pedro Miraldo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5411_ECCV_2024_paper.php": {
    "title": "ReMatching: Low-Resolution Representations for Scalable Shape Correspondence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Filippo Maggioli*",
      "Daniele Baieri",
      "Emanuele Rodola",
      "Simone Melzi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5418_ECCV_2024_paper.php": {
    "title": "Where am I? Scene Retrieval with Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqi Chen*",
      "Daniel Barath",
      "Iro Armeni",
      "Marc Pollefeys",
      "Hermann Blum"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5439_ECCV_2024_paper.php": {
    "title": "This Probably Looks Exactly Like That: An Invertible Prototypical Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zachariah Carmichael*",
      "Timothy P Redgrave",
      "Daniel Gonzalez Cedre",
      "Walter Scheirer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5442_ECCV_2024_paper.php": {
    "title": "Arc2Face: A Foundation Model for ID-Consistent Human Faces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Foivos Paraperas Papantoniou*",
      "Alexandros Lattas",
      "Stylianos Moschoglou",
      "Jiankang Deng",
      "Bernhard Kainz",
      "Stefanos Zafeiriou"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5446_ECCV_2024_paper.php": {
    "title": "PhysAvatar: Learning the Physics of Dressed 3D Avatars from Visual Observations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Zheng*",
      "Qingqing Zhao",
      "Guandao Yang",
      "Wang Yifan",
      "Donglai Xiang",
      "Florian Dubost",
      "Dmitry Lagun",
      "Thabo Beeler",
      "Federico Tombari",
      "Leonidas Guibas",
      "Gordon Wetzstein"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5452_ECCV_2024_paper.php": {
    "title": "Revisiting Feature Disentanglement Strategy in Diffusion Training and Breaking Conditional Independence Assumption in Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wonwoong Cho*",
      "Hareesh Ravi*",
      "Midhun Harikumar",
      "Vinh Khuc",
      "Krishna Kumar Singh",
      "Jingwan Lu",
      "David Iseri Inouye*",
      "Ajinkya Kale*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5454_ECCV_2024_paper.php": {
    "title": "SweepNet: Unsupervised Learning Shape Abstraction via Neural Sweepers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingrui Zhao*",
      "Yizhi Wang",
      "Fenggen Yu",
      "Changqing Zou",
      "Ali Mahdavi-Amiri"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5459_ECCV_2024_paper.php": {
    "title": "Leveraging Thermal Modality to Enhance Reconstruction in Low-Light Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiacong Xu*",
      "Mingqian Liao",
      "Ram Prabhakar Kathirvel",
      "Vishal Patel"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5462_ECCV_2024_paper.php": {
    "title": "On the Viability of Monocular Depth Pre-training for Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dong Lao*",
      "Fengyu Yang",
      "Daniel Wang",
      "Hyoungseob Park",
      "Samuel Lu",
      "Alex Wong",
      "Stefano Soatto"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5464_ECCV_2024_paper.php": {
    "title": "Fairness-aware Vision Transformer via Debiased Self-Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yao Qiang",
      "Chengyin Li",
      "Prashant Khanduri",
      "Dongxiao Zhu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5469_ECCV_2024_paper.php": {
    "title": "EgoPet: Egomotion and Interaction Data from an Animal's Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amir Bar*",
      "Arya Bakhtiar",
      "Danny L Tran",
      "Antonio Loquercio",
      "Jathushan Rajasegaran",
      "yann lecun",
      "Amir Globerson",
      "Trevor Darrell"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5472_ECCV_2024_paper.php": {
    "title": "Deep Companion Learning: Enhancing Generalization Through Historical Consistency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruizhao Zhu*",
      "Venkatesh Saligrama*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5476_ECCV_2024_paper.php": {
    "title": "Neural graphics texture compression supporting random access",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Farzad Farhadzadeh*",
      "Qiqi Hou",
      "Hoang Le",
      "Amir Said",
      "Randall R Rauwendaal",
      "Alex Bourd",
      "Fatih Porikli"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5479_ECCV_2024_paper.php": {
    "title": "Contrastive Learning with Synthetic Positives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dewen Zeng*",
      "Xinrong Hu",
      "Yawen Wu",
      "Xiaowei Xu",
      "Yiyu Shi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5481_ECCV_2024_paper.php": {
    "title": "GeneralAD: Anomaly Detection Across Domains by Attending to Distorted Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luc P.J. Sträter*",
      "Mohammadreza Salehi",
      "Efstratios Gavves",
      "Cees G.M. Snoek",
      "Yuki M. Asano"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5486_ECCV_2024_paper.php": {
    "title": "Interpretability-Guided Test-Time Adversarial Defense",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akshay Kulkarni*",
      "Tsui-Wei Weng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5491_ECCV_2024_paper.php": {
    "title": "DIM: Dyadic Interaction Modeling for Social Behavior Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minh Tran*",
      "Di Chang",
      "Maksim Siniukov",
      "Mohammad Soleymani"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5492_ECCV_2024_paper.php": {
    "title": "Tri^{2}-plane: Thinking Head Avatar via Feature Pyramid",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luchuan Song*",
      "Pinxin Liu",
      "Lele Chen",
      "Guojun Yin",
      "Chenliang Xu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5494_ECCV_2024_paper.php": {
    "title": "ControlCap: Controllable Region-level Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuzhong Zhao",
      "Liu Yue",
      "Zonghao Guo",
      "weijia wu",
      "Chen Gong",
      "Qixiang Ye",
      "Fang Wan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5495_ECCV_2024_paper.php": {
    "title": "Free Lunch for Gait Recognition: A Novel Relation Descriptor",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jilong Wang*",
      "Saihui Hou",
      "Yan Huang",
      "Chunshui Cao",
      "Xu Liu",
      "Yongzhen Huang",
      "Tianzhu Zhang",
      "Liang Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5498_ECCV_2024_paper.php": {
    "title": "SegVG: Transferring Object Bounding Box to Segmentation for Visual Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weitai Kang*",
      "Gaowen Liu",
      "Mubarak Shah",
      "Yan Yan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5502_ECCV_2024_paper.php": {
    "title": "Adaptive Correspondence Scoring for Unsupervised Medical Image Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoran Zhang*",
      "John C. Stendahl",
      "Lawrence H. Staib",
      "Albert J. Sinusas",
      "Alex Wong",
      "James S. Duncan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5506_ECCV_2024_paper.php": {
    "title": "MaxFusion: Plug&Play Multi-Modal Generation in Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nithin Gopalakrishnan Nair*",
      "Jeya Maria Jose Valanarasu",
      "Vishal Patel"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5512_ECCV_2024_paper.php": {
    "title": "Watch Your Steps: Local Image and Scene Editing by Text Instructions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashkan Mirzaei*",
      "Tristan T Aumentado-Armstrong",
      "Marcus A Brubaker",
      "Jonathan Kelly",
      "Alex Levinshtein",
      "Konstantinos G Derpanis",
      "Igor Gilitschenski"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5513_ECCV_2024_paper.php": {
    "title": "Forget More to Learn More: Domain-specific Feature Unlearning for Semi-supervised and Unsupervised Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hritam Basak*",
      "Zhaozheng Yin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5514_ECCV_2024_paper.php": {
    "title": "3x2: 3D Object Part Segmentation by 2D Semantic Correspondences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anh Thai*",
      "Weiyao Wang",
      "Hao Tang",
      "Stefan Stojanov",
      "James M Rehg",
      "Matt Feiszli"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5515_ECCV_2024_paper.php": {
    "title": "Idea2Img: Iterative Self-Refinement with GPT-4V for Automatic Image Design and Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengyuan Yang*",
      "Jianfeng Wang",
      "Linjie Li",
      "Kevin Lin",
      "Chung-Ching Lin",
      "Zicheng Liu",
      "Lijuan Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5518_ECCV_2024_paper.php": {
    "title": "Human-in-the-Loop Visual Re-ID for Population Size Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gustavo Perez*",
      "Daniel Sheldon",
      "Grant Van Horn",
      "Subhransu Maji"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5519_ECCV_2024_paper.php": {
    "title": "SEGIC: Unleashing the Emergent Correspondence for In-Context Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingchen Meng",
      "Shiyi Lan",
      "Hengduo Li",
      "Jose M Alvarez",
      "Zuxuan Wu*",
      "Yu-Gang Jiang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5521_ECCV_2024_paper.php": {
    "title": "PointNeRF++: A multi-scale, point-based Neural Radiance Field",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiwei Sun",
      "Eduard Trulls",
      "Yang-Che Tseng",
      "Sneha Sambandam",
      "Gopal Sharma",
      "Andrea Tagliasacchi",
      "Kwang Moo Yi*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5527_ECCV_2024_paper.php": {
    "title": "A Semantic Space is Worth 256 Language Descriptions: Make Stronger Segmentation Models with Descriptive Properties",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junfei Xiao",
      "Ziqi Zhou",
      "Wenxuan Li",
      "Shiyi Lan",
      "Jieru Mei",
      "Zhiding Yu",
      "Bingchen Zhao",
      "Alan Yuille",
      "Yuyin Zhou",
      "Cihang Xie*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5528_ECCV_2024_paper.php": {
    "title": "UMG-CLIP: A Unified Multi-Granularity Vision Generalist for Open-World Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowen Shi",
      "Peisen Zhao",
      "Zichen Wang",
      "Yuhang Zhang",
      "Yaoming Wang",
      "Jin Li",
      "Wenrui Dai",
      "Junni Zou",
      "Hongkai Xiong",
      "Qi Tian",
      "Xiaopeng Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5533_ECCV_2024_paper.php": {
    "title": "Fast View Synthesis of Casual Videos with Soup-of-Planes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yao-Chih Lee*",
      "Zhoutong Zhang",
      "Kevin Blackburn-Matzen",
      "Simon Niklaus",
      "Jianming Zhang",
      "Jia-Bin Huang",
      "Feng Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5542_ECCV_2024_paper.php": {
    "title": "Adaptive Human Trajectory Prediction via Latent Corridors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Neerja Thakkar*",
      "Karttikeya Mangalam",
      "Andrea Bajcsy",
      "Jitendra Malik"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5543_ECCV_2024_paper.php": {
    "title": "Video Question Answering with Procedural Programs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rohan Choudhury*",
      "Koichiro Niinuma",
      "Kris Kitani",
      "Laszlo A Jeni"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5548_ECCV_2024_paper.php": {
    "title": "DGR-MIL: Exploring Diverse Global Representation in Multiple Instance Learning for Whole Slide Image Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhui Zhu*",
      "Xiwen Chen",
      "Peijie Qiu",
      "Aristeidis Sotiras",
      "Abolfazl Razi",
      "Yalin Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5550_ECCV_2024_paper.php": {
    "title": "TexGen: Text-Guided 3D Texture Generation with Multi-view Sampling and Resampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dong Huo*",
      "Zixin Guo",
      "Xinxin Zuo",
      "Zhihao Shi",
      "Juwei Lu",
      "Peng Dai",
      "Songcen Xu",
      "Li Cheng",
      "Yee-Hong Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5552_ECCV_2024_paper.php": {
    "title": "C2C: Component-to-Composition Learning for Zero-Shot Compositional Action Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rongchang Li",
      "Zhenhua Feng",
      "Tianyang Xu",
      "Linze Li",
      "Xiao-Jun Wu*",
      "Muhammad Awais",
      "Sara Atito",
      "Josef Kittler"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5554_ECCV_2024_paper.php": {
    "title": "LLMGA: Multimodal Large Language Model based Generation Assistant",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "bin xia*",
      "Shiyin Wang",
      "Yingfan Tao",
      "Yitong Wang",
      "Jiaya Jia"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5558_ECCV_2024_paper.php": {
    "title": "Put Myself in Your Shoes: Lifting the Egocentric Perspective from Exocentric Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mi Luo*",
      "Zihui Xue",
      "Alex Dimakis",
      "Kristen Grauman"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5559_ECCV_2024_paper.php": {
    "title": "Shape from Heat Conduction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sriram Narayanan*",
      "Mani Ramanagopal",
      "Mark Sheinin",
      "Aswin C. Sankaranarayanan",
      "Srinivasa G. Narasimhan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5560_ECCV_2024_paper.php": {
    "title": "An Adaptive Screen-Space Meshing Approach for Normal Integration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Moritz Heep*",
      "Eduard Zell"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5562_ECCV_2024_paper.php": {
    "title": "Parrot: Pareto-optimal Multi-Reward Reinforcement Learning Framework for Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seung Hyun Lee*",
      "Yinxiao Li",
      "Junjie Ke",
      "Innfarn Yoo",
      "Han Zhang",
      "Jiahui Yu",
      "Qifei Wang",
      "Fei Deng",
      "Glenn Entis",
      "Junfeng He",
      "Gang Li",
      "Sangpil Kim",
      "Irfan Essa",
      "Feng Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5563_ECCV_2024_paper.php": {
    "title": "HandDGP: Camera-Space Hand Mesh Prediction with Differentiable Global Positioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eugene Valassakis",
      "Guillermo Garcia-Hernando*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5568_ECCV_2024_paper.php": {
    "title": "Towards Latent Masked Image Modeling for Self-Supervised Visual Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yibing Wei*",
      "Abhinav Gupta",
      "Pedro Morgado*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5571_ECCV_2024_paper.php": {
    "title": "Nuvo: Neural UV Mapping for Unruly 3D Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pratul Srinivasan*",
      "Stephan J Garbin",
      "Dor Verbin",
      "Jonathan T Barron",
      "Ben Mildenhall"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5572_ECCV_2024_paper.php": {
    "title": "Towards High-Quality 3D Motion Transfer with Realistic Apparel Animation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rong Wang*",
      "Wei Mao",
      "Changsheng Lu",
      "HONGDONG LI"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5573_ECCV_2024_paper.php": {
    "title": "AnyHome: Open-Vocabulary Large-Scale Indoor Scene Generation with First-Person View Exploration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rao Fu*",
      "Zehao Wen",
      "Zichen Liu ",
      "Srinath Sridhar"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5575_ECCV_2024_paper.php": {
    "title": "Better Call SAL: Towards Learning to Segment Anything in Lidar",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aljosa Osep*",
      "Tim Meinhardt",
      "Francesco Ferroni",
      "Neehar Peri",
      "Deva Ramanan",
      "Laura Leal-Taixé"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5579_ECCV_2024_paper.php": {
    "title": "DGInStyle: Domain-Generalizable Semantic Segmentation with Image Diffusion Models and Stylized Semantic Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuru Jia",
      "Lukas Hoyer",
      "Shengyu Huang",
      "Tianfu Wang",
      "Luc Van Gool",
      "Konrad Schindler",
      "Anton Obukhov*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5580_ECCV_2024_paper.php": {
    "title": "DECOLLAGE: 3D Detailization by Controllable, Localized, and Learned Geometry Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qimin Chen*",
      "Zhiqin Chen",
      "Vladimir G. Kim",
      "Noam Aigerman",
      "Hao Zhang",
      "Siddhartha Chaudhuri"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5581_ECCV_2024_paper.php": {
    "title": "Scene-aware Human Motion Forecasting via Mutual Distance Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaoyue Xing*",
      "Wei Mao",
      "Miaomiao Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5583_ECCV_2024_paper.php": {
    "title": "FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zehao Zhu",
      "Zhiwen Fan*",
      "Yifan Jiang",
      "Zhangyang Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5584_ECCV_2024_paper.php": {
    "title": "Open Panoramic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junwei Zheng",
      "Ruiping Liu",
      "Yufan Chen",
      "Kunyu Peng",
      "Chengzhi Wu",
      "Kailun Yang",
      "Jiaming Zhang*",
      "Rainer Stiefelhagen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5586_ECCV_2024_paper.php": {
    "title": "iMatching: Imperative Correspondence Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zitong Zhan*",
      "Dasong Gao",
      "Yun-Jou Lin",
      "Youjie Xia",
      "Chen Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5589_ECCV_2024_paper.php": {
    "title": "COSMU: Complete 3D human shape from monocular unconstrained images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marco Pesavento*",
      "Marco Volino",
      "Adrian Hilton"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5590_ECCV_2024_paper.php": {
    "title": "MAP-ADAPT: Real-Time Quality-Adaptive Semantic 3D Maps",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianhao Zheng*",
      "Daniel Barath",
      "Marc Pollefeys",
      "Iro Armeni*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5594_ECCV_2024_paper.php": {
    "title": "Appearance-based Refinement for Object-Centric Motion Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyu Xie*",
      "Weidi Xie",
      "Andrew Zisserman"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5598_ECCV_2024_paper.php": {
    "title": "SemiVL: Semi-Supervised Semantic Segmentation with Vision-Language Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lukas Hoyer*",
      "David Joseph Tan",
      "Muhammad Ferjad Naeem",
      "Luc Van Gool",
      "Federico Tombari"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5599_ECCV_2024_paper.php": {
    "title": "Open Vocabulary Multi-Label Video Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rohit Gupta*",
      "Mamshad Nayeem Rizve",
      "Jayakrishnan Unnikrishnan",
      "Ashish Tawari",
      "Son Tran",
      "Mubarak Shah",
      "Benjamin Yao",
      "Trishul A Chilimbi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5600_ECCV_2024_paper.php": {
    "title": "Optimal Transport of Diverse Unsupervised Tasks for Robust Learning from Noisy Few-Shot Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaofan Que",
      "Qi Yu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5607_ECCV_2024_paper.php": {
    "title": "Regularizing Dynamic Radiance Fields with Kinematic Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Woobin Im",
      "Geonho Cha",
      "Sebin Lee",
      "Jumin Lee",
      "Juhyeong Seon",
      "Dongyoon Wee",
      "Sungeui Yoon*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5609_ECCV_2024_paper.php": {
    "title": "MICDrop: Masking Image and Depth Features via Complementary Dropout for Domain-Adaptive Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linyan Yang*",
      "Lukas Hoyer*",
      "Mark Weber",
      "Tobias Fischer",
      "Dengxin Dai",
      "Laura Leal-Taixé",
      "Daniel Cremers",
      "Marc Pollefeys",
      "Luc Van Gool"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5611_ECCV_2024_paper.php": {
    "title": "Efficient Pre-training for Localized Instruction Generation of Procedural Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anil Batra*",
      "Davide Moltisanti",
      "Laura Sevilla-Lara",
      "Marcus Rohrbach",
      "Frank Keller"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5614_ECCV_2024_paper.php": {
    "title": "MTKD: Multi-Teacher Knowledge Distillation for Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Jiang*",
      "Chen Feng",
      "Fan Zhang",
      "David Bull"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5615_ECCV_2024_paper.php": {
    "title": "DEAL: Disentangle and Localize Concept-level Explanations for VLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tang Li*",
      "Mengmeng Ma",
      "Xi Peng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5618_ECCV_2024_paper.php": {
    "title": "Fast Encoding and Decoding for Implicit Video Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Chen*",
      "Saining Xie",
      "Ser-Nam Lim",
      "Abhinav Shrivastava"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5619_ECCV_2024_paper.php": {
    "title": "Surf-D: Generating High-Quality Surfaces of Arbitrary Topologies Using Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengming Yu*",
      "Zhiyang Dou",
      "Xiaoxiao Long",
      "Cheng Lin",
      "Zekun Li",
      "Yuan Liu",
      "Norman Müller",
      "Taku Komura",
      "Marc Habermann",
      "Christian Theobalt",
      "Xin Li",
      "Wenping Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5621_ECCV_2024_paper.php": {
    "title": "Diffusion-Refined VQA Annotations for Semi-Supervised Gaze Following",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiaomu Miao*",
      "Alexandros Graikos",
      "Jingwei Zhang",
      "Sounak Mondal",
      "Minh Hoai",
      "Dimitris Samaras"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5625_ECCV_2024_paper.php": {
    "title": "IMMA: Immunizing text-to-image Models against Malicious Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amber Yijia Zheng*",
      "Raymond A. Yeh"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5635_ECCV_2024_paper.php": {
    "title": "Motion-Oriented Compositional Neural Radiance Fields for Monocular Dynamic Human Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaehyeok Kim",
      "Dongyoon Wee",
      "Dan Xu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5636_ECCV_2024_paper.php": {
    "title": "GeoCalib: Learning Single-image Calibration with Geometric Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Veicht*",
      "Paul-Edouard Sarlin*",
      "Philipp Lindenberger",
      "Marc Pollefeys"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5642_ECCV_2024_paper.php": {
    "title": "3D Open-Vocabulary Panoptic Segmentation with 2D-3D Vision-Language Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihao Xiao*",
      "Longlong Jing",
      "Shangxuan Wu",
      "Alex Zihao Zhu",
      "Jingwei Ji",
      "Chiyu Max Jiang",
      "Wei-Chih Hung",
      "Thomas Funkhouser",
      "Weicheng Kuo",
      "Anelia Angelova",
      "Yin Zhou",
      "Shiwei Sheng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5645_ECCV_2024_paper.php": {
    "title": "Semicalibrated Relative Pose from an Affine Correspondence and Monodepth",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Petr Hruby*",
      "Marc Pollefeys",
      "Daniel Barath"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5646_ECCV_2024_paper.php": {
    "title": "Global Structure-from-Motion Revisited",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linfei Pan*",
      "Daniel Barath",
      "Marc Pollefeys",
      "Johannes L Schönberger"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5647_ECCV_2024_paper.php": {
    "title": "MobileNetV4: Universal Models for the Mobile Ecosystem",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Danfeng Qin*",
      "Chas H Leichner",
      "Manolis Delakis",
      "Marco Fornoni",
      "Shixin Luo",
      "Fan Yang",
      "Weijun Wang",
      "Colby Banbury",
      "Chengxi Ye",
      "Berkin Akin",
      "Vaibhav Aggarwal",
      "Tenghui Zhu",
      "Daniele Moro",
      "Andrew Howard"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5651_ECCV_2024_paper.php": {
    "title": "Gravity-aligned Rotation Averaging with Circular Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linfei Pan*",
      "Marc Pollefeys",
      "Daniel Barath"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5653_ECCV_2024_paper.php": {
    "title": "MoMA: Multimodal LLM Adapter for Fast Personalized Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kunpeng Song*",
      "Yizhe Zhu*",
      "Bingchen Liu*",
      "Qing Yan*",
      "Ahmed Elgammal*",
      "Xiao Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5654_ECCV_2024_paper.php": {
    "title": "Find n' Propagate: Open-Vocabulary 3D Object Detection in Urban Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Djamahl Etchegaray*",
      "Zi Helen Huang",
      "Tatsuya Harada",
      "Yadan Luo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5655_ECCV_2024_paper.php": {
    "title": "Quanta Video Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prateek Chennuri*",
      "Yiheng Chi",
      "Enze Jiang",
      "GM Dilshan Godaliyadda*",
      "Abhiram Gnanasambandam*",
      "Hamid R Sheikh",
      "Istvan Gyongy",
      "Stanley H Chan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5660_ECCV_2024_paper.php": {
    "title": "Concept Sliders: LoRA Adaptors for Precise Control in Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rohit Gandikota*",
      "Joanna Materzynska",
      "Tingrui Zhou",
      "Antonio Torralba",
      "David Bau"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5662_ECCV_2024_paper.php": {
    "title": "CAT-SAM: Conditional Tuning for Few-Shot Adaptation of Segment Anything Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aoran Xiao",
      "Weihao Xuan",
      "Heli Qi",
      "Yun Xing",
      "Ruijie Ren",
      "Xiaoqin Zhang",
      "Ling Shao",
      "Shijian Lu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5664_ECCV_2024_paper.php": {
    "title": "ScribblePrompt: Fast and Flexible Interactive Segmentation for Any Biomedical Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hallee E. Wong*",
      "Marianne Rakic",
      "John Guttag",
      "Adrian V. Dalca"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5670_ECCV_2024_paper.php": {
    "title": "POCA: Post-training Quantization with Temporal Alignment for Codec Avatars",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Meng*",
      "Yuecheng Li*",
      "Leo (Chenghui) Li",
      "Syed Shakib Sarwar",
      "Dilin Wang",
      "Jae-sun Seo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5671_ECCV_2024_paper.php": {
    "title": "HYPE: Hyperbolic Entailment Filtering for Underspecified Images and Texts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wonjae Kim*",
      "Sanghyuk Chun",
      "Taekyung Kim",
      "Dongyoon Han",
      "Sangdoo Yun"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5674_ECCV_2024_paper.php": {
    "title": "Finding Meaning in Points: Weakly Supervised Semantic Segmentation for Event Cameras",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hoonhee Cho",
      "Sung-Hoon Yoon",
      "Hyeokjun Kweon",
      "Kuk-Jin Yoon*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5675_ECCV_2024_paper.php": {
    "title": "Unsupervised Dense Prediction using Differentiable Normalized Cuts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanbin Liu*",
      "Stephen Gould"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5678_ECCV_2024_paper.php": {
    "title": "Boosting the Power of Small Multimodal Reasoning Models to Match Larger Models with Self-Consistency Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Tan*",
      "Jingxuan Wei*",
      "Zhangyang Gao",
      "Linzhuang Sun",
      "Siyuan Li",
      "Ruifeng Guo",
      "BiHui Yu",
      "Stan Z. Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5680_ECCV_2024_paper.php": {
    "title": "Scaling Up Personalized Image Aesthetic Assessment via Task Vector Customization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jooyeol Yun*",
      "Jaegul Choo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5684_ECCV_2024_paper.php": {
    "title": "AutoDIR: Automatic All-in-One Image Restoration with Latent Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yitong Jiang*",
      "Zhaoyang Zhang",
      "Tianfan Xue",
      "Jinwei Gu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5685_ECCV_2024_paper.php": {
    "title": "Receler: Reliable Concept Erasing of Text-to-Image Diffusion Models via Lightweight Erasers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chi-Pin Huang*",
      "Kai-Po Chang",
      "Chung-Ting Tsai",
      "Yung-Hsuan Lai",
      "Fu-En Yang",
      "Yu-Chiang Frank Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5687_ECCV_2024_paper.php": {
    "title": "EINet: Point Cloud Completion via Extrapolation and Interpolation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pingping Cai*",
      "Canyu Zhang",
      "LINGJIA SHI",
      "Lili Wang",
      "Nasrin Imanpour",
      "Song Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5689_ECCV_2024_paper.php": {
    "title": "Personalized Video Relighting With an At-Home Light Stage",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Myeong Choi*",
      "Max Christman",
      "Roni Sengupta"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5690_ECCV_2024_paper.php": {
    "title": "Temporal Residual Guided Diffusion Framework for Event-Driven Video Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lin Zhu*",
      "Yunlong Zheng",
      "Yijun Zhang",
      "Xiao Wang",
      "Lizhi Wang",
      "Hua Huang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5695_ECCV_2024_paper.php": {
    "title": "A Secure Image Watermarking Framework with Statistical Guarantees via Adversarial Attacks on Secret Key Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feiyu CHEN*",
      "Wei Lin",
      "Ziquan Liu",
      "Antoni Chan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5700_ECCV_2024_paper.php": {
    "title": "SPIRE: Semantic Prompt-Driven Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenyang QI*",
      "Zhengzhong Tu",
      "Keren Ye",
      "Mauricio Delbracio",
      "Peyman Milanfar",
      "Qifeng Chen",
      "Hossein Talebi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5708_ECCV_2024_paper.php": {
    "title": "Free-ATM: Harnessing Free Attention Masks for Representation Learning on Diffusion-Generated Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Junhao Zhang*",
      "Mutian Xu",
      "Jay Zhangjie Wu",
      "Chuhui Xue",
      "Wenqing Zhang",
      "Xiaoguang Han",
      "Song Bai",
      "Mike Zheng Shou*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5709_ECCV_2024_paper.php": {
    "title": "HiT-SR: Hierarchical Transformer for Efficient Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "XIANG ZHANG*",
      "Yulun Zhang",
      "Fisher Yu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5712_ECCV_2024_paper.php": {
    "title": "Audio-Synchronized Visual Animation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lin Zhang",
      "Shentong Mo",
      "Yijing Zhang",
      "Pedro Morgado*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5715_ECCV_2024_paper.php": {
    "title": "Expressive Whole-Body 3D Gaussian Avatar",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gyeongsik Moon*",
      "Takaaki Shiratori",
      "Shunsuke Saito"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5717_ECCV_2024_paper.php": {
    "title": "Canonical Shape Projection is All You Need for 3D Few-shot Class Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali Cheraghian*",
      "Zeeshan Hayder",
      "Sameeea Ramasinghe",
      "Shafin Rahman",
      "Javad Jafaryahya",
      "Lars Petersson",
      "Mehrtash Harandi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5720_ECCV_2024_paper.php": {
    "title": "Controllable Human-Object Interaction Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaman Li*",
      "Alexander Clegg",
      "Roozbeh Mottaghi",
      "Jiajun Wu",
      "Xavier Puig",
      "C. Karen Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5728_ECCV_2024_paper.php": {
    "title": "High-Fidelity and Transferable NeRF Editing by Frequency Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yisheng He*",
      "Weihao Yuan*",
      "Siyu Zhu",
      "Zilong Dong",
      "Liefeng Bo",
      "Qixing Huang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5729_ECCV_2024_paper.php": {
    "title": "DoughNet: A Visual Predictive Model for Topological Manipulation of Deformable Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dominik Bauer*",
      "Zhenjia Xu",
      "Shuran Song"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5730_ECCV_2024_paper.php": {
    "title": "PAV: Personalized Head Avatar from Unstructured Video Collection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akin Caliskan*",
      "Berkay Kicanaoglu",
      "Hyeongwoo Kim"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5733_ECCV_2024_paper.php": {
    "title": "Strike a Balance in Continual Panoptic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinpeng Chen",
      "Runmin Cong*",
      "Yuxuan Luo",
      "Horace Ho Shing Ip",
      "Sam Kwong*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5739_ECCV_2024_paper.php": {
    "title": "In Defense of Lazy Visual Grounding for Open-Vocabulary Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dahyun Kang",
      "Minsu Cho*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5743_ECCV_2024_paper.php": {
    "title": "MultiDelete for Multimodal Machine Unlearning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiali Cheng*",
      "Hadi Amiri"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5744_ECCV_2024_paper.php": {
    "title": "Unified Local-Cloud Decision-Making via Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kathakoli Sengupta",
      "Zhongkai Shangguan",
      "Sandesh Bharadwaj",
      "Sanjay Arora",
      "Eshed Ohn-Bar*",
      "Renato Mancuso"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5747_ECCV_2024_paper.php": {
    "title": "UniTalker: Scaling up Audio-Driven 3D Facial Animation through A Unified Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangyu Fan*",
      "Jiaqi Li",
      "Zhiqian Lin",
      "Weiye Xiao",
      "Lei Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5748_ECCV_2024_paper.php": {
    "title": "Robo-ABC: Affordance Generalization Beyond Categories via Semantic Correspondence for Robot Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanchen Ju",
      "Kaizhe Hu",
      "Guowei Zhang",
      "Gu Zhang",
      "Mingrun Jiang",
      "Huazhe Xu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5751_ECCV_2024_paper.php": {
    "title": "Efficient Frequency-Domain Image Deraining with Contrastive Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ning Gao",
      "Xingyu Jiang",
      "Xiuhui Zhang",
      "Yue Deng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5757_ECCV_2024_paper.php": {
    "title": "Stitched ViTs are Flexible Vision Backbones",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zizheng Pan*",
      "Jing Liu",
      "Haoyu He",
      "Jianfei Cai",
      "Bohan Zhuang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5758_ECCV_2024_paper.php": {
    "title": "TrajPrompt: Aligning Color Trajectory with Vision-Language Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li-Wu Tsao*",
      "Hao-Tang Tsui",
      "Yu-Rou Tuan",
      "Pei-Chi Chen",
      "Kuan-Lin Wang",
      "Jhih-Ciang Wu",
      "Hong-Han Shuai*",
      "Wen-Huang Cheng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5759_ECCV_2024_paper.php": {
    "title": "SemReg: Semantics Constrained Point Cloud Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheldon Fung",
      "Xuequan Lu*",
      "Dasith de Silva Edirimuni",
      "Wei Pan",
      "Xiao Liu",
      "HONGDONG LI"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5774_ECCV_2024_paper.php": {
    "title": "Cascade-Zero123: One Image to Highly Consistent 3D with Self-Prompted Nearby Views",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yabo Chen",
      "Jiemin Fang",
      "Yuyang Huang",
      "Taoran Yi",
      "Xiaopeng Zhang*",
      "Lingxi Xie",
      "Xinggang Wang",
      "Wenrui Dai*",
      "Hongkai Xiong",
      "Qi Tian"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5780_ECCV_2024_paper.php": {
    "title": "RoScenes: A Large-scale Multi-view 3D Dataset for Roadside Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaosu Zhu",
      "Hualian Sheng",
      "Sijia Cai",
      "Bing Deng",
      "Shaopeng Yang",
      "Qiao Liang",
      "Ken Chen",
      "Lianli Gao",
      "Jingkuan Song*",
      "Jieping Ye*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5783_ECCV_2024_paper.php": {
    "title": "ReSyncer: Rewiring Style-based Generator for Unified Audio-Visually Synced Facial Performer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiazhi Guan*",
      "Zhiliang Xu",
      "Hang Zhou",
      "Kaisiyuan Wang",
      "Shengyi He",
      "Zhanwang Zhang",
      "Borong Liang",
      "Haocheng Feng",
      "Errui Ding",
      "Jingtuo Liu",
      "Jingdong Wang",
      "Youjian Zhao",
      "Ziwei Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5788_ECCV_2024_paper.php": {
    "title": "Language-Driven Physics-Based Scene Synthesis and Editing via Feature Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ri-Zhao Qiu*",
      "Ge Yang",
      "Weijia Zeng",
      "Xiaolong Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5794_ECCV_2024_paper.php": {
    "title": "AlignDiff: Aligning Diffusion Models for General Few-Shot Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ri-Zhao Qiu*",
      "Yu-Xiong Wang",
      "Kris Hauser"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5796_ECCV_2024_paper.php": {
    "title": "SkateFormer: Skeletal-Temporal Transformer for Human Action Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeonghyeok Do",
      "Munchurl Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5800_ECCV_2024_paper.php": {
    "title": "R^2-Tuning: Efficient Image-to-Video Transfer Learning for Video Temporal Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ye Liu",
      "Jixuan He",
      "Wanhua Li*",
      "Junsik Kim",
      "Donglai Wei",
      "Hanspeter Pfister",
      "Chang Wen Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5802_ECCV_2024_paper.php": {
    "title": "Tree-D Fusion: Simulation-Ready Tree Dataset from Single Images with Diffusion Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jae Joong Lee",
      "Bosheng Li",
      "Sara M Beery",
      "Jonathan Huang",
      "Songlin Fei",
      "Raymond A. Yeh",
      "Bedrich Benes*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5803_ECCV_2024_paper.php": {
    "title": "Parameterization-driven Neural Surface Reconstruction for Object-oriented Editing in Neural Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baixin Xu",
      "Jiangbei Hu",
      "Fei Hou",
      "Kwan-Yee Lin",
      "Wayne Wu",
      "Chen Qian",
      "Ying He*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5806_ECCV_2024_paper.php": {
    "title": "DomainFusion: Generalizing To Unseen Domains with Latent Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuyang Huang",
      "Yabo Chen",
      "Yuchen Liu",
      "xiaopeng zhang*",
      "Wenrui Dai*",
      "Hongkai Xiong",
      "Qi Tian"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5808_ECCV_2024_paper.php": {
    "title": "Open-Set Recognition in the Age of Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dimity Miller*",
      "Niko Suenderhauf",
      "Alex Kenna",
      "Keita Mason"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5810_ECCV_2024_paper.php": {
    "title": "Unsqueeze [CLS] Bottleneck to Learn Rich Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qing Su*",
      "Shihao Ji"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5817_ECCV_2024_paper.php": {
    "title": "Robust Multimodal Learning via Representation Decoupling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shicai Wei",
      "Yang Luo",
      "Yuji Wang",
      "Chunbo Luo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5822_ECCV_2024_paper.php": {
    "title": "Object-Conditioned Energy-Based Attention Map Alignment in Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yasi Zhang*",
      "Peiyu Yu",
      "Ying Nian Wu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5826_ECCV_2024_paper.php": {
    "title": "WiMANS: A Benchmark Dataset for WiFi-based Multi-user Activity Sensing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuokang Huang*",
      "Kaihan Li",
      "Di You",
      "Yichong Chen",
      "Arvin Lin",
      "Siying Liu",
      "Xiaohui Li",
      "Julie A. McCann*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5831_ECCV_2024_paper.php": {
    "title": "Embedding-Free Transformer with Inference Spatial Reduction for Efficient Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyunwoo Yu",
      "Yubin Cho",
      "Beoungwoo Kang",
      "Seunghun Moon",
      "Kyeongbo Kong",
      "Suk-Ju Kang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5836_ECCV_2024_paper.php": {
    "title": "VeCLIP: Improving CLIP Training via Visual-enriched Captions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengfeng Lai*",
      "Haotian Zhang",
      "Bowen Zhang",
      "Wentao Wu",
      "Haoping Bai",
      "Aleksei Timofeev",
      "Xianzhi Du",
      "Zhe Gan",
      "Jiulong Shan",
      "Chen-Nee Chuah",
      "Yinfei Yang",
      "Meng Cao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5837_ECCV_2024_paper.php": {
    "title": "Three Things We Need to Know About Transferring Stable Diffusion to Visual Dense Prediciton Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manyuan Zhang*",
      "Guanglu Song",
      "Xiaoyu Shi",
      "Yu Liu",
      "Hongsheng Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5841_ECCV_2024_paper.php": {
    "title": "Learning Representations from Foundation Models for Domain Generalized Stereo Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongjian Zhang",
      "Longguang Wang",
      "Kunhong Li",
      "WANG Yun",
      "Yulan Guo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5843_ECCV_2024_paper.php": {
    "title": "Spike-Temporal Latent Representation for Energy-Efficient Event-to-Video Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianxiong Tang*",
      "Jian-Huang Lai*",
      "Lingxiao Yang",
      "Xiaohua Xie"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5850_ECCV_2024_paper.php": {
    "title": "Effective Lymph Nodes Detection in CT Scans Using Location Debiased Query Selection and Contrastive Query Representation in Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qinji Yu*",
      "Yirui Wang*",
      "Ke Yan",
      "Haoshen Li",
      "Dazhou Guo",
      "Li Zhang",
      "Na Shen",
      "Qifeng Wang",
      "Xiaowei Ding",
      "Le Lu",
      "Xianghua Ye*",
      "Dakai Jin*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5851_ECCV_2024_paper.php": {
    "title": "Chat-Edit-3D: Interactive 3D Scene Editing via Text Prompts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "shuangkang fang*",
      "Yufeng Wang*",
      "Yi-Hsuan Tsai",
      "Yi Yang",
      "Wenrui Ding",
      "Shuchang Zhou",
      "Ming-Hsuan Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5857_ECCV_2024_paper.php": {
    "title": "Event-Adapted Video Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyu Xiao",
      "Dachun Kai",
      "Yueyi Zhang",
      "Zheng-Jun Zha",
      "Xiaoyan Sun",
      "Zhiwei Xiong*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5859_ECCV_2024_paper.php": {
    "title": "Look Hear: Gaze Prediction for Speech-directed Human Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sounak Mondal*",
      "Seoyoung Ahn",
      "Zhibo Yang",
      "Niranjan Balasubramanian",
      "Dimitris Samaras",
      "Gregory Zelinsky",
      "Minh Hoai"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5862_ECCV_2024_paper.php": {
    "title": "Raising the Ceiling: Conflict-Free Local Feature Matching with Dynamic View Switching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyong Lu*",
      "Songlin Du*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5863_ECCV_2024_paper.php": {
    "title": "Q&A Prompts: Discovering Rich Visual Clues through Mining Question-Answer Prompts for VQA requiring Diverse World Knowledge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haibo Wang*",
      "Weifeng Ge*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5875_ECCV_2024_paper.php": {
    "title": "Catastrophic Overfitting: A Potential Blessing in Disguise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "MN Zhao",
      "Lihe Zhang*",
      "Yuqiu Kong",
      "Baocai Yin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5881_ECCV_2024_paper.php": {
    "title": "Long-range Turbulence Mitigation: A Large-scale Dataset and A Coarse-to-fine Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengqi Xu",
      "Run Sun",
      "Yi Chang*",
      "Shuning Cao",
      "Xueyao Xiao",
      "Luxin Yan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5885_ECCV_2024_paper.php": {
    "title": "SparseCtrl: Adding Sparse Controls to Text-to-Video Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuwei Guo",
      "Ceyuan Yang*",
      "Anyi Rao",
      "Maneesh Agrawala",
      "Dahua Lin*",
      "Bo Dai*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5894_ECCV_2024_paper.php": {
    "title": "Visual Alignment Pre-training for Sign Language Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peiqi Jiao",
      "Yuecong Min",
      "Xilin Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5898_ECCV_2024_paper.php": {
    "title": "Parrot Captions Teach CLIP to Spot Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiqi Lin",
      "Conghui He*",
      "Alex Jinpeng Wang",
      "Bin Wang",
      "Weijia Li",
      "Mike Zheng Shou"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5902_ECCV_2024_paper.php": {
    "title": "Solving Motion Planning Tasks with a Scalable Generative Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihan Hu*",
      "Siqi Chai",
      "Zhening Yang",
      "Jingyu Qian",
      "Kun Li",
      "Wenxin Shao",
      "Haichao Zhang",
      "Wei Xu",
      "Qiang Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5904_ECCV_2024_paper.php": {
    "title": "Griffon: Spelling out All Object Locations at Any Granularity with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufei Zhan",
      "Yousong Zhu*",
      "Zhiyang Chen",
      "Fan Yang",
      "Ming Tang",
      "Jinqiao Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5909_ECCV_2024_paper.php": {
    "title": "Vision-Language Action Knowledge Learning for Semantic-Aware Action Quality Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huangbiao Xu",
      "Xiao Ke*",
      "Yuezhou Li",
      "Rui Xu",
      "Huanqi Wu",
      "Xiaofeng Lin",
      "Wenzhong Guo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5912_ECCV_2024_paper.php": {
    "title": "Knowledge Transfer with Simulated Inter-Image Erasing for Weakly Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Chen*",
      "Xiruo Jiang",
      "Gensheng Pei",
      "Zeren Sun",
      "Yucheng Wang",
      "Yazhou Yao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5913_ECCV_2024_paper.php": {
    "title": "BurstM: Deep Burst Multi-scale SR using Fourier Space with Optical Flow",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "EungGu Kang*",
      "Byeonghun Lee",
      "Sunghoon Im",
      "Kyong Hwan Jin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5914_ECCV_2024_paper.php": {
    "title": "Diffusion Reward: Learning Rewards via Conditional Video Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Huang*",
      "Guangqi Jiang",
      "Yanjie Ze",
      "Huazhe Xu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5917_ECCV_2024_paper.php": {
    "title": "Recursive Visual Programming",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxin Ge*",
      "Sanjay Subramanian",
      "Baifeng Shi",
      "Roei Herzig",
      "Trevor Darrell"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5918_ECCV_2024_paper.php": {
    "title": "LLaVA-Grounding: Grounded Visual Chat with Large Multimodal Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Zhang*",
      "Hongyang Li",
      "Feng Li",
      "Tianhe Ren",
      "Xueyan Zou",
      "Shilong Liu",
      "Shijia Huang",
      "Jianfeng Gao",
      "Lei Zhang",
      "Chunyuan Li",
      "Jianwei Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5923_ECCV_2024_paper.php": {
    "title": "Prompt-Driven Contrastive Learning for Transferable Adversarial Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hunmin Yang",
      "Jongoh Jeong",
      "Kuk-Jin Yoon*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5934_ECCV_2024_paper.php": {
    "title": "Learning to Adapt SAM for Segmenting Cross-domain Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xidong Peng",
      "Runnan Chen",
      "Feng Qiao",
      "Lingdong Kong",
      "Youquan Liu",
      "Yujing Sun",
      "Tai Wang",
      "Xinge Zhu*",
      "Yuexin Ma*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5939_ECCV_2024_paper.php": {
    "title": "Learning to Enhance Aperture Phasor Field for Non-Line-of-Sight Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "In Cho",
      "Hyunbo Shim",
      "Seon Joo Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5951_ECCV_2024_paper.php": {
    "title": "ViewFormer: Exploring Spatiotemporal Modeling for Multi-View 3D Occupancy Perception via View-Guided Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinke Li*",
      "Xiao He*",
      "Chonghua Zhou",
      "Xiaoqiang Cheng",
      "Yang Wen",
      "Dan Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5952_ECCV_2024_paper.php": {
    "title": "Fine-grained Dynamic Network for Generic Event Boundary Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziwei Zheng",
      "Lijun He",
      "Le Yang",
      "Fan Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5955_ECCV_2024_paper.php": {
    "title": "Take A Step Back: Rethinking the Two Stages in Visual Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingyu Zhang",
      "Jiting Cai",
      "Mingyu Liu",
      "Yue Xu",
      "Cewu Lu",
      "Yong-Lu Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5957_ECCV_2024_paper.php": {
    "title": "AlignZeg: Mitigating Objective Misalignment for Zero-shot Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiannan Ge*",
      "Lingxi Xie",
      "Hongtao Xie",
      "Pandeng Li",
      "Xiaopeng Zhang",
      "Yongdong Zhang",
      "Qi Tian"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5958_ECCV_2024_paper.php": {
    "title": "Learning with Counterfactual Explanations for Radiology Report Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingjie Li*",
      "Haokun Lin",
      "Liang Qiu",
      "Xiaodan Liang*",
      "Ling Chen",
      "Abdulmotaleb Elsaddik",
      "Xiaojun Chang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5967_ECCV_2024_paper.php": {
    "title": "SpeedUpNet: A Plug-and-Play Adapter Network for Accelerating Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weilong Chai*",
      "Dandan Zheng",
      "Jiajiong Cao",
      "Zhiquan Chen",
      "Changbao Wang",
      "Chenguang Ma"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5971_ECCV_2024_paper.php": {
    "title": "Better Regression Makes Better Test-time Adaptive 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiakang Yuan",
      "Bo Zhang",
      "Kaixiong Gong",
      "Xiangyu Yue",
      "Botian Shi",
      "Yu Qiao",
      "Tao Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5974_ECCV_2024_paper.php": {
    "title": "ShapeLLM: Universal 3D Object Understanding for Embodied Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zekun Qi",
      "Runpei Dong",
      "Shaochen Zhang",
      "Haoran Geng",
      "Chunrui Han",
      "Zheng Ge",
      "Li Yi*",
      "Kaisheng Ma*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5976_ECCV_2024_paper.php": {
    "title": "Content-Aware Radiance Fields: Aligning Model Complexity with Scene Intricacy Through Learned Bitwidth Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weihang Liu",
      "Xue Xian Zheng",
      "Jingyi Yu",
      "Xin Lou*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5978_ECCV_2024_paper.php": {
    "title": "Finding Visual Task Vectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alberto Hojel*",
      "Yutong Bai",
      "Trevor Darrell",
      "Amir Globerson",
      "Amir Bar*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5982_ECCV_2024_paper.php": {
    "title": "Connecting Consistency Distillation to Score Distillation for Text-to-3D Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zongrui Li*",
      "Minghui Hu",
      "Qian Zheng*",
      "Xudong Jiang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5986_ECCV_2024_paper.php": {
    "title": "Event Camera Data Dense Pre-training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Yang",
      "Liyuan Pan*",
      "Liu liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5989_ECCV_2024_paper.php": {
    "title": "Distractors-Immune Representation Learning with Cross-modal Contrastive Regularization for Change Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunbin Tu*",
      "Liang Li",
      "Li Su",
      "Chenggang Yan",
      "Qingming Huang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5996_ECCV_2024_paper.php": {
    "title": "Rethinking Image-to-Video Adaptation: An Object-centric Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Qian*",
      "Shuangrui Ding",
      "Dahua Lin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5997_ECCV_2024_paper.php": {
    "title": "Layer-Wise Relevance Propagation with Conservation Property for ResNet",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seitaro Otsuki*",
      "Tsumugi Iida*",
      "Félix Doublet*",
      "Tsubasa Hirakawa*",
      "Takayoshi Yamashita*",
      "Hironobu Fujiyoshi*",
      "Komei Sugiura*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6001_ECCV_2024_paper.php": {
    "title": "DECap: Towards Generalized Explicit Caption Editing via Diffusion Mechanism",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Wang",
      "Xinyun Jiang",
      "Jun Xiao",
      "Tao Chen",
      "Long Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6006_ECCV_2024_paper.php": {
    "title": "EgoLifter: Open-world 3D Segmentation for Egocentric Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiao Gu*",
      "Zhaoyang Lv*",
      "Duncan Frost",
      "Simon Green",
      "Julian Straub",
      "Chris Sweeney*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6012_ECCV_2024_paper.php": {
    "title": "MEVG : Multi-event Video Generation with Text-to-Video Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gyeongrok Oh*",
      "Jaehwan Jeong",
      "Sieun Kim",
      "Wonmin Byeon",
      "Jinkyu Kim",
      "Sungwoong Kim",
      "Sangpil Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6014_ECCV_2024_paper.php": {
    "title": "Open-Vocabulary SAM: Segment and Recognize Twenty-thousand Classes Interactively",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haobo Yuan",
      "Xiangtai Li*",
      "Chong Zhou",
      "Yining Li",
      "Kai Chen",
      "Chen Change Loy"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6020_ECCV_2024_paper.php": {
    "title": "Data-to-Model Distillation: Data-Efficient Learning Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ahmad Sajedi*",
      "Samir Khaki",
      "Lucy Z. Liu",
      "Ehsan Amjadian",
      "Yuri A. Lawryshyn",
      "Konstantinos N. Plataniotis"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6022_ECCV_2024_paper.php": {
    "title": "DiffuX2CT: Diffusion Learning to Reconstruct CT Images from Biplanar X-Rays",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuhui Liu",
      "Zhi Qiao",
      "Runkun Liu",
      "Hong Li",
      "Xiantong Zhen*",
      "Zhen Qian",
      "Juan Zhang*",
      "Baochang Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6023_ECCV_2024_paper.php": {
    "title": "AdaIFL: Adaptive Image Forgery Localization via a Dynamic and Importance-aware Transformer Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxi Li*",
      "Fuyuan Cheng",
      "Wangbo Yu",
      "Guangshuo Wang",
      "Guibo Luo*",
      "Yuesheng Zhu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6025_ECCV_2024_paper.php": {
    "title": "ComFusion: Enhancing Personalized Generation by Instance-Scene Compositing and Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Hong*",
      "Yuxuan Duan",
      "Bo Zhang",
      "Haoxing Chen",
      "Jun Lan",
      "Huijia Zhu",
      "Weiqiang Wang",
      "Jianfu Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6026_ECCV_2024_paper.php": {
    "title": "ML-SemReg: Boosting Point Cloud Registration with Multi-level Semantic Consistency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaocheng Yan",
      "Pengcheng Shi",
      "Jiayuan Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6027_ECCV_2024_paper.php": {
    "title": "Mask as Supervision: Leveraging Unified Mask Information for Unsupervised 3D Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchen Yang",
      "Yu Qiao",
      "Xiao Sun*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6030_ECCV_2024_paper.php": {
    "title": "MoVideo: Motion-Aware Video Generation with Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyun Liang*",
      "Yuchen Fan",
      "Kai Zhang*",
      "Radu Timofte",
      "Luc Van Gool",
      "Rakesh Ranjan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6031_ECCV_2024_paper.php": {
    "title": "SHERL: Synthesizing High Accuracy and Efficient Memory for Resource-Limited Transfer Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiwen Diao*",
      "Bo Wan",
      "Xu Jia",
      "Yunzhi Zhuge",
      "Ying Zhang",
      "Huchuan Lu*",
      "Long Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6035_ECCV_2024_paper.php": {
    "title": "MonoTTA: Fully Test-Time Adaptation for Monocular 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongbin Lin",
      "Yifan Zhang",
      "Shuaicheng Niu",
      "Shuguang Cui",
      "Zhen Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6037_ECCV_2024_paper.php": {
    "title": "RangeLDM: Fast Realistic LiDAR Point Cloud Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qianjiang Hu",
      "Zhimin Zhang",
      "Wei Hu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6039_ECCV_2024_paper.php": {
    "title": "Learn to Optimize Denoising Scores: A Unified and Improved Diffusion Prior for 3D Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaofeng Yang*",
      "Yiwen Chen",
      "Cheng Chen",
      "Chi Zhang",
      "Yi Xu",
      "Xulei Yang",
      "Fayao Liu",
      "Guosheng Lin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6040_ECCV_2024_paper.php": {
    "title": "Be-Your-Outpainter: Mastering Video Outpainting through Input-Specific Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fu-Yun Wang*",
      "Xiaoshi Wu",
      "Zhaoyang Huang",
      "Xiaoyu Shi",
      "Dazhong Shen",
      "Guanglu Song",
      "Yu Liu",
      "Hongsheng Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6042_ECCV_2024_paper.php": {
    "title": "Physically Plausible Color Correction for Neural Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Zhang*",
      "Ying Feng",
      "HONGDONG LI*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6043_ECCV_2024_paper.php": {
    "title": "Unifying 3D Vision-Language Understanding via Promptable Queries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "ziyu zhu*",
      "Zhuofan Zhang",
      "Xiaojian Ma",
      "Xuesong Niu",
      "Yixin Chen",
      "Baoxiong Jia",
      "Zhidong Deng*",
      "Siyuan Huang*",
      "Qing Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6044_ECCV_2024_paper.php": {
    "title": "Model Stock: All we need is just a few fine-tuned models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dong-Hwan Jang",
      "Sangdoo Yun",
      "Dongyoon Han*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6048_ECCV_2024_paper.php": {
    "title": "Motion-Guided Latent Diffusion for Temporally Consistent Real-world Video Super-resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xi Yang*",
      "Chenhang He",
      "Jianqi Ma",
      "Lei Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6050_ECCV_2024_paper.php": {
    "title": "PoseCrafter: One-Shot Personalized Video Synthesis Following Flexible Pose Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yong Zhong",
      "Min Zhao",
      "Zebin You",
      "Xiaofeng Yu",
      "Changwang Zhang",
      "Chongxuan Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6052_ECCV_2024_paper.php": {
    "title": "MAD-DR: Map Compression for Visual Localization with Matchness Aware Descriptor Dimension Reduction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiang Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6053_ECCV_2024_paper.php": {
    "title": "Benchmarking Object Detectors with COCO: A New Path Forward",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shweta Singh",
      "Aayan Yadav",
      "Jitesh Jain",
      "Humphrey Shi",
      "Justin Johnson",
      "Karan Desai*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6054_ECCV_2024_paper.php": {
    "title": "Adaptive High-Frequency Transformer for Diverse Wildlife Re-Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenyue Li",
      "Shuoyi Chen",
      "Mang Ye*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6063_ECCV_2024_paper.php": {
    "title": "WPS-SAM: Towards Weakly-Supervised Part Segmentation with Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin-Jian Wu*",
      "Ruisong Zhang",
      "Jie Qin",
      "Shijie Ma",
      "Cheng-Lin Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6068_ECCV_2024_paper.php": {
    "title": "Lane Graph as Path: Continuity-preserving Path-wise Modeling for Online Lane Graph Construction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bencheng Liao",
      "Shaoyu Chen",
      "Bo Jiang",
      "Tianheng Cheng",
      "Qian Zhang",
      "Wenyu Liu",
      "Chang Huang",
      "Xinggang Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6071_ECCV_2024_paper.php": {
    "title": "DeCo: Decoupled Human-Centered Diffusion Video Editing with Motion Consistency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaojing Zhong",
      "Xinyi Huang",
      "Xiaofeng Yang",
      "Guosheng Lin*",
      "Qingyao Wu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6072_ECCV_2024_paper.php": {
    "title": "Unleashing the Potential of the Semantic Latent Space in Diffusion Models for Image Dehazing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zizheng Yang",
      "Hu Yu",
      "Bing Li",
      "Jinghao Zhang",
      "Jie Huang",
      "Feng Zhao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6074_ECCV_2024_paper.php": {
    "title": "Uncertainty-aware sign language video retrieval with probability distribution modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan Wu*",
      "Hongxiang Li",
      "yuanjiang luo",
      "Xuxin Cheng",
      "Xianwei Zhuang",
      "Meng Cao",
      "Keren Fu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6076_ECCV_2024_paper.php": {
    "title": "NeRMo: Learning Implicit Neural Representations for 3D Human Motion Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dong Wei",
      "Huaijiang Sun",
      "Xiaoning Sun*",
      "Shengxiang Hu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6085_ECCV_2024_paper.php": {
    "title": "Bridging Synthetic and Real Worlds for Pre-training Scene Text Detectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tongkun Guan",
      "Wei Shen*",
      "Xue Yang",
      "Xuehui Wang",
      "Xiaokang Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6086_ECCV_2024_paper.php": {
    "title": "VLAD-BuFF: Burst-aware Fast Feature Aggregation for Visual Place Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ahmad Khaliq",
      "Ming Xu",
      "Stephen Hausler",
      "Michael J Milford",
      "Sourav Garg*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6090_ECCV_2024_paper.php": {
    "title": "DSA: Discriminative Scatter Analysis for Early Smoke Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lujian Yao*",
      "Haitao Zhao*",
      "Jingchao Peng",
      "Zhongze Wang",
      "Kaijie Zhao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6102_ECCV_2024_paper.php": {
    "title": "SAFARI: Adaptive Sequence Transformer for Weakly Supervised Referring Expression Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sayan Nag*",
      "Koustava Goswami",
      "Srikrishna Karanam"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6103_ECCV_2024_paper.php": {
    "title": "KFD-NeRF: Rethinking Dynamic NeRF with Kalman Filter",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Zhan",
      "Zhuoxiao Li",
      "Muyao Niu",
      "Zhihang Zhong",
      "Shohei Nobuhara",
      "Ko Nishino",
      "Yinqiang Zheng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6110_ECCV_2024_paper.php": {
    "title": "Physical-Based Event Camera Simulator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiqian Han",
      "Jiacheng Lyu",
      "Jianing Li*",
      "Henglu Wei",
      "Cheng Li",
      "Yajing Wei",
      "SHU CHEN",
      "Xiangyang Ji*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6117_ECCV_2024_paper.php": {
    "title": "V-IRL: Grounding Virtual Intelligence in Real Life",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jihan Yang*",
      "Runyu Ding",
      "Ellis L Brown",
      "Xiaojuan Qi",
      "Saining Xie"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6118_ECCV_2024_paper.php": {
    "title": "Adversarial Prompt Tuning for Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaming Zhang",
      "Xingjun Ma*",
      "Xin Wang",
      "Lingyu Qiu",
      "Jiaqi Wang",
      "Yu-Gang Jiang",
      "Jitao Sang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6121_ECCV_2024_paper.php": {
    "title": "Relightable 3D Gaussians: Realistic Point Cloud Relighting with BRDF Decomposition and Ray Tracing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Gao",
      "chun gu",
      "Youtian Lin",
      "Zhihao Li",
      "Hao Zhu",
      "Xun Cao",
      "Li Zhang*",
      "Yao Yao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6123_ECCV_2024_paper.php": {
    "title": "Mono-ViFI: A Unified Learning Framework for Self-supervised Single- and Multi-frame Monocular Depth Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinfeng Liu*",
      "Lingtong Kong",
      "Bo Li",
      "Zerong Wang",
      "Hong Gu",
      "Jinwei Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6124_ECCV_2024_paper.php": {
    "title": "CC-SAM: Enhancing SAM with Cross-feature Attention and Context for Ultrasound Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shreyank N Gowda*",
      "David A Clifton"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6128_ECCV_2024_paper.php": {
    "title": "An Efficient and Effective Transformer Decoder-Based Framework for Multi-Task Visual Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Chen",
      "Long Chen",
      "Yu Wu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6129_ECCV_2024_paper.php": {
    "title": "Think2Drive: Efficient Reinforcement Learning by Thinking with Latent World Model for Autonomous Driving (in CARLA-v2)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qifeng Li*",
      "Xiaosong Jia",
      "Shaobo Wang",
      "Junchi Yan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6138_ECCV_2024_paper.php": {
    "title": "PanGu-Draw: Advancing Resource-Efficient Text-to-Image Synthesis with Time-Decoupled Training and Reusable Coop-Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guansong Lu*",
      "Yuanfan Guo",
      "Jianhua Han",
      "Minzhe Niu",
      "Yihan Zeng",
      "Songcen Xu",
      "Zeyi Huang",
      "Zhao Zhong",
      "Wei Zhang",
      "Hang Xu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6140_ECCV_2024_paper.php": {
    "title": "X-InstructBLIP: A Framework for Aligning Image, 3D, Audio, Video to LLMs and its Emergent Cross-modal Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Artemis Panagopoulou*",
      "Le Xue",
      "Ning Yu",
      "LI JUNNAN",
      "DONGXU LI",
      "Shafiq Joty",
      "Ran Xu",
      "Silvio Savarese",
      "Caiming Xiong",
      "Juan Carlos Niebles"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6141_ECCV_2024_paper.php": {
    "title": "Learning Neural Volumetric Pose Features for Camera Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyu Lin",
      "Jiaqi Gu",
      "Bojian Wu",
      "Lubin Fan*",
      "Renjie Chen*",
      "Ligang Liu",
      "Jieping Ye"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6142_ECCV_2024_paper.php": {
    "title": "Betrayed by Attention: A Simple yet Effective Approach for Self-supervised Video Object Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuangrui Ding*",
      "Rui Qian",
      "Haohang Xu",
      "Dahua Lin",
      "Hongkai Xiong"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6148_ECCV_2024_paper.php": {
    "title": "REFRAME: Reflective Surface Real-Time Rendering for Mobile Devices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaojie Ji*",
      "Yufeng Li",
      "Yiyi Liao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6150_ECCV_2024_paper.php": {
    "title": "Self-Training Room Layout via Geometry-aware Ray-casting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bolivar Solarte*",
      "Chin-Hsuan Wu*",
      "Jin-Cheng Jhang*",
      "Jonathan Lee*",
      "Yi-Hsuan Tsai*",
      "Min Sun*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6162_ECCV_2024_paper.php": {
    "title": "Closed-Loop Unsupervised Representation Disentanglement with $\\\\beta$-VAE Distillation and Diffusion Probabilistic Feedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Jin*",
      "Bohan Li*",
      "Baao Xie",
      "Wenyao Zhang",
      "Jinming Liu",
      "Ziqiang Li",
      "Tao Yang",
      "Wenjun Zeng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6164_ECCV_2024_paper.php": {
    "title": "Rethinking Weakly-supervised Video Temporal Grounding From a Game Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Fang",
      "Zeyu Xiong",
      "Wanlong Fang",
      "Xiaoye Qu",
      "Chen Chen",
      "Jianfeng Dong",
      "Keke Tang",
      "Pan Zhou*",
      "Yu Cheng",
      "Daizong Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6171_ECCV_2024_paper.php": {
    "title": "Every Pixel Has its Moments: Ultra-High-Resolution Unpaired Image-to-Image Translation via Dense Normalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ming-Yang Ho",
      "Che-Ming Wu",
      "Min-Sheng Wu",
      "‪Yufeng Jane Tseng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6174_ECCV_2024_paper.php": {
    "title": "ZoLA: Zero-Shot Creative Long Animation Generation with Short Video Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fu-Yun Wang*",
      "Zhaoyang Huang*",
      "Qiang Ma",
      "Guanglu Song",
      "Xudong LU",
      "Weikang Bian",
      "Yijin Li",
      "Yu Liu",
      "Hongsheng Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6176_ECCV_2024_paper.php": {
    "title": "Parameter-Efficient and Memory-Efficient Tuning for Vision Transformer: A Disentangled Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taolin Zhang",
      "Jiawang Bai",
      "Zhihe Lu",
      "Dongze Lian",
      "genping wang*",
      "Xinchao Wang*",
      "Shu-Tao Xia"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6185_ECCV_2024_paper.php": {
    "title": "Restore Anything with Masks: Leveraging Mask Image Modeling for Blind All-in-One Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chujie Qin",
      "Ruiqi Wu",
      "Zikun Liu",
      "Xin Lin",
      "Chun-Le Guo",
      "Hyun Hee Park",
      "Chongyi Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6190_ECCV_2024_paper.php": {
    "title": "When Fast Fourier Transform Meets Transformer for Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyu Jiang",
      "Xiuhui Zhang",
      "Ning Gao",
      "Yue Deng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6203_ECCV_2024_paper.php": {
    "title": "Dolphins: Multimodal Language Model for Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingzi Ma",
      "Yulong Cao",
      "Jiachen Sun",
      "Marco Pavone",
      "Chaowei Xiao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6210_ECCV_2024_paper.php": {
    "title": "Rethinking Video Deblurring with Wavelet-Aware Dynamic Transformer and Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Rao",
      "Guangyuan Li",
      "Zehua Lan",
      "Jiakai Sun",
      "Junsheng Luan",
      "Wei Xing*",
      "Lei Zhao*",
      "Huaizhong Lin*",
      "Jianfeng Dong",
      "Dalong Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6215_ECCV_2024_paper.php": {
    "title": "CamoTeacher: Dual-Rotation Consistency Learning for Semi-Supervised Camouflaged Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "xunfa lai",
      "Zhiyu Yang",
      "Jie Hu",
      "ShengChuan Zhang*",
      "Liujuan Cao",
      "Guannan Jiang",
      "Songan Zhang",
      "zhiyu wang",
      "Rongrong Ji"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6218_ECCV_2024_paper.php": {
    "title": "Placing Objects in Context via Inpainting for Out-of-distribution Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pau de Jorge Aranda*",
      "Riccardo Volpi",
      "Puneet Dokania",
      "Philip Torr",
      "Gregory Rogez"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6219_ECCV_2024_paper.php": {
    "title": "Textual Grounding for Open-vocabulary Visual Information Extraction in Layout-diversified Documents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengjun Cheng",
      "Chengquan Zhang",
      "Chang Liu*",
      "Yuke Li",
      "Bohan Li",
      "Kun Yao",
      "Xiawu Zheng",
      "Rongrong Ji",
      "Jie Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6222_ECCV_2024_paper.php": {
    "title": "Teddy: Efficient Large-Scale Dataset Distillation via Taylor-Approximated Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruonan Yu",
      "Songhua Liu",
      "Jingwen Ye",
      "Xinchao Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6227_ECCV_2024_paper.php": {
    "title": "Rethinking and Improving Visual Prompt Selection for In-Context Learning Segmentation Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Suo",
      "Lanqing Lai",
      "Mengyang Sun",
      "Hanwang Zhang",
      "Peng Wang*",
      "Yanning Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6231_ECCV_2024_paper.php": {
    "title": "D4-VTON: Dynamic Semantics Disentangling for Differential Diffusion based Virtual Try-On",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaotong Yang",
      "Zicheng Jiang",
      "Xinzhe Li",
      "Huiyu Zhou",
      "Junyu Dong",
      "Huaidong Zhang",
      "Yong Du*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6234_ECCV_2024_paper.php": {
    "title": "TC4D: Trajectory-Conditioned Text-to-4D Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sherwin Bahmani*",
      "Xian Liu",
      "Wang Yifan",
      "Ivan Skorokhodov",
      "Victor Rong",
      "Ziwei Liu",
      "Xihui Liu",
      "Jeong Joon Park",
      "Sergey Tulyakov",
      "Gordon Wetzstein",
      "Andrea Tagliasacchi",
      "David B Lindell"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6235_ECCV_2024_paper.php": {
    "title": "Blind Image Deconvolution by Generative-based Kernel Prior and Initializer via Latent Encoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiangtao Zhang",
      "Zongsheng Yue*",
      "Hui Wang",
      "Qian Zhao*",
      "Deyu Meng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6243_ECCV_2024_paper.php": {
    "title": "AdvDiff: Generating Unrestricted Adversarial Examples using Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuelong Dai*",
      "Kaisheng Liang",
      "Bin Xiao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6251_ECCV_2024_paper.php": {
    "title": "Improving Text-guided Object Inpainting with Semantic Pre-inpainting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifu Chen",
      "Jingwen Chen",
      "Yingwei Pan*",
      "Yehao Li",
      "Ting Yao",
      "Zhineng Chen",
      "Tao Mei"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6262_ECCV_2024_paper.php": {
    "title": "Personalized Federated Domain-Incremental Learning based on Adaptive Knowledge Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yichen Li",
      "Wenchao Xu",
      "Haozhao Wang*",
      "Yining Qi*",
      "Jingcai Guo",
      "Ruixuan Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6265_ECCV_2024_paper.php": {
    "title": "ST-LDM: A Universal Framework for Text-Grounded Object Generation in Real Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangtian Xue",
      "Jiasong Wu*",
      "Youyong Kong",
      "Lotfi Senhadji",
      "Huazhong Shu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6268_ECCV_2024_paper.php": {
    "title": "RS-NeRF: Neural Radiance Fields from Rolling Shutter Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muyao Niu",
      "Tong Chen",
      "Yifan Zhan",
      "Zhuoxiao Li",
      "Xiang Ji",
      "Yinqiang Zheng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6270_ECCV_2024_paper.php": {
    "title": "Region-Adaptive Transform with Segmentation Prior for Image Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxi Liu*",
      "Wenhan Yang",
      "Huihui Bai",
      "Yunchao Wei",
      "Yao Zhao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6271_ECCV_2024_paper.php": {
    "title": "Enhancing Tracking Robustness with Auxiliary Adversarial Defense Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhewei Wu",
      "Ruilong Yu",
      "Qihe Liu*",
      "Shuying Cheng",
      "Shilin Qiu",
      "Shijie Zhou"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6273_ECCV_2024_paper.php": {
    "title": "SLIM: Spuriousness Mitigation with Minimal Human Annotations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiwei Xuan*",
      "Ziquan Deng",
      "Hsuan-Tien Lin",
      "Kwan-Liu Ma"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6275_ECCV_2024_paper.php": {
    "title": "Uncertainty Calibration with Energy Based Instance-wise Scaling in the Wild Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mijoo Kim",
      "Junseok Kwon*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6277_ECCV_2024_paper.php": {
    "title": "X-Pose: Detecting Any Keypoints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Yang",
      "Ailing Zeng*",
      "Ruimao Zhang*",
      "Lei Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6281_ECCV_2024_paper.php": {
    "title": "M^2Depth: Self-supervised Two-Frame Multi-camera Metric Depth Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingshuang Zou*",
      "Yikang Ding",
      "Xi Qiu",
      "Haoqian Wang*",
      "Haotian Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6283_ECCV_2024_paper.php": {
    "title": "UniMD: Towards Unifying Moment Retrieval and Temporal Action Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingsen Zeng",
      "Yujie Zhong*",
      "Chengjian Feng",
      "Lin Ma"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6288_ECCV_2024_paper.php": {
    "title": "DyFADet: Dynamic Feature Aggregation for Temporal Action Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Le Yang*",
      "Ziwei Zheng",
      "Yizeng Han",
      "Hao Cheng",
      "Shiji Song",
      "Gao Huang",
      "Fan Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6290_ECCV_2024_paper.php": {
    "title": "LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanwei Li*",
      "Chengyao Wang",
      "Jiaya Jia"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6292_ECCV_2024_paper.php": {
    "title": "MetaCap: Meta-learning Priors from Multi-View Imagery for Sparse-view Human Performance Capture and Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guoxing Sun*",
      "Rishabh Dabral",
      "Pascal Fua",
      "Christian Theobalt",
      "Marc Habermann"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6294_ECCV_2024_paper.php": {
    "title": "DiffPMAE: Diffusion Masked Autoencoders for Point Cloud Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanlong LI*",
      "Chamara Madarasingha",
      "Kanchana Thilakarathna"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6296_ECCV_2024_paper.php": {
    "title": "Multi-branch Collaborative Learning Network for 3D Visual Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhipeng Qian",
      "Yiwei Ma",
      "Zhekai Lin",
      "Jiayi Ji",
      "Xiawu Zheng",
      "Xiaoshuai Sun*",
      "Rongrong Ji"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6298_ECCV_2024_paper.php": {
    "title": "DynamiCrafter: Animating Open-domain Images with Video Diffusion Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinbo Xing*",
      "Menghan Xia",
      "Yong Zhang",
      "Haoxin Chen",
      "Wangbo Yu",
      "Hanyuan Liu",
      "Gongye Liu",
      "Xintao Wang",
      "Ying Shan",
      "Tien-Tsin Wong"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6299_ECCV_2024_paper.php": {
    "title": "Motion Aware Event Representation-driven Image Deblurring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhijing Sun",
      "Xueyang Fu",
      "Longzhuo Huang",
      "Aiping Liu",
      "Zheng-Jun Zha*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6301_ECCV_2024_paper.php": {
    "title": "Turbo: Informativity-Driven Acceleration Plug-In for Vision-Language Large Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Ju*",
      "Haicheng Wang",
      "Haozhe Cheng",
      "Xu Chen",
      "Zhonghua Zhai",
      "Weilin Huang",
      "Jinsong Lan",
      "Shuai Xiao*",
      "Bo Zheng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6306_ECCV_2024_paper.php": {
    "title": "WildRefer: 3D Object Localization in Large-scale Dynamic Scenes with Multi-modal Visual Data and Natural Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenxiang Lin",
      "Xidong Peng",
      "Peishan Cong",
      "Ge Zheng",
      "Yujing Sun",
      "Yuenan HOU",
      "Xinge Zhu",
      "Sibei Yang",
      "Yuexin Ma*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6307_ECCV_2024_paper.php": {
    "title": "RCS-Prompt: Learning Prompt to Rearrange Class Space for Prompt-based Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Longrong Yang",
      "Hanbin Zhao",
      "Yunlong Yu*",
      "Xiaodong Zeng",
      "Xi Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6313_ECCV_2024_paper.php": {
    "title": "Text-Anchored Score Composition: Tackling Condition Misalignment in Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luozhou Wang*",
      "Guibao Shen",
      "Wenhang Ge",
      "Guangyong Chen",
      "Yijun Li",
      "Yingcong Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6319_ECCV_2024_paper.php": {
    "title": "Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shilong Liu*",
      "Zhaoyang Zeng",
      "Tianhe Ren",
      "Feng Li",
      "Hao Zhang",
      "Jie Yang",
      "Qing Jiang",
      "Chunyuan Li",
      "Jianwei Yang",
      "Hang Su",
      "Jun Zhu",
      "Lei Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6320_ECCV_2024_paper.php": {
    "title": "Make Your ViT-based Multi-view 3D Detectors Faster via Token Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dingyuan Zhang",
      "Dingkang Liang*",
      "Zichang Tan",
      "Xiaoqing Ye",
      "Cheng Zhang",
      "Jingdong Wang",
      "Xiang Bai*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6321_ECCV_2024_paper.php": {
    "title": "OV-Uni3DETR: Towards Unified Open-Vocabulary 3D Object Detection via Cycle-Modality Propagation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenyu Wang*",
      "Ya-Li Li",
      "TAICHI LIU",
      "Hengshuang Zhao",
      "Shengjin Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6326_ECCV_2024_paper.php": {
    "title": "CatchBackdoor: Backdoor Detection via Critical Trojan Neural Path Fuzzing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haibo Jin",
      "Ruoxi Chen",
      "Jinyin Chen",
      "Haibin Zheng",
      "Yang Zhang",
      "Haohan Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6331_ECCV_2024_paper.php": {
    "title": "UCIP: A Universal Framework for Compressed Image Super-Resolution using Dynamic Prompt",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Li*",
      "Bingchen Li",
      "Yeying Jin",
      "Cuiling Lan",
      "Hanxin Zhu",
      "Yulin Ren",
      "Zhibo Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6341_ECCV_2024_paper.php": {
    "title": "LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shilong Liu*",
      "Hao Cheng",
      "Haotian Liu",
      "Hao Zhang",
      "Feng Li",
      "Tianhe Ren",
      "Xueyan Zou",
      "Jianwei Yang",
      "Hang Su",
      "Jun Zhu",
      "Lei Zhang",
      "Jianfeng Gao",
      "Chunyuan Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6346_ECCV_2024_paper.php": {
    "title": "ClearCLIP: Decomposing CLIP Representations for Dense Vision-Language Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengcheng Lan",
      "Chaofeng Chen",
      "Yiping Ke",
      "Xinjiang Wang",
      "Litong Feng*",
      "Wayne Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6348_ECCV_2024_paper.php": {
    "title": "Two-Stage Active Learning for Efficient Temporal Action Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhao Su",
      "Ehsan Elhamifar*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6349_ECCV_2024_paper.php": {
    "title": "TexDreamer: Towards Zero-Shot High-Fidelity 3D Human Texture Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufei Liu",
      "Junwei Zhu",
      "Junshu Tang",
      "Shijie Zhang",
      "Jiangning Zhang",
      "Weijian Cao",
      "Chengjie Wang",
      "Yunsheng Wu",
      "Dongjin Huang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6350_ECCV_2024_paper.php": {
    "title": "MVPGS: Excavating Multi-view Priors for Gaussian Splatting from Sparse Input Views",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wangze Xu",
      "Huachen Gao",
      "Shihe Shen",
      "Rui Peng",
      "Jianbo Jiao",
      "Ronggang Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6352_ECCV_2024_paper.php": {
    "title": "Domain-Adaptive 2D Human Pose Estimation via Dual Teachers in Extremely Low-Light Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihao Ai*",
      "Yifei Qi",
      "Bo Wang",
      "Yu Cheng",
      "Xinchao Wang",
      "Robby T. Tan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6355_ECCV_2024_paper.php": {
    "title": "Towards More Practical Group Activity Detection: A New Benchmark and Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongkeun Kim",
      "Youngkil Song",
      "Minsu Cho",
      "Suha Kwak*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6356_ECCV_2024_paper.php": {
    "title": "Depicting Beyond Scores: Advancing Image Quality Assessment through Multi-modal Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyuan You*",
      "Zheyuan Li",
      "Jinjin Gu*",
      "Zhenfei Yin",
      "Tianfan Xue*",
      "Chao Dong*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6357_ECCV_2024_paper.php": {
    "title": "Zero-Shot Image Feature Consensus with Deep Functional Maps",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinle Cheng",
      "Congyue Deng*",
      "Adam Harley",
      "Yixin Zhu*",
      "Leonidas Guibas*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6362_ECCV_2024_paper.php": {
    "title": "WindPoly: Polygonal Mesh Reconstruction via Winding Numbers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin He",
      "Chenlei Lv",
      "Pengdi Huang",
      "Hui Huang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6365_ECCV_2024_paper.php": {
    "title": "MinD-3D: Reconstruct High-quality 3D objects in Human Brain",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianxiong Gao",
      "Yuqian Fu",
      "Yun Wang",
      "Xuelin Qian",
      "Jianfeng Feng",
      "Yanwei Fu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6366_ECCV_2024_paper.php": {
    "title": "Tokenize Anything via Prompting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ting Pan*",
      "Lulu Tang",
      "Xinlong Wang*",
      "Shiguang Shan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6376_ECCV_2024_paper.php": {
    "title": "Geospecific View Generation - Geometry-Context Aware High-resolution Ground View Inference from Satellite Views",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ningli Xu",
      "Rongjun Qin*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6379_ECCV_2024_paper.php": {
    "title": "Scissorhands: Scrub Data Influence via Connection Sensitivity in Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Wu*",
      "Mehrtash Harandi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6388_ECCV_2024_paper.php": {
    "title": "City-on-Web: Real-time Neural Rendering of Large-scale Scenes on the Web",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaiwen Song",
      "Xiaoyi Zeng",
      "Chenqu Ren",
      "Juyong Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6392_ECCV_2024_paper.php": {
    "title": "GRAPE: Generalizable and Robust Multi-view Facial Capture",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Li",
      "Di Kang",
      "Zhenyu He*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6399_ECCV_2024_paper.php": {
    "title": "Training-Free Model Merging for Multi-target Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenyi Li",
      "Huan-ang Gao",
      "Mingju Gao",
      "Beiwen Tian",
      "Rong Zhi",
      "Hao Zhao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6405_ECCV_2024_paper.php": {
    "title": "Multi-RoI Human Mesh Recovery with Camera Consistency and Contrastive Losses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongwei Nie",
      "Changzhen Liu",
      "Chengjiang Long",
      "Qing Zhang",
      "Guiqing Li",
      "Hongmin Cai*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6407_ECCV_2024_paper.php": {
    "title": "Co-Student: Collaborating Strong and Weak Students for Sparsely Annotated Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lianjun Wu",
      "Jiangxiao Han",
      "Zengqiang Zheng",
      "Xinggang Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6409_ECCV_2024_paper.php": {
    "title": "Open-Vocabulary Camouflaged Object Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youwei Pang",
      "Xiaoqi Zhao",
      "JiaMing Zuo",
      "Lihe Zhang*",
      "Huchuan Lu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6412_ECCV_2024_paper.php": {
    "title": "SmartControl: Enhancing ControlNet for Handling Rough Visual Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyu Liu",
      "Yuxiang Wei",
      "Ming Liu*",
      "Xianhui Lin",
      "Peiran Ren",
      "xuansong xie",
      "Wangmeng Zuo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6414_ECCV_2024_paper.php": {
    "title": "InterFusion: Text-Driven Generation of 3D Human-Object Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sisi Dai",
      "Wenhao Li",
      "Haowen Sun",
      "Haibin Huang",
      "Chongyang Ma",
      "Hui Huang",
      "Kai Xu*",
      "Ruizhen Hu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6415_ECCV_2024_paper.php": {
    "title": "GLARE: Low Light Image Enhancement via Generative Latent Feature based Codebook Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Zhou",
      "Wei Dong",
      "Xiaohong Liu*",
      "Shuaicheng Liu",
      "Xiongkuo Min",
      "Guangtao Zhai",
      "Jun Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6416_ECCV_2024_paper.php": {
    "title": "DriveDreamer: Towards Real-world-driven World Models for Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaofeng Wang*",
      "Zheng Zhu",
      "Guan Huang",
      "Chen Xinze",
      "Jiagang Zhu",
      "Jiwen Lu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6418_ECCV_2024_paper.php": {
    "title": "Flow-Assisted Motion Learning Network for Weakly-Supervised Group Activity Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Adi Nugroho*",
      "Sangmin Woo",
      "Sumin Lee",
      "Jinyoung Park",
      "Yooseung Wang",
      "Donguk Kim",
      "Changick Kim"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6424_ECCV_2024_paper.php": {
    "title": "NeRF-XL: NeRF at Any Scale with Multi-GPU",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruilong Li*",
      "Sanja Fidler",
      "Angjoo Kanazawa",
      "Francis Williams"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6425_ECCV_2024_paper.php": {
    "title": "CoSIGN: Few-Step Guidance of ConSIstency Model to Solve General INverse Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiankun Zhao",
      "Bowen Song",
      "Liyue Shen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6429_ECCV_2024_paper.php": {
    "title": "The First to Know: How Token Distributions Reveal Hidden Knowledge in Large Vision-Language Models?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qinyu Zhao*",
      "Ming Xu",
      "Kartik Gupta",
      "Akshay Asthana",
      "Liang Zheng",
      "Stephen Gould"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6434_ECCV_2024_paper.php": {
    "title": "Compositional Substitutivity of Visual Reasoning for Visual Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuanhao Li",
      "Zhen Li",
      "Chenchen Jing*",
      "Yuwei Wu*",
      "Mingliang Zhai",
      "Yunde Jia"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6440_ECCV_2024_paper.php": {
    "title": "LightenDiffusion: Unsupervised Low-Light Image Enhancement with Latent-Retinex Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hai Jiang",
      "Ao Luo",
      "Xiaohong Liu",
      "Songchen Han",
      "Shuaicheng Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6442_ECCV_2024_paper.php": {
    "title": "DNI: Dilutional Noise Initialization for Diffusion Video Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sunjae Yoon",
      "Gwanhyeong Koo",
      "Ji Woo Hong",
      "Chang D. Yoo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6444_ECCV_2024_paper.php": {
    "title": "Two-Stage Video Shadow Detection via Temporal-Spatial Adaption",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Duan",
      "Yu Cao",
      "Lei Zhu",
      "Gang Fu",
      "Xin Wang",
      "Renjie ZHANG",
      "Ping Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6445_ECCV_2024_paper.php": {
    "title": "Towards Physical World Backdoor Attacks against Skeleton Action Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qichen Zheng",
      "Yi Yu",
      "SIYUAN YANG*",
      "Jun Liu",
      "Kwok-Yan Lam",
      "Alex Kot"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6451_ECCV_2024_paper.php": {
    "title": "SAM-guided Graph Cut for 3D Instance Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu Guo*",
      "He Zhu",
      "Sida Peng",
      "Yuang Wang",
      "Yujun Shen",
      "Ruizhen Hu*",
      "Xiaowei Zhou*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6457_ECCV_2024_paper.php": {
    "title": "Fully Authentic Visual Question Answering Dataset from Online Communities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chongyan Chen*",
      "Mengchen Liu",
      "Noel C Codella",
      "Yunsheng Li",
      "Lu Yuan",
      "Danna Gurari"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6459_ECCV_2024_paper.php": {
    "title": "Active Generation for Image Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Huang",
      "Jiaqi Liu",
      "Shan You*",
      "Chang Xu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6460_ECCV_2024_paper.php": {
    "title": "FuseTeacher: Modality-fused Encoders are Strong Vision Supervisors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen-Wei Xie*",
      "Siyang Sun",
      "Liming Zhao",
      "Pandeng Li",
      "Shuailei Ma",
      "Yun Zheng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6463_ECCV_2024_paper.php": {
    "title": "Learning Local Pattern Modularization for Point Cloud Reconstruction from Unseen Classes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Chen",
      "Yu-Shen Liu*",
      "Zhizhong Han"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6472_ECCV_2024_paper.php": {
    "title": "Understanding Multi-compositional learning in Vision and Language models via Category Theory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sotirios Panagiotis Chytas*",
      "Hyunwoo J Kim",
      "Vikas Singh"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6476_ECCV_2024_paper.php": {
    "title": "FedRA: A Random Allocation Strategy for Federated Tuning to Unleash the Power of Heterogeneous Clients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shangchao Su",
      "Bin Li*",
      "Xiangyang Xue"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6479_ECCV_2024_paper.php": {
    "title": "Panel-Specific Degradation Representation for Raw Under-Display Camera Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youngjin Oh*",
      "Keuntek Lee",
      "Jooyoung Lee",
      "Dae-Hyun Lee",
      "Nam Ik Cho"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6480_ECCV_2024_paper.php": {
    "title": "Unlocking Textual and Visual Wisdom: Open-Vocabulary 3D Object Detection Enhanced by Comprehensive Guidance from Text and Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengkun Jiao*",
      "Na Zhao*",
      "Jingjing Chen",
      "Yu-Gang Jiang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6482_ECCV_2024_paper.php": {
    "title": "Diffusion-Guided Weakly Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sung-Hoon Yoon",
      "Hoyong Kwon",
      "Jaeseok Jeong",
      "Daehee Park",
      "Kuk-Jin Yoon*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6485_ECCV_2024_paper.php": {
    "title": "Weakly-Supervised Spatio-Temporal Video Grounding with Variational Cross-Modal Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Jin*",
      "Yadong Mu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6487_ECCV_2024_paper.php": {
    "title": "When Pedestrian Detection Meets Multi-Modal Learning: Generalist Model and Benchmark Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Zhang",
      "Wang Zeng",
      "Sheng Jin",
      "Chen Qian*",
      "Ping Luo",
      "Wentao Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6489_ECCV_2024_paper.php": {
    "title": "NVS-Adapter: Plug-and-Play Novel View Synthesis from a Single Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yoonwoo Jeong",
      "Jinwoo Lee",
      "Chiheon Kim",
      "Minsu Cho*",
      "Doyup Lee*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6495_ECCV_2024_paper.php": {
    "title": "Segment and Recognize Anything at Any Granularity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feng Li*",
      "Hao Zhang",
      "Peize Sun",
      "Xueyan Zou",
      "Shilong Liu",
      "Chunyuan Li",
      "Jianwei Yang",
      "Lei Zhang*",
      "Jianfeng Gao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6496_ECCV_2024_paper.php": {
    "title": "Real-time Holistic Robot Pose Estimation with Unknown States",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shikun Ban",
      "Juling Fan",
      "Xiaoxuan Ma",
      "Wentao Zhu*",
      "Yu QIAO*",
      "Yizhou Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6497_ECCV_2024_paper.php": {
    "title": "CLOSER: Towards Better Representation Learning for Few-Shot Class-Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junghun Oh",
      "Sungyong Baik",
      "Kyoung Mu Lee*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6499_ECCV_2024_paper.php": {
    "title": "A Simple Baseline for Spoken Language to Sign Language Translation with 3D Avatars",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ronglai Zuo",
      "Fangyun Wei*",
      "Zenggui Chen",
      "Brian Mak",
      "Jiaolong Yang",
      "Xin Tong"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6511_ECCV_2024_paper.php": {
    "title": "An accurate detection is not all you need to combat label noise in web-noisy datasets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paul Albert*",
      "Kevin McGuinness",
      "Eric Arazo",
      "Tarun Krishna",
      "Noel O Connor",
      "Jack Valmadre"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6513_ECCV_2024_paper.php": {
    "title": "Online Vectorized HD Map Construction using Geometry",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhixin Zhang",
      "Yiyuan Zhang",
      "Xiaohan Ding",
      "Fusheng Jin*",
      "Xiangyu Yue"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6517_ECCV_2024_paper.php": {
    "title": "Image-adaptive 3D Lookup Tables for Real-time Image Enhancement with Bilateral Grids",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wontae Kim*",
      "Nam Ik Cho*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6526_ECCV_2024_paper.php": {
    "title": "Learned HDR Image Compression for Perceptually Optimal Storage and Display",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peibei Cao",
      "HAOYU CHEN",
      "Jingzhe Ma",
      "Yu-Chieh Yuan",
      "Zhiyong Xie",
      "Xin Xie",
      "Haiqing Bai",
      "Kede Ma*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6528_ECCV_2024_paper.php": {
    "title": "Sparse Beats Dense: Rethinking Supervision in Radar-Camera Depth Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huadong Li",
      "Minhao Jing",
      "Jin Wang",
      "Shichao Dong",
      "Jiajun Liang",
      "Haoqiang Fan",
      "Renhe Ji*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6534_ECCV_2024_paper.php": {
    "title": "Non-Exemplar Domain Incremental Learning via Cross-Domain Concept Integration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiang Wang*",
      "Yuhang He",
      "Songlin Dong",
      "Xinyuan Gao",
      "Shaokun Wang",
      "Yihong Gong"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6543_ECCV_2024_paper.php": {
    "title": "Free-VSC: Free Semantics from Visual Foundation Models for Unsupervised Video Semantic Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Tian*",
      "Guo Lu*",
      "Guangtao Zhai*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6545_ECCV_2024_paper.php": {
    "title": "Improving Virtual Try-On with Garment-focused Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siqi Wan",
      "Yehao Li",
      "Jingwen Chen",
      "Yingwei Pan*",
      "Ting Yao",
      "Yang Cao",
      "Tao Mei"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6549_ECCV_2024_paper.php": {
    "title": "Ray Denoising: Depth-aware Hard Negative Sampling for Multi-view 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feng Liu*",
      "Tengteng Huang",
      "Qianjing Zhang",
      "Haotian Yao",
      "Chi Zhang",
      "Fang Wan",
      "Qixiang Ye",
      "Yanzhao Zhou*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6554_ECCV_2024_paper.php": {
    "title": "Disentangled Generation and Aggregation for Robust Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shihe Shen",
      "Huachen Gao",
      "Wangze Xu",
      "Rui Peng",
      "Luyang Tang",
      "Kaiqiang Xiong",
      "Jianbo Jiao",
      "Ronggang Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6561_ECCV_2024_paper.php": {
    "title": "UNIKD: UNcertainty-Filtered Incremental Knowledge Distillation for Neural Implicit Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengqi Guo*",
      "Chen Li",
      "Hanlin Chen",
      "Gim Hee Lee"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6562_ECCV_2024_paper.php": {
    "title": "Subspace Prototype Guidance for Mitigating Class Imbalance in Point Cloud Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawei Han",
      "Kaiqi Liu*",
      "Wei Li",
      "Guangzhi Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6579_ECCV_2024_paper.php": {
    "title": "MoAI: Mixture of All Intelligence for Large Language and Vision Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Byung-Kwan Lee",
      "Beomchan Park",
      "Chae Won Kim",
      "Yong Man Ro*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6583_ECCV_2024_paper.php": {
    "title": "Semantic-guided Robustness Tuning for Few-Shot Transfer Across Extreme Domain Shift",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "kangyu xiao*",
      "Zilei Wang",
      "junjie li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6586_ECCV_2024_paper.php": {
    "title": "Revisit Event Generation Model: Self-Supervised Learning of Event-to-Video Reconstruction with Implicit Neural Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zipeng Wang*",
      "yunfan lu",
      "Lin Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6593_ECCV_2024_paper.php": {
    "title": "SDPT: Synchronous Dual Prompt Tuning for Fusion-based Visual-Language Pre-trained Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Zhou*",
      "Yongjian Wu",
      "Jiya Saiyin",
      "Bingzheng Wei",
      "Maode Lai",
      "Eric I Chang",
      "Yan Xu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6595_ECCV_2024_paper.php": {
    "title": "Open-World Dynamic Prompt and Continual Visual Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youngeun Kim",
      "Jun Fang*",
      "Qin Zhang",
      "Zhaowei Cai",
      "Yantao Shen",
      "Rahul Duggal",
      "Dripta S. Raychaudhuri",
      "Zhuowen Tu",
      "Yifan Xing",
      "Onkar Dabeer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6601_ECCV_2024_paper.php": {
    "title": "Learning Video Context as Interleaved Multimodal Sequences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kevin Qinghong Lin",
      "Pengchuan Zhang",
      "Difei Gao",
      "Xide Xia",
      "Joya Chen",
      "Ziteng Gao",
      "Jinheng Xie",
      "Xuhong Xiao",
      "Mike Zheng Shou*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6611_ECCV_2024_paper.php": {
    "title": "Learning Unsigned Distance Functions from Multi-view Images with Volume Rendering Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenyuan Zhang",
      "Kanle Shi",
      "Yu-Shen Liu*",
      "Zhizhong Han"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6612_ECCV_2024_paper.php": {
    "title": "Dense Multimodal Alignment for Open-Vocabulary 3D Scene Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruihuang Li*",
      "Zhengqiang ZHANG",
      "Chenhang He",
      "Zhiyuan Ma",
      "Vishal Patel",
      "Lei Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6613_ECCV_2024_paper.php": {
    "title": "Deep Feature Surgery: Towards Accurate and Efficient Multi-Exit Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Gong",
      "Yao Chen*",
      "Qiuyang Luo",
      "Ye Lu",
      "Tao Li",
      "Yuzhi Zhang",
      "Yufei Sun*",
      "Le Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6619_ECCV_2024_paper.php": {
    "title": "Multi-scale Cross Distillation for Object Detection in Aerial Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kun Wang",
      "Zi Wang",
      "Zhang Li*",
      "Xichao Teng",
      "Yang Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6621_ECCV_2024_paper.php": {
    "title": "Progressive Proxy Anchor Propagation for Unsupervised Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyun Seok Seong",
      "WonJun Moon",
      "SuBeen Lee",
      "Jae-Pil Heo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6624_ECCV_2024_paper.php": {
    "title": "Within the Dynamic Context: Inertia-aware 3D Human Modeling with Pose Sequence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yutong Chen",
      "Yifan Zhan",
      "Zhihang Zhong*",
      "Wei Wang",
      "Xiao Sun*",
      "Yu Qiao",
      "Yinqiang Zheng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6630_ECCV_2024_paper.php": {
    "title": "Revisit Human-Scene Interaction via Space Occupancy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinpeng Liu",
      "Haowen Hou",
      "Yanchao Yang",
      "Yong-Lu Li*",
      "Cewu Lu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6634_ECCV_2024_paper.php": {
    "title": "Face Adapter for Pre-Trained Diffusion Models with Fine-Grained ID and Attribute Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Han*",
      "Junwei Zhu",
      "Keke He",
      "Xu Chen",
      "Yanhao Ge",
      "Wei Li",
      "Xiangtai Li",
      "Jiangning Zhang",
      "Chengjie Wang",
      "Yong Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6635_ECCV_2024_paper.php": {
    "title": "WeConvene: Learned Image Compression with Wavelet-Domain Convolution and Entropy Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haisheng Fu*",
      "Jie Liang",
      "Zhenman Fang",
      "Jingning Han",
      "Feng Liang",
      "Guohe Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6640_ECCV_2024_paper.php": {
    "title": "Grid-Attention: Enhancing Computational Efficiency of Large Vision Models without Fine-Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengyu Li*",
      "biao wang",
      "Tianchu Guo",
      "Xian-Sheng Hua"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6641_ECCV_2024_paper.php": {
    "title": "Mitigating Background Shift in Class-Incremental Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gilhan Park",
      "WonJun Moon",
      "SuBeen Lee",
      "Tae-Young Kim",
      "Jae-Pil Heo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6646_ECCV_2024_paper.php": {
    "title": "Relation DETR: Exploring Explicit Position Relation Prior for Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiuquan Hou",
      "Meiqin Liu*",
      "Senlin Zhang",
      "Ping Wei",
      "Badong Chen",
      "Xuguang Lan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6649_ECCV_2024_paper.php": {
    "title": "BKDSNN: Enhancing the Performance of Learning-based Spiking Neural Networks Training with Blurred Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zekai Xu",
      "Kang You",
      "Qinghai Guo",
      "Xiang Wang",
      "Zhezhi He*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6652_ECCV_2024_paper.php": {
    "title": "Agent Attention: On the Integration of Softmax and Linear Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongchen Han",
      "Tianzhu Ye",
      "Yizeng Han",
      "Zhuofan Xia",
      "Siyuan Pan",
      "Pengfei Wan",
      "Shiji Song",
      "Gao Huang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6656_ECCV_2024_paper.php": {
    "title": "Learning by Aligning 2D Skeleton Sequences and Multi-Modality Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quoc-Huy Tran*",
      "Muhammad Ahmed",
      "Murad Popattia",
      "Muhammad Hassan Ahmed",
      "Andrey Konin",
      "Zeeshan Zia"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6658_ECCV_2024_paper.php": {
    "title": "Resolving Scale Ambiguity in Multi-view 3D Reconstruction using Dual-Pixel Sensors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kohei Ashida*",
      "Hiroaki Santo",
      "Fumio Okura",
      "Yasuyuki Matsushita"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6669_ECCV_2024_paper.php": {
    "title": "Object-Oriented Anchoring and Modal Alignment in Multimodal Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shibin Mei",
      "Bingbing Ni*",
      "Hang Wang",
      "Chenglong Zhao",
      "fengfa hu",
      "Zhiming Pi",
      "BiLian Ke"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6670_ECCV_2024_paper.php": {
    "title": "Towards Stable 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiabao Wang",
      "Qiang Meng",
      "Guochao Liu",
      "Liujiang Yan",
      "Ke Wang",
      "Ming-Ming Cheng",
      "Qibin Hou*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6675_ECCV_2024_paper.php": {
    "title": "FYI: Flip Your Images for Dataset Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Byunggwan Son*",
      "Youngmin Oh",
      "Donghyeon Baek",
      "Bumsub Ham*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6676_ECCV_2024_paper.php": {
    "title": "On-the-fly Category Discovery for LiDAR Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyeonseong Kim",
      "Sung-Hoon Yoon",
      "Minseok Kim",
      "Kuk-Jin Yoon*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6678_ECCV_2024_paper.php": {
    "title": "Dual-Camera Smooth Zoom on Mobile Phones",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Renlong Wu",
      "Zhilu Zhang*",
      "Yu Yang",
      "Wangmeng Zuo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6685_ECCV_2024_paper.php": {
    "title": "ProtoComp: Diverse Point Cloud Completion with Controllable Prototype",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xumin Yu",
      "Yanbo Wang",
      "Jie Zhou",
      "Jiwen Lu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6695_ECCV_2024_paper.php": {
    "title": "CONDA: Condensed Deep Association Learning for Co-Salient Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Long Li",
      "Nian Liu*",
      "Dingwen Zhang",
      "Zhongyu Li",
      "Salman Khan",
      "Rao Anwer",
      "Hisham Cholakkal",
      "Junwei Han*",
      "Fahad Shahbaz Khan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6701_ECCV_2024_paper.php": {
    "title": "Cascade Prompt Learning for Visual-Language Model Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ge Wu",
      "Xin Zhang",
      "Zheng Li",
      "Zhaowei Chen",
      "Jiajun Liang",
      "Jian Yang",
      "Xiang Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6708_ECCV_2024_paper.php": {
    "title": "PolyRoom: Room-aware Transformer for Floorplan Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuzhou Liu",
      "Lingjie Zhu",
      "Xiaodong Ma",
      "Hanqiao Ye",
      "Xiang Gao",
      "Xianwei Zheng",
      "Shuhan Shen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6710_ECCV_2024_paper.php": {
    "title": "BenchLMM: Benchmarking Cross-style Visual Capability of Large Multimodal Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rizhao Cai*",
      "Zirui Song",
      "Dayan Guan*",
      "Zhenhao Chen",
      "Yaohang Li",
      "Xing Luo",
      "Chenyu Yi",
      "Alex Kot"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6713_ECCV_2024_paper.php": {
    "title": "SMFANet: A Lightweight Self-Modulation Feature Aggregation Network for Efficient Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "mingjun zheng",
      "Long Sun",
      "Jiangxin Dong",
      "Jinshan Pan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6715_ECCV_2024_paper.php": {
    "title": "HENet: Hybrid Encoding for End-to-end Multi-task 3D Perception from Multi-view Cameras",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongyu Xia",
      "ZhiWei Lin",
      "Xinhao Wang",
      "Yongtao Wang*",
      "Yun Xing",
      "Shengxiang Qi",
      "Nan Dong",
      "Ming-Hsuan Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6726_ECCV_2024_paper.php": {
    "title": "Hierarchical Unsupervised Relation Distillation for Source Free Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowei Xing*",
      "Xianghua Ying",
      "Ruibin Wang",
      "Ruohao Guo",
      "Ji Shi",
      "Wenzhen Yue"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6727_ECCV_2024_paper.php": {
    "title": "Customized Generation Reimagined: Fidelity and Editability Harmonized",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Jin",
      "Yang Shen",
      "Zhenyong Fu*",
      "Jian Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6729_ECCV_2024_paper.php": {
    "title": "AUFormer: Vision Transformers are Parameter-Efficient Facial Action Unit Detectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaishen Yuan",
      "Zitong Yu*",
      "Xin Liu*",
      "Weicheng Xie",
      "Huanjing Yue",
      "Jingyu Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6733_ECCV_2024_paper.php": {
    "title": "Improving Video Segmentation via Dynamic Anchor Queries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yikang Zhou",
      "Tao Zhang*",
      "Xiangtai Li*",
      "Shunping Ji*",
      "Shuicheng Yan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6734_ECCV_2024_paper.php": {
    "title": "Controllable Contextualized Image Captioning: Directing the Visual Narrative through User-Defined Highlights",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shunqi Mao*",
      "Chaoyi Zhang",
      "Hang Su",
      "Hwanjun Song",
      "Igor Shalyminov",
      "Weidong Cai"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6735_ECCV_2024_paper.php": {
    "title": "Diffusion Models as Optimizers for Efficient Planning in Offline RL",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Renming Huang",
      "Yunqiang Pei",
      "Guoqing Wang*",
      "Yangming Zhang",
      "Yang Yang",
      "Peng Wang",
      "Heng Tao Shen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6736_ECCV_2024_paper.php": {
    "title": "Enhanced Sparsification via Stimulative Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengji Tang",
      "Weihao Lin",
      "Hancheng Ye",
      "Peng Ye",
      "Chong Yu",
      "Baopu Li",
      "Tao Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6741_ECCV_2024_paper.php": {
    "title": "How Many Unicorns Are in This Image? A Safety Evaluation Benchmark for Vision LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoqin Tu*",
      "Chenhang Cui",
      "Zijun Wang",
      "Yiyang Zhou",
      "Bingchen Zhao",
      "Junlin Han",
      "Wangchunshu Zhou",
      "Huaxiu Yao",
      "Cihang Xie*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6746_ECCV_2024_paper.php": {
    "title": "NeuroPictor: Refining fMRI-to-Image Reconstruction via Multi-individual Pretraining and Multi-level Modulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyang Huo",
      "Yikai Wang",
      "Yanwei Fu*",
      "Xuelin Qian",
      "Chong Li",
      "Yun Wang",
      "Jianfeng Feng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6748_ECCV_2024_paper.php": {
    "title": "Coarse-to-Fine Implicit Representation Learning for 3D Hand-Object Reconstruction from a Single RGB-D Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyu Liu",
      "Pengfei Ren",
      "Jingyu Wang*",
      "Qi Qi",
      "Haifeng Sun",
      "Zirui Zhuang*",
      "Jianxin Liao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6750_ECCV_2024_paper.php": {
    "title": "Efficient Snapshot Spectral Imaging: Calibration-Free Parallel Structure with Aperture Diffraction Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Lv*",
      "Lihao Hu",
      "Shiqiao Li",
      "Chenglong Huang",
      "Xun Cao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6751_ECCV_2024_paper.php": {
    "title": "Enhancing Recipe Retrieval with Foundation Models: A Data Augmentation Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangzhou Song",
      "Bin Zhu",
      "Yanbin Hao*",
      "Shuo Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6758_ECCV_2024_paper.php": {
    "title": "PapMOT: Exploring Adversarial Patch Attack against Multiple Object Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahuan Long*",
      "Tingsong Jiang*",
      "Wen Yao*",
      "Shuai Jia*",
      "Weijia Zhang*",
      "Weien Zhou*",
      "Chao Ma*",
      "Xiaoqian Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6764_ECCV_2024_paper.php": {
    "title": "HiDiffusion: Unlocking Higher-Resolution Creativity and Efficiency in Pretrained Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shen Zhang",
      "Zhaowei CHEN",
      "Zhenyu Zhao",
      "Yuhao Chen",
      "Yao Tang",
      "Jiajun Liang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6766_ECCV_2024_paper.php": {
    "title": "On the Approximation Risk of Few-Shot Class-Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan Wang",
      "Zhong Ji*",
      "Xiyao Liu",
      "Yanwei Pang",
      "Jungong Han"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6768_ECCV_2024_paper.php": {
    "title": "Syn-to-Real Domain Adaptation for Point Cloud Completion via Part-based Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunseo Yang",
      "Jihun Kim",
      "Kuk-Jin Yoon*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6773_ECCV_2024_paper.php": {
    "title": "Learn to Preserve and Diversify: Parameter-Efficient Group with Orthogonal Regularization for Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiajun Hu",
      "Jian Zhang",
      "Lei Qi*",
      "Yinghuan Shi*",
      "Yang Gao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6776_ECCV_2024_paper.php": {
    "title": "SCOMatch: Alleviating Overtrusting in Open-set Semi-supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zerun Wang*",
      "Liuyu Xiang",
      "Lang Huang",
      "Jiafeng Mao",
      "Ling Xiao",
      "Toshihiko Yamasaki"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6781_ECCV_2024_paper.php": {
    "title": "Region-aware Distribution Contrast: A Novel Approach to Multi-Task Partially Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meixuan Li",
      "Tianyu Li",
      "Guoqing Wang*",
      "Peng Wang",
      "Yang Yang",
      "Jie Zou"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6786_ECCV_2024_paper.php": {
    "title": "MasterWeaver: Taming Editability and Face Identity for Personalized Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxiang Wei",
      "Zhilong Ji",
      "Jinfeng Bai",
      "Hongzhi Zhang",
      "Lei Zhang*",
      "Wangmeng Zuo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6787_ECCV_2024_paper.php": {
    "title": "PointRegGPT: Boosting 3D Point Cloud Registration using Generative Point-Cloud Pairs for Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suyi Chen",
      "Hao Xu",
      "Haipeng Li",
      "Kunming Luo",
      "Guanghui Liu",
      "Chi-Wing Fu",
      "Ping Tan",
      "Shuaicheng Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6788_ECCV_2024_paper.php": {
    "title": "General Geometry-aware Weakly Supervised 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guowen Zhang*",
      "Junsong Fan",
      "Liyi Chen",
      "Zhaoxiang Zhang",
      "Zhen Lei",
      "Lei Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6793_ECCV_2024_paper.php": {
    "title": "Long-CLIP: Unlocking the Long-Text Capability of CLIP",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Beichen Zhang*",
      "Pan Zhang",
      "Xiaoyi Dong*",
      "Yuhang Zang",
      "Jiaqi Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6812_ECCV_2024_paper.php": {
    "title": "Dolfin: Diffusion Layout Transformers without Autoencoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yilin Wang",
      "Zeyuan Chen",
      "Liangjun Zhong",
      "Zheng Ding",
      "Zhuowen Tu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6814_ECCV_2024_paper.php": {
    "title": "Real-time 3D-aware Portrait Editing from a Single Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingyan Bai*",
      "Zifan Shi",
      "Yinghao Xu",
      "Hao Ouyang",
      "Qiuyu Wang",
      "Ceyuan Yang",
      "Xuan Wang",
      "Gordon Wetzstein",
      "Yujun Shen*",
      "Qifeng Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6815_ECCV_2024_paper.php": {
    "title": "StructLDM: Structured Latent Diffusion for 3D Human Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Hu",
      "Fangzhou Hong",
      "Ziwei Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6823_ECCV_2024_paper.php": {
    "title": "Image Compression for Machine and Human Vision With Spatial-Frequency Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Li*",
      "Shaohui Li*",
      "Shuangrui Ding",
      "Wenrui Dai*",
      "Maida Cao",
      "Chenglin Li",
      "Junni Zou",
      "Hongkai Xiong"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6824_ECCV_2024_paper.php": {
    "title": "Beyond the Contact: Discovering Comprehensive Affordance for 3D Objects from Pre-trained 2D Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyeonwoo Kim",
      "Sookwan Han",
      "Patrick Kwon",
      "Hanbyul Joo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6825_ECCV_2024_paper.php": {
    "title": "Norma: A Noise Robust Memory-Augmented Framework for Whole Slide Image Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Bai",
      "Bo Zhang*",
      "Zheng Zhang",
      "Shuo Yan",
      "Zibo Ma",
      "Wu Liu",
      "Xiuzhuang Zhou",
      "Xiangyang Gong",
      "Wendong Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6826_ECCV_2024_paper.php": {
    "title": "Continuous Memory Representation for Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joo Chan Lee*",
      "Taejune Kim",
      "Eunbyung Park*",
      "Simon S Woo*",
      "Jong Hwan Ko*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6828_ECCV_2024_paper.php": {
    "title": "InstaStyle: Inversion Noise of a Stylized Image is Secretly a Style Adviser",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xing Cui",
      "Zekun Li",
      "Peipei Li*",
      "Huaibo Huang",
      "Xuannan Liu",
      "Zhaofeng He"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6837_ECCV_2024_paper.php": {
    "title": "PACE: Pose Annotations in Cluttered Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang You*",
      "kai xiong",
      "Zhening Yang",
      "Zhengxiang Huang",
      "Junwei Zhou",
      "Ruoxi Shi",
      "Zhou FANG",
      "Adam Harley",
      "Leonidas Guibas",
      "Cewu Lu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6838_ECCV_2024_paper.php": {
    "title": "CMTA: Cross-Modal Temporal Alignment for Event-guided Video Deblurring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taewoo Kim",
      "Hoonhee Cho",
      "Kuk-Jin Yoon*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6839_ECCV_2024_paper.php": {
    "title": "CountFormer: Multi-View Crowd Counting Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hong Mo*",
      "Xiong Zhang*",
      "Jianchao Tan",
      "Cheng Yang",
      "Qiong Gu",
      "Bo Hang",
      "Wenqi Ren"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6840_ECCV_2024_paper.php": {
    "title": "Textual Knowledge Matters: Cross-Modality Co-Teaching for Generalized Visual Class Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiyang Zheng",
      "Nan Pu",
      "Wenjing Li*",
      "Nicu Sebe",
      "Zhun Zhong*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6843_ECCV_2024_paper.php": {
    "title": "Continuous SO(3) Equivariant Convolution for 3D Point Cloud Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaein Kim",
      "HEE BIN YOO",
      "Dong-Sig Han",
      "Yeon-Ji Song",
      "Byoung-Tak Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6845_ECCV_2024_paper.php": {
    "title": "EA-VTR: Event-Aware Video-Text Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zongyang Ma*",
      "Ziqi Zhang",
      "Yuxin Chen",
      "Zhongang Qi",
      "Chunfeng Yuan",
      "Bing Li",
      "Yingmin Luo",
      "Xu LI",
      "Xiaojuan Qi",
      "Ying Shan",
      "Weiming Hu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6850_ECCV_2024_paper.php": {
    "title": "Privacy-Preserving Adaptive Re-Identification without Image Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hamza Rami*",
      "Jhony H. Giraldo",
      "Nicolas Winckler",
      "Stéphane Lathuilière"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6851_ECCV_2024_paper.php": {
    "title": "A Simple Low-bit Quantization Framework for Video Snapshot Compressive Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miao Cao*",
      "Lishun Wang",
      "Huan Wang",
      "Xin Yuan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6852_ECCV_2024_paper.php": {
    "title": "DIFFender: Diffusion-Based Adversarial Defense against Patch Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Caixin Kang*",
      "Yinpeng Dong",
      "Zhengyi Wang",
      "Shouwei Ruan",
      "Yubo Chen",
      "Hang Su*",
      "Xingxing Wei*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6854_ECCV_2024_paper.php": {
    "title": "Hybrid Video Diffusion Models with 2D Triplane and 3D Wavelet Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kihong Kim",
      "Haneol Lee",
      "Jihye Park",
      "Seyeon Kim",
      "Kwang Hee Lee",
      "Seungryong Kim*",
      "Jaejun Yoo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6856_ECCV_2024_paper.php": {
    "title": "Background Adaptation with Residual Modeling for Exemplar-Free Class-Incremental Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anqi Zhang",
      "Guangyu Gao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6858_ECCV_2024_paper.php": {
    "title": "Efficient Diffusion-Driven Corruption Editor for Test-Time Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yeongtak Oh",
      "Jonghyun Lee",
      "Jooyoung Choi",
      "Dahuin Jung",
      "Uiwon Hwang*",
      "Sungroh Yoon*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6863_ECCV_2024_paper.php": {
    "title": "Learning to Unlearn for Robust Machine Unlearning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mark He Huang*",
      "Lin Geng Foo",
      "Jun Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6864_ECCV_2024_paper.php": {
    "title": "Emergent Visual-Semantic Hierarchies in Image-Text Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Morris Alper*",
      "Hadar Averbuch-Elor"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6867_ECCV_2024_paper.php": {
    "title": "Context-Guided Spatial Feature Reconstruction for Efficient Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenliang Ni",
      "Xinghao Chen*",
      "Yingjie Zhai",
      "Yehui Tang",
      "Yunhe Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6870_ECCV_2024_paper.php": {
    "title": "DriveLM: Driving with Graph Visual Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chonghao Sima*",
      "Katrin Renz",
      "Kashyap Chitta",
      "Li Chen",
      "Zhang Hanxue",
      "Chengen Xie",
      "Jens Beißwenger",
      "Ping Luo",
      "Andreas Geiger",
      "Hongyang Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6871_ECCV_2024_paper.php": {
    "title": "Neural Spectral Decomposition for Dataset Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaolei Yang",
      "Shen Cheng",
      "Mingbo Hong",
      "Haoqiang Fan",
      "Xing Wei",
      "Shuaicheng Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6872_ECCV_2024_paper.php": {
    "title": "Beyond Viewpoint: Robust 3D Object Recognition under Arbitrary Views through Joint Multi-Part Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linlong Fan",
      "Ye Huang*",
      "Yanqi Ge",
      "Wen Li",
      "Lixin Duan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6873_ECCV_2024_paper.php": {
    "title": "Learning Non-Linear Invariants for Unsupervised Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lars Doorenbos*",
      "Raphael Sznitman",
      "Pablo Márquez Neila"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6893_ECCV_2024_paper.php": {
    "title": "Dynamic Retraining-Updating Mean Teacher for Source-Free Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Trinh Le Ba Khanh*",
      "Huy-Hung Nguyen",
      "Long Hoang Pham",
      "Duong Nguyen-Ngoc Tran",
      "Jae Wook Jeon*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6901_ECCV_2024_paper.php": {
    "title": "Knowledge-enhanced Visual-Language Pretraining for Computational Pathology",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Zhou",
      "Xiaoman Zhang",
      "Chaoyi Wu",
      "Ya Zhang",
      "Weidi Xie",
      "Yan-Feng Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6903_ECCV_2024_paper.php": {
    "title": "Adaptive Multi-modal Fusion of Spatially Variant Kernel Refinement with Diffusion Model for Blind Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junxiong Lin*",
      "Yan Wang",
      "Zeng Tao",
      "Boyang Wang",
      "Qing Zhao",
      "Haoran Wang",
      "Xuan Tong",
      "Xinji Mai",
      "Yuxuan Lin",
      "Wei Song",
      "Jiawen Yu",
      "Shaoqi Yan",
      "Wenqiang Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6908_ECCV_2024_paper.php": {
    "title": "Disentangled Clothed Avatar Generation from Text Descriptions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jionghao Wang*",
      "Yuan Liu",
      "Zhiyang Dou",
      "Zhengming Yu",
      "Yongqing Liang",
      "Cheng Lin",
      "Rong Xie",
      "Li Song*",
      "Xin Li",
      "Wenping Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6913_ECCV_2024_paper.php": {
    "title": "Real Appearance Modeling for More General Deepfake Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahe Tian",
      "Cai Yu",
      "Xi Wang",
      "Peng Chen",
      "Zihao Xiao",
      "Jiao Dai",
      "Yesheng Chai*",
      "Jizhong Han"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6914_ECCV_2024_paper.php": {
    "title": "6DGS: 6D Pose Estimation from a Single Image and a 3D Gaussian Splatting Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matteo Bortolon*",
      "Theodore Tsesmelis",
      "Stuart James",
      "Fabio Poiesi",
      "Alessio Del Bue"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6917_ECCV_2024_paper.php": {
    "title": "Dual-Decoupling Learning and Metric-Adaptive Thresholding for Semi-Supervised Multi-Label Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jia-Hao Xiao",
      "Ming-Kun Xie",
      "Heng-Bo Fan",
      "Gang Niu",
      "Masashi Sugiyama",
      "Sheng-Jun Huang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6926_ECCV_2024_paper.php": {
    "title": "V2X-Real: a Largs-Scale Dataset for Vehicle-to-Everything Cooperative Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Xiang",
      "Xin Xia",
      "Zhaoliang Zheng",
      "Runsheng Xu",
      "Letian Gao",
      "Zewei Zhou",
      "xu han",
      "Xinkai Ji",
      "Mingxi Li",
      "Zonglin Meng",
      "Li Jin",
      "Mingyue Lei",
      "Zhaoyang Ma",
      "Zihang He",
      "Haoxuan Ma",
      "Yunshuang Yuan",
      "Yingqian Zhao",
      "Jiaqi Ma*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6929_ECCV_2024_paper.php": {
    "title": "VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guénolé Fiche*",
      "Simon Leglaive",
      "Xavier Alameda-Pineda",
      "Antonio Agudo",
      "Francesc Moreno"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6931_ECCV_2024_paper.php": {
    "title": "Attention Beats Linear for Fast Implicit Neural Representation Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuyi Zhang",
      "Ke Liu",
      "Jingjun Gu",
      "Xiaoxu Cai",
      "Zhihua Wang",
      "Jiajun Bu",
      "Haishuai Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6938_ECCV_2024_paper.php": {
    "title": "HARIVO: Harnessing Text-to-Image Models for Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingi Kwon",
      "Seoung Wug Oh",
      "Yang Zhou",
      "Joon-Young Lee",
      "Difan Liu",
      "Haoran Cai",
      "Baqiao Liu",
      "Feng Liu",
      "Youngjung Uh*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6944_ECCV_2024_paper.php": {
    "title": "Deep Online Probability Aggregation Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Yan",
      "Na Lu*",
      "Ruofan Yan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6947_ECCV_2024_paper.php": {
    "title": "WRIM-Net: Wide-Ranging Information Mining Network for Visible-Infrared Person Re-Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yonggan Wu",
      "Ling-Chao Meng*",
      "Yuan Zichao",
      "Sixian Chan",
      "Hong-Qiang Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6950_ECCV_2024_paper.php": {
    "title": "Reliable and Efficient Concept Erasure of Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Gong*",
      "Kai Chen",
      "Zhipeng Wei",
      "Jingjing Chen*",
      "Yu-Gang Jiang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6951_ECCV_2024_paper.php": {
    "title": "Visual Text Generation in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanzhi Zhu",
      "Jiawei Liu",
      "Feiyu Gao",
      "Wenyu Liu*",
      "Xinggang Wang",
      "Peng Wang",
      "Fei Huang",
      "Cong Yao",
      "Zhibo Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6952_ECCV_2024_paper.php": {
    "title": "Length-Aware Motion Synthesis via Latent Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alessio Sampieri*",
      "Alessio Palma",
      "Indro Spinelli",
      "Fabio Galasso"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6954_ECCV_2024_paper.php": {
    "title": "Attention-Challenging Multiple Instance Learning for Whole Slide Image Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunlong Zhang*",
      "Honglin Li",
      "YUXUAN SUN",
      "Chenglu Zhu",
      "Sunyi Zheng",
      "Lin Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6958_ECCV_2024_paper.php": {
    "title": "An Optimal Control View of LoRA and Binary Controller Design for Vision Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chi Zhang*",
      "Jingpu Cheng",
      "Qianxiao Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6963_ECCV_2024_paper.php": {
    "title": "Exploring Phrase-Level Grounding with Text-to-Image Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Danni Yang",
      "Ruohan Dong",
      "Jiayi Ji",
      "Yiwei Ma",
      "Haowei Wang",
      "Xiaoshuai Sun*",
      "Rongrong Ji"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6966_ECCV_2024_paper.php": {
    "title": "FocusDiffuser: Perceiving Local Disparities for Camouflaged Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianwei Zhao*",
      "Xin Li",
      "Fan Yang",
      "Qiang Zhai*",
      "Ao Luo",
      "Zhicheng Jiao",
      "Hong Cheng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6969_ECCV_2024_paper.php": {
    "title": "Improving image synthesis with diffusion-negative sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alakh Desai*",
      "Nuno Vasconcelos"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6979_ECCV_2024_paper.php": {
    "title": "AvatarPose: Avatar-guided 3D Pose Estimation of Close Human Interaction from Sparse Multi-view Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feichi Lu*",
      "Zijian Dong*",
      "Jie Song",
      "Otmar Hilliges"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6981_ECCV_2024_paper.php": {
    "title": "FedVAD: Enhancing Federated Video Anomaly Detection with GPT-Driven Semantic Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan Qi*",
      "Ruijie Pan",
      "Huaiwen Zhang",
      "Changsheng Xu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6988_ECCV_2024_paper.php": {
    "title": "SignGen: End-to-End Sign Language Video Generation with Latent Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan Qi*",
      "Yu Duan",
      "Changsheng Xu",
      "Huaiwen Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6989_ECCV_2024_paper.php": {
    "title": "Idling Neurons, Appropriately Lenient Workload During Fine-tuning Leads to Better Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongjing Niu*",
      "Hanting Li",
      "Bin Li",
      "Feng Zhao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6993_ECCV_2024_paper.php": {
    "title": "Diffusion Prior-Based Amortized Variational Inference for Noisy Inverse Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sojin Lee",
      "Dogyun Park",
      "Inho Kong",
      "Hyunwoo J. Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7003_ECCV_2024_paper.php": {
    "title": "The Gaussian Discriminant Variational Autoencoder (GdVAE): A Self-Explainable Model with Counterfactual Explanations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anselm Haselhoff*",
      "Kevin Trelenberg",
      "Fabian Küppers",
      "Jonas Schneider"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7004_ECCV_2024_paper.php": {
    "title": "Accelerating Image Generation with Sub-path Linear Approximation Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Xu",
      "Tianhui Song",
      "Weixin Feng",
      "Xubin Li",
      "Tiezheng Ge",
      "Bo Zheng",
      "Limin Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7009_ECCV_2024_paper.php": {
    "title": "Safe-CLIP: Removing NSFW Concepts from Vision-and-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuele Poppi*",
      "Tobia Poppi*",
      "Federico Cocchi",
      "Marcella Cornia",
      "Lorenzo Baraldi",
      "Rita Cucchiara"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7010_ECCV_2024_paper.php": {
    "title": "TetraDiffusion: Tetrahedral Diffusion Models for 3D Shape Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikolai Kalischek*",
      "Torben Peters",
      "Jan Dirk Wegner",
      "Konrad Schindler"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7011_ECCV_2024_paper.php": {
    "title": "Camera Calibration using a Collimator System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shunkun Liang",
      "Banglei Guan*",
      "Zhenbao Yu",
      "Pengju Sun",
      "Yang Shang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7015_ECCV_2024_paper.php": {
    "title": "Label-free Neural Semantic Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayi Wang*",
      "Kevin A Laube",
      "Yumeng Li",
      "Jan Hendrik Metzen",
      "Shin-I Cheng",
      "Julio Borges",
      "Anna Khoreva"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7022_ECCV_2024_paper.php": {
    "title": "Exploring Reliable Matching with Phase Enhancement for Night-time Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuwen Pan*",
      "Rui Sun",
      "Naisong Luo",
      "Tianzhu Zhang",
      "Yongdong Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7025_ECCV_2024_paper.php": {
    "title": "Multiscale Sliced Wasserstein Distances as Perceptual Color Difference Measures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqi He",
      "Zhihua Wang",
      "Leon Wang",
      "Tsein-I Liu",
      "Yuming Fang",
      "Qilin Sun*",
      "Kede Ma"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7026_ECCV_2024_paper.php": {
    "title": "DiscoMatch: Fast Discrete Optimisation for Geometrically Consistent 3D Shape Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paul Roetzer*",
      "Ahmed Abbas*",
      "Dongliang Cao",
      "Florian Bernard",
      "Paul Swoboda"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7035_ECCV_2024_paper.php": {
    "title": "Switch Diffusion Transformer: Synergizing Denoising Tasks with Sparse Mixture-of-Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Byeongjun Park",
      "Hyojun Go",
      "Jin-Young Kim",
      "Sangmin Woo",
      "Seokil Ham",
      "Changick Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7037_ECCV_2024_paper.php": {
    "title": "FARSE-CNN: Fully Asynchronous, Recurrent and Sparse Event-Based CNN",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Riccardo Santambrogio*",
      "Marco Cannici",
      "Matteo Matteucci"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7038_ECCV_2024_paper.php": {
    "title": "ConDense: Consistent 2D-3D Pre-training for Dense and Sparse Features from Multi-View Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoshuai Zhang*",
      "Zhicheng Wang",
      "Howard Zhou",
      "Soham Ghosh",
      "Danushen L Gnanapragasam",
      "Varun Jampani",
      "Hao Su",
      "Leonidas Guibas"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7040_ECCV_2024_paper.php": {
    "title": "MTA-CLIP: Language-Guided Semantic Segmentation with Mask-Text Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anurag Das*",
      "Xinting Hu",
      "Li Jiang",
      "Bernt Schiele"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7043_ECCV_2024_paper.php": {
    "title": "Event-Aided Time-To-Collision Estimation for Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinghang Li",
      "Bangyan Liao",
      "Xiuyuan Lu",
      "Peidong Liu",
      "Shaojie Shen",
      "Yi Zhou*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7047_ECCV_2024_paper.php": {
    "title": "The Devil is in the Statistics: Mitigating and Exploiting Statistics Difference for Generalizable Semi-supervised Medical Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muyang Qiu",
      "Jian Zhang",
      "Lei Qi",
      "Qian Yu",
      "Yinghuan Shi*",
      "Yang Gao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7051_ECCV_2024_paper.php": {
    "title": "VEON: Vocabulary-Enhanced Occupancy Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jilai Zheng",
      "Pin Tang",
      "Zhongdao Wang",
      "Guoqing Wang",
      "Xiangxuan Ren",
      "Bailan Feng",
      "Chao Ma*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7052_ECCV_2024_paper.php": {
    "title": "Adapt without Forgetting: Distill Proximity from Dual Teachers in Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengyu Zheng*",
      "Yehui Tang",
      "Zhiwei Hao",
      "Kai Han",
      "Yunhe Wang",
      "Chang Xu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7067_ECCV_2024_paper.php": {
    "title": "The Sky's the Limit: Relightable Outdoor Scenes via a Sky-pixel Constrained Illumination Prior and Outside-In Visibility",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "James A D Gardner*",
      "Evgenii Kashin",
      "Bernhard Egger",
      "William Smith"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7068_ECCV_2024_paper.php": {
    "title": "DiffFAS: Face Anti-Spoofing via Generative Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinxu Ge",
      "Xin Liu*",
      "Zitong Yu*",
      "Jingang Shi",
      "Chun Qi",
      "Jie Li",
      "Heikki Kälviäinen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7071_ECCV_2024_paper.php": {
    "title": "Hetecooper: Feature Collaboration Graph for Heterogeneous Collaborative Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Congzhang Shao",
      "Guiyang Luo*",
      "Quan Yuan*",
      "Yifu Chen",
      "Yilin Liu",
      "Gong Kexin",
      "Jinglin Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7072_ECCV_2024_paper.php": {
    "title": "Learning-based Axial Video Motion Magnification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kwon Byung-Ki",
      "Oh Hyun-Bin",
      "Kim Jun-Seong",
      "Hyunwoo Ha",
      "Tae-Hyun Oh*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7083_ECCV_2024_paper.php": {
    "title": "Simplifying Source-Free Domain Adaptation for Object Detection: Effective Self-Training Strategies and Performance Insights",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Hao",
      "Florent Forest*",
      "Olga Fink"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7092_ECCV_2024_paper.php": {
    "title": "Class-Incremental Learning with CLIP: Adaptive Representation Adjustment and Parameter Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linlan Huang",
      "Xusheng Cao",
      "Haori Lu",
      "Xialei Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7100_ECCV_2024_paper.php": {
    "title": "cDP-MIL: Robust Multiple Instance Learning via Cascaded Dirichlet Process",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihang Chen",
      "Tsai Hor Chan",
      "Guosheng Yin",
      "Yuming Jiang",
      "Lequan Yu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7107_ECCV_2024_paper.php": {
    "title": "Causality-inspired Discriminative Feature Learning in Triple Domains for Gait Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haijun Xiong",
      "Bin Feng*",
      "Xinggang Wang",
      "Wenyu Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7118_ECCV_2024_paper.php": {
    "title": "Retargeting Visual Data with Deformation Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tim Elsner*",
      "Julia Berger",
      "Tong Wu",
      "Victor Czech",
      "Lin Gao",
      "Leif Kobbelt"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7122_ECCV_2024_paper.php": {
    "title": "Delving Deep into Engagement Prediction of Short Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "dasong Li",
      "Wenjie Li",
      "Baili Lu",
      "Hongsheng Li",
      "Sizhuo Ma",
      "Gurunandan Krishnan",
      "Jian Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7132_ECCV_2024_paper.php": {
    "title": "Flexible Distribution Alignment: Towards Long-tailed Semi-supervised Learning with Proper Calibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emanuel Sanchez Aimar*",
      "Nathaniel D Helgesen",
      "Yonghao Xu",
      "Marco Kuhlmann",
      "Michael Felsberg"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7133_ECCV_2024_paper.php": {
    "title": "CLEO: Continual Learning of Evolving Ontologies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shishir Muralidhara*",
      "Saqib Bukhari",
      "Georg Dr. Schneider",
      "Didier Stricker",
      "René Schuster"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7134_ECCV_2024_paper.php": {
    "title": "SpecFormer: Guarding Vision Transformer Robustness via Maximum Singular Value Penalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xixu Hu",
      "Runkai Zheng",
      "Jindong Wang*",
      "Cheuk Hang Leung",
      "Qi Wu*",
      "Xing Xie"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7137_ECCV_2024_paper.php": {
    "title": "Wavelet Convolutions for Large Receptive Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shahaf E Finder*",
      "Roy Amoyal",
      "Eran Treister",
      "Oren Freifeld*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7138_ECCV_2024_paper.php": {
    "title": "BK-SDM: A Lightweight, Fast, and Cheap Version of Stable Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo-Kyeong Kim*",
      "Hyoung-Kyu Song",
      "Thibault Castells",
      "Shinkook Choi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7145_ECCV_2024_paper.php": {
    "title": "Language-Assisted Skeleton Action Understanding for Skeleton-Based Temporal Action Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu Ji",
      "Bowen Chen",
      "Xinglong Xu",
      "Weihong Ren",
      "Zhiyong Wang*",
      "Honghai Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7149_ECCV_2024_paper.php": {
    "title": "Leveraging scale- and orientation-covariant features for planar motion estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marcus Valtonen Örnhag*",
      "Alberto Jaenal"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7150_ECCV_2024_paper.php": {
    "title": "Understanding and Mitigating Human-Labelling Errors in Supervised Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijun Long*",
      "Lipeng Zhuang",
      "George W Killick",
      "Richard Mccreadie",
      "Gerardo Aragon-Camarasa",
      "Paul Henderson"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7153_ECCV_2024_paper.php": {
    "title": "Adaptive Parametric Activation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Konstantinos P Alexandridis*",
      "Jiankang Deng",
      "Anh Nguyen",
      "Shan Luo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7158_ECCV_2024_paper.php": {
    "title": "Distractor-Free Novel View Synthesis via Exploiting Memorization Effect in Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yukun Wang*",
      "Kunhong Li",
      "Minglin Chen",
      "Longguang Wang",
      "Shunbo Zhou",
      "Kaiwen Xue",
      "Yulan Guo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7159_ECCV_2024_paper.php": {
    "title": "VEGS: View Extrapolation of Urban Scenes in 3D Gaussian Splatting using Learned Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sungwon Hwang",
      "Min-Jung Kim",
      "Taewoong Kang",
      "Jayeon Kang",
      "Jaegul Choo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7166_ECCV_2024_paper.php": {
    "title": "HGL: Hierarchical Geometry Learning for Test-time Adaptation in 3D Point Cloud Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianpei Zou",
      "Sanqing Qu",
      "Zhijun Li",
      "Alois C. Knoll",
      "何 良华",
      "Guang Chen*",
      "Changjun Jiang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7170_ECCV_2024_paper.php": {
    "title": "SWinGS: Sliding Windows for Dynamic 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Richard Shaw*",
      "Michal Nazarczuk",
      "Jifei Song",
      "Arthur Moreau",
      "Sibi Catley-Chandar",
      "Helisa Dhamo",
      "Eduardo Pérez Pellitero"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7171_ECCV_2024_paper.php": {
    "title": "Temporal-Mapping Photography for Event Cameras",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhan Bao",
      "Lei Sun*",
      "Yuqin Ma",
      "Kaiwei Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7173_ECCV_2024_paper.php": {
    "title": "Shape2Scene: 3D Scene Representation Learning Through Pre-training on Shape Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tuo Feng",
      "Wenguan Wang",
      "Ruijie Quan",
      "Yi Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7181_ECCV_2024_paper.php": {
    "title": "LineFit: A Geometric Approach for Fitting Line Segments in Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marion Boyer",
      "David Youssefi",
      "Florent Lafarge*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7183_ECCV_2024_paper.php": {
    "title": "Six-Point Method for Multi-Camera Systems with Reduced Solution Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Banglei Guan",
      "Ji Zhao*",
      "Laurent Kneip"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7189_ECCV_2024_paper.php": {
    "title": "Mew: Multiplexed Immunofluorescence Image Analysis through an Efficient Multiplex Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sukwon Yun",
      "Jie Peng",
      "Alexandro E Trevino",
      "Chanyoung Park",
      "Tianlong Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7192_ECCV_2024_paper.php": {
    "title": "Champ: Controllable and Consistent Human Image Animation with 3D Parametric Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shenhao Zhu",
      "Junming Leo Chen",
      "Zuozhuo Dai",
      "Zilong Dong",
      "Yinghui Xu",
      "Xun Cao",
      "Yao Yao",
      "Hao Zhu*",
      "Siyu Zhu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7206_ECCV_2024_paper.php": {
    "title": "AdaDistill: Adaptive Knowledge Distillation for Deep Face Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fadi Boutros*",
      "Vitomir Struc",
      "Naser Damer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7209_ECCV_2024_paper.php": {
    "title": "HERGen: Elevating Radiology Report Generation with Longitudinal Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fuying Wang",
      "Shenghui Du",
      "Lequan Yu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7212_ECCV_2024_paper.php": {
    "title": "Labeled Data Selection for Category Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bingchen Zhao*",
      "Nico Lang",
      "Serge Belongie",
      "Oisin Mac Aodha*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7216_ECCV_2024_paper.php": {
    "title": "Dependency-aware Differentiable Neural Architecture Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Buang Zhang*",
      "Xinle Wu",
      "Hao Miao",
      "Bin Yang",
      "Chenjuan Guo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7220_ECCV_2024_paper.php": {
    "title": "WAS: Dataset and Methods for Artistic Text Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xudong Xie",
      "Yuzhe Li",
      "Yang Liu",
      "Zhifei Zhang",
      "Zhaowen Wang",
      "Wei Xiong",
      "Xiang Bai*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7221_ECCV_2024_paper.php": {
    "title": "CLIFF: Continual Latent Diffusion for Open-Vocabulary Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wuyang Li",
      "Xinyu Liu",
      "Jiayi Ma",
      "Yixuan Yuan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7223_ECCV_2024_paper.php": {
    "title": "GMT: Enhancing Generalizable Neural Rendering via Geometry-Driven Multi-Reference Texture Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youngho Yoon",
      "Hyun-Kurl Jang",
      "Kuk-Jin Yoon*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7224_ECCV_2024_paper.php": {
    "title": "Norface: Improving Facial Expression Analysis by Identity Normalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanwei Liu*",
      "Rudong An",
      "Zhimeng Zhang",
      "Bowen Ma",
      "Wei Zhang",
      "Yan Song",
      "Yujing Hu",
      "Chen Wei",
      "Yu Ding*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7226_ECCV_2024_paper.php": {
    "title": "Unlocking Attributes' Contribution to Successful Camouflage: A Combined Textual and Visual Analysis Strategy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hong Zhang",
      "Yixuan Lyu",
      "Qian Yu",
      "Hanyang Liu",
      "Huimin Ma",
      "Yuan Ding",
      "Yifan Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7231_ECCV_2024_paper.php": {
    "title": "SNeRV: Spectra-preserving Neural Representation for Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jina Kim*",
      "Jihoo Lee*",
      "Jewon Kang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7232_ECCV_2024_paper.php": {
    "title": "COMO: Compact Mapping and Odometry",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eric Dexheimer*",
      "Andrew Davison"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7236_ECCV_2024_paper.php": {
    "title": "OAT: Object-Level Attention Transformer for Gaze Scanpath Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yini Fang*",
      "Jingling Yu",
      "Haozheng Zhang",
      "Ralf van der Lans",
      "Bertram E Shi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7239_ECCV_2024_paper.php": {
    "title": "SelfSwapper: Self-Supervised Face Swapping via Shape Agnostic Masked AutoEncoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaeseong Lee*",
      "Junha Hyung*",
      "Sohyun Jeong",
      "Jaegul Choo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7241_ECCV_2024_paper.php": {
    "title": "EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenhongyi Yang*",
      "Anastasia Tkach",
      "Shreyas Hampali",
      "Linguang Zhang",
      "Elliot J Crowley",
      "Cem Keskin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7242_ECCV_2024_paper.php": {
    "title": "An Information Theoretical View for Out-Of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hu Jinjing",
      "Wenrui Liu",
      "Hong Chang*",
      "Bingpeng MA",
      "Shiguang Shan",
      "Xilin Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7243_ECCV_2024_paper.php": {
    "title": "DMiT: Deformable Mipmapped Tri-Plane Representation for Dynamic Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing-Wen Yang",
      "Jia-Mu Sun",
      "Yong-Liang Yang",
      "Jie Yang",
      "Ying Shan",
      "Yan-Pei Cao",
      "Lin Gao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7244_ECCV_2024_paper.php": {
    "title": "Gated Temporal Diffusion for Stochastic Long-term Dense Anticipation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Olga Zatsarynna*",
      "Emad Bahrami*",
      "Yazan Abu Farha",
      "Gianpiero Francesca",
      "Jürgen Gall*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7246_ECCV_2024_paper.php": {
    "title": "Gradient-Aware for Class-Imbalanced Semi-supervised Medical Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenbo Qi",
      "Jiafei Wu*",
      "S. C. Chan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7249_ECCV_2024_paper.php": {
    "title": "HowToCaption: Prompting LLMs to Transform Video Annotations at Scale",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nina Shvetsova*",
      "Anna Kukleva",
      "Xudong Hong",
      "Christian Rupprecht",
      "Bernt Schiele",
      "Hilde Kuehne"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7250_ECCV_2024_paper.php": {
    "title": "LabelDistill: Label-guided Cross-modal Knowledge Distillation for Camera-based 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanmin Kim",
      "Youngseok Kim",
      "Sihwan Hwang",
      "Hyeonjun Jeong",
      "Dongsuk Kum*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7256_ECCV_2024_paper.php": {
    "title": "Beyond the Data Imbalance: Employing the Heterogeneous Datasets for Vehicle Maneuver Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyeongseok Jeon",
      "Sanmin Kim",
      "Abi Rahman Syamil",
      "Junsoo Kim",
      "Dongsuk Kum*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7257_ECCV_2024_paper.php": {
    "title": "On Pretraining Data Diversity for Self-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hasan Abed Al Kader Hammoud*",
      "Tuhin Das",
      "Fabio Pizzati*",
      "Philip Torr",
      "Adel Bibi",
      "Bernard Ghanem"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7259_ECCV_2024_paper.php": {
    "title": "Look Around and Learn: Self-Training Object Detection by Exploration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gianluca Scarpellini*",
      "Stefano Rosa*",
      "Pietro Morerio",
      "Lorenzo Natale",
      "Alessio Del Bue"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7260_ECCV_2024_paper.php": {
    "title": "Bayesian Self-Training for Semi-Supervised 3D Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ozan Unal*",
      "Christos Sakaridis",
      "Luc Van Gool"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7261_ECCV_2024_paper.php": {
    "title": "Motion and Structure from Event-based Normal Flow",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongyang Ren",
      "Bangyan Liao",
      "Delei Kong",
      "Jinghang Li",
      "Peidong Liu",
      "Laurent Kneip",
      "Guillermo Gallego",
      "Yi Zhou*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7265_ECCV_2024_paper.php": {
    "title": "ParCo: Part-Coordinating Text-to-Motion Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiran Zou",
      "Shangyuan Yuan",
      "Shian Du",
      "Yu Wang",
      "Chang Liu",
      "Yi Xu",
      "Jie Chen",
      "Xiangyang Ji*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7269_ECCV_2024_paper.php": {
    "title": "Learning to Complement and to Defer to Multiple Users",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheng Zhang",
      "Wenjie Ai",
      "Kevin Wells",
      "David M Rosewarne",
      "Thanh-Toan Do",
      "Gustavo Carneiro*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7282_ECCV_2024_paper.php": {
    "title": "Tiny Models are the Computational Saver for Large Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingyuan Wang*",
      "Barry Cardiff",
      "Antoine Frappé",
      "Benoit Larras",
      "Deepu John*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7284_ECCV_2024_paper.php": {
    "title": "DragVideo: Interactive Drag-style Video Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufan Deng",
      "Ruida WANG",
      "Yuhao ZHANG",
      "Yu-Wing Tai*",
      "Chi-Keung Tang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7285_ECCV_2024_paper.php": {
    "title": "Multi-Sentence Grounding for Long-term Instructional Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeqian Li",
      "Qirui Chen",
      "Tengda Han",
      "Ya Zhang",
      "Yan-Feng Wang",
      "Weidi Xie*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7293_ECCV_2024_paper.php": {
    "title": "Do Generalised Classifiers really work on Human Drawn Sketches?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hmrishav Bandyopadhyay*",
      "Pinaki Nath Chowdhury",
      "Aneeshan Sain",
      "Subhadeep Koley",
      "Tao Xiang",
      "Ayan Kumar Bhunia",
      "Yi-Zhe Song"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7316_ECCV_2024_paper.php": {
    "title": "KMTalk: Speech-Driven 3D Facial Animation with Key Motion Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihao Xu",
      "Shengjie Gong",
      "Jiapeng Tang",
      "Lingyu Liang",
      "Yining Huang",
      "Haojie Li",
      "Shuangping Huang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7318_ECCV_2024_paper.php": {
    "title": "Head360: Learning a Parametric 3D Full-Head for Free-View Synthesis in 360°",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxiao He",
      "Yiyu Zhuang",
      "Yanwen Wang",
      "Yao Yao",
      "Siyu Zhu",
      "Xiaoyu Li",
      "Qi Zhang",
      "Xun Cao",
      "Hao Zhu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7327_ECCV_2024_paper.php": {
    "title": "MotionDirector: Motion Customization of Text-to-Video Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Zhao",
      "Yuchao Gu",
      "Jay Zhangjie Wu",
      "David Junhao Zhang",
      "Jia-Wei Liu",
      "weijia wu",
      "Jussi Keppo",
      "Mike Zheng Shou*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7328_ECCV_2024_paper.php": {
    "title": "Text2LiDAR: Text-guided LiDAR Point Clouds Generation via Equirectangular Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Wu*",
      "Kaihua Zhang",
      "Jianjun Qian",
      "Jin Xie*",
      "Jian Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7336_ECCV_2024_paper.php": {
    "title": "Enhanced Motion Forecasting with Visual Relation Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sungjune Kim",
      "Hadam Baek",
      "Seunggwan Lee",
      "Hyung-gun Chi",
      "Hyerin Lim",
      "Jinkyu Kim*",
      "Sangpil Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7338_ECCV_2024_paper.php": {
    "title": "Rate-Distortion-Cognition Controllable Versatile Neural Image Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinming Liu*",
      "Ruoyu Feng",
      "Yunpeng Qi",
      "Qiuyu Chen",
      "Zhibo Chen",
      "Wenjun Zeng",
      "Xin Jin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7344_ECCV_2024_paper.php": {
    "title": "Temporal As a Plugin: Unsupervised Video Denoising with Pre-Trained Image Denoisers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixuan Fu*",
      "Lanqing Guo",
      "Chong Wang",
      "Yufei Wang",
      "Zhihao Li",
      "Bihan Wen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7349_ECCV_2024_paper.php": {
    "title": "LiDAR-based All-weather 3D Object Detection via Prompting and Distilling 4D Radar",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujeong Chae",
      "Hyeonseong Kim",
      "Changgyoon Oh",
      "Minseok Kim",
      "Kuk-Jin Yoon*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7350_ECCV_2024_paper.php": {
    "title": "MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Liu*",
      "Yichen Zhu",
      "Jindong Gu",
      "Yunshi Lan",
      "Chao Yang",
      "Yu Qiao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7353_ECCV_2024_paper.php": {
    "title": "Post-training Quantization with Progressive Calibration and Activation Relaxing for Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siao Tang",
      "Xin Wang*",
      "Hong Chen",
      "Chaoyu Guan",
      "Zewen Wu",
      "Yansong Tang",
      "Wenwu Zhu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7356_ECCV_2024_paper.php": {
    "title": "Scene Coordinate Reconstruction: Posing of Image Collections via Incremental Learning of a Relocalizer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eric Brachmann*",
      "Jamie Wynn",
      "Shuai Chen",
      "Tommaso Cavallari",
      "Aron Monszpart",
      "Daniyar Turmukhambetov",
      "Victor Adrian Prisacariu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7357_ECCV_2024_paper.php": {
    "title": "Diffusion Models are Geometry Critics: Single Image 3D Editing Using Pre-Trained Diffusion Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruicheng Wang*",
      "Jianfeng Xiang",
      "Jiaolong Yang",
      "Xin Tong"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7359_ECCV_2024_paper.php": {
    "title": "Weakly Supervised Co-training with Swapping Assignments for Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyu Yang*",
      "Hossein Rahmani",
      "Dame S Black",
      "Bryan M Williams"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7360_ECCV_2024_paper.php": {
    "title": "StoryImager: A Unified and Efficient Framework for Coherent Story Visualization and Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ming Tao*",
      "Bingkun Bao*",
      "Hao Tang",
      "Yaowei Wang",
      "Changsheng Xu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7364_ECCV_2024_paper.php": {
    "title": "ST-LLM: Large Language Models Are Effective Temporal Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruyang Liu",
      "Chen Li",
      "Haoran Tang",
      "Yixiao Ge",
      "Ying Shan",
      "Ge Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7377_ECCV_2024_paper.php": {
    "title": "Exact Diffusion Inversion via Bidirectional Integration Approximation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guoqiang Zhang*",
      "j.p. lewis",
      "W. Bastiaan Kleijn"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7384_ECCV_2024_paper.php": {
    "title": "Textual Query-Driven Mask Transformer for Domain Generalized Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Byeonghyun Pak",
      "Byeongju Woo",
      "Sunghwan Kim",
      "Dae-hwan Kim",
      "Hoseong Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7387_ECCV_2024_paper.php": {
    "title": "EmoTalk3D: High-Fidelity Free-View Synthesis of Emotional 3D Talking Head",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qianyun He",
      "Xinya Ji",
      "Yicheng Gong",
      "Yuanxun Lu",
      "Zhengyu Diao",
      "Linjia Huang",
      "Yao Yao",
      "Siyu Zhu",
      "Zhan Ma",
      "Songcen Xu",
      "Xiaofei Wu",
      "Zixiao Zhang",
      "Xun Cao",
      "Hao Zhu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7392_ECCV_2024_paper.php": {
    "title": "Arbitrary-Scale Video Super-Resolution with Structural and Textural Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Shang*",
      "Dongwei Ren*",
      "Wanying Zhang",
      "Yuming Fang",
      "Wangmeng Zuo",
      "Kede Ma"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7396_ECCV_2024_paper.php": {
    "title": "Object-Centric Diffusion for Efficient Video Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kumara Kahatapitiya*",
      "Adil Karjauv",
      "Davide Abati*",
      "Fatih Porikli",
      "Yuki M Asano",
      "Amirhossein Habibian"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7404_ECCV_2024_paper.php": {
    "title": "Single-Mask Inpainting for Voxel-based Neural Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiafu Chen*",
      "Tianyi Chu",
      "Jiakai Sun",
      "Wei Xing",
      "Lei Zhao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7408_ECCV_2024_paper.php": {
    "title": "McGrids: Monte Carlo-Driven Adaptive Grids for Iso-Surface Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daxuan Ren*",
      "Hezi Shi",
      "Jianmin Zheng",
      "Jianfei Cai"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7423_ECCV_2024_paper.php": {
    "title": "Freeview Sketching: View-Aware Fine-Grained Sketch-Based Image Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aneeshan Sain*",
      "Pinaki Nath Chowdhury",
      "Subhadeep Koley",
      "Ayan Kumar Bhunia",
      "Yi-Zhe Song"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7430_ECCV_2024_paper.php": {
    "title": "Adapt2Reward: Adapting Video-Language Models to Generalizable Robotic Rewards via Failure Prompts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanting Yang",
      "Minghao Chen*",
      "Qibo Qiu",
      "Jiahao WU",
      "Wenxiao Wang",
      "Binbin Lin",
      "Ziyu Guan",
      "Xiaofei He"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7436_ECCV_2024_paper.php": {
    "title": "Diffusion for Natural Image Matting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihan Hu*",
      "Yiheng Lin",
      "Wei Wang",
      "Yao Zhao",
      "Yunchao Wei*",
      "Humphrey Shi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7441_ECCV_2024_paper.php": {
    "title": "Agglomerative Token Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joakim Bruslund Haurum*",
      "Sergio Escalera",
      "Graham W. Taylor*",
      "Thomas B. Moeslund"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7443_ECCV_2024_paper.php": {
    "title": "CMD: A Cross Mechanism Domain Adaptation Dataset for 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinhao Deng",
      "Wei Ye",
      "Hai Wu",
      "Qiming Xia",
      "Xun Huang",
      "Xin Li",
      "Jin Fang",
      "Wei Li*",
      "Chenglu Wen*",
      "Cheng Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7445_ECCV_2024_paper.php": {
    "title": "Unleashing Text-to-Image Diffusion Prior for Zero-Shot Image Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianjie Luo",
      "Jingwen Chen",
      "Yehao Li",
      "Yingwei Pan*",
      "Jianlin Feng",
      "Hongyang Chao",
      "Ting Yao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7447_ECCV_2024_paper.php": {
    "title": "ClusteringSDF: Self-Organized Neural Implicit Surfaces for 3D Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianhao Wu*",
      "Chuanxia Zheng",
      "Qianyi Wu",
      "Tat-Jen Cham"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7448_ECCV_2024_paper.php": {
    "title": "NAMER: Non-Autoregressive Modeling for Handwritten Mathematical Expression Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenyu Liu",
      "Jia Pan",
      "Jinshui Hu",
      "Baocai Yin",
      "Bing Yin",
      "Mingjun Chen",
      "Cong Liu",
      "Jun Du*",
      "Qingfeng Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7451_ECCV_2024_paper.php": {
    "title": "GIVT: Generative Infinite-Vocabulary Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Tschannen*",
      "Cian Eastwood",
      "Fabian Mentzer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7453_ECCV_2024_paper.php": {
    "title": "Mismatch Quest: Visual and Textual Feedback for Image-Text Misalignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brian Gordon*",
      "Yonatan Bitton*",
      "Yonatan Shafir",
      "Roopal Garg",
      "Xi Chen",
      "Dani Lischinski",
      "Daniel Cohen-Or",
      "Idan Szpektor"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7456_ECCV_2024_paper.php": {
    "title": "Regulating Model Reliance on Non-Robust Features by Smoothing Input Marginal Density",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peiyu Yang*",
      "Naveed Akhtar",
      "Mubarak Shah",
      "Ajmal Mian"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7457_ECCV_2024_paper.php": {
    "title": "Multi-Modal Video Dialog State Tracking in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adnen Abdessaied*",
      "Lei Shi",
      "Andreas Bulling"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7469_ECCV_2024_paper.php": {
    "title": "Factorized Diffusion: Perceptual Illusions by Noise Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Geng*",
      "Inbum Park",
      "Andrew Owens"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7479_ECCV_2024_paper.php": {
    "title": "To Generate or Not? Safety-Driven Unlearned Diffusion Models Are Still Easy To Generate Unsafe Images ... For Now",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yimeng Zhang*",
      "jinghan jia",
      "Xin Chen",
      "Aochuan Chen",
      "Yihua Zhang",
      "Jiancheng Liu",
      "Ke Ding",
      "Sijia Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7483_ECCV_2024_paper.php": {
    "title": "Dissecting Dissonance: Benchmarking Large Multimodal Models Against Self-Contradictory Instructions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jin Gao",
      "Lei Gan",
      "Yuankai Li",
      "Yixin Ye",
      "Dequan Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7485_ECCV_2024_paper.php": {
    "title": "StereoGlue: Joint Feature Matching and Robust Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Barath*",
      "Dmytro Mishkin",
      "Luca Cavalli",
      "Paul-Edouard Sarlin",
      "Petr Hruby",
      "Marc Pollefeys"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7488_ECCV_2024_paper.php": {
    "title": "Boosting Transferability in Vision-Language Attacks via Diversification along the Intersection Region of Adversarial Trajectory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sensen Gao",
      "Xiaojun Jia*",
      "Xuhong Ren",
      "Ivor Tsang",
      "Qing Guo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7492_ECCV_2024_paper.php": {
    "title": "Leveraging Enhanced Queries of Point Sets for Vectorized Map Construction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihao Liu",
      "Xiaoyu Zhang",
      "Guangwei Liu",
      "Ji Zhao*",
      "Ningyi Xu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7495_ECCV_2024_paper.php": {
    "title": "Robust Zero-Shot Crowd Counting and Localization with Adaptive Resolution SAM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jia Wan*",
      "Qiangqiang Wu",
      "Wei Lin",
      "Antoni Chan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7496_ECCV_2024_paper.php": {
    "title": "AWOL: Analysis WithOut synthesis using Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Silvia Zuffi*",
      "Michael J. Black"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7505_ECCV_2024_paper.php": {
    "title": "OneVOS: Unifying Video Object Segmentation with All-in-One Transformer Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wanyun Li",
      "Pinxue Guo",
      "Xinyu Zhou",
      "Lingyi Hong",
      "Yangji He",
      "Xiangyu Zheng",
      "Wei Zhang*",
      "Wenqiang Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7509_ECCV_2024_paper.php": {
    "title": "M3DBench: Towards Omni 3D Assistant with Interleaved Multi-modal Instructions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingsheng Li",
      "Xin Chen",
      "Chi Zhang",
      "Sijin Chen",
      "Hongyuan Zhu",
      "Fukun Yin",
      "Zhuoyuan Li",
      "Gang Yu",
      "Tao Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7514_ECCV_2024_paper.php": {
    "title": "MSD: A Benchmark Dataset for Floor Plan Generation of Building Complexes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Casper van Engelenburg*",
      "Fatemeh Mostafavi",
      "Emanuel Kuhn",
      "Yuntae Jeon",
      "Michael Franzen",
      "Matthias Standfest",
      "Jan van Gemert",
      "Seyran Khademi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7526_ECCV_2024_paper.php": {
    "title": "End-to-End Rate-Distortion Optimized 3D Gaussian Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Henan Wang*",
      "Hanxin Zhu",
      "Tianyu He",
      "Runsen Feng",
      "Jiajun Deng",
      "Jiang Bian",
      "Zhibo Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7531_ECCV_2024_paper.php": {
    "title": "Temporal Residual Jacobians for Rig-free Motion Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanjeev Muralikrishnan*",
      "Niladri Shekhar Dutt",
      "Siddhartha Chaudhuri",
      "Noam Aigerman",
      "Vladimir Kim",
      "Matthew Fisher",
      "Niloy Mitra"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7537_ECCV_2024_paper.php": {
    "title": "LetsMap: Unsupervised Representation Learning for Label-Efficient Semantic BEV Mapping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikhil Gosala*",
      "Kürsat Petek",
      "B Ravi Kiran",
      "Senthil Yogamani",
      "Paulo L. J. Drews-Jr",
      "Wolfram Burgard",
      "Abhinav Valada"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7539_ECCV_2024_paper.php": {
    "title": "Deblurring 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Byeonghyeon Lee*",
      "Howoong Lee",
      "Xiangyu Sun",
      "Usman Ali",
      "Eunbyung Park*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7541_ECCV_2024_paper.php": {
    "title": "Taming Lookup Tables for Efficient Image Retouching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sidi Yang",
      "Binxiao Huang",
      "Mingdeng Cao",
      "Yatai Ji",
      "Hanzhong Guo",
      "Ngai Wong",
      "Yujiu Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7547_ECCV_2024_paper.php": {
    "title": "DualDn: Dual-domain Denoising via Differentiable ISP",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruikang Li",
      "Yujin Wang*",
      "Shiqi Chen",
      "Fan Zhang",
      "Jinwei Gu",
      "Tianfan Xue"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7548_ECCV_2024_paper.php": {
    "title": "Quantization-Friendly Winograd Transformations for Convolutional Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vladimir Protsenko*",
      "Vladimir Kryzhanovskiy",
      "Alexander Filippov"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7554_ECCV_2024_paper.php": {
    "title": "A Task is Worth One Word: Learning with Task Prompts for High-Quality Versatile Image Inpainting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junhao Zhuang",
      "Yanhong Zeng",
      "WENRAN LIU",
      "Chun Yuan*",
      "Kai Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7555_ECCV_2024_paper.php": {
    "title": "Self-supervised Shape Completion via Involution and Implicit Correspondences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengya Liu*",
      "Ajad Chhatkuli",
      "Janis Postels",
      "Luc Van Gool",
      "Federico Tombari"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7557_ECCV_2024_paper.php": {
    "title": "From Fake to Real: Pretraining on Balanced Synthetic Images to Prevent Spurious Correlations in Image Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maan Qraitem*",
      "Kate Saenko",
      "Bryan A. Plummer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7562_ECCV_2024_paper.php": {
    "title": "Cross-Domain Few-Shot Object Detection via Enhanced Open-Set Object Detector",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuqian Fu*",
      "Yu Wang",
      "Yixuan Pan",
      "Xingyu Qiu",
      "Lian Huai",
      "Zeyu Shangguan",
      "Tong Liu",
      "Yanwei Fu",
      "Luc Van Gool",
      "Xingqun Jiang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7564_ECCV_2024_paper.php": {
    "title": "NICP: Neural ICP for 3D Human Registration at Scale",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Riccardo Marin*",
      "Enric Corona",
      "Gerard Pons-Moll"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7567_ECCV_2024_paper.php": {
    "title": "PredBench: Benchmarking Spatio-Temporal Prediction across Diverse Disciplines",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "ZiDong Wang*",
      "Zeyu Lu*",
      "Di Huang*",
      "Tong He",
      "Xihui Liu",
      "Wanli Ouyang",
      "Lei Bai*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7569_ECCV_2024_paper.php": {
    "title": "FontStudio: Shape-Adaptive Diffusion Model for Coherent and Consistent Font Effect Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinzhi Mu*",
      "Li Chen",
      "Bohan CHEN",
      "Shuyang Gu",
      "Jianmin Bao",
      "Dong Chen",
      "Ji Li",
      "Yuhui Yuan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7570_ECCV_2024_paper.php": {
    "title": "Chronologically Accurate Retrieval for Temporal Grounding of Motion-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kent Fujiwara*",
      "Mikihiro Tanaka",
      "Qing Yu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7572_ECCV_2024_paper.php": {
    "title": "StableDrag: Stable Dragging for Point-based Image Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yutao Cui",
      "Xiaotong Zhao",
      "Guozhen Zhang",
      "Shengming Cao",
      "Kai Ma",
      "Limin Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7582_ECCV_2024_paper.php": {
    "title": "Improving Feature Stability during Upsampling -- Spectral Artifacts and the Importance of Spatial Context",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shashank Agnihotri*",
      "Julia Grabinski",
      "Margret Keuper"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7587_ECCV_2024_paper.php": {
    "title": "Dynamic Data Selection for Efficient SSL via Coarse-to-Fine Refinement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditay Tripathi*",
      "Pradeep Shenoy",
      "Anirban Chakraborty"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7593_ECCV_2024_paper.php": {
    "title": "Neural Surface Detection for Unsigned Distance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Federico Stella*",
      "Nicolas Talabot",
      "Hieu Le",
      "Pascal Fua"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7598_ECCV_2024_paper.php": {
    "title": "One-Shot Diffusion Mimicker for Handwritten Text Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gang Dai",
      "Yifan Zhang",
      "Quhui Ke",
      "Qiangya Guo",
      "Shuangping Huang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7601_ECCV_2024_paper.php": {
    "title": "Event-Based Motion Magnification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yutian Chen",
      "Shi Guo*",
      "Yu Fangzheng",
      "Feng Zhang",
      "Jinwei Gu",
      "Tianfan Xue"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7603_ECCV_2024_paper.php": {
    "title": "Improving Neural Surface Reconstruction with Feature Priors from Multi-View Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinlin Ren*",
      "Chenjie Cao",
      "Yanwei Fu*",
      "Xiangyang Xue"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7606_ECCV_2024_paper.php": {
    "title": "Towards Multimodal Sentiment Analysis Debiasing via Bias Purification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dingkang Yang",
      "Mingcheng Li",
      "Dongling Xiao",
      "Yang Liu",
      "Kun Yang",
      "Zhaoyu Chen",
      "Yuzheng Wang",
      "Peng Zhai*",
      "Ke Li",
      "Lihua Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7611_ECCV_2024_paper.php": {
    "title": "Kernel Diffusion: An Alternate Approach to Blind Deconvolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yash Sanghvi*",
      "Yiheng Chi",
      "Stanley Chan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7626_ECCV_2024_paper.php": {
    "title": "MUSES: The Multi-Sensor Semantic Perception Dataset for Driving under Uncertainty",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tim Broedermann*",
      "David Brüggemann",
      "Christos Sakaridis",
      "Kevin Ta",
      "Odysseas Liagouris",
      "Jason Corkill",
      "Luc Van Gool"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7629_ECCV_2024_paper.php": {
    "title": "Discovering Novel Actions from Open World Egocentric Videos with Object-Grounded Visual Commonsense Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanjoy Kundu",
      "Shubham Trehan",
      "Sathyanarayanan N Aakur*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7631_ECCV_2024_paper.php": {
    "title": "Bidirectional Progressive Transformer for Interaction Intention Anticipation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zichen Zhang*",
      "Hongchen Luo",
      "Wei Zhai*",
      "Yu Kang",
      "Yang Cao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7633_ECCV_2024_paper.php": {
    "title": "Reinforcement Learning Meets Visual Odometry",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nico Messikommer*",
      "Giovanni Cioffi",
      "Mathias Gehrig",
      "Davide Scaramuzza"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7634_ECCV_2024_paper.php": {
    "title": "Bucketed Ranking-based Losses for Efficient Training of Object Detectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feyza Yavuz*",
      "Baris Can Cam",
      "Adnan Harun Dogan",
      "Kemal Oksuz",
      "Emre Akbas",
      "Sinan Kalkan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7642_ECCV_2024_paper.php": {
    "title": "Robustness Tokens: Towards Adversarial Robustness of Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brian Pulfer*",
      "Yury Belousov",
      "Slava Voloshynovskiy"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7643_ECCV_2024_paper.php": {
    "title": "RSL-BA: Rolling Shutter Line Bundle Adjustment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongcong Zhang",
      "Bangyan Liao",
      "Yifei Xue",
      "Lu Chen",
      "Peidong Liu",
      "Yizhen Lao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7644_ECCV_2024_paper.php": {
    "title": "DecentNeRFs: Decentralized Neural Radiance Fields from Crowdsourced Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zaid Tasneem*",
      "Akshat Dave",
      "Abhishek Singh",
      "Kushagra Tiwary",
      "Praneeth Vepakomma",
      "Ashok Veeraraghavan",
      "Ramesh Raskar"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7646_ECCV_2024_paper.php": {
    "title": "DreamMesh: Jointly Manipulating and Texturing Triangle Meshes for Text-to-3D Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haibo Yang",
      "Yang Chen",
      "Yingwei Pan*",
      "Ting Yao",
      "Zhineng Chen",
      "Zuxuan Wu",
      "Yu-Gang Jiang",
      "Tao Mei"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7650_ECCV_2024_paper.php": {
    "title": "Unveiling Typographic Deceptions: Insights of the Typographic Vulnerability in Large Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Cheng",
      "Erjia Xiao",
      "Jindong Gu",
      "Le Yang",
      "Jinhao Duan",
      "Jize Zhang",
      "Jiahang Cao",
      "Kaidi Xu",
      "Renjing Xu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7652_ECCV_2024_paper.php": {
    "title": "N2F2: Hierarchical Scene Understanding with Nested Neural Feature Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yash Bhalgat*",
      "Iro Laina",
      "Joao F Henriques",
      "Andrew Zisserman",
      "Andrea Vedaldi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7654_ECCV_2024_paper.php": {
    "title": "ConceptExpress: Harnessing Diffusion Models for Single-image Unsupervised Concept Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaozhe Hao*",
      "Kai Han*",
      "Zhengyao Lv",
      "Shihao Zhao",
      "Kwan-Yee K. Wong*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7655_ECCV_2024_paper.php": {
    "title": "PairingNet: A Learning-based Pair-searching and -matching Network for Image Fragments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rixin Zhou*",
      "Ding Xia",
      "YI ZHANG",
      "honglin pang",
      "Xi Yang",
      "chuntao li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7657_ECCV_2024_paper.php": {
    "title": "Skeleton-based Group Activity Recognition via Spatial-Temporal Panoramic Graph",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengcen Li",
      "Xinle Chang",
      "Yueran Li",
      "Jingyong Su*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7658_ECCV_2024_paper.php": {
    "title": "Towards Multimodal Open-Set Domain Generalization and Adaptation through Self-supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Dong*",
      "Eleni Chatzi*",
      "Olga Fink*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7666_ECCV_2024_paper.php": {
    "title": "ReCON: Training-Free Acceleration for Text-to-Image Synthesis with Retrieval of Concept Prompt Trajectories",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen-Yi Lu*",
      "Shubham Agarwal",
      "Md Mehrab Tanjim",
      "Kanak Mahadik",
      "Anup Rao",
      "Subrata Mitra",
      "Shiv K Saini",
      "Saurabh Bagchi",
      "Somali Chaterji"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7670_ECCV_2024_paper.php": {
    "title": "AMES: Asymmetric and Memory-Efficient Similarity Estimation for Instance-level Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pavel Suma*",
      "Giorgos Kordopatis-Zilos",
      "Ahmet Iscen",
      "Giorgos Tolias"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7673_ECCV_2024_paper.php": {
    "title": "TCAN: Animating Human Images with Temporally Consistent Pose Guidance using Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeongho Kim*",
      "Min-Jung Kim*",
      "Junsoo Lee",
      "Jaegul Choo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7674_ECCV_2024_paper.php": {
    "title": "3D Hand Sequence Recovery from Real Blurry Images and Event Stream",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "JoonKyu Park",
      "Gyeongsik Moon",
      "Weipeng Xu",
      "Evan Kaseman",
      "Takaaki Shiratori",
      "Kyoung Mu Lee*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7675_ECCV_2024_paper.php": {
    "title": "GlobalPointer: Large-Scale Plane Adjustment with Bi-Convex Relaxation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bangyan Liao",
      "Zhenjun Zhao",
      "Lu Chen",
      "Haoang Li",
      "Daniel Cremers",
      "Peidong Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7681_ECCV_2024_paper.php": {
    "title": "Dissolving Is Amplifying: Towards Fine-Grained Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Shi*",
      "Pengyi Zhang",
      "Ni Zhang",
      "Hakim Ghazzai",
      "Peter Wonka"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7682_ECCV_2024_paper.php": {
    "title": "StyleCity: Large-Scale 3D Urban Scenes Stylization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingshu Chen",
      "Huajian Huang*",
      "Tuan-Anh Vu",
      "Ka Chun Shum",
      "Sai-Kit Yeung"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7685_ECCV_2024_paper.php": {
    "title": "ViG-Bias: Visually Grounded Bias Discovery and Mitigation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Badr-Eddine Marani*",
      "Mohamed Hanini",
      "Nihitha Malayarukil",
      "Stergios Christodoulidis",
      "Maria Vakalopoulou",
      "Enzo Ferrante"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7690_ECCV_2024_paper.php": {
    "title": "DiffBIR: Toward Blind Image Restoration with Generative Diffusion Prior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinqi Lin*",
      "Jingwen He",
      "Ziyan Chen",
      "Zhaoyang Lyu",
      "Bo Dai",
      "Fanghua Yu",
      "Yu Qiao",
      "Wanli Ouyang",
      "Chao Dong*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7691_ECCV_2024_paper.php": {
    "title": "Assessing Sample Quality via the Latent Space of Generative Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyi Xu*",
      "Hieu Le",
      "Dimitris Samaras"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7695_ECCV_2024_paper.php": {
    "title": "Relightable Neural Actor with Intrinsic Decomposition and Pose Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Diogo Carbonera Luvizon*",
      "Vladislav Golyanik",
      "Adam Kortylewski",
      "Marc Habermann",
      "Christian Theobalt"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7696_ECCV_2024_paper.php": {
    "title": "Sur^2f: A Hybrid Representation for High-Quality and Efficient Surface Reconstruction from Multi-view Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhangjin Huang*",
      "Zhihao Liang",
      "Kui Jia*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7707_ECCV_2024_paper.php": {
    "title": "HO-Gaussian: Hybrid Optimization of 3D Gaussian Splatting for Urban Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuopeng Li*",
      "Yilin Zhang",
      "Chenming Wu",
      "Jianke Zhu*",
      "Liangjun Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7708_ECCV_2024_paper.php": {
    "title": "Pseudo-keypoint RKHS Learning for Self-supervised 6DoF Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yangzheng Wu*",
      "Michael Alan Greenspan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7710_ECCV_2024_paper.php": {
    "title": "Consistent 3D Line Mapping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xulong Bai",
      "Hainan Cui*",
      "Shuhan Shen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7717_ECCV_2024_paper.php": {
    "title": "Distributed Active Client Selection With Noisy Clients Using Model Association Scores",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kwang In Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7720_ECCV_2024_paper.php": {
    "title": "PixOOD: Pixel-Level Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tomas Vojir*",
      "Jan Sochman",
      "Jiri Matas"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7721_ECCV_2024_paper.php": {
    "title": "GarmentCodeData: A Dataset of 3D Made-to-Measure Garments With Sewing Patterns",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maria Korosteleva*",
      "Timur Levent Kesdogan",
      "Fabian Kemper",
      "Stephan Wenninger",
      "Jasmin Koller",
      "Yuhan Zhang",
      "Mario Botsch",
      "Olga Sorkine-Hornung"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7726_ECCV_2024_paper.php": {
    "title": "Towards a Density Preserving Objective Function for Learning on Point Sets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haritha Jayasinghe*",
      "Ioannis Brilakis"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7731_ECCV_2024_paper.php": {
    "title": "AnatoMask: Enhancing Medical Image Segmentation with Reconstruction-guided Self-masking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuheng Li",
      "Tianyu Luan",
      "Yizhou Wu",
      "Shaoyan Pan",
      "Yenho Chen",
      "Xiaofeng Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7733_ECCV_2024_paper.php": {
    "title": "VF-NeRF: Viewshed Fields for Rigid NeRF Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leo Segre*",
      "Shai Avidan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7734_ECCV_2024_paper.php": {
    "title": "Task-Driven Uncertainty Quantification in Inverse Problems via Conformal Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeffrey Wen*",
      "Rizwan Ahmad",
      "Phillip Schniter"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7743_ECCV_2024_paper.php": {
    "title": "Trainable Highly-expressive Activation Functions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Irit Chelly*",
      "Shahaf E. Finder",
      "Shira Ifergane",
      "Oren Freifeld"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7745_ECCV_2024_paper.php": {
    "title": "Region-Aware Sequence-to-Sequence Learning for Hyperspectral Denoising",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "JiaHua Xiao",
      "Yang Liu",
      "Xing Wei*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7748_ECCV_2024_paper.php": {
    "title": "Self-Supervised Representation Learning for Adversarial Attack Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Li*",
      "Plamen Angelov",
      "Neeraj Suri"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7757_ECCV_2024_paper.php": {
    "title": "Do text-free diffusion models learn discriminative visual representations?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soumik Mukhopadhyay*",
      "Matthew A Gwilliam*",
      "Yosuke Yamaguchi",
      "Vatsal Agarwal",
      "Namitha Padmanabhan",
      "Archana Swaminathan",
      "Tianyi Zhou",
      "Jun Ohya",
      "Abhinav Shrivastava"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7761_ECCV_2024_paper.php": {
    "title": "Clean & Compact: Efficient Data-Free Backdoor Defense with Model Compactness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huy Phan*",
      "Jinqi Xiao",
      "Yang Sui",
      "Tianfang Zhang",
      "Zijie Tang",
      "Cong Shi",
      "Yan Wang",
      "Yingying Chen",
      "Bo Yuan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7764_ECCV_2024_paper.php": {
    "title": "DOCCI: Descriptions of Connected and Contrasting Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yasumasa Onoe*",
      "Sunayana Rane",
      "Zachary E Berger",
      "Yonatan Bitton",
      "Jaemin Cho",
      "Roopal Garg",
      "Alexander Ku",
      "Zarana Parekh",
      "Jordi Pont-Tuset",
      "Garrett Tanzer",
      "Su Wang",
      "Jason M Baldridge"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7766_ECCV_2024_paper.php": {
    "title": "EAS-SNN: End-to-End Adaptive Sampling and Representation for Event-based Detection with Recurrent Spiking Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziming Wang",
      "Ziling Wang",
      "Huaning Li",
      "Lang Qin",
      "Runhao Jiang",
      "De Ma*",
      "Huajin Tang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7767_ECCV_2024_paper.php": {
    "title": "AttentionHand: Text-driven Controllable Hand Image Generation for 3D Hand Reconstruction in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junho Park",
      "Kyeongbo Kong",
      "Suk-Ju Kang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7772_ECCV_2024_paper.php": {
    "title": "Dataset Quantization with Active Learning based Adaptive Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenghao Zhao*",
      "Yuzhang Shang",
      "Junyi Wu",
      "Yan Yan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7775_ECCV_2024_paper.php": {
    "title": "LogoSticker: Inserting Logos into Diffusion Models for Customized Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingkang Zhu",
      "Xi CHEN",
      "Zhongdao Wang",
      "Hengshuang Zhao*",
      "Jiaya Jia*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7776_ECCV_2024_paper.php": {
    "title": "LEROjD: Lidar Extended Radar-Only Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Patrick Palmer*",
      "Martin Krüger",
      "Stefan Schütte",
      "Richard Altendorfer",
      "Ganesh Adam",
      "Torsten Bertram"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7778_ECCV_2024_paper.php": {
    "title": "ProCreate, Don't Reproduce! Propulsive Energy Diffusion for Creative Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jack Lu*",
      "Ryan Teehan*",
      "Mengye Ren*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7780_ECCV_2024_paper.php": {
    "title": "Match-Stereo-Videos: Bidirectional Alignment for Consistent Dynamic Stereo Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junpeng Jing*",
      "Ye Mao",
      "Krystian Mikolajczyk*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7781_ECCV_2024_paper.php": {
    "title": "Probabilistic Image-Driven Traffic Modeling via Remote Sensing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Scott Workman*",
      "Armin Hadzic"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7782_ECCV_2024_paper.php": {
    "title": "IntrinsicAnything: Learning Diffusion Priors for Inverse Rendering Under Unknown Illumination",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xi Chen*",
      "Sida Peng",
      "Dongchen Yang",
      "Yuan Liu",
      "Bowen Pan",
      "Chengfei Lyu",
      "Xiaowei Zhou*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7783_ECCV_2024_paper.php": {
    "title": "VideoStudio: Generating Consistent-Content and Multi-Scene Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fuchen Long",
      "Zhaofan Qiu*",
      "Ting Yao",
      "Tao Mei"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7790_ECCV_2024_paper.php": {
    "title": "Semantic Residual Prompts for Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Martin Menabue*",
      "Emanuele Frascaroli",
      "Matteo Boschini",
      "Enver Sangineto",
      "Lorenzo Bonicelli",
      "Angelo Porrello*",
      "SIMONE CALDERARA"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7791_ECCV_2024_paper.php": {
    "title": "TransCAD: A Hierarchical Transformer for CAD Sequence Inference from Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elona Dupont*",
      "Kseniya Cherenkova",
      "Dimitrios Mallis",
      "Gleb A Gusev",
      "Anis Kacem",
      "Djamila Aouada"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7792_ECCV_2024_paper.php": {
    "title": "ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siming Yan*",
      "Min Bai",
      "Weifeng Chen",
      "Xiong Zhou",
      "Qixing Huang",
      "Li Erran Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7799_ECCV_2024_paper.php": {
    "title": "Mixture of Efficient Diffusion Experts Through Automatic Interval and Sub-Network Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alireza Ganjdanesh*",
      "Yan Kang",
      "Yuchen Liu",
      "Richard Zhang",
      "Zhe Lin",
      "Heng Huang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7801_ECCV_2024_paper.php": {
    "title": "Occupancy as Set of Points",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiang Shi",
      "Tianheng Cheng",
      "Qian Zhang",
      "Wenyu Liu",
      "Xinggang Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7802_ECCV_2024_paper.php": {
    "title": "UAV First-Person Viewers Are Radiance Field Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liqi Yan*",
      "Qifan Wang",
      "Junhan Zhao",
      "Qiang Guan",
      "Zheng Tang",
      "Jianhui Zhang",
      "Dongfang Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7806_ECCV_2024_paper.php": {
    "title": "Rethinking Few-shot Class-incremental Learning: Learning from Yourself",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu-Ming Tang",
      "Yi-Xing Peng",
      "Jingke Meng*",
      "Wei-Shi Zheng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7811_ECCV_2024_paper.php": {
    "title": "ProSub: Probabilistic Open-Set Semi-Supervised Learning with Subspace-Based Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Erik Wallin*",
      "Lennart Svensson",
      "Fredrik Kahl",
      "Lars Hammarstrand"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7817_ECCV_2024_paper.php": {
    "title": "A Fair Ranking and New Model for Panoptic Scene Graph Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julian Lorenz*",
      "Alexander Pest",
      "Daniel Kienzle",
      "Katja Ludwig",
      "Rainer Lienhart"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7822_ECCV_2024_paper.php": {
    "title": "Pick-a-back: Selective Device-to-Device Knowledge Transfer in Federated Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "HyungJune Lee*",
      "JinYi Yoon"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7823_ECCV_2024_paper.php": {
    "title": "Compensation Sampling for Improved Convergence in Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hui Lu*",
      "Albert Ali Salah",
      "Ronald Poppe"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7826_ECCV_2024_paper.php": {
    "title": "Situated Instruction Following",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "So Yeon Min*",
      "Xavier Puig",
      "Devendra Singh Chaplot",
      "Tsung-Yen Yang",
      "Priyam Parashar",
      "Akshara Rai",
      "Ruslan Salakhutdinov",
      "Yonatan Bisk",
      "Roozbeh Mottaghi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7832_ECCV_2024_paper.php": {
    "title": "Holodepth: Programmable Depth-Varying Projection via Computer-Generated Holography",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dorian Chan*",
      "Matthew O'Toole",
      "Sizhuo Ma",
      "Jian Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7833_ECCV_2024_paper.php": {
    "title": "SceneScript: Reconstructing Scenes With An Autoregressive Structured Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Armen Avetisyan*",
      "Christopher Xie",
      "Henry Howard-Jenkins",
      "Tsun-Yi Yang",
      "Samir Aroudj",
      "Suvam Patra",
      "Fuyang Zhang",
      "Luke Holland",
      "Duncan Frost",
      "Campbell Orme",
      "Jakob Engel",
      "Edward Miller",
      "Richard Newcombe",
      "Vasileios Balntas"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7834_ECCV_2024_paper.php": {
    "title": "GalLop: Learning global and local prompts for vision-language models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marc Lafon*",
      "Elias Ramzi*",
      "Clément Rambour",
      "Nicolas Audebert",
      "Nicolas Thome"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7836_ECCV_2024_paper.php": {
    "title": "Depth on Demand: Streaming Dense Depth from a Low Frame Rate Active Sensor",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrea Conti*",
      "Matteo Poggi",
      "Valerio Cambareri",
      "Stefano Mattoccia"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7844_ECCV_2024_paper.php": {
    "title": "Lossy Image Compression with Foundation Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucas Relic*",
      "Roberto Azevedo",
      "Markus Gross",
      "Christopher Schroers*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7850_ECCV_2024_paper.php": {
    "title": "CLIP-DINOiser: Teaching CLIP a few DINO tricks for open-vocabulary semantic segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Monika Wysoczańska*",
      "Oriane Siméoni",
      "Michaël Ramamonjisoa",
      "Andrei Bursuc",
      "Tomasz Trzciński",
      "Patrick Pérez"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7855_ECCV_2024_paper.php": {
    "title": "FMBoost: Boosting Latent Diffusion with Flow Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Johannes S Fischer*",
      "Ming Gui",
      "Pingchuan Ma",
      "Nick Stracke",
      "Stefan Andreas Baumann",
      "Vincent Tao Hu",
      "Björn Ommer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7860_ECCV_2024_paper.php": {
    "title": "COMPOSE: Comprehensive Portrait Shadow Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrew Z Hou*",
      "Zhixin Shu",
      "Xuaner Zhang",
      "He Zhang",
      "Yannick Hold-Geoffroy",
      "Jae Shin Yoon",
      "Xiaoming Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7862_ECCV_2024_paper.php": {
    "title": "LNL+K: Enhancing Learning with Noisy Labels Through Noise Source Knowledge Integration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siqi Wang*",
      "Bryan Plummer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7865_ECCV_2024_paper.php": {
    "title": "Diffusion Models as Data Mining Tools",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ioannis Siglidis*",
      "Aleksander Holynski",
      "Alexei A. Efros",
      "Mathieu Aubry",
      "Shiry Ginosar"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7866_ECCV_2024_paper.php": {
    "title": "Graph Neural Network Causal Explanation via Neural Causal Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arman Behnam*",
      "Binghui Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7868_ECCV_2024_paper.php": {
    "title": "Unsupervised, Online and On-The-Fly Anomaly Detection For Non-Stationary Image Distributions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Declan GD McIntosh*",
      "Alexandra Branzan Albu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7871_ECCV_2024_paper.php": {
    "title": "Photorealistic Object Insertion with Diffusion-Guided Inverse Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruofan Liang",
      "Zan Gojcic",
      "Merlin Nimier-David",
      "David Acuna",
      "Nandita Vijaykumar",
      "Sanja Fidler",
      "Zian Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7875_ECCV_2024_paper.php": {
    "title": "GAReT: Cross-view Video Geolocalization with Adapters and Auto-Regressive Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manu S Pillai*",
      "Mamshad Nayeem Rizve",
      "Mubarak Shah"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7882_ECCV_2024_paper.php": {
    "title": "SAMFusion: Sensor-Adaptive Multimodal Fusion for 3D Object Detection in Adverse Weather",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Edoardo Palladin*",
      "Roland Dietze*",
      "Praveen Narayanan",
      "Mario Bijelic",
      "Felix Heide"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7885_ECCV_2024_paper.php": {
    "title": "Generating Physically Realistic and Directable Human Motions from Multi-Modal Inputs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aayam Shrestha",
      "Pan Liu*",
      "German Ros",
      "Kai Yuan*",
      "Alan Fern"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7890_ECCV_2024_paper.php": {
    "title": "CoTracker: It is Better to Track Together",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikita Karaev*",
      "Ignacio Rocco",
      "Ben Graham",
      "Natalia Neverova",
      "Andrea Vedaldi",
      "Christian Rupprecht"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7894_ECCV_2024_paper.php": {
    "title": "SPHINX: A Mixer of Weights, Visual Embeddings and Image Scales for Multi-modal Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyi Lin",
      "Dongyang Liu",
      "Renrui Zhang",
      "Peng Gao*",
      "Longtian Qiu",
      "Han Xiao",
      "Han Qiu",
      "Wenqi Shao",
      "Keqin Chen",
      "Jiaming Han",
      "Siyuan Huang",
      "Yichi Zhang",
      "Xuming He",
      "Yu Qiao*",
      "Hongsheng Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7899_ECCV_2024_paper.php": {
    "title": "PathMMU: A Massive Multimodal Expert-Level Benchmark for Understanding and Reasoning in Pathology",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Sun*",
      "Hao Wu",
      "Chenglu Zhu",
      "Sunyi Zheng",
      "Qizi Chen",
      "Kai Zhang",
      "Yunlong Zhang",
      "Dan Wan",
      "Xiaoxiao Lan",
      "Mengyue Zheng",
      "Jingxiong Li",
      "Xinheng Lyu",
      "Tao Lin*",
      "Lin Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7902_ECCV_2024_paper.php": {
    "title": "Improving Adversarial Transferability via Model Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Avery Ma*",
      "Amir-massoud Farahmand",
      "Yangchen Pan",
      "Philip Torr",
      "Jindong Gu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7903_ECCV_2024_paper.php": {
    "title": "RealGen: Retrieval Augmented Generation for Controllable Traffic Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhao Ding*",
      "Yulong Cao",
      "DING ZHAO",
      "Chaowei Xiao",
      "Marco Pavone"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7904_ECCV_2024_paper.php": {
    "title": "ADen: Adaptive Density Representations for Sparse-view Camera Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Tang",
      "Weiyao Wang",
      "Pierre Gleize",
      "Matt Feiszli*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7908_ECCV_2024_paper.php": {
    "title": "Embodied Understanding of Driving Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunsong Zhou*",
      "Linyan Huang",
      "Qingwen Bu",
      "Jia Zeng",
      "Tianyu Li",
      "Hang Qiu",
      "Hongzi Zhu",
      "Minyi Guo",
      "Yu Qiao",
      "Hongyang Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7910_ECCV_2024_paper.php": {
    "title": "Learning to Drive via Asymmetric Self-Play",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chris Zhang*",
      "Sourav Biswas",
      "Kelvin Wong",
      "Kion Fallah",
      "Lunjun Zhang",
      "Dian Chen",
      "Sergio Casas",
      "Raquel Urtasun"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7914_ECCV_2024_paper.php": {
    "title": "OpenIns3D: Snap and Lookup for 3D Open-vocabulary Instance Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhening Huang",
      "Xiaoyang Wu",
      "Xi Chen",
      "Hengshuang Zhao*",
      "Lei Zhu",
      "Joan Lasenby*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7919_ECCV_2024_paper.php": {
    "title": "ViLA: Efficient Video-Language Alignment for Video Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xijun Wang*",
      "Junbang Liang",
      "Chun-Kai Wang",
      "Kenan Deng",
      "Yu Lou",
      "Ming C Lin",
      "Shan Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7921_ECCV_2024_paper.php": {
    "title": "Factorizing Text-to-Video Generation by Explicit Image Conditioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rohit Girdhar*",
      "Mannat Singh",
      "Andrew Brown",
      "Quentin Duval",
      "Samaneh Azadi",
      "Sai Saketh Rambhatla",
      "Mian Akbar Shah",
      "Xi Yin",
      "Devi Parikh",
      "Ishan Misra"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7923_ECCV_2024_paper.php": {
    "title": "MobileDiffusion: Instant Text-to-Image Generation on Mobile Devices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Zhao*",
      "Zhisheng Xiao*",
      "Yanwu Xu",
      "Haolin Jia",
      "Tingbo Hou"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7925_ECCV_2024_paper.php": {
    "title": "Open-Set Biometrics: Beyond Good Closed-Set Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiyang Su",
      "Minchul Kim",
      "Feng Liu",
      "Anil Jain",
      "Xiaoming Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7926_ECCV_2024_paper.php": {
    "title": "UNIT: Backdoor Mitigation via Automated Neural Distribution Tightening",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyuan Cheng*",
      "Guangyu Shen",
      "Kaiyuan Zhang",
      "Guanhong Tao",
      "Shengwei An",
      "Hanxi Guo",
      "Shiqing Ma",
      "Xiangyu Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7930_ECCV_2024_paper.php": {
    "title": "Which Model Generated This Image? A Model-Agnostic Approach for Origin Attribution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fengyuan Liu",
      "Haochen Luo",
      "Yiming Li",
      "Philip Torr",
      "Jindong Gu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7932_ECCV_2024_paper.php": {
    "title": "Osmosis: RGBD Diffusion Prior for Underwater Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Opher Bar Nathan*",
      "Deborah Levy",
      "Tali Treibitz",
      "Dan Rosenbaum"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7935_ECCV_2024_paper.php": {
    "title": "Towards Adaptive Pseudo-label Learning for Semi-Supervised Temporal Action Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feixiang Zhou",
      "Bryan Williams",
      "Hossein Rahmani*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7936_ECCV_2024_paper.php": {
    "title": "Computing the Lipschitz constant needed for fast scene recovery from CASSI measurements",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Niels Chr Overgaard*",
      "Anders Holst"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7937_ECCV_2024_paper.php": {
    "title": "DatasetNeRF: Efficient 3D-aware Data Factory with Generative Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Chi*",
      "Fangneng Zhan",
      "Sibo Wu",
      "Christian Theobalt",
      "Adam Kortylewski"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7941_ECCV_2024_paper.php": {
    "title": "Flowed Time of Flight Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mikhail Okunev*",
      "Marc Mapeke",
      "Benjamin Attal",
      "Christian Richardt",
      "Matthew O'Toole",
      "James Tompkin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7943_ECCV_2024_paper.php": {
    "title": "3D-GOI: 3D GAN Omni-Inversion for Multifaceted and Multi-object Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoran Li",
      "Long Ma",
      "Haolin Shi",
      "Yanbin Hao",
      "Yong Liao*",
      "Lechao Cheng",
      "Peng Yuan Zhou*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7945_ECCV_2024_paper.php": {
    "title": "Fast Registration of Photorealistic Avatars for VR Facial Animation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaitanya Patel*",
      "Shaojie Bai",
      "Te-Li Wang",
      "Jason Saragih",
      "Shih-En Wei"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7950_ECCV_2024_paper.php": {
    "title": "CoPT: Unsupervised Domain Adaptive Segmentation using Domain-Agnostic Text Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cristina Mata*",
      "Kanchana N Ranasinghe",
      "Michael S Ryoo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7957_ECCV_2024_paper.php": {
    "title": "HiFi-Score: Fine-grained Image Description Evaluation with Hierarchical Parsing Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziwei Yao",
      "Ruiping Wang*",
      "Xilin Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7960_ECCV_2024_paper.php": {
    "title": "Image-to-Lidar Relational Distillation for Autonomous Driving Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anas Mahmoud*",
      "Ali Harakeh",
      "Steven Waslander"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7965_ECCV_2024_paper.php": {
    "title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gemma Canet Tarrés*",
      "Zhe Lin",
      "Zhifei Zhang",
      "Jianming Zhang",
      "Yizhi Song",
      "Dan Ruta",
      "Andrew Gilbert",
      "John Collomosse",
      "Soo Ye Kim"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7968_ECCV_2024_paper.php": {
    "title": "Large-scale Reinforcement Learning for Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinan Zhang*",
      "Eric Tzeng",
      "Yilun Du",
      "Dmitry Kislyuk*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7969_ECCV_2024_paper.php": {
    "title": "CoMusion: Towards Consistent Stochastic Human Motion Prediction via Motion Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiarui Sun*",
      "Girish Chowdhary*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7971_ECCV_2024_paper.php": {
    "title": "FedHARM: Harmonizing Model Architectural Diversity in Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anestis Kastellos*",
      "Athanasios Psaltis",
      "Charalampos Z Patrikakis",
      "Petros Daras"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7976_ECCV_2024_paper.php": {
    "title": "EAGLES: Efficient Accelerated 3D Gaussians with Lightweight EncodingS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sharath Girish*",
      "Kamal Gupta",
      "Abhinav Shrivastava"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7978_ECCV_2024_paper.php": {
    "title": "Global Counterfactual Directions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bartłomiej Sobieski*",
      "Przemyslaw Biecek*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7983_ECCV_2024_paper.php": {
    "title": "TCLC-GS: Tightly Coupled LiDAR-Camera Gaussian Splatting for Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Zhao*",
      "su sun",
      "Ruoyu Wang",
      "Yuliang Guo",
      "Jun-Jun Wan",
      "Zhou Huang",
      "Xinyu Huang",
      "Yingjie Victor Chen",
      "Liu Ren"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7989_ECCV_2024_paper.php": {
    "title": "RT-Pose: A 4D Radar-Tensor based 3D Human Pose Estimation and Localization Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan-Hao Ho",
      "Jen-Hao Cheng",
      "Sheng Yao Kuan",
      "Zhongyu Jiang",
      "Wenhao Chai",
      "Hsiang-Wei Huang",
      "Chih-Lung Lin",
      "Jenq-Neng Hwang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7991_ECCV_2024_paper.php": {
    "title": "EditShield: Protecting Unauthorized Image Editing by Instruction-guided Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruoxi Chen",
      "Haibo Jin",
      "Yixin Liu",
      "Jinyin Chen*",
      "Haohan Wang",
      "Lichao Sun"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8001_ECCV_2024_paper.php": {
    "title": "RICA^2: Rubric-Informed, Calibrated Assessment of Actions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abrar Majeedi",
      "Viswanatha Reddy Gajjala",
      "Satya Sai Srinath Namburi GNVV",
      "Yin Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8007_ECCV_2024_paper.php": {
    "title": "Region-centric Image-Language Pretraining for Open-Vocabulary Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dahun Kim*",
      "Anelia Angelova",
      "Weicheng Kuo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8008_ECCV_2024_paper.php": {
    "title": "Commonly Interesting Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fitim Abdullahu*",
      "Helmut Grabner*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8009_ECCV_2024_paper.php": {
    "title": "Contrasting Deepfakes Diffusion via Contrastive Learning and Global-Local Similarities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lorenzo Baraldi*",
      "Federico Cocchi",
      "Marcella Cornia",
      "Lorenzo Baraldi",
      "Alessandro Nicolosi",
      "Rita Cucchiara"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8023_ECCV_2024_paper.php": {
    "title": "CriSp: Leveraging Tread Depth Maps for Enhanced Crime-Scene Shoeprint Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samia Shafique*",
      "Shu Kong",
      "Charless Fowlkes"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8026_ECCV_2024_paper.php": {
    "title": "Caltech Aerial RGB-Thermal Dataset in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Connor Lee*",
      "Matthew Anderson",
      "Nikhil Ranganathan",
      "Xingxing Zuo",
      "Kevin T Do",
      "Georgia Gkioxari",
      "Soon-Jo Chung"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8027_ECCV_2024_paper.php": {
    "title": "Diffusion Soup: Model Merging for Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin J Biggs*",
      "Arjun Seshadri",
      "Yang Zou",
      "Achin Jain",
      "Aditya Golatkar",
      "Yusheng Xie",
      "Alessandro Achille",
      "Ashwin Swaminathan",
      "Stefano Soatto"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8028_ECCV_2024_paper.php": {
    "title": "Volumetric Rendering with Baked Quadrature Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gopal Sharma*",
      "Daniel Rebain",
      "Kwang Moo Yi",
      "Andrea Tagliasacchi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8031_ECCV_2024_paper.php": {
    "title": "CityGuessr: City-Level Video Geo-Localization on a Global Scale",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Parth Parag Kulkarni*",
      "Gaurav Kumar Nayak",
      "Mubarak Shah"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8036_ECCV_2024_paper.php": {
    "title": "Pseudo-Labelling Should Be Aware of Disguising Channel Activations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changrui Chen",
      "Kurt Debattista",
      "Jungong Han*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8038_ECCV_2024_paper.php": {
    "title": "Bayesian Detector Combination for Object Detection with Crowdsourced Annotations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhi Qin Tan*",
      "Olga Isupova",
      "Gustavo Carneiro",
      "Xiatian Zhu",
      "Yunpeng Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8041_ECCV_2024_paper.php": {
    "title": "Revising Densification in Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuel Rota Bulò*",
      "Lorenzo Porzi",
      "Peter Kontschieder"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8042_ECCV_2024_paper.php": {
    "title": "FlexiEdit: Frequency-Aware Latent Refinement for Enhanced Non-Rigid Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gwanhyeong Koo",
      "Sunjae Yoon",
      "Ji Woo Hong",
      "Chang D. Yoo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8045_ECCV_2024_paper.php": {
    "title": "Smoothness, Synthesis, and Sampling: Re-thinking Unsupervised Multi-View Stereo with DIV Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alex Rich*",
      "Noah Stier",
      "Pradeep Sen",
      "Tobias Hollerer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8050_ECCV_2024_paper.php": {
    "title": "Text Motion Translator: A Bi-Directional Model for Enhanced 3D Human Motion Generation from Open-Vocabulary Descriptions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yijun Qian*",
      "Jack Urbanek",
      "Alexander Hauptmann",
      "Jungdam Won"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8054_ECCV_2024_paper.php": {
    "title": "UL-VIO: Ultra-lightweight Visual-Inertial Odometry with Noise Robust Test-time Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinho Park*",
      "Se Young Chun",
      "Mingoo Seok"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8055_ECCV_2024_paper.php": {
    "title": "PolyOculus: Simultaneous Multi-view Image-based Novel View Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jason J. Yu*",
      "Tristan Aumentado-Armstrong",
      "Fereshteh Forghani",
      "Konstantinos G. Derpanis",
      "Marcus A. Brubaker"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8056_ECCV_2024_paper.php": {
    "title": "R3DS: Reality-linked 3D Scenes for Panoramic Scene Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qirui Wu*",
      "Sonia Raychaudhuri",
      "Daniel Ritchie",
      "Manolis Savva",
      "Angel X Chang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8058_ECCV_2024_paper.php": {
    "title": "A Graph-Based Approach for Category-Agnostic Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Or Hirschorn*",
      "Shai Avidan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8062_ECCV_2024_paper.php": {
    "title": "Depth-guided NeRF Training via Earth Mover's Distance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anita Rau*",
      "Josiah Aklilu",
      "Floyd C Holsinger",
      "Serena Yeung-Levy"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8064_ECCV_2024_paper.php": {
    "title": "INTRA: Interaction Relationship-aware Weakly Supervised Affordance Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ji Ha Jang",
      "Hoigi Seo",
      "Se Young Chun*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8067_ECCV_2024_paper.php": {
    "title": "DEPICT: Diffusion-Enabled Permutation Importance for Image Classification Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sarah Jabbour*",
      "Gregory Kondas",
      "Ella Kazerooni",
      "Michael Sjoding",
      "David Fouhey",
      "Jenna Wiens"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8071_ECCV_2024_paper.php": {
    "title": "Meerkat: Audio-Visual Large Language Model for Grounding in Space and Time",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanjoy Chowdhury*",
      "Sayan Nag",
      "Subhrajyoti Dasgupta",
      "Jun Chen",
      "Mohamed Elhoseiny",
      "Ruohan Gao",
      "Dinesh Manocha"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8072_ECCV_2024_paper.php": {
    "title": "Diagnosing and Re-learning for Balanced Multimodal Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yake Wei",
      "Siwei Li",
      "Ruoxuan Feng",
      "Di Hu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8074_ECCV_2024_paper.php": {
    "title": "Contribution-based Low-Rank Adaptation with Pre-training Model for Real Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongwon Park",
      "Hayeon Kim",
      "Se Young Chun*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8076_ECCV_2024_paper.php": {
    "title": "Elucidating the Hierarchical Nature of Behavior with Masked Autoencoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucas Stoffl",
      "Andy Bonnetto",
      "Stéphane D'Ascoli",
      "Alexander Mathis*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8077_ECCV_2024_paper.php": {
    "title": "BeyondScene: Higher-Resolution Human-Centric Scene Generation With Pretrained Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gwanghyun Kim",
      "Hayeon Kim",
      "Hoigi Seo",
      "Dong Un Kang",
      "Se Young Chun*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8084_ECCV_2024_paper.php": {
    "title": "SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Xu",
      "Ang Li",
      "Linghao Chen",
      "Yulin Liu",
      "Ruoxi Shi",
      "Hao Su*",
      "Minghua Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8085_ECCV_2024_paper.php": {
    "title": "MMEarth: Exploring Multi-Modal Pretext Tasks For Geospatial Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vishal Nedungadi",
      "Ankit Kariryaa",
      "Stefan Oehmcke",
      "Serge Belongie",
      "Christian Igel",
      "Nico Lang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8086_ECCV_2024_paper.php": {
    "title": "Discovering Unwritten Visual Classifiers with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mia Chiquier*",
      "Utkarsh Mall",
      "Carl Vondrick"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8090_ECCV_2024_paper.php": {
    "title": "LITA: Language Instructed Temporal-Localization Assistant",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "De-An Huang*",
      "Shijia Liao",
      "Subhashree Radhakrishnan",
      "Hongxu Yin",
      "Pavlo Molchanov",
      "Zhiding Yu",
      "Jan Kautz"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8092_ECCV_2024_paper.php": {
    "title": "MARs: Multi-view Attention Regularizations for Patch-based Feature Recognition of Space Terrain",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Timothy Chase Jr*",
      "Karthik Dantu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8095_ECCV_2024_paper.php": {
    "title": "Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keen You*",
      "Haotian Zhang",
      "Eldon Schoop",
      "Floris Weers",
      "Amanda Swearngin",
      "Jeff Nichols",
      "Yinfei Yang",
      "Zhe Gan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8099_ECCV_2024_paper.php": {
    "title": "Bridging the Pathology Domain Gap: Efficiently Adapting CLIP for Pathology Image Analysis with Limited Labeled Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengfeng Lai*",
      "Joohi Chauhan",
      "Brittany N. Dugger",
      "Chen-Nee Chuah"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8100_ECCV_2024_paper.php": {
    "title": "AugUndo: Scaling Up Augmentations for Monocular Depth Completion and Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yangchao Wu*",
      "Tian Yu Liu",
      "Hyoungseob Park",
      "Stefano Soatto",
      "Dong Lao",
      "Alex Wong"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8101_ECCV_2024_paper.php": {
    "title": "CARB-Net: Camera-Assisted Radar-Based Network for Vulnerable Road User Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei-Yu Lee*",
      "Martin Dimitrievski",
      "David Van Hamme",
      "Jan Aelterman",
      "Ljubomir Jovanov",
      "Wilfried Philips"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8105_ECCV_2024_paper.php": {
    "title": "SAH-SCI: Self-Supervised Adapter for Efficient Hyperspectral Snapshot Compressive Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haijin Zeng",
      "Yuxi Liu",
      "Yongyong Chen*",
      "Youfa Liu",
      "Chong Peng",
      "Jingyong Su"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8113_ECCV_2024_paper.php": {
    "title": "Minimalist Vision with Freeform Pixels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeremy Klotz*",
      "Shree Nayar"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8116_ECCV_2024_paper.php": {
    "title": "All You Need is Your Voice: Emotional Face Representation with Audio Perspective for Emotional Talking Face Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seongho Kim",
      "Byung Cheol Song*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8118_ECCV_2024_paper.php": {
    "title": "LatentEditor: Text Driven Local Editing of 3D Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Umar Khalid*",
      "Hasan Iqbal",
      "Muhammad Tayyab",
      "Md Nazmul Karim",
      "Jing Hua",
      "Chen Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8121_ECCV_2024_paper.php": {
    "title": "Single-Photon 3D Imaging with Equi-Depth Photon Histograms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaustubh Sadekar*",
      "David Maier",
      "Atul Ingle"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8133_ECCV_2024_paper.php": {
    "title": "Asynchronous Bioplausible Neuron for Spiking Neural Networks for Event-Based Vision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hussain Sajwani",
      "Dimitrios Makris",
      "Yahya Prof. Zweiri",
      "Fariborz Baghaei Naeini",
      "Sanket Mr Kachole*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8139_ECCV_2024_paper.php": {
    "title": "Viewpoint textual inversion: discovering scene representations and 3D view control in 2D diffusion models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "James Burgess*",
      "Kuan-Chieh Wang",
      "Serena Yeung-Levy"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8141_ECCV_2024_paper.php": {
    "title": "POET: Prompt Offset Tuning for Continual Human Action Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prachi Garg*",
      "Joseph K J",
      "Vineeth N Balasubramanian",
      "Necati Cihan Camgoz",
      "Chengde Wan",
      "Kenrick Kin",
      "Weiguang Si",
      "Shugao Ma",
      "Fernando de la Torre"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8142_ECCV_2024_paper.php": {
    "title": "Domain Generalization of 3D Object Detection by Density-Resampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuangzhi Li",
      "Lei Ma",
      "Xingyu Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8150_ECCV_2024_paper.php": {
    "title": "IG Captioner: Information Gain Captioners are Strong Zero-shot Classifiers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenglin Yang*",
      "Siyuan Qiao",
      "Yuan Cao",
      "Yu Zhang",
      "Tao Zhu",
      "Alan Yuille",
      "Jiahui Yu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8153_ECCV_2024_paper.php": {
    "title": "MRSP: Learn Multi-Representations of Single Primitive for Compositional Zero-Shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongyao Jiang",
      "Hui Chen",
      "Haodong Jing",
      "Yongqiang Ma",
      "Nanning Zheng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8161_ECCV_2024_paper.php": {
    "title": "Cross-Domain Semantic Segmentation on Inconsistent Taxonomy using VLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeongkee Lim",
      "Yusung Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8163_ECCV_2024_paper.php": {
    "title": "TrafficNight : An Aerial Multimodal Benchmark For Nighttime Vehicle Surveillance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guoxing Zhang",
      "Yiming Liu",
      "xiaoyu yang",
      "Chao Huang*",
      "HUANG Hailong"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8166_ECCV_2024_paper.php": {
    "title": "Loc3Diff: Local Diffusion for 3D Human Head Synthesis and Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yushi Lan*",
      "Feitong Tan",
      "Qiangeng Xu",
      "Di Qiu",
      "Kyle Genova",
      "Zeng Huang",
      "Rohit Pandey",
      "Sean Fanello",
      "Thomas Funkhouser",
      "Chen Change Loy",
      "Yinda Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8171_ECCV_2024_paper.php": {
    "title": "Towards Open Domain Text-Driven Synthesis of Multi-Person Motions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengyi Shan",
      "Lu Dong",
      "Yutao Han",
      "Yuan Yao",
      "Tao Liu",
      "Ifeoma Nwogu",
      "Guo-Jun Qi",
      "Mitchell K Hill*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8174_ECCV_2024_paper.php": {
    "title": "Generative End-to-End Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenzhao Zheng",
      "Ruiqi Song",
      "Xianda Guo*",
      "Chenming Zhang",
      "Long Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8177_ECCV_2024_paper.php": {
    "title": "Learning to Distinguish Samples for Generalized Category Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fengxiang Yang",
      "Nan Pu",
      "Wenjing Li",
      "Zhiming Luo*",
      "Shaozi Li",
      "Nicu Sebe",
      "Zhun Zhong*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8183_ECCV_2024_paper.php": {
    "title": "COM Kitchens: An Unedited Overhead-view Procedural Videos Dataset a Vision-Language Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Atsushi Hashimoto*",
      "Koki Maeda",
      "Tosho Hirasawa",
      "Jun Harashima",
      "Leszek Rybicki",
      "Yusuke Fukasawa",
      "Yoshitaka Ushiku"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8185_ECCV_2024_paper.php": {
    "title": "PILoRA: Prototype Guided Incremental LoRA for Federated Class-Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiyang Guo*",
      "Fei Zhu",
      "Wenzhuo Liu",
      "Xu-Yao Zhang*",
      "Cheng-Lin Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8193_ECCV_2024_paper.php": {
    "title": "Diff-Reg: Diffusion Model in Doubly Stochastic Matrix Space for Registration Problem",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qianliang Wu*",
      "Haobo Jiang*",
      "Lei Luo",
      "Jun Li",
      "Yaqing Ding*",
      "Jin Xie*",
      "Jian Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8195_ECCV_2024_paper.php": {
    "title": "WBP: Training-time Backdoor Attacks through Hardware-based Weight Bit Poisoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kunbei Cai*",
      "Zhenkai Zhang",
      "Qian Lou",
      "Fan Yao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8206_ECCV_2024_paper.php": {
    "title": "Towards Dual Transparent Liquid Level Estimation in Biomedical Lab: Dataset, Methods and Practice",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiayu Wang",
      "Ke Ma",
      "Ruiyun Zhong",
      "Xinggang Wang",
      "Yi Fang",
      "Yang Xiao",
      "Tian Xia*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8207_ECCV_2024_paper.php": {
    "title": "Encapsulating Knowledge in One Prompt",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Li*",
      "Runpeng Yu*",
      "Xinchao Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8210_ECCV_2024_paper.php": {
    "title": "Cross-Input Certified Training for Universal Perturbations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changming Xu*",
      "Gagandeep Singh"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8217_ECCV_2024_paper.php": {
    "title": "Visual Relationship Transformation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyu Xu*",
      "Jiayan Qiu",
      "Baosheng Yu",
      "Zhou Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8221_ECCV_2024_paper.php": {
    "title": "Not Just Change the Labels, Learn the Features: Watermarking Deep Neural Networks with Multi-View Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Li",
      "Sarthak Kumar Maharana",
      "Yunhui Guo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8224_ECCV_2024_paper.php": {
    "title": "Delving into Adversarial Robustness on Document Tampering Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huiru Shao",
      "Zhuang Qian",
      "Kaizhu Huang",
      "Wei Wang",
      "Xiaowei Huang",
      "Qiufeng Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8236_ECCV_2024_paper.php": {
    "title": "Adaptive Selection of Sampling-Reconstruction in Fourier Compressed Sensing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seongmin Hong",
      "Jaehyeok Bae",
      "Jongho Lee*",
      "Se Young Chun*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8254_ECCV_2024_paper.php": {
    "title": "Confidence-Based Iterative Generation for Real-World Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jialun Peng",
      "Xin Luo",
      "Jingjing Fu*",
      "Dong Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8257_ECCV_2024_paper.php": {
    "title": "Learning Scalable Model Soup on a Single GPU: An Efficient Subspace Training Strategy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Li*",
      "Weisen Jiang",
      "Fanghui Liu",
      "Xiaolin Huang",
      "James Kwok"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8264_ECCV_2024_paper.php": {
    "title": "Correspondences of the Third Kind: Camera Pose Estimation from Object Reflection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kohei Yamashita*",
      "Vincent Lepetit",
      "Ko Nishino"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8266_ECCV_2024_paper.php": {
    "title": "Seeing Faces in Things: A Model and Dataset for Pareidolia",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mark T Hamilton*",
      "Simon Stent",
      "Vasha G DuTell",
      "Anne Harrington",
      "Jennifer E Corbett",
      "Ruth Rosenholtz",
      "William T. Freeman"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8267_ECCV_2024_paper.php": {
    "title": "Cocktail Universal Adversarial Attack on Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaoxin Li*",
      "Xiaofeng Liao",
      "Xin Che",
      "Xintong Li",
      "Yong Zhang",
      "Lingyang Chu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8270_ECCV_2024_paper.php": {
    "title": "Gaussian Frosting: Editable Complex Radiance Fields with Real-Time Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antoine Guédon*",
      "Vincent Lepetit"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8272_ECCV_2024_paper.php": {
    "title": "AMD: Automatic Multi-step Distillation of Large-scale Vision Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Han",
      "Qifan Wang",
      "Sohail A Dianat",
      "Majid Rabbani",
      "Raghuveer Rao",
      "Yi Fang",
      "Qiang Guan",
      "Lifu Huang",
      "Dongfang Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8276_ECCV_2024_paper.php": {
    "title": "FairViT: Fair Vision Transformer via Adaptive Masking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowei Tian",
      "Ruijie Du",
      "Yanning Shen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8278_ECCV_2024_paper.php": {
    "title": "TrojVLM: Backdoor Attack Against Vision Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weimin Lyu*",
      "Lu Pang",
      "Tengfei Ma",
      "Haibin Ling",
      "Chao Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8280_ECCV_2024_paper.php": {
    "title": "VisionLLaMA: A Unified LLaMA Backbone for Vision Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangxiang Chu*",
      "Jianlin Su",
      "Bo Zhang*",
      "Chunhua Shen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8282_ECCV_2024_paper.php": {
    "title": "Frugal 3D Point Cloud Model Training via Progressive Near Point Filtering and Fused Aggregation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Donghyun Lee",
      "Yejin Lee",
      "Jae W. Lee*",
      "Hongil Yoon*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8284_ECCV_2024_paper.php": {
    "title": "HVCLIP: High-dimensional Vector in CLIP for Unsupervised Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Noranart Vesdapunt*",
      "Kah Kuen Fu",
      "Yue Wu",
      "Xu Zhang",
      "Pradeep Natarajan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8286_ECCV_2024_paper.php": {
    "title": "Improving 3D Semi-supervised Learning by Effectively Utilizing All Unlabelled Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sneha Paul*",
      "Zachary Patterson",
      "Nizar Bouguila"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8287_ECCV_2024_paper.php": {
    "title": "PRET: Planning with Directed Fidelity Trajectory for Vision and Language Navigation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Renjie Lu",
      "Jingke Meng*",
      "WEI-SHI ZHENG"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8288_ECCV_2024_paper.php": {
    "title": "MART: MultiscAle Relational Transformer Networks for Multi-agent Trajectory Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seongju Lee",
      "Junseok Lee",
      "Yeonguk Yu",
      "Taeri Kim",
      "Kyoobin Lee*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8290_ECCV_2024_paper.php": {
    "title": "Expanding Scene Graph Boundaries: Fully Open-vocabulary Scene Graph Generation via Visual-Concept Alignment and Retention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zuyao Chen",
      "Jinlin Wu",
      "Zhen Lei",
      "Zhaoxiang Zhang",
      "Chang Wen Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8292_ECCV_2024_paper.php": {
    "title": "Few-shot NeRF by Adaptive Rendering Loss Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingshan Xu*",
      "Xuanyu Yi",
      "Jianyao Xu",
      "Wenbing Tao",
      "Yew Soon Ong",
      "Hanwang Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8294_ECCV_2024_paper.php": {
    "title": "Investigating Style Similarity in Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gowthami Somepalli*",
      "Anubhav Gupta",
      "Kamal Gupta",
      "Shramay Palta",
      "Micah Goldblum",
      "Jonas A. Geiping",
      "Abhinav Shrivastava",
      "Tom Goldstein"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8296_ECCV_2024_paper.php": {
    "title": "JDT3D: Addressing the Gaps in LiDAR-Based Tracking-by-Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brian Cheong*",
      "Jiachen Zhou*",
      "Steven L Waslander*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8299_ECCV_2024_paper.php": {
    "title": "MagicMirror: Fast and High-Quality Avatar Generation with Constrained Search Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Armand Comas",
      "Di Qiu*",
      "Menglei Chai",
      "Marcel C. Bühler",
      "Amit Raj",
      "Ruiqi Gao",
      "Qiangeng Xu",
      "Mark J Matthews",
      "Paulo Gotardo",
      "Sergio Orts-Escolano",
      "Thabo Beeler"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8301_ECCV_2024_paper.php": {
    "title": "EntAugment: Entropy-Driven Adaptive Data Augmentation Framework for Image Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suorong Yang*",
      "Furao Shen*",
      "Jian Zhao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8312_ECCV_2024_paper.php": {
    "title": "Timestep-Aware Correction for Quantized Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuzhe Yao",
      "Feng Tian",
      "Jun Chen*",
      "Haonan Lin",
      "Guang Dai",
      "Yong Liu",
      "Jingdong Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8320_ECCV_2024_paper.php": {
    "title": "SPARO: Selective Attention for Robust and Compositional Transformer Encodings for Vision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ankit Vani*",
      "Bac Nguyen",
      "Samuel Lavoie",
      "Ranjay Krishna",
      "Aaron Courville"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8321_ECCV_2024_paper.php": {
    "title": "Towards compact reversible image representations for neural style transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiyao Liu",
      "Siyu Yang",
      "Jian Zhang*",
      "Gerald Schaefer",
      "Jiya Li",
      "Xunli FAN",
      "Songtao Wu",
      "Hui Fang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8331_ECCV_2024_paper.php": {
    "title": "Out-of-Bounding-Box Triggers: A Stealthy Approach to Cheat Object Detectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Lin*",
      "lijia Yu*",
      "Gaojie Jin*",
      "Renjue Li*",
      "Peng Wu*",
      "Lijun Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8341_ECCV_2024_paper.php": {
    "title": "GTMS: A Gradient-driven Tree-guided Mask-free Referring Image Segmentation Method",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoxin Lv",
      "Tianxiong Zhong",
      "Sanyuan Zhao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8346_ECCV_2024_paper.php": {
    "title": "Long-term Temporal Context Gathering for Neural Video Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linfeng Qi",
      "Zhaoyang Jia",
      "Jiahao Li",
      "Bin Li",
      "Houqiang Li",
      "Yan Lu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8347_ECCV_2024_paper.php": {
    "title": "VQA-Diff: Exploiting VQA and Diffusion for Zero-Shot Image-to-3D Vehicle Asset Generation in Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "YIBO LIU*",
      "Zheyuan Yang",
      "Guile Wu",
      "Yuan Ren",
      "Kejian Lin",
      "Liu Bingbing",
      "Yang Liu",
      "JINJUN SHAN"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8356_ECCV_2024_paper.php": {
    "title": "From Pixels to Objects: A Hierarchical Approach for Part and Object Segmentation Using Local and Global Aggregation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunfei Xie*",
      "Cihang Xie",
      "Alan Yuille",
      "Jieru Mei"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8360_ECCV_2024_paper.php": {
    "title": "Leveraging Text Localization for Scene Text Removal via Text-aware Masked Image Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixiao Wang*",
      "Hongtao Xie",
      "YuXin Wang",
      "Yadong Qu",
      "Fengjun Guo",
      "Pengwei Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8365_ECCV_2024_paper.php": {
    "title": "Unmasking Bias in Diffusion Model Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hu Yu",
      "Li Shen",
      "Jie Huang",
      "Hongsheng Li",
      "Feng Zhao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8369_ECCV_2024_paper.php": {
    "title": "Multimodal Label Relevance Ranking via Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taian Guo",
      "Taolin Zhang",
      "Haoqian Wu",
      "Hanjun Li",
      "Ruizhi Qiao*",
      "Xing Sun"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8371_ECCV_2024_paper.php": {
    "title": "Animate Your Motion: Turning Still Images into Dynamic Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingxiao Li*",
      "Bo Wan*",
      "Sien Moens",
      "Tinne Tuytelaars"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8372_ECCV_2024_paper.php": {
    "title": "Layered Rendering Diffusion Model for Controllable Zero-Shot Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zipeng Qi",
      "Guoxi Huang*",
      "Chenyang Liu",
      "Fei Ye"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8373_ECCV_2024_paper.php": {
    "title": "CIC-BART-SSA: : Controllable Image Captioning with Structured Semantic Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kalliopi Basioti*",
      "Mohamed A Abdelsalam*",
      "Federico Fancellu*",
      "Vladimir Pavlovic*",
      "Afsaneh Fazly*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8374_ECCV_2024_paper.php": {
    "title": "A Simple Background Augmentation Method for Object Detection with Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhang Li",
      "Xin Dong",
      "Chen Chen",
      "Weiming Zhuang",
      "Lingjuan Lyu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8380_ECCV_2024_paper.php": {
    "title": "Echoes of the Past: Boosting Long-tail Recognition via Reflective Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qihao Zhao",
      "Yalun Dai",
      "Shen Lin",
      "Wei Hu",
      "Fan Zhang*",
      "Jun Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8381_ECCV_2024_paper.php": {
    "title": "BlinkVision: A Benchmark for Optical Flow, Scene Flow and Point Tracking Estimation using RGB Frames and Events",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yijin Li",
      "Yichen Shen",
      "Zhaoyang Huang",
      "Shuo Chen",
      "Weikang Bian",
      "Xiaoyu Shi",
      "Fu-Yun Wang",
      "Keqiang Sun",
      "Hujun Bao",
      "Zhaopeng Cui",
      "Guofeng Zhang*",
      "Hongsheng Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8382_ECCV_2024_paper.php": {
    "title": "A Unified Anomaly Synthesis Strategy with Gradient Ascent for Industrial Anomaly Detection and Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiyu Chen",
      "Huiyuan Luo",
      "Chengkan Lv*",
      "Zhengtao Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8385_ECCV_2024_paper.php": {
    "title": "Deep Polarization Cues for Single-shot Shape and Subsurface Scattering Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenhao Li*",
      "Trung Thanh Ngo",
      "Hajime Nagahara"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8386_ECCV_2024_paper.php": {
    "title": "Rethinking Features-Fused-Pyramid-Neck for Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hulin Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8391_ECCV_2024_paper.php": {
    "title": "Spatial-Temporal Multi-level Association for Video Object Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Deshui Miao",
      "Xin Li",
      "Zhenyu He*",
      "Huchuan Lu",
      "Ming-Hsuan Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8392_ECCV_2024_paper.php": {
    "title": "Sparse Refinement for Efficient High-Resolution Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhijian Liu",
      "Zhuoyang Zhang",
      "Samir Khaki",
      "Shang Yang",
      "Haotian Tang",
      "Chenfeng Xu",
      "Kurt Keutzer",
      "Song Han*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8393_ECCV_2024_paper.php": {
    "title": "Safeguard Text-to-Image Diffusion Models with Human Feedback Inversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanghyun Kim*",
      "Seohyeon Jung",
      "Balhae Kim",
      "Moonseok Choi",
      "Jinwoo Shin",
      "Juho Lee*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8395_ECCV_2024_paper.php": {
    "title": "An Explainable Vision Question Answer Model via Diffusion Chain-of-Thought",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chunhao LU",
      "Qiang Lu*",
      "Jake Luo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8396_ECCV_2024_paper.php": {
    "title": "RaFE: Generative Radiance Fields Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongkai Wu",
      "Ziyu Wan",
      "Jing Zhang*",
      "Jing Liao",
      "Dong Xu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8399_ECCV_2024_paper.php": {
    "title": "UniProcessor: A Text-induced Unified Low-level Image Processor",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huiyu Duan*",
      "Xiongkuo Min",
      "Sijing Wu",
      "Wei Shen",
      "Guangtao Zhai"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8402_ECCV_2024_paper.php": {
    "title": "Fast Sprite Decomposition from Animated Graphics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tomoyuki Suzuki*",
      "Kotaro Kikuchi",
      "Kota Yamaguchi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8405_ECCV_2024_paper.php": {
    "title": "Learning Unified Reference Representation for Unsupervised Multi-class Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liren He",
      "Zhengkai Jiang",
      "Jinlong Peng",
      "Wenbing Zhu",
      "Liang Liu",
      "Qiangang Du",
      "Xiaobin Hu",
      "Mingmin Chi*",
      "Yabiao Wang*",
      "Chengjie Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8408_ECCV_2024_paper.php": {
    "title": "IRSAM: Advancing Segment Anything Model for Infrared Small Target Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingjin Zhang",
      "Yuchun Wang*",
      "Jie Guo*",
      "Yunsong Li",
      "Xinbo Gao",
      "Jing Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8418_ECCV_2024_paper.php": {
    "title": "PatchRefiner: Leveraging Synthetic Data for Real-Domain High-Resolution Monocular Metric Depth Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenyu Li*",
      "Shariq Farooq Bhat",
      "Peter Wonka"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8419_ECCV_2024_paper.php": {
    "title": "A Geometric Distortion Immunized Deep Watermarking Framework with Robustness Generalizability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linfeng Ma",
      "Han Fang*",
      "Tianyi Wei",
      "Zijin Yang",
      "Zehua Ma*",
      "Weiming Zhang",
      "Nenghai Yu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8432_ECCV_2024_paper.php": {
    "title": "Towards Robust Event-based Networks for Nighttime via Unpaired Day-to-Night Event Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhwan Jeong",
      "Hoonhee Cho",
      "Kuk-Jin Yoon*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8434_ECCV_2024_paper.php": {
    "title": "CLAMP-ViT: Contrastive Data-Free Learning for Adaptive Post-Training Quantization of ViTs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akshat Ramachandran*",
      "Souvik Kundu*",
      "Tushar Krishna*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8440_ECCV_2024_paper.php": {
    "title": "A Riemannian Approach for Spatiotemporal Analysis and Generation of 4D Tree-shaped Structures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tahmina Khanam",
      "Mohammed Bennamoun",
      "Guan Wang",
      "Guanjin Wang",
      "Ferdous Sohel",
      "Farid Boussaid",
      "Anuj Srivastava",
      "Hamid Laga*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8443_ECCV_2024_paper.php": {
    "title": "Dual-Path Adversarial Lifting for Domain Shift Correction in Online Test-time Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yushun Tang",
      "Shuoshuo Chen",
      "Zhihe Lu",
      "Xinchao Wang",
      "Zhihai He*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8448_ECCV_2024_paper.php": {
    "title": "Data Overfitting for On-Device Super-Resolution with Dynamic Algorithm and Compiler Co-Design",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gen Li*",
      "zhihao shu",
      "Jie Ji",
      "Minghai Qin",
      "Fatemeh Afghah",
      "Wei Niu",
      "Xiaolong Ma*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8449_ECCV_2024_paper.php": {
    "title": "The Role of Masking for Efficient Supervised Knowledge Distillation of Vision Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seungwoo Son*",
      "Jegwang Ryu",
      "Namhoon Lee",
      "Jaeho Lee*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8451_ECCV_2024_paper.php": {
    "title": "Training A Small Emotional Vision Language Model for Visual Art Comprehension",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Zhang",
      "Liang Zheng*",
      "Meng Wang",
      "Dan Guo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8453_ECCV_2024_paper.php": {
    "title": "UGG: Unified Generative Grasping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxin Lu",
      "Hao Kang",
      "Haoxiang Li",
      "Bo Liu",
      "Yiding Yang",
      "Qixing Huang",
      "Gang Hua*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8460_ECCV_2024_paper.php": {
    "title": "FrePolad: Frequency-Rectified Point Latent Diffusion for Point Cloud Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenliang Zhou*",
      "Fangcheng Zhong",
      "Param Hanji",
      "Zhilin Guo",
      "Kyle Thomas Fogarty",
      "Alejandro Sztrajman",
      "Hongyun Gao",
      "A. Cengiz Oztireli"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8462_ECCV_2024_paper.php": {
    "title": "Learning to Detect Multi-class Anomalies with Just One Normal Image Prompt",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bin-Bin Gao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8466_ECCV_2024_paper.php": {
    "title": "GAMMA-FACE: GAussian Mixture Models Amend Diffusion Models for Bias Mitigation in Face Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Basudha Pal*",
      "Arunkumar Kannan*",
      "Ram Prabhakar Kathirvel",
      "Alice O'Toole",
      "Rama Chellappa"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8467_ECCV_2024_paper.php": {
    "title": "Reinforcement Learning Friendly Vision-Language Model for Minecraft",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haobin Jiang",
      "Junpeng Yue",
      "Hao Luo",
      "Ziluo Ding",
      "Zongqing Lu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8471_ECCV_2024_paper.php": {
    "title": "Pseudo-RIS: Distinctive Pseudo-supervision Generation for Referring Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seonghoon Yu",
      "Paul Hongsuck Seo*",
      "Jeany Son*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8472_ECCV_2024_paper.php": {
    "title": "Training-free Composite Scene Generation for Layout-to-Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqi Liu*",
      "Tao Huang",
      "Chang Xu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8486_ECCV_2024_paper.php": {
    "title": "Robustness Preserving Fine-tuning using Neuron Importance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangrui Li",
      "Rahul Duggal*",
      "Aaditya Singh",
      "Kaustav Kundu",
      "Bing Shuai",
      "Jonathan Wu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8490_ECCV_2024_paper.php": {
    "title": "ProxyCLIP: Proxy Attention Improves CLIP for Open-Vocabulary Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengcheng Lan",
      "Chaofeng Chen",
      "Yiping Ke",
      "Xinjiang Wang",
      "Litong Feng",
      "Wayne Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8492_ECCV_2024_paper.php": {
    "title": "PEA-Diffusion: Parameter-Efficient Adapter with Knowledge Distillation in non-English Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "jian ma",
      "Chen Chen*",
      "Qingsong Xie",
      "Haonan Lu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8514_ECCV_2024_paper.php": {
    "title": "Similarity of Neural Architectures using Adversarial Attack Transferability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaehui Hwang",
      "Dongyoon Han",
      "Byeongho Heo",
      "Song Park",
      "Sanghyuk Chun*",
      "Jong-Seok Lee"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8521_ECCV_2024_paper.php": {
    "title": "Dual-Rain: Video Rain Removal using Assertive and Gentle Teachers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tingting Chen*",
      "Beibei Lin",
      "Yeying Jin",
      "Wending Yan",
      "WEI YE",
      "Yuan Yuan",
      "Robby T. Tan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8535_ECCV_2024_paper.php": {
    "title": "PMT: Progressive Mean Teacher via Exploring Temporal Consistency for Semi-Supervised Medical Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ning Gao",
      "Sanping Zhou*",
      "Le Wang",
      "Nanning Zheng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8538_ECCV_2024_paper.php": {
    "title": "OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raghav Kapoor*",
      "Yash Parag Butala*",
      "Melisa A Russak",
      "Jing Yu Koh",
      "Kiran Kamble",
      "Waseem AlShikh",
      "Ruslan Salakhutdinov"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8542_ECCV_2024_paper.php": {
    "title": "AutoEval-Video: An Automatic Benchmark for Assessing Large Vision Language Models in Open-Ended Video Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiuyuan Chen",
      "Yuan Lin*",
      "Yuchen Zhang*",
      "Weiran Huang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8550_ECCV_2024_paper.php": {
    "title": "Reflective Instruction Tuning: Mitigating Hallucinations in Large Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinrui Zhang",
      "Teng Wang",
      "Haigang Zhang",
      "Ping Lu",
      "Feng Zheng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8556_ECCV_2024_paper.php": {
    "title": "Unsupervised Variational Translator for Bridging Image Restoration and High-Level Vision Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawei Wu",
      "Zhi Jin*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8562_ECCV_2024_paper.php": {
    "title": "Diffusion Model for Robust Multi-Sensor Fusion in 3D Object Detection and BEV Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Duy Tho Le*",
      "Hengcan Shi*",
      "Jianfei Cai",
      "Hamid Rezatofighi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8563_ECCV_2024_paper.php": {
    "title": "MeshAvatar: Learning High-quality Triangular Human Avatars from Multi-view Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yushuo Chen*",
      "Zerong Zheng",
      "Zhe Li",
      "Chao Xu",
      "Yebin Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8568_ECCV_2024_paper.php": {
    "title": "Fast Point Cloud Geometry Compression with Context-based Residual Coding and INR-based Refinement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Xu*",
      "Xi Zhang",
      "Xiaolin Wu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8589_ECCV_2024_paper.php": {
    "title": "Scene-Conditional 3D Object Stylization and Composition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinghao Zhou*",
      "Tomas Jakab",
      "Philip Torr",
      "Christian Rupprecht"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8591_ECCV_2024_paper.php": {
    "title": "GenView: Enhancing View Quality with Pretrained Generative Model for Self-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaojie Li",
      "Yibo Yang*",
      "Xiangtai Li",
      "Jianlong Wu*",
      "Yue Yu",
      "Bernard Ghanem",
      "Min Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8592_ECCV_2024_paper.php": {
    "title": "Revisit Anything: Visual Place Recognition via Image Segment Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kartik Garg",
      "Sai Shubodh",
      "Shishir N Y Kolathaya",
      "Madhava Krishna",
      "Sourav Garg*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8613_ECCV_2024_paper.php": {
    "title": "EcoMatcher: Efficient Clustering Oriented Matcher for Detector-free Image Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peiqi Chen*",
      "Lei Yu",
      "Yi Wan*",
      "Yongjun Zhang*",
      "Jian Wang",
      "Liheng Zhong",
      "Jingdong Chen",
      "Ming Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8614_ECCV_2024_paper.php": {
    "title": "DGD: Dynamic 3D Gaussians Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Isaac Labe",
      "Noam Issachar",
      "Itai Lang",
      "Sagie Benaim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8616_ECCV_2024_paper.php": {
    "title": "Semantic Diversity-aware Prototype-based Learning for Unbiased Scene Graph Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaehyeong Jeon*",
      "Kibum Kim",
      "Kanghoon Yoon",
      "Chanyoung Park"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8619_ECCV_2024_paper.php": {
    "title": "DiffuMatting: Synthesizing Arbitrary Objects with Matting-level Annotation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaobin Hu",
      "Xu Peng",
      "Donghao Luo*",
      "Xiaozhong Ji",
      "Jinlong Peng",
      "ZhengKai Jiang",
      "Jiangning Zhang",
      "Taisong Jin*",
      "Chengjie Wang",
      "Rongrong Ji"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8641_ECCV_2024_paper.php": {
    "title": "Self-Guided Generation of Minority Samples Using Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soobin Um",
      "Jong Chul Ye*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8652_ECCV_2024_paper.php": {
    "title": "DEVIAS: Learning Disentangled Video Representations of Action and Scene",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kyungho Bae",
      "Youngrae Kim",
      "Geo Ahn",
      "Jinwoo Choi*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8661_ECCV_2024_paper.php": {
    "title": "AD3: Introducing a score for Anomaly Detection Dataset Difficulty assessment using VIADUCT dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jan D Lehr*",
      "Jan H Philipps",
      "Alik Sargsyan",
      "Martin Pape",
      "Jörg Krüger"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8662_ECCV_2024_paper.php": {
    "title": "RoomTex: Texturing Compositional Indoor Scenes via Iterative Inpainting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi WANG*",
      "Ruijie Lu",
      "Xudong XU",
      "Jingbo Wang",
      "Michael Yu Wang",
      "Bo Dai",
      "Gang Zeng",
      "Dan Xu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8663_ECCV_2024_paper.php": {
    "title": "Class-Agnostic Object Counting with Text-to-Image Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaofei Hui",
      "Qian Wu",
      "Hossein Rahmani",
      "Jun Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8664_ECCV_2024_paper.php": {
    "title": "Mask2Map: Vectorized HD Map Construction Using Bird's Eye View Segmentation Masks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sehwan Choi*",
      "Jun Won Choi",
      "Jungho Kim",
      "Hongjae Shin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8665_ECCV_2024_paper.php": {
    "title": "SUP-NeRF: A Streamlined Unification of Pose Estimation and NeRF for Monocular 3D Object Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuliang Guo*",
      "Abhinav Kumar",
      "Cheng Zhao",
      "Ruoyu Wang",
      "Xinyu Huang",
      "Liu Ren"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8668_ECCV_2024_paper.php": {
    "title": "Forbes: Face Obfuscation Rendering via Backpropagation Refinement Scheme",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jintae Kim",
      "Seungwon Yang",
      "Seong-Gyun Jeong",
      "Chang-Su Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8675_ECCV_2024_paper.php": {
    "title": "Pyramid Diffusion for Fine 3D Large Scene Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuheng Liu*",
      "Xinke Li",
      "Xueting Li",
      "Lu Qi*",
      "Chongshou Li",
      "Ming-Hsuan Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8688_ECCV_2024_paper.php": {
    "title": "ShoeModel: Learning to Wear on the User-specified Shoes via Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenyu Li*",
      "Binghui Chen",
      "Yifeng Geng",
      "Xuansong Xie",
      "Wangmeng Zuo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8694_ECCV_2024_paper.php": {
    "title": "A Watermark-Conditioned Diffusion Model for IP Protection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Min*",
      "Sen Li*",
      "Hongyang Chen*",
      "Minhao Cheng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8696_ECCV_2024_paper.php": {
    "title": "Finding NeMo: Negative-mined Mosaic Augmentation for Referring Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seongsu Ha",
      "Chaeyun Kim",
      "Donghwa Kim",
      "Junho Lee",
      "Sangho Lee",
      "Joonseok Lee*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8701_ECCV_2024_paper.php": {
    "title": "SAFT: Towards Out-of-Distribution Generalization in Fine-Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bac Nguyen*",
      "Stefan Uhlich",
      "Fabien Cardinaux",
      "Lukas Mauch",
      "Marzieh Edraki",
      "Aaron Courville"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8702_ECCV_2024_paper.php": {
    "title": "FTBC: Forward Temporal Bias Correction for Optimizing ANN-SNN Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaofeng Wu*",
      "Velibor Bojkovic",
      "Bin Gu*",
      "Kun Suo",
      "Kai Zou"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8703_ECCV_2024_paper.php": {
    "title": "Improving Vision and Language Concepts Understanding with Multimodal Counterfactual Samples",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengen Lai",
      "Shengli Song*",
      "Sitong Yan",
      "Guangneng Hu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8704_ECCV_2024_paper.php": {
    "title": "Centering the Value of Every Modality: Towards Efficient and Resilient Modality-agnostic Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xu Zheng*",
      "Yuanhuiyi Lyu",
      "jiazhou zhou",
      "Lin Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8711_ECCV_2024_paper.php": {
    "title": "GTPT: Group-based Token Pruning Transformer for Efficient Human Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haonan Wang",
      "Jie Liu*",
      "Jie Tang",
      "Gangshan Wu",
      "Bo Xu",
      "Yanbing Chou",
      "Yong Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8713_ECCV_2024_paper.php": {
    "title": "Lost in Translation: Modern Neural Networks Still Struggle With Small Realistic Image Transformations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ofir Shifman*",
      "Yair Weiss"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8737_ECCV_2024_paper.php": {
    "title": "DIAL: Dense Image-text ALignment for Weakly Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soojin Jang",
      "JungMin Yun",
      "JuneHyoung Kwon",
      "Eunju Lee",
      "YoungBin Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8753_ECCV_2024_paper.php": {
    "title": "Rethinking Normalization Layers for Domain Generalizable Person Re-identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ren Nie",
      "Jin Ding",
      "Xue Zhou*",
      "Xi Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8759_ECCV_2024_paper.php": {
    "title": "Generalizing to Unseen Domains via Text-guided Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daiqing Qi*",
      "Handong Zhao",
      "Aidong Zhang",
      "Sheng Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8761_ECCV_2024_paper.php": {
    "title": "VCP-CLIP: A visual context prompting model for zero-shot anomaly segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Qu",
      "Xian Tao*",
      "Mukesh Prasad",
      "Fei Shen",
      "Zhengtao Zhang",
      "Xinyi Gong",
      "Guiguang Ding"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8763_ECCV_2024_paper.php": {
    "title": "Lost in Translation: Latent Concept Misalignment in Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juntu Zhao",
      "Junyu Deng",
      "Yixin Ye",
      "Chongxuan Li",
      "Zhijie Deng*",
      "Dequan Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8766_ECCV_2024_paper.php": {
    "title": "Crowd-SAM:SAM as a smart annotator for object detection in crowded scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhi Cai",
      "Yingjie Gao",
      "Yaoyan Zheng",
      "Nan Zhou",
      "Di Huang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8774_ECCV_2024_paper.php": {
    "title": "Zero-shot Text-guided Infinite Image Synthesis with LLM guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soyeong Kwon",
      "Taegyeong Lee",
      "Taehwan Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8780_ECCV_2024_paper.php": {
    "title": "Learning Dual-Level Deformable Implicit Representation for Real-World Scale Arbitrary Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiheng Li",
      "Muheng Li",
      "Jixuan Fan",
      "Lei Chen*",
      "Yansong Tang",
      "Jiwen Lu",
      "Jie Zhou"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8783_ECCV_2024_paper.php": {
    "title": "Boosting Gaze Object Prediction via Pixel-level Supervision from Vision Foundation Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Jin",
      "Lei Zhang",
      "Shi Yan",
      "Bin Fan",
      "Binglu Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8795_ECCV_2024_paper.php": {
    "title": "Pro2SAM: Mask Prompt to SAM with Grid Points for Weakly Supervised Object Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xi Yang",
      "Songsong Duan*",
      "Nannan Wang",
      "Xinbo Gao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8798_ECCV_2024_paper.php": {
    "title": "Adaptive Multi-head Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Wang*",
      "Piotr Koniusz",
      "Tom Gedeon",
      "Liang Zheng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8803_ECCV_2024_paper.php": {
    "title": "Rotated Orthographic Projection for Self-Supervised 3D Human Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "YAO YAO",
      "Yixuan Pan",
      "Wenjun Shi",
      "Dongchen Zhu",
      "Lei Wang",
      "Jiamao Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8816_ECCV_2024_paper.php": {
    "title": "Easing 3D Pattern Reasoning with Side-view Features for Semantic Scene Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linxi Huan",
      "Mingyue Dong",
      "Linwei Yue",
      "Shuhan Shen",
      "Xianwei Zheng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8824_ECCV_2024_paper.php": {
    "title": "DSMix: Distortion-Induced Saliency Map Based Pre-training for No-Reference Image Quality Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinsong Shi",
      "Pan Gao*",
      "Xiaojiang Peng",
      "Jie Qin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8825_ECCV_2024_paper.php": {
    "title": "MO-EMT-NAS: Multi-Objective Continuous Transfer of Architectural Knowledge Between Tasks from Different Datasets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "PENG LIAO*",
      "Xilu Wang*",
      "Yaochu Jin*",
      "Wenli Du*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8828_ECCV_2024_paper.php": {
    "title": "Text-to-Sticker: Style Tailoring Latent Diffusion Models for Human Expression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Animesh Sinha*",
      "Bo Sun",
      "Anmol Kalia",
      "Arantxa Casanova",
      "Elliot Blanchard",
      "David Yan",
      "Winnie Zhang",
      "Tony Nelli",
      "Jiahui Chen",
      "Hardik Shah",
      "Licheng Yu",
      "Mitesh Kumar Singh",
      "Ankit Ramchandani",
      "Maziar Sanjabi",
      "Sonal Gupta",
      "Amy L Bearman",
      "Dhruv Mahajan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8834_ECCV_2024_paper.php": {
    "title": "Adaptive Annealing for Robust Averaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sidhartha Chitturi*",
      "Venu Madhav Govindu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8838_ECCV_2024_paper.php": {
    "title": "GRIDS: Grouped Multiple-Degradation Restoration with Image Degradation Similarity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuo Cao",
      "Yihao Liu",
      "Wenlong Zhang",
      "Yu Qiao",
      "Chao Dong*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8845_ECCV_2024_paper.php": {
    "title": "MaxMI: A Maximal Mutual Information Criterion for Manipulation Concept Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pei Zhou",
      "Yanchao Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8847_ECCV_2024_paper.php": {
    "title": "High-Quality Mesh Blendshape Generation from Face Videos via Neural Inverse Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Ming",
      "Jiawei Li",
      "Jingwang Ling",
      "Libo Zhang",
      "Feng Xu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8861_ECCV_2024_paper.php": {
    "title": "Disentangling Masked Autoencoders for Unsupervised Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "An Zhang*",
      "Han Wang",
      "Xiang Wang",
      "Tat-Seng Chua"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8862_ECCV_2024_paper.php": {
    "title": "Early Anticipation of Driving Maneuvers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abdul Wasi Lone",
      "Shankar Gangisetty*",
      "Shyam Nandan Rai",
      "C. V. Jawahar"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8866_ECCV_2024_paper.php": {
    "title": "Bottom-Up Domain Prompt Tuning for Generalized Face Anti-Spoofing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siqi Liu*",
      "Qirui Wang",
      "Pong C. Yuen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8870_ECCV_2024_paper.php": {
    "title": "SG-NeRF: Neural Surface Reconstruction with Scene Graph Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiyang Chen",
      "Siyan Dong*",
      "Xulong Wang",
      "Lulu Cai",
      "Youyi Zheng",
      "Yanchao Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8871_ECCV_2024_paper.php": {
    "title": "On the Evaluation Consistency of Attribution-based Explanations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiarui Duan",
      "Haoling Li",
      "Haofei Zhang",
      "Hao Jiang",
      "Mengqi Xue",
      "Li Sun",
      "Mingli Song",
      "Jie Song*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8881_ECCV_2024_paper.php": {
    "title": "Unified Embedding Alignment for Open-Vocabulary Video Instance Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Fang",
      "Peng Wu",
      "Yawei Li",
      "Xinxin Zhang",
      "Xiankai Lu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8882_ECCV_2024_paper.php": {
    "title": "InfoNorm: Mutual Information Shaping of Normals for Sparse-View Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xulong Wang",
      "Siyan Dong*",
      "Youyi Zheng",
      "Yanchao Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8897_ECCV_2024_paper.php": {
    "title": "DreamReward: Aligning Human Preference in Text-to-3D Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junliang Ye",
      "Fangfu Liu",
      "Qixiu Li",
      "Zhengyi Wang",
      "Yikai Wang",
      "Xinzhou Wang",
      "Yueqi Duan*",
      "Jun Zhu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8903_ECCV_2024_paper.php": {
    "title": "Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changan Chen*",
      "Puyuan Peng",
      "Ami Baid",
      "Zihui Xue",
      "Wei-Ning Hsu",
      "David Harwath",
      "Kristen Grauman"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8905_ECCV_2024_paper.php": {
    "title": "Frontier-enhanced Topological Memory with Improved Exploration Awareness for Embodied Visual Navigation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinru Cui",
      "Qiming Liu",
      "Zhe Liu",
      "Hesheng Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8907_ECCV_2024_paper.php": {
    "title": "MTMamba: Enhancing Multi-Task Dense Scene Understanding by Mamba-Based Decoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baijiong Lin*",
      "Weisen Jiang",
      "Pengguang Chen",
      "Yu Zhang",
      "Shu Liu",
      "Yingcong Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8910_ECCV_2024_paper.php": {
    "title": "VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shicheng Li",
      "Lei Li",
      "Yi Liu",
      "Shuhuai Ren",
      "Yuanxin Liu",
      "Rundong Gao",
      "Xu Sun*",
      "Lu Hou"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8911_ECCV_2024_paper.php": {
    "title": "Learning a Dynamic Privacy-preserving Camera Robust to Inversion Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiacheng Cheng*",
      "Xiang Dai",
      "Jia Wan",
      "Nick Antipa",
      "Nuno Vasconcelos"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8913_ECCV_2024_paper.php": {
    "title": "CadVLM: Bridging Language and Vision in the Generation of Parametric CAD Sketches",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sifan Wu*",
      "Amir Hosein Khasahmadi",
      "Mor Katz",
      "Pradeep Kumar Jayaraman",
      "Yewen Pu",
      "Karl D.D. Willis",
      "Bang Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8915_ECCV_2024_paper.php": {
    "title": "Towards Image Ambient Lighting Normalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Florin-Alexandru Vasluianu*",
      "Tim Seizinger",
      "Zongwei WU*",
      "Rakesh Ranjan",
      "Radu Timofte"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8916_ECCV_2024_paper.php": {
    "title": "FedHide: Federated Learning by Hiding in the Neighbors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyunsin Park*",
      "Sungrack Yun"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8921_ECCV_2024_paper.php": {
    "title": "Toward INT4 Fixed-Point Training via Exploring Quantization Error for Gradients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dohyung Kim",
      "Junghyup Lee",
      "Jeimin Jeon",
      "JAEHYEON MOON",
      "Bumsub Ham*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8926_ECCV_2024_paper.php": {
    "title": "SelEx: Self-Expertise in Fine-Grained Generalized Category Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sarah Rastegar*",
      "Mohammadreza Salehi",
      "Yuki M Asano",
      "Hazel Doughty",
      "Cees Snoek"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8927_ECCV_2024_paper.php": {
    "title": "Self-Cooperation Knowledge Distillation for Novel Class Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuzheng Wang*",
      "Zhaoyu Chen",
      "Dingkang Yang",
      "Yunquan Sun",
      "Lizhe Qi*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8931_ECCV_2024_paper.php": {
    "title": "EventBind: Learning a Unified Representation to Bind Them All for Event-based Open-world Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "jiazhou zhou*",
      "Xu Zheng",
      "Yuanhuiyi Lyu",
      "Lin Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8940_ECCV_2024_paper.php": {
    "title": "GLAD: Towards Better Reconstruction with Global and Local Adaptive Diffusion Models for Unsupervised Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hang Yao",
      "Ming Liu*",
      "Zhicun Yin",
      "Zifei Yan",
      "Xiaopeng Hong",
      "Wangmeng Zuo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8947_ECCV_2024_paper.php": {
    "title": "MedRAT: Unpaired Medical Report Generation via Auxiliary Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elad Hirsch*",
      "Gefen Dawidowicz",
      "Ayellet Tal"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8953_ECCV_2024_paper.php": {
    "title": "Are Synthetic Data Useful for Egocentric Hand-Object Interaction Detection?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rosario Leonardi*",
      "Antonino Furnari",
      "Francesco Ragusa",
      "Giovanni Maria Farinella"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8959_ECCV_2024_paper.php": {
    "title": "PoseEmbroider: Towards a 3D, Visual, Semantic-aware Human Pose Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ginger Delmas*",
      "Philippe Weinzaepfel",
      "Francesc Moreno-Noguer",
      "Gregory Rogez"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8961_ECCV_2024_paper.php": {
    "title": "A Comparative Study of Image Restoration Networks for General Backbone Network Design",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangyu Chen*",
      "Zheyuan Li",
      "Yuandong Pu",
      "Yihao Liu",
      "Jiantao Zhou*",
      "Yu Qiao",
      "Chao Dong*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8963_ECCV_2024_paper.php": {
    "title": "Learned Image Enhancement via Color Naming",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Serrano-Lozano*",
      "Luis Herranz",
      "Michael S Brown",
      "Javier Vazquez-Corral"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8964_ECCV_2024_paper.php": {
    "title": "Synthesizing Time-varying BRDFs via Latent Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Takuto Narumoto*",
      "Hiroaki Santo",
      "Fumio Okura"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8965_ECCV_2024_paper.php": {
    "title": "HoloADMM: High-Quality Holographic Complex Field Recovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mazen Mel*",
      "Paul Springer",
      "Pietro Zanuttigh",
      "Haitao Zhou",
      "Alexander Gatto"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8966_ECCV_2024_paper.php": {
    "title": "Fundamental Matrix Estimation Using Relative Depths",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaqing Ding*",
      "Václav Vávra",
      "Snehal Bhayani",
      "Qianliang Wu",
      "Jian Yang",
      "Zuzana Kukelova"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8967_ECCV_2024_paper.php": {
    "title": "Gaussian Splatting on the Move: Blur and Rolling Shutter Compensation for Natural Camera Motion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Otto Seiskari*",
      "Jerry Ylilammi",
      "Valtteri Kaatrasalo",
      "Pekka Rantalankila",
      "Matias Turkulainen",
      "Juho Kannala",
      "Esa Rahtu",
      "Arno Solin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8968_ECCV_2024_paper.php": {
    "title": "MTaDCS: Moving Trace and Feature Density-based Confidence Sample Selection under Label Noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingzheng Huang",
      "Xilin He",
      "Xiaole Xian",
      "Qinliang Lin",
      "Weicheng Xie*",
      "Siyang Song",
      "Linlin Shen",
      "Zitong Yu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8972_ECCV_2024_paper.php": {
    "title": "Towards Open-World Object-based Anomaly Detection via Self-Supervised Outlier Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brian Kostadinov Shalon Isaac-Medina*",
      "Yona Falinie Abdul Gaus*",
      "Neelanjan Bhowmik",
      "Toby P Breckon"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8974_ECCV_2024_paper.php": {
    "title": "GroundUp: Rapid Sketch-Based 3D City Massing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gizem Esra Unlu*",
      "Mohamed Sayed",
      "Yulia Gryaditskaya",
      "Gabriel Brostow"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8987_ECCV_2024_paper.php": {
    "title": "Guide-and-Rescale: Self-Guidance Mechanism for Effective Tuning-Free Real Image Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vadim Titov*",
      "Madina Khalmatova*",
      "Alexandra Ivanova*",
      "Dmitry P Vetrov",
      "Aibek Alanov*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8991_ECCV_2024_paper.php": {
    "title": "DataDream: Few-shot Guided Dataset Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jae Myung Kim*",
      "Jessica Bader",
      "Stephan Alaniz",
      "Cordelia Schmid",
      "Zeynep Akata"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8994_ECCV_2024_paper.php": {
    "title": "LPViT: Low-Power Semi-structured Pruning for Vision Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaixin Xu*",
      "Zhe Wang*",
      "Chunyun Chen",
      "Xue Geng",
      "Jie Lin",
      "Xulei Yang",
      "Min Wu*",
      "Xiaoli Li",
      "Weisi Lin*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8996_ECCV_2024_paper.php": {
    "title": "CipherDM: Secure Three-Party Inference for Diffusion Model Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Zhao",
      "Xiaojun Chen*",
      "Xudong Chen",
      "He Li",
      "Tingyu Fan",
      "Zhendong Zhao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9002_ECCV_2024_paper.php": {
    "title": "Weighted Ensemble Models Are Strong Continual Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Imad Eddine MAROUF*",
      "Subhankar Roy",
      "Enzo Tartaglione",
      "Stéphane Lathuilière"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9006_ECCV_2024_paper.php": {
    "title": "GGRt: Towards Generalizable 3D Gaussians without Pose Priors in Real-Time",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Li",
      "Yuanyuan Gao",
      "Dingwen Zhang*",
      "Chenming Wu",
      "YALUN DAI",
      "Chen Zhao",
      "Haocheng Feng",
      "Errui Ding",
      "Jingdong Wang",
      "Junwei Han"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9009_ECCV_2024_paper.php": {
    "title": "A Unified Image Compression Method for Human Perception and Multiple Vision Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sha Guo",
      "Lin Sui",
      "Chen-Lin Zhang",
      "Zhuo Chen",
      "Wenhan Yang",
      "Lingyu Duan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9015_ECCV_2024_paper.php": {
    "title": "UniVoxel: Fast Inverse Rendering by Unified Voxelization of Scene Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuang Wu",
      "Songlin Tang",
      "Guangming Lu",
      "Jianzhuang Liu",
      "Wenjie Pei*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9025_ECCV_2024_paper.php": {
    "title": "Audio-visual Generalized Zero-shot Learning the Easy Way",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shentong Mo*",
      "Pedro Morgado"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9027_ECCV_2024_paper.php": {
    "title": "PartImageNet++ Dataset: Scaling up Part-based Models for Robust Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Li*",
      "Yining Liu",
      "Na Dong",
      "Sitian Qin",
      "Xiaolin Hu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9030_ECCV_2024_paper.php": {
    "title": "Learning Equilibrium Transformation for Gamut Expansion and Color Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Xiao*",
      "Changjian Shui",
      "Zhi-Song Liu",
      "Qian Ye",
      "Kin-Man Lam"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9032_ECCV_2024_paper.php": {
    "title": "Dyn-Adapter: Towards Disentangled Representation for Efficient Visual Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yurong Zhang*",
      "Honghao Chen",
      "Zhang Xinyu",
      "Xiangxiang Chu",
      "Li Song"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9034_ECCV_2024_paper.php": {
    "title": "Physics-informed Knowledge Transfer for Underwater Monocular Depth Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinghe Yang*",
      "Mingming Gong",
      "Ye Pu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9049_ECCV_2024_paper.php": {
    "title": "Robust Nearest Neighbors for Source-Free Domain Adaptation under Class Distribution Shift",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antonio Tejero-de-Pablos*",
      "Riku Togashi",
      "Mayu Otani",
      "Shin'ichi Satoh"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9056_ECCV_2024_paper.php": {
    "title": "Chains of Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanheng Wei*",
      "Lianghua Huang*",
      "Zhi-Fan Wu",
      "Wei Wang",
      "Yu Liu",
      "Mingda Jia",
      "Shuailei Ma"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9070_ECCV_2024_paper.php": {
    "title": "Time-Efficient and Identity-Consistent Virtual Try-On Using A Variant of Altered Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Phuong Hoang Dam*",
      "Jihoon Jeong*",
      "Anh T Tran*",
      "Daeyoung Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9073_ECCV_2024_paper.php": {
    "title": "Feature Diversification and Adaptation for Federated Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seunghan Yang*",
      "Seokeon Choi",
      "Hyunsin Park",
      "Sungha Choi",
      "Simyung Chang",
      "Sungrack Yun"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9080_ECCV_2024_paper.php": {
    "title": "Grounding Image Matching in 3D with MASt3R",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vincent Leroy*",
      "Yohann Cabon",
      "Jerome Revaud"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9082_ECCV_2024_paper.php": {
    "title": "TP2O: Creative Text Pair-to-Object Generation using Balance Swap-Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Li*",
      "Zedong Zhang",
      "Jian Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9088_ECCV_2024_paper.php": {
    "title": "RoDUS: Robust Decomposition of Static and Dynamic Elements in Urban Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thang-Anh-Quan Nguyen*",
      "Luis G Roldao Jimenez*",
      "Nathan Piasco*",
      "Moussab Bennehar*",
      "Dzmitry Tsishkou*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9090_ECCV_2024_paper.php": {
    "title": "RecurrentBEV: A Long-term Temporal Fusion Framework for Multi-view 3D Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ming Chang",
      "Xishan Zhang*",
      "Rui Zhang",
      "Zhipeng Zhao",
      "Guanhua He",
      "Shaoli Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9097_ECCV_2024_paper.php": {
    "title": "Efficient Bias Mitigation Without Privileged Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mateo Espinosa Zarlenga*",
      "Swami Sankaranarayanan",
      "Jerone T. A. Andrews",
      "Zohreh Shams",
      "Mateja Jamnik",
      "Alice Xiang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9103_ECCV_2024_paper.php": {
    "title": "MC-PanDA: Mask Confidence for Panoptic Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ivan Martinović*",
      "Josip Šarić",
      "Siniša Šegvić"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9107_ECCV_2024_paper.php": {
    "title": "Learning Neural Deformation Representation for 4D Dynamic Shape Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gyojin Han*",
      "Jiwan Hur",
      "Jaehyun Choi",
      "Junmo Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9110_ECCV_2024_paper.php": {
    "title": "Dynamic Guidance Adversarial Distillation with Enhanced Teacher Knowledge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyejin Park",
      "Dongbo Min*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9112_ECCV_2024_paper.php": {
    "title": "Decomposition Betters Tracking Everything Everywhere",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Li",
      "Dong Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9113_ECCV_2024_paper.php": {
    "title": "Straightforward Layer-wise Pruning for More Efficient Visual Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruizi Han*",
      "Jinglei Tang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9116_ECCV_2024_paper.php": {
    "title": "Synchronization is All You Need: Exocentric-to-Egocentric Transfer for Temporal Action Segmentation with Unlabeled Synchronized Video Pairs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Camillo Quattrocchi*",
      "Antonino Furnari",
      "Daniele Di Mauro",
      "Mario Valerio Giuffrida",
      "Giovanni Maria Farinella"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9117_ECCV_2024_paper.php": {
    "title": "LAPT: Label-driven Automated Prompt Tuning for OOD Detection with Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yabin Zhang*",
      "Wenjie Zhu",
      "Chenhang He",
      "Lei Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9119_ECCV_2024_paper.php": {
    "title": "Domain Shifting: A Generalized Solution for Heterogeneous Cross-Modality Person Re-Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Jiang",
      "Xu Cheng*",
      "Hao Yu",
      "Xingyu Liu",
      "Haoyu Chen",
      "Guoying Zhao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9123_ECCV_2024_paper.php": {
    "title": "Self-Supervised Video Desmoking for Laparoscopic Surgery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Renlong Wu",
      "Zhilu Zhang*",
      "Shuohao Zhang",
      "Longfei Gou",
      "Haobin Chen",
      "Lei Zhang",
      "Hao Chen*",
      "Wangmeng Zuo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9133_ECCV_2024_paper.php": {
    "title": "Removing Rows and Columns of Tokens in Vision Transformer enables Faster Dense Prediction without Retraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Diwei Su",
      "cheng fei",
      "Jianxu Luo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9140_ECCV_2024_paper.php": {
    "title": "Continuity Preserving Online CenterLine Graph Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunhui Han",
      "Kun Yu",
      "Zhiwei Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9144_ECCV_2024_paper.php": {
    "title": "Decomposition of Neural Discrete Representations for Large-Scale 3D Mapping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minseong Park",
      "Suhan Woo",
      "Euntai Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9159_ECCV_2024_paper.php": {
    "title": "MirrorGaussian: Reflecting 3D Gaussians for Reconstructing Mirror Reflections",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayue Liu",
      "Xiao Tang",
      "Freeman Cheng",
      "Zihao Yang",
      "Zhihao Li*",
      "Jianzhuang Liu",
      "Yi Huang",
      "Jiaqi Lin",
      "Shiyong Liu",
      "Xiaofei Wu",
      "Songcen Xu",
      "Chun Yuan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9160_ECCV_2024_paper.php": {
    "title": "Leveraging Representations from Intermediate Encoder-blocks for Synthetic Image Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christos Koutlis*",
      "Symeon Papadopoulos"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9164_ECCV_2024_paper.php": {
    "title": "Exploring Vulnerabilities in Spiking Neural Networks: Direct Adversarial Attacks on Raw Event Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanmeng Yao",
      "Xiaohan Zhao",
      "Bin Gu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9167_ECCV_2024_paper.php": {
    "title": "HSR: Holistic 3D Human-Scene Reconstruction from Monocular Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lixin Xue*",
      "Chen Guo",
      "Chengwei Zheng",
      "Fangjinhua Wang",
      "Tianjian Jiang",
      "Hsuan-I Ho",
      "Manuel Kaufmann",
      "Jie Song",
      "Otmar Hilliges"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9175_ECCV_2024_paper.php": {
    "title": "Online Video Quality Enhancement with Spatial-Temporal Look-up Tables",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zefan Qu",
      "Xinyang Jiang*",
      "Yifan Yang",
      "Dongsheng Li",
      "Cairong Zhao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9180_ECCV_2024_paper.php": {
    "title": "PARIS3D: Reasoning-based 3D Part Segmentation Using Large Multimodal Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amrin Kareem*",
      "Jean Lahoud",
      "Hisham Cholakkal*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9184_ECCV_2024_paper.php": {
    "title": "Self-Rectifying Diffusion Sampling with Perturbed-Attention Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Donghoon Ahn",
      "Hyoungwon Cho",
      "Jaewon Min",
      "Jungwoo Kim",
      "Wooseok Jang",
      "SeonHwa Kim",
      "Hyun Hee Park",
      "Kyong Hwan Jin*",
      "Seungryong Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9199_ECCV_2024_paper.php": {
    "title": "Localization and Expansion: A Decoupled Framework for Point Cloud Few-shot Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoyang Li*",
      "Yuan Wang",
      "Wangkai Li",
      "Rui Sun",
      "Tianzhu Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9204_ECCV_2024_paper.php": {
    "title": "Think before Placement: Common Sense Enhanced Transformer for Object Placement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaxuan Qin",
      "Jiayu Xu",
      "Ruiping Wang*",
      "Xilin Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9215_ECCV_2024_paper.php": {
    "title": "Oulu Remote-photoplethysmography Physical Domain Attacks Database (ORPDAD)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marko Savic",
      "Guoying Zhao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9216_ECCV_2024_paper.php": {
    "title": "Leveraging Imperfect Restoration for Data Availability Attack",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "YI HUANG*",
      "Jeremy Styborski*",
      "Mingzhi Lyu*",
      "Fan Wang*",
      "Wai-Kin Adams Kong*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9223_ECCV_2024_paper.php": {
    "title": "3D Weakly Supervised Semantic Segmentation with 2D Vision-Language Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoxu Xu",
      "Yitian Yuan",
      "Jinlong Li",
      "Qiudan Zhang",
      "Zequn Jie",
      "Lin Ma",
      "Hao Tang",
      "Nicu Sebe",
      "Xu Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9228_ECCV_2024_paper.php": {
    "title": "Open-set Domain Adaptation via Joint Error based Multi-class Positive and Unlabeled Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dexuan Zhang*",
      "Thomas Westfechtel",
      "Tatsuya Harada"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9231_ECCV_2024_paper.php": {
    "title": "DoubleTake: Geometry Guided Depth Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohamed Sayed*",
      "Filippo Aleotti",
      "Jamie Watson",
      "Zawar Qureshi",
      "Guillermo Garcia-Hernando",
      "Gabriel Brostow",
      "Sara Vicente",
      "Michael Firman"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9241_ECCV_2024_paper.php": {
    "title": "Empowering Embodied Visual Tracking with Visual Foundation Models and Offline RL",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangwei Zhong*",
      "Kui Wu",
      "Hai Ci",
      "Chu-ran Wang",
      "Hao Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9243_ECCV_2024_paper.php": {
    "title": "Street Gaussians: Modeling Dynamic Urban Scenes with Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunzhi Yan*",
      "Haotong Lin",
      "Chenxu Zhou",
      "Weijie Wang",
      "Haiyang Sun",
      "Kun Zhan",
      "Xianpeng Lang",
      "Xiaowei Zhou",
      "Sida Peng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9265_ECCV_2024_paper.php": {
    "title": "Images are Achilles' Heel of Alignment: Exploiting Visual Vulnerabilities for Jailbreaking Multimodal Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Li*",
      "hangyu guo",
      "Kun Zhou",
      "Wayne Xin Zhao",
      "Ji-Rong Wen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9268_ECCV_2024_paper.php": {
    "title": "Edge-Guided Fusion and Motion Augmentation for Event-Image Stereo",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fengan Zhao*",
      "Qianang Zhou",
      "Junlin Xiong*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9270_ECCV_2024_paper.php": {
    "title": "MetaWeather: Few-Shot Weather-Degraded Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youngrae Kim*",
      "Younggeol Cho",
      "Thanh-Tung Nguyen",
      "Seunghoon Hong",
      "Dongman Lee*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9272_ECCV_2024_paper.php": {
    "title": "CPT-VR: Improving Surface Rendering via Closest Point Transform with View-Reflection Appearance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhipeng Hu",
      "Yongqiang Zhang*",
      "Chen Liu",
      "Lincheng Li*",
      "Sida Peng",
      "Xiaowei Zhou",
      "Changjie Fan",
      "Xin Yu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9275_ECCV_2024_paper.php": {
    "title": "Close, But Not There: Boosting Geographic Distance Sensitivity in Visual Place Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sergio Izquierdo*",
      "Javier Civera*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9283_ECCV_2024_paper.php": {
    "title": "HiFi-123: Towards High-fidelity One Image to 3D Content Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wangbo Yu*",
      "Li Yuan",
      "Yan-Pei Cao",
      "Xiangjun Gao",
      "Xiaoyu Li",
      "Wenbo Hu",
      "Long Quan",
      "Ying Shan",
      "Yonghong Tian"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9285_ECCV_2024_paper.php": {
    "title": "Revisiting Adaptive Cellular Recognition Under Domain Shifts: A Contextual Correspondence View",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianan Fan*",
      "Dongnan Liu",
      "Canran Li",
      "Hang Chang",
      "Heng Huang",
      "Filip Braet",
      "Mei Chen",
      "Weidong Cai*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9286_ECCV_2024_paper.php": {
    "title": "Good Teachers Explain: Explanation-Enhanced Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amin Parchami-Araghi*",
      "Moritz Böhle",
      "Sukrut Rao",
      "Bernt Schiele"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9290_ECCV_2024_paper.php": {
    "title": "Stepping Stones: A Progressive Training Strategy for Audio-Visual Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juncheng Ma",
      "Peiwen Sun",
      "Yaoting Wang",
      "Di Hu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9294_ECCV_2024_paper.php": {
    "title": "FRDiff : Feature Reuse for Universal Training-free Acceleration of Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junhyuk So",
      "Jungwon Lee",
      "Eunhyeok Park*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9297_ECCV_2024_paper.php": {
    "title": "Möbius Transform for Mitigating Perspective Distortions in Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prakash Chandra Chhipa*",
      "Meenakshi Subhash Chippa",
      "Kanjar De",
      "Rajkumar Saini",
      "Marcus Liwicki",
      "Mubarak Shah"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9304_ECCV_2024_paper.php": {
    "title": "TAG: Text Prompt Augmentation for Zero-Shot Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xixi Liu*",
      "Christopher Zach"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9336_ECCV_2024_paper.php": {
    "title": "CVT-Occ: Cost Volume Temporal Fusion for 3D Occupancy Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhangchen Ye",
      "Tao Jiang",
      "Chenfeng Xu",
      "Yiming Li",
      "Hang Zhao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9344_ECCV_2024_paper.php": {
    "title": "SPVLoc: Semantic Panoramic Viewport Matching for 6D Camera Localization in Unseen Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Niklas Gard*",
      "Anna Hilsmann",
      "Peter Eisert"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9349_ECCV_2024_paper.php": {
    "title": "Continual Learning and Unknown Object Discovery in 3D Scenes via Self-Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohamed El Amine Boudjoghra*",
      "Jean Lahoud",
      "Salman Khan",
      "Hisham Cholakkal",
      "Rao M Anwer",
      "Fahad Shahbaz Khan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9350_ECCV_2024_paper.php": {
    "title": "DiffCD: A Symmetric Differentiable Chamfer Distance for Neural Implicit Surface Fitting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linus Härenstam-Nielsen*",
      "Lu Sang",
      "Abhishek Saroha",
      "Nikita Araslanov*",
      "Daniel Cremers*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9351_ECCV_2024_paper.php": {
    "title": "Lost and Found: Overcoming Detector Failures in Online Multi-Object Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lorenzo Vaquero*",
      "Yihong Xu",
      "Xavier Alameda-Pineda",
      "Victor M. Brea",
      "Manuel Mucientes"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9354_ECCV_2024_paper.php": {
    "title": "Local Occupancy-Enhanced Object Grasping with Multiple Triplanar Projection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kangqi Ma*",
      "Hao Dong",
      "Yadong Mu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9359_ECCV_2024_paper.php": {
    "title": "Region-Native Visual Tokenization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengyu Wang*",
      "Yuyao Huang",
      "Henghui Ding",
      "Xinlong Wang",
      "Tiejun Huang",
      "Yao Zhao",
      "Yunchao Wei",
      "Shuicheng Yan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9361_ECCV_2024_paper.php": {
    "title": "SparseCraft: Few-Shot Neural Reconstruction through Stereopsis Guided Geometric Linearization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mae Younes*",
      "Amine Ouasfi",
      "Adnane Boukhayma"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9384_ECCV_2024_paper.php": {
    "title": "Sketch2Vox: Learning 3D Reconstruction from a Single Monocular Sketch Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fei Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9386_ECCV_2024_paper.php": {
    "title": "DGE: Direct Gaussian 3D Editing by Consistent Multi-view Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minghao Chen*",
      "Iro Laina",
      "Andrea Vedaldi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9390_ECCV_2024_paper.php": {
    "title": "The Lottery Ticket Hypothesis in Denoising: Towards Semantic-Driven Initialization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiafeng Mao*",
      "Xueting Wang",
      "Kiyoharu Aizawa"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9395_ECCV_2024_paper.php": {
    "title": "Diffusion for Out-of-Distribution Detection on Road Scenes and Beyond",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Silvio Galesso*",
      "Philipp Schröppel*",
      "Hssan Driss",
      "Thomas Brox"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9398_ECCV_2024_paper.php": {
    "title": "Rethinking Directional Parameterization in Neural Implicit Surface Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijie Jiang*",
      "Tianhan Xu*",
      "Hiroharu Kato"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9400_ECCV_2024_paper.php": {
    "title": "A Comprehensive Study of Multimodal Large Language Models for Image Quality Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianhe Wu",
      "Kede Ma*",
      "Jie Liang",
      "Yujiu Yang*",
      "Lei Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9402_ECCV_2024_paper.php": {
    "title": "Semi-Supervised Teacher-Reference-Student Architecture for Action Quality Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wulian Yun",
      "Mengshi Qi",
      "Fei Peng",
      "Huadong Ma*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9437_ECCV_2024_paper.php": {
    "title": "Efficient Neural Video Representation with Temporally Coherent Modulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seungjun Shin*",
      "Suji Kim*",
      "Dokwan Oh"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9443_ECCV_2024_paper.php": {
    "title": "Ref-AVS: Refer and Segment Objects in Audio-Visual Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaoting Wang",
      "Peiwen Sun",
      "Dongzhan Zhou",
      "Guangyao Li",
      "Honggang Zhang",
      "Di Hu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9444_ECCV_2024_paper.php": {
    "title": "DreamScene: 3D Gaussian-based Text-to-3D Scene Generation via Formation Pattern Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoran Li",
      "Haolin Shi",
      "Wenli Zhang",
      "Wenjun Wu",
      "Yong Liao*",
      "Lin Wang",
      "Lik-Hang Lee",
      "Peng Yuan Zhou*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9445_ECCV_2024_paper.php": {
    "title": "Multi-modal Crowd Counting via a Broker Modality",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoliang Meng",
      "Xiaopeng Hong*",
      "Chenhao Wang",
      "Miao Shang",
      "Wangmeng Zuo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9447_ECCV_2024_paper.php": {
    "title": "FastPCI: Motion-Structure Guided Fast Point Cloud Frame Interpolation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "tianyu zhang",
      "Guocheng Qian",
      "Jin Xie*",
      "Jian Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9454_ECCV_2024_paper.php": {
    "title": "Made to Order: Discovering monotonic temporal changes via self-supervised video ordering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Charig Yang*",
      "Weidi Xie",
      "Andrew Zisserman"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9456_ECCV_2024_paper.php": {
    "title": "PARE-Net: Position-Aware Rotation-Equivariant Networks for Robust Point Cloud Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runzhao Yao",
      "Shaoyi Du*",
      "Wenting Cui",
      "Canhui Tang",
      "Chengwu Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9460_ECCV_2024_paper.php": {
    "title": "Open-Vocabulary RGB-Thermal Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "GuoQiang Zhao",
      "JunJie Huang",
      "Xiaoyun Yan*",
      "Zhaojing Wang",
      "Junwei Tang",
      "Yangjun Ou",
      "Xinrong Hu",
      "Tao Peng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9464_ECCV_2024_paper.php": {
    "title": "MeshVPR: Citywide Visual Place Recognition Using 3D Meshes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gabriele Berton*",
      "Lorenz Junglas",
      "Riccardo Zaccone",
      "Thomas Pollok",
      "Barbara Caputo",
      "Carlo Masone"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9465_ECCV_2024_paper.php": {
    "title": "Can Textual Semantics Mitigate Sounding Object Segmentation Preference?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaoting Wang",
      "Peiwen Sun",
      "Yuanchao Li",
      "Honggang Zhang",
      "Di Hu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9469_ECCV_2024_paper.php": {
    "title": "Concise Plane Arrangements for Low-Poly Surface and Volume Modelling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raphael Sulzer",
      "Florent Lafarge*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9481_ECCV_2024_paper.php": {
    "title": "KeypointDETR: An End-to-End 3D Keypoint Detector",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hairong Jin",
      "Yuefan Shen",
      "Jianwen Lou",
      "Kun Zhou",
      "Youyi Zheng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9492_ECCV_2024_paper.php": {
    "title": "ViPer: Visual Personalization of Generative Models via Individual Preference Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sogand Salehi*",
      "Mahdi Shafiei",
      "Roman Bachmann",
      "Teresa Yeo",
      "Amir Zamir"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9503_ECCV_2024_paper.php": {
    "title": "MLPHand: Real Time Multi-View 3D Hand Reconstruction via MLP Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Yang",
      "Jiakun Li",
      "Guoming Li",
      "Huaiyu Wu",
      "Zhen Shen",
      "Zhaoxin Fan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9508_ECCV_2024_paper.php": {
    "title": "uCAP: An Unsupervised Prompting Method for Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "A. Tuan Nguyen*",
      "Kai Sheng Tai",
      "Bor-Chun Chen",
      "Satya Narayan Shukla",
      "Hanchao Yu",
      "Philip Torr",
      "Tai-Peng Tian",
      "Ser-Nam Lim"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9511_ECCV_2024_paper.php": {
    "title": "LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dilxat Muhtar",
      "Zhenshi Li",
      "Feng Gu",
      "Xueliang Zhang*",
      "Pengfeng Xiao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9512_ECCV_2024_paper.php": {
    "title": "How Far Can a 1-Pixel Camera Go? Solving Vision Tasks using Photoreceptors and Computationally Designed Visual Morphology",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrei Atanov*",
      "Rishubh Singh",
      "Jiawei Fu",
      "Isabella Yu",
      "Andrew Spielberg",
      "Amir Zamir"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9513_ECCV_2024_paper.php": {
    "title": "MONTAGE: Monitoring Training for Attribution of Generative Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonathan Brokman*",
      "Omer Hofman",
      "Roman Vainshtein",
      "Amit Giloni",
      "Toshiya Shimizu",
      "Inderjeet Singh",
      "Oren Rachmil",
      "Alon Zolfi",
      "Asaf Shabtai",
      "Yuki Unno",
      "Hisashi Kojima"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9516_ECCV_2024_paper.php": {
    "title": "Affective Visual Dialog: A Large-Scale Benchmark for Emotional Reasoning Based on Visually Grounded Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kilichbek Haydarov*",
      "Xiaoqian Shen",
      "Avinash Madasu",
      "Mahmoud Salem",
      "Li-Jia Li",
      "Gamaleldin F Elsayed",
      "Mohamed Elhoseiny"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9518_ECCV_2024_paper.php": {
    "title": "Watching it in Dark: A Target-aware Representation Learning Framework for High-Level Vision Tasks in Low Illumination",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunan Li*",
      "Yihao Zhang",
      "Shoude Li",
      "Long Tian",
      "DOU QUAN",
      "Chaoneng Li",
      "Qiguang Miao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9526_ECCV_2024_paper.php": {
    "title": "Self-supervised visual learning from interactions with objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arthur Aubret*",
      "Céline Teulière",
      "Jochen Triesch"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9531_ECCV_2024_paper.php": {
    "title": "OP-Align: Object-level and Part-level Alignment for Self-supervised Category-level Articulated Object Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchen Che*",
      "Ryo Furukawa",
      "Asako Kanezaki"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9553_ECCV_2024_paper.php": {
    "title": "BAFFLE: A Baseline of Backpropagation-Free Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haozhe Feng*",
      "Tianyu Pang*",
      "Chao Du",
      "Wei Chen*",
      "Shuicheng Yan",
      "Min Lin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9557_ECCV_2024_paper.php": {
    "title": "Sequential Representation Learning via Static-Dynamic Conditional Disentanglement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mathieu Cyrille Simon*",
      "Pascal Frossard",
      "Christophe De Vleeschouwer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9562_ECCV_2024_paper.php": {
    "title": "OmniNOCS: A unified NOCS dataset and model for 3D lifting of 2D objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akshay Krishnan*",
      "Abhijit Kundu*",
      "Kevis-Kokitsi Maninis",
      "James Hays",
      "Matthew Brown"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9564_ECCV_2024_paper.php": {
    "title": "3R-INN: How to be climate friendly while consuming/delivering videos?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "ZOUBIDA AMEUR*",
      "Claire-Helene Demarty",
      "Olivier LE MEUR",
      "Daniel Menard"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9565_ECCV_2024_paper.php": {
    "title": "Rethinking Deep Unrolled Model for Accelerated MRI Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bingyu Xin*",
      "Meng Ye",
      "Leon Axel",
      "Dimitris N. Metaxas"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9567_ECCV_2024_paper.php": {
    "title": "Towards Robust Full Low-bit Quantization of Super Resolution Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Denis S. Makhov*",
      "Irina Zhelavskaya",
      "Ruslan Ostapets",
      "Dehua Song",
      "Kirill Solodskikh"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9574_ECCV_2024_paper.php": {
    "title": "Omni6DPose: A Benchmark and Model for Universal 6D Object Pose Estimation and Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiyao Zhang",
      "Weiyao Huang",
      "Bo Peng",
      "Mingdong Wu",
      "Fei Hu",
      "Zijian Chen",
      "Bo Zhao",
      "Hao Dong*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9575_ECCV_2024_paper.php": {
    "title": "Diverse Text-to-3D Synthesis with Augmented Text Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Uy Dieu Tran*",
      "Minh N. Hoang Luu*",
      "Phong Ha Nguyen*",
      "Khoi Nguyen*",
      "Binh-Son Hua*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9583_ECCV_2024_paper.php": {
    "title": "Style-Extracting Diffusion Models for Semi-Supervised Histopathology Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mathias Öttl*",
      "Frauke Wilm",
      "Jana Steenpass",
      "Jingna Qiu",
      "Matthias Rübner",
      "Prof Arndt Hartmann",
      "Matthias W. Beckmann",
      "Peter Fasching",
      "Andreas K Maier",
      "Ramona Erber",
      "Bernhard Kainz",
      "Katharina Breininger"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9585_ECCV_2024_paper.php": {
    "title": "LLMCO4MR: LLMs-aided Neural Combinatorial Optimization for Ancient Manuscript Restoration from Fragments with Case Studies on Dunhuang",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuqing Zhang",
      "Hangqi Li",
      "Shengyu Zhang*",
      "Runzhong Wang",
      "Baoyi He",
      "Huaiyong Dou",
      "Junchi Yan*",
      "Yongquan Zhang",
      "Fei Wu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9596_ECCV_2024_paper.php": {
    "title": "Model Breadcrumbs: Scaling Multi-Task Model Merging with Sparse Masks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "MohammadReza Davari*",
      "Eugene Belilovsky"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9603_ECCV_2024_paper.php": {
    "title": "AdversariaLeak: External Information Leakage Attack Using Adversarial Samples on Face Recognition Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roye Katzav*",
      "Amit Giloni",
      "Edita Grolman*",
      "Hiroo Saito",
      "Tomoyuki Shibata",
      "Tsukasa Omino",
      "Misaki Komatsu",
      "Yoshikazu Hanatani",
      "Yuval Elovici",
      "Asaf Shabtai"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9607_ECCV_2024_paper.php": {
    "title": "iHuman: Instant Animatable Digital Humans From Monocular Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pramish Paudel*",
      "Anubhav Khanal",
      "Danda Pani Paudel",
      "Jyoti Tandukar",
      "Ajad Chhatkuli"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9612_ECCV_2024_paper.php": {
    "title": "SphereHead: Stable 3D Full-head Synthesis with Spherical Tri-plane Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heyuan Li*",
      "Ce Chen",
      "Tianhao Shi",
      "Yuda Qiu",
      "Sizhe An",
      "Guanying CHEN",
      "Xiaoguang Han*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9617_ECCV_2024_paper.php": {
    "title": "Beyond Pixels: Semi-Supervised Semantic Segmentation with a Multi-scale Patch-based Multi-Label Classifier",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prantik Howlader*",
      "Srijan Das",
      "Hieu Le",
      "Dimitris Samaras"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9622_ECCV_2024_paper.php": {
    "title": "Glyph-ByT5: A Customized Text Encoder for Accurate Visual Text Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyu Liu",
      "Weicong Liang",
      "Zhanhao Liang",
      "Chong Luo",
      "Ji Li",
      "Gao Huang",
      "Yuhui Yuan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9626_ECCV_2024_paper.php": {
    "title": "Solving the inverse problem of microscopy deconvolution with a residual Beylkin-Coifman-Rokhlin neural network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Li",
      "Mikhail Kudryashev",
      "Artur Yakimovich*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9627_ECCV_2024_paper.php": {
    "title": "Face Reconstruction Transfer Attack as Out-of-Distribution Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yoon Gyo Jung*",
      "Jaewoo Park",
      "Xingbo Dong",
      "Hojin Park",
      "Andrew Beng Jin Teoh",
      "Octavia Camps*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9634_ECCV_2024_paper.php": {
    "title": "FreeZe: Training-free zero-shot 6D pose estimation with geometric and vision foundation models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrea Caraffa*",
      "Davide Boscaini",
      "Amir Hamza",
      "Fabio Poiesi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9637_ECCV_2024_paper.php": {
    "title": "Deep Diffusion Image Prior for Efficient OOD Adaptation in 3D Inverse Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyungjin Chung",
      "Jong Chul Ye*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9643_ECCV_2024_paper.php": {
    "title": "Weighting Pseudo-Labels via High-Activation Feature Index Similarity and Object Detection for Semi-Supervised Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prantik Howlader*",
      "Hieu Le",
      "Dimitris Samaras"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9666_ECCV_2024_paper.php": {
    "title": "PartGLEE: A Foundation Model for Recognizing and Parsing Any Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyi Li",
      "Junfeng Wu",
      "Weizhi Zhao",
      "Song Bai",
      "Xiang Bai*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9667_ECCV_2024_paper.php": {
    "title": "WTS: A Pedestrian-Centric Traffic Video Dataset for Fine-grained Spatial-Temporal Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quan Kong*",
      "Yuki Kawana",
      "Rajat Saini",
      "Ashutosh Kumar",
      "Jingjing Pan",
      "Ta Gu",
      "Yohei Ozao",
      "Balazs Opra",
      "Yoichi Sato",
      "Norimasa Kobori"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9680_ECCV_2024_paper.php": {
    "title": "Spiking Wavelet Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuetong Fang",
      "Ziqing Wang",
      "Lingfeng Zhang",
      "Jiahang Cao",
      "Honglei Chen",
      "Renjing Xu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9682_ECCV_2024_paper.php": {
    "title": "WAVE: Warping DDIM Inversion Features for Zero-shot Text-to-Video Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yutang Feng",
      "Sicheng Gao*",
      "Yuxiang Bao",
      "Xiaodi Wang",
      "Shumin Han*",
      "Juan Zhang*",
      "Baochang Zhang",
      "Angela Yao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9702_ECCV_2024_paper.php": {
    "title": "PDT Uav Target Detection Dataset for Pests and Diseases Tree",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingle Zhou",
      "Rui Xing",
      "Delong Han",
      "Zhiyong Qi",
      "Gang Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9712_ECCV_2024_paper.php": {
    "title": "Hypernetworks for Generalizable BRDF Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fazilet Gokbudak*",
      "Alejandro Sztrajman",
      "Chenliang Zhou",
      "Fangcheng Zhong",
      "Rafal Mantiuk",
      "A. Cengiz Oztireli"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9716_ECCV_2024_paper.php": {
    "title": "Photon Inhibition for Energy-Efficient Single-Photon Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucas J Koerner*",
      "Shantanu Gupta",
      "Atul N Ingle",
      "Mohit Gupta"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9729_ECCV_2024_paper.php": {
    "title": "COD: Learning Conditional Invariant Representation for Domain Adaptation Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao-Ran Yang",
      "Chuan-Xian Ren*",
      "You-Wei Luo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9733_ECCV_2024_paper.php": {
    "title": "RANRAC: Robust Neural Scene Representations via Random Ray Consensus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benno Buschmann*",
      "Andreea Dogaru",
      "Elmar Eisemann",
      "Michael Weinmann",
      "Bernhard Egger"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9737_ECCV_2024_paper.php": {
    "title": "LayerDiff: Exploring Text-guided Multi-layered Composable Image Synthesis via Layer-Collaborative Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runhui Huang",
      "Kaixin Cai",
      "Jianhua Han",
      "Xiaodan Liang*",
      "Renjing Pei",
      "Guansong Lu",
      "Songcen Xu",
      "Wei Zhang",
      "Hang Xu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9738_ECCV_2024_paper.php": {
    "title": "Characterizing Model Robustness via Natural Input Gradients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adrian Rodriguez-Munoz*",
      "Tongzhou Wang",
      "Antonio Torralba"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9748_ECCV_2024_paper.php": {
    "title": "UpFusion: Novel View Diffusion from Unposed Sparse View Observations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bharath Raj Nagoor Kani*",
      "Hsin-Ying Lee",
      "Sergey Tulyakov",
      "Shubham Tulsiani"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9749_ECCV_2024_paper.php": {
    "title": "Four Ways to Improve Verbo-visual Fusion for Dense 3D Visual Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ozan Unal*",
      "Christos Sakaridis",
      "Suman Saha",
      "Luc Van Gool"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9762_ECCV_2024_paper.php": {
    "title": "SIMBA: Split Inference - Mechanisms, Benchmarks and Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhishek Singh*",
      "Vivek Sharma",
      "Rohan Sukumaran",
      "John J Mose",
      "Jeffrey K Chiu",
      "Justin Yu",
      "Ramesh Raskar"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9769_ECCV_2024_paper.php": {
    "title": "Tuning-Free Image Customization with Image and Text Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengzhi Li",
      "Qiang Nie",
      "Ying Chen",
      "Xi Jiang",
      "Kai Wu",
      "Yuhuan Lin",
      "Yong Liu",
      "Jinlong Peng",
      "Chengjie Wang",
      "Feng Zheng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9771_ECCV_2024_paper.php": {
    "title": "FairDomain: Achieving Fairness in Cross-Domain Medical Image Segmentation and Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Tian*",
      "Congcong Wen",
      "Min Shi",
      "Muhammad Muneeb Afzal",
      "Hao Huang",
      "Muhammad Osama Khan",
      "Yan Luo",
      "Yi Fang",
      "Mengyu Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9774_ECCV_2024_paper.php": {
    "title": "Emerging Property of Masked Token for Effective Pre-training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyesong Choi",
      "Hunsang Lee",
      "Seyoung Joung",
      "Hyejin Park",
      "Jiyeong Kim",
      "Dongbo Min*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9775_ECCV_2024_paper.php": {
    "title": "DQ-DETR: DETR with Dynamic Query for Tiny Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi-Xin Huang*",
      "Hou-I Liu",
      "Hong-Han Shuai",
      "Wen-Huang Cheng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9800_ECCV_2024_paper.php": {
    "title": "Track2Act: Predicting Point Tracks from Internet Videos enables Generalizable Robot Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Homanga Bharadhwaj*",
      "Roozbeh Mottaghi",
      "Abhinav Gupta",
      "Shubham Tulsiani"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9801_ECCV_2024_paper.php": {
    "title": "SWAG: Splatting in the Wild images with Appearance-conditioned Gaussians",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hiba Dahmani*",
      "Moussab Bennehar",
      "Nathan Piasco",
      "Luis G Roldao Jimenez",
      "Dzmitry Tsishkou"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9802_ECCV_2024_paper.php": {
    "title": "Gaussian in the wild: 3D Gaussian Splatting for Unconstrained Image Collections",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongbin Zhang*",
      "Chuming Wang",
      "Weitao Wang",
      "Peihao Li",
      "Minghan Qin",
      "Haoqian Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9803_ECCV_2024_paper.php": {
    "title": "Few-shot Defect Image Generation based on Consistency Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingfeng Shi",
      "Jing Wei",
      "Fei Shen*",
      "Zhengtao Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9809_ECCV_2024_paper.php": {
    "title": "Taming CLIP for Fine-grained and Structured Visual Understanding of Museum Exhibits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ada-Astrid Balauca*",
      "Danda Pani Paudel",
      "Kristina Toutanova",
      "Luc Van Gool"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9810_ECCV_2024_paper.php": {
    "title": "CLIP-DPO: Vision-Language Models as a Source of Preference for Fixing Hallucinations in LVLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yassine Ouali*",
      "Adrian Bulat*",
      "Brais Martinez",
      "Georgios Tzimiropoulos"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9814_ECCV_2024_paper.php": {
    "title": "Masked Motion Prediction with Semantic Contrast for Point Cloud Sequence Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "yuehui han*",
      "Can Xu",
      "Rui Xu",
      "Jianjun Qian",
      "Jin Xie"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9815_ECCV_2024_paper.php": {
    "title": "Prompt-Based Test-Time Real Image Dehazing: A Novel Pipeline",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixuan Chen",
      "Zewei He*",
      "Ziqian Lu",
      "Xuecheng Sun",
      "Zheming Lu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9816_ECCV_2024_paper.php": {
    "title": "Video Editing via Factorized Diffusion Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Uriel Singer*",
      "Amit Zohar*",
      "Yuval Kirstain",
      "Shelly Sheynin",
      "Adam Polyak",
      "Devi Parikh",
      "Yaniv Taigman"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9819_ECCV_2024_paper.php": {
    "title": "Trackastra: Transformer-based cell tracking for live-cell microscopy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Gallusser",
      "Martin Weigert*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9827_ECCV_2024_paper.php": {
    "title": "CogView3: Finer and Faster Text-to-Image Generation via Relay Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wendi Zheng*",
      "Jiayan Teng",
      "Zhuoyi Yang",
      "Weihan Wang",
      "Jidong Chen",
      "Xiaotao Gu",
      "Yuxiao Dong*",
      "Ming Ding*",
      "Jie Tang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9828_ECCV_2024_paper.php": {
    "title": "SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nanye Ma*",
      "Mark Goldstein",
      "Michael Albergo",
      "Nicholas M Boffi",
      "Eric Vanden-Eijnden*",
      "Saining Xie*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9837_ECCV_2024_paper.php": {
    "title": "Learn to Memorize and to Forget: A Continual Learning Perspective of Dynamic SLAM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baicheng Li*",
      "Zike Yan*",
      "Dong Wu",
      "Hanqing Jiang",
      "Hongbin Zha*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9842_ECCV_2024_paper.php": {
    "title": "Forecasting Future Videos from Novel Views via Disentangled 3D Scene Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sudhir Yarram*",
      "Junsong Yuan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9846_ECCV_2024_paper.php": {
    "title": "GMM-IKRS: Gaussian Mixture Models for Interpretable Keypoint Refinement and Scoring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emanuele Santellani*",
      "Martin Zach",
      "Christian Sormann",
      "Mattia Rossi",
      "Andreas Kuhn",
      "Friedrich Fraundorfer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9856_ECCV_2024_paper.php": {
    "title": "Get Your Embedding Space in Order: Domain-Adaptive Regression for Forest Monitoring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sizhuo Li",
      "Dimitri Gominski*",
      "Martin Brandt",
      "Xiaoye Tong",
      "Philippe Ciais"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9857_ECCV_2024_paper.php": {
    "title": "ObjectDrop: Bootstrapping Counterfactuals for Photorealistic Object Removal and Insertion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Winter*",
      "Matan Cohen",
      "Shlomi Fruchter",
      "Yael Pritch",
      "Alex Rav-Acha",
      "Yedid Hoshen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9859_ECCV_2024_paper.php": {
    "title": "CoDA: Instructive Chain-of-Domain Adaptation with Severity-Aware Visual Prompt Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "ZiYang Gong",
      "FuHao Li",
      "Yupeng Deng",
      "Deblina Bhattacharjee",
      "Xianzheng Ma*",
      "Xiangwei Zhu*",
      "Zhenming Ji*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9862_ECCV_2024_paper.php": {
    "title": "Curved Diffusion: A Generative Model With Optical Geometry Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrey Voynov*",
      "Amir Hertz",
      "Moab Arar",
      "Shlomi Fruchter",
      "Daniel Cohen-Or"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9866_ECCV_2024_paper.php": {
    "title": "Mini-Splatting: Representing Scenes with a Constrained Number of Gaussians",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangchi Fang",
      "Bing Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9881_ECCV_2024_paper.php": {
    "title": "MeshSegmenter: Zero-Shot Mesh Segmentation via Texture Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziming Zhong*",
      "Yanyu Xu",
      "Jing Li",
      "Jiale Xu",
      "Zhengxin Li",
      "Chaohui Yu",
      "Shenghua Gao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9900_ECCV_2024_paper.php": {
    "title": "OTSeg: Multi-prompt Sinkhorn Attention for Zero-Shot Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kwanyoung Kim",
      "Yujin Oh",
      "Jong Chul Ye*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9904_ECCV_2024_paper.php": {
    "title": "Skeleton Recall Loss for Connectivity Conserving and Resource Efficient Segmentation of Thin Tubular Structures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yannick Kirchhoff*",
      "Maximilian R Rokuss*",
      "Saikat Roy*",
      "Balint Kovacs",
      "Constantin Ulrich",
      "Tassilo Wald",
      "Maximilian Zenk",
      "Philipp Vollmuth",
      "Jens Kleesiek",
      "Fabian Isensee",
      "Klaus H. Maier-Hein"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9905_ECCV_2024_paper.php": {
    "title": "Conceptual Codebook Learning for Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Zhang*",
      "Ke Yu",
      "Siqi Wu",
      "Zhihai He*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9911_ECCV_2024_paper.php": {
    "title": "LingoQA: Video Question Answering for Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ana-Maria Marcu*",
      "Long Chen",
      "Jan Hünermann",
      "Alice Karnsund",
      "Benoit Hanotte",
      "Prajwal Chidananda",
      "Saurabh Nair",
      "Vijay Badrinarayanan",
      "Alex Kendall",
      "Jamie Shotton",
      "Elahe Arani",
      "Oleg Sinavski"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9918_ECCV_2024_paper.php": {
    "title": "AnimateMe: 4D Facial Expressions via Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dimitrios Gerogiannis*",
      "Foivos Paraperas Papantoniou",
      "Rolandos Alexandros Potamias",
      "Alexandros Lattas",
      "Stylianos Moschoglou",
      "Stylianos Ploumpis",
      "Stefanos Zafeiriou"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9919_ECCV_2024_paper.php": {
    "title": "HaloQuest: A Visual Hallucination Dataset for Advancing Multimodal Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhecan Wang",
      "Garrett Bingham*",
      "Adams Wei Yu",
      "Quoc V. Le",
      "Thang Luong",
      "Golnaz Ghiasi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9925_ECCV_2024_paper.php": {
    "title": "LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kevin Xie*",
      "Tianshi Cao",
      "Jonathan P Lorraine",
      "Jun Gao",
      "James R Lucas",
      "Antonio Torralba",
      "Sanja Fidler",
      "Xiaohui Zeng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9936_ECCV_2024_paper.php": {
    "title": "PreSight: Enhancing Autonomous Vehicle Perception with City-Scale NeRF Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyuan Yuan*",
      "Yucheng Mao",
      "Jiawei Yang",
      "Yicheng LIU",
      "Yue Wang",
      "Hang Zhao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9937_ECCV_2024_paper.php": {
    "title": "Unveiling and Mitigating Memorization in Text-to-image Diffusion Models through Cross Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Ren*",
      "Yaxin Li",
      "Shenglai Zeng",
      "Han Xu",
      "Lingjuan Lyu",
      "Yue Xing",
      "Jiliang Tang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9941_ECCV_2024_paper.php": {
    "title": "iNeMo: Incremental Neural Mesh Models for Robust Class-Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tom Fischer*",
      "Yaoyao Liu",
      "Artur Jesslen",
      "Noor Ahmed",
      "Prakhar Kaushik",
      "Angtian Wang",
      "Alan Yuille",
      "Adam Kortylewski",
      "Eddy Ilg"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9942_ECCV_2024_paper.php": {
    "title": "Context Diffusion: In-Context Aware Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ivona Najdenkoska*",
      "Animesh Sinha",
      "Abhimanyu Dubey",
      "Dhruv Mahajan",
      "Vignesh Ramanathan",
      "Filip Radenovic"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9947_ECCV_2024_paper.php": {
    "title": "Pose Guided Fine-Grained Sign Language Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tongkai Shi",
      "Lianyu Hu",
      "Fanhua Shang",
      "Jichao Feng",
      "liu peidong",
      "Wei Feng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9950_ECCV_2024_paper.php": {
    "title": "RAP: Retrieval-Augmented Planner for Adaptive Procedure Planning in Instructional Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali Zare*",
      "Yulei Niu",
      "Hammad Ayyubi",
      "Shih-Fu Chang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9971_ECCV_2024_paper.php": {
    "title": "Certifiably Robust Image Watermark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengyuan Jiang*",
      "Moyang Guo",
      "Yuepeng Hu",
      "Jinyuan Jia",
      "Neil Zhenqiang Gong"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9973_ECCV_2024_paper.php": {
    "title": "Discover-then-Name: Task-Agnostic Concept Bottlenecks via Automated Concept Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sukrut Rao*",
      "Sweta Mahajan*",
      "Moritz Böhle",
      "Bernt Schiele"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9976_ECCV_2024_paper.php": {
    "title": "Online Zero-Shot Classification with CLIP",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Qian*",
      "Juhua Hu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9979_ECCV_2024_paper.php": {
    "title": "SeA: Semantic Adversarial Augmentation for Last Layer Features from Unsupervised Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Qian*",
      "Yuanhong Xu",
      "Juhua Hu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9992_ECCV_2024_paper.php": {
    "title": "Unlocking the Potential of Federated Learning: The Symphony of Dataset Distillation via Deep Generative Latents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuqi Jia",
      "Saeed Vahidian*",
      "Jingwei Sun",
      "Jianyi Zhang",
      "Vyacheslav Kungurtsev",
      "Neil Zhenqiang Gong",
      "Yiran Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9996_ECCV_2024_paper.php": {
    "title": "Rethinking Fast Adversarial Training: A Splitting Technique To Overcome Catastrophic Overfitting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Masoumeh Zareapoor",
      "Pourya Shamsolmoali*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9997_ECCV_2024_paper.php": {
    "title": "Quality Assured: Rethinking Annotation Strategies in Imaging AI",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tim Rädsch*",
      "Annika Reinke",
      "Vivienn Weru",
      "Minu D. Tizabi",
      "Nicholas Heller",
      "Fabian Isensee",
      "Annette Kopp-Schneider",
      "Lena Maier-Hein*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10005_ECCV_2024_paper.php": {
    "title": "BRIDGE: Bridging Gaps in Image Captioning Evaluation with Stronger Visual Cues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sara Sarto*",
      "Marcella Cornia",
      "Lorenzo Baraldi",
      "Rita Cucchiara"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10006_ECCV_2024_paper.php": {
    "title": "Enhancing Plausibility Evaluation for Generated Designs with Denoising Autoencoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiajie Fan*",
      "Amal Trigui*",
      "Thomas Bäck",
      "Hao Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10017_ECCV_2024_paper.php": {
    "title": "Weakly-Supervised 3D Hand Reconstruction with Knowledge Prior and Uncertainty Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufei Zhang*",
      "Jeffrey Kephart",
      "Qiang Ji*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10029_ECCV_2024_paper.php": {
    "title": "3D Reconstruction of Objects in Hands without Real World 3D Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditya Prakash*",
      "Matthew Chang",
      "Matthew Jin",
      "Ruisen Tu",
      "Saurabh Gupta"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10031_ECCV_2024_paper.php": {
    "title": "To Supervise or Not to Supervise: Understanding and Addressing the Key Challenges of Point Cloud Transfer Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Souhail Hadgi*",
      "Lei Li",
      "Maks Ovsjanikov"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10032_ECCV_2024_paper.php": {
    "title": "Parameterized Quasi-Physical Simulators for Dexterous Manipulations Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xueyi Liu*",
      "Kangbo Lyu",
      "jieqiong zhang",
      "Tao Du",
      "Li Yi*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10034_ECCV_2024_paper.php": {
    "title": "3D Hand Pose Estimation in Everyday Egocentric Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditya Prakash*",
      "Ruisen Tu",
      "Matthew Chang",
      "Saurabh Gupta"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10036_ECCV_2024_paper.php": {
    "title": "Mitigating Perspective Distortion-induced Shape Ambiguity in Image Crops",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditya Prakash*",
      "Arjun Gupta",
      "Saurabh Gupta"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10038_ECCV_2024_paper.php": {
    "title": "Towards Neuro-Symbolic Video Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minkyu Choi*",
      "Harsh Goel",
      "Mohammad Omama",
      "Yunhao Yang",
      "Sahil Shah",
      "Sandeep Chinchali"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10054_ECCV_2024_paper.php": {
    "title": "Optimization-based Uncertainty Attribution Via Learning Informative Perturbations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanjing Wang*",
      "Bashirul Azam Biswas",
      "Qiang Ji"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10056_ECCV_2024_paper.php": {
    "title": "Context-Aware Action Recognition: Introducing a Comprehensive Dataset for Behavior Contrast",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tatsuya Sasaki*",
      "Yoshiki Ito",
      "Satoshi Kondo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10058_ECCV_2024_paper.php": {
    "title": "Semi-supervised Segmentation of Histopathology Images with Noise-Aware Topological Consistency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meilong Xu*",
      "Xiaoling Hu",
      "Saumya Gupta",
      "Shahira Abousamra",
      "Chao Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10059_ECCV_2024_paper.php": {
    "title": "Adaptive Compressed Sensing with Diffusion-Based Posterior Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Noam Elata*",
      "Tomer Michaeli",
      "Michael Elad"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10061_ECCV_2024_paper.php": {
    "title": "Instant Uncertainty Calibration of NeRFs Using a Meta-Calibrator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Niki Amini-Naieni*",
      "Tomas Jakab",
      "Andrea Vedaldi",
      "Ronald Clark"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10062_ECCV_2024_paper.php": {
    "title": "MetaAT: Active Testing for Label-Efficient Evaluation of Dense Recognition Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanbao Su",
      "Xin Li*",
      "Thang Doan",
      "Sima Behpour",
      "Wenbin He",
      "Liang Gou",
      "Fei Miao",
      "Liu Ren"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10063_ECCV_2024_paper.php": {
    "title": "Salience-Based Adaptive Masking: Revisiting Token Dynamics for Enhanced Pre-training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyesong Choi",
      "Hyejin Park",
      "Kwang Moo Yi",
      "Sungmin Cha",
      "Dongbo Min*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10071_ECCV_2024_paper.php": {
    "title": "Data Augmentation via Latent Diffusion for Saliency Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bahar Aydemir*",
      "Deblina Bhattacharjee",
      "Tong Zhang",
      "Mathieu Salzmann",
      "Sabine Süsstrunk"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10075_ECCV_2024_paper.php": {
    "title": "Explorative Inbetweening of Time and Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiwen Feng*",
      "Zheng Ding",
      "Zhihao Xia",
      "Simon Niklaus",
      "Victoria Fernandez Abrevaya",
      "Michael J. Black",
      "Xuaner Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10081_ECCV_2024_paper.php": {
    "title": "A Diffusion Model for Simulation Ready Coronary Anatomy with Morpho-skeletal Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karim Kadry*",
      "Shreya Gupta",
      "Jonas Sogbadji",
      "Michiel Schaap",
      "Kersten Petersen",
      "Takuya Mizukami",
      "Carlos Collet",
      "Farhad R. Nezami",
      "Elazer R Edelman"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10082_ECCV_2024_paper.php": {
    "title": "Learning to Make Keypoints Sub-Pixel Accurate",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shinjeong Kim*",
      "Marc Pollefeys",
      "Daniel Barath"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10086_ECCV_2024_paper.php": {
    "title": "Imaging with Confidence: Uncertainty Quantification for High-dimensional Undersampled MR Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Frederik Hoppe*",
      "Claudio Mayrink Verdun",
      "Hannah Sophie Laus",
      "Sebastian Endt",
      "Marion Irene Menzel",
      "Felix Krahmer",
      "Holger Rauhut"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10096_ECCV_2024_paper.php": {
    "title": "Generalizable Human Gaussians for Sparse View Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "YoungJoong Kwon*",
      "Baole Fang",
      "Yixing Lu",
      "Haoye Dong",
      "Cheng Zhang",
      "Francisco Vicente Carrasco",
      "Albert Mosella-Montoro",
      "Jianjin Xu",
      "Shingo J Takagi",
      "Daeil Kim",
      "Aayush Prakash",
      "Fernando de la Torre"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10097_ECCV_2024_paper.php": {
    "title": "DrivingDiffusion: Layout-Guided Multi-View Driving Scenarios Video Generation with Latent Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li Xiaofan*",
      "Zhang Yifu*",
      "Ye Xiaoqing*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10111_ECCV_2024_paper.php": {
    "title": "Evaluating the Adversarial Robustness of Semantic Segmentation: Trying Harder Pays Off",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Levente Halmosi",
      "Bálint Mohos",
      "Márk Jelasity*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10113_ECCV_2024_paper.php": {
    "title": "SkyScenes: A Synthetic Dataset for Aerial Scene Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sahil S Khose*",
      "Anisha Pal",
      "Aayushi Agarwal",
      ". Deepanshi",
      "Judy Hoffman",
      "Prithvijit Chattopadhyay"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10114_ECCV_2024_paper.php": {
    "title": "Large-Scale Multi-Hypotheses Cell Tracking Using Ultrametric Contours Maps",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jordão Bragantini*",
      "Merlin Lange",
      "Loïc A Royer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10117_ECCV_2024_paper.php": {
    "title": "GSD: View-Guided Gaussian Splatting Diffusion for 3D Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Mu*",
      "Xinxin Zuo",
      "Chuan Guo",
      "Yilin Wang",
      "Juwei Lu",
      "Xiaofei Wu",
      "Songcen Xu",
      "Peng Dai",
      "Youliang Yan",
      "Li Cheng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10120_ECCV_2024_paper.php": {
    "title": "AdaDiff: Accelerating Diffusion Models through Step-Wise Adaptive Computation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengkun Tang*",
      "Yaqing Wang",
      "Caiwen Ding",
      "Yi Liang",
      "Yao Li",
      "Dongkuan Xu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10133_ECCV_2024_paper.php": {
    "title": "PFedEdit: Personalized Federated Learning via Automated Model Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haolin Yuan*",
      "William Paul",
      "John Aucott",
      "Philippe Burlina",
      "Yinzhi Cao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10138_ECCV_2024_paper.php": {
    "title": "De-Confusing Pseudo-Labels in Source-Free Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Idit Diamant*",
      "Amir Rosenfeld",
      "Idan Achituve",
      "Jacob Goldberger",
      "Arnon Netzer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10147_ECCV_2024_paper.php": {
    "title": "GenerateCT: Text-Conditional Generation of 3D Chest CT Volumes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ibrahim Ethem Hamamci*",
      "Sezgin Er",
      "Anjany Sekuboyina",
      "Enis Simsar",
      "Alperen Tezcan",
      "Ayse Gulnihan Simsek",
      "Sevval Nil Esirgun",
      "Furkan Almas",
      "Irem Dogan",
      "Muhammed Furkan Dasdelen",
      "Chinmay Prabhakar",
      "Hadrien Reynaud",
      "Sarthak Pati",
      "Christian Bluethgen",
      "Mehmet Kemal Ozdemir",
      "Bjoern Menze"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10156_ECCV_2024_paper.php": {
    "title": "EraseDraw : Learning to Insert Objects by Erasing Them from Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alper Canberk*",
      "Maksym Bondarenko",
      "Ege Ozguroglu",
      "Ruoshi Liu",
      "Carl Vondrick"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10165_ECCV_2024_paper.php": {
    "title": "SuperFedNAS: Cost-Efficient Federated Neural Architecture Search for On-Device Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alind Khare*",
      "Animesh Agrawal",
      "Aditya Annavajjala",
      "Payman Behnam",
      "Myungjin Lee",
      "Hugo M Latapie",
      "Alexey Tumanov"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10167_ECCV_2024_paper.php": {
    "title": "Towards Reliable Evaluation and Fast Training of Robust Semantic Segmentation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francesco Croce*",
      "Naman D. Singh",
      "Matthias Hein*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10172_ECCV_2024_paper.php": {
    "title": "Contrastive Region Guidance: Improving Grounding in Vision-Language Models without Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Wan*",
      "Jaemin Cho",
      "Elias Stengel-Eskin",
      "Mohit Bansal"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10180_ECCV_2024_paper.php": {
    "title": "Keypoint Promptable Re-Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vladimir Somers*",
      "Alexandre Alahi",
      "Christophe De Vleeschouwer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10190_ECCV_2024_paper.php": {
    "title": "Merging and Splitting Diffusion Paths for Semantically Coherent Panoramas",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fabio Quattrini*",
      "Vittorio Pippi",
      "Silvia Cascianelli*",
      "Rita Cucchiara"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10191_ECCV_2024_paper.php": {
    "title": "DynMF: Neural Motion Factorization for Real-time Dynamic View Synthesis with 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Angelos Kratimenos*",
      "Jiahui Lei",
      "Kostas Daniilidis"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10197_ECCV_2024_paper.php": {
    "title": "Animal Avatars: Reconstructing Animatable 3D Animals from Casual Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Remy Sabathier*",
      "David Novotny",
      "Niloy Mitra"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10198_ECCV_2024_paper.php": {
    "title": "Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers' Opinion Scores",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucas Goncalves",
      "Prashant Mathur*",
      "Chandrashekhar Lavania",
      "Metehan Cekic",
      "Marcello Federico",
      "Kyu Han"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10205_ECCV_2024_paper.php": {
    "title": "MMVR: Millimeter-wave Multi-View Radar Dataset and Benchmark for Indoor Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Mahbubur Rahman",
      "Ryoma Yataka",
      "Sorachi Kato",
      "Pu Wang*",
      "Peizhao Li",
      "Adriano Cardace",
      "Petros Boufounos"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10236_ECCV_2024_paper.php": {
    "title": "Training A Secure Model against Data-Free Model Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenyi Wang*",
      "Li Shen*",
      "junfeng guo",
      "Tiehang Duan",
      "Siyu Luan",
      "Tongliang Liu",
      "Mingchen Gao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10237_ECCV_2024_paper.php": {
    "title": "EpipolarGAN: Omnidirectional Image Synthesis with Explicit Camera Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christopher May*",
      "Daniel Aliaga"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10248_ECCV_2024_paper.php": {
    "title": "TriNeRFLet: A Wavelet Based Triplane NeRF Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rajaei Khatib*",
      "Raja Giryes*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10261_ECCV_2024_paper.php": {
    "title": "EgoBody3M: Egocentric Body Tracking on a VR Headset using a Diverse Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amy Zhao",
      "Chengcheng Tang",
      "Lezi Wang",
      "Yijing Li",
      "Mihika Dave",
      "Lingling Tao*",
      "Christopher D. Twigg",
      "Robert Y. Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10270_ECCV_2024_paper.php": {
    "title": "Photorealistic Video Generation with Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Agrim Gupta*",
      "Lijun Yu",
      "Kihyuk Sohn",
      "Xiuye Gu",
      "Meera Hahn",
      "Li Fei-Fei",
      "Irfan Essa",
      "Lu Jiang",
      "Jose Lezama"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10278_ECCV_2024_paper.php": {
    "title": "RAVE: Residual Vector Embedding for CLIP-Guided Backlit Image Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tatiana Gaintseva*",
      "Martin Benning",
      "Gregory Slabaugh*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10285_ECCV_2024_paper.php": {
    "title": "TIBET: Identifying and Evaluating Biases in Text-to-Image Generative Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditya Chinchure*",
      "Pushkar Shukla*",
      "Gaurav Bhatt",
      "Kiri Salij",
      "Kartik Hosanagar",
      "Leonid Sigal",
      "Matthew Turk"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10290_ECCV_2024_paper.php": {
    "title": "Object-Aware Query Perturbation for Cross-Modal Image-Text Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Naoya Sogi*",
      "Takashi Shibata*",
      "Makoto Terao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10297_ECCV_2024_paper.php": {
    "title": "DECIDER: Leveraging Foundation Model Priors for Improved Model Failure Detection and Explanation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rakshith Subramanyam*",
      "Kowshik Thopalli*",
      "Vivek Sivaraman Narayanaswamy",
      "Jayaraman J. Thiagarajan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10301_ECCV_2024_paper.php": {
    "title": "Ex2Eg-MAE: A Framework for Adaptation of Exocentric Video Masked Autoencoders for Egocentric Social Role Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minh Tran*",
      "Yelin Kim",
      "Che-Chun Su",
      "Min Sun",
      "Cheng-Hao Kuo",
      "Mohammad Soleymani"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10310_ECCV_2024_paper.php": {
    "title": "Self-Supervised Audio-Visual Soundscape Stylization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tingle Li*",
      "Renhao Wang",
      "Po-Yao Huang",
      "Andrew Owens",
      "Gopala Krishna Anumanchipalli"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10311_ECCV_2024_paper.php": {
    "title": "SAVE: Protagonist Diversification with Structure Agnostic Video Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yeji Song*",
      "Wonsik Shin",
      "Junsoo Lee",
      "Jeesoo Kim",
      "Nojun Kwak*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10325_ECCV_2024_paper.php": {
    "title": "VideoAgent: Long-form Video Understanding with Large Language Model as Agent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaohan Wang*",
      "Yuhui Zhang",
      "Orr Zohar",
      "Serena Yeung-Levy"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10330_ECCV_2024_paper.php": {
    "title": "Meta-optimized Angular Margin Contrastive Framework for Video-Language Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thong Thanh Nguyen*",
      "Yi Bin",
      "Xiaobao Wu",
      "Xinshuai Dong",
      "Zhiyuan Hu",
      "Khoi M Le",
      "Cong-Duy Nguyen",
      "See Kiong Ng",
      "Anh Tuan Luu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10331_ECCV_2024_paper.php": {
    "title": "Source-Free Domain-Invariant Performance Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ekaterina Khramtsova*",
      "Mahsa Baktashmotlagh",
      "Guido Zuccon",
      "Xi Wang",
      "Mathieu Salzmann"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10339_ECCV_2024_paper.php": {
    "title": "Improving Robustness to Model Inversion Attacks via Sparse Coding Architectures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sayanton V. Dibbo*",
      "Adam Breuer",
      "Juston Moore",
      "Michael Teti"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10348_ECCV_2024_paper.php": {
    "title": "Constructing Concept-based Models to Mitigate Spurious Correlations with Minimal Human Effort",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeeyung Kim*",
      "Ze Wang",
      "Qiang Qiu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10350_ECCV_2024_paper.php": {
    "title": "Direct Distillation between Different Domains",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jialiang Tang",
      "Shuo Chen*",
      "Gang Niu",
      "Hongyuan Zhu",
      "Joey Tianyi Zhou",
      "Chen Gong*",
      "Masashi Sugiyama"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10352_ECCV_2024_paper.php": {
    "title": "Contrastive ground-level image and remote sensing pre-training improves representation learning for natural world imagery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andy V Huynh*",
      "Lauren Gillespie",
      "Jael Lopez-Saucedo",
      "Claire Tang",
      "Rohan Sikand",
      "Moisés Expósito-Alonso"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10354_ECCV_2024_paper.php": {
    "title": "V-Trans4Style: Visual Transition Recommendation for Video Production Style Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pooja Guhan*",
      "Tsung-Wei Huang",
      "Guan-Ming Su",
      "Subhadra Gopalakrishnan",
      "Dinesh Manocha"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10356_ECCV_2024_paper.php": {
    "title": "GRiT: A Generative Region-to-text Transformer for Object Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jialian Wu*",
      "Jianfeng Wang",
      "Zhengyuan Yang",
      "Zhe Gan",
      "Zicheng Liu",
      "Junsong Yuan",
      "Lijuan Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10364_ECCV_2024_paper.php": {
    "title": "LRSLAM: Low-rank Representation of Signed Distance Fields in Dense Visual SLAM System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongbeen Park",
      "Minjeong Park",
      "Giljoo Nam",
      "Jinkyu Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10369_ECCV_2024_paper.php": {
    "title": "Learning Representation for Multitask Learning through Self-Supervised Auxiliary Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seokwon Shin",
      "Hyungrok Do",
      "Youngdoo Son*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10378_ECCV_2024_paper.php": {
    "title": "Neural Poisson Solver: A Universal and Continuous Framework for Natural Signal Blending",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Delong Wu",
      "Hao Zhu",
      "Qi Zhang",
      "You Li",
      "Xun Cao*",
      "Zhan Ma*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10383_ECCV_2024_paper.php": {
    "title": "Geometry Fidelity for Spherical Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anders Christensen*",
      "Nooshin Mojab*",
      "Khushman Patel",
      "Karan Ahuja",
      "Zeynep Akata",
      "Ole Winther",
      "Mar Gonzalez Franco",
      "Andrea Colaco"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10394_ECCV_2024_paper.php": {
    "title": "BAGS: Blur Agnostic Gaussian Splatting through Multi-Scale Kernel Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Peng*",
      "Yutao Tang",
      "Yifan Zhou",
      "Nengyu Wang",
      "Xijun Liu",
      "Deming Li",
      "Rama Chellappa"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10398_ECCV_2024_paper.php": {
    "title": "CroMo-Mixup: Augmenting Cross-Model Representations for Continual Self-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Erum Mushtaq*",
      "Duygu Nur Yaldiz",
      "Yavuz Faruk Bakman",
      "Jie Ding",
      "Chenyang Tao",
      "Dimitrios Dimitriadis",
      "Salman Avestimehr"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10410_ECCV_2024_paper.php": {
    "title": "WoVoGen: World Volume-aware Diffusion for Controllable Multi-camera Driving Scene Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiachen Lu",
      "Ze Huang",
      "Zeyu Yang",
      "Zhang Jiahui",
      "Li Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10418_ECCV_2024_paper.php": {
    "title": "Benchmarking Spurious Bias in Few-Shot Image Classifiers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangtao Zheng*",
      "Wenqian Ye",
      "Aidong Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10421_ECCV_2024_paper.php": {
    "title": "TurboEdit: Real-time text-based disentangled real image editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zongze Wu*",
      "Nicholas I Kolkin",
      "Jonathan Brandt",
      "Richard Zhang",
      "Eli Shechtman"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10427_ECCV_2024_paper.php": {
    "title": "Soft Shadow Diffusion (SSD): Physics-inspired Learning for 3D Computational Periscopy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fadlullah A Raji*",
      "John Murray-Bruce*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10431_ECCV_2024_paper.php": {
    "title": "Augmented Neural Fine-tuning for Efficient Backdoor Purification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nazmul Karim*",
      "Abdullah Al Arafat",
      "Umar Khalid",
      "Zhishan Guo",
      "Nazanin Rahnavard"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10433_ECCV_2024_paper.php": {
    "title": "REDIR: Refocus-free Event-based De-occlusion Image Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Guo",
      "Hailong Shi*",
      "Huan Li",
      "Jinsheng Xiao",
      "Xingyu Gao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10441_ECCV_2024_paper.php": {
    "title": "Free-Editor: Zero-shot Text-driven 3D Scene Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nazmul Karim*",
      "Hasan Iqbal",
      "Umar Khalid",
      "Chen Chen",
      "Jing Hua"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10442_ECCV_2024_paper.php": {
    "title": "DPA-Net: Structured 3D Abstraction from Sparse Views via Differentiable Primitive Assembly",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fenggen Yu*",
      "Yiming Qian",
      "Xu Zhang",
      "Francisca Gil-Ureta",
      "Brian Jackson",
      "Eric Bennett",
      "Hao Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10455_ECCV_2024_paper.php": {
    "title": "An Empirical Study and Analysis of Text-to-Image Generation Using Large Language Model-Powered Textual Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyu Tan",
      "Mengping Yang",
      "Luozheng Qin ",
      "Hao Yang",
      "Ye Qian ",
      "Qiang Zhou",
      "Cheng Zhang",
      "Hao Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10470_ECCV_2024_paper.php": {
    "title": "Few-shot Class Incremental Learning with Attention-Aware Self-Adaptive Prompt",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenxi Liu*",
      "Zhenyi Wang",
      "Tianyi Xiong",
      "Ruibo Chen",
      "Yihan Wu",
      "junfeng guo",
      "Heng Huang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10478_ECCV_2024_paper.php": {
    "title": "An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liang Chen",
      "Haozhe Zhao",
      "Tianyu Liu",
      "Shuai Bai",
      "Junyang Lin",
      "Chang Zhou",
      "Baobao Chang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10485_ECCV_2024_paper.php": {
    "title": "Generalizable Symbolic Optimizer Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaotian Song",
      "Peng Zeng",
      "Yanan Sun*",
      "Andy Song"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10490_ECCV_2024_paper.php": {
    "title": "Online Continuous Generalized Category Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keon-Hee Park",
      "Hakyung Lee",
      "Kyungwoo Song*",
      "Gyeong-Moon Park*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10495_ECCV_2024_paper.php": {
    "title": "Bridging Different Language Models and Generative Vision Models for Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shihao Zhao*",
      "Shaozhe Hao",
      "Bojia Zi",
      "Huaizhe Xu",
      "Kwan-Yee K. Wong*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10498_ECCV_2024_paper.php": {
    "title": "Tackling Structural Hallucination in Image Translation with Local Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seunghoi Kim*",
      "Chen Jin",
      "Tom Diethe",
      "Matteo Figini",
      "Henry FJ Tregidgo",
      "Asher Mullokandov",
      "Philip A Teare",
      "Daniel Alexander"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10502_ECCV_2024_paper.php": {
    "title": "Hierarchical Separable Video Transformer for Snapshot Compressive Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ping Wang*",
      "Yulun Zhang",
      "Lishun Wang",
      "Xin Yuan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10510_ECCV_2024_paper.php": {
    "title": "Unified Medical Image Pre-training in Language-Guided Common Semantic Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoxuan He",
      "Yifan Yang",
      "Xinyang Jiang",
      "Xufang Luo*",
      "Haoji Hu",
      "Siyun Zhao",
      "Dongsheng Li",
      "Yuqing Yang",
      "Lili Qiu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10529_ECCV_2024_paper.php": {
    "title": "On the Vulnerability of Skip Connections to Model Inversion Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Hao Koh*",
      "Sy-Tuyen Ho",
      "Ngoc-Bao Nguyen",
      "Ngai-Man Cheung"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10530_ECCV_2024_paper.php": {
    "title": "Adversarial Robustification via Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daewon Choi",
      "Jongheon Jeong",
      "Huiwon Jang",
      "Jinwoo Shin*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10531_ECCV_2024_paper.php": {
    "title": "Overcome Modal Bias in Multi-modal Federated Learning via Balanced Modality Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunfeng FAN*",
      "Wenchao Xu*",
      "Haozhao Wang",
      "Fushuo Huo",
      "Jinyu Chen",
      "Song Guo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10537_ECCV_2024_paper.php": {
    "title": "Comprehensive Attribution: Inherently Explainable Vision Model with Feature Detector",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xianren Zhang",
      "Dongwon Lee",
      "Suhang Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10541_ECCV_2024_paper.php": {
    "title": "Reinforcement Learning via Auxillary Task Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhinav N Harish*",
      "Larry Heck",
      "Josiah P Hanna",
      "Zsolt Kira",
      "Andrew Szot"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10547_ECCV_2024_paper.php": {
    "title": "DHR: Dual Features-Driven Hierarchical Rebalancing in Inter- and Intra-Class Regions for Weakly-Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanghyun Jo",
      "Fei Pan",
      "In-Jae Yu",
      "Kyungsu Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10552_ECCV_2024_paper.php": {
    "title": "Pre-trained Visual Dynamics Representations for Efficient Policy Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Luo*",
      "Bohan Zhou",
      "Zongqing Lu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10555_ECCV_2024_paper.php": {
    "title": "View-Consistent Hierarchical 3D Segmentation Using Ultrametric Feature Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haodi He",
      "Colton Stearns",
      "Adam Harley",
      "Leonidas Guibas*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10564_ECCV_2024_paper.php": {
    "title": "Plug and Play: A Representation Enhanced Domain Adapter for Collaborative Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyou Luo*",
      "Quan Yuan*",
      "Yuchen Xia",
      "Guiyang Luo",
      "Yujia Yang",
      "Jinglin Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10568_ECCV_2024_paper.php": {
    "title": "Follow the Rules: Reasoning for Video Anomaly Detection with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchen Yang*",
      "Kwonjoon Lee",
      "Behzad Dariush",
      "Yinzhi Cao*",
      "Shao-Yuan Lo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10570_ECCV_2024_paper.php": {
    "title": "SAM4MLLM: Enhance Multi-Modal Large Language Model for Referring Expression Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi-Chia Chen",
      "Wei-Hua Li",
      "Cheng Sun",
      "Yu-Chiang Frank Wang",
      "Chu-Song Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10592_ECCV_2024_paper.php": {
    "title": "TTD: Text-Tag Self-Distillation Enhancing Image-Text Alignment in CLIP to Alleviate Single Tag Bias",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanghyun Jo",
      "Soohyun Ryu",
      "Sungyub Kim",
      "Eunho Yang",
      "Kyungsu Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10614_ECCV_2024_paper.php": {
    "title": "Learning Quantized Adaptive Conditions for Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchen Liang*",
      "Yuchuan Tian",
      "Lei Yu",
      "Huaao Tang",
      "Jie Hu",
      "Xiangzhong Fang",
      "Hanting Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10615_ECCV_2024_paper.php": {
    "title": "STAMP: Outlier-Aware Test-Time Adaptation with Stable Memory Replay",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Yongcan",
      "Lijun Sheng",
      "Ran He",
      "Jian Liang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10618_ECCV_2024_paper.php": {
    "title": "Remove Projective LiDAR Depthmap Artifacts via Exploiting Epipolar Geometry",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengjie Zhu*",
      "Girish Chandar Ganesan",
      "Abhinav Kumar",
      "Xiaoming Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10635_ECCV_2024_paper.php": {
    "title": "Accelerating Online Mapping and Behavior Prediction via Direct BEV Feature Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xunjiang Gu",
      "Guanyu Song",
      "Igor Gilitschenski",
      "Marco Pavone",
      "Boris Ivanovic*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10650_ECCV_2024_paper.php": {
    "title": "High-Fidelity Modeling of Generalizable Wrinkle Deformation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingfan Guo",
      "Jae Shin Yoon",
      "Shunsuke Saito",
      "Takaaki Shiratori",
      "Hyun Soo Park*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10652_ECCV_2024_paper.php": {
    "title": "Instruction Tuning-free Visual Token Complement for Multimodal LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongsheng Wang*",
      "Jiequan Cui",
      "Miaoge Li",
      "Wang Lin",
      "Bo Chen",
      "Hanwang Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10677_ECCV_2024_paper.php": {
    "title": "Exploring Conditional Multi-Modal Prompts for Zero-shot HOI Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ting Lei",
      "Shaofeng Yin",
      "Yuxin Peng",
      "Yang Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10687_ECCV_2024_paper.php": {
    "title": "Training-free Video Temporal Grounding using Large-scale Pre-trained Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minghang Zheng",
      "Xinhao Cai",
      "Qingchao Chen",
      "Yuxin Peng",
      "Yang Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10690_ECCV_2024_paper.php": {
    "title": "Revisit Self-supervision with Local Structure-from-Motion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengjie Zhu*",
      "Xiaoming Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10720_ECCV_2024_paper.php": {
    "title": "FAMOUS: High-Fidelity Monocular 3D Human Digitization Using View Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vishnu Mani Hema*",
      "Shubhra Aich",
      "Christian Haene",
      "Jean-Charles Bazin",
      "Fernando de la Torre"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10733_ECCV_2024_paper.php": {
    "title": "Efficient Learning of Event-based Dense Representation using Hierarchical Memories with Adaptive Update",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Uday Kamal*",
      "Saibal Mukhopadhyay"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10737_ECCV_2024_paper.php": {
    "title": "SNP: Structured Neuron-level Pruning to Preserve Attention Scores",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "KyungHwan Shim",
      "Jaewoong Yun",
      "Shinkook Choi*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10738_ECCV_2024_paper.php": {
    "title": "Multi-Granularity Sparse Relationship Matrix Prediction Network for End-to-End Scene Graph Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "lei wang",
      "Zejian Yuan",
      "Badong Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10739_ECCV_2024_paper.php": {
    "title": "Flash-Splat: 3D Reflection Removal with Flash Cues and Gaussian Splats",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingyang Xie*",
      "Haoming Cai",
      "Sachin Shah",
      "Yiran Xu",
      "Brandon Y. Feng",
      "Jia-Bin Huang",
      "Christopher A. Metzler"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10743_ECCV_2024_paper.php": {
    "title": "PALM: Predicting Actions through Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanghwan Kim*",
      "Daoji Huang",
      "Yongqin Xian",
      "Otmar Hilliges",
      "Luc Van Gool",
      "Xi Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10749_ECCV_2024_paper.php": {
    "title": "Motion Keyframe Interpolation for Any Human Skeleton using Point Cloud-based Human Motion Data Homogenisation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Clinton A Mo",
      "Kun Hu*",
      "Chengjiang Long",
      "Dong Yuan",
      "Zhiyong Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10756_ECCV_2024_paper.php": {
    "title": "SwiftBrush v2: Make Your One-step Diffusion Model Better Than Its Teacher",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Trung Tuan Dao*",
      "Thuan Hoang Nguyen",
      "Thanh Van Le",
      "Duc H Vu",
      "Khoi Nguyen",
      "Cuong Pham",
      "Anh T Tran*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10759_ECCV_2024_paper.php": {
    "title": "Learning to Localize Actions in Instructional Videos with LLM-Based Multi-Pathway Text-Video Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxiao Chen*",
      "Kai Li",
      "Wentao Bao",
      "Deep Patel",
      "Yu Kong",
      "Martin Renqiang Min",
      "Dimitris N. Metaxas*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10766_ECCV_2024_paper.php": {
    "title": "Improving Hyperbolic Representations via Gromov-Wasserstein Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei Yang",
      "Wonjun Lee",
      "Dongmian Zou*",
      "Gilad Lerman"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10768_ECCV_2024_paper.php": {
    "title": "VSViG: Real-time Video-based Seizure Detection via Skeleton-based Spatiotemporal ViG",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yankun Xu*",
      "Junzhe Wang",
      "Yun-Hsuan Chen",
      "Jie Yang",
      "Wenjie Ming",
      "Shuang Wang",
      "Mohamad Sawan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10778_ECCV_2024_paper.php": {
    "title": "DiffSurf: A Transformer-based Diffusion Model for Generating and Reconstructing 3D Surfaces in Pose",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yusuke Yoshiyasu*",
      "Leyuan Sun"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10783_ECCV_2024_paper.php": {
    "title": "Exploiting Supervised Poison Vulnerability to Strengthen Self-Supervised Defense",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeremy Styborski*",
      "Mingzhi Lyu*",
      "Yi Huang*",
      "Adams Kong*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10784_ECCV_2024_paper.php": {
    "title": "Dense Hand-Object(HO) GraspNet with Full Grasping Taxonomy and Dynamics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Woojin Cho",
      "Jihyun Lee",
      "Minjae Yi",
      "Minje Kim",
      "Taeyun Woo",
      "Donghwan Kim",
      "Taewook Ha",
      "Hyokeun Lee",
      "Je-Hwan Ryu",
      "Woontack Woo",
      "Tae-Kyun (T-K) Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10791_ECCV_2024_paper.php": {
    "title": "Human Pose Recognition via Occlusion-Preserving Abstract Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saad Manzur*",
      "Wayne B Hayes*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10792_ECCV_2024_paper.php": {
    "title": "DA-BEV: Unsupervised Domain Adaptation for Bird's Eye View Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Jiang*",
      "Jiaxing Huang",
      "Weiying Xie",
      "Jie Lei",
      "Yunsong Li",
      "Ling Shao",
      "Shijian Lu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10822_ECCV_2024_paper.php": {
    "title": "SlimFlow: Training Smaller One-Step Diffusion Models with Rectified Flow",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanzhi Zhu*",
      "Xingchao Liu",
      "Qiang Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10827_ECCV_2024_paper.php": {
    "title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaowei Liu",
      "Zhongzheng Ren",
      "Saurabh Gupta",
      "Shenlong Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10835_ECCV_2024_paper.php": {
    "title": "Depth-Aware Blind Image Decomposition for Real-World Adverse Weather Recovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Wang*",
      "Zhedong Zheng",
      "Ruijie Quan",
      "Yi Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10837_ECCV_2024_paper.php": {
    "title": "DreamSampler: Unifying Diffusion Sampling and Score Distillation for Image Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeongsol Kim",
      "Geon Yeong Park",
      "Jong Chul Ye*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10845_ECCV_2024_paper.php": {
    "title": "Reshaping the Online Data Buffering and Organizing Mechanism for Continual Test-Time Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhilin Zhu*",
      "Xiaopeng Hong*",
      "Zhiheng Ma",
      "Weijun Zhuang",
      "YaoHui Ma",
      "Yong Dai",
      "Yaowei Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10846_ECCV_2024_paper.php": {
    "title": "Personalized Privacy Protection Mask Against Unauthorized Facial Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ka-Ho Chow*",
      "Sihao Hu",
      "Tiansheng Huang",
      "Ling Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10849_ECCV_2024_paper.php": {
    "title": "PosterLlama: Bridging Design Ability of Langauge Model to Content-Aware Layout Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaejung Seol",
      "SeoJun Kim",
      "Jaejun Yoo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10860_ECCV_2024_paper.php": {
    "title": "PreciseControl: Enhancing Text-To-Image Diffusion Models with Fine-Grained Attribute Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rishubh Parihar*",
      "Sachidanand VS",
      "Sabariswaran Mani",
      "Tejan Karmali",
      "Venkatesh Babu RADHAKRISHNAN"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10868_ECCV_2024_paper.php": {
    "title": "LG-Gaze: Learning Geometry-aware Continuous Prompts for Language-Guided Gaze Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengwei Yin*",
      "Jingjing Wang",
      "Guanzhong Zeng",
      "Di Xie",
      "Jiang Zhu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10873_ECCV_2024_paper.php": {
    "title": "Efficient Training with Denoised Neural Weights",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Gong*",
      "Zheng Zhan",
      "Yanyu Li",
      "Yerlan Idelbayev",
      "Andrey Zharkov",
      "Kfir Aberman",
      "Sergey Tulyakov",
      "Yanzhi Wang",
      "Jian Ren"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10882_ECCV_2024_paper.php": {
    "title": "Learning the Unlearned: Mitigating Feature Suppression in Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jihai Zhang",
      "Xiang Lan",
      "Xiaoye Qu",
      "Yu Cheng",
      "Mengling Feng*",
      "Bryan Hooi*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10886_ECCV_2024_paper.php": {
    "title": "Integration of Global and Local Representations for Fine-grained Cross-modal Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seungwan Jin",
      "Hoyoung Choi",
      "Taehyung Noh",
      "Kyungsik Han*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10888_ECCV_2024_paper.php": {
    "title": "Local and Global Flatness for Federated Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Yan",
      "Yuhong Guo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10919_ECCV_2024_paper.php": {
    "title": "SRPose: Two-view Relative Pose Estimation with Sparse Keypoints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Yin",
      "Yulun Zhang",
      "Zherong Pan",
      "Jianjun Zhu",
      "Cheng Wang",
      "Biao Jia*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10922_ECCV_2024_paper.php": {
    "title": "Deep Reward Supervisions for Tuning Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoshi Wu",
      "Yiming Hao",
      "Manyuan Zhang*",
      "Keqiang Sun",
      "Zhaoyang Huang",
      "Guanglu Song",
      "Yu Liu",
      "Hongsheng Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10933_ECCV_2024_paper.php": {
    "title": "Paying More Attention to Images: A Training-Free Method for Alleviating Hallucination in LVLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shi Liu*",
      "Kecheng Zheng*",
      "Wei Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10940_ECCV_2024_paper.php": {
    "title": "Inf-DiT: Upsampling any-resolution image with memory-efficient diffusion transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuoyi Yang*",
      "Heyang Jiang",
      "Wenyi Hong",
      "Jiayan Teng",
      "Wendi Zheng",
      "Yuxiao Dong",
      "Ming Ding",
      "Jie Tang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10941_ECCV_2024_paper.php": {
    "title": "Implicit Neural Models to Extract Heart Rate from Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pradyumna Chari*",
      "Anirudh Bindiganavale Harish",
      "Adnan Armouti",
      "Alexander Vilesov",
      "Sanjit Sarda",
      "Laleh Jalilian",
      "Achuta Kadambi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10961_ECCV_2024_paper.php": {
    "title": "Boost Your NeRF: A Model-Agnostic Mixture of Experts Framework for High Quality and Efficient Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francesco Di Sario*",
      "Riccardo Renzulli",
      "Marco Grangetto",
      "Enzo Tartaglione"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10987_ECCV_2024_paper.php": {
    "title": "PFGS: High Fidelity Point Cloud Rendering via Feature Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxu Wang",
      "Zhang Ziyi",
      "Junhao He",
      "Renjing Xu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11002_ECCV_2024_paper.php": {
    "title": "Few-Shot Anomaly-Driven Generation for Anomaly Classification and Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guan Gui",
      "Bin-Bin Gao*",
      "Jun Liu",
      "Chengjie Wang",
      "Yunsheng Wu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11011_ECCV_2024_paper.php": {
    "title": "E3M: Zero-Shot Spatio-Temporal Video Grounding with Expectation-Maximization Multimodal Modulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peijun Bao*",
      "Zihao Shao",
      "Wenhan Yang",
      "Boon Poh Ng",
      "Alex Kot"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11028_ECCV_2024_paper.php": {
    "title": "EMO: Emote Portrait Alive - Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linrui Tian*",
      "Qi Wang*",
      "Bang Zhang*",
      "Liefeng Bo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11031_ECCV_2024_paper.php": {
    "title": "LMT-GP: Combined Latent Mean-Teacher and Gaussian Process for Semi-supervised Low-light Image Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ye Yu",
      "Fengxin Chen",
      "Jun Yu*",
      "Zhen Kan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11044_ECCV_2024_paper.php": {
    "title": "Veil Privacy on Visual Data: Concealing Privacy for Humans, Unveiling for DNNs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuchao Pang*",
      "Ruhao Ma",
      "Bing Li*",
      "Yongbin Zhou",
      "Yazhou Yao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11047_ECCV_2024_paper.php": {
    "title": "Efficient Vision Transformers with Partial Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan-Thuy Vo*",
      "Duy-Linh Nguyen",
      "Adri Priadana",
      "Kang-Hyun Jo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11051_ECCV_2024_paper.php": {
    "title": "Generalized Coverage for More Robust Low-Budget Active Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wonho Bae",
      "Junhyug Noh",
      "Danica J. Sutherland*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11059_ECCV_2024_paper.php": {
    "title": "Rasterized Edge Gradients: Handling Discontinuities Differentially",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stanislav Pidhorskyi*",
      "Tomas Simon",
      "Gabriel Schwartz",
      "He Wen",
      "Yaser Sheikh",
      "Jason Saragih"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11062_ECCV_2024_paper.php": {
    "title": "Enhancing Cross-Subject fMRI-to-Video Decoding with Global-Local Functional Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chong Li*",
      "Xuelin Qian",
      "Yun Wang",
      "Jingyang Huo",
      "Xiangyang Xue*",
      "Yanwei Fu*",
      "Jianfeng Feng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11069_ECCV_2024_paper.php": {
    "title": "FedTSA: A Cluster-based Two-Stage Aggregation Method for Model-heterogeneous Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boyu Fan*",
      "Chenrui Wu",
      "Xiang Su",
      "Pan HUI"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11080_ECCV_2024_paper.php": {
    "title": "LLaVA-UHD: an LMM Perceiving any Aspect Ratio and High-Resolution Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zonghao Guo",
      "Ruyi Xu",
      "Yuan Yao*",
      "Junbo Cui",
      "Zanlin Ni",
      "Chunjiang Ge",
      "Tat-Seng Chua",
      "Zhiyuan Liu",
      "Gao Huang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11103_ECCV_2024_paper.php": {
    "title": "Learning Natural Consistency Representation for Face Forgery Video Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daichi Zhang*",
      "Zihao Xiao",
      "Shikun Li",
      "Fanzhao Lin",
      "Jianmin Li",
      "Shiming Ge*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11109_ECCV_2024_paper.php": {
    "title": "ZeroI2V: Zero-Cost Adaptation of Pre-Trained Transformers from Image to Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinhao Li",
      "Yuhan Zhu",
      "Limin Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11114_ECCV_2024_paper.php": {
    "title": "Zero-Shot Adaptation for Approximate Posterior Sampling of Diffusion Models in Inverse Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yasar U Alcalar*",
      "Mehmet Akcakaya"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11117_ECCV_2024_paper.php": {
    "title": "R.A.C.E.: Robust Adversarial Concept Erasure for Secure Text-to-Image Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changhoon Kim*",
      "Kyle Min*",
      "Yezhou Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11118_ECCV_2024_paper.php": {
    "title": "OpenSight: A Simple Open-Vocabulary Framework for LiDAR-Based Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hu Zhang",
      "xu jianhua",
      "Tao Tang",
      "Haiyang Sun",
      "Xin Yu*",
      "Zi Helen Huang*",
      "Kaicheng Yu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11133_ECCV_2024_paper.php": {
    "title": "Few-Shot Image Generation by Conditional Relaxing Diffusion Inversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Cao*",
      "Shaogang Gong"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11142_ECCV_2024_paper.php": {
    "title": "Data Poisoning Quantization Backdoor Attack",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tran Huynh*",
      "Anh Tran",
      "Khoa Doan",
      "Tung Pham"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11182_ECCV_2024_paper.php": {
    "title": "DailyDVS-200: A Comprehensive Benchmark Dataset for Event-Based Action Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Wang",
      "Zhou Xu",
      "Yuming Lin",
      "Jingtao Ye",
      "Hongsheng Li",
      "Guangming Zhu",
      "Syed Afaq Ali Shah",
      "Mohammed Bennamoun",
      "Liang Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11184_ECCV_2024_paper.php": {
    "title": "On the Topology Awareness and Generalization Performance of Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junwei Su*",
      "Chuan Wu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11187_ECCV_2024_paper.php": {
    "title": "T-CorresNet: Template Guided 3D Point Cloud Completion with Correspondence Pooling Query Generation Strategy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan Duan",
      "Jiahao Yu",
      "Li Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11188_ECCV_2024_paper.php": {
    "title": "A high-quality robust diffusion framework for corrupted dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quan Dao*",
      "Binh Ta",
      "Tung Pham",
      "Anh Tran"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11199_ECCV_2024_paper.php": {
    "title": "Efficient 3D-Aware Facial Image Editing via Attribute-Specific Prompt Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amandeep Kumar*",
      "Muhammad Awais",
      "Sanath Narayan",
      "Hisham Cholakkal",
      "Salman Khan",
      "Rao Muhammad Anwer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11200_ECCV_2024_paper.php": {
    "title": "Distilling Knowledge from Large-Scale Image Models for Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gang Li*",
      "Wenhai Wang",
      "Xiang Li",
      "Ziheng Li",
      "Jian Yang",
      "Jifeng Dai",
      "Yu Qiao",
      "Shanshan Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11201_ECCV_2024_paper.php": {
    "title": "Embracing Events and Frames with Hierarchical Feature Refinement Network for Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hu Cao",
      "Zehua Zhang",
      "Yan Xia",
      "Xinyi Li",
      "Jiahao Xia",
      "Guang Chen*",
      "Alois C. Knoll"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11208_ECCV_2024_paper.php": {
    "title": "TimeLens-XL: Real-time Event-based Video Frame Interpolation with Large Motion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shi Guo",
      "Yutian Chen",
      "Tianfan Xue",
      "Jinwei Gu",
      "Yongrui Ma*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11211_ECCV_2024_paper.php": {
    "title": "Scene-Graph ViT: End-to-End Open-Vocabulary Visual Relationship Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tim Salzmann",
      "Markus Ryll",
      "Alex Bewley",
      "Matthias Minderer*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11219_ECCV_2024_paper.php": {
    "title": "Self-Supervised Underwater Caustics Removal and Descattering via Deep Monocular SLAM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonathan Sauder*",
      "Devis Tuia"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11222_ECCV_2024_paper.php": {
    "title": "Enriching Information and Preserving Semantic Consistency in Expanding Curvilinear Object Segmentation Datasets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qin Lei*",
      "Jiang Zhong",
      "Qizhu Dai"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11229_ECCV_2024_paper.php": {
    "title": "Retrieval Robust to Object Motion Blur",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rong Zou",
      "Marc Pollefeys",
      "Denys Rozumnyi*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11230_ECCV_2024_paper.php": {
    "title": "Unsupervised Representation Learning by Balanced Self Attention Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Shalam*",
      "Simon Korman*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11238_ECCV_2024_paper.php": {
    "title": "DualBEV: Unifying Dual View Transformation with Probabilistic Correspondences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peidong Li*",
      "Wancheng Shen",
      "Qihao Huang",
      "Dixiao Cui*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11241_ECCV_2024_paper.php": {
    "title": "Identity-Consistent Diffusion Network for Grading Knee Osteoarthritis Progression in Radiographic Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhua Wu",
      "Kun Hu*",
      "Wenxi Yue",
      "Wei Li",
      "Milena Simic",
      "Changyang Li",
      "Wei Xiang",
      "Zhiyong Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11249_ECCV_2024_paper.php": {
    "title": "Learned Neural Physics Simulation for Articulated 3D Human Pose Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Misha Andriluka*",
      "Baruch Tabanpour",
      "Daniel Freeman",
      "Cristian Sminchisescu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11254_ECCV_2024_paper.php": {
    "title": "Enhancing Source-Free Domain Adaptive Object Detection with Low-confidence Pseudo Label Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ilhoon Yoon",
      "Hyeongjun Kwon",
      "Jin Kim",
      "Junyoung Park",
      "Hyunsung Jang",
      "Kwanghoon Sohn*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11278_ECCV_2024_paper.php": {
    "title": "Fast Training of Diffusion Transformer with Extreme Masking for 3D Point Clouds Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shentong Mo",
      "Enze Xie*",
      "Yue Wu",
      "Junsong Chen",
      "Matthias Niessner",
      "Zhenguo Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11288_ECCV_2024_paper.php": {
    "title": "Make a Strong Teacher with Label Assistance: A Novel Knowledge Distillation Approach for Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shoumeng Qiu",
      "Jie Chen",
      "Xinrun Li",
      "Ru Wan",
      "Xiangyang Xue",
      "Jian Pu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11290_ECCV_2024_paper.php": {
    "title": "Make-Your-3D: Fast and Consistent Subject-Driven 3D Content Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangfu Liu",
      "Hanyang Wang",
      "Weiliang Chen",
      "Haowen Sun",
      "Yueqi Duan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11297_ECCV_2024_paper.php": {
    "title": "Segment, Lift and Fit: Automatic 3D Shape Labeling from 2D Prompts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianhao Li",
      "Tianyu Sun",
      "Zhongdao Wang*",
      "Enze Xie",
      "Bailan Feng",
      "Hongbo Zhang",
      "Ze Yuan",
      "Ke Xu",
      "Jiaheng Liu*",
      "Ping Luo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11303_ECCV_2024_paper.php": {
    "title": "SCOD: From Heuristics to Theory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vojtech Franc*",
      "Jakub Paplham*",
      "Daniel Prusa*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11330_ECCV_2024_paper.php": {
    "title": "Preventing Catastrophic Forgetting through Memory Networks in Continuous Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gaurav Bhatt*",
      "Leonid Sigal",
      "James Ross"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11332_ECCV_2024_paper.php": {
    "title": "Improving Zero-shot Generalization of Learned Prompts via Unsupervised Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marco Mistretta*",
      "Alberto Baldrati",
      "Marco Bertini",
      "Andrew D. Bagdanov"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11339_ECCV_2024_paper.php": {
    "title": "Teach CLIP to Develop a Number Sense for Ordinal Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yao DU*",
      "Qiang Zhai",
      "Weihang Dai",
      "Xiaomeng Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11348_ECCV_2024_paper.php": {
    "title": "Compact 3D Scene Representation via Self-Organizing Gaussian Grids",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wieland Morgenstern*",
      "Florian Barthel",
      "Anna Hilsmann",
      "Peter Eisert"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11350_ECCV_2024_paper.php": {
    "title": "Pix2Gif: Motion-Guided Diffusion for GIF Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hitesh Kandala*",
      "Jianfeng Gao",
      "Jianwei Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11352_ECCV_2024_paper.php": {
    "title": "VETRA: A Dataset for Vehicle Tracking in Aerial Imagery - New Challenges for Multi-Object Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jens Hellekes*",
      "Manuel Mühlhaus",
      "Reza Bahmanyar",
      "Seyed Majid Azimi",
      "Franz Kurz"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11356_ECCV_2024_paper.php": {
    "title": "SelfGeo: Self-supervised and Geodesic-consistent Estimation of Keypoints on Deformable Shapes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Zohaib*",
      "Luca Cosmo",
      "Alessio Del Bue"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11358_ECCV_2024_paper.php": {
    "title": "Beyond Prompt Learning: Continual Adapter for Efficient Rehearsal-Free Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyuan Gao",
      "Songlin Dong",
      "Yuhang He*",
      "Qiang Wang",
      "Yihong Gong"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11361_ECCV_2024_paper.php": {
    "title": "T2IShield: Defending Against Backdoors on Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongqi Wang",
      "Jie Zhang*",
      "Shiguang Shan",
      "Xilin Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11377_ECCV_2024_paper.php": {
    "title": "ExMatch: Self-guided Exploitation for Semi-Supervised Learning with Scarce Labeled Samples",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Noo-ri Kim",
      "Jin-Seop Lee",
      "Jee-Hyong Lee*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11379_ECCV_2024_paper.php": {
    "title": "Towards Certifiably Robust Face Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seunghun Paik",
      "Dongsoo Kim",
      "Chanwoo Hwang",
      "Sunpill Kim",
      "Jae Hong Seo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11380_ECCV_2024_paper.php": {
    "title": "Linking in Style: Understanding learned features in deep learning models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maren Wehrheim*",
      "Pamela Osuna Vargas",
      "Matthias Kaschube"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11389_ECCV_2024_paper.php": {
    "title": "Stable Video Portraits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mirela Ostrek*",
      "Justus Thies"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11391_ECCV_2024_paper.php": {
    "title": "UDA-Bench: Revisiting Common Assumptions in Unsupervised Domain Adaptation Using a Standardized Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tarun Kalluri*",
      "Sreyas Ravichandran",
      "Manmohan Chandraker"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11393_ECCV_2024_paper.php": {
    "title": "CliffPhys: Camera-based Respiratory Measurement using Clifford Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Omar Ghezzi*",
      "Giuseppe Boccignone",
      "Giuliano Grossi",
      "Raffaella Lanzarotti",
      "Alessandro D'Amelio"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11394_ECCV_2024_paper.php": {
    "title": "Learned Rate Control for Frame-Level Adaptive Neural Video Compression via Dynamic Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenhao Zhang",
      "Wei Gao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11397_ECCV_2024_paper.php": {
    "title": "PDiscoFormer: Relaxing Part Discovery Constraints with Vision Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ananthu Aniraj*",
      "Cassio F. Dantas",
      "Dino Ienco",
      "Diego Marcos"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11399_ECCV_2024_paper.php": {
    "title": "Vision-Language Dual-Pattern Matching for Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihan Zhang",
      "Zhuo Xu",
      "Xiang Xiang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11417_ECCV_2024_paper.php": {
    "title": "Synthesizing Environment-Specific People in Photographs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mirela Ostrek*",
      "Carol O'Sullivan",
      "Michael J. Black",
      "Justus Thies"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11424_ECCV_2024_paper.php": {
    "title": "Weight Conditioning for Smooth Optimization of Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hemanth Saratchandran*",
      "Thomas X Wang",
      "Simon Lucey"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11433_ECCV_2024_paper.php": {
    "title": "Energy-Clibrated VAE with Test Time Free Lunch",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihong Luo",
      "Siya Qiu",
      "Xingjian Tao",
      "Yujun Cai",
      "Jing Tang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11465_ECCV_2024_paper.php": {
    "title": "MoEAD: A Parameter-efficient Model for Multi-class Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiyuan Meng",
      "Wenchao Meng*",
      "Qihang Zhou",
      "Shizhong Li",
      "Weiye Hou",
      "Shibo He"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11481_ECCV_2024_paper.php": {
    "title": "SceneTeller: Language-to-3D Scene Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Basak Melis Ocal*",
      "Maxim Tatarchenko",
      "Sezer Karaoglu",
      "Theo Gevers"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11489_ECCV_2024_paper.php": {
    "title": "MagMax: Leveraging Model Merging for Seamless Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Marczak*",
      "Bartlomiej Twardowski*",
      "Tomasz Trzcinski*",
      "Sebastian Cygert*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11491_ECCV_2024_paper.php": {
    "title": "InternVideo2: Scaling Foundation Models for Multimodal Video Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Wang*",
      "Kunchang Li",
      "Xinhao Li",
      "Jiashuo Yu",
      "Yinan He",
      "Guo Chen",
      "Baoqi Pei",
      "Rongkun Zheng",
      "Jilan Xu",
      "Zun Wang",
      "Yansong Shi",
      "Tianxiang Jiang",
      "SongZe Li",
      "hongjie Zhang",
      "Yifei Huang",
      "Yu Qiao*",
      "Yali Wang*",
      "Limin Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11492_ECCV_2024_paper.php": {
    "title": "DiffusionPen: Towards Controlling the Style of Handwritten Text Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Konstantina Nikolaidou*",
      "George Retsinas",
      "Giorgos Sfikas",
      "Marcus Liwicki"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11496_ECCV_2024_paper.php": {
    "title": "Debiasing surgeon: fantastic weights and how to find them",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Remi Nahon",
      "Ivan Luiz De Moura Matos",
      "Van-Tam Nguyen",
      "Enzo Tartaglione*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11504_ECCV_2024_paper.php": {
    "title": "Denoising Vision Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawei Yang*",
      "Katie Z Luo",
      "Jiefeng Li",
      "Congyue Deng",
      "Leonidas Guibas",
      "Dilip Krishnan",
      "Kilian Weinberger",
      "Yonglong Tian",
      "Yue Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11515_ECCV_2024_paper.php": {
    "title": "Differentiable Product Quantization for Memory Efficient Camera Relocalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zakaria Laskar*",
      "Iaroslav Melekhov",
      "Assia Benbihi",
      "Shuzhe Wang",
      "Juho Kannala"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11525_ECCV_2024_paper.php": {
    "title": "Spline-based Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prashanth Chandran*",
      "Agon Serifi*",
      "Markus Gross",
      "Moritz Bächer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11528_ECCV_2024_paper.php": {
    "title": "Learning Pseudo 3D Guidance for View-consistent Texturing with 2D Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kehan Li",
      "Yanbo Fan*",
      "Yang Wu",
      "Zhongqian Sun",
      "Wei Yang",
      "Xiangyang Ji",
      "Li Yuan",
      "Jie Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11535_ECCV_2024_paper.php": {
    "title": "TreeSBA: Tree-Transformer for Self-Supervised Sequential Brick Assembly",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengqi Guo*",
      "Chen Li",
      "Yuyang Zhao",
      "Gim Hee Lee"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11537_ECCV_2024_paper.php": {
    "title": "SparseRadNet: Sparse Perception Neural Network on Subsampled Radar Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jialong Wu*",
      "Mirko Meuter",
      "Markus Schoeler",
      "Matthias Rottmann"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11554_ECCV_2024_paper.php": {
    "title": "Enhancing Semantic Fidelity in Text-to-Image Synthesis: Attention Regulation in Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Zhang*",
      "Tze Tzun Teoh",
      "Wei Hern Lim",
      "Kenji Kawaguchi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11557_ECCV_2024_paper.php": {
    "title": "Adversarial Diffusion Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Axel Sauer*",
      "Dominik Lorenz",
      "Andreas Blattmann",
      "Robin Rombach"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11581_ECCV_2024_paper.php": {
    "title": "Fake It till You Make It: Curricular Dynamic Forgery Augmentations towards General Deepfake Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuzhen Lin*",
      "Wentang Song",
      "Bin Li*",
      "Yuezun Li",
      "Jiangqun Ni",
      "Han Chen",
      "Qiushi Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11588_ECCV_2024_paper.php": {
    "title": "Explain via Any Concept: Concept Bottleneck Model with Open Vocabulary Concepts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andong Tan",
      "Fengtao Zhou",
      "Hao Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11599_ECCV_2024_paper.php": {
    "title": "Explore the Potential of CLIP for Training-Free Open Vocabulary Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tong Shao",
      "Zhuotao Tian*",
      "Hang Zhao",
      "Jingyong Su*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11606_ECCV_2024_paper.php": {
    "title": "A Multimodal Benchmark Dataset and Model for Crop Disease Diagnosis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Liu",
      "Zhaoxiang Liu*",
      "Huan Hu",
      "Zezhou Chen",
      "Kohou Wang",
      "Kai Wang",
      "Shiguo Lian*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11614_ECCV_2024_paper.php": {
    "title": "Missing Modality Prediction for Unpaired Multimodal Learning via Joint Embedding of Unimodal Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taesup Kim*",
      "Donggeun Kim"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11621_ECCV_2024_paper.php": {
    "title": "Learning Where to Look: Self-supervised Viewpoint Selection for Active Localization using Geometrical Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luca Di Giammarino*",
      "Boyang Sun",
      "Giorgio Grisetti",
      "Marc Pollefeys",
      "Hermann Blum",
      "Daniel Barath"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11626_ECCV_2024_paper.php": {
    "title": "Improving Diffusion Models for Authentic Virtual Try-on in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yisol Choi*",
      "Sangkyung Kwak",
      "Kyungmin Lee",
      "Hyungwon Choi",
      "Jinwoo Shin*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11636_ECCV_2024_paper.php": {
    "title": "Exploiting Semantic Reconstruction to Mitigate Hallucinations in Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minchan Kim",
      "Minyeong Kim",
      "Junik Bae",
      "Suhwan Choi",
      "Sungkyung Kim",
      "Buru Chang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11661_ECCV_2024_paper.php": {
    "title": "LISO: Lidar-only Self-Supervised 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stefan Andreas Baur*",
      "Frank Moosmann",
      "Andreas Geiger"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11664_ECCV_2024_paper.php": {
    "title": "Text-Conditioned Resampler For Long Form Video Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bruno Korbar*",
      "Yongqin Xian",
      "Alessio Tonioni",
      "Andrew Zisserman",
      "Federico Tombari"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11671_ECCV_2024_paper.php": {
    "title": "Implicit Steganography Beyond the Constraints of Modality",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sojeong Song*",
      "Seoyun Yang*",
      "Chang D. Yoo*",
      "Junmo Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11672_ECCV_2024_paper.php": {
    "title": "Using My Artistic Style? You Must Obtain My Authorization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiuli Bi",
      "Haowei Liu",
      "Weisheng Li",
      "Bo Liu*",
      "Bin Xiao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11673_ECCV_2024_paper.php": {
    "title": "LookupViT: Compressing visual information to a limited number of tokens",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rajat Koner",
      "Gagan Jain",
      "Sujoy Paul*",
      "Volker Tresp",
      "Prateek Jain"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11674_ECCV_2024_paper.php": {
    "title": "Fast Diffusion-Based Counterfactuals for Shortcut Removal and Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nina Weng*",
      "Paraskevas Pegios",
      "Eike Petersen",
      "Aasa Feragen",
      "Siavash Arjomand Bigdeli"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11688_ECCV_2024_paper.php": {
    "title": "UMERegRobust – Universal Manifold Embedding Compatible Features for Robust Point Cloud Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuval Haitman*",
      "Amit Efraim",
      "Joseph M Francos"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11714_ECCV_2024_paper.php": {
    "title": "Non-transferable Pruning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruyi Ding*",
      "Lili Su",
      "A. Adam Ding",
      "Yunsi Fei"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11720_ECCV_2024_paper.php": {
    "title": "A Compact Dynamic 3D Gaussian Representation for Real-Time Dynamic View Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Katsumata*",
      "Duc Minh Vo",
      "Hideki Nakayama"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11739_ECCV_2024_paper.php": {
    "title": "Fast Context-Based Low-Light Image Enhancement via Neural Implicit Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tomáš Chobola*",
      "Yu Liu",
      "Hanyi Zhang",
      "Julia A Schnabel",
      "Tingying Peng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11741_ECCV_2024_paper.php": {
    "title": "Toward Open Vocabulary Aerial Object Detection with CLIP-Activated Student-Teacher Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Li",
      "Weiwei Guo*",
      "Xue Yang",
      "Ning Liao",
      "Dunyun He",
      "Jiaqi Zhou",
      "Wenxian Yu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11742_ECCV_2024_paper.php": {
    "title": "Affine steerers for structured keypoint description",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Georg Bökman*",
      "Johan Edstedt",
      "Michael Felsberg",
      "Fredrik Kahl"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11754_ECCV_2024_paper.php": {
    "title": "Score Distillation Sampling with Learned Manifold Corrective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thiemo Alldieck*",
      "Nikos Kolotouros",
      "Cristian Sminchisescu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11758_ECCV_2024_paper.php": {
    "title": "FipTR: A Simple yet Effective Transformer Framework for Future Instance Prediction in Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingtai Gui*",
      "Tengteng Huang",
      "Haonan Shao",
      "Haotian Yao",
      "Chi Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11762_ECCV_2024_paper.php": {
    "title": "Benchmarking the Robustness of Cross-view Geo-localization Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingwang Zhang",
      "Yingying Zhu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11763_ECCV_2024_paper.php": {
    "title": "GroCo: Ground Constraint for Metric Self-Supervised Monocular Depth",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aurélien Cecille*",
      "Stefan Duffner",
      "Franck Davoine",
      "Thibault Neveu",
      "Rémi Agier"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11787_ECCV_2024_paper.php": {
    "title": "SUMix: Mixup with Semantic and Uncertain Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huafeng Qin",
      "Xin Jin*",
      "Hongyu Zhu",
      "Hongchao Liao",
      "Mounim A. El Yacoubi",
      "Xinbo Gao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11789_ECCV_2024_paper.php": {
    "title": "Flatness-aware Sequential Learning Generates Resilient Backdoors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hoang Pham*",
      "The-Anh Ta",
      "Anh T Tran",
      "Khoa D Doan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11800_ECCV_2024_paper.php": {
    "title": "Iterative Ensemble Training with Anti-Gradient Control for Mitigating Memorization in Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Liu",
      "Xiaoliu Guan",
      "Yu Wu*",
      "Jiaxu Miao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11802_ECCV_2024_paper.php": {
    "title": "IFTR: An Instance-Level Fusion Transformer for Visual Collaborative Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaohong Wang",
      "Lu Bin",
      "Xinyu Xiao",
      "Zhiyu Xiang",
      "Hangguan Shan",
      "Eryun Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11819_ECCV_2024_paper.php": {
    "title": "DiffClass: Diffusion-Based Class Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zichong Meng",
      "Jie Zhang",
      "Changdi Yang",
      "Zheng Zhan",
      "Pu Zhao*",
      "Yanzhi Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11823_ECCV_2024_paper.php": {
    "title": "Convex Relaxations for Manifold-Valued Markov Random Fields with Approximation Guarantees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Robin Kenis*",
      "Emanuel Laude",
      "Panagiotis Patrinos"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11825_ECCV_2024_paper.php": {
    "title": "Instant 3D Human Avatar Generation using Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikos Kolotouros*",
      "Thiemo Alldieck",
      "Enric Corona",
      "Eduard Gabriel Bazavan",
      "Cristian Sminchisescu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11826_ECCV_2024_paper.php": {
    "title": "PromptFusion: Decoupling Stability and Plasticity for Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoran Chen",
      "Zuxuan Wu*",
      "Xintong Han",
      "Menglin Jia",
      "Yu-Gang Jiang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11844_ECCV_2024_paper.php": {
    "title": "Improving Geo-diversity of Generated Images with Contextualized Vendi Score Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Reyhane Askari Hemmat*",
      "Melissa Hall*",
      "Alicia Yi Sun",
      "Candace Ross",
      "Michal Drozdzal",
      "Adriana Romero-Soriano"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11855_ECCV_2024_paper.php": {
    "title": "Adapting to Shifting Correlations with Unlabeled Data Calibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minh Nguyen*",
      "Alan Q Wang",
      "Heejong Kim",
      "Mert Sabuncu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11856_ECCV_2024_paper.php": {
    "title": "Masked Generative Video-to-Audio Transformers with Enhanced Synchronicity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Santiago Pascual",
      "Chunghsin YEH*",
      "Ioannis Tsiamas",
      "Joan Serrà"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11862_ECCV_2024_paper.php": {
    "title": "Information Bottleneck Based Data Correction in Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuai Chen",
      "mingyi zhang",
      "Junge Zhang*",
      "Kaiqi Huang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11865_ECCV_2024_paper.php": {
    "title": "On Spectral Properties of Gradient-based Explanation Methods",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amir Mehrpanah*",
      "Erik Englesson",
      "Hossein Azizpour"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11870_ECCV_2024_paper.php": {
    "title": "Contextual Correspondence Matters: Bidirectional Graph Matching for Video Summarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunzuo Zhang*",
      "Yameng Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11877_ECCV_2024_paper.php": {
    "title": "O2V-Mapping: Online Open-Vocabulary Mapping with Neural Implicit Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muer Tie",
      "Julong Wei",
      "Zhengjun Wang",
      "Ke Wu",
      "Shanshuai Yuan",
      "Kaizhao Zhang",
      "Jie Jia",
      "Jieru Zhao",
      "Zhongxue Gan*",
      "Wenchao Ding*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11886_ECCV_2024_paper.php": {
    "title": "Dataset Distillation by Automatic Training Trajectories",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dai Liu*",
      "Jindong Gu*",
      "Hu Cao",
      "Carsten Trinitis",
      "Martin Schulz*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11904_ECCV_2024_paper.php": {
    "title": "FAFA: Frequency-Aware Flow-Aided Self-Supervision for Underwater Object Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyi Tang*",
      "Gu Wang",
      "Zeyu Chen",
      "Shengquan Li",
      "Xiu Li*",
      "Xiangyang Ji"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11918_ECCV_2024_paper.php": {
    "title": "EMIE-MAP: Large-Scale Road Surface Reconstruction Based on Explicit Mesh and Implicit Encoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhua Wu",
      "Qi Wang",
      "Guangming Wang",
      "Junping Wang",
      "Tiankun Zhao",
      "Yang Liu",
      "Dongchao Gao",
      "Zhe Liu*",
      "Hesheng Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11927_ECCV_2024_paper.php": {
    "title": "UniIR: Training and Benchmarking Universal Multimodal Information Retrievers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cong Wei*",
      "Yang Chen",
      "Haonan Chen",
      "Hexiang Hu",
      "Ge Zhang",
      "Jie Fu",
      "Alan Ritter",
      "Wenhu Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11933_ECCV_2024_paper.php": {
    "title": "SSL-Cleanse: Trojan Detection and Mitigation in Self-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengxin Zheng*",
      "Jiaqi Xue",
      "Zihao Wang",
      "Xun Chen",
      "Qian Lou",
      "Lei Jiang",
      "Xiaofeng Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11936_ECCV_2024_paper.php": {
    "title": "Skews in the Phenomenon Space Hinder Generalization in Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingshan Chang*",
      "Yasi Zhang",
      "Zhiyuan Fang",
      "Ying Nian Wu",
      "Yonatan Bisk",
      "Feng Gao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11949_ECCV_2024_paper.php": {
    "title": "Bones Can't Be Triangles: Accurate and Efficient Vertebrae Keypoint Estimation through Collaborative Error Revision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinhee Kim",
      "Taesung Kim",
      "Jaegul Choo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11950_ECCV_2024_paper.php": {
    "title": "latentSplat: Autoencoding Variational Gaussians for Fast Generalizable 3D Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christopher Wewer*",
      "Kevin Raj",
      "Eddy Ilg",
      "Bernt Schiele",
      "Jan E. Lenssen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11979_ECCV_2024_paper.php": {
    "title": "HyperSpaceX: Radial and Angular Exploration of HyperSpherical Dimensions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chiranjeev Chiranjeev",
      "Muskan Dosi",
      "Kartik Thakral",
      "Mayank Vatsa*",
      "Richa Singh"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12016_ECCV_2024_paper.php": {
    "title": "InstructGIE: Towards Generalizable Image Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zichong Meng",
      "Changdi Yang",
      "Jun Liu",
      "Hao Tang*",
      "Pu Zhao*",
      "Yanzhi Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12018_ECCV_2024_paper.php": {
    "title": "HandDAGT: A Denoising Adaptive Graph Transformer for 3D Hand Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "WENCAN CHENG",
      "Eunji Kim",
      "Jong Hwan Ko*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12020_ECCV_2024_paper.php": {
    "title": "Navigating Text-to-Image Generative Bias across Indic Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Surbhi Mittal*",
      "Arnav Sudan",
      "Mayank Vatsa*",
      "Richa Singh",
      "Tamar Glaser",
      "Tal Hassner"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12030_ECCV_2024_paper.php": {
    "title": "Correspondence-Free SE(3) Point Cloud Registration in RKHS via Unsupervised Equivariant Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ray Zhang*",
      "Zheming Zhou",
      "Min Sun",
      "Omid Ghasemalizadeh",
      "Cheng-Hao Kuo",
      "Ryan M. Eustice",
      "Maani Ghaffari Jadidi",
      "Arnie Sen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12052_ECCV_2024_paper.php": {
    "title": "CTRLorALTer: Conditional LoRAdapter for Efficient 0-Shot Control & Altering of T2I Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nick Stracke*",
      "Stefan Andreas Baumann",
      "Joshua Susskind",
      "Miguel Angel Bautista",
      "Bjorn Ommer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12073_ECCV_2024_paper.php": {
    "title": "Nickel and Diming Your GAN: A Dual-Method Approach to Enhancing GAN Efficiency via Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sangyeop Yeo",
      "Yoojin Jang",
      "Jaejun Yoo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12075_ECCV_2024_paper.php": {
    "title": "VividDreamer: Invariant Score Distillation for Hyper-Realistic Text-to-3D Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenjie Zhuo*",
      "Fan Ma",
      "Hehe Fan",
      "Yi Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12117_ECCV_2024_paper.php": {
    "title": "A Framework for Efficient Model Evaluation through Stratification, Sampling, and Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Riccardo Fogliato*",
      "Pratik Patil",
      "Mathew Monfort",
      "Pietro Perona"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12130_ECCV_2024_paper.php": {
    "title": "Towards Scene Graph Anticipation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rohith Peddi*",
      "Saksham Singh",
      "Saurabh .",
      "Parag Singla",
      "Vibhav Gogate"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12133_ECCV_2024_paper.php": {
    "title": "Non-Line-of-Sight Estimation of Fast Human Motion with Slow Scanning Imagers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Javier Grau Chopite*",
      "Patrick Hähn",
      "Matthias B Hullin*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12134_ECCV_2024_paper.php": {
    "title": "Distributed Semantic Segmentation with Efficient Joint Source and Task Decoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Danish Nazir*",
      "Timo Bartels",
      "Jan Piewek",
      "Thorsten Bagdonat",
      "Tim Fingscheidt"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12148_ECCV_2024_paper.php": {
    "title": "NePhi: Neural Deformation Fields for Approximately Diffeomorphic Medical Image Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lin Tian*",
      "Thomas H Greer",
      "Raul San Jose Estepar",
      "Roni Sengupta",
      "Marc Niethammer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12149_ECCV_2024_paper.php": {
    "title": "Aligning Neuronal Coding of Dynamic Visual Scenes with Foundation Vision Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rining Wu*",
      "Feixiang Zhou",
      "Ziwei Yin",
      "Jian Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12164_ECCV_2024_paper.php": {
    "title": "Image Manipulation Detection With Implicit Neural Representation and Limited Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenfei Zhang*",
      "Mingyang Li",
      "Xin Li",
      "Ming-Ching Chang",
      "Jun-Wei Hsieh"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12191_ECCV_2024_paper.php": {
    "title": "Scalar Function Topology Divergence: Comparing Topology of 3D Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ilya Trofimov*",
      "Daria Voronkova",
      "Eduard Tulchinskii",
      "Evgeny Burnaev",
      "Serguei Barannikov"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12192_ECCV_2024_paper.php": {
    "title": "Introducing Routing Functions to Vision-Language Parameter-Efficient Fine-Tuning with Low-Rank Bottlenecks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tingyu Qu*",
      "Tinne Tuytelaars",
      "Marie-Francine Moens"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12206_ECCV_2024_paper.php": {
    "title": "Concept Arithmetics for Circumventing Concept Inhibition in Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vitali Petsiuk*",
      "Kate Saenko"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12207_ECCV_2024_paper.php": {
    "title": "DeTra: A Unified Model for Object Detection and Trajectory Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sergio Casas*",
      "Ben T Agro",
      "Jiageng Mao",
      "Thomas Gilles",
      "ALEXANDER Y CUI",
      "Enxu Li",
      "Raquel Urtasun"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12254_ECCV_2024_paper.php": {
    "title": "ControlNet-XS: Rethinking the Control of Text-to-Image Diffusion Models as Feedback-Control Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Denis Zavadski*",
      "Johann-Friedrich Feiden",
      "Carsten Rother"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12292_ECCV_2024_paper.php": {
    "title": "Adaptive Bounding Box Uncertainties via Two-Step Conformal Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Timans*",
      "Christoph-Nikolas Straehle",
      "Kaspar Sakmann",
      "Eric Nalisnick"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12295_ECCV_2024_paper.php": {
    "title": "Common Sense Reasoning for Deep Fake Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Zhang*",
      "Ben Colman",
      "Xiao Guo",
      "Ali Shahriyari",
      "Gaurav Bharaj*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12305_ECCV_2024_paper.php": {
    "title": "Let the Avatar Talk using Texts without Paired Training Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiuzhe Wu",
      "Yang-Tian Sun",
      "Handi Chen",
      "Hang Zhou",
      "Jingdong Wang",
      "Zhengzhe Liu",
      "Xiaojuan Qi*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12306_ECCV_2024_paper.php": {
    "title": "NeRF-MAE: Masked AutoEncoders for Self-Supervised 3D Representation Learning for Neural Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Zubair Irshad*",
      "Sergey Zakharov",
      "Vitor Guizilini",
      "Adrien Gaidon",
      "Zsolt Kira",
      "Rares Ambrus"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12324_ECCV_2024_paper.php": {
    "title": "GOEmbed: Gradient Origin Embeddings for Representation Agnostic 3D Feature Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Animesh Karnewar*",
      "Roman Shapovalov",
      "Tom Monnier",
      "Andrea Vedaldi",
      "Niloy J. Mitra*",
      "David Novotny*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12325_ECCV_2024_paper.php": {
    "title": "Causal Subgraphs and Information Bottlenecks: Redefining OOD Robustness in Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weizhi An",
      "Wenliang Zhong",
      "Feng Jiang",
      "Hehuan Ma",
      "Junzhou Huang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12349_ECCV_2024_paper.php": {
    "title": "AddBiomechanics Dataset: Capturing the Physics of Human Motion at Scale",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keenon Werling*",
      "Janelle M Kaneda",
      "Tian Tan",
      "Rishi Agarwal",
      "Six Skov",
      "Tom Van Wouwe",
      "Scott Uhlrich",
      "Scott Delp",
      "Karen Liu",
      "Nicholas A Bianco",
      "Carmichael Ong",
      "Antoine Falisse",
      "Shardul Sapkota",
      "Aidan Jai Chandra",
      "Joshua A Carter",
      "Ezio Preatoni",
      "Benjamin J Fregly",
      "Jennifer Hicks"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12363_ECCV_2024_paper.php": {
    "title": "How to Train the Teacher Model for Effective Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shayan Mohajer Hamidi*",
      "Xizhen Deng",
      "Renhao Tan",
      "Linfeng Ye",
      "Ahmed Hussein Salamah"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12373_ECCV_2024_paper.php": {
    "title": "Tight and Efficient Upper Bound on Spectral Norm of Convolutional Layers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ekaterina Grishina*",
      "Mikhail Gorbunov",
      "Maxim Rakhuba"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12388_ECCV_2024_paper.php": {
    "title": "Deciphering the Role of Representation Disentanglement: Investigating Compositional Generalization in CLIP Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Reza Abbasi",
      "Mohammad Rohban",
      "Mahdieh Soleymani Baghshah*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12401_ECCV_2024_paper.php": {
    "title": "Modality Translation for Object Detection Adaptation without forgetting prior knowledge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heitor Rapela Medeiros*",
      "Masih Aminbeidokhti",
      "Fidel A Guerrero Pena",
      "David Latortue",
      "Eric Granger",
      "Marco Pedersoli"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12410_ECCV_2024_paper.php": {
    "title": "FroSSL: Frobenius Norm Minimization for Efficient Multiview Self-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oscar Skean*",
      "Aayush Dhakal",
      "Nathan Jacobs",
      "Luis G Sanchez Giraldo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12427_ECCV_2024_paper.php": {
    "title": "Learning Multimodal Latent Generative Models with Energy-Based Prior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiyu Yuan*",
      "Jiali Cui",
      "Hanao Li",
      "Tian Han"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12446_ECCV_2024_paper.php": {
    "title": "On Learning Discriminative Features from Synthesized Data for Self-Supervised Fine-Grained Visual Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihu Wang*",
      "Lingqiao Liu",
      "Scott Ricardo Figueroa Weston",
      "Samuel Tian",
      "Peng Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12460_ECCV_2024_paper.php": {
    "title": "LaWa: Using Latent Space for In-Generation Image Watermarking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ahmad Rezaei*",
      "Mohammad Akbari*",
      "Saeed Ranjbar Alvar",
      "Arezou Fatemi",
      "Yong Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12475_ECCV_2024_paper.php": {
    "title": "Hierarchical Conditioning of Diffusion Models Using Tree-of-Life for Studying Species Evolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mridul Khurana*",
      "Arka Daw",
      "M. Maruf",
      "Josef C. Uyeda",
      "Wasila Dahdul",
      "Caleb Charpentier",
      "Yasin Bakış",
      "Henry L. Bart Jr.",
      "Paula M. Mabee",
      "Hilmar Lapp",
      "James P. Balhoff",
      "Wei-Lun Chao",
      "Charles Stewart",
      "Tanya Berger-Wolf",
      "Anuj Karpatne*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12478_ECCV_2024_paper.php": {
    "title": "Markov Knowledge Distillation: Make Nasty Teachers trained by Self-undermining Knowledge Distillation Fully Distillable",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "En-hui Yang",
      "Linfeng Ye*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12483_ECCV_2024_paper.php": {
    "title": "Co-speech Gesture Video Generation with 3D Human Meshes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aniruddha Mahapatra*",
      "Richa Mishra*",
      "Ziyi Chen",
      "Boyang Ding",
      "Renda Li",
      "Shoulei Wang",
      "Jun-Yan Zhu",
      "Peng Chang",
      "Mei Han",
      "Jing Xiao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12484_ECCV_2024_paper.php": {
    "title": "When and How do negative prompts take effect?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanhao Ban",
      "Ruochen Wang",
      "Tianyi Zhou",
      "Minhao Cheng",
      "Boqing Gong",
      "Cho-Jui Hsieh*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12486_ECCV_2024_paper.php": {
    "title": "GS2Mesh: Surface Reconstruction from Gaussian Splatting via Novel Stereo Views",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaniv Wolf*",
      "Amit Bracha",
      "Ron Kimmel"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12520_ECCV_2024_paper.php": {
    "title": "CARFF: Conditional Auto-encoded Radiance Field for 3D Scene Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiezhi Yang*",
      "Khushi P Desai*",
      "Charles Packer*",
      "Harshil bhatia",
      "Nicholas Rhinehart",
      "Rowan McAllister",
      "Joseph E Gonzalez*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12535_ECCV_2024_paper.php": {
    "title": "Snuffy: Efficient Whole Slide Image Classifier",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hossein Jafarinia*",
      "Alireza Alipanah",
      "Saeed Razavi",
      "Nahal Mirzaie",
      "Mohammad Hossein Rohban*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12536_ECCV_2024_paper.php": {
    "title": "Learning to Build by Building Your Own Instructions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aaron T Walsman*",
      "Muru Zhang",
      "Adam Fishman",
      "Ali Farhadi",
      "Dieter Fox"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12574_ECCV_2024_paper.php": {
    "title": "Exploring Active Learning in Meta-Learning: Enhancing Context Set Labeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wonho Bae",
      "Jing Wang",
      "Danica J. Sutherland*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12578_ECCV_2024_paper.php": {
    "title": "BlenderAlchemy: Editing 3D Graphics with Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ian Huang*",
      "Guandao Yang",
      "Leonidas Guibas"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12638_ECCV_2024_paper.php": {
    "title": "DεpS: Delayed ε-Shrinking for Faster Once-For-All Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditya Annavajjala*",
      "Alind Khare*",
      "Animesh Agrawal",
      "Igor Fedorov",
      "Hugo M Latapie",
      "Myungjin Lee",
      "Alexey Tumanov"
    ]
  }
}