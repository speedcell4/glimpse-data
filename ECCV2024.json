{
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4_ECCV_2024_paper.php": {
    "title": "Is Retain Set All You Need in Machine Unlearning? Restoring Performance of Unlearned Models with Out-Of-Distribution Images",
    "volume": "main",
    "abstract": "In this paper, we introduce Selective-distillation for Class and Architecture-agnostic unleaRning (SCAR), a novel approximate unlearning method. SCAR efficiently eliminates specific information while preserving the model's test accuracy without using a retain set, which is a key component in state-of-the-art approximate unlearning algorithms. Our approach utilizes a modified Mahalanobis distance to guide the unlearning of the feature vectors of the instances to be forgotten, aligning them to the nearest wrong class distribution. Moreover, we propose a distillation-trick mechanism that distills the knowledge of the original model into the unlearning model with out-of-distribution images for retaining the original model's test performance without using any retain set. Importantly, we propose a self-forget version of SCAR that unlearns without having access to the forget set. We experimentally verified the effectiveness of our method, on three public datasets, comparing it with state-of-the-art methods. Our method obtains performance higher than methods that operate without the retain set and comparable w.r.t the best methods that rely on the retain set",
    "checked": true,
    "id": "6dddeeeee9927a63ef7996e4a35b768d8da0282d",
    "semantic_title": "is retain set all you need in machine unlearning? restoring performance of unlearned models with out-of-distribution images",
    "citation_count": 1,
    "authors": [
      "Jacopo Bonato*",
      "Marco Cotogni",
      "Luigi Sabetta*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6_ECCV_2024_paper.php": {
    "title": "Octopus: Embodied Vision-Language Programmer from Environmental Feedback",
    "volume": "main",
    "abstract": "Large vision-language models (VLMs) have achieved substantial progress in multimodal perception and reasoning. When integrated into an embodied agent, existing embodied VLM works either output detailed action sequences at the manipulation level or only provide plans at an abstract level, leaving a gap between high-level planning and real-world manipulation. To bridge this gap, we introduce Octopus, an embodied vision-language programmer that uses executable code generation as a medium to connect planning and manipulation. Octopus is designed to 1) proficiently comprehend an agent's visual and textual task objectives, 2) formulate intricate action sequences, and 3) generate executable code. To facilitate Octopus model development, we introduce OctoVerse: a suite of environments tailored for benchmarking vision-based code generators on a wide spectrum of tasks, ranging from mundane daily chores in simulators to sophisticated interactions in complex video games such as Grand Theft Auto (GTA) and Minecraft. To train Octopus, we leverage GPT-4 to control an explorative agent that generates training data, i.e., action blueprints and corresponding executable code. We also collect feedback that enables an enhanced training scheme called Reinforcement Learning with Environmental Feedback (RLEF). Through a series of experiments, we demonstrate Octopus's functionality and present compelling results, showing that the proposed RLEF refines the agent's decision-making. By open-sourcing our simulation environments, dataset, and model architecture, we aspire to ignite further innovation and foster collaborative applications within the broader embodied AI community. The project page is available at https://choiszt.github.io/Octopus/",
    "checked": true,
    "id": "b6c8c1745a18d6e59c7a8a99f0df7aa4c18a1e73",
    "semantic_title": "octopus: embodied vision-language programmer from environmental feedback",
    "citation_count": 29,
    "authors": [
      "Jingkang Yang",
      "Yuhao Dong",
      "Shuai Liu",
      "Bo Li",
      "Ziyue Wang",
      "ChenCheng Jiang",
      "Haoran Tan",
      "Jiamu Kang",
      "Yuanhan Zhang",
      "Kaiyang Zhou",
      "Ziwei Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10_ECCV_2024_paper.php": {
    "title": "FunQA: Towards Surprising Video Comprehension",
    "volume": "main",
    "abstract": "Surprising videos, e.g., funny clips, creative performances, or visual illusions, attract significant attention. Enjoyment of these videos is not simply a response to visual stimuli; rather, it hinges on the human capacity to understand (and appreciate) commonsense violations depicted in these videos. We introduce FunQA, a challenging video question answering (QA) dataset specifically designed to evaluate and enhance the depth of video reasoning based on counter-intuitive and fun videos. Unlike most video QA benchmarks which focus on less surprising contexts, e.g., cooking or instructional videos, FunQA covers three previously unexplored types of surprising videos: 1) HumorQA, 2) CreativeQA, and 3) MagicQA. For each subset, we establish rigorous QA tasks designed to assess the model's capability in counter-intuitive timestamp localization, detailed video description, and reasoning around counter-intuitiveness. We also pose higher-level tasks, such as attributing a fitting and vivid title to the video, and scoring the video creativity. In total, the FunQA benchmark consists of 312K free-text QA pairs derived from 4.3K video clips, spanning a total of 24 video hours. Moreover, we propose FunMentor, an agent designed for Vision-Language Models (VLMs) that uses multi-turn dialogues to enhance models' understanding of counter-intuitiveness. Extensive experiments with existing VLMs demonstrate the effectiveness of FunMentor and reveal significant performance gaps for the FunQA videos across spatial-temporal reasoning, visual-centered reasoning, and free-text generation",
    "checked": true,
    "id": "cbf052125c90f78d8750f7d447121193ef53d2f8",
    "semantic_title": "funqa: towards surprising video comprehension",
    "citation_count": 11,
    "authors": [
      "Binzhu Xie",
      "Sicheng Zhang",
      "Zitang Zhou",
      "Bo Li",
      "Yuanhan Zhang",
      "Jack Hessel",
      "Jingkang Yang",
      "Ziwei Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/19_ECCV_2024_paper.php": {
    "title": "4D Contrastive Superflows are Dense 3D Representation Learners",
    "volume": "main",
    "abstract": "In the realm of autonomous driving, accurate 3D perception is the foundation. However, developing such models relies on extensive human annotations – a process that is both costly and labor-intensive. To address this challenge from a data representation learning perspective, we introduce SuperFlow, a novel framework designed to harness consecutive LiDAR-camera pairs for establishing spatiotemporal pretraining objectives. SuperFlow stands out by integrating two key designs: 1) a dense-to-sparse consistency regularization, which promotes insensitivity to point cloud density variations during feature learning, and 2) a flow-based contrastive learning module, carefully crafted to extract meaningful temporal cues from readily available sensor calibrations. To further boost learning efficiency, we incorporate a plug-and-play view consistency module that enhances the alignment of the knowledge distilled from camera views. Extensive comparative and ablation studies across 11 heterogeneous LiDAR datasets validate our effectiveness and superiority. Additionally, we observe several interesting emerging properties by scaling up the 2D and 3D backbones during pretraining, shedding light on the future research of 3D foundation models for LiDAR-based perception. Code is publicly available at https: //github.com/Xiangxu-0103/SuperFlow",
    "checked": true,
    "id": "7b1e7b2495d4a6ccba363dfc9031249bc3ead699",
    "semantic_title": "4d contrastive superflows are dense 3d representation learners",
    "citation_count": 2,
    "authors": [
      "Xiang Xu*",
      "Lingdong Kong",
      "Hui Shuai",
      "Wenwei Zhang",
      "Liang Pan",
      "Kai Chen",
      "Ziwei Liu",
      "Qingshan Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/22_ECCV_2024_paper.php": {
    "title": "ItTakesTwo: Leveraging Peer Representations for Semi-supervised LiDAR Semantic Segmentation",
    "volume": "main",
    "abstract": "The costly and time-consuming annotation process to produce large training sets for modelling semantic LiDAR segmentation methods has motivated the development of semi-supervised learning (SSL) methods. However, such SSL approaches often concentrate on employing consistency learning only for individual LiDAR representations. This narrow focus results in limited perturbations that generally fail to enable effective consistency learning. Additionally, these SSL approaches employ contrastive learning based on the sampling from a limited set of positive and negative embedding samples. This paper introduces a novel semi-supervised LiDAR semantic segmentation framework called ItTakesTwo (IT2). IT2 is designed to ensure consistent predictions from peer LiDAR representations, thereby improving the perturbation effectiveness in consistency learning. Furthermore, our contrastive learning employs informative samples drawn from a distribution of positive and negative embeddings learned from the entire training set. Results on public benchmarks show that our approach achieves remarkable improvements over the previous state-of-the-art (SOTA) methods in the field. redThe code is available at: https://github.com/yyliu01/IT2",
    "checked": true,
    "id": "5d5158de7def0df3b739af9df7e4c7181ac8e30f",
    "semantic_title": "ittakestwo: leveraging peer representations for semi-supervised lidar semantic segmentation",
    "citation_count": 0,
    "authors": [
      "Yuyuan Liu*",
      "Yuanhong Chen",
      "Hu Wang",
      "Vasileios Belagiannis",
      "Ian Reid",
      "Gustavo Carneiro"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/29_ECCV_2024_paper.php": {
    "title": "Ponymation: Learning Articulated 3D Animal Motions from Unlabeled Online Videos",
    "volume": "main",
    "abstract": "We introduce a new method for learning a generative model of articulated 3D animal motions from raw, unlabeled online videos. Unlike existing approaches for 3D motion synthesis, our model requires no pose annotations or parametric shape models for training; it learns purely from a collection of unlabeled web video clips, leveraging semantic correspondences distilled from self-supervised image features. At the core of our method is a video Photo-Geometric Auto-Encoding framework that decomposes each training video clip into a set of explicit geometric and photometric representations, including a rest-pose 3D shape, an articulated pose sequence, and texture, with the objective of re-rendering the input video via a differentiable renderer. This decomposition allows us to learn a generative model over the underlying articulated pose sequences akin to a Variational Auto-Encoding (VAE) formulation, but without requiring any external pose annotations. At inference time, we can generate new motion sequences by sampling from the learned motion VAE, and create plausible 4D animations of an animal automatically within seconds given a single input image",
    "checked": true,
    "id": "a595fbb4d760022f96cd531ff95e9a814362ee22",
    "semantic_title": "ponymation: learning articulated 3d animal motions from unlabeled online videos",
    "citation_count": 0,
    "authors": [
      "Keqiang Sun",
      "Dor Litvak",
      "Yunzhi Zhang",
      "Hongsheng Li",
      "Jiajun Wu*",
      "Shangzhe Wu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/37_ECCV_2024_paper.php": {
    "title": "Robust Fitting on a Gate Quantum Computer",
    "volume": "main",
    "abstract": "Gate quantum computers generate significant interest due to their potential to solve certain difficult problems such as prime factorization in polynomial time. Computer vision researchers have long been attracted to the power of quantum computers. Robust fitting, which is fundamentally important to many computer vision pipelines, has recently been shown to be amenable to gate quantum computing. The previous proposed solution was to compute Boolean influence as a measure of outlyingness using the Bernstein-Vazirani quantum circuit. However, the method assumed a quantum implementation of an ℓ∞ feasibility test, which has not been demonstrated. In this paper, we take a big stride towards quantum robust fitting: we propose a quantum circuit to solve the ℓ∞ feasibility test in the 1D case, which allows to demonstrate for the first time quantum robust fitting on a real gate quantum computer, the IonQ Aria. We also show how 1D Boolean influences can be accumulated to compute Boolean influences for higher-dimensional non-linear models, which we experimentally validate on real benchmark datasets",
    "checked": true,
    "id": "b6f21d350de6268d1eaa82913d673cde3adeb5e3",
    "semantic_title": "robust fitting on a gate quantum computer",
    "citation_count": 0,
    "authors": [
      "Frances F Yang*",
      "Michele Sasdelli",
      "Tat-Jun Chin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/41_ECCV_2024_paper.php": {
    "title": "H-V2X: A Large Scale Highway Dataset for BEV Perception",
    "volume": "main",
    "abstract": "Vehicle-to-everything (V2X) technology has become an area of interest in research due to the availability of roadside infrastructure perception datasets. However, these datasets primarily focus on urban intersections and lack data on highway scenarios. Additionally, the perception tasks in the datasets are mainly MONO 3D due to limited synchronized data across multiple sensors. To bridge this gap, we propose Highway-V2X (H-V2X), the first large-scale highway Bird's-Eye-View (BEV) perception dataset captured by sensors in the real world. The dataset covers over 100 kilometers of highway, with a diverse range of road and weather conditions. H-V2X consists of over 1.9 million fine-grained categorized samples in BEV space, captured by multiple synchronized cameras, with vector map provided. We performed joint 2D-3D calibrations to ensure correct projection and human labor was involved to ensure data quality. Furthermore, we propose three highly relevant tasks to the highway scenario: BEV detection, BEV tracking, and trajectory prediction. We conducted benchmarks for each task, and innovative methods incorporating vector map information were proposed. We hope that H-V2X and benchmark methods will facilitate highway BEV perception research direction. The dataset is available at https://pan.quark.cn/s/86d19da10d18",
    "checked": true,
    "id": "868ad3a6b78fe11f97ff3569b22a55db65c51d45",
    "semantic_title": "h-v2x: a large scale highway dataset for bev perception",
    "citation_count": 0,
    "authors": [
      "Chang Liu*",
      "MingXu zhu",
      "Cong Ma"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/51_ECCV_2024_paper.php": {
    "title": "Learning Camouflaged Object Detection from Noisy Pseudo Label",
    "volume": "main",
    "abstract": "Existing Camouflaged Object Detection (COD) methods rely heavily on large-scale pixel-annotated training sets, which are both time-consuming and labor-intensive. Although weakly supervised methods offer higher annotation efficiency, their performance is far behind due to the unclear visual demarcations between foreground and background in camouflaged images. In this paper, we explore the potential of using boxes as prompts in camouflaged scenes and introduce the first weakly semi-supervised COD method, aiming for budget-efficient and high-precision camouflaged object segmentation with an extremely limited number of fully labeled images. Critically, learning from such limited set inevitably generates pseudo labels with serious noisy pixels. To address this, we propose a noise correction loss that facilitates the model's learning of correct pixels in the early learning stage, and corrects the error risk gradients dominated by noisy pixels in the memorization stage, ultimately achieving accurate segmentation of camouflaged objects from noisy labels. When using only 20% of fully labeled data, our method shows superior performance over the state-of-the-art methods",
    "checked": true,
    "id": "1c351a5b5489ef8556f7bad69dff45991c0247d9",
    "semantic_title": "learning camouflaged object detection from noisy pseudo label",
    "citation_count": 1,
    "authors": [
      "Jin Zhang*",
      "Ruiheng Zhang*",
      "Yanjiao Shi",
      "Zhe Cao",
      "Nian Liu",
      "Fahad Shahbaz Khan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/55_ECCV_2024_paper.php": {
    "title": "Weakly Supervised 3D Object Detection via Multi-Level Visual Guidance",
    "volume": "main",
    "abstract": "Weakly supervised 3D object detection aims to learn a 3D detector with lower annotation cost, e.g., 2D labels. Unlike prior work which still relies on few accurate 3D annotations, we propose a framework to study how to leverage constraints between 2D and 3D domains without requiring any 3D labels. Specifically, we employ visual data from three perspectives to establish connections between 2D and 3D domains. First, we design a feature-level constraint to align LiDAR and image features based on object-aware regions. Second, the output-level constraint is developed to enforce the overlap between 2D and projected 3D box estimations. Finally, the training-level constraint is utilized by producing accurate and consistent 3D pseudo-labels that align with the visual data. We conduct extensive experiments on the KITTI dataset to validate the effectiveness of the proposed three constraints. Without using any 3D labels, our method achieves favorable performance against state-of-the-art approaches and is competitive with the method that uses 500-frame 3D annotations. Code and models will be made publicly available",
    "checked": true,
    "id": "7aa02182b817cba2e7037a79a411d11b1a346cc2",
    "semantic_title": "weakly supervised 3d object detection via multi-level visual guidance",
    "citation_count": 2,
    "authors": [
      "Kuan-Chih Huang*",
      "Yi-Hsuan Tsai",
      "Ming-Hsuan Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/69_ECCV_2024_paper.php": {
    "title": "Deblur e-NeRF: NeRF from Motion-Blurred Events under High-speed or Low-light Conditions",
    "volume": "main",
    "abstract": "The distinctive design philosophy of event cameras makes them ideal for high-speed, high dynamic range & low-light environments, where standard cameras underperform. However, event cameras also suffer from motion blur, especially under these challenging conditions, contrary to what most think. This is due to the limited bandwidth of the event sensor pixel, which is mostly proportional to the light intensity. Thus, to ensure event cameras can truly excel in such conditions where it has an edge over standard cameras, event motion blur must be accounted for in downstream tasks, especially reconstruction. However, no prior work on reconstructing Neural Radiance Fields (NeRFs) from events, nor event simulators, have considered the full effects of event motion blur. To this end, we propose, Deblur e-NeRF, a novel method to directly and effectively reconstruct blur-minimal NeRFs from motion-blurred events, generated under high-speed or low-light conditions. The core component of this work is a physically-accurate pixel bandwidth model that accounts for event motion blur. We also introduce a threshold-normalized total variation loss to better regularize large textureless patches. Experiments on real & novel realistically simulated sequences verify our effectiveness. Our code, event simulator and synthetic event dataset are open-sourced",
    "checked": true,
    "id": "c3e950220f47dcdaebd596f4258f1e6042958f02",
    "semantic_title": "deblur e-nerf: nerf from motion-blurred events under high-speed or low-light conditions",
    "citation_count": 0,
    "authors": [
      "Weng Fei Low*",
      "Gim Hee Lee"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/72_ECCV_2024_paper.php": {
    "title": "CLR-GAN: Improving GANs Stability and Quality via Consistent Latent Representation and Reconstruction",
    "volume": "main",
    "abstract": "Generative Adversarial Networks(GANs) have received considerable attention due to its outstanding ability to generate images. However, training a GAN is hard since the game between the Generator(G) and the Discriminator(D) is unfair. Towards making the competition fairer, we propose a new perspective of training GANs, named Consistent Latent Representation and Reconstruction(CLR-GAN). In this paradigm, we treat the G and D as an inverse process, the discriminator has an additional task to restore the pre-defined latent code while the generator also needs to reconstruct the real input, thus obtaining a relationship between the latent space of G and the out-features of D. Based on this prior, we can put D and G on an equal position during training using a new criterion. Experimental results on various datasets and architectures prove our paradigm can make GANs more stable and generate better quality images(31.22% gain of FID on CIFAR10 and 39.5% on AFHQ-Cat, respectively). We hope that the proposed perspective can inspire researchers to explore different ways of viewing GANs training, rather than being limited to a two-player game. The code is publicly available at https://github.com/Petecheco/CLR-GAN",
    "checked": true,
    "id": "ce3156cb13297b7f5728d67a76048bca134b360f",
    "semantic_title": "clr-gan: improving gans stability and quality via consistent latent representation and reconstruction",
    "citation_count": 0,
    "authors": [
      "Shengke Sun",
      "Ziqian Luan",
      "Zhanshan Zhao*",
      "Shijie Luo",
      "Shuzhen Han*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/73_ECCV_2024_paper.php": {
    "title": "Learn from the Learnt: Source-Free Active Domain Adaptation via Contrastive Sampling and Visual Persistence",
    "volume": "main",
    "abstract": "Domain Adaptation (DA) facilitates knowledge transfer from a source domain to a related target domain. This paper investigates a practical DA paradigm, namely Source data-Free Active Domain Adaptation (SFADA), where source data becomes inaccessible during adaptation, and a minimum amount of annotation budget is available in the target domain. Without referencing the source data, new challenges emerge in identifying the most informative target samples for labeling, establishing cross-domain alignment during adaptation, and ensuring continuous performance improvements through the iterative query-and-adaptation process. In response, we present learn from the learnt (LFTL), a novel paradigm for SFADA to leverage the learnt knowledge from the source pretrained model and actively iterated models without extra overhead. We propose Contrastive Active Sampling to learn from the hypotheses of the preceding model, thereby querying target samples that are both informative to the current model and persistently challenging throughout active learning. During adaptation, we learn from features of actively selected anchors obtained from previous intermediate models, so that the Visual Persistence-guided Adaptation can facilitate feature distribution alignment and active sample exploitation. Extensive experiments on three widely-used benchmarks show that our LFTL achieves state-of-the-art performance, superior computational efficiency and continuous improvements as the annotation budget increases. Our code is available at https://github.com/lyumengyao/lftl",
    "checked": true,
    "id": "ea2c010f24f955d63489408f1ba951b1b0cfdbd8",
    "semantic_title": "learn from the learnt: source-free active domain adaptation via contrastive sampling and visual persistence",
    "citation_count": 3,
    "authors": [
      "Mengyao Lyu",
      "Tianxiang Hao",
      "Xinhao Xu",
      "Hui Chen*",
      "Zijia Lin",
      "Jungong Han",
      "Guiguang Ding*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/75_ECCV_2024_paper.php": {
    "title": "PromptIQA: Boosting the Performance and Generalization for No-Reference Image Quality Assessment via Prompts",
    "volume": "main",
    "abstract": "Due to the diversity of assessment requirements in various application scenarios for the IQA task, existing IQA methods struggle to directly adapt to these varied requirements after training. Thus, when facing new requirements, a typical approach is fine-tuning these models on datasets specifically created for those requirements. However, it is time-consuming to establish IQA datasets. In this work, we propose a Prompt-based IQA (PromptIQA) that can fast adapt to new requirements without fine-tuning after training. On one hand, it utilizes a short sequence of Image-Score Pairs (ISP) as prompts for targeted predictions, which significantly reduces the dependency on the data requirements. On the other hand, PromptIQA is trained on a mixed dataset with two proposed data augmentation strategies to learn diverse requirements, thus enabling it to fast adapt to new requirements. Experiments indicate that the PromptIQA outperforms SOTA methods with higher performance and better generalization. The code is available at the link",
    "checked": true,
    "id": "5f686a772ae233e0acea26298191251df4022617",
    "semantic_title": "promptiqa: boosting the performance and generalization for no-reference image quality assessment via prompts",
    "citation_count": 4,
    "authors": [
      "Zewen Chen",
      "Haina Qin",
      "Juan Wang",
      "Chunfeng Yuan",
      "Bing Li*",
      "Weiming Hu",
      "Leon Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/100_ECCV_2024_paper.php": {
    "title": "Motion Mamba: Efficient and Long Sequence Motion Generation",
    "volume": "main",
    "abstract": "Human motion generation stands as a significant pursuit in generative computer vision, while achieving long-sequence and efficient motion generation remains challenging. Recent advancements in state space models (SSMs), notably Mamba, have showcased considerable promise in long sequence modeling with an efficient hardware-aware design, which appears to be a significant direction upon building motion generation model. Nevertheless, adapting SSMs to motion generation faces hurdles since the lack of a specialized design architecture to model motion sequence. To address these challenges, we propose Motion Mamba, a simple yet efficient approach that presents the pioneering motion generation model utilized SSMs. Specifically, we design a Hierarchical Temporal Mamba (HTM) block to process temporal data by ensemble varying numbers of isolated SSM modules across a symmetric U-Net architecture aimed at preserving motion consistency between frames. We also design a Bidirectional Spatial Mamba (BSM) block to bidirectionally process latent poses, to enhance accurate motion generation within a temporal frame. Our proposed method achieves up to 50% FID improvement and up to 4 times faster on the HumanML3D and KIT-ML datasets compared to the previous best diffusion-based method, which demonstrates strong capabilities of high-quality long sequence motion modeling and real-time human motion generation",
    "checked": false,
    "id": "b9646f057887825d7471ec01664494b0b7ca5a83",
    "semantic_title": "motion mamba: efficient and long sequence motion generation with hierarchical and bidirectional selective ssm",
    "citation_count": 26,
    "authors": [
      "Zeyu Zhang",
      "Akide Liu",
      "Ian Reid",
      "RICHARD HARTLEY",
      "Bohan Zhuang",
      "Hao Tang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/111_ECCV_2024_paper.php": {
    "title": "Radiative Gaussian Splatting for Efficient X-ray Novel View Synthesis",
    "volume": "main",
    "abstract": "X-ray is widely applied for transmission imaging due to its stronger penetration than natural light. When rendering novel view X-ray projections, existing methods mainly based on NeRF suffer from long training time and slow inference speed. In this paper, we propose a 3D Gaussian splatting-based method, namely X-Gaussian, for X-ray novel view synthesis. Firstly, we redesign a radiative Gaussian point cloud model inspired by the isotropic nature of X-ray imaging. Our model excludes the influence of view direction when learning to predict the radiation intensity of 3D points. Based on this model, we develop a Differentiable Radiative Rasterization (DRR) with CUDA implementation. Secondly, we customize an Angle-pose Cuboid Uniform Initialization (ACUI) strategy that directly uses the parameters of the X-ray scanner to compute the camera information and then uniformly samples point positions within a cuboid enclosing the scanned object. Experiments show that our X-Gaussian outperforms state-of-the-art methods by 6.5 dB while enjoying less than 15% training time and over 73× inference speed. The application on CT reconstruction also reveals the practical values of our method. Code is at https://github.com/caiyuanhao1998/X-Gaussian",
    "checked": true,
    "id": "e00fa03ef1ec8f082a63a50ed2d6741377eb3771",
    "semantic_title": "radiative gaussian splatting for efficient x-ray novel view synthesis",
    "citation_count": 9,
    "authors": [
      "Yuanhao Cai*",
      "Yixun Liang",
      "Jiahao Wang",
      "Angtian Wang",
      "Yulun Zhang",
      "Xiaokang Yang",
      "Zongwei Zhou",
      "Alan Yuille"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/113_ECCV_2024_paper.php": {
    "title": "Tracking Meets LoRA: Faster Training, Larger Model, Stronger Performance",
    "volume": "main",
    "abstract": "Motivated by the Parameter-Efficient Fine-Tuning (PEFT) in large language models, we propose LoRAT, a method that unveils the power of larger Vision Transformers (ViT) for tracking within laboratory-level resources. The essence of our work lies in adapting LoRA, a technique that fine-tunes a small subset of model parameters without adding inference latency, to the domain of visual tracking. However, unique challenges and potential domain gaps make this transfer not as easy as the first intuition. Firstly, a transformer-based tracker constructs unshared position embedding for template and search image. This poses a challenge for the transfer of LoRA, usually requiring consistency in the design when applied to the pre-trained backbone, to downstream tasks. Secondly, the inductive bias inherent in convolutional heads diminishes the effectiveness of parameter-efficient fine-tuning in tracking models. To overcome these limitations, we first decouple the position embeddings in transformer-based trackers into shared spatial ones and independent type ones. The shared embeddings, which describe the absolute coordinates of multi-resolution images (namely, the template and search images), are inherited from the pre-trained backbones. In contrast, the independent embeddings indicate the sources of each token and are learned from scratch. Furthermore, we design an anchor-free head solely based on a multilayer perceptron (MLP) to adapt PETR, enabling better performance with less computational overhead. With our design, 1) it becomes practical to train trackers with the ViT-g backbone on GPUs with only memory of 25.8GB (batch size of 16); 2) we reduce the training time of the L-224 variant from 35.0 to 10.8 GPU hours; 3) we improve the LaSOT SUC score from 0.703 to 0.742 with the L-224 variant; 4) we fast the inference speed of the L-224 variant from 52 to 119 FPS. Code and models are available at https://github.com/LitingLin/LoRAT",
    "checked": true,
    "id": "eb2c30897dd1cf01f5a1d31ae402eb641a9123a1",
    "semantic_title": "tracking meets lora: faster training, larger model, stronger performance",
    "citation_count": 3,
    "authors": [
      "Liting Lin",
      "Heng Fan",
      "Zhipeng Zhang",
      "Yaowei Wang*",
      "Yong Xu",
      "Haibin Ling*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/126_ECCV_2024_paper.php": {
    "title": "A Direct Approach to Viewing Graph Solvability",
    "volume": "main",
    "abstract": "The viewing graph is a useful way to represent uncalibrated cameras and their geometric relationships: nodes correspond to cameras and edges represent fundamental matrices. By analyzing this graph, it is possible to establish if the problem is \"solvable\" in the sense that there exists a unique (up to a single projective transformation) set of cameras that are compliant with the given fundamental matrices. In this paper, we take several steps forward in the study of viewing graph solvability: we propose a new formulation of the problem that is more direct than previous literature, based on a formula that explicitly links pairs of cameras via their fundamental matrix; we introduce the new concept of \"\", demonstrating its usefulness in understanding real structure from motion graphs; we propose an algorithm for testing and extracting components of unsolvable cases, that is more efficient than previous work; we set up an open question on the connection between and solvability",
    "checked": true,
    "id": "856cc53455bb9d843c8859cdaf56b794809a9798",
    "semantic_title": "a direct approach to viewing graph solvability",
    "citation_count": 0,
    "authors": [
      "Federica Arrigoni*",
      "Andrea Fusiello",
      "Tomas Pajdla"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/139_ECCV_2024_paper.php": {
    "title": "CoR-GS: Sparse-View 3D Gaussian Splatting via Co-Regularization",
    "volume": "main",
    "abstract": "3D Gaussian Splatting (3DGS) creates a radiance field consisting of 3D Gaussians to represent a scene. With sparse training views, 3DGS easily suffers from overfitting, negatively impacting rendering. This paper introduces a new co-regularization perspective for improving sparse-view 3DGS. When training two 3D Gaussian radiance fields, we observe that the two radiance fields exhibit point disagreement and rendering disagreement that can unsupervisedly predict reconstruction quality, stemming from the randomness of densification implementation. We further quantify the two disagreements and demonstrate the negative correlation between them and accurate reconstruction, which allows us to identify inaccurate reconstruction without accessing ground-truth information. Based on the study, we propose CoR-GS, which identifies and suppresses inaccurate reconstruction based on the two disagreements: (i) Co-pruning considers Gaussians that exhibit high point disagreement in inaccurate positions and prunes them. (ii) Pseudo-view co-regularization considers pixels that exhibit high rendering disagreement are inaccurate and suppress the disagreement. Results on LLFF, Mip-NeRF360, DTU, and Blender demonstrate that CoR-GS effectively regularizes the scene geometry, reconstructs the compact representations, and achieves state-of-the-art novel view synthesis quality under sparse training views. Project page: https: //jiaw-z.github.io/CoR-GS",
    "checked": true,
    "id": "58d7c088a9cc9697cffb9397258ac9e457b08410",
    "semantic_title": "cor-gs: sparse-view 3d gaussian splatting via co-regularization",
    "citation_count": 7,
    "authors": [
      "Jiawei Zhang",
      "Jiahe Li",
      "Xiaohan Yu",
      "Lei Huang",
      "Lin Gu",
      "Jin Zheng*",
      "Xiao Bai*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/143_ECCV_2024_paper.php": {
    "title": "SeFlow: A Self-Supervised Scene Flow Method in Autonomous Driving",
    "volume": "main",
    "abstract": "Scene flow estimation predicts the 3D motion at each point in successive LiDAR scans. This detailed, point-level, information can help autonomous vehicles to accurately predict and understand dynamic changes in their surroundings. Current state-of-the-art methods require annotated data to train scene flow networks and the expense of labeling inherently limits their scalability. Self-supervised approaches can overcome the above limitations, yet face two principal challenges that hinder optimal performance: point distribution imbalance and disregard for object-level motion constraints. In this paper, we propose SeFlow, a self-supervised method that integrates efficient dynamic classification into a learning-based scene flow pipeline. We demonstrate that classifying static and dynamic points helps design targeted objective functions for different motion patterns. We also emphasize the importance of internal cluster consistency and correct object point association to refine the scene flow estimation, in particular on object details. Our real-time capable method achieves state-of-the-art performance on the self-supervised scene flow task on Argoverse 2 and Waymo datasets. The code is open-sourced at https://github.com/KTH-RPL/SeFlow",
    "checked": true,
    "id": "143a44051a8542585339f1f7ae08c2c1b457e745",
    "semantic_title": "seflow: a self-supervised scene flow method in autonomous driving",
    "citation_count": 4,
    "authors": [
      "Qingwen Zhang*",
      "Yi Yang",
      "Peizheng Li",
      "Olov Andersson",
      "Patric Jensfelt"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/144_ECCV_2024_paper.php": {
    "title": "ZeST: Zero-Shot Material Transfer from a Single Image",
    "volume": "main",
    "abstract": "We propose , a method for zero-shot material transfer to an object in the input image given a material exemplar image. leverages existing diffusion adapters to extract implicit material representation from the exemplar image. This representation is used to transfer the material using pre-trained inpainting diffusion model on the object in the input image using depth estimates as geometry cue and grayscale object shading as illumination cues. The method works on real images without any training resulting a zero-shot approach. Both qualitative and quantitative results on real and synthetic datasets demonstrate that outputs photorealistic images with transferred materials. We also show the application of to perform multiple edits and robust material assignment under different illuminations. Project Page: https://ttchengab.github.io/zest",
    "checked": true,
    "id": "8d36accc2237a5800de8b2f6af438beb6de1c207",
    "semantic_title": "zest: zero-shot material transfer from a single image",
    "citation_count": 1,
    "authors": [
      "Ta-Ying Cheng",
      "Prafull Sharma",
      "Andrew Markham",
      "Niki Trigoni",
      "Varun Jampani*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/145_ECCV_2024_paper.php": {
    "title": "3D Congealing: 3D-Aware Image Alignment in the Wild",
    "volume": "main",
    "abstract": "We propose , a novel problem of 3D-aware alignment for 2D images capturing semantically similar objects. Given a collection of unlabeled Internet images, our goal is to associate the shared semantic parts from the inputs and aggregate the knowledge from 2D images to a shared 3D canonical space. We introduce a general framework that tackles the task without assuming shape templates, poses, or any camera parameters. At its core is a canonical 3D representation that encapsulates geometric and semantic information. The framework optimizes for the canonical representation together with the pose for each input image, and a per-image coordinate map that warps 2D pixel coordinates to the 3D canonical frame to account for the shape matching. The optimization procedure fuses prior knowledge from a pre-trained image generative model and semantic information from input images. The former provides strong knowledge guidance for this under-constraint task, while the latter provides the necessary information to mitigate the training data bias from the pre-trained model. Our framework can be used for various tasks such as pose estimation and image editing, achieving strong results on real-world image datasets under challenging illumination conditions and on in-the-wild online image collections. Project page at https://ai.stanford. edu/~yzzhang/projects/3d-congealing/",
    "checked": true,
    "id": "c9385ded43b296ae78bceb05be6462293c4e4fc6",
    "semantic_title": "3d congealing: 3d-aware image alignment in the wild",
    "citation_count": 0,
    "authors": [
      "Yunzhi Zhang*",
      "Zizhang Li",
      "Amit Raj",
      "Andreas Engelhardt",
      "Yuanzhen Li",
      "Tingbo Hou",
      "Jiajun Wu",
      "Varun Jampani"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/147_ECCV_2024_paper.php": {
    "title": "SMooDi: Stylized Motion Diffusion Model",
    "volume": "main",
    "abstract": "We introduce a novel Stylized Motion Diffusion model, dubbed , to generate stylized motion driven by content texts and style motion sequences. Unlike existing methods that either generate motion of various content or transfer style from one sequence to another, can rapidly generate motion across a broad range of content and diverse styles. To this end, we tailor a pre-trained text-to-motion model for stylization. Specifically, we propose style guidance to ensure that the generated motion closely matches the reference style, alongside a lightweight style adaptor that directs the motion towards the desired style while ensuring realism. Experiments across various applications demonstrate that our proposed framework outperforms existing methods in stylized motion generation. Project Page: https://neu-vi.github.io/SMooDi/",
    "checked": true,
    "id": "42b3728c96b2ad1ffe702ed5c692f35037d61690",
    "semantic_title": "smoodi: stylized motion diffusion model",
    "citation_count": 4,
    "authors": [
      "Lei Zhong",
      "Yiming Xie",
      "Varun Jampani",
      "Deqing Sun",
      "Huaizu Jiang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/148_ECCV_2024_paper.php": {
    "title": "ZipLoRA: Any Subject in Any Style by Effectively Merging LoRAs",
    "volume": "main",
    "abstract": "Methods for finetuning generative models for concept-driven personalization generally achieve strong results for subject-driven or style-driven generation. Recently, low-rank adaptations () have been proposed as a parameter-efficient way of achieving concept-driven personalization. While recent work explores the combination of separate LoRAs to achieve joint generation of learned styles and subjects, existing techniques do not reliably address the problem, so that either subject fidelity or style fidelity are compromised. We propose , a method to cheaply and effectively merge independently trained style and subject LoRAs in order to achieve generation of any user-provided subject in any user-provided style. Experiments on a wide range of subject and style combinations show that can generate compelling results with meaningful improvements over baselines in subject and style fidelity while preserving the ability to recontextualize",
    "checked": true,
    "id": "185e88645dfc07d6ca81a55dfc66bd3452400276",
    "semantic_title": "ziplora: any subject in any style by effectively merging loras",
    "citation_count": 55,
    "authors": [
      "Viraj Shah",
      "Nataniel Ruiz",
      "Forrester Cole",
      "Erika Lu",
      "Svetlana Lazebnik",
      "Yuanzhen Li",
      "Varun Jampani*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/150_ECCV_2024_paper.php": {
    "title": "SV3D: Novel Multi-view Synthesis and 3D Generation from a Single Image using Latent Video Diffusion",
    "volume": "main",
    "abstract": "We present Stable Video 3D (SV3D) — a latent video diffusion model for high-resolution, image-to-multi-view generation of orbital videos around a 3D object. Recent works propose to adapt 2D generative models for novel view synthesis (NVS) and 3D optimization. However, these methods have several disadvantages due to limited views or inconsistent NVS, affecting the performance of 3D object generation. In this work, we propose SV3D that adapts image-to-video diffusion model for novel multi-view synthesis and 3D generation, thereby leveraging the generalization and multi-view consistency of the video models, while further adding explicit camera control for NVS. We also propose improved 3D optimization techniques for image-to-3D generation using SV3D and its NVS outputs. Extensive experiments on multiple datasets with 2D and 3D metrics and user study demonstrate SV3D's state-of-the-art performance on NVS as well as 3D reconstruction compared to prior works",
    "checked": true,
    "id": "03e2cfa44b64489fb98f09dfbd940043fbef90ad",
    "semantic_title": "sv3d: novel multi-view synthesis and 3d generation from a single image using latent video diffusion",
    "citation_count": 67,
    "authors": [
      "Vikram Voleti*",
      "Chun-Han Yao",
      "Mark Boss",
      "Adam Letts",
      "David Pankratz",
      "Dmitrii Tochilkin",
      "Christian Laforte",
      "Robin Rombach",
      "Varun Jampani*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/151_ECCV_2024_paper.php": {
    "title": "WordRobe: Text-Guided Generation of Textured 3D Garments",
    "volume": "main",
    "abstract": "In this paper, we tackle a new and challenging problem of text-driven generation of 3D garments with high-quality textures. We propose, WordRobe, a novel framework for the generation of unposed & textured 3D garment meshes from user-friendly text prompts. We achieve this by first learning a latent representation of 3D garments using a novel coarse-to-fine training strategy and a loss for latent disentanglement, promoting better latent interpolation. Subsequently, we align the garment latent space to the CLIP embedding space in a weakly supervised manner, enabling text-driven 3D garment generation and editing. For appearance modeling, we leverage the zero-shot generation capability of ControlNet to synthesize view-consistent texture maps in a single feed-forward inference step, thereby drastically decreasing the generation time as compared to existing methods. We demonstrate superior performance over current SOTAs for learning 3D garment latent space, garment interpolation, and text-driven texture synthesis, supported by quantitative evaluation and qualitative user study. The unposed 3D garment meshes generated using WordRobe can be directly fed to standard cloth simulation & animation pipelines without any post-processing",
    "checked": true,
    "id": "a19f249557e1a0b2a8b6388645fb88892f0c3887",
    "semantic_title": "wordrobe: text-guided generation of textured 3d garments",
    "citation_count": 4,
    "authors": [
      "Astitva Srivastava*",
      "Pranav Manu",
      "Amit Raj",
      "Varun Jampani",
      "Avinash Sharma"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/159_ECCV_2024_paper.php": {
    "title": "Learning to Generate Conditional Tri-plane for 3D-aware Expression Controllable Portrait Animation",
    "volume": "main",
    "abstract": "In this paper, we present , a one-shot 3D-aware portrait animation method that is able to control the facial expression and camera view of a given portrait image. To achieve this, we introduce a tri-plane generator with an effective expression conditioning method, which directly generates a tri-plane of 3D prior by transferring the expression parameter of 3DMM into the source image. The tri-plane is then decoded into the image of different view through a differentiable volume rendering. Existing portrait animation methods heavily rely on image warping to transfer the expression in the motion space, challenging on disentanglement of appearance and expression. In contrast, we propose a contrastive pre-training framework for appearance-free expression parameter, eliminating undesirable appearance swap when transferring a cross-identity expression. Extensive experiments show that our pre-training framework can learn the appearance-free expression representation hidden in 3DMM, and our model can generate 3D-aware expression controllable portrait images without appearance swap in the cross-identity manner",
    "checked": true,
    "id": "87dae5e837789163fe5202c939adf226b5ce5a73",
    "semantic_title": "learning to generate conditional tri-plane for 3d-aware expression controllable portrait animation",
    "citation_count": 2,
    "authors": [
      "Taekyung Ki*",
      "Dongchan Min",
      "Gyeongsu Chae*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/161_ECCV_2024_paper.php": {
    "title": "SimPB: A Single Model for 2D and 3D Object Detection from Multiple Cameras",
    "volume": "main",
    "abstract": "The field of autonomous driving has attracted considerable interest in approaches that directly infer 3D objects in the Bird's Eye View (BEV) from multiple cameras. Some attempts have also explored utilizing 2D detectors from single images to enhance the performance of 3D detection. However, these approaches rely on a two-stage process with separate detectors, where the 2D detection results are utilized only once for token selection or query initialization. In this paper, we present a single model termed SimPB, which Simultaneously detects 2D objects in the Perspective view and 3D objects in the BEV space from multiple cameras. To achieve this, we introduce a hybrid decoder consisting of several multi-view 2D decoder layers and several 3D decoder layers, specifically designed for their respective detection tasks. A Dynamic Query Allocation module and an Adaptive Query Aggregation module are proposed to continuously update and refine the interaction between 2D and 3D results, in a cyclic 3D-2D-3D manner. Additionally, Query-group Attention is utilized to strengthen the interaction among 2D queries within each camera group. In the experiments, we evaluate our method on the nuScenes dataset and demonstrate promising results for both 2D and 3D detection tasks. Our code is available at: https: //github.com/nullmax-vision/SimPB",
    "checked": true,
    "id": "88880b6ca5b975ef124cb0b2e05e1bbed7d45f3b",
    "semantic_title": "simpb: a single model for 2d and 3d object detection from multiple cameras",
    "citation_count": 0,
    "authors": [
      "Yingqi Tang",
      "Zhaotie Meng",
      "Guoliang Chen",
      "Erkang Cheng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/168_ECCV_2024_paper.php": {
    "title": "EMDM: Efficient Motion Diffusion Model for Fast, High-Quality Human Motion Generation",
    "volume": "main",
    "abstract": "We introduce Efficient Motion Diffusion Model (EMDM) for fast and high-quality human motion generation. Current state-of-the-art generative diffusion models have produced impressive results but struggle to achieve fast generation without sacrificing quality. On the one hand, previous works, like motion latent diffusion, conduct diffusion within a latent space for efficiency, but learning such a latent space can be a non-trivial effort. On the other hand, accelerating generation by naively increasing the sampling step size, e.g., DDIM, often leads to quality degradation as it fails to approximate the complex denoising distribution. To address these issues, we propose EMDM, which captures the complex distribution during multiple sampling steps in the diffusion model, allowing for much fewer sampling steps and significant acceleration in generation. This is achieved by a conditional denoising diffusion GAN to capture multimodal data distributions among arbitrary (and potentially larger) step sizes conditioned on control signals, enabling fewer-step motion sampling with high fidelity and diversity. To minimize undesired motion artifacts, geometric losses are imposed during network learning. As a result, EMDM achieves real-time motion generation and significantly improves the efficiency of motion diffusion models compared to existing methods while achieving high-quality motion generation. Our code is available at https: //github.com/Frank-ZY-Dou/EMDM",
    "checked": false,
    "id": "24b7abcd5a876146d613cbca0ad5e09d9851b259",
    "semantic_title": "emdm: efficient motion diffusion model for fast, high-quality motion generation",
    "citation_count": 19,
    "authors": [
      "Wenyang Zhou",
      "Zhiyang Dou*",
      "Zeyu Cao",
      "Zhouyingcheng Liao",
      "Jingbo Wang",
      "Wenjia Wang",
      "Yuan Liu",
      "Taku Komura",
      "Wenping Wang",
      "Lingjie Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/175_ECCV_2024_paper.php": {
    "title": "Editable Image Elements for Controllable Synthesis",
    "volume": "main",
    "abstract": "Diffusion models have made significant advances in text-guided synthesis tasks. However, editing user-provided images remains challenging, as the high dimensional noise input space of diffusion models is not naturally suited for image inversion or spatial editing. In this work, we propose an image representation that promotes spatial editing of input images using a diffusion model. Concretely, we learn to encode an input into \"image elements\" that can faithfully reconstruct an input image. These elements can be intuitively edited by a user, and are decoded by a diffusion model into realistic images. We show the effectiveness of our representation on various image editing tasks, such as object resizing, rearrangement, dragging, de-occlusion, removal, variation, and image composition",
    "checked": true,
    "id": "a9e3061b0948084d8af6197b38f9442d0c1882ed",
    "semantic_title": "editable image elements for controllable synthesis",
    "citation_count": 3,
    "authors": [
      "Jiteng Mu*",
      "Michaël Gharbi",
      "Richard Zhang",
      "Eli Shechtman",
      "Nuno Vasconcelos",
      "Xiaolong Wang",
      "Taesung Park*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/176_ECCV_2024_paper.php": {
    "title": "Improving 2D Feature Representations by 3D-Aware Fine-Tuning",
    "volume": "main",
    "abstract": "Current visual foundation models are trained purely on unstructured 2D data, limiting their understanding of 3D structure of objects and scenes. In this work, we show that fine-tuning on 3D-aware data improves the quality of emerging semantic features. We design a method to lift semantic 2D features into an efficient 3D Gaussian representation, which allows us to re-render them for arbitrary views. Using the rendered 3D-aware features, we design a fine-tuning strategy to transfer such 3D awareness into a 2D foundation model. We demonstrate that models fine-tuned in that way produce features that readily improve downstream task performance in semantic segmentation and depth estimation through simple linear probing. Notably, though fined-tuned on a single indoor dataset, the improvement is transferable to a variety of indoor datasets and out-of-domain datasets. We hope our study encourages the community to consider injecting 3D awareness when training 2D foundation models. Project page: https://ywyue.github.io/FiT3D",
    "checked": true,
    "id": "270eff2aac446af35c64ed0efbb7e57fc8f859f0",
    "semantic_title": "improving 2d feature representations by 3d-aware fine-tuning",
    "citation_count": 1,
    "authors": [
      "Yuanwen Yue*",
      "Anurag Das",
      "Francis Engelmann",
      "Siyu Tang",
      "Jan Eric Lenssen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/180_ECCV_2024_paper.php": {
    "title": "Self-supervised Feature Adaptation for 3D Industrial Anomaly Detection",
    "volume": "main",
    "abstract": "Industrial anomaly detection is generally addressed as an unsupervised task that aims at locating defects with only normal training samples. Recently, numerous 2D anomaly detection methods have been proposed and have achieved promising results, however, using only the 2D RGB data as input is not sufficient to identify imperceptible geometric surface anomalies. Hence, in this work, we focus on multi-modal anomaly detection. Specifically, we investigate early multi-modal approaches that attempted to utilize models pre-trained on large-scale visual datasets, i.e., ImageNet, to construct feature databases. And we empirically find that directly using these pre-trained models is not optimal, it can either fail to detect subtle defects or mistake abnormal features as normal ones. This may be attributed to the domain gap between target industrial data and source data. Towards this problem, we propose a Local-to-global Self-supervised Feature Adaptation (LSFA) method to finetune the adaptors and learn task-oriented representation toward anomaly detection. Both intra-modal adaptation and cross-modal alignment are optimized from a local-to-global perspective in LSFA to ensure the representation quality and consistency in the inference stage. Extensive experiments demonstrate that our method not only brings a significant performance boost to feature embedding based approaches, but also outperforms previous State-of-The-Art (SoTA) methods prominently on both MVTec-3D AD and Eyecandies datasets, e.g., LSFA achieves 97.1% I-AUROC on MVTec-3D, surpass previous SoTA by +3.4%. Code is available at https://github.com/ yuanpengtu/LSFA",
    "checked": true,
    "id": "023d614a25a0bfa48cfe6574a9fa6b29f68b09df",
    "semantic_title": "self-supervised feature adaptation for 3d industrial anomaly detection",
    "citation_count": 2,
    "authors": [
      "Yuanpeng Tu",
      "Boshen Zhang",
      "Liang Liu",
      "YUXI LI",
      "Jiangning Zhang",
      "Yabiao Wang*",
      "Chengjie Wang",
      "cairong zhao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/187_ECCV_2024_paper.php": {
    "title": "PCF-Lift: Panoptic Lifting by Probabilistic Contrastive Fusion",
    "volume": "main",
    "abstract": "Panoptic lifting is an effective technique to address the 3D panoptic segmentation task by unprojecting 2D panoptic segmentations from multi-views to 3D scene. However, the quality of its results largely depends on the 2D segmentations, which could be noisy and error-prone, so its performance often drops significantly for complex scenes. In this work, we design a new pipeline coined PCF-Lift based on our Probabilis-tic Contrastive Fusion (PCF) to learn and embed probabilistic features throughout our pipeline to actively consider inaccurate segmentations and inconsistent instance IDs. Technical-wise, we first model the probabilistic feature embeddings through multivariate Gaussian distributions. To fuse the probabilistic features, we incorporate the probability product kernel into the contrastive loss formulation and design a cross-view constraint to enhance the feature consistency across different views. For the inference, we introduce a new probabilistic clustering method to effectively associate prototype features with the underlying 3D object instances for the generation of consistent panoptic segmentation results. Further, we provide a theoretical analysis to justify the superiority of the proposed probabilistic solution. By conducting extensive experiments, our PCF-lift not only significantly outperforms the state-of-the-art methods on widely used benchmarks including the ScanNet dataset and the challenging Messy Room dataset (4.4% improvement of scene-level PQ), but also demonstrates strong robustness when incorporating various 2D segmentation models or different levels of hand-crafted noise",
    "checked": true,
    "id": "fe9705d25869a12ee0bd6d89ca61ea09e4fc909f",
    "semantic_title": "pcf-lift: panoptic lifting by probabilistic contrastive fusion",
    "citation_count": 0,
    "authors": [
      "Runsong Zhu*",
      "Shi Qiu*",
      "Qianyi Wu",
      "Ka-Hei Hui",
      "Pheng-Ann Heng",
      "Chi-Wing Fu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/193_ECCV_2024_paper.php": {
    "title": "SemGrasp: Semantic Grasp Generation via Language Aligned Discretization",
    "volume": "main",
    "abstract": "Generating natural human grasps necessitates consideration of not just object geometry but also semantic information. Solely depending on object shape for grasp generation confines the applications of prior methods in downstream tasks. This paper presents a novel semantic-based grasp generation method, termed , which generates a static human grasp pose by incorporating semantic information into the grasp representation. We introduce a discrete representation that aligns the grasp space with semantic space, enabling the generation of grasp postures in accordance with language instructions. A Multimodal Large Language Model (MLLM) is subsequently fine-tuned, integrating object, grasp, and language within a unified semantic space. To facilitate the training of , we compile a large-scale, grasp-text-aligned dataset named , featuring over 300k detailed captions and 50k diverse grasps. Experimental findings demonstrate that efficiently generates natural human grasps in alignment with linguistic intentions. Our code, models, and dataset will be made publicly available.Our code, models, and dataset are available publicly at:",
    "checked": true,
    "id": "dd7b2817d54c20e6c0bf854b3463c17bb18014a0",
    "semantic_title": "semgrasp: semantic grasp generation via language aligned discretization",
    "citation_count": 6,
    "authors": [
      "Kailin Li*",
      "Jingbo Wang",
      "Lixin Yang",
      "Cewu Lu*",
      "Bo Dai"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/194_ECCV_2024_paper.php": {
    "title": "MANIKIN: Biomechanically Accurate Neural Inverse Kinematics for Human Motion Estimation",
    "volume": "main",
    "abstract": "Mixed Reality systems aim to estimate a user's full-body joint configurations from just the pose of the end effectors, primarily head and hand poses. Existing methods often involve solving inverse kinematics (IK) to obtain the full skeleton from just these sparse observations, usually directly optimizing the joint angle parameters of a human skeleton. Since this accumulates error through the kinematic tree, predicted end effector poses fail to align with the provided input pose. This leads to discrepancies between the predicted and the actual hand positions or feet that penetrate the ground. In this paper, we first refine the commonly used SMPL parametric model by embedding anatomical constraints that reduce the degrees of freedom for specific parameters to more closely mirror human biomechanics. This ensures that our model produces physically plausible pose predictions. We then propose a biomechanically accurate neural inverse kinematics solver () for full-body motion tracking. is based on swivel angle prediction and perfectly matches input poses while avoiding ground penetration. We evaluate in extensive experiments on motion capture datasets and demonstrate that our method surpasses the state of the art in quantitative and qualitative results at fast inference speed",
    "checked": true,
    "id": "5bc6dd6f25e5635b77a609e5a63b70384c6318da",
    "semantic_title": "manikin: biomechanically accurate neural inverse kinematics for human motion estimation",
    "citation_count": 1,
    "authors": [
      "Jiaxi Jiang*",
      "Paul Streli",
      "Xuejing Luo",
      "Christoph Gebhardt",
      "Christian Holz"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/195_ECCV_2024_paper.php": {
    "title": "Simple Unsupervised Knowledge Distillation With Space Similarity",
    "volume": "main",
    "abstract": "As per recent studies, Self-supervised learning (SSL) does not readily extend to smaller architectures. One direction to mitigate this shortcoming while simultaneously training a smaller network without labels is to adopt unsupervised knowledge distillation (UKD). Existing UKD approaches handcraft preservation worthy inter/intra sample relationships between the teacher and its student. However, this may overlook/ignore other key relationships present in the mapping of a teacher. In this paper, instead of heuristically constructing preservation worthy relationships between samples, we directly motivate the student to model the teacher's embedding manifold. If the mapped manifold is similar, all inter/intra sample relationships are indirectly conserved. We first demonstrate that prior methods cannot preserve teacher's latent manifold due to their sole reliance on L2 normalised embedding features. Subsequently, we propose a simple objective to capture the lost information due to normalisation. Our proposed loss component, termed space similarity, motivates each dimension of a student's feature space to be similar to the corresponding dimension of its teacher. We perform extensive experiments demonstrating strong performance of our proposed approach on various benchmarks",
    "checked": true,
    "id": "91e72a77b65449fde2baacbaff566117eceb5208",
    "semantic_title": "simple unsupervised knowledge distillation with space similarity",
    "citation_count": 0,
    "authors": [
      "Aditya Singh*",
      "Haohan Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/201_ECCV_2024_paper.php": {
    "title": "DragAPart: Learning a Part-Level Motion Prior for Articulated Objects",
    "volume": "main",
    "abstract": "We introduce , a method that, given an image and a set of drags as input, generates a new image of the same object that responds to the action of the drags. Differently from prior works that focused on repositioning objects, predicts part-level interactions, such as opening and closing a drawer. We study this problem as a proxy for learning a generalist motion model, not restricted to a specific kinematic structure or object category. We start from a pre-trained image generator and fine-tune it on a new synthetic dataset, , which we introduce. Combined with a new encoding for the drags and dataset randomization, the model generalizes well to real images and different categories. Compared to prior motion-controlled generators, we demonstrate much better part-level motion understanding",
    "checked": true,
    "id": "c91b2c65da5e50df129b877da57ec07bb7f9c363",
    "semantic_title": "dragapart: learning a part-level motion prior for articulated objects",
    "citation_count": 7,
    "authors": [
      "Ruining Li*",
      "Chuanxia Zheng",
      "Christian Rupprecht",
      "Andrea Vedaldi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/203_ECCV_2024_paper.php": {
    "title": "Diffusion Bridges for 3D Point Cloud Denoising",
    "volume": "main",
    "abstract": "In this work, we address the task of point cloud denoising using a novel framework adapting Diffusion Schrödinger bridges to unstructured data like point sets. Unlike previous works that predict point-wise displacements from point features or learned noise distributions, our method learns an optimal transport plan between paired point clouds. In experiments on object datasets such as the PU-Net dataset and real-world datasets like ScanNet++ and ARKitScenes, improves by a notable margin over existing methods. Although our method demonstrates promising results utilizing solely point coordinates, we demonstrate that incorporating additional features like RGB information and point-wise DINOV2 features further improves the results.Code and pretrained networks are available at https://github.com/matvogel/P2P-Bridge",
    "checked": false,
    "id": "6e872cdad76830c2dbc67696cd82563010d8f09b",
    "semantic_title": "p2p-bridge: diffusion bridges for 3d point cloud denoising",
    "citation_count": 0,
    "authors": [
      "Mathias Vogel Hüni",
      "Keisuke Tateno",
      "Marc Pollefeys",
      "Federico Tombari",
      "Marie-Julie Rakotosaona",
      "Francis Engelmann*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/206_ECCV_2024_paper.php": {
    "title": "Optimizing Illuminant Estimation in Dual-Exposure HDR Imaging",
    "volume": "main",
    "abstract": "High dynamic range (HDR) imaging involves capturing a series of frames of the same scene, each with different exposure settings, to broaden the dynamic range of light. This can be achieved through burst capturing or using staggered HDR sensors that capture long and short exposures simultaneously in the camera image signal processor (ISP). Within camera ISP pipeline, illuminant estimation is a crucial step aiming to estimate the color of the global illuminant in the scene. This estimation is used in camera ISP white-balance module to remove undesirable color cast in the final image. Despite the multiple frames captured in the HDR pipeline, conventional illuminant estimation methods often rely only on a single frame of the scene. In this paper, we explore leveraging information from frames captured with different exposure times. Specifically, we introduce a simple feature extracted from dual-exposure images to guide illuminant estimators, referred to as the dual-exposure feature (DEF). To validate the efficiency of DEF, we employed two illuminant estimators using the proposed DEF: 1) a multilayer perceptron network (MLP), referred to as exposure-based MLP (EMLP), and 2) a modified version of the convolutional color constancy (CCC) to integrate our DEF, that we call ECCC. Both EMLP and ECCC achieve promising results, in some cases surpassing prior methods that require hundreds of thousands or millions of parameters, with only a few hundred parameters for EMLP and a few thousand parameters for ECCC",
    "checked": true,
    "id": "7416437f4b15f4ee6ec175074eb5954b6895f48a",
    "semantic_title": "optimizing illuminant estimation in dual-exposure hdr imaging",
    "citation_count": 0,
    "authors": [
      "Mahmoud Afifi*",
      "Zhenhua Hu",
      "Liang Liang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/212_ECCV_2024_paper.php": {
    "title": "BAM-DETR: Boundary-Aligned Moment Detection Transformer for Temporal Sentence Grounding in Videos",
    "volume": "main",
    "abstract": "Temporal sentence grounding aims to localize moments relevant to a language description. Recently, DETR-like approaches achieved notable progress by predicting the center and length of a target moment. However, they suffer from the issue of center misalignment raised by the inherent ambiguity of moment centers, leading to inaccurate predictions. To remedy this problem, we propose a novel boundary-oriented moment formulation. In our paradigm, the model no longer needs to find the precise center but instead suffices to predict any anchor point within the interval, from which the boundaries are directly estimated. Based on this idea, we design a boundary-aligned moment detection transformer, equipped with a dual-pathway decoding process. Specifically, it refines the anchor and boundaries within parallel pathways using global and boundary-focused attention, respectively. This separate design allows the model to focus on desirable regions, enabling precise refinement of moment predictions. Further, we propose a quality-based ranking method, ensuring that proposals with high localization qualities are prioritized over incomplete ones. Experiments on three benchmarks validate the effectiveness of the proposed methods. The code is available here",
    "checked": true,
    "id": "04f791d5d4676d096fafe9ea952aa7f2ab1c5d15",
    "semantic_title": "bam-detr: boundary-aligned moment detection transformer for temporal sentence grounding in videos",
    "citation_count": 2,
    "authors": [
      "Pilhyeon Lee*",
      "Hyeran Byun"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/223_ECCV_2024_paper.php": {
    "title": "MarineInst: A Foundation Model for Marine Image Analysis with Instance Visual Description",
    "volume": "main",
    "abstract": "Recent foundation models trained on a tremendous scale of data have shown great promise in a wide range of computer vision tasks and application domains. However, less attention has been paid to the marine realms, which in contrast cover the majority of our blue planet. The scarcity of labeled data is the most hindering issue, and marine photographs illustrate significantly different appearances and contents from general in-air images. Using existing foundation models for marine visual analysis does not yield satisfactory performance, due to not only the data distribution shift, but also the intrinsic limitations of the existing foundation models (, lacking semantics, redundant mask generation, or restricted to image-level scene understanding). In this work, we emphasize both model and data approaches for understanding marine ecosystems. We introduce MarineInst, a foundation model for the analysis of the marine realms with instance visual description, which outputs instance masks and captions for marine object instances. To train MarineInst, we acquire MarineInst20M, the largest marine image dataset to date, which contains a wide spectrum of marine images with high-quality semantic instance masks constructed by a mixture of human-annotated instance masks and model-generated instance masks from our automatic procedure of binary instance filtering. To generate informative and detailed semantic instance captions, we use vision-language models to produce semantic richness with various granularities. Our model and dataset support a wide range of marine visual analysis tasks, from image-level scene understanding to regional mask-level instance understanding. More significantly, MarineInst exhibits strong generalization ability and flexibility to support a wide range of downstream tasks with state-of-the-art performance as demonstrated in fig:teaser. Project website: https://marineinst.hkustvgd.com",
    "checked": true,
    "id": "1b007dd47051ec83bc20a09809b85c79061bd2d8",
    "semantic_title": "marineinst: a foundation model for marine image analysis with instance visual description",
    "citation_count": 1,
    "authors": [
      "Ziqiang Zheng*",
      "Yiwei Chen",
      "Huimin Zeng",
      "Tuan-Anh Vu",
      "Binh-Son Hua",
      "Sai-Kit Yeung"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/234_ECCV_2024_paper.php": {
    "title": "Superpixel-informed Implicit Neural Representation for Multi-Dimensional Data",
    "volume": "main",
    "abstract": "Recently, implicit neural representations (INRs) have attracted increasing attention for multi-dimensional data recovery. However, INRs simply map coordinates via a multi-layer perceptron (MLP) to corresponding values, ignoring the inherent semantic information of the data. To leverage semantic priors from the data, we propose a novel Superpixel-informed INR (S-INR). Specifically, we suggest utilizing generalized superpixel instead of pixel as an alternative basic unit of INR for multi-dimensional data (e.g., images and weather data). The coordinates of generalized superpixels are first fed into exclusive attention-based MLPs, and then the intermediate results interact with a shared dictionary matrix. The elaborately designed modules in S-INR allow us to ingenuously exploit the semantic information within and across generalized superpixels. Extensive experiments on various applications validate the effectiveness and efficacy of our S-INR compared to state-of-the-art INR methods",
    "checked": true,
    "id": "414e28f1c17c83f62dffa3e7313caee9835fd182",
    "semantic_title": "superpixel-informed implicit neural representation for multi-dimensional data",
    "citation_count": 0,
    "authors": [
      "Jia-Yi Li",
      "Xi-Le Zhao*",
      "Jian-Li Wang",
      "Chao Wang",
      "Min Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/248_ECCV_2024_paper.php": {
    "title": "EgoPoser: Robust Real-Time Egocentric Pose Estimation from Sparse and Intermittent Observations Everywhere",
    "volume": "main",
    "abstract": "Full-body egocentric pose estimation from head and hand poses alone has become an active area of research to power articulate avatar representations on headset-based platforms. However, existing methods over-rely on the indoor motion-capture spaces in which datasets were recorded, while simultaneously assuming continuous joint motion capture and uniform body dimensions. We propose to overcome these limitations with four main contributions. 1) robustly models body pose from intermittent hand position and orientation tracking only when inside a headset's field of view. 2) We rethink input representations for headset-based ego-pose estimation and introduce a novel global motion decomposition method that predicts full-body pose independent of global positions. 3) We enhance pose estimation by capturing longer motion time series through an efficient SlowFast module design that maintains computational efficiency. 4) generalizes across various body shapes for different users. We experimentally evaluate our method and show that it outperforms state-of-the-art methods both qualitatively and quantitatively while maintaining a high inference speed of over 600 fps. establishes a robust baseline for future work where full-body pose estimation no longer needs to rely on outside-in capture and can scale to large-scale and unseen environments",
    "checked": true,
    "id": "914865b7b5ac4edd99bc15a8e8184204837b0342",
    "semantic_title": "egoposer: robust real-time egocentric pose estimation from sparse and intermittent observations everywhere",
    "citation_count": 3,
    "authors": [
      "Jiaxi Jiang*",
      "Paul Streli",
      "Manuel Meier",
      "Christian Holz"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/252_ECCV_2024_paper.php": {
    "title": "Physics-Free Spectrally Multiplexed Photometric Stereo under Unknown Spectral Composition",
    "volume": "main",
    "abstract": "In this paper, we present a groundbreaking spectrally multiplexed photometric stereo approach for recovering surface normals of dynamic surfaces without the need for calibrated lighting or sensors, a notable advancement in the field traditionally hindered by stringent prerequisites and spectral ambiguity. By embracing spectral ambiguity as an advantage, our technique enables the generation of training data without specialized multispectral rendering frameworks. We introduce a unique, physics-free network architecture, SpectraM-PS, that effectively processes multiplexed images to determine surface normals across a wide range of conditions and material types, without relying on specific physically-based knowledge. Additionally, we establish the first benchmark dataset, SpectraM14, for spectrally multiplexed photometric stereo, facilitating comprehensive evaluations against existing calibrated methods. Our contributions significantly enhance the capabilities for dynamic surface recovery, particularly in uncalibrated setups, marking a pivotal step forward in the application of photometric stereo across various domains",
    "checked": true,
    "id": "a6baaa842038d662297ba56fd23ca5da138b452e",
    "semantic_title": "physics-free spectrally multiplexed photometric stereo under unknown spectral composition",
    "citation_count": 0,
    "authors": [
      "Satoshi Ikehata*",
      "Yuta Asano"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/254_ECCV_2024_paper.php": {
    "title": "SplatFields: Neural Gaussian Splats for Sparse 3D and 4D Reconstruction",
    "volume": "main",
    "abstract": "Digitizing 3D static scenes and 4D dynamic events from multi-view images has long been a challenge in computer vision and graphics. Recently, 3D Gaussian Splatting (3DGS) has emerged as a practical and scalable reconstruction method, gaining popularity due to its impressive reconstruction quality, real-time rendering capabilities, and compatibility with widely used visualization tools. However, the method requires a substantial number of input views to achieve high-quality scene reconstruction, introducing a significant practical bottleneck. This challenge is especially severe in capturing dynamic scenes, where deploying an extensive camera array can be prohibitively costly. In this work, we identify the lack of spatial autocorrelation of splat features as one of the factors contributing to the suboptimal performance of the 3DGS technique in sparse reconstruction settings. To address the issue, we propose an optimization strategy that effectively regularizes splat features by modeling them as the outputs of a corresponding implicit neural field. This results in a consistent enhancement of reconstruction quality across various scenarios. Our approach effectively handles static and dynamic cases, as demonstrated by extensive testing across different setups and scene complexities",
    "checked": true,
    "id": "dc16c52e7f78ad6bb5e12467c64e0d2c9b49d6ad",
    "semantic_title": "splatfields: neural gaussian splats for sparse 3d and 4d reconstruction",
    "citation_count": 1,
    "authors": [
      "Marko Mihajlovic*",
      "Sergey Prokudin",
      "Siyu Tang",
      "Robert Maier",
      "Federica Bogo",
      "Tony Tung",
      "Edmond Boyer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/255_ECCV_2024_paper.php": {
    "title": "VFusion3D: Learning Scalable 3D Generative Models from Video Diffusion Models",
    "volume": "main",
    "abstract": "This paper presents a novel method for building scalable 3D generative models utilizing pre-trained video diffusion models. The primary obstacle in developing foundation 3D generative models is the limited availability of 3D data. Unlike images, texts, or videos, 3D data are not readily accessible and are difficult to acquire. This results in a significant disparity in scale compared to the vast quantities of other types of data. To address this issue, we propose using a video diffusion model, trained with extensive volumes of text, images, and videos, as a knowledge source for 3D data. By unlocking its multi-view generative capabilities through fine-tuning, we generate a large-scale synthetic multi-view dataset to train a feed-forward 3D generative model. The proposed model, VFusion3D, trained on nearly 3M synthetic multi-view data, can generate a 3D asset from a single image in seconds and achieves superior performance when compared to current SOTA feed-forward 3D generative models, with users preferring our results over 90% of the time",
    "checked": true,
    "id": "77b3af0bc3b3967529ecea7f72c53941ae4ba357",
    "semantic_title": "vfusion3d: learning scalable 3d generative models from video diffusion models",
    "citation_count": 22,
    "authors": [
      "Junlin Han*",
      "Filippos Kokkinos",
      "Philip Torr"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/259_ECCV_2024_paper.php": {
    "title": "Alignist: CAD-Informed Orientation Distribution Estimation by Fusing Shape and Correspondences",
    "volume": "main",
    "abstract": "Object pose distribution estimation is crucial in robotics for better path planning and handling of symmetric objects. Recent distribution estimation approaches employ contrastive learning-based approaches by maximizing the likelihood of a single pose estimate in the absence of a CAD model. We propose a pose distribution estimation method leveraging symmetry respecting correspondence distributions and shape information obtained using a CAD model. Contrastive learning-based approaches require an exhaustive amount of training images from different viewpoints to learn the distribution properly, which is not possible in realistic scenarios. Instead, we propose a pipeline that can leverage correspondence distributions and shape information from the CAD model, which are later used to learn pose distributions. Besides, having access to pose distribution based on correspondences before learning pose distributions conditioned on images, can help formulate the loss between distributions. The prior knowledge of distribution also helps the network to focus on getting sharper modes instead. With the CAD prior, our approach converges much faster and learns distribution better by focusing on learning sharper distribution near all the valid modes, unlike contrastive approaches, which focus on a single mode at a time. We achieve benchmark results on SYMSOL-I and T-Less datasets",
    "checked": true,
    "id": "63dbed46eba424c8f1d4325ecccedd9613601195",
    "semantic_title": "alignist: cad-informed orientation distribution estimation by fusing shape and correspondences",
    "citation_count": 0,
    "authors": [
      "Shishir Reddy Vutukur*",
      "Junwen Huang",
      "Rasmus Laurvig Haugaard",
      "Benjamin Busam",
      "Tolga Birdal"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/261_ECCV_2024_paper.php": {
    "title": "Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs",
    "volume": "main",
    "abstract": "Prompt ensembling of Large Language Model (LLM) generated category-specific prompts has emerged as an effective method to enhance zero-shot recognition ability of Vision-Language Models (VLMs). To obtain these category-specific prompts, the present methods rely on hand-crafting the prompts to the LLMs for generating VLM prompts for the downstream tasks. However, this requires manually composing these task-specific prompts and still, they might not cover the diverse set of visual concepts and task-specific styles associated with the categories of interest. To effectively take humans out of the loop and completely automate the prompt generation process for zero-shot recognition, we propose Meta-Prompting for Visual Recognition (). Taking as input only minimal information about the target task, in the form of its short natural language description, and a list of associated class labels, automatically produces a diverse set of category-specific prompts resulting in a strong zero-shot classifier. generalizes effectively across various popular zero-shot image recognition benchmarks belonging to widely different domains when tested with multiple LLMs and VLMs. For example, obtains a zero-shot recognition improvement over CLIP by up to 19.8% and 18.2% (5.0% and 4.5% on average over 20 datasets) leveraging GPT and Mixtral LLMs, respectively",
    "checked": true,
    "id": "08f11c8f4ba6f10b4df26ed4376df4f2f3106da9",
    "semantic_title": "meta-prompting for automating zero-shot visual recognition with llms",
    "citation_count": 6,
    "authors": [
      "Muhammad Jehanzeb Mirza*",
      "Leonid Karlinsky",
      "Wei Lin",
      "Sivan Doveh",
      "Jakub Micorek",
      "Mateusz Kozinski",
      "Hilde Kuehne",
      "Horst Possegger"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/270_ECCV_2024_paper.php": {
    "title": "Physics-Based Interaction with 3D Objects via Video Generation",
    "volume": "main",
    "abstract": "Realistic object interactions are crucial for creating immersive virtual experiences, yet synthesizing realistic 3D object dynamics in response to novel interactions remains a significant challenge. Unlike unconditional or text-conditioned dynamics generation, action-conditioned dynamics requires perceiving the physical material properties of objects and grounding the 3D motion prediction on these properties, such as object stiffness. However, estimating physical material properties is an open problem due to the lack of material ground-truth data, as measuring these properties for real objects is highly difficult. We present , a physics-based approach that endows static 3D objects with interactive dynamics by leveraging the object dynamics priors learned by video generation models. By distilling these priors, enables the synthesis of realistic object responses to novel interactions, such as external forces or agent manipulations. We demonstrate our approach on diverse examples of elastic objects and evaluate the realism of the synthesized interactions through a user study. takes a step towards more engaging and realistic virtual experiences by enabling static 3D objects to dynamically respond to interactive stimuli in a physically plausible manner. See our project page at https://physdreamer.github.io/",
    "checked": false,
    "id": "280f4625876c1d32d803e2e5d73660665740048c",
    "semantic_title": "physdreamer: physics-based interaction with 3d objects via video generation",
    "citation_count": 15,
    "authors": [
      "Tianyuan Zhang*",
      "Hong-Xing Yu",
      "Rundi Wu",
      "Brandon Y Feng",
      "Changxi Zheng",
      "Noah Snavely",
      "Jiajun Wu",
      "William T. Freeman"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/271_ECCV_2024_paper.php": {
    "title": "Reconstruction and Simulation of Elastic Objects with Spring-Mass 3D Gaussians",
    "volume": "main",
    "abstract": "Reconstructing and simulating elastic objects from visual observations is crucial for applications in computer vision and robotics. Existing methods, such as 3D Gaussians, model 3D appearance and geometry, but lack the ability to estimate physical properties for objects and simulate them. The core challenge lies in integrating an expressive yet efficient physical dynamics model. We propose , a 3D physical object representation for reconstructing and simulating elastic objects from videos of the object from multiple viewpoints. In particular, we develop and integrate a 3D Spring-Mass model into 3D Gaussian kernels, enabling the reconstruction of the visual appearance, shape, and physical dynamics of the object. Our approach enables future prediction and simulation under various initial states and environmental properties. We evaluate on both synthetic and real-world datasets, demonstrating accurate reconstruction and simulation of elastic objects. Project page:",
    "checked": true,
    "id": "8056aa94116869410459b813169a57e50f04ff8b",
    "semantic_title": "reconstruction and simulation of elastic objects with spring-mass 3d gaussians",
    "citation_count": 8,
    "authors": [
      "Licheng Zhong",
      "Hong-Xing Yu",
      "Jiajun Wu",
      "Yunzhu Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/272_ECCV_2024_paper.php": {
    "title": "Deep Patch Visual SLAM",
    "volume": "main",
    "abstract": "Recent work in Visual Odometry and SLAM has shown the effectiveness of using deep network backbones. Despite excellent accuracy, such approaches are often expensive to run or do not generalize well zero-shot. To address this problem, we introduce Deep Patch Visual-SLAM, a new system for monocular visual SLAM based on the DPVO visual odometry system. We introduce two loop closure mechanisms which significantly improve the accuracy with minimal runtime and memory overhead. On real-world datasets, DPV-SLAM runs at 1x-3x real-time framerates. We achieve comparable accuracy to DROID-SLAM on EuRoC and TartanAir while running twice as fast using a third of the VRAM. We also outperform DROID-SLAM by large margins on KITTI. As DPV-SLAM is an extension to DPVO, its code can be found in the same repository: https: //github.com/princeton-vl/DPVO",
    "checked": true,
    "id": "f55f8b26e95e9a1847b3c144cfa334d955bf1774",
    "semantic_title": "deep patch visual slam",
    "citation_count": 1,
    "authors": [
      "Lahav Lipson*",
      "Zachary Teed",
      "Jia Deng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/274_ECCV_2024_paper.php": {
    "title": "Surface Reconstruction for 3D Gaussian Splatting via Local Structural Hints",
    "volume": "main",
    "abstract": "This paper presents a novel approach for surface mesh reconstruction from 3D Gaussian Splatting (3DGS) [?], a technique renowned for its efficiency in novel view synthesis but challenged for surface reconstruction. The key obstacle is the lack of geometry hints to regulate the optimization of millions of unorganized Gaussian blobs to align to the true surface. This paper introduces local structural hints during training to address the challenge. We first leverage the prior knowledge from monocular normal and depth estimations to refine the covariance and mean of Gaussian primitives, enhancing their organization and providing crucial normal information for surface extraction. However, due to the highly discrete nature of Gaussian primitives, such geometry guidance remains insufficient for the alignment with the true surface. We then propose to construct a signed distance field by a moving least square (MLS) function over the Gaussians in each local region. More importantly, we further propose to jointly learn a neural implicit network to mimic and regularize the MLS function. The joint optimization helps the optimization of Gaussian Splatting towards accurate surface alignment. Extensive experimental results demonstrate the effectiveness of our method in achieving superior mesh quality compared with the SoTA surface reconstruction for 3DGS. More resources can be found on our project page: https://qianyiwu.github.io/gsrec",
    "checked": false,
    "id": "059b5ecd5d7b0a93854b1472b8acb78bd5aa36f8",
    "semantic_title": "surface reconstruction from 3d gaussian splatting via local structural hints",
    "citation_count": 0,
    "authors": [
      "Qianyi Wu*",
      "Jianmin Zheng",
      "Jianfei Cai"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/280_ECCV_2024_paper.php": {
    "title": "HeadGaS: Real-Time Animatable Head Avatars via 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "3D head animation has seen major quality and runtime improvements over the last few years, particularly empowered by the advances in differentiable rendering and neural radiance fields. Real-time rendering is a highly desirable goal for real-world applications. We propose HeadGaS, a model that uses 3D Gaussian Splats (3DGS) for 3D head reconstruction and animation. In this paper we introduce a hybrid model that extends the explicit 3DGS representation with a base of learnable latent features, which can be linearly blended with low-dimensional parameters from parametric head models to obtain expression-dependent color and opacity values. We demonstrate that HeadGaS delivers state-of-the-art results in real-time inference frame rates, surpassing baselines by up to 2 dB, while accelerating rendering speed by over ×10",
    "checked": true,
    "id": "22492ecd9881bcd190f7500e1ccabb9ee2584afe",
    "semantic_title": "headgas: real-time animatable head avatars via 3d gaussian splatting",
    "citation_count": 18,
    "authors": [
      "Helisa Dhamo*",
      "Yinyu Nie",
      "Arthur Moreau",
      "Jifei Song",
      "Richard Shaw",
      "Yiren Zhou",
      "Eduardo Pérez-Pellitero*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/289_ECCV_2024_paper.php": {
    "title": "LayeredFlow: A Real-World Benchmark for Non-Lambertian Multi-Layer Optical Flow",
    "volume": "main",
    "abstract": "Achieving 3D understanding of non-Lambertian objects is an important task with many useful applications, but most existing algorithms struggle to deal with such objects. One major obstacle towards progress in this field is the lack of holistic non-Lambertian benchmarks—most benchmarks have low scene and object diversity, and none provide multi-layer 3D annotations for objects occluded by transparent surfaces. In this paper, we introduce , a real world benchmark containing multi-layer ground truth annotation for optical flow of non-Lambertian objects. Compared to previous benchmarks, our benchmark exhibits greater scene and object diversity, with 150k high quality optical flow and stereo pairs taken over 185 indoor and outdoor scenes and 360 unique objects. Using as evaluation data, we propose a new task called multi-layer optical flow. To provide training data for this task, we introduce a large-scale densely-annotated synthetic dataset containing 60k images within 30 scenes tailored for non-Lambertian objects. Training on our synthetic dataset enables model to predict multi-layer optical flow, while fine-tuning existing optical flow methods on the dataset notably boosts their performance on non-Lambertian objects without compromising the performance on diffuse objects",
    "checked": true,
    "id": "15d6dff07cccefd7fb3246d5dab55d2f7c8715f8",
    "semantic_title": "layeredflow: a real-world benchmark for non-lambertian multi-layer optical flow",
    "citation_count": 0,
    "authors": [
      "Hongyu Wen*",
      "Erich Liang",
      "Jia Deng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/294_ECCV_2024_paper.php": {
    "title": "Learning 3D Geometry and Feature Consistent Gaussian Splatting for Object Removal",
    "volume": "main",
    "abstract": "This paper tackles the intricate challenge of object removal to update the radiance field using the 3D Gaussian Splatting. The main challenges of this task lie in the preservation of geometric consistency and the maintenance of texture coherence in the presence of the substantial discrete nature of Gaussian primitives. We introduce a robust framework specifically designed to overcome these obstacles. The key insight of our approach is the enhancement of information exchange among visible and invisible areas, facilitating content restoration in terms of both geometry and texture. Our methodology begins with optimizing the positioning of Gaussian primitives to improve geometric consistency across both removed and visible areas, guided by an online registration process informed by monocular depth estimation. Following this, we employ a novel feature propagation mechanism to bolster texture coherence, leveraging a cross-attention design that bridges sampling Gaussians from both uncertain and certain areas. This innovative approach significantly refines the texture coherence within the final radiance field. Extensive experiments validate that our method not only elevates the quality of novel view synthesis for scenes undergoing object removal but also showcases notable efficiency gains in training and rendering speeds. Project Page: https://w-ted.github.io/publications/gscream",
    "checked": false,
    "id": "07a056ab69ac80d9ac35b7d4004a052592a30c06",
    "semantic_title": "gscream: learning 3d geometry and feature consistent gaussian splatting for object removal",
    "citation_count": 3,
    "authors": [
      "Yuxin Wang",
      "Qianyi Wu",
      "Guofeng Zhang",
      "Dan Xu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/304_ECCV_2024_paper.php": {
    "title": "Motion-prior Contrast Maximization for Dense Continuous-Time Motion Estimation",
    "volume": "main",
    "abstract": "Current optical flow and point-tracking methods rely heavily on synthetic datasets. Event cameras are novel vision sensors with advantages in challenging visual conditions, but state-of-the-art frame-based methods cannot be easily adapted to event data due to the limitations of current event simulators. We introduce a novel self-supervised loss combining the Contrast Maximization framework with a non-linear motion prior in the form of pixel-level trajectories and propose an efficient solution to solve the high-dimensional assignment problem between non-linear trajectories and events. Their effectiveness is demonstrated in two scenarios: In dense continuous-time motion estimation, our method improves the zero-shot performance of a synthetically trained model on the real-world dataset EVIMO2 by 29%. In optical flow estimation, our method elevates a simple UNet to achieve state-of-the-art performance among self-supervised methods on the DSEC optical flow benchmark. Our code is available at https: //github.com/tub-rip/MotionPriorCMax",
    "checked": true,
    "id": "283f25f2d7437cb9749025402c3d57ceff9ddbe0",
    "semantic_title": "motion-prior contrast maximization for dense continuous-time motion estimation",
    "citation_count": 1,
    "authors": [
      "Friedhelm Hamann*",
      "Ziyun Wang",
      "Ioannis Asmanis",
      "Kenneth Chaney",
      "Guillermo Gallego",
      "Kostas Daniilidis"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/305_ECCV_2024_paper.php": {
    "title": "Efficient Few-Shot Action Recognition via Multi-Level Post-Reasoning",
    "volume": "main",
    "abstract": "The integration with CLIP (Contrastive Vision-Language Pre-training) has significantly refreshed the accuracy leaderboard of FSAR (Few-Shot Action Recognition). However, the trainable overhead of ensuring that the domain alignment of CLIP and FSAR is often unbearable. To mitigate this issue, we present an Efficient Multi-Level Post-Reasoning Network, namely EMP-Net. By design, a post-reasoning mechanism is proposed for domain adaptation, which avoids most gradient backpropagation, improving the efficiency; meanwhile, a multi-level representation is utilised during the reasoning and matching processes to improve the discriminability, ensuring effectiveness. Specifically, the proposed EMP-Net starts with a skip-fusion involving cached multi-stage features extracted by CLIP. After that, the fused feature is decoupled into multi-level representations, including global-level, patch-level, and frame-level. The ensuing spatiotemporal reasoning module operates on multi-level representations to generate discriminative features. As for matching, the contrasts between text-visual and support-query are integrated to provide comprehensive guidance. The experimental results demonstrate that EMP-Net can unlock the potential performance of CLIP in a more efficient manner. The code and supplementary material can be found at https://github.com/cong-wu/EMP-Net",
    "checked": false,
    "id": "514f746b92c2b038906806f94cc6a0e4f11e278f",
    "semantic_title": "hierarchical reasoning network with contrastive learning for few-shot human-object interaction recognition",
    "citation_count": 2,
    "authors": [
      "Cong Wu",
      "Xiao-Jun Wu*",
      "Linze Li",
      "Tianyang Xu",
      "Zhenhua Feng",
      "Josef Kittler"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/308_ECCV_2024_paper.php": {
    "title": "Text2Place: Affordance-aware Text Guided Human Placement",
    "volume": "main",
    "abstract": "For a given scene, humans can easily reason for the locations and pose to place objects. Designing a computational model to reason about these affordances poses a significant challenge, mirroring the intuitive reasoning abilities of humans. This work tackles the problem of realistic human insertion in a given background scene termed as Semantic Human Placement. This task is extremely challenging given the diverse backgrounds, scale, and pose of the generated person and, finally, the identity preservation of the person. We divide the problem into the following two stages i) learning semantic masks using text guidance for localizing regions in the image to place humans and ii) subject-conditioned inpainting to place a given subject adhering to the scene affordance within the semantic masks. For learning semantic masks, we leverage rich object-scene priors learned from the text-to-image generative models and optimize a novel parameterization of the semantic mask, eliminating the need for large-scale training. To the best of our knowledge, we are the first ones to provide an effective solution for realistic human placements in diverse real-world scenes. The proposed method can generate highly realistic scene compositions while preserving the background and subject identity. Further, we present results for several downstream tasks - scene hallucination from a single or multiple generated persons and text-based attribute editing. With extensive comparisons against strong baselines, we show the superiority of our method in realistic human placement",
    "checked": true,
    "id": "09cb8b8800749c266d596b21f089aa9df52a0adf",
    "semantic_title": "text2place: affordance-aware text guided human placement",
    "citation_count": 0,
    "authors": [
      "Rishubh Parihar*",
      "Harsh Gupta",
      "Sachidanand VS",
      "Venkatesh Babu RADHAKRISHNAN"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/319_ECCV_2024_paper.php": {
    "title": "OGNI-DC: Robust Depth Completion with Optimization-Guided Neural Iterations",
    "volume": "main",
    "abstract": "Depth completion is the task of generating a dense depth map given an image and a sparse depth map as inputs. In this paper, we present OGNI-DC, a novel framework for depth completion. The key to our method is \"Optimization-Guided Neural Iterations\" (OGNI). It consists of a recurrent unit that refines a depth gradient field and a differentiable depth integrator that integrates the depth gradients into a depth map. OGNI-DC exhibits strong generalization, outperforming baselines by a large margin on unseen datasets and across various sparsity levels. Moreover, OGNI-DC has high accuracy, achieving state-of-the-art performance on the NYUv2 and the KITTI benchmarks. Code is available at https:// github.com/princeton-vl/OGNI-DC",
    "checked": true,
    "id": "19260f01e6db7f4d50fae9a08b02c3c9c9e748a3",
    "semantic_title": "ogni-dc: robust depth completion with optimization-guided neural iterations",
    "citation_count": 1,
    "authors": [
      "Yiming Zuo*",
      "Jia Deng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/324_ECCV_2024_paper.php": {
    "title": "Zero-Shot Multi-Object Scene Completion",
    "volume": "main",
    "abstract": "We present a 3D scene completion method that recovers the complete geometry of multiple unseen objects in complex scenes from a single RGB-D image. Despite notable advancements in single-object 3D shape completion, high-quality reconstructions in highly cluttered real-world multi-object scenes remains a challenge. To address this issue, we propose OctMAE, an architecture that leverages an Octree U-Net and a latent 3D MAE to achieve high-quality and near real-time multi-object scene completion through both local and global geometric reasoning. Because a naive 3D MAE can be computationally intractable and memory intensive even in the latent space, we introduce a novel occlusion masking strategy and adopt 3D rotary embeddings, which significantly improve the runtime and scene completion quality. To generalize to a wide range of objects in diverse scenes, we create a large-scale photorealistic dataset, featuring a diverse set of 12K 3D object models from the Objaverse dataset that are rendered in multi-object scenes with physics-based positioning. Our method outperforms the current state-of-the-art on both synthetic and real-world datasets and demonstrates a strong zero-shot capability. https://sh8.io/#/oct_mae",
    "checked": true,
    "id": "6f8f62c3034c41a4f92173cddb31db19964f224b",
    "semantic_title": "zero-shot multi-object scene completion",
    "citation_count": 0,
    "authors": [
      "Shun Iwase*",
      "Katherine Liu",
      "Vitor Guizilini",
      "Adrien Gaidon",
      "Kris Kitani",
      "Rareș A Ambruș",
      "Sergey Zakharov"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/328_ECCV_2024_paper.php": {
    "title": "Beta-Tuned Timestep Diffusion Model",
    "volume": "main",
    "abstract": "Diffusion models have received a lot of attention in the field of generation due to their ability to produce high-quality samples. However, several recent studies indicate that treating all distributions equally in diffusion model training is sub-optimal. In this paper, we conduct an in-depth theoretical analysis of the forward process of diffusion models. Our findings reveal that the distribution variations are non-uniform throughout the diffusion process and the most drastic variations in distribution occur in the initial stages. Consequently, simple uniform timestep sampling strategy fail to align with these properties, potentially leading to sub-optimal training of diffusion models. To address this, we propose the Beta-Tuned Timestep Diffusion Model (B-TTDM), which devises a timestep sampling strategy based on the beta distribution. By choosing the correct parameters, B-TTDM aligns the timestep sampling distribution with the properties of the forward diffusion process. Extensive experiments on different benchmark datasets validate the effectiveness of B-TTDM",
    "checked": false,
    "id": "f4cdc3d3651878de407d95aff4a530ab3bb390f4",
    "semantic_title": "memory-efficient fine-tuning for quantized diffusion model",
    "citation_count": 2,
    "authors": [
      "Tianyi Zheng*",
      "Peng-Tao Jiang",
      "Ben Wan",
      "Hao Zhang",
      "Jinwei Chen",
      "Jia Wang*",
      "Bo Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/333_ECCV_2024_paper.php": {
    "title": "POA: Pre-training Once for Models of All Sizes",
    "volume": "main",
    "abstract": "Large-scale self-supervised pre-training has paved the way for one foundation model to handle many different vision tasks. Most pre-training methodologies train a single model of a certain size at one time. Nevertheless, various computation or storage constraints in real-world scenarios require substantial efforts to develop a series of models with different sizes to deploy. Thus, in this study, we propose a novel tri-branch self-supervised training framework, termed as POA (Pre-training Once for All), to tackle this aforementioned issue. Our approach introduces an innovative elastic student branch into a modern self-distillation paradigm. At each pre-training step, we randomly sample a sub-network from the original student to form the elastic student and train all branches in a self-distilling fashion. Once pre-trained, POA allows the extraction of pre-trained models of diverse sizes for downstream tasks. Remarkably, the elastic student facilitates the simultaneous pre-training of multiple models with different sizes, which also acts as an additional ensemble of models of various sizes to enhance representation learning. Extensive experiments, including k-nearest neighbors, linear probing evaluation and assessments on multiple downstream tasks demonstrate the effectiveness and advantages of our POA. It achieves state-of-the-art performance using ViT, Swin Transformer and ResNet backbones, producing around a hundred models with different sizes through a single pre-training session. The code is available at: https://github.com/Qichuzyy/POA",
    "checked": true,
    "id": "cf61b64c37ed553e10000c06e4a9e16863dd0a25",
    "semantic_title": "poa: pre-training once for models of all sizes",
    "citation_count": 0,
    "authors": [
      "Yingying Zhang*",
      "Xin Guo",
      "Jiangwei Lao",
      "Lei Yu",
      "Lixiang Ru",
      "Jian Wang",
      "Guo Ye",
      "HUIMEI HE",
      "Jingdong Chen",
      "Ming Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/354_ECCV_2024_paper.php": {
    "title": "Taming Latent Diffusion Model for Neural Radiance Field Inpainting",
    "volume": "main",
    "abstract": "Neural Radiance Field (NeRF) is a representation for 3D reconstruction from multi-view images. Despite some recent work showing preliminary success in editing a reconstructed NeRF with diffusion prior, they remain struggling to synthesize reasonable geometry in completely uncovered regions. One major reason is the high diversity of synthetic contents from the diffusion model, which hinders the radiance field from converging to a crisp and deterministic geometry. Moreover, applying latent diffusion models on real data often yields a textural shift incoherent to the image condition due to auto-encoding errors. These two problems are further reinforced with the use of pixel-distance losses. To address these issues, we propose tempering the diffusion model's stochasticity with per-scene customization and mitigating the textural shift with masked adversarial training. During the analyses, we also found the commonly used pixel and perceptual losses are harmful in the NeRF inpainting task. Through rigorous experiments, our framework yields state-of-the-art NeRF inpainting results on various real-world scenes",
    "checked": true,
    "id": "6f866b79c54da19f7573cce186831796df2e0953",
    "semantic_title": "taming latent diffusion model for neural radiance field inpainting",
    "citation_count": 2,
    "authors": [
      "Chieh Hubert Lin*",
      "Changil Kim",
      "Jia-Bin Huang",
      "Qinbo Li",
      "Chih-Yao Ma",
      "Johannes Kopf",
      "Ming-Hsuan Yang",
      "Hung-Yu Tseng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/358_ECCV_2024_paper.php": {
    "title": "MapDistill: Boosting Efficient Camera-based HD Map Construction via Camera-LiDAR Fusion Model Distillation",
    "volume": "main",
    "abstract": "Online high-definition (HD) map construction is an important and challenging task in autonomous driving. Recently, there has been a growing interest in cost-effective multi-view camera-based methods without relying on other sensors like LiDAR. However, these methods suffer from a lack of explicit depth information, necessitating the use of large models to achieve satisfactory performance. To address this, we employ the Knowledge Distillation (KD) idea for efficient HD map construction for the first time and introduce a novel KD-based approach called MapDistill to transfer knowledge from a high-performance camera-LiDAR fusion model to a lightweight camera-only model. Specifically, we adopt the teacher-student architecture, , a camera-LiDAR fusion model as the teacher and a lightweight camera model as the student, and devise a dual BEV transform module to facilitate cross-modal knowledge distillation while maintaining cost-effective camera-only deployment. Additionally, we present a comprehensive distillation scheme encompassing cross-modal relation distillation, dual-level feature distillation, and map head distillation. This approach alleviates knowledge transfer challenges between modalities, enabling the student model to learn improved feature representations for HD map construction. Experimental results on the challenging nuScenes dataset demonstrate the effectiveness of MapDistill, surpassing existing competitors by over 7.7 mAP or 4.5× speedup",
    "checked": true,
    "id": "986c0c00583bb724a265bd107e0fc54704bf3982",
    "semantic_title": "mapdistill: boosting efficient camera-based hd map construction via camera-lidar fusion model distillation",
    "citation_count": 2,
    "authors": [
      "Xiaoshuai Hao*",
      "Ruikai Li",
      "Hui Zhang",
      "Rong Yin",
      "Dingzhe Li",
      "Sangil Jung",
      "Seung-In Park",
      "ByungIn Yoo",
      "Haimei Zhao",
      "Jing Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/359_ECCV_2024_paper.php": {
    "title": "ByteEdit: Boost, Comply and Accelerate Generative Image Editing",
    "volume": "main",
    "abstract": "Recent advancements in diffusion-based generative image editing have sparked a profound revolution, reshaping the landscape of image outpainting and inpainting tasks. Despite these strides, the field grapples with inherent challenges, including: i) inferior quality; ii) poor consistency; iii) insufficient instrcution adherence; iv) suboptimal generation efficiency. To address these obstacles, we present ByteEdit, an innovative feedback learning framework meticulously designed to Boost, Comply, and Accelerate Generative Image Editing tasks. ByteEdit seamlessly integrates image reward models dedicated to enhancing aesthetics and image-text alignment, while also introducing a dense, pixel-level reward model tailored to foster coherence in the output. Furthermore, we propose a pioneering adversarial and progressive feedback learning strategy to expedite the model's inference speed. Through extensive large-scale user evaluations, we demonstrate that ByteEdit surpasses leading generative image editing products, including Adobe, Canva, and MeiTu, in both generation quality and consistency. ByteEdit-Outpainting exhibits a remarkable enhancement of 388% and 135% in quality and consistency, respectively, when compared to the baseline model. Experiments also verfied that our acceleration models maintains excellent performance results in terms of quality and consistency",
    "checked": true,
    "id": "63cba94e9d5988736f4b58a01877748fb0ee97cc",
    "semantic_title": "byteedit: boost, comply and accelerate generative image editing",
    "citation_count": 1,
    "authors": [
      "Yuxi Ren",
      "Jie Wu*",
      "Yanzuo Lu",
      "Huafeng Kuang",
      "Xin Xia",
      "Xionghui Wang",
      "Qianqian Wang",
      "Yixing Zhu",
      "Pan Xie",
      "Shiyin Wang",
      "Xuefeng Xiao",
      "Yitong Wang",
      "Min Zheng",
      "Lean FU"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/367_ECCV_2024_paper.php": {
    "title": "ProDepth: Boosting Self-Supervised Multi-Frame Monocular Depth with Probabilistic Fusion",
    "volume": "main",
    "abstract": "Self-supervised multi-frame monocular depth estimation relies on the geometric consistency between successive frames under the assumption of a static scene. However, the presence of moving objects in dynamic scenes introduces inevitable inconsistencies, causing misaligned multi-frame feature matching and misleading self-supervision during training. In this paper, we propose a novel framework called ProDepth, which effectively addresses the mismatch problem caused by dynamic objects using a probabilistic approach. We initially deduce the uncertainty associated with static scene assumption by adopting an auxiliary decoder. This decoder analyzes inconsistencies embedded in the cost volume, inferring the probability of areas being dynamic. We then directly rectify the erroneous cost volume for dynamic areas through a Probabilistic Cost Volume Modulation (PCVM) module. Specifically, we derive probability distributions of depth candidates from both single-frame and multi-frame cues, modulating the cost volume by adaptively fusing those distributions based on the inferred uncertainty. Additionally, we present a self-supervision loss reweighting strategy that not only masks out incorrect supervision with high uncertainty but also mitigates the risks in remaining possible dynamic areas in accordance with the probability. Our proposed method excels over state-of-the-art approaches in all metrics on both Cityscapes and KITTI datasets, and demonstrates superior generalization ability on the Waymo Open dataset",
    "checked": true,
    "id": "734b59d44e30144bfa42807e05ad46ea04313982",
    "semantic_title": "prodepth: boosting self-supervised multi-frame monocular depth with probabilistic fusion",
    "citation_count": 1,
    "authors": [
      "Sungmin Woo*",
      "Wonjoon Lee",
      "Woo Jin Kim",
      "Dogyoon Lee",
      "Sangyoun Lee*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/368_ECCV_2024_paper.php": {
    "title": "High-Resolution and Few-shot View Synthesis from Asymmetric Dual-lens Inputs",
    "volume": "main",
    "abstract": "Novel view synthesis has achieved remarkable quality and efficiency by the paradigm of 3D Gaussian Splatting (3D-GS), but still faces two challenges: 1) significant performance degradation when trained with only few-shot samples due to a lack of geometry constraint, and 2) incapability of rendering at a higher resolution that is beyond the input resolution of training samples. In this paper, we propose Dual-Lens 3D-GS (DL-GS) to achieve high-resolution (HR) and few-shot view synthesis, by leveraging the characteristics of the asymmetric dual-lens system commonly equipped on mobile devices. This kind of system captures the same scene with different focal lengths (i.e., wide-angle and telephoto) under an asymmetric stereo configuration, which naturally provides geometric hints for few-shot training and HR guidance for resolution improvement. Nevertheless, there remain two major technical problems to achieving this goal. First, how to effectively exploit the geometry information from the asymmetric stereo configuration? To this end, we propose a consistency-aware training strategy, which integrates a dual-lens-consistent loss to regularize the 3D-GS optimization. Second, how to make the best use of the dual-lens training samples to effectively improve the resolution of newly synthesized views? To this end, we design a multi-reference-guided refinement module to select proper telephoto and wide-angle guided images from training samples based on the camera pose distances, and then exploit their information for high-frequency detail enhancement. Extensive experiments on simulated and real-captured datasets validate the distinct superiority of our DL-GS over various competitors on the task of HR and few-shot view synthesis. The implementation code is available at https://github.com/XrKang/ DL-GS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruikang Xu",
      "Mingde Yao",
      "Yue Li",
      "Yueyi Zhang",
      "Zhiwei Xiong*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/370_ECCV_2024_paper.php": {
    "title": "Accelerating Image Super-Resolution Networks with Pixel-Level Classification",
    "volume": "main",
    "abstract": "In recent times, the need for effective super-resolution (SR) techniques has surged, especially for large-scale images ranging 2K to 8K resolutions. For DNN-based SISR, decomposing images into overlapping patches is typically necessary due to computational constraints. In such patch-decomposing scheme, one can allocate computational resources differently based on each patch's difficulty to further improve efficiency while maintaining SR performance. However, this approach has a limitation: computational resources is uniformly allocated within a patch, leading to lower efficiency when the patch contain pixels with varying levels of restoration difficulty. To address the issue, we propose the Pixel-level Classifier for Single Image Super-Resolution (PCSR), a novel method designed to distribute computational resources adaptively at the pixel level. A PCSR model comprises a backbone, a pixel-level classifier, and a set of pixel-level upsamplers with varying capacities. The pixel-level classifier assigns each pixel to an appropriate upsampler based on its restoration difficulty, thereby optimizing computational resource usage. Our method allows for performance and computational cost balance during inference without re-training. Our experiments demonstrate PCSR's advantage over existing patch-distributing methods in PSNR-FLOP trade-offs across different backbone models and benchmarks. The code will be available at https://github.com/3587jjh/PCSR",
    "checked": true,
    "id": "dd364a44227f7d17af413f16570d7e257c1a8da2",
    "semantic_title": "accelerating image super-resolution networks with pixel-level classification",
    "citation_count": 0,
    "authors": [
      "Jinho Jeong",
      "Jinwoo Kim",
      "Younghyun Jo",
      "Seon Joo Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/378_ECCV_2024_paper.php": {
    "title": "LASS3D: Language-Assisted Semi-Supervised 3D Semantic Segmentation with Progressive Unreliable Data Exploitation",
    "volume": "main",
    "abstract": "Precisely annotating large-scale 3D datasets for point cloud segmentation is laborious. To alleviate the annotation burden, several semi-supervised 3D segmentation methods have been proposed in literature. However, two issues remain to be tackled: 1) The utilization of large language-vision models (LVM) in semi-supervised 3D semantic segmentation remains under-explored. 2) The unlabeled points with low-confidence predictions are directly discarded by existing methods. Taking these two issues into consideration, we propose a language-assisted semi-supervised 3D semantic segmentation method named LASS3D, which is built upon the commonly used MeanTeacher framework. In LASS3D, we use two off-the-shelf LVM to generate multi-level captions and leverage the images as the bridge to connect the text data and point clouds. Then, a semantic-aware adaptive fusion module is explored in the student branch, where the semantic information encoded in the embeddings of multi-level captions is injected into 3D features by adaptive fusion and then the semantic information in the text-enhanced 3D features is transferred to the teacher branch by knowledge distillation. In addition, a progressive exploitation strategy is explored for the unreliable points in the teacher branch, which can effectively exploit the information encapsulated in unreliable points via negative learning. Experimental results on both outdoor and indoor datasets demonstrate that LASS3D outperforms the comparative methods in most cases",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianan Li*",
      "Qiulei Dong*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/391_ECCV_2024_paper.php": {
    "title": "Contourlet Residual for Prompt Learning Enhanced Infrared Image Super-Resolution",
    "volume": "main",
    "abstract": "Image super-resolution (SR) is a critical technique for enhancing image quality, playing a vital role in image enhancement. While recent advancements, notably transformer-based methods, have advanced the field, infrared image SR remains a formidable challenge. Due to the inherent characteristics of infrared sensors, such as limited resolution, temperature sensitivity, high noise levels, and environmental impacts, existing deep learning methods result in suboptimal enhancement outcomes when applied to infrared images. To address these challenges, we propose a specialized Contourlet residual framework tailored for infrared images to restore and enhance the critical details from the multi-scale and multi-directional infrared spectra decomposition. It precisely captures and amplifies the high-pass subbands of infrared images, such as edge details and texture nuances, which are vital for achieving superior reconstruction quality. Moreover, recognizing the limitations of traditional learning techniques in capturing the inherent characteristics of infrared images, we incorporate a prompt-based learning paradigm. This approach facilitates a more nuanced understanding and targeted optimization process for infrared images by leveraging the semantic comprehension offered by the visual language model. Our approach not only addresses the common pitfalls associated with infrared imaging but also sets a new paradigm for infrared image SR. Extensive experiments demonstrate that our approach obtains superior results, attaining state-of-the-art performance. Project page: https://github.com/hey-it-s-me/CoRPLE",
    "checked": false,
    "id": "e19018549f0216a6d3ff02bd119fba2663815143",
    "semantic_title": "human tooth crack image analysis with multiple deep learning approaches",
    "citation_count": 0,
    "authors": [
      "Xingyuan Li",
      "Jinyuan Liu*",
      "ZHIXIN CHEN",
      "Yang Zou",
      "Long Ma",
      "Xin Fan",
      "Risheng Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/406_ECCV_2024_paper.php": {
    "title": "Click-Gaussian: Interactive Segmentation to Any 3D Gaussians",
    "volume": "main",
    "abstract": "Interactive segmentation of 3D Gaussians opens a great opportunity for real-time manipulation of 3D scenes thanks to the real-time rendering capability of 3D Gaussian Splatting. However, the current methods suffer from time-consuming post-processing to deal with noisy segmentation output. Also, they struggle to provide detailed segmentation, which is important for fine-grained manipulation of 3D scenes. In this study, we propose Click-Gaussian, which learns distinguishable feature fields of two-level granularity, facilitating segmentation without time-consuming post-processing. We delve into challenges stemming from inconsistently learned feature fields resulting from 2D segmentation obtained independently from a 3D scene. 3D segmentation accuracy deteriorates when 2D segmentation results across the views, primary cues for 3D segmentation, are in conflict. To overcome these issues, we propose Global Feature-guided Learning (GFL). GFL constructs the clusters of global feature candidates from noisy 2D segments across the views, which smooths out noises when training the features of 3D Gaussians. Our method runs in 10 ms per click, 15 to 130 times as fast as the previous methods, while also significantly improving segmentation accuracy",
    "checked": true,
    "id": "2d81e6af252b16238b10cf6009514a61ed60d77a",
    "semantic_title": "click-gaussian: interactive segmentation to any 3d gaussians",
    "citation_count": 2,
    "authors": [
      "Seokhun Choi",
      "Hyeonseop Song",
      "Jaechul Kim",
      "Taehyeong Kim*",
      "Hoseok Do*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/412_ECCV_2024_paper.php": {
    "title": "Random Walk on Pixel Manifolds for Anomaly Segmentation of Complex Driving Scenes",
    "volume": "main",
    "abstract": "In anomaly segmentation for complex driving scenes, state-of-the-art approaches utilize anomaly scoring functions to calculate anomaly scores. For these functions, accurately predicting the logits of inlier classes for each pixel is crucial for precisely inferring the anomaly score. However, in real-world driving scenarios, the diversity of scenes often results in distorted manifolds of pixel embeddings in the space. This effect is not conducive to directly using the pixel embeddings for the logit prediction during inference, a concern overlooked by existing methods. To address this problem, we propose a novel method called Random Walk on Pixel Manifolds (RWPM). RWPM utilizes random walks to reveal the intrinsic relationships among pixels to refine the pixel embeddings. The refined pixel embeddings alleviate the distortion of manifolds, improving the accuracy of anomaly scores. Our extensive experiments show that RWPM consistently improve the performance of the existing anomaly segmentation methods and achieve the best results 1 . 1 Code is available at: https://github.com/ZelongZeng/RWPM",
    "checked": true,
    "id": "3d3d5cf8103e04b3235f8ab2b81ac0839ee7bd4f",
    "semantic_title": "random walk on pixel manifolds for anomaly segmentation of complex driving scenes",
    "citation_count": 0,
    "authors": [
      "Zelong Zeng*",
      "Kaname Tomite"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/414_ECCV_2024_paper.php": {
    "title": "DySeT: a Dynamic Masked Self-distillation Approach for Robust Trajectory Prediction",
    "volume": "main",
    "abstract": "The lack of generalization capability of behavior prediction models for autonomous vehicles is a crucial concern for safe motion planning. One way to address this is via self-supervised pre-training through masked trajectory prediction. However, the existing models rely on uniform random sampling of tokens, which is sub-optimal because it implies that all components of driving scenes are equally informative. In this paper, to enable more robust representation learning, we introduce a dynamic masked self-distillation approach to identify and utilize informative aspects of the scenes, particularly those corresponding to complex driving behaviors, such as overtaking. Specifically, for targeted sampling, we propose a dynamic method that prioritizes tokens, such as trajectory or lane segments, based on their informativeness. The latter is determined via an auxiliary network that estimates token distributions. Through sampler optimization, more informative tokens are rewarded and selected as visible based on the policy gradient algorithm adopted from reinforcement learning. In addition, we propose a masked self-distillation approach to transfer knowledge from fully visible to masked scene representations. The distillation process not only enriches the semantic information within the visible token set but also progressively refines the sampling process. Further, we use an integrated training regime to enhance the model's ability to learn meaningful representations from informative tokens. Our extensive evaluation on two large-scale trajectory prediction datasets demonstrates the superior performance of the proposed method and its improved prediction robustness across different scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mozghan Pourkeshavarz*",
      "Arielle Zhang",
      "Amir Rasouli"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/418_ECCV_2024_paper.php": {
    "title": "Track Everything Everywhere Fast and Robustly",
    "volume": "main",
    "abstract": "We propose a novel test-time optimization approach for efficiently and robustly tracking any pixel at any time in a video. The latest state-of-the-art optimization-based tracking technique, OmniMotion, requires a prohibitively long optimization time, rendering it impractical for downstream applications. OmniMotion is sensitive to the choice of random seeds, leading to unstable convergence. To improve efficiency and robustness, we introduce a novel invertible deformation network, CaDeX++, which factorizes the function representation into a local spatial-temporal feature grid and enhances the expressivity of the coupling blocks with non-linear functions. While CaDeX++ incorporates a stronger geometric bias within its architectural design, it also takes advantage of the inductive bias provided by the vision foundation models. Our system utilizes monocular depth estimation to represent scene geometry and enhances the objective by incorporating DINOv2 long-term semantics to regulate the optimization process. Our experiments demonstrate a substantial improvement in training speed (more than 10 times faster), robustness, and accuracy in tracking over the SoTA optimization-based method OmniMotion",
    "checked": true,
    "id": "3153a00a59a4c91c946b05b9ff3ef7b56eb330f8",
    "semantic_title": "track everything everywhere fast and robustly",
    "citation_count": 2,
    "authors": [
      "Yunzhou Song",
      "Jiahui Lei*",
      "Ziyun Wang",
      "Lingjie Liu",
      "Kostas Daniilidis"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/422_ECCV_2024_paper.php": {
    "title": "Towards Open-ended Visual Quality Comparison",
    "volume": "main",
    "abstract": "Comparative settings (pairwise choice, listwise ranking) have been adopted by a wide range of subjective studies for image quality assessment (IQA), as it inherently standardizes the evaluation criteria across different observers and offer more clear-cut responses. In this work, we extend the edge of emerging large multi-modality models (LMMs) to further advance visual quality comparison into open-ended settings, that 1) can respond to deepgreenopen-range questions on quality comparison; 2) can provide deepgreendetailed reasonings beyond direct answers. To this end, we propose the . To train this first-of-its-kind open-source open-ended visual quality comparer, we collect the Co-Instruct-562K dataset, from two sources: (a) LLM-merged single image quality description, (b) GPT-4V \"teacher\" responses on unlabeled data. Furthermore, to better evaluate this setting, we propose the , the first benchmark on multi-image comparison for LMMs. We demonstrate that not only achieves in average 30% higher accuracy than state-of-the-art open-source LMMs, but also outperforms GPT-4V (its teacher ), on both existing related benchmarks and the proposed . Our code, model and data are released on https://github.com/Q-Future/ Co-Instruct",
    "checked": true,
    "id": "3d54adf4e688602548f81386fcfeab23728fd441",
    "semantic_title": "towards open-ended visual quality comparison",
    "citation_count": 26,
    "authors": [
      "Haoning Wu",
      "Hanwei Zhu",
      "Zicheng Zhang",
      "Erli Zhang",
      "Chaofeng Chen",
      "Liang Liao",
      "Chunyi Li",
      "Annan Wang",
      "Wenxiu Sun",
      "Qiong Yan",
      "Xiaohong Liu",
      "Guangtao Zhai",
      "Shiqi Wang",
      "Weisi Lin*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/423_ECCV_2024_paper.php": {
    "title": "FreeInit: Bridging Initialization Gap in Video Diffusion Models",
    "volume": "main",
    "abstract": "Though diffusion-based video generation has witnessed rapid progress, the inference results of existing models still exhibit unsatisfactory temporal consistency and unnatural dynamics. In this paper, we delve deep into the noise initialization of video diffusion models, and discover an implicit training-inference gap that attributes to the unsatisfactory inference quality. Our key findings are: 1) the spatial-temporal frequency distribution of the initial noise at inference is intrinsically different from that for training, and 2) the denoising process is significantly influenced by the low-frequency components of the initial noise. Motivated by these observations, we propose a concise yet effective inference sampling strategy, FreeInit, which significantly improves temporal consistency of videos generated by diffusion models. Through iteratively refining the spatial-temporal low-frequency components of the initial latent during inference, FreeInit is able to compensate the initialization gap between training and inference, thus effectively improving the subject appearance and temporal consistency of generation results. Extensive experiments demonstrate that FreeInit consistently enhances the generation results of various text-to-video generation models without additional training",
    "checked": true,
    "id": "08e2fa11bb6af1ed39a459210665fb89104933c2",
    "semantic_title": "freeinit: bridging initialization gap in video diffusion models",
    "citation_count": 28,
    "authors": [
      "Tianxing Wu*",
      "Chenyang Si",
      "Yuming Jiang",
      "Ziqi Huang",
      "Ziwei Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/430_ECCV_2024_paper.php": {
    "title": "DenseNets Reloaded: Paradigm Shift Beyond ResNets and ViTs",
    "volume": "main",
    "abstract": "This paper revives Densely Connected Convolutional Networks (DenseNets) and reveals the underrated effectiveness over predominant ResNet-style architectures. We believe DenseNets' potential was overlooked due to untouched training methods and traditional design elements not fully revealing their capabilities. Our pilot study shows dense connections through concatenation are strong, demonstrating that DenseNets can be revitalized to compete with modern architectures. We methodically refine suboptimal components - architectural adjustments, block redesign, and improved training recipes towards widening DenseNets and boosting memory efficiency while keeping concatenation shortcuts. Our models, employing simple architectural elements, ultimately surpass Swin Transformer, ConvNeXt, and DeiT-III — key architectures in the residual learning lineage. Furthermore, our models exhibit near state-of-the-art performance on ImageNet-1K, competing with the very recent models and downstream tasks, ADE20k semantic segmentation, and COCO object detection/instance segmentation. Finally, we provide empirical analyses that uncover the merits of the concatenation over additive shortcuts, steering a renewed preference towards DenseNet-style designs. Our code is available at https://github.com/naver-ai/rdnet",
    "checked": true,
    "id": "bd703f339cd704522707efa977483f7bd3ea3858",
    "semantic_title": "densenets reloaded: paradigm shift beyond resnets and vits",
    "citation_count": 0,
    "authors": [
      "DongHyun Kim",
      "Byeongho Heo",
      "Dongyoon Han*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/450_ECCV_2024_paper.php": {
    "title": "Eliminating Feature Ambiguity for Few-Shot Segmentation",
    "volume": "main",
    "abstract": "Recent advancements in few-shot segmentation (FSS) have exploited pixel-by-pixel matching between query and support features, typically based on cross attention, which selectively activate query foreground (FG) features that correspond to the same-class support FG features. However, due to the large receptive fields in deep layers of the backbone, the extracted query and support FG features are inevitably mingled with background (BG) features, impeding the FG-FG matching in cross attention. Hence, the query FG features are fused with less support FG features, , the support information is not well utilized. This paper presents a novel plug-in termed ambiguity elimination network (AENet), which can be plugged into any existing cross attention-based FSS methods. The main idea is to mine discriminative query FG regions to rectify the ambiguous FG features, increasing the proportion of FG information, so as to suppress the negative impacts of the doped BG features. In this way, the FG-FG matching is naturally enhanced. We plug AENet into three baselines CyCTR, SCCAN and HDMNet for evaluation, and their scores are improved by large margins, , the 1-shot performance of SCCAN can be improved by 3.0%+ on both PASCAL-5i and COCO-20i . The code is available at https://github.com/Sam1224/AENet",
    "checked": true,
    "id": "876e3cb192c177a59396e0509b74f7303cb77d2e",
    "semantic_title": "eliminating feature ambiguity for few-shot segmentation",
    "citation_count": 1,
    "authors": [
      "Qianxiong Xu*",
      "Guosheng Lin",
      "Chen Change Loy",
      "Cheng Long",
      "Ziyue Li",
      "Rui Zhao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/454_ECCV_2024_paper.php": {
    "title": "Soft Prompt Generation for Domain Generalization",
    "volume": "main",
    "abstract": "Large pre-trained vision language models (VLMs) have shown impressive zero-shot ability on downstream tasks with manually designed prompt. To further adapt VLMs to downstream tasks, soft prompt is proposed to replace manually designed prompt, which undergoes fine-tuning based on specific domain data. Prior prompt learning methods primarily learn a fixed prompt or residuled prompt from training samples. However, the learned prompts lack diversity and ignore information about unseen domains. In this paper, we reframe the prompt learning framework from a generative perspective and propose a simple yet efficient method for the Domain Generalization (DG) task, namely Soft Prompt Generation (SPG). Specifically, SPG consists of a two-stage training phase and an inference phase. During the training phase, we introduce soft prompt label for each domain, aiming to incorporate the generative model domain knowledge. During the inference phase, the generator of the generative model is employed to obtain instance-specific soft prompts for the unseen target domain. Extensive experiments on five domain generalization benchmarks of three DG tasks demonstrate that SPG achieves state-of-the-art performance. The code is available at https://github.com/renytek13/Soft-Prompt-Generation-with-CGAN",
    "checked": true,
    "id": "c36cb4a41369369d837ea170397f7818d02150dd",
    "semantic_title": "soft prompt generation for domain generalization",
    "citation_count": 1,
    "authors": [
      "Shuanghao Bai*",
      "Yuedi Zhang",
      "Wanqi Zhou",
      "Zhirong Luan",
      "Badong Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/457_ECCV_2024_paper.php": {
    "title": "Shedding More Light on Robust Classifiers under the lens of Energy-based Models",
    "volume": "main",
    "abstract": "By reinterpreting a robust discriminative classifier as Energy-based Model (EBM), we offer a new take on the dynamics of adversarial training (AT). Our analysis of the energy landscape during AT reveals that untargeted attacks generate adversarial images much more in-distribution (lower energy) than the original data from the point of view of the model. Conversely, we observe the opposite for targeted attacks. On the ground of our thorough analysis, we present new theoretical and practical results that show how interpreting AT energy dynamics unlocks a better understanding: (1) AT dynamic is governed by three phases and robust overfitting occurs in the third phase with a drastic divergence between natural and adversarial energies (2) by rewriting TRADES loss in terms of energies, we show that TRADES implicitly alleviates overfitting by means of aligning the natural energy with the adversarial one (3) we empirically show that all recent state-of-the-art robust classifiers are smoothing the energy landscape and we reconcile a variety of studies about understanding AT and weighting the loss function under the umbrella of EBMs. Motivated by rigorous evidence, we propose Weighted Energy Adversarial Training (WEAT), a novel sample weighting scheme that yields robust accuracy matching the state-of-the-art on multiple benchmarks such as CIFAR-10 and SVHN and going beyond in CIFAR-100 and Tiny-ImageNet. We further show that robust classifiers vary in the intensity and quality of their generative capabilities, and offer a simple method to push this capability, reaching a remarkable Inception Score (IS) and FID using a robust classifier without training for generative modeling. The code to reproduce our results is available at github.com/OmnAI-Lab/Robust-Classifiers-under-the-lens-of-EBM",
    "checked": true,
    "id": "4ebc229abb15553f5c54465da222cc73b0ab5eac",
    "semantic_title": "shedding more light on robust classifiers under the lens of energy-based models",
    "citation_count": 1,
    "authors": [
      "Mujtaba Hussain Mirza*",
      "Maria Rosaria Briglia*",
      "Senad Beadini*",
      "Iacopo Masi*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/465_ECCV_2024_paper.php": {
    "title": "LGM: Large Multi-View Gaussian Model for High-Resolution 3D Content Creation",
    "volume": "main",
    "abstract": "3D content creation has achieved significant progress in terms of both quality and speed. Although current feed-forward models can produce 3D objects in seconds, their resolution is constrained by the intensive computation required during training. In this paper, we introduce Large Multi-View Gaussian Model (LGM), a novel framework designed to generate high-resolution 3D models from text prompts or single-view images. Our key insights are two-fold: 1) 3D Representation: We propose multi-view Gaussian features as an efficient yet powerful representation, which can then be fused together for differentiable rendering. 2) 3D Backbone: We present an asymmetric U-Net as a high-throughput backbone operating on multi-view images, which can be produced from text or single-view image input by leveraging multi-view diffusion models. Extensive experiments demonstrate the high fidelity and efficiency of our approach. Notably, we maintain the fast speed to generate 3D objects within 5 seconds while boosting the training resolution to 512, thereby achieving high-resolution 3D content generation. Our project page is available at https://me.kiui.moe/ lgm/",
    "checked": true,
    "id": "11665dbecb17ef4d3d71b75b8666ce0e61bd43fa",
    "semantic_title": "lgm: large multi-view gaussian model for high-resolution 3d content creation",
    "citation_count": 158,
    "authors": [
      "Jiaxiang Tang*",
      "Zhaoxi Chen",
      "Xiaokang Chen",
      "Tengfei Wang",
      "Gang Zeng",
      "Ziwei Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/471_ECCV_2024_paper.php": {
    "title": "Mahalanobis Distance-based Multi-view Optimal Transport for Multi-view Crowd Localization",
    "volume": "main",
    "abstract": "Multi-view crowd localization predicts the ground locations of all people in the scene. Typical methods usually estimate the crowd density maps on the ground plane first, and then obtain the crowd locations. However, existing methods' performances are limited by the ambiguity of the density maps in crowded areas, where local peaks can be smoothed away. To mitigate the weakness of density map supervision, optimal transport-based point supervision methods have been proposed in the single-image crowd localization tasks, but have not been explored for multi-view crowd localization yet. Thus, in this paper, we propose a novel Mahalanobis distance-based multi-view optimal transport (M-MVOT) loss specifically designed for multi-view crowd localization. First, we replace the Euclidean-based transport cost with the Mahalanobis distance, which defines elliptical iso-contours in the cost function whose long-axis and short-axis directions are guided by the view ray direction. Second, the object-to-camera distance in each view is used to adjust the optimal transport cost of each location further, where the wrong predictions far away from the camera are more heavily penalized. Finally, we propose a strategy to consider all the input camera views in the model loss (M-MVOT) by computing the optimal transport cost for each ground-truth point based on its closest camera. Experiments demonstrate the advantage of the proposed method over density map-based or common Euclidean distance-based optimal transport loss on several multi-view crowd localization datasets",
    "checked": true,
    "id": "dc15011f8547879f863d6da9a769621b2c6f8213",
    "semantic_title": "mahalanobis distance-based multi-view optimal transport for multi-view crowd localization",
    "citation_count": 0,
    "authors": [
      "Qi Zhang",
      "Kaiyi Zhang",
      "Antoni B. Chan",
      "Hui Huang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/484_ECCV_2024_paper.php": {
    "title": "RAW-Adapter: Adapting Pretrained Visual Model to Camera RAW Images",
    "volume": "main",
    "abstract": "sRGB images are now the predominant choice for pre-training visual models in computer vision research, owing to their ease of acquisition and efficient storage. Meanwhile, the advantage of RAW images lies in their rich physical information under variable real-world challenging lighting conditions. For computer vision tasks directly based on camera RAW data, most existing studies adopt methods of integrating image signal processor (ISP) with backend networks, yet often overlook the interaction capabilities between the ISP stages and subsequent networks. Drawing inspiration from ongoing adapter research in NLP and CV areas, we introduce RAW-Adapter, a novel approach aimed at adapting sRGB pre-trained models to camera RAW data. RAW-Adapter comprises input-level adapters that employ learnable ISP stages to adjust RAW inputs, as well as model-level adapters to build connections between ISP stages and subsequent high-level networks. Additionally, RAW-Adapter is a general framework that could be used in various computer vision frameworks. Abundant experiments under different lighting conditions have shown our algorithm's state-of-the-art (SOTA) performance, demonstrating its effectiveness and efficiency across a range of real-world and synthetic datasets. Code is available at this url",
    "checked": false,
    "id": "189e4c59782255f06f126af82cc1a9b00ab9f365",
    "semantic_title": "raw-adapter: adapting pre-trained visual model to camera raw images",
    "citation_count": 0,
    "authors": [
      "Ziteng Cui*",
      "Tatsuya Harada"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/490_ECCV_2024_paper.php": {
    "title": "SLEDGE: Synthesizing Driving Environments with Generative Models and Rule-Based Traffic",
    "volume": "main",
    "abstract": "SLEDGE is the first generative simulator for vehicle motion planning trained on real-world driving logs. Its core component is a learned model that is able to generate agent bounding boxes and lane graphs. The model's outputs serve as an initial state for rule-based traffic simulation. The unique properties of the entities to be generated for SLEDGE, such as their connectivity and variable count per scene, render the naive application of most modern generative models to this task non-trivial. Therefore, together with a systematic study of existing lane graph representations, we introduce a novel raster-to-vector autoencoder. It encodes agents and the lane graph into distinct channels in a rasterized latent map. This facilitates both lane-conditioned agent generation and combined generation of lanes and agents with a Diffusion Transformer. Using generated entities in SLEDGE enables greater control over the simulation, e.g. upsampling turns or increasing traffic density. Further, SLEDGE can support 500m long routes, a capability not found in existing data-driven simulators like nuPlan. It presents new challenges for planning algorithms, evidenced by failure rates of over 40% for PDM, the winner of the 2023 nuPlan challenge, when tested on hard routes and dense traffic generated by our model. Compared to nuPlan, SLEDGE requires 500× less storage to set up (¡4 GB), making it a more accessible option and helping with democratizing future research in this field",
    "checked": true,
    "id": "a0fba82ec86a08f4ea7ea4fc3b555bd26a345397",
    "semantic_title": "sledge: synthesizing driving environments with generative models and rule-based traffic",
    "citation_count": 1,
    "authors": [
      "Kashyap Chitta*",
      "Daniel Dauner",
      "Andreas Geiger"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/497_ECCV_2024_paper.php": {
    "title": "AFreeCA: Annotation-Free Counting for All",
    "volume": "main",
    "abstract": "Object counting methods typically rely on manually annotated datasets. The cost of creating such datasets has restricted the versatility of these networks to count objects from specific classes (such as humans or penguins), and counting objects from diverse categories remains a challenge. The availability of robust text-to-image latent diffusion models (LDMs) raises the question of whether these models can be utilized to generate counting datasets. However, LDMs struggle to create images with an exact number of objects based solely on text prompts but they can be used to offer a dependable sorting signal by adding and removing objects within an image. Leveraging this data, we initially introduce an unsupervised sorting methodology to learn object-related features that are subsequently refined and anchored for counting purposes using counting data generated by LDMs. Further, we present a density classifier-guided method for dividing an image into patches containing objects that can be reliably counted. Consequently, we can generate counting data for any type of object and count them in an unsupervised manner. Our approach outperforms unsupervised and few-shot alternatives and is not restricted to specific object classes for which counting data is available. Code available at: github.com/adrian-dalessandro/AFreeCA",
    "checked": true,
    "id": "8661397c2520acd06fff2ddbbc48359c3227d819",
    "semantic_title": "afreeca: annotation-free counting for all",
    "citation_count": 0,
    "authors": [
      "Adriano D'Alessandro*",
      "Ali Mahdavi-Amiri",
      "Ghassan Hamarneh"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/499_ECCV_2024_paper.php": {
    "title": "Adversarially Robust Distillation by Reducing the Student-Teacher Variance Gap",
    "volume": "main",
    "abstract": "Adversarial robustness generally relies on large-scale architectures and datasets, hindering resource-efficient deployment. For scalable solutions, adversarially robust knowledge distillation has emerged as a principle strategy, facilitating the transfer of robustness from a large-scale teacher model to a lightweight student model. However, existing works focus solely on sample-to-sample alignment of features or predictions between the teacher and student models, overlooking the vital role of their statistical alignment. Thus, we propose a novel adversarially robust knowledge distillation method that integrates the alignment of feature distributions between the teacher and student backbones under adversarial and clean sample sets. To motivate our idea, for an adversarially trained model (, student or teacher), we show that the robust accuracy (evaluated on testing adversarial samples under an increasing perturbation radius) correlates negatively with the gap between the feature variance evaluated on testing adversarial samples and testing clean samples. Such a negative correlation exhibits a strong linear trend, suggesting that aligning the feature covariance of the student model toward the feature covariance of the teacher model should improve the adversarial robustness of the student model by reducing the variance gap. A similar trend is observed by reducing the variance gap between the gram matrices of the student and teacher models. Extensive evaluations highlight the state-of-the-art adversarial robustness and natural performance of our method across diverse datasets and distillation scenarios",
    "checked": true,
    "id": "ddcc8743009d9da0306f6f372bed8f19ba96c705",
    "semantic_title": "adversarially robust distillation by reducing the student-teacher variance gap",
    "citation_count": 0,
    "authors": [
      "Junhao Dong",
      "Piotr Koniusz*",
      "Junxi Chen",
      "Yew-Soon Ong*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/501_ECCV_2024_paper.php": {
    "title": "LN3Diff: Scalable Latent Neural Fields Diffusion for Speedy 3D Generation",
    "volume": "main",
    "abstract": "The field of neural rendering has witnessed significant progress with advancements in generative models and differentiable rendering techniques. Though 2D diffusion has achieved success, a unified 3D diffusion pipeline remains unsettled. This paper introduces a novel framework called to address this gap and enable fast, high-quality, and generic conditional 3D generation. Our approach harnesses a 3D-aware architecture and variational autoencoder (VAE) to encode the input image(s) into a structured, compact, and 3D latent space. The latent is decoded by a transformer-based decoder into a high-capacity 3D neural field. Through training a diffusion model on this 3D-aware latent space, our method achieves superior performance on Objaverse, ShapeNet and FFHQ for conditional 3D generation. Moreover, it surpasses existing 3D diffusion methods in terms of inference speed, requiring no per-instance optimization. Video demos can be found on our project webpage: https://nirvanalan.github.io/projects/ ln3diff",
    "checked": true,
    "id": "c115fe03ce13e8cd7a1809e4325f95c36c06ef23",
    "semantic_title": "ln3diff: scalable latent neural fields diffusion for speedy 3d generation",
    "citation_count": 5,
    "authors": [
      "Yushi Lan",
      "Fangzhou Hong",
      "Shuai Yang",
      "Shangchen Zhou",
      "Xuyi Meng",
      "Bo Dai",
      "Xingang Pan",
      "Chen Change Loy*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/502_ECCV_2024_paper.php": {
    "title": "Hierarchical Temporal Context Learning for Camera-based Semantic Scene Completion",
    "volume": "main",
    "abstract": "Camera-based 3D semantic scene completion (SSC) is pivotal for predicting complicated 3D layouts with limited 2D image observations. The existing mainstream solutions generally leverage temporal information by roughly stacking history frames to supplement the current frame, such straightforward temporal modeling inevitably diminishes valid clues and increases learning difficulty. To address this problem, we present HTCL, a novel Hierarchical Temporal Context Learning paradigm for improving camera-based semantic scene completion. The primary innovation of this work involves decomposing temporal context learning into two hierarchical steps: (a) cross-frame affinity measurement and (b) affinity-based dynamic refinement. Firstly, to separate critical relevant context from redundant information, we introduce the pattern affinity with scale-aware isolation and multiple independent learners for fine-grained contextual correspondence modeling. Subsequently, to dynamically compensate for incomplete observations, we adaptively refine the feature sampling locations based on initially identified locations with high affinity and their neighboring relevant regions. Our method ranks 1st on the SemanticKITTI benchmark and even surpasses LiDAR-based methods in terms of mIoU on the OpenOccupancy benchmark. Our code is available on https://github.com/Arlo0o/HTCL",
    "checked": true,
    "id": "fcf89076b1e8f4b2850e6784fc23c88fae5aac52",
    "semantic_title": "hierarchical temporal context learning for camera-based semantic scene completion",
    "citation_count": 1,
    "authors": [
      "Bohan Li*",
      "Jiajun Deng",
      "Wenyao Zhang",
      "Zhujin Liang",
      "Dalong Du",
      "Xin Jin",
      "Wenjun Zeng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/521_ECCV_2024_paper.php": {
    "title": "Equi-GSPR: Equivariant SE(3) Graph Network Model for Sparse Point Cloud Registration",
    "volume": "main",
    "abstract": "Point cloud registration is a foundational task for 3D alignment and reconstruction applications. While both traditional and learning-based registration approaches have succeeded, leveraging the intrinsic symmetry of point cloud data, including rotation equivariance, has received insufficient attention. This prohibits the model from learning effectively, resulting in a requirement for more training data and increased model complexity. To address these challenges, we propose a graph neural network model embedded with a local Spherical Euclidean 3D equivariance property through SE(3) message passing based propagation. Our model is composed mainly of a descriptor module, equivariant graph layers, match similarity, and the final regression layers. Such modular design enables us to utilize sparsely sampled input points and initialize the descriptor by self-trained or pre-trained geometric feature descriptors easily. Experiments conducted on the 3DMatch and KITTI datasets exhibit the compelling and robust performance of our model compared to state-of-the-art approaches, while the model complexity remains relatively low at the same time",
    "checked": true,
    "id": "ea350944d25562cfcf1832007b31c040c0e6715a",
    "semantic_title": "equi-gspr: equivariant se(3) graph network model for sparse point cloud registration",
    "citation_count": 0,
    "authors": [
      "Xueyang Kang*",
      "Zhaoliang Luan",
      "Kourosh Khoshelham",
      "Bing WANG*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/523_ECCV_2024_paper.php": {
    "title": "GTP-4o: Modality-prompted Heterogeneous Graph Learning for Omni-modal Biomedical Representation",
    "volume": "main",
    "abstract": "Recent advances in learning multi-modal representation have witnessed the success in biomedical domains. While established techniques enable handling multi-modal information, the challenges are posed when extended to various clinical modalities and practical modality-missing setting due to the inherent modality gaps. To tackle these, we propose an innovative Modality-prompted Heterogeneous Graph for Omni-modal Learning (GTP-4o), which embeds the numerous disparate clinical modalities into a unified representation, completes the deficient embedding of missing modality and reformulates the cross-modal learning with a graph-based aggregation. Specially, we establish a heterogeneous graph embedding to explicitly capture the diverse semantic properties on both the modality-specific features (nodes) and the cross-modal relations (edges). Then, we design a modality-prompted completion that enables completing the inadequate graph representation of missing modality through a graph prompting mechanism, which generates hallucination graphic topologies to steer the missing embedding towards the intact representation. Through the completed graph, we meticulously develop a knowledge-guided hierarchical cross-modal aggregation consisting of a global meta-path neighbouring to uncover the potential heterogeneous neighbors along the pathways driven by domain knowledge, and a local multi-relation aggregation module for the comprehensive cross-modal interaction across various heterogeneous relations. We assess the efficacy of our methodology on rigorous benchmarking experiments against prior state-of-the-arts. In a nutshell, GTP-4o presents an initial foray into the intriguing realm of embedding, relating and perceiving the heterogeneous patterns from various clinical modalities holistically via a graph theory. Project page: https://gtp-4-o.github.io/",
    "checked": true,
    "id": "8967a871d24eaa982539fdfb39b2738e7f7a00c5",
    "semantic_title": "gtp-4o: modality-prompted heterogeneous graph learning for omni-modal biomedical representation",
    "citation_count": 5,
    "authors": [
      "Chenxin Li*",
      "Xinyu Liu",
      "Cheng Wang",
      "Yifan Liu",
      "Weihao Yu",
      "Jing Shao",
      "Yixuan Yuan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/524_ECCV_2024_paper.php": {
    "title": "PromptCCD: Learning Gaussian Mixture Prompt Pool for Continual Category Discovery",
    "volume": "main",
    "abstract": "We tackle the problem of Continual Category Discovery (CCD), which aims to automatically discover novel categories in a continuous stream of unlabeled data while mitigating the challenge of catastrophic forgetting – an open problem that persists even in conventional, fully supervised continual learning. To address this challenge, we propose PromptCCD, a simple yet effective framework that utilizes a Gaussian Mixture Model (GMM) as a prompting method for CCD. At the core of PromptCCD lies the Gaussian Mixture Prompting (GMP) module, which acts as a dynamic pool that updates over time to facilitate representation learning and prevent forgetting during category discovery. Moreover, GMP enables on-the-fly estimation of category numbers, allowing PromptCCD to discover categories in unlabeled data without prior knowledge of the category numbers. We extend the standard evaluation metric for Generalized Category Discovery (GCD) to CCD and benchmark state-of-the-art methods on diverse public datasets. PromptCCD significantly outperforms existing methods, demonstrating its effectiveness. Project page: https://visual-ai.github.io/promptccd",
    "checked": true,
    "id": "2c9a49d66422afccb69928a66a3aae0265a4e1e7",
    "semantic_title": "promptccd: learning gaussian mixture prompt pool for continual category discovery",
    "citation_count": 1,
    "authors": [
      "Fernando Julio Cendra",
      "Bingchen Zhao",
      "Kai Han*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/529_ECCV_2024_paper.php": {
    "title": "Sapiens: Foundation for Human Vision Models",
    "volume": "main",
    "abstract": "We present Sapiens, a family of models for four fundamental human-centric vision tasks – 2D pose estimation, body-part segmentation, depth estimation, and surface normal prediction. Our models natively support 1K high-resolution inference and are extremely easy to adapt for individual tasks by simply fine-tuning foundation models pretrained on over 300 million in-the-wild human images. We observe that, given the same computational budget, self-supervised pretraining on a curated dataset of human images significantly boosts the performance for a diverse set of human-centric tasks. The resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. Our simple model design also brings scalability – model performance across tasks significantly improves as we scale the number of parameters from 0.3 to 2 billion. Sapiens consistently surpasses existing complex baselines across various human-centric benchmarks. Specifically, we achieve significant improvements over the prior state-of-the-art on Humans-5K (pose) by 7.6 mAP, Humans-2K (part-seg) by 17.1 mIoU, Hi4D (depth) by 22.4% relative RMSE, and THuman2 (normal) by 53.5% relative angular error",
    "checked": true,
    "id": "19de22fc50c5f8609b2c31854f5e568fa58c6041",
    "semantic_title": "sapiens: foundation for human vision models",
    "citation_count": 0,
    "authors": [
      "Rawal Khirodkar*",
      "Timur Bagautdinov",
      "Julieta Martinez",
      "Zhaoen Su",
      "Austin T James",
      "Peter Selednik",
      "Stuart Anderson",
      "Shunsuke Saito"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/540_ECCV_2024_paper.php": {
    "title": "Linearly Controllable GAN: Unsupervised Feature Categorization and Decomposition for Image Generation and Manipulation",
    "volume": "main",
    "abstract": "This paper introduces an approach to linearly controllable generative adversarial networks (LC-GAN) driven by unsupervised learning. Departing from traditional methods relying on supervision signals or post-processing for latent feature disentanglement, our proposed technique enables unsupervised learning using only image data through contrastive feature categorization and spectral regularization. In our framework, the discriminator constructs geometry- and appearance-related feature spaces using a combination of image augmentation and contrastive representation learning. Leveraging these feature spaces, the generator autonomously categorizes input latent codes into geometry- and appearance-related features. Subsequently, the categorized features undergo projection into a subspace via our proposed spectral regularization, with each component controlling a distinct aspect of the generated image. Beyond providing fine-grained control over the generative model, our approach achieves state-of-the-art image generation quality on benchmark datasets, including FFHQ, CelebA-HQ, and AFHQ-V2",
    "checked": true,
    "id": "984621a4a6ba241dd828d88eefc05f59adcc76f2",
    "semantic_title": "linearly controllable gan: unsupervised feature categorization and decomposition for image generation and manipulation",
    "citation_count": 0,
    "authors": [
      "sehyung lee*",
      "Mijung Kim",
      "Yeongnam Chae",
      "Bjorn Stenger"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/549_ECCV_2024_paper.php": {
    "title": "Generating Human Interaction Motions in Scenes with Text Control",
    "volume": "main",
    "abstract": "We present , a text-controlled scene-aware motion generation method based on denoising diffusion models. Previous text-to-motion methods focus on characters in isolation without considering scenes due to the limited availability of datasets that include motion, text descriptions, and interactive scenes. Our approach begins with pre-training a scene-agnostic text-to-motion diffusion model, emphasizing goal-reaching constraints on large-scale motion-capture datasets. We then enhance this model with a scene-aware component, fine-tuned using data augmented with detailed scene information, including ground plane and object shapes. To facilitate training, we embed annotated navigation and interaction motions within scenes. The proposed method produces realistic and diverse human-object interactions, such as navigation and sitting, in different scenes with various object shapes, orientations, initial body positions, and poses. Extensive experiments demonstrate that our approach surpasses prior techniques in terms of the plausibility of human-scene interactions and the realism and variety of the generated motions. Code and data are available at",
    "checked": true,
    "id": "422fe25c07c7b622339960e74c59af586054db1e",
    "semantic_title": "generating human interaction motions in scenes with text control",
    "citation_count": 8,
    "authors": [
      "Hongwei Yi*",
      "Justus Thies",
      "Michael J. Black",
      "Xue Bin Peng",
      "Davis Rempe*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/553_ECCV_2024_paper.php": {
    "title": "NOVUM: Neural Object Volumes for Robust Object Classification",
    "volume": "main",
    "abstract": "Discriminative models for object classification typically learn image-based representations that do not capture the compositional and 3D nature of objects. In this work, we show that explicitly integrating 3D compositional object representations into deep networks for image classification leads to a largely enhanced generalization in out-of-distribution scenarios. In particular, we introduce a novel architecture, referred to as , that consists of a feature extractor and a neural object volume for every target object class. Each neural object volume is a composition of 3D Gaussians that emit feature vectors. This compositional object representation allows for a highly robust and fast estimation of the object class by independently matching the features of the 3D Gaussians of each category to features extracted from an input image. Additionally, the object pose can be estimated via inverse rendering of the corresponding neural object volume. To enable the classification of objects, the neural features at each 3D Gaussian are trained discriminatively to be distinct from (i) the features of 3D Gaussians in other categories, (ii) features of other 3D Gaussians of the same object, and (iii) the background features. Our experiments show that offers intriguing advantages over standard architectures due to the 3D compositional structure of the object representation, namely: (1) An exceptional robustness across a spectrum of real-world and synthetic out-of-distribution shifts and (2) an enhanced human interpretability compared to standard models, all while maintaining real-time inference and a competitive accuracy on in-distribution data. Code and model can be found at /GenIntel/NOVUM",
    "checked": true,
    "id": "7f6b686b1a9ae3983dd4facfb23038d49f16dcc4",
    "semantic_title": "novum: neural object volumes for robust object classification",
    "citation_count": 5,
    "authors": [
      "Artur Jesslen*",
      "Guofeng Zhang",
      "Angtian Wang",
      "Wufei Ma",
      "Alan Yuille",
      "Adam Kortylewski"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/560_ECCV_2024_paper.php": {
    "title": "Align before Collaborate: Mitigating Feature Misalignment for Robust Multi-Agent Perception",
    "volume": "main",
    "abstract": "Collaborative perception has received widespread attention recently since it enhances the perception ability of autonomous vehicles via inter-agent information sharing. However, the performance of existing systems is hindered by the unavoidable collaboration noises, which induce feature-level spatial misalignment over the collaborator-shared information. In this paper, we propose a model-agnostic and lightweight plugin to mitigate the feature-level misalignment issue, called dynamic feature alignment (NEAT). The merits of the NEAT plugin are threefold. First, we introduce an importance-guided query proposal to predict potential foreground regions with space-channel semantics and exclude environmental redundancies. On this basis, a deformable feature alignment is presented to explicitly align the collaborator-shared features through query-aware spatial associations, aggregating multi-grained visual clues with corrective mismatch properties. Ultimately, we perform a region cross-attention reinforcement to facilitate aligned representation diffusion and achieve global feature semantic enhancement. NEAT can be readily inserted into existing collaborative perception procedures and significantly improves the robustness of vanilla baselines against pose errors and transmission delay. Extensive experiments on four collaborative 3D object detection datasets under noisy settings confirm that NEAT provides consistent gains for most methods with distinct structures",
    "checked": true,
    "id": "8ad27f3a29907cc44e30d414460d2310957f6804",
    "semantic_title": "align before collaborate: mitigating feature misalignment for robust multi-agent perception",
    "citation_count": 0,
    "authors": [
      "Dingkang Yang",
      "Dingkang Yang",
      "Ke Li",
      "Dongling Xiao",
      "Zedian Shao",
      "Peng Sun",
      "Liang Song*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/564_ECCV_2024_paper.php": {
    "title": "HIMO: A New Benchmark for Full-Body Human Interacting with Multiple Objects",
    "volume": "main",
    "abstract": "Generating human-object interactions (HOIs) is critical with the tremendous advances of digital avatars. Existing datasets are typically limited to humans interacting with a single object while neglecting the ubiquitous manipulation of multiple objects. Thus, we propose HIMO, a large-scale MoCap dataset of full-body human interacting with multiple objects, containing 3.3K 4D HOI sequences and 4.08M 3D HOI frames. We also annotate HIMO with detailed textual descriptions and temporal segments, benchmarking two novel tasks of HOI synthesis conditioned on either the whole text prompt or the segmented text prompts as fine-grained timeline control. To address these novel tasks, we propose a dual-branch conditional diffusion model with a mutual interaction module for HOI synthesis. Besides, an auto-regressive generation pipeline is also designed to obtain smooth transitions between HOI segments. Experimental results demonstrate the generalization ability to unseen object geometries and temporal compositions. Our data, codes, and models will be publicly available for research purposes",
    "checked": true,
    "id": "d46bf8b9f4388f017f70a3f2c82356fd0ca1b537",
    "semantic_title": "himo: a new benchmark for full-body human interacting with multiple objects",
    "citation_count": 0,
    "authors": [
      "Xintao Lv",
      "Liang Xu",
      "Yichao Yan*",
      "Xin Jin",
      "Congsheng Xu",
      "Wu Shuwen",
      "Yifan Liu",
      "Lincheng Li",
      "Mengxiao Bi",
      "Wenjun Zeng",
      "Xiaokang Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/565_ECCV_2024_paper.php": {
    "title": "SAIR: Learning Semantic-aware Implicit Representation",
    "volume": "main",
    "abstract": "Implicit representation of an image can map arbitrary coordinates in the continuous domain to their corresponding color values, presenting a powerful capability for image reconstruction. Nevertheless, existing implicit representation approaches only focus on building continuous appearance mapping, ignoring the continuities of the semantic information across pixels. Consequently, achieving the desired reconstruction results becomes challenging when the semantic information within input image is corrupted, such as when a large region is missing. To address the issue, we suggest learning semantic-aware implicit representation ( SAIR), that is, we make the implicit representation of each pixel rely on both its appearance and semantic information (, which object does the pixel belong to). To this end, we propose a framework with two modules: (1) a semantic implicit representation (SIR) for a corrupted image. Given an arbitrary coordinate in the continuous domain, we can obtain its respective text-aligned embedding indicating the object the pixel belongs. (2) an appearance implicit representation (AIR) based on the SIR. Given an arbitrary coordinate in the continuous domain, we can reconstruct its color whether or not the pixel is missed in the input. We validate the novel semantic-aware implicit representation method on the image inpainting task, and the extensive experiments demonstrate that our method surpasses state-of-the-art approaches by a significant margin",
    "checked": true,
    "id": "cbd3736a82ab1dfe25ef37c27cfec4da34de9be8",
    "semantic_title": "sair: learning semantic-aware implicit representation",
    "citation_count": 2,
    "authors": [
      "Canyu Zhang*",
      "Xiaoguang Li*",
      "Qing Guo*",
      "Song Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/578_ECCV_2024_paper.php": {
    "title": "ColorMNet: A Memory-based Deep Spatial-Temporal Feature Propagation Network for Video Colorization",
    "volume": "main",
    "abstract": "How to effectively explore spatial-temporal features is important for video colorization. Instead of stacking multiple frames along the temporal dimension or recurrently propagating estimated features that will accumulate errors or cannot explore information from far-apart frames, we develop a memory-based feature propagation module that can establish reliable connections with features from far-apart frames and alleviate the influence of inaccurately estimated features. To extract better features from each frame for the above-mentioned feature propagation, we explore the features from large-pretrained visual models to guide the feature estimation of each frame so that the estimated features can model complex scenarios. In addition, we note that adjacent frames usually contain similar contents. To explore this property for better spatial and temporal feature utilization, we develop a local attention module to aggregate the features from adjacent frames in a spatial-temporal neighborhood. We formulate our memory-based feature propagation module, large-pretrained visual model guided feature estimation module, and local attention module into an end-to-end trainable network (named ColorMNet) and show that it performs favorably against state-of-the-art methods on both the benchmark datasets and real-world scenarios. Our source codes and pre-trained models are available at: https://github.com/yyang181/colormnet",
    "checked": true,
    "id": "c41d4b8cfe61d546ce7288a23a6a9b890ff9a632",
    "semantic_title": "colormnet: a memory-based deep spatial-temporal feature propagation network for video colorization",
    "citation_count": 0,
    "authors": [
      "Yixin Yang",
      "Jiangxin Dong",
      "Jinhui Tang",
      "Jinshan Pan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/581_ECCV_2024_paper.php": {
    "title": "UNIC: Universal Classification Models via Multi-teacher Distillation",
    "volume": "main",
    "abstract": "Pretrained models have become a commodity and offer strong results on a broad range of tasks. In this work, we focus on classification and seek to learn a unique encoder able to take from several complementary pretrained models. We aim at even stronger generalization across a variety of classification tasks. We propose to learn such an encoder via multi-teacher distillation. We first thoroughly analyze standard distillation when driven by multiple strong teachers with complementary strengths. Guided by this analysis, we gradually propose improvements to the basic distillation setup. Among those, we enrich the architecture of the encoder with a ladder of expendable projectors, which increases the impact of intermediate features during distillation, and we introduce teacher dropping, a regularization mechanism that better balances the teachers' influence. Our final distillation strategy leads to student models of the same capacity as any of the teachers, while retaining or improving upon the performance of the best teacher for each task",
    "checked": true,
    "id": "4d1a04561776455bc2b11d0deae7ad99f5e251b2",
    "semantic_title": "unic: universal classification models via multi-teacher distillation",
    "citation_count": 1,
    "authors": [
      "Yannis Kalantidis",
      "Diane Larlus",
      "Mert Bulent Sariyildiz*",
      "Philippe Weinzaepfel",
      "Thomas LUCAS"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/589_ECCV_2024_paper.php": {
    "title": "Instance-dependent Noisy-label Learning with Graphical Model Based Noise-rate Estimation",
    "volume": "main",
    "abstract": "Deep learning faces a formidable challenge when handling noisy labels, as models tend to overfit samples affected by label noise. This challenge is further compounded by the presence of instance-dependent noise (IDN), a realistic form of label noise arising from ambiguous sample information. To address IDN, Label Noise Learning (LNL) incorporates a sample selection stage to differentiate clean and noisy-label samples. This stage uses an arbitrary criterion and a pre-defined curriculum that initially selects most samples as noisy and gradually decreases this selection rate during training. Such curriculum is sub-optimal since it does not consider the actual label noise rate in the training set. This paper addresses this issue with a new noise-rate estimation method that is easily integrated with most state-of-the-art (SOTA) LNL methods to produce a more effective curriculum. Synthetic and real-world benchmarks' results demonstrate that integrating our approach with SOTA LNL methods improves accuracy in most cases.1 1 Code is available at https://github.com/arpit2412/NoiseRateLearning. Supported by the Engineering and Physical Sciences Research Council (EPSRC) through grant EP/Y018036/1 and the Australian Research Council (ARC) through grant FT190100525",
    "checked": true,
    "id": "c6da4c98b00ddd8b2d85b31b2bda3ed7cc164690",
    "semantic_title": "instance-dependent noisy-label learning with graphical model based noise-rate estimation",
    "citation_count": 1,
    "authors": [
      "Arpit Garg*",
      "Cuong Cao Nguyen",
      "RAFAEL FELIX",
      "Thanh-Toan Do",
      "Gustavo Carneiro"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/608_ECCV_2024_paper.php": {
    "title": "Eliminating Warping Shakes for Unsupervised Online Video Stitching",
    "volume": "main",
    "abstract": "In this paper, we retarget video stitching to an emerging issue, named warping shake, when extending image stitching to video stitching. It unveils the temporal instability of warped content in non-overlapping regions, despite image stitching having endeavored to preserve the natural structures. Therefore, in most cases, even if the input videos to be stitched are stable, the stitched video will inevitably cause undesired warping shakes and affect the visual experience. To eliminate the shakes, we propose StabStitch to simultaneously realize video stitching and video stabilization in a unified unsupervised learning framework. Starting from the camera paths in video stabilization, we first derive the expression of stitching trajectories in video stitching by elaborately integrating spatial and temporal warps. Then a warp smoothing model is presented to optimize them with a comprehensive consideration regarding content alignment, trajectory smoothness, spatial consistency, and online collaboration. To establish an evaluation benchmark and train the learning framework, we build a video stitching dataset with a rich diversity in camera motions and scenes. Compared with existing stitching solutions, StabStitch exhibits significant superiority in scene robustness and inference speed in addition to stitching and stabilization performance, contributing to a robust and real-time online video stitching system. The codes and dataset are available at https://github.com/nie-lang/StabStitch",
    "checked": true,
    "id": "03dc0dd9636120f668d5a470dc083721038f5b1e",
    "semantic_title": "eliminating warping shakes for unsupervised online video stitching",
    "citation_count": 0,
    "authors": [
      "Lang Nie",
      "Chunyu Lin*",
      "Kang Liao",
      "Yun Zhang",
      "Shuaicheng Liu",
      "Rui Ai",
      "Yao Zhao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/610_ECCV_2024_paper.php": {
    "title": "Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models",
    "volume": "main",
    "abstract": "Most Large Vision-Language Models (LVLMs) enjoy the same vision vocabulary, i.e., CLIP, for common vision tasks. However, for some special task that needs dense and fine-grained perception, the CLIP-style vocabulary may encounter low efficiency in tokenizing corresponding vision knowledge and even suffer out-of-vocabulary problems. Accordingly, we propose Vary, an efficient and productive method to scale up the Vision vocabulary of LVLMs. The procedures of Vary are naturally divided into two folds: the generation and integration of a new vision vocabulary. In the first phase, we devise a vocabulary network along with a tiny decoder-only transformer to compress rich vision signals. Next, we scale up the vanilla vision vocabulary by merging the new with the original one (CLIP), enabling the LVLMs to garner new features effectively. We present frameworks with two sizes: Vary-base (7B) and Vary-toy (1.8B), both of which enjoy excellent fine-grained perception performance while maintaining great general ability",
    "checked": true,
    "id": "b240a1d8ec2860bdd7370daa3144268ce46ac018",
    "semantic_title": "vary: scaling up the vision vocabulary for large vision-language models",
    "citation_count": 54,
    "authors": [
      "Haoran Wei*",
      "Lingyu Kong",
      "Jinyue Chen",
      "Liang Zhao",
      "Zheng Ge",
      "Jinrong Yang",
      "Jianjian Sun",
      "Chunrui Han",
      "Xiangyu Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/613_ECCV_2024_paper.php": {
    "title": "Merlin: Empowering Multimodal LLMs with Foresight Minds",
    "volume": "main",
    "abstract": "Humans can foresee the future based on present observations, a skill we term as foresight minds. However, this capability remains under-explored within existing MLLMs, hindering their capacity to understand intentions behind subjects. To address this, we integrate the future modeling into MLLMs. By utilizing the trajectory, a highly structured representation, as a learning objective, we aim to equip the model to understand spatiotemporal dynamics. Inspired by the learning paradigm of LLMs, we first propose Foresight Pre-Training (FPT) that jointly learns various tasks centered on trajectories, enabling MLLMs to predict entire trajectories from a given initial observation. Then, we propose Foresight Instruction-Tuning (FIT) that requires MLLMs to reason about potential future events based on predicted trajectories. Aided by FPT and FIT, we build an unified MLLM named Merlin that supports complex future reasoning. Experiments show Merlin's foresight minds with impressive performance on both future reasoning and visual comprehension tasks. Project page: https: //ahnsun.github.io/merlin",
    "checked": true,
    "id": "40de3296157b9d7a7882b61f967e37b3cc93f197",
    "semantic_title": "merlin: empowering multimodal llms with foresight minds",
    "citation_count": 13,
    "authors": [
      "En Yu",
      "Liang Zhao",
      "YANA WEI",
      "Jinrong Yang",
      "Dongming Wu",
      "Lingyu Kong",
      "Haoran Wei",
      "Tiancai Wang",
      "Zheng Ge",
      "Xiangyu Zhang",
      "Wenbing Tao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/629_ECCV_2024_paper.php": {
    "title": "ViC-MAE: Self-Supervised Representation Learning from Images and Video with Contrastive Masked Autoencoders",
    "volume": "main",
    "abstract": "We propose , a model that combines both Masked AutoEncoders (MAE) and contrastive learning. is trained using a global representation obtained by pooling the local features learned under an MAE reconstruction loss and using this representation under a contrastive objective across images and video frames. We show that visual representations learned under generalize well to video and image classification tasks. Particularly, obtains state-of-the-art transfer learning performance from video to images on Imagenet-1k compared to the recently proposed OmniMAE by achieving a top-1 accuracy of 86% (+1.3% absolute improvement) when trained on the same data and 87.1% (+2.4% absolute improvement) when training on extra data. At the same time, outperforms most other methods on video benchmarks by obtaining 75.9% top-1 accuracy on the challenging Something something-v2 video benchmark. When training on videos and images from diverse datasets, our method maintains a balanced transfer-learning performance between video and image classification benchmarks, coming only as a close second to the best-supervised method",
    "checked": true,
    "id": "e7085ec3f5e0abd496342feec7c23c4ca0b629fb",
    "semantic_title": "vic-mae: self-supervised representation learning from images and video with contrastive masked autoencoders",
    "citation_count": 0,
    "authors": [
      "Jefferson Hernandez*",
      "Ruben Villegas",
      "Vicente Ordonez"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/632_ECCV_2024_paper.php": {
    "title": "E.T. the Exceptional Trajectory: Text-to-camera-trajectory generation with character awareness",
    "volume": "main",
    "abstract": "Stories and emotions in movies emerge through the effect of well-thought-out directing decisions, in particular camera placement and movement over time. Crafting compelling camera trajectories remains a complex iterative process, even for skilful artists. To tackle this, in this paper, we propose a dataset called the Exceptional Trajectories (E.T.) with camera trajectories along with character information and textual captions encompassing descriptions of both camera and character. To our knowledge, this is the first dataset of its kind. To show the potential applications of the E.T. dataset, we propose a diffusion-based approach, named Director, which generates complex camera trajectories from textual captions that describe the relation and synchronisation between the camera and characters. To ensure robust and accurate evaluations, we train on the E.T. dataset CLaTr, a Contrastive Language-Trajectory embedding for evaluation metrics. We posit that our proposed dataset and method significantly advance the democratization of cinematography, making it more accessible to common users",
    "checked": false,
    "id": "93a7e0defa0fc675ffb7fd6ec9f1b21cbeb1cd41",
    "semantic_title": "e.t. the exceptional trajectories: text-to-camera-trajectory generation with character awareness",
    "citation_count": 0,
    "authors": [
      "Robin Courant*",
      "Nicolas Dufour",
      "Xi WANG",
      "Marc Christie",
      "Vicky Kalogeiton"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/633_ECCV_2024_paper.php": {
    "title": "OphNet: A Large-Scale Video Benchmark for Ophthalmic Surgical Workflow Understanding",
    "volume": "main",
    "abstract": "Surgical scene perception via videos is critical for advancing robotic surgery, telesurgery, and AI-assisted surgery, particularly in ophthalmology. However, the scarcity of diverse and richly annotated video datasets has hindered the development of intelligent systems for surgical workflow analysis. Existing datasets face challenges such as small scale, lack of diversity in surgery and phase categories, and absence of time-localized annotations. These limitations impede action understanding and model generalization validation in complex and diverse real-world surgical scenarios. To address this gap, we introduce OphNet, a large-scale, expert-annotated video benchmark for ophthalmic surgical workflow understanding. OphNet features: 1) A diverse collection of 2,278 surgical videos spanning 66 types of cataract, glaucoma, and corneal surgeries, with detailed annotations for 102 unique surgical phases and 150 fine-grained operations. 2) Sequential and hierarchical annotations for each surgery, phase, and operation, enabling comprehensive understanding and improved interpretability. 3) Time-localized annotations, facilitating temporal localization and prediction tasks within surgical workflows. With approximately 285 hours of surgical videos, OphNet is about 20 times larger than the largest existing surgical workflow analysis benchmark. Code and dataset are available at: https: //minghu0830.github.io/OphNet-benchmark/",
    "checked": true,
    "id": "80ae23fba0fd41a42934107664d63e8a63362f61",
    "semantic_title": "ophnet: a large-scale video benchmark for ophthalmic surgical workflow understanding",
    "citation_count": 6,
    "authors": [
      "Ming Hu*",
      "Peng Xia",
      "Lin Wang",
      "Siyuan Yan",
      "Feilong Tang",
      "zhongxing xu",
      "Yimin Luo",
      "Kaimin Song",
      "Jurgen Leitner",
      "Xuelian Cheng",
      "Jun Cheng",
      "Chi Liu",
      "Kaijing Zhou*",
      "Zongyuan Ge*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/653_ECCV_2024_paper.php": {
    "title": "SignAvatars: A Large-scale 3D Sign Language Holistic Motion Dataset and Benchmark",
    "volume": "main",
    "abstract": "We present SignAvatars1 , the first large-scale, multi-prompt 3D sign language (SL) motion dataset designed to bridge the communication gap for Deaf and hard-of-hearing individuals. While there has been an exponentially growing number of research regarding digital communication, the majority of existing communication technologies primarily cater to spoken or written languages, instead of SL, the essential communication method for Deaf and hard-of-hearing communities. Existing SL datasets, dictionaries, and sign language production (SLP) methods are typically limited to 2D as annotating 3D models and avatars for SL is usually an entirely manual and labor-intensive process conducted by SL experts, often resulting in unnatural avatars. In response to these challenges, we compile and curate the SignAvatars dataset, which comprises 70,000 videos from 153 signers, totaling 8.34 million frames, covering both isolated signs and continuous, co-articulated signs, with multiple prompts including HamNoSys, spoken language, and words. To yield 3D holistic annotations, including meshes and biomechanically-valid poses of body, hands, and face, as well as 2D and 3D keypoints, we introduce an automated annotation pipeline operating on our large corpus of SL videos. SignAvatars facilitates various tasks such as 3D sign language recognition (SLR) and the novel 3D SL production (SLP) from diverse inputs like text scripts, individual words, and HamNoSys notation. Hence, to evaluate the potential of SignAvatars, we further propose a unified benchmark of 3D SL holistic motion production. We believe that this work is a significant step forward towards bringing the digital world to the Deaf and hard-of-hearing communities as well as people interacting with them. 1 https://signavatars.github.io/",
    "checked": true,
    "id": "33aa057a8268a5965e1682904f4148fbd16cf1ee",
    "semantic_title": "signavatars: a large-scale 3d sign language holistic motion dataset and benchmark",
    "citation_count": 0,
    "authors": [
      "Zhengdi Yu",
      "Shaoli Huang*",
      "yongkang cheng",
      "Tolga Birdal"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/666_ECCV_2024_paper.php": {
    "title": "AttnZero: Efficient Attention Discovery for Vision Transformers",
    "volume": "main",
    "abstract": "In this paper, we present AttnZero, the first framework for automatically discovering efficient attention modules tailored for Vision Transformers (ViTs). While traditional self-attention in ViTs suffers from quadratic computation complexity, linear attention offers a more efficient alternative with linear complexity approximation. However, existing hand-crafted linear attention suffers from performance degradation. To address these issues, our AttnZero constructs search spaces and employs evolutionary algorithms to discover potential linear attention formulations. Specifically, our search space consists of six kinds of computation graphs and advanced activation, normalize, and binary operators. To enhance generality, we derive results of candidate attention applied to multiple advanced ViTs as the multi-objective for the evolutionary search. To expedite the search process, we utilize program checking and rejection protocols to filter out unpromising candidates swiftly. Additionally, we develop Attn-Bench-101, which provides precomputed performance of 2,000 attentions in the search spaces, enabling us to summarize attention design insights. Experimental results demonstrate that the discovered AttnZero module generalizes well to different tasks and consistently achieves improved performance across various ViTs. For instance, the tiny model of DeiT—PVT—Swin—CSwin trained with AttnZero on ImageNet reaches 74.9%—78.1%—82.1%—82.9% top-1 accuracy. Codes at: https://github.com/lliai/AttnZero",
    "checked": false,
    "id": "cd6734fd52338aad94ad8c369ed41ad201f229b3",
    "semantic_title": "abstract 6183: exploring the role of microenvironment context on diagnostic accuracy in tissue microarray core selection",
    "citation_count": 0,
    "authors": [
      "Lujun Li",
      "Zimian Wei*",
      "Peijie Dong",
      "Wenhan Luo",
      "Wei Xue",
      "Qifeng Liu*",
      "Yike Guo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/668_ECCV_2024_paper.php": {
    "title": "Auto-GAS: Automated Proxy Discovery for Training-free Generative Architecture Search",
    "volume": "main",
    "abstract": "In this paper, we introduce Auto-GAS, the first training-free Generative Architecture Search (GAS) framework enabled by an auto-discovered proxy. Generative models like Generative Adversarial Networks (GANs) are now widely used in many real-time applications. Previous GAS methods use differentiable or evolutionary search to find optimal GAN generators for fast inference and memory efficiency. However, the high computational overhead of these training-based GAS techniques limits their adoption. To improve search efficiency, we explore training-free GAS but find existing zero-cost proxies designed for classification tasks underperform on generation benchmarks. To address this challenge, we develop a custom proxy search framework tailored for GAS tasks to enhance predictive power. Specifically, we construct an information-aware proxy that takes feature statistics as inputs and utilizes advanced transform, encoding, reduction, and augment operations to represent candidate proxies. Then, we employ an evolutionary algorithm to perform crossover and mutation on superior candidates within the population based on correlation evaluation. Finally, we perform generator search without training using the optimized proxy. Thus, Auto-GAS enables automated proxy discovery for GAS while significantly accelerating the search before training stage. Extensive experiments on image generation and image-to-image translation tasks demonstrate that Auto-GAS strikes superior accuracy-speed tradeoffs over state-of-the-art methods. Remarkably, Auto-GAS achieves competitive scores with 110× faster search than GAN Compression. Code at: https://github.com/lliai/Auto-GAS",
    "checked": false,
    "id": "4cbcd2d7d89878502a28a72ecaf6391265c40ba9",
    "semantic_title": "saswot: real-time semantic segmentation architecture search without training",
    "citation_count": 6,
    "authors": [
      "Lujun Li",
      "Haosen Sun",
      "Shiwen Li",
      "Peijie Dong",
      "Wenhan Luo",
      "Wei Xue",
      "Qifeng Liu*",
      "Yike Guo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/676_ECCV_2024_paper.php": {
    "title": "Auto-DAS: Automated Proxy Discovery for Training-free Distillation-aware Architecture Search",
    "volume": "main",
    "abstract": "Distillation-aware Architecture Search (DAS) seeks to discover the ideal student architecture that delivers superior performance by distilling knowledge from a given teacher model. Previous DAS methods involve time-consuming training-based search processes. Recently, the training-free DAS method (, DisWOT) proposes KD-based proxies and achieves significant search acceleration. However, we observe that DisWOT suffers from limitations such as the need for manual design and poor generalization to diverse architectures, such as the Vision Transformer (ViT). To address these issues, we present Auto-DAS, an automatic proxy discovery framework using an Evolutionary Algorithm (EA) for training-free DAS. Specifically, we empirically find that proxies conditioned on student instinct statistics and teacher-student interaction statistics can effectively predict distillation accuracy. Then, we represent the proxy with computation graphs and construct the proxy search space using instinct and interaction statistics as inputs. To identify promising proxies, our search space incorporates various types of basic transformations and network distance operators inspired by previous proxy and KD-loss designs. Next, our EA initializes populations, evaluates, performs crossover and mutation operations, and selects the best correlation candidate with distillation accuracy. We introduce an adaptive-elite selection strategy to enhance search efficiency and strive for a balance between exploitation and exploration. Finally, we conduct training-free DAS with discovered proxy before the optimal student distillation phase. In this way, our auto-discovery framework eliminates the need for manual design and tuning, while also adapting to different search spaces through direct correlation optimization. Extensive experiments demonstrate that Auto-DAS generalizes well to various architectures and search spaces (, ResNet, ViT, NAS-Bench-101, and NAS-Bench-201), achieving state-of-the-art results in both ranking correlation and final searched accuracy. Code at: https://github.com/lliai/Auto-DAS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haosen Sun",
      "Lujun Li*",
      "Peijie Dong",
      "Zimian Wei",
      "Shitong Shao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/698_ECCV_2024_paper.php": {
    "title": "UniDream: Unifying Diffusion Priors for Relightable Text-to-3D Generation",
    "volume": "main",
    "abstract": "Recent advancements in text-to-3D generation technology have significantly advanced the conversion of textual descriptions into imaginative well-geometrical and finely textured 3D objects. Despite these developments, a prevalent limitation arises from the use of RGB data in diffusion or reconstruction models, which often results in models with inherent lighting and shadows effects that detract from their realism, thereby limiting their usability in applications that demand accurate relighting capabilities. To bridge this gap, we present UniDream, a text-to-3D generation framework by incorporating unified diffusion priors. Our approach consists of three main components: (1) a dual-phase training process to get albedo-normal aligned multi-view diffusion and reconstruction models, (2) a progressive generation procedure for geometry and albedo-textures based on Score Distillation Sample (SDS) using the trained reconstruction and diffusion models, and (3) an innovative application of SDS for finalizing PBR generation while keeping a fixed albedo based on Stable Diffusion model. Extensive evaluations demonstrate that UniDream surpasses existing methods in generating 3D objects with clearer albedo textures, smoother surfaces, enhanced realism, and superior relighting capabilities. The project homepage is at: https://UniDream.github.io",
    "checked": true,
    "id": "210e63599d49abdb848a4440d4244cdcdedeadff",
    "semantic_title": "unidream: unifying diffusion priors for relightable text-to-3d generation",
    "citation_count": 25,
    "authors": [
      "Zexiang Liu",
      "Yangguang Li",
      "Youtian Lin",
      "Xin Yu",
      "Sida Peng",
      "Yan-Pei Cao",
      "Xiaojuan Qi",
      "Xiaoshui Huang",
      "Ding Liang*",
      "Wanli Ouyang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/720_ECCV_2024_paper.php": {
    "title": "TimeCraft: Navigate Weakly-Supervised Temporal Grounded Video Question Answering via Bi-directional Reasoning",
    "volume": "main",
    "abstract": "Video reasoning typically operates within the Video Question-Answering (VQA) paradigm, which demands that the models understand and reason about video content from temporal and causal perspectives. Traditional supervised VQA methods gain this capability through meticulously annotated QA datasets, while advanced visual-language models exhibit remarkable performance due to large-scale visual-text pretraining data. Nevertheless, due to potential language bias and spurious visual-text correlations in cross-modal learning, concerns about the reliability of their answers persist in real-world applications. In this paper, we focus on the grounded VQA task, which necessitates models to provide answers along with explicit visual evidence, i.e., certain video segments. As temporal annotation is not available during training, we propose a novel bi-directional reasoning framework to perform grounded VQA in a weakly-supervised setting. Specifically, our framework consists of two parallel but dual reasoning paths. They conduct temporal grounding and answering based on the video content, approaching it from two dual directions that are symmetrical in terms of temporal order or causal relationships. By constructing a cycle-consistency relationship between these two branches, the model is prompted to provide self-guidance supervision for both temporal grounding and answering. Experiments conducted on the Next-GQA and Env-QA datasets demonstrate that our framework achieves superior performance in grounded VQA and can provide reasonable temporal locations that validate the answers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huabin Liu",
      "Xiao Ma",
      "Cheng Zhong",
      "Yang Zhang",
      "Weiyao Lin*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/725_ECCV_2024_paper.php": {
    "title": "Spectral Subsurface Scattering for Material Classification",
    "volume": "main",
    "abstract": "This study advances material classification using Spectral Sub-Surface Scattering (4) measurements. While spectrum and subsurface scattering measurements have individually been used in material classification, we argue that the strong spectral dependence of subsurface scattering lends itself to highly discriminative features. However, obtaining 4 measurements requires a time-consuming hyperspectral scan. We avoid this by showing that a carefully chosen 2D projection of the 4 point spread function is sufficient for material estimation. We also design and implement a novel imaging setup, consisting of a point illumination and a spectrally-dispersing camera, to make the desired 2D projections. Finally, through comprehensive experiments, we demonstrate the superiority of 4 imaging over spectral and sub-surface scattering measurements for the task of material classification",
    "checked": true,
    "id": "a35db7a027be591fa77f8848d6275b06f78d0106",
    "semantic_title": "spectral subsurface scattering for material classification",
    "citation_count": 0,
    "authors": [
      "Haejoon Lee*",
      "Aswin Sankaranarayanan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/730_ECCV_2024_paper.php": {
    "title": "nuCraft: Crafting High Resolution 3D Semantic Occupancy for Unified 3D Scene Understanding",
    "volume": "main",
    "abstract": "Existing benchmarks for 3D semantic occupancy prediction in autonomous driving are limited by low resolution (up to [512×512×40] with 0.2m voxel size) and inaccurate annotations, hindering the unification of 3D scene understanding through the occupancy representation. Moreover, previous methods can only generate occupancy predictions at 0.4m resolution or lower, requiring post-upsampling to reach their full resolution (0.2m). The root of these limitations lies in the sparsity, noise, and even errors present in the raw data. In this paper, we overcome these challenges by introducing nuCraft, a high-resolution and accurate semantic occupancy dataset derived from nuScenes. nuCraft offers an 8× increase in resolution ([1024 × 1024 × 80] with voxel size of 0.1m) and more precise semantic annotations compared to previous benchmarks. To address the high memory cost of high-resolution occupancy prediction, we propose VQ-Occ, a novel method that encodes occupancy data into a compact latent feature space using a VQ-VAE. This approach simplifies semantic occupancy prediction into feature simulation in the VQ latent space, making it easier and more memory-efficient. Our method enables direct generation of semantic occupancy fields at high resolution without post-upsampling, facilitating a more unified approach to 3D scene understanding. We validate the superior quality of nuCraft and the effectiveness of VQ-Occ through extensive experiments, demonstrating significant advancements over existing benchmarks and methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjin Zhu*",
      "zhe wang",
      "Hongsheng Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/742_ECCV_2024_paper.php": {
    "title": "Dynamic Neural Radiance Field From Defocused Monocular Video",
    "volume": "main",
    "abstract": "Dynamic Neural Radiance Field (NeRF) from monocular videos has recently been explored for space-time novel view synthesis and achieved excellent results. However, defocus blur caused by depth variation often occurs in video capture, compromising the quality of dynamic reconstruction because the lack of sharp details interferes with modeling temporal consistency between input views. To tackle this issue, we propose , the first dynamic NeRF method designed to restore sharp novel views from defocused monocular videos. We introduce layered Depth-of-Field (DoF) volume rendering to model the defocus blur and reconstruct a sharp NeRF supervised by defocused views. The blur model is inspired by the connection between DoF rendering and volume rendering. The opacity in volume rendering aligns with the layer visibility in DoF rendering. To execute the blurring, we modify the layered blur kernel to the ray-based kernel and employ an optimized sparse kernel to gather the input rays efficiently and render the optimized rays with our layered DoF volume rendering. We synthesize a dataset with defocused dynamic scenes for our task, and extensive experiments on our dataset show that our method outperforms existing approaches in synthesizing all-in-focus novel views from defocus blur while maintaining spatial-temporal consistency in the scene",
    "checked": true,
    "id": "0bc467097e4c45dd0302bea75b9d45ce9bc1dbbb",
    "semantic_title": "dynamic neural radiance field from defocused monocular video",
    "citation_count": 0,
    "authors": [
      "Xianrui Luo",
      "Huiqiang Sun",
      "Juewen Peng",
      "Zhiguo Cao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/744_ECCV_2024_paper.php": {
    "title": "PiTe: Pixel-Temporal Alignment for Large Video-Language Model",
    "volume": "main",
    "abstract": "Fueled by the Large Language Models (LLMs) wave, Large Visual-Language Models (LVLMs) have emerged as a pivotal advancement, bridging the gap between image and text. However, video making it challenging for LVLMs to perform adequately due to the complexity of the relationship between language and spatial-temporal data structure. Recent Large Video-Language Models (LVidLMs) align feature of static visual data like image into latent space of language feature, by general multi-modal tasks to leverage abilities of LLMs sufficiently. In this paper, we explore fine-grained alignment approach via object trajectory for different modalities across both spatial and temporal dimensions simultaneously. Thus, we propose a novel LVidLM by trajectory-guided Pixel-Temporal Alignment, dubbed PiTe, that exhibits promising applicable model property. To achieve fine-grained video-language alignment, we curate a multi-modal pre-training dataset PiTe-143k, the dataset provision of moving trajectories in pixel level for all individual objects, that appear and mention in the video and caption both, by our automatic annotation pipeline. Meanwhile, PiTe demonstrates astounding capabilities on myriad video-related multi-modal tasks through beat the state-of-the-art methods by a large margin",
    "checked": true,
    "id": "13533fd66425486da20df73851e190c6ceddfc0c",
    "semantic_title": "pite: pixel-temporal alignment for large video-language model",
    "citation_count": 0,
    "authors": [
      "Yang Liu*",
      "Pengxiang Ding",
      "Siteng Huang",
      "Min Zhang",
      "Han Zhao",
      "Donglin Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/753_ECCV_2024_paper.php": {
    "title": "CarFormer: Self-Driving with Learned Object-Centric Representations",
    "volume": "main",
    "abstract": "The choice of representation plays a key role in self-driving. Bird's eye view (BEV) representations have shown remarkable performance in recent years. In this paper, we propose to learn object-centric representations in BEV to distill a complex scene into more actionable information for self-driving. We first learn to place objects into slots with a slot attention model on BEV sequences. Based on these object-centric representations, we then train a transformer to learn to drive as well as reason about the future of other vehicles. We found that object-centric slot representations outperform both scene-level and object-level approaches that use the exact attributes of objects. Slot representations naturally incorporate information about objects from their spatial and temporal context such as position, heading, and speed without explicitly providing it. Our model with slots achieves an increased completion rate of the provided routes and, consequently, a higher driving score, with a lower variance across multiple runs, affirming slots as a reliable alternative in object-centric approaches. Additionally, we validate our model's performance as a world model through forecasting experiments, demonstrating its capability to predict future slot representations accurately. The code and the pre-trained models can be found at https://kuis-ai.github.io/CarFormer/",
    "checked": true,
    "id": "bdee94697937b6fea59e64f262888e9c80ed519a",
    "semantic_title": "carformer: self-driving with learned object-centric representations",
    "citation_count": 1,
    "authors": [
      "Shadi Hamdan*",
      "Fatma Guney"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/759_ECCV_2024_paper.php": {
    "title": "FreeDiff: Progressive Frequency Truncation for Image Editing with Diffusion Models",
    "volume": "main",
    "abstract": "Precise image editing with text-to-image models has attracted increasing interest due to their remarkable generative capabilities and user-friendly nature. However, such attempts face the pivotal challenge of misalignment between the intended precise editing target regions and the broader area impacted by the guidance in practice. Despite excellent methods leveraging attention mechanisms that have been developed to refine the editing guidance, these approaches necessitate modifications through complex network architecture and are limited to specific editing tasks. In this work, we re-examine the diffusion process and misalignment problem from a frequency perspective, revealing that, due to the power law of natural images and the decaying noise schedule, the denoising network primarily recovers low-frequency image components during the earlier timesteps and thus brings excessive low-frequency signals for editing. Leveraging this insight, we introduce a novel fine-tuning free approach that employs progressive Frequency truncation to refine the guidance of Diff usion models for universal editing tasks (FreeDiff ). Our method achieves comparable results with state-of-the-art methods across a variety of editing tasks and on a diverse set of images, highlighting its potential as a versatile tool in image editing applications",
    "checked": true,
    "id": "c1ec1123e69e14db895cdf426eedd22942b2d231",
    "semantic_title": "freediff: progressive frequency truncation for image editing with diffusion models",
    "citation_count": 0,
    "authors": [
      "Wei WU*",
      "Qingnan Fan",
      "Shuai Qin",
      "Hong Gu",
      "Ruoyu Zhao",
      "Antoni Chan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/763_ECCV_2024_paper.php": {
    "title": "Plain-Det: A Plain Multi-Dataset Object Detector",
    "volume": "main",
    "abstract": "Recent advancements in large-scale foundational models have sparked widespread interest in training highly proficient large vision models. A common consensus revolves around the necessity of aggregating extensive, high-quality annotated data. However, given the inherent challenges in annotating dense tasks in computer vision, such as object detection and segmentation, a practical strategy is to combine and leverage all available data for training purposes. In this work, we propose Plain-Det, which offers flexibility to accommodate new datasets, robustness in performance across diverse datasets, training efficiency, and compatibility with various detection architectures. We utilize Def-DETR, with the assistance of Plain-Det, to achieve a mAP of 51.9 on COCO, matching the current state-of-the-art detectors. We conduct extensive experiments on 13 downstream datasets and Plain-Det demonstrates strong generalization capability. Code is release at https://github.com/ChengShiest/Plain-Det",
    "checked": true,
    "id": "b5ccbfa9380682f1078a185630cfd4dd1a593167",
    "semantic_title": "plain-det: a plain multi-dataset object detector",
    "citation_count": 0,
    "authors": [
      "Cheng Shi",
      "Yuchen Zhu",
      "Sibei Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/764_ECCV_2024_paper.php": {
    "title": "Alternate Diverse Teaching for Semi-supervised Medical Image Segmentation",
    "volume": "main",
    "abstract": "Semi-supervised medical image segmentation has shown promise in training models with limited labeled data. However, current dominant teacher-student based approaches can suffer from the confirmation bias. To address this challenge, we propose AD-MT, an alternate diverse teaching approach in a teacher-student framework. It involves a single student model and two non-trainable teacher models that are momentum-updated periodically and randomly in an alternate fashion. To mitigate the confirmation bias via the diverse supervision, the core of AD-MT lies in two proposed modules: the Random Periodic Alternate (RPA) Updating Module and the Conflict-Combating Module (CCM). The RPA schedules an alternating diverse updating process with complementary unlabeled data batches, distinct data augmentation, and random switching periods to encourage diverse reasoning from different teaching perspectives. The CCM employs an entropy-based ensembling strategy to encourage the model to learn from both the consistent and conflicting predictions between the teachers. Experimental results demonstrate the effectiveness and superiority of AD-MT on the 2D and 3D medical segmentation benchmarks across various semi-supervised settings",
    "checked": true,
    "id": "a03020c47a5b3daf35781d4e5b5d1ac46116f584",
    "semantic_title": "alternate diverse teaching for semi-supervised medical image segmentation",
    "citation_count": 3,
    "authors": [
      "Zhen Zhao*",
      "Zicheng Wang",
      "Dian Yu",
      "Longyue Wang*",
      "Yixuan Yuan",
      "Luping Zhou"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/780_ECCV_2024_paper.php": {
    "title": "Cs2K: Class-specific and Class-shared Knowledge Guidance for Incremental Semantic Segmentation",
    "volume": "main",
    "abstract": "Incremental semantic segmentation endeavors to segment newly encountered classes while maintaining knowledge of old classes. However, existing methods either 1) lack guidance from class-specific knowledge (i.e., old class prototypes), leading to a bias towards new classes, or 2) constrain class-shared knowledge (i.e., old model weights) excessively without discrimination, resulting in a preference for old classes. In this paper, to trade off model performance, we propose the Class-specific and Class-shared Knowledge (Cs2 K) guidance for incremental semantic segmentation. Specifically, from the class-specific knowledge aspect, we design a prototype-guided pseudo labeling that exploits feature proximity from prototypes to correct pseudo labels, thereby overcoming catastrophic forgetting. Meanwhile, we develop a prototype-guided class adaptation that aligns class distribution across datasets via learning old augmented prototypes. Moreover, from the class-shared knowledge aspect, we propose a weight-guided selective consolidation to strengthen old memory while maintaining new memory by integrating old and new model weights based on weight importance relative to old classes. Experiments on public datasets demonstrate that our proposed Cs2 K significantly improves segmentation performance and is plug-and-play",
    "checked": true,
    "id": "017c7d81093ae136c340e36115578add194c6678",
    "semantic_title": "cs2k: class-specific and class-shared knowledge guidance for incremental semantic segmentation",
    "citation_count": 0,
    "authors": [
      "Wei Cong*",
      "Yang Cong",
      "Yuyang Liu",
      "Gan Sun"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/786_ECCV_2024_paper.php": {
    "title": "Synchronous Diffusion for Unsupervised Smooth Non-Rigid 3D Shape Matching",
    "volume": "main",
    "abstract": "Most recent unsupervised non-rigid 3D shape matching methods are based on the functional map framework due to its efficiency and superior performance. Nevertheless, respective methods struggle to obtain spatially smooth pointwise correspondences due to the lack of proper regularisation. In this work, inspired by the success of message passing on graphs, we propose a synchronous diffusion process which we use as regularisation to achieve smoothness in non-rigid 3D shape matching problems. The intuition of synchronous diffusion is that diffusing the same input function on two different shapes results in consistent outputs. Using different challenging datasets, we demonstrate that our novel regularisation can substantially improve the state-of-the-art in shape matching, especially in the presence of topological noise",
    "checked": true,
    "id": "24e8741b8ec40aff686daf9efea18b817869f77c",
    "semantic_title": "synchronous diffusion for unsupervised smooth non-rigid 3d shape matching",
    "citation_count": 0,
    "authors": [
      "Dongliang Cao*",
      "Zorah Laehner",
      "Florian Bernard"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/790_ECCV_2024_paper.php": {
    "title": "Text-Guided Video Masked Autoencoder",
    "volume": "main",
    "abstract": "Recent video masked autoencoder (MAE) works have designed improved masking algorithms focused on saliency. These works leverage visual cues such as motion to mask the most salient regions. However, the robustness of such visual cues depends on how often input videos match underlying assumptions. On the other hand, natural language description is an information dense representation of video that implicitly captures saliency without requiring modality-specific assumptions, and has not been explored yet for video MAE. To this end, we introduce a novel text-guided masking algorithm (TGM) that masks the video regions with highest correspondence to paired captions. Without leveraging any explicit visual cues for saliency, our TGM is competitive with state-of-the-art masking algorithms such as motion-guided masking. To further benefit from the semantics of natural language for masked reconstruction, we next introduce a unified framework for joint MAE and masked video-text contrastive learning. We show that across existing masking algorithms, unifying MAE and masked video-text contrastive learning improves downstream performance compared to pure MAE on a variety of video recognition tasks, especially for linear probe. Within this unified framework, our TGM achieves the best relative performance on five action recognition and one egocentric datasets, highlighting the complementary nature of natural language for masked video modeling",
    "checked": true,
    "id": "f631ff4f13b6b85f67d06f5c050c5ef270f89267",
    "semantic_title": "text-guided video masked autoencoder",
    "citation_count": 0,
    "authors": [
      "David Fan*",
      "Jue Wang",
      "Shuai Liao",
      "Zhikang Zhang",
      "Vimal Bhat",
      "Xinyu Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/794_ECCV_2024_paper.php": {
    "title": "Diffusion Models for Open-Vocabulary Segmentation",
    "volume": "main",
    "abstract": "Open-vocabulary segmentation is the task of segmenting anything that can be named in an image. Recently, large-scale vision-language modelling has led to significant advances in open-vocabulary segmentation, but at the cost of gargantuan and increasing training and annotation efforts. Hence, we ask if it is possible to use existing foundation models to synthesise on-demand efficient segmentation algorithms for specific class sets, making them applicable in an open-vocabulary setting without the need to collect further data, annotations or perform training. To that end, we present , a novel method that leverages generative text-to-image diffusion models for unsupervised open-vocabulary segmentation. synthesises support image sets for arbitrary textual categories, creating for each a set of prototypes representative of both the category and its surrounding context (background). It relies solely on pre-trained components and outputs the synthesised segmenter directly, without training. Our approach shows strong performance on a range of benchmarks, obtaining a lead of more than 5% over prior work on PASCAL VOC",
    "checked": true,
    "id": "fd6d5e39d4e6641f3a1b7bdebd9f649c2c3705a8",
    "semantic_title": "diffusion models for open-vocabulary segmentation",
    "citation_count": 52,
    "authors": [
      "Laurynas Karazija*",
      "Iro Laina",
      "Andrea Vedaldi",
      "Christian Rupprecht"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/796_ECCV_2024_paper.php": {
    "title": "Textual-Visual Logic Challenge: Understanding and Reasoning in Text-to-Image Generation",
    "volume": "main",
    "abstract": "Text-to-image generation plays a pivotal role in computer vision and natural language processing by translating textual descriptions into visual representations. However, understanding complex relations in detailed text prompts filled with rich relational content remains a significant challenge. To address this, we introduce a novel task: Logic-Rich Text-to-Image generation. Unlike conventional image generation tasks that rely on short and structurally simple natural language inputs, our task focuses on intricate text inputs abundant in relational information. To tackle these complexities, we collect the Textual-Visual Logic dataset, designed to evaluate the performance of text-to-image generation models across diverse and complex scenarios. Furthermore, we propose a baseline model as a benchmark for this task. Our model comprises three key components: a relation understanding module, a multimodality fusion module, and a negative pair discriminator. These components enhance the model's ability to handle disturbances in informative tokens and prioritize relational elements during image generation. https:// github.com/IntelLabs/Textual-Visual-Logic-Challenge",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peixi Xiong*",
      "Michael A Kozuch",
      "Nilesh Jain"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/799_ECCV_2024_paper.php": {
    "title": "EvSign: Sign Language Recognition and Translation with Streaming Events",
    "volume": "main",
    "abstract": "Sign language is one of the most effective communication tools for people with hearing difficulties. Most existing works focus on improving the performance of sign language tasks on RGB videos, which may suffer from degraded recording conditions, such as fast movement of hands with motion blur and textured signer's appearance. The bio-inspired event camera, which asynchronously captures brightness change with high speed, could naturally perceive dynamic hand movements, providing rich manual clues for sign language tasks. In this work, we aim at exploring the potential of event camera in continuous sign language recognition (CSLR) and sign language translation (SLT). To promote the research, we first collect an event-based benchmark EvSign for those tasks with both gloss and spoken language annotations. EvSign dataset offers a substantial amount of high-quality event streams and an extensive vocabulary of glosses and words, thereby facilitating the development of sign language tasks. In addition, we propose an efficient transformer-based framework for event-based SLR and SLT tasks, which fully leverages the advantages of streaming events. The sparse backbone is employed to extract visual features from sparse events. Then, the temporal coherence is effectively utilized through the proposed local token fusion and gloss-aware temporal aggregation modules. Extensive experimental results are reported on both simulated (PHOENIX14T) and EvSign datasets. Our method performs favorably against existing state-of-the-art approaches with only 0.34% computational cost (0.84G FLOPS per video) and 44.2% network parameters. The project is available at https://zhang-pengyu.github.io/EVSign",
    "checked": true,
    "id": "5fadb52eeebcf45283a87972ffb28b56b22a8cb6",
    "semantic_title": "evsign: sign language recognition and translation with streaming events",
    "citation_count": 1,
    "authors": [
      "Pengyu Zhang*",
      "Hao Yin",
      "Zeren Wang",
      "Wenyue Chen",
      "Sheng Ming Li",
      "Dong Wang",
      "Huchuan Lu",
      "Xu Jia"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/808_ECCV_2024_paper.php": {
    "title": "QUAR-VLA: Vision-Language-Action Model for Quadruped Robots",
    "volume": "main",
    "abstract": "The important manifestation of robot intelligence is the ability to naturally interact and autonomously make decisions. Traditional quadruped robot learning typically handles language interaction and visual autonomous perception separately, which, while simplifying system design, also limits the synergy between different information streams. This separation poses challenges in achieving seamless autonomous reasoning, decision-making, and action execution. To address these limitations, a novel paradigm, named Vision-Language-Action tasks for QUAdruped Robots (QUAR-VLA), has been introduced in this paper. This approach tightly integrates visual information and instructions to generate executable actions, effectively merging perception, planning, and decision-making. The central idea is to elevate the overall intelligence of the robot. Within this framework, a notable challenge lies in aligning fine-grained instructions with visual perception information. This emphasizes the complexity involved in ensuring that the robot accurately interprets and acts upon detailed instructions in harmony with its visual observations. Consequently, we propose QUAdruped Robotic Transformer (QUART), a VLA model to integrate visual information and instructions from diverse modalities as input and generates executable actions for real-world robots and present QUAdruped Robot Dataset (QUARD), a large-scale multi-task dataset including perception, navigation and advanced capability like whole-body manipulation tasks for training QUART model. Our extensive evaluation shows that our approach leads to performant robotic policies and enables QUART to obtain a range of generalization capabilities",
    "checked": true,
    "id": "c0b3a44ff6b482a073138eb243da9b762eb1cd1f",
    "semantic_title": "quar-vla: vision-language-action model for quadruped robots",
    "citation_count": 4,
    "authors": [
      "Pengxiang Ding",
      "Han Zhao",
      "Wenjie Zhang",
      "Wenxuan Song",
      "Min Zhang",
      "Siteng Huang",
      "Ningxi Yang",
      "Donglin Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/812_ECCV_2024_paper.php": {
    "title": "Zero-shot Object Counting with Good Exemplars",
    "volume": "main",
    "abstract": "Zero-shot object counting (ZOC) aims to enumerate objects in images using only the names of object classes during testing, without the need for manual annotations. However, a critical challenge in current ZOC methods lies in their inability to identify high-quality exemplars effectively. This deficiency hampers scalability across diverse classes and undermines the development of strong visual associations between the identified classes and image content. To this end, we propose the Visual Association-based Zero-shot Object Counting (VA-Count) framework. VA-Count consists of an Exemplar Enhancement Module (EEM) and a Noise Suppression Module (NSM) that synergistically refine the process of class exemplar identification while minimizing the consequences of incorrect object identification. The EEM utilizes advanced vision-language pretaining models to discover potential exemplars, ensuring the framework's adaptability to various classes. Meanwhile, the NSM employs contrastive learning to differentiate between optimal and suboptimal exemplar pairs, reducing the negative effects of erroneous exemplars. VA-Count demonstrates its effectiveness and scalability in zero-shot contexts with superior performance on two object counting datasets",
    "checked": true,
    "id": "8f769e9a659e0f5ffa6ca5d0f465827bf6d8905f",
    "semantic_title": "zero-shot object counting with good exemplars",
    "citation_count": 0,
    "authors": [
      "Huilin Zhu",
      "Jingling Yuan",
      "Zhengwei Yang",
      "Yu Guo",
      "Xian Zhong*",
      "Zheng Wang",
      "Shengfeng He*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/813_ECCV_2024_paper.php": {
    "title": "TextDiffuser-2: Unleashing the Power of Language Models for Text Rendering",
    "volume": "main",
    "abstract": "The diffusion model has been proven a powerful generative model in recent years, yet it remains a challenge in generating visual text. Although existing work has endeavored to enhance the accuracy of text rendering, these methods still suffer from several drawbacks, such as (1) limited flexibility and automation, (2) constrained capability of layout prediction, and (3) restricted diversity. In this paper, we present TextDiffuser-2, aiming to unleash the power of language models for text rendering while taking these three aspects into account. Firstly, we fine-tune a large language model for layout planning. The large language model is capable of automatically generating keywords and placing the text in optimal positions for text rendering. Secondly, we utilize the language model within the diffusion model to encode the position and content of keywords at the line level. Unlike previous methods that employed tight character-level guidance, our approach generates more diverse text images. We conduct extensive experiments and incorporate user studies involving human participants and GPT-4V, validating TextDiffuser-2's capacity to achieve a more rational text layout and generation with enhanced diversity. Furthermore, the proposed methods are compatible with existing text rendering techniques, such as TextDiffuser and GlyphControl, serving to enhance automation and diversity, as well as augment the rendering accuracy. For instance, by using the proposed layout planner, TextDiffuser is capable of rendering text with more aesthetically pleasing line breaks and alignment, meanwhile obviating the need for explicit keyword specification. Furthermore, GlyphControl can leverage the layout planner to achieve diverse layouts without the necessity for user-specified glyph images, and the rendering F-measure can be boosted by 6.51% when using the proposed layout encoding training technique. The code and model are available at https: //aka.ms/textdiffuser-2",
    "checked": true,
    "id": "1c6e2a4da1ead685a95c079751bf4d7a727d8180",
    "semantic_title": "textdiffuser-2: unleashing the power of language models for text rendering",
    "citation_count": 31,
    "authors": [
      "Jingye Chen*",
      "Yupan Huang",
      "Tengchao Lv",
      "Lei Cui",
      "Qifeng Chen",
      "Furu Wei"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/815_ECCV_2024_paper.php": {
    "title": "SFPNet: Sparse Focal Point Network for Semantic Segmentation on General LiDAR Point Clouds",
    "volume": "main",
    "abstract": "Although LiDAR semantic segmentation advances rapidly, state-of-the-art methods often incorporate specifically designed inductive bias derived from benchmarks originating from mechanical spinning LiDAR. This can limit model generalizability to other kinds of LiDAR technologies and make hyperparameter tuning more complex. To tackle these issues, we propose a generalized framework to accommodate various types of LiDAR prevalent in the market by replacing window-attention with our sparse focal point modulation. Our SFPNet is capable of extracting multi-level contexts and dynamically aggregating them using a gate mechanism. By implementing a channel-wise information query, features that incorporate both local and global contexts are encoded. We also introduce a novel large-scale hybrid-solid LiDAR semantic segmentation dataset for robotic applications. SFPNet demonstrates competitive performance on conventional benchmarks derived from mechanical spinning LiDAR, while achieving state-of-the-art results on benchmark derived from solid-state LiDAR. Additionally, it outperforms existing methods on our novel dataset sourced from hybrid-solid LiDAR. Code and dataset are available at https://github.com/Cavendish518/SFPNet and https://www.semanticindustry.top",
    "checked": true,
    "id": "3fff2afffcf18086cfdf8f43c65fe899821ffca7",
    "semantic_title": "sfpnet: sparse focal point network for semantic segmentation on general lidar point clouds",
    "citation_count": 2,
    "authors": [
      "Yanbo Wang*",
      "Wentao Zhao",
      "Cao Chuan",
      "Tianchen Deng",
      "Jingchuan Wang",
      "Weidong Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/817_ECCV_2024_paper.php": {
    "title": "PartSTAD: 2D-to-3D Part Segmentation Task Adaptation",
    "volume": "main",
    "abstract": "We introduce , a method designed for the task adaptation of 2D-to-3D segmentation lifting. Recent studies have highlighted the advantages of utilizing 2D segmentation models to achieve high-quality 3D segmentation through few-shot adaptation. However, previous approaches have focused on adapting 2D segmentation models for domain shift to rendered images and synthetic text descriptions, rather than optimizing the model specifically for 3D segmentation. Our proposed task adaptation method finetunes a 2D bounding box prediction model with an objective function for 3D segmentation. We introduce weights for 2D bounding boxes for adaptive merging and learn the weights using a small additional neural network. Additionally, we incorporate SAM, a foreground segmentation model on a bounding box, to improve the boundaries of 2D segments and consequently those of 3D segmentation. Our experiments on the PartNet-Mobility dataset show significant improvements with our task adaptation approach, achieving a 7.0%p increase in mIoU and a 5.2%p improvement in mAP50 for semantic and instance segmentation compared to the SotA few-shot 3D segmentation model. The code is available at https://github.com/KAIST-Visual-AI-Group/PartSTAD",
    "checked": true,
    "id": "be3992ba496b493295699d647553fecf6482bfd8",
    "semantic_title": "partstad: 2d-to-3d part segmentation task adaptation",
    "citation_count": 2,
    "authors": [
      "Hyunjin Kim",
      "Minhyuk Sung*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/828_ECCV_2024_paper.php": {
    "title": "FutureDepth: Learning to Predict the Future Improves Video Depth Estimation",
    "volume": "main",
    "abstract": "In this paper, we propose a novel video depth estimation approach, , which enables the model to implicitly leverage multi-frame and motion cues to improve depth estimation by making it learn to predict the future at training. More specifically, we propose a future prediction network, F-Net, which takes the features of multiple consecutive frames and is trained to predict multi-frame features one time step ahead iteratively. In this way, F-Net learns the underlying motion and correspondence information, and we incorporate its features into the depth decoding process. Additionally, to enrich the learning of multi-frame correspondence cues, we further leverage a reconstruction network, R-Net, which is trained via adaptively masked auto-encoding of multi-frame feature volumes. At inference time, both F-Net and R-Net are used to produce queries to work with the depth decoder, as well as a final refinement network. Through extensive experiments on several benchmarks, i.e., NYUDv2, KITTI, DDAD, and Sintel, which cover indoor, driving, and open-domain scenarios, we show that significantly improves upon baseline models, outperforms existing video depth estimation methods, and sets new state-of-the-art (SOTA) accuracy. Furthermore, is more efficient than existing SOTA video depth estimation models and has similar latencies when comparing to monocular models",
    "checked": true,
    "id": "a09e2608b6036d8b7f5de575e0aeb30b4c81a002",
    "semantic_title": "futuredepth: learning to predict the future improves video depth estimation",
    "citation_count": 3,
    "authors": [
      "Rajeev Yasarla*",
      "Manish Kumar Singh",
      "Hong Cai",
      "Yunxiao Shi",
      "Jisoo Jeong",
      "Yinhao Zhu",
      "Shizhong Han",
      "Risheek Garrepalli",
      "Fatih Porikli"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/833_ECCV_2024_paper.php": {
    "title": "LLM as Copilot for Coarse-grained Vision-and-Language Navigation",
    "volume": "main",
    "abstract": "Vision-and-Language Navigation (VLN) involves guiding an agent through indoor environments using human-provided textual instructions. Coarse-grained VLN, with short and high-level instructions, has gained popularity as it closely mirrors real-world scenarios. However, a significant challenge is these instructions are often too concise for agents to comprehend and act upon. Previous studies have explored allowing agents to seek assistance during navigation, but typically offer rigid support from pre-existing datasets or simulators. The advent of Large Language Models (LLMs) presents a novel avenue for aiding VLN agents. This paper introduces VLN-Copilot, a framework enabling agents to actively seek assistance when encountering confusion, with the LLM serving as a copilot to facilitate navigation. Our approach includes the introduction of a confusion score, quantifying the level of uncertainty in an agent's action decisions, while the LLM offers real-time detailed guidance for navigation. Experimental results on two coarse-grained VLN datasets show the efficacy of our method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanyuan Qiao*",
      "Qianyi Liu",
      "Jiajun Liu",
      "Jing Liu",
      "Qi Wu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/841_ECCV_2024_paper.php": {
    "title": "Raindrop Clarity: A Dual-Focused Dataset for Day and Night Raindrop Removal",
    "volume": "main",
    "abstract": "Existing raindrop removal datasets have two shortcomings. First, they consist of images captured by cameras with a focus on the background, leading to the presence of blurry raindrops. To our knowledge, none of these datasets include images where the focus is specifically on raindrops, which results in a blurry background. Second, these datasets predominantly consist of daytime images, thereby lacking nighttime raindrop scenarios. Consequently, algorithms trained on these datasets may struggle to perform effectively in raindrop-focused or nighttime scenarios. The absence of datasets specifically designed for raindrop-focused and nighttime raindrops constrains research in this area. In this paper, we introduce a large-scale, real-world raindrop removal dataset called Raindrop Clarity. Raindrop Clarity comprises 15,186 high-quality pairs/triplets (raindrops, blur, and background) of images with raindrops and the corresponding clear background images. There are 5,442 daytime raindrop images and 9,744 nighttime raindrop images. Specifically, the 5,442 daytime images include 3,606 raindropand 1,836 background-focused images. While the 9,744 nighttime images contain 4,834 raindrop- and 4,906 background-focused images. Our dataset will enable the community to explore background-focused and raindrop-focused images, including challenges unique to daytime and nighttime conditions. 1 1 Our data and code are available at: https://github.com/jinyeying/RaindropClarity",
    "checked": true,
    "id": "a977ffc91624b93634ff1f5c1a09bc5292d93d88",
    "semantic_title": "raindrop clarity: a dual-focused dataset for day and night raindrop removal",
    "citation_count": 0,
    "authors": [
      "Yeying Jin*",
      "Xin Li",
      "Jiadong Wang",
      "Yan Zhan",
      "Malu Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/851_ECCV_2024_paper.php": {
    "title": "Unsupervised Moving Object Segmentation with Atmospheric Turbulence",
    "volume": "main",
    "abstract": "Moving object segmentation in the presence of atmospheric turbulence is highly challenging due to turbulence-induced irregular and time-varying distortions. In this paper, we present an unsupervised approach for segmenting moving objects in videos downgraded by atmospheric turbulence. Our key approach is a detect-then-grow scheme: we first identify a small set of moving object pixels with high confidence, then gradually grow a foreground mask from those seeds to segment all moving objects. This method leverages rigid geometric consistency among video frames to disentangle different types of motions, and then uses the Sampson distance to initialize the seedling pixels. After growing per-frame foreground masks, we use spatial grouping loss and temporal consistency loss to further refine the masks in order to ensure their spatio-temporal consistency. Our method is unsupervised and does not require training on labeled data. For validation, we collect and release the first real-captured long-range turbulent video dataset with ground truth masks for moving objects. Results show that our method achieves good accuracy in segmenting moving objects and is robust for long-range videos with various turbulence strengths",
    "checked": true,
    "id": "94f454011ab17da4b07450cdd84f7004d5b8fbe2",
    "semantic_title": "unsupervised moving object segmentation with atmospheric turbulence",
    "citation_count": 0,
    "authors": [
      "Dehao Qin*",
      "Ripon k Saha",
      "Woojeh Chung",
      "Suren Jayasuriya",
      "Jinwei Ye",
      "Nianyi Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/857_ECCV_2024_paper.php": {
    "title": "AccDiffusion: An Accurate Method for Higher-Resolution Image Generation",
    "volume": "main",
    "abstract": "This paper attempts to address the object repetition issue in patch-wise higher-resolution image generation. We propose AccDiffusion, an accurate method for patch-wise higher-resolution image generation without training. An in-depth analysis in this paper reveals an identical text prompt for different patches causes repeated object generation, while no prompt compromises the image details. Therefore, our AccDiffusion, for the first time, proposes to decouple the vanilla image-content-aware prompt into a set of patch-content-aware prompts, each of which serves as a more precise description of an image patch. Besides, AccDiffusion also introduces dilated sampling with window interaction for better global consistency in higher-resolution image generation. Experimental comparison with existing methods demonstrates that our AccDiffusion effectively addresses the issue of repeated object generation and leads to better performance in higher-resolution image generation. Our code is released at https://github. com/lzhxmu/AccDiffusion",
    "checked": true,
    "id": "1006a18cce8fb5326b097a21b24b1e208060fe93",
    "semantic_title": "accdiffusion: an accurate method for higher-resolution image generation",
    "citation_count": 7,
    "authors": [
      "Zhihang Lin",
      "Mingbao Lin",
      "Meng Zhao",
      "Rongrong Ji*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/861_ECCV_2024_paper.php": {
    "title": "Uncertainty-Driven Spectral Compressive Imaging with Spatial-Frequency Transformer",
    "volume": "main",
    "abstract": "Recently, learning-based Hyperspectral image (HSI) reconstruction methods have demonstrated promising performance. However, existing learning-based methods still face two issues. 1) They rarely consider both the spatial sparsity and inter-spectral similarity priors of HSI. 2) They treat all image regions equally, ignoring that texture-rich and edge regions are more difficult to reconstruct than smooth regions. To address these issues, we propose an uncertainty-driven HSI reconstruction method termed Specformer. Specifically, we first introduce a frequency-wise self-attention (FWSA) module, and combine it with a spatial-wise local-window self-attention (LWSA) module in parallel to form a Spatial-Frequency (SF) block. LWSA can guide the network to focus on the regions with dense spectral information, and FWSA can capture the inter-spectral similarity. Parallel design helps the network to model cross-window connections, and expand its receptive fields while maintaining linear complexity. We use SF-block as the main building block in a multi-scale U-shape network to form our Specformer. In addition, we introduce an uncertainty-driven loss function, which can reinforce the network's attention to the challenging regions with rich textures and edges. Experiments on simulated and real HSI datasets show that our Specformer outperforms state-of-the-art methods with lower computational and memory costs. The code is available at https://github.com/bianlab/Specformer",
    "checked": true,
    "id": "3ba9f0ae519f98bbdb991c45f48df984bbd54fc9",
    "semantic_title": "uncertainty-driven spectral compressive imaging with spatial-frequency transformer",
    "citation_count": 0,
    "authors": [
      "Lintao Peng",
      "Siyu Xie",
      "Liheng Bian*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/869_ECCV_2024_paper.php": {
    "title": "CaesarNeRF: Calibrated Semantic Representation for Few-Shot Generalizable Neural Rendering",
    "volume": "main",
    "abstract": "Generalizability and few-shot learning are key challenges in Neural Radiance Fields (NeRF), often due to the lack of a holistic understanding in pixel-level rendering. We introduce CaesarNeRF, an end-to-end approach that leverages scene-level CAlibratEd SemAntic Representation along with pixel-level representations to advance few-shot, generalizable neural rendering, facilitating a holistic understanding without compromising high-quality details. CaesarNeRF explicitly models pose differences of reference views to combine scene-level semantic representations, providing a calibrated holistic understanding. This calibration process aligns various viewpoints with precise location and is further enhanced by sequential refinement to capture varying details. Extensive experiments on public datasets, including LLFF, Shiny, mip-NeRF 360, and MVImgNet, show that CaesarNeRF delivers state-of-the-art performance across varying numbers of reference views, † proving effective even with a single reference image. ∗ Equal contribution. Corresponding author. This work was done when Haidong Zhu was an intern at Microsoft",
    "checked": true,
    "id": "23c3144a1f1447adfdda92f8cda08a1b2e7b3d69",
    "semantic_title": "caesarnerf: calibrated semantic representation for few-shot generalizable neural rendering",
    "citation_count": 3,
    "authors": [
      "Haidong Zhu",
      "Tianyu Ding*",
      "Tianyi Chen",
      "Ilya Zharkov",
      "Ram Nevatia",
      "Luming Liang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/926_ECCV_2024_paper.php": {
    "title": "MapTracker: Tracking with Strided Memory Fusion for Consistent Vector HD Mapping",
    "volume": "main",
    "abstract": "This paper presents a vector HD-mapping algorithm that formulates the mapping as a tracking task and uses a history of memory latents to ensure consistent reconstructions over time. Our method, , accumulates a sensor stream into memory buffers of two latent representations: 1) Raster latents in the bird's-eye-view (BEV) space and 2) Vector latents over the road elements (i.e., pedestrian-crossings, lane-dividers, and road-boundaries). The approach borrows the query propagation paradigm from the tracking literature that explicitly associates tracked road elements from the previous frame to the current, while fusing a subset of memory latents selected with distance strides to further enhance temporal consistency. A vector latent is decoded to reconstruct the geometry of a road element. The paper further makes benchmark contributions by 1) Improving processing code for existing datasets to produce consistent ground truth with temporal alignments and 2) Augmenting existing mAP metrics with consistency checks. significantly outperforms existing methods on both nuScenes and Agroverse2 datasets by over 8% and 19% on the conventional and the new consistency-aware metrics, respectively. The code and models are available on our project page: https://map-tracker.github.io",
    "checked": true,
    "id": "68e0eb0041be94fd527c1e3907a352a49583fa64",
    "semantic_title": "maptracker: tracking with strided memory fusion for consistent vector hd mapping",
    "citation_count": 11,
    "authors": [
      "Jiacheng Chen*",
      "Yuefan Wu",
      "Jiaqi Tan",
      "Hang Ma",
      "Yasutaka Furukawa*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/929_ECCV_2024_paper.php": {
    "title": "Image Demoireing in RAW and sRGB Domains",
    "volume": "main",
    "abstract": "Moiré patterns frequently appear when capturing screens with smartphones or cameras, potentially compromising image quality. Previous studies suggest that moiré pattern elimination in the RAW domain offers greater effectiveness compared to demoiréing in the sRGB domain. Nevertheless, relying solely on RAW data for image demoiréing is insufficient in mitigating the color cast due to the absence of essential information required for the color correction by the image signal processor (ISP). In this paper, we propose to jointly utilize both RAW and sRGB data for image demoiréing (RRID), which are readily accessible in modern smartphones and DSLR cameras. We develop Skip-Connection-based Demoiréing Module (SCDM) with Gated Feedback Module (GFM) and Frequency Selection Module (FSM) embedded in skip-connections for the efficient and effective demoiréing of RAW and sRGB features, respectively. Subsequently, we design a RGB Guided ISP (RGISP) to learn a device-dependent ISP, assisting the process of color recovery. Extensive experiments demonstrate that our RRID outperforms state-of-the-art approaches, in terms of the performance in moiré pattern removal and color cast correction by 0.62dB in PSNR and 0.003 in SSIM. Code is available at https://github.com/rebeccaeexu/RRID",
    "checked": true,
    "id": "6c4c8868c08e0c17b57d5d727d8fe8a02e098c74",
    "semantic_title": "image demoireing in raw and srgb domains",
    "citation_count": 1,
    "authors": [
      "Shuning Xu",
      "Binbin Song",
      "Xiangyu Chen",
      "Xina Liu",
      "Jiantao Zhou*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/932_ECCV_2024_paper.php": {
    "title": "LiDAR-Event Stereo Fusion with Hallucinations",
    "volume": "main",
    "abstract": "Event stereo matching is an emerging technique to estimate depth from neuromorphic cameras; however, events are unlikely to trigger in the absence of motion or the presence of large, untextured regions, making the correspondence problem extremely challenging. Purposely, we propose integrating a stereo event camera with a fixed-frequency active sensor – e.g., a LiDAR – collecting sparse depth measurements, overcoming the aforementioned limitations. Such depth hints are used by hallucinating – i.e., inserting fictitious events – the stacks or raw input streams, compensating for the lack of information in the absence of brightness changes. Our techniques are general, can be adapted to any structured representation to stack events and outperform state-of-the-art fusion methods applied to event-based stereo",
    "checked": true,
    "id": "7736b2cc87b8ef769e7cbefac0865ad2e04acb0a",
    "semantic_title": "lidar-event stereo fusion with hallucinations",
    "citation_count": 1,
    "authors": [
      "Luca Bartolomei*",
      "Matteo Poggi",
      "Andrea Conti",
      "Stefano Mattoccia*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/935_ECCV_2024_paper.php": {
    "title": "X-Former: Unifying Contrastive and Reconstruction Learning for MLLMs",
    "volume": "main",
    "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have revolutionized the field of vision-language understanding by integrating visual perception capabilities into Large Language Models (LLMs). The prevailing trend in this field involves the utilization of a vision encoder derived from vision-language contrastive learning (CL), showing expertise in capturing overall representations while facing difficulties in capturing detailed local patterns. In this work, we focus on enhancing the visual representations for MLLMs by combining high-frequency and detailed visual representations, obtained through masked image modeling (MIM), with semantically-enriched low-frequency representations captured by CL. To achieve this goal, we introduce X-Former which is a lightweight transformer module designed to exploit the complementary strengths of CL and MIM through an innovative interaction mechanism. Specifically, X-Former first bootstraps vision-language representation learning and multimodal-to-multimodal generative learning from two frozen vision encoders, i.e., CLIP-ViT (CL-based) and MAE-ViT (MIM-based). It further bootstraps vision-to-language generative learning from a frozen LLM to ensure visual features from X-Former can be interpreted by the LLM. To demonstrate the effectiveness of our approach, we assess its performance on tasks demanding detailed visual understanding. Extensive evaluations indicate that X-Former excels in visual reasoning tasks involving both structural and semantic categories in the GQA dataset. Assessment on fine-grained visual perception benchmark further confirms its superior capabilities in visual understanding",
    "checked": true,
    "id": "bd6c3f2fe1d0ae1bf9a842e923a052cddef586f9",
    "semantic_title": "x-former: unifying contrastive and reconstruction learning for mllms",
    "citation_count": 0,
    "authors": [
      "Sirnam Swetha*",
      "Jinyu Yang",
      "Tal Neiman",
      "Mamshad Nayeem Rizve",
      "Son Tran",
      "Benjamin Yao",
      "Trishul A Chilimbi",
      "Mubarak Shah"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/941_ECCV_2024_paper.php": {
    "title": "Learning Anomalies with Normality Prior for Unsupervised Video Anomaly Detection",
    "volume": "main",
    "abstract": "Unsupervised video anomaly detection (UVAD) aims to detect abnormal events in videos without any annotations. It remains challenging because anomalies are rare, diverse, and usually not well-defined. Existing UVAD methods are purely data-driven and perform unsupervised learning by identifying various abnormal patterns in videos. Since these methods largely rely on the feature representation and data distribution, they can only learn salient anomalies that are substantially different from normal events but ignore the less distinct ones. To address this challenge, this paper pursues a different approach that leverages data-irrelevant prior knowledge about normal and abnormal events for UVAD. We first propose a new normality prior for UVAD, suggesting that the start and end of a video are predominantly normal. We then propose normality propagation, which propagates normal knowledge based on relationships between video snippets to estimate the normal magnitudes of unlabeled snippets. Finally, unsupervised learning of abnormal detection is performed based on the propagated labels and a new loss re-weighting method. These components are complementary to normality propagation and mitigate the negative impact of incorrectly propagated labels. Extensive experiments on the ShanghaiTech and UCF-Crime benchmarks demonstrate the superior performance of our method. The code is available at https://github.com/shyern/LANP-UVAD.git",
    "checked": true,
    "id": "4ff90ddf809566ab99b6737d6eac05228efb5809",
    "semantic_title": "learning anomalies with normality prior for unsupervised video anomaly detection",
    "citation_count": 0,
    "authors": [
      "Haoyue Shi",
      "Le Wang*",
      "Sanping Zhou",
      "Gang Hua",
      "Wei Tang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/944_ECCV_2024_paper.php": {
    "title": "Revisiting Supervision for Continual Representation Learning",
    "volume": "main",
    "abstract": "In the field of continual learning, models are designed to learn tasks one after the other. While most research has centered on supervised continual learning, there is a growing interest in unsupervised continual learning, which makes use of the vast amounts of unlabeled data. Recent studies have highlighted the strengths of unsupervised methods, particularly self-supervised learning, in providing robust representations. The improved transferability of those representations built with self-supervised methods is often associated with the role played by the multi-layer perceptron projector. In this work, we depart from this observation and reexamine the role of supervision in continual representation learning. We reckon that additional information, such as human annotations, should not deteriorate the quality of representations. Our findings show that supervised models when enhanced with a multi-layer perceptron head, can outperform self-supervised models in continual representation learning. This highlights the importance of the multi-layer perceptron projector in shaping feature transferability across a sequence of tasks in continual learning. The code is available on github",
    "checked": true,
    "id": "c843cd3e0d83dc364ec458f79fb53539f2b5e5dd",
    "semantic_title": "revisiting supervision for continual representation learning",
    "citation_count": 0,
    "authors": [
      "Daniel Marczak*",
      "Sebastian Cygert*",
      "Tomasz Trzcinski*",
      "Bartlomiej Twardowski*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/951_ECCV_2024_paper.php": {
    "title": "FLAT: Flux-aware Imperceptible Adversarial Attacks on 3D Point Clouds",
    "volume": "main",
    "abstract": "Adversarial attacks on point clouds play a vital role in assessing and enhancing the adversarial robustness of 3D deep learning models. While employing a variety of geometric constraints, existing adversarial attack solutions often display unsatisfactory imperceptibility due to inadequate consideration of uniformity changes. In this paper, we propose , a novel framework designed to generate imperceptible adversarial point clouds by addressing the issue from a flux perspective. Specifically, during adversarial attacks, we assess the extent of uniformity alterations by calculating the flux of the local perturbation vector field. Upon identifying a high flux, which signals potential disruption in uniformity, the directions of the perturbation vectors are adjusted to minimize these alterations, thereby improving imperceptibility. Extensive experiments validate the effectiveness of in generating imperceptible adversarial point clouds, and its superiority to the state-of-the-art methods",
    "checked": true,
    "id": "4df3bdf0e604b155e8315d04bb7c87ac06968dc3",
    "semantic_title": "flat: flux-aware imperceptible adversarial attacks on 3d point clouds",
    "citation_count": 0,
    "authors": [
      "Keke Tang",
      "Lujie Huang",
      "Weilong Peng*",
      "Daizong Liu",
      "Xiaofei Wang",
      "Yang Ma",
      "Ligang Liu",
      "Zhihong Tian"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/959_ECCV_2024_paper.php": {
    "title": "MMBENCH: Is Your Multi-Modal Model an All-around Player?",
    "volume": "main",
    "abstract": "Large vision-language models (VLMs) have recently achieved remarkable progress, exhibiting impressive multimodal perception and reasoning abilities. However, effectively evaluating these large VLMs remains a major challenge, hindering future development in this domain. Traditional benchmarks like VQAv2 or COCO Caption provide quantitative performance measurements but lack fine-grained ability assessment and robust evaluation metrics. Meanwhile, subjective benchmarks, such as OwlEval, offer comprehensive evaluations of a model's abilities by incorporating human labor, which is not scalable and may display significant bias. In response to these challenges, we propose MMBench, a bilingual benchmark for assessing the multi-modal capabilities of VLMs. MMBench methodically develops a comprehensive evaluation pipeline, primarily comprised of the following key features: 1. MMBench is meticulously curated with well-designed quality control schemes, surpassing existing similar benchmarks in terms of the number and variety of evaluation questions and abilities; 2. MMBench introduces a rigorous CircularEval strategy and incorporates large language models to convert free-form predictions into pre-defined choices, which helps to yield accurate evaluation results for models with limited instruction-following capabilities. 3. MMBench incorporates multiple-choice questions in both English and Chinese versions, enabling an apples-to-apples comparison of VLMs' performance under a bilingual context. To summarize, MMBench is a systematically designed objective benchmark for a robust and holistic evaluation of vision-language models. We hope MMBench will assist the research community in better evaluating their models and facilitate future progress in this area. MMBench has been supported in VLMEvalKit1 . 1 https://github.com/open-compass/VLMEvalKit",
    "checked": true,
    "id": "b37b1dc72b1882858f5120f2cd6883134089a6ed",
    "semantic_title": "mmbench: is your multi-modal model an all-around player?",
    "citation_count": 514,
    "authors": [
      "Yuan Liu*",
      "Haodong Duan*",
      "Yuanhan Zhang",
      "Bo Li",
      "Songyang Zhang",
      "Wangbo Zhao",
      "Yike Yuan",
      "Jiaqi Wang",
      "Conghui He",
      "Ziwei Liu",
      "Kai Chen",
      "Dahua Lin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/960_ECCV_2024_paper.php": {
    "title": "Implicit Filtering for Learning Neural Signed Distance Functions from 3D Point Clouds",
    "volume": "main",
    "abstract": "Neural signed distance functions (SDFs) have shown powerful ability in fitting the shape geometry. However, inferring continuous signed distance fields from discrete unoriented point clouds still remains a challenge. The neural network typically fits the shape with a rough surface and omits fine-grained geometric details such as shape edges and corners. In this paper, we propose a novel non-linear implicit filter to smooth the implicit field while preserving high-frequency geometry details. Our novelty lies in that we can filter the surface (zero level set) by the neighbor input points with gradients of the signed distance field. By moving the input raw point clouds along the gradient, our proposed implicit filtering can be extended to non-zero level sets to keep the promise consistency between different level sets, which consequently results in a better regularization of the zero level set. We conduct comprehensive experiments in surface reconstruction from objects and complex scene point clouds, the numerical and visual comparisons demonstrate our improvements over the state-of-the-art methods under the widely used benchmarks. Project page: https://list17.github.io/ImplicitFilter",
    "checked": true,
    "id": "8a0dc936ad7aba7e4766f5f08438d2e70ae8cc12",
    "semantic_title": "implicit filtering for learning neural signed distance functions from 3d point clouds",
    "citation_count": 1,
    "authors": [
      "Shengtao Li*",
      "Ge Gao",
      "Yudong Liu",
      "Ming Gu",
      "Yu-Shen Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/986_ECCV_2024_paper.php": {
    "title": "Unsupervised Exposure Correction",
    "volume": "main",
    "abstract": "Current exposure correction methods have three challenges, labor-intensive paired data annotation, limited generalizability, and performance degradation in low-level computer vision tasks. In this work, we introduce an innovative Unsupervised Exposure Correction (UEC) method that eliminates the need for manual annotations, offers improved generalizability, and enhances performance in low-level downstream tasks. Our model is trained using freely available paired data from an emulated Image Signal Processing (ISP) pipeline. This approach does not need expensive manual annotations, thereby minimizing individual style biases from the annotation and consequently improving its generalizability. Furthermore, we present a large-scale Radiometry Correction Dataset, specifically designed to emphasize exposure variations, to facilitate unsupervised learning. In addition, we develop a transformation function that preserves image details and outperforms state-of-the-art supervised methods[?], while utilizing only 0.01% of their parameters. Our work further investigates the broader impact of exposure correction on downstream tasks, including edge detection, demonstrating its effectiveness in mitigating the adverse effects of poor exposure on low-level features. The source code and dataset are publicly available at https://github.com/BeyondHeaven/uec_code",
    "checked": true,
    "id": "9410b47d851c3a792a1ab6d035f310cc43236820",
    "semantic_title": "unsupervised exposure correction",
    "citation_count": 0,
    "authors": [
      "Ruodai Cui*",
      "Li Niu",
      "Guosheng Hu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/990_ECCV_2024_paper.php": {
    "title": "Anytime Continual Learning for Open Vocabulary Classification",
    "volume": "main",
    "abstract": "We propose an approach for anytime continual learning (AnytimeCL) for open vocabulary image classification. The AnytimeCL problem aims to break away from batch training and rigid models by requiring that a system can predict any set of labels at any time and efficiently update and improve when receiving one or more training samples at any time. Despite the challenging goal, we achieve substantial improvements over recent methods. We propose a dynamic weighting between predictions of a partially fine-tuned model and a fixed open vocabulary model that enables continual improvement when training samples are available for a subset of a task's labels. We also propose an attention-weighted PCA compression of training features that reduces storage and computation with little impact to model accuracy. Our methods are validated with experiments that test flexibility of learning and inference",
    "checked": true,
    "id": "2b762bd591e54217e1905279f285c30d7bd4abe2",
    "semantic_title": "anytime continual learning for open vocabulary classification",
    "citation_count": 0,
    "authors": [
      "Zhen Zhu*",
      "Yiming Gong",
      "Derek Hoiem*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/991_ECCV_2024_paper.php": {
    "title": "External Knowledge Enhanced 3D Scene Generation from Sketch",
    "volume": "main",
    "abstract": "Generating realistic 3D scenes is challenging due to the complexity of room layouts and object geometries. We propose a sketch based knowledge enhanced diffusion architecture (SEK) for generating customized, diverse, and plausible 3D scenes. SEK conditions the denoising process with a hand-drawn sketch of the target scene and cues from an object relationship knowledge base. We first construct an external knowledge base containing object relationships and then leverage knowledge enhanced graph reasoning to assist our model in understanding hand-drawn sketches. A scene is represented as a combination of 3D objects and their relationships, and then incrementally diffused to reach a Gaussian distribution. We propose a 3D denoising scene transformer that learns to reverse the diffusion process, conditioned by a hand-drawn sketch along with knowledge cues, to regressively generate the scene including the 3D object instances as well as their layout. Experiments on the 3D-FRONT dataset show that our model improves FID, CKL by 17.41%, 37.18% in 3D scene generation and FID, KID by 19.12%, 20.06% in 3D scene completion compared to the nearest competitor DiffuScene",
    "checked": true,
    "id": "2ed081f094af37fa45b90994d9408b3fd48db46e",
    "semantic_title": "external knowledge enhanced 3d scene generation from sketch",
    "citation_count": 1,
    "authors": [
      "Zijie Wu",
      "Mingtao Feng*",
      "Yaonan Wang",
      "He Xie",
      "Weisheng Dong",
      "Bo Miao",
      "Ajmal Mian"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/992_ECCV_2024_paper.php": {
    "title": "G3R: Gradient Guided Generalizable Reconstruction",
    "volume": "main",
    "abstract": "Large scale 3D scene reconstruction is important for applications such as virtual reality and simulation. Existing neural rendering approaches (, NeRF, 3DGS) have achieved realistic reconstructions on large scenes, but optimize per scene, which is expensive and slow, and exhibit noticeable artifacts under large view changes due to overfitting. Generalizable approaches, or large reconstruction models, are fast, but primarily work for small scenes/objects and often produce lower quality rendering results. In this work, we introduce , a generalizable reconstruction approach that can efficiently predict high-quality 3D scene representations for large scenes. We propose to learn a reconstruction network that takes the gradient feedback signals from differentiable rendering to iteratively update a 3D scene representation, combining the benefits of high photorealism from per-scene optimization with data-driven priors from fast feed-forward prediction methods. Experiments on urban-driving and drone datasets show that generalizes across diverse large scenes and accelerates the reconstruction process by at least 10× while achieving comparable or better realism compared to 3DGS, and also being more robust to large view changes. Please visit our project page for more results: https://waabi.ai/g3r",
    "checked": true,
    "id": "485a016b60c3fc4d580036ce0d517cbc59eda9be",
    "semantic_title": "g3r: gradient guided generalizable reconstruction",
    "citation_count": 1,
    "authors": [
      "Yun Chen*",
      "Jingkang Wang",
      "Ze Yang",
      "Sivabalan Manivasagam*",
      "Raquel Urtasun*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/996_ECCV_2024_paper.php": {
    "title": "DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic Gaussian Splatting",
    "volume": "main",
    "abstract": "The increasing demand for virtual reality applications has highlighted the significance of crafting immersive 3D assets. We present a text-to-3D 360◦ scene generation pipeline that facilitates the creation of comprehensive 360◦ scenes for in-the-wild environments in a matter of minutes. Our approach utilizes the generative power of a 2D diffusion model and prompt self-refinement to create a high-quality and globally coherent panoramic image. This image acts as a preliminary \"flat\" (2D) scene representation. Subsequently, it is lifted into 3D Gaussians, employing splatting techniques to enable real-time exploration. To produce consistent 3D geometry, our pipeline constructs a spatially coherent structure by aligning the 2D monocular depth into a globally optimized point cloud. This point cloud serves as the initial state for the centroids of 3D Gaussians. In order to address invisible issues inherent in single-view inputs, we impose semantic and geometric constraints on both synthesized and input camera views as regularizations. These guide the optimization of Gaussians, aiding in the reconstruction of unseen regions. In summary, our method offers a globally consistent 3D scene within a 360◦ perspective, providing an enhanced immersive experience over existing techniques. Project website at: http:// dreamscene360.github.io/",
    "checked": true,
    "id": "4a0baaa32e955093d36746d9b3569d95ceee58c5",
    "semantic_title": "dreamscene360: unconstrained text-to-3d scene generation with panoramic gaussian splatting",
    "citation_count": 13,
    "authors": [
      "Shijie Zhou*",
      "Zhiwen Fan",
      "Dejia Xu",
      "Haoran Chang",
      "Pradyumna Chari",
      "Tejas K Bharadwaj",
      "Suya You",
      "Zhangyang Wang",
      "Achuta Kadambi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1001_ECCV_2024_paper.php": {
    "title": "Frequency-Spatial Entanglement Learning for Camouflaged Object Detection",
    "volume": "main",
    "abstract": "Camouflaged object detection has attracted a lot of attention in computer vision. The main challenge lies in the high degree of similarity between camouflaged objects and their surroundings in the spatial domain, making identification difficult. Existing methods attempt to reduce the impact of pixel similarity by maximizing the distinguishing ability of spatial features with complicated design, but often ignore the sensitivity and locality of features in the spatial domain, leading to sub-optimal results. In this paper, we propose a new approach to address this issue by jointly exploring the representation in the frequency and spatial domains, introducing the Frequency-Spatial Entanglement Learning (FSEL) method. This method consists of a series of well-designed Entanglement Transformer Blocks (ETB) for representation learning, a Joint Domain Perception Module for semantic enhancement, and a Dual-domain Reverse Parser for feature integration in the frequency and spatial domains. Specifically, the ETB utilizes frequency self-attention to effectively characterize the relationship between different frequency bands, while the entanglement feed-forward network facilitates information interaction between features of different domains through entanglement learning. Our extensive experiments demonstrate the superiority of our FSEL over 21 state-of-the-art methods, through comprehensive quantitative and qualitative comparisons in three widely-used datasets. The source code is available at: bluehttps://github.com/CSYSI/FSEL",
    "checked": true,
    "id": "df087f3b8cce3575662e10147c0242bf0d48d10a",
    "semantic_title": "frequency-spatial entanglement learning for camouflaged object detection",
    "citation_count": 1,
    "authors": [
      "Yanguang Sun",
      "Chunyan Xu",
      "Jian Yang",
      "Hanyu Xuan*",
      "Lei Luo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1005_ECCV_2024_paper.php": {
    "title": "VisionTrap: Vision-Augmented Trajectory Prediction Guided by Textual Descriptions",
    "volume": "main",
    "abstract": "Predicting future trajectories for other road agents is an essential task for autonomous vehicles. Established trajectory prediction methods primarily use agent tracks generated by a detection and tracking system and HD map as inputs. In this work, we propose a novel method that also incorporates visual input from surround-view cameras, allowing the model to utilize visual cues such as human gazes and gestures, road conditions, vehicle turn signals, etc, which are typically hidden from the model in prior methods. Furthermore, we use textual descriptions generated by a Vision-Language Model (VLM) and refined by a Large Language Model (LLM) as supervision during training to guide the model on what to learn from the input data. Despite using these extra inputs, our method achieves a latency of 53 ms, making it feasible for real-time processing, which is significantly faster than that of previous single-agent prediction methods with similar performance. Our experiments show that both the visual inputs and the textual descriptions contribute to improvements in trajectory prediction performance, and our qualitative analysis highlights how the model is able to exploit these additional inputs. Lastly, in this work we create and release the nuScenes-Text dataset, which augments the established nuScenes dataset with rich textual annotations for every scene, demonstrating the positive impact of utilizing VLM on trajectory prediction. Our project page is at https://moonseokha.github.io/VisionTrap",
    "checked": true,
    "id": "7ed3f3624eb358faa1bfeaa9a50fdd5bc3b74033",
    "semantic_title": "visiontrap: vision-augmented trajectory prediction guided by textual descriptions",
    "citation_count": 0,
    "authors": [
      "Seokha Moon",
      "Hyun Woo",
      "Hongbeen Park",
      "Haeji Jung",
      "Reza Mahjourian",
      "Hyung-gun Chi",
      "Hyerin Lim",
      "Sangpil Kim",
      "Jinkyu Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1016_ECCV_2024_paper.php": {
    "title": "Occluded Gait Recognition with Mixture of Experts: An Action Detection Perspective",
    "volume": "main",
    "abstract": "Extensive occlusions in real-world scenarios pose challenges to gait recognition due to missing and noisy information, as well as body misalignment in position and scale. We argue that rich dynamic contextual information within a gait sequence inherently possesses occlusion-solving traits: 1) Adjacent frames with gait continuity allow holistic body regions to infer occluded body regions; 2) Gait cycles allow information integration between holistic actions and occluded actions. Therefore, we introduce an action detection perspective where a gait sequence is regarded as a composition of actions. To detect accurate actions under complex occlusion scenarios, we propose an Action Detection Based Mixture of Experts (GaitMoE), consisting of Mixture of Temporal Experts (MTE) and Mixture of Action Experts (MAE). MTE adaptively constructs action anchors by temporal experts and MAE adaptively constructs action proposals from action anchors by action experts. Especially, action detection as a proxy task with gait recognition is an end-to-end joint training only with ID labels. In addition, due to the lack of a unified occluded benchmark, we construct a pioneering Occluded Gait database (OccGait), containing rich occlusion scenarios and annotations of occlusion types. Extensive experiments on OccGait, OccCASIA-B, Gait3D and GREW demonstrate the superior performance of GaitMoE. OccGait is available at https://github.com/BNU-IVC/OccGait",
    "checked": true,
    "id": "acb7d265c6ee4de392f7677194764320d36ab4c6",
    "semantic_title": "occluded gait recognition with mixture of experts: an action detection perspective",
    "citation_count": 0,
    "authors": [
      "Panjian Huang",
      "Yunjie Peng",
      "Saihui Hou*",
      "Chunshui Cao",
      "Xu Liu",
      "Zhiqiang He",
      "Yongzhen Huang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1019_ECCV_2024_paper.php": {
    "title": "EDTalk: Efficient Disentanglement for Emotional Talking Head Synthesis",
    "volume": "main",
    "abstract": "Achieving disentangled control over multiple facial motions and accommodating diverse input modalities greatly enhances the application and entertainment of the talking head generation. This necessitates a deep exploration of the decoupling space for facial features, ensuring that they a) operate independently without mutual interference and b) can be preserved to share with different modal inputs—both aspects often neglected in existing methods. To address this gap, this paper proposes a novel Efficient Disentanglement framework for Talking head generation (EDTalk). Our framework enables individual manipulation of mouth shape, head pose, and emotional expression, conditioned on video or audio inputs. Specifically, we employ three lightweight modules to decompose the facial dynamics into three distinct latent spaces representing mouth, pose, and expression, respectively. Each space is characterized by a set of learnable bases whose linear combinations define specific motions. To ensure independence and accelerate training, we enforce orthogonality among bases and devise an efficient training strategy to allocate motion responsibilities to each space without relying on external knowledge. The learned bases are then stored in corresponding banks, enabling shared visual priors with audio input. Furthermore, considering the properties of each space, we propose an Audio-to-Motion module for audio-driven talking head synthesis. Experiments are conducted to demonstrate the effectiveness of EDTalk. The code and pretrained models are released at: https://tanshuai0219.github.io/EDTalk/",
    "checked": true,
    "id": "f9511d7409f72ecaa2634af02f4be3a8643c4037",
    "semantic_title": "edtalk: efficient disentanglement for emotional talking head synthesis",
    "citation_count": 7,
    "authors": [
      "Shuai Tan*",
      "Bin Ji",
      "Mengxiao Bi",
      "ye pan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1023_ECCV_2024_paper.php": {
    "title": "Groma: Localized Visual Tokenization for Grounding Multimodal Large Language Models",
    "volume": "main",
    "abstract": "We introduce Groma, a Multimodal Large Language Model (MLLM) with grounded and fine-grained visual perception ability. Beyond holistic image understanding, Groma is adept at region-level tasks such as region captioning and visual grounding. Such capabilities are built upon a localized visual tokenization mechanism, where an image input is decomposed into regions of interest and subsequently encoded into region tokens. By integrating region tokens into user instructions and model responses, we seamlessly enable Groma to understand user-specified region inputs and ground its textual output to images. Besides, to enhance the grounded chat ability of Groma, we curate a visually grounded instruction dataset by leveraging the powerful GPT-4V and visual prompting techniques. Compared with MLLMs that rely on the language model or external module for localization, Groma consistently demonstrates superior performances in standard referring and grounding benchmarks, highlighting the advantages of embedding localization into image tokenization. Project page: https://groma-mllm.github.io/",
    "checked": true,
    "id": "0be923cad65522c921dfaa04e517b051593121ce",
    "semantic_title": "groma: localized visual tokenization for grounding multimodal large language models",
    "citation_count": 15,
    "authors": [
      "Chuofan Ma*",
      "Yi Jiang*",
      "Jiannan Wu",
      "Zehuan Yuan",
      "Xiaojuan Qi*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1025_ECCV_2024_paper.php": {
    "title": "On the Utility of 3D Hand Poses for Action Recognition",
    "volume": "main",
    "abstract": "3D hand pose is an underexplored modality for action recognition. Poses are compact yet informative and can greatly benefit applications with limited compute budgets. However, poses alone offer an incomplete understanding of actions, as they cannot fully capture objects and environments with which humans interact. We propose HandFormer, a novel multimodal transformer, to efficiently model hand-object interactions. HandFormer combines 3D hand poses at a high temporal resolution for fine-grained motion modeling with sparsely sampled RGB frames for encoding scene semantics. Observing the unique characteristics of hand poses, we temporally factorize hand modeling and represent each joint by its short-term trajectories. This factorized pose representation combined with sparse RGB samples is remarkably efficient and highly accurate. Unimodal HandFormer with only hand poses outperforms existing skeleton-based methods at 5× fewer FLOPs. With RGB, we achieve new state-of-the-art performance on Assembly101 and H2O with significant improvements in egocentric action recognition",
    "checked": true,
    "id": "241ecc8392ef3dda094119992681fde070edad31",
    "semantic_title": "on the utility of 3d hand poses for action recognition",
    "citation_count": 2,
    "authors": [
      "Md Salman Shamil*",
      "Dibyadip Chatterjee",
      "Fadime Sener",
      "Shugao Ma",
      "Angela Yao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1036_ECCV_2024_paper.php": {
    "title": "DG-PIC: Domain Generalized Point-In-Context Learning for Point Cloud Understanding",
    "volume": "main",
    "abstract": "Recent point cloud understanding research suffers from performance drops on unseen data, due to the distribution shifts across different domains. While recent studies use Domain Generalization (DG) techniques to mitigate this by learning domain-invariant features, most are designed for a single task and neglect the potential of testing data. Despite In-Context Learning (ICL) showcasing multi-task learning capability, it usually relies on high-quality context-rich data and considers a single dataset, and has rarely been studied in point cloud understanding. In this paper, we introduce a novel, practical, multi-domain multi-task setting, handling multiple domains and multiple tasks within one unified model for domain generalized point cloud understanding. To this end, we propose Domain Generalized Point-In-Context Learning (DG-PIC) that boosts the generalizability across various tasks and domains at testing time. In particular, we develop dual-level source prototype estimation that considers both global-level shape contextual and local-level geometrical structures for representing source domains and a dual-level test-time feature shifting mechanism that leverages both macro-level domain semantic information and micro-level patch positional relationships to pull the target data closer to the source ones during the testing. Our DG-PIC does not require any model updates during the testing and can handle unseen domains and multiple tasks, i.e., point cloud reconstruction, denoising, and registration, within one unified model. We also introduce a benchmark for this new setting. Comprehensive experiments demonstrate that DG-PIC outperforms state-of-the-art techniques significantly",
    "checked": true,
    "id": "fd3915245c765aa063351b865aa147014d3a093a",
    "semantic_title": "dg-pic: domain generalized point-in-context learning for point cloud understanding",
    "citation_count": 5,
    "authors": [
      "Jincen Jiang",
      "Qianyu Zhou",
      "Yuhang Li",
      "Xuequan Lu*",
      "Meili Wang*",
      "Lizhuang Ma",
      "Jian Chang",
      "Jian Jun Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1043_ECCV_2024_paper.php": {
    "title": "Operational Open-Set Recognition and PostMax Refinement",
    "volume": "main",
    "abstract": "Open-Set Recognition (OSR) is a problem with mainly practical applications. However, recent evaluations have largely focused on small-scale data and tuning thresholds over the test set, which disregard the real-world operational needs of parameter selection. Thus, we revisit the original goals of OSR and propose a new evaluation metric, Operational Open-Set Accuracy (OOSA), which requires predicting an operationally relevant threshold from a validation set with known and a surrogate set with unknown samples, and then applying this threshold during testing. With this new measure in mind, we develop a large-scale evaluation protocol suited for operational scenarios. Additionally, we introduce the novel PostMax algorithm that performs post-processing refinement of the logit of the maximal class. This refinement involves normalizing logits by deep feature magnitudes and utilizing an extreme-value-based generalized Pareto distribution to map them into proper probabilities. We evaluate multiple pre-trained deep networks, including leading transformer and convolution-based architectures, on different selections of large-scale surrogate and test sets. Our experiments demonstrate that PostMax advances the state of the art in open-set recognition, showing statistically significant improvements in our novel OOSA metric as well as in previously used metrics such as AUROC, FPR95, and others",
    "checked": true,
    "id": "fb22a09669fefa4d7bb040eae61440c5ea623e9e",
    "semantic_title": "operational open-set recognition and postmax refinement",
    "citation_count": 0,
    "authors": [
      "Steve Cruz*",
      "Ryan Rabinowitz",
      "Manuel Günther",
      "Terrance E. Boult"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1046_ECCV_2024_paper.php": {
    "title": "ScaleDreamer: Scalable Text-to-3D Synthesis with Asynchronous Score Distillation",
    "volume": "main",
    "abstract": "By leveraging the text-to-image diffusion prior, score distillation can synthesize 3D contents without paired text-3D training data. Instead of spending hours of online optimization per text prompt, recent studies have been focused on learning a text-to-3D generative network for amortizing multiple text-3D relations, which can synthesize 3D contents in seconds. However, existing score distillation methods are hard to scale up to a large amount of text prompts due to the difficulties in aligning pretrained diffusion prior with the distribution of rendered images from various text prompts. Current state-of-the-arts such as Variational Score Distillation finetune the pretrained diffusion model to minimize the noise prediction error so as to align the distributions, which are however unstable to train and will impair the model's comprehension capability to numerous text prompts. Based on the observation that the diffusion models tend to have lower noise prediction errors at earlier timesteps, we propose Asynchronous Score Distillation (ASD), which minimizes the noise prediction error by shifting the diffusion timestep to earlier ones. ASD is stable to train and can scale up to 100k prompts. It reduces the noise prediction error without changing the weights of pre-trained diffusion model, thus keeping its strong comprehension capability to prompts. We conduct extensive experiments using different text-to-3D architectures, including Hyper-iNGP and 3DConv-Net. The results demonstrate ASD's effectiveness in stable 3D generator training, high-quality 3D content synthesis, and its superior prompt-consistency, especially under large prompt corpus. Code is available at https://github.com/theEricMa/ScaleDreamer",
    "checked": true,
    "id": "84dba30ad62b37212d1f3f709afdcffbd028800b",
    "semantic_title": "scaledreamer: scalable text-to-3d synthesis with asynchronous score distillation",
    "citation_count": 2,
    "authors": [
      "Zhiyuan Ma*",
      "Yuxiang Wei",
      "Yabin Zhang",
      "Xiangyu Zhu",
      "Zhen Lei",
      "Lei Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1055_ECCV_2024_paper.php": {
    "title": "SINDER: Repairing the Singular Defects of DINOv2",
    "volume": "main",
    "abstract": "Vision Transformer models trained on large-scale datasets, although effective, often exhibit artifacts in the patch token they extract. While such defects can be alleviated by re-training the entire model with additional classification tokens, the underlying reasons for the presence of these tokens remain unclear. In this paper, we conduct a thorough investigation of this phenomenon, combining theoretical analysis with empirical observations. Our findings reveal that these artifacts originate from the pre-trained network itself, specifically stemming from the leading left singular vector of the network's weights. Furthermore, to mitigate these defects, we propose a novel fine-tuning smooth regularization that rectifies structural deficiencies using only a small dataset, thereby avoiding the need for complete re-training. We validate our method on various downstream tasks, including unsupervised segmentation, classification, supervised segmentation, and depth estimation, demonstrating its effectiveness in improving model performance. Codes and checkpoints are available at https://github.com/haoqiwang/sinder",
    "checked": true,
    "id": "d0a6fb583971a5b3ffc89780ff914d8beae02a9c",
    "semantic_title": "sinder: repairing the singular defects of dinov2",
    "citation_count": 0,
    "authors": [
      "Haoqi Wang",
      "Tong Zhang",
      "Mathieu Salzmann*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1065_ECCV_2024_paper.php": {
    "title": "SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow",
    "volume": "main",
    "abstract": "We introduce SEA-RAFT, a more simple, efficient, and accurate RAFT for optical flow. Compared with RAFT, SEA-RAFT is trained with a new loss (mixture of Laplace). It directly regresses an initial flow for faster convergence in iterative refinements and introduces rigid-motion pre-training to improve generalization. SEA-RAFT achieves state-of-the-art accuracy on the Spring benchmark with a 3.69 endpoint-error (EPE) and a 0.36 1-pixel outlier rate (1px), representing 22.9% and 17.8% error reduction from best published results. In addition, SEA-RAFT obtains the best cross-dataset generalization on KITTI and Spring. With its high efficiency, SEA-RAFT operates at least 2.3× faster than existing methods while maintaining competitive performance. The code is publicly available at https://github.com/princeton-vl/SEA-RAFT",
    "checked": true,
    "id": "561ecdcac76f52d8c74df916c6d49d08f10522b4",
    "semantic_title": "sea-raft: simple, efficient, accurate raft for optical flow",
    "citation_count": 5,
    "authors": [
      "Yihan Wang*",
      "Lahav O Lipson",
      "Jia Deng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1066_ECCV_2024_paper.php": {
    "title": "Learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation",
    "volume": "main",
    "abstract": "While the success of deep learning relies on large amounts of training datasets, data is often limited in privacy-sensitive domains. To address this challenge, generative model learning with differential privacy has emerged as a solution to train private generative models for desensitized data generation. However, the quality of the images generated by existing methods is limited due to the complexity of modeling data distribution. We build on the success of diffusion models and introduce DP-SAD, which trains a private diffusion model by a stochastic adversarial distillation method. Specifically, we first train a diffusion model as a teacher and then train a student by distillation, in which we achieve differential privacy by adding noise to the gradients from other models to the student. For better generation quality, we introduce a discriminator to distinguish whether an image is from the teacher or the student, which forms the adversarial training. Extensive experiments and analysis clearly demonstrate the effectiveness of our proposed method",
    "checked": true,
    "id": "1f35a841e42722d3554784d66c8c9ccb12f4b1be",
    "semantic_title": "learning differentially private diffusion models via stochastic adversarial distillation",
    "citation_count": 1,
    "authors": [
      "Bochao Liu",
      "Pengju Wang",
      "Shiming Ge*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1070_ECCV_2024_paper.php": {
    "title": "General and Task-Oriented Video Segmentation",
    "volume": "main",
    "abstract": "We present GvSeg, a general video segmentation framework for addressing four different video segmentation tasks (, instance, semantic, panoptic, and exemplar-guided) while maintaining an identical architectural design. Currently, there is a trend towards developing general video segmentation solutions that can be applied across multiple tasks. This streamlines research endeavors and simplifies deployment. However, such a highly homogenized framework in current design, where each element maintains uniformity, could overlook the inherent diversity among different tasks and lead to suboptimal performance. To tackle this, GvSeg: i) provides a holistic disentanglement and modeling for segment targets, thoroughly examining them from the perspective of appearance, position, and shape, and on this basis, ii) reformulates the query initialization, matching and sampling strategies in alignment with the task-specific requirement. These architecture-agnostic innovations empower GvSeg to effectively address each unique task by accommodating the specific properties that characterize them. Extensive experiments on seven gold-standard benchmark datasets demonstrate that GvSeg surpasses all existing specialized/general solutions by a significant margin on four different video segmentation tasks",
    "checked": true,
    "id": "761aa8a28485281fe4a10562bd6af51fc75a7183",
    "semantic_title": "general and task-oriented video segmentation",
    "citation_count": 3,
    "authors": [
      "Mu Chen",
      "Liulei Li",
      "Wenguan Wang",
      "Ruijie Quan",
      "Yi Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1071_ECCV_2024_paper.php": {
    "title": "VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement",
    "volume": "main",
    "abstract": "In recent years, online Video Instance Segmentation (VIS) methods have shown remarkable advancement with their powerful query-based detectors. Utilizing the output queries of the detector at the frame-level, these methods achieve high accuracy on challenging benchmarks. However, our observations demonstrate that these methods heavily rely on location information, which often causes incorrect associations between objects. This paper presents that a key axis of object matching in trackers is appearance information, which becomes greatly instructive under conditions where positional cues are insufficient for distinguishing their identities. Therefore, we suggest a simple yet powerful extension to object decoders that explicitly extract embeddings from backbone features and drive queries to capture the appearances of objects, which greatly enhances instance association accuracy. Furthermore, recognizing the limitations of existing benchmarks in fully evaluating appearance awareness, we have constructed a synthetic dataset to rigorously validate our method. By effectively resolving the over-reliance on location information, we achieve state-of-the-art results on YouTube-VIS 2019/2021 and Occluded VIS (OVIS). Code is available at https://github.com/KimHanjung/VISAGE",
    "checked": true,
    "id": "7785b8a68e540b7685f83d86c6db037a82da59c9",
    "semantic_title": "visage: video instance segmentation with appearance-guided enhancement",
    "citation_count": 0,
    "authors": [
      "Hanjung Kim",
      "Jaehyun Kang",
      "Miran Heo",
      "Sukjun Hwang",
      "Seoung Wug Oh",
      "Seon Joo Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1086_ECCV_2024_paper.php": {
    "title": "LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors",
    "volume": "main",
    "abstract": "We present a simple self-supervised method to enhance the performance of ViT features for dense downstream tasks. Our Lightweight Feature Transform (LiFT) is a straightforward and compact postprocessing network that can be applied to enhance the features of any pre-trained ViT backbone. LiFT is fast and easy to train with a self-supervised objective, and it boosts the density of ViT features for minimal extra inference cost. Furthermore, we demonstrate that LiFT can be applied with approaches that use additional task-specific downstream modules, as we integrate LiFT with ViTDet for COCO detection and segmentation. Despite the simplicity of LiFT, we find that it is not simply learning a more complex version of bilinear interpolation. Instead, our LiFT training protocol leads to several desirable emergent properties that benefit ViT features in dense downstream tasks. This includes greater scale invariance for features, and better object boundary maps. By simply training LiFT for a few epochs, we show improved performance on keypoint correspondence, detection, segmentation, and object discovery tasks. Overall, LiFT provides an easy way to unlock the benefits of denser feature arrays for a fraction of the computational cost. For more details, refer to our magentaproject page",
    "checked": true,
    "id": "dab3e9c381b1aa2e471e65d0acc2db4cdbb005d8",
    "semantic_title": "lift: a surprisingly simple lightweight feature transform for dense vit descriptors",
    "citation_count": 0,
    "authors": [
      "Saksham Suri*",
      "Matthew Walmer",
      "Kamal Gupta",
      "Abhinav Shrivastava"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1091_ECCV_2024_paper.php": {
    "title": "ControlNet++: Improving Conditional Controls with Efficient Consistency Feedback",
    "volume": "main",
    "abstract": "To enhance the controllability of text-to-image diffusion models, existing efforts like ControlNet incorporated image-based conditional controls. In this paper, we reveal that existing methods still face significant challenges in generating images that align with the image conditional controls. To this end, we propose ControlNet++, a novel approach that improves controllable generation by explicitly optimizing pixel-level cycle consistency between generated images and conditional controls. Specifically, for an input conditional control, we use a pre-trained discriminative reward model to extract the corresponding condition of the generated images, and then optimize the consistency loss between the input conditional control and extracted condition. A straightforward implementation would be generating images from random noises and then calculating the consistency loss, but such an approach requires storing gradients for multiple sampling timesteps, leading to considerable time and memory costs. To address this, we introduce an efficient reward strategy that deliberately disturbs the input images by adding noise, and then uses the single-step denoised images for reward fine-tuning. This avoids the extensive costs associated with image sampling, allowing for more efficient reward fine-tuning. Extensive experiments show that ControlNet++ significantly improves controllability under various conditional controls. For example, it achieves improvements over ControlNet by 11.1% mIoU, 13.4% SSIM, and 7.6% RMSE, respectively, for segmentation mask, line-art edge, and depth conditions. All the code, models, demo and organized data have been open sourced on our magentaGithub Repo",
    "checked": true,
    "id": "502e7c8921de673f79d957e602cef356cb9d7956",
    "semantic_title": "controlnet++: improving conditional controls with efficient consistency feedback",
    "citation_count": 10,
    "authors": [
      "Ming Li*",
      "Taojiannan Yang",
      "Huafeng Kuang",
      "Jie Wu",
      "Zhaoning Wang",
      "Xuefeng Xiao",
      "Chen Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1098_ECCV_2024_paper.php": {
    "title": "TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-Spoofing",
    "volume": "main",
    "abstract": "Generalizable Face anti-spoofing (FAS) approaches have recently garnered considerable attention due to their robustness in unseen scenarios. Some recent methods incorporate vision-language models into FAS, leveraging their impressive pre-trained performance to improve the generalization. However, these methods only utilize coarse-grained or single-element prompts for fine-tuning FAS tasks, without fully exploring the potential of language supervision, leading to unsatisfactory generalization ability. To address these concerns, we propose a novel framework called TF-FAS, which aims to thoroughly explore and harness twofold-element fine-grained semantic guidance to enhance generalization. Specifically, the Content Element Decoupling Module (CEDM) is proposed to comprehensively explore the semantic elements related to content. It is subsequently employed to supervise the decoupling of categorical features from content-related features, thereby enhancing the generalization abilities. Moreover, recognizing the subtle differences within the data of each class in FAS, we present a Fine-Grained Categorical Element Module (FCEM) to explore fine-grained categorical element guidance, then adaptively integrate them to facilitate the distribution modeling for each class. Comprehensive experiments and analysis demonstrate the superiority of our method over state-of-the-art competitors. Code:https://github.com/xudongww/TF-FAS",
    "checked": true,
    "id": "2752e9a88de87426027c9a6d60b5e655229b5677",
    "semantic_title": "tf-fas: twofold-element fine-grained semantic guidance for generalizable face anti-spoofing",
    "citation_count": 1,
    "authors": [
      "Xudong Wang",
      "Ke-Yue Zhang",
      "Taiping Yao*",
      "Qianyu Zhou",
      "Shouhong Ding",
      "Pingyang Dai*",
      "Rongrong Ji"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1102_ECCV_2024_paper.php": {
    "title": "Prompting Future Driven Diffusion Model for Hand Motion Prediction",
    "volume": "main",
    "abstract": "Hand motion prediction from both first- and third-person perspectives is vital for enhancing user experience in AR/VR and ensuring safe remote robotic arm control. Previous works typically focus on predicting hand motion trajectories or human body motion, with direct hand motion prediction remaining largely unexplored - despite the additional challenges posed by compact skeleton size. To address this, we propose a prompt-based Future Driven Diffusion Model (PromptFDDM) for predicting hand motion with guidance and prompts. Specifically, we develop a Spatial-Temporal Extractor Network (STEN) to predict hand motion with guidance, a Ground Truth Extractor Network (GTEN), and a Reference Data Generator Network (RDGN), which extract ground truth and substitute future data with generated reference data, respectively, to guide STEN. Additionally, interactive prompts generated from observed motions further enhance model performance. Experimental results on the FPHA and HO3D datasets demonstrate that the proposed PromptFDDM achieves state-of-the-art performance in both first- and third-person perspectives",
    "checked": true,
    "id": "4d21fd54779700481721d952882eef8c96b4bae0",
    "semantic_title": "prompting future driven diffusion model for hand motion prediction",
    "citation_count": 0,
    "authors": [
      "Bowen Tang*",
      "Kaihao Zhang*",
      "Wenhan Luo*",
      "Wei Liu",
      "HONGDONG LI"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1105_ECCV_2024_paper.php": {
    "title": "Defect Spectrum: A Granular Look of Large-scale Defect Datasets with Rich Semantics",
    "volume": "main",
    "abstract": "Defect inspection is paramount within the closed-loop manufacturing system. However, existing datasets for defect inspection often lack the precision and semantic granularity required for practical applications. In this paper, we introduce the Defect Spectrum, a comprehensive benchmark that offers precise, semantic-abundant, and large-scale annotations for a wide range of industrial defects. Building on four key industrial benchmarks, our dataset refines existing annotations and introduces rich semantic details, distinguishing multiple defect types within a single image. With our dataset, we were able to achieve an increase of 10.74% in the Recall rate, and a decrease of 33.10% in the False Positive Rate (FPR) from the industrial simulation experiment. Furthermore, we introduce Defect-Gen, a two-stage diffusion-based generator designed to create high-quality and diverse defective images, even when working with limited defective data. The synthetic images generated by Defect-Gen significantly enhance the performance of defect segmentation models, achieving an improvement in mIoU scores up to 9.85 on Defect-Spectrum subsets. Overall, The Defect Spectrum dataset demonstrates its potential in defect inspection research, offering a solid platform for testing and refining advanced models. Our project page is in https://envision-research.github.io/Defect_Spectrum/",
    "checked": true,
    "id": "24d9542bffc4ce0f9928757895f93f2b7ceeeebc",
    "semantic_title": "defect spectrum: a granular look of large-scale defect datasets with rich semantics",
    "citation_count": 4,
    "authors": [
      "Shuai Yang",
      "ZhiFei Chen",
      "Pengguang Chen",
      "Xi Fang",
      "Yixun Liang",
      "Shu Liu*",
      "Yingcong Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1108_ECCV_2024_paper.php": {
    "title": "Unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement",
    "volume": "main",
    "abstract": "Previous low-light image enhancement (LLIE) approaches, while employing frequency decomposition techniques to address the intertwined challenges of low frequency (e.g., illumination recovery) and high frequency (e.g., noise reduction), primarily focused on the development of dedicated and complex networks to achieve improved performance. In contrast, we reveal that an advanced disentanglement paradigm is sufficient to consistently enhance state-of-the-art methods with minimal computational overhead. Leveraging the image Laplace decomposition scheme, we propose a novel low-frequency consistency method, facilitating improved frequency disentanglement optimization. Our method, seamlessly integrating with various models such as CNNs, Transformers, and flow-based and diffusion models, demonstrates remarkable adaptability. Noteworthy improvements are showcased across five popular benchmarks, with up to 7.68dB gains on PSNR achieved for six state-of-the-art models. Impressively, our approach maintains efficiency with only 88K extra parameters, setting a new standard in the challenging realm of low-light image enhancement. https://github.com/redrock303/ ADF-LLIE",
    "checked": true,
    "id": "213b10f79180f96ee99b9a91f5e42bd773d827eb",
    "semantic_title": "unveiling advanced frequency disentanglement paradigm for low-light image enhancement",
    "citation_count": 0,
    "authors": [
      "Kun Zhou*",
      "Xinyu Lin",
      "Wenbo Li",
      "Xiaogang Xu",
      "Yuanhao Cai",
      "Zhonghang Liu",
      "Xiaoguang Han",
      "Jiangbo Lu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1129_ECCV_2024_paper.php": {
    "title": "RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation",
    "volume": "main",
    "abstract": "[-4] 3D point clouds play a pivotal role in outdoor scene perception, especially in the context of autonomous driving. Recent advancements in 3D LiDAR segmentation often focus intensely on the spatial positioning and distribution of points for accurate segmentation. However, these methods, while robust in variable conditions, encounter challenges due to sole reliance on coordinates and point intensity, leading to poor isometric invariance and suboptimal segmentation. To tackle this challenge, our work introduces Range-Aware Pointwise Distance Distribution () features and the associated architecture. Our features exhibit rigid transformation invariance and effectively adapt to variations in point density, with a design focus on capturing the localized geometry of neighboring structures. They utilize inherent LiDAR isotropic radiation and semantic categorization for enhanced local representation and computational efficiency, while incorporating a 4D distance metric that integrates geometric and surface material reflectivity for improved semantic segmentation. To effectively embed high-dimensional features, we propose a double-nested autoencoder structure with a novel class-aware embedding objective to encode high-dimensional features into manageable voxel-wise embeddings. Additionally, we propose which incorporates a channel-wise attention fusion and two effective -Seg variants, further optimizing the embedding for enhanced performance and generalization. Our method outperforms contemporary LiDAR segmentation work in terms of mIoU on SemanticKITTI (76.1) and nuScenes (83.6) datasets",
    "checked": true,
    "id": "b573e5cfb40c23cee6466c800ed2ec8f4ef66d46",
    "semantic_title": "rapid-seg: range-aware pointwise distance distribution networks for 3d lidar segmentation",
    "citation_count": 2,
    "authors": [
      "Li Li*",
      "Hubert P. H. Shum",
      "Toby P Breckon"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1133_ECCV_2024_paper.php": {
    "title": "UMBRAE: Unified Multimodal Brain Decoding",
    "volume": "main",
    "abstract": "We address prevailing challenges of the brain-powered research, departing from the observation that the literature hardly recover accurate spatial information and require subject-specific models. To address these challenges, we propose UMBRAE, a unified multimodal decoding of brain signals. First, to extract instance-level conceptual and spatial details from neural signals, we introduce an efficient universal brain encoder for multimodal-brain alignment and recover object descriptions at multiple levels of granularity from subsequent multimodal large language model (MLLM). Second, we introduce a cross-subject training strategy mapping subject-specific features to a common feature space. This allows a model to be trained on multiple subjects without extra resources, even yielding superior results compared to subject-specific models. Further, we demonstrate this supports weakly-supervised adaptation to new subjects, with only a fraction of the total training data. Experiments demonstrate that not only achieves superior results in the newly introduced tasks but also outperforms methods in well established tasks. To assess our method, we construct and share with the community a comprehensive brain understanding benchmark . Our code and benchmark are available at https://weihaox.github.io/UMBRAE",
    "checked": true,
    "id": "1698bf38d144749e46cc9c3604ddfa5fcd647c0e",
    "semantic_title": "umbrae: unified multimodal brain decoding",
    "citation_count": 0,
    "authors": [
      "Weihao Xia*",
      "Raoul de Charette",
      "A. Cengiz Oztireli",
      "Jing-Hao Xue"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1143_ECCV_2024_paper.php": {
    "title": "NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models",
    "volume": "main",
    "abstract": "Capitalizing on the remarkable advancements in Large Language Models (LLMs), there is a burgeoning initiative to harness LLMs for instruction following robotic navigation. Such a trend underscores the potential of LLMs to generalize navigational reasoning and diverse language understanding. However, a significant discrepancy in agent performance is observed when integrating LLMs in the Vision-and-Language navigation (VLN) tasks compared to previous downstream specialist models. Furthermore, the inherent capacity of language to interpret and facilitate communication in agent interactions is often underutilized in these integrations. In this work, we strive to bridge the divide between VLN-specialized models and LLM-based navigation paradigms, while maintaining the interpretative prowess of LLMs in generating linguistic navigational reasoning. By aligning visual content in a frozen LLM, we encompass visual observation comprehension for LLMs and exploit a way to incorporate LLMs and navigation policy networks for effective action predictions and navigational reasoning. We demonstrate the data efficiency of the proposed methods and eliminate the gap between LM-based agents and state-of-the-art VLN specialists. The source code is available at https://github.com/GengzeZhou/NavGPT-2",
    "checked": true,
    "id": "c2d858527df96e269f072e3dee37ec7bae281c0f",
    "semantic_title": "navgpt-2: unleashing navigational reasoning capability for large vision-language models",
    "citation_count": 2,
    "authors": [
      "Gengze Zhou*",
      "Yicong Hong",
      "Zun Wang",
      "Xin Eric Wang",
      "Qi Wu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1145_ECCV_2024_paper.php": {
    "title": "3D Single-object Tracking in Point Clouds with High Temporal Variation",
    "volume": "main",
    "abstract": "The high temporal variation of the point clouds is the key challenge of 3D single-object tracking (3D SOT). Existing approaches rely on the assumption that the shape variation of the point clouds and the motion of the objects across neighboring frames are smooth, failing to cope with high temporal variation data. In this paper, we present a novel framework for 3D SOT in point clouds with high temporal variation, called HVTrack. HVTrack proposes three novel components to tackle the challenges in the high temporal variation scenario: 1) A Relative-Pose-Aware Memory module to handle temporal point cloud shape variations; 2) a Base-Expansion Feature Cross-Attention module to deal with similar object distractions in expanded search areas; 3) a Contextual Point Guided Self-Attention module for suppressing heavy background noise. We construct a dataset with high temporal variation (KITTI-HV) by setting different frame intervals for sampling in the KITTI dataset. On the KITTI-HV with 5 frame intervals, our HVTrack surpasses the state-of-the-art tracker CXTracker by 11.3%/15.7% in Success/Precision",
    "checked": true,
    "id": "11e74c0ed42851810e6db4e6d02e18e4a581c53d",
    "semantic_title": "3d single-object tracking in point clouds with high temporal variation",
    "citation_count": 0,
    "authors": [
      "Qiao Wu",
      "Kun Sun",
      "Pei An",
      "Mathieu Salzmann",
      "Yanning Zhang",
      "Jiaqi Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1149_ECCV_2024_paper.php": {
    "title": "Adaptive Multi-task Learning for Few-shot Object Detection",
    "volume": "main",
    "abstract": "The majority of few-shot object detection methods use a shared feature map for both classification and localization, despite the conflicting requirements of these two tasks. Localization needs scale and positional sensitive features, whereas classification requires features that are robust to scale and positional variations. Although few methods have recognized this challenge and attempted to address it, they may not provide a comprehensive resolution to the issue. To overcome the contradictory preferences between classification and localization in few-shot object detection, an adaptive multi-task learning method, featuring a novel precision-driven gradient balancer, is proposed. This balancer effectively mitigates the conflicts by dynamically adjusting the backward gradient ratios for both tasks. Furthermore, a knowledge distillation and classification refinement scheme based on CLIP is introduced, aiming to enhance individual tasks by leveraging the capabilities of large vision-language models. Experimental results of the proposed method consistently show improvements over strong few-shot detection baselines on benchmark datasets. https://github.com/RY-Paper/MTL-FSOD",
    "checked": true,
    "id": "532b967ed725e52eca11d84adcc5c658289e0233",
    "semantic_title": "adaptive multi-task learning for few-shot object detection",
    "citation_count": 0,
    "authors": [
      "Yan Ren*",
      "Yanling Li",
      "Adams Wai-Kin Kong"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1155_ECCV_2024_paper.php": {
    "title": "Event Trojan: Asynchronous Event-based Backdoor Attacks",
    "volume": "main",
    "abstract": "As asynchronous event data is more frequently engaged in various vision tasks, the risk of backdoor attacks becomes more evident. However, research into the potential risk associated with backdoor attacks in asynchronous event data has been scarce, leaving related tasks vulnerable to potential threats. This paper has uncovered the possibility of directly poisoning event data streams by proposing Event Trojan framework, including two kinds of triggers, , immutable and mutable triggers. Specifically, our two types of event triggers are based on a sequence of simulated event spikes, which can be easily incorporated into any event stream to initiate backdoor attacks. Additionally, for the mutable trigger, we design an adaptive learning mechanism to maximize its aggressiveness. To improve the stealthiness, we introduce a novel loss function that constrains the generated contents of mutable triggers, minimizing the difference between triggers and original events while maintaining effectiveness. Extensive experiments on public event datasets show the effectiveness of the proposed backdoor triggers. We hope that this paper can draw greater attention to the potential threats posed by backdoor attacks on event-based tasks",
    "checked": true,
    "id": "84a050644e4c9030fad4505d6d1aa534f0bd838c",
    "semantic_title": "event trojan: asynchronous event-based backdoor attacks",
    "citation_count": 2,
    "authors": [
      "Ruofei Wang*",
      "Qing Guo",
      "Haoliang Li",
      "Renjie Wan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1159_ECCV_2024_paper.php": {
    "title": "Stepwise Multi-grained Boundary Detector for Point-supervised Temporal Action Localization",
    "volume": "main",
    "abstract": "Point-supervised temporal action localization pursues high-accuracy action detection under low-cost data annotation. Despite recent advances, a significant challenge remains: sparse labeling of individual frames leads to semantic ambiguity in determining action boundaries due to the lack of continuity in the highly sparse point-supervision scheme. We propose a Stepwise Multi-grained Boundary Detector (SMBD), which is comprised of a Background Anchor Generator (BAG) and a Dual Boundary Detector (DBD) to provide fine-grained supervision. Specifically, for each epoch in the training process, BAG computes the optimal background snippet between each pair of adjacent action labels, which we term Background Anchor. Subsequently, DBD leverages the background anchor and the action labels to locate the action boundaries from the perspectives of detecting action changes and scene changes. Then, the corresponding labels can be assigned to each side of the boundaries, with the boundaries continuously updated throughout the training process. Consequently, the proposed SMBD could ensure that more snippets contribute to the training process. Extensive experiments on the THUMOS'14, GTEA and BEOID datasets demonstrate that the proposed method outperforms existing state-of-the-art methods",
    "checked": true,
    "id": "e7363c72acb1fb58309d96162771a573dd20622e",
    "semantic_title": "stepwise multi-grained boundary detector for point-supervised temporal action localization",
    "citation_count": 0,
    "authors": [
      "Mengnan Liu",
      "Le Wang*",
      "Sanping Zhou",
      "Kun Xia",
      "Qi Wu",
      "Qilin Zhang",
      "Gang Hua"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1161_ECCV_2024_paper.php": {
    "title": "Imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems",
    "volume": "main",
    "abstract": "Electromagnetic Inverse Scattering Problems (EISP) have gained wide applications in computational imaging. By solving EISP, the internal relative permittivity of the scatterer can be non-invasively determined based on the scattered electromagnetic fields. Despite previous efforts to address EISP, achieving better solutions to this problem has remained elusive, due to the challenges posed by inversion and discretization. This paper tackles those challenges in EISP via an implicit approach. By representing the scatterer's relative permittivity as a continuous implicit representation, our method is able to address the low-resolution problems arising from discretization. Further, optimizing this implicit representation within a forward framework allows us to conveniently circumvent the challenges posed by inverse estimation. Our approach outperforms existing methods on standard benchmark datasets. Project page: https://luo-ziyuan.github.io/Imaging-Interiors",
    "checked": true,
    "id": "eef8bb955ea7a4df54a47d4e5b2c7727846b7e6f",
    "semantic_title": "imaging interiors: an implicit solution to electromagnetic inverse scattering problems",
    "citation_count": 2,
    "authors": [
      "Ziyuan Luo",
      "Boxin Shi",
      "Haoliang Li",
      "Renjie Wan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1163_ECCV_2024_paper.php": {
    "title": "Dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning",
    "volume": "main",
    "abstract": "Parameter-efficient fine-tuning methods adjust a small subset of parameters in large models, achieving performance comparable to or even surpassing that of models fine-tuned with the full parameter set, and significantly reducing the time and computational costs associated with the fine-tuning process. Despite the developments of parameter-efficient fine-tuning methods for large models, we observe significant performance disparities across different vision tasks. We attribute this pronounced performance variability to the insufficient robustness of current parameter-efficient fine-tuning methods. In this paper, we propose a robust reparameterization framework for parameter-efficient fine-tuning. This framework has a dynamic training structure and introduces no additional computational overhead during the inference stage. Specifically, we propose Dropout-Mixture Low-Rank Adaptation (DMLoRA), which incorporates multiple up and down branches, to provide the model with a more robust gradient descent path. As training proceeds, DMLoRA gradually drops out branches to achieve a balance between accuracy and regularization. Additionally, we employ a 2-Stage Learning Scalar (LS) strategy to optimize the scale factor for each layer's DMLoRA module. Experimental results demonstrate that our method achieves state-of-the-art performance on the benchmark VTAB-1k and FGVC datasets for parameter-efficient fine-tuning. Paramter-Efficient Fine-Tuning Dropout-Mixture Low-Rank Adaptation Gradual Branch Dropout 2-Stage Learning Scalar",
    "checked": true,
    "id": "7ba9047ac7c9e87d9291589db087dc8f4253d351",
    "semantic_title": "dropout mixture low-rank adaptation for visual parameters-efficient fine-tuning",
    "citation_count": 0,
    "authors": [
      "Zhengyi Fang",
      "Yue Wang",
      "Ran Yi*",
      "Lizhuang Ma"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1174_ECCV_2024_paper.php": {
    "title": "OneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers",
    "volume": "main",
    "abstract": "Existing end-to-end trackers for vision-based 3D perception suffer from performance degradation due to the conflict between detection and tracking tasks. In this work, we get to the bottom of this conflict, which was vaguely attributed to incompatible task-specific object features previously. We find the conflict between the two tasks lies in their partially conflicted classification gradients, which stems from their subtle difference in positive sample assignments. Based on this observation, we propose to coordinate those conflicted gradients from object queries with contradicted polarity in the two tasks. We also dynamically split all object queries into four groups based on their polarity in the two tasks. Attention between query sets with conflicted positive sample assignments is masked. The tracking classification loss is modified to suppress inaccurate predictions. To this end, we propose , the first one-stage joint detection and tracking model that bridges the gap between detection and tracking under a unified object feature representation. On the nuScenes camera-based object tracking benchmark, outperforms previous works by 6.9% AMOTA on the validation set and by 3.1% AMOTA on the test set",
    "checked": true,
    "id": "5a122fc35b8856202c93cddfdbb7576e945eecff",
    "semantic_title": "onetrack: demystifying the conflict between detection and tracking in end-to-end 3d trackers",
    "citation_count": 0,
    "authors": [
      "Qitai Wang",
      "Jiawei He",
      "Yuntao Chen",
      "Zhaoxiang Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1177_ECCV_2024_paper.php": {
    "title": "LoA-Trans: Enhancing Visual Grounding by Location-Aware Transformers",
    "volume": "main",
    "abstract": "Given an image and text description, visual grounding will find target region in the image explained by the text. It has two task settings: referring expression comprehension (REC) to estimate bounding-box and referring expression segmentation (RES) to predict segmentation mask. Currently the most promising visual grounding approaches are to learn REC and RES jointly by giving rich ground truth of both bounding-box and segmentation mask of the target object. However, we argue that a very simple but strong constraint has been overlooked by the existing approaches: given an image and a text description, REC and RES refer to the same object. We propose Location Aware Transformer (LoA-Trans) making this constraint explicit by a center prompt, where the system first predicts the center of the target object by Location-Aware Network, and feeds it as a common prompt to both REC and RES. In this way, the system constrains that REC and RES refer to the same object. To mitigate possible inaccuracies in center estimation, we introduce a query selection mechanism. Instead of random initialization queries for bounding-box and segmentation mask decoding, the query selection mechanism generates possible object locations other than the estimated center and use them as location-aware queries as a remedy for possible inaccurate center estimation. We also introduce a TaskSyn Network in the decoder to better coordination between REC and RES. Our method achieved state-of-the-art performance on three commonly used datasets: Refcoco, Refcoco+, and Refcocog. Extensive ablation studies demonstrated the validity of each of the proposed components",
    "checked": true,
    "id": "3e9675d4d98ddacd3c1e8d79eb9531bc27a02e25",
    "semantic_title": "loa-trans: enhancing visual grounding by location-aware transformers",
    "citation_count": 0,
    "authors": [
      "Ziling Huang*",
      "Shin'ichi Satoh"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1178_ECCV_2024_paper.php": {
    "title": "HAC: Hash-grid Assisted Context for 3D Gaussian Splatting Compression",
    "volume": "main",
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a promising framework for novel view synthesis, boasting rapid rendering speed with high fidelity. However, the substantial Gaussians and their associated attributes necessitate effective compression techniques. Nevertheless, the sparse and unorganized nature of the point cloud of Gaussians (or anchors in our paper) presents challenges for compression. To address this, we make use of the relations between the unorganized anchors and the structured hash grid, leveraging their mutual information for context modeling, and propose a Hash-grid Assisted Context (HAC) framework for highly compact 3DGS representation. Our approach introduces a binary hash grid to establish continuous spatial consistencies, allowing us to unveil the inherent spatial relations of anchors through a carefully designed context model. To facilitate entropy coding, we utilize Gaussian distributions to accurately estimate the probability of each quantized attribute, where an adaptive quantization module is proposed to enable high-precision quantization of these attributes for improved fidelity restoration. Additionally, we incorporate an adaptive masking strategy to eliminate invalid Gaussians and anchors. Importantly, our work is the pioneer to explore context-based compression for 3DGS representation, resulting in a remarkable size reduction of over 75× compared to vanilla 3DGS, while simultaneously improving fidelity, and achieving over 11× size reduction over SoTA 3DGS compression approach Scaffold-GS. Our code is available redhere",
    "checked": true,
    "id": "1be2584d082822e2ba9d6d8c47313a4ad3e653a5",
    "semantic_title": "hac: hash-grid assisted context for 3d gaussian splatting compression",
    "citation_count": 22,
    "authors": [
      "Yihang Chen*",
      "Qianyi Wu",
      "Weiyao Lin*",
      "Mehrtash Harandi",
      "Jianfei Cai"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1183_ECCV_2024_paper.php": {
    "title": "Energy-induced Explicit quantification for Multi-modality MRI fusion",
    "volume": "main",
    "abstract": "Multi-modality magnetic resonance imaging (MRI) is crucial for accurate disease diagnosis and surgical planning by comprehensively analyzing multi-modality information fusion. This fusion is characterized by unique patterns of information aggregation for each disease across modalities, influenced by distinct inter-dependencies and shifts in information flow. Existing fusion methods implicitly identify distinct aggregation patterns for various tasks, indicating the potential for developing a unified and explicit aggregation pattern. In this study, we propose a novel aggregation pattern, Energy-induced Explicit Propagation and Alignment (E2 PA), to explicitly quantify and optimize the properties of multi-modality MRI fusion to adapt to different scenarios. In E2 PA, (1) An energy-guided hierarchical fusion (EHF) uncovers the quantification and optimization of inter-dependencies propagation among multi-modalities by hierarchical same energy among patients. (2) An energy-regularized space alignment (ESA) measures the consistency of information flow in multi-modality aggregation by the alignment on space factorization and energy minimization. Through the extensive experiments on three public multi-modality MRI datasets (with different modality combinations and tasks), the superiority of E2 PA can be demonstrated from the comparison with state-of-the-art methods. Our code is available at https://github.com/JerryQseu/EEPA",
    "checked": true,
    "id": "bffe789c088d43ba14fc34817a62cb6782ef2fa2",
    "semantic_title": "energy-induced explicit quantification for multi-modality mri fusion",
    "citation_count": 0,
    "authors": [
      "Xiaoming Qi*",
      "Yuan Zhang",
      "Tong Wang",
      "Guanyu Yang*",
      "Yueming Jin*",
      "Shuo Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1191_ECCV_2024_paper.php": {
    "title": "ColorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement",
    "volume": "main",
    "abstract": "Text-to-Image (T2I) generation has made significant advancements with the advent of diffusion models. These models exhibit remarkable abilities to produce images based on textual prompts. Current T2I models allow users to specify object colors using linguistic color names. However, these labels encompass broad color ranges, making it difficult to achieve precise color matching. To tackle this challenging task, named color prompt learning, we propose to learn specific color prompts tailored to user-selected colors. Existing T2I personalization methods tend to result in color-shape entanglement. To overcome this, we generate several basic geometric objects in the target color, allowing for color and shape disentanglement during the color prompt learning. Our method, denoted as , successfully assists the T2I models to peel off the novel color prompts from these colored shapes. In the experiments, we demonstrate the efficacy of in achieving precise color generation with T2I models. Furthermore, we generalize to effectively learn abstract attribute concepts, including textures, materials, etc. Our findings represent a significant step towards improving precision and versatility of T2I models, offering new opportunities for creative applications and design tasks. Our project is available at https://moatifbutt.github.io/colorpeel/",
    "checked": true,
    "id": "70ee19452224c1a97fc06cd672ba6d5bb9eb6f4e",
    "semantic_title": "colorpeel: color prompt learning with diffusion models via color and shape disentanglement",
    "citation_count": 1,
    "authors": [
      "Muhammad Atif Butt*",
      "Kai Wang",
      "Javier Vazquez-Corral",
      "Joost van de Weijer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1192_ECCV_2024_paper.php": {
    "title": "Exemplar-free Continual Representation Learning via Learnable Drift Compensation",
    "volume": "main",
    "abstract": "Exemplar-free class-incremental learning using a backbone trained from scratch and starting from a small first task presents a significant challenge for continual representation learning. Prototype-based approaches, when continually updated, face the critical issue of semantic drift due to which the old class prototypes drift to different positions in the new feature space. Through an analysis of prototype-based continual learning, we show that forgetting is not due to diminished discriminative power of the feature extractor, and can potentially be corrected by drift compensation. To address this, we propose Learnable Drift Compensation (LDC), which can effectively mitigate drift in any moving backbone, whether supervised or unsupervised. LDC is fast and straightforward to integrate on top of existing continual learning approaches. Furthermore, we showcase how LDC can be applied in combination with self-supervised CL methods, resulting in the first exemplar-free semi-supervised continual learning approach. We achieve state-of-the-art performance in both supervised and semi-supervised settings across multiple datasets. Code is available at https: //github.com/alviur/ldc",
    "checked": true,
    "id": "3ff5d43ca2e7f74bf41b24eff510e80f60a42a0d",
    "semantic_title": "exemplar-free continual representation learning via learnable drift compensation",
    "citation_count": 0,
    "authors": [
      "Alex Gomez-Villa*",
      "Dipam Goswami",
      "Kai Wang",
      "Andy Bagdanov",
      "Bartlomiej Twardowski",
      "Joost van de Weijer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1205_ECCV_2024_paper.php": {
    "title": "Walker: Self-supervised Multiple Object Tracking by Walking on Temporal Object Appearance Graphs",
    "volume": "main",
    "abstract": "The supervision of state-of-the-art multiple object tracking (MOT) methods requires enormous annotation efforts to provide bounding boxes for all frames of all videos, and instance IDs to associate them through time. To this end, we introduce Walker, the first self-supervised tracker that learns from videos with sparse bounding box annotations, and no tracking labels. First, we design a quasi-dense temporal object appearance graph, and propose a novel multi-positive contrastive objective to optimize random walks on the graph and learn instance similarities. Then, we introduce an algorithm to enforce mutually-exclusive connective properties across instances in the graph, optimizing the learned topology for MOT. At inference time, we propose to associate detected instances to tracklets based on the max-likelihood transition state under motion-constrained bi-directional walks. Walker is the first self-supervised tracker to achieve competitive performance on MOT17, DanceTrack, and BDD100K. Remarkably, our proposal outperforms the previous self-supervised trackers even when drastically reducing the annotation requirements by up to 400x",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mattia Segù*",
      "Luigi Piccinelli",
      "Siyuan Li",
      "Luc Van Gool",
      "Fisher Yu",
      "Bernt Schiele"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1223_ECCV_2024_paper.php": {
    "title": "Spatio-Temporal Proximity-Aware Dual-Path Model for Panoramic Activity Recognition",
    "volume": "main",
    "abstract": "Panoramic Activity Recognition (PAR) seeks to identify diverse human activities across different scales, from individual actions to social group and global activities in crowded panoramic scenes. PAR presents two major challenges: 1) recognizing the nuanced interactions among numerous individuals and 2) understanding multi-granular human activities. To address these, we propose Social Proximity-aware Dual-Path Network (SPDP-Net) based on two key design principles. First, while previous works often focus on spatial distance among individuals within an image, we argue to consider the spatio-temporal proximity. It is crucial for individual relation encoding to correctly understand social dynamics. Secondly, deviating from existing hierarchical approaches (individual-to-social-to-global activity), we introduce a dual-path architecture for multi-granular activity recognition. This architecture comprises individual-to-global and individual-to-social paths, mutually reinforcing each other's task with global-local context through multiple layers. Through extensive experiments, we validate the effectiveness of the spatio-temporal proximity among individuals and the dual-path architecture in PAR. Furthermore, SPDP-Net achieves new state-of-the-art performance with 46.5% of overall F1 score on JRDB-PAR dataset",
    "checked": true,
    "id": "cbf4604e52e063b8ec06c21b01f863e39a535820",
    "semantic_title": "spatio-temporal proximity-aware dual-path model for panoramic activity recognition",
    "citation_count": 0,
    "authors": [
      "Sumin Lee*",
      "Yooseung Wang",
      "Sangmin Woo",
      "Changick Kim"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1231_ECCV_2024_paper.php": {
    "title": "DiffiT: Diffusion Vision Transformers for Image Generation",
    "volume": "main",
    "abstract": "Diffusion models with their powerful expressivity and high sample quality have achieved State-Of-The-Art (SOTA) performance in the generative domain. The pioneering Vision Transformer (ViT) has also demonstrated strong modeling capabilities and scalability, especially for recognition tasks. In this paper, we study the effectiveness of ViTs in diffusion-based generative learning and propose a new model denoted as Diffusion Vision Transformers (DiffiT). Specifically, we propose a methodology for finegrained control of the denoising process and introduce the Time-dependant Multihead Self Attention (TMSA) mechanism. DiffiT is surprisingly effective in generating high-fidelity images with significantly better parameter efficiency. We also propose latent and image space DiffiT models and show SOTA performance on a variety of class-conditional and unconditional synthesis tasks at different resolutions. The Latent DiffiT model achieves a new SOTA FID score of 1.73 on ImageNet-256 dataset while having 19.85%, 16.88% less parameters than other Transformer-based diffusion models such as MDT and DiT, respectively",
    "checked": true,
    "id": "6ce3a7c0c0035a39420d27c2ce01f48f5fab79c3",
    "semantic_title": "diffit: diffusion vision transformers for image generation",
    "citation_count": 34,
    "authors": [
      "Ali Hatamizadeh*",
      "Jiaming Song",
      "Guilin Liu",
      "Jan Kautz",
      "Arash Vahdat"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1237_ECCV_2024_paper.php": {
    "title": "WebRPG: Automatic Web Rendering Parameters Generation for Visual Presentation",
    "volume": "main",
    "abstract": "In the era of content creation revolution propelled by advancements in generative models, the field of web design remains unexplored despite its critical role in modern digital communication. The web design process is complex and often time-consuming, especially for those with limited expertise. In this paper, we introduce Web Rendering Parameters Generation (WebRPG), a new task that aims at automating the generation for visual presentation of web pages based on their HTML code. WebRPG would contribute to a faster web development workflow. Since there is no existing benchmark available, we develop a new dataset for WebRPG through an automated pipeline. Moreover, we present baseline models, utilizing VAE to manage numerous elements and rendering parameters, along with custom HTML embedding for capturing essential semantic and hierarchical information from HTML. Extensive experiments, including customized quantitative evaluations for this specific task, are conducted to evaluate the quality of the generated results. The dataset and code can be accessed at GitHub1 . 1 https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/ DocumentUnderstanding/WebRPG",
    "checked": true,
    "id": "a6a52671751e9216c32bcb08e469fb222fb8bb6f",
    "semantic_title": "webrpg: automatic web rendering parameters generation for visual presentation",
    "citation_count": 1,
    "authors": [
      "Zirui Shao",
      "Feiyu Gao",
      "Hangdi Xing",
      "Zepeng Zhu",
      "Zhi Yu*",
      "Jiajun Bu",
      "Qi Zheng",
      "Cong Yao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1247_ECCV_2024_paper.php": {
    "title": "GPSFormer: A Global Perception and Local Structure Fitting-based Transformer for Point Cloud Understanding",
    "volume": "main",
    "abstract": "Despite the significant advancements in pre-training methods for point cloud understanding, directly capturing intricate shape information from irregular point clouds without reliance on external data remains a formidable challenge. To address this problem, we propose GPSFormer, an innovative Global Perception and Local Structure Fitting-based Transformer, which learns detailed shape information from point clouds with remarkable precision. The core of GPSFormer is the Global Perception Module (GPM) and the Local Structure Fitting Convolution (LSFConv). Specifically, GPM utilizes Adaptive Deformable Graph Convolution (ADGConv) to identify short-range dependencies among similar features in the feature space and employs Multi-Head Attention (MHA) to learn long-range dependencies across all positions within the feature space, ultimately enabling flexible learning of contextual representations. Inspired by Taylor series, we design LSFConv, which learns both low-order fundamental and high-order refinement information from explicitly encoded local geometric structures. Integrating the GPM and LSFConv as fundamental components, we construct GPSFormer, a cutting-edge Transformer that effectively captures global and local structures of point clouds. Extensive experiments validate GPSFormer's effectiveness in three point cloud tasks: shape classification, part segmentation, and few-shot learning. The code of GPSFormer is available at https://github.com/changshuowang/ GPSFormer",
    "checked": true,
    "id": "978d8bd9f8ffe4ecda863bd50234b4181145e351",
    "semantic_title": "gpsformer: a global perception and local structure fitting-based transformer for point cloud understanding",
    "citation_count": 3,
    "authors": [
      "Changshuo Wang*",
      "Meiqing Wu",
      "Siew-Kei Lam",
      "Xin Ning",
      "Shangshu Yu",
      "Ruiping Wang",
      "Weijun Li",
      "Thambipillai Srikanthan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1249_ECCV_2024_paper.php": {
    "title": "FreeMotion: A Unified Framework for Number-free Text-to-Motion Synthesis",
    "volume": "main",
    "abstract": "Text-to-motion synthesis is a crucial task in computer vision. Existing methods are limited in their universality, as they are tailored for single-person or two-person scenarios and can not be applied to generate motions for more individuals. To achieve the number-free motion synthesis, this paper reconsiders motion generation and proposes to unify the single and multi-person motion by the conditional motion distribution. Furthermore, a generation module and an interaction module are designed for our FreeMotion framework to decouple the process of conditional motion generation and finally support the number-free motion synthesis. Besides, based on our framework, the current single-person motion spatial control method could be seamlessly integrated, achieving precise control of multi-person motion. Extensive experiments demonstrate the superior performance of our method and our capability to infer single and multi-human motions simultaneously",
    "checked": true,
    "id": "f9dfeadca81f89700d8f1a2e7eb67ed1395f85a1",
    "semantic_title": "freemotion: a unified framework for number-free text-to-motion synthesis",
    "citation_count": 4,
    "authors": [
      "Ke Fan",
      "Junshu Tang",
      "Weijian Cao",
      "Ran Yi*",
      "Moran Li",
      "Jingyu Gong",
      "Jiangning Zhang",
      "Yabiao Wang",
      "Chengjie Wang",
      "Lizhuang Ma*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1250_ECCV_2024_paper.php": {
    "title": "FSD-BEV: Foreground Self-Distillation for Multi-view 3D Object Detection",
    "volume": "main",
    "abstract": "Although multi-view 3D object detection based on the Bird's-Eye-View (BEV) paradigm has garnered widespread attention as an economical and deployment-friendly perception solution for autonomous driving, there is still a performance gap compared to LiDAR-based methods. In recent years, several cross-modal distillation methods have been proposed to transfer beneficial information from teacher models to student models, with the aim of enhancing performance. However, these methods face challenges due to discrepancies in feature distribution originating from different data modalities and network structures, making knowledge transfer exceptionally challenging. In this paper, we propose a Foreground Self-Distillation (FSD) scheme that effectively avoids the issue of distribution discrepancies, maintaining remarkable distillation effects without the need for pre-trained teacher models or cumbersome distillation strategies. Additionally, we design two Point Cloud Intensification (PCI) strategies to compensate for the sparsity of point clouds by frame combination and pseudo point assignment. Finally, we develop a Multi-Scale Foreground Enhancement (MSFE) module to extract and fuse multi-scale foreground features by predicted elliptical Gaussian heatmap, further improving the model's performance. We integrate all the above innovations into a unified framework named FSD-BEV. Extensive experiments on the nuScenes dataset exhibit that FSD-BEV achieves state-of-the-art performance, highlighting its effectiveness. The code and models are available at: https: // github. com/ CocoBoom/ fsd-bev",
    "checked": true,
    "id": "640916e72e859278f18eed902ffa5fb706c7186a",
    "semantic_title": "fsd-bev: foreground self-distillation for multi-view 3d object detection",
    "citation_count": 0,
    "authors": [
      "Zheng Jiang",
      "Jinqing Zhang",
      "Yanan Zhang",
      "Qingjie Liu*",
      "Zhenghui HU*",
      "Baohui Wang",
      "Yunhong Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1255_ECCV_2024_paper.php": {
    "title": "SceneGraphLoc: Cross-Modal Coarse Visual Localization on 3D Scene Graphs",
    "volume": "main",
    "abstract": "We introduce the task of localizing an input image within a multi-modal reference map represented by a collection of 3D scene graphs. These scene graphs comprise multiple modalities, including object-level point clouds, images, attributes, and relationships between objects, offering a lightweight and efficient alternative to conventional methods that rely on extensive image databases. Given these modalities, the proposed method learns a fixed-sized embedding for each node (, representing object instances) in the scene graph, enabling effective matching with the objects visible in the input query image. This strategy significantly outperforms other cross-modal methods, even without incorporating images into the map representation. With images, achieves performance close to that of state-of-the-art techniques depending on large image databases, while requiring three orders-of-magnitude less storage and operating orders-of-magnitude faster. Code and models are available at https://scenegraphloc.github.io",
    "checked": true,
    "id": "4f8e0b5ca78f1e3925089ea5ba0623db65e9a51f",
    "semantic_title": "scenegraphloc: cross-modal coarse visual localization on 3d scene graphs",
    "citation_count": 0,
    "authors": [
      "Yang Miao",
      "Francis Engelmann",
      "Olga Vysotska",
      "Federico Tombari",
      "Marc Pollefeys",
      "Daniel Barath*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1267_ECCV_2024_paper.php": {
    "title": "ScanReason: Empowering 3D Visual Grounding with Reasoning Capabilities",
    "volume": "main",
    "abstract": "Although great progress has been made in 3D visual grounding, current models still rely on explicit textual descriptions for grounding and lack the ability to reason human intentions from implicit instructions. We propose a new task called and introduce a new benchmark ScanReason which provides over 10K question-answer-location pairs from five reasoning types that require the synerization of reasoning and grounding. We further design our approach, , composed of the visual-centric reasoning module empowered by Multi-modal Large Language Model (MLLM) and the 3D grounding module to obtain accurate object locations by looking back to the enhanced geometry and fine-grained details from the 3D scenes. A chain-of-grounding mechanism is proposed to further boost the performance with interleaved reasoning and grounding steps during inference. Extensive experiments on the proposed benchmark validate the effectiveness of our proposed approach",
    "checked": true,
    "id": "fd36c54ae4f81f1fca80c52125a14bee3a0b6711",
    "semantic_title": "scanreason: empowering 3d visual grounding with reasoning capabilities",
    "citation_count": 4,
    "authors": [
      "Chenming Zhu",
      "Tai Wang",
      "Wenwei Zhang",
      "Kai Chen",
      "Xihui Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1270_ECCV_2024_paper.php": {
    "title": "MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?",
    "volume": "main",
    "abstract": "The remarkable progress of Multi-modal Large Language Models (MLLMs) has gained unparalleled attention. However, their capabilities in visual math problem-solving remain insufficiently evaluated and understood. We investigate current benchmarks to incorporate excessive visual content within textual questions, which potentially assist MLLMs in deducing answers without truly interpreting the input diagrams. To this end, we introduce , an all-around visual math benchmark designed for an equitable and in-depth evaluation of MLLMs. We meticulously collect 2,612 high-quality, multi-subject math problems with diagrams from publicly available sources. Each problem is then transformed by human annotators into six distinct versions, each offering varying degrees of information content in multi-modality, contributing to 15K test samples in total. This approach allows to comprehensively assess whether and how much MLLMs can truly understand the visual diagrams for mathematical reasoning. In addition, we propose a Chain-of-Thought (CoT) evaluation strategy for a fine-grained assessment of the output answers. Rather than naively judging true or false, we employ GPT-4(V) to adaptively assess each step with error analysis to derive a total score, which can reveal the inner CoT reasoning quality by MLLMs. With , we unveil that, most existing MLLMs struggle to understand math diagrams, relying heavily on textual questions. Surprisingly, some of them even achieve 5%+ higher accuracy without the visual input. Besides, GPT-4V and MAVIS-7B achieve the best overall performance within closed-source and open-source models, respectively. We hope the benchmark may provide unique insights to guide the future development of MLLMs. Project page: https://mathverse-cuhk.github.io. ∗ Equal contribution ‡ Project lead † Corresponding author",
    "checked": true,
    "id": "6d017adda6b2b1ea627dde2f0e85401ebb9fe566",
    "semantic_title": "mathverse: does your multi-modal llm truly see the diagrams in visual math problems?",
    "citation_count": 63,
    "authors": [
      "Renrui Zhang",
      "Dongzhi Jiang",
      "Yichi Zhang",
      "Haokun Lin",
      "Ziyu Guo",
      "Pengshuo Qiu",
      "Aojun Zhou",
      "Pan Lu",
      "Kai-Wei Chang",
      "Peng Gao",
      "Hongsheng Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1280_ECCV_2024_paper.php": {
    "title": "See and Think: Embodied Agent in Virtual Environment",
    "volume": "main",
    "abstract": "Large language models (LLMs) have achieved impressive pro-gress on several open-world tasks. Recently, using LLMs to build embodied agents has been a hotspot. This paper proposes STEVE, a comprehensive and visionary embodied agent in the Minecraft virtual environment. STEVE comprises three key components: vision perception, language instruction, and code action. Vision perception involves interpreting visual information in the environment, which is then integrated into the LLMs component with agent state and task instruction. Language instruction is responsible for iterative reasoning and decomposing complex tasks into manageable guidelines. Code action generates executable skill actions based on retrieval in skill database, enabling the agent to interact effectively within the Minecraft environment. We also collect STEVE-21K dataset, which includes 600+ vision-environment pairs, 20K knowledge question-answering pairs, and 200+ skill-code pairs. We conduct continuous block search, knowledge question and answering, and tech tree mastery to evaluate the performance. Extensive experiments show that STEVE achieves at most 1.5× faster unlocking key tech trees and 2.5× quicker in block search tasks",
    "checked": true,
    "id": "a756b584f8f8b4307e52895ae2120bc339580ad8",
    "semantic_title": "see and think: embodied agent in virtual environment",
    "citation_count": 14,
    "authors": [
      "Zhonghan Zhao",
      "Xuan Wang",
      "Wenhao Chai",
      "Boyi Li",
      "Shengyu Hao",
      "Shidong Cao",
      "Tian Ye",
      "Gaoang Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1284_ECCV_2024_paper.php": {
    "title": "PISR: Polarimetric Neural Implicit Surface Reconstruction for Textureless and Specular Objects",
    "volume": "main",
    "abstract": "Neural implicit surface reconstruction has achieved remarkable progress recently. Despite resorting to complex radiance modeling, state-of-the-art methods still struggle with textureless and specular surfaces. Different from RGB images, polarization images can provide direct constraints on the azimuth angles of the surface normals. In this paper, we present PISR, a novel method that utilizes a geometrically accurate polarimetric loss to refine shape independently of appearance. In addition, PISR smooths surface normals in image space to eliminate severe shape distortions and leverages the hash-grid-based neural signed distance function to accelerate the reconstruction. Experimental results demonstrate that PISR achieves higher accuracy and robustness, with an L1 Chamfer distance of 0.5 mm and an F-score of 99.5% at 1 mm, while converging 4 ∼ 30× faster than previous polarimetric surface reconstruction methods. The source code is available at https://github.com/GCChen97/PISR",
    "checked": true,
    "id": "bc0e117f7b5f027e346083d4c47788749915c80f",
    "semantic_title": "pisr: polarimetric neural implicit surface reconstruction for textureless and specular objects",
    "citation_count": 0,
    "authors": [
      "Guangcheng Chen*",
      "Yicheng He",
      "Li He",
      "Hong Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1286_ECCV_2024_paper.php": {
    "title": "Bridging the Gap Between Human Motion and Action Semantics via Kinematics Phrases",
    "volume": "main",
    "abstract": "Motion understanding aims to establish a reliable mapping between motion and action semantics, while it is a challenging many-to-many problem. An abstract action semantic (i.e., walk forwards) could be conveyed by perceptually diverse motions (walking with arms up or swinging). In contrast, a motion could carry different semantics w.r.t. its context and intention. This makes an elegant mapping between them difficult. Previous attempts adopted direct-mapping paradigms with limited reliability. Also, current automatic metrics fail to provide reliable assessments of the consistency between motions and action semantics. We identify the source of these problems as the significant gap between the two modalities. To alleviate this gap, we propose Kinematic Phrases (KP) that take the objective kinematic facts of human motion with proper abstraction, interpretability, and generality. Based on KP, we can unify a motion knowledge base and build a motion understanding system. Meanwhile, KP can be automatically converted from motions to text descriptions with no subjective bias, inspiring Kinematic Prompt Generation (KPG) as a novel white-box motion generation benchmark. In extensive experiments, our approach shows superiority over other methods. Our project is available at https://foruck.github.io/KP/",
    "checked": false,
    "id": "6eb922eb6ce09d1a7f853a17000c8dec99243514",
    "semantic_title": "bridging the gap between human motion and action semantics via kinematic phrases",
    "citation_count": 3,
    "authors": [
      "Xinpeng Liu",
      "Yong-Lu Li*",
      "Ailing Zeng",
      "Zizheng Zhou",
      "Yang You",
      "Cewu Lu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1287_ECCV_2024_paper.php": {
    "title": "VisFocus: Prompt-Guided Vision Encoders for OCR-Free Dense Document Understanding",
    "volume": "main",
    "abstract": "In recent years, notable advancements have been made in the domain of visual document understanding, with the prevailing architecture comprising a cascade of vision and language models. The text component can either be extracted explicitly with the use of external OCR models in OCR-based approaches, or alternatively, the vision model can be endowed with reading capabilities in OCR-free approaches. Typically, the queries to the model are input exclusively to the language component, necessitating the visual features to encompass the entire document. In this paper, we present VisFocus, an OCR-free method designed to better exploit the vision encoder's capacity by coupling it directly with the language prompt. To do so, we replace the down-sampling layers with layers that receive the input prompt and allow highlighting relevant parts of the document, while disregarding others. We pair the architecture enhancements with a novel pre-training task, using language masking on a snippet of the document text fed to the visual encoder in place of the prompt, to empower the model with focusing capabilities. Consequently, VisFocus learns to allocate its attention to text patches pertinent to the provided prompt. Our experiments demonstrate that this prompt-guided visual encoding approach significantly improves performance, achieving state-of-the-art results on various benchmarks. *Work done during an internshipemployment at Amazon† Corresponding author: nivnay@amazon.com",
    "checked": true,
    "id": "1ab3c15e56a91c6a7b3c22c7004f1f79e7f4ed8c",
    "semantic_title": "visfocus: prompt-guided vision encoders for ocr-free dense document understanding",
    "citation_count": 1,
    "authors": [
      "Ofir Abramovich*",
      "Niv Nayman*",
      "Sharon Fogel",
      "Inbal Lavi",
      "Ron Litman",
      "Shahar Tsiper",
      "Royee Tichauer",
      "Srikar Appalaraju",
      "Shai Mazor",
      "R. Manmatha"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1288_ECCV_2024_paper.php": {
    "title": "Masked Angle-Aware Autoencoder for Remote Sensing Images",
    "volume": "main",
    "abstract": "To overcome the inherent domain gap between remote sensing (RS) images and natural images, some self-supervised representation learning methods have made promising progress. However, they have overlooked the diverse angles present in RS objects. This paper proposes the Masked Angle-Aware Autoencoder (MA3E) to perceive and learn angles during pre-training. We design a scaling center crop operation to create the rotated crop with random orientation on each original image, introducing the explicit angle variation. MA3E inputs this composite image while reconstruct the original image, aiming to effectively learn rotation-invariant representations by restoring the angle variation introduced on the rotated crop. To avoid biases caused by directly reconstructing the rotated crop, we propose an Optimal Transport (OT) loss that automatically assigns similar original image patches to each rotated crop patch for reconstruction. MA3E1 demonstrates more competitive performance than existing pre-training methods on seven different RS image datasets in three downstream tasks. 1 Our code will be released at: https://github.com/benesakitam/MA3E",
    "checked": true,
    "id": "1598b8e202e6a358ec13193d8a14850d88205dff",
    "semantic_title": "masked angle-aware autoencoder for remote sensing images",
    "citation_count": 1,
    "authors": [
      "Zhihao Li*",
      "Biao Hou",
      "Siteng Ma",
      "zitong wu",
      "Xianpeng Guo",
      "bo ren",
      "Licheng Jiao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1291_ECCV_2024_paper.php": {
    "title": "Infinite-ID: Identity-preserved Personalization via ID-semantics Decoupling Paradigm",
    "volume": "main",
    "abstract": "Drawing on recent advancements in diffusion models for text-to-image generation, identity-preserved personalization has made significant progress in accurately capturing specific identities with just a single reference image. However, existing methods primarily integrate reference images within the text embedding space, leading to a complex entanglement of image and text information, which poses challenges for preserving both identity fidelity and semantic consistency. To tackle this challenge, we propose Infinite-ID, an ID-semantics decoupling paradigm for identity-preserved personalization. Specifically, we introduce identity-enhanced training, incorporating an additional image cross-attention module to capture sufficient ID information while deactivating the original text cross-attention module of the diffusion model. This ensures that the image stream faithfully represents the identity provided by the reference image while mitigating interference from textual input. Additionally, we introduce a feature interaction mechanism that combines a mixed attention module with an AdaIN-mean operation to seamlessly merge the two streams. This mechanism not only enhances the fidelity of identity and semantic consistency but also enables convenient control over the styles of the generated images. Extensive experimental results on both raw photo generation and style image generation demonstrate the superior performance of our proposed method",
    "checked": true,
    "id": "7d86baec4e76b6ae20439380596453180865b03d",
    "semantic_title": "infinite-id: identity-preserved personalization via id-semantics decoupling paradigm",
    "citation_count": 7,
    "authors": [
      "Yi Wu",
      "Ziqiang Li",
      "Heliang Zheng",
      "Chaoyue Wang*",
      "Bin Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1296_ECCV_2024_paper.php": {
    "title": "MultiGen: Zero-shot Image Generation from Multi-modal Prompts",
    "volume": "main",
    "abstract": "The field of text-to-image generation has witnessed substantial advancements in the preceding years, allowing the generation of high-quality images based solely on text prompts. However, accurately describing objects through text alone is challenging, necessitating the integration of additional modalities like coordinates and images for more precise image generation. Existing methods often require fine-tuning or only support using single object as the constraint, leaving the zero-shot image generation from multi-object multi-modal prompts as an unresolved challenge. In this paper, we propose MultiGen, a novel method designed to address this problem. Given an image-text pair, we obtain object-level text, coordinates and images, and integrate the information into an \"augmented token\" for each object. The augmented tokens serve as additional conditions and are trained alongside text prompts in the diffusion model, enabling our model to handle multi-object multi-modal prompts. To manage the absence of modalities during inference, we leverage a coordinate model and a feature model to generate object-level coordinates and features based on text prompts. Consequently, our method can generate images from text prompts alone or from various combinations of multi-modal prompts. Through extensive qualitative and quantitative experiments, we demonstrate that our method not only outperforms existing methods but also enables a wide range of tasks",
    "checked": false,
    "id": "ef4b604fca0c62dcd0d5caf7ca24ad74e285632d",
    "semantic_title": "multiqg-ti: towards question generation from multi-modal sources",
    "citation_count": 5,
    "authors": [
      "Zhi-Fan Wu*",
      "Lianghua Huang",
      "Wei Wang",
      "Yanheng Wei",
      "Yu Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1308_ECCV_2024_paper.php": {
    "title": "GazeXplain: Learning to Predict Natural Language Explanations of Visual Scanpaths",
    "volume": "main",
    "abstract": "While exploring visual scenes, humans' scanpaths are driven by their underlying attention processes. Understanding visual scanpaths is essential for various applications. Traditional scanpath models predict the where and when of gaze shifts without providing explanations, creating a gap in understanding the rationale behind fixations. To bridge this gap, we introduce GazeXplain, a novel study of visual scanpath prediction and explanation. This involves annotating natural-language explanations for fixations across eye-tracking datasets and proposing a general model with an attention-language decoder that jointly predicts scanpaths and generates explanations. It integrates a unique semantic alignment mechanism to enhance the consistency between fixations and explanations, alongside a cross-dataset co-training approach for generalization. These novelties present a comprehensive and adaptable solution for explainable human visual scanpath prediction. Extensive experiments on diverse eye-tracking datasets demonstrate the effectiveness of GazeXplain in both scanpath prediction and explanation, offering valuable insights into human visual attention and cognitive processes",
    "checked": true,
    "id": "f50e886a981fdb7f2c7b0756dab37d83a6230580",
    "semantic_title": "gazexplain: learning to predict natural language explanations of visual scanpaths",
    "citation_count": 1,
    "authors": [
      "Xianyu Chen*",
      "Ming Jiang",
      "Qi Zhao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1309_ECCV_2024_paper.php": {
    "title": "Learning Chain of Counterfactual Thought for Bias-Robust Vision-Language Reasoning",
    "volume": "main",
    "abstract": "Despite the remarkable success of large vision-language models (LVLMs) on various tasks, their susceptibility to knowledge bias inherited from training data hinders their ability to generalize to new scenarios and limits their real-world applicability. To address this challenge, we propose the Counterfactual Bias-Robust Reasoning (CoBRa) dataset that tackles knowledge bias by offering a novel collection of VQA examples designed to evaluate and mitigate bias in LVLMs. These examples encourage counterfactual thinking by providing edited knowledge graphs and image contents, with detailed annotations of reasoning processes to facilitate a comprehensive understanding of the examples. Based on the dataset, we introduce a Chain of Counterfactual Thought (CoCT) method that learns the bias-robust reasoning processes and provides in-context examples demonstrating how existing reasoning generalizes to counterfactual scenarios. This enables LVLMs to explicitly reason step-by-step rather than relying on biased knowledge, leading to more generalizable solutions. Our extensive evaluation demonstrates that CoCT outperforms existing approaches on tasks requiring reasoning under knowledge bias. Our work is available at https://github. com/SuperJohnZhang/CoBRa",
    "checked": true,
    "id": "feac15fe9b4f84af64ee8545cfdb62259d042148",
    "semantic_title": "learning chain of counterfactual thought for bias-robust vision-language reasoning",
    "citation_count": 0,
    "authors": [
      "Yifeng Zhang",
      "Ming Jiang",
      "Qi Zhao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1317_ECCV_2024_paper.php": {
    "title": "SegGen: Supercharging Segmentation Models with Text2Mask and Mask2Img Synthesis",
    "volume": "main",
    "abstract": "We present , a new data generation approach that pushes the performance boundaries of state-of-the-art image segmentation models. One major bottleneck of previous data synthesis methods for segmentation is the design of \"segmentation labeler module\", which is used to synthesize segmentation masks for images [?]. The segmentation labeler modules, which are segmentation models by themselves, bound the performance of downstream segmentation models trained on the synthetic masks. These methods encounter a \"chicken or egg dilemma\" and thus fail to outperform existing segmentation models. To address this issue, we propose a novel method that reverses the traditional data generation process: we first (i) generate highly diverse segmentation masks that match real-world distribution from text prompts, and then (ii) synthesize realistic images conditioned on the segmentation masks. In this way, we avoid the need for any segmentation labeler module. integrates two data generation strategies, namely MaskSyn and ImgSyn, to largely improve data diversity in synthetic masks and images. Notably, the high quality of our synthetic data enables our method to outperform the previous data synthesis method [?] by +25.2 mIoU on ADE20K when trained with pure synthetic data. On the highly competitive ADE20K and COCO benchmarks, our data generation method markedly improves the performance of state-of-the-art segmentation models in semantic segmentation, panoptic segmentation, and instance segmentation. Moreover, experiments show that training with our synthetic data makes the segmentation models more robust towards unseen data domains, including real-world and AI-generated images",
    "checked": true,
    "id": "80cc3f4aad61277286b26520599db8a6aa58e8c2",
    "semantic_title": "seggen: supercharging segmentation models with text2mask and mask2img synthesis",
    "citation_count": 7,
    "authors": [
      "Hanrong Ye*",
      "Jason Kuen",
      "Qing Liu",
      "Zhe Lin",
      "Brian Price",
      "Dan Xu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1318_ECCV_2024_paper.php": {
    "title": "Sync from the Sea: Retrieving Alignable Videos from Large-Scale Datasets",
    "volume": "main",
    "abstract": "Temporal video alignment aims to synchronize the key events like object interactions or action phase transitions in two videos. Such methods could benefit various video editing, processing, and understanding tasks. However, existing approaches operate under the restrictive assumption that a suitable video pair for alignment is given, significantly limiting their broader applicability. To address this, we re-pose temporal alignment as a search problem and introduce the task of Alignable Video Retrieval (AVR). Given a query video, our approach can identify well-alignable videos from a large collection of clips and temporally synchronize them to the query. To achieve this, we make three key contributions: 1) we introduce DRAQ, a video alignability indicator to identify and re-rank the best alignable video from a set of candidates; 2) we propose an effective and generalizable frame-level video feature design to improve the alignment performance of several off-the-shelf feature representations, and 3) we propose a novel benchmark and evaluation protocol for AVR using cycle-consistency metrics. Our experiments on 3 datasets, including large-scale Kinetics700, demonstrate the effectiveness of our approach in identifying alignable video pairs from diverse datasets",
    "checked": true,
    "id": "421d6206c1d6d4eead488f24aa29c611f2e0a740",
    "semantic_title": "sync from the sea: retrieving alignable videos from large-scale datasets",
    "citation_count": 1,
    "authors": [
      "Ishan Rajendrakumar Dave*",
      "Fabian Caba",
      "Mubarak Shah",
      "Simon Jenni*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1319_ECCV_2024_paper.php": {
    "title": "FinePseudo: Improving Pseudo-Labelling through Temporal-Alignablity for Semi-Supervised Fine-Grained Action Recognition",
    "volume": "main",
    "abstract": "Real-life applications of action recognition often require a fine-grained understanding of subtle movements, e.g., in sports analytics, user interactions in AR/VR, and surgical videos. Although fine-grained actions are more costly to annotate, existing semi-supervised action recognition has mainly focused on coarse-grained action recognition. Since fine-grained actions are more challenging due to the absence of scene bias, classifying these actions requires an understanding of action-phases. Hence, existing coarse-grained semi-supervised methods do not work effectively. In this work, we for the first time thoroughly investigate semi-supervised fine-grained action recognition (FGAR). We observe that alignment distances like dynamic time warping (DTW) provide a suitable action-phase-aware measure for comparing fine-grained actions, a concept previously unexploited in FGAR. However, since regular DTW distance is pairwise and assumes strict alignment between pairs, it is not directly suitable for classifying fine-grained actions. To utilize such alignment distances in a limited-label setting, we propose an Alignability-Verification-based Metric learning technique to effectively discriminate between fine-grained action pairs. Our learnable alignability score provides a better phase-aware measure, which we use to refine the pseudo-labels of the primary video encoder. Our collaborative pseudo-labeling-based framework ‘FinePseudo' significantly outperforms prior methods on four fine-grained action recognition datasets: Diving48, FineGym99, FineGym288, and FineDiving, and shows improvement on existing coarse-grained datasets: Kinetics400 and Something-SomethingV2. We also demonstrate the robustness of our collaborative pseudo-labeling in handling novel unlabeled classes in open-world semi-supervised setups",
    "checked": true,
    "id": "49b18c86140bfafb3f99742146f98d0af17e129f",
    "semantic_title": "finepseudo: improving pseudo-labelling through temporal-alignablity for semi-supervised fine-grained action recognition",
    "citation_count": 1,
    "authors": [
      "Ishan Rajendrakumar Dave*",
      "Mamshad Nayeem Rizve*",
      "Mubarak Shah"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1329_ECCV_2024_paper.php": {
    "title": "Elegantly Written: Disentangling Writer and Character Styles for Enhancing Online Chinese Handwriting",
    "volume": "main",
    "abstract": "The electronic writing tools, while enhancing convenience, sacrifice the readability and efficiency of handwritten content. Balancing high efficiency with readable handwriting poses a challenging research task. In this paper, we propose a method sequence-based models to beautify user handwritten traces. Unlike most existing methods that treat Chinese handwriting as images and cannot reflect the human writing process, we capture individual writing characteristics from a small amount of user handwriting trajectories and beautify the user's traces by mimicking their writing style and process. We fully consider the style of radicals and components between the content and reference glyphs, assigning appropriate fine-grained styles to strokes in the content glyphs through a cross-attention mechanism module. Additionally, we find that many style features contribute minimally to the final stylized results. Therefore, we decompose the style features into the Cartesian product of single-dimensional variable sets, effectively removing redundant features with limited impact on the stylization effect while preserving key style information. Qualitative and quantitative experiments both demonstrate the superiority of our approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Liu",
      "Fatimah binti Khalid",
      "Lei Wang",
      "Youxi Zhang",
      "Cunrui Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1331_ECCV_2024_paper.php": {
    "title": "UniCode : Learning a Unified Codebook for Multimodal Large Language Models",
    "volume": "main",
    "abstract": "In this paper, we propose UniCode, a novel approach within the domain of multimodal large language models (MLLMs) that learns a unified codebook to efficiently tokenize visual, text, and potentially other types of signals. This innovation addresses a critical limitation in existing MLLMs: their reliance on a text-only codebook, which restricts MLLMs' ability to generate images and texts in a multimodal context. Towards this end, we propose a language-driven iterative training paradigm, coupled with an in-context pre-training task we term \"image decompression\", enabling our model to interpret compressed visual data and generate high-quality images. The unified codebook empowers our model to extend visual instruction tuning to non-linguistic generation tasks. Moreover, UniCode is adaptable to diverse stacked quantization approaches in order to compress visual signals into a more compact token representation. Despite using significantly fewer parameters and less data during training, UniCode demonstrates promising capabilities in visual reconstruction and generation. It also achieves performance comparable to leading MLLMs across a spectrum of VQA benchmarks",
    "checked": false,
    "id": "835424063bf3c4035f96374a60147d821d474c2a",
    "semantic_title": "unicode: learning a unified codebook for multimodal large language models",
    "citation_count": 5,
    "authors": [
      "Sipeng Zheng*",
      "Bohan Zhou",
      "Yicheng Feng",
      "Ye Wang",
      "Zongqing Lu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1334_ECCV_2024_paper.php": {
    "title": "When Do We Not Need Larger Vision Models?",
    "volume": "main",
    "abstract": "Scaling up the size of vision models has been the de facto standard to obtain more powerful visual representations. In this work, we discuss the point beyond which larger vision models are not necessary. First, we demonstrate the power of Scaling on Scales (), whereby a pre-trained and frozen smaller vision model (, ViT-B or ViT-L), run over multiple image scales, can outperform larger models (, ViT-H or ViT-G) on classification, segmentation, depth estimation, Multimodal LLM (MLLM) benchmarks, and robotic manipulation. Notably, achieves state-of-the-art performance in detailed understanding of MLLM on the V∗ benchmark, surpassing models such as GPT-4V. We examine the conditions under which is a preferred scaling approach compared to scaling on model size. While larger models have the advantage of better generalization on hard examples, we show that features of larger vision models can be well approximated by those of multi-scale smaller models. This suggests most, if not all, of the representations learned by current large pre-trained models can also be obtained from multi-scale smaller models. Our results show that a multi-scale smaller model has comparable learning capacity to a larger model, and pre-training smaller models with can match or even exceed the advantage of larger models. We release a Python package that can apply on any vision model with one line of code: https://github.com/bfshi/scaling_on_scales",
    "checked": true,
    "id": "dbdfe71fdf641bebac5a052a60de75342871e3df",
    "semantic_title": "when do we not need larger vision models?",
    "citation_count": 26,
    "authors": [
      "Baifeng Shi*",
      "Ziyang Wu",
      "Maolin Mao",
      "Xin Wang",
      "Trevor Darrell"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1340_ECCV_2024_paper.php": {
    "title": "GVGEN: Text-to-3D Generation with Volumetric Representation",
    "volume": "main",
    "abstract": "In recent years, 3D Gaussian splatting has emerged as a powerful technique for 3D reconstruction and generation, known for its fast and high-quality rendering capabilities. Nevertheless, these methods often come with limitations, either lacking the ability to produce diverse samples or requiring prolonged inference times. To address these shortcomings, this paper introduces a novel diffusion-based framework, GVGEN, designed to efficiently generate 3D Gaussian representations from text input. We propose two innovative techniques: (1) Structured Volumetric Representation. We first arrange disorganized 3D Gaussian points as a structured form GaussianVolume. This transformation allows the capture of intricate texture details within a volume composed of a fixed number of Gaussians. To better optimize the representation of these details, we propose a unique pruning and densifying method named the Candidate Pool Strategy, enhancing detail fidelity through selective optimization. (2) Coarse-to-fine Generation Pipeline. To simplify the generation of GaussianVolume and empower the model to generate instances with detailed 3D geometry, we propose a coarse-to-fine pipeline. It initially constructs a basic geometric structure, followed by the prediction of complete Gaussian attributes. Our framework, GVGEN, demonstrates superior performance in qualitative and quantitative assessments compared to existing 3D generation methods. Simultaneously, it maintains a fast generation speed (∼7 seconds), effectively striking a balance between quality and efficiency. Our project page is https://gvgen.github.io/. ∗ Equal Contribution. † Corresponding Authors",
    "checked": true,
    "id": "0ecb340140fb81b4a114a6cbc7fac00129ff3231",
    "semantic_title": "gvgen: text-to-3d generation with volumetric representation",
    "citation_count": 9,
    "authors": [
      "Xianglong He",
      "Junyi Chen",
      "Sida Peng",
      "Di Huang",
      "Yangguang Li",
      "Xiaoshui Huang",
      "Chun Yuan*",
      "Wanli Ouyang",
      "Tong He*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1341_ECCV_2024_paper.php": {
    "title": "Bidirectional Stereo Image Compression with Cross-Dimensional Entropy Model",
    "volume": "main",
    "abstract": "With the rapid advancement of stereo vision technologies, stereo image compression has emerged as a crucial field that continues to draw significant attention. Previous approaches have primarily employed a unidirectional paradigm, where the compression of one view is dependent on the other, resulting in imbalanced compression. To address this issue, we introduce a symmetric bidirectional stereo image compression architecture, named BiSIC. Specifically, we propose a 3D convolution based codec backbone to capture local features and incorporate bidirectional attention blocks to exploit global features. Moreover, we design a novel cross-dimensional entropy model that integrates various conditioning factors, including the spatial context, channel context, and stereo dependency, to effectively estimate the distribution of latent representations for entropy coding. Extensive experiments demonstrate that our proposed BiSIC outperforms conventional image/video compression standards, as well as state-of-the-art learning-based methods, in terms of both PSNR and MS-SSIM",
    "checked": true,
    "id": "1fd9e5ff4e4471c061ebdb97d302fbf977b1d08e",
    "semantic_title": "bidirectional stereo image compression with cross-dimensional entropy model",
    "citation_count": 1,
    "authors": [
      "Zhening Liu",
      "Xinjie Zhang",
      "Jiawei Shao",
      "Zehong Lin*",
      "Jun Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1346_ECCV_2024_paper.php": {
    "title": "UniINR: Event-guided Unified Rolling Shutter Correction, Deblurring, and Interpolation",
    "volume": "main",
    "abstract": "Video frames captured by rolling shutter (RS) cameras during fast camera movement frequently exhibit RS distortion and blur simultaneously. Naturally, recovering high-frame-rate global shutter (GS) sharp frames from an RS blur frame must simultaneously consider RS correction, deblur, and frame interpolation. A naive way is to decompose the whole process into separate tasks and cascade existing methods; however, this results in cumulative errors and noticeable artifacts. Event cameras enjoy many advantages, , high temporal resolution, making them potential for our problem. To this end, we propose the first and novel approach, named UniINR, to recover arbitrary frame-rate sharp GS frames from an RS blur frame and paired events. Our key idea is unifying spatial-temporal implicit neural representation (INR) to directly map the position and time coordinates to color values to address the interlocking degradations. Specifically, we introduce spatial-temporal implicit encoding (STE) to convert an RS blur image and events into a spatial-temporal representation (STR). To query a specific sharp frame (GS or RS), we embed the exposure time into STR and decode the embedded features pixel-by-pixel to recover a sharp frame. Our method features a lightweight model with only 0.38M parameters, and it also enjoys high inference efficiency, achieving 2.83ms/f rame in 31× frame interpolation of an RS blur frame. Extensive experiments show that our method significantly outperforms prior methods. Code is available at https: //github.com/yunfanLu/UniINR",
    "checked": true,
    "id": "0655201b70109a862e87de06088fcbe9cf29f133",
    "semantic_title": "uniinr: event-guided unified rolling shutter correction, deblurring, and interpolation",
    "citation_count": 1,
    "authors": [
      "Yunfan Lu*",
      "Guoqiang Liang",
      "Yusheng Wang",
      "Lin Wang",
      "Hui Xiong*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1355_ECCV_2024_paper.php": {
    "title": "ReLoo: Reconstructing Humans Dressed in Loose Garments from Monocular Video in the Wild",
    "volume": "main",
    "abstract": "While previous years have seen great progress in the 3D reconstruction of humans from monocular videos, few of the state-of-the-art methods are able to handle loose garments that exhibit large non-rigid surface deformations during articulation. This limits the application of such methods to humans that are dressed in standard pants or T-shirts. Our method, , overcomes this limitation and reconstructs high-quality 3D models of humans dressed in loose garments from monocular in-the-wild videos. To tackle this problem, we first establish a layered neural human representation that decomposes clothed humans into a neural inner body and outer clothing. On top of the layered neural representation, we further introduce a non-hierarchical virtual bone deformation module for the clothing layer that can freely move, which allows the accurate recovery of non-rigidly deforming loose clothing. A global optimization jointly optimizes the shape, appearance, and deformations of the human body and clothing via multi-layer differentiable volume rendering. To evaluate , we record subjects with dynamically deforming garments in a multi-view capture studio. This evaluation, both on existing and our novel dataset, demonstrates 's clear superiority over prior art on both indoor datasets and in-the-wild videos",
    "checked": true,
    "id": "a28b49291045de0140b42c6e46d2d4fd2430231b",
    "semantic_title": "reloo: reconstructing humans dressed in loose garments from monocular video in the wild",
    "citation_count": 2,
    "authors": [
      "Chen Guo*",
      "Tianjian Jiang",
      "Manuel Kaufmann",
      "Chengwei Zheng",
      "Julien Valentin",
      "Jie Song*",
      "Otmar Hilliges"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1358_ECCV_2024_paper.php": {
    "title": "Weakly-supervised Camera Localization by Ground-to-satellite Image Registration",
    "volume": "main",
    "abstract": "The ground-to-satellite image matching/retrieval was initially proposed for city-scale ground camera localization. This work addresses the problem of improving camera pose accuracy by ground-to-satellite image matching after a coarse location and orientation have been obtained, either from the city-scale retrieval or from consumer-level GPS and compass sensors. Existing learning-based methods for solving this task require accurate GPS labels of ground images for network training. However, obtaining such accurate GPS labels is difficult, often requiring an expensive blackReal Time Kinematics (RTK) setup and suffering from signal occlusion, multi-path signal disruptions, . To alleviate this issue, this paper proposes a weakly supervised learning strategy for ground-to-satellite image registration when only noisy pose labels for ground images are available for network training. It derives positive and negative satellite images for each ground image and leverages contrastive learning to learn feature representations for ground and satellite images useful for translation estimation. We also propose a self-supervision strategy for cross-view image relative rotation estimation, which trains the network by creating pseudo query and reference image pairs. Experimental results show that our weakly supervised learning strategy achieves the best performance on cross-area evaluation compared to recent state-of-the-art methods that are reliant on accurate pose labels for supervision",
    "checked": true,
    "id": "ac56fdc7236db09715e020ce5872fcce2d8e2d47",
    "semantic_title": "weakly-supervised camera localization by ground-to-satellite image registration",
    "citation_count": 0,
    "authors": [
      "Yujiao Shi*",
      "HONGDONG LI",
      "Akhil Perincherry",
      "Ankit Vora"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1370_ECCV_2024_paper.php": {
    "title": "Dataset Growth",
    "volume": "main",
    "abstract": "Deep learning benefits from the growing abundance of available data. Meanwhile, efficiently dealing with the growing data scale has become a challenge. Data publicly available are from different sources with various qualities, and it is impractical to do manual cleaning against noise and redundancy given today's data scale. There are existing techniques for cleaning/selecting the collected data. However, these methods are mainly proposed for offline settings that target one of the cleanness and redundancy problems. In practice, data are growing exponentially with both problems. This leads to repeated data curation with sub-optimal efficiency. To tackle this challenge, we propose InfoGrowth, an efficient online algorithm for data cleaning and selection, resulting in a growing dataset that keeps up to date with awareness of cleanliness and diversity. InfoGrowth can improve data quality/efficiency on both single-modal and multi-modal tasks, with an efficient and scalable design. Its framework makes it practical for real-world data engines",
    "checked": true,
    "id": "1ccc1ec26f3b35f2e719e8ea1494d31ffc81a10b",
    "semantic_title": "dataset growth",
    "citation_count": 0,
    "authors": [
      "Ziheng Qin*",
      "zhaopan xu",
      "YuKun Zhou",
      "Kai Wang*",
      "Zangwei Zheng",
      "Zebang Cheng",
      "Hao Tang",
      "Lei Shang",
      "Baigui Sun",
      "Radu Timofte",
      "Xiaojiang Peng",
      "Hongxun Yao*",
      "Yang You*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1372_ECCV_2024_paper.php": {
    "title": "MaRINeR: Enhancing Novel Views by Matching Rendered Images with Nearby References",
    "volume": "main",
    "abstract": "Rendering realistic images from 3D reconstruction is an essential task of many Computer Vision and Robotics pipelines, notably for mixed-reality applications as well as training autonomous agents in simulated environments. However, the quality of novel views heavily depends of the source reconstruction which is often imperfect due to noisy or missing geometry and appearance. Inspired by the recent success of reference-based super-resolution networks, we propose MaRINeR, a refinement method that leverages information of a nearby mapping image to improve the rendering of a target viewpoint. We first establish matches between the raw rendered image of the scene geometry from the target viewpoint and the nearby reference based on deep features, followed by hierarchical detail transfer. We show improved renderings in quantitative metrics and qualitative examples from both explicit and implicit scene representations. We further employ our method on the downstream tasks of pseudo-ground-truth validation, synthetic data enhancement and detail recovery for renderings of reduced 3D reconstructions",
    "checked": true,
    "id": "91271535eccac967542a6c464fe901e7b844a498",
    "semantic_title": "mariner: enhancing novel views by matching rendered images with nearby references",
    "citation_count": 0,
    "authors": [
      "Lukas Bösiger*",
      "Mihai Dusmanu",
      "Marc Pollefeys",
      "Zuria Bauer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1377_ECCV_2024_paper.php": {
    "title": "Teaching Tailored to Talent: Adverse Weather Restoration via Prompt Pool and Depth-Anything Constraint",
    "volume": "main",
    "abstract": "Recent advancements in adverse weather restoration have shown potential, yet the unpredictable and varied combinations of weather degradations in the real world pose significant challenges. Previous methods typically struggle with dynamically handling intricate degradation combinations and carrying on background reconstruction precisely, leading to performance and generalization limitations. Drawing inspiration from prompt learning and the \"Teaching Tailored to Talent\" concept, we introduce a novel pipeline, T3 -DiffWeather. Specifically, we employ a prompt pool that allows the network to autonomously combine sub-prompts to construct weather-prompts, harnessing the necessary attributes to adaptively tackle unforeseen weather input. Moreover, from a scene modeling perspective, we incorporate general prompts constrained by Depth-Anything feature to provide the scene-specific condition for the diffusion process. Furthermore, by incorporating contrastive prompt loss, we ensures distinctive representations for both types of prompts by a mutual pushing strategy. Experimental results demonstrate that our method achieves state-of-the-art performance across various synthetic and real-world datasets, markedly outperforming existing diffusion techniques in terms of computational efficiency",
    "checked": true,
    "id": "4a596d19d8a513422629a551253c62121c4c2c2d",
    "semantic_title": "teaching tailored to talent: adverse weather restoration via prompt pool and depth-anything constraint",
    "citation_count": 0,
    "authors": [
      "Sixiang Chen",
      "Tian Ye",
      "Kai Zhang",
      "Zhaohu Xing",
      "Yunlong Lin",
      "Lei Zhu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1381_ECCV_2024_paper.php": {
    "title": "MoE-DiffIR: Task-customized Diffusion Priors for Universal Compressed Image Restoration",
    "volume": "main",
    "abstract": "We present MoE-DiffIR, an innovative universal compressed image restoration (CIR) method with task-customized diffusion priors. This intends to handle two pivotal challenges in the existing CIR methods: (i) lacking adaptability and universality for different image codecs, , JPEG and WebP; (ii) poor texture generation capability, particularly at low bitrates. Specifically, our MoE-DiffIR develops the powerful mixture-of-experts (MoE) prompt module, where some basic prompts cooperate to excavate the task-customized diffusion priors from Stable Diffusion (SD) for each compression task. Moreover, the degradation-aware routing mechanism is proposed to enable the flexible assignment of basic prompts. To activate and reuse the cross-modality generation prior of SD, we design the visual-to-text adapter for MoE-DiffIR, which aims to adapt the embedding of low-quality images from the visual domain to the textual domain as the textual guidance for SD, enabling more consistent and reasonable texture generation. We also construct one comprehensive benchmark dataset for universal CIR, covering 21 types of degradations from 7 popular traditional and learned codecs. Extensive experiments on universal CIR have demonstrated the excellent robustness and texture restoration capability of our proposed MoE-DiffIR. The project can be found atmagenta https://renyulin-f.github.io/MoE-DiffIR.github.io/",
    "checked": true,
    "id": "3e681b0b942dcddca457ca20b95dfa4314fe7ec9",
    "semantic_title": "moe-diffir: task-customized diffusion priors for universal compressed image restoration",
    "citation_count": 1,
    "authors": [
      "Yulin Ren",
      "Xin Li*",
      "Bingchen Li",
      "Xingrui Wang",
      "Mengxi China Guo",
      "Shijie Zhao",
      "Li Zhang",
      "Zhibo Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1383_ECCV_2024_paper.php": {
    "title": "LEGO: Learning EGOcentric Action Frame Generation via Visual Instruction Tuning",
    "volume": "main",
    "abstract": "Generating instructional images of human daily actions from an egocentric viewpoint serves as a key step towards efficient skill transfer. In this paper, we introduce a novel problem – egocentric action frame generation. The goal is to synthesize an image depicting an action in the user's context (, action frame) by conditioning on a user prompt and an input egocentric image. Notably, existing egocentric action datasets lack the detailed annotations that describe the execution of actions. Additionally, existing diffusion-based image manipulation models are sub-optimal in controlling the state transition of an action in egocentric image pixel space because of the domain gap. To this end, we propose to Learn EGOcentric (LEGO) action frame generation via visual instruction tuning. First, we introduce a prompt enhancement scheme to generate enriched action descriptions from a visual large language model (VLLM) by visual instruction tuning. Then we propose a novel method to leverage image and text embeddings from the VLLM as additional conditioning to improve the performance of a diffusion model. We validate our model on two egocentric datasets – Ego4D and Epic-Kitchens. Our experiments show substantial improvement over prior image manipulation models in both quantitative and qualitative evaluation. We also conduct detailed ablation studies and analysis to provide insights in our method. More details of the dataset and code are available on the website (https://bolinlai.github.io/Lego_EgoActGen/)",
    "checked": true,
    "id": "b92289123a94f6076505487adfb4513bd3495c1d",
    "semantic_title": "lego: learning egocentric action frame generation via visual instruction tuning",
    "citation_count": 4,
    "authors": [
      "Bolin Lai*",
      "Xiaoliang Dai",
      "Lawrence Chen",
      "Guan Pang",
      "James M Rehg",
      "Miao Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1393_ECCV_2024_paper.php": {
    "title": "SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant",
    "volume": "main",
    "abstract": "Recent advances in vision-language models have shown notable generalization in broad tasks through visual instruction tuning. However, bridging the gap between the pre-trained vision encoder and the large language models (LLMs) becomes the whole network's bottleneck. To improve cross-modality alignment, existing works usually consider more visual instruction data covering a broader range of vision tasks to fine-tune the model for question-answering, which, however, is costly to obtain and has not thoroughly explored the rich contextual information contained in images. This paper first attempts to harness the overlooked context within visual instruction data, training the model to self-supervised \"learning\" how to ask high-quality questions. In this way, we introduce a novel framework named SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant. SQ-LLaVA exhibits proficiency in generating flexible and meaningful image-related questions while analyzing the visual clue and prior language knowledge, signifying an advanced level of generalized visual understanding. Moreover, fine-tuning SQ-LLaVA on higher-quality instruction data shows a performance improvement compared with traditional visual-instruction tuning methods. This improvement highlights the efficacy of self-questioning techniques in achieving a deeper and more nuanced comprehension of visual content across various contexts. Our code is available at https://github.com/heliossun/SQ-LLaVA",
    "checked": true,
    "id": "0a7aad85e06dd46ce96bf0e0d20979678b5d4cd3",
    "semantic_title": "sq-llava: self-questioning for large vision-language assistant",
    "citation_count": 2,
    "authors": [
      "Guohao Sun*",
      "Can Qin",
      "JIAMINAN WANG",
      "Zeyuan Chen",
      "Ran Xu",
      "Zhiqiang Tao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1394_ECCV_2024_paper.php": {
    "title": "Mesh2NeRF: Direct Mesh Supervision for Neural Radiance Field Representation and Generation",
    "volume": "main",
    "abstract": "We present , an approach to derive ground-truth radiance fields from textured meshes for 3D generation tasks. Many 3D generative approaches represent 3D scenes as radiance fields for training. Their ground-truth radiance fields are usually fitted from multi-view renderings from a large-scale synthetic 3D dataset, which often results in artifacts due to occlusions or under-fitting issues. In , we propose an analytic solution to directly obtain ground-truth radiance fields from 3D meshes, characterizing the density field with an occupancy function featuring a defined surface thickness, and determining view-dependent color through a reflection function considering both the mesh and environment lighting. extracts accurate radiance fields which provides direct supervision for training generative NeRFs and single scene representation. We validate the effectiveness of Mesh2NeRF across various tasks, achieving a noteworthy 3.12dB improvement in PSNR for view synthesis in single scene representation on the ABO dataset, a 0.69 PSNR enhancement in the single-view conditional generation of ShapeNet Cars, and notably improved mesh extraction from NeRF in the unconditional generation of Objaverse Mugs",
    "checked": true,
    "id": "5ad600caeb6427025e5d6922d068b806357931c8",
    "semantic_title": "mesh2nerf: direct mesh supervision for neural radiance field representation and generation",
    "citation_count": 0,
    "authors": [
      "Yujin Chen*",
      "Yinyu Nie",
      "Benjamin Ummenhofer",
      "Reiner Birkl",
      "Michael Paulitsch",
      "Matthias Müller",
      "Matthias Niessner"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1396_ECCV_2024_paper.php": {
    "title": "Listen to Look into the Future: Audio-Visual Egocentric Gaze Anticipation",
    "volume": "main",
    "abstract": "Egocentric gaze anticipation serves as a key building block for the emerging capability of Augmented Reality. Notably, gaze behavior is driven by both visual cues and audio signals during daily activities. Motivated by this observation, we introduce the first model that leverages both the video and audio modalities for egocentric gaze anticipation. Specifically, we propose a Contrastive Spatial-Temporal Separable (CSTS) fusion approach that adopts two modules to separately capture audio-visual correlations in spatial and temporal dimensions, and applies a contrastive loss on the re-weighted audio-visual features from fusion modules for representation learning. We conduct extensive ablation studies and thorough analysis using two egocentric video datasets: Ego4D and Aria, to validate our model design. We demonstrate that audio improves the performance by +2.5% and +2.4% on the two datasets. Our model also outperforms the prior state-of-the-art methods by at least +1.9% and +1.6%. Moreover, we provide visualizations to show the gaze anticipation results and share additional insights into audio-visual representation learning. The code and data split are available on our website (https://bolinlai.github.io/CSTS-EgoGazeAnticipation/)",
    "checked": true,
    "id": "004c8f4f37ecbef5d3a347a7e6ba00ecaac733e0",
    "semantic_title": "listen to look into the future: audio-visual egocentric gaze anticipation",
    "citation_count": 6,
    "authors": [
      "Bolin Lai*",
      "Fiona Ryan",
      "Wenqi Jia",
      "Miao Liu",
      "James M Rehg"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1397_ECCV_2024_paper.php": {
    "title": "R^2-Bench: Benchmarking the Robustness of Referring Perception Models under Perturbations",
    "volume": "main",
    "abstract": "Referring perception, which aims at grounding visual objects with multimodal referring guidance, is essential for bridging the gap between humans, who provide instructions, and the environment where intelligent systems perceive. Despite progress in this field, the robustness of referring perception models (RPMs) against disruptive perturbations is not well explored. This work thoroughly assesses the resilience of RPMs against various perturbations in both general and specific contexts. Recognizing the complex nature of referring perception tasks, we present a comprehensive taxonomy of perturbations, and then develop a versatile toolbox for synthesizing and evaluating the effects of composite disturbances. Employing this toolbox, we construct R2 -Bench, a benchmark for assessing the Robustness of Referring perception models under noisy conditions across five key tasks. Moreover, we propose the R2 -Agent, an LLM-based agent that simplifies and automates model evaluation via natural language instructions. Our investigation uncovers the vulnerabilities of current RPMs to various perturbations and provides tools for assessing model robustness, potentially promoting the safe and resilient integration of intelligent systems into complex real-world scenarios",
    "checked": false,
    "id": "911fa15f466548eab6fba0f592bc6c9908a386ae",
    "semantic_title": "r2-bench: benchmarking the robustness of referring perception models under perturbations",
    "citation_count": 0,
    "authors": [
      "Xiang Li*",
      "Kai Qiu",
      "Jinglu Wang",
      "Xiaohao Xu",
      "Kashu Yamazaki",
      "Hao Chen",
      "Rita Singh",
      "Xiaonan Huang",
      "Bhiksha Raj"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1402_ECCV_2024_paper.php": {
    "title": "Self-supervised co-salient object detection via feature correspondences at multiple scales",
    "volume": "main",
    "abstract": "Our paper introduces a novel two-stage self-supervised approach for detecting co-occurring salient objects (CoSOD) in image groups without requiring segmentation annotations. Unlike existing unsupervised methods that rely solely on patch-level information (clustering patch descriptors) or on computation heavy off-the-shelf components for CoSOD, our lightweight model leverages feature correspondences at both patch and region levels, significantly improving prediction performance. In the first stage, we train a self-supervised network that detects co-salient regions by computing local patch-level feature correspondences across images. We obtain the segmentation predictions using confidence-based adaptive thresholding. In the next stage, we refine these intermediate segmentations by eliminating the detected regions (within each image) whose averaged feature representations are dissimilar to the foreground feature representation averaged across all the thresholded cross-attention maps (from the previous stage). Extensive experiments on three CoSOD benchmark datasets show that our self-supervised model outperforms the corresponding state-of-the-art models by a huge margin (, on the CoCA dataset, our model has a 13.7% F-measure gain over the SOTA unsupervised CoSOD model). Notably, our self-supervised model also outperforms several recent fully supervised CoSOD models on the three test datasets (, on the CoCA dataset, our model has a 4.6% F-measure gain over a recent supervised CoSOD model). Our code is available at: https://github.com/sourachakra/ SCoSPARC",
    "checked": false,
    "id": "e6012265697eacf941d56f22c4eb34232a351ffa",
    "semantic_title": "self-supervised co-salient object detection via feature correspondence at multiple scales",
    "citation_count": 0,
    "authors": [
      "Souradeep Chakraborty*",
      "Dimitris Samaras"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1403_ECCV_2024_paper.php": {
    "title": "Differentiable Convex Polyhedra Optimization from Multi-view Images",
    "volume": "main",
    "abstract": "This paper presents a novel approach for the differentiable rendering of convex polyhedra, addressing the limitations of recent methods that rely on implicit field supervision. Our technique introduces a strategy that combines non-differentiable computation of hyperplane intersection through duality transform with differentiable optimization for vertex positioning with three-plane intersection, enabling gradient-based optimization without the need for 3D implicit fields. This allows for efficient shape representation across a range of applications, from shape parsing to compact mesh reconstruction. This work not only overcomes the challenges of previous approaches but also sets a new standard for representing shapes with convex polyhedra",
    "checked": true,
    "id": "e408eca420d56159ea61fc05b82a8e8e03c96640",
    "semantic_title": "differentiable convex polyhedra optimization from multi-view images",
    "citation_count": 0,
    "authors": [
      "Daxuan Ren*",
      "Haiyi Mei",
      "Hezi Shi",
      "Jianmin Zheng",
      "Jianfei Cai",
      "Lei Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1406_ECCV_2024_paper.php": {
    "title": "SlotLifter: Slot-guided Feature Lifting for Learning Object-Centric Radiance Fields",
    "volume": "main",
    "abstract": "The ability to distill object-centric abstractions from intricate visual scenes underpins human-level generalization. Despite the significant progress in object-centric learning methods, learning object-centric representations in the 3D physical world remains a crucial challenge. In this work, we propose , a novel object-centric radiance model addressing scene reconstruction and decomposition jointly via slot-guided feature lifting. Such a design unites object-centric learning representations and image-based rendering methods, offering performance in scene decomposition and novel-view synthesis on four challenging synthetic and four complex real-world datasets, outperforming existing 3D object-centric learning methods by a large margin. Through extensive ablative studies, we showcase the efficacy of designs in , revealing key insights for potential future directions",
    "checked": true,
    "id": "7037816e0a4628d5dff9c316f18ad267215df535",
    "semantic_title": "slotlifter: slot-guided feature lifting for learning object-centric radiance fields",
    "citation_count": 0,
    "authors": [
      "Yu Liu",
      "Baoxiong Jia*",
      "Yixin Chen",
      "Siyuan Huang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1407_ECCV_2024_paper.php": {
    "title": "SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding",
    "volume": "main",
    "abstract": "3D vision-language (3dvl) grounding, which aims to align language with 3D physical environments, stands as a cornerstone in developing embodied agents. In comparison to recent advancements in the 2D domain, grounding language in 3D scenes faces two significant challenges: (i) the scarcity of paired 3dvl data to support grounded learning of 3D scenes, especially considering complexities within diverse object configurations, rich attributes, and intricate relationships; and (ii) the absence of a unified learning framework to distill knowledge from grounded 3D data. In this work, we aim to address these major challenges in 3D-VL by examining the potential of systematically upscaling 3D-VL learning in indoor scenes. We introduce the first million-scale 3D-VL dataset, , encompassing indoor scenes and comprising vision-language pairs collected from both human annotations and our scalable scene-graph-based generation approach. We demonstrate that this scaling allows for a unified pre-training framework, Grounded Pre-training for Scenes (), for 3D-VL learning. Through extensive experiments, we showcase the effectiveness of by achieving performance on existing 3D visual grounding and question-answering benchmarks. We also show that the data scaling effect is not limited to , but is generally beneficial for models on tasks like 3D semantic segmentation. The vast potential of and is unveiled through zero-shot transfer experiments in challenging 3dvl tasks",
    "checked": true,
    "id": "b01e8f939c685dbf11cb38d3af7604ef1e77306f",
    "semantic_title": "sceneverse: scaling 3d vision-language learning for grounded scene understanding",
    "citation_count": 21,
    "authors": [
      "Baoxiong Jia*",
      "Yixin Chen",
      "Huangyue Yu",
      "Yan Wang",
      "Xuesong Niu",
      "Tengyu Liu",
      "Qing Li",
      "Siyuan Huang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1418_ECCV_2024_paper.php": {
    "title": "ADMap: Anti-disturbance Framework for Vectorized HD Map Construction",
    "volume": "main",
    "abstract": "In the field of autonomous driving, online High-definition (HD) map construction is crucial for planning tasks. Recent studies have developed several high-performance HD map construction models to meet the demand. However, the point sequences generated by recent HD map construction models are jittery or jagged due to prediction bias and impact subsequent tasks. To mitigate this jitter issue, we propose the Anti-Disturbance Map construction framework (ADMap), which contains Multi-scale Perception Neck (MPN), Instance Interactive Attention (IIA), and Vector Direction Difference Loss (VDDL). By exploring the point sequence relations between and within instances in a cascading manner, our proposed ADMap effectively monitors the point sequence prediction process, and achieves state-of-the-art performance on the nuScenes and Argoverse2 datasets. Extensive results demonstrate its ability to produce stable and reliable map elements in complex and changing driving scenarios",
    "checked": true,
    "id": "570ddb3d25392939bffae6b7dae2cf7367cc9581",
    "semantic_title": "admap: anti-disturbance framework for vectorized hd map construction",
    "citation_count": 0,
    "authors": [
      "Haotian Hu",
      "Fanyi Wang*",
      "Yaonong Wang",
      "Laifeng Hu",
      "Jingwei Xu",
      "Zhiwang Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1421_ECCV_2024_paper.php": {
    "title": "GaussianImage: 1000 FPS Image Representation and Compression by 2D Gaussian Splatting",
    "volume": "main",
    "abstract": "Implicit neural representations (INRs) recently achieved great success in image representation and compression, offering high visual quality and fast rendering speeds with 10-1000 FPS, assuming sufficient GPU resources are available. However, this requirement often hinders their use on low-end devices with limited memory. In response, we propose a groundbreaking paradigm of image representation and compression by 2D Gaussian Splatting, named GaussianImage. We first introduce 2D Gaussian to represent the image, where each Gaussian has 8 parameters including position, covariance and color. Subsequently, we unveil a novel rendering algorithm based on accumulated summation. Remarkably, our method with a minimum of 3× lower GPU memory usage and 5× faster fitting time not only rivals INRs (e.g., WIRE, I-NGP) in representation performance, but also delivers a faster rendering speed of 1500-2000 FPS regardless of parameter size. Furthermore, we integrate existing vector quantization technique to build an image codec. Experimental results demonstrate that our codec attains rate-distortion performance comparable to compression-based INRs such as COIN and COIN++, while facilitating decoding speeds of approximately 2000 FPS. Additionally, preliminary proof of concept shows that our codec surpasses COIN and COIN++ in performance when using partial bits-back coding. Code is available at https://github.com/Xinjie-Q/GaussianImage",
    "checked": true,
    "id": "b955b789019a39a847779abbf35e42975be89ed6",
    "semantic_title": "gaussianimage: 1000 fps image representation and compression by 2d gaussian splatting",
    "citation_count": 5,
    "authors": [
      "Xinjie Zhang",
      "Xingtong Ge",
      "Tongda Xu",
      "Dailan He",
      "Yan Wang",
      "Hongwei Qin",
      "Guo Lu",
      "Jing Geng*",
      "Jun Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1431_ECCV_2024_paper.php": {
    "title": "PanoVOS: Bridging Non-panoramic and Panoramic Views with Transformer for Video Segmentation",
    "volume": "main",
    "abstract": "Panoramic videos contain richer spatial information and have attracted tremendous amounts of attention due to their exceptional experience in some fields such as autonomous driving and virtual reality. However, existing datasets for video segmentation only focus on conventional planar images. To address the challenge, in this paper, we present a panoramic video dataset, i.e., PanoVOS. The dataset provides 150 videos with high video resolutions and diverse motions. To quantify the domain gap between 2D planar videos and panoramic videos, we evaluate 15 off-the-shelf video object segmentation (VOS) models on PanoVOS. Through error analysis, we found that all of them fail to tackle pixel-level content discontinues of panoramic videos. Thus, we present a Panoramic Space Consistency Transformer (PSCFormer), which can effectively utilize the semantic boundary information of the previous frame for pixel-level matching with the current frame. Extensive experiments demonstrate that compared with the previous SOTA models, our PSCFormer network exhibits a great advantage in terms of segmentation results under the panoramic setting. Our dataset poses new challenges in panoramic VOS and we hope that our PanoVOS can advance the development of panoramic segmentation/tracking. The dataset, codes, and pre-train models will be published at https://github.com/shilinyan99/PanoVOS",
    "checked": true,
    "id": "1cf8bc57014f76d3636b86e13712e5332053a8a5",
    "semantic_title": "panovos: bridging non-panoramic and panoramic views with transformer for video segmentation",
    "citation_count": 3,
    "authors": [
      "Shilin Yan*",
      "Xiaohao Xu",
      "Renrui Zhang",
      "Lingyi Hong",
      "wenchao chen",
      "Wenqiang Zhang",
      "Wei Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1435_ECCV_2024_paper.php": {
    "title": "Evaluating Text-to-Visual Generation with Image-to-Text Generation",
    "volume": "main",
    "abstract": "Despite significant progress in generative AI, comprehensive evaluation remains challenging because of the lack of effective metrics and standardized benchmarks. For instance, the widely-used CLIPScore measures the alignment between a (generated) image and text prompt, but it fails to produce reliable scores for complex prompts involving compositions of objects, attributes, and relations. One reason is that text encoders of CLIP can notoriously act as a \"bag of words\", conflating prompts such as the horse is eating the grass with the grass is eating the horse [?, ?, ?]. To address this, we introduce the VQAScore, which uses a visual-question-answering (VQA) model to produce an alignment score by computing the probability of a Yes answer to a simple Does this figure show {text}? question. Though simpler than prior art, VQAScore computed with off-the-shelf models produces state-of-the-art results across many (8) image-text alignment benchmarks. We also compute VQAScore with an in-house model that follows best practices in the literature. For example, we use a bidirectional image-question encoder that allows image embeddings to depend on the question being asked (and vice versa). Our in-house model, CLIP-FlanT5, outperforms even the strongest baselines that make use of the proprietary GPT-4V. Interestingly, although we train with only images, VQAScore can also align text with video and 3D models. VQAScore allows researchers to benchmark text-to-visual generation using complex texts that capture the compositional structure of real-world prompts. Towards this end, we introduce GenAI-Bench, a more challenging benchmark with 1,600 compositional text prompts that require parsing scenes, objects, attributes, relationships, and high-order reasoning such as comparison and logic. GenAI-Bench also collects over 15,000 human ratings for leading image and video models such as Stable Diffusion, DALL-E 3, Midjourney, and Gen2. We open-source our data, model, and code at link",
    "checked": true,
    "id": "ec45c3f0f88c8ce1deb5baa71c2c0e14ad64d249",
    "semantic_title": "evaluating text-to-visual generation with image-to-text generation",
    "citation_count": 32,
    "authors": [
      "Zhiqiu Lin*",
      "Deepak Pathak",
      "Baiqi Li",
      "Jiayao Li",
      "Xide Xia",
      "Graham Neubig",
      "Pengchuan Zhang",
      "Deva Ramanan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1436_ECCV_2024_paper.php": {
    "title": "SENC: Handling Self-collision in Neural Cloth Simulation",
    "volume": "main",
    "abstract": "We present SENC, a novel self-supervised neural cloth simulator that addresses the challenge of cloth self-collision. This problem has remained unresolved due to the gap in simulation setup between recent collision detection and response approaches and self-supervised neural simulators. The former requires collision-free initial setups, while the latter necessitates random cloth instantiation during training. To tackle this issue, we propose a novel loss based on Global Intersection Analysis (GIA). This loss extracts the volume surrounded by the cloth region that forms the penetration. By constructing an energy based on this volume, our self-supervised neural simulator can effectively address cloth self-collisions. Moreover, we develop a self-collision-aware graph neural network capable of learning to handle self-collisions, even for parts that are topologically distant from one another. Additionally, we introduce an effective external force scheme that enables the simulation to learn the cloth's behavior in response to random external forces. We validate the efficacy of SENC through extensive quantitative and qualitative experiments, demonstrating that it effectively reduces cloth self-collision while maintaining high-quality animation results",
    "checked": true,
    "id": "6e27f41cc9a106e991352a5880567d4a46711156",
    "semantic_title": "senc: handling self-collision in neural cloth simulation",
    "citation_count": 2,
    "authors": [
      "Zhouyingcheng Liao*",
      "Sinan Wang",
      "Taku Komura"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1439_ECCV_2024_paper.php": {
    "title": "HybridBooth: Hybrid Prompt Inversion for Efficient Subject-Driven Generation",
    "volume": "main",
    "abstract": "Recent advancements in text-to-image diffusion models have shown remarkable creative capabilities with textual prompts, but generating personalized instances based on specific subjects, known as subject-driven generation, remains challenging. To tackle this issue, we present a new hybrid framework called , which merges the benefits of optimization-based and direct-regression methods. operates in two stages: the Word Embedding Probe, which generates a robust initial word embedding using a fine-tuned encoder, and the Word Embedding Refinement, which further adapts the encoder to specific subject images by optimizing key parameters. This approach allows for effective and fast inversion of visual concepts into textual embedding, even from a single image, while maintaining the model's generalization capabilities",
    "checked": true,
    "id": "80559dd7fd5e6444a4951b5c71862b6c35986bd9",
    "semantic_title": "hybridbooth: hybrid prompt inversion for efficient subject-driven generation",
    "citation_count": 0,
    "authors": [
      "Shanyan Guan",
      "Yanhao Ge",
      "Ying Tai*",
      "Jian Yang",
      "Wei Li",
      "Mingyu You*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1451_ECCV_2024_paper.php": {
    "title": "PartCraft: Crafting Creative Objects by Parts",
    "volume": "main",
    "abstract": "This paper propels creative control in generative visual AI by allowing users to \"select\". Departing from traditional text or sketch-based methods, we for the first time allow users to choose visual concepts by parts for their creative endeavors. The outcome is fine-grained generation that precisely captures selected visual concepts, ensuring a holistically faithful and plausible result. To achieve this, we first parse objects into parts through unsupervised feature clustering. Then, we encode parts into text tokens and introduce an entropy-based normalized attention loss that operates on them. This loss design enables our model to learn generic prior topology knowledge about object's part composition, and further generalize to novel part compositions to ensure the generation looks holistically faithful. Lastly, we employ a bottleneck encoder to project the part tokens. This not only enhances fidelity but also accelerates learning, by leveraging shared knowledge and facilitating information exchange among instances. Visual results in the paper and supplementary material showcase the compelling power of in crafting highly customized, innovative creations, exemplified by the \"charming\" and creative birds in Fig. ??. Code is released at https://github.com/kamwoh/partcraft",
    "checked": true,
    "id": "f42f1a767bc1265408f3cb41497bc410f886181f",
    "semantic_title": "partcraft: crafting creative objects by parts",
    "citation_count": 1,
    "authors": [
      "Kam Woh Ng*",
      "Xiatian Zhu",
      "Yi-Zhe Song",
      "Tao Xiang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1453_ECCV_2024_paper.php": {
    "title": "GeometrySticker: Enabling Ownership Claim of Recolorized Neural Radiance Fields",
    "volume": "main",
    "abstract": "Remarkable advancements in the recolorization of Neural Radiance Fields (NeRF) have simplified the process of modifying NeRF's color attributes. Yet, with the potential of NeRF to serve as shareable digital assets, there's a concern that malicious users might alter the color of NeRF models and falsely claim the recolorized version as their own. To safeguard against such breaches of ownership, enabling original NeRF creators to establish rights over recolorized NeRF is crucial. While approaches like CopyRNeRF have been introduced to embed binary messages into NeRF models as digital signatures for copyright protection, the process of recolorization can remove these binary messages. In our paper, we present GeometrySticker, a method for seamlessly integrating binary messages into the geometry components of radiance fields, akin to applying a sticker. GeometrySticker can embed binary messages into NeRF models while preserving the effectiveness of these messages against recolorization. Our comprehensive studies demonstrate that GeometrySticker is adaptable to prevalent NeRF architectures and maintains a commendable level of robustness against various distortions. Project page: https://kevinhuangxf.github.io/GeometrySticker",
    "checked": true,
    "id": "13d4e1c2f09828526d7e5daa31f7d381fac4ac6a",
    "semantic_title": "geometrysticker: enabling ownership claim of recolorized neural radiance fields",
    "citation_count": 2,
    "authors": [
      "Xiufeng HUANG*",
      "Ka Chun Cheung",
      "Simon See",
      "Renjie Wan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1465_ECCV_2024_paper.php": {
    "title": "PYRA: Parallel Yielding Re-Activation for Training-Inference Efficient Task Adaptation",
    "volume": "main",
    "abstract": "Recently, the scale of transformers has grown rapidly, which introduces considerable challenges in terms of training overhead and inference efficiency in the scope of task adaptation. Existing works, namely Parameter-Efficient Fine-Tuning (PEFT) and model compression, have separately investigated the challenges. However, PEFT cannot guarantee the inference efficiency of the original backbone, especially for large-scale models. Model compression requires significant training costs for structure searching and re-training. Consequently, a simple combination of them cannot guarantee accomplishing both training efficiency and inference efficiency with minimal costs. In this paper, we propose a novel Parallel Yielding Re-Activation (PYRA) method for such a challenge of training-inference efficient task adaptation. PYRA first utilizes parallel yielding adaptive weights to comprehensively perceive the data distribution in downstream tasks. A re-activation strategy for token modulation is then applied for tokens to be merged, leading to calibrated token features. Extensive experiments demonstrate that PYRA outperforms all competing methods under both low compression rate and high compression rate, demonstrating its effectiveness and superiority in maintaining both training efficiency and inference efficiency for large-scale foundation models. Our code is available at https://github.com/ THU-MIG/PYRA",
    "checked": true,
    "id": "438825debb5f7c129b0bbc7e3b5c95606545f5f0",
    "semantic_title": "pyra: parallel yielding re-activation for training-inference efficient task adaptation",
    "citation_count": 8,
    "authors": [
      "Yizhe Xiong",
      "Hui Chen*",
      "Tianxiang Hao",
      "Zijia Lin",
      "Jungong Han",
      "Yuesong Zhang",
      "Guoxin Wang",
      "Yongjun Bao",
      "Guiguang Ding"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1467_ECCV_2024_paper.php": {
    "title": "FineMatch: Aspect-based Fine-grained Image and Text Mismatch Detection and Correction",
    "volume": "main",
    "abstract": "Recent progress in large-scale pre-training has led to the development of advanced vision-language models (VLMs) with remarkable proficiency in comprehending and generating multimodal content. Despite the impressive ability to perform complex reasoning for VLMs, current models often struggle to effectively and precisely capture the compositional information on both the image and text sides. To address this, we propose , a new aspect-based fine-grained text and image matching benchmark, focusing on text and image mismatch detection and correction. This benchmark introduces a novel task for boosting and evaluating the VLMs' compositionality for aspect-based fine-grained text and image matching. In this task, models are required to identify mismatched aspect phrases within a caption, determine the aspect's class, and propose corrections for an image-text pair that may contain between 0 and 3 mismatches. To evaluate the models' performance on this new task, we propose a new evaluation metric named ITM-IoU for which our experiments show a high correlation to human evaluation. In addition, we also provide a comprehensive experimental analysis of existing mainstream VLMs, including fully supervised learning and in-context learning settings. We have found that models trained on demonstrate enhanced proficiency in detecting fine-grained text and image mismatches. Moreover, models (e.g., GPT-4V, Gemini Pro Vision) with strong abilities to perform multimodal in-context learning are not as skilled at fine-grained compositional image and text matching analysis. With , we are able to build a system for text-to-image generation hallucination detection and correction. Resources are available at https://hanghuacs.github.io/finematch/",
    "checked": true,
    "id": "55df271485a4b620f5a2b83af46e4ce49fd1a405",
    "semantic_title": "finematch: aspect-based fine-grained image and text mismatch detection and correction",
    "citation_count": 6,
    "authors": [
      "Hang Hua*",
      "Jing Shi",
      "Kushal Kafle",
      "Simon Jenni",
      "Daoan Zhang",
      "John Collomosse",
      "Scott Cohen",
      "Jiebo Luo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1470_ECCV_2024_paper.php": {
    "title": "CrossScore: A Multi-View Approach to Image Evaluation and Scoring",
    "volume": "main",
    "abstract": "We introduce a novel cross-reference image quality assessment method that effectively fills the gap in the image assessment landscape, complementing the array of established evaluation schemes – ranging from full-reference metrics like SSIM [?], no-reference metrics such as NIQE [?], to general-reference metrics including FID [?], and Multi-modal-reference metrics, CLIPScore [?]. Utilising a neural network with the cross-attention mechanism and a unique data collection pipeline from NVS optimisation, our method enables accurate image quality assessment without requiring ground truth references. By comparing a query image against multiple views of the same scene, our method addresses the limitations of existing metrics in novel view synthesis (NVS) and similar tasks where direct reference images are unavailable. Experimental results show that our method is closely correlated to the full-reference metric SSIM, while not requiring ground truth references",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zirui Wang*",
      "Wenjing Bian",
      "Victor Adrian Prisacariu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1481_ECCV_2024_paper.php": {
    "title": "Modeling and Driving Human Body Soundfields through Acoustic Primitives",
    "volume": "main",
    "abstract": "While rendering and animation of photorealistic 3D human body models have matured and reached an impressive quality over the past years, modeling the spatial audio associated with such full body models has been largely ignored so far. In this work, we present a framework that allows for high-quality spatial audio generation, capable of rendering the full 3D soundfield generated by a human body, including speech, footsteps, hand-body interactions, and others. Given a basic audio-visual representation of the body in form of 3D body pose and audio from a head-mounted microphone, we demonstrate that we can render the full acoustic scene at any point in 3D space efficiently and accurately. To enable near-field and realtime rendering of sound, we borrow the idea of volumetric primitives from graphical neural rendering and transfer them into the acoustic domain. Our acoustic primitives result in an order of magnitude smaller soundfield representations and overcome deficiencies in near-field rendering compared to previous approaches. Our project page: https: //wikichao.github.io/Acoustic-Primitives/",
    "checked": true,
    "id": "2ee8de540f890887f024b9cf046e89d1082f4100",
    "semantic_title": "modeling and driving human body soundfields through acoustic primitives",
    "citation_count": 1,
    "authors": [
      "Chao Huang*",
      "Dejan Markovic*",
      "Chenliang Xu*",
      "Alexander Richard*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1486_ECCV_2024_paper.php": {
    "title": "m&m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks",
    "volume": "main",
    "abstract": "Real-world multi-modal problems are rarely solved by a single machine learning model, and often require multi-step computational plans that involve stitching several models. Tool-augmented LLMs hold tremendous promise for automating the generation of such computational plans. However, the lack of standardized benchmarks for evaluating LLMs as planners for multi-step multi-modal tasks has prevented a systematic study of planner design decisions. Should LLMs generate a full plan in a single shot or step-by-step? Should they invoke tools directly with Python code or through structured data formats like JSON? Does feedback improve planning? To answer these questions and more, we introduce : a benchmark containing 4K+ multi-step multi-modal tasks involving 33 tools that include multi-modal models, (free) public APIs, and image processing modules. For each of these task queries, we provide automatically generated plans using this realistic toolset. We further provide a high-quality subset of 1,565 task plans that are human-verified and correctly executable. With , we evaluate popular LLMs with 2 planning strategies (multi-step vs. step-by-step planning), 2 plan formats (JSON vs. code), and 3 types of feedback (parsing/verification/execution). Finally, we summarize takeaways from our extensive experiments and provide practical recommendations for designing planners for multi-step multi-modal tasks. Our dataset and evaluation code are available on HuggingFace1 and Github2 respectively. 1 https://huggingface.co/datasets/zixianma/mms 2 https://github.com/RAIVNLab/mms",
    "checked": true,
    "id": "ab12d8c5e607d702f2fc02ea711960509f9ec1a9",
    "semantic_title": "m&m's: a benchmark to evaluate tool-use for multi-step multi-modal tasks",
    "citation_count": 9,
    "authors": [
      "Zixian Ma*",
      "Weikai Huang",
      "Jieyu Zhang",
      "Tanmay Gupta",
      "Ranjay Krishna"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1492_ECCV_2024_paper.php": {
    "title": "Label-anticipated Event Disentanglement for Audio-Visual Video Parsing",
    "volume": "main",
    "abstract": "Audio-Visual Video Parsing (AVVP) task aims to detect and temporally locate events within audio and visual modalities. Multiple events can overlap in the timeline, making identification challenging. While traditional methods usually focus on improving the early audio-visual encoders to embed more effective features, the decoding phase – crucial for final event classification, often receives less attention. We aim to advance the decoding phase and improve its interpretability. Specifically, we introduce a new decoding paradigm, label semantic-based projection (LEAP), that employs labels texts of event categories, each bearing distinct and explicit semantics, for parsing potentially overlapping events. LEAP works by iteratively projecting encoded latent features of audio/visual segments onto semantically independent label embeddings. This process, enriched by modeling cross-modal (audio/visual-label) interactions, gradually disentangles event semantics within video segments to refine relevant label embeddings, guaranteeing a more discriminative and interpretable decoding process. To facilitate the LEAP paradigm, we propose a semantic-aware optimization strategy, which includes a novel audio-visual semantic similarity loss function. This function leverages the Intersection over Union of audio and visual events (EIoU) as a novel metric to calibrate audio-visual similarities at the feature level, accommodating the varied event densities across modalities. Extensive experiments demonstrate the superiority of our method, achieving new state-of-the-art performance for AVVP and also enhancing the relevant audio-visual event localization task.: Corresponding authors ({guodan,wangmeng}@hfut.edu.cn)",
    "checked": true,
    "id": "78c51e8c493d990054e8602b71dfc33080e29f57",
    "semantic_title": "label-anticipated event disentanglement for audio-visual video parsing",
    "citation_count": 1,
    "authors": [
      "Jinxing Zhou*",
      "Dan Guo*",
      "Yuxin Mao",
      "Yiran Zhong",
      "Xiaojun Chang",
      "Meng Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1495_ECCV_2024_paper.php": {
    "title": "High-Fidelity 3D Textured Shapes Generation by Sparse Encoding and Adversarial Decoding",
    "volume": "main",
    "abstract": "3D vision is inherently characterized by sparse spatial structures, which propels the necessity for an efficient paradigm tailored to 3D generation. Another discrepancy is the amount of training data, which undeniably affects generalization if we only use limited 3D data. To solve these, we design a 3D generation framework that maintains most of the building blocks of StableDiffusion with minimal adaptations for textured shape generation. We design a Sparse Encoding Module for details preservation and an Adversarial Decoding Module for better shape recovery. Moreover, we clean up data and build a benchmark on the biggest 3D dataset (Objaverse). We drop the concept of ‘specific class' and treat the 3D Textured Shapes Generation as an open-vocabulary problem. We first validate our network design on ShapeNetV2 with 55K samples on single-class unconditional generation and multi-class conditional generation tasks. Then we report metrics on processed G-Objaverse with 200K samples on the image conditional generation task. Extensive experiments demonstrate our proposal outperforms SOTA methods and takes a further step towards open-vocabulary 3D generation. We release the processed data at https://aigc3d.github.io/gobjaverse/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Zuo*",
      "Xiaodong Gu",
      "Yuan Dong",
      "Zhengyi Zhao",
      "Weihao Yuan",
      "Qiu Lingteng",
      "Liefeng Bo",
      "Zilong Dong"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1500_ECCV_2024_paper.php": {
    "title": "Semi-Supervised Video Desnowing Network via Temporal Decoupling Experts and Distribution-Driven Contrastive Regularization",
    "volume": "main",
    "abstract": "Snow degradations present formidable challenges to the advancement of computer vision tasks by the undesirable corruption in outdoor scenarios. While current deep learning-based desnowing approaches achieve success on synthetic benchmark datasets, they struggle to restore out-of-distribution real-world snowy videos due to the deficiency of paired real-world training data. To address this bottleneck, we devise a new paradigm for video desnowing in a semi-supervised spirit to involve unlabeled real data for the generalizable snow removal. Specifically, we construct a real-world dataset with 85 snowy videos, and then present a Semi-supervised Video Desnowing Network (SemiVDN) equipped by a novel Distribution-driven Contrastive Regularization. The elaborated contrastive regularization mitigates the distribution gap between the synthetic and real data, and consequently maintains the desired snow-invariant background details. Furthermore, based on the atmospheric scattering model, we introduce a Prior-guided Temporal Decoupling Experts module to decompose the physical components that make up a snowy video in a frame-correlated manner. We evaluate our SemiVDN on benchmark datasets and the collected real snowy data. The experimental results demonstrate the superiority of our approach against state-of-the-art imageand video-level desnowing methods. Our code and the dataset are available at https://github.com/TonyHongtaoWu/SemiVDN",
    "checked": true,
    "id": "644c9a9951b4541dd1c7e9895e2fd6eb60649bac",
    "semantic_title": "semi-supervised video desnowing network via temporal decoupling experts and distribution-driven contrastive regularization",
    "citation_count": 0,
    "authors": [
      "Hongtao Wu",
      "Angelica I Aviles-Rivero",
      "Yijun Yang",
      "Jingjing Ren",
      "Sixiang Chen",
      "Haoyu Chen",
      "Lei Zhu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1503_ECCV_2024_paper.php": {
    "title": "I-MedSAM: Implicit Medical Image Segmentation with Segment Anything",
    "volume": "main",
    "abstract": "With the development of Deep Neural Networks (DNNs), many efforts have been made to handle medical image segmentation. Traditional methods such as nnUNet train specific segmentation models on the individual datasets. Plenty of recent methods have been proposed to adapt the foundational Segment Anything Model (SAM) to medical image segmentation. However, they still focus on discrete representations to generate pixel-wise predictions, which are spatially inflexible and scale poorly to higher resolution. In contrast, implicit methods learn continuous representations for segmentation, which is crucial for medical image segmentation. In this paper, we propose I-MedSAM, which leverages the benefits of both continuous representations and SAM, to obtain better cross-domain ability and accurate boundary delineation. Since medical image segmentation needs to predict detailed segmentation boundaries, we designed a novel adapter to enhance the SAM features with high-frequency information during Parameter-Efficient Fine-Tuning (PEFT). To convert the SAM features and coordinates into continuous segmentation output, we utilize Implicit Neural Representation (INR) to learn an implicit segmentation decoder. We also propose an uncertainty-guided sampling strategy for efficient learning of INR. Extensive evaluations on 2D medical image segmentation tasks have shown that our proposed method with only 1.6M trainable parameters outperforms existing methods including discrete and implicit methods. The code will be available at: https://github.com/ucwxb/I-MedSAM",
    "checked": true,
    "id": "03262d71228e57d58939f2ecea85f462fe459a73",
    "semantic_title": "i-medsam: implicit medical image segmentation with segment anything",
    "citation_count": 5,
    "authors": [
      "Xiaobao Wei",
      "Jiajun Cao",
      "Yizhu Jin",
      "Ming Lu",
      "Guangyu Wang",
      "Shanghang Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1522_ECCV_2024_paper.php": {
    "title": "ReMamber: Referring Image Segmentation with Mamba Twister",
    "volume": "main",
    "abstract": "Referring Image Segmentation (RIS) leveraging transformers has achieved great success on the interpretation of complex visual-language tasks. However, the quadratic computation cost makes it resource-consuming in capturing long-range visual-language dependencies. Fortunately, Mamba addresses this with efficient linear complexity in processing. However, directly applying Mamba to multi-modal interactions presents challenges, primarily due to inadequate channel interactions for the effective fusion of multi-modal data. In this paper, we propose , a novel RIS architecture that integrates the power of Mamba with a multi-modal Mamba Twister block. The Mamba Twister explicitly models image-text interaction, and fuses textual and visual features through its unique channel and spatial twisting mechanism. We achieve competitive results on three challenging benchmarks with a simple and efficient architecture. Moreover, we conduct thorough analyses of and discuss other fusion designs using Mamba. These provide valuable perspectives for future research. The code has been released at: https:// github.com/yyh-rain-song/ReMamber",
    "checked": true,
    "id": "9bd60a0b1b5e70c9e6ccfde513f8fdea61d8b503",
    "semantic_title": "remamber: referring image segmentation with mamba twister",
    "citation_count": 14,
    "authors": [
      "Yuhuan Yang",
      "Chaofan Ma",
      "Jiangchao Yao",
      "Zhun Zhong*",
      "Ya Zhang",
      "Yanfeng Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1539_ECCV_2024_paper.php": {
    "title": "TalkingGaussian: Structure-Persistent 3D Talking Head Synthesis via Gaussian Splatting",
    "volume": "main",
    "abstract": "Radiance fields have demonstrated impressive performance in synthesizing lifelike 3D talking heads. However, due to the difficulty in fitting steep appearance changes, the prevailing paradigm that presents facial motions by directly modifying point appearance may lead to distortions in dynamic regions. To tackle this challenge, we introduce TalkingGaussian, a deformation-based radiance fields framework for high-fidelity talking head synthesis. Leveraging the point-based Gaussian Splatting, facial motions can be represented in our method by applying smooth and continuous deformations to persistent Gaussian primitives, without requiring to learn the difficult appearance change like previous methods. Due to this simplification, precise facial motions can be synthesized while keeping a highly intact facial feature. Under such a deformation paradigm, we further identify a face-mouth motion inconsistency that would affect the learning of detailed speaking motions. To address this conflict, we decompose the model into two branches separately for the face and inside mouth areas, therefore simplifying the learning tasks to help reconstruct more accurate motion and structure of the mouth region. Extensive experiments demonstrate that our method renders high-quality lip-synchronized talking head videos, with better facial fidelity and higher efficiency compared with previous methods. Code is available at: https://github.com/Fictionarry/TalkingGaussian",
    "checked": true,
    "id": "17a7117f0f24dead58eee434da920edefa94c000",
    "semantic_title": "talkinggaussian: structure-persistent 3d talking head synthesis via gaussian splatting",
    "citation_count": 4,
    "authors": [
      "Jiahe Li",
      "Jiawei Zhang",
      "Xiao Bai*",
      "Jin Zheng*",
      "Xin Ning",
      "Jun Zhou",
      "Lin Gu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1541_ECCV_2024_paper.php": {
    "title": "CAT: Enhancing Multimodal Large Language Model to Answer Questions in Dynamic Audio-Visual Scenarios",
    "volume": "main",
    "abstract": "This paper focuses on the challenge of answering questions in scenarios that are composed of rich and complex dynamic audio-visual components. Although existing Multimodal Large Language Models (MLLMs) can respond to audio-visual content, these responses are sometimes ambiguous and fail to describe specific audio-visual events. To overcome this limitation, we introduce the CAT, which enhances MLLM in three ways: 1) besides straightforwardly bridging audio and video, we design a clue aggregator that aggregates question-related clues in dynamic audio-visual scenarios to enrich the detailed knowledge required for large language models. 2) CAT is trained on a mixed multimodal dataset, allowing direct application in audio-visual scenarios. Notably, we collect an audio-visual joint instruction dataset named AVinstruct, to further enhance the capacity of CAT to model cross-semantic correlations. 3) we propose AI-assisted ambiguity-aware direct preference optimization, a strategy specialized in retraining the model to favor the non-ambiguity response and improve the ability to localize specific audio-visual objects. Extensive experimental results demonstrate that CAT outperforms existing methods on multimodal tasks, especially in Audio-Visual Question Answering (AVQA) tasks. The codes and the collected instructions will be released soon",
    "checked": true,
    "id": "eb6b054789ff8c9edf7c1d50667be5bdd95e019b",
    "semantic_title": "cat: enhancing multimodal large language model to answer questions in dynamic audio-visual scenarios",
    "citation_count": 6,
    "authors": [
      "Qilang Ye",
      "Zitong Yu*",
      "Rui Shao",
      "Xinyu Xie",
      "Philip Torr",
      "Xiaochun Cao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1546_ECCV_2024_paper.php": {
    "title": "Segmentation-guided Layer-wise Image Vectorization with Gradient Fills",
    "volume": "main",
    "abstract": "The widespread use of vector graphics creates a significant demand for vectorization methods. While recent learning-based techniques have shown their capability to create vector images of clear topology, filling these primitives with gradients remains a challenge. In this paper, we propose a segmentation-guided vectorization framework to convert raster images into concise vector graphics with radial gradient fills. With the guidance of an embedded gradient-aware segmentation subroutine, our approach progressively appends gradient-filled Bézier paths to the output, where primitive parameters are initiated with our newly designed initialization technique and are optimized to minimize our novel loss function. We build our method on a differentiable renderer with traditional segmentation algorithms to develop it as a model-free tool for raster-to-vector conversion. It is tested on various inputs to demonstrate its feasibility, independent of datasets, to synthesize vector graphics with improved visual quality and layer-wise topology compared to prior work",
    "checked": true,
    "id": "4dca1992bd5d8ce1228b578d7d58bfb2391112d6",
    "semantic_title": "segmentation-guided layer-wise image vectorization with gradient fills",
    "citation_count": 0,
    "authors": [
      "Hengyu Zhou",
      "Hui Zhang*",
      "Bin Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1549_ECCV_2024_paper.php": {
    "title": "Implicit Style-Content Separation using B-LoRA",
    "volume": "main",
    "abstract": "Image stylization involves manipulating the visual appearance and texture (style) of an image while preserving its underlying objects, structures, and concepts (content). The separation of style and content is essential for manipulating the image's style independently from its content, ensuring a harmonious and visually pleasing result. Achieving this separation requires a deep understanding of both the visual and semantic characteristics of images, often necessitating the training of specialized models or employing heavy optimization. In this paper, we introduce B-LoRA, a method that leverages LoRA (Low-Rank Adaptation) to implicitly separate the style and content components of a single image, facilitating various image stylization tasks. By analyzing the architecture of SDXL combined with LoRA, we find that jointly learning the LoRA weights of two specific blocks (referred to as B-LoRAs) achieves style-content separation that cannot be achieved by training each B-LoRA independently. Consolidating the training into only two blocks and separating style and content allows for significantly improving style manipulation and overcoming overfitting issues often associated with model fine-tuning. Once trained, the two B-LoRAs can be used as independent components to allow various image stylization tasks, including image style transfer, text-based image stylization, consistent style generation, and style-content mixing",
    "checked": true,
    "id": "e04d2413139ac947ce9ebe44510424964206f6ad",
    "semantic_title": "implicit style-content separation using b-lora",
    "citation_count": 9,
    "authors": [
      "Yarden Frenkel*",
      "Yael Vinker",
      "Ariel Shamir",
      "Danny Cohen-Or"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1552_ECCV_2024_paper.php": {
    "title": "OpenPSG: Open-set Panoptic Scene Graph Generation via Large Multimodal Models",
    "volume": "main",
    "abstract": "Panoptic Scene Graph Generation (PSG) aims to segment objects and recognize their relations, enabling the structured understanding of an image. Previous methods focus on predicting predefined object and relation categories, hence limiting their applications in the open world scenarios. With the rapid development of large multimodal models (LMMs), significant progress has been made in open-set object detection and segmentation, yet open-set relation prediction in PSG remains unexplored. In this paper, we focus on the task of open-set relation prediction integrated with a pretrained open-set panoptic segmentation model to achieve true open-set panoptic scene graph generation (OpenPSG). Our OpenPSG leverages LMMs to achieve open-set relation prediction in an autoregressive manner. We introduce a relation query transformer to efficiently extract visual features of object pairs and estimate the existence of relations between them. The latter can enhance the prediction efficiency by filtering irrelevant pairs. Finally, we design the generation and judgement instructions to perform open-set relation prediction in PSG autoregressively. To our knowledge, we are the first to propose the open-set PSG task. Extensive experiments demonstrate that our method achieves state-of-the-art performance in open-set relation prediction and panoptic scene graph generation. Code is available at https://github.com/franciszzj/OpenPSG",
    "checked": true,
    "id": "2e3912b56db90b2df913b58e3b78c0679af3b35f",
    "semantic_title": "openpsg: open-set panoptic scene graph generation via large multimodal models",
    "citation_count": 0,
    "authors": [
      "Zijian Zhou*",
      "Zheng Zhu",
      "Holger Caesar",
      "Miaojing Shi*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1553_ECCV_2024_paper.php": {
    "title": "ActionVOS: Actions as Prompts for Video Object Segmentation",
    "volume": "main",
    "abstract": "Delving into the realm of egocentric vision, the advancement of referring video object segmentation (RVOS) stands as pivotal in understanding human activities. However, existing RVOS task primarily relies on static attributes such as object names to segment target objects, posing challenges in distinguishing target objects from background objects and in identifying objects undergoing state changes. To address these problems, this work proposes a novel action-aware RVOS setting called , aiming at segmenting only active objects in egocentric videos using human actions as a key language prompt. This is because human actions precisely describe the behavior of humans, thereby helping to identify the objects truly involved in the interaction and to understand possible state changes. We also build a method tailored to work under this specific setting. Specifically, we develop an action-aware labeling module with an efficient action-guided focal loss. Such designs enable ActionVOS model to prioritize active objects with existing readily-available annotations. Experimental results on the VISOR dataset reveal that significantly reduces the mis-segmentation of inactive objects, confirming that actions help the model understand objects' involvement. Further evaluations on VOST and VSCOS datasets show that the novel ActionVOS setting enhances segmentation performance when encountering challenging circumstances involving object state changes. We will make our implementation available at https://github.com/ut-vision/ActionVOS",
    "checked": true,
    "id": "073074655acca7c5e3a63f35f02a777830eb2b6e",
    "semantic_title": "actionvos: actions as prompts for video object segmentation",
    "citation_count": 0,
    "authors": [
      "Liangyang Ouyang*",
      "Ruicong Liu",
      "Yifei Huang*",
      "Ryosuke Furuta",
      "Yoichi Sato*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1558_ECCV_2024_paper.php": {
    "title": "FALIP: Visual Prompt as Foveal Attention Boosts CLIP Zero-Shot Performance",
    "volume": "main",
    "abstract": "CLIP has achieved impressive zero-shot performance after pretraining on a large-scale dataset consisting of paired image-text data. Previous works have utilized CLIP by incorporating manually designed visual prompts like colored circles and blur masks into the images to guide the model's attention, showing enhanced zero-shot performance in downstream tasks. Although these methods have achieved promising results, they inevitably alter the original information of the images, which can lead to failure in specific tasks. We propose a train-free method Foveal-Attention CLIP (FALIP), which adjusts the CLIP's attention by inserting foveal attention masks into the multi-head self-attention module. We demonstrate FALIP effectively boosts CLIP zero-shot performance in tasks such as referring expressions comprehension, image classification, and 3D point cloud recognition. Experimental results further show that FALIP outperforms existing methods on most metrics and can augment current methods to enhance their performance. Our project page is link to https://pumpkin805.github.io/FALIP/",
    "checked": true,
    "id": "588a832668175bd71cff327089b091dd45082b7c",
    "semantic_title": "falip: visual prompt as foveal attention boosts clip zero-shot performance",
    "citation_count": 0,
    "authors": [
      "Jiedong Zhuang",
      "Jiaqi Hu",
      "Lianrui Mu",
      "Rui Hu",
      "Xiaoyu Liang",
      "Jiangnan Ye",
      "Haoji Hu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1566_ECCV_2024_paper.php": {
    "title": "U-COPE: Taking a Further Step to Universal 9D Category-level Object Pose Estimation",
    "volume": "main",
    "abstract": "Rigid and articulated objects are common in our daily lives. Pose estimation tasks for both types of objects have been extensively studied within their respective domains. However, a universal framework capable of estimating the pose of both rigid and articulated objects has yet to be reported. In this paper, we introduce a Universal 9D Category-level Object Pose Estimation (U-COPE) framework, designed to address this gap. Our approach offers a novel perspective on rigid and articulated objects, redefining their pose estimation problems to unify them into a common task. Leveraging either 3D point cloud or RGB-D image inputs, we extract Point Pair Features (PPF) independently from each object part for end-to-end learning. Moreover, instead of direct prediction as seen in prior art, we employ a universal voting strategy to derive decisive parameters crucial for object pose estimation. Our network is trained end-to-end to optimize three key objectives: Joint Information, Part Segmentation, and 9D pose estimation through parameter voting. Extensive experiments validate the robustness of our method in estimating poses for both rigid and articulated objects, which demonstrates the generalizability to unseen object instances, too. Notably, our approach achieves state-of-the-art performance on synthetic datasets and real-world datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "li zhang*",
      "Weiqing Meng",
      "Yan Zhong",
      "Bin Kong",
      "Mingliang Xu",
      "Jianming Du",
      "Xue Wang",
      "Rujing Wang",
      "Liu Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1578_ECCV_2024_paper.php": {
    "title": "Integrating Markov Blanket Discovery into Causal Representation Learning for Domain Generalization",
    "volume": "main",
    "abstract": "Identifying low-dimensional, semantic latent causal representations for high-dimensional data has become a dynamic field in computer vision and machine learning. Causal domain generalization methods aim to identify latent causal variables that generate input data and build invariant causal mechanisms for prediction tasks, thereby improving out-of-distribution (OOD) prediction performance. However, there is no consensus on the best approach for selecting causal variables for prediction. Existing methods typically choose causal or anti-causal variables, excluding other invariant, discriminative features. In this paper, we propose using Markov Blanket features due to their property of being the minimal set that possesses the maximum mutual information with the target. To achieve this, we establish a Causal Markov Blanket Representation Learning (CMBRL) framework, which allows for Markov Blanket discovery in the latent space. We then construct an invariant prediction mechanism using the identified Markov Blanket features, making it suitable for predictions across domains. Compared to state-of-the-art domain generalization methods, our approach exhibits robustness and adaptability under distribution shifts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Naiyu Yin*",
      "Hanjing Wang",
      "Yue Yu",
      "Tian Gao",
      "Amit Dhurandhar",
      "Qiang Ji"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1584_ECCV_2024_paper.php": {
    "title": "Rotary Position Embedding for Vision Transformer",
    "volume": "main",
    "abstract": "Rotary Position Embedding (RoPE) performs remarkably on language models, especially for length extrapolation of Transformers. However, the impacts of RoPE on computer vision domains have been underexplored, even though RoPE appears capable of enhancing Vision Transformer (ViT) performance in a way similar to the language domain. This study provides a comprehensive analysis of RoPE when applied to ViTs, utilizing practical implementations of RoPE for 2D vision data. The analysis reveals that RoPE demonstrates impressive extrapolation performance, i.e., maintaining precision while increasing image resolution at inference. It eventually leads to performance improvement for ImageNet-1k, COCO detection, and ADE-20k segmentation. We believe this study provides thorough guidelines to apply RoPE into ViT, promising improved backbone performance with minimal extra computational overhead. Our code and pre-trained models are available at https://github.com/naver-ai/rope-vit",
    "checked": true,
    "id": "6b597704044b71cbf5c224a441eb5d803445ac1c",
    "semantic_title": "rotary position embedding for vision transformer",
    "citation_count": 10,
    "authors": [
      "Byeongho Heo*",
      "Song Park",
      "Dongyoon Han",
      "Sangdoo Yun"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1595_ECCV_2024_paper.php": {
    "title": "Local All-Pair Correspondence for Point Tracking",
    "volume": "main",
    "abstract": "We introduce , a highly accurate and efficient model designed for the task of tracking any point (TAP) across video sequences. Previous approaches in this task often rely on local 2D correlation maps to establish correspondences from a point in the query image to a local region in the target image, which often struggle with homogeneous regions or repetitive features, leading to matching ambiguities. overcomes this challenge with a novel approach that utilizes all-pair correspondences across regions, , local 4D correlation, to establish precise correspondences, with bidirectional correspondence and matching smoothness significantly enhancing robustness against ambiguities. We also incorporate a lightweight correlation encoder to enhance computational efficiency, and a compact Transformer architecture to integrate long-term temporal information. achieves unmatched accuracy on all TAP-Vid benchmarks and operates at a speed almost 6× faster than the current state-of-the-art",
    "checked": true,
    "id": "96abb51a78ba429e40dfab84db0f10cadb69e157",
    "semantic_title": "local all-pair correspondence for point tracking",
    "citation_count": 2,
    "authors": [
      "Seokju Cho",
      "Jiahui Huang",
      "Jisu Nam",
      "Honggyu An",
      "Seungryong Kim*",
      "Joon-Young Lee*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1600_ECCV_2024_paper.php": {
    "title": "MonoWAD: Weather-Adaptive Diffusion Model for Robust Monocular 3D Object Detection",
    "volume": "main",
    "abstract": "Monocular 3D object detection is an important challenging task in autonomous driving. Existing methods mainly focus on performing 3D detection in ideal weather conditions, characterized by scenarios with clear and optimal visibility. However, the challenge of autonomous driving requires the ability to handle changes in weather conditions, such as foggy weather, not just clear weather. We introduce MonoWAD, a novel weather-robust monocular 3D object detector with a weather-adaptive diffusion model. It contains two components: (1) the weather codebook to memorize the knowledge of the clear weather and generate a weather-reference feature for any input, and (2) the weather-adaptive diffusion model to enhance the feature representation of the input feature by incorporating a weather-reference feature. This serves an attention role in indicating how much improvement is needed for the input feature according to the weather conditions. To achieve this goal, we introduce a weather-adaptive enhancement loss to enhance the feature representation under both clear and foggy weather conditions. Extensive experiments under various weather conditions demonstrate that MonoWAD achieves weather-robust monocular 3D object detection. The code and dataset are released at https://github.com/VisualAIKHU/MonoWAD",
    "checked": true,
    "id": "51080877bc06c9469587045642242d130fd3b594",
    "semantic_title": "monowad: weather-adaptive diffusion model for robust monocular 3d object detection",
    "citation_count": 0,
    "authors": [
      "Youngmin Oh",
      "Hyung-Il Kim",
      "Seong Tae Kim*",
      "Jung Uk Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1610_ECCV_2024_paper.php": {
    "title": "ReALFRED: An Embodied Instruction Following Benchmark in Photo-Realistic Environments",
    "volume": "main",
    "abstract": "Simulated virtual environments have been widely used to learn robotic agents that perform daily household tasks. These environments encourage research progress by far, but often provide limited object interactability, visual appearance different from real-world environments, or relatively smaller environment sizes. This prevents the learned models in the virtual scenes from being readily deployable. To bridge the gap between these learning environments and deploying (, real) environments, we propose the benchmark that employs real-world scenes, objects, and room layouts to learn agents to complete household tasks by understanding free-form language instructions and interacting with objects in large, multi-room and 3D-captured scenes. Specifically, we extend the ALFRED benchmark with updates for larger environmental spaces with smaller visual domain gaps. With , we analyze previously crafted methods for the ALFRED benchmark and observe that they consistently yield lower performance in all metrics, encouraging the community to develop methods in more realistic environments. Our code and data are publicly available1 . 1 Homepage: https://github.com/snumprlab/realfred",
    "checked": true,
    "id": "6cabe56120d2fc01ca422e4e7442401ee6ab1e86",
    "semantic_title": "realfred: an embodied instruction following benchmark in photo-realistic environments",
    "citation_count": 1,
    "authors": [
      "Taewoong Kim",
      "Cheolhong Min",
      "Byeonghwi Kim",
      "Jinyeon Kim",
      "Wonje Jeung",
      "Jonghyun Choi*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1618_ECCV_2024_paper.php": {
    "title": "S^3D-NeRF: Single-Shot Speech-Driven Neural Radiance Field for High Fidelity Talking Head Synthesis",
    "volume": "main",
    "abstract": "Talking head synthesis is a practical technique with wide applications. Current Neural Radiance Field (NeRF) based approaches have shown their superiority on driving one-shot talking heads with videos or signals regressed from audio. However, most of them failed to take the audio as driven information directly, unable to enjoy the flexibility and availability of speech. Since mapping audio signals to face deformation is non-trivial, we design a Single-Shot Speech-Driven Neural Radiance Field () method in this paper to tackle the following three difficulties: learning a representative appearance feature for each identity, modeling motion of different face regions with audio, and keeping the temporal consistency of the lip area. To this end, we introduce a to learn multi-scale representations for catching the appearance of different speakers, and elaborate a to perform speech animation according to the relationship between the audio signal and different face regions. Moreover, to enhance the temporal consistency of the important lip area, we introduce a lip-sync discriminator to penalize the out-of-sync audio-visual sequences. Extensive experiments have shown that our surpasses previous arts on both video fidelity and audio-lip synchronization",
    "checked": false,
    "id": "7e07c3ac2ba3ffe5746d0c465e37ebe58a34e8e0",
    "semantic_title": "s3d-nerf: single-shot speech-driven neural radiance field for high fidelity talking head synthesis",
    "citation_count": 0,
    "authors": [
      "Dongze Li*",
      "Kang Zhao*",
      "Wei Wang*",
      "Yifeng Ma",
      "Bo Peng",
      "Yingya Zhang",
      "Jing Dong"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1621_ECCV_2024_paper.php": {
    "title": "ActionSwitch: Class-agnostic Detection of Simultaneous Actions in Streaming Videos",
    "volume": "main",
    "abstract": "Online Temporal Action Localization (On-TAL) is a critical task that aims to instantaneously identify action instances in untrimmed streaming videos as soon as an action concludes—a major leap from frame-based Online Action Detection (OAD). Yet, the challenge of detecting overlapping actions is often overlooked even though it is a common scenario in streaming videos. Current methods that can address concurrent actions depend heavily on class information, limiting their flexibility. This paper introduces ActionSwitch, the first class-agnostic On-TAL framework capable of detecting overlapping actions. By obviating the reliance on class information, ActionSwitch provides wider applicability to various situations, including overlapping actions of the same class or scenarios where class information is unavailable. This approach is complemented by the proposed \"conservativeness loss\", which directly embeds a conservative decision-making principle into the loss function for On-TAL. Our ActionSwitch achieves state-of-the-art performance in complex datasets, including Epic-Kitchens 100 targeting the challenging egocentric view and FineAction consisting of fine-grained actions",
    "checked": true,
    "id": "92255479429b3c62c4579ae02ee74fdf93d935d5",
    "semantic_title": "actionswitch: class-agnostic detection of simultaneous actions in streaming videos",
    "citation_count": 0,
    "authors": [
      "Hyolim Kang",
      "Jeongseok Hyun",
      "Joungbin An",
      "Youngjae Yu",
      "Seon Joo Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1622_ECCV_2024_paper.php": {
    "title": "Hierarchically Structured Neural Bones for Reconstructing Animatable Objects from Casual Videos",
    "volume": "main",
    "abstract": "We propose a new framework for creating and easily manipulating 3D models of arbitrary objects using casually captured videos. Our core ingredient is a novel hierarchy deformation model, which captures motions of objects with a tree-structured bones. Our hierarchy system decomposes motions based on the granularity and reveals the correlations between parts without exploiting any prior structural knowledge. We further propose to regularize the bones to be positioned at the basis of motions, centers of parts, sufficiently covering related surfaces of the part. This is achieved by our bone occupancy function, which identifies whether a given 3D point is placed within the bone. Coupling the proposed components, our framework offers several clear advantages: (1) Users can obtain animatable 3D models of the arbitrary objects in improved quality from their casual videos, (2) users can manipulate 3D models in an intuitive manner with minimal costs, and (3) users can interactively add or delete control points as necessary. The experimental results demonstrate the efficacy of our framework on diverse instances, in reconstruction quality, interpretability and easier manipulation. Our code is available at https://github.com/subin6/ HSNB",
    "checked": true,
    "id": "e18bb987166b8048a2dbec5117bdf7d126090fa4",
    "semantic_title": "hierarchically structured neural bones for reconstructing animatable objects from casual videos",
    "citation_count": 0,
    "authors": [
      "Subin Jeon",
      "In Cho",
      "Minsu Kim",
      "Woong Oh Cho",
      "Seon Joo Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1627_ECCV_2024_paper.php": {
    "title": "PQ-SAM: Post-training Quantization for Segment Anything Model",
    "volume": "main",
    "abstract": "Segment anything model (SAM) is a promising prompt-guided vision foundation model to segment objects of interest. However, the extensive computational requirements of SAM have limited its applicability in resource-constraint edge devices. Post-training quantization (PTQ) is an effective potential for fast-deploying SAM. Nevertheless, SAM's billion-scale pretraining creates a highly asymmetric activation distribution with detrimental outliers in excessive channels, resulting in significant performance degradation of the low-bit PTQ. In this paper, we propose PQ-SAM, the first PTQ method customized for SAM. To achieve a quantization-friendly tensor-wise distribution, PQ-SAM incorporates a novel grouped activation distribution transformation (GADT) based on a two-stage outlier hierarchical clustering (OHC) scheme to scale and shift each channel. Firstly, OHC identifies and truncates extreme outliers to reduce the scale variance of different channels. Secondly, OHC iteratively allocates learnable shifting and scaling sizes to each group of channels with similar distributions, reducing the number of learnable parameters and easing the optimization difficulty. These shifting and scaling sizes are used to adjust activation channels, and jointly optimized with quantization step sizes for optimal results. Extensive experiments demonstrate that PQ-SAM outperforms existing PTQ methods on nine zero-shot datasets, and pushes the 4-bit PTQ of SAM to a usable level",
    "checked": false,
    "id": "112eb0d25bf07aa4d4a8f0bfd829fcd8643bc0ec",
    "semantic_title": "ptq4sam: post-training quantization for segment anything",
    "citation_count": 4,
    "authors": [
      "Xiaoyu Liu*",
      "Xin Ding",
      "Lei Yu",
      "Yuanyuan Xi",
      "Wei Li",
      "Zhijun Tu",
      "jie hu",
      "Hanting Chen",
      "Baoqun YIN",
      "Zhiwei Xiong*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1634_ECCV_2024_paper.php": {
    "title": "CPM: Class-conditional Prompting Machine for Audio-visual Segmentation",
    "volume": "main",
    "abstract": "Audio-visual segmentation (AVS) is an emerging task that aims to accurately segment sounding objects based on audio-visual cues. The success of AVS learning systems depends on the effectiveness of cross-modal interaction. Such a requirement can be naturally fulfilled by leveraging transformer-based segmentation architecture due to its inherent ability to capture long-range dependencies and flexibility in handling different modalities. However, the inherent training issues of transformer-based methods, such as the low efficacy of cross-attention and unstable bipartite matching, can be amplified in AVS, particularly when the learned audio query does not provide a clear semantic clue. In this paper, we address these two issues with the new Class-conditional Prompting Machine (CPM). CPM improves the bipartite matching with a learning strategy combining class-agnostic queries with class-conditional queries. The efficacy of cross-modal attention is upgraded with new learning objectives for the audio, visual and joint modalities. We conduct experiments on AVS benchmarks, demonstrating that our method achieves state-of-the-art (SOTA) segmentation accuracy1 . 1 This project is supported by the Australian Research Council (ARC) through grant FT190100525",
    "checked": true,
    "id": "1ae06071367ee0de619768a679249e214303758c",
    "semantic_title": "cpm: class-conditional prompting machine for audio-visual segmentation",
    "citation_count": 0,
    "authors": [
      "Yuanhong Chen*",
      "Chong Wang",
      "Yuyuan Liu",
      "Hu Wang",
      "Gustavo Carneiro"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1635_ECCV_2024_paper.php": {
    "title": "Optimizing Factorized Encoder Models: Time and Memory Reduction for Scalable and Efficient Action Recognition",
    "volume": "main",
    "abstract": "In this paper, we address the challenges posed by the substantial training time and memory consumption associated with video transformers, focusing on the ViViT (Video Vision Transformer) model, in particular the Factorised Encoder version, as our baseline for action recognition tasks. The factorised encoder variant follows the late-fusion approach that is adopted by many state of the art approaches. Despite standing out for its favorable speed/accuracy tradeoffs among the different variants of ViViT, its considerable training time and memory requirements still pose a significant barrier to entry. Our method is designed to lower this barrier and is based on the idea of freezing the spatial transformer during training. This leads to a low accuracy model if naively done. But we show that by (1) appropriately initializing the temporal transformer (a module responsible for processing temporal information) (2) introducing a compact adapter model connecting frozen spatial representations (a module that selectively focuses on regions of the input image) to the temporal transformer, we can enjoy the benefits of freezing the spatial transformer without sacrificing accuracy. Through extensive experimentation over 6 benchmarks, we demonstrate that our proposed training strategy significantly reduces training costs (by ) and memory consumption while maintaining or slightly improving performance by up to 1.79% compared to the baseline model. Our approach additionally unlocks the capability to utilize larger image transformer models as our spatial transformer and access more frames with the same memory consumption. We also show the generalization of this approach to other factorized encoder models. The advancements made in this work have the potential to advance research in the video understanding domain and provide valuable insights for researchers and practitioners with limited resources, paving the way for more efficient and scalable alternatives in the action recognition field",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shreyank N Gowda*",
      "Anurag Arnab",
      "Jonathan Huang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1652_ECCV_2024_paper.php": {
    "title": "DVLO: Deep Visual-LiDAR Odometry with Local-to-Global Feature Fusion and Bi-Directional Structure Alignment",
    "volume": "main",
    "abstract": "Information inside visual and LiDAR data is well complementary derived from the fine-grained texture of images and massive geometric information in point clouds. However, it remains challenging to explore effective visual-LiDAR fusion, mainly due to the intrinsic data structure inconsistency between two modalities: Image pixels are regular and dense, but LiDAR points are unordered and sparse. To address the problem, we propose a local-to-global fusion network (DVLO) with bi-directional structure alignment. To obtain locally fused features, we project points onto the image plane as cluster centers and cluster image pixels around each center. Image pixels are pre-organized as pseudo points for image-to-point structure alignment. Then, we convert points to pseudo images by cylindrical projection (point-to-image structure alignment) and perform adaptive global feature fusion between point features and local fused features. Our method achieves state-of-the-art performance on KITTI odometry and FlyingThings3D scene flow datasets compared to both single-modal and multi-modal methods. Codes are released at https://github.com/IRMVLab/DVLO",
    "checked": true,
    "id": "7db0ef9b5f07d7f585f6686cfb8e23d60e1d3c5e",
    "semantic_title": "dvlo: deep visual-lidar odometry with local-to-global feature fusion and bi-directional structure alignment",
    "citation_count": 3,
    "authors": [
      "Jiuming Liu",
      "Dong Zhuo",
      "Zhiheng Feng",
      "Siting Zhu",
      "Chensheng Peng",
      "Zhe Liu",
      "Hesheng Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1653_ECCV_2024_paper.php": {
    "title": "CoLeaF: A Contrastive-Collaborative Learning Framework for Weakly Supervised Audio-Visual Video Parsing",
    "volume": "main",
    "abstract": "Weakly supervised audio-visual video parsing (AVVP) methods aim to detect audible-only, visible-only, and audible-visible events using only video-level labels. Existing approaches tackle this by leveraging unimodal and cross-modal contexts. However, we argue that while cross-modal learning is beneficial for detecting audible-visible events, in the weakly supervised scenario, it negatively impacts unaligned audible or visible events by introducing irrelevant modality information. In this paper, we propose , a novel learning framework that optimizes the integration of cross-modal context in the embedding space such that the network explicitly learns to combine cross-modal information for audible-visible events while filtering them out for unaligned events. Additionally, as videos often involve complex class relationships, modelling them improves performance. However, this introduces extra computational costs into the network. Our framework is designed to leverage cross-class relationships during training without incurring additional computations at inference. Furthermore, we propose new metrics to better evaluate a method's capabilities in performing AVVP. Our extensive experiments demonstrate that significantly improves the state-of-the-art results by an average of 1.9% and 2.4% F-score on the LLP and UnAV-100 datasets, respectively. Code is available at: https://github.com/faeghehsardari/coleaf",
    "checked": true,
    "id": "d6a24db54df194551a7f2238159b9d7292565974",
    "semantic_title": "coleaf: a contrastive-collaborative learning framework for weakly supervised audio-visual video parsing",
    "citation_count": 0,
    "authors": [
      "Faegheh Sardari*",
      "Armin Mustafa",
      "Philip JB Jackson",
      "Adrian Hilton"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1688_ECCV_2024_paper.php": {
    "title": "Noise-assisted Prompt Learning for Image Forgery Detection and Localization",
    "volume": "main",
    "abstract": "We present CLIP-IFDL, a novel image forgery detection and localization (IFDL) model that harnesses the power of Contrastive Language Image Pre-Training (CLIP). However, directly incorporating CLIP in forgery detection poses challenges, given its lack of specific prompts and forgery consciousness. To overcome these challenges, we tailor the CLIP model for forgery detection and localization leveraging a noise-assisted prompt learning framework. This framework comprises instance-aware dual-stream prompt learning and a forgery-enhanced noise adapter. We initially create a pair of learnable prompts as negative-positive samples in place of discrete prompts, then fine-tune these prompts based on each image's features and categories. Additionally, we constrain the text-image similarity between the prompts and their corresponding images to update the prompts. Moreover, We design a forgery-enhanced noise adapter that augments the image encoder's forgery perceptual ability via multi-domain fusion and zero linear layers. By doing so, our method not only extracts pertinent features but also benefits from the generalizability of the open-world CLIP prior. Comprehensive tests indicate that our method outperforms existing ones in terms of accuracy and generalizability while effectively reducing false alarms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dong Li",
      "Jiaying Zhu",
      "Xueyang Fu*",
      "Xun Guo",
      "Yidi Liu",
      "Gang Yang",
      "Jiawei Liu",
      "Zheng-Jun Zha"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1689_ECCV_2024_paper.php": {
    "title": "Data Collection-free Masked Video Modeling",
    "volume": "main",
    "abstract": "Pre-training video transformers generally requires a large amount of data, presenting significant challenges in terms of data collection costs and concerns related to privacy, licensing, and inherent biases. Synthesizing data is one of the promising ways to solve these issues, yet pre-training solely on synthetic data has its own challenges. In this paper, we introduce an effective self-supervised learning framework for videos that leverages readily available and less costly static images. Specifically, we define the Pseudo Motion Generator (PMG) module that recursively applies image transformations to generate pseudo-motion videos from images. These pseudo-motion videos are then leveraged in masked video modeling. Our approach is applicable to synthetic images as well, thus entirely freeing video pre-training from data collection costs and other concerns in real data. Through experiments in action recognition tasks, we demonstrate that this framework allows effective learning of spatio-temporal features through pseudo-motion videos, significantly improving over existing methods which also use static images and partially outperforming those using both real and synthetic videos. These results uncover fragments of what video transformers learn through masked video modeling",
    "checked": true,
    "id": "63a8952a98c9b26e3a36f6cde82118215a4d1e01",
    "semantic_title": "data collection-free masked video modeling",
    "citation_count": 1,
    "authors": [
      "Yuchi Ishikawa*",
      "Masayoshi Kondo",
      "Yoshimitsu Aoki"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1691_ECCV_2024_paper.php": {
    "title": "Protecting NeRFs' Copyright via Plug-And-Play Watermarking Base Model",
    "volume": "main",
    "abstract": "Neural Radiance Fields (NeRFs) have become a key method for 3D scene representation. With the rising prominence and influence of NeRF, safeguarding its intellectual property has become increasingly important. In this paper, we propose NeRFProtector, which adopts a plug-and-play strategy to protect NeRF's copyright during its creation. NeRFProtector utilizes a pre-trained watermarking base model, enabling NeRF creators to embed binary messages directly while creating their NeRF. Our plug-and-play property ensures NeRF creators can flexibly choose NeRF variants without excessive modifications. Leveraging our newly designed progressive distillation, we demonstrate performance on par with several leading-edge neural rendering methods. Our project is available at: https://qsong2001.github.io/NeRFProtector",
    "checked": true,
    "id": "c5ad9f8ae4a035cc883d998034c11144a4eccc3a",
    "semantic_title": "protecting nerfs' copyright via plug-and-play watermarking base model",
    "citation_count": 2,
    "authors": [
      "Qi Song*",
      "Ziyuan Luo",
      "Ka Chun Cheung",
      "Simon See",
      "Renjie Wan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1705_ECCV_2024_paper.php": {
    "title": "Pixel-Aware Stable Diffusion for Realistic Image Super-Resolution and Personalized Stylization",
    "volume": "main",
    "abstract": "Diffusion models have demonstrated impressive performance in various image generation, editing, enhancement and translation tasks. In particular, the pre-trained text-to-image stable diffusion models provide a potential solution to the challenging realistic image super-resolution (Real-ISR) and image stylization problems with their strong generative priors. However, the existing methods along this line often fail to keep faithful pixel-wise image structures. If extra skip connections between the encoder and the decoder of a VAE are used to reproduce details, additional training in image space will be required, limiting the application to tasks in latent space such as image stylization. In this work, we propose a pixel-aware stable diffusion (PASD) network to achieve robust Real-ISR and personalized image stylization. Specifically, a pixel-aware cross attention module is introduced to enable diffusion models perceiving image local structures in pixel-wise level, while a degradation removal module is used to extract degradation insensitive features to guide the diffusion process together with image high level information. An adjustable noise schedule is introduced to further improve the image restoration results. By simply replacing the base diffusion model with a stylized one, PASD can generate diverse stylized images without collecting pairwise training data, and by shifting the base model with an aesthetic one, PASD can bring old photos back to life. Extensive experiments in a variety of image enhancement and stylization tasks demonstrate the effectiveness of our proposed PASD approach. Our source codes are available at https: //github.com/yangxy/PASD/",
    "checked": true,
    "id": "ccf43c35160954616b8f1c3c00e939883b666c2f",
    "semantic_title": "pixel-aware stable diffusion for realistic image super-resolution and personalized stylization",
    "citation_count": 55,
    "authors": [
      "Tao Yang*",
      "Rongyuan Wu",
      "Peiran Ren",
      "Xuansong Xie",
      "Lei Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1706_ECCV_2024_paper.php": {
    "title": "AnyControl: Create Your Artwork with Versatile Control on Text-to-Image Generation",
    "volume": "main",
    "abstract": "The field of text-to-image (T2I) generation has made significant progress in recent years, largely driven by advancements in diffusion models. Linguistic control enables effective content creation, but struggles with fine-grained control over image generation. This challenge has been explored, to a great extent, by incorporating additional user-supplied spatial conditions, such as depth maps and edge maps, into pre-trained T2I models through extra encoding. However, multi-control image synthesis still faces several challenges. Specifically, current approaches are limited in handling free combinations of diverse input control signals, overlook the complex relationships among multiple spatial conditions, and often fail to maintain semantic alignment with provided textual prompts. This can lead to suboptimal user experiences. To address these challenges, we propose , a multi-control image synthesis framework that supports arbitrary combinations of diverse control signals. develops a novel Multi-Control Encoder that extracts a unified multi-modal embedding to guide the generation process. This approach enables a holistic understanding of user inputs, and produces high-quality, faithful results under versatile control signals, as demonstrated by extensive quantitative and qualitative evaluations. Our project page is available in https://any-control.github.io",
    "checked": true,
    "id": "10e84acf3ceec6338b95f7b131173eb681815b8e",
    "semantic_title": "anycontrol: create your artwork with versatile control on text-to-image generation",
    "citation_count": 1,
    "authors": [
      "Yanan Sun*",
      "Yanchen Liu",
      "Yinhao Tang",
      "Wenjie Pei",
      "Kai Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1713_ECCV_2024_paper.php": {
    "title": "SEED: A Simple and Effective 3D DETR in Point Clouds",
    "volume": "main",
    "abstract": "Recently, detection transformers (DETRs) have gradually taken a dominant position in 2D detection thanks to their elegant framework. However, DETR-based detectors for 3D point clouds are still difficult to achieve satisfactory performance. We argue that the main challenges are twofold: 1) How to obtain the appropriate object queries is challenging due to the high sparsity and uneven distribution of point clouds; 2) How to implement an effective query interaction by exploiting the rich geometric structure of point clouds is not fully explored. To this end, we propose a Simple and EffEctive 3D DETR method () for detecting 3D objects from point clouds, which involves a dual query selection (DQS) module and a deformable grid attention (DGA) module. More concretely, to obtain appropriate queries, DQS first ensures a high recall to retain a large number of queries by the predicted confidence scores and then further picks out high-quality queries according to the estimated quality scores. DGA uniformly divides each reference box into grids as the reference points and then utilizes the predicted offsets to achieve a flexible receptive field, allowing the network to focus on relevant regions and capture more informative features. Extensive ablation studies on DQS and DGA demonstrate its effectiveness. Furthermore, our achieves state-of-the-art detection performance on both the large-scale Waymo and nuScenes datasets, illustrating the superiority of our proposed method. The code is available at https://github.com/happinesslz/SEED",
    "checked": true,
    "id": "3b977e575291c3a40d246c17ed610a12df1af5c7",
    "semantic_title": "seed: a simple and effective 3d detr in point clouds",
    "citation_count": 0,
    "authors": [
      "Zhe Liu",
      "Jinghua Hou",
      "Xiaoqing Ye",
      "Tong Wang",
      "Jingdong Wang",
      "Xiang Bai*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1714_ECCV_2024_paper.php": {
    "title": "AEDNet: Adaptive Embedding and Multiview-Aware Disentanglement for Point Cloud Completion",
    "volume": "main",
    "abstract": "Point cloud completion involves inferring missing parts of 3D objects from incomplete point cloud data. It requires a model that understands the global structure of the object and reconstructs local details. To this end, we propose a global perception and local attention network, termed AEDNet, for point cloud completion. The proposed AEDNet utilizes designed adaptive point cloud embedding and disentanglement (AED) module in both the encoder and decoder to globally embed and locally disentangle the given point cloud. In the AED module, we introduce a global embedding operator that employs the devised slot attention to compose point clouds into different embeddings, each focusing on specific parts of 3D objects. Then, we proposed a multiview-aware disentanglement operator to disentangle geometric information from those embeddings in the 3D viewpoints generated on a unit sphere. These 3D viewpoints enable us to observe point clouds from the outside rather than from within, resulting in a comprehensive understanding of their geometry. Additionally, the arbitrary number of points and point-wise features can be disentangled by changing the number of viewpoints, reaching high flexibility. Experiments show that our proposed method achieves state-of-the-art results on both MVP and PCN datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiheng Fu",
      "Longguang Wang",
      "Lian Xu",
      "Zhiyong Wang",
      "Hamid Laga",
      "Yulan Guo*",
      "Farid Boussaid",
      "Mohammed Bennamoun"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1721_ECCV_2024_paper.php": {
    "title": "Synergy of Sight and Semantics: Visual Intention Understanding with CLIP",
    "volume": "main",
    "abstract": "Multi-label Intention Understanding (MIU) for images is a critical yet challenging domain, primarily due to the ambiguity of intentions leading to a resource-intensive annotation process. Current leading approaches are held back by the limited amount of labeled data. To mitigate the scarcity of annotated data, we leverage the Contrastive Language-Image Pre-training (CLIP) model, renowned for its wealth knowledge in textual and visual modalities. We introduce a novel framework, Intention Understanding with CLIP (IntCLIP), which utilizes a dual-branch approach. This framework exploits the ‘Sight'-oriented knowledge inherent in CLIP to augment ‘Semantic'-centric MIU tasks. Additionally, we propose Hierarchical Class Integration to effectively manage the complex layered label structure, aligning it with CLIP's nuanced sentence feature extraction capabilities. Our Sight-assisted Aggregation further refines this model by infusing the semantic feature map with essential visual cues, thereby enhancing the intention understanding ability. Through extensive experiments conducted on the standard MIU benchmark and other subjective tasks such as Image Emotion Recognition, IntCLIP clearly demonstrates superiority over current state-of-the-art techniques. Code is available at https://github.com/yan9qu/ IntCLIP",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qu Yang",
      "Mang Ye*",
      "Dacheng Tao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1723_ECCV_2024_paper.php": {
    "title": "Intrinsic Single-Image HDR Reconstruction",
    "volume": "main",
    "abstract": "The low dynamic range (LDR) of common cameras fails to capture the rich contrast in natural scenes, resulting in loss of color and details in saturated pixels. Reconstructing the high dynamic range (HDR) of luminance present in the scene from single LDR photographs is an important task with many applications in computational photography and realistic display of images. The HDR reconstruction task aims to infer the lost details using the context present in the scene, requiring neural networks to understand high-level geometric and illumination cues. This makes it challenging for data-driven algorithms to generate accurate and high-resolution results. In this work, we introduce a physically-inspired remodeling of the HDR reconstruction problem in the intrinsic domain. The intrinsic model allows us to train separate networks to extend the dynamic range in the shading domain and to recover lost color details in the albedo domain. We show that dividing the problem into two simpler sub-tasks improves performance in a wide variety of photographs",
    "checked": true,
    "id": "88f3ff86568706e30517d0a37c1651936d74bec6",
    "semantic_title": "intrinsic single-image hdr reconstruction",
    "citation_count": 0,
    "authors": [
      "Sebastian Dille*",
      "Chris Careaga*",
      "Yagiz Aksoy"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1724_ECCV_2024_paper.php": {
    "title": "T-MAE: Temporal Masked Autoencoders for Point Cloud Representation Learning",
    "volume": "main",
    "abstract": "The scarcity of annotated data in LiDAR point cloud understanding hinders effective representation learning. Consequently, scholars have been actively investigating efficacious self-supervised pre-training paradigms. Nevertheless, temporal information, which is inherent in the LiDAR point cloud sequence, is consistently disregarded. To better utilize this property, we propose an effective pre-training strategy, namely Temporal Masked Auto-Encoders (T-MAE), which takes as input temporally adjacent frames and learns temporal dependency. A SiamWCA backbone, containing a Siamese encoder and a windowed cross-attention (WCA) module, is established for the two-frame input. Considering that the movement of an ego-vehicle alters the view of the same instance, temporal modeling also serves as a robust and natural data augmentation, enhancing the comprehension of target objects. is a powerful architecture but heavily relies on annotated data. Our pre-training strategy alleviates its demand for annotated data. Comprehensive experiments demonstrate that achieves the best performance on both Waymo and ONCE datasets among competitive self-supervised approaches",
    "checked": true,
    "id": "e43873bbb8f87120620bdf52a2e1f6d90d3ecc4d",
    "semantic_title": "t-mae: temporal masked autoencoders for point cloud representation learning",
    "citation_count": 1,
    "authors": [
      "Weijie Wei*",
      "Fatemeh Karimi Nejadasl",
      "Theo Gevers",
      "Martin R. Oswald*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1736_ECCV_2024_paper.php": {
    "title": "Pathology-knowledge Enhanced Multi-instance Prompt Learning for Few-shot Whole Slide Image Classification",
    "volume": "main",
    "abstract": "Current multi-instance learning algorithms for pathology image analysis often require a substantial number of Whole Slide Images for effective training but exhibit suboptimal performance in scenarios with limited learning data. In clinical settings, restricted access to pathology slides is inevitable due to patient privacy concerns and the prevalence of rare or emerging diseases. The emergence of the Few-shot Weakly Supervised WSI Classification accommodates the significant challenge of the limited slide data and sparse slide-level labels for diagnosis. Prompt learning based on the pre-trained models (, CLIP) appears to be a promising scheme for this setting; however, current research in this area is limited, and existing algorithms often focus solely on patch-level prompts or confine themselves to language prompts. This paper proposes a multi-instance prompt learning framework enhanced with pathology knowledge, , integrating visual and textual prior knowledge into prompts at both patch and slide levels. The training process employs a combination of static and learnable prompts, effectively guiding the activation of pre-trained models and further facilitating the diagnosis of key pathology patterns. Lightweight Messenger (self-attention) and Summary (attention-pooling) layers are introduced to model relationships between patches and slides within the same patient data. Additionally, alignment-wise contrastive losses ensure the feature-level alignment between visual and textual learnable prompts for both patches and slides. Our method demonstrates superior performance in three challenging clinical tasks, significantly outperforming comparative few-shot methods",
    "checked": true,
    "id": "bc65aa0327a6c57e236ed26347931ac3675a5618",
    "semantic_title": "pathology-knowledge enhanced multi-instance prompt learning for few-shot whole slide image classification",
    "citation_count": 1,
    "authors": [
      "Linhao Qu*",
      "Dingkang Yang",
      "Dan Huang",
      "Qinhao Guo",
      "rongkui luo",
      "Shaoting Zhang",
      "Xiaosong Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1738_ECCV_2024_paper.php": {
    "title": "Towards Natural Language-Guided Drones: GeoText-1652 Benchmark with Spatial Relation Matching",
    "volume": "main",
    "abstract": "Navigating drones through natural language commands remains challenging due to the dearth of accessible multi-modal datasets and the stringent precision requirements for aligning visual and textual data. To address this pressing need, we introduce GeoText-1652, a new natural language-guided geolocalization benchmark. This dataset is systematically constructed through an interactive human-computer process leveraging Large Language Model (LLM) driven annotation techniques in conjunction with pre-trained vision models. GeoText-1652 extends the established University-1652 image dataset with spatial-aware text annotations, thereby establishing one-to-one correspondences between image, text, and bounding box elements. We further introduce a new optimization objective to leverage fine-grained spatial associations, called blending spatial matching, for region-level spatial relation matching. Extensive experiments reveal that our approach maintains a competitive recall rate comparing other prevailing cross-modality methods. This underscores the promising potential of our approach in elevating drone control and navigation through the seamless integration of natural language commands in real-world scenarios",
    "checked": true,
    "id": "65b63322cac4f4fd1b2dbc7155be69aed5f95b68",
    "semantic_title": "towards natural language-guided drones: geotext-1652 benchmark with spatial relation matching",
    "citation_count": 2,
    "authors": [
      "Meng Chu",
      "Zhedong Zheng*",
      "Wei Ji",
      "Tingyu Wang",
      "Tat-Seng Chua"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1750_ECCV_2024_paper.php": {
    "title": "BEAF: Observing BEfore-AFter Changes to Evaluate Hallucination in Vision-language Models",
    "volume": "main",
    "abstract": "Vision language models (VLMs) perceive the world through a combination of a visual encoder and a large language model (LLM). The visual encoder, pre-trained on large-scale vision-text datasets, provides zero-shot generalization to visual data, and the LLM endows its high reasoning ability to VLMs. It leads VLMs to achieve high performance on wide benchmarks without fine-tuning, exhibiting zero or few-shot capability. However, recent studies show that VLMs are vulnerable to hallucination. This undesirable behavior degrades reliability and credibility, thereby making users unable to fully trust the output from VLMs. To enhance trustworthiness and better tackle the hallucination of VLMs, we curate a new evaluation dataset, called the BEfore-AFter hallucination dataset (BEAF), and introduce new metrics: True Understanding (TU), IGnorance (IG), StuBbornness (SB), and InDecision (ID). Unlike prior works that focus only on constructing questions and answers, the key idea of our benchmark is to manipulate visual scene information by image editing models and to design the metrics based on scene changes. This allows us to clearly assess whether VLMs correctly understand a given scene by observing the ability to perceive changes. We also visualize image-wise object relationship by virtue of our two-axis view: vision and text. Upon evaluating VLMs with our dataset, we observed that our metrics reveal different aspects of VLM hallucination that have not been reported before. Project page: https://beafbench.github.io/",
    "checked": true,
    "id": "7bcd5c0b17560ee560aec903ea42487a1a54e5d9",
    "semantic_title": "beaf: observing before-after changes to evaluate hallucination in vision-language models",
    "citation_count": 3,
    "authors": [
      "Moon Ye-Bin",
      "Nam Hyeon-Woo",
      "Wonseok Choi",
      "Tae-Hyun Oh*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1751_ECCV_2024_paper.php": {
    "title": "Approaching Outside: Scaling Unsupervised 3D Object Detection from 2D Scene",
    "volume": "main",
    "abstract": "The unsupervised 3D object detection is to accurately detect objects in unstructured environments with no explicit supervisory signals. This task, given sparse LiDAR point clouds, often results in compromised performance for detecting distant or small objects due to the inherent sparsity and limited spatial resolution. In this paper, we are among the early attempts to integrate LiDAR data with 2D images for unsupervised 3D detection and introduce a new method, dubbed LiDAR-2D Self-paced Learning (LiSe). We argue that RGB images serve as a valuable complement to LiDAR data, offering precise 2D localization cues, particularly when scarce LiDAR points are available for certain objects. Considering the unique characteristics of both modalities, our framework devises a self-paced learning pipeline that incorporates adaptive sampling and weak model aggregation strategies. The adaptive sampling strategy dynamically tunes the distribution of pseudo labels during training, countering the tendency of models to overfit easily detected samples, such as nearby and large-sized objects. By doing so, it ensures a balanced learning trajectory across varying object scales and distances. The weak model aggregation component consolidates the strengths of models trained under different pseudo label distributions, culminating in a robust and powerful final model. Experimental evaluations validate the efficacy of our proposed LiSe method, manifesting significant improvements of +7.1% APBEV and +3.4% AP3D on nuScenes, and +8.3% APBEV and +7.4% AP3D on Lyft compared to existing techniques",
    "checked": true,
    "id": "22a4923324bc68d71de9430f1c96d8b2e5480d29",
    "semantic_title": "approaching outside: scaling unsupervised 3d object detection from 2d scene",
    "citation_count": 1,
    "authors": [
      "Ruiyang Zhang*",
      "Hu Zhang",
      "Hang Yu",
      "Zhedong Zheng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1752_ECCV_2024_paper.php": {
    "title": "DATENeRF: Depth-Aware Text-based Editing of NeRFs",
    "volume": "main",
    "abstract": "Recent diffusion models have demonstrated impressive capabilities for text-based 2D image editing. Applying similar ideas to edit a NeRF scene [?] remains challenging as editing 2D frames individually does not produce multiview-consistent results. We make the key observation that the geometry of a NeRF scene provides a way to unify these 2D edits. We leverage this geometry in depth-conditioned ControlNet [?] to improve the consistency of individual 2D image edits. Furthermore, we propose an inpainting scheme that uses the NeRF scene depth to propagate 2D edits across images while staying robust to errors and resampling issues. We demonstrate that this leads to more consistent, realistic and detailed editing results compared to previous state-of-the-art text-based NeRF editing methods",
    "checked": true,
    "id": "2cd2e2d0c689bdc0cdf511025d4c5c8b727e10f1",
    "semantic_title": "datenerf: depth-aware text-based editing of nerfs",
    "citation_count": 2,
    "authors": [
      "Sara Rojas Martinez*",
      "Julien Philip",
      "Kai Zhang",
      "Sai Bi",
      "Fujun Luan",
      "Bernard Ghanem",
      "Kalyan Sunkavalli"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1755_ECCV_2024_paper.php": {
    "title": "XPSR: Cross-modal Priors for Diffusion-based Image Super-Resolution",
    "volume": "main",
    "abstract": "Diffusion-based methods, endowed with a formidable generative prior, have received increasing attention in Image Super-Resolution (ISR) recently. However, as low-resolution (LR) images often undergo severe degradation, it is challenging for ISR models to perceive the semantic and degradation information, resulting in restoration images with incorrect content or unrealistic artifacts. To address these issues, we propose a Cross-modal Priors for Super-Resolution (XPSR) framework. Within XPSR, to acquire precise and comprehensive semantic conditions for the diffusion model, cutting-edge Multimodal Large Language Models (MLLMs) are utilized. To facilitate better fusion of cross-modal priors, a Semantic-Fusion Attention is raised. To distill semantic-preserved information instead of undesired degradations, a Degradation-Free Constraint is attached between LR and its high-resolution (HR) counterpart. Quantitative and qualitative results show that XPSR is capable of generating high-fidelity and high-realism images across synthetic and real-world datasets. Codes are released at https: //github.com/qyp2000/XPSR",
    "checked": true,
    "id": "4402330e00ded11f2472f7a70cebed022c8c4d49",
    "semantic_title": "xpsr: cross-modal priors for diffusion-based image super-resolution",
    "citation_count": 3,
    "authors": [
      "Qu Yunpeng*",
      "Kun Yuan",
      "Kai Zhao",
      "Qizhi Xie",
      "Jinhua Hao",
      "Ming Sun",
      "Chao Zhou"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1761_ECCV_2024_paper.php": {
    "title": "ABC Easy as 123: A Blind Counter for Exemplar-Free Multi-Class Class-agnostic Counting",
    "volume": "main",
    "abstract": "Class-agnostic counting methods enumerate objects of an arbitrary class, providing tremendous utility in many fields. Prior works have limited usefulness as they require either a set of examples of the type to be counted or that the query image contains only a single type of object. A significant factor in these shortcomings is the lack of a dataset to properly address counting in settings with more than one kind of object present. To address these issues, we propose the first Multi-class, Class-Agnostic Counting dataset (MCAC) and A Blind Counter (ABC123), a method that can count multiple types of objects simultaneously without using examples of type during training or inference. ABC123 introduces a new paradigm where instead of requiring exemplars to guide the enumeration, examples are found after the counting stage to help a user understand the generated outputs. We show that ABC123 outperforms contemporary methods on MCAC without needing human in-the-loop annotations. We also show that this performance transfers to FSC-147, the standard class-agnostic counting dataset. MCAC is available at MCAC.active.vision and ABC123 is available at ABC123.active.vision",
    "checked": true,
    "id": "97d62ca2cb2d0a1707a639251535ae5edd84bf58",
    "semantic_title": "abc easy as 123: a blind counter for exemplar-free multi-class class-agnostic counting",
    "citation_count": 0,
    "authors": [
      "Michael A Hobley*",
      "Victor Adrian Prisacariu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1766_ECCV_2024_paper.php": {
    "title": "Category Adaptation Meets Projected Distillation in Generalized Continual Category Discovery",
    "volume": "main",
    "abstract": "Generalized Continual Category Discovery (GCCD) tackles learning from sequentially arriving, partially labeled datasets while uncovering new categories. Traditional methods depend on feature distillation to prevent forgetting the old knowledge. However, this strategy restricts the model's ability to adapt and effectively distinguish new categories. To address this, we introduce a novel technique integrating a learnable projector with feature distillation, thus enhancing model adaptability without sacrificing past knowledge. The resulting distribution shift of the previously learned categories is mitigated with the auxiliary category adaptation network. We demonstrate that while each component offers modest benefits individually, their combination – dubbed CAMP (Category Adaptation Meets Projected distillation) – significantly improves the balance between learning new information and retaining old. CAMP exhibits superior performance across several GCCD and Class Incremental Learning scenarios. The code is available on Github",
    "checked": true,
    "id": "2d41c762d7acd024d9ea2063d7ec5b2a50fbfbd8",
    "semantic_title": "category adaptation meets projected distillation in generalized continual category discovery",
    "citation_count": 1,
    "authors": [
      "Grzegorz Rypeść*",
      "Daniel Marczak",
      "Sebastian Cygert",
      "Tomasz Trzcinski",
      "Bartlomiej Twardowski"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1769_ECCV_2024_paper.php": {
    "title": "LaRa: Efficient Large-Baseline Radiance Fields",
    "volume": "main",
    "abstract": "Radiance field methods have achieved photorealistic novel view synthesis and geometry reconstruction. But they are mostly applied in per-scene optimization or small-baseline settings. While several recent works investigate feed-forward reconstruction with large baselines by utilizing transformers, they all operate with a standard global attention mechanism and hence ignore the local nature of 3D reconstruction. We propose a method that unifies local and global reasoning in transformer layers, resulting in improved quality and faster convergence. Our model represents scenes as Gaussian Volumes and combines this with an image encoder and Group Attention Layers for efficient feed-forward reconstruction. Experimental results demonstrate that our model, trained for two days on four GPUs, demonstrates high fidelity in reconstructing 360◦ radiance fields, and robustness to zero-shot and out-of-domain testing",
    "checked": true,
    "id": "a21bbcffe55e44bba64e400a495aa1bbfa6023fe",
    "semantic_title": "lara: efficient large-baseline radiance fields",
    "citation_count": 5,
    "authors": [
      "Anpei Chen*",
      "Haofei Xu",
      "Stefano Esposito",
      "Siyu Tang",
      "Andreas Geiger"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1774_ECCV_2024_paper.php": {
    "title": "Bi-TTA: Bidirectional Test-Time Adapter for Remote Physiological Measurement",
    "volume": "main",
    "abstract": "Remote photoplethysmography (rPPG) is gaining prominence for its non-invasive approach to monitoring physiological signals using only cameras. Despite its promise, the adaptability of rPPG models to new, unseen domains is hindered due to the environmental sensitivity of physiological signals. To address this issue, we pioneer the Test-Time Adaptation (TTA) in rPPG, enabling the adaptation of pre-trained models to the target domain during inference, sidestepping the need for annotations or source data due to privacy considerations. Particularly, utilizing only the user's face video stream as the accessible target domain data, the rPPG model is adjusted by tuning on each single instance it encounters. However, 1) TTA algorithms are designed predominantly for classification tasks, ill-suited in regression tasks such as rPPG due to inadequate supervision. 2) Tuning pre-trained models in a single-instance manner introduces variability and instability, posing challenges to effectively filtering domain-relevant from domain-irrelevant features while simultaneously preserving the learned information. To overcome these challenges, we present Bi-TTA, a novel expert knowledge-based Bidirectional Test-Time Adapter framework. Specifically, leveraging two expert-knowledge priors for providing self-supervision, our Bi-TTA primarily comprises two modules: a prospective adaptation (PA) module using sharpness-aware minimization to eliminate domain-irrelevant noise, enhancing the stability and efficacy during the adaptation process, and a retrospective stabilization (RS) module to dynamically reinforce crucial learned model parameters, averting performance degradation caused by overfitting or catastrophic forgetting. To this end, we established a large-scale benchmark for rPPG tasks under TTA protocol, promoting advancements in both the rPPG and TTA fields. The experimental results demonstrate the significant superiority of our approach over the state-of-the-art (SoTA)",
    "checked": true,
    "id": "aee03883cf6a35892b1cba69ba705008b6c5b364",
    "semantic_title": "bi-tta: bidirectional test-time adapter for remote physiological measurement",
    "citation_count": 0,
    "authors": [
      "Haodong LI*",
      "Hao LU",
      "Yingcong Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1775_ECCV_2024_paper.php": {
    "title": "MAGR: Manifold-Aligned Graph Regularization for Continual Action Quality Assessment",
    "volume": "main",
    "abstract": "Action Quality Assessment (AQA) evaluates diverse skills but models struggle with non-stationary data. We propose Continual AQA (CAQA) to refine models using sparse new data. Feature replay preserves memory without storing raw inputs. However, the misalignment between static old features and the dynamically changing feature manifold causes severe catastrophic forgetting. To address this novel problem, we propose Manifold-Aligned Graph Regularization (MAGR), which first aligns deviated old features to the current feature manifold, ensuring representation consistency. It then constructs a graph jointly arranging old and new features aligned with quality scores. Experiments show MAGR outperforms recent strong baselines with up to 6.56%, 5.66%, 15.64%, and 9.05% correlation gains on the MTL-AQA, FineDiving, UNLV-Dive, and JDM-MSA split datasets, respectively. This validates MAGR for continual assessment challenges arising from non-stationary skill variations. Code is available at https://github.com/ZhouKanglei/MAGR CAQA",
    "checked": true,
    "id": "8fb8af2c119e35a52ec608b86462b0f4bced8e5e",
    "semantic_title": "magr: manifold-aligned graph regularization for continual action quality assessment",
    "citation_count": 0,
    "authors": [
      "Kanglei Zhou",
      "Liyuan Wang",
      "Xingxing Zhang",
      "Hubert P. H. Shum",
      "Frederick W. B. Li",
      "Jianguo Li",
      "Xiaohui Liang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1783_ECCV_2024_paper.php": {
    "title": "Grounding Language Models for Visual Entity Recognition",
    "volume": "main",
    "abstract": "We introduce , an Autoregressive model for Visual Entity Recognition. Our model extends an autoregressive Multimodal Large Language Model by employing retrieval augmented constrained generation. It mitigates low performance on out-of-domain entities while excelling in queries that require visual reasoning. Our method learns to distinguish similar entities within a vast label space by contrastively training on hard negative pairs in parallel with a sequence-to-sequence objective without an external retriever. During inference, a list of retrieved candidate answers explicitly guides language generation by removing invalid decoding paths. The proposed method achieves significant improvements across different dataset splits in the recently proposed benchmark with accuracy on the Entity split rising from 32.7% to 61.5%. It demonstrates superior performance on the and query splits by a substantial double-digit margin, while also preserving the ability to effectively transfer to other generic visual question answering benchmarks without further training",
    "checked": true,
    "id": "7ff441587afcd88860971210450cc1df7c772ff6",
    "semantic_title": "grounding language models for visual entity recognition",
    "citation_count": 4,
    "authors": [
      "Zilin Xiao*",
      "Ming Gong",
      "Paola Cascante-Bonilla",
      "Xingyao Zhang",
      "Jie Wu",
      "Vicente Ordonez*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1791_ECCV_2024_paper.php": {
    "title": "ELSE: Efficient Deep Neural Network Inference through Line-based Sparsity Exploration",
    "volume": "main",
    "abstract": "Brain-inspired computer architecture facilitates low-power, low-latency deep neural network inference for embedded AI applications. The hardware performance crucially hinges on the quantity of non-zero activations (i.e., events) during inference. Thus, we propose a novel event suppression method, dubbed ELSE, which enhances inference Efficiency via Line-based Sparsity Exploration. Specifically, it exploits spatial correlation between adjacent lines in activation maps to reduce network events. ELSE reduces event-triggered computations by 3.14∼6.49× for object detection and by 2.43∼5.75× for pose estimation across various network architectures compared to conventional processing. Additionally, we show that combining ELSE with other event suppression methods can either significantly enhance computation savings for spatial suppression or reduce state memory footprint by > 2× for temporal suppression. The latter alleviates the challenge of temporal execution exceeding the resource constraints of real-world embedded platforms. These results highlight ELSE's significant event suppression ability and its capacity to deliver complementary performance enhancements for SOTA methods",
    "checked": false,
    "id": "fbbbf694ad6a8d558d9ca5d48c221fe0eac79b45",
    "semantic_title": "adaptive sparse deep neural network inference on resource-constrained cost-efficient gpus",
    "citation_count": 2,
    "authors": [
      "Zeqi Zhu*",
      "Alberto Garcia-Ortiz",
      "Luc Waeijen",
      "Egor Bondarev",
      "Arash Pourtaherian",
      "Orlando Moreira"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1793_ECCV_2024_paper.php": {
    "title": "DiffusionDepth: Diffusion Denoising Approach for Monocular Depth Estimation",
    "volume": "main",
    "abstract": "Monocular depth estimation is a challenging task that predicts the pixel-wise depth from a single 2D image. Current methods typically model this problem as a regression or classification task. We propose DiffusionDepth, a new approach that reformulates monocular depth estimation as a denoising diffusion process. It learns an iterative denoising process to ‘denoise' random depth distribution into a depth map with the guidance of monocular visual conditions. The process is performed in the latent space encoded by a dedicated depth encoder and decoder. Instead of diffusing ground truth (GT) depth, the model learns to reverse the process of diffusing the refined depth of itself into random depth distribution. This self-diffusion formulation overcomes the difficulty of applying generative models to sparse GT depth scenarios. The proposed approach benefits this task by refining depth estimation step by step, which is superior for generating accurate and highly detailed depth maps. Experimental results from both offline and online evaluations using the KITTI and NYU-Depth-V2 datasets indicate that the proposed method can achieve state-of-the-art performance in both indoor and outdoor settings while maintaining a reasonable inference time. The codes 1 are available online. 1 https://github.com/duanyiqun/DiffusionDepth",
    "checked": true,
    "id": "3091ce3cdab52801b95d456c72639aa3be5b9d67",
    "semantic_title": "diffusiondepth: diffusion denoising approach for monocular depth estimation",
    "citation_count": 38,
    "authors": [
      "Yiqun Duan*",
      "Xianda Guo*",
      "Zheng Zhu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1795_ECCV_2024_paper.php": {
    "title": "DC-Solver: Improving Predictor-Corrector Diffusion Sampler via Dynamic Compensation",
    "volume": "main",
    "abstract": "Diffusion probabilistic models (DPMs) have shown remarkable performance in visual synthesis but are computationally expensive due to the need for multiple evaluations during the sampling. Recent predictor-corrector diffusion samplers have significantly reduced the required number of function evaluations (NFE), but inherently suffer from a misalignment issue caused by the extra corrector step, especially with a large classifier-free guidance scale (CFG). In this paper, we introduce a new fast DPM sampler called DC-Solver, which leverages dynamic compensation (DC) to mitigate the misalignment of the predictor-corrector samplers. The dynamic compensation is controlled by compensation ratios that are adaptive to the sampling steps and can be optimized on only 10 datapoints by pushing the sampling trajectory toward a ground truth trajectory. We further propose a cascade polynomial regression (CPR) which can instantly predict the compensation ratios on unseen sampling configurations. Additionally, we find that the proposed dynamic compensation can also serve as a plug-and-play module to boost the performance of predictor-only samplers. Extensive experiments on both unconditional sampling and conditional sampling demonstrate that our DC-Solver can consistently improve the sampling quality over previous methods on different DPMs with a wide range of resolutions up to 1024×1024. Notably, we achieve 10.38 FID (NFE=5) on unconditional FFHQ and 0.394 MSE (NFE=5, CFG=7.5) on Stable-Diffusion-2.1. Code is available at https: //github.com/wl-zhao/DC-Solver",
    "checked": true,
    "id": "68a7e95c4ba1c53c67cf116e32d6d22c897a62f6",
    "semantic_title": "dc-solver: improving predictor-corrector diffusion sampler via dynamic compensation",
    "citation_count": 0,
    "authors": [
      "Wenliang Zhao",
      "Haolin Wang",
      "Jie Zhou",
      "Jiwen Lu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1796_ECCV_2024_paper.php": {
    "title": "TRAM: Global Trajectory and Motion of 3D Humans from in-the-wild Videos",
    "volume": "main",
    "abstract": "We propose TRAM, a two-stage method to reconstruct a human's global trajectory and motion from in-the-wild videos. TRAM robustifies SLAM to recover the camera motion in the presence of dynamic humans and uses the scene background to derive the motion scale. Using the recovered camera as a metric-scale reference frame, we introduce a video transformer model (VIMO) to regress the kinematic body motion of a human. By composing the two motions, we achieve accurate recovery of 3D humans in the world space, reducing global motion errors by a large margin from prior work. https://yufu-wang.github. io/tram4d/",
    "checked": true,
    "id": "37ce0ce4169badcda33995c45fefad576b65aec9",
    "semantic_title": "tram: global trajectory and motion of 3d humans from in-the-wild videos",
    "citation_count": 3,
    "authors": [
      "Yufu Wang*",
      "Ziyun Wang",
      "Lingjie Liu",
      "Kostas Daniilidis"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1812_ECCV_2024_paper.php": {
    "title": "MutDet: Mutually Optimizing Pre-training for Remote Sensing Object Detection",
    "volume": "main",
    "abstract": "Detection pre-training methods for the DETR series detector have been extensively studied in natural scenes, e.g., DETReg. However, the detection pre-training remains unexplored in remote sensing scenes. In existing pre-training methods, alignment between object embeddings extracted from a pre-trained backbone and detector features is significant. However, due to differences in feature extraction methods, a pronounced feature discrepancy still exists and hinders the pre-training performance. The remote sensing images with complex environments and more densely distributed objects exacerbate the discrepancy. In this work, we propose a novel Mutually optimizing pre-training framework for remote sensing object Detection, dubbed as MutDet. In MutDet, we propose a systemic solution against this challenge. Firstly, we propose a mutual enhancement module, which fuses the object embeddings and detector features bidirectionally in the last encoder layer, enhancing their information interaction. Secondly, contrastive alignment loss is employed to guide this alignment process softly and simultaneously enhances detector features' discriminativity. Finally, we design an auxiliary siamese head to mitigate the task gap arising from the introduction of enhancement module. Comprehensive experiments on various settings show new state-of-the-art transfer performance. The improvement is particularly pronounced when data quantity is limited. When using 10 % of the DIOR-R data, MutDet improves DetReg by 6.1% in AP50 . Codes and models are available at: https://github.com/floatingstarZ/MutDet",
    "checked": true,
    "id": "8d6b89a4ec679086d7933acd77b13e0d82a67354",
    "semantic_title": "mutdet: mutually optimizing pre-training for remote sensing object detection",
    "citation_count": 0,
    "authors": [
      "Ziyue Huang",
      "Yongchao Feng",
      "Qingjie Liu*",
      "Yunhong Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1818_ECCV_2024_paper.php": {
    "title": "Self-Supervised Video Copy Localization with Regional Token Representation",
    "volume": "main",
    "abstract": "The task of video copy localization aims at finding the start and end timestamps of all copied segments within a pair of untrimmed videos. Recent approaches usually extract frame-level features and generate a frame-to-frame similarity map for the video pair. Learned detectors are used to identify distinctive patterns in the similarity map to localize the copied segments. There are two major limitations associated with these methods. First, they often rely on a single feature for each frame, which is inadequate in capturing local information for typical scenarios in video copy editing, such as picture-in-picture cases. Second, the training of the detectors requires a significant amount of human annotated data, which is highly expensive and time-consuming to acquire. In this paper, we propose a self-supervised video copy localization framework to tackle these issues. We incorporate a Regional Token into the Vision Transformer, which learns to focus on local regions within each frame using an asymmetric training procedure. A novel strategy that leverages the Transitivity Property is proposed to generate copied video pairs automatically, which facilitates the training of the detector. Extensive experiments and visualizations demonstrate the effectiveness of the proposed approach, which is able to outperform the state-of-the-art without using any human annotated data",
    "checked": false,
    "id": "5e2fabb5dd520c2e9be8eb67514b1c02715b373f",
    "semantic_title": "ptan: principal token-aware adjacent network for compositional temporal grounding",
    "citation_count": 0,
    "authors": [
      "Minlong Lu*",
      "Yichen Lu",
      "Siwei Nie",
      "Xudong Yang",
      "Xiaobo Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1824_ECCV_2024_paper.php": {
    "title": "Enhancing Perceptual Quality in Video Super-Resolution through Temporally-Consistent Detail Synthesis using Diffusion Models",
    "volume": "main",
    "abstract": "In this paper, we address the problem of enhancing perceptual quality in video super-resolution (VSR) using Diffusion Models (DMs) while ensuring temporal consistency among frames. We present StableVSR, a VSR method based on DMs that can significantly enhance the perceptual quality of upscaled videos by synthesizing realistic and temporally-consistent details. We introduce the Temporal Conditioning Module (TCM) into a pre-trained DM for single image super-resolution to turn it into a VSR method. TCM uses the novel Temporal Texture Guidance, which provides it with spatially-aligned and detail-rich texture information synthesized in adjacent frames. This guides the generative process of the current frame toward high-quality and temporally-consistent results. In addition, we introduce the novel Frame-wise Bidirectional Sampling strategy to encourage the use of information from past to future and vice-versa. This strategy improves the perceptual quality of the results and the temporal consistency across frames. We demonstrate the effectiveness of StableVSR in enhancing the perceptual quality of upscaled videos while achieving better temporal consistency compared to existing state-of-the-art methods for VSR. The project page is available at https://github.com/claudiom4sir/StableVSR",
    "checked": true,
    "id": "9447a78a9bc75c1c6a4cd4d38621466def4ff760",
    "semantic_title": "enhancing perceptual quality in video super-resolution through temporally-consistent detail synthesis using diffusion models",
    "citation_count": 2,
    "authors": [
      "Claudio Rota*",
      "Marco Buzzelli",
      "Joost van de Weijer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1833_ECCV_2024_paper.php": {
    "title": "RoGUENeRF: A Robust Geometry-Consistent Universal Enhancer for NeRF",
    "volume": "main",
    "abstract": "Recent advances in neural rendering have enabled highly photorealistic 3D scene reconstruction and novel view synthesis. Despite this progress, current state-of-the-art methods struggle to reconstruct high frequency detail, due to factors such as a low-frequency bias of radiance fields and inaccurate camera calibration. One approach to mitigate this issue is to enhance images post-rendering. 2D enhancers can be pre-trained to recover some detail but are agnostic to scene geometry and do not easily generalize to new distributions of image degradation. Conversely, existing 3D enhancers are able to transfer detail from nearby training images in a generalizable manner, but suffer from inaccurate camera calibration and can propagate errors from the geometry into rendered images. We propose a neural rendering enhancer, , which exploits the best of both paradigms. Our method is pre-trained to learn a general enhancer while also leveraging information from nearby training images via robust 3D alignment and geometry-aware fusion. Our approach restores high-frequency textures while maintaining geometric consistency and is also robust to inaccurate camera calibration. We show that substantially enhances the rendering quality of a wide range of neural rendering baselines, e.g. improving the PSNR of MipNeRF360 by 0.63dB and Nerfacto by 1.34dB on the real world 360v2 dataset. Project page: https://sib1.github.io/projects/roguenerf/",
    "checked": true,
    "id": "c7430abe3868c9b4c7a2fb3c61c9c04e1f70fee4",
    "semantic_title": "roguenerf: a robust geometry-consistent universal enhancer for nerf",
    "citation_count": 2,
    "authors": [
      "Sibi Catley-Chandar*",
      "Richard Shaw",
      "Gregory Slabaugh",
      "Eduardo Pérez Pellitero"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1834_ECCV_2024_paper.php": {
    "title": "Bridging the Gap: Studio-like Avatar Creation from a Monocular Phone Capture",
    "volume": "main",
    "abstract": "Creating photorealistic avatars for individuals traditionally involves extensive capture sessions with complex and expensive studio devices like the LightStage system. While recent strides in neural representations have enabled the generation of photorealistic and animatable 3D avatars from quick phone scans, they have the capture-time lighting baked-in, lack facial details and have missing regions in areas such as the back of the ears. Thus, they lag in quality compared to studio-captured avatars. In this paper, we propose a method that bridges this gap by generating studio-like illuminated texture maps from short, monocular phone captures. We do this by parameterizing the phone texture maps using the W + space of a StyleGAN2, enabling near-perfect reconstruction. Then, we finetune a StyleGAN2 by sampling in the W + parameterized space using a very small set of studio-captured textures as an adversarial training signal. To further enhance the realism and accuracy of facial details, we super-resolve the output of the StyleGAN2 using carefully designed diffusion model that is guided by image gradients of the phone-captured texture map. Once trained, our method excels at producing studio-like facial texture maps from casual monocular smartphone videos. Demonstrating its capabilities, we showcase the generation of photorealistic, uniformly lit, complete avatars from monocular phone captures",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "ShahRukh Athar*",
      "Shunsuke Saito",
      "Stanislav Pidhorskyi",
      "Zhengyu Yang",
      "Chen Cao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1838_ECCV_2024_paper.php": {
    "title": "ControlLLM: Augment Language Models with Tools by Searching on Graphs",
    "volume": "main",
    "abstract": "We present ControlLLM, a novel framework that enables large language models (LLMs) to utilize multi-modal tools for solving complex real-world tasks. Despite the remarkable performance of LLMs, they still struggle with tool invocation due to ambiguous user prompts, inaccurate tool selection and mismatched input arguments. To overcome these challenges, our framework comprises three key components: (1) a task decomposer that breaks down a complex task into clear subtasks with well-defined inputs and outputs; (2) a Thoughts-on-Graph (ToG) paradigm that searches the optimal solution path on a pre-built tool graph, which specifies the parameter and dependency relations among different tools; and (3) an execution engine with a rich toolbox that interprets the solution path and runs the tools efficiently on different computational devices. We evaluate our framework on diverse tasks involving image, audio, and video processing, demonstrating its superior accuracy, efficiency, and versatility compared to existing methods. The code is available at https://github.com/OpenGVLab/ ControlLLM",
    "checked": true,
    "id": "288e7224d53d68669eb67f2496e068dc965c639e",
    "semantic_title": "controlllm: augment language models with tools by searching on graphs",
    "citation_count": 24,
    "authors": [
      "Zhaoyang Liu",
      "Zeqiang Lai",
      "Zhangwei Gao",
      "erfei cui",
      "Ziheng Li",
      "Xizhou Zhu",
      "Lewei Lu",
      "Qifeng Chen*",
      "Yu Qiao",
      "Jifeng Dai",
      "Wenhai Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1843_ECCV_2024_paper.php": {
    "title": "UniTraj: A Unified Framework for Scalable Vehicle Trajectory Prediction",
    "volume": "main",
    "abstract": "Vehicle trajectory prediction has increasingly relied on data-driven solutions, but their ability to scale to different data domains and the impact of larger dataset sizes on their generalization remain under-explored. While these questions can be studied by employing multiple datasets, it is challenging due to several discrepancies, e.g., in data formats, map resolution, and semantic annotation types. To address these challenges, we introduce UniTraj, a comprehensive framework that unifies various datasets, models, and evaluation criteria, presenting new opportunities for the vehicle trajectory prediction field. In particular, using UniTraj, we conduct extensive experiments and find that model performance significantly drops when transferred to other datasets. However, enlarging data size and diversity can substantially improve performance, leading to a new state-of-the-art result for the nuScenes dataset. We provide insights into dataset characteristics to explain these findings. The code can be found here: https://github.com/vita-epfl/UniTraj",
    "checked": true,
    "id": "064ce398af766bc33ac830ef3d60c8979868ca8a",
    "semantic_title": "unitraj: a unified framework for scalable vehicle trajectory prediction",
    "citation_count": 6,
    "authors": [
      "Lan Feng",
      "Mohammadhossein Bahari*",
      "Kaouther Messaoud",
      "Eloi Zablocki",
      "Matthieu Cord",
      "Alexandre Alahi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1847_ECCV_2024_paper.php": {
    "title": "DreamDissector: Learning Disentangled Text-to-3D Generation from 2D Diffusion Priors",
    "volume": "main",
    "abstract": "Text-to-3D generation has recently seen significant progress. To enhance its practicality in real-world applications, it is crucial to generate multiple independent objects with interactions, similar to layer-compositing in 2D image editing. However, existing text-to-3D methods struggle with this task, as they are designed to generate either non-independent objects or independent objects lacking spatially plausible interactions. Addressing this, we propose DreamDissector, a text-to-3D method capable of generating multiple independent objects with interactions. DreamDissector accepts a multi-object text-to-3D NeRF as input and produces independent textured meshes. To achieve this, we introduce the Neural Category Field (NeCF) for disentangling the input NeRF. Additionally, we present the Category Score Distillation Sampling (CSDS), facilitated by a Deep Concept Mining (DCM) module, to tackle the concept gap issue in diffusion models. By leveraging NeCF and CSDS, we can effectively derive sub-NeRFs from the original scene. Further refinement enhances geometry and texture. Our experimental results validate the effectiveness of DreamDissector, providing users with novel means to control 3D synthesis at the object level and potentially opening avenues for various creative applications in the future",
    "checked": true,
    "id": "f8f030962cb5f04673ce7d070f49089819291efb",
    "semantic_title": "dreamdissector: learning disentangled text-to-3d generation from 2d diffusion priors",
    "citation_count": 1,
    "authors": [
      "Zizheng Yan*",
      "Jiapeng Zhou",
      "Fanpeng Meng",
      "Yushuang Wu",
      "Lingteng Qiu",
      "Zisheng Ye",
      "Shuguang Cui",
      "Guanying CHEN",
      "Xiaoguang Han*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1860_ECCV_2024_paper.php": {
    "title": "Vamos: Versatile Action Models for Video Understanding",
    "volume": "main",
    "abstract": "What makes good representations for video understanding, such as anticipating future activities, or answering video-conditioned questions? While earlier approaches focus on end-to-end learning directly from video pixels, we propose to revisit text-based representations, such as general-purpose video captions, which are interpretable and can be directly consumed by large language models (LLMs). Intuitively, different video understanding tasks may require representations that are complementary and at different granularity. To this end, we propose versatile action models (Vamos), a learning framework powered by a large language model as the \"reasoner\", and can flexibly leverage visual embedding and free-form text descriptions as its input. To interpret the important text evidence for question answering, we generalize the concept bottleneck model to work with tokens and nonlinear models, which uses hard attention to select a small subset of tokens from the free-form text as inputs to the LLM reasoner. We evaluate Vamos on five complementary benchmarks, Ego4D, NeXT-QA, IntentQA, Spacewalk-18, and EgoSchema, on its capability to model temporal dynamics, encode visual history, and perform reasoning. Surprisingly, we observe that text-based representations consistently achieve competitive performance on all benchmarks, and that visual embeddings provide marginal or no performance improvement, demonstrating the effectiveness of text-based video representation in the LLM era. We also demonstrate that our token bottleneck model is able to select relevant evidence from free-form text, support test-time intervention, and achieves nearly 5 times inference speedup while keeping a competitive question answering performance. Code and models are publicly released at https://brown-palm.github.io/Vamos/",
    "checked": true,
    "id": "1e838bd5fa2f5bca805493d0f672d03514b36869",
    "semantic_title": "vamos: versatile action models for video understanding",
    "citation_count": 12,
    "authors": [
      "Shijie Wang*",
      "Qi Zhao",
      "Minh Quan Do",
      "Nakul Agarwal",
      "Kwonjoon Lee",
      "Chen Sun"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1872_ECCV_2024_paper.php": {
    "title": "Prioritized Semantic Learning for Zero-shot Instance Navigation",
    "volume": "main",
    "abstract": "We study zero-shot instance navigation, in which the agent navigates to a specific object without using object annotations for training. Previous object navigation approaches apply the image-goal navigation () task (go to the location of an image) for pretraining, and transfer the agent to achieve object goals using a vision-language model. However, these approaches lead to issues of semantic neglect, where the model fails to learn meaningful semantic alignments. In this paper, we propose a () method to improve the semantic understanding ability of navigation agents. Specifically, a semantic-enhanced PSL agent is proposed and a strategy is introduced to select goal images that exhibit clear semantic supervision and relax the reward function from strict exact view matching. At inference time, a scheme is designed to preserve the same granularity level of the goal-semantic as training. Furthermore, for the popular HM3D environment, we present an Instance Navigation () task that requires going to a specific object instance with detailed descriptions, as opposed to the Object Navigation () task where the goal is defined merely by the object category. Our PSL agent outperforms the previous state-of-the-art by 66% on zero-shot in terms of success rate and is also superior on the new task. Code will be released at https://github.com/XinyuSun/PSL-InstanceNav",
    "checked": true,
    "id": "33052588852fee3eff541a55b481cf776c091480",
    "semantic_title": "prioritized semantic learning for zero-shot instance navigation",
    "citation_count": 1,
    "authors": [
      "xinyu sun*",
      "Lizhao Liu",
      "Hongyan Zhi",
      "Ronghe Qiu",
      "Junwei Liang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1878_ECCV_2024_paper.php": {
    "title": "RoadPainter: Points Are Ideal Navigators for Topology transformER",
    "volume": "main",
    "abstract": "Topology reasoning aims to provide a precise understanding of road scenes, enabling autonomous systems to identify safe and efficient routes. In this paper, we present RoadPainter, an innovative approach for detecting and reasoning the topology of lane centerlines using multi-view images. The core concept behind RoadPainter is to extract a set of points from each centerline mask to improve the accuracy of centerline prediction. We start by implementing a transformer decoder that integrates a hybrid attention mechanism and a real-virtual separation strategy to predict coarse lane centerlines and establish topological associations. Then, we generate centerline instance masks guided by the centerline points from the transformer decoder. Moreover, we derive an additional set of points from each mask and combine them with previously detected centerline points for further refinement. Additionally, we introduce an optional module that incorporates a Standard Definition (SD) map to further optimize centerline detection and enhance topological reasoning performance. Experimental evaluations on the OpenLane-V2 dataset demonstrate the state-of-the-art performance of RoadPainter",
    "checked": true,
    "id": "ed78dc799a75f298c71db8ae9bf54286e97f0b60",
    "semantic_title": "roadpainter: points are ideal navigators for topology transformer",
    "citation_count": 1,
    "authors": [
      "Zhongxing Ma",
      "Liang Shuang",
      "Yongkun Wen",
      "Weixin Lu",
      "Guowei Wan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1879_ECCV_2024_paper.php": {
    "title": "FouriScale: A Frequency Perspective on Training-Free High-Resolution Image Synthesis",
    "volume": "main",
    "abstract": "In this study, we delve into the generation of high-resolution images from pre-trained diffusion models, addressing persistent challenges, such as repetitive patterns and structural distortions, that emerge when models are applied beyond their trained resolutions. To address this issue, we introduce an innovative, training-free approach FouriScale from the perspective of frequency domain analysis. We replace the original convolutional layers in pre-trained diffusion models by incorporating a dilation technique along with a low-pass operation, intending to achieve structural consistency and scale consistency across resolutions, respectively. Further enhanced by a padding-then-crop strategy, our method can flexibly handle text-to-image generation of various aspect ratios. By using the FouriScale as guidance, our method successfully balances the structural integrity and fidelity of generated images, achieving arbitrary-size, high-resolution, and high-quality generation. With its simplicity and compatibility, our method can provide valuable insights for future explorations into the synthesis of ultra-high-resolution images. The source code is available at https://github.com/LeonHLJ/FouriScale",
    "checked": true,
    "id": "5c80f5947fed7c5c2b58a4aebfb7c3be4b3ce35d",
    "semantic_title": "fouriscale: a frequency perspective on training-free high-resolution image synthesis",
    "citation_count": 12,
    "authors": [
      "Linjiang Huang*",
      "Rongyao Fang",
      "Aiping Zhang",
      "Guanglu Song",
      "Si Liu",
      "Yu Liu",
      "Hongsheng Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1889_ECCV_2024_paper.php": {
    "title": "Can OOD Object Detectors Learn from Foundation Models?",
    "volume": "main",
    "abstract": "Out-of-distribution (OOD) object detection is a challenging task due to the absence of open-set OOD data. Inspired by recent advancements in text-to-image generative models, such as Stable Diffusion, we study the potential of generative models trained on large-scale open-set data to synthesize OOD samples, thereby enhancing OOD object detection. We introduce SyncOOD, a simple data curation method that capitalizes on the capabilities of large foundation models to automatically extract meaningful OOD data from text-to-image generative models. This offers the model access to open-world knowledge encapsulated within off-the-shelf foundation models. The synthetic OOD samples are then employed to augment the training of a lightweight, plug-and-play OOD detector, thus effectively optimizing the in-distribution (ID)/OOD decision boundaries. Extensive experiments across multiple benchmarks demonstrate that SyncOOD significantly outperforms existing methods, establishing new state-of-the-art performance with minimal synthetic data usage. The project is available at https://github.com/CVMI-Lab/SyncOOD",
    "checked": true,
    "id": "0b9064738cdf433f16266de25f3aa8c2db949966",
    "semantic_title": "can ood object detectors learn from foundation models?",
    "citation_count": 0,
    "authors": [
      "Jiahui Liu*",
      "Xin Wen",
      "Shizhen Zhao",
      "Yingxian Chen",
      "Xiaojuan Qi*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1890_ECCV_2024_paper.php": {
    "title": "Videoshop: Localized Semantic Video Editing with Noise-Extrapolated Diffusion Inversion",
    "volume": "main",
    "abstract": "We introduce , a training-free video editing algorithm for localized semantic edits. allows users to use any editing software, including Photoshop and generative inpainting, to modify the first frame; it automatically propagates those changes, with semantic, spatial, and temporally consistent motion, to the remaining frames. Unlike existing methods that enable edits only through imprecise textual instructions, allows users to add or remove objects, semantically change objects, insert stock photos into videos, etc. with fine-grained control over locations and appearance. We achieve this through image-based video editing by inverting latents with noise extrapolation, from which we generate videos conditioned on the edited image. produces higher quality edits against 6 baselines on 2 editing benchmarks using 10 evaluation metrics",
    "checked": true,
    "id": "92dd157d7f7cde3e2919281feeba18ab20d52813",
    "semantic_title": "videoshop: localized semantic video editing with noise-extrapolated diffusion inversion",
    "citation_count": 1,
    "authors": [
      "Xiang Fan*",
      "Anand Bhattad",
      "Ranjay Krishna"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1894_ECCV_2024_paper.php": {
    "title": "MERLiN: Single-Shot Material Estimation and Relighting for Photometric Stereo",
    "volume": "main",
    "abstract": "Photometric stereo typically demands intricate data acquisition setups involving multiple light sources to recover surface normals accurately. In this paper, we propose MERLiN, an attention-based hourglass network that integrates single image-based inverse rendering and relighting within a single unified framework. We evaluate the performance of photometric stereo methods using these relit images and demonstrate how they can circumvent the underlying challenge of complex data acquisition. Our physically-based model is trained on a large synthetic dataset containing complex shapes with spatially varying BRDF and is designed to handle indirect illumination effects to improve material reconstruction and relighting. Through extensive qualitative and quantitative evaluation, we demonstrate that the proposed framework generalizes well to real-world images, achieving high-quality shape, material estimation, and relighting. We assess these synthetically relit images over photometric stereo benchmark methods for their physical correctness and resulting normal estimation accuracy, paving the way towards single-shot photometric stereo through physically-based relighting. This work allows us to address the single image-based inverse rendering problem holistically, applying well to both synthetic and real data and taking a step towards mitigating the challenge of data acquisition in photometric stereo",
    "checked": true,
    "id": "f92517678a17a352b4aeabe2ade6606817e9e36e",
    "semantic_title": "merlin: single-shot material estimation and relighting for photometric stereo",
    "citation_count": 0,
    "authors": [
      "Ashish Tiwari*",
      "Satoshi Ikehata",
      "Shanmuganathan Raman"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1900_ECCV_2024_paper.php": {
    "title": "Boosting 3D Single Object Tracking with 2D Matching Distillation and 3D Pre-training",
    "volume": "main",
    "abstract": "3D single object tracking (SOT) is an essential task in autonomous driving and robotics. However, learning robust 3D SOT trackers remains challenging due to the limited category-specific point cloud data and the inherent sparsity and incompleteness of LiDAR scans. To tackle these issues, we propose a unified 3D SOT framework that leverages 3D generative pre-training and learns robust 3D matching abilities from 2D pre-trained foundation trackers. Our framework features a consistent target-matching architecture with the widely used 2D trackers, facilitating the transfer of 2D matching knowledge. Specifically, we first propose a lightweight Target-Aware Projection (TAP) module, allowing the pre-trained 2D tracker to work well on the projected point clouds without further fine-tuning. We then propose a novel IoU-guided matching-distillation framework that utilizes the powerful 2D pre-trained trackers to guide 3D matching learning in the 3D tracker, i.e., the 3D template-to-search matching should be consistent with its corresponding 2D template-to-search matching obtained from 2D pre-trained trackers. Our designs are applied to two mainstream 3D SOT frameworks: memory-less Siamese and contextual memory-based approaches, which are respectively named SiamDisst and MemDisst. Extensive experiments show that SiamDisst and MemDisst achieve state-of-the-art performance on KITTI, Waymo Open Dataset and nuScenes benchmarks, while running at above real-time speed of 25 and 90 FPS on a RTX3090 GPU",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiangqiang Wu",
      "Yan Xia*",
      "Jia Wan",
      "Antoni Chan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1909_ECCV_2024_paper.php": {
    "title": "Diffusion-Based Image-to-Image Translation by Noise Correction via Prompt Interpolation",
    "volume": "main",
    "abstract": "We propose a simple but effective training-free approach tailored to diffusion-based image-to-image translation. Our approach revises the original noise prediction network of a pretrained diffusion model by introducing a noise correction term. We formulate the noise correction term as the difference between two noise predictions; one is computed from the denoising network with a progressive interpolation of the source and target prompt embeddings, while the other is the noise prediction with the source prompt embedding. The final noise prediction network is given by a linear combination of the standard denoising term and the noise correction term, where the former is designed to reconstruct must-be-preserved regions while the latter aims to effectively edit regions of interest relevant to the target prompt. Our approach can be easily incorporated into existing image-to-image translation methods based on diffusion models. Extensive experiments verify that the proposed technique achieves outstanding performance with low latency and consistently improves existing frameworks when combined with them",
    "checked": true,
    "id": "cc20f4e61f1a10fd12b360edcca2216b84e32154",
    "semantic_title": "diffusion-based image-to-image translation by noise correction via prompt interpolation",
    "citation_count": 0,
    "authors": [
      "Junsung Lee",
      "Minsoo Kang",
      "Bohyung Han*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1910_ECCV_2024_paper.php": {
    "title": "Real-data-driven 2000 FPS Color Video from Mosaicked Chromatic Spikes",
    "volume": "main",
    "abstract": "The spike camera continuously records scene radiance with high-speed, high dynamic range, and low data redundancy properties, as a promising replacement for frame-based high-speed cameras. Previous methods for reconstructing color videos from monochromatic spikes are constrained in capturing full-temporal color information due to their reliance on compensating colors from low-speed RGB frames. Applying a Bayer-pattern color filter array to the spike sensor yields mosaicked chromatic spikes, which complicates noise distribution in high-speed conditions. By validating that the noise of short-term frames follows a zero-mean distribution, we leverage this hypothesis to develop a self-supervised denoising module trained exclusively on real-world data. Although noise is reduced in short-term frames, the long-term accumulation of incident photons is still necessary to construct HDR frames. Therefore, we introduce a progressive warping module to generate pseudo long-term exposure frames. This approach effectively mitigates motion blur artifacts in high-speed conditions. Integrating these modules forms a real-data-driven reconstruction method for mosaicked chromatic spikes. Extensive experiments conducted on both synthetic and real-world data demonstrate that our approach is effective in reconstructing 2000FPS color HDR videos with significantly reduced noise and motion blur compared to existing methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siqi Yang*",
      "Zhaojun Huang",
      "Yakun Chang",
      "Bin Fan",
      "Zhaofei Yu",
      "Boxin Shi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1918_ECCV_2024_paper.php": {
    "title": "Brain-ID: Learning Contrast-agnostic Anatomical Representations for Brain Imaging",
    "volume": "main",
    "abstract": "Recent learning-based approaches have made astonishing advances in calibrated medical imaging like computerized tomography (CT). Yet, they struggle to generalize in uncalibrated modalities – notably magnetic resonance (MR) imaging, where performance is highly sensitive to the differences in MR contrast, resolution, and orientation. This prevents broad applicability to diverse real-world clinical protocols. We introduce Brain-ID, an anatomical representation learning model for brain imaging. With the proposed \"mild-to-severe\" intra-subject generation, Brain-ID is robust to the subject-specific brain anatomy regardless of the appearance of acquired images. Trained entirely on synthetic inputs, Brain-ID readily adapts to various downstream tasks through one layer. We present new metrics to validate the intra/inter-subject robustness of Brain-ID features, and evaluate their performance on four downstream applications, covering contrast-independent (anatomy reconstruction, brain segmentation), and contrast-dependent (super-resolution, bias field estimation) tasks (showcase). Extensive experiments on six public datasets demonstrate that Brain-ID achieves state-of-the-art performance in all tasks on different MR contrasts and CT, and more importantly, preserves its performance on low-resolution and small datasets. Code is available at https://github.com/peirong26/Brain-ID",
    "checked": true,
    "id": "f63a4dcc4f1a5bf77c7ec7afdb6c9109af46d883",
    "semantic_title": "brain-id: learning contrast-agnostic anatomical representations for brain imaging",
    "citation_count": 3,
    "authors": [
      "Peirong Liu*",
      "Oula Puonti",
      "Xiaoling Hu",
      "Daniel C. Alexander",
      "Juan E. Iglesias"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1921_ECCV_2024_paper.php": {
    "title": "TTT-MIM: Test-Time Training with Masked Image Modeling for Denoising Distribution Shifts",
    "volume": "main",
    "abstract": "Neural networks trained end-to-end give state-of-the-art performance for image denoising. However, when applied to an image outside of the training distribution, the performance often degrades significantly. In this work, we propose a test-time training (TTT) method based on masked image modeling (MIM) to improve denoising performance for out-of-distribution images. The method, termed TTT-MIM, consists of a training stage and a test time adaptation stage. At training, we minimize a standard supervised loss and a self-supervised loss aimed at reconstructing masked image patches. At test-time, we minimize a self-supervised loss to fine-tune the network to adapt to a single noisy image. Experiments show that our method can improve performance under natural distribution shifts, in particular it adapts well to real-world camera and microscope noise. A competitor to our method of training and finetuning is to use a zero-shot denoiser that does not rely on training data. However, compared to state-of-the-art zero-shot denoisers, our method shows superior performance, and is much faster, suggesting that training and finetuning on the test instance is a more efficient approach to image denoising than zero-shot methods in setups where little to no data is available. Our GitHub page is: https://github.com/MLI-lab/TTT Denoising",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youssef Mansour*",
      "Xuyang Zhong",
      "Serdar Caglar",
      "Reinhard Heckel"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1923_ECCV_2024_paper.php": {
    "title": "RadEdit: stress-testing biomedical vision models via diffusion image editing",
    "volume": "main",
    "abstract": "Biomedical imaging datasets are often small and biased, meaning that real-world performance of predictive models can be substantially lower than expected from internal testing. This work proposes using generative image editing to simulate dataset shifts and diagnose failure modes of biomedical vision models; this can be used in advance of deployment to assess readiness, potentially reducing cost and patient harm. Existing editing methods can produce undesirable changes, with spurious correlations learned due to the co-occurrence of disease and treatment interventions, limiting practical applicability. To address this, we train a text-to-image diffusion model on multiple chest X-ray datasets and introduce a new editing method, , that uses multiple image masks, if present, to constrain changes and ensure consistency in the edited images, minimising bias. We consider three types of dataset shifts: acquisition shift, manifestation shift, and population shift, and demonstrate that our approach can diagnose failures and quantify model robustness without additional data collection, complementing more qualitative tools for explainable AI",
    "checked": true,
    "id": "12e3c20a9703ccb07ff96dc2fd433a3ee71ce882",
    "semantic_title": "radedit: stress-testing biomedical vision models via diffusion image editing",
    "citation_count": 2,
    "authors": [
      "Fernando Pérez-García",
      "Sam Bond-Taylor",
      "Pedro Sanchez",
      "Boris van Breugel",
      "Daniel Coelho de Castro",
      "Harshita Sharma",
      "Valentina Salvatelli",
      "Maria Teodora A Wetscherek",
      "Hannah CM Richardson",
      "Lungren Matthew",
      "Aditya Nori",
      "Javier Alvarez-Valle",
      "Ozan Oktay",
      "Maximilian Ilse*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1927_ECCV_2024_paper.php": {
    "title": "SPAMming Labels: Efficient Annotations for the Trackers of Tomorrow",
    "volume": "main",
    "abstract": "Increasing the annotation efficiency of trajectory annotations from videos has the potential to enable the next generation of data-hungry tracking algorithms to thrive on large-scale datasets. Despite the importance of this task, there are currently very few works exploring how to efficiently label tracking datasets comprehensively. In this work, we introduce SPAM, a video label engine that provides high-quality labels with minimal human intervention. SPAM is built around two key insights: i) most tracking scenarios can be easily resolved. To take advantage of this, we utilize a pre-trained model to generate high-quality pseudo-labels, reserving human involvement for a smaller subset of more difficult instances; ii) handling the spatiotemporal dependencies of track annotations across time can be elegantly and efficiently formulated through graphs. Therefore, we use a unified graph formulation to address the annotation of both detections and identity association for tracks across time. Based on these insights, SPAM produces high-quality annotations with a fraction of ground truth labeling cost. We demonstrate that trackers trained on SPAM labels achieve comparable performance to those trained on human annotations while requiring only 3−20% of the human labeling effort. Hence, SPAM paves the way towards highly efficient labeling of large-scale tracking datasets. We release all models and code",
    "checked": true,
    "id": "2fc8a18e4cc83f35607ecdf3c62a3f6577dc49fb",
    "semantic_title": "spamming labels: efficient annotations for the trackers of tomorrow",
    "citation_count": 0,
    "authors": [
      "Orcun Cetintas*",
      "Tim Meinhardt",
      "Guillem Brasó",
      "Laura Leal-Taixé"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1944_ECCV_2024_paper.php": {
    "title": "AdaDiffSR: Adaptive Region-aware Dynamic acceleration Diffusion Model for Real-World Image Super-Resolution",
    "volume": "main",
    "abstract": "Diffusion models (DMs) have shown promising results on single-image super-resolution and other image-to-image translation tasks. Benefiting from more computational resources and longer inference times, they are able to yield more realistic images. Existing DMs-based super-resolution methods try to achieve an overall average recovery over all regions via iterative refinement, ignoring the consideration that different input image regions require different timesteps to reconstruct. In this work, we notice that previous DMs-based super-resolution methods suffer from wasting computational resources to reconstruct invisible details. To further improve the utilization of computational resources, we propose AdaDiffSR, a DMs-based SR pipeline with dynamic timesteps sampling strategy (DTSS). Specifically, by introducing the multi-metrics latent entropy module (MMLE), we can achieve dynamic perception of the latent spatial information gain during the denoising process, thereby guiding the dynamic selection of the timesteps. In addition, we adopt a progressive feature injection module (PFJ), which dynamically injects the original image features into the denoising process based on the current information gain, so as to generate images with both fidelity and realism. Experiments show that our AdaDiffSR achieves comparable performance over current state-of-the-art DMs-based SR methods while consuming less computational resources and inference time on both synthetic and real-world datasets",
    "checked": true,
    "id": "df6bdf5d7531c055cf60c7c40c8c05dacfd16e0a",
    "semantic_title": "adadiffsr: adaptive region-aware dynamic acceleration diffusion model for real-world image super-resolution",
    "citation_count": 0,
    "authors": [
      "Yuanting Fan",
      "Chengxu Liu",
      "Nengzhong Yin",
      "Changlong Gao",
      "Xueming Qian*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1946_ECCV_2024_paper.php": {
    "title": "Explicitly Guided Information Interaction Network for Cross-modal Point Cloud Completion",
    "volume": "main",
    "abstract": "∗ Equal contribution Corresponding authorIn this paper, we explore a novel framework, EGIInet (Explicitly Guided Information Interaction Network), a model for View-guided Point cloud Completion (ViPC) task, which aims to restore a complete point cloud from a partial one with a single view image. In comparison with previous methods that relied on the global semantics of input images, EGIInet efficiently combines the information from two modalities by leveraging the geometric nature of the completion task. Specifically, we propose an explicitly guided information interaction strategy supported by modal alignment for point cloud completion. First, in contrast to previous methods which simply use 2D and 3D backbones to encode features respectively, we unified the encoding process to promote modal alignment. Second, we propose a novel explicitly guided information interaction strategy that could help the network identify critical information within images, thus achieving better guidance for completion. Extensive experiments demonstrate the effectiveness of our framework, and we achieved a new state-of-the-art (+16% CD over XMFnet) in benchmark datasets despite using fewer parameters than the previous methods. The pre-trained model and code and are available at https://github.com/WHU-USI3DV/EGIInet",
    "checked": true,
    "id": "b5cb75b029d56ef5a1f756af100598bb3c500963",
    "semantic_title": "explicitly guided information interaction network for cross-modal point cloud completion",
    "citation_count": 1,
    "authors": [
      "Xu Hang",
      "Chen Long",
      "Wenxiao Zhang*",
      "Yuan Liu",
      "Zhen Cao",
      "Zhen Dong",
      "Bisheng Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1982_ECCV_2024_paper.php": {
    "title": "Towards Real-world Event-guided Low-light Video Enhancement and Deblurring",
    "volume": "main",
    "abstract": "In low-light conditions, capturing videos with frame-based cameras often requires long exposure times, resulting in motion blur and reduced visibility. While frame-based motion deblurring and low-light enhancement have been studied, they still pose significant challenges. Event cameras have emerged as a promising solution for improving image quality in low-light environments and addressing motion blur. They provide two key advantages: capturing scene details well even in low light due to their high dynamic range, and effectively capturing motion information during long exposures due to their high temporal resolution. Despite efforts to tackle low-light enhancement and motion deblurring using event cameras separately, previous work has not addressed both simultaneously. To explore the joint task, we first establish real-world datasets for event-guided low-light enhancement and deblurring using a hybrid camera system based on beam splitters. Subsequently, we introduce an end-to-end framework to effectively handle these tasks. Our framework incorporates a module to efficiently leverage temporal information from events and frames. Furthermore, we propose a module to utilize cross-modal feature information to employ a low-pass filter for noise suppression while enhancing the main structural information. Our proposed method significantly outperforms existing approaches in addressing the joint task. Our project pages are available at https://github.com/intelpro/ELEDNet",
    "checked": true,
    "id": "5f635d114a1b61b1fdcb15c882aea0ac0b6c72fe",
    "semantic_title": "towards real-world event-guided low-light video enhancement and deblurring",
    "citation_count": 0,
    "authors": [
      "Taewoo Kim",
      "Jaeseok Jeong",
      "Hoonhee Cho",
      "Yuhwan Jeong",
      "Kuk-Jin Yoon*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1985_ECCV_2024_paper.php": {
    "title": "Exploring Pre-trained Text-to-Video Diffusion Models for Referring Video Object Segmentation",
    "volume": "main",
    "abstract": "In this paper, we explore the visual representations produced from a pre-trained text-to-video (T2V) diffusion model for video understanding tasks. We hypothesize that the latent representation learned from a pretrained generative T2V model encapsulates rich semantics and coherent temporal correspondences, thereby naturally facilitating video understanding. Our hypothesis is validated through the classic referring video object segmentation (R-VOS) task. We introduce a novel framework, termed \"VD-IT\", tailored with dedicatedly designed components built upon a fixed pretrained T2V model. Specifically, VD-IT uses textual information as a conditional input, ensuring semantic consistency across time for precise temporal instance matching. It further incorporates image tokens as supplementary textual inputs, enriching the feature set to generate detailed and nuanced masks. Besides, instead of using the standard Gaussian noise, we propose to predict the video-specific noise with an extra noise prediction module, which can help preserve the feature fidelity and elevates segmentation quality. Through extensive experiments, we surprisingly observe that fixed generative T2V diffusion models, unlike commonly used video backbones (e.g., Video Swin Transformer) pretrained with discriminative image/video pre-tasks, exhibit better potential to maintain semantic alignment and temporal consistency. On existing standard benchmarks, our VD-IT achieves highly competitive results, surpassing many existing state-of-the-art methods. The code is available at https://github.com/buxiangzhiren/VD-IT",
    "checked": true,
    "id": "2f612c24f3442ce771dbc848d6ac54e19bc0cf57",
    "semantic_title": "exploring pre-trained text-to-video diffusion models for referring video object segmentation",
    "citation_count": 2,
    "authors": [
      "Xuelu Feng",
      "Dongdong Chen",
      "Junsong Yuan",
      "Chunming Qiao",
      "Gang Hua",
      "Zixin Zhu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1990_ECCV_2024_paper.php": {
    "title": "TrackNeRF: Bundle Adjusting NeRF from Sparse and Noisy Views via Feature Tracks",
    "volume": "main",
    "abstract": "Neural radiance fields (NeRFs) generally require many images with accurate poses for accurate novel view synthesis, which does not reflect realistic setups where views can be sparse and poses can be noisy. Previous solutions for learning NeRFs with sparse views and noisy poses only consider local geometry consistency with pairs of views. Closely following bundle adjustment in Structure-from-Motion (SfM), we introduce TrackNeRF for more globally consistent geometry reconstruction and more accurate pose optimization. TrackNeRF introduces feature tracks, connected pixel trajectories across all visible views that correspond to the same 3D points. By enforcing reprojection consistency among feature tracks, TrackNeRF encourages holistic 3D consistency explicitly. Through extensive experiments, TrackNeRF sets a new benchmark in noisy and sparse view reconstruction. In particular, TrackNeRF shows significant improvements over the state-of-the-art BARF and SPARF by ∼ 8 and ∼ 1 in terms of PSNR on DTU under various sparse and noisy view setups. The code is available at purplehttps://tracknerf.github.io/",
    "checked": true,
    "id": "6cd2904c05f8b7bfbb8fad7444432eea5c610bba",
    "semantic_title": "tracknerf: bundle adjusting nerf from sparse and noisy views via feature tracks",
    "citation_count": 1,
    "authors": [
      "Jinjie Mai*",
      "Wenxuan Zhu",
      "Sara Rojas",
      "Jesus Zarzar",
      "Abdullah Hamdi",
      "Guocheng Qian",
      "Bing Li",
      "Silvio Giancola",
      "Bernard Ghanem"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2007_ECCV_2024_paper.php": {
    "title": "COHO: Context-Sensitive City-Scale Hierarchical Urban Layout Generation",
    "volume": "main",
    "abstract": "The generation of large-scale urban layouts has garnered substantial interest across various disciplines. Prior methods have utilized procedural generation requiring manual rule coding or deep learning needing abundant data. However, prior approaches have not considered the context-sensitive nature of urban layout generation. Our approach addresses this gap by leveraging a canonical graph representation for the entire city, which facilitates scalability and captures the multi-layer semantics inherent in urban layouts. We introduce a novel graph-based masked autoencoder (GMAE) for city-scale urban layout generation. The method encodes attributed buildings, city blocks, communities and cities into a unified graph structure, enabling self-supervised masked training for graph autoencoder. Additionally, we employ scheduled iterative sampling for 2.5D layout generation, prioritizing the generation of important city blocks and buildings. Our approach achieves good realism, semantic consistency, and correctness across the heterogeneous urban styles in 330 US cities. Codes and datasets are released at https://github.com/Arking1995/COHO",
    "checked": true,
    "id": "8b2713af6cf97621e09285b960ed2acf7e582ba1",
    "semantic_title": "coho: context-sensitive city-scale hierarchical urban layout generation",
    "citation_count": 2,
    "authors": [
      "Liu He*",
      "Daniel Aliaga"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2012_ECCV_2024_paper.php": {
    "title": "Joint RGB-Spectral Decomposition Model Guided Image Enhancement in Mobile Photography",
    "volume": "main",
    "abstract": "The integration of miniaturized spectrometers into mobile devices offers new avenues for image quality enhancement and facilitates novel downstream tasks. However, the broader application of spectral sensors in mobile photography is hindered by the inherent complexity of spectral images and the constraints of spectral imaging capabilities. To overcome these challenges, we propose a joint RGB-Spectral decomposition model guided enhancement framework, which consists of two steps: joint decomposition and prior-guided enhancement. Firstly, we leverage the complementarity between RGB and Low-resolution Multi-Spectral Images (Lr-MSI) to predict shading, reflectance, and material semantic priors. Subsequently, these priors are seamlessly integrated into the established HDRNet to promote dynamic range enhancement, color mapping, and grid expert learning, respectively. Additionally, we construct a high-quality Mobile-Spec dataset to support our research, and our experiments validate the effectiveness of Lr-MSI in the tone enhancement task. This work aims to establish a solid foundation for advancing spectral vision in mobile photography. The code is available at https://github.com/CalayZhou/JDM-HDRNet",
    "checked": true,
    "id": "0b1480280b1422e8c05ca8f25a825c1ef91bc204",
    "semantic_title": "joint rgb-spectral decomposition model guided image enhancement in mobile photography",
    "citation_count": 0,
    "authors": [
      "Kailai Zhou*",
      "Lijing Cai",
      "Yibo Wang",
      "Mengya Zhang",
      "Bihan Wen",
      "Qiu Shen*",
      "Xun Cao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2019_ECCV_2024_paper.php": {
    "title": "SpatialFormer: Towards Generalizable Vision Transformers with Explicit Spatial Understanding",
    "volume": "main",
    "abstract": "Vision transformers have demonstrated promising results and become core components in many tasks. Most existing works focus on context feature extraction and incorporate spatial information through additional positional embedding. However, they only consider the local positional information within each image token and cannot effectively model the global spatial relations of the underlying scene. To address this challenge, we propose an efficient vision transformer architecture, SpatialFormer, with explicit spatial understanding for generalizable image representation learning. Specifically, we accompany the image tokens with adaptive spatial tokens to represent the context and spatial information respectively. We initialize the spatial tokens with positional encoding to introduce general spatial priors and augment them with learnable embeddings to model adaptive spatial information. For better generalization, we employ a decoder-only overall architecture and propose a bilateral cross-attention block for efficient interactions between context and spatial tokens. SpatialFormer learns transferable image representations with explicit scene understanding, where the output spatial tokens can further serve as enhanced initial queries for task-specific decoders for better adaptations to downstream tasks. Extensive experiments on image classification, semantic segmentation, and 2D/3D object detection tasks demonstrate the efficiency and transferability of the proposed SpatialFormer architecture. Code is available at https://github.com/Euphoria16/SpatialFormer",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Xiao",
      "Wenzhao Zheng",
      "Sicheng Zuo",
      "Peng Gao",
      "Jie Zhou",
      "Jiwen Lu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2024_ECCV_2024_paper.php": {
    "title": "OccWorld: Learning a 3D Occupancy World Model for Autonomous Driving",
    "volume": "main",
    "abstract": "Understanding how the 3D scene evolves is vital for making decisions in autonomous driving. Most existing methods achieve this by predicting the movements of object boxes, which cannot capture more fine-grained scene information. In this paper, we explore a new framework of learning a world model, OccWorld, in the 3D occupancy space to simultaneously predict the movement of the ego car and the evolution of the surrounding scenes. We propose to learn a world model based on 3D occupancy rather than 3D bounding boxes and segmentation maps for three reasons: 1) expressiveness. 3D occupancy can describe the more fine-grained 3D structure of the scene; 2) efficiency. 3D occupancy is more economical to obtain (e.g., from sparse LiDAR points). 3) versatility. 3D occupancy can adapt to both vision and LiDAR. To facilitate the modeling of the world evolution, we learn a reconstruction-based scene tokenizer on the 3D occupancy to obtain discrete scene tokens to describe the surrounding scenes. We then adopt a GPT-like spatial-temporal generative transformer to generate subsequent scene and ego tokens to decode the future occupancy and ego trajectory. Extensive experiments on nuScenes demonstrate the ability of OccWorld to effectively model the driving scene evolutions. OccWorld also produces competitive planning results without using instance and map supervision. Code: https://github.com/wzzheng/OccWorld",
    "checked": true,
    "id": "0e4e899e8d4de9bd8d0ea43e32afd0baca7b6bf0",
    "semantic_title": "occworld: learning a 3d occupancy world model for autonomous driving",
    "citation_count": 35,
    "authors": [
      "Wenzhao Zheng",
      "Weiliang Chen",
      "Yuanhui Huang",
      "Borui Zhang",
      "Yueqi Duan",
      "Jiwen Lu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2028_ECCV_2024_paper.php": {
    "title": "MyVLM: Personalizing VLMs for User-Specific Queries",
    "volume": "main",
    "abstract": "Recent large-scale vision-language models (VLMs) have demonstrated remarkable capabilities in understanding and generating textual descriptions for visual content. However, these models lack an understanding of user-specific concepts. In this work, we take a first step toward the personalization of VLMs, enabling them to learn and reason over user-provided concepts. For example, we explore whether these models can learn to recognize you in an image and communicate what you are doing, tailoring the model to reflect your personal experiences and relationships. To effectively recognize a variety of user-specific concepts, we augment the VLM with external concept heads that function as toggles for the model, enabling the VLM to identify the presence of specific target concepts in a given image. Having recognized the concept, we learn a new concept embedding in the intermediate feature space of the VLM. This embedding is tasked with guiding the language model to naturally integrate the target concept in its generated response. We apply our technique to BLIP-2 and LLaVA for personalized image captioning and further show its applicability for personalized visual question-answering. Our experiments demonstrate our ability to generalize to unseen images of learned concepts while preserving the model behavior on unrelated inputs. Code and data will be made available upon acceptance",
    "checked": true,
    "id": "62690d78c1b9609036f7a456cc36b19c5dba6eb6",
    "semantic_title": "myvlm: personalizing vlms for user-specific queries",
    "citation_count": 4,
    "authors": [
      "Yuval Alaluf*",
      "Elad Richardson",
      "Sergey Tulyakov",
      "Kfir Aberman",
      "Danny Cohen-Or"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2032_ECCV_2024_paper.php": {
    "title": "AMEGO: Active Memory from long EGOcentric videos",
    "volume": "main",
    "abstract": "Egocentric videos provide a unique perspective into individuals' daily experiences, yet their unstructured nature presents challenges for perception. In this paper, we introduce , a novel approach aimed at enhancing the comprehension of very-long egocentric videos. Inspired by the human's ability to maintain information from a single watching, focuses on constructing a self-contained representations from one egocentric video, capturing key locations and object interactions. This representation is semantic-free and facilitates multiple queries without the need to reprocess the entire visual content. Additionally, to evaluate our understanding of very-long egocentric videos, we introduce the new (), composed of more than 20K of highly challenging visual queries from EPIC-KITCHENS. These queries cover different levels of video reasoning (sequencing, concurrency and temporal grounding) to assess detailed video understanding capabilities. We showcase improved performance of on , surpassing other video QA baselines by a substantial margin",
    "checked": true,
    "id": "92a3bba45f6bb056aff19b2470b45152e44c418e",
    "semantic_title": "amego: active memory from long egocentric videos",
    "citation_count": 0,
    "authors": [
      "Gabriele Goletto*",
      "Tushar Nagarajan",
      "Giuseppe Averta",
      "Dima Damen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2034_ECCV_2024_paper.php": {
    "title": "Power Variable Projection for Initialization-Free Large-Scale Bundle Adjustment",
    "volume": "main",
    "abstract": "Most Bundle Adjustment (BA) solvers like the Levenberg-Marquard algorithm require a good initialization. Instead, initialization-free BA remains a largely uncharted territory. The under-explored Variable Projection algorithm (VarPro) exhibits a wide convergence basin even without initialization. Coupled with object space error formulation, recent works have shown its ability to solve small-scale initialization-free bundle adjustment problem. To make such initialization-free BA approaches scalable, we introduce Power Variable Projection (PoVar), extending a recent inverse expansion method based on power series. Importantly, we link the power series expansion to Riemannian manifold optimization. This projective framework is crucial to solve large-scale bundle adjustment problems without initialization. Using the real-world BAL dataset, we experimentally demonstrate that our solver achieves state-of-the-art results in terms of speed and accuracy. To our knowledge, this work is the first to address the scalability of BA without initialization opening new venues for initialization-free structure-from-motion",
    "checked": true,
    "id": "7fedd0d2f58e2b34b5de1bf4d9affd8696fa5555",
    "semantic_title": "power variable projection for initialization-free large-scale bundle adjustment",
    "citation_count": 0,
    "authors": [
      "Simon Weber*",
      "Je Hyeong Hong",
      "Daniel Cremers"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2035_ECCV_2024_paper.php": {
    "title": "Collaborative Control for Geometry-Conditioned PBR Image Generation",
    "volume": "main",
    "abstract": "Graphics pipelines require physically-based rendering (PBR) materials, yet current 3D content generation approaches are built on RGB models. We propose to model the PBR image distribution directly, avoiding photometric inaccuracies in RGB generation and the inherent ambiguity in extracting PBR from RGB. As existing paradigms for cross-modal fine-tuning are not suited for PBR generation due to both a lack of data and the high dimensionality of the output modalities, we propose to train a new PBR model that is tightly linked to a frozen RGB model using a novel cross-network communication paradigm. As the base RGB model is fully frozen, the proposed method retains its general performance and remains compatible with IPAdapters for that base model",
    "checked": true,
    "id": "685a5fc3094e13218907e9adb5477dabdeb1bc9b",
    "semantic_title": "collaborative control for geometry-conditioned pbr image generation",
    "citation_count": 4,
    "authors": [
      "Shimon Vainer",
      "Mark Boss",
      "Mathias Parger",
      "Konstantin Kutsy",
      "Dante De Nigris",
      "Ciara Rowles",
      "Nicolas Perony",
      "Simon Donné*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2037_ECCV_2024_paper.php": {
    "title": "Co-synthesis of Histopathology Nuclei Image-Label Pairs using a Context-Conditioned Joint Diffusion Model",
    "volume": "main",
    "abstract": "In multi-class histopathology nuclei analysis tasks, the lack of training data becomes a main bottleneck for the performance of learning-based methods. To tackle this challenge, previous methods have utilized generative models to increase data by generating synthetic samples. However, existing methods often overlook the importance of considering the context of biological tissues (e.g., shape, spatial layout, and tissue type) in the synthetic data. Moreover, while generative models have shown superior performance in synthesizing realistic histopathology images, none of the existing methods are capable of producing image-label pairs at the same time. In this paper, we introduce a novel framework for co-synthesizing histopathology nuclei images and paired semantic labels using a context-conditioned joint diffusion model. We propose conditioning of a diffusion model using nucleus centroid layouts with structure-related text prompts to incorporate spatial and structural context information into the generation targets. Moreover, we enhance the granularity of our synthesized semantic labels by generating instance-wise nuclei labels using distance maps synthesized concurrently in conjunction with the images and semantic labels. We demonstrate the effectiveness of our framework in generating high-quality samples on multi-institutional, multi-organ, and multi-modality datasets. Our synthetic data consistently outperforms existing augmentation methods in the downstream tasks of nuclei segmentation and classification",
    "checked": true,
    "id": "3520ca207cc3e705ba1b2a872764ecc4e359ca7a",
    "semantic_title": "co-synthesis of histopathology nuclei image-label pairs using a context-conditioned joint diffusion model",
    "citation_count": 0,
    "authors": [
      "Seonghui Min",
      "Hyun-Jic Oh",
      "Won-Ki Jeong*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2053_ECCV_2024_paper.php": {
    "title": "One-stage Prompt-based Continual Learning",
    "volume": "main",
    "abstract": "Prompt-based Continual Learning (PCL) has gained considerable attention as a promising continual learning solution because it achieves state-of-the-art performance while preventing privacy violations and memory overhead problems. Nonetheless, existing PCL approaches face significant computational burdens because of two Vision Transformer (ViT) feed-forward stages; one is for the query ViT that generates a prompt query to select prompts inside a prompt pool; the other one is a backbone ViT that mixes information between selected prompts and image tokens. To address this, we introduce a one-stage PCL framework by directly using the intermediate layer's token embedding as a prompt query. This design removes the need for an additional feed-forward stage for query ViT, resulting in ∼ 50% computational cost reduction for both training and inference with marginal accuracy drop (≤ 1%). We further introduce a Query-Pool Regularization (QR) loss that regulates the relationship between the prompt query and the prompt pool to improve representation power. The QR loss is only applied during training time, so there is no computational overhead at inference from the QR loss. With the QR loss, our approach maintains ∼ 50% computational cost reduction during inference as well as outperforms the prior two-stage PCL methods by ∼ 1.4% on public class-incremental continual learning benchmarks including CIFAR-100, ImageNet-R, and DomainNet",
    "checked": true,
    "id": "66abc5325d3d240016813766f5363dc224c4aa11",
    "semantic_title": "one-stage prompt-based continual learning",
    "citation_count": 0,
    "authors": [
      "Youngeun Kim*",
      "Yuhang Li",
      "Priyadarshini Panda"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2054_ECCV_2024_paper.php": {
    "title": "SpaceJAM: a Lightweight and Regularization-free Method for Fast Joint Alignment of Images",
    "volume": "main",
    "abstract": "The unsupervised task of Joint Alignment (JA) of images is beset by challenges such as high complexity, geometric distortions, and convergence to poor local or even global optima. Although Vision Transformers (ViT) have recently provided valuable features for JA, they fall short of fully addressing these issues. Consequently, researchers frequently depend on expensive models and numerous regularization terms, resulting in long training times and challenging hyperparameter tuning. We introduce the Spatial Joint Alignment Model (SpaceJAM), a novel approach that addresses the JA task with efficiency and simplicity. SpaceJAM leverages a compact architecture with only ∼16K trainable parameters and uniquely operates without the need for regularization or atlas maintenance. Evaluations on SPair-71K and CUB datasets demonstrate that SpaceJAM matches the alignment capabilities of existing methods while significantly reducing computational demands and achieving at least a 10x speedup. SpaceJAM sets a new standard for rapid and effective image alignment, making the process more accessible and efficient. Our code is available at: https://bgu-cs-vil.github.io/ SpaceJAM/",
    "checked": true,
    "id": "7cd715e9bfeb5991db59a2060db63f9ffa1540cd",
    "semantic_title": "spacejam: a lightweight and regularization-free method for fast joint alignment of images",
    "citation_count": 0,
    "authors": [
      "Nir Barel*",
      "Ron A Shapira Weber*",
      "Nir Mualem",
      "Shahaf E Finder",
      "Oren Freifeld*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2055_ECCV_2024_paper.php": {
    "title": "APL: Anchor-based Prompt Learning for One-stage Weakly Supervised Referring Expression Comprehension",
    "volume": "main",
    "abstract": "Referring Expression Comprehension (REC) aims to ground the target object based on a given referring expression, which requires expensive instance-level annotations for training. To address this issue, recent advances explore an efficient one-stage weakly supervised REC model called RefCLIP. Particularly, RefCLIP utilizes anchor features of pre-trained one-stage detection networks to represent candidate objects and conducts anchor-text ranking to locate the referent. Despite the effectiveness, we identify that visual semantics of RefCLIP are ambiguous and insufficient for weakly supervised REC modeling. To address this issue, we propose a novel method that enriches visual semantics with various prompt information, called anchor-based prompt learning (APL). Specifically, APL contains an innovative anchor-based prompt encoder (APE) to produce discriminative prompts covering three aspects of REC modeling, e.g., position, color and category. These prompts are dynamically fused into anchor features to improve the visual description power. In addition, we propose two novel auxiliary objectives to achieve accurate vision-language alignment in APL, namely text reconstruction loss and visual alignment loss. To validate APL, we conduct extensive experiments on four REC benchmarks, namely RefCOCO, RefCOCO+, RefCOCOg and ReferIt. Experimental results not only show the state-of-the-art performance of APL against existing methods on four benchmarks, e.g., +6.44% over RefCLIP on RefCOCO, but also confirm its strong generalization ability on weakly supervised referring expression segmentation. Source codes released at: https://github.com/Yaxin9Luo/APL",
    "checked": false,
    "id": "4903d91b841ee3f3652ca5dacfcc86e4acb9ffee",
    "semantic_title": "refclip: a universal teacher for weakly supervised referring expression comprehension",
    "citation_count": 9,
    "authors": [
      "Yaxin Luo",
      "Jiayi Ji",
      "Xiaofu Chen",
      "Yuxin Zhang",
      "Tianhe Ren",
      "Gen Luo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2058_ECCV_2024_paper.php": {
    "title": "GenQ: Quantization in Low Data Regimes with Generative Synthetic Data",
    "volume": "main",
    "abstract": "In the realm of deep neural network deployment, low-bit quantization presents a promising avenue for enhancing computational efficiency. However, it often hinges on the availability of training data to mitigate quantization errors, a significant challenge when data availability is scarce or restricted due to privacy or copyright concerns. Addressing this, we introduce , a novel approach employing an advanced Generative AI model to generate photorealistic, high-resolution synthetic data, overcoming the limitations of traditional methods that struggle to accurately mimic complex objects in extensive datasets like ImageNet. Our methodology is underscored by two robust filtering mechanisms designed to ensure the synthetic data closely aligns with the intrinsic characteristics of the actual training data. In case of limited data availability, the actual data is used to guide the synthetic data generation process, enhancing fidelity through the inversion of learnable token embeddings. Through rigorous experimentation, establishes new benchmarks in data-free and data-scarce quantization, significantly outperforming existing methods in accuracy and efficiency, thereby setting a new standard for quantization in low data regimes. Code is released at https: //github.com/Intelligent-Computing-Lab-Yale/GenQ",
    "checked": true,
    "id": "75f0b8f9ed85eabe4141e25f0c7af850d4d1c41f",
    "semantic_title": "genq: quantization in low data regimes with generative synthetic data",
    "citation_count": 0,
    "authors": [
      "Yuhang Li*",
      "Youngeun Kim",
      "Donghyun Lee",
      "Souvik Kundu",
      "Priyadarshini Panda"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2081_ECCV_2024_paper.php": {
    "title": "MVDD: Multi-View Depth Diffusion Models",
    "volume": "main",
    "abstract": "Denoising diffusion models have demonstrated outstanding results in 2D image generation, yet it remains a challenge to replicate its success in 3D shape generation. In this paper, we propose leveraging multi-view depth, which represents complex 3D shapes in a 2D data format that is easy to denoise. We pair this representation with a diffusion model, MVDD, that is capable of generating high-quality dense point clouds with 20K+ points with fine-grained details. To enforce 3D consistency in multi-view depth, we introduce an epipolar line segment attention that conditions the denoising step for a view on its neighboring views. Additionally, a depth fusion module is incorporated into diffusion steps to further ensure the alignment of depth maps. When augmented with surface reconstruction, MVDD can also produce high-quality 3D meshes. Furthermore, MVDD stands out in other tasks such as depth completion, and can serve as a 3D prior, significantly boosting many downstream tasks, such as GAN inversion. State-of-the-art results from extensive experiments demonstrate MVDD's excellent ability in 3D shape generation, depth completion, and its potential as a 3D prior for downstream tasks",
    "checked": true,
    "id": "73234ad785a5914c1abc8ee96ad762dcef9a0f78",
    "semantic_title": "mvdd: multi-view depth diffusion models",
    "citation_count": 2,
    "authors": [
      "Zhen Wang*",
      "Qiangeng Xu",
      "Feitong Tan",
      "Menglei Chai",
      "Shichen Liu",
      "Rohit Pandey",
      "Sean Fanello",
      "Achuta Kadambi",
      "Yinda Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2083_ECCV_2024_paper.php": {
    "title": "Rethinking Video-Text Understanding: Retrieval from Counterfactually Augmented Data",
    "volume": "main",
    "abstract": "Recent video-text foundation models have demonstrated strong performance on a wide variety of downstream video understanding tasks. Can these video-text models genuinely understand the contents of natural videos? Standard video-text evaluations could be misleading as many questions can be inferred merely from the objects and contexts in a single frame or biases inherent in the datasets. In this paper, we aim to better assess the capabilities of current video-text models and understand their limitations. We propose a novel evaluation task for video-text understanding, namely retrieval from counterfactually augmented data (RCAD), and a new dataset. To succeed on our new evaluation task, models must derive a comprehensive understanding of the video from cross-frame reasoning. Analyses show that previous video-text foundation models can be easily fooled by counterfactually augmented data and are far behind human-level performance. In order to narrow the gap between video-text models and human performance on RCAD, we identify a key limitation of current contrastive approaches on video-text data and introduce , a more effective approach to learn action semantics by leveraging knowledge obtained from a pretrained large language model. Experiments and analyses show that our approach successfully learn more discriminative action embeddings and improves results on when applied to multiple video-text models. Our dataset and project page is available here",
    "checked": true,
    "id": "f7beb4826351aec9be1f0cad4e5e2ff2b436d44b",
    "semantic_title": "rethinking video-text understanding: retrieval from counterfactually augmented data",
    "citation_count": 0,
    "authors": [
      "Wufei Ma*",
      "Kai Li",
      "Zhongshi Jiang",
      "Moustafa Meshry",
      "Qihao Liu",
      "Huiyu Wang",
      "Christian Haene",
      "Alan Yuille"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2087_ECCV_2024_paper.php": {
    "title": "Risk-Aware Self-Consistent Imitation Learning for Trajectory Planning in Autonomous Driving",
    "volume": "main",
    "abstract": "Planning for the ego vehicle is the ultimate goal of autono-mous driving. Although deep learning-based methods have been widely applied to predict future trajectories of other agents in traffic scenes, directly using them to plan for the ego vehicle is often unsatisfactory. This is due to misaligned objectives during training and deployment: a planner that only aims to imitate human driver trajectories is insufficient to accomplish driving tasks well. We argue that existing training processes may not endow models with an understanding of how the physical world evolves. To address this gap, we propose RaSc, which stands for Risk-aware Self-consistent imitation learning. RaSc not only imitates driving trajectories, but also learns the motivations behind human driver behaviors (to be risk-aware) and the consequences of its own actions (by being self-consistent). These two properties stem from our novel prediction branch and training objectives regarding Time-To-Collision (TTC). Moreover, we enable the model to better mine hard samples during training by checking its self-consistency. Our experiments on the large-scale real-world nuPlan dataset demonstrate that RaSc outperforms previous state-of-the-art learning-based methods, in both open-loop and, more importantly, closed-loop settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixuan Fan*",
      "Ya-Li Li",
      "Shengjin Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2090_ECCV_2024_paper.php": {
    "title": "Dual-level Adaptive Self-Labeling for Novel Class Discovery in Point Cloud Segmentation",
    "volume": "main",
    "abstract": "We tackle the novel class discovery in point cloud segmentation, which discovers novel classes based on the semantic knowledge of seen classes. Existing work proposes an online point-wise clustering method with a simplified equal class-size constraint on the novel classes to avoid degenerate solutions. However, the inherent imbalanced distribution of novel classes in point clouds typically violates the equal class-size constraint. Moreover, point-wise clustering ignores the rich spatial context information of objects, which results in less expressive representation for semantic segmentation. To address the above challenges, we propose a novel self-labeling strategy that adaptively generates high-quality pseudo-labels for imbalanced classes during model training. In addition, we develop a dual-level representation that incorporates regional consistency into the point-level classifier learning, reducing noise in generated segmentation. Finally, we conduct extensive experiments on two widely used datasets, SemanticKITTI and SemanticPOSS, and the results show our method outperforms the state of the art by a large margin",
    "checked": true,
    "id": "c85e79c91620c14c95f89dcea4fef64a4c12d028",
    "semantic_title": "dual-level adaptive self-labeling for novel class discovery in point cloud segmentation",
    "citation_count": 2,
    "authors": [
      "Ruijie Xu*",
      "CHUYU ZHANG",
      "Hui Ren",
      "Xuming He"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2096_ECCV_2024_paper.php": {
    "title": "EBDM: Exemplar-guided Image Translation with Brownian-bridge Diffusion Models",
    "volume": "main",
    "abstract": "Exemplar-guided image translation, synthesizing photo-realistic images that conform to both structural control and style exemplars, is attracting attention due to its ability to enhance user control over style manipulation. Previous methodologies have predominantly depended on establishing dense correspondences across cross-domain inputs. Despite these efforts, they incur quadratic memory and computational costs for establishing dense correspondence, resulting in limited versatility and performance degradation. In this paper, we propose a novel approach termed Exemplar-guided Image Translation with Brownian-Bridge Diffusion Models (EBDM). Our method formulates the task as a stochastic Brownian bridge process, a diffusion process with a fixed initial point as structure control and translates into the corresponding photo-realistic image while being conditioned solely on the given exemplar image. To efficiently guide the diffusion process toward the style of exemplar, we delineate three pivotal components: the Global Encoder, the Exemplar Network, and the Exemplar Attention Module to incorporate global and detailed texture information from exemplar images. Leveraging Bridge diffusion, the network can translate images from structure control while exclusively conditioned on the exemplar style, leading to more robust training and inference processes. We illustrate the superiority of our method over competing approaches through comprehensive benchmark evaluations and visual results",
    "checked": true,
    "id": "13854de546b52d6d20e8e3c11e9a8f8a1bf0e99a",
    "semantic_title": "ebdm: exemplar-guided image translation with brownian-bridge diffusion models",
    "citation_count": 0,
    "authors": [
      "Eungbean Lee",
      "Somi Jeong",
      "Kwanghoon Sohn*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2100_ECCV_2024_paper.php": {
    "title": "DreamDrone: Text-to-Image Diffusion Models are Zero-shot Perpetual View Generators",
    "volume": "main",
    "abstract": "We introduce DreamDrone, a novel zero-shot and training-free pipeline for generating unbounded flythrough scenes from textual prompts. Different from other methods that focus on warping images frame by frame, we advocate explicitly warping the intermediate latent code of the pre-trained text-to-image diffusion model for high-quality image generation and generalization ability. To further enhance the fidelity of the generated images, we also propose a feature-correspondence-guidance diffusion process and a high-pass filtering strategy to promote geometric consistency and high-frequency detail consistency, respectively. Extensive experiments reveal that DreamDrone significantly surpasses existing methods, delivering highly authentic scene generation with exceptional visual quality, without training or fine-tuning on datasets or reconstructing 3D point clouds in advance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanyang Kong*",
      "Dongze Lian",
      "Michael Bi Mi",
      "Xinchao Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2103_ECCV_2024_paper.php": {
    "title": "Harnessing Text-to-Image Diffusion Models for Category-Agnostic Pose Estimation",
    "volume": "main",
    "abstract": "Category-Agnostic Pose Estimation (CAPE) aims to detect keypoints of an arbitrary unseen category in images, based on several provided examples of that category. This is a challenging task, as the limited data of unseen categories makes it difficult for models to generalize effectively. To address this challenge, previous methods typically train models on a set of predefined base categories with extensive annotations. In this work, we propose to harness rich knowledge in the off-the-shelf text-to-image diffusion model to effectively address CAPE, without training on carefully prepared base categories. To this end, we propose a Prompt Pose Matching (PPM) framework, which learns pseudo prompts corresponding to the keypoints in the provided few-shot examples via the text-to-image diffusion model. These learned pseudo prompts capture semantic information of keypoints, which can then be used to locate the same type of keypoints from images. We also design a Category-shared Prompt Training (CPT) scheme, to further boost our PPM's performance. Extensive experiments demonstrate the efficacy of our approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Duo Peng",
      "Zhengbo Zhang",
      "Ping Hu",
      "Qiuhong Ke",
      "David Yau",
      "Jun Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2109_ECCV_2024_paper.php": {
    "title": "SC4D: Sparse-Controlled Video-to-4D Generation and Motion Transfer",
    "volume": "main",
    "abstract": "Recent advances in 2D/3D generative models enable the generation of dynamic 3D objects from a single-view video. Existing approaches utilize score distillation sampling to form the dynamic scene as dynamic NeRF or dense 3D Gaussians. However, these methods struggle to strike a balance among reference view alignment, spatio-temporal consistency, and motion fidelity under single-view conditions due to the implicit nature of NeRF or the intricate dense Gaussian motion prediction. To address these issues, this paper proposes an efficient, sparse-controlled video-to-4D framework named SC4D, that decouples motion and appearance to achieve superior video-to-4D generation. Moreover, we introduce Adaptive Gaussian (AG) initialization and Gaussian Alignment (GA) loss to mitigate shape degeneration issue, ensuring the fidelity of the learned motion and shape. Comprehensive experimental results demonstrate that our method surpasses existing methods in both quality and efficiency. In addition, facilitated by the disentangled modeling of motion and appearance of SC4D, we devise a novel application that seamlessly transfers the learned motion onto a diverse array of 4D entities according to textual descriptions",
    "checked": true,
    "id": "8c6583a8334bb7688505d6f745a0dcf29203ee47",
    "semantic_title": "sc4d: sparse-controlled video-to-4d generation and motion transfer",
    "citation_count": 7,
    "authors": [
      "Zijie Wu*",
      "Chaohui Yu",
      "Yanqin Jiang",
      "Chenjie Cao",
      "Fan Wang",
      "Xiang Bai*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2121_ECCV_2024_paper.php": {
    "title": "Overcoming Distribution Mismatch in Quantizing Image Super-Resolution Networks",
    "volume": "main",
    "abstract": "Although quantization has emerged as a promising approach to reducing computational complexity across various high-level vision tasks, it inevitably leads to accuracy loss in image super-resolution (SR) networks. This is due to the significantly divergent feature distributions across different channels and input images of the SR networks, which complicates the selection of a fixed quantization range. Existing works address this distribution mismatch problem by dynamically adapting quantization ranges to the varying distributions during test time. However, such a dynamic adaptation incurs additional computational costs during inference. In contrast, we propose a new quantization-aware training scheme that effectively Overcomes the Distribution Mismatch problem in SR networks without the need for dynamic adaptation. Intuitively, this mismatch can be mitigated by regularizing the distance between the feature and a fixed quantization range. However, we observe that such regularization can conflict with the reconstruction loss during training, negatively impacting SR accuracy. Therefore, we opt to regularize the mismatch only when the gradients of the regularization are aligned with those of the reconstruction loss. Additionally, we introduce a layer-wise weight clipping correction scheme to determine a more suitable quantization range for layer-wise weights. Experimental results demonstrate that our framework effectively reduces the distribution mismatch and achieves state-of-the-art performance with minimal computational overhead. Codes are available at https://github.com/Cheeun/ODM",
    "checked": true,
    "id": "3b525fa77c38b10b707a6acd485dbb866b541657",
    "semantic_title": "overcoming distribution mismatch in quantizing image super-resolution networks",
    "citation_count": 1,
    "authors": [
      "Cheeun Hong",
      "Kyoung Mu Lee*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2125_ECCV_2024_paper.php": {
    "title": "Large Motion Model for Unified Multi-Modal Motion Generation",
    "volume": "main",
    "abstract": "Human motion generation, a cornerstone technique in animation and video production, has widespread applications in various tasks like text-to-motion and music-to-dance. Previous works focus on developing specialist models tailored for each task without scalability. In this work, we present Large Motion Model (LMM), a motion-centric, multi-modal framework that unifies mainstream motion generation tasks into a generalist model. A unified motion model is appealing since it can leverage a wide range of motion data to achieve broad generalization beyond a single task. However, it is also challenging due to the heterogeneous nature of substantially different motion data and tasks. LMM tackles these challenges from three principled aspects: 1) Data: We consolidate datasets with different modalities, formats and tasks into a comprehensive yet unified motion generation dataset, MotionVerse, comprising 10 tasks, 16 datasets, a total of 320k sequences, and 100 million frames. 2) Architecture: We design an articulated attention mechanism that incorporates body part-aware modeling into Diffusion Transformer backbone. 3) Pre-Training: We propose a novel pre-training strategy for LMM, which employs variable frame rates and masking forms, to better exploit knowledge from diverse training data. Extensive experiments demonstrate that our generalist LMM achieves competitive performance across various standard motion generation tasks over state-of-the-art specialist models. Notably, LMM exhibits strong generalization capabilities and emerging properties across many unseen tasks. Additionally, our ablation studies reveal valuable insights about training and scaling up large motion models for future research",
    "checked": true,
    "id": "0f170ef970b4febb9115b5b142123cce5a1c6396",
    "semantic_title": "large motion model for unified multi-modal motion generation",
    "citation_count": 12,
    "authors": [
      "Mingyuan Zhang*",
      "Daisheng Jin",
      "Chenyang Gu",
      "Fangzhou Hong",
      "Zhongang Cai",
      "Jingfang Huang",
      "Chongzhi Zhang",
      "Xinying Guo",
      "Lei Yang",
      "Ying He",
      "Ziwei Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2130_ECCV_2024_paper.php": {
    "title": "FisherRF: Active View Selection and Mapping with Radiance Fields using Fisher Information",
    "volume": "main",
    "abstract": "This study addresses the challenging problem of active view selection and uncertainty quantification within the domain of Radiance Fields. Neural Radiance Fields (NeRF) have greatly advanced image rendering and reconstruction, but the cost of acquiring images poses the need to select the most informative viewpoints efficiently. Existing approaches depend on modifying the model architecture or hypothetical perturbation field to indirectly approximate the model uncertainty. However, selecting views from indirect approximation does not guarantee optimal information gain for the model. By leveraging Fisher Information, we directly quantify observed information on the parameters of Radiance Fields and select candidate views by maximizing the Expected Information Gain (EIG). Our method achieves state-of-the-art results on multiple tasks, including view selection, active mapping, and uncertainty quantification, demonstrating its potential to advance the field of Radiance Fields",
    "checked": false,
    "id": "0554f17b5769333d2906213b3fbb233492525e69",
    "semantic_title": "fisherrf: active view selection and uncertainty quantification for radiance fields using fisher information",
    "citation_count": 17,
    "authors": [
      "Wen Jiang*",
      "BOSHU LEI",
      "Kostas Daniilidis*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2132_ECCV_2024_paper.php": {
    "title": "Occlusion Handling in 3D Human Pose Estimation with Perturbed Positional Encoding",
    "volume": "main",
    "abstract": "Understanding human behavior fundamentally relies on accurate 3D human pose estimation. Graph Convolutional Networks (GCNs) have recently shown promising advancements, delivering state-of-the-art performance with rather lightweight architectures. In the context of graph-structured data, leveraging the eigenvectors of the graph Laplacian matrix for positional encoding is effective. Yet, the approach does not specify how to handle scenarios where edges in the input graph are missing. To this end, we propose a novel positional encoding technique, PerturbPE, that extracts consistent and regular components from the eigenbasis. Our method involves applying multiple perturbations and taking their average to extract the consistent and regular component from the eigenbasis. PerturbPE leverages the Rayleigh-Schrodinger Perturbation Theorem (RSPT) for calculating the perturbed eigenvectors. Employing this labeling technique enhances the robustness and generalizability of the model. Our results support our theoretical findings, e.g. our experimental analysis observed a performance enhancement of up to 12% on the Human3.6M dataset in instances where occlusion resulted in the absence of one edge. Furthermore, our novel approach significantly enhances performance in scenarios where two edges are missing, setting a new benchmark for state-of-the-art",
    "checked": true,
    "id": "611620398894d773bc037f5677bd43ac5abc684a",
    "semantic_title": "occlusion handling in 3d human pose estimation with perturbed positional encoding",
    "citation_count": 0,
    "authors": [
      "Niloofar Azizi*",
      "Mohsen Fayyaz",
      "Horst Bischof"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2138_ECCV_2024_paper.php": {
    "title": "Gradient-based Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "One of the challenges for neural networks in real-life applications is the overconfident errors these models make when the data is not from the original training distribution. Addressing this issue is known as Out-of-Distribution (OOD) detection. Many state-of-the-art OOD methods employ an auxiliary dataset as a surrogate for OOD data during training to achieve improved performance. However, these methods fail to fully exploit the local information embedded in the auxiliary dataset. In this work, we propose the idea of leveraging the information embedded in the gradient of the loss function during training to enable the network to not only learn a desired OOD score for each sample but also to exhibit similar behavior in a local neighborhood around each sample. We also develop a novel energy-based sampling method to allow the network to be exposed to more informative OOD samples during the training phase. This is especially important when the auxiliary dataset is large. We demonstrate the effectiveness of our method through extensive experiments on several OOD benchmarks, improving the existing state-of-the-art FPR95 by 4% on our ImageNet experiment. We further provide a theoretical analysis through the lens of certified robustness and Lipschitz analysis to showcase the theoretical foundation of our work. Our code is available at https://github.com/o4lc/Greg-OOD",
    "checked": false,
    "id": "c5b439fa6766e4d9dabf09d1b0d686311b494914",
    "semantic_title": "gradient-regularized out-of-distribution detection",
    "citation_count": 2,
    "authors": [
      "Taha Entesari*",
      "Sina Sharifi*",
      "Bardia Safaei*",
      "Vishal Patel",
      "Mahyar Fazlyab"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2142_ECCV_2024_paper.php": {
    "title": "Event-based Mosaicing Bundle Adjustment",
    "volume": "main",
    "abstract": "We tackle the problem of mosaicing bundle adjustment (i.e., simultaneous refinement of camera orientations and scene map) for a purely rotating event camera. We formulate the problem as a regularized non-linear least squares optimization. The objective function is defined using the linearized event generation model in the camera orientations and the panoramic gradient map of the scene. We show that this BA optimization has an exploitable block-diagonal sparsity structure, so that the problem can be solved efficiently. To the best of our knowledge, this is the first work to leverage such sparsity to speed up the optimization in the context of event-based cameras, without the need to convert events into image-like representations. We evaluate our method, called EMBA, on both synthetic and real-world datasets to show its effectiveness (50% photometric error decrease), yielding results of unprecedented quality. In addition, we demonstrate EMBA using high spatial resolution event cameras, yielding delicate panoramas in the wild, even without an initial map. The code is available at https: //github.com/tub-rip/emba",
    "checked": true,
    "id": "382492403ccc338db02e395a1a0bda35b598fb41",
    "semantic_title": "event-based mosaicing bundle adjustment",
    "citation_count": 0,
    "authors": [
      "Shuang Guo*",
      "Guillermo Gallego"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2146_ECCV_2024_paper.php": {
    "title": "ProMerge: Prompt and Merge for Unsupervised Instance Segmentation",
    "volume": "main",
    "abstract": "Unsupervised instance segmentation aims to segment distinct object instances in an image without relying on human-labeled data. This field has recently seen significant advancements, partly due to the strong local correspondences afforded by rich visual feature representations from self-supervised models (e.g., DINO). Recent state-of-the-art approaches use self-supervised features to represent images as graphs and solve a generalized eigenvalue system (i.e., normalized-cut) to generate foreground masks. While effective, this strategy is limited by its attendant computational demands, leading to slow inference speeds. In this paper, we propose Prompt and Merge (), which leverages self-supervised visual features to obtain initial groupings of patches and applies a strategic merging to these segments, aided by a sophisticated background-based mask pruning technique. ProMerge not only yields competitive results but also offers a significant reduction in inference time compared to state-of-the-art normalized-cut-based approaches. Furthermore, when training an object detector using our mask predictions as pseudo-labels, the resulting detector surpasses the current leading unsupervised model on various challenging instance segmentation benchmarks. Keywords: Unsupervised Instance Segmentation · Prompt and Merge",
    "checked": true,
    "id": "3376b10fa23867202f67c761ebd40a75a62136b2",
    "semantic_title": "promerge: prompt and merge for unsupervised instance segmentation",
    "citation_count": 0,
    "authors": [
      "Dylan J Li",
      "Gyungin Shin*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2148_ECCV_2024_paper.php": {
    "title": "M2D2M: Multi-Motion Generation from Text with Discrete Diffusion Models",
    "volume": "main",
    "abstract": "We introduce the Multi-Motion Discrete Diffusion Models (M2D2M), a novel approach for human motion generation from textual descriptions of multiple actions, utilizing the strengths of discrete diffusion models. This approach adeptly addresses the challenge of generating multi-motion sequences, ensuring seamless transitions of motions and coherence across a series of actions. The strength of M2D2M lies in its dynamic transition probability within the discrete diffusion model, which adapts transition probabilities based on the proximity between motion tokens, encouraging mixing between different modes. Complemented by a two-phase sampling strategy that includes independent and joint denoising steps, M2D2M effectively generates long-term, smooth, and contextually coherent human motion sequences, utilizing a model trained for single-motion generation. Extensive experiments demonstrate that M2D2M surpasses current state-of-the-art benchmarks for motion generation from text descriptions, showcasing its efficacy in interpreting language semantics and generating dynamic, realistic motions",
    "checked": true,
    "id": "4af09ac56b2d9321efac3c8cb8f17ca04eeb61e0",
    "semantic_title": "m2d2m: multi-motion generation from text with discrete diffusion models",
    "citation_count": 1,
    "authors": [
      "Seunggeun Chi*",
      "Hyung-gun Chi",
      "Hengbo Ma",
      "Nakul Agarwal",
      "Faizan Siddiqui",
      "Karthik Ramani*",
      "Kwonjoon Lee*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2149_ECCV_2024_paper.php": {
    "title": "The Hard Positive Truth about Vision-Language Compositionality",
    "volume": "main",
    "abstract": "Several benchmarks have concluded that our best vision-language models (, CLIP) are lacking in compositionality. Given an image, these benchmarks probe a model's ability to identify its associated caption amongst a set of compositional distractors. In response, a surge of recent proposals show improvements by finetuning CLIP with distractors as hard negatives. Our investigations reveal that these improvements have, in fact, been overstated — because existing benchmarks do not probe whether finetuned models remain invariant to hard positives. By curating an evaluation dataset with 112, 382 hard negatives and hard positives, we uncover that including hard positives decreases CLIP's performance by 12.9%, while humans perform effortlessly at 99%. CLIP finetuned with hard negatives results in an even larger decrease, up to 38.7%. With this finding, we then produce a 1,775,259 image-text training set with both hard negative and hard positive captions. By training with both, we see improvements on existing benchmarks while simultaneously improving performance on hard positives, indicating a more robust improvement in compositionality. Our work suggests the need for future research to rigorously test and improve CLIP's understanding of semantic relationships between related \"positive\" concepts",
    "checked": true,
    "id": "0360d615efc4ad5fdf09a14f198c085fd18f0395",
    "semantic_title": "the hard positive truth about vision-language compositionality",
    "citation_count": 2,
    "authors": [
      "Amita Kamath*",
      "Cheng-Yu Hsieh",
      "Kai-Wei Chang",
      "Ranjay Krishna"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2153_ECCV_2024_paper.php": {
    "title": "GaussCtrl: Multi-View Consistent Text-Driven 3D Gaussian Splatting Editing",
    "volume": "main",
    "abstract": "We propose , a text-driven method to edit a 3D scene reconstructed by the 3D Gaussian Splatting (3DGS). Our method first renders a collection of images by using the 3DGS and edits them by using a pre-trained 2D diffusion model (ControlNet) based on the input prompt, which is then used to optimise the 3D model. Our key contribution is multi-view consistent editing, which enables editing all images together instead of iteratively editing one image while updating the 3D model as in previous works. It leads to faster editing as well as higher visual quality. This is achieved by the two terms: (a) depth-conditioned editing that enforces geometric consistency across multi-view images by leveraging naturally consistent depth maps. (b) attention-based latent code alignment that unifies the appearance of edited images by conditioning their editing to several reference views through self and cross-view attention between images' latent representations. Experiments demonstrate that our method achieves faster editing and better visual results than previous state-of-the-art methods. Project website: https://gaussctrl.active.vision/",
    "checked": true,
    "id": "3a713aaf8fff8841573f576beaef0bb20d094d34",
    "semantic_title": "gaussctrl: multi-view consistent text-driven 3d gaussian splatting editing",
    "citation_count": 8,
    "authors": [
      "Jing Wu*",
      "Jia-Wang Bian",
      "Xinghui Li",
      "Guangrun Wang",
      "Ian Reid",
      "Philip Torr",
      "Victor Adrian Prisacariu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2155_ECCV_2024_paper.php": {
    "title": "Shapefusion: 3D localized human diffusion models",
    "volume": "main",
    "abstract": "In the realm of 3D computer vision, parametric models have emerged as a ground-breaking methodology for the creation of realistic and expressive 3D avatars. Traditionally, they rely on Principal Component Analysis (PCA), given its ability to decompose data to an orthonormal space that maximally captures shape variations. However, due to the orthogonality constraints and the global nature of PCA's decomposition, these models struggle to perform localized and disentangled editing of 3D shapes, which severely affects their use in applications requiring fine control such as face sculpting. In this paper, we leverage diffusion models to enable diverse and fully localized edits on 3D meshes, while completely preserving the un-edited regions. We propose an effective diffusion masking training strategy that, by design, facilitates localized manipulation of any shape region, without being limited to predefined regions or to sparse sets of predefined control vertices. Following our framework, a user can explicitly set their manipulation region of choice and define an arbitrary set of vertices as handles to edit a 3D mesh. Compared to the current state-of-the-art our method leads to more interpretable shape manipulations than methods relying on latent code state, greater localization and generation diversity while offering faster inference than optimization based approaches. Project page: https://rolpotamias.github.io/Shapefusion/",
    "checked": false,
    "id": "055638a8c1a633575146828c4a1a4b57ff7b7486",
    "semantic_title": "shapefusion: a 3d diffusion model for localized shape editing",
    "citation_count": 1,
    "authors": [
      "Rolandos Alexandros Potamias*",
      "Michael Tarasiou",
      "Stylianos Ploumpis",
      "Stefanos Zafeiriou"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2157_ECCV_2024_paper.php": {
    "title": "Eta Inversion: Designing an Optimal Eta Function for Diffusion-based Real Image Editing",
    "volume": "main",
    "abstract": "Diffusion models have achieved remarkable success in the domain of text-guided image generation and, more recently, in text-guided image editing. A commonly adopted strategy for editing real images involves inverting the diffusion process to obtain a noisy representation of the original image, which is then denoised to achieve the desired edits. However, current methods for diffusion inversion often struggle to produce edits that are both faithful to the specified text prompt and closely resemble the source image. To overcome these limitations, we introduce a novel and adaptable diffusion inversion technique for real image editing, which is grounded in a theoretical analysis of the role of η in the DDIM sampling equation for enhanced editability. By designing a universal diffusion inversion method with a time- and region-dependent η function, we enable flexible control over the editing extent. Through a comprehensive series of quantitative and qualitative assessments, involving a comparison with a broad array of recent methods, we demonstrate the superiority of our approach. Our method not only sets a new benchmark in the field but also significantly outperforms existing strategies",
    "checked": true,
    "id": "4789eb65ddc5b42e804d5d000cd735cb70f30b0a",
    "semantic_title": "eta inversion: designing an optimal eta function for diffusion-based real image editing",
    "citation_count": 0,
    "authors": [
      "Wonjun Kang",
      "Kevin Galim",
      "Hyung Il Koo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2160_ECCV_2024_paper.php": {
    "title": "Prompting Language-Informed Distribution for Compositional Zero-Shot Learning",
    "volume": "main",
    "abstract": "Compositional zero-shot learning (CZSL) task aims to recognize unseen compositional visual concepts, , sliced tomatoes, where the model is learned only from the seen compositions, , sliced potatoes and red tomatoes. Thanks to the prompt tuning on large pre-trained visual language models such as CLIP, recent literature shows impressively better CZSL performance than traditional vision-based methods. However, the key aspects that impact the generalization to unseen compositions, including the diversity and informativeness of class context, and the entanglement between visual primitives, , state and object, are not properly addressed in existing CLIP-based CZSL literature. In this paper, we propose a model by prompting the language-informed distribution, aka., P LID, for the CZSL task. Specifically, the P LID leverages pre-trained large language models (LLM) to (i ) formulate the language-informed class distributions which are diverse and informative, and (ii ) enhance the compositionality of the class embedding. Moreover, a visual-language primitive decomposition (VLPD) module is proposed to dynamically fuse the classification decisions from the compositional and the primitive space. Orthogonal to the existing literature of soft, hard, or distributional prompts, our method advocates prompting the LLM-supported class distributions, leading to a better zero-shot generalization. Experimental results on MIT-States, UT-Zappos, and C-GQA datasets show the superior performance of the P LID to the prior arts. Our code and models are released: https://github.com/Cogito2012/PLID",
    "checked": true,
    "id": "2ff69c238e26c473a6d8bcbb9292ded74d7fd1c2",
    "semantic_title": "prompting language-informed distribution for compositional zero-shot learning",
    "citation_count": 4,
    "authors": [
      "Wentao Bao*",
      "Lichang Chen",
      "Heng Huang",
      "Yu Kong"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2167_ECCV_2024_paper.php": {
    "title": "Wear-Any-Way: Manipulable Virtual Try-on via Sparse Correspondence Alignment",
    "volume": "main",
    "abstract": "This paper introduces a novel framework for virtual try-on, termed . Different from previous methods, is a customizable solution. Besides generating high-fidelity results, our method supports users to precisely manipulate the wearing style. To achieve this goal, we first construct a strong pipeline for standard virtual try-on, supporting single/multiple garment try-on and model-to-model settings in complicated scenarios. To make it manipulable, we propose sparse correspondence alignment which involves point-based control to guide the generation for specific locations. With this design, Wear-Any-Way gets state-of-the-art performance for the standard setting and provides a novel interaction form for customizing the wearing style. For instance, it supports users to drag the sleeve to make it rolled up, drag the coat to make it open, and utilize clicks to control the style of tuck, etc. enables more liberated and flexible expressions of the attires, holding profound implications in the fashion industry. Project page is mengtingchen.github.io/wear-any-way-page",
    "checked": true,
    "id": "7143cb3e4975c12271f01e1fa3f5dead597a8dfb",
    "semantic_title": "wear-any-way: manipulable virtual try-on via sparse correspondence alignment",
    "citation_count": 11,
    "authors": [
      "Mengting Chen*",
      "Xi Chen",
      "Zhonghua Zhai",
      "Chen Ju",
      "Xuewen Hong",
      "Jinsong Lan",
      "Shuai Xiao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2170_ECCV_2024_paper.php": {
    "title": "3iGS: Factorised Tensorial Illumination for 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "The use of 3D Gaussians as representation of radiance fields has enabled high quality novel view synthesis at real-time rendering speed. However, the choice of optimising the outgoing radiance of each Gaussian independently as spherical harmonics results in unsatisfactory view dependent effects. In response to these limitations, our work, Factorised Tensorial Illumination for 3D Gaussian Splatting, or 3iGS, improves upon 3D Gaussian Splatting (3DGS) rendering quality. Instead of optimising a single outgoing radiance parameter, 3iGS enhances 3DGS view-dependent effects by expressing the outgoing radiance as a function of a local illumination field and Bidirectional Reflectance Distribution Function (BRDF) features. We optimise a continuous incident illumination field through a Tensorial Factorisation representation, while separately fine-tuning the BRDF features of each 3D Gaussian relative to this illumination field. Our methodology significantly enhances the rendering quality of specular view-dependent effects of 3DGS, while maintaining rapid training and rendering speeds",
    "checked": true,
    "id": "6e59a0738bc553680510087334ce609e0ffc80e5",
    "semantic_title": "3igs: factorised tensorial illumination for 3d gaussian splatting",
    "citation_count": 0,
    "authors": [
      "Zhe Jun Tang*",
      "Tat-Jen Cham"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2177_ECCV_2024_paper.php": {
    "title": "Distribution-Aware Robust Learning from Long-Tailed Data with Noisy Labels",
    "volume": "main",
    "abstract": "Deep neural networks have demonstrated remarkable advancements in various fields using large, well-annotated datasets. However, real-world data often exhibit long-tailed distributions and label noise, significantly degrading generalization performance. Recent studies addressing these issues have focused on noisy sample selection methods that estimate the centroid of each class based on high-confidence samples within each target class. The performance of these methods is limited because they use only the training samples within each class for class centroid estimation, making the quality of centroids susceptible to long-tailed distributions and noisy labels. In this study, we present a robust training framework called Distribution-aware Sample Selection and Contrastive Learning (DaSC). Specifically, DaSC introduces a Distribution-aware Class Centroid Estimation (DaCC) to generate enhanced class centroids. DaCC performs weighted averaging of the features from all samples, with weights determined based on model predictions. Additionally, we propose a confidence-aware contrastive learning strategy to obtain balanced and robust representations. The training samples are categorized into high-confidence and low-confidence samples. Our method then applies Semi-supervised Balanced Contrastive Loss (SBCL) using high-confidence samples, leveraging reliable label information to mitigate class bias. For the low-confidence samples, our method computes Mixup-enhanced Instance Discrimination Loss (MIDL) to improve their representations in a self-supervised manner. Our experimental results on CIFAR and real-world noisy-label datasets demonstrate the superior performance of the proposed DaSC compared to previous approaches. Our code is available at https://github.com/JaesoonBaik1213/DaSC",
    "checked": true,
    "id": "7128401afa9dec32502a8162a87921ccfaff381a",
    "semantic_title": "distribution-aware robust learning from long-tailed data with noisy labels",
    "citation_count": 0,
    "authors": [
      "Jae Soon Baik*",
      "In Young Yoon",
      "Kun Hoon Kim",
      "Jun Won Choi*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2178_ECCV_2024_paper.php": {
    "title": "Free-Viewpoint Video of Outdoor Sports Using a Drone",
    "volume": "main",
    "abstract": "We propose a novel drone application under real-world scenarios – free-viewpoint rendering of outdoor sports scenes, including the dynamic athlete and the 360° background. Outdoor sports have long-range human motions and large-scale scene structures which make the task rather challenging. Existing methods either rely on dense camera arrays which costs much, or a handheld moving camera which struggles to handle real sports scenes. We build a novel drone-based system using an RGB camera to reconstruct the 4D dynamic human along with the 3D unbounded scene, rendering free-viewpoint videos at any time. We also propose submodules for calibration and human motion capture, as a system-level design for improved robustness and efficiency. We collect a dataset AerialRecon and conduct extensive experiments on real-world scenarios. Compared with existing SOTA systems, our system demonstrates superior performance and applicability to real-world outdoor sports scenes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengdong Hong*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2182_ECCV_2024_paper.php": {
    "title": "Wavelength-Embedding-guided Filter-Array Transformer for Spectral Demosaicing",
    "volume": "main",
    "abstract": "Spectral imaging offers the capability to unveil hidden details within the world around us. However, to fully harness this potential, it is imperative to develop effective spectral demosaicing techniques. Despite the success of learning based spectral demosaicing methods, three challenges hinder their practical use. Firstly, existing convolutional neural networks and attention-based models, struggle to capture spectral similarities and long-range dependencies. Secondly, their performance is unstable when optical characteristics, like multispectral filter array (MSFA) arrangement and wavelength distribution, change. Lastly, they lack a structured approach to incorporating imaging system physics, such as MSFA pattern. Addressing these challenges, our paper introduces the Wavelength Embedding guided Filter Array Attention Transformer (WeFAT) for effective spectral demosaicing. Specifically, inspired by the timestep embedding in denoising diffusion models, we propose a Wavelength Embedding guided Multi-head Self-Attention (We-MSA) mechanism to imbue our model with wavelength memory, facilitating adaptation to diverse cameras. This approach treats each spectral feature as a token, directly integrating wavelength information into attention calculation. Additionally, we developed a MSFA-attention Mechanism (MaM) steering We-MSA to focus on spatial regions yielding high-quality spectral data. Experimental results affirm that WeFAT exhibits strong performance consistency across diverse cameras characterized by varying spectral distributions and MSFA patterns, trained solely on ARAD dataset. It also outperforms current state-of-the-art methods in both simulated and real datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haijin Zeng*",
      "Hiep Luong",
      "Wilfried Philips"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2187_ECCV_2024_paper.php": {
    "title": "ConGeo: Robust Cross-view Geo-localization across Ground View Variations",
    "volume": "main",
    "abstract": "∗ Equal contribution Corresponding author (xuchangeis@whu.edu.cn) Cross-view geo-localization aims at localizing a ground-level query image by matching it to its corresponding geo-referenced aerial view. In real-world scenarios, the task requires accommodating diverse ground images captured by users with varying orientations and reduced field of views (FoVs). However, existing learning pipelines are orientation-specific or FoV-specific, demanding separate model training for different ground view variations. Such models heavily depend on the North-aligned spatial correspondence and predefined FoVs in the training data, compromising their robustness across different settings. To tackle this challenge, we propose ConGeo, a single- and cross-view Contrastive method for Geo-localization: it enhances robustness and consistency in feature representations to improve a model's invariance to orientation and its resilience to FoV variations, by enforcing proximity between ground view variations of the same location. As a generic learning objective for cross-view geo-localization, when integrated into state-of-the-art pipelines, ConGeo significantly boosts the performance of three base models on four geo-localization benchmarks for diverse ground view variations and outperforms competing methods that train separate models for each ground view variation",
    "checked": true,
    "id": "7f3b60e6e9961861417ae84a8e505ec0c6ce3a35",
    "semantic_title": "congeo: robust cross-view geo-localization across ground view variations",
    "citation_count": 1,
    "authors": [
      "Li Mi",
      "Chang Xu*",
      "Javiera Castillo Navarro",
      "SYRIELLE MONTARIOL",
      "Wen Yang",
      "Antoine Bosselut",
      "Devis Tuia"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2196_ECCV_2024_paper.php": {
    "title": "Generalizable Facial Expression Recognition",
    "volume": "main",
    "abstract": "SOTA facial expression recognition (FER) methods fail on test sets that have domain gaps with the train set. Recent domain adaptation FER methods need to acquire labeled or unlabeled samples of target domains to fine-tune the FER model, which might be infeasible in real-world deployment. In this paper, we aim to improve the zero-shot generalization ability of FER methods on different unseen test sets using only one train set. Inspired by how humans first detect faces and then select expression features, we propose a novel FER pipeline to extract expression-related features from any given face images. Our method is based on the generalizable face features extracted by large models like CLIP. However, it is non-trivial to adapt the general features of CLIP for specific tasks like FER. To preserve the generalization ability of CLIP and the high precision of the FER model, we design a novel approach that learns sigmoid masks based on the fixed CLIP face features to extract expression features. To further improve the generalization ability on unseen test sets, we separate the channels of the learned masked features according to the expression classes to directly generate logits and avoid using the FC layer to reduce overfitting. We also introduce a channel-diverse loss to make the learned masks separated. Extensive experiments on five different FER datasets verify that our method outperforms SOTA FER methods by large margins. Code is available in https: //github.com/zyh-uaiaaaa/Generalizable-FER",
    "checked": true,
    "id": "f557306e1983ca1a823ae4cffbfb71b6175cb91c",
    "semantic_title": "generalizable facial expression recognition",
    "citation_count": 0,
    "authors": [
      "Yuhang Zhang",
      "Xiuqi Zheng",
      "Chenyi Liang",
      "Jiani Hu*",
      "Weihong Deng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2197_ECCV_2024_paper.php": {
    "title": "GAURA: Generalizable Approach for Unified Restoration and Rendering of Arbitrary Views",
    "volume": "main",
    "abstract": "Neural rendering methods can achieve near-photorealistic image synthesis of scenes from posed input images. However, when the images are imperfect, e.g., captured in very low-light conditions, state-of-the-art methods fail to reconstruct high-quality 3D scenes. Recent approaches have tried to address this limitation by modeling various degradation processes in the image formation model; however, this limits them to specific image degradations. In this paper, we propose a generalizable neural rendering method that can perform high-fidelity novel view synthesis under several degradations. Our method, GAURA, is learning-based and does not require any test-time scene-specific optimization. It is trained on a synthetic dataset that includes several degradation types. GAURA outperforms state-of-the-art methods on several benchmarks for low-light enhancement, dehazing, deraining, and on-par for motion deblurring. Further, our model can be efficiently fine-tuned to any new incoming degradation using minimal data. We thus demonstrate adaptation results on two unseen degradations, desnowing and removing defocus blur. Code and video results are available at vinayak-vg.github.io/GAURA",
    "checked": true,
    "id": "80670996ccac70d302269402caa3ff2841c4a816",
    "semantic_title": "gaura: generalizable approach for unified restoration and rendering of arbitrary views",
    "citation_count": 0,
    "authors": [
      "Vinayak Gupta*",
      "Rongali Simhachala Venkata Girish",
      "Mukund Varma T",
      "Ayush Tewari",
      "Kaushik Mitra"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2209_ECCV_2024_paper.php": {
    "title": "Self-Supervised Any-Point Tracking by Contrastive Random Walks",
    "volume": "main",
    "abstract": "We present a simple, self-supervised approach to the Tracking Any Point (TAP) problem. We train a global matching transformer to find cycle consistent tracks through video via contrastive random walks, using the transformer's attention-based global matching to define the transition matrices for a random walk on a space-time graph. The ability to perform \"all pairs\" comparisons between points allows the model to obtain high spatial precision and to obtain a strong contrastive learning signal, while avoiding many of the complexities of recent approaches (such as coarse-to-fine matching). To do this, we propose a number of design decisions that allow global matching architectures to be trained through self-supervision using cycle consistency. For example, we identify that transformer-based methods are sensitive to shortcut solutions, and propose a data augmentation scheme to address them. Our method achieves strong performance on the TapVid benchmarks, outperforming previous self-supervised tracking methods, such as DIFT, and is competitive with several supervised methods",
    "checked": true,
    "id": "e47304c365d2a36596c01958afb8e4a94230d9c3",
    "semantic_title": "self-supervised any-point tracking by contrastive random walks",
    "citation_count": 0,
    "authors": [
      "Ayush Shrivastava*",
      "Andrew Owens"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2212_ECCV_2024_paper.php": {
    "title": "MixDQ: Memory-Efficient Few-Step Text-to-Image Diffusion Models with Metric-Decoupled Mixed Precision Quantization",
    "volume": "main",
    "abstract": "Few-step diffusion models, which enable high-quality text-to-image generation with only a few denoising steps, have substantially reduced inference time. However, considerable memory consumption (5-10GB) still poses limitations for practical deployment on mobile devices. Post-Training Quantization (PTQ) proves to be an effective method for enhancing efficiency in both memory and operational complexity. However, when applied to few-step diffusion models, existing methods designed for multi-step diffusion face challenges in preserving both visual quality and text alignment. In this paper, we discover that the quantization is bottlenecked by highly sensitive layers. Consequently, we introduce a mixed-precision quantization method: MixDQ. Firstly, we identify some highly sensitive layers are caused by outliers in text embeddings, and design a specialized Begin-Of-Sentence (BOS)-aware quantization to address this issue. Subsequently, we investigate the drawback of existing sensitivity metrics, and introduce metric-decoupled sensitivity analysis to accurately estimate sensitivity for both image quality and content. Finally, we develop an integer-programming-based method to obtain the optimal mixed-precision configuration. In the challenging 1-step Stable Diffusion XL text-to-image task, current quantization methods fall short at W8A8. Remarkably, MixDQ achieves W3.66A16 and W4A8 quantization with negligible degradation in both visual quality and text alignment. Compared with FP16, it achieves 3-4× reduction in model size and memory costs, along with a 1.5× latency speedup. The project URL is purplehttps://a-suozhang.xyz/mixdq.github.io/",
    "checked": true,
    "id": "a2df3d49d5d0592efc1ab2b55cdf66dea1c2a6a5",
    "semantic_title": "mixdq: memory-efficient few-step text-to-image diffusion models with metric-decoupled mixed precision quantization",
    "citation_count": 4,
    "authors": [
      "Tianchen Zhao*",
      "Xuefei Ning",
      "Tongcheng Fang",
      "Enshu Liu",
      "Guyue Huang",
      "Zinan Lin",
      "Shengen Yan",
      "Guohao Dai",
      "Yu Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2220_ECCV_2024_paper.php": {
    "title": "Siamese Vision Transformers are Scalable Audio-visual Learners",
    "volume": "main",
    "abstract": "Traditional audio-visual methods rely on independent audio and visual backbones, which is costly and not scalable. In this work, we investigate using an audio-visual siamese network () for efficient and scalable audio-visual pretraining. Our framework uses a single shared vision transformer backbone to process audio and visual inputs, improving its parameter efficiency, reducing the GPU memory footprint, and allowing us to scale our method to larger datasets and model sizes. We pretrain our model using a contrastive audio-visual matching objective with a multi-ratio random masking scheme, which enables our model to process larger audio-visual instance batches, helpful for contrastive learning. Unlike prior audio-visual methods, our method can robustly handle audio, visual, and audio-visual inputs with a single shared ViT backbone. Furthermore, despite using the shared backbone for both modalities, achieves competitive or even better results than prior methods on AudioSet and VGGSound for audio-visual classification and retrieval. Our code is available at https://github.com/ GenjiB/AVSiam",
    "checked": true,
    "id": "07b86500fa814ea09b5c6adc71b638206cf9b3d5",
    "semantic_title": "siamese vision transformers are scalable audio-visual learners",
    "citation_count": 1,
    "authors": [
      "Yan-Bo Lin*",
      "Gedas Bertasius"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2228_ECCV_2024_paper.php": {
    "title": "LCM-Lookahead for Encoder-based Text-to-Image Personalization",
    "volume": "main",
    "abstract": "Recent advancements in diffusion models have introduced fast sampling methods that can effectively produce high-quality images in just one or a few denoising steps. Interestingly, when these are distilled from existing diffusion models, they often maintain alignment with the original model, retaining similar outputs for similar prompts and seeds. These properties present opportunities to leverage fast sampling methods as a shortcut-mechanism, using them to create a preview of denoised outputs through which we can backpropagate image-space losses. In this work, we explore the potential of using such shortcut-mechanisms to guide the personalization of text-to-image models to specific facial identities. We focus on encoder-based personalization approaches, and demonstrate that by augmenting their training with a lookahead identity loss, we can achieve higher identity fidelity, without sacrificing layout diversity or prompt alignment. Code at https://lcm-lookahead.github.io/",
    "checked": true,
    "id": "544f01263571d95a3eeb214bfb58771b698681b5",
    "semantic_title": "lcm-lookahead for encoder-based text-to-image personalization",
    "citation_count": 7,
    "authors": [
      "Rinon Gal*",
      "Or Lichter",
      "Elad Richardson",
      "Or Patashnik",
      "Amit Bermano",
      "Gal Chechik",
      "Danny Cohen-Or"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2230_ECCV_2024_paper.php": {
    "title": "Towards Architecture-Agnostic Untrained Networks Priors for Image Reconstruction with Frequency Regularization",
    "volume": "main",
    "abstract": "Untrained networks inspired by deep image priors have shown promising capabilities in recovering high-quality images from noisy or partial measurements without requiring training sets. Their success is widely attributed to implicit regularization due to the spectral bias of suitable network architectures. However, the application of such network-based priors often entails superfluous architectural decisions, risks of overfitting, and lengthy optimization processes, all of which hinder their practicality. To address these challenges, we propose efficient architecture-agnostic techniques to directly modulate the spectral bias of network priors: 1) bandwidth-constrained input, 2) bandwidth-controllable upsamplers, and 3) Lipschitz-regularized convolutional layers. We show that, with just a few lines of code, we can reduce overfitting in underperforming architectures and close performance gaps with high-performing counterparts, minimizing the need for extensive architecture tuning. This makes it possible to employ a more compact model to achieve performance similar or superior to larger models while reducing runtime. Demonstrated on inpainting-like MRI reconstruction task, our results signify for the first time that architectural biases, overfitting, and runtime issues of untrained network priors can be simultaneously addressed without architectural modifications. Our code is publicly available 1 . 1 https://github.com/YilinLiu97/Untrained-Recon.git",
    "checked": false,
    "id": "95cdaa3b92dfe7a70b9f6fdda3941616ef0f8d5b",
    "semantic_title": "towards architecture-agnostic untrained network priors for image reconstruction with frequency regularization",
    "citation_count": 0,
    "authors": [
      "Yilin Liu",
      "Yunkui Pang",
      "Jiang Li",
      "Yong Chen",
      "Pew-Thian Yap*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2233_ECCV_2024_paper.php": {
    "title": "Towards Open-Ended Visual Recognition with Large Language Models",
    "volume": "main",
    "abstract": "Localizing and recognizing objects in the open-ended physical world poses a long-standing challenge within the domain of machine perception. Recent methods have endeavored to address the issue by employing a class-agnostic mask (or box) proposal model, complemented by an open-vocabulary classifier (, CLIP) using pre-extracted text embeddings. However, it is worth noting that these open-vocabulary recognition models still exhibit limitations in practical applications. On one hand, they rely on the provision of class names during testing, where the recognition performance heavily depends on this predefined set of semantic classes by users. On the other hand, when training with multiple datasets, human intervention is required to alleviate the label definition conflict between them. In this paper, we introduce the (), a novel Large Language Model (LLM) based mask classifier, as a straightforward and effective solution to the aforementioned challenges. Specifically, predicts class labels in a generative manner, thus removing the supply of class names during both training and testing. It also enables cross-dataset training without any human interference, exhibiting robust generalization capabilities due to the world knowledge acquired from the LLM. By combining with an off-the-shelf mask proposal model, we present promising results on various benchmarks, and demonstrate its effectiveness in handling novel concepts. Code and models are available at https://github. com/bytedance/OmniScient-Model",
    "checked": false,
    "id": "16513bc0dc13902334a9cb3657056763efdcec6f",
    "semantic_title": "towards open-ended visual recognition with large language model",
    "citation_count": 7,
    "authors": [
      "Qihang Yu*",
      "Xiaohui Shen",
      "Liang-Chieh Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2234_ECCV_2024_paper.php": {
    "title": "Ray-Distance Volume Rendering for Neural Scene Reconstruction",
    "volume": "main",
    "abstract": "Existing methods in neural scene reconstruction utilize the Signed Distance Function (SDF) to model the density function. However, in indoor scenes, the density computed from the SDF for a sampled point may not consistently reflect its real importance in volume rendering, often due to the influence of neighboring objects. To tackle this issue, our work proposes a novel approach for indoor scene reconstruction, which instead parameterizes the density function with the Signed Ray Distance Function (SRDF). Firstly, the SRDF is predicted by the network and transformed to a ray-conditioned density function for volume rendering. We argue that the ray-specific SRDF only considers the surface along the camera ray, from which the derived density function is more consistent to the real occupancy than that from the SDF. Secondly, although SRDF and SDF represent different aspects of scene geometries, their values should share the same sign indicating the underlying spatial occupancy. Therefore, this work introduces a SRDF-SDF consistency loss to constrain the signs of the SRDF and SDF outputs. Thirdly, this work proposes a self-supervised visibility task, introducing the physical visibility geometry to the reconstruction task. The visibility task combines prior from predicted SRDF and SDF as pseudo labels, and contributes to generating more accurate 3D geometry. Our method implemented with different representations has been validated on indoor datasets, achieving improved performance in both reconstruction and view synthesis",
    "checked": true,
    "id": "b055fd903d29222e353c49e5b9c4a9eed6b8f26a",
    "semantic_title": "ray-distance volume rendering for neural scene reconstruction",
    "citation_count": 0,
    "authors": [
      "Ruihong Yin*",
      "Yunlu Chen",
      "Sezer Karaoglu",
      "Theo Gevers"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2236_ECCV_2024_paper.php": {
    "title": "ReNoise: Real Image Inversion Through Iterative Noising",
    "volume": "main",
    "abstract": "Recent advancements in text-guided diffusion models have unlocked powerful image manipulation capabilities. However, applying these methods to real images necessitates the inversion of the images into the domain of the pretrained diffusion model. Achieving faithful inversion remains a challenge, particularly for more recent models trained to generate images with a small number of denoising steps. In this work, we introduce an inversion method with a high quality-to-operation ratio, enhancing reconstruction accuracy without increasing the number of operations. Building on reversing the diffusion sampling process, our method employs an iterative renoising mechanism at each inversion sampling step. This mechanism refines the approximation of a predicted point along the forward diffusion trajectory, by iteratively applying the pretrained diffusion model, and averaging these predictions. We evaluate the performance of our ReNoise technique using various sampling algorithms and models, including recent accelerated diffusion models. Through comprehensive evaluations and comparisons, we show its effectiveness in terms of both accuracy and speed. Furthermore, we confirm that our method preserves editability by demonstrating text-driven image editing on real images.[-17pt]",
    "checked": true,
    "id": "29f68790aeb8d4f0642660166e52dba3373bb1b6",
    "semantic_title": "renoise: real image inversion through iterative noising",
    "citation_count": 19,
    "authors": [
      "Daniel Garibi*",
      "Or Patashnik",
      "Andrey Voynov",
      "Hadar Averbuch-Elor",
      "Danny Cohen-Or"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2238_ECCV_2024_paper.php": {
    "title": "Attention Decomposition for Cross-Domain Semantic Segmentation",
    "volume": "main",
    "abstract": "This work addresses cross-domain semantic segmentation. While recent CNNs and proposal-free transformers led to significant advances, we introduce a new transformer with a lighter encoder and more complex decoder with query tokens for predicting segmentation masks, called . The domain gap between the source and target domains is reduced with two mechanisms. First, we decompose cross-attention in the decoder into domain-independent and domain-specific parts to enforce the query tokens interact with the domain-independent aspects of the image tokens, shared by the source and target domains, rather than domain-specific counterparts which induce the domain gap. Second, we use the gradient reverse block to control back-propagation of the gradient, and hence introduce adversarial learning in the decoder of . Our results on two benchmark domain shifts – GTA to Cityscapes and SYNTHIA to Cityscapes – show that outperforms SOTA proposal-free methods with significantly lower complexity. The implementation is available at https://github.com/helq2612/ADFormer",
    "checked": true,
    "id": "2c2af399a7b914aea27304c7275d5e5cbfd1b7bd",
    "semantic_title": "attention decomposition for cross-domain semantic segmentation",
    "citation_count": 0,
    "authors": [
      "Liqiang He*",
      "Sinisa Todorovic"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2245_ECCV_2024_paper.php": {
    "title": "Be Yourself: Bounded Attention for Multi-Subject Text-to-Image Generation",
    "volume": "main",
    "abstract": "Text-to-image diffusion models have an unprecedented ability to generate diverse and high-quality images. However, they often struggle to faithfully capture the intended semantics of complex input prompts that include multiple subjects. Recently, numerous layout-to-image extensions have been introduced to improve user control, aiming to localize subjects represented by specific tokens. Yet, these methods often produce semantically inaccurate images, especially when dealing with multiple semantically or visually similar subjects. In this work, we study and analyze the causes of these limitations. Our exploration reveals that the primary issue stems from inadvertent semantic leakage between subjects in the denoising process. This leakage is attributed to the diffusion model's attention layers, which tend to blend the visual features of different subjects. To address these issues, we introduce Bounded Attention, a training-free method for bounding the information flow in the sampling process. Bounded Attention prevents detrimental leakage among subjects and enables guiding the generation to promote each subject's individuality, even with complex multi-subject conditioning. Through extensive experimentation, we demonstrate that our method empowers the generation of multiple subjects that better align with given prompts and layouts",
    "checked": true,
    "id": "b3e6399398f6229f59f964c8330c8b43de18981d",
    "semantic_title": "be yourself: bounded attention for multi-subject text-to-image generation",
    "citation_count": 8,
    "authors": [
      "Omer Dahary*",
      "Or Patashnik",
      "Kfir Aberman",
      "Danny Cohen-Or"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2249_ECCV_2024_paper.php": {
    "title": "Handling The Non-Smooth Challenge in Tensor SVD: A Multi-Objective Tensor Recovery Framework",
    "volume": "main",
    "abstract": "Recently, numerous tensor singular value decomposition (t-SVD)-based tensor recovery methods have shown promise in processing visual data, such as color images and videos. However, these methods often suffer from severe performance degradation when confronted with tensor data exhibiting non-smooth changes. It has been commonly observed in real-world scenarios but ignored by the traditional t-SVD-based methods. In this work, we introduce a novel tensor recovery model with a learnable tensor nuclear norm to address such challenge. We develop a new optimization algorithm named the Alternating Proximal Multiplier Method (APMM) to iteratively solve the proposed tensor completion model. Theoretical analysis demonstrates the convergence of the proposed APMM to the Karush–Kuhn–Tucker (KKT) point of the optimization problem. In addition, we propose a multi-objective tensor recovery framework based on APMM to efficiently explore the correlations of tensor data across its various dimensions, providing a new perspective on extending the t-SVD-based method to higher-order tensor cases. Numerical experiments demonstrated the effectiveness of the proposed method in tensor completion",
    "checked": true,
    "id": "f9db7c8894f1d081896e9e715a9f3607237072d4",
    "semantic_title": "handling the non-smooth challenge in tensor svd: a multi-objective tensor recovery framework",
    "citation_count": 0,
    "authors": [
      "Jingjing Zheng",
      "Wanglong Lu",
      "Wenzhe Wang",
      "Yankai Cao*",
      "Xiaoqin Zhang",
      "Xianta Jiang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2255_ECCV_2024_paper.php": {
    "title": "RodinHD: High-Fidelity 3D Avatar Generation with Diffusion Models",
    "volume": "main",
    "abstract": "We present RodinHD, which can generate high-fidelity 3D avatars from a portrait image. Existing methods fail to capture intricate details such as hairstyles which we tackle in this paper. We first identify an overlooked problem of catastrophic forgetting that arises when fitting triplanes sequentially on many avatars, caused by the MLP decoder sharing scheme. To overcome this issue, we raise a novel data scheduling strategy and a weight consolidation regularization term, which improves the decoder's capability of rendering sharper details. Additionally, we optimize the guiding effect of the portrait image by computing a finer-grained hierarchical representation that captures rich 2D texture cues, and injecting them to the 3D diffusion model at multiple layers via cross-attention. When trained on 46K avatars with a noise schedule optimized for triplanes, the resulting model can generate 3D avatars with notably better details than previous methods and can generalize to in-the-wild portrait input. See fig:teaser for some examples. Project page: https://rodinhd.github.io/",
    "checked": true,
    "id": "92ca0270f7f7757a0db308935c90ebf225eec473",
    "semantic_title": "rodinhd: high-fidelity 3d avatar generation with diffusion models",
    "citation_count": 3,
    "authors": [
      "Bowen Zhang",
      "Yiji Cheng",
      "Chunyu Wang*",
      "Ting Zhang",
      "Jiaolong Yang",
      "Yansong Tang",
      "Feng Zhao",
      "Dong Chen",
      "Baining Guo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2257_ECCV_2024_paper.php": {
    "title": "GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction and Generation",
    "volume": "main",
    "abstract": "We introduce GRM, a large-scale reconstructor capable of recovering a 3D asset from sparse-view images in around 0.1s. GRM is a feed-forward transformer-based model that efficiently incorporates multi-view information to translate the input pixels into pixel-aligned Gaussians, which are unprojected to create a set of densely distributed 3D Gaussians representing a scene. Together, our transformer architecture and the use of 3D Gaussians unlock a scalable and efficient reconstruction framework. Extensive experimental results demonstrate the superiority of our method over alternatives regarding both reconstruction quality and efficiency. We also showcase the potential of GRM in generative tasks, i.e., text-to-3D and image-to-3D, by integrating it with existing multi-view diffusion models. Our project website is at: https://justimyhxu.github.io/projects/grm/",
    "checked": true,
    "id": "e6cb30effd90d563bc680430ecc1317842d33b6e",
    "semantic_title": "grm: large gaussian reconstruction model for efficient 3d reconstruction and generation",
    "citation_count": 63,
    "authors": [
      "Yinghao Xu*",
      "Zifan Shi",
      "Wang Yifan",
      "Hansheng Chen",
      "Ceyuan Yang",
      "Sida Peng",
      "Yujun Shen",
      "Gordon Wetzstein"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2265_ECCV_2024_paper.php": {
    "title": "IRGen: Generative Modeling for Image Retrieval",
    "volume": "main",
    "abstract": "While generative modeling has become prevalent across numerous research fields, its integration into the realm of image retrieval remains largely unexplored and underjustified. In this paper, we present a novel methodology, reframing image retrieval as a variant of generative modeling and employing a sequence-to-sequence model. This approach is harmoniously aligned with the current trend towards unification in research, presenting a cohesive framework that allows for end-to-end differentiable searching. This, in turn, facilitates superior performance via direct optimization techniques. The development of our model, dubbed IRGen, addresses the critical technical challenge of converting an image into a concise sequence of semantic units, which is pivotal for enabling efficient and effective search. Extensive experiments demonstrate that our model achieves state-of-the-art performance on three widely-used image retrieval benchmarks as well as two million-scale datasets, yielding significant improvement compared to prior competitive retrieval methods. In addition, the notable surge in precision scores facilitated by generative modeling presents the potential to bypass the reranking phase, which is traditionally indispensable in practical retrieval workflows. The code is publicly available at https://github.com/yakt00/IRGen",
    "checked": true,
    "id": "4651f455fcdc3add3740a55ffa110e1eb253a539",
    "semantic_title": "irgen: generative modeling for image retrieval",
    "citation_count": 10,
    "authors": [
      "Yidan Zhang*",
      "Ting Zhang*",
      "Dong Chen",
      "Yujing Wang",
      "Qi Chen",
      "Xing Xie",
      "Hao Sun",
      "Weiwei Deng",
      "Qi Zhang",
      "Fan Yang",
      "Mao Yang",
      "Qingmin Liao",
      "Jingdong Wang",
      "Baining Guo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2295_ECCV_2024_paper.php": {
    "title": "Learning Trimodal Relation for Audio-Visual Question Answering with Missing Modality",
    "volume": "main",
    "abstract": "Recent Audio-Visual Question Answering (AVQA) methods rely on complete visual and audio input to answer questions accurately. However, in real-world scenarios, issues such as device malfunctions and data transmission errors frequently result in missing audio or visual modality. In such cases, existing AVQA methods suffer significant performance degradation. In this paper, we propose a framework that ensures robust AVQA performance even when a modality is missing. First, we propose a Relation-aware Missing Modal (RMM) generator with Relation-aware Missing Modal Recalling (RMMR) loss to enhance the ability of the generator to recall missing modal information by understanding the relationships and context among the available modalities. Second, we design an Audio-Visual Relation-aware (AVR) diffusion model with Audio-Visual Enhancing (AVE) loss to further enhance audio-visual features by leveraging the relationships and shared cues between the audio-visual modalities. As a result, our method can provide accurate answers by effectively utilizing available information even when input modalities are missing. We believe our method holds potential applications not only in AVQA research but also in various multi-modal scenarios. The code is available at https://github.com/VisualAIKHU/Missing-AVQA",
    "checked": true,
    "id": "9be5d623682321eeb2cccc88eda845fef345a724",
    "semantic_title": "learning trimodal relation for audio-visual question answering with missing modality",
    "citation_count": 1,
    "authors": [
      "Kyu Ri Park",
      "Hong Joo Lee*",
      "Jung Uk Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2302_ECCV_2024_paper.php": {
    "title": "FastCAD: Real-Time CAD Retrieval and Alignment from Scans and Videos",
    "volume": "main",
    "abstract": "Digitising the 3D world into a clean, CAD model-based representation has important applications for augmented reality and robotics. Current state-of-the-art methods are computationally intensive as they individually encode each detected object and optimise CAD alignments in a second stage. In this work, we propose FastCAD, a real-time method that simultaneously retrieves and aligns CAD models for all objects in a given scene. In contrast to previous works, we directly predict alignment parameters and shape embeddings. We achieve high-quality shape retrievals by learning CAD embeddings in a contrastive learning framework and distilling those into FastCAD. Our single-stage method accelerates the inference time by a factor of 50 compared to other methods operating on RGB-D scans while outperforming them on the challenging Scan2CAD alignment benchmark. Further, our approach collaborates seamlessly with online 3D reconstruction techniques. This enables the real-time generation of precise CAD model-based reconstructions from videos at 10 FPS. Doing so, we significantly improve the Scan2CAD alignment accuracy in the video setting from 43.0% to 48.2% and the reconstruction accuracy from 22.9% to 29.6%",
    "checked": true,
    "id": "7c52e21ce6d054c5ee0f84a71754f4e2dd3a248c",
    "semantic_title": "fastcad: real-time cad retrieval and alignment from scans and videos",
    "citation_count": 0,
    "authors": [
      "Florian Maximilian Langer*",
      "Jihong Ju",
      "Georgi Dikov",
      "Gerhard Reitmayr",
      "Mohsen Ghafoorian"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2304_ECCV_2024_paper.php": {
    "title": "A Simple Latent Diffusion Approach for Panoptic Segmentation and Mask Inpainting",
    "volume": "main",
    "abstract": "Panoptic and instance segmentation networks are often trained with specialized object detection modules, complex loss functions, and ad-hoc post-processing steps to manage the permutation-invariance of the instance masks. This work builds upon Stable Diffusion and proposes a latent diffusion approach for panoptic segmentation, resulting in a simple architecture that omits these complexities. Our training consists of two steps: (1) training a shallow autoencoder to project the segmentation masks to latent space; (2) training a diffusion model to allow image-conditioned sampling in latent space. This generative approach unlocks the exploration of mask completion or inpainting. The experimental validation on COCO and ADE20k yields strong segmentation results. Finally, we demonstrate our model's adaptability to multi-tasking by introducing learnable task embeddings. The code and models will be made available.1 1 https://github.com/segments-ai/latent-diffusion-segmentation",
    "checked": true,
    "id": "1e99d24f9e853f2f3f5718aef201a5550f02147b",
    "semantic_title": "a simple latent diffusion approach for panoptic segmentation and mask inpainting",
    "citation_count": 7,
    "authors": [
      "Wouter Van Gansbeke*",
      "Bert De Brabandere"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2307_ECCV_2024_paper.php": {
    "title": "VISA: Reasoning Video Object Segmentation via Large Language Model",
    "volume": "main",
    "abstract": "Existing Video Object Segmentation (VOS) relies on explicit user instructions, such as categories, masks, or short phrases, restricting their ability to perform complex video segmentation requiring reasoning with world knowledge. In this paper, we introduce a new task, Reasoning Video Object Segmentation (ReasonVOS). This task aims to generate a sequence of segmentation masks in response to implicit text queries that require complex reasoning abilities based on world knowledge and video contexts, which is crucial for structured environment understanding and object-centric interactions, pivotal in the development of embodied AI. To tackle ReasonVOS, we introduce VISA (Video-based large language Instructed Segmentation Assistant), to leverage the world knowledge reasoning capabilities of multi-modal LLMs while possessing the ability to segment and track objects in videos with a mask decoder. Moreover, we establish a comprehensive benchmark consisting of 35,074 instruction-mask sequence pairs from 1,042 diverse videos, which incorporates complex world knowledge reasoning into segmentation tasks for instruction-tuning and evaluation purposes of ReasonVOS models. Experiments conducted on 8 datasets demonstrate the effectiveness of VISA in tackling complex reasoning segmentation and vanilla referring segmentation in both video and image domains. The code and dataset are available at https: //github.com/cilinyan/VISA",
    "checked": false,
    "id": "b31cb3d535af74fc910c42a4dd6ef0f73f8a7385",
    "semantic_title": "visa: reasoning video object segmentation via large language models",
    "citation_count": 3,
    "authors": [
      "Cilin Yan",
      "Haochen Wang",
      "Shilin Yan",
      "Xiaolong Jiang",
      "Yao Hu",
      "Guoliang Kang*",
      "Weidi Xie",
      "Efstratios Gavves"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2309_ECCV_2024_paper.php": {
    "title": "Lego: Learning to Disentangle and Invert Personalized Concepts Beyond Object Appearance in Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "Text-to-Image (T2I) models excel at synthesizing concepts such as nouns, appearances, and styles. To enable customized content creation based on a few example images of a concept, methods such as Textual Inversion and DreamBooth invert the desired concept and enable synthesizing it in new scenes. However, inverting personalized1 concepts that go beyond object appearance and style (adjectives and verbs) through natural language, remains a challenge. Two key characteristics of these concepts contribute to the limitations of current inversion methods. 1) Adjectives and verbs are entangled with nouns (subject) and can hinder appearance-based inversion methods, where the subject appearance leaks into the concept embedding and 2) describing such concepts often extends beyond single word embeddings. In this study, we introduce Lego, a textual inversion method designed to invert subject entangled concepts from a few example images. Lego disentangles concepts from their associated subjects using a simple yet effective Subject Separation step and employs a Context Loss that guides the inversion of single/multi-embedding concepts. In a thorough user study, Lego-generated concepts were preferred over 70% of the time when compared to the baseline in terms of authentically generating concepts according to a reference. Additionally, visual question answering using an LLM suggested Lego-generated concepts are better aligned with the text description of the concept. 1 Please refer to Figure ?? for our definition of personalized",
    "checked": true,
    "id": "2fd71c10e3c8a56b516d7e470cab4d54afe28eb8",
    "semantic_title": "lego: learning to disentangle and invert personalized concepts beyond object appearance in text-to-image diffusion models",
    "citation_count": 8,
    "authors": [
      "Saman Motamed*",
      "Danda Pani Paudel",
      "Luc Van Gool"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2312_ECCV_2024_paper.php": {
    "title": "IDOL: Unified Dual-Modal Latent Diffusion for Human-Centric Joint Video-Depth Generation",
    "volume": "main",
    "abstract": "Significant advances have been made in human-centric video generation, yet the joint video-depth generation problem remains underexplored. Most existing monocular depth estimation methods may not generalize well to synthesized images or videos, and multi-view-based methods have difficulty controlling the human appearance and motion. In this work, we present () for high-quality human-centric joint video-depth generation. Our consists of two novel designs. First, to enable dual-modal generation and maximize the information exchange between video and depth generation, we propose a unified dual-modal U-Net, a parameter-sharing framework for joint video and depth denoising, wherein a modality label guides the denoising target, and cross-modal attention enables the mutual information flow. Second, to ensure a precise video-depth spatial alignment, we propose a motion consistency loss that enforces consistency between the video and depth feature motion fields, leading to harmonized outputs. Additionally, a cross-attention map consistency loss is applied to align the cross-attention map of the video denoising with that of the depth denoising, further facilitating spatial alignment. Extensive experiments on the TikTok and NTU120 datasets show our superior performance, significantly surpassing existing methods in terms of video FVD and depth accuracy",
    "checked": true,
    "id": "fe03ce17f9eedea74001511d931e7ef67052e1c3",
    "semantic_title": "idol: unified dual-modal latent diffusion for human-centric joint video-depth generation",
    "citation_count": 1,
    "authors": [
      "Yuanhao Zhai*",
      "Kevin Lin",
      "Linjie Li",
      "Chung-Ching Lin",
      "Jianfeng Wang",
      "Zhengyuan Yang",
      "David Doermann",
      "Junsong Yuan",
      "Zicheng Liu",
      "Lijuan Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2315_ECCV_2024_paper.php": {
    "title": "Scaling Backwards: Minimal Synthetic Pre-training?",
    "volume": "main",
    "abstract": "Pre-training and transfer learning are an important building block of current computer vision systems. While pre-training is usually performed on large real-world image datasets, in this paper we ask whether this is truly necessary. To this end, we search for a minimal, purely synthetic pre-training dataset that allows us to achieve performance similar to the 1 million images of ImageNet-1k. We construct such a dataset from a single fractal with perturbations. With this, we contribute three main findings. (i) We show that pre-training is effective even with minimal synthetic images, with performance on par with large-scale pre-training datasets like ImageNet-1k for full fine-tuning. (ii) We investigate the single parameter with which we construct artificial categories for our dataset. We find that while the shape differences can be indistinguishable to humans, they are crucial for obtaining strong performances. (iii) Finally, we investigate the minimal requirements for successful pre-training. Surprisingly, we find that a substantial reduction of synthetic images from 1k to 1 can even lead to an increase in pre-training performance, a motivation to further investigate \"scaling backwards\". Finally, we extend our method from synthetic images to real images to see if a single real image can show similar pre-training effect through shape augmentation. We find that the use of grayscale images and affine transformations allows even real images to \"scale backwards\". The code is available at https://github.com/SUPER-TADORY/1p-frac",
    "checked": true,
    "id": "575df82fa431d0d855175be88f035611f29b477e",
    "semantic_title": "scaling backwards: minimal synthetic pre-training?",
    "citation_count": 0,
    "authors": [
      "Ryo Nakamura*",
      "Ryu Tadokoro*",
      "Ryosuke Yamada*",
      "Yuki M Asano*",
      "Iro Laina*",
      "Christian Rupprecht*",
      "Nakamasa Inoue*",
      "Rio Yokota*",
      "Hirokatsu Kataoka*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2316_ECCV_2024_paper.php": {
    "title": "BAMM: Bidirectional Autoregressive Motion Model",
    "volume": "main",
    "abstract": "Generating human motion from text has been dominated by denoising motion models either through diffusion or generative masking process. However, these models face great limitations in usability by requiring prior knowledge of the motion length. Conversely, autoregressive motion models address this limitation by adaptively predicting motion endpoints, at the cost of degraded generation quality and editing capabilities. To address these challenges, we propose Bidirectional Autoregressive Motion Model (BAMM), a novel text-to-motion generation framework. BAMM consists of two key components: (1) a motion tokenizer that transforms 3D human motion into discrete tokens in latent space, and (2) a masked self-attention transformer that autoregressively predicts randomly masked tokens via a hybrid attention masking strategy. By unifying generative masked modeling and autoregressive modeling, BAMM captures rich and bidirectional dependencies among motion tokens, while learning the probabilistic mapping from textual inputs to motion outputs with dynamically-adjusted motion sequence length. This feature enables BAMM to simultaneously achieving high-quality motion generation with enhanced usability and built-in motion editability. Extensive experiments on HumanML3D and KIT-ML datasets demonstrate that BAMM surpasses current state-of-the-art methods in both qualitative and quantitative measures. Our project page is available at https://exitudio.github.io/BAMM-page",
    "checked": true,
    "id": "0dde7e650ed368b2c37559b6fddc21f3617a40ee",
    "semantic_title": "bamm: bidirectional autoregressive motion model",
    "citation_count": 3,
    "authors": [
      "Ekkasit Pinyoanuntapong*",
      "Muhammad Usama Saleem",
      "Pu Wang",
      "Minwoo Lee",
      "Srijan Das",
      "Chen Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2323_ECCV_2024_paper.php": {
    "title": "Event-based Head Pose Estimation: Benchmark and Method",
    "volume": "main",
    "abstract": "Head pose estimation (HPE) is crucial for various applications, including human-computer interaction, augmented reality, and driver monitoring. However, traditional RGB-based methods struggle in challenging conditions like sudden movement and extreme lighting. Event cameras, as a neuromorphic sensor, have the advantages of high temporal resolution and high dynamic range, offering a promising solution for HPE. However, the lack of paired event and head pose data hinders the full potential of event-based HPE. To address this, we introduce two large-scale, diverse event-based head pose datasets encompassing 282 sequences across different resolutions and scenarios. Furthermore, we propose the event-based HPE network, featuring two novel modules: the Event Spatial-Temporal Fusion (ESTF) module and the Event Motion Perceptual Attention (EMPA) module. The ESTF module effectively combines spatial and temporal information from the event streams, while the EMPA module captures crucial motion details across the scene using a large receptive field. We also propose a unified loss function to optimize the network using both angle and rotation matrix information. Extensive experiments demonstrate the superior performance of our network on both datasets, showcasing its effectiveness in handling HPE across various challenging scenarios. The datasets and code are available at https://github. com/Jiahui-Yuan-1/EVHPE",
    "checked": false,
    "id": "54b12666cfa2de1f18cf5493c6214c64c732cc7f",
    "semantic_title": "ds-hpe: deep set for head pose estimation",
    "citation_count": 0,
    "authors": [
      "Jiahui Yuan*",
      "Hebei Li",
      "Yansong Peng",
      "Jin Wang",
      "Yuheng Jiang",
      "Yueyi Zhang*",
      "Xiaoyan Sun"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2330_ECCV_2024_paper.php": {
    "title": "Avatar Fingerprinting for Authorized Use of Synthetic Talking-Head Videos",
    "volume": "main",
    "abstract": "Modern avatar generators allow anyone to synthesize photorealistic real-time talking avatars, ushering in a new era of avatar-based human communication, such as with immersive AR/VR interactions or videoconferencing with limited bandwidths. Their safe adoption, however, requires a mechanism to verify if the rendered avatar is trustworthy: does it use the appearance of an individual without their consent? We term this task avatar fingerprinting. To tackle it, we first introduce a large-scale dataset of real and synthetic videos of people interacting on a video call, where the synthetic videos are generated using the facial appearance of one person and the expressions of another. We verify the identity driving the expressions in a synthetic video, by learning motion signatures that are independent of the facial appearance shown. Our solution, the first in this space, achieves an average AUC of 0.85. Critical to its practical use, it also generalizes to new generators never seen in training (average AUC of 0.83). The proposed dataset and other resources can be found at: https: //research.nvidia.com/labs/nxp/avatar-fingerprinting/",
    "checked": true,
    "id": "ec4ae221f77e107bd68c7d30dc6b21a9262d5937",
    "semantic_title": "avatar fingerprinting for authorized use of synthetic talking-head videos",
    "citation_count": 1,
    "authors": [
      "Ekta Prashnani*",
      "Koki Nagano",
      "Shalini De Mello",
      "David P Luebke",
      "Orazio Gallo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2339_ECCV_2024_paper.php": {
    "title": "Towards Multi-modal Transformers in Federated Learning",
    "volume": "main",
    "abstract": "Multi-modal transformers mark significant progress in different domains, but privacy concerns on high-quality data hinder their further improvement. Federated learning (FL) has emerged as a promising privacy-preserving paradigm for training models without direct access to the raw data held by different clients. Despite its potential, a considerable research direction regarding the unpaired uni-modal clients and the transformer architecture in FL remains unexplored. To fill this gap, this paper explores a transfer multi-modal federated learning (MFL) scenario within the vision-language domain, where clients possess data of various modalities distributed across different datasets. We systematically evaluate the performance of existing methods when a transformer architecture is utilized and introduce a novel framework called Federated modality complementary and collaboration (FedCola) by addressing the in-modality and cross-modality gaps among clients. Through extensive experiments across various FL settings, FedCola demonstrates superior performance over previous approaches, offering new perspectives on future federated training of multi-modal transformers. Code is available at magentahttps://github.com/imguangyu/FedCola",
    "checked": true,
    "id": "85bd131643a1eb1e59c2cf59d5495d4cb997a41d",
    "semantic_title": "towards multi-modal transformers in federated learning",
    "citation_count": 0,
    "authors": [
      "Guangyu Sun*",
      "Matias Mendieta",
      "Aritra Dutta",
      "Xin Li",
      "Chen Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2353_ECCV_2024_paper.php": {
    "title": "Fisher Calibration for Backdoor-Robust Heterogeneous Federated Learning",
    "volume": "main",
    "abstract": "Federated learning presents massive potential for privacy-friendly vision task collaboration. However, the federated visual performance is deeply affected by backdoor attacks, where malicious clients optimize on triggered samples to mislead the global model into targeted mispredictions. Existing backdoor defensive solutions are normally based on two assumptions: data homogeneity and minority malicious ratio for the elaborate client-wise defensive rules. To address existing limitations, we argue that heterogeneous clients and backdoor attackers both bring divergent optimization directions and thus it is hard to discriminate them precisely. In this paper, we argue that parameters appear in different important degrees towards distinct distribution and instead consider meaningful and meaningless parameters for the ideal target distribution. We propose the Self-Driven Fisher Calibration (SDFC), which utilizes the Fisher Information to calculate the parameter importance degree for the local agnostic and global validation distribution and regulate those elements with large important differences. Furthermore, we allocate high aggregation weight for clients with relatively small overall parameter differences, which encourages clients with close local distribution to the global distribution, to contribute more to the federation. This endows SDFC to handle backdoor attackers in heterogeneous federated learning. Various vision task performances demonstrate the effectiveness of SDFC. The codes are released at https://github.com/WenkeHuang/SDFC",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenke Huang",
      "Mang Ye*",
      "zekun shi",
      "Bo Du*",
      "Dacheng Tao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2355_ECCV_2024_paper.php": {
    "title": "QueryCDR: Query-based Controllable Distortion Rectification Network for Fisheye Images",
    "volume": "main",
    "abstract": "Fisheye image rectification aims to correct distortions in images taken with fisheye cameras. Although current models show promising results on images with a similar degree of distortion as the training data, they will produce sub-optimal results when the degree of distortion changes and without retraining. The lack of generalization ability for dealing with varying degrees of distortion limits their practical application. In this paper, we take one step further to enable effective distortion rectification for images with varying degrees of distortion without retraining. We propose a novel Query-based Controllable Distortion Rectification network for fisheye images (QueryCDR). In particular, we first present the Distortion-aware Learnable Query Mechanism (DLQM), which defines the latent spatial relationships for different distortion degrees as a series of learnable queries. Each query can be learned to obtain position-dependent rectification control conditions, providing control over the rectification process. Then, we propose two kinds of controllable modulating blocks to enable the control conditions to guide the modulation of the distortion features better. These core components cooperate with each other to effectively boost the generalization ability of the model at varying degrees of distortion. Extensive experiments on fisheye image datasets with different distortion degrees demonstrate our approach achieves high-quality and controllable distortion rectification. Code is available at https://github.com/PbGuo/QueryCDR",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengbo Guo",
      "Chengxu Liu",
      "Xingsong Hou*",
      "Xueming Qian"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2356_ECCV_2024_paper.php": {
    "title": "Latent-INR: A Flexible Framework for Implicit Representations of Videos with Discriminative Semantics",
    "volume": "main",
    "abstract": "Implicit Neural Networks (INRs) have emerged as powerful representations to encode all forms of data, including images, videos, audios, and scenes. With video, many INRs for video have been proposed for the compression task, and recent methods feature significant improvements with respect to encoding time, storage, and reconstruction quality. However, these encoded representations lack semantic meaning, so they cannot be used for any downstream tasks that require such properties, such as retrieval. This can act as a barrier for adoption of video INRs over traditional codecs as they do not offer any significant edge apart from compression. To alleviate this, we propose a flexible framework that decouples the spatial and temporal aspects of the video INR. We accomplish this with a dictionary of per-frame latents that are learned jointly with a set of video specific hypernetworks, such that given a latent, these hypernetworks can predict the INR weights to reconstruct the given frame. This framework not only retains the compression efficiency, but the learned latents can be aligned with features from large vision models, which grants them discriminative properties. We align these latents with CLIP and show good performance for both compression and video retrieval tasks. By aligning with VideoLlama, we are able to perform open-ended chat with our learned latents as the visual inputs. Additionally, the learned latents serve as a proxy for the underlying weights, allowing us perform tasks like video interpolation. These semantic properties and applications, existing simultaneously with ability to perform compression, interpolation, and superresolution properties, are a first in this field of work",
    "checked": true,
    "id": "6ed019667f87242caf51393c01a02244df528711",
    "semantic_title": "latent-inr: a flexible framework for implicit representations of videos with discriminative semantics",
    "citation_count": 0,
    "authors": [
      "Shishira R Maiya*",
      "Anubhav Gupta",
      "Matthew A Gwilliam",
      "Max Ehrlich",
      "Abhinav Shrivastava"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2357_ECCV_2024_paper.php": {
    "title": "DCDM: Diffusion-Conditioned-Diffusion Model for Scene Text Image Super-Resolution",
    "volume": "main",
    "abstract": "Severe blurring of scene text images, resulting in the loss of critical strokes and textual information, has a profound impact on text readability and recognizability. Therefore, scene text image super-resolution, aiming to enhance text resolution and legibility in low-resolution images, is a crucial task. In this paper, we introduce a novel generative model for scene text super-resolution called diffusion-conditioned-diffusion model (DCDM). The model is designed to learn the distribution of high-resolution images via two conditions: 1) the low-resolution image and 2) the character-level text embedding generated by a latent diffusion text model. The latent diffusion text module is specifically designed to generate character-level text embedding space from the latent space of low-resolution images. Additionally, the character-level CLIP module has been used to align the high-resolution character-level text embeddings with low-resolution embeddings. This ensures visual alignment with the semantics of scene text image characters. Our experiments on the TextZoom and Real-CE datasets demonstrate the superiority of the proposed method to state-of-the-art methods. The source codes and other resources will be available through the project page: https://github.com/shreygithub/DCDM",
    "checked": false,
    "id": "4e9774a7c69a43f87dd717c2f27e396e05e24f9b",
    "semantic_title": "recognition-guided diffusion model for scene text image super-resolution",
    "citation_count": 1,
    "authors": [
      "Shrey Singh*",
      "Prateek Keserwani",
      "Masakazu Iwamura*",
      "Partha Pratim Roy"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2361_ECCV_2024_paper.php": {
    "title": "Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "As 3D Gaussian Splatting (3DGS) provides fast and high-quality novel view synthesis, it is a natural extension to deform a canonical 3DGS to multiple frames for representing a dynamic scene. However, previous works fail to accurately reconstruct complex dynamic scenes. We attribute the failure to the design of the deformation field, which is built as a coordinate-based function. This approach is problematic because 3DGS is a mixture of multiple fields centered at the Gaussians, not just a single coordinate-based framework. To resolve this problem, we define the deformation as a function of per-Gaussian embeddings and temporal embeddings. Moreover, we decompose deformations as coarse and fine deformations to model slow and fast movements, respectively. Also, we introduce a local smoothness regularization for per-Gaussian embedding to improve the details in dynamic regions. Project page: https://jeongminb.github.io/e-d3dgs/",
    "checked": true,
    "id": "db8bed00c7757e067f36e8e7dfaef2f4ab8ffac0",
    "semantic_title": "per-gaussian embedding-based deformation for deformable 3d gaussian splatting",
    "citation_count": 7,
    "authors": [
      "Jeongmin Bae",
      "Seoha Kim",
      "Youngsik Yun",
      "Hahyun Lee",
      "Gun Bang",
      "Youngjung Uh*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2366_ECCV_2024_paper.php": {
    "title": "DreamMover: Leveraging the Prior of Diffusion Models for Image Interpolation with Large Motion",
    "volume": "main",
    "abstract": "We study the problem of generating intermediate images from image pairs with large motion while maintaining semantic consistency. Due to the large motion, the intermediate semantic information may be absent in input images. Existing methods either limit to small motion or focus on topologically similar objects, leading to artifacts and inconsistency in the interpolation results. To overcome this challenge, we delve into pre-trained image diffusion models for their capabilities in semantic cognition and representations, ensuring consistent expression of the absent intermediate semantic representations with the input. To this end, we propose DreamMover, a novel image interpolation framework with three main components: 1) A natural flow estimator based on the diffusion model that can implicitly reason about the semantic correspondence between two images. 2) To avoid the loss of detailed information during fusion, our key insight is to fuse information in two parts, high-level space and low-level space. 3) To enhance the consistency between the generated images and input, we propose the self-attention concatenation and replacement approach. Lastly, we present a challenging benchmark dataset called InterpBench to evaluate the semantic consistency of generated results. Extensive experiments demonstrate the effectiveness of our method. Our project is available at https://dreamm0ver.github.io",
    "checked": true,
    "id": "e3cc93a0888fce9996b68dad3d7173eb429abac4",
    "semantic_title": "dreammover: leveraging the prior of diffusion models for image interpolation with large motion",
    "citation_count": 0,
    "authors": [
      "Liao Shen",
      "Tianqi Liu",
      "Huiqiang Sun",
      "Xinyi Ye",
      "Baopu Li",
      "Jianming Zhang",
      "Zhiguo Cao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2371_ECCV_2024_paper.php": {
    "title": "CoLA: Conditional Dropout and Language-driven Robust Dual-modal Salient Object Detection",
    "volume": "main",
    "abstract": "The depth/thermal information is beneficial for detecting salient object with conventional RGB images. However, in dual-modal salient object detection (SOD) model, the robustness against noisy inputs and modality missing is crucial but rarely studied. To tackle this problem, we introduce Conditional Dropout and LAnguage-driven(CoLA) framework comprising two core components. 1) Language-driven Quality Assessment (LQA): Leveraging a pretrained vision-language model with a prompt learner, the LQA recalibrates image contributions without requiring additional quality annotations. This approach effectively mitigates the impact of noisy inputs. 2) Conditional Dropout (CD): A learning method to strengthen the model's adaptability in scenarios with missing modalities, while preserving its performance under complete modalities. The CD serves as a plug-in training scheme that treats modality-missing as conditions, strengthening the overall robustness of various dual-modal SOD models. Extensive experiments demonstrate that the proposed method outperforms state-of-the-art dual-modal SOD models, under both modality-complete and modality-missing conditions. The code is avaliable at https://github.com/ssecv/CoLA",
    "checked": true,
    "id": "d0ff09bfb479e5c4e81bb63a55338cd173d5e04e",
    "semantic_title": "cola: conditional dropout and language-driven robust dual-modal salient object detection",
    "citation_count": 1,
    "authors": [
      "Shuang Hao",
      "Chunlin Zhong",
      "He Tang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2375_ECCV_2024_paper.php": {
    "title": "Image-Feature Weak-to-Strong Consistency: An Enhanced Paradigm for Semi-Supervised Learning",
    "volume": "main",
    "abstract": "Image-level weak-to-strong consistency serves as the predominant paradigm in semi-supervised learning (SSL) due to its simplicity and impressive performance. Nonetheless, this approach confines all perturbations to the image level and suffers from the excessive presence of naive samples, thus necessitating further improvement. In this paper, we introduce feature-level perturbation with varying intensities and forms to expand the augmentation space, establishing the image-feature weak-to-strong consistency paradigm. Furthermore, our paradigm develops a triple-branch structure, which facilitates interactions between both types of perturbations within one branch to boost their synergy. Additionally, we present a confidence-based identification strategy to distinguish between naive and challenging samples, thus introducing additional challenges exclusively for naive samples. Notably, our paradigm can seamlessly integrate with existing SSL methods. We apply the proposed paradigm to several representative algorithms and conduct experiments on multiple benchmarks, including both balanced and imbalanced distributions for labeled samples. The results demonstrate a significant enhancement in the performance of existing SSL algorithms",
    "checked": true,
    "id": "99b77c679ef652dba3062fed58a1c6e1e0c9cafb",
    "semantic_title": "image-feature weak-to-strong consistency: an enhanced paradigm for semi-supervised learning",
    "citation_count": 0,
    "authors": [
      "Zhiyu Wu*",
      "Jinshi Cui*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2379_ECCV_2024_paper.php": {
    "title": "RPBG: Towards Robust Neural Point-based Graphics in the Wild",
    "volume": "main",
    "abstract": "Point-based representations have recently gained popularity in novel view synthesis, for their unique advantages, , intuitive geometric representation, simple manipulation, and faster convergence. However, based on our observation, these point-based neural re-rendering methods are only expected to perform well under ideal conditions and suffer from noisy, patchy points and unbounded scenes, which are challenging to handle but defacto common in real applications. To this end, we revisit one such influential method, known as Neural Point-based Graphics (NPBG), as our baseline, and propose Robust Point-based Graphics (RPBG). We in-depth analyze the factors that prevent NPBG from achieving satisfactory renderings on generic datasets, and accordingly reform the pipeline to make it more robust to varying datasets in-the-wild. Inspired by the practices in image restoration, we greatly enhance the neural renderer to enable the attention-based correction of point visibility and the inpainting of incomplete rasterization, with only acceptable overheads. We also seek for a simple and lightweight alternative for environment modeling and an iterative method to alleviate the problem of poor geometry. By thorough evaluation on a wide range of datasets with different shooting conditions and camera trajectories, RPBG stably outperforms the baseline by a large margin, and exhibits its great robustness over state-of-the-art NeRF-based variants. Code available at https://github.com/QT-Zhu/RPBG",
    "checked": true,
    "id": "a34be82b1e138062c8fdc3c7349c720c47af9c4d",
    "semantic_title": "rpbg: towards robust neural point-based graphics in the wild",
    "citation_count": 1,
    "authors": [
      "Qingtian Zhu",
      "Zizhuang Wei",
      "Zhongtian Zheng",
      "Yifan Zhan",
      "Zhuyu Yao",
      "Jiawang Zhang",
      "Kejian Wu",
      "Yinqiang Zheng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2380_ECCV_2024_paper.php": {
    "title": "GaussReg: Fast 3D Registration with Gaussian Splatting",
    "volume": "main",
    "abstract": "Point cloud registration is a fundamental problem for large-scale 3D scene scanning and reconstruction. With the help of deep learning, registration methods have evolved significantly, reaching a nearly-mature stage. As the introduction of Neural Radiance Fields (NeRF), it has become the most popular 3D scene representation as its powerful view synthesis capabilities. Regarding NeRF representation, its registration is also required for large-scale scene reconstruction. However, this topic extremly lacks exploration. This is due to the inherent challenge to model the geometric relationship among two scenes with implicit representations. The existing methods usually convert the implicit representation to explicit representation for further registration. Most recently, Gaussian Splatting (GS) is introduced, employing explicit 3D Gaussian. This method significantly enhances rendering speed while maintaining high rendering quality. Given two scenes with explicit GS representations, in this work, we explore the 3D registration task between them. To this end, we propose GaussReg, a novel coarse-to-fine framework, both fast and accurate. The coarse stage follows existing point cloud registration methods and estimates a rough alignment for point clouds from GS. We further newly present an image-guided fine registration approach, which renders images from GS to provide more detailed geometric information for precise alignment. To support comprehensive evaluation, we carefully build a scene-level dataset called ScanNet-GSReg with 1379 scenes obtained from the ScanNet dataset and collect an in-the-wild dataset called GSReg. Experimental results demonstrate our method achieves state-of-the-art performance on multiple datasets. Our GaussReg is 44× faster than HLoc (SuperPoint as the feature extractor and SuperGlue as the matcher) with comparable accuracy",
    "checked": true,
    "id": "a6c20317fdf59b796363c958beef07ab96e76432",
    "semantic_title": "gaussreg: fast 3d registration with gaussian splatting",
    "citation_count": 4,
    "authors": [
      "Jiahao Chang*",
      "Yinglin Xu",
      "Yihao Li",
      "Yuantao Chen",
      "Wensen Feng",
      "Xiaoguang Han"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2391_ECCV_2024_paper.php": {
    "title": "Efficient Diffusion Transformer with Step-wise Dynamic Attention Mediators",
    "volume": "main",
    "abstract": "This paper identifies significant redundancy in the query-key interactions within self-attention mechanisms of diffusion transformer models, particularly during the early stages of denoising diffusion steps. In response to this observation, we present a novel diffusion transformer framework incorporating an additional set of mediator tokens to engage with queries and keys separately. By modulating the number of mediator tokens during the denoising generation phases, our model initiates the denoising process with a precise, non-ambiguous stage and gradually transitions to a phase enriched with detail. Concurrently, integrating mediator tokens simplifies the attention module's complexity to a linear scale, enhancing the efficiency of global attention processes. Additionally, we propose a time-step dynamic mediator token adjustment mechanism that further decreases the required computational FLOPs for generation, simultaneously facilitating the generation of high-quality images within the constraints of varied inference budgets. Extensive experiments demonstrate that the proposed method can improve the generated image quality while also reducing the inference cost of diffusion transformers. When integrated with the recent work SiT, our method achieves a state-of-the-art FID score of 2.01. The source code is available at https://github.com/LeapLabTHU/Attention-Mediators",
    "checked": true,
    "id": "8c2142406a1d1712f034aca134282b508aa40879",
    "semantic_title": "efficient diffusion transformer with step-wise dynamic attention mediators",
    "citation_count": 4,
    "authors": [
      "Yifan Pu*",
      "Zhuofan Xia",
      "Jiayi Guo",
      "Dongchen Han",
      "Qixiu Li",
      "Duo Li",
      "Yuhui Yuan",
      "Ji Li",
      "Yizeng Han",
      "Shiji Song",
      "Gao Huang*",
      "Xiu Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2396_ECCV_2024_paper.php": {
    "title": "Open Vocabulary 3D Scene Understanding via Geometry Guided Self-Distillation",
    "volume": "main",
    "abstract": "The scarcity of large-scale 3D-text paired data poses a great challenge on open vocabulary 3D scene understanding, and hence it is popular to leverage internet-scale 2D data and transfer their open vocabulary capabilities to 3D models through knowledge distillation. However, the existing distillation-based 3D scene understanding approaches rely on the representation capacity of 2D models, disregarding the exploration of geometric priors and inherent representational advantages offered by 3D data. In this paper, we propose an effective approach, namely Geometry Guided Self-Distillation (GGSD), to learn superior 3D representations from 2D pre-trained models. Specifically, we first design a geometry guided distillation module to distill knowledge from 2D models, and then leverage the 3D geometric priors to alleviate the inherent noise in 2D models and enhance the representation learning process. Due to the advantages of 3D representation, the performance of the distilled 3D student model can significantly surpass that of the 2D teacher model. This motivates us to further leverage the representation advantages of 3D data through self-distillation. As a result, our proposed GGSD approach outperforms the existing open vocabulary 3D scene understanding methods by a large margin, as demonstrated by our experiments on both indoor and outdoor benchmark datasets. Codes are available at https://github.com/ Wang-pengfei/GGSD",
    "checked": true,
    "id": "7b74273eff7fa41e71fe845217481f4727ba7e13",
    "semantic_title": "open vocabulary 3d scene understanding via geometry guided self-distillation",
    "citation_count": 0,
    "authors": [
      "Pengfei Wang*",
      "Yuxi Wang",
      "Shuai Li",
      "Zhaoxiang Zhang",
      "Zhen Lei",
      "Lei Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2398_ECCV_2024_paper.php": {
    "title": "IAM-VFI : Interpolate Any Motion for Video Frame Interpolation with motion complexity map",
    "volume": "main",
    "abstract": "Within the video, different regions have varying motion complexity, with simple regions containing static or global motion and complex regions containing fast motion or lots of local motion. In recent years, the performance of flow-based Video Frame Interpolation (VFI) algorithms has improved significantly. However, existing training methods train on randomly cropped regions of train data without considering the complexity of the motion. As a result, they cannot handle all regions of the frame that contain varying motion complexity. To solve this problem, we propose a novel VFI approach (IAM-VFI) that can interpolate any motion by considering the motion complexity of all regions in the frame. First, we propose a training data classification method for motion optimization based on each motion complexity. Then, using the proposed data, a flow estimation network generates optimized results for each complexity. Finally, we propose a Motion Complexity Estimation Network (MCENet) to generate a Motion Complexity Map (MCM) that can estimate the motion complexity of each region. Our proposed methods can be easily applied to most flow-based VFI algorithms. Experimental results show that the proposed method can interpolate any motion and significantly improve the performance of existing VFI algorithms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kihwan Yoon*",
      "Yong Han Kim",
      "Sungjei Kim*",
      "Jinwoo Jeong*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2403_ECCV_2024_paper.php": {
    "title": "TIP: Tabular-Image Pre-training for Multimodal Classification with Incomplete Data",
    "volume": "main",
    "abstract": "Images and structured tables are essential parts of real-world databases. Though tabular-image representation learning is promising for creating new insights, it remains a challenging task, as tabular data is typically heterogeneous and incomplete, presenting significant modality disparities with images. Earlier works have mainly focused on simple modality fusion strategies in complete data scenarios, without considering the missing data issue, and thus are limited in practice. In this paper, we propose , a novel tabular-image pre-training framework for learning multimodal representations robust to incomplete tabular data. Specifically, investigates a novel self-supervised learning (SSL) strategy, including a masked tabular reconstruction task to tackle data missingness, and image-tabular matching and contrastive learning objectives to capture multimodal information. Moreover, proposes a versatile tabular encoder tailored for incomplete, heterogeneous tabular data and a multimodal interaction module for inter-modality representation learning. Experiments are performed on downstream multimodal classification tasks using both natural and medical image datasets. The results show that outperforms state-of-the-art supervised/SSL image/multimodal methods in both complete and incomplete data scenarios. Our code is available at https://github.com/siyi-wind/TIP",
    "checked": true,
    "id": "ea6891a02b673cb88885d9e73dbaecb1cc5d78f1",
    "semantic_title": "tip: tabular-image pre-training for multimodal classification with incomplete data",
    "citation_count": 0,
    "authors": [
      "Siyi Du*",
      "Shaoming Zheng",
      "Yinsong Wang",
      "Wenjia Bai",
      "Declan P. O'Regan",
      "Chen Qin*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2408_ECCV_2024_paper.php": {
    "title": "Diffusion Model is a Good Pose Estimator from 3D RF-Vision",
    "volume": "main",
    "abstract": "Human pose estimation (HPE) from Radio Frequency vision (RF-vision) performs human sensing using RF signals that penetrate obstacles without revealing privacy (e.g., facial information). Recently, mmWave radar has emerged as a promising RF-vision sensor, providing radar point clouds by processing RF signals. However, the mmWave radar has a limited resolution with severe noise, leading to inaccurate and inconsistent human pose estimation. This work proposes mmDiff, a novel diffusion-based pose estimator tailored for noisy radar data. Our approach aims to provide reliable guidance as conditions to diffusion models. Two key challenges are addressed by mmDiff: (1) miss-detection of parts of human bodies, which is addressed by a module that isolates feature extraction from different body parts, and (2) signal inconsistency due to environmental interference, which is tackled by incorporating prior knowledge of body structure and motion. Several modules are designed to achieve these goals, whose features work as the conditions for the subsequent diffusion model, eliminating the miss-detection and instability of HPE based on RF-vision. Extensive experiments demonstrate that mmDiff outperforms existing methods significantly, achieving state-of-the-art performances on public datasets. 1 1 The project page of mmDiff is https://fanjunqiao.github.io/mmDiff-site/",
    "checked": true,
    "id": "3e2092d98424eaf3fbc090466e8f0ee24c4793ad",
    "semantic_title": "diffusion model is a good pose estimator from 3d rf-vision",
    "citation_count": 0,
    "authors": [
      "Junqiao Fan",
      "Jianfei Yang*",
      "Yuecong Xu",
      "Lihua Xie"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2418_ECCV_2024_paper.php": {
    "title": "UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues",
    "volume": "main",
    "abstract": "We introduce UPose3D, a novel approach for multi-view 3D human pose estimation, addressing challenges in accuracy and scalability. Our method advances existing pose estimation frameworks by improving robustness and flexibility without requiring direct 3D annotations. At the core of our method, a pose compiler module refines predictions from a 2D keypoints estimator that operates on a single image by leveraging temporal and cross-view information. Our novel cross-view fusion strategy is scalable to any number of cameras, while our synthetic data generation strategy ensures generalization across diverse actors, scenes, and viewpoints. Finally, UPose3D leverages the prediction uncertainty of both the 2D keypoint estimator and the pose compiler module. This provides robustness to outliers and noisy data, resulting in state-of-the-art performance in out-of-distribution settings. In addition, for in-distribution settings, UPose3D yields performance rivalling methods that rely on 3D annotated data while being the state-of-the-art among methods relying only on 2D supervision",
    "checked": true,
    "id": "05ebe515f45812070b0d58baf9112d282a554764",
    "semantic_title": "upose3d: uncertainty-aware 3d human pose estimation with cross-view and temporal cues",
    "citation_count": 0,
    "authors": [
      "Vandad Davoodnia*",
      "Saeed Ghorbani",
      "Marc-André Carbonneau",
      "Alexandre Messier",
      "Ali Etemad"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2419_ECCV_2024_paper.php": {
    "title": "Learning 3D-aware GANs from Unposed Images with Template Feature Field",
    "volume": "main",
    "abstract": "Collecting accurate camera poses of training images has been shown to well serve the learning of 3D-aware generative adversarial networks (GANs) yet can be quite expensive in practice. This work targets learning 3D-aware GANs from unposed images, for which we propose to perform on-the-fly pose estimation of training images with a learned template feature field (). Concretely, in addition to a generative radiance field as in previous approaches, we ask the generator to also learn a field from 2D semantic features while sharing the density from the radiance field. Such a framework allows us to acquire a canonical 3D feature template leveraging the dataset mean discovered by the generative model, and further efficiently estimate the pose parameters on real data. Experimental results on various challenging datasets demonstrate the superiority of our approach over state-of-the-art alternatives from both the qualitative and the quantitative perspectives. Project page: https://XDimlab. github.io/TeFF",
    "checked": true,
    "id": "5f234cc34b9e610a173214df178b18454a40e225",
    "semantic_title": "learning 3d-aware gans from unposed images with template feature field",
    "citation_count": 0,
    "authors": [
      "Xinya Chen",
      "Hanlei Guo",
      "Yanrui Bin",
      "Shangzhan Zhang",
      "Yuanbo Yang",
      "Yujun Shen",
      "Yue Wang",
      "Yiyi Liao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2422_ECCV_2024_paper.php": {
    "title": "TAPTR: Tracking Any Point with Transformers as Detection",
    "volume": "main",
    "abstract": "In this paper, we propose a simple yet effective approach for Tracking Any Point with TRansformers (). Based on the observation that point tracking bears a great resemblance to object detection and tracking, we borrow designs from DETR-like algorithms to address the task of TAP. In , in each video frame, each tracking point is represented as a point query, which consists of a positional part and a content part. As in DETR, each query (its position and content feature) is naturally updated layer by layer. Its visibility is predicted by its updated content feature. Queries belonging to the same tracking point can exchange information through self-attention along the temporal dimension. As all such operations are well-designed in DETR-like algorithms, the model is conceptually very simple. We also adopt some useful designs such as cost volume from optical flow models and develop simple designs to provide long temporal information while mitigating the feature drifting issue. demonstrates strong performance with state-of-the-art performance on various datasets with faster inference speed",
    "checked": true,
    "id": "2e116e9e67bb4fb2f8cfa75925932620585e7978",
    "semantic_title": "taptr: tracking any point with transformers as detection",
    "citation_count": 7,
    "authors": [
      "Hongyang Li*",
      "Hao Zhang",
      "Shilong Liu",
      "Zhaoyang Zeng",
      "Tianhe Ren",
      "Feng Li",
      "Lei Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2429_ECCV_2024_paper.php": {
    "title": "Token Compensator: Altering Inference Cost of Vision Transformer without Re-Tuning",
    "volume": "main",
    "abstract": "Token compression expedites the training and inference of Vision Transformers (ViTs) by reducing the number of the redundant tokens, , pruning inattentive tokens or merging similar tokens. However, when applied to downstream tasks, these approaches suffer from significant performance drop when the compression degrees are mismatched between training and inference stages, which limits the application of token compression on off-the-shelf trained models. In this paper, we propose a model arithmetic framework to decouple the compression degrees between the two stages. In advance, we additionally perform a fast parameter-efficient self-distillation stage on the pre-trained models to obtain a small plugin, called Token Compensator (ToCom), which describes the gap between models across different compression degrees. During inference, ToCom can be directly inserted into any downstream off-the-shelf models with any mismatched training and inference compression degrees to acquire universal performance improvements without further training. Experiments on over 20 downstream tasks demonstrate the effectiveness of our framework. On CIFAR100, fine-grained visual classification, and VTAB-1k benchmark, ToCom can yield up to a maximum improvement of 2.3%, 1.5%, and 2.0% in the average performance of DeiT-B, respectively. 1 Corresponding Author",
    "checked": true,
    "id": "69664a34dc8bcfbffb17397af3b14497751679a9",
    "semantic_title": "token compensator: altering inference cost of vision transformer without re-tuning",
    "citation_count": 0,
    "authors": [
      "Shibo Jie",
      "Yehui Tang",
      "Jianyuan Guo",
      "Zhi-Hong Deng*",
      "Kai Han*",
      "Yunhe Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2430_ECCV_2024_paper.php": {
    "title": "Point-supervised Panoptic Segmentation via Estimating Pseudo Labels from Learnable Distance",
    "volume": "main",
    "abstract": "To bridge the gap between point labels and per-pixel labels, existing point-supervised panoptic segmentation methods usually estimate dense pseudo labels by assigning unlabeled pixels to corresponding instances according to rule-based pixel-to-instance distances. These distances cannot be optimized by point labels end to end and are usually suboptimal, which result in inaccurate pseudo labels. Here we propose to assign unlabeled pixels to corresponding instances based on a learnable distance. Specifically, we represent each instance as an anchor query, then predict the pixel-to-instance distance based on the cross-attention between anchor queries and pixel features through a distance branch, the predicted distance is supervised by point labels end to end. In order that each query can accurately represent the corresponding instance, we iteratively improve anchor queries through query aggregating and query enhancing processes, then improved distance results and pseudo labels are predicted with these queries. We have experimentally demonstrated the effectiveness of our approach and achieved state-of-the-art results",
    "checked": false,
    "id": "ac5d90e50ba6758beb37fe3bc4205dbf196dbbd8",
    "semantic_title": "weakly supervised lymph nodes segmentation based on partial instance annotations with pre-trained dual-branch network and pseudo label learning",
    "citation_count": 1,
    "authors": [
      "Jing Li",
      "Junsong Fan*",
      "Zhaoxiang Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2433_ECCV_2024_paper.php": {
    "title": "BRAVE: Broadening the visual encoding of vision-language models",
    "volume": "main",
    "abstract": "Vision-language models (VLMs) are typically composed of a vision encoder, e.g. CLIP, and a language model (LM) that interprets the encoded features to solve downstream tasks. Despite remarkable progress, VLMs are subject to several shortcomings due to the limited capabilities of vision encoders, e.g. \"blindness\" to certain image features, visual hallucination, etc. To address these issues, we study broadening the visual encoding capabilities of VLMs. We first comprehensively benchmark several vision encoders with different inductive biases for solving VLM tasks. We observe that there is no single encoding configuration that consistently achieves top performance across different tasks, and encoders with different biases can perform surprisingly similarly. Motivated by this, we introduce a method, named , that consolidates features from multiple frozen encoders into a more versatile representation that can be directly fed as the input to a frozen LM. achieves state-of-the-art performance on a broad range of captioning and VQA benchmarks and significantly reduces the aforementioned issues of VLMs, while requiring a smaller number of trainable parameters than existing methods and having a more compressed representation. Our results highlight the potential of incorporating different visual biases for a more broad and contextualized visual understanding of VLMs",
    "checked": true,
    "id": "9738dde55ab77cc26271a5753db7dd7851176fd6",
    "semantic_title": "brave: broadening the visual encoding of vision-language models",
    "citation_count": 9,
    "authors": [
      "Oğuzhan Fatih Kar*",
      "Alessio Tonioni*",
      "Petra Poklukar",
      "Achin Kulshrestha",
      "Amir Zamir",
      "Federico Tombari"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2441_ECCV_2024_paper.php": {
    "title": "HUMOS: Human Motion Model Conditioned on Body Shape",
    "volume": "main",
    "abstract": "Generating realistic human motion is crucial for many computer vision and graphics applications. The rich diversity of human body shapes and sizes significantly influences how people move. However, existing motion models typically overlook these differences, using a normalized, average body instead. This results in a homogenization of motion across human bodies, with motions not aligning with their physical attributes, thus limiting diversity. To address this, we propose a novel approach to learn a generative motion model conditioned on body shape. We demonstrate that it is possible to learn such a model from unpaired training data using cycle consistency, intuitive physics, and stability constraints that model the correlation between identity and movement. The resulting model produces diverse, physically plausible, and dynamically stable human motions that are quantitatively and qualitatively more realistic than existing state of the art. More details are available on our project page",
    "checked": true,
    "id": "b375ffd17b084a6f78c67bcaa779cb2fe9492d5e",
    "semantic_title": "humos: human motion model conditioned on body shape",
    "citation_count": 1,
    "authors": [
      "Shashank Tripathi*",
      "Omid Taheri",
      "Christoph Lassner*",
      "Michael J. Black*",
      "Daniel Holden*",
      "Carsten Stoll*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2444_ECCV_2024_paper.php": {
    "title": "Omni-Recon: Harnessing Image-based Rendering for General-Purpose Neural Radiance Fields",
    "volume": "main",
    "abstract": "Recent breakthroughs in Neural Radiance Fields (NeRFs) have sparked significant demand for their integration into real-world 3D applications. However, the varied functionalities required by different 3D applications often necessitate diverse NeRF models with various pipelines, leading to tedious NeRF training for each target task and cumbersome trial-and-error experiments. Drawing inspiration from the generalization capability and adaptability of emerging foundation models, our work aims to develop one general-purpose NeRF for handling diverse 3D tasks. We achieve this by proposing a framework called , which is capable of (1) generalizable 3D reconstruction and zero-shot multitask scene understanding, and (2) adaptability to diverse downstream 3D applications such as real-time rendering and scene editing. Our key insight is that an image-based rendering pipeline, with accurate geometry and appearance estimation, can lift 2D image features into their 3D counterparts, thus extending widely explored 2D tasks to the 3D world in a generalizable manner. Specifically, our features a general-purpose NeRF model using image-based rendering with two decoupled branches: one complex transformer-based branch that progressively fuses geometry and appearance features for accurate geometry estimation, and one lightweight branch for predicting blending weights of source views. This design achieves state-of-the-art (SOTA) generalizable 3D surface reconstruction quality with blending weights reusable across diverse tasks for zero-shot multitask scene understanding. In addition, it can enable real-time rendering after baking the complex geometry branch into meshes, swift adaptation to achieve SOTA generalizable 3D understanding performance, and seamless integration with 2D diffusion models for text-guided 3D editing. Our code is available at: https://github.com/GATECH-EIC/Omni-Recon",
    "checked": true,
    "id": "5cd446597b114bf91e9d1df14a5d9cf1a258a0e3",
    "semantic_title": "omni-recon: harnessing image-based rendering for general-purpose neural radiance fields",
    "citation_count": 0,
    "authors": [
      "Yonggan Fu",
      "Huaizhi Qu",
      "Zhifan Ye",
      "Chaojian Li",
      "Kevin Zhao",
      "Yingyan (Celine) Lin*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2446_ECCV_2024_paper.php": {
    "title": "MVDiffHD: A Dense High-resolution Multi-view Diffusion Model for Single or Sparse-view 3D Object Reconstruction",
    "volume": "main",
    "abstract": "This paper presents a neural architecture for 3D object reconstruction that synthesizes dense and high-resolution views of an object given one or a few images without camera poses. achieves superior flexibility and scalability with two surprisingly simple ideas: 1) A \"pose-free architecture\" where standard self-attention among 2D latent features learns 3D consistency across an arbitrary number of conditional and generation views without explicitly using camera pose information; and 2) A \"view dropout strategy\" that discards a substantial number of output views during training, which reduces the training-time memory footprint and enables dense and high-resolution view synthesis at test time. We use the Objaverse for training and the Google Scanned Objects for evaluation with standard novel view synthesis and 3D reconstruction metrics, where significantly outperforms the current state of the arts. We also demonstrate a text-to-3D application example by combining with a text-to-image generative model. The project page is at https://mvdiffusion-plusplus.github.io",
    "checked": false,
    "id": "888e36b348b786538e3a74c459fe884a3457a36f",
    "semantic_title": "mvdiffusion++: a dense high-resolution multi-view diffusion model for single or sparse-view 3d object reconstruction",
    "citation_count": 40,
    "authors": [
      "Shitao Tang*",
      "Jiacheng Chen",
      "Dilin Wang",
      "Chengzhou Tang",
      "Fuyang Zhang",
      "Yuchen Fan",
      "Vikas Chandra",
      "Yasutaka Furukawa",
      "Rakesh Ranjan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2447_ECCV_2024_paper.php": {
    "title": "FlowCon: Out-of-Distribution Detection using Flow-based Contrastive Learning",
    "volume": "main",
    "abstract": "Identifying Out-of-distribution (OOD) data is becoming increasingly critical as the real-world applications of deep learning methods expand. Post-hoc methods modify softmax scores fine-tuned on outlier data or leverage intermediate feature layers to identify distinctive patterns between In-Distribution (ID) and OOD samples. Other methods focus on employing diverse OOD samples to learn discrepancies between ID and OOD. These techniques, however, are typically dependent on the quality of the outlier samples assumed. Density-based methods explicitly model class-conditioned distributions but this requires long training time or retraining the classifier. To tackle these issues, we introduce FlowCon, a new density-based OOD detection technique. Our main innovation lies in efficiently combining the properties of normalizing flow with supervised contrastive learning, ensuring robust representation learning with tractable density estimation. Empirical evaluation shows the enhanced performance of our method across common vision datasets such as CIFAR-10 and CIFAR-100 pretrained on ResNet18 and WideResNet classifiers. We also perform quantitative analysis using likelihood plots and qualitative visualization using UMAP embeddings and demonstrate the robustness of the proposed method under various OOD contexts. Code can be found at https://github.com/saandeepa93/FlowCon_OOD",
    "checked": true,
    "id": "54936bdfa177491a8f287780a577a5f056ba222d",
    "semantic_title": "flowcon: out-of-distribution detection using flow-based contrastive learning",
    "citation_count": 0,
    "authors": [
      "Saandeep Aathreya*",
      "Shaun Canavan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2455_ECCV_2024_paper.php": {
    "title": "LEIA: Latent View-invariant Embeddings for Implicit 3D Articulation",
    "volume": "main",
    "abstract": "Neural Radiance Fields (NeRFs) have revolutionized the reconstruction of static scenes and objects in 3D, offering unprecedented quality. However, extending NeRFs to model dynamic objects or object articulations remains a challenging problem. Previous works have tackled this issue by focusing on part-level reconstruction and motion estimation for objects, but they often rely on heuristics regarding the number of moving parts or object categories, which can limit their practical use. In this work, we introduce , a novel approach for representing dynamic 3D objects. Our method involves observing the object at distinct time steps or \"states\" and conditioning a hypernetwork on the current state, using this to parameterize our NeRF. This approach allows us to learn a view-invariant latent representation for each state. We further demonstrate that by interpolating between these states, we can generate novel articulation configurations in 3D space that were previously unseen. Our experimental results highlight the effectiveness of our method in articulating objects in a manner that is independent of the viewing angle and joint configuration. Notably, our approach outperforms previous methods that rely on motion information for articulation registration",
    "checked": true,
    "id": "523425088d4e35802418e907ff8fa711813e1471",
    "semantic_title": "leia: latent view-invariant embeddings for implicit 3d articulation",
    "citation_count": 1,
    "authors": [
      "Archana Swaminathan*",
      "Anubhav Gupta",
      "Kamal Gupta",
      "Shishira R Maiya",
      "Vatsal Agarwal",
      "Abhinav Shrivastava"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2459_ECCV_2024_paper.php": {
    "title": "Un-EVIMO: Unsupervised Event-based Independent Motion Segmentation",
    "volume": "main",
    "abstract": "Event cameras are a novel type of biologically inspired vision sensor known for their high temporal resolution, high dynamic range, and low power consumption. Because of these properties, they are well-suited for processing fast motions that require rapid reactions. Event cameras have shown competitive performance in unsupervised optical flow estimation. However, performance in detecting independently moving objects (IMOs) is lacking behind, although event-based methods would be suited for this task based on their low latency and HDR properties. Previous approaches to event-based IMO segmentation heavily depended on labeled data. However, biological vision systems have developed the ability to avoid moving objects through daily tasks without using explicit labels. In this work, we propose the first event framework that generates IMO pseudo-labels using geometric constraints. Due to its unsupervised nature, our method can flexibly handle a non-predetermined arbitrary number of objects and is easily scalable to datasets where expensive IMO labels are not readily available. Our approach shows competitive performance on the EVIMO dataset compared with supervised methods, both quantitatively and qualitatively. See the project website for details: https://www.cis.upenn.edu/~ziyunw/un_evimo/",
    "checked": false,
    "id": "e6d41baca5eb9ebae207014950c5edb8873062bd",
    "semantic_title": "un-evmoseg: unsupervised event-based independent motion segmentation",
    "citation_count": 1,
    "authors": [
      "Ziyun Wang*",
      "Jinyuan Guo",
      "Kostas Daniilidis"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2469_ECCV_2024_paper.php": {
    "title": "Seeing the Unseen: A Frequency Prompt Guided Transformer for Image Restoration",
    "volume": "main",
    "abstract": "How to explore useful features from images as prompts to guide the deep image restoration models is an effective way to solve image restoration. In contrast to mining spatial relations within images as prompt, which leads to characteristics of different frequencies being neglected and further remaining subtle or undetectable artifacts in the restored image, we develop a Frequency Prompting image restoration method, dubbed FPro, which can effectively provide prompt components from a frequency perspective to guild the restoration model address these differences. Specifically, we first decompose input features into separate frequency parts via dynamically learned filters, where we introduce a gating mechanism for suppressing the less informative elements within the kernels. To propagate useful frequency information as prompt, we then propose a dual prompt block, consisting of a low-frequency prompt modulator (LPM) and a high-frequency prompt modulator (HPM), to handle signals from different bands respectively. Each modulator contains a generation process to incorporate prompting components into the extracted frequency maps, and a modulation part that modifies the prompt feature with the guidance of the decoder features. Experimental results on several popular datasets have demonstrated the favorable performance of our pipeline against SOTA methods on 5 image restoration tasks, including deraining, deraindrop, demoiréing, deblurring, and dehazing. The source code is available at https://github.com/joshyZhou/FPro",
    "checked": true,
    "id": "a8f96eba4e565da131f76617d5dbcb1adcc5ca9d",
    "semantic_title": "seeing the unseen: a frequency prompt guided transformer for image restoration",
    "citation_count": 0,
    "authors": [
      "Shihao Zhou",
      "Jinshan Pan",
      "Jinglei Shi*",
      "Duosheng Chen",
      "Lishen Qu",
      "Jufeng Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2472_ECCV_2024_paper.php": {
    "title": "CityGaussian: Real-time High-quality Large-Scale Scene Rendering with Gaussians",
    "volume": "main",
    "abstract": "The advancement of real-time 3D scene reconstruction and novel view synthesis has been significantly propelled by 3D Gaussian Splatting (3DGS). However, effectively training large-scale 3DGS and rendering it in real-time across various scales remains challenging. This paper introduces CityGaussian (CityGS), which employs a novel divide-and-conquer training approach and Level-of-Detail (LoD) strategy for efficient large-scale 3DGS training and rendering. Specifically, the global scene prior and adaptive training data selection enables efficient training and seamless fusion. Based on fused Gaussian primitives, we generate different detail levels through compression, and realize fast rendering across various scales through the proposed block-wise detail levels selection and aggregation strategy. Extensive experimental results on large-scale scenes demonstrate that our approach attains state-of-the-art rendering quality, enabling consistent real-time rendering of large-scale scenes across vastly different scales. Our project page is available at https://dekuliutesla.github.io/citygs/",
    "checked": true,
    "id": "d0aaeb36661475a6cc840c1353adf25f5890d27a",
    "semantic_title": "citygaussian: real-time high-quality large-scale scene rendering with gaussians",
    "citation_count": 22,
    "authors": [
      "Yang Liu",
      "Chuanchen Luo",
      "Lue Fan",
      "Naiyan Wang",
      "Junran Peng*",
      "Zhaoxiang Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2475_ECCV_2024_paper.php": {
    "title": "Bayesian Evidential Deep Learning for Online Action Detection",
    "volume": "main",
    "abstract": "Online action detection aims at identifying the ongoing action in a streaming video without seeing the future. Timely and reliable response is critical for real-world applications. In this paper, we introduce Bayesian Evidential Deep Learning (BEDL), an efficient and generalizable framework for online action detection and uncertainty quantification. Specifically, we combine Bayesian neural networks and evidential deep learning by a teacher-student architecture. The teacher model is built in a Bayesian manner and transfers its mutual information and distribution to the student model through evidential deep learning. In this way, the student model can make accurate online inference while efficiently quantifying the uncertainty. Compared to existing evidential deep learning methods, BEDL estimates uncertainty more accurately by leveraging the Bayesian teacher model. In addition, we designed an attention module for active OAD, which actively selects important features based on the Bayesian mutual information instead of using all the features. We evaluated BEDL on benchmark datasets including THUMPS'14, TVSeries, and HDD. BEDL achieves competitive performance while keeping efficient inference. Extensive ablation studies demonstrate the effectiveness of each component. To verify the uncertainty quantification, we perform experiments of online anomaly detection with different types of uncertainties",
    "checked": false,
    "id": "1c713724a7b3a2a6edbdb0c74bb5bc59e7c5e0e3",
    "semantic_title": "sustainable strategy for online physical education teaching using resnet34 and big data",
    "citation_count": 2,
    "authors": [
      "Hongji Guo",
      "Hanjing Wang",
      "Qiang Ji*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2478_ECCV_2024_paper.php": {
    "title": "AdaNAT: Exploring Adaptive Policy for Token-Based Image Generation",
    "volume": "main",
    "abstract": "Recent studies have demonstrated the effectiveness of token-based methods for visual content generation. As a representative work, non-autoregressive Transformers (NATs) are able to synthesize images with decent quality in a small number of steps. However, NATs usually necessitate configuring a complicated generation policy comprising multiple manually-designed scheduling rules. These heuristic-driven rules are prone to sub-optimality and come with the requirements of expert knowledge and labor-intensive efforts. Moreover, their one-size-fits-all nature cannot flexibly adapt to the diverse characteristics of each individual sample. To address these issues, we propose , a learnable approach that automatically configures a suitable policy tailored for every sample to be generated. In specific, we formulate the determination of generation policies as a Markov decision process. Under this framework, a lightweight policy network for generation can be learned via reinforcement learning. Importantly, we demonstrate that simple reward designs such as FID or pre-trained reward models, may not reliably guarantee the desired quality or diversity of generated samples. Therefore, we propose an adversarial reward design to guide the training of policy networks effectively. Comprehensive experiments on four benchmark datasets, , ImageNet-2562 &5122 , MS-COCO, and CC3M, validate the effectiveness of . Code and pre-trained models will be released at https://github.com/LeapLabTHU/AdaNAT",
    "checked": true,
    "id": "d0ae1637238ffe1a48efd39d462fa7a57ebc591d",
    "semantic_title": "adanat: exploring adaptive policy for token-based image generation",
    "citation_count": 2,
    "authors": [
      "Zanlin Ni",
      "Yulin Wang",
      "Renping Zhou",
      "Rui Lu",
      "Jiayi Guo",
      "Jinyi Hu",
      "Zhiyuan Liu",
      "Yuan Yao*",
      "Gao Huang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2480_ECCV_2024_paper.php": {
    "title": "Rethinking Data Augmentation for Robust LiDAR Semantic Segmentation in Adverse Weather",
    "volume": "main",
    "abstract": "Existing LiDAR semantic segmentation methods often struggle with performance declines in adverse weather conditions. Previous work has addressed this issue by simulating adverse weather or employing universal data augmentation during training. However, these methods lack a detailed analysis and understanding of how adverse weather negatively affects LiDAR semantic segmentation performance. Motivated by this issue, we identified key factors of adverse weather and conducted a toy experiment to pinpoint the main causes of performance degradation: (1) Geometric perturbation due to refraction caused by fog or droplets in the air and (2) Point drop due to energy absorption and occlusions. Based on these findings, we propose new strategic data augmentation techniques. First, we introduced a Selective Jittering (SJ) that jitters points in the random range of depth (or angle) to mimic geometric perturbation. Additionally, we developed a Learnable Point Drop (LPD) to learn vulnerable erase patterns with a Deep Q-Learning Network to approximate the point drop phenomenon from adverse weather conditions. Without precise weather simulation, these techniques strengthen the LiDAR semantic segmentation model by exposing it to vulnerable conditions identified by our data-centric analysis. Experimental results confirmed the suitability of the proposed data augmentation methods for enhancing robustness against adverse weather conditions. Our method achieves a notable 39.5 mIoU on the SemanticKITTI-to-SemanticSTF benchmark, improving the baseline by 8.1%p and establishing a new state-of-the-art. Our code will be released at https://github.com/engineerJPark/LiDARWeather",
    "checked": true,
    "id": "5a3d5a36f7f209bdb36c5367b1c9292b62501f2f",
    "semantic_title": "rethinking data augmentation for robust lidar semantic segmentation in adverse weather",
    "citation_count": 0,
    "authors": [
      "Junsung Park",
      "Kyungmin Kim",
      "Hyunjung Shim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2488_ECCV_2024_paper.php": {
    "title": "Diffusion-Generated Pseudo-Observations for High-Quality Sparse-View Reconstruction",
    "volume": "main",
    "abstract": "Novel view synthesis via Neural Radiance Fields (NeRFs) or 3D Gaussian Splatting (3DGS) typically necessitates dense observations with hundreds of input images to circumvent artifacts. We introduce Deceptive-NeRF/3DGS1 to enhance sparse-view reconstruction with only a limited set of input images, by leveraging a diffusion model pre-trained from multiview datasets. Different from using diffusion priors to regularize representation optimization, our method directly uses diffusion-generated images to train NeRF/3DGS as if they were real input views. Specifically, we propose a deceptive diffusion model turning noisy images rendered from few-view reconstructions into high-quality photorealistic pseudo-observations. To resolve consistency among pseudo-observations and real input views, we develop an uncertainty measure to guide the diffusion model's generation. Our system progressively incorporates diffusion-generated pseudo-observations into the training image sets, ultimately densifying the sparse input observations by 5 to 10 times. Extensive experiments across diverse and challenging datasets validate that our approach outperforms existing state-of-the-art methods and is capable of synthesizing novel views with super-resolution in the few-view setting. Project page: https://xinhangliu.com/deceptive-nerf-3dgs. 1 In harmonic progression, a Deceptive Cadence may disrupt expectations of chord progression but enriches the emotional expression of the music. Our Deceptive-X, where \"X\" can be NeRF, 3DGS, or a pertinent 3D reconstruction framework—counters overfitting to sparse input views by densely synthesizing consistent pseudo-observations, enriching the original sparse inputs by fivefold to tenfold",
    "checked": false,
    "id": "d7cffb13dc787cd5bca894a65bb3815dae2d5e74",
    "semantic_title": "deceptive-nerf/3dgs: diffusion-generated pseudo-observations for high-quality sparse-view reconstruction",
    "citation_count": 9,
    "authors": [
      "Xinhang Liu*",
      "Jiaben Chen",
      "Shiu-Hong Kao",
      "Yu-Wing Tai",
      "Chi-Keung Tang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2494_ECCV_2024_paper.php": {
    "title": "Memory-Efficient Fine-Tuning for Quantized Diffusion Model",
    "volume": "main",
    "abstract": "The emergence of billion-parameter diffusion models such as Stable Diffusion XL, Imagen, and DALL-E 3 has significantly propelled the domain of generative AI. However, their large-scale architecture presents challenges in fine-tuning and deployment due to high resource demands and slow inference speed. This paper explores the relatively unexplored yet promising realm of fine-tuning quantized diffusion models. Our analysis revealed that the baseline neglects the distinct patterns in model weights and the different roles throughout time steps when finetuning the diffusion model. To address these limitations, we introduce a novel memory-efficient fine-tuning method specifically designed for quantized diffusion models, dubbed TuneQDM. Our approach introduces quantization scales as separable functions to consider inter-channel weight patterns. Then, it optimizes these scales in a timestep-specific manner for effective reflection of the role of each time step. TuneQDM achieves performance on par with its full-precision counterpart while simultaneously offering significant memory efficiency. Experimental results demonstrate that our method consistently outperforms the baseline in both single-/multi-subject generations, exhibiting high subject fidelity and prompt fidelity comparable to the full precision model",
    "checked": true,
    "id": "f4cdc3d3651878de407d95aff4a530ab3bb390f4",
    "semantic_title": "memory-efficient fine-tuning for quantized diffusion model",
    "citation_count": 2,
    "authors": [
      "Hyogon Ryu",
      "Seohyun Lim",
      "Hyunjung Shim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2497_ECCV_2024_paper.php": {
    "title": "VCD-Texture: Variance Alignment based 3D-2D Co-Denoising for Text-Guided Texturing",
    "volume": "main",
    "abstract": "Recent research on texture synthesis for 3D shapes benefits a lot from dramatically developed 2D text-to-image diffusion models, including inpainting-based and optimization-based approaches. However, these methods ignore the modal gap between the 2D diffusion model and 3D objects, which primarily render 3D objects into 2D images and texture each image separately. In this paper, we revisit the texture synthesis and propose a Variance alignment based 3D-2D Collaborative Denoising framework, dubbed VCD-Texture, to address these issues. Formally, we first unify both 2D and 3D latent feature learning in diffusion self-attention modules with re-projected 3D attention receptive fields. Subsequently, the denoised multi-view 2D latent features are aggregated into 3D space and then rasterized back to formulate more consistent 2D predictions. However, the rasterization process suffers from an intractable variance bias, which is theoretically addressed by the proposed variance alignment, achieving high-fidelity texture synthesis. Moreover, we present an inpainting refinement to further improve the details with conflicting regions. Notably, there is not a publicly available benchmark to evaluate texture synthesis, which hinders its development. Thus we construct a new evaluation set built upon three open-source 3D datasets and propose to use four metrics to thoroughly validate the texturing performance. Comprehensive experiments demonstrate that VCD-Texture achieves superior performance against other counterparts",
    "checked": true,
    "id": "8a6a1180242d2d705f8186c06bac3479a0763e6b",
    "semantic_title": "vcd-texture: variance alignment based 3d-2d co-denoising for text-guided texturing",
    "citation_count": 1,
    "authors": [
      "Shang Liu*",
      "Chaohui Yu",
      "Chenjie Cao",
      "Wen Qian",
      "Fan Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2504_ECCV_2024_paper.php": {
    "title": "MotionLCM: Real-time Controllable Motion Generation via Latent Consistency Model",
    "volume": "main",
    "abstract": "This work introduces MotionLCM, extending controllable motion generation to a real-time level. Existing methods for spatial-temporal control in text-conditioned motion generation suffer from significant runtime inefficiency. To address this issue, we first propose the motion latent consistency model (MotionLCM) for motion generation, building upon the latent diffusion model [?]. By adopting one-step (or few-step) inference, we further improve the runtime efficiency of the motion latent diffusion model for motion generation. To ensure effective controllability, we incorporate a motion ControlNet within the latent space of MotionLCM and enable explicit control signals (e.g., initial poses) in the vanilla motion space to control the generation process directly, similar to controlling other latent-free diffusion models [?, ?] for motion generation. By employing these techniques, our approach can generate human motions with text and control signals in real-time. Experimental results demonstrate the remarkable generation and controlling capabilities of MotionLCM while maintaining real-time runtime efficiency",
    "checked": true,
    "id": "31cf335ceee3e5ed44aaaf04bd5631645c7d954c",
    "semantic_title": "motionlcm: real-time controllable motion generation via latent consistency model",
    "citation_count": 11,
    "authors": [
      "Wenxun Dai",
      "Ling-Hao Chen",
      "Jingbo Wang*",
      "Jinpeng Liu",
      "Bo Dai*",
      "Yansong Tang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2505_ECCV_2024_paper.php": {
    "title": "Human Hair Reconstruction with Strand-Aligned 3D Gaussians",
    "volume": "main",
    "abstract": "We introduce a new hair modeling method that uses a dual representation of classical hair strands and 3D Gaussians to produce accurate and realistic strand-based reconstructions from multi-view data. In contrast to recent approaches that leverage unstructured Gaussians to model human avatars, our method reconstructs the hair using 3D polylines, or strands. This fundamental difference allows the use of the resulting hairstyles out-of-the-box in modern computer graphics engines for editing, rendering, and simulation. Our 3D lifting method relies on unstructured Gaussians to generate multi-view ground truth data to supervise the fitting of hair strands. The hairstyle itself is represented in the form of the so-called strand-aligned 3D Gaussians. This representation allows us to combine strand-based hair priors, which are essential for realistic modeling of the inner structure of hairstyles, with the differentiable rendering capabilities of 3D Gaussian Splatting. Our method, named Gaussian Haircut, is evaluated on synthetic and real scenes and demonstrates state-of-the-art performance in the task of strand-based hair reconstruction. For more results, please refer to our project page: https://eth-ait.github.io/GaussianHaircut",
    "checked": true,
    "id": "65589048c551df96f7de21bc5928201ad1b0dc7a",
    "semantic_title": "human hair reconstruction with strand-aligned 3d gaussians",
    "citation_count": 1,
    "authors": [
      "Egor Zakharov*",
      "Vanessa Sklyarova",
      "Michael J. Black",
      "Giljoo Nam",
      "Justus Thies",
      "Otmar Hilliges"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2507_ECCV_2024_paper.php": {
    "title": "COIN: Control-Inpainting Diffusion Prior for Human and Camera Motion Estimation",
    "volume": "main",
    "abstract": "Estimating global human motion from moving cameras is challenging due to the entanglement of human and camera motions. To mitigate the ambiguity, existing methods leverage learned human motion priors, which however often result in oversmoothed motions with misaligned 2D projections. To tackle this problem, we propose , a control-inpainting motion diffusion prior that enables fine-grained control to disentangle human and camera motions. Although pre-trained motion diffusion models encode rich motion priors, we find it non-trivial to leverage such knowledge to guide global motion estimation from RGB videos. introduces a novel control-inpainting score distillation sampling method to ensure well-aligned, consistent, and high-quality motion from the diffusion prior within a joint optimization framework. Furthermore, we introduce a new human-scene relation loss to alleviate the scale ambiguity by enforcing consistency among the humans, camera, and scene. Experiments on three challenging benchmarks demonstrate the effectiveness of , which outperforms the state-of-the-art methods in terms of global human motion estimation and camera motion estimation. As an illustrative example, COIN outperforms the state-of-the-art method by 33% in world joint position error (W-MPJPE) on the RICH dataset",
    "checked": true,
    "id": "023972e326fbf902dcc15adca0c39f5d31001d7e",
    "semantic_title": "coin: control-inpainting diffusion prior for human and camera motion estimation",
    "citation_count": 0,
    "authors": [
      "Jiefeng Li*",
      "Ye Yuan",
      "Davis Rempe",
      "Haotian Zhang",
      "Pavlo Molchanov",
      "Cewu Lu",
      "Jan Kautz",
      "Umar Iqbal*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2508_ECCV_2024_paper.php": {
    "title": "SA-DVAE: Improving Zero-Shot Skeleton-Based Action Recognition by Disentangled Variational Autoencoders",
    "volume": "main",
    "abstract": "Existing zero-shot skeleton-based action recognition methods utilize projection networks to learn a shared latent space of skeleton features and semantic embeddings. The inherent imbalance in action recognition datasets, characterized by variable skeleton sequences yet constant class labels, presents significant challenges for alignment. To address the imbalance, we propose SA-DVAE—Semantic Alignment via Disentangled Variational Autoencoders, a method that first adopts feature disentanglement to separate skeleton features into two independent parts—one is semantic-related and another is irrelevant—to better align skeleton and semantic features. We implement this idea via a pair of modality-specific variational autoencoders coupled with a total correction penalty. We conduct experiments on three benchmark datasets: NTU RGB+D, NTU RGB+D 120 and PKU-MMD, and our experimental results show that SA-DAVE produces improved performance over existing methods. The code is available at https://github.com/pha123661/ SA-DVAE",
    "checked": true,
    "id": "1a079a97c7793e10a4b9500abfb5bea8014df767",
    "semantic_title": "sa-dvae: improving zero-shot skeleton-based action recognition by disentangled variational autoencoders",
    "citation_count": 0,
    "authors": [
      "Sheng-Wei Li",
      "Zi-Xiang Wei",
      "Wei-Jie Chen",
      "Yi-Hsin Yu",
      "Chih-Yuan Yang*",
      "Jane Yung-jen Hsu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2511_ECCV_2024_paper.php": {
    "title": "Bridge Past and Future: Overcoming Information Asymmetry in Incremental Object Detection",
    "volume": "main",
    "abstract": "In incremental object detection, knowledge distillation has been proven to be an effective way to alleviate catastrophic forgetting. However, previous works focused on preserving the knowledge of old models, ignoring that images could simultaneously contain categories from past, present, and future stages. The co-occurrence of objects makes the optimization objectives inconsistent across different stages since the definition for foreground objects differs across various stages, which limits the model's performance greatly. To overcome this problem, we propose a method called \"Bridge Past and Future\" (BPF), which aligns models across stages, ensuring consistent optimization directions. In addition, we propose a novel Distillation with Future (DwF) loss, fully leveraging the background probability to mitigate the forgetting of old classes while ensuring a high level of adaptability in learning new classes. Extensive experiments are conducted on both Pascal VOC and MS COCO benchmarks. Without memory, BPF outperforms current state-of-the-art methods under various settings. The code is available at https://github.com/iSEE-Laboratory/BPF",
    "checked": true,
    "id": "4af5ac57d6df79dd457c80d90bac170906b5185b",
    "semantic_title": "bridge past and future: overcoming information asymmetry in incremental object detection",
    "citation_count": 1,
    "authors": [
      "Qijie Mo",
      "Yipeng Gao",
      "Shenghao Fu",
      "Junkai Yan",
      "Ancong Wu*",
      "Wei-Shi Zheng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2513_ECCV_2024_paper.php": {
    "title": "Global-to-Pixel Regression for Human Mesh Recovery",
    "volume": "main",
    "abstract": "Existing human mesh recovery (HMR) methods commonly leverage the global or dense-annotations-based local features to produce a single prediction from the input image. However, the compressed global and local features disrupt the spatial geometry of the human body and make it hard to capture the local dynamics, resulting in visual-mesh misalignment. Moreover, dense annotations are labor-intensive and expensive. Toward the above issues, we propose a global-to-local prediction framework to preserve spatial information and obtain precise visual-mesh alignments for top-down HMR. Specifically, we present an adaptive 2D Keypoint-Guided Local Encoding Module to enable per-pixel features to capture fine-grained body part information with structure and local context maintained. The acquisition of local features relies exclusively on sparse 2D keypoint guidance without dense annotations or heuristics keypoint-based ROI (Region of Interested) pooling. The enhanced pixel features are used to predict residuals for rectifying the initial estimation produced by global features. Secondly, we introduce a Dynamic Matching Strategy that determines positive/negative pixels by only calculating the classification and 2D keypoint costs to further improve visual-mesh alignments. The comprehensive experiments demonstrate the effectiveness of network design. Our framework outperforms previous local regression methods by a large margin and achieves state-of-the-art performance on Human3.6M and 3DPW datasets",
    "checked": false,
    "id": "b5a878716aa4ab11e84ba0973d58ab35c68711a1",
    "semantic_title": "delving deep into pixel alignment feature for accurate multi-view human mesh recovery",
    "citation_count": 6,
    "authors": [
      "Yabo Xiao",
      "Mingshu HE*",
      "Dongdong Yu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2515_ECCV_2024_paper.php": {
    "title": "Visible and Clear: Finding Tiny Objects in Difference Map",
    "volume": "main",
    "abstract": "Tiny object detection is one of the key challenges for most generic detectors. The main difficulty lies in extracting effective features of tiny objects. Existing methods usually perform generation-based feature enhancement, which is seriously affected by spurious textures and artifacts, making it difficult to make the tiny-object-specific features visible and clear for detection. To address this issue, we propose a self-reconstructed tiny object detection (SR-TOD) framework. We for the first time introduce a self-reconstruction mechanism in the detection model, and discover the strong correlation between it and the tiny objects. Specifically, we impose a reconstruction head in-between the neck of a detector, constructing a difference map of the reconstructed image and the input, which shows high sensitivity to tiny objects. This inspires us to enhance the weak representations of tiny objects under the guidance of the difference maps. Thus, improving the visibility of tiny objects for the detectors. Building on this, we further develop a Difference Map Guided Feature Enhancement (DGFE) module to make the tiny feature representation more clear. In addition, we further propose a new multi-instance anti-UAV dataset. Extensive experiments demonstrate our effectiveness. The code is available: https://github.com/ Hiyuur/SR-TOD",
    "checked": true,
    "id": "09f5e5ced4c1cdbf3bd5ded5531fb71f313b7fd8",
    "semantic_title": "visible and clear: finding tiny objects in difference map",
    "citation_count": 1,
    "authors": [
      "Bing Cao",
      "Haiyu Yao",
      "Pengfei Zhu*",
      "Qinghua Hu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2525_ECCV_2024_paper.php": {
    "title": "Rethinking Image Super Resolution from Training Data Perspectives",
    "volume": "main",
    "abstract": "In this work, we investigate the understudied effect of the training data used for image super-resolution (SR). Most commonly, novel SR methods are developed and benchmarked on common training datasets such as DIV2K and DF2K. However, we investigate and rethink the training data from the perspectives of diversity and quality, thereby addressing the question of \"How important is SR training for SR models?\". To this end, we propose an automated image evaluation pipeline. With this, we stratify existing high-resolution image datasets and larger-scale image datasets such as ImageNet and PASS to compare their performances. We find that datasets with (i) low compression artifacts, (ii) high within-image diversity as judged by the number of different objects, and (iii) a large number of images from ImageNet or PASS all positively affect SR performance. We hope that the proposed simple-yet-effective dataset curation pipeline will inform the construction of SR datasets in the future and yield overall better models. Code is available at: https://github.com/gohtanii/DiverSeg-dataset",
    "checked": false,
    "id": "b4d3f0f2b665705d7e14265db9e55b50f0a7d605",
    "semantic_title": "rethinking image super-resolution from training data perspectives",
    "citation_count": 0,
    "authors": [
      "Go Ohtani*",
      "Ryu Tadokoro",
      "Ryosuke Yamada",
      "Yuki M Asano",
      "Iro Laina",
      "Christian Rupprecht",
      "Nakamasa Inoue",
      "Rio Yokota",
      "Hirokatsu Kataoka",
      "Yoshimitsu Aoki"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2526_ECCV_2024_paper.php": {
    "title": "BlazeBVD: Make Scale-Time Equalization Great Again for Blind Video Deflickering",
    "volume": "main",
    "abstract": "Developing blind video deflickering (BVD) algorithms to enhance video temporal consistency, is gaining importance amid the flourish of image processing and video generation. However, the intricate nature of video data complicates the training of deep learning methods, leading to high resource consumption and instability, notably under severe lighting flicker. This underscores the critical need for a compact representation beyond pixel values to advance BVD research and applications. Inspired by the classic scale-time equalization (STE), our work introduces the histogram-assisted solution, called BlazeBVD, for high-fidelity and rapid BVD. Compared with STE, which directly corrects pixel values by temporally smoothing color histograms, BlazeBVD leverages smoothed illumination histograms within STE filtering to ease the challenge of learning temporal data using neural networks. In technique, BlazeBVD begins by condensing pixel values into illumination histograms that precisely capture flickering and local exposure variations. These histograms are then smoothed to produce singular frames set, filtered illumination maps, and exposure maps. Resorting to these deflickering priors, BlazeBVD utilizes a 2D network to restore faithful and consistent texture impacted by lighting changes or localized exposure issues. BlazeBVD also incorporates a lightweight 3D network to amend slight temporal inconsistencies, avoiding the resource consumption issue. Comprehensive experiments on synthetic, real-world and generated videos, showcase the superior qualitative and quantitative results of BlazeBVD, achieving inference speeds up to 10× faster than state-of-the-arts",
    "checked": true,
    "id": "5bf2026badbca044d4590d30fd81ce5c795be18b",
    "semantic_title": "blazebvd: make scale-time equalization great again for blind video deflickering",
    "citation_count": 0,
    "authors": [
      "Xinmin Qiu",
      "Congying Han",
      "Zicheng Zhang",
      "Bonan Li*",
      "Tiande Guo",
      "Pingyu Wang",
      "Xuecheng Nie"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2528_ECCV_2024_paper.php": {
    "title": "Efficient Inference of Vision Instruction-Following Models with Elastic Cache",
    "volume": "main",
    "abstract": "In the field of instruction-following large vision-language models (LVLMs), the efficient deployment of these models faces challenges, notably due to the high memory demands of their key-value (KV) caches. Conventional cache management strategies for LLMs focus on cache eviction, which often fails to address the specific needs of multimodal instruction-following models. Recognizing this gap, in this paper, we introduce Elastic Cache, a novel approach that benefits from applying distinct acceleration methods for instruction encoding and output generation stages. We investigate the metrics of importance in different stages and propose an ‘importance-driven cache merging' strategy to prune redundancy caches. Instead of discarding less important caches, our strategy identifies important key/value vectors as anchor points. Surrounding less important caches are then merged with these anchors, enhancing the preservation of contextual information in the KV caches while yielding an arbitrary acceleration ratio. For instruction encoding, we utilize the frequency to evaluate the importance of caches. Regarding output generation, we prioritize tokens based on their ‘distance' with an offset, by which both the initial and most recent tokens are retained. Results on a range of LVLMs demonstrate that Elastic Cache not only boosts efficiency but also notably outperforms existing pruning methods in language generation across various tasks. Code is available at https://github. com/liuzuyan/ElasticCache",
    "checked": true,
    "id": "31a1bbafebae80a1024302b34618fc98ecd80e98",
    "semantic_title": "efficient inference of vision instruction-following models with elastic cache",
    "citation_count": 3,
    "authors": [
      "Zuyan Liu",
      "Benlin Liu",
      "Jiahui Wang",
      "Yuhao Dong",
      "Guangyi Chen",
      "Yongming Rao",
      "Ranjay Krishna",
      "Jiwen Lu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2529_ECCV_2024_paper.php": {
    "title": "FreeCompose: Generic Zero-Shot Image Composition with Diffusion Prior",
    "volume": "main",
    "abstract": "[width=0.985]assets/teaser.pdf Figure 1: harnesses the generative prior of pre-trained diffusion models to achieve versatile image composition, such as appearance editing (image harmonization) and semantic editing (semantic image composition). Furthermore, it can be extended to various downstream applications, including object removal and multi-character customization. We offer a novel approach to image composition, which integrates multiple input images into a single, coherent image. Rather than concentrating on specific use cases such as appearance editing (image harmonization) or semantic editing (semantic image composition), we showcase the potential of utilizing the powerful generative prior inherent in large-scale pre-trained diffusion models to accomplish generic image composition applicable to both scenarios. We observe that the pre-trained diffusion models automatically identify simple copy-paste boundary areas as low-density regions during denoising. Building on this insight, we propose to optimize the composed image towards high-density regions guided by the diffusion prior. In addition, we introduce a novel mask-guided loss to further enable flexible semantic image composition. Extensive experiments validate the superiority of our approach in achieving generic zero-shot image composition. Additionally, our approach shows promising potential in various tasks, such as object removal and multi-concept customization. Project webpage: https://github.com/aim-uofa/FreeCompose",
    "checked": true,
    "id": "21447135c54b31849ec24ad647828905fc12395e",
    "semantic_title": "freecompose: generic zero-shot image composition with diffusion prior",
    "citation_count": 0,
    "authors": [
      "Zhekai Chen",
      "Wen Wang",
      "Zhen Yang",
      "Zeqing Yuan",
      "Hao Chen*",
      "Chunhua Shen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2547_ECCV_2024_paper.php": {
    "title": "Learning to Robustly Reconstruct Dynamic Scenes from Low-light Spike Streams",
    "volume": "main",
    "abstract": "Spike camera with high temporal resolution can fire continuous binary spike streams to record per-pixel light intensity. By using reconstruction methods, the scene details in high-speed scenes can be restored from spike streams. However, existing methods struggle to perform well in low-light environments due to insufficient information in spike streams. To this end, we propose a bidirectional recurrent-based reconstruction framework to better handle such extreme conditions. In more detail, a light-robust representation (LR-Rep) is designed to aggregate temporal information in spike streams. Moreover, a fusion module is used to extract temporal features. Besides, we synthesize a reconstruction dataset for high-speed low-light scenes where light sources are carefully designed to be consistent with reality. The experiment shows the superiority of our method. Importantly, our method also generalizes well to real spike streams. Our project is: https://github.com/Acnext/Learning-to-Robustly-Reconstruct-Dynamic-",
    "checked": false,
    "id": "ac3fb01c6a697e0d905665e4f869fcc147c96b1d",
    "semantic_title": "learning to robustly reconstruct low-light dynamic scenes from spike streams",
    "citation_count": 0,
    "authors": [
      "Liwen Hu*",
      "Ziluo Ding",
      "Mianzhi Liu",
      "Lei Ma*",
      "Tiejun Huang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2551_ECCV_2024_paper.php": {
    "title": "MarvelOVD: Marrying Object Recognition and Vision-Language Models for Robust Open-Vocabulary Object Detection",
    "volume": "main",
    "abstract": "Learning from pseudo-labels that generated with VLMs (Vision Language Models) has been shown as a promising solution to assist open vocabulary detection (OVD) in recent studies. However, due to the domain gap between VLM and vision-detection tasks, pseudo-labels produced by the VLMs are prone to be noisy, while the training design of the detector further amplifies the bias. In this work, we investigate the root cause of VLMs' biased prediction under the OVD context. Our observations lead to a simple yet effective paradigm, coded , that generates significantly better training targets and optimizes the learning procedure in an online manner by marrying the capability of the detector with the vision-language model. Our key insight is that the detector itself can act as a strong auxiliary guidance to accommodate VLM's inability of understanding both the \"background\" and the context of a proposal within the image. Based on it, we greatly purify the noisy pseudo-labels via Online Mining and propose Adaptive Reweighting to effectively suppress the biased training boxes that are not well aligned with the target object. In addition, we also identify a neglected \"base-novel-conflict\" problem and introduce stratified label assignments to prevent it. Extensive experiments on COCO and LVIS datasets demonstrate that our method outperforms the other state-of-the-arts by significant margins. Codes are available at https://github.com/wkfdb/ MarvelOVD",
    "checked": true,
    "id": "fc5f1166c8ed52a1bb518db9eb3c95ed6b58b2fa",
    "semantic_title": "marvelovd: marrying object recognition and vision-language models for robust open-vocabulary object detection",
    "citation_count": 0,
    "authors": [
      "Kuo Wang",
      "Lechao Cheng*",
      "Weikai Chen",
      "Pingping Zhang",
      "Liang Lin",
      "Fan Zhou",
      "Guanbin Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2554_ECCV_2024_paper.php": {
    "title": "WildVidFit: Video Virtual Try-On in the Wild via Image-Based Controlled Diffusion Models",
    "volume": "main",
    "abstract": "Video virtual try-on aims to generate realistic sequences that maintain garment identity and adapt to a person's pose and body shape in source videos. Traditional image-based methods, relying on warping and blending, struggle with complex human movements and occlusions, limiting their effectiveness in video try-on applications. Moreover, video-based models require extensive, high-quality data and substantial computational resources. To tackle these issues, we reconceptualize video try-on as a process of generating videos conditioned on garment descriptions and human motion. Our solution, WildVidFit, employs image-based controlled diffusion models for a streamlined, one-stage approach. This model, conditioned on specific garments and individuals, is trained on still images rather than videos. It leverages diffusion guidance from pre-trained models including a video masked autoencoder for segment smoothness improvement and a self-supervised model for feature alignment of adjacent frame in the latent space. This integration markedly boosts the model's ability to maintain temporal coherence, enabling more effective video try-on within an image-based framework. Our experiments on the VITON-HD and DressCode datasets, along with tests on the VVT and TikTok datasets, demonstrate WildVidFit's capability to generate fluid and coherent videos. The project page website is at wildvidfit-project.github. io",
    "checked": true,
    "id": "86aed3cde4b224fa954d1999fb73bc09f1424623",
    "semantic_title": "wildvidfit: video virtual try-on in the wild via image-based controlled diffusion models",
    "citation_count": 0,
    "authors": [
      "Zijian He",
      "Peixin Chen",
      "Guangrun Wang",
      "Guanbin Li*",
      "Philip Torr",
      "Liang Lin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2556_ECCV_2024_paper.php": {
    "title": "Interactive 3D Object Detection with Prompts",
    "volume": "main",
    "abstract": "The evolution of 3D object detection hinges not only on advanced models but also on effective and efficient annotation strategies. Despite this progress, the labor-intensive nature of 3D object annotation remains a bottleneck, hindering further development in the field. This paper introduces a novel approach, incorporated with \"prompt in 2D, detect in 3D\" and \"detect in 3D, refine in 3D\" strategies, to 3D object annotation: multi-modal interactive 3D object detection. Firstly, by allowing users to engage with simpler 2D interaction prompts (e.g., clicks or boxes on a camera image or a bird's eye view), we bridge the complexity gap between 2D and 3D spaces, reimagining the annotation workflow. Besides, Our framework also supports flexible iterative refinement to the initial 3D annotations, further assisting annotators in achieving satisfying results. Evaluation on the nuScenes dataset demonstrates the effectiveness of our method. And thanks to the prompt-driven and interactive designs, our approach also exhibits outstanding performance in open-set scenarios. This work not only offers a potential solution to the 3D object annotation problem but also paves the way for further innovations in the 3D object detection community",
    "checked": false,
    "id": "3ae960164cd6c018138d3a4d871f584becc4d27a",
    "semantic_title": "realtime 3d object detection for headsets",
    "citation_count": 2,
    "authors": [
      "Ruifei Zhang",
      "Xiangru Lin",
      "Wei Zhang",
      "Jincheng Lu",
      "Xuekuan Wang",
      "Xiao Tan",
      "Yingying Li",
      "Errui Ding",
      "Jingdong Wang",
      "Guanbin Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2562_ECCV_2024_paper.php": {
    "title": "How Video Meetings Change Your Expression",
    "volume": "main",
    "abstract": "Do our facial expressions change when we speak over video calls? Given two unpaired sets of videos of people, we seek to automatically find spatio-temporal patterns that are distinctive of each set. Existing methods use discriminative approaches and perform post-hoc explainability analysis. Such methods are insufficient as they are unable to provide insights beyond obvious dataset biases, and the explanations are useful only if humans themselves are good at the task. Instead, we tackle the problem through the lens of generative domain translation: our method generates a detailed report of learned, input-dependent spatio-temporal features and the extent to which they vary between the domains. We demonstrate that our method can discover behavioral differences between conversing face-to-face (F2F) and on video-calls (VCs). We also show the applicability of our method on discovering differences in presidential communication styles. Additionally, we are able to predict temporal change-points in videos that decouple expressions in an unsupervised way, and increase the interpretability and usefulness of our model. Finally, our method, being generative, can be used to transform a video call to appear as if it were recorded in a F2F setting. Experiments and visualizations show our approach is able to discover a range of behaviors, taking a step towards deeper understanding of human behaviors. Video results, code and data can be found at facet.cs.columbia.edu",
    "checked": true,
    "id": "7aade50400371b8da40d43d0a4faddcbaa083ecb",
    "semantic_title": "how video meetings change your expression",
    "citation_count": 0,
    "authors": [
      "Sumit Sarin*",
      "Utkarsh Mall",
      "Purva Tendulkar",
      "Carl Vondrick"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2569_ECCV_2024_paper.php": {
    "title": "GRACE: Graph-Based Contextual Debiasing for Fair Visual Question Answering",
    "volume": "main",
    "abstract": "Large language models (LLMs) exhibit exceptional reasoning capabilities and have played significant roles in knowledge-based visual question-answering (VQA) systems. By conditioning on in-context examples and task-specific prompts, they comprehensively understand input questions and provide answers relevant to the context. However, due to the reliance on in-context examples, LLMs are susceptible to inheriting dataset biases in context descriptions and the provided examples. Innovative methods are required to ensure that LLMs can deliver unbiased yet contextually relevant responses. To tackle this challenge, we present GRAph-based Contextual DEbiasing (GRACE), a novel graph-based method for debiasing knowledge-based VQA models. This approach consists of two novel and generally applicable components. First, we propose an unsupervised context graph learning method that combats biases by explicitly creating a balanced context graph under the guidance of fairness constraints. Second, building upon the context graph, we consider both semantic features and reasoning processes to enhance prompting with more relevant and diverse in-context examples. Through extensive experimentation on both in-distribution (OK-VQA) and out-of-distribution (VQA-CP, GQA-OOD) datasets, we demonstrate the effectiveness of GRACE in mitigating biases and achieving generalization. Additionally, analyses of the model performance across gender groups demonstrate GRACE's potential impacts on social equity. Our source code is publicly available at https://github.com/SuperJohnZhang/ContextGraphKVQA",
    "checked": true,
    "id": "55b80acf326d573f498934ebaf68831c8eb955d6",
    "semantic_title": "grace: graph-based contextual debiasing for fair visual question answering",
    "citation_count": 0,
    "authors": [
      "Yifeng Zhang",
      "Ming Jiang",
      "Qi Zhao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2571_ECCV_2024_paper.php": {
    "title": "Neural Volumetric World Models for Autonomous Driving",
    "volume": "main",
    "abstract": "Effectively navigating a dynamic 3D world requires a comprehensive understanding of the 3D geometry and motion of surrounding objects and layouts. However, existing methods for perception and planning in autonomous driving primarily rely on a 2D spatial representation, based on a bird's eye perspective of the scene, which is insufficient for modeling motion characteristics and decision-making in real-world 3D settings with occlusion, partial observability, subtle motions, and varying terrains. Motivated by this key insight, we present a novel framework for learning end-to-end autonomous driving based on volumetric representations. Our proposed neural volumetric world modeling approach, NeMo, can be trained in a self-supervised manner for image reconstruction and occupancy prediction tasks, benefiting scalable training and deployment paradigms such as imitation learning. Specifically, we demonstrate how the higher-fidelity modeling of 3D volumetric representations benefits vision-based motion planning. We further propose a motion flow module to model complex dynamic scenes, enabling additional robust spatiotemporal consistency supervision. Moreover, a temporal attention module is introduced to effectively integrate predicted future volumetric features for the planning task. Our proposed sensorimotor agent achieves state-of-the-art driving performance on nuScenes and CARLA, outperforming prior baseline methods by over 18%",
    "checked": false,
    "id": "947e90c4a61f322fac0bdd2c8008a4ba87a5126e",
    "semantic_title": "planning with adaptive world models for autonomous driving",
    "citation_count": 0,
    "authors": [
      "Zanming Huang*",
      "Jimuyang Zhang*",
      "Eshed Ohn-Bar*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2577_ECCV_2024_paper.php": {
    "title": "IVTP: Instruction-guided Visual Token Pruning for Large Vision-Language Models",
    "volume": "main",
    "abstract": "Inspired by the remarkable achievements of Large Language Models (LLMs), Large Vision-Language Models (LVLMs) have likewise experienced significant advancements. However, the increased computational cost and token budget occupancy associated with lengthy visual tokens pose significant challenge to the practical applications. Considering that not all visual tokens are essential to the final response, selectively pruning redundant visual tokens can effectively alleviate this challenge. In this paper, we present a novel Instruction-guided Visual Token Pruning (IVTP) approach for LVLMs, which is designed to strike a better balance between computational efficiency and the performance. Specifically, a Group-wise Token Pruning (GTP) module based on attention rollout is integrated into the grouped transformer layer to achieve intra-group attention aggregation via residual connection, thereby improving the assessment of visual token importance, especially for LVLMs with a frozen visual encoder. We then extend the module to LLM in order to further filter out visual tokens that are pertinent to the current textual instructions, by introducing a semantically related pseudo CLS token to serve as a reference for token pruning. This two-stage token pruning mechanism permits a systematic and efficient reduction in the quantity of visual tokens while preserving essential visual information. We apply the proposed method to the most representative LVLM, i.e. LLaVA-1.5. Experimental results demonstrate that when the number of visual tokens is reduced by 88.9%, the computational complexity is decreased by over 46%, with only an average 1.0% accuracy drop across 12 benchmarks, and remarkably surpasses the state-of-the-art token pruning methods",
    "checked": false,
    "id": "dd9f8b8e28af59512908d8c9f6e51146740f3cff",
    "semantic_title": "madtp: multimodal alignment-guided dynamic token pruning for accelerating vision-language transformer",
    "citation_count": 5,
    "authors": [
      "Kai Huang*",
      "Hao Zou",
      "Ye Xi",
      "Bochen Wang",
      "Zhen Xie",
      "Liang Yu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2585_ECCV_2024_paper.php": {
    "title": "RegionDrag: Fast Region-Based Image Editing with Diffusion Models",
    "volume": "main",
    "abstract": "Point-drag-based image editing methods, like DragDiffusion, have attracted significant attention. However, point-drag-based approaches suffer from computational overhead and misinterpretation of user intentions, due to the sparsity of point-based editing instructions. In this paper, we propose a region-based copy-and-paste dragging method, , to overcome these limitations. allows users to express their editing instructions in the form of handle and target regions, enabling more precise control and alleviating ambiguity. In addition, region-based operations complete editing in one iteration and are much faster than point-drag-based methods. We also incorporate the attention-swapping technique for enhanced stability during editing. To validate our approach, we extend existing point-drag-based datasets with region-based dragging instructions. Experimental results demonstrate that outperforms existing point-drag-based approaches in terms of speed, accuracy, and alignment with user intentions. Remarkably, completes the edit on an image with a resolution of 512×512 in less than 2 seconds, which is more than 100× faster than DragDiffusion, while achieving better performance. Project page: https://visual-ai.github.io/regiondrag",
    "checked": true,
    "id": "021f4d28ac16d074c484a0700d14efd93c6bed2d",
    "semantic_title": "regiondrag: fast region-based image editing with diffusion models",
    "citation_count": 0,
    "authors": [
      "Jingyi Lu",
      "Xinghui Li",
      "Kai Han*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2588_ECCV_2024_paper.php": {
    "title": "On the Error Analysis of 3D Gaussian Splatting and an Optimal Projection Strategy",
    "volume": "main",
    "abstract": "3D Gaussian Splatting has garnered extensive attention and application in real-time neural rendering. Concurrently, concerns have been raised about the limitations of this technology in aspects such as point cloud storage, performance, and robustness in sparse viewpoints, leading to various improvements. However, there has been a notable lack of attention to the fundamental problem of projection errors introduced by the local affine approximation inherent in the splatting itself, and the consequential impact of these errors on the quality of photo-realistic rendering. This paper addresses the projection error function of 3D Gaussian Splatting, commencing with the residual error from the first-order Taylor expansion of the projection function. The analysis establishes a correlation between the error and the Gaussian mean position. Subsequently, leveraging function optimization theory, this paper analyzes the function's minima to provide an optimal projection strategy for Gaussian Splatting referred to Optimal Gaussian Splatting, which can accommodate a variety of camera models. Experimental validation further confirms that this projection methodology reduces artifacts, resulting in a more convincingly realistic rendering",
    "checked": true,
    "id": "ae002bbeeb8cdec0013bd4555b27542fc9ea5be2",
    "semantic_title": "on the error analysis of 3d gaussian splatting and an optimal projection strategy",
    "citation_count": 7,
    "authors": [
      "Letian Huang",
      "Jiayang Bai",
      "Jie Guo*",
      "Yuanqi Li",
      "Yanwen Guo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2596_ECCV_2024_paper.php": {
    "title": "Bad Students Make Great Teachers: Active Learning Accelerates Large-Scale Visual Understanding",
    "volume": "main",
    "abstract": "Power-law scaling indicates that large-scale training with uniform sampling is prohibitively slow. Active learning methods aim to increase data efficiency by prioritizing learning on the most relevant examples. Despite their appeal, these methods have yet to be widely adopted since no one algorithm has been shown to a) generalize across models and tasks b) scale to large datasets and c) yield overall FLOP savings when accounting for the overhead of data selection. In this work we propose a method which satisfies these three properties, leveraging small, cheap proxy models to estimate \"learnability\" scores for datapoints, which are used to prioritize data for training much larger models. As a result, models trained using our methods – ClassAct and ActiveCLIP – require 46% and 51% fewer training updates and up to 25% less total computation to reach the same performance as uniformly-trained visual classifiers on JFT and multimodal models on ALIGN, respectively. Finally, we find our data-prioritization scheme to be complementary with recent data-curation and learning objectives, yielding a new state-of-the-art in several multimodal transfer tasks",
    "checked": true,
    "id": "1474e217837d8ca031374d87f0f00be5e578a3de",
    "semantic_title": "bad students make great teachers: active learning accelerates large-scale visual understanding",
    "citation_count": 6,
    "authors": [
      "Talfan Evans*",
      "Shreya Pathak",
      "Hamza Merzic",
      "Jonathan Richard Schwarz",
      "Ryutaro Tanno",
      "Olivier Henaff*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2597_ECCV_2024_paper.php": {
    "title": "Analytic-Splatting: Anti-Aliased 3D Gaussian Splatting via Analytic Integration",
    "volume": "main",
    "abstract": "3D Gaussian Splatting (3DGS) recently gained popularity by combining the advantages of both primitive-based and volumetric 3D representations, resulting in improved quality and efficiency for 3D scene rendering. However, 3DGS is not alias-free and still produces severe blurring or jaggies when rendered at varying resolutions because the discrete sampling scheme used treats each pixel as an isolated single point, which is insensitive to changes in the footprints of pixels and is restricted in sampling bandwidth. In this paper, we use a conditioned logistic function as the analytic approximation of the cumulative distribution function (CDF) of the Gaussian signal and calculate the integral by subtracting the CDFs. We introduce this approximation to two-dimensional pixel shading and present Analytic-Splatting, which analytically approximates the Gaussian integral within the 2D-pixel window area to better capture the intensity response of each pixel. Then, we use the approximated response of the pixel window integral area to participate in the transmittance calculation of volume rendering, making Analytic-Splatting sensitive to the changes in pixel footprint at different resolutions. Extensive experiments on various datasets validate that our approach has better anti-aliasing capability that gives more details and better fidelity",
    "checked": true,
    "id": "20164d2c808859ff85ea1eb742ff5d9027157544",
    "semantic_title": "analytic-splatting: anti-aliased 3d gaussian splatting via analytic integration",
    "citation_count": 10,
    "authors": [
      "Zhihao Liang*",
      "Qi Zhang*",
      "Wenbo Hu",
      "Ying Feng",
      "Lei ZHU",
      "Kui Jia*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2600_ECCV_2024_paper.php": {
    "title": "GRA: Detecting Oriented Objects through Group-wise Rotating and Attention",
    "volume": "main",
    "abstract": "Oriented object detection, an emerging task in recent years, aims to identify and locate objects across varied orientations. This requires the detector to accurately capture the orientation information, which varies significantly within and across images. Despite the existing substantial efforts, simultaneously ensuring model effectiveness and parameter efficiency remains challenging in this scenario. In this paper, we propose a lightweight yet effective Group-wise Rotating and Attention (GRA) module to replace the convolution operations in backbone networks for oriented object detection. GRA can adaptively capture fine-grained features of objects with diverse orientations, comprising two key components: Group-wise Rotating and Group-wise Attention. Group-wise Rotating first divides the convolution kernel into groups, where each group extracts different object features by rotating at a specific angle according to the object orientation. Subsequently, Group-wise Attention is employed to adaptively enhance the object-related regions in the feature. The collaborative effort of these components enables GRA to effectively capture the various orientation information while maintaining parameter efficiency. Extensive experimental results demonstrate the superiority of our method. For example, GRA achieves a new state-of-the-art (SOTA) on the DOTA-v2.0 benchmark, while saving the parameters by nearly 50% compared to the previous SOTA method. Code is available at https://github.com/wangjiangshan0725/GRA",
    "checked": true,
    "id": "37682682ac04d63d8529814a71c04552f7d2f03c",
    "semantic_title": "gra: detecting oriented objects through group-wise rotating and attention",
    "citation_count": 2,
    "authors": [
      "Jiangshan Wang*",
      "Yifan Pu",
      "Yizeng Han",
      "Jiayi Guo",
      "Yiru Wang",
      "Xiu Li*",
      "Gao Huang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2615_ECCV_2024_paper.php": {
    "title": "Portrait4D-v2: Pseudo Multi-View Data Creates Better 4D Head Synthesizer",
    "volume": "main",
    "abstract": "In this paper, we propose a novel learning approach for feed-forward one-shot 4D head avatar synthesis. Different from existing methods that often learn from reconstructing monocular videos guided by 3DMM, we employ pseudo multi-view videos to learn a 4D head synthesizer in a data-driven manner, avoiding reliance on inaccurate 3DMM reconstruction that could be detrimental to the synthesis performance. The key idea is to first learn a 3D head synthesizer using synthetic multi-view images to convert monocular real videos into multi-view ones, and then utilize the pseudo multi-view videos to learn a 4D head synthesizer via cross-view self-reenactment. By leveraging a simple vision transformer backbone with motion-aware cross-attentions, our method exhibits superior performance compared to previous methods in terms of reconstruction fidelity, geometry consistency, and motion control accuracy. We hope our method offers novel insights into integrating 3D priors with 2D supervisions for improved 4D head avatar creation",
    "checked": true,
    "id": "5844c5f90b9becc2aeb25125328f0a3c1aa3f7e3",
    "semantic_title": "portrait4d-v2: pseudo multi-view data creates better 4d head synthesizer",
    "citation_count": 4,
    "authors": [
      "Yu Deng*",
      "Duomin Wang",
      "Baoyuan Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2623_ECCV_2024_paper.php": {
    "title": "CSOT: Cross-Scan Object Transfer for Semi-Supervised LiDAR Object Detection",
    "volume": "main",
    "abstract": "Large-scale 3D bounding box annotation is crucial for LiDAR object detection but comes at a high cost. Semi-supervised object detection (SSOD) offers promising solutions to leverage unannotated data, but the predominant pseudo-labeling approach requires careful hyperparameter tuning for training on noisy teacher labels. In this work, we propose a () paradigm for LiDAR SSOD. Central to our approach is , a transformer-based network that predicts possible placement locations and the object-place fitness scores for inserting annotated objects into unlabeled scans in a semantic coherence manner. Based on , successfully enables object copy-paste in LiDAR SSOD for the first time. To train object detectors on partially annotated scans generated by , we adopt a spatial-aware classification loss throughout our partial supervision to handle false negative issues caused by treating all unlabeled objects as background. We conduct extensive experiments to verify the efficacy and generality of our method. Compared to other state-of-the-art label-efficient methods used in LiDAR detection, our approach requires the least amount of annotation while achieves the best detector. Using only 1% of the labeled data on the Waymo dataset, our semi-supervised detector achieves performance on par with the fully supervised baseline. Similarly, on the nuScenes dataset, our semi-supervised CenterPoint reaches 99% of the fully supervised model's detection performance in terms of NDS score, while using just 5% of the labeled data. Code is released at https://github.com/JinglinZhan/CSOT",
    "checked": false,
    "id": "6e1919e340ec6117eb4681a172b72049ebe8dfe8",
    "semantic_title": "alleviating foreground sparsity for semi-supervised monocular 3d object detection",
    "citation_count": 2,
    "authors": [
      "Jinglin Zhan",
      "Tiejun Liu",
      "Rengang Li",
      "Zhaoxiang Zhang",
      "Yuntao Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2627_ECCV_2024_paper.php": {
    "title": "Learning from the Web: Language Drives Weakly-Supervised Incremental Learning for Semantic Segmentation",
    "volume": "main",
    "abstract": "Current weakly-supervised incremental learning for semantic segmentation (WILSS) approaches only consider replacing pixel-level annotations with image-level labels, while the training images are still from well-designed datasets. In this work, we argue that widely available web images can also be considered for the learning of new classes. To achieve this, firstly we introduce a strategy to select web images which are similar to previously seen examples in the latent space using a Fourier-based domain discriminator. Then, an effective caption-driven reharsal strategy is proposed to preserve previously learnt classes. To our knowledge, this is the first work to rely solely on web images for both the learning of new concepts and the preservation of the already learned ones in WILSS. Experimental results show that the proposed approach can reach state-of-the-art performances without using manually selected and annotated data in the incremental steps",
    "checked": true,
    "id": "4e31ab8e5e7e13eeb42cdf6e0dea4c3251db428c",
    "semantic_title": "learning from the web: language drives weakly-supervised incremental learning for semantic segmentation",
    "citation_count": 0,
    "authors": [
      "Chang Liu",
      "Giulia Rizzoli",
      "Pietro Zanuttigh",
      "Fu Li",
      "Yi Niu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2629_ECCV_2024_paper.php": {
    "title": "ShareGPT4V: Improving Large Multi-Modal Models with Better Captions",
    "volume": "main",
    "abstract": "Modality alignment serves as the cornerstone for large multi-modal models (LMMs). However, the impact of different attributes (e.g., data type, quality, and scale) of training data on facilitating effective alignment is still under-explored. In this paper, we delve into the influence of training data on LMMs, uncovering three pivotal findings: 1) Highly detailed captions enable more nuanced vision-language alignment, significantly boosting the performance of LMMs in diverse benchmarks, surpassing outcomes from brief captions or VQA data; 2) Cutting-edge LMMs can be close to the captioning capability of costly human annotators, and open-source LMMs could reach similar quality after lightweight fine-tuning; 3) The performance of LMMs scales with the number of detailed captions, exhibiting remarkable improvements across a range from thousands to millions of captions. Drawing from these findings, we introduce the ShareGPT4V series for advanced modality alignment. It includes ShareGPT4V, consisting of 100K high-quality captions curated from GPT4-Vision; ShareGPT4V-PT, containing 1.2M captions produced by our Share-Captioner that can be close to the captioning capabilities of GPT4-Vision; and ShareGPT4V-7B, a simple yet superior LMM excelling in most multi-modal benchmarks, which realized better alignment based on our large-scale high-quality captions. The project is available at https://sharegpt4v.github.io/",
    "checked": true,
    "id": "f68f6f2a057c4e6e5a3c91fc8563533d9bf6e560",
    "semantic_title": "sharegpt4v: improving large multi-modal models with better captions",
    "citation_count": 329,
    "authors": [
      "Lin Chen*",
      "Jinsong Li",
      "Xiaoyi Dong",
      "Pan Zhang",
      "Conghui He",
      "Jiaqi Wang",
      "Feng Zhao*",
      "Dahua Lin*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2634_ECCV_2024_paper.php": {
    "title": "Eyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text Transformation",
    "volume": "main",
    "abstract": "Multimodal large language models (MLLMs) have shown impressive reasoning abilities. However, they are also more vulnerable to jailbreak attacks than their LLM predecessors. Although still capable of detecting the unsafe responses, we observe that safety mechanisms of the pre-aligned LLMs in MLLMs can be easily bypassed with the introduction of image features. To construct robust MLLMs, we propose ECSO (Eyes Closed, Safety On), a novel training-free protecting approach that exploits the inherent safety awareness of MLLMs, and generates safer responses via adaptively transforming unsafe images into texts to activate the intrinsic safety mechanism of pre-aligned LLMs in MLLMs. Experiments on five state-of-the-art (SoTA) MLLMs demonstrate that ECSO enhances model safety significantly (, 37.6% improvement on the MM-SafetyBench (SD+OCR) and 71.3% on VLSafe with LLaVA-1.5-7B), while consistently maintaining utility results on common MLLM benchmarks. Furthermore, we show that ECSO can be used as a data engine to generate supervised-finetuning (SFT) data for MLLM alignment without extra human intervention",
    "checked": true,
    "id": "6a6d8794f77dcdabebbf80b8f79956f707f16eb9",
    "semantic_title": "eyes closed, safety on: protecting multimodal llms via image-to-text transformation",
    "citation_count": 15,
    "authors": [
      "Yunhao Gou*",
      "Kai Chen",
      "Zhili LIU",
      "Lanqing Hong",
      "Hang Xu",
      "Zhenguo Li",
      "Dit-Yan Yeung",
      "James Kwok",
      "Yu Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2637_ECCV_2024_paper.php": {
    "title": "Invertible Neural Warp for NeRF",
    "volume": "main",
    "abstract": "This paper tackles the simultaneous optimization of pose and Neural Radiance Fields (NeRF). Departing from the conventional practice of using explicit global representations for camera pose, we propose a novel overparameterized representation that models camera poses as learnable rigid warp functions. We establish that modeling the rigid warps must be tightly coupled with constraints and regularization imposed. Specifically, we highlight the critical importance of enforcing invertibility when learning rigid warp functions via neural network and propose the use of an Invertible Neural Network (INN) coupled with a geometry-informed constraint for this purpose. We present results on synthetic and real-world datasets, and demonstrate that our approach outperforms existing baselines in terms of pose estimation and high-fidelity reconstruction due to enhanced optimization convergence",
    "checked": true,
    "id": "1c9961649b5887d763a253661fe49df274bd5845",
    "semantic_title": "invertible neural warp for nerf",
    "citation_count": 0,
    "authors": [
      "Shin-Fang Chng*",
      "Ravi Garg",
      "Hemanth Saratchandran",
      "Simon Lucey"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2641_ECCV_2024_paper.php": {
    "title": "Enhancing Vectorized Map Perception with Historical Rasterized Maps",
    "volume": "main",
    "abstract": "In autonomous driving, there is growing interest in end-to-end online vectorized map perception in bird's-eye-view (BEV) space, with an expectation that it could replace traditional high-cost offline high-definition (HD) maps. However, the accuracy and robustness of these methods can be easily compromised in challenging conditions, such as occlusion or adverse weather, when relying only on onboard sensors. In this paper, we propose , leveraging a low-cost Historical Rasterized Map to enhance online vectorized map perception. The historical rasterized map can be easily constructed from past predicted vectorized results and provides valuable complementary information. To fully exploit a historical map, we propose two novel modules to enhance BEV features and map element queries. For BEV features, we employ a feature aggregation module to encode features from both onboard images and the historical map. For map element queries, we design a query initialization module to endow queries with priors from the historical map. The two modules contribute to leveraging map information in online perception. Our can be integrated with most online vectorized map perception methods. We integrate it in two state-of-the-art methods, significantly improving their performance on both the nuScenes and Argoverse 2 datasets. The source code is released at https://github.com/HXMap/HRMapNet",
    "checked": true,
    "id": "8961ab89ea91bdaa398b985570809361515ce1d5",
    "semantic_title": "enhancing vectorized map perception with historical rasterized maps",
    "citation_count": 1,
    "authors": [
      "Xiaoyu Zhang",
      "Guangwei Liu",
      "Zihao Liu",
      "Ningyi Xu",
      "Yunhui Liu*",
      "Ji Zhao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2642_ECCV_2024_paper.php": {
    "title": "Efficient and Versatile Robust Fine-Tuning of Zero-shot Models",
    "volume": "main",
    "abstract": "Large-scale image-text pre-trained models enable zero-shot classification and provide consistent accuracy across various data distributions. Nonetheless, optimizing these models in downstream tasks typically requires fine-tuning, which reduces generalization to out-of-distribution (OOD) data and demands extensive computational resources. We introduce Robust Adapter (R-Adapter), a novel method for fine-tuning zero-shot models to downstream tasks while simultaneously addressing both these issues. Our method integrates lightweight modules into the pre-trained model and employs novel self-ensemble techniques to boost OOD robustness and reduce storage expenses substantially. Furthermore, we propose MPM-NCE loss designed for fine-tuning on vision-language downstream tasks. It ensures precise alignment of multiple image-text pairs and discriminative feature learning. By extending the benchmark for robust fine-tuning beyond classification to include diverse tasks such as cross-modal retrieval and open vocabulary segmentation, we demonstrate the broad applicability of R-Adapter. Our extensive experiments demonstrate that R-Adapter achieves state-of-the-art performance across a diverse set of tasks, tuning only 13% of the parameters of the CLIP encoders",
    "checked": true,
    "id": "2a24920c85abdb4d83e8ed414178a0b2938e8ab5",
    "semantic_title": "efficient and versatile robust fine-tuning of zero-shot models",
    "citation_count": 0,
    "authors": [
      "Sungyeon Kim*",
      "Boseung Jeong",
      "Donghyun Kim",
      "Suha Kwak*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2657_ECCV_2024_paper.php": {
    "title": "Part2Object: Hierarchical Unsupervised 3D Instance Segmentation",
    "volume": "main",
    "abstract": "Unsupervised 3D instance segmentation aims to segment objects from a 3D point cloud without any annotations. Existing methods face the challenge of either too loose or too tight clustering, leading to under-segmentation or over-segmentation. To address this issue, we propose Part2Object, hierarchical clustering with object guidance. Part2Object employs multi-layer clustering from points to object parts and objects, allowing objects to manifest at any layer. Additionally, it extracts and utilizes 3D objectness priors from temporally consecutive 2D RGB frames to guide the clustering process. Moreover, we propose Hi-Mask3D to support hierarchical 3D object part and instance segmentation. By training Hi-Mask3D on the objects and object parts extracted from Part2Object, we achieve consistent and superior performance compared to state-of-the-art models in various settings, including unsupervised instance segmentation, data-efficient fine-tuning, and cross-dataset generalization. Code is release at https://github. com/ChengShiest/Part2Object",
    "checked": true,
    "id": "d518403e04de71c709e31484cdad7717e5ceddd1",
    "semantic_title": "part2object: hierarchical unsupervised 3d instance segmentation",
    "citation_count": 0,
    "authors": [
      "Cheng Shi",
      "Yulin Zhang",
      "Bin Yang",
      "Jiajin Tang",
      "Yuexin Ma",
      "Sibei Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2660_ECCV_2024_paper.php": {
    "title": "PetFace: A Large-Scale Dataset and Benchmark for Animal Identification",
    "volume": "main",
    "abstract": "Automated animal face identification plays a crucial role in the monitoring of behaviors, conducting of surveys, and finding of lost animals. Despite the advancements in human face identification, the lack of datasets and benchmarks in the animal domain has impeded progress. In this paper, we introduce the dataset, a comprehensive resource for animal face identification encompassing 257,484 unique individuals across 13 animal families and 319 breed categories, including both experimental and pet animals. This large-scale collection of individuals facilitates the investigation of unseen animal face verification, an area that has not been sufficiently explored in existing datasets due to the limited number of individuals. Moreover, also has fine-grained annotations such as sex, breed, color, and pattern. We provide multiple benchmarks including re-identification for seen individuals and verification for unseen individuals. The models trained on our dataset outperform those trained on prior datasets, even for detailed breed variations and unseen animal families. Our result also indicates that there is some room to improve the performance of integrated identification on multiple animal families. We hope the dataset will facilitate animal face identification and encourage the development of non-invasive animal automatic identification methods. Our dataset and code are available at https: //dahlian00.github.io/PetFacePage/",
    "checked": true,
    "id": "e9f37e9986a7323a5372e399476a76a749acb235",
    "semantic_title": "petface: a large-scale dataset and benchmark for animal identification",
    "citation_count": 1,
    "authors": [
      "Risa Shinoda*",
      "Kaede Shiohara"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2662_ECCV_2024_paper.php": {
    "title": "MVSGaussian: Fast Generalizable Gaussian Splatting Reconstruction from Multi-View Stereo",
    "volume": "main",
    "abstract": "We present MVSGaussian, a new generalizable 3D Gaussian representation approach derived from Multi-View Stereo (MVS) that can efficiently reconstruct unseen scenes. Specifically, 1) we leverage MVS to encode geometry-aware Gaussian representations and decode them into Gaussian parameters. 2) To further enhance performance, we propose a hybrid Gaussian rendering that integrates an efficient volume rendering design for novel view synthesis. 3) To support fast fine-tuning for specific scenes, we introduce a multi-view geometric consistent aggregation strategy to effectively aggregate the point clouds generated by the generalizable model, serving as the initialization for per-scene optimization. Compared with previous generalizable NeRF-based methods, which typically require minutes of fine-tuning and seconds of rendering per image, MVSGaussian achieves real-time rendering with better synthesis quality for each scene. Compared with the vanilla 3D-GS, MVSGaussian achieves better view synthesis with less training computational cost. Extensive experiments on DTU, Real Forward-facing, NeRF Synthetic, and Tanks and Temples datasets validate that MVSGaussian attains state-of-the-art performance with convincing generalizability, real-time rendering speed, and fast per-scene optimization",
    "checked": true,
    "id": "3043ba3f7cb0656120032151f47f774d4aa61c80",
    "semantic_title": "mvsgaussian: fast generalizable gaussian splatting reconstruction from multi-view stereo",
    "citation_count": 4,
    "authors": [
      "Tianqi Liu",
      "Guangcong Wang",
      "Shoukang Hu",
      "Liao Shen",
      "Xinyi Ye",
      "Yuhang Zang",
      "Zhiguo Cao*",
      "Wei Li",
      "Ziwei Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2665_ECCV_2024_paper.php": {
    "title": "Zero-Shot Detection of AI-Generated Images",
    "volume": "main",
    "abstract": "Detecting AI-generated images has become an extraordinarily difficult challenge as new generative architectures emerge on a daily basis with more and more capabilities and unprecedented realism. New versions of many commercial tools, such as DALL·E, Midjourney, and Stable Diffusion, have been released recently, and it is impractical to continually update and retrain supervised forensic detectors to handle such a large variety of models. To address this challenge, we propose a zero-shot entropy-based detector () that neither needs AI-generated training data nor relies on knowledge of generative architectures to artificially synthesize their artifacts. Inspired by recent works on machine-generated text detection, our idea is to measure how surprising the image under analysis is compared to a model of real images. To this end, we rely on a lossless image encoder that estimates the probability distribution of each pixel given its context. To ensure computational efficiency, the encoder has a multi-resolution architecture and contexts comprise mostly pixels of the lower-resolution version of the image. Since only real images are needed to learn the model, the detector is independent of generator architectures and synthetic training data. Using a single discriminative feature, the proposed detector achieves state-of-the-art performance. On a wide variety of generative models it achieves an average improvement of more than 3% over the SoTA in terms of accuracy. Code is available at https://grip-unina.github.io/ZED/",
    "checked": true,
    "id": "c3659e3249946630ef009a0aecca58414d8da2c6",
    "semantic_title": "zero-shot detection of ai-generated images",
    "citation_count": 0,
    "authors": [
      "Davide Cozzolino",
      "GIovanni Poggi",
      "Matthias Niessner",
      "Luisa Verdoliva*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2666_ECCV_2024_paper.php": {
    "title": "Language-Image Pre-training with Long Captions",
    "volume": "main",
    "abstract": "Language-image pre-training largely relies on how precisely and thoroughly a text describes its paired image. In practice, however, the contents of an image can be so rich that well describing them requires lengthy captions (e.g., with 10 sentences), which are usually missing in existing datasets. Consequently, there are currently no clear evidences on whether and how language-image pre-training could benefit from long captions. To figure this out, we first re-caption 30M images with detailed descriptions using a pre-trained Multi-modality Large Language Model (MLLM), and then study the usage of the resulting captions under a contrastive learning framework. We observe that, each sentence within a long caption is very likely to describe the image partially (e.g., an object). Motivated by this, we propose to dynamically sample sub-captions from the text label to construct multiple positive pairs, and introduce a grouping loss to match the embeddings of each sub-caption with its corresponding local image patches in a self-supervised manner. Experimental results on a wide range of downstream tasks demonstrate the consistent superiority of our method, termed , over previous alternatives, highlighting its fine-grained representational capacity. It is noteworthy that, on the tasks of image-text retrieval and semantic segmentation, our model trained with 30M image-text pairs achieves on par or even better performance than CLIP trained with 400M pairs. Project page is available at https://zyf0619sjtu.github.io/dream-lip",
    "checked": false,
    "id": "0f284b2fdf001ced671ef87bea3435849c1e8059",
    "semantic_title": "dreamlip: language-image pre-training with long captions",
    "citation_count": 8,
    "authors": [
      "Kecheng Zheng*",
      "Yifei Zhang",
      "Wei Wu",
      "Fan Lu",
      "Shuailei Ma",
      "Xin Jin",
      "Wei Chen",
      "Yujun Shen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2675_ECCV_2024_paper.php": {
    "title": "GKGNet: Group K-Nearest Neighbor based Graph Convolutional Network for Multi-Label Image Recognition",
    "volume": "main",
    "abstract": "Multi-Label Image Recognition (MLIR) is a challenging task that aims to predict multiple object labels in a single image while modeling the complex relationships between labels and image regions. Although convolutional neural networks and vision transformers have succeeded in processing images as regular grids of pixels or patches, these representations are sub-optimal for capturing irregular and discontinuous regions of interest. In this work, we present the first fully graph convolutional model, Group K-nearest neighbor based Graph convolutional Network (GKGNet), which models the connections between semantic label embeddings and image patches in a flexible and unified graph structure. To address the scale variance of different objects and to capture information from multiple perspectives, we propose the Group KGCN module for dynamic graph construction and message passing. Our experiments demonstrate that GKGNet achieves state-of-the-art performance with significantly lower computational costs on the challenging multi-label datasets, MS-COCO and VOC2007 datasets. Codes are available at https://github.com/jin-s13/GKGNet. : Corresponding authors",
    "checked": true,
    "id": "1753798797406ece3e3be69ba556a1205e842d82",
    "semantic_title": "gkgnet: group k-nearest neighbor based graph convolutional network for multi-label image recognition",
    "citation_count": 1,
    "authors": [
      "Ruijie Yao",
      "Sheng Jin",
      "Lumin Xu",
      "Wang Zeng",
      "Wentao Liu",
      "Chen Qian*",
      "Ping Luo",
      "Ji Wu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2686_ECCV_2024_paper.php": {
    "title": "DISCO: Embodied Navigation and Interaction via Differentiable Scene Semantics and Dual-level Control",
    "volume": "main",
    "abstract": "Building a general-purpose intelligent home-assistant agent skilled in diverse tasks by human commands is a long-term blueprint of embodied AI research, which poses requirements on task planning, environment modeling, and object interaction. In this work, we study primitive mobile manipulations for embodied agents, how to navigate and interact based on an instructed verb-noun pair. We propose DISCO, which features non-trivial advancements in contextualized scene modeling and efficient controls. In particular, DISCO incorporates differentiable scene representations of rich semantics in object and affordance, which is dynamically learned on the fly and facilitates navigation planning. Besides, we propose dual-level coarse-to-fine action controls leveraging both global and local cues to accomplish mobile manipulation tasks efficiently. DISCO easily integrates into embodied tasks such as embodied instruction following. To validate our approach, we take the ALFRED benchmark of large-scale long-horizon vision-language navigation and interaction tasks as a test bed. In extensive experiments, we make comprehensive evaluations and demonstrate that DISCO outperforms the art by a sizable +8.6% success rate margin in unseen scenes even without step-by-step instructions. Our code is publicly released at https://github.com/AllenXuuu/DISCO",
    "checked": true,
    "id": "84fcfebaa211c6ae95c04bfcd7aecb601e5ff7ed",
    "semantic_title": "disco: embodied navigation and interaction via differentiable scene semantics and dual-level control",
    "citation_count": 0,
    "authors": [
      "Xinyu Xu*",
      "Shengcheng Luo",
      "Yanchao Yang",
      "Yong-Lu Li*",
      "Cewu Lu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2688_ECCV_2024_paper.php": {
    "title": "You Only Learn One Query: Learning Unified Human Query for Single-Stage Multi-Person Multi-Task Human-Centric Perception",
    "volume": "main",
    "abstract": "Human-centric perception (detection, segmentation, pose estimation, and attribute analysis) is a long-standing problem for computer vision. This paper introduces a unified and versatile framework (HQNet) for single-stage multi-person multi-task human-centric perception (HCP). Our approach centers on learning a unified human query representation, denoted as Human Query, which captures intricate instance-level features for individual persons and disentangles complex multi-person scenarios. Although different HCP tasks have been well-studied individually, single-stage multi-task learning of HCP tasks has not been fully exploited in the literature due to the absence of a comprehensive benchmark dataset. To address this gap, we propose COCO-UniHuman benchmark to enable model development and comprehensive evaluation. Experimental results demonstrate the proposed method's state-of-the-art performance among multi-task HCP models and its competitive performance compared to task-specific HCP models. Moreover, our experiments underscore Human Query's adaptability to new HCP tasks, thus demonstrating its robust generalization capability. Codes and data are available at https://github.com/lishuhuai527/COCO-UniHuman",
    "checked": true,
    "id": "38bf9a99866d76a7fa3986a1fb428ae1fff43ba7",
    "semantic_title": "you only learn one query: learning unified human query for single-stage multi-person multi-task human-centric perception",
    "citation_count": 2,
    "authors": [
      "Sheng Jin",
      "Shuhuai Li",
      "Tong Li",
      "Wentao Liu*",
      "Chen Qian",
      "Ping Luo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2698_ECCV_2024_paper.php": {
    "title": "Towards Real-World Adverse Weather Image Restoration: Enhancing Clearness and Semantics with Vision-Language Models",
    "volume": "main",
    "abstract": "This paper addresses the limitations of adverse weather image restoration approaches trained on synthetic data when applied to real-world scenarios. We formulate a semi-supervised learning framework employing vision-language models to enhance restoration performance across diverse adverse weather conditions in real-world settings. Our approach involves assessing image clearness and providing semantics using vision-language models on real data, serving as supervision signals for training restoration models. For clearness enhancement, we use real-world data, utilizing a dual-step strategy with pseudo-labels assessed by vision-language models and weather prompt learning. For semantic enhancement, we integrate real-world data by adjusting weather conditions in vision-language model descriptions while preserving semantic meaning. Additionally, we introduce an effective training strategy to bootstrap restoration performance. Our approach achieves superior results in real-world adverse weather image restoration, demonstrated through qualitative and quantitative comparisons with state-of-the-art works",
    "checked": true,
    "id": "80235a7e5d3c4a6451b15e47aef689e772475299",
    "semantic_title": "towards real-world adverse weather image restoration: enhancing clearness and semantics with vision-language models",
    "citation_count": 0,
    "authors": [
      "Jiaqi Xu*",
      "Mengyang Wu",
      "Xiaowei Hu*",
      "Chi-Wing Fu",
      "Qi Dou",
      "Pheng-Ann Heng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2709_ECCV_2024_paper.php": {
    "title": "Facial Affective Behavior Analysis with Instruction Tuning",
    "volume": "main",
    "abstract": "Facial affective behavior analysis (FABA) is crucial for understanding human mental states from images. However, traditional approaches primarily deploy models to discriminate among discrete emotion categories, and lack the fine granularity and reasoning capability for complex facial behaviors. The advent of Multi-modal Large Language Models (MLLMs) has been proven successful in general visual understanding tasks. However, directly harnessing MLLMs for FABA is challenging due to the scarcity of datasets and benchmarks, neglecting facial prior knowledge, and low training efficiency. To address these challenges, we introduce (i ) an instruction-following dataset for two FABA tasks, , facial emotion and action unit recognition, (ii ) a benchmark FABA-Bench with a new metric considering both recognition and generation ability, and (iii ) a new MLLM EmoLA as a strong baseline to the community. Our initiative on the dataset and benchmarks reveal the nature and rationale of facial affective behaviors, , fine-grained facial movement, interpretability, and reasoning. Moreover, to build an effective and efficient FABA MLLM, we introduce a facial prior expert module with face structure knowledge and a low-rank adaptation module into pre-trained MLLM. We conduct extensive experiments on FABA-Bench and four commonly-used FABA datasets. The results demonstrate that the proposed facial prior expert can boost the performance and EmoLA achieves the best results on our FABA-Bench. On commonly-used FABA datasets, EmoLA is competitive rivaling task-specific state-of-the-art models. The dataset and codes are available: https://johnx69.github.io/FABA/",
    "checked": true,
    "id": "b34111fe8e230d371dd9446d52388a84b5592390",
    "semantic_title": "facial affective behavior analysis with instruction tuning",
    "citation_count": 8,
    "authors": [
      "Yifan Li*",
      "Anh Dao",
      "Wentao Bao",
      "Zhen Tan",
      "Tianlong Chen",
      "Huan Liu",
      "Yu Kong"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2711_ECCV_2024_paper.php": {
    "title": "CoReS: Orchestrating the Dance of Reasoning and Segmentation",
    "volume": "main",
    "abstract": "The reasoning segmentation task, which demands a nuanced comprehension of intricate queries to accurately pinpoint object regions, is attracting increasing attention. However, Multi-modal Large Language Models (MLLM) often find it difficult to accurately localize the objects described in complex reasoning contexts. We believe that the act of reasoning segmentation should mirror the cognitive stages of human visual search, where each step is a progressive refinement of thought toward the final object. Thus we introduce the Chains of Reasoning and Segmenting (CoReS) and find this top-down visual hierarchy indeed enhances the visual search process. Specifically, we propose a dual-chain structure that generates multi-modal, chain-like outputs to aid the segmentation process. Furthermore, to steer the MLLM's outputs into this intended hierarchy, we incorporate in-context inputs as guidance. Extensive experiments demonstrate the superior performance of our CoReS, which surpasses the state-of-the-art method by 6.5% on the ReasonSeg dataset",
    "checked": true,
    "id": "882e2f6c4646ee1ddf5fff8590bfcb6bb3fefe79",
    "semantic_title": "cores: orchestrating the dance of reasoning and segmentation",
    "citation_count": 1,
    "authors": [
      "Xiaoyi Bao",
      "Siyang Sun",
      "Shuailei Ma",
      "Kecheng Zheng",
      "Yuxin Guo",
      "Guosheng Zhao",
      "Yun Zheng",
      "Xingang Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2738_ECCV_2024_paper.php": {
    "title": "MagDiff: Multi-Alignment Diffusion for High-Fidelity Video Generation and Editing",
    "volume": "main",
    "abstract": "The diffusion model is widely leveraged for either video generation or video editing. As each field has its task-specific problems, it is difficult to merely develop a single diffusion for completing both tasks simultaneously. Video diffusion sorely relying on the text prompt can be adapted to unify the two tasks. However, it lacks a high capability of aligning heterogeneous modalities between text and image, leading to various misalignment problems. In this work, we are the first to propose a unified Multi-alignment Diff usion, dubbed as MagDiff, for both tasks of high-fidelity video generation and editing. The proposed MagDiff introduces three types of alignments, including subject-driven alignment, adaptive prompts alignment, and high-fidelity alignment. Particularly, the subject-driven alignment is put forward to trade off the image and text prompts, serving as a unified foundation generative model for both tasks. The adaptive prompts alignment is introduced to emphasize different strengths of homogeneous and heterogeneous alignments by assigning different values of weights to the image and the text prompts. The high-fidelity alignment is developed to further enhance the fidelity of both video generation and editing by taking the subject image as an additional model input. Experimental results on four benchmarks suggest that our method outperforms the previous method on each task",
    "checked": true,
    "id": "f373c5569b45bf580b7502729a83761a791ee209",
    "semantic_title": "magdiff: multi-alignment diffusion for high-fidelity video generation and editing",
    "citation_count": 6,
    "authors": [
      "Haoyu Zhao",
      "Tianyi Lu",
      "Jiaxi Gu",
      "Xing Zhang",
      "Qingping Zheng",
      "Zuxuan Wu*",
      "Hang Xu",
      "Yu-Gang Jiang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2740_ECCV_2024_paper.php": {
    "title": "MambaIR: A Simple Baseline for Image Restoration with State-Space Model",
    "volume": "main",
    "abstract": "Recent years have seen significant advancements in image restoration, largely attributed to the development of modern deep neural networks, such as CNNs and Transformers. However, existing restoration backbones often face the dilemma between global receptive fields and efficient computation, hindering their application in practice. Recently, the Selective Structured State Space Model, especially the improved version Mamba, has shown great potential for long-range dependency modeling with linear complexity, which offers a way to resolve the above dilemma. However, the standard Mamba still faces certain challenges in low-level vision such as local pixel forgetting and channel redundancy. In this work, we introduce a simple but effective baseline, named MambaIR, which introduces both local enhancement and channel attention to improve the vanilla Mamba. In this way, our MambaIR takes advantage of the local pixel similarity and reduces the channel redundancy. Extensive experiments demonstrate the superiority of our method, for example, MambaIR outperforms SwinIR by up to 0.45dB on image SR, using similar computational cost but with a global receptive field. Code is available at https://github.com/csguoh/MambaIR",
    "checked": true,
    "id": "e730beb44042499763d36214c0498434e470dfd5",
    "semantic_title": "mambair: a simple baseline for image restoration with state-space model",
    "citation_count": 76,
    "authors": [
      "Hang Guo*",
      "Jinmin Li",
      "Tao Dai*",
      "Zhihao Ouyang",
      "Xudong Ren",
      "Shu-Tao Xia"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2757_ECCV_2024_paper.php": {
    "title": "I Can't Believe It's Not Scene Flow!",
    "volume": "main",
    "abstract": "State-of-the-art scene flow methods broadly fail to describe the motion of small objects, and existing evaluation protocols hide this failure by averaging over many points. To address this limitation, we propose Bucket Normalized EPE, a new class-aware and speed-normalized evaluation protocol that better contextualizes error comparisons between object types that move at vastly different speeds. In addition, we propose TrackFlow, a frustratingly simple supervised scene flow baseline that combines a high-quality 3D object detector (trained using standard class re-balancing techniques) with a simple Kalman filter-based tracker. Notably, TrackFlow achieves state-of-the-art performance on existing metrics and shows large improvements over prior work on our proposed metric. Our results highlight that scene flow evaluation must be class and speed aware, and supervised scene flow methods must address point-level class imbalances. Our evaluation toolkit and code is available on GitHub",
    "checked": true,
    "id": "58be977a2327a58239a1db1da4858bf407222c00",
    "semantic_title": "i can't believe it's not scene flow!",
    "citation_count": 3,
    "authors": [
      "Ishan Khatri*",
      "Kyle Vedder*",
      "Neehar Peri",
      "Deva Ramanan",
      "James Hays"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2763_ECCV_2024_paper.php": {
    "title": "Rethinking Unsupervised Outlier Detection via Multiple Thresholding",
    "volume": "main",
    "abstract": "In the realm of unsupervised image outlier detection, assigning outlier scores holds greater significance than its subsequent task: thresholding for predicting labels. This is because determining the optimal threshold on non-separable outlier score functions is an ill-posed problem. However, the lack of predicted labels not only hinders some real applications of current outlier detectors but also causes these methods not to be enhanced by leveraging the dataset's self-supervision. To advance existing scoring methods, we propose a multiple thresholding (Multi-T) module. It generates two thresholds that isolate inliers and outliers from the unlabelled target dataset, whereas outliers are employed to obtain better feature representation while inliers provide an uncontaminated manifold. Extensive experiments verify that Multi-T can significantly improve proposed outlier scoring methods. Moreover, Multi-T contributes to a naive distance-based method being state-of-the-art. Code is available at: https://github.com/zhliu-uod/Multi-T",
    "checked": true,
    "id": "8477a7d347353edb844302e768c2076c7978abce",
    "semantic_title": "rethinking unsupervised outlier detection via multiple thresholding",
    "citation_count": 0,
    "authors": [
      "Zhonghang Liu*",
      "Panzhong Lu",
      "Guoyang Xie",
      "Zhichao Lu",
      "Wen-Yan Lin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2764_ECCV_2024_paper.php": {
    "title": "Compress3D: a Compressed Latent Space for 3D Generation from a Single Image",
    "volume": "main",
    "abstract": "3D generation has witnessed significant advancements, yet efficiently producing high-quality 3D assets from a single image remains challenging. In this paper, we present a triplane autoencoder, which encodes 3D models into a compact triplane latent space to effectively compress both the 3D geometry and texture information. Within the autoencoder framework, we introduce a 3D-aware cross-attention mechanism, which utilizes low-resolution latent representations to query features from a high-resolution 3D feature volume, thereby enhancing the representation capacity of the latent space. Subsequently, we train a diffusion model on this refined latent space. In contrast to solely relying on image embedding for 3D generation, our proposed method advocates for the simultaneous utilization of both image embedding and shape embedding as conditions. Specifically, the shape embedding is estimated via a diffusion prior model conditioned on the image embedding. Through comprehensive experiments, we demonstrate that our method outperforms state-of-the-art algorithms, achieving superior performance while requiring less training data and time. Our approach enables the generation of high-quality 3D assets in merely 7 seconds on a single A100 GPU. More results and visualization can be found on our project page: https://compress3d.github.io/",
    "checked": true,
    "id": "73156f2c1975f64c88bf52f515f5cad626e6f315",
    "semantic_title": "compress3d: a compressed latent space for 3d generation from a single image",
    "citation_count": 3,
    "authors": [
      "Bowen Zhang*",
      "Tianyu Yang*",
      "Yu Li",
      "Lei Zhang",
      "Xi Zhao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2766_ECCV_2024_paper.php": {
    "title": "Scalable Group Choreography via Variational Phase Manifold Learning",
    "volume": "main",
    "abstract": "Generating group dance motion from the music is a challenging task with several industrial applications. Although several methods have been proposed to tackle this problem, most of them prioritize optimizing the fidelity in dancing movement, constrained by predetermined dancer counts in datasets. This limitation impedes adaptability to real-world applications. Our study addresses the scalability problem in group choreography while preserving naturalness and synchronization. In particular, we propose a phase-based variational generative model for group dance generation on learning a generative manifold. Our method achieves high-fidelity group dance motion and enables the generation with an unlimited number of dancers while consuming only a minimal and constant amount of memory. The intensive experiments on two public datasets show that our proposed method outperforms recent state-of-the-art approaches by a large margin and is scalable to a great number of dancers beyond the training data",
    "checked": true,
    "id": "8815c8230b8467481026a8e7a78f394764f89388",
    "semantic_title": "scalable group choreography via variational phase manifold learning",
    "citation_count": 0,
    "authors": [
      "Nhat Le",
      "Khoa Do",
      "Xuan Bui",
      "Tuong Do",
      "Erman Tjiputra",
      "Quang D.Tran",
      "Anh Nguyen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2781_ECCV_2024_paper.php": {
    "title": "Masked Video and Body-worn IMU Autoencoder for Egocentric Action Recognition",
    "volume": "main",
    "abstract": "Compared with visual signals, Inertial Measurement Units (IMUs) placed on human limbs can capture accurate motion signals while being robust to lighting variation and occlusion. While these characteristics are intuitively valuable to help egocentric action recognition, the potential of IMUs remains under-explored. In this work, we present a novel method for action recognition that integrates motion data from body-worn IMUs with egocentric video. Due to the scarcity of labeled multimodal data, we design an MAE-based self-supervised pretraining method, obtaining strong multi-modal representations via modeling the natural correlation between visual and motion signals. To model the complex relation of multiple IMU devices placed across the body, we exploit the collaborative dynamics in multiple IMU devices and propose to embed the relative motion features of human joints into a graph structure. Experiments show our method can achieve state-of-the-art performance on multiple public datasets. The effectiveness of our MAE-based pretraining and graph-based IMU modeling are further validated by experiments in more challenging scenarios, including partially missing IMU devices and video quality corruption, promoting more flexible usages in the real world",
    "checked": true,
    "id": "50ffed3127c3cf7ffbca12a77ba968bef43847f1",
    "semantic_title": "masked video and body-worn imu autoencoder for egocentric action recognition",
    "citation_count": 0,
    "authors": [
      "Mingfang Zhang",
      "Yifei Huang*",
      "Ruicong Liu",
      "Yoichi Sato"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2790_ECCV_2024_paper.php": {
    "title": "Mutual Learning for Acoustic Matching and Dereverberation via Visual Scene-driven Diffusion",
    "volume": "main",
    "abstract": "Visual acoustic matching (VAM) is pivotal for enhancing the immersive experience, and the task of dereverberation is effective in improving audio intelligibility. Existing methods treat each task independently, overlooking the inherent reciprocity between them. Moreover, these methods depend on paired training data, which is challenging to acquire, impeding the utilization of extensive unpaired data. In this paper, we introduce MVSD, a mutual learning framework based on diffusion models. MVSD considers the two tasks symmetrically, exploiting the reciprocal relationship to facilitate learning from inverse tasks and overcome data scarcity. Furthermore, we employ the diffusion model as foundational conditional converters to circumvent the training instability and over-smoothing drawbacks of conventional GAN architectures. Specifically, MVSD employs two converters: one for VAM called reverberator and one for dereverberation called dereverberator. The dereverberator judges whether the reverberation audio generated by reverberator sounds like being in the conditional visual scenario, and vice versa. By forming a closed loop, these two converters can generate informative feedback signals to optimize the inverse tasks, even with easily acquired one-way unpaired data. Extensive experiments on two standard benchmarks, , SoundSpaces-Speech and Acoustic AVSpeech, exhibit that our framework can improve the performance of the reverberator and dereverberator and better match specified visual scenarios",
    "checked": true,
    "id": "4660bf564938bdff3890d0511aac99b98c3db764",
    "semantic_title": "mutual learning for acoustic matching and dereverberation via visual scene-driven diffusion",
    "citation_count": 0,
    "authors": [
      "Jian Ma",
      "Wenguan Wang*",
      "Yi Yang",
      "Feng Zheng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2792_ECCV_2024_paper.php": {
    "title": "PoseSOR: Human Pose Can Guide Our Attention",
    "volume": "main",
    "abstract": "Salient Object Ranking (SOR) aims to study how human observers shift their attention among various objects within a scene. Previous works attempt to excavate explicit visual saliency cues, , spatial frequency and semantic context, to tackle this challenge. However, these visual saliency cues may fall short in handling real-world scenarios, which often involve various human activities and interactions. We observe that human observers' attention can be reflexively guided by the poses and gestures of the people in the scene, which indicate their activities. For example, observers tend to shift their attention to follow others' head orientation or running/walking direction to anticipate what will happen. Inspired by this observation, we propose to exploit human poses in understanding high-level interactions between human participants and their surroundings for robust salient object ranking. Specifically, we propose PoseSOR, a human pose-aware SOR model for the SOR task, with two novel modules: 1) a Pose-Aware Interaction (PAI) module to integrate human pose knowledge into salient object queries for learning high-level interactions, and 2) a Pose-Driven Ranking (PDR) module to apply pose knowledge as directional cues to help predict where human attention will shift to. To our knowledge, our approach is the first to explore human pose for salient object ranking. Extensive experiments demonstrate the effectiveness of our method, particularly in complex scenes, and our model sets the new state-of-the-art on the SOR benchmarks. Code and dataset are available at https://github.com/guanhuankang/ECCV24PoseSOR",
    "checked": true,
    "id": "2e0d9d96b0e9499a8cbf64b979764c4cb014c60d",
    "semantic_title": "posesor: human pose can guide our attention",
    "citation_count": 0,
    "authors": [
      "Huankang Guan",
      "Rynson W.H. Lau*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2800_ECCV_2024_paper.php": {
    "title": "TOD3Cap: Towards 3D Dense Captioning in Outdoor Scenes",
    "volume": "main",
    "abstract": "3D dense captioning stands as a cornerstone in achieving a comprehensive understanding of 3D scenes through natural language. It has recently witnessed remarkable achievements, particularly in indoor settings. However, the exploration of 3D dense captioning in outdoor scenes is hindered by two major challenges: 1) the domain gap between indoor and outdoor scenes, such as dynamics and sparse visual inputs, makes it difficult to adapt existing indoor methods directly; 2) the lack of data with comprehensive box-caption pair annotations specifically tailored for outdoor scenes. To this end, we introduce the new task of outdoor 3D dense captioning. As input, we assume a LiDAR point cloud and a set of RGB images captured by the panoramic camera rig. The expected output is a set of object boxes with captions. To tackle this task, we propose the T OD3 Cap network, which leverages the BEV representation to generate object box proposals and integrates Relation Q-Former with LLaMA-Adapter to generate rich captions for these objects. We also introduce the T OD3 Cap dataset, the first million-scale dataset to our knowledge for 3D dense captioning in outdoor scenes, which contains 2.3M descriptions of 64.3K outdoor objects from 850 scenes in nuScenes. Notably, our T OD3 Cap network can effectively localize and caption 3D objects in outdoor scenes, which outperforms baseline methods by a significant margin (+9.6 CiDEr@0.5IoU). Code, dataset and models are publicly available at https://github.com/jxbbb/TOD3Cap",
    "checked": true,
    "id": "c2b2cb0fb220d1cf64ca0942f3ffa637ca318116",
    "semantic_title": "tod3cap: towards 3d dense captioning in outdoor scenes",
    "citation_count": 5,
    "authors": [
      "Bu Jin",
      "Yupeng Zheng*",
      "Pengfei Li",
      "Weize Li",
      "Yuhang Zheng",
      "Sujie Hu",
      "Xinyu Liu",
      "Jinwei Zhu",
      "Zhijie Yan",
      "Haiyang Sun",
      "Kun Zhan",
      "Peng Jia",
      "Xiaoxiao Long",
      "Yilun Chen",
      "Hao Zhao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2804_ECCV_2024_paper.php": {
    "title": "Bi-directional Contextual Attention for 3D Dense Captioning",
    "volume": "main",
    "abstract": "3D dense captioning is a task involving the localization of objects and the generation of descriptions for each object in a 3D scene. Recent approaches have attempted to incorporate contextual information by modeling relationships with object pairs or aggregating the nearest neighbor features of an object. However, the contextual information constructed in these scenarios is limited in two aspects: first, objects have multiple positional relationships that exist across the entire global scene, not only near the object itself. Second, it faces with contradicting objectives–where localization and attribute descriptions are generated better with tight localization, while descriptions involving global positional relations are generated better with contextualized features of the global scene. To overcome this challenge, we introduce , a transformer encoder-decoder pipeline that engages in 3D dense captioning for each object with Bi-directional Contextual Attention. Leveraging parallelly decoded instance queries for objects and context queries for non-object contexts, BiCA generates object-aware contexts, where the contexts relevant to each object is summarized, and context-aware objects, where the objects relevant to the summarized object-aware contexts are aggregated. This extension relieves previous methods from the contradicting objectives, enhancing both localization performance and enabling the aggregation of contextual features throughout the global scene; thus improving caption generation performance simultaneously. Extensive experiments on two of the most widely-used 3D dense captioning datasets demonstrate that our proposed method achieves a significant improvement over prior methods",
    "checked": true,
    "id": "66f39b1ae23c74815bf5abf4d96d3f14995b9a63",
    "semantic_title": "bi-directional contextual attention for 3d dense captioning",
    "citation_count": 0,
    "authors": [
      "Minjung Kim*",
      "Hyung Suk Lim",
      "Soonyoung Lee",
      "Bumsoo Kim*",
      "Gunhee Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2808_ECCV_2024_paper.php": {
    "title": "Multi-Person Pose Forecasting with Individual Interaction Perceptron and Prior Learning",
    "volume": "main",
    "abstract": "Human Pose Forecasting is a major problem in human intention comprehension that can be addressed through learning the historical poses via deep methods. However, existing methods often lack the modeling of the person's role in the event in multi-person scenes. This leads to limited performance in complicated scenes with variant interactions happening at the same time. In this paper, we introduce the Interaction-Aware Pose Forecasting Transformer (IAFormer) framework to better learn the interaction features. With the key insight that the event often involves only part of the people in the scene, we designed the Interaction Perceptron Module (IPM) to evaluate the human-to-event interaction level. With the interaction evaluation, the human-independent features are extracted with the attention mechanism for interaction-aware forecasting. In addition, an Interaction Prior Learning Module (IPLM) is presented to learn and accumulate prior knowledge of high-frequency interactions, encouraging semantic pose forecasting rather than simple trajectory pose forecasting. We conduct experiments using datasets such as CMU-Mocap, UMPM, CHI3D, Human3.6M, and synthesized crowd datasets. The results demonstrate that our method significantly outperforms state-of-the-art approaches considering scenarios with varying numbers of people. Code is available at purplehttps: //github.com/ArcticPole/IAFormer",
    "checked": true,
    "id": "a23d4bc8e2763504a0f6041810229e268356be77",
    "semantic_title": "multi-person pose forecasting with individual interaction perceptron and prior learning",
    "citation_count": 1,
    "authors": [
      "Peng Xiao",
      "Yi Xie",
      "Xuemiao Xu*",
      "Weihong Chen",
      "Huaidong Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2810_ECCV_2024_paper.php": {
    "title": "InfMAE: A Foundation Model in The Infrared Modality",
    "volume": "main",
    "abstract": "In recent years, foundation models have swept the computer vision field, facilitating the advancement of various tasks within different modalities. However, effectively designing an infrared foundation model remains an open question. In this paper, we introduce InfMAE, a foundation model tailored specifically for the infrared modality. Initially, we present Inf30, an infrared dataset developed to mitigate the scarcity of large-scale data for self-supervised learning within the infrared vision community. Moreover, considering the intrinsic characteristics of infrared images, we design an information-aware masking strategy. It allows for a greater emphasis on the regions with richer information in infrared images during the self-supervised learning process, which is conducive to learning strong representations. Additionally, to enhance generalization capabilities in downstream tasks, we employ a multi-scale encoder for latent representation learning. Finally, we develop an infrared decoder to reconstruct images. Extensive experiments show that our proposed method InfMAE outperforms other supervised and self-supervised learning methods in three key downstream tasks: infrared image semantic segmentation, object detection, and small target detection",
    "checked": true,
    "id": "90374f57e0937c4c6e80abafdc4af749561ebbab",
    "semantic_title": "infmae: a foundation model in the infrared modality",
    "citation_count": 2,
    "authors": [
      "Fangcen Liu",
      "Chenqiang Gao*",
      "Yaming Zhang",
      "Junjie Guo",
      "Jinghao Wang",
      "Deyu Meng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2818_ECCV_2024_paper.php": {
    "title": "TPA3D: Triplane Attention for Fast Text-to-3D Generation",
    "volume": "main",
    "abstract": "Due to the lack of large-scale text-3D correspondence data, recent text-to-3D generation works mainly rely on utilizing 2D diffusion models for synthesizing 3D data. Since diffusion-based methods typically require significant optimization time for both training and inference, the use of GAN-based models would still be desirable for fast 3D generation. In this work, we propose Triplane Attention for text-guided 3D generation (TPA3D), an end-to-end trainable GAN-based deep learning model for fast text-to-3D generation. With only 3D shape data and their rendered 2D images observed during training, our TPA3D is designed to retrieve detailed visual descriptions for synthesizing the corresponding 3D mesh data. This is achieved by the proposed attention mechanisms on the extracted sentence and word-level text features. In our experiments, we show that TPA3D generates high-quality 3D textured shapes aligned with fine-grained descriptions, while impressive computation efficiency can be observed",
    "checked": true,
    "id": "0fd97918661df32752c1d5ec70531be8bc793d5e",
    "semantic_title": "tpa3d: triplane attention for fast text-to-3d generation",
    "citation_count": 0,
    "authors": [
      "Bin-Shih Wu*",
      "Hong-En Chen*",
      "Sheng-Yu Huang",
      "Yu-Chiang Frank Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2819_ECCV_2024_paper.php": {
    "title": "Multi-Memory Matching for Unsupervised Visible-Infrared Person Re-Identification",
    "volume": "main",
    "abstract": "Unsupervised visible-infrared person re-identification (USL-VI-ReID) is a promising yet highly challenging retrieval task. The key challenges in USL-VI-ReID are to accurately generate pseudo-labels and establish pseudo-label correspondences across modalities without relying on any prior annotations. Recently, clustered pseudo-label methods have gained more attention in USL-VI-ReID. However, most existing methods don't fully exploit the intra-class nuances, as they simply utilize a single memory that represents an identity to establish cross-modality correspondences, resulting in noisy cross-modality correspondences. To address the problem, we propose a Multi-Memory Matching (MMM) framework for USL-VI-ReID. We first design a simple yet effective Cross-Modality Clustering (CMC) module to generate the pseudo-labels through clustering together both two modality samples. To associate cross-modality clustered pseudo-labels, we design a Multi-Memory Learning and Matching (MMLM) module, ensuring that optimization explicitly focuses on the nuances of individual perspectives and establishes reliable cross-modality correspondences. Finally, we design a Soft Cluster-level Alignment (SCA) loss to narrow the modality gap while mitigating the effect of noisy pseudo-labels through a soft many-to-many alignment strategy. Extensive experiments on the public SYSU-MM01 and RegDB datasets demonstrate the reliability of the established cross-modality correspondences and the effectiveness of MMM",
    "checked": true,
    "id": "bcce530e61d04424d1d39e3dd258c6c0ff97e1f9",
    "semantic_title": "multi-memory matching for unsupervised visible-infrared person re-identification",
    "citation_count": 8,
    "authors": [
      "Jiangming Shi",
      "Xiangbo Yin",
      "Yeyun Chen",
      "Yachao Zhang",
      "Zhizhong Zhang",
      "Yuan Xie*",
      "Yanyun Qu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2821_ECCV_2024_paper.php": {
    "title": "LivePhoto: Real Image Animation with Text-guided Motion Control",
    "volume": "main",
    "abstract": "Despite the recent progress in text-to-video generation, existing studies usually overlook the issue that only spatial contents but not temporal motions in synthesized videos are under the control of text. Towards such a challenge, this work presents a practical system, named , which allows users to animate an image of their interest with text descriptions. We first establish a strong baseline that helps a well-learned text-to-image generator (i.e., Stable Diffusion) take an image as a further input. We then equip the improved generator with a motion module for temporal modeling and propose a carefully designed training pipeline to better link texts and motions. In particular, considering the facts that (1) text can only describe motions roughly (e.g., regardless of the moving speed) and (2) text may include both content and motion descriptions, we introduce a motion intensity estimation module as well as a text re-weighting module to reduce the ambiguity of text-to-motion mapping. Empirical evidence suggests that our approach is capable of well decoding motion-related textual instructions into videos, such as actions, camera movements, or even conjuring new contents from thin air (e.g., pouring water into an empty glass). Interestingly, thanks to the proposed intensity learning mechanism, our system offers users an additional control signal (i.e., the motion intensity) besides text for video customization. Project page is xavierchen34.github.io/LivePhoto-Page",
    "checked": true,
    "id": "f3f47c5fc916a244b501613fd0e6f338c196208b",
    "semantic_title": "livephoto: real image animation with text-guided motion control",
    "citation_count": 0,
    "authors": [
      "Xi Chen",
      "Zhiheng Liu",
      "Mengting Chen",
      "Yutong Feng",
      "Yu Liu",
      "Yujun Shen",
      "Hengshuang Zhao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2823_ECCV_2024_paper.php": {
    "title": "NeuSDFusion: A Spatial-Aware Generative Model for 3D Shape Completion, Reconstruction, and Generation",
    "volume": "main",
    "abstract": "3D shape generation aims to produce innovative 3D content adhering to specific conditions and constraints. Existing methods often decompose 3D shapes into a sequence of localized components, treating each element in isolation without considering spatial consistency. As a result, these approaches exhibit limited versatility in 3D data representation and shape generation, hindering their ability to generate highly diverse 3D shapes that comply with the specified constraints. In this paper, we introduce a novel spatial-aware 3D shape generation framework that leverages 2D plane representations for enhanced 3D shape modeling. To ensure spatial coherence and reduce memory usage, we incorporate a hybrid shape representation technique that directly learns a continuous signed distance field representation of the 3D shape using orthogonal 2D planes. Additionally, we meticulously enforce spatial correspondences across distinct planes using a transformer-based autoencoder structure, promoting the preservation of spatial relationships in the generated 3D shapes. This yields an algorithm that consistently outperforms state-of-the-art 3D shape generation methods on various tasks, including unconditional shape generation, multi-modal shape completion, single-view reconstruction, and text-to-shape synthesis. Our project page is available at https://weizheliu.github.io/NeuSDFusion/",
    "checked": true,
    "id": "9aca390813572256417c9ee08f824db02eef105d",
    "semantic_title": "neusdfusion: a spatial-aware generative model for 3d shape completion, reconstruction, and generation",
    "citation_count": 5,
    "authors": [
      "Ruikai Cui",
      "Weizhe Liu*",
      "Weixuan Sun",
      "Senbo Wang",
      "Taizhang Shang",
      "Yang Li",
      "Xibin Song",
      "Han Yan",
      "ZHENNAN WU",
      "Shenzhou Chen",
      "HONGDONG LI",
      "Pan Ji"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2824_ECCV_2024_paper.php": {
    "title": "AID-AppEAL: Automatic Image Dataset and Algorithm for Content Appeal Enhancement and Assessment Labeling",
    "volume": "main",
    "abstract": "We propose Image Content Appeal Assessment (), a novel metric that quantifies the level of positive interest an image's content generates for viewers, such as the appeal of food in a photograph. This is fundamentally different from traditional Image-Aesthetics Assessment (IAA), which judges an image's artistic quality. While previous studies often confuse the concepts of \"aesthetics\" and \"appeal,\" our work addresses this by being the first to study explicitly. To do this, we propose a novel system that automates dataset creation and implements algorithms to estimate and boost content appeal. We use our pipeline to generate two large-scale datasets (70K+ images each) in diverse domains (food and room interior design) to train our models, which revealed little correlation between content appeal and aesthetics. Our user study, with more than 76% of participants preferring the appeal-enhanced images, confirms that our appeal ratings accurately reflect user preferences, establishing as a unique evaluative criterion. Our code and datasets are available at https://github.com/SherryXTChen/AID-Appeal",
    "checked": true,
    "id": "6eaef722536d8e69ca5918ba66354a1d14c33139",
    "semantic_title": "aid-appeal: automatic image dataset and algorithm for content appeal enhancement and assessment labeling",
    "citation_count": 0,
    "authors": [
      "Sherry X. Chen*",
      "Yaron Vaxman",
      "Elad Ben Baruch",
      "David Asulin",
      "Aviad Moreshet",
      "Misha Sra",
      "Pradeep Sen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2829_ECCV_2024_paper.php": {
    "title": "SEDiff: Structure Extraction for Domain Adaptive Depth Estimation via Denoising Diffusion Models",
    "volume": "main",
    "abstract": "In monocular depth estimation, it is challenging to acquire a large amount of depth-annotated training data, which leads to a reliance on synthetic datasets. However, the inherent discrepancies between the synthetic environment and the real-world result in a domain shift and sub-optimal performance. In this paper, we introduce SEDiff which firstly leverages a diffusion-based generative model to extract essential structural information for accurate depth estimation. SEDiff wipes out the domain-specific components in the synthetic data and enables structural-consistent style transfer to mitigate the performance degradation due to the domain gap. Extensive experiments demonstrate the superiority of SEDiff over state-of-the-art methods in various scenarios for domain-adaptive depth estimation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongseok Shim*",
      "Hyoun Jin Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2833_ECCV_2024_paper.php": {
    "title": "Quantized Prompt for Efficient Generalization of Vision-Language Models",
    "volume": "main",
    "abstract": "In the past few years, large-scale pre-trained vision-language models like CLIP have achieved tremendous success in various fields. Naturally, how to transfer the rich knowledge in such huge pre-trained models to downstream tasks and datasets becomes a hot topic. During downstream adaptation, the most challenging problems are overfitting and catastrophic forgetting, which can cause the model to overly focus on the current data and lose more crucial domain-general knowledge. Existing works use classic regularization techniques to solve the problems. As solutions become increasingly complex, the ever-growing storage and inference costs are also a significant problem that urgently needs to be addressed. While in this paper, we start from an observation that proper random noise can suppress overfitting and catastrophic forgetting. Then we regard quantization error as a kind of noise, and explore quantization for regularizing vision-language model, which is quite efficiency and effective. Furthermore, to improve the model's generalization capability while maintaining its specialization capacity at minimal cost, we deeply analyze the characteristics of the weight distribution in prompts, conclude several principles for quantization module design and follow such principles to create several competitive baselines. The proposed method is significantly efficient due to its inherent lightweight nature, making it possible to adapt on extremely resource-limited devices. Our method can be fruitfully integrated into many existing approaches like MaPLe, enhancing accuracy while reducing storage overhead, making it more powerful yet versatile. Extensive experiments on 11 datasets shows great superiority of our method sufficiently. Code is available at github",
    "checked": true,
    "id": "7112d579583ce8c82acfc88cbfde2698df2925e5",
    "semantic_title": "quantized prompt for efficient generalization of vision-language models",
    "citation_count": 3,
    "authors": [
      "Tianxiang Hao",
      "Xiaohan Ding*",
      "Juexiao Feng",
      "Yuhong Yang",
      "Hui Chen",
      "Guiguang Ding*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2834_ECCV_2024_paper.php": {
    "title": "Online Temporal Action Localization with Memory-Augmented Transformer",
    "volume": "main",
    "abstract": "Online temporal action localization (On-TAL) is the task of identifying multiple action instances given a streaming video. Since existing methods take as input only a video segment of fixed size per iteration, they are limited in considering long-term context and require tuning the segment size carefully. To overcome these limitations, we propose memory-augmented transformer (MATR). MATR utilizes the memory queue that selectively preserves the past segment features, allowing to leverage long-term context for inference. We also propose a novel action localization method that observes the current input segment to predict the end time of the ongoing action and accesses the memory queue to estimate the start time of the action. Our method outperformed existing methods on two datasets, THUMOS14 and MUSES, surpassing not only TAL methods in the online setting but also some offline TAL methods",
    "checked": true,
    "id": "6ea7346ad9d43c88a6e3e8ade97a957b180d5f17",
    "semantic_title": "online temporal action localization with memory-augmented transformer",
    "citation_count": 0,
    "authors": [
      "Youngkil Song",
      "Dongkeun Kim",
      "Minsu Cho",
      "Suha Kwak*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2840_ECCV_2024_paper.php": {
    "title": "Efficient Cascaded Multiscale Adaptive Network for Image Restoration",
    "volume": "main",
    "abstract": "Image restoration, encompassing tasks such as deblurring, denoising, and super-resolution, remains a pivotal area in computer vision. However, efficiently addressing the spatially varying artifacts of various low-quality images with local adaptiveness and handling their degradations at different scales poses significant challenges. To efficiently tackle these issues, we propose the novel Efficient Cascaded Multiscale Adaptive (ECMA) Network. ECMA employs Local Adaptive Module, LAM, which dynamically adjusts convolution kernels across local image regions to efficiently handle varying artifacts. Thus, LAM addresses the local adaptiveness challenge more efficiently than costlier mechanisms like self-attention, due to its less computationally intensive convolutions. To construct a basic ECMA block, three cascading LAMs with convolution kernels from large to small sizes are employed to capture features at different scales. This cascaded multiscale learning effectively handles degradations at different scales, critical for diverse image restoration tasks. Finally, ECMA blocks are stacked in a U-Net architecture to build ECMA networks, which efficiently achieve both local adaptiveness and multiscale processing. Experiments show ECMA's high performance and efficiency, achieving comparable or superior restoration performance to state-of-the-art methods while reducing computational costs by 1.2× to 9.7× across various image restoration tasks, e.g., image deblurring, denoising and super-resolution",
    "checked": false,
    "id": "4a6570f1732c7d6ac3cd6118de8d8d939c8efdb6",
    "semantic_title": "drm-ir: task-adaptive deep unfolding network for all-in-one image restoration",
    "citation_count": 3,
    "authors": [
      "Yichen Zhou*",
      "Pan Zhou*",
      "Teck Khim Ng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2842_ECCV_2024_paper.php": {
    "title": "MOFA-Video: Controllable Image Animation via Generative Motion Field Adaptions in Frozen Image-to-Video Diffusion Model",
    "volume": "main",
    "abstract": "We present MOFA-Video, an advanced controllable image animation method that generates video from the given image using various additional controllable signals (such as human landmarks reference, manual trajectories, and another even provided video) or their combinations. This is different from previous methods which only can work on a specific motion domain or show weak control abilities with diffusion prior. To achieve our goal, we design several domain-aware motion field adapters (, MOFA-Adapters) to control the generated motions in the video generation pipeline. For MOFA-Adapters, we consider the temporal motion consistency of the video and generate the dense motion flow from the given sparse control conditions first, and then, the multi-scale features of the given image are wrapped as a guided feature for stable video diffusion generation. We naively train two motion adapters for the manual trajectories and the human landmarks individually since they both contain sparse information about the control. After training, the MOFA-Adapters in different domains can also work together for more controllable video generation. Codes available: https: //github.com/MyNiuuu/MOFA-Video",
    "checked": true,
    "id": "f0303b7de76c9e7747fa373f6cb60c889e03e4f9",
    "semantic_title": "mofa-video: controllable image animation via generative motion field adaptions in frozen image-to-video diffusion model",
    "citation_count": 5,
    "authors": [
      "Muyao Niu",
      "Xiaodong Cun*",
      "Xintao Wang",
      "Yong Zhang",
      "Ying Shan",
      "Yinqiang Zheng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2865_ECCV_2024_paper.php": {
    "title": "Occlusion-Aware Seamless Segmentation",
    "volume": "main",
    "abstract": "Panoramic images can broaden the Field of View (FoV), occlusion-aware prediction can deepen the understanding of the scene, and domain adaptation can transfer across viewing domains. In this work, we introduce a novel task, Occlusion-Aware Seamless Segmentation (OASS), which simultaneously tackles all these three challenges. For benchmarking OASS, we establish a new human-annotated dataset for Blending Panoramic Amodal Seamless Segmentation, , BlendPASS. Besides, we propose the first solution UnmaskFormer, aiming at unmasking the narrow FoV, occlusions, and domain gaps all at once. Specifically, UnmaskFormer includes the crucial designs of Unmasking Attention (UA) and Amodal-oriented Mix (AoMix). Our method achieves state-of-the-art performance on the BlendPASS dataset, reaching a remarkable mAPQ of 26.58% and mIoU of 43.66%. On public panoramic semantic segmentation datasets, , SynPASS and DensePASS, our method outperforms previous methods and obtains 45.34% and 48.08% in mIoU, respectively. The fresh BlendPASS dataset and our source code are available at https://github.com/yihong-97/OASS",
    "checked": true,
    "id": "5679556570e9645f9416f50a7bc066d956899b0c",
    "semantic_title": "occlusion-aware seamless segmentation",
    "citation_count": 0,
    "authors": [
      "Yihong Cao",
      "Jiaming Zhang",
      "Hao Shi",
      "Kunyu Peng",
      "Yuhongxuan Zhang",
      "Hui Zhang*",
      "Rainer Stiefelhagen",
      "Kailun Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2866_ECCV_2024_paper.php": {
    "title": "OpenKD: Opening Prompt Diversity for Zero- and Few-shot Keypoint Detection",
    "volume": "main",
    "abstract": "Exploiting foundation models (, CLIP) to build a versatile keypoint detector has gained increasing attention. Most existing models accept either the text prompt (, \"the nose of a cat\"), or the visual prompt (, support image with keypoint annotations), to detect the corresponding keypoints in query image, thereby, exhibiting either zero-shot or few-shot detection ability. However, the research on multimodal prompting is still underexplored, and the prompt diversity in semantics and language is far from opened. For example, how to handle unseen text prompts for novel keypoint detection and the diverse text prompts like \"Can you detect the nose and ears of a cat?\" In this work, we open the prompt diversity in three aspects: modality, semantics (seen unseen), and language, to enable a more general zero- and few-shot keypoint detection (Z-FSKD). We propose a novel OpenKD model which leverages a multimodal prototype set to support both visual and textual prompting. Further, to infer the keypoint location of unseen texts, we add the auxiliary keypoints and texts interpolated in visual and textual domains into training, which improves the spatial reasoning of our model and significantly enhances zero-shot novel keypoint detection. We also find large language model (LLM) is a good parser, which achieves over 96% accuracy when parsing keypoints from texts. With LLM, OpenKD can handle diverse text prompts. Experimental results show that our method achieves state-of-the-art performance on Z-FSKD and initiates new ways of dealing with unseen text and diverse texts. The source code and data are available at https://github.com/AlanLuSun/OpenKD. Recently the (visual) prompt based keypoint detection has attracted the research interest in community as it provides more general detection paradigm compared to traditional close-set keypoint detection. Existing prompts include the textual prompt, visual prompt, or the both. However, the diversity of text prompt is quite limited in semantics and only using stereotype templates, which severely hinders the real-world application. In this work, we are further opening the prompt diversity, ranging from the easy text prompt to hard and unseen text prompt, pushing towards a more general keypoint detection by transferring the knowledge of large-scale pre-trained models such as vision-language model CLIP and large-language model Vicuna/Llama 2. Specifically, we propose a novel OpenKD model which consists of lexical/text parsing module, dual-modal prompting mechanism, dual-contrastive loss for signal alignment and knowledge transfer, and others. To test the model efficacy, we construct diverse text prompt sets for existing keypoint detection datasets. Our model not only supports both visual or textual modality prompting, but also has the capability to infer keypoint locations of unseen text prompts, realizing the first zero-shot novel keypoint detection. The experiments highlight the effectiveness of the proposed approach",
    "checked": true,
    "id": "b135510dae495908812e6ed422553d097e5ab443",
    "semantic_title": "openkd: opening prompt diversity for zero- and few-shot keypoint detection",
    "citation_count": 0,
    "authors": [
      "Changsheng Lu*",
      "Zheyuan Liu",
      "Piotr Koniusz*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2873_ECCV_2024_paper.php": {
    "title": "Referring Atomic Video Action Recognition",
    "volume": "main",
    "abstract": "We introduce a new task called Referring Atomic Video Action Recognition (RAVAR), aimed at identifying atomic actions of a particular person based on a textual description and the video data of this person. This task differs from traditional action recognition and localization, where predictions are delivered for all present individuals. In contrast, we focus on recognizing the correct atomic action of a specific individual, guided by text. To explore this task, we present the RefAVA dataset, containing 36, 630 instances with manually annotated textual descriptions of the individuals. To establish a strong initial benchmark, we implement and validate baselines from various domains, e.g., atomic action localization, video question answering, and text-video retrieval. Since these existing methods underperform on RAVAR, we introduce RefAtomNet – a novel cross-stream attention-driven method specialized for the unique challenges of RAVAR: the need to interpret a textual referring expression for the targeted individual, utilize this reference to guide the spatial localization and harvest the prediction of the atomic actions for the referring person. The key ingredients are: (1) a multi-stream architecture that connects video, text, and a new location-semantic stream, and (2) cross-stream agent attention fusion and agent token fusion which amplify the most relevant information across these streams and consistently surpasses standard attention-based fusion on RAVAR. Extensive experiments demonstrate the effectiveness of RefAtomNet and its building blocks for recognizing the action of the described individual. The dataset and code will be made publicly available at RAVAR",
    "checked": true,
    "id": "f3ab0b44a71fbe8f10e715d2a1a219f6513f26c6",
    "semantic_title": "referring atomic video action recognition",
    "citation_count": 1,
    "authors": [
      "Kunyu Peng*",
      "Jia Fu",
      "Kailun Yang",
      "Di Wen",
      "Yufan Chen",
      "Ruiping Liu",
      "Junwei Zheng",
      "Jiaming Zhang",
      "Saquib Sarfraz",
      "Rainer Stiefelhagen",
      "Alina Roitberg"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2877_ECCV_2024_paper.php": {
    "title": "Agent3D-Zero: An Agent for Zero-shot 3D Understanding",
    "volume": "main",
    "abstract": "The ability to understand and reason the 3D real world is a crucial milestone towards artificial general intelligence. The current common practice is to finetune Large Language Models (LLMs) with 3D data and texts to enable 3D understanding. Despite their effectiveness, these approaches are inherently limited by the scale and diversity of the available 3D data. Alternatively, in this work, we introduce , an innovative 3D-aware agent framework addressing the 3D scene understanding in a zero-shot manner. The essence of our approach centers on reconceptualizing the challenge of 3D scene perception as a process of understanding and synthesizing insights from multiple images, inspired by how our human beings attempt to understand 3D scenes. By consolidating this idea, we propose a novel way to make use of a Large Visual Language Model (VLM) via actively selecting and analyzing a series of viewpoints for 3D understanding. Specifically, given an input 3D scene, first processes a bird's-eye view image with custom-designed visual prompts, then iteratively chooses the next viewpoints to observe and summarize the underlying knowledge. A distinctive advantage of is the introduction of novel visual prompts, which significantly unleash the VLMs' ability to identify the most informative viewpoints and thus facilitate observing 3D scenes. Extensive experiments demonstrate the effectiveness of the proposed framework in understanding diverse and previously unseen 3D environments. project page",
    "checked": true,
    "id": "3f117b5a442d488a923ca44b0ef7b62c7d6b74c3",
    "semantic_title": "agent3d-zero: an agent for zero-shot 3d understanding",
    "citation_count": 4,
    "authors": [
      "sha zhang",
      "Di Huang",
      "Jiajun Deng*",
      "Shixiang Tang",
      "Wanli Ouyang",
      "Tong He*",
      "Yanyong Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2879_ECCV_2024_paper.php": {
    "title": "Stream Query Denoising for Vectorized HD-Map Construction",
    "volume": "main",
    "abstract": "This paper introduces the Stream Query Denoising (SQD) strategy, a novel and general approach for high-definition map (HD-map) construction. SQD is designed to improve the modeling capability of map elements by learning temporal consistency. Specifically, SQD involves the process of denoising the queries, which are generated by the noised ground truth of the previous frame. This process aims to reconstruct the ground truth of the current frame during training. Our method can be applied to both static and temporal methods, showing the great effectiveness of SQD strategy. Extensive experiments on nuScenes and Argoverse2 show that our framework achieves superior performance, compared to other existing methods across all settings. Code will be available here",
    "checked": false,
    "id": "6c2f3ada43d204344e56644269b6370837f98849",
    "semantic_title": "stream query denoising for vectorized hd map construction",
    "citation_count": 8,
    "authors": [
      "Shuo Wang*",
      "Fan Jia",
      "Weixin Mao",
      "Yingfei Liu",
      "Yucheng Zhao",
      "Zehui Chen",
      "Tiancai Wang",
      "Chi Zhang",
      "Xiangyu Zhang",
      "Feng Zhao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2887_ECCV_2024_paper.php": {
    "title": "SAGS: Structure-Aware 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "Following the advent of NeRFs, 3D Gaussian Splatting (3D-GS) has paved the way to real-time neural rendering overcoming the computational burden of volumetric methods. Several extensions of 3D-GS have been proposed to achieve compressible and high-fidelity performance. However, by employing a geometry-agnostic optimization scheme, these methods neglect the inherent 3D structure of the scene, thereby restricting the expressivity and the quality of the representation, resulting in various floating points and artifacts. In this work, we propose a structure-aware Gaussian Splatting method (SAGS) that implicitly encodes the geometry of the scene, which reflects to state-of-the-art rendering performance and reduced storage requirements on benchmark datasets. SAGS is founded on a local-global graph representation that facilitates the learning of complex scenes and enforces meaningful point displacements that preserve the scene's geometry. Additionally, we introduce a lightweight version of SAGS, using a simple yet effective mid-point interpolation scheme, which showcases a compact representation of the scene with up to 24× size reduction without the reliance on any compression strategies. Extensive experiments across multiple benchmark datasets demonstrate the superiority of SAGS compared to state-of-the-art 3D-GS methods under both rendering quality and model size. Besides, we demonstrate that our structure-aware method can effectively mitigate floating artifacts and irregular distortions of previous methods while obtaining precise depth maps",
    "checked": true,
    "id": "f57591b7cafe491ca2f851544d08809ce320a941",
    "semantic_title": "sags: structure-aware 3d gaussian splatting",
    "citation_count": 6,
    "authors": [
      "Evangelos Ververas",
      "Rolandos Alexandros Potamias*",
      "Jifei Song",
      "Jiankang Deng",
      "Stefanos Zafeiriou"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2904_ECCV_2024_paper.php": {
    "title": "Spherical Linear Interpolation and Text-Anchoring for Zero-shot Composed Image Retrieval",
    "volume": "main",
    "abstract": "Composed Image Retrieval (CIR) is a complex task that retrieves images using a query, which is configured with an image and a caption that describes desired modifications to that image. Supervised CIR approaches have shown strong performance, but their reliance on expensive manually-annotated datasets restricts their scalability and broader applicability. To address these issues, previous studies have proposed pseudo-word token-based Zero-Shot CIR (ZS-CIR) methods, which utilize a projection module to map images to word tokens. However, we conjecture that this approach has a downside: the projection module distorts the original image representation and confines the resulting composed embeddings to the text-side. In order to resolve this, we introduce a novel ZS-CIR method that uses Spherical Linear Interpolation (Slerp) to directly merge image and text representations by identifying an intermediate embedding of both. Furthermore, we introduce Text-Anchored-Tuning (TAT), a method that fine-tunes the image encoder while keeping the text encoder fixed. TAT closes the modality gap between images and text, making the Slerp process much more effective. Notably, the TAT method is not only efficient in terms of the scale of the training dataset and training time, but it also serves as an excellent initial checkpoint for training supervised CIR models, thereby highlighting its wider potential. The integration of the Slerp-based ZS-CIR with a TAT-tuned model enables our approach to deliver state-of-the-art retrieval performance across CIR benchmarks. Code is available at https://github.com/youngkyunJang/SLERP-TAT",
    "checked": true,
    "id": "11944c9e7b5668f0e50f2815006c6b076a968550",
    "semantic_title": "spherical linear interpolation and text-anchoring for zero-shot composed image retrieval",
    "citation_count": 4,
    "authors": [
      "Young Kyun Jang*",
      "Dat B Huynh",
      "Ashish Shah",
      "Wen-Kai Chen",
      "Ser-Nam Lim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2906_ECCV_2024_paper.php": {
    "title": "OneRestore: A Universal Restoration Framework for Composite Degradation",
    "volume": "main",
    "abstract": "In real-world scenarios, image impairments often manifest as composite degradations, presenting a complex interplay of elements such as low light, haze, rain, and snow. Despite this reality, existing restoration methods typically target isolated degradation types, thereby falling short in environments where multiple degrading factors coexist. To bridge this gap, our study proposes a versatile imaging model that consolidates four physical corruption paradigms to accurately represent complex, composite degradation scenarios. In this context, we propose OneRestore, a novel transformer-based framework designed for adaptive, controllable scene restoration. The proposed framework leverages a unique cross-attention mechanism, merging degraded scene descriptors with image features, allowing for nuanced restoration. Our model allows versatile input scene descriptors, ranging from manual text embeddings to automatic extractions based on visual attributes. Our methodology is further enhanced through a composite degradation restoration loss, using extra degraded images as negative samples to fortify model constraints. Comparative results on synthetic and real-world datasets demonstrate OneRestore as a superior solution, significantly advancing the state-of-the-art in addressing complex, composite degradations",
    "checked": true,
    "id": "7da6f70ecc09a2ac615d475d82e2a1d747428403",
    "semantic_title": "onerestore: a universal restoration framework for composite degradation",
    "citation_count": 1,
    "authors": [
      "Yu Guo*",
      "Yuan Gao",
      "Yuxu Lu",
      "Huilin Zhu",
      "Wen Liu",
      "Shengfeng He"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2909_ECCV_2024_paper.php": {
    "title": "Beat-It: Beat-Synchronized Multi-Condition 3D Dance Generation",
    "volume": "main",
    "abstract": "Dance, as an art form, fundamentally hinges on the precise synchronization with musical beats. However, achieving aesthetically pleasing dance sequences from music is challenging, with existing methods often falling short in controllability and beat alignment. To address these shortcomings, this paper introduces Beat-It, a novel framework for beat-specific, key pose-guided dance generation. Unlike prior approaches, Beat-It uniquely integrates explicit beat awareness and key pose guidance, effectively resolving two main issues: the misalignment of generated dance motions with musical beats, and the inability to map key poses to specific beats, critical for practical choreography. Our approach disentangles beat conditions from music using a nearest beat distance representation and employs a hierarchical multi-condition fusion mechanism. This mechanism seamlessly integrates key poses, beats, and music features, mitigating condition conflicts and offering rich, multi-conditioned guidance for dance generation. Additionally, a specially designed beat alignment loss ensures the generated dance movements remain in sync with the designated beats. Extensive experiments confirm Beat-It's superiority over existing state-of-the-art methods in terms of beat alignment and motion controllability",
    "checked": true,
    "id": "aceb4eb743c0e21c9c177092eceef8dfb3079ae6",
    "semantic_title": "beat-it: beat-synchronized multi-condition 3d dance generation",
    "citation_count": 0,
    "authors": [
      "Zikai Huang",
      "Xuemiao Xu*",
      "Cheng Xu*",
      "Huaidong Zhang",
      "Chenxi Zheng",
      "Jing Qin",
      "Shengfeng He"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2910_ECCV_2024_paper.php": {
    "title": "SkyMask: Attack-agnostic Robust Federated Learning with Fine-grained Learnable Masks",
    "volume": "main",
    "abstract": "Federated Learning (FL) is becoming a popular paradigm for leveraging distributed data and preserving data privacy. However, due to the distributed characteristic, FL systems are vulnerable to Byzantine attacks that compromised clients attack the global model by uploading malicious model updates. With the development of layer-level and parameter-level fine-grained attacks, the attacks' stealthiness and effectiveness have been significantly improved. The existing defense mechanisms solely analyze the model-level statistics of individual model updates uploaded by clients to mitigate Byzantine attacks, which are ineffective against fine-grained attacks due to unawareness or overreaction. To address this problem, we propose SkyMask, a new attack-agnostic robust FL system that firstly leverages fine-grained learnable masks to identify malicious model updates at the parameter level. Specifically, the FL server freezes and multiplies the model updates uploaded by clients with the parameter-level masks, and trains the masks over a small clean dataset (i.e., root dataset) to learn the subtle difference between benign and malicious model updates in a high-dimension space. Our extensive experiments involve different models on three public datasets under state-of-the-art (SOTA) attacks, where the results show that achieves up to 14% higher testing accuracy compared with SOTA defense strategies under the same attacks and successfully defends against attacks with malicious clients of a high fraction up to 80%. Code is available at https://github.com/ KoalaYan/SkyMask",
    "checked": true,
    "id": "76cae8e9ea45a5d6f5472a3cdc945c7c6c580c51",
    "semantic_title": "skymask: attack-agnostic robust federated learning with fine-grained learnable masks",
    "citation_count": 1,
    "authors": [
      "Peishen Yan",
      "Hao Wang",
      "Tao Song*",
      "Yang Hua",
      "Ruhui Ma",
      "Ningxin Hu",
      "Mohammad Reza Haghighat",
      "Haibing Guan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2925_ECCV_2024_paper.php": {
    "title": "RePOSE: 3D Human Pose Estimation via Spatio-Temporal Depth Relational Consistency",
    "volume": "main",
    "abstract": "We introduce RePOSE, a simple yet effective approach for addressing occlusion challenges in the learning of 3D human pose estimation (HPE) from videos. Conventional approaches typically employ absolute depth signals as supervision, which are adept at discernible keypoints but become less reliable when keypoints are occluded, resulting in vague and inconsistent learning trajectories for the neural network. RePOSE overcomes this limitation by introducing spatio-temporal relational depth consistency into the supervision signals. The core rationale of our method lies in prioritizing the precise sequencing of occluded keypoints. This is achieved by using a relative depth consistency loss that operates in both spatial and temporal domains. By doing so, RePOSE shifts the focus from learning absolute depth values, which can be misleading in occluded scenarios, to relative positioning, which provides a more robust and reliable cue for accurate pose estimation. This subtle yet crucial shift facilitates more consistent and accurate 3D HPE under occlusion conditions. The elegance of our core idea lies in its simplicity and ease of implementation, requiring only a few lines of code. Extensive experiments validate that RePOSE not only outperforms existing state-of-the-art methods but also significantly enhances the robustness and precision of 3D HPE in challenging occluded environments",
    "checked": false,
    "id": "64bb6de103e3f7dc1dd12441610adc0932a3eed0",
    "semantic_title": "from human pose similarity metric to 3d human pose estimator: temporal propagating lstm networks",
    "citation_count": 10,
    "authors": [
      "Ziming Sun",
      "Yuan Liang",
      "Zejun Ma",
      "Tianle Zhang",
      "Linchao Bao",
      "Guiqing Li",
      "Shengfeng He*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2926_ECCV_2024_paper.php": {
    "title": "Pixel-GS Density Control with Pixel-aware Gradient for 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis results and advancing real-time rendering performance. However, the effectiveness of 3DGS heavily relies on the quality of the initial point cloud, as poor initialization can result in blurring and needle-like artifacts. This issue is mainly due to the point cloud growth condition, which only considers the average gradient magnitude of points from observable views, thereby failing to grow for large Gaussians that are observable from many viewpoints while many of them are only covered in the boundaries. To address this, we introduce Pixel-GS to take the area covered by the Gaussian in each view into account during the computation of the growth condition. The covered area is employed to adaptively weigh the gradients from different views, thereby facilitating the growth of large Gaussians. Consequently, Gaussians within the regions with insufficient initializing points can grow more effectively, leading to a more accurate and detailed reconstruction. Besides, we propose a simple yet effective strategy to suppress floaters near the camera by scaling the gradient field according to the distance to the camera. Extensive qualitative and quantitative experiments validate that our method achieves state-of-the-art rendering quality while maintaining real-time rendering, on challenging datasets such as Mip-NeRF 360 and Tanks & Temples. Code and demo are available at: https://pixelgs.github.io",
    "checked": false,
    "id": "42751309b000f38ab1553f13333fa99e121e9ded",
    "semantic_title": "pixel-gs: density control with pixel-aware gradient for 3d gaussian splatting",
    "citation_count": 13,
    "authors": [
      "Zheng Zhang",
      "Wenbo Hu*",
      "Yixing Lao",
      "Tong He",
      "Hengshuang Zhao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2927_ECCV_2024_paper.php": {
    "title": "WorldPose: A World Cup Dataset for Global 3D Human Pose Estimation",
    "volume": "main",
    "abstract": "We present , a novel dataset for advancing research in multi-person global pose estimation in the wild, featuring footage from the 2022 FIFA World Cup. While previous datasets have primarily focused on local poses, often limited to a single person or in constrained, indoor settings, the infrastructure deployed for this sporting event allows access to multiple fixed and moving cameras in different stadiums. We exploit the static multi-view setup of HD cameras to recover the 3D player poses and motions with unprecedented accuracy given capture areas of more than 1.75 acres (7k m2 ). We then leverage the captured players' motions and field markings to calibrate a moving broadcasting camera. The resulting dataset comprises 88 sequences with more than 2.5 million 3D poses and a total traveling distance of over 120 km. Subsequently, we conduct an in-depth analysis of the SOTA methods for global pose estimation. Our experiments demonstrate that challenges existing multi-person techniques, supporting the potential for new research in this area and others, such as sports analysis. All pose annotations (in SMPL format), broadcasting camera parameters and footage will be released for academic research purposes",
    "checked": true,
    "id": "b64093f0a083d79a90618f760f4aee3e6d1f9539",
    "semantic_title": "worldpose: a world cup dataset for global 3d human pose estimation",
    "citation_count": 0,
    "authors": [
      "Tianjian Jiang*",
      "Johsan Billingham",
      "Sebastian Müksch",
      "Juan J Zarate",
      "Nicolas Evans",
      "Martin R. Oswald",
      "Marc Pollefeys",
      "Otmar Hilliges",
      "Manuel Kaufmann",
      "Jie Song"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2933_ECCV_2024_paper.php": {
    "title": "Language-Driven 6-DoF Grasp Detection Using Negative Prompt Guidance",
    "volume": "main",
    "abstract": "6-DoF grasp detection has been a fundamental and challenging problem in robotic vision. While previous works have focused on ensuring grasp stability, they often do not consider human intention conveyed through natural language, hindering effective collaboration between robots and users in complex 3D environments. In this paper, we present a new approach for language-driven 6-DoF grasp detection in cluttered point clouds. We first introduce Grasp-Anything-6D, a large-scale dataset for the language-driven 6-DoF grasp detection task with 1M point cloud scenes and more than 200M language-associated 3D grasp poses. We further introduce a novel diffusion model that incorporates a new negative prompt guidance learning strategy. The proposed negative prompt strategy directs the detection process toward the desired object while steering away from unwanted ones given the language input. Our method enables an end-to-end framework where humans can command the robot to grasp desired objects in a cluttered scene using natural language. Intensive experimental results show the effectiveness of our method in both benchmarking experiments and real-world scenarios, surpassing other baselines. In addition, we demonstrate the practicality of our approach in real-world robotic applications. Our project is available at https://airvlab.github.io/grasp-anything/",
    "checked": true,
    "id": "46809dd156471fc156be503ef16fb055c2eacaf0",
    "semantic_title": "language-driven 6-dof grasp detection using negative prompt guidance",
    "citation_count": 4,
    "authors": [
      "Toan Nguyen",
      "Minh Nhat Nhat Vu",
      "Baoru Huang",
      "An Dinh Vuong",
      "Quan Vuong",
      "Ngan Le",
      "Thieu Vo",
      "Anh Nguyen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2943_ECCV_2024_paper.php": {
    "title": "COIN-Matting: Confounder Intervention for Image Matting",
    "volume": "main",
    "abstract": "Deep learning methods have significantly advanced the performance of image matting. However, dataset biases can mislead the matting models to biased behavior. In this paper, we identify the two typical biases in existing matting models, specifically contrast bias and transparency bias, and discuss their origins in matting datasets. To address these biases, we model the image matting task from the perspective of causal inference and identify the root causes of these biases: the confounders. To mitigate the effects of these confounders, we employ causal intervention through backdoor adjustment and introduce a novel model-agnostic cofounder intervened (COIN) matting framework. Extensive experiments across various matting methods and datasets have demonstrated that our COIN framework can significantly diminish such biases, thereby enhancing the performance of existing matting models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaohe Liao",
      "Jiangtong Li",
      "Jun Lan",
      "Huijia Zhu",
      "Weiqiang Wang",
      "Li Niu*",
      "Liqing Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2948_ECCV_2024_paper.php": {
    "title": "SHINE: Saliency-aware HIerarchical NEgative Ranking for Compositional Temporal Grounding",
    "volume": "main",
    "abstract": "Temporal grounding, also known as video moment retrieval, aims at locating video segments corresponding to a given query sentence. The compositional nature of natural language enables the localization beyond predefined events, posing a certain challenge to the compositional generalizability of existing methods. Recent studies establish the correspondence between videos and queries through a decompose-reconstruct manner to achieve compositional generalization. However, they only consider dominant primitives and build negative queries through random sampling and recombination, resulting in semantically implausible negatives that hinder the models from learning rational compositions. In addition, recent DETR-based methods still underperform in compositional temporal grounding, showing irrational saliency responses when given negative queries that have subtle differences from positive queries. To address these limitations, we first propose a large language model-driven method for negative query construction, utilizing GPT-3.5 Turbo to generate semantically plausible hard negative queries. Subsequently, we introduce a coarse-to-fine saliency ranking strategy, which encourages the model to learn the multi-granularity semantic relationships between videos and hierarchical negative queries to boost compositional generalization. Extensive experiments on two challenging benchmarks validate the effectiveness and generalizability of our proposed method. Our code is available at https://github.com/zxccade/SHINE",
    "checked": true,
    "id": "fd10f02fd67b14c1a2acae9b3435e41d838143e0",
    "semantic_title": "shine: saliency-aware hierarchical negative ranking for compositional temporal grounding",
    "citation_count": 0,
    "authors": [
      "Zixu Cheng*",
      "Yujiang Pu*",
      "Shaogang Gong",
      "Parisa Kordjamshidi",
      "Yu Kong"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2950_ECCV_2024_paper.php": {
    "title": "Audio-driven Talking Face Generation with Stabilized Synchronization Loss",
    "volume": "main",
    "abstract": "Talking face generation aims to create realistic videos with accurate lip synchronization and high visual quality, using given audio and reference video while preserving identity and visual characteristics. In this paper, we start by identifying several issues with existing synchronization learning methods. These involve unstable training, lip synchronization, and visual quality issues caused by lip-sync loss, SyncNet, and lip leaking from the identity reference. To address these issues, we first tackle the lip leaking problem by introducing a silent-lip generator, which changes the lips of the identity reference to alleviate leakage. We then introduce stabilized synchronization loss and AVSyncNet to overcome problems caused by lip-sync loss and SyncNet. Experiments show that our model outperforms state-of-the-art methods in both visual quality and lip synchronization. Comprehensive ablation studies further validate our individual contributions and their cohesive effects",
    "checked": true,
    "id": "11148b42124489cdaebc6655cd88f036f2f0a06a",
    "semantic_title": "audio-driven talking face generation with stabilized synchronization loss",
    "citation_count": 1,
    "authors": [
      "Dogucan Yaman*",
      "Fevziye Irem Eyiokur",
      "Leonard Bärmann",
      "HAZIM KEMAL EKENEL",
      "Alexander Waibel"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2965_ECCV_2024_paper.php": {
    "title": "Propose, Assess, Search: Harnessing LLMs for Goal-Oriented Planning in Instructional Videos",
    "volume": "main",
    "abstract": "Goal-oriented planning, or anticipating a series of actions that transition an agent from its current state to a predefined objective, is crucial for developing intelligent assistants aiding users in daily procedural tasks. The problem presents significant challenges due to the need for comprehensive knowledge of temporal and hierarchical task structures, as well as strong capabilities in reasoning and planning. To achieve this, prior work typically relies on extensive training on the target dataset, which often results in significant dataset bias and a lack of generalization to unseen tasks. In this work, we introduce , an integrated framework designed for zero/few-shot goal-oriented planning in instructional videos. leverages large language models (LLMs) as both the knowledge base and the assessment tool for generating and evaluating action plans, thus overcoming the challenges of acquiring procedural knowledge from small-scale, low-diversity datasets. Moreover, employs a breadth-first search algorithm for optimal plan generation, in which a composite of value functions designed for goal-oriented planning is utilized to assess the predicted actions at each step. Extensive experiments demonstrate that offers a unified framework for different goal-oriented planning setups, e.g., visual planning for assistance (VPA) and procedural planning (PP), and achieves remarkable performance in zero-shot and few-shot setups. Specifically, our few-shot model outperforms the prior fully supervised state-of-the-art method by +7.7% in VPA and +4.81% PP task on the COIN dataset while predicting 4 future actions",
    "checked": true,
    "id": "fad70318f51bd280980d592d1f0638f21b52a8d1",
    "semantic_title": "propose, assess, search: harnessing llms for goal-oriented planning in instructional videos",
    "citation_count": 0,
    "authors": [
      "Md Mohaiminul Islam*",
      "Tushar Nagarajan",
      "Huiyu Wang",
      "FU-JEN CHU",
      "Kris Kitani",
      "Gedas Bertasius",
      "Xitong Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2968_ECCV_2024_paper.php": {
    "title": "Train Till You Drop: Towards Stable and Robust Source-free Unsupervised 3D Domain Adaptation",
    "volume": "main",
    "abstract": "We tackle the challenging problem of source-free unsupervised domain adaptation (SFUDA) for 3D semantic segmentation. It amounts to performing domain adaptation on an unlabeled target domain without any access to source data; the available information is a model trained to achieve good performance on the source domain. A common issue with existing SFUDA approaches is that performance degrades after some training time, which is a by-product of an under-constrained and ill-posed problem. We discuss two strategies to alleviate this issue. First, we propose a sensible way to regularize the learning problem. Second, we introduce a novel criterion based on agreement with a reference model. It is used (1) to stop the training when appropriate and (2) as validator to select hyperparameters without any knowledge on the target domain. Our contributions are easy to implement and readily amenable for all SFUDA methods, ensuring stable improvements over all baselines. We validate our findings on various 3D lidar settings, achieving state-of-the-art performance. The project repository (with code) is: github.com/valeoai/TTYD",
    "checked": true,
    "id": "861d96065a8018f0596d3dc5e4fd8470f5dd48d1",
    "semantic_title": "train till you drop: towards stable and robust source-free unsupervised 3d domain adaptation",
    "citation_count": 0,
    "authors": [
      "Björn Michele*",
      "Alexandre Boulch",
      "Tuan-Hung VU",
      "Gilles Puy",
      "Renaud Marlet",
      "Nicolas Courty"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2971_ECCV_2024_paper.php": {
    "title": "Learning to Obstruct Few-Shot Image Classification over Restricted Classes",
    "volume": "main",
    "abstract": "Advancements in open-source pre-trained backbones make it relatively easy to fine-tune a model for new tasks. However, this lowered entry barrier poses potential risks, e.g., bad actors developing models for harmful applications. A question arises: Is possible to develop a pre-trained model that is difficult to fine-tune for certain downstream tasks? To begin studying this, we focus on few-shot classification (FSC). Specifically, we investigate methods to make FSC more challenging for a set of restricted classes while maintaining the performance of other classes. We propose to meta-learn over the pre-trained backbone in a manner that renders it a \"poor initialization\". Our proposed Learning to Obstruct (LTO) algorithm successfully obstructs four FSC methods across three datasets, including ImageNet and CIFAR100 for image classification, as well as CelebA for attribute classification",
    "checked": true,
    "id": "f47fcd83655c114ae52e30b9eab9163dd60b7f49",
    "semantic_title": "learning to obstruct few-shot image classification over restricted classes",
    "citation_count": 1,
    "authors": [
      "Amber Yijia Zheng*",
      "Chiao-An Yang*",
      "Raymond A. Yeh"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2977_ECCV_2024_paper.php": {
    "title": "RoofDiffusion: Constructing Roofs from Severely Corrupted Point Data via Diffusion",
    "volume": "main",
    "abstract": "Accurate completion and denoising of roof height maps are crucial to reconstructing high-quality 3D buildings. Repairing sparse points can enhance low-cost sensor use and reduce UAV flight overlap. is a new end-to-end self-supervised diffusion technique for robustly completing, in particular difficult, roof height maps. leverages widely-available curated footprints and can so handle up to 99% point sparsity and 80% roof area occlusion (regional incompleteness). A variant, , simultaneously predicts building footprints and heights. Both quantitatively outperform state-of-the-art unguided depth completion and representative inpainting methods for Digital Elevation Models (DEM), on both a roof-specific benchmark and the BuildingNet dataset. Qualitative assessments show the effectiveness of for datasets with real-world scans including AHN3, Dales3D, and USGS 3DEP LiDAR. Tested with the leading City3D algorithm, preprocessing height maps with noticeably improves 3D building reconstruction. is complemented by a new dataset of 13k complex roof geometries, focusing on long-tail issues in remote sensing; a novel simulation of tree occlusion; and a wide variety of large-area roof cut-outs for data augmentation and benchmarking. Code and dataset1 : github.com/kylelo/RoofDiffusion 1 Created and released by the University of Florida",
    "checked": true,
    "id": "ee5f0ef5962b1bf7b42f504bc5710611fae7867d",
    "semantic_title": "roofdiffusion: constructing roofs from severely corrupted point data via diffusion",
    "citation_count": 0,
    "authors": [
      "Kyle Shih-Huang Lo*",
      "Jorg Peters",
      "Eric Spellman"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2988_ECCV_2024_paper.php": {
    "title": "L-DiffER: Single Image Reflection Removal with Language-based Diffusion Model",
    "volume": "main",
    "abstract": "In this paper, we introduce L-DiffER, a language-based diffusion model designed for the ill-posed single image reflection removal task. Although having shown impressive performance for image generation, existing language-based diffusion models struggle with precise control and faithfulness in image restoration. To overcome these limitations, we propose an iterative condition refinement strategy to resolve the problem of inaccurate control conditions. A multi-condition constraint mechanism is employed to ensure the recovery faithfulness of image color and structure while retaining the generation capability to handle low-transmitted reflections. We demonstrate the superiority of the proposed method through extensive experiments, showcasing both quantitative and qualitative improvements over existing methods",
    "checked": false,
    "id": "3bedc0ca1ed536e49d32e0483348de2c137618ca",
    "semantic_title": "l-di ﬀ er: single image reﬂection removal with language-based di ﬀ usion model",
    "citation_count": 0,
    "authors": [
      "Yuchen Hong*",
      "Haofeng Zhong*",
      "Shuchen Weng",
      "Jinxiu S Liang",
      "Boxin Shi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2990_ECCV_2024_paper.php": {
    "title": "AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting",
    "volume": "main",
    "abstract": "With the advent and widespread deployment of Multimodal Large Language Models (MLLMs), the imperative to ensure their safety has become increasingly pronounced. However, with the integration of additional modalities, MLLMs are exposed to new vulnerabilities, rendering them prone to structured-based jailbreak attacks, where semantic content (e.g. \"harmful text\") has been injected into the images to mislead MLLMs. In this work, we aim to defend against such threats. Specifically, we propose Adaptive Shield Prompting (AdaShield), which prepends inputs with defense prompts to defend MLLMs against structure-based jailbreak attacks without fine-tuning MLLMs or training additional modules (e.g., post-stage content detector). Initially, we present a manually designed static defense prompt, which thoroughly examines the image and instruction content step by step and specifies response methods to malicious queries. Furthermore, we introduce an adaptive auto-refinement framework, consisting of a target MLLM and a LLM-based defense prompt generator (Defender). These components collaboratively and iteratively communicate to generate a defense prompt. Extensive experiments on the popular structure-based jailbreak attacks and benign datasets show that our methods can consistently improve MLLMs' robustness against structure-based jailbreak attacks without compromising the model's general capabilities evaluated on standard benign tasks. Our code is available at https://rain305f.github.io/AdaShield-P red Disclaimer: This paper contains offensive content that may be disturbing to some readers",
    "checked": true,
    "id": "8a25739903cab07c74556b8c2d9743749e1be1e5",
    "semantic_title": "adashield: safeguarding multimodal large language models from structure-based attack via adaptive shield prompting",
    "citation_count": 16,
    "authors": [
      "Yu Wang*",
      "Xiaogeng Liu*",
      "Yu Li*",
      "Muhao Chen",
      "Chaowei Xiao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3004_ECCV_2024_paper.php": {
    "title": "OccGen: Generative Multi-modal 3D Occupancy Prediction for Autonomous Driving",
    "volume": "main",
    "abstract": "Existing 3D semantic occupancy prediction methods typically treat the task as a one-shot 3D voxel-wise segmentation problem, focusing on a single-step mapping between the inputs and occupancy maps, which limits their ability to refine and complete local regions gradually. In this paper, we introduce OccGen, a simple yet powerful generative perception model for 3D semantic occupancy prediction. OccGen adopts a \"noise-to-occupancy\" generative paradigm, progressively inferring and refining the occupancy map by predicting and eliminating noise originating from a random Gaussian distribution. OccGen consists of two main components: a conditional encoder that is capable of processing multi-modal inputs, and a progressive refinement decoder that applies diffusion denoising using the multi-modal features as conditions. A key insight of this generative pipeline is that the diffusion denoising process is naturally able to model the coarse-to-fine refinement of the dense 3D occupancy map, therefore producing more detailed predictions. Extensive experiments on several occupancy benchmarks demonstrate the effectiveness of the proposed method compared to the state-of-the-art methods. For instance, OccGen relatively enhances the mIoU by 9.5%, 6.3%, and 13.3% on nuScenes-Occupancy dataset under the muli-modal, LiDAR-only, and camera-only settings, respectively. Moreover, as a generative perception model, OccGen exhibits desirable properties that discriminative models cannot achieve, such as providing uncertainty estimates alongside its multiple-step predictions",
    "checked": true,
    "id": "9902eb528ce793dd9459bb50055b7be050d8b1ab",
    "semantic_title": "occgen: generative multi-modal 3d occupancy prediction for autonomous driving",
    "citation_count": 6,
    "authors": [
      "Guoqing Wang",
      "Zhongdao Wang",
      "Pin Tang",
      "Jilai Zheng",
      "Xiangxuan Ren",
      "Bailan Feng",
      "Chao Ma*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3008_ECCV_2024_paper.php": {
    "title": "CrossGLG: LLM Guides One-shot Skeleton-based 3D Action Recognition in a Cross-level Manner",
    "volume": "main",
    "abstract": "Most existing one-shot skeleton-based action recognition focuses on raw low-level information (, joint location), and may suffer from local information loss and low generalization ability. To alleviate these, we propose to leverage text description generated from large language models (LLM) that contain high-level human knowledge, to guide feature learning, in a global-local-global way. Particularly, during training, we design 2 prompts to gain global and local text descriptions of each action from an LLM. We first utilize the global text description to guide the skeleton encoder focus on informative joints (,global-to-local). Then we build non-local interaction between local text and joint features, to form the final global representation (, local-to-global). To mitigate the asymmetry issue between the training and inference phases, we further design a dual-branch architecture that allows the model to perform novel class inference without any text input, also making the additional inference cost neglectable compared with the base skeleton encoder. Extensive experiments on three different benchmarks show that CrossGLG consistently outperforms the existing SOTA methods with large margins, and the inference cost (model size) is only 2.8% than the previous SOTA. Code is available at [RGB]255,106,106CrossGLG. † Yang Xiao and Wenzheng Zeng are corresponding authors",
    "checked": true,
    "id": "1e4c3f7cbfa75a6fe732479da98eb41f51b078b3",
    "semantic_title": "crossglg: llm guides one-shot skeleton-based 3d action recognition in a cross-level manner",
    "citation_count": 1,
    "authors": [
      "Tingbing Yan",
      "Wenzheng Zeng*",
      "Yang Xiao*",
      "Xingyu Tong",
      "Bo Tan",
      "Zhiwen Fang",
      "Zhiguo Cao",
      "Joey Tianyi Zhou"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3012_ECCV_2024_paper.php": {
    "title": "HYDRA: A Hyper Agent for Dynamic Compositional Visual Reasoning",
    "volume": "main",
    "abstract": "Recent advances in visual reasoning (VR), particularly with the aid of Large Vision-Language Models (VLMs), show promise but require access to large-scale datasets and face challenges such as high computational costs and limited generalization capabilities. Compositional visual reasoning approaches have emerged as effective strategies; however, they heavily rely on the commonsense knowledge encoded in Large Language Models (LLMs) to perform planning, reasoning, or both, without considering the effect of their decisions on the visual reasoning process, which can lead to errors or failed procedures. To address these challenges, we introduce , a multi-stage dynamic compositional visual reasoning framework designed for reliable and incrementally progressive general reasoning. integrates three essential modules: a planner, a Reinforcement Learning (RL) agent serving as a cognitive controller, and a reasoner. The planner and reasoner modules utilize an LLM to generate instruction samples and executable code from the selected instruction, respectively, while the RL agent dynamically interacts with these modules, making high-level decisions on selection of the best instruction sample given information from the historical state stored through a feedback loop. This adaptable design enables to adjust its actions based on previous feedback received during the reasoning process, leading to more reliable reasoning outputs and ultimately enhancing its overall effectiveness. Our framework demonstrates state-of-the-art performance in various VR tasks on four different widely-used datasets",
    "checked": true,
    "id": "d5f1391a226895faea2ef70dbd69bcf2d78547ab",
    "semantic_title": "hydra: a hyper agent for dynamic compositional visual reasoning",
    "citation_count": 1,
    "authors": [
      "Fucai Ke*",
      "Zhixi Cai",
      "Simindokht Jahangard",
      "Weiqing Wang",
      "Pari Delir Haghighi",
      "Hamid Rezatofighi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3014_ECCV_2024_paper.php": {
    "title": "BrushNet: A Plug-and-Play Image Inpainting Model with Decomposed Dual-Branch Diffusion",
    "volume": "main",
    "abstract": "Image inpainting, the process of restoring corrupted images, has seen significant advancements with the advent of diffusion models (DMs). Despite these advancements, current DM adaptations for inpainting, which involve modifications to the sampling strategy or the development of inpainting-specific DMs, frequently suffer from semantic inconsistencies and reduced image quality. Addressing these challenges, our work introduces a novel paradigm: the division of masked image features and noisy latent into separate branches. This division dramatically diminishes the model's learning load, facilitating a nuanced incorporation of essential masked image information in a hierarchical fashion. Herein, we present , a novel plug-and-play dual-branch model engineered to embed pixel-level masked image features into any pre-trained DM, guaranteeing coherent and enhanced image inpainting outcomes. Additionally, we introduce and to facilitate segmentation-based inpainting training and performance assessment. Our extensive experimental analysis demonstrates BrushNet's superior performance over existing models across seven key metrics, including image quality, mask region preservation, and textual coherence",
    "checked": true,
    "id": "90c428ba9488c60bd860344d5a1299f01810ae51",
    "semantic_title": "brushnet: a plug-and-play image inpainting model with decomposed dual-branch diffusion",
    "citation_count": 7,
    "authors": [
      "Xuan Ju*",
      "Xian Liu",
      "Xintao Wang*",
      "Yuxuan Bian",
      "Ying Shan",
      "Qiang Xu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3022_ECCV_2024_paper.php": {
    "title": "LayoutDETR: Detection Transformer Is a Good Multimodal Layout Designer",
    "volume": "main",
    "abstract": "Graphic layout designs play an essential role in visual communication. Yet handcrafting layout designs is skill-demanding, time-consuming, and non-scalable to batch production. Generative models emerge to make design automation scalable but it remains non-trivial to produce designs that comply with designers' multimodal desires, i.e., constrained by background images and driven by foreground content. We propose LayoutDETR that inherits the high quality and realism from generative modeling, while reformulating content-aware requirements as a detection problem: we learn to detect in a background image the reasonable locations, scales, and spatial relations for multimodal foreground elements in a layout. Our solution sets a new state-of-the-art performance for layout generation on public benchmarks and on our newly-curated ad banner dataset. We integrate our solution into a graphical system that facilitates user studies, and show that users prefer our designs over baselines by significant margins. Code, models, dataset, and demos are available at GitHub",
    "checked": true,
    "id": "70299b50bbb9554cdf095862b4dd3c4e0b97e126",
    "semantic_title": "layoutdetr: detection transformer is a good multimodal layout designer",
    "citation_count": 4,
    "authors": [
      "Ning Yu*",
      "Chia-chih Chen",
      "Zeyuan Chen",
      "Rui Meng",
      "Gang Wu",
      "Paul W Josel",
      "Juan Carlos Niebles",
      "Caiming Xiong",
      "Ran Xu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3024_ECCV_2024_paper.php": {
    "title": "Blind image deblurring with noise-robust kernel estimation",
    "volume": "main",
    "abstract": "Blind deblurring is an ill-posed inverse problem involving the retrieval of a clear image and blur kernel from a single blurry image. The challenge arises considerably when strong noise, where its level remains unknown, is introduced. Existing blind deblurring methods are highly susceptible to noise due to overfitting and disturbances in the solution space. Here, we propose a blind deblurring method based on a noise-robust kernel estimation function and deep image prior (DIP). Specifically, the proposed kernel estimation function effectively estimates the blur kernel even for strongly noisy blurry images given a clear image and optimal condition. Therefore, DIP is adopted for the generation of a clear image to leverage its natural image prior. Additionally, the multiple kernel estimation scheme is designed to address a wide range of unknown noise levels. Extensive experimental studies, including simulated images and real-world examples, demonstrate the superior deblurring performance of the proposed method. The official code is uploaded in https://github.com/csleemooo/BD_noise_robust_kernel_estimation",
    "checked": false,
    "id": "442e0feeffc291f3fadd7ff8ccdf7398fb6e825a",
    "semantic_title": "blind image deblurring via bayesian estimation using expected loss",
    "citation_count": 0,
    "authors": [
      "Chanseok Lee*",
      "Jeongsol Kim",
      "Seungmin Lee",
      "Jaehwang Jung",
      "Yunje Cho",
      "Taejoong Kim",
      "Taeyong Jo",
      "Myungjun Lee",
      "Mooseok Jang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3025_ECCV_2024_paper.php": {
    "title": "Binomial Self-compensation for Motion Error in Dynamic 3D Scanning",
    "volume": "main",
    "abstract": "Phase shifting profilometry (PSP) is favored in high-precision 3D scanning due to its high accuracy, robustness, and pixel-wise property. However, a fundamental assumption of PSP that the object should remain static is violated in dynamic measurement, making PSP susceptible to object moving, resulting in ripple-like errors in the point clouds. We propose a pixel-wise and frame-wise loopable binomial self-compensation (BSC) algorithm to effectively and flexibly eliminate motion error in the four-step PSP. Our mathematical model demonstrates that by summing successive motion-affected phase frames weighted by binomial coefficients, motion error exponentially diminishes as the binomial order increases, accomplishing automatic error compensation through the motion-affected phase sequence, without the assistance of any intermediate variable. Extensive experiments show that our BSC outperforms the existing methods in reducing motion error, while achieving a depth map frame rate equal to the camera's acquisition rate (90 fps), enabling high-accuracy 3D reconstruction with a quasi-single-shot frame rate. The code is available at https://github.com/GeyouZhang/BSC",
    "checked": true,
    "id": "fbcfdfe98a7f7db4472b9e77a47bec058b7d3c24",
    "semantic_title": "binomial self-compensation for motion error in dynamic 3d scanning",
    "citation_count": 0,
    "authors": [
      "Geyou Zhang",
      "Ce Zhu*",
      "Kai Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3028_ECCV_2024_paper.php": {
    "title": "AddMe: Zero-shot Group-photo Synthesis by Inserting People into Scenes",
    "volume": "main",
    "abstract": "While large text-to-image diffusion models have made significant progress in high-quality image generation, challenges persist when users insert their portraits into existing photos, especially group photos. Concretely, existing customization methods struggle to insert facial identities at desired locations in existing images, and it is difficult for existing local image editing methods to deal with facial details. To address these limitations, we propose AddMe, a powerful diffusion-based portrait generator that can insert a given portrait into a desired location in an existing scene image in a zero-shot manner. Specifically, we propose a novel identity adapter to learn a facial representation decoupled from existing characters in the scene. Meanwhile, to ensure that the generated portrait can interact properly with others in the existing scene, we design an enhanced portrait attention module to capture contextual information during the generation process. Our method is compatible with both text and various spatial conditions, enabling precise control over the generated portraits. Extensive experiments demonstrate significant improvements in both performance and efficiency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongxu Yue",
      "Maomao Li",
      "Yunfei Liu",
      "Ailing Zeng",
      "Tianyu Yang",
      "Qin Guo",
      "Yu Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3030_ECCV_2024_paper.php": {
    "title": "Distill Gold from Massive Ores: Bi-level Data Pruning towards Efficient Dataset Distillation",
    "volume": "main",
    "abstract": "Data-efficient learning has garnered significant attention, especially given the current trend of large multi-modal models. Recently, dataset distillation has become an effective approach by synthesizing data samples that are essential for network training. However, it remains to be explored which samples are essential for the dataset distillation process itself. In this work, we study the data efficiency and selection for the dataset distillation task. By re-formulating the dynamics of distillation, we provide insight into the inherent redundancy in the real dataset, both theoretically and empirically. We propose to use the empirical loss value as a static data pruning criterion. To further compensate for the variation of the data value in training, we find the most contributing samples based on their causal effects on the distillation. The proposed selection strategy can efficiently exploit the training dataset, outperform the previous SOTA distillation algorithms, and consistently enhance the distillation algorithms, even on much larger-scale and more heterogeneous datasets, , full ImageNet-1K and Kinetics-400. We believe this paradigm will open up new avenues in the dynamics of distillation and pave the way for efficient dataset distillation. Our code is available on https://github.com/silicx/GoldFromOres-BiLP",
    "checked": true,
    "id": "386c93b47761f4b96db3cb1f6c70724f1f77b5eb",
    "semantic_title": "distill gold from massive ores: bi-level data pruning towards efficient dataset distillation",
    "citation_count": 0,
    "authors": [
      "Yue Xu",
      "Yong-Lu Li*",
      "Kaitong Cui",
      "Ziyu Wang",
      "Cewu Lu",
      "Yu-Wing Tai",
      "Chi-Keung Tang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3032_ECCV_2024_paper.php": {
    "title": "VersatileGaussian: Real-time Neural Rendering for Versatile Tasks using Gaussian Splatting",
    "volume": "main",
    "abstract": "The acquisition of multi-task (MT) labels in 3D scenes is crucial for a wide range of real-world applications. Traditional methods generally employ an analysis-by-synthesis approach, generating 2D label maps on novel synthesized views, or utilize Neural Radiance Field (NeRF), which concurrently represents label maps. Yet, these approaches often struggle to balance inference efficiency with MT label quality. Specifically, they face limitations such as (a) constrained rendering speeds due to NeRF pipelines, and (b) the implicit representation of MT fields that can result in continuity artifacts during rendering. Recently, 3D Gaussian Splatting has shown promise in achieving real-time rendering speeds without compromising rendering quality. In our research, we address the challenge of enabling 3D Gaussian Splatting to represent Versatile MT labels. Simply attaching MT attributes to explicit Gaussians compromises rendering quality due to the lack of cross-task information flow during optimization. We introduce architectural and rasterizer design to effectively overcome this issue. Our VersatileGaussian model innovatively associates Gaussians with shared MT features and incorporates a feature map rasterizer. The key element of this versatile rasterization is the Task Correlation Attention module, which utilizes cross-task correlations through a soft weighting mechanism that disseminates task-specific knowledge. Across experiments on the ScanNet and Replica datasets shows that VersatileGaussian not only sets a new benchmark in MT accuracy but also maintains real-time rendering speeds (35 FPS). Importantly, this model design facilitates mutual benefits across tasks, leading to improved quality in novel view synthesis even in situations that the ground-truth dense labels are absent, and with the assistant of dense labels from off-the-shelf 2D predictors",
    "checked": false,
    "id": "e082a0217af0ba98b6e39486a7900d0be2d7464a",
    "semantic_title": "rd-slam: real-time dense slam using gaussian splatting",
    "citation_count": 0,
    "authors": [
      "Renjie Li",
      "Zhiwen Fan*",
      "Bohua Wang",
      "Peihao Wang",
      "Zhangyang Wang",
      "Xi Wu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3037_ECCV_2024_paper.php": {
    "title": "Momentum Auxiliary Network for Supervised Local Learning",
    "volume": "main",
    "abstract": "Deep neural networks conventionally employ end-to-end backpropagation for their training process, which lacks biological credibility and triggers a locking dilemma during network parameter updates, leading to significant GPU memory use. Supervised local learning, which segments the network into multiple local blocks updated by independent auxiliary networks. However, these methods cannot replace end-to-end training due to lower accuracy, as gradients only propagate within their local block, creating a lack of information exchange between blocks. To address this issue and establish information transfer across blocks, we propose a Momentum Auxiliary Network (MAN) that establishes a dynamic interaction mechanism. The MAN leverages an exponential moving average (EMA) of the parameters from adjacent local blocks to enhance information flow. This auxiliary network, updated through EMA, helps bridge the informational gap between blocks. Nevertheless, we observe that directly applying EMA parameters has certain limitations due to feature discrepancies among local blocks. To overcome this, we introduce learnable biases, further boosting performance. We have validated our method on four image classification datasets (CIFAR-10, STL-10, SVHN, ImageNet), attaining superior performance and substantial memory savings. Notably, our method can reduce GPU memory usage by more than 45% on the ImageNet dataset compared to end-to-end training, while achieving higher performance. The Momentum Auxiliary Network thus offers a new perspective for supervised local learning. Our code is available at: https://github.com/JunhaoSu0/MAN. ∗ Equal Contribution. Corresponding Authors: Chengyang (chenyang.si.mail@gmail.com) and Dongzhi Guan (guandongzhi@seu.edu.cn). Si",
    "checked": true,
    "id": "b785258615835e5470bd487c4eeeacb36162387f",
    "semantic_title": "momentum auxiliary network for supervised local learning",
    "citation_count": 3,
    "authors": [
      "Junhao Su",
      "Changpeng Cai",
      "Feiyu Zhu",
      "Chenghao He",
      "Xiaojie Xu",
      "Dongzhi Guan*",
      "Chenyang Si*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3039_ECCV_2024_paper.php": {
    "title": "HPFF: Hierarchical Locally Supervised Learning with Patch Feature Fusion",
    "volume": "main",
    "abstract": "Traditional deep learning relies on end-to-end backpropagation for training, but it suffers from drawbacks such as high memory consumption and not aligning with biological neural networks. Recent advancements have introduced locally supervised learning, which divides networks into modules with isolated gradients and trains them locally. However, this approach can lead to performance lag due to limited interaction between these modules, and the design of auxiliary networks occupies a certain amount of GPU memory. To overcome these limitations, we propose a novel model called HPFF that performs hierarchical locally supervised learning and patch-level feature computation on the auxiliary networks. Hierarchical Locally Supervised Learning (HiLo) enables the network to learn features at different granularity levels along their respective local paths. Specifically, the network is divided into two-level local modules: independent local modules and cascade local modules. The cascade local modules combine two adjacent independent local modules, incorporating both updates within the modules themselves and information exchange between adjacent modules. Patch Feature Fusion (PFF) reduces GPU memory usage by splitting the input features of the auxiliary networks into patches for computation. By averaging these patch-level features, it enhances the network's ability to focus more on those patterns that are prevalent across multiple patches. Furthermore, our method exhibits strong generalization capabilities and can be seamlessly integrated with existing techniques. We conduct experiments on CIFAR-10, STL-10, SVHN, and ImageNet datasets, and the results demonstrate that our proposed HPFF significantly outperforms previous approaches, consistently achieving state-of-the-art performance across different datasets. Our code is available at: https://github.com/ Zeudfish/HPFF. ∗ Equal Contribution. (chenyang.si.mail@gmail.com). Corresponding Author: Chengyang Si",
    "checked": true,
    "id": "b70125673c977fdbbeaa5f574db6d1faea83ab39",
    "semantic_title": "hpff: hierarchical locally supervised learning with patch feature fusion",
    "citation_count": 2,
    "authors": [
      "Junhao Su",
      "Chenghao He",
      "Feiyu Zhu",
      "Xiaojie Xu",
      "Dongzhi Guan",
      "Chenyang Si*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3042_ECCV_2024_paper.php": {
    "title": "Rethinking LiDAR Domain Generalization: Single Source as Multiple Density Domains",
    "volume": "main",
    "abstract": "In the realm of LiDAR-based perception, significant strides have been made, yet domain generalization remains a substantial challenge. The performance often deteriorates when models are applied to unfamiliar datasets with different LiDAR sensors or deployed in new environments, primarily due to variations in point cloud density distributions. To tackle this challenge, we propose a Density Discriminative Feature Embedding (DDFE) module, capitalizing on the observation that a single source LiDAR point cloud encompasses a spectrum of densities. The DDFE module is meticulously designed to extract density-specific features within a single source domain, facilitating the recognition of objects sharing similar density characteristics across different LiDAR sensors. In addition, we introduce a simple yet effective density augmentation technique aimed at expanding the spectrum of density in source data, thereby enhancing the capabilities of the DDFE. Our DDFE stands out as a versatile and lightweight domain generalization module. It can be seamlessly integrated into various 3D backbone networks, where it has demonstrated superior performance over current state-of-the-art domain generalization methods. Code is available at https://github.com/ dgist-cvlab/MultiDensityDG",
    "checked": true,
    "id": "b93922418dca9406fce5949d0b91c49501956a3d",
    "semantic_title": "rethinking lidar domain generalization: single source as multiple density domains",
    "citation_count": 0,
    "authors": [
      "Jaeyeul Kim",
      "Jungwan Woo",
      "Jeonghoon Kim",
      "Sunghoon Im*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3044_ECCV_2024_paper.php": {
    "title": "Improving Zero-Shot Generalization for CLIP with Variational Adapter",
    "volume": "main",
    "abstract": "The excellent generalization capability of pre-trained Vision-Language Models (VLMs) makes fine-tuning VLMs for downstream zero-shot tasks a popular choice. Despite achieving promising performance in the professionality of base classes, most existing fine-tuned methods suffer from feature confusion of novel classes, resulting in unsatisfactory transferability. To address this problem, we propose a divide-and-conquer approach called Prompt-based Variational Adapter (PVA) that explicitly reduces the prediction bias by separating base and novel samples. Specifically, we design two variational adapters with learnable textual tokens to align latent representations for each modality in a shared latent space. Once trained, we can separate novel samples from entangled space using the similarity metric of latent features, i.e., converting confusion task into two independent ones (One for base classes and the other for novel classes). Moreover, to improve the transferability for novel classes, we further refine the output features of the learned adapters with the global features via a residual connection. We conduct extensive experiments on Generalized Zero-Shot Learning and Cross-Dataset Transfer Learning to demonstrate the superiority of our approach and establish a new state-of-the-art on four popular benchmarks",
    "checked": false,
    "id": "30b00365e0e2c6994ad4dfc456e0d706aefde27d",
    "semantic_title": "caps-adapter: caption-based multimodal adapter in zero-shot classification",
    "citation_count": 1,
    "authors": [
      "Ziqian Lu",
      "Fengli Shen",
      "Mushui Liu",
      "Yunlong Yu*",
      "Xi Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3045_ECCV_2024_paper.php": {
    "title": "Realistic Human Motion Generation with Cross-Diffusion Models",
    "volume": "main",
    "abstract": "In this work, we introduce the Cross Human Motion Diffusion Model (CrossDiff1 ), a novel approach for generating high-quality human motion based on textual descriptions. Our method integrates 3D and 2D information using a shared transformer network within the training of the diffusion model, unifying motion noise into a single feature space. This enables cross-decoding of features into both 3D and 2D motion representations, regardless of their original dimension. The primary advantage of CrossDiff is its cross-diffusion mechanism, which allows the model to reverse either 2D or 3D noise into clean motion during training. This capability leverages the complementary information in both motion representations, capturing intricate human movement details often missed by models relying solely on 3D information. Consequently, CrossDiff effectively combines the strengths of both representations to generate more realistic motion sequences. In our experiments, our model demonstrates competitive state-of-the-art performance on text-to-motion benchmarks. Moreover, our method consistently provides enhanced motion generation quality, capturing complex full-body movement intricacies. Additionally, with a pre-trained model, our approach accommodates using in-the-wild 2D motion data without 3D motion ground truth during training to generate 3D motion, highlighting its potential for broader applications and efficient use of available data resources. 1 https://wonderno.github.io/CrossDiff-webpage/",
    "checked": true,
    "id": "d9a5b7891ceddb65c339dd16b1b56853c3b01da5",
    "semantic_title": "realistic human motion generation with cross-diffusion models",
    "citation_count": 0,
    "authors": [
      "Zeping Ren",
      "Shaoli Huang*",
      "Xiu Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3057_ECCV_2024_paper.php": {
    "title": "EgoExo-Fitness: Towards Egocentric and Exocentric Full-Body Action Understanding",
    "volume": "main",
    "abstract": "We present EgoExo-Fitness, a new full-body action understanding dataset, featuring fitness sequence videos recorded from synchronized egocentric and fixed exocentric (third-person) cameras. Compared with existing full-body action understanding datasets, EgoExo-Fitness not only contains videos from first-person perspectives, but also provides rich annotations. Specifically, two-level temporal boundaries are provided to localize single action videos along with sub-steps of each action. More importantly, EgoExo-Fitness introduces innovative annotations for interpretable action judgement–including technical keypoint verification, natural language comments on action execution, and action quality scores. Combining all of these, EgoExo-Fitness provides new resources to study egocentric and exocentric full-body action understanding across dimensions of \"what\", \"when\", and \"how well\". To facilitate research on egocentric and exocentric full-body action understanding, we construct benchmarks on a suite of tasks (, action classification, action localization, cross-view sequence verification, cross-view skill determination, and a newly proposed task of guidance-based execution verification), together with detailed analysis. Data and code are available at https://github.com/iSEE-Laboratory/EgoExo-Fitness/tree/main",
    "checked": true,
    "id": "c29682b87f178e56de4dfea2b909d3be65527d52",
    "semantic_title": "egoexo-fitness: towards egocentric and exocentric full-body action understanding",
    "citation_count": 3,
    "authors": [
      "Yuan-Ming Li",
      "Wei-Jin Huang",
      "An-Lan Wang",
      "Ling-An Zeng",
      "Jing-Ke Meng*",
      "Wei-Shi Zheng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3058_ECCV_2024_paper.php": {
    "title": "Any Target Can be Offense: Adversarial Example Generation via Generalized Latent Infection",
    "volume": "main",
    "abstract": "Targeted adversarial attack, which aims to mislead a model to recognize any image as a target object by imperceptible perturbations, has become a mainstream tool for vulnerability assessment of deep neural networks (DNNs). Since existing targeted attackers only learn to attack known target classes, they cannot generalize well to unknown classes. To tackle this issue, we propose Generalized Adversarial attacKER (GAKer), which is able to construct adversarial examples to any target class. The core idea behind GAKer is to craft a latently infected representation during adversarial example generation. To this end, the extracted latent representations of the target object are first injected into intermediate features of an input image in an adversarial generator. Then, the generator is optimized to ensure visual consistency with the input image while being close to the target object in the feature space. Since the GAKer is class-agnostic yet model-agnostic, it can be regarded as a general tool that not only reveals the vulnerability of more DNNs but also identifies deficiencies of DNNs in a wider range of classes. Extensive experiments have demonstrated the effectiveness of our proposed method in generating adversarial examples for both known and unknown classes. Notably, compared with other generative methods, our method achieves an approximately 14.13% higher attack success rate for unknown classes and an approximately 4.23% higher success rate for known classes. Our code is available in https://github.com/VL-Group/GAKer",
    "checked": true,
    "id": "5a200ddf1000b03b28bee1fe04156ecae4110997",
    "semantic_title": "any target can be offense: adversarial example generation via generalized latent infection",
    "citation_count": 0,
    "authors": [
      "Youheng Sun",
      "Shengming Yuan",
      "Xuanhan Wang*",
      "Lianli Gao",
      "Jingkuan Song"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3062_ECCV_2024_paper.php": {
    "title": "Towards Reliable Advertising Image Generation Using Human Feedback",
    "volume": "main",
    "abstract": "In the e-commerce realm, compelling advertising images are pivotal for attracting customer attention. While generative models automate image generation, they often produce substandard images that may mislead customers and require significant labor costs to inspect. This paper delves into increasing the rate of available generated images. We first introduce a multi-modal Reliable Feedback Network (RFNet) to automatically inspect the generated images. Combining the RFNet into a recurrent process, Recurrent Generation, results in a higher number of available advertising images. To further enhance production efficiency, we fine-tune diffusion models with an innovative Consistent Condition regularization utilizing the feedback from RFNet (RFFT). This results in a remarkable increase in the available rate of generated images, reducing the number of attempts in Recurrent Generation, and providing a highly efficient production process without sacrificing visual appeal. We also construct a Reliable Feedback 1 Million (RF1M) dataset which comprises over one million generated advertising images annotated by human, which helps to train RFNet to accurately assess the availability of generated images and faithfully reflect the human feedback. Generally speaking, our approach offers a reliable solution for advertising image generation. Our dataset and code are available at https://github.com/ZhenbangDu/ Reliable_AD",
    "checked": true,
    "id": "8cee54df89130080164d7d71ee286e67c065300b",
    "semantic_title": "towards reliable advertising image generation using human feedback",
    "citation_count": 0,
    "authors": [
      "Zhenbang Du*",
      "Wei Feng",
      "Haohan Wang",
      "Yaoyu Li",
      "Jingsen Wang",
      "Jian Li",
      "Zheng Zhang",
      "Jingjing Lv",
      "Xin Zhu",
      "Junsheng Jin",
      "Junjie Shen",
      "Zhangang Lin",
      "Jingping Shao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3067_ECCV_2024_paper.php": {
    "title": "Topology-Preserving Downsampling of Binary Images",
    "volume": "main",
    "abstract": "We present a novel discrete optimization-based approach to generate downsampled versions of binary images that are guaranteed to have the same topology as the original, measured by the zeroth and first Betti numbers of the black regions, while having good similarity to the original image as measured by IoU and Dice scores. To our best knowledge, all existing binary image downsampling methods don't have such topology-preserving guarantees. We also implemented a baseline morphological operation (dilation)-based approach that always generates topologically correct results. However, we found the similarity scores to be much worse. We demonstrate several applications of our approach. First, generating smaller versions of medical image segmentation masks for easier human inspection. Second, improving the efficiency of binary image operations, including persistent homology computation and shortest path computation, by substituting the original images with smaller ones. In particular, the latter is a novel application that is made feasible only by the full topology-preservation guarantee of our method",
    "checked": true,
    "id": "d4b530309659dbf44aa36f22dda4bbfeb9772d42",
    "semantic_title": "topology-preserving downsampling of binary images",
    "citation_count": 0,
    "authors": [
      "Chia-Chia Chen*",
      "Chi-Han Peng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3072_ECCV_2024_paper.php": {
    "title": "ColorMAE: Exploring data-independent masking strategies in Masked AutoEncoders",
    "volume": "main",
    "abstract": "Masked AutoEncoders (MAE) have emerged as a robust self-supervised framework, offering remarkable performance across a wide range of downstream tasks. To increase the difficulty of the pretext task and learn richer visual representations, existing works have focused on replacing standard random masking with more sophisticated strategies, such as adversarial-guided and teacher-guided masking. However, these strategies depend on the input data thus commonly increasing the model complexity and requiring additional calculations to generate the mask patterns. This raises the question: Can we enhance MAE performance beyond random masking without relying on input data or incurring additional computational costs? In this work, we introduce a simple yet effective data-independent method, termed , which generates different binary mask patterns by filtering random noise. Drawing inspiration from color noise in image processing, we explore four types of filters to yield mask patterns with different spatial and semantic priors. requires no additional learnable parameters or computational overhead in the network, yet it significantly enhances the learned representations. We provide a comprehensive empirical evaluation, demonstrating our strategy's superiority in downstream tasks compared to random masking. Notably, we report an improvement of 2.72 in mIoU in semantic segmentation tasks relative to baseline MAE implementations",
    "checked": true,
    "id": "f00664971b2d42437859a3af07c64c7310d1dad1",
    "semantic_title": "colormae: exploring data-independent masking strategies in masked autoencoders",
    "citation_count": 0,
    "authors": [
      "Carlos Hinojosa*",
      "Shuming Liu",
      "Bernard Ghanem"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3079_ECCV_2024_paper.php": {
    "title": "Classification Matters: Improving Video Action Detection with Class-Specific Attention",
    "volume": "main",
    "abstract": "Video action detection (VAD) aims to detect actors and classify their actions in a video. We figure that VAD suffers more from classification rather than localization of actors. Hence, we analyze how prevailing methods form features for classification and find that they prioritize actor regions, yet often overlooking the essential contextual information necessary for accurate classification. Accordingly, we propose to reduce the bias toward actor and encourage paying attention to the context that is relevant to each action class. By assigning a class-dedicated query to each action class, our model can dynamically determine where to focus for effective classification. The proposed model demonstrates superior performance on three challenging benchmarks with significantly fewer parameters and less computation",
    "checked": true,
    "id": "6ad722bdaa8799c02bd1c9384b0e3c34e63c4b29",
    "semantic_title": "classification matters: improving video action detection with class-specific attention",
    "citation_count": 0,
    "authors": [
      "Jinsung Lee",
      "Taeoh Kim",
      "Inwoong Lee",
      "Minho Shim",
      "Dongyoon Wee",
      "Minsu Cho",
      "Suha Kwak*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3080_ECCV_2024_paper.php": {
    "title": "Improving Medical Multi-modal Contrastive Learning with Expert Annotations",
    "volume": "main",
    "abstract": "We introduce eCLIP, an enhanced version of the CLIP model that integrates expert annotations in the form of radiologist eye-gaze heatmaps. It tackles key challenges in contrastive multi-modal medical imaging analysis, notably data scarcity and the \"modality gap\" – a significant disparity between image and text embeddings that diminishes the quality of representations and hampers cross-modal interoperability. eCLIP integrates a heatmap processor and leverages mixup augmentation to efficiently utilize the scarce expert annotations, thus boosting the model's learning effectiveness. eCLIP is designed to be generally applicable to any variant of CLIP without requiring any modifications of the core architecture. Through detailed evaluations across several tasks, including zero-shot inference, linear probing, cross-modal retrieval, and Retrieval Augmented Generation (RAG) of radiology reports using a frozen Large Language Model, eCLIP showcases consistent improvements in embedding quality. The outcomes reveal enhanced alignment and uniformity, affirming eCLIP's capability to harness high-quality annotations for enriched multi-modal analysis in the medical imaging domain",
    "checked": true,
    "id": "8cc4cd653c157e4f96dfd963def73dbefa19dc9e",
    "semantic_title": "improving medical multi-modal contrastive learning with expert annotations",
    "citation_count": 5,
    "authors": [
      "Yogesh Kumar*",
      "Pekka Marttinen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3084_ECCV_2024_paper.php": {
    "title": "Rethinking Data Bias: Dataset Copyright Protection via Embedding Class-wise Hidden Bias",
    "volume": "main",
    "abstract": "Public datasets play a crucial role in advancing data-centric AI, yet they remain vulnerable to illicit uses. This paper presents ‘undercover bias,' a novel dataset watermarking method that can reliably identify and verify unauthorized data usage. Our approach is inspired by an observation that trained models often inadvertently learn biased knowledge and can function on bias-only data, even without any information directly related to a target task. Leveraging this, we deliberately embed class-wise hidden bias via unnoticeable watermarks, which are unrelated to the target dataset but share the same labels. Consequently, a model trained on this watermarked data covertly learns to classify these watermarks. The model's performance in classifying the watermarks serves as irrefutable evidence of unauthorized usage, which cannot be achieved by chance. Our approach presents multiple benefits: 1) stealthy and model-agnostic watermarks; 2) minimal impact on the target task; 3) irrefutable evidence of misuse; and 4) improved applicability in practical scenarios. We validate these benefits through extensive experiments and extend our method to fine-grained classification and image segmentation tasks. Our implementation is available at here1 . 1 https://github.com/jjh6297/UndercoverBias",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinhyeok Jang*",
      "ByungOk Han",
      "Jaehong Kim",
      "Chan-Hyun Youn"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3092_ECCV_2024_paper.php": {
    "title": "Pose-Aware Self-Supervised Learning with Viewpoint Trajectory Regularization",
    "volume": "main",
    "abstract": "Learning visual features from unlabeled images has proven successful for semantic categorization, often by mapping different views of the same object to the same feature to achieve recognition invariance. However, visual recognition involves not only identifying what an object is but also understanding how it is presented. For example, seeing a car from the side versus head-on is crucial for deciding whether to stay put or jump out of the way. While unsupervised feature learning for downstream viewpoint reasoning is important, it remains under-explored, partly due to the lack of a standardized evaluation method and benchmarks. We introduce a new dataset of adjacent image triplets obtained from a viewpoint trajectory, without any semantic or pose labels. We benchmark both semantic classification and pose estimation accuracies on the same visual feature. Additionally, we propose a viewpoint trajectory regularization loss for learning features from unlabeled image triplets. Our experiments demonstrate that this approach helps develop a visual representation that encodes object identity and organizes objects by their poses, retaining semantic classification accuracy while achieving emergent global pose awareness and better generalization to novel objects. Our dataset and code are available at http://pwang.pw/trajSSL/",
    "checked": true,
    "id": "b9e8a9b8f53a1a750463e32c6016a2f11d4a6287",
    "semantic_title": "pose-aware self-supervised learning with viewpoint trajectory regularization",
    "citation_count": 1,
    "authors": [
      "Jiayun Wang*",
      "Yubei Chen",
      "Stella X. Yu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3093_ECCV_2024_paper.php": {
    "title": "SILC: Improving Vision Language Pretraining with Self-Distillation",
    "volume": "main",
    "abstract": "Image-Text pretraining on web-scale image caption datasets has become the default recipe for open vocabulary classification and retrieval models thanks to the success of CLIP and its variants. Several works have also used CLIP features for dense prediction tasks and have shown the emergence of open-set abilities. However, the contrastive objective used by these models only focuses on image-text alignment and does not incentivise image feature learning for dense prediction tasks. In this work, we introduce SILC, a novel framework for vision language pretraining. SILC improves image-text contrastive learning with the simple addition of local-to-global correspondence learning by self-distillation. We show that distilling local image features from an exponential moving average (EMA) teacher model significantly improves model performance on dense predictions tasks like detection and segmentation, while also providing improvements on image-level tasks such as classification and retrieval. models sets a new state of the art for zero-shot classification, few shot classification, image and text retrieval, zero-shot segmentation, and open vocabulary segmentation. We further show that SILC features greatly benefit open vocabulary detection, captioning and visual question answering",
    "checked": true,
    "id": "5c18aca1f2f5cd8064024962b19d8ba3ec03311e",
    "semantic_title": "silc: improving vision language pretraining with self-distillation",
    "citation_count": 16,
    "authors": [
      "Muhammad Ferjad Naeem*",
      "Yongqin Xian",
      "Xiaohua Zhai",
      "Lukas Hoyer",
      "Luc Van Gool",
      "Federico Tombari"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3098_ECCV_2024_paper.php": {
    "title": "Learning Semantic Latent Directions for Accurate and Controllable Human Motion Prediction",
    "volume": "main",
    "abstract": "In the realm of stochastic human motion prediction (SHMP), researchers have often turned to generative models like GANS, VAEs and diffusion models. However, most previous approaches have struggled to accurately predict motions that are both realistic and coherent with past motion due to a lack of guidance on the latent distribution. In this paper, we introduce Semantic Latent Directions (SLD) as a solution to this challenge, aiming to constrain the latent space to learn meaningful motion semantics and enhance the accuracy of SHMP. SLD defines a series of orthogonal latent directions and represents the hypothesis of future motion as a linear combination of these directions. By creating such an information bottleneck, SLD excels in capturing meaningful motion semantics, thereby improving the precision of motion predictions. Moreover, SLD offers controllable prediction capabilities by adjusting the coefficients of the latent directions during the inference phase. Expanding on SLD, we introduce a set of motion queries to enhance the diversity of predictions. By aligning these motion queries with the SLD space, SLD is further promoted to more accurate and coherent motion predictions. Through extensive experiments conducted on widely used benchmarks, we showcase the superiority of our method in accurately predicting motions while maintaining a balance of realism and diversity. Our code and pretrained models are available at https://github.com/GuoweiXu368/SLD-HMP",
    "checked": true,
    "id": "20735467df9a3b544216e5adcfb7392b6fdb0fb0",
    "semantic_title": "learning semantic latent directions for accurate and controllable human motion prediction",
    "citation_count": 0,
    "authors": [
      "Guowei Xu",
      "Jiale Tao",
      "Wen Li*",
      "Lixin Duan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3099_ECCV_2024_paper.php": {
    "title": "Leveraging temporal contextualization for video action recognition",
    "volume": "main",
    "abstract": "We propose a novel framework for video understanding, called (), which leverages essential temporal information through global interactions in a spatio-temporal domain within a video. To be specific, we introduce Temporal Contextualization (TC), a layer-wise temporal information infusion mechanism for videos, which 1) extracts core information from each frame, 2) connects relevant information across frames for the summarization into context tokens, and 3) leverages the context tokens for feature encoding. Furthermore, the Video-conditional Prompting (VP) module processes context tokens to generate informative prompts in the text modality. Extensive experiments in zero-shot, few-shot, base-to-novel, and fully-supervised action recognition validate the effectiveness of our model. Ablation studies for TC and VP support our design choices. Our project page with the source code is available at https://github.com/naver-ai/tc-clip",
    "checked": true,
    "id": "723156908f9daa627d37feea8bbb666757200b60",
    "semantic_title": "leveraging temporal contextualization for video action recognition",
    "citation_count": 1,
    "authors": [
      "Minji Kim",
      "Dongyoon Han",
      "Taekyung Kim*",
      "Bohyung Han*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3114_ECCV_2024_paper.php": {
    "title": "ChEX: Interactive Localization and Region Description in Chest X-rays",
    "volume": "main",
    "abstract": "Report generation models offer fine-grained textual interpretations of medical images like chest X-rays, yet they often lack interactivity (the ability to steer the generation process through user queries) and localized interpretability (visually grounding their predictions), which we deem essential for future adoption in clinical practice. While there have been efforts to tackle these issues, they are either limited in their interactivity by not supporting textual queries or fail to also offer localized interpretability. Therefore, we propose a novel multitask architecture and training paradigm integrating textual prompts and bounding boxes for diverse aspects like anatomical regions and pathologies. We call this approach the Chest X-Ray Explainer (ChEX). Evaluations across a heterogeneous set of 9 chest X-ray tasks, including localized image interpretation and report generation, showcase its competitiveness with SOTA models while additional analysis demonstrates ChEX's interactive capabilities. Code: https://github. com/philip-mueller/chex",
    "checked": true,
    "id": "b1218e05bb299195c1e807edef286a8b85d3e0af",
    "semantic_title": "chex: interactive localization and region description in chest x-rays",
    "citation_count": 1,
    "authors": [
      "Philip Müller*",
      "Georgios Kaissis",
      "Daniel Rueckert"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3128_ECCV_2024_paper.php": {
    "title": "AdaGlimpse: Active Visual Exploration with Arbitrary Glimpse Position and Scale",
    "volume": "main",
    "abstract": "Active Visual Exploration (AVE) is a task that involves dynamically selecting observations (glimpses), which is critical to facilitate comprehension and navigation within an environment. While modern AVE methods have demonstrated impressive performance, they are constrained to fixed-scale glimpses from rigid grids. In contrast, existing mobile platforms equipped with optical zoom capabilities can capture glimpses of arbitrary positions and scales. To address this gap between software and hardware capabilities, we introduce . It uses Soft Actor-Critic, a reinforcement learning algorithm tailored for exploration tasks, to select glimpses of arbitrary position and scale. This approach enables our model to rapidly establish a general awareness of the environment before zooming in for detailed analysis. Experimental results demonstrate that surpasses previous methods across various visual tasks while maintaining greater applicability in realistic AVE scenarios",
    "checked": true,
    "id": "76848d697c975a0c216bb50712641d1cd927b51c",
    "semantic_title": "adaglimpse: active visual exploration with arbitrary glimpse position and scale",
    "citation_count": 0,
    "authors": [
      "Adam Pardyl*",
      "Michał Wronka",
      "Maciej Wołczyk",
      "Kamil Adamczewski",
      "Tomasz Trzcinski",
      "Bartosz Zieliński*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3131_ECCV_2024_paper.php": {
    "title": "CLAP: Isolating Content from Style through Contrastive Learning with Augmented Prompts",
    "volume": "main",
    "abstract": "Contrastive vision-language models, such as CLIP, have garnered considerable attention for various dowmsteam tasks, mainly due to the remarkable ability of the learned features for generalization. However, the features they learned often blend content and style information, which somewhat limits their generalization capabilities under distribution shifts. To address this limitation, we adopt a causal generative perspective for multimodal data and propose contrastive learning with data augmentation to disentangle content features from the original representations. To achieve this, we begin with exploring image augmentation techniques and develop a method to seamlessly integrate them into pre-trained CLIP-like models to extract pure content features. Taking a step further, recognizing the inherent semantic richness and logical structure of text data, we explore the use of text augmentation to isolate latent content from style features. This enables CLIP-like model's encoders to concentrate on latent content information, refining the learned representations by pre-trained CLIP-like models. Our extensive experiments across diverse datasets demonstrate significant improvements in zero-shot and few-shot classification tasks, alongside enhanced robustness to various perturbations. These results underscore the effectiveness of our proposed methods in refining vision-language representations and advancing the state-of-the-art in multimodal learning. 1 1 Our code is available at https://github.com/YichaoCai1/CLAP",
    "checked": true,
    "id": "3750af62488e39c0595d097e399c970187be3428",
    "semantic_title": "clap: isolating content from style through contrastive learning with augmented prompts",
    "citation_count": 1,
    "authors": [
      "Yichao Cai*",
      "Yuhang Liu",
      "Zhen Zhang",
      "Javen Qinfeng Shi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3141_ECCV_2024_paper.php": {
    "title": "ZigMa: A DiT-style Zigzag Mamba Diffusion Model",
    "volume": "main",
    "abstract": "The diffusion model has long been plagued by scalability and quadratic complexity issues, especially within transformer-based structures. In this study, we aim to leverage the long sequence modeling capability of a State-Space Model called Mamba to extend its applicability to visual data generation. Firstly, we identify a critical oversight in most current Mamba-based vision methods, namely the lack of consideration for spatial continuity in the scan scheme of Mamba. Secondly, building upon this insight, we introduce Zigzag Mamba, a simple, plug-and-play, minimal-parameter burden, DiT style solution, which outperforms Mamba-based baselines and demonstrates improved speed and memory utilization compared to transformer-based baselines, also this heterogeneous layerwise scan enables zero memory and speed burden when we consider more scan paths. Lastly, we integrate Zigzag Mamba with the Stochastic Interpolant framework to investigate the scalability of the model on large-resolution visual datasets, such as FacesHQ 1024 × 1024 and UCF101, MultiModal-CelebA-HQ, and MS COCO 256 × 256",
    "checked": true,
    "id": "0c17c30c7194c2d1d6c37a0f1c2cbd0c9f89899e",
    "semantic_title": "zigma: a dit-style zigzag mamba diffusion model",
    "citation_count": 14,
    "authors": [
      "Vincent Tao Hu*",
      "Stefan A Baumann",
      "Ming Gui",
      "Olga Grebenkova",
      "Pingchuan Ma",
      "Johannes S Fischer",
      "Bjorn Ommer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3146_ECCV_2024_paper.php": {
    "title": "EchoScene: Indoor Scene Generation via Information Echo over Scene Graph Diffusion",
    "volume": "main",
    "abstract": "We present EchoScene, an interactive and controllable generative model that generates 3D indoor scenes on scene graphs. EchoScene leverages a dual-branch diffusion model that dynamically adapts to scene graphs. Existing methods struggle to handle scene graphs due to varying numbers of nodes, multiple edge combinations, and manipulator-induced node-edge operations. EchoScene overcomes this by associating each node with a denoising process and enables collaborative information exchange, enhancing controllable and consistent generation aware of global constraints. This is achieved through an information echo scheme in both shape and layout branches. At every denoising step, all processes share their denoising data with an information exchange unit that combines these updates using graph convolution. The scheme ensures that the denoising processes are influenced by a holistic understanding of the scene graph, facilitating the generation of globally coherent scenes. The resulting scenes can be manipulated during inference by editing the input scene graph and sampling the noise in the diffusion model. Extensive experiments validate our approach, which maintains scene controllability and surpasses previous methods in generation fidelity. Moreover, the generated scenes are of high quality and thus directly compatible with off-the-shelf texture generation. Our code and models are open-sourced",
    "checked": true,
    "id": "2d37e13661002ddfda599acb97074284c6162cb7",
    "semantic_title": "echoscene: indoor scene generation via information echo over scene graph diffusion",
    "citation_count": 4,
    "authors": [
      "Guangyao Zhai*",
      "Evin Pınar Örnek",
      "Dave Zhenyu Chen",
      "Ruotong Liao",
      "Yan Di",
      "Nassir Navab",
      "Federico Tombari",
      "Benjamin Busam"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3148_ECCV_2024_paper.php": {
    "title": "On Calibration of Object Detectors: Pitfalls, Evaluation and Baselines",
    "volume": "main",
    "abstract": "Reliable usage of object detectors require them to be calibrated—a crucial problem that requires careful attention. Recent approaches towards this involve (1) designing new loss functions to obtain calibrated detectors by training them from scratch, and (2) post-hoc Temperature Scaling (TS) that learns to scale the likelihood of a trained detector to output calibrated predictions. These approaches are then evaluated based on a combination of Detection Expected Calibration Error (D-ECE) and Average Precision. In this work, via extensive analysis and insights, we highlight that these recent evaluation frameworks, evaluation metrics, and the use of TS have notable drawbacks leading to incorrect conclusions. As a step towards fixing these issues, we propose a principled evaluation framework to jointly measure calibration and accuracy of object detectors. We also tailor efficient and easy-to-use post-hoc calibration approaches such as Platt Scaling and Isotonic Regression specifically for object detection task. Contrary to the common notion, our experiments show that once designed and evaluated properly, post-hoc calibrators, which are extremely cheap to build and use, are much more powerful and effective than the recent train-time calibration methods. To illustrate, D-DETR with our post-hoc Isotonic Regression calibrator outperforms the recent train-time state-of-the-art calibration method Cal-DETR by more than 7 D-ECE on the COCO dataset. Additionally, we propose improved versions of the recently proposed Localization-aware ECE and show the efficacy of our method on these metrics. Code is available at: https://github.com/fiveai/detection_ calibration",
    "checked": true,
    "id": "60e6b1dc9edd2ebf6eea0523478b12604fed20e7",
    "semantic_title": "on calibration of object detectors: pitfalls, evaluation and baselines",
    "citation_count": 0,
    "authors": [
      "Selim Kuzucu*",
      "Kemal Oksuz*",
      "Jonathan Sadeghi",
      "Puneet Dokania"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3153_ECCV_2024_paper.php": {
    "title": "HAT: History-Augmented Anchor Transformer for Online Temporal Action Localization",
    "volume": "main",
    "abstract": "Online video understanding often relies on individual frames, leading to frame-by-frame predictions. Recent advancements such as Online Temporal Action Localization (OnTAL), extend this approach to instance-level predictions. However, existing methods mainly focus on short-term context, neglecting historical information. To address this, we introduce the History-Augmented Anchor Transformer (HAT) Framework for OnTAL. By integrating historical context, our framework enhances the synergy between long-term and short-term information, improving the quality of anchor features crucial for classification and localization. We evaluate our model on both procedural egocentric (PREGO) datasets (EGTEA and EPIC) and standard non-PREGO OnTAL datasets (THUMOS and MUSES). Results show that our model outperforms state-of-the-art approaches significantly on PREGO datasets and achieves comparable or slightly superior performance on non-PREGO datasets, underscoring the importance of leveraging long-term history, especially in procedural and egocentric action scenarios. Code is available at: https://github.com/sakibreza/ECCV24-HAT/",
    "checked": true,
    "id": "358ce51ba2448d8cbbf72e30839bad39cda0d8dc",
    "semantic_title": "hat: history-augmented anchor transformer for online temporal action localization",
    "citation_count": 0,
    "authors": [
      "Sakib Reza",
      "Yuexi Zhang",
      "Mohsen Moghaddam",
      "Octavia Camps*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3155_ECCV_2024_paper.php": {
    "title": "Deep Nets with Subsampling Layers Unwittingly Discard Useful Activations at Test-Time",
    "volume": "main",
    "abstract": "Subsampling layers play a crucial role in deep nets by discarding a portion of an activation map to reduce its spatial dimensions. This encourages the deep net to learn higher-level representations. Contrary to this motivation, we hypothesize that the discarded activations are useful and can be incorporated on the fly to improve models' prediction. To validate our hypothesis, we propose a search and aggregate method to find useful activation maps to be used at test-time. We applied our approach to the task of image classification and semantic segmentation. Extensive experiments over nine different architectures on ImageNet, CityScapes, and ADE20K show that our method consistently improves model test-time performance. Additionally, it complements existing test-time augmentation techniques to provide further performance gains",
    "checked": true,
    "id": "fd4c51aea178d7ddac7ebd86e758ea072d194216",
    "semantic_title": "deep nets with subsampling layers unwittingly discard useful activations at test-time",
    "citation_count": 0,
    "authors": [
      "Chiao-An Yang*",
      "Ziwei Liu",
      "Raymond Yeh"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3157_ECCV_2024_paper.php": {
    "title": "Safe-Sim: Safety-Critical Closed-Loop Traffic Simulation with Diffusion-Controllable Adversaries",
    "volume": "main",
    "abstract": "Evaluating the performance of autonomous vehicle planning algorithms necessitates simulating long-tail safety-critical traffic scenarios. However, traditional methods for generating such scenarios often fall short in terms of controllability and realism; they also neglect the dynamics of agent interactions. To address these limitations, we introduce , a novel diffusion-based controllable closed-loop safety-critical simulation framework. Our approach yields two distinct advantages: 1) generating realistic long-tail safety-critical scenarios that closely reflect real-world conditions, and 2) providing controllable adversarial behavior for more comprehensive and interactive evaluations. We develop a novel approach to simulate safety-critical scenarios through an adversarial term in the denoising process of diffusion models, which allows an adversarial agent to challenge a planner with plausible maneuvers while all agents in the scene exhibit reactive and realistic behaviors. Furthermore, we propose novel guidance objectives and a partial diffusion process that enables users to control key aspects of the scenarios, such as the collision type and aggressiveness of the adversarial agent, while maintaining the realism of the behavior. We validate our framework empirically using the nuScenes and nuPlan datasets across multiple planners, demonstrating improvements in both realism and controllability. These findings affirm that diffusion models provide a robust and versatile foundation for safety-critical, interactive traffic simulation, extending their utility across the broader autonomous driving landscape. Project website: https://safe-sim.github.io/",
    "checked": true,
    "id": "5afe4abb639f32eb491b8ba1155d75eff7869de4",
    "semantic_title": "safe-sim: safety-critical closed-loop traffic simulation with diffusion-controllable adversaries",
    "citation_count": 3,
    "authors": [
      "Wei-Jer Chang*",
      "Francesco Pittaluga",
      "Masayoshi Tomizuka",
      "Wei Zhan",
      "Manmohan Chandraker"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3170_ECCV_2024_paper.php": {
    "title": "Analysis-by-Synthesis Transformer for Single-View 3D Reconstruction",
    "volume": "main",
    "abstract": "Deep learning approaches have made significant success in single-view 3D reconstruction, but they often rely on expensive 3D annotations for training. Recent efforts tackle this challenge by adopting an analysis-by-synthesis paradigm to learn 3D reconstruction with only 2D annotations. However, existing methods face limitations in both shape reconstruction and texture generation. This paper introduces an innovative Analysis-by-Synthesis Transformer that addresses these limitations in a unified framework by effectively modeling pixel-to-shape and pixel-to-texture relationships. It consists of a Shape Transformer and a Texture Transformer. The Shape Transformer employs learnable shape queries to fetch pixel-level features from the image, thereby achieving high-quality mesh reconstruction and recovering occluded vertices. The Texture Transformer employs texture queries for non-local gathering of texture information and thus eliminates the incorrect inductive bias. Experimental results on CUB-200-2011 and ShapeNet datasets demonstrate superior performance in shape reconstruction and texture generation compared to previous methods. The code is available at https://github.com/DianJJ/AST",
    "checked": false,
    "id": "491bbfc1cfda90c645d4db485be071a7feadd27c",
    "semantic_title": "transformers as meta-learners for implicit neural representations",
    "citation_count": 47,
    "authors": [
      "Dian Jia",
      "Xiaoqian Ruan",
      "Kun Xia",
      "Zhiming Zou",
      "Le Wang",
      "Wei Tang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3171_ECCV_2024_paper.php": {
    "title": "Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine Unlearning",
    "volume": "main",
    "abstract": "The trustworthy machine learning (ML) community is increasingly recognizing the crucial need for models capable of selectively ‘unlearning' data points after training. This leads to the problem of machine unlearning (), aiming to eliminate the influence of chosen data points on model performance, while still maintaining the model's utility post-unlearning. Despite various methods for data influence erasure, evaluations have largely focused on random data forgetting, ignoring the vital inquiry into which subset should be chosen to truly gauge the authenticity of unlearning performance. To tackle this issue, we introduce a new evaluative angle for from an adversarial viewpoint. We propose identifying the data subset that presents the most significant challenge for influence erasure, i.e., pinpointing the worst-case forget set. Utilizing a bi-level optimization principle, we amplify unlearning challenges at the upper optimization level to emulate worst-case scenarios, while simultaneously engaging in standard training and unlearning at the lower level, achieving a balance between data influence erasure and model utility. Our proposal offers a worst-case evaluation of 's resilience and effectiveness. Through extensive experiments across different datasets (including CIFAR-10, 100, CelebA, Tiny ImageNet, and ImageNet) and models (including both image classifiers and generative models), we expose critical pros and cons in existing (approximate) unlearning strategies. Our results illuminate the complex challenges of in practice, guiding the future development of more accurate and robust unlearning algorithms. The code and supplementary material (appendix) are available at https://github.com/OPTML-Group/Unlearn-WorstCase",
    "checked": true,
    "id": "f01404f5fd84c468b11a9c0557167a8afac81d45",
    "semantic_title": "challenging forgets: unveiling the worst-case forget sets in machine unlearning",
    "citation_count": 14,
    "authors": [
      "Chongyu Fan",
      "Jiancheng Liu*",
      "Alfred Hero",
      "Sijia Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3174_ECCV_2024_paper.php": {
    "title": "WaSt-3D: Wasserstein-2 Distance for Scene-to-Scene Stylization on 3D Gaussians",
    "volume": "main",
    "abstract": "While style transfer techniques have been well-developed for 2D image stylization, the extension of these methods to 3D scenes remains relatively unexplored. Existing approaches demonstrate proficiency in transferring colors and textures but often struggle with replicating the geometry of the scenes. In our work, we leverage an explicit Gaussian Scale (GS) representation and directly match the distributions of Gaussians between style and content scenes using the Earth Mover's Distance (EMD). By employing the entropy-regularized Wasserstein-2 distance, we ensure that the transformation maintains spatial smoothness. Additionally, we decompose the scene stylization problem into smaller chunks to enhance efficiency. This paradigm shift reframes stylization from a pure generative process driven by latent space losses to an explicit matching of distributions between two Gaussian representations. Our method achieves high-resolution 3D stylization by faithfully transferring details from 3D style scenes onto the content scene. Furthermore, WaSt-3D consistently delivers results across diverse content and style scenes without necessitating any training, as it relies solely on optimization-based techniques. See our project page for additional results and source code: https://compvis.github.io/wast3d/",
    "checked": true,
    "id": "1772cd3ab5f29ae0bd9f29397b70451c39d96c49",
    "semantic_title": "wast-3d: wasserstein-2 distance for scene-to-scene stylization on 3d gaussians",
    "citation_count": 0,
    "authors": [
      "Dmytro Kotovenko*",
      "Olga Grebenkova*",
      "Nikolaos Sarafianos",
      "Avinash Paliwal",
      "Pingchuan Ma",
      "Omid Poursaeed",
      "Sreyas Mohan",
      "Yuchen Fan",
      "Yilei Li",
      "Rakesh Ranjan",
      "Bjorn Ommer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3178_ECCV_2024_paper.php": {
    "title": "SCLIP: Rethinking Self-Attention for Dense Vision-Language Inference",
    "volume": "main",
    "abstract": "Recent advances in contrastive language-image pretraining (CLIP) have demonstrated strong capabilities in zero-shot classification by aligning visual and textual features at an image level. However, in dense prediction tasks, CLIP often struggles to localize visual features within an image and fails to attain favorable pixel-level segmentation results. In this work, we investigate in CLIP's spatial reasoning mechanism and identify that its failure of dense prediction is caused by a location misalignment issue in the self-attention process. Based on this observation, we propose a training-free adaptation approach for CLIP's semantic segmentation, which only introduces a very simple modification to CLIP but can effectively address the issue of location misalignment. Specifically, we reform the self-attention mechanism with leveraging query-to-query and key-to-key similarity to determine attention scores. Remarkably, this minimal modification to CLIP significantly enhances its capability in dense prediction, improving the original CLIP's 14.1% average zero-shot mIoU over eight semantic segmentation benchmarks to 38.2%, and outperforming the existing SoTA's 33.9% by a large margin. Code is available at https://github.com/wangf3014/SCLIP",
    "checked": true,
    "id": "db26827a859ab3d63c84d151e6c2356c68560480",
    "semantic_title": "sclip: rethinking self-attention for dense vision-language inference",
    "citation_count": 17,
    "authors": [
      "Feng Wang*",
      "Jieru Mei",
      "Alan Yuille"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3184_ECCV_2024_paper.php": {
    "title": "Flying with Photons: Rendering Novel Views of Propagating Light",
    "volume": "main",
    "abstract": "We present an imaging and neural rendering technique that seeks to synthesize videos of light propagating through a scene from novel, moving camera viewpoints. Our approach relies on a new ultrafast imaging setup to capture a first-of-its kind, multi-viewpoint video dataset with picosecond-level temporal resolution. Combined with this dataset, we introduce an efficient neural volume rendering framework based on the transient field. This field is defined as a mapping from a 3D point and 2D direction to a high-dimensional, discrete-time signal that represents time-varying radiance at ultrafast timescales. Rendering with transient fields naturally accounts for effects due to the finite speed of light, including viewpoint-dependent appearance changes caused by light propagation delays to the camera. We render a range of complex effects, including scattering, specular reflection, refraction, and diffraction. Additionally, we demonstrate removing viewpoint-dependent propagation delays using a time warping procedure, rendering of relativistic effects, and video synthesis of direct and global components of light transport",
    "checked": true,
    "id": "58f552e4f34436c50bce031a7566ecaf564639ac",
    "semantic_title": "flying with photons: rendering novel views of propagating light",
    "citation_count": 2,
    "authors": [
      "Anagh Malik*",
      "Noah Juravsky",
      "Ryan Po",
      "Gordon Wetzstein",
      "Kiriakos N. Kutulakos",
      "David B. Lindell"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3186_ECCV_2024_paper.php": {
    "title": "RGNet: A Unified Clip Retrieval and Grounding Network for Long Videos",
    "volume": "main",
    "abstract": "Locating specific moments within long videos (20–120 minutes) presents a significant challenge, akin to finding a needle in a haystack. Adapting existing short video (5–30 seconds) grounding methods to this problem yields poor performance. Since most real-life videos, such as those on YouTube and AR/VR, are lengthy, addressing this issue is crucial. Existing methods typically operate in two stages: clip retrieval and grounding. However, this disjoint process limits the retrieval module's fine-grained event understanding, crucial for specific moment detection. We propose RGNet which deeply integrates clip retrieval and grounding into a single network capable of processing long videos into multiple granular levels, e.g., clips and frames. Its core component is a novel transformer encoder, RG-Encoder, that unifies the two stages through shared features and mutual optimization. The encoder incorporates a sparse attention mechanism and an attention loss to model both granularity jointly. Moreover, we introduce a contrastive clip sampling technique to mimic the long video paradigm closely during training. RGNet surpasses prior methods, showcasing state-of-the-art performance on long video temporal grounding (LVTG) datasets MAD and Ego4D. The code is released at https://github.com/Tanveer81/RGNet",
    "checked": true,
    "id": "89f3fa05dc9e925537be261700b3f4bcf8f45404",
    "semantic_title": "rgnet: a unified clip retrieval and grounding network for long videos",
    "citation_count": 0,
    "authors": [
      "Tanveer Hannan*",
      "Md Mohaiminul Islam",
      "Thomas Seidl",
      "Gedas Bertasius"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3187_ECCV_2024_paper.php": {
    "title": "MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images",
    "volume": "main",
    "abstract": "We introduce , an efficient model that, given sparse multi-view images as input, predicts clean feed-forward 3D Gaussians. To accurately localize the Gaussian centers, we build a cost volume representation via plane sweeping, where the cross-view feature similarities stored in the cost volume can provide valuable geometry cues to the estimation of depth. We also learn other Gaussian primitives' parameters jointly with the Gaussian centers while only relying on photometric supervision. We demonstrate the importance of the cost volume representation in learning feed-forward Gaussians via extensive experimental evaluations. On the large-scale RealEstate10K and ACID benchmarks, achieves state-of-the-art performance with the fastest feed-forward inference speed (22 fps). More impressively, compared to the latest state-of-the-art method pixelSplat, uses 10× fewer parameters and infers more than 2× faster while providing higher appearance and geometry quality as well as better cross-dataset generalization",
    "checked": true,
    "id": "fa0c7c850b6298fa49518e42e9f9e492dd4c5541",
    "semantic_title": "mvsplat: efficient 3d gaussian splatting from sparse multi-view images",
    "citation_count": 42,
    "authors": [
      "Yuedong Chen*",
      "Haofei Xu",
      "Chuanxia Zheng",
      "Bohan Zhuang",
      "Marc Pollefeys",
      "Andreas Geiger",
      "Tat-Jen Cham",
      "Jianfei Cai"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3191_ECCV_2024_paper.php": {
    "title": "3DGazeNet: Generalizing Gaze Estimation with Weak Supervision from Synthetic Views",
    "volume": "main",
    "abstract": "Developing gaze estimation models that generalize well to unseen domains and in-the-wild conditions remains a challenge with no known best solution. This is mostly due to the difficulty of acquiring ground truth data that cover the distribution of faces, head poses, and environments that exist in the real world. Most recent methods attempt to close the gap between specific source and target domains using domain adaptation. In this work, we propose to train general gaze estimation models which can be directly employed in novel environments without adaptation. To do so, we leverage the observation that head, body, and hand pose estimation benefit from revising them as dense 3D coordinate prediction, and similarly express gaze estimation as regression of dense 3D eye meshes. To close the gap between image domains, we create a large-scale dataset of diverse faces with gaze pseudo-annotations, which we extract based on the 3D geometry of the face, and design a multi-view supervision framework to balance their effect during training. We test our method in the task of gaze generalization, in which we demonstrate improvement of up to 23% compared to state-of-the-art when no ground truth data are available, and up to 10% when they are",
    "checked": false,
    "id": "8070c2be53ff7a6892464ca0a1d00f43d39ba63f",
    "semantic_title": "3dgazenet: generalizing gaze estimation with weak-supervision from synthetic views",
    "citation_count": 2,
    "authors": [
      "Evangelos Ververas*",
      "Polydefkis Gkagkos",
      "Jiankang Deng",
      "Michail C Doukas",
      "Jia Guo",
      "Stefanos Zafeiriou"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3192_ECCV_2024_paper.php": {
    "title": "Removing Distributional Discrepancies in Captions Improves Image-Text Alignment",
    "volume": "main",
    "abstract": "In this paper, we introduce a model designed to improve the prediction of image-text alignment, targeting the challenge of compositional understanding in current visual-language models. Our approach focuses on generating high-quality training datasets for the alignment task by producing mixed-type negative captions derived from positive ones. Critically, we address the distribution imbalance between positive and negative captions to ensure that the alignment model does not depend solely on textual information but also considers the associated images for predicting alignment accurately. By creating this enhanced training data, we fine-tune an existing leading visual-language model to boost its capability in understanding alignment. Our model significantly outperforms current top-performing methods across various datasets. We also demonstrate the applicability of our model by ranking the images generated by text-to-image models based on text alignment. Project page: https://yuheng-li.github. io/LLaVA-score/",
    "checked": true,
    "id": "4b08e1d742f3327e9784216d632aae1d3c16d4e0",
    "semantic_title": "removing distributional discrepancies in captions improves image-text alignment",
    "citation_count": 0,
    "authors": [
      "Mu Cai",
      "Haotian Liu",
      "Yuheng Li*",
      "Yijun Li",
      "Eli Shechtman",
      "Zhe Lin",
      "Yong Jae Lee",
      "Krishna Kumar Singh"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3193_ECCV_2024_paper.php": {
    "title": "Resilience of Entropy Model in Distributed Neural Networks",
    "volume": "main",
    "abstract": "Distributed have emerged as a key technique to reduce communication overhead without sacrificing performance in edge computing systems. Recently, entropy coding has been introduced to further reduce the communication overhead. The key idea is to train the distributed jointly with an entropy model, which is used as side information during inference time to adaptively encode latent representations into bit streams with variable length. To the best of our knowledge, the resilience of entropy models is yet to be investigated. As such, in this paper we formulate and investigate the resilience of entropy models to intentional interference (, adversarial attacks) and unintentional interference (, weather changes and motion blur). Through an extensive experimental campaign with 3 different architectures, 2 entropy models and 4 rate-distortion trade-off factors, we demonstrate that the entropy attacks can increase the communication overhead by up to 95%. By separating compression features in frequency and spatial domain, we propose a new defense mechanism that can reduce the transmission overhead of the attacked input by about 9% compared to unperturbed data, with only about 2% accuracy loss. Importantly, the proposed defense mechanism is a standalone approach which can be applied in conjunction with approaches such as adversarial training to further improve robustness. Code is available at https://github.com/Restuccia-Group/EntropyR",
    "checked": true,
    "id": "c03cd95d59c1fc25a3e462bf76af6d1aaef94af2",
    "semantic_title": "resilience of entropy model in distributed neural networks",
    "citation_count": 0,
    "authors": [
      "Milin Zhang*",
      "Mohammad Abdi",
      "Shahriar Rifat",
      "Francesco Restuccia"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3195_ECCV_2024_paper.php": {
    "title": "Rejection Sampling IMLE: Designing Priors for Better Few-Shot Image Synthesis",
    "volume": "main",
    "abstract": "An emerging area of research aims to learn deep generative models with limited training data. Implicit Maximum Likelihood Estimation (IMLE), a recent technique, successfully addresses the mode collapse issue of GANs and has been adapted to the few-shot setting, achieving state-of-the-art performance. However, current IMLE-based approaches encounter challenges due to inadequate correspondence between the latent codes selected for training and those drawn during inference. This results in suboptimal test-time performance. We theoretically show a way to address this issue and propose RS-IMLE, a novel approach that changes the prior distribution used for training. This leads to substantially higher quality image generation compared to existing GAN and IMLE-based methods, as validated by comprehensive experiments conducted on nine few-shot image datasets",
    "checked": true,
    "id": "54cb1dd5dab443746e3e9561157d64d0d0f81e77",
    "semantic_title": "rejection sampling imle: designing priors for better few-shot image synthesis",
    "citation_count": 0,
    "authors": [
      "Chirag Vashist*",
      "Shichong Peng",
      "Ke Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3200_ECCV_2024_paper.php": {
    "title": "Implicit Concept Removal of Diffusion Models",
    "volume": "main",
    "abstract": "Text-to-image (T2I) diffusion models often inadvertently generate unwanted concepts such as watermarks and unsafe images. These concepts, termed \"implicit concepts\", can be unintentionally learned during training and then be generated uncontrollably during inference. Existing removal methods still struggle to eliminate implicit concepts primarily due to their dependency on the model's ability to recognize concepts it actually can not discern. To address this, we utilize the intrinsic geometric characteristics of implicit concepts and present , a novel concept removal method based on geometric-driven control. Specifically, once an unwanted implicit concept is identified, we integrate the existence and geometric information of the concept into the text prompts with the help of an accessible classifier or detector model. Subsequently, the model is optimized to identify and disentangle this information, which is then adopted as negative prompts during generation. Moreover, we introduce the Implicit Concept Dataset (ICD), a novel image-text dataset imbued with three typical implicit concepts (QR codes, watermarks, and text), reflecting real-life situations where implicit concepts are easily injected. effectively mitigates the generation of implicit concepts, achieving state-of-the-art results on the Inappropriate Image Prompts (I2P) and our challenging Implicit Concept Dataset (ICD) benchmarks",
    "checked": true,
    "id": "37fbaa328e7867e4b63de070ee834a3fb510f666",
    "semantic_title": "implicit concept removal of diffusion models",
    "citation_count": 5,
    "authors": [
      "Zhili Liu*",
      "Kai Chen",
      "Yifan Zhang",
      "Jianhua Han",
      "Lanqing Hong",
      "Hang Xu",
      "Zhenguo Li",
      "Dit-Yan Yeung",
      "James Kwok"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3209_ECCV_2024_paper.php": {
    "title": "PLOT: Text-based Person Search with Part Slot Attention for Corresponding Part Discovery",
    "volume": "main",
    "abstract": "Text-based person search, employing free-form text queries to identify individuals within a vast image collection, presents a unique challenge in aligning visual and textual representations, particularly at the human part level. Existing methods often struggle with part feature extraction and alignment due to the lack of direct part-level supervision and reliance on heuristic features. We propose a novel framework that leverages a part discovery module based on slot attention to autonomously identify and align distinctive parts across modalities, enhancing interpretability and retrieval accuracy without explicit part-level correspondence supervision. Additionally, text-based dynamic part attention adjusts the importance of each part, further improving retrieval outcomes. Our method is evaluated on three public benchmarks, significantly outperforming existing methods",
    "checked": true,
    "id": "1a5d03a44274aa59c38dfbd59a04e71fd8d0004d",
    "semantic_title": "plot: text-based person search with part slot attention for corresponding part discovery",
    "citation_count": 0,
    "authors": [
      "Jicheol Park",
      "Dongwon Kim",
      "Boseung Jeong",
      "Suha Kwak*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3212_ECCV_2024_paper.php": {
    "title": "GS-LRM: Large Reconstruction Model for 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "We propose , a scalable large reconstruction model that can predict high-quality 3D Gaussian primitives from 2-4 posed sparse images in ∼0.23 seconds on single A100 GPU. Our model features a very simple transformer-based architecture; we patchify input posed images, pass the concatenated multi-view image tokens through a sequence of transformer blocks, and decode final per-pixel Gaussian parameters directly from these tokens for differentiable rendering. In contrast to previous LRMs that can only reconstruct objects, by predicting per-pixel Gaussians, naturally handles scenes with large variations in scale and complexity. We show that our model can work on both object and scene captures by training it on Objaverse and RealEstate10K respectively. In both scenarios, the models outperform state-of-the-art baselines by a wide margin. We also demonstrate applications of our model in downstream 3D generation tasks. Our project webpage is available at: https://sai-bi.github.io/project/gs-lrm/",
    "checked": true,
    "id": "8ed0477f640fa3a2d5411155e445d13752821629",
    "semantic_title": "gs-lrm: large reconstruction model for 3d gaussian splatting",
    "citation_count": 42,
    "authors": [
      "Kai Zhang*",
      "Sai Bi",
      "Hao Tan",
      "Yuanbo Xiangli",
      "Nanxuan Zhao",
      "Kalyan Sunkavalli",
      "Zexiang Xu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3220_ECCV_2024_paper.php": {
    "title": "Robust-Wide: Robust Watermarking against Instruction-driven Image Editing",
    "volume": "main",
    "abstract": "Instruction-driven image editing allows users to quickly edit an image according to text instructions in a forward pass. Nevertheless, malicious users can easily exploit this technique to create fake images, which could cause a crisis of trust and harm the rights of the original image owners. Watermarking is a common solution to trace such malicious behavior. Unfortunately, instruction-driven image editing can significantly change the watermarked image at the semantic level, making current state-of-the-art watermarking methods ineffective. To remedy it, we propose , the first robust watermarking methodology against instruction-driven image editing. Specifically, we follow the classic structure of deep robust watermarking, consisting of the encoder, noise layer, and decoder. To achieve robustness against semantic distortions, we introduce a novel Partial Instruction-driven Denoising Sampling Guidance () module, which consists of a large variety of instruction injections and substantial modifications of images at different semantic levels. With , the encoder tends to embed the watermark into more robust and semantic-aware areas, which remains in existence even after severe image editing. Experiments demonstrate that can effectively extract the watermark from the edited image with a low bit error rate of nearly 2.6% for 64-bit watermark messages. Meanwhile, it only induces a neglectable influence on the visual quality and editability of the original images. blackMoreover, holds general robustness against different sampling configurations and other popular image editing methods such as ControlNet-InstructPix2Pix, MagicBrush, Inpainting, and DDIM Inversion. Codes and models are available at https://github.com/hurunyi/Robust-Wide",
    "checked": true,
    "id": "3b2b503aa3dd80496c19fec4c73e31e5332f472a",
    "semantic_title": "robust-wide: robust watermarking against instruction-driven image editing",
    "citation_count": 1,
    "authors": [
      "Runyi Hu",
      "Jie Zhang*",
      "Ting Xu",
      "Jiwei Li",
      "Tianwei Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3228_ECCV_2024_paper.php": {
    "title": "OAPT: Offset-Aware Partition Transformer for Double JPEG Artifacts Removal",
    "volume": "main",
    "abstract": "Deep learning-based methods have shown remarkable performance in single JPEG artifacts removal task. However, existing methods tend to degrade on double JPEG images, which are prevalent in real-world scenarios. To address this issue, we propose Offset-Aware Partition Transformer for double JPEG artifacts removal, termed as OAPT. We conduct an analysis of double JPEG compression that results in up to four patterns within each 8×8 block and design our model to cluster the similar patterns to remedy the difficulty of restoration. Our OAPT consists of two components: compression offset predictor and image reconstructor. Specifically, the predictor estimates pixel offsets between the first and second compression, which are then utilized to divide different patterns. The reconstructor is mainly based on several Hybrid Partition Attention Blocks (HPAB), combining vanilla window-based self-attention and sparse attention for clustered pattern features. Extensive experiments demonstrate that OAPT outperforms the state-of-the-art method by more than 0.16dB in double JPEG image restoration task. Moreover, without increasing any computation cost, the pattern clustering module in HPAB can serve as a plugin to enhance other transformer-based image restoration methods. The code will be available at https://github.com/QMoQ/OAPT.git",
    "checked": true,
    "id": "b6dae388b0fa625f05f8b46d5b36064990ae8610",
    "semantic_title": "oapt: offset-aware partition transformer for double jpeg artifacts removal",
    "citation_count": 0,
    "authors": [
      "Qiao Mo",
      "Yukang Ding",
      "Jinhua Hao*",
      "Qiang Zhu",
      "Ming Sun",
      "Chao Zhou",
      "Feiyu Chen",
      "Shuyuan Zhu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3233_ECCV_2024_paper.php": {
    "title": "Formula-Supervised Visual-Geometric Pre-training",
    "volume": "main",
    "abstract": "Throughout the history of computer vision, while research has explored the integration of images (visual) and point clouds (geometric), many advancements in image and 3D object recognition have tended to process these modalities separately. We aim to bridge this divide by integrating images and point clouds on a unified transformer model. This approach integrates the modality-specific properties of images and point clouds and achieves fundamental downstream tasks in image and 3D object recognition on a unified transformer model by learning visual-geometric representations. In this work, we introduce Formula-Supervised Visual-Geometric Pre-training (FSVGP), a novel synthetic pre-training method that automatically generates aligned synthetic images and point clouds from mathematical formulas. Through cross-modality supervision, we enable supervised pre-training between visual and geometric modalities. FSVGP also reduces reliance on real data collection, cross-modality alignment, and human annotation. Our experimental results show that FSVGP pre-trains more effectively than VisualAtom and PC-FractalDB across six tasks: image and 3D object classification, detection, and segmentation. These achievements demonstrate FSVGP's superior generalization in image and 3D object recognition and underscore the potential of synthetic pre-training in visual-geometric representation learning. Our project website is available at https://ryosuke-yamada.github.io/fdsl-fsvgp/",
    "checked": true,
    "id": "c3b5fe0d03476b4f3eb9fa01d9b010d531ef9806",
    "semantic_title": "formula-supervised visual-geometric pre-training",
    "citation_count": 0,
    "authors": [
      "Ryosuke Yamada*",
      "Kensho Hara*",
      "Hirokatsu Kataoka",
      "Koshi Makihara",
      "Nakamasa Inoue",
      "Rio Yokota",
      "Yutaka Satoh"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3241_ECCV_2024_paper.php": {
    "title": "VideoAgent: A Memory-augmented Multimodal Agent for Video Understanding",
    "volume": "main",
    "abstract": "We explore how reconciling several foundation models (large language models and vision-language models) with a novel unified memory mechanism could tackle the challenging video understanding problem, especially capturing the long-term temporal relations in lengthy videos. In particular, the proposed multimodal agent : 1) constructs a structured memory to store both the generic temporal event descriptions and object-centric tracking states of the video; 2) given an input task query, it employs tools including video segment localization and object memory querying along with other visual foundation models to interactively solve the task, utilizing the zero-shot tool-use ability of LLMs. demonstrates impressive performances on several long-horizon video understanding benchmarks, an average increase of 6.6% on NExT-QA and 26.0% on EgoSchema over baselines, closing the gap between open-sourced models and private counterparts including Gemini 1.5 Pro. The code and demo can be found at https:// videoagent.github.io",
    "checked": true,
    "id": "8cac6e0e5c7c308b5eb33bcfccf8bfffbec9fd8e",
    "semantic_title": "videoagent: a memory-augmented multimodal agent for video understanding",
    "citation_count": 16,
    "authors": [
      "Yue Fan",
      "Xiaojian Ma*",
      "Rujie Wu",
      "yuntao du",
      "Jiaqi Li",
      "Zhi Gao",
      "Qing Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3248_ECCV_2024_paper.php": {
    "title": "Towards Unified Representation of Invariant-Specific Features in Missing Modality Face Anti-Spoofing",
    "volume": "main",
    "abstract": "The effectiveness of Vision Transformers (ViTs) diminishes considerably in multi-modal face anti-spoofing (FAS) under missing modality scenarios. Existing approaches rely on modality-invariant features to alleviate this issue but ignore modality-specific features. To solve this issue, we propose a Missing Modality Adapter framework for Face Anti-Spoofing (MMA-FAS), which leverages modality-disentangle adapters and LBP-guided contrastive loss for explicit combination of modality-invariant and modality-specific features. Modality-disentangle adapters disentangle features into modality-invariant and -specific features from the view of frequency decomposition. LBP-guided contrastive loss, together with batch-level and sample-level modality masking strategies, forces the model to cluster samples according to attack types and modal combinations, which further enhances modality-specific and -specific features. Moreover, we propose an adaptively modal combination sampling strategy, which dynamically adjusts the sample probability in masking strategies to balance the training process of different modal combinations. Extensive experiments demonstrate that our proposed method achieves state-of-the-art intra-dataset and cross-dataset performance in all the missing modality scenarios",
    "checked": true,
    "id": "69fa1fbb6b1f10c79ca0ae1c0c65b80b8e59bd65",
    "semantic_title": "towards unified representation of invariant-specific features in missing modality face anti-spoofing",
    "citation_count": 0,
    "authors": [
      "Guanghao Zheng",
      "Yuchen Liu",
      "Wenrui Dai*",
      "Chenglin Li",
      "Junni Zou",
      "Hongkai Xiong"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3249_ECCV_2024_paper.php": {
    "title": "Restoring Images in Adverse Weather Conditions via Histogram Transformer",
    "volume": "main",
    "abstract": "Transformer-based image restoration methods in adverse wea-ther have achieved significant progress. Most of them use self-attention along the channel dimension or within spatially fixed-range blocks to reduce computational load. However, such a compromise results in limitations in capturing long-range spatial features. Inspired by the observation that the weather-induced degradation factors mainly cause similar occlusion and brightness, in this work, we propose an efficient Histogram Transformer (Histoformer) for restoring images affected by adverse weather. It is powered by a mechanism dubbed histogram self-attention, which sorts and segments spatial features into intensity-based bins. Self-attention is then applied across bins or within each bin to selectively focus on spatial features of dynamic range and process similar degraded pixels of the long range together. To boost histogram self-attention, we present a dynamic-range convolution enabling conventional convolution to conduct operation over similar pixels rather than neighbor pixels. We also observe that the common pixel-wise losses neglect linear association and correlation between output and ground-truth. Thus, we propose to leverage the Pearson correlation coefficient as a loss function to enforce the recovered pixels following the identical order as ground-truth. Extensive experiments demonstrate the efficacy and superiority of our proposed method. We have released the codes in Github",
    "checked": true,
    "id": "4ac14d3721847c0779700edb41f7f0b1858150f8",
    "semantic_title": "restoring images in adverse weather conditions via histogram transformer",
    "citation_count": 4,
    "authors": [
      "Shangquan Sun",
      "Wenqi Ren*",
      "Xinwei Gao",
      "Rui Wang",
      "Xiaochun Cao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3251_ECCV_2024_paper.php": {
    "title": "PosFormer: Recognizing Complex Handwritten Mathematical Expression with Position Forest Transformer",
    "volume": "main",
    "abstract": "Handwritten Mathematical Expression Recognition (HMER) has wide applications in human-machine interaction scenarios, such as digitized education and automated offices. Recently, sequence-based models with encoder-decoder architectures have been commonly adopted to address this task by directly predicting LaTeX sequences of expression images. However, these methods only implicitly learn the syntax rules provided by LaTeX, which may fail to describe the position and hierarchical relationship between symbols due to complex structural relations and diverse handwriting styles. To overcome this challenge, we propose a position forest transformer (PosFormer) for HMER, which jointly optimizes two tasks: expression recognition and position recognition, to explicitly enable position-aware symbol feature representation learning. Specifically, we first design a position forest that models the mathematical expression as a forest structure and parses the relative position relationships between symbols. Without requiring extra annotations, each symbol is assigned a position identifier in the forest to denote its relative spatial position. Second, we propose an implicit attention correction module to accurately capture attention for HMER in the sequence-based decoder architecture. Extensive experiments validate the superiority of PosFormer, which consistently outperforms the state-of-the-art methods 2.03%/1.22%/2.00%, 1.83%, and 4.62% gains on the single-line CROHME 2014/2016/2019, multi-line M2 E, and complex MNE datasets, respectively, with no additional latency or computational cost",
    "checked": true,
    "id": "0ddbf327bbcc6ddd86feaf4490643c12c616ced2",
    "semantic_title": "posformer: recognizing complex handwritten mathematical expression with position forest transformer",
    "citation_count": 0,
    "authors": [
      "Tongkun Guan",
      "Chengyu Lin",
      "Wei Shen*",
      "Xiaokang Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3255_ECCV_2024_paper.php": {
    "title": "NGP-RT: Fusing Multi-Level Hash Features with Lightweight Attention for Real-Time Novel View Synthesis",
    "volume": "main",
    "abstract": "This paper presents NGP-RT, a novel approach for enhancing the rendering speed of Instant-NGP to achieve real-time novel view synthesis. As a classic NeRF-based method, Instant-NGP stores implicit features in multi-level grids or hash tables and applies a shallow MLP to convert the implicit features into explicit colors and densities. Although it achieves fast training speed, there is still a lot of room for improvement in its rendering speed due to the per-point MLP executions for implicit multi-level feature aggregation, especially for real-time applications. To address this challenge, our proposed NGP-RT explicitly stores colors and densities as hash features, and leverages a lightweight attention mechanism to disambiguate the hash collisions instead of using computationally intensive MLP. At the rendering stage, NGP-RT incorporates a pre-computed occupancy distance grid into the ray marching strategy to inform the distance to the nearest occupied voxel, thereby reducing the number of marching points and global memory access. Experimental results show that on the challenging Mip-NeRF 360 dataset, NGP-RT achieves better rendering quality than previous NeRF-based methods, achieving 108 fps at 1080p resolution on a single Nvidia RTX 3090 GPU. Our approach is promising for NeRF-based real-time applications that require efficient and high-quality rendering",
    "checked": true,
    "id": "10c271fd426ebb527e692a14de9b5f6990ed7666",
    "semantic_title": "ngp-rt: fusing multi-level hash features with lightweight attention for real-time novel view synthesis",
    "citation_count": 0,
    "authors": [
      "Yubin Hu",
      "Xiaoyang Guo",
      "Yang Xiao",
      "Jingwei Huang",
      "Yong-Jin Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3256_ECCV_2024_paper.php": {
    "title": "Elysium: Exploring Object-level Perception in Videos through Semantic Integration Using MLLMs",
    "volume": "main",
    "abstract": "Multi-modal Large Language Models (MLLMs) have demonstrated their ability to perceive objects in still images, but their application in video-related tasks, such as object tracking, remains understudied. This lack of exploration is primarily due to two key challenges. Firstly, extensive pretraining on large-scale video datasets is required to equip MLLMs with the capability to perceive objects across multiple frames and understand inter-frame relationships. Secondly, processing a large number of frames within the context window of Large Language Models (LLMs) can impose a significant computational burden. To address the first challenge, we introduce , a large-scale video dataset supported for three tasks: Single Object Tracking (SOT), Referring Single Object Tracking (RSOT), and Video Referring Expression Generation (Video-REG). contains 1.27 million annotated video frames with corresponding object boxes and descriptions. Leveraging this dataset, we conduct training of MLLMs and propose a token-compression model T-Selector to tackle the second challenge. Our proposed approach, Elysium: Exploring Object-level Perception in Videos via MLLM, is an end-to-end trainable MLLM that attempts to conduct object-level tasks in videos without requiring any additional plug-in or expert models. All codes and datasets are released at https://github.com/Hon-Wong/Elysium",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Wang*",
      "Yanjie Wang",
      "Ye Yongjie",
      "Yuxiang Nie",
      "Can Huang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3259_ECCV_2024_paper.php": {
    "title": "G2fR: Frequency Regularization in Grid-based Feature Encoding Neural Radiance Fields",
    "volume": "main",
    "abstract": "Neural Radiance Field (NeRF) methodologies have garnered considerable interest, particularly with the introduction of grid-based feature encoding (GFE) approaches such as Instant-NGP and TensoRF. Conventional NeRF employs positional encoding (PE) and represents a scene with a Multi-Layer Perceptron (MLP). Frequency regularization has been identified as an effective strategy to overcome primary challenges in PE-based NeRFs, including dependency on known camera poses and the requirement for extensive image datasets. While several studies have endeavored to extend frequency regularization to GFE approaches, there is still a lack of basic theoretical foundations for these methods. Therefore, we first clarify the underlying mechanisms of frequency regularization. Subsequently, we conduct a comprehensive investigation into the expressive capability of GFE-based NeRFs and attempt to connect frequency regularization with GFE methods. Moreover, we propose a generalized strategy, : Generalized Grid-based Frequency Regularization, to address issues of camera pose optimization and few-shot reconstruction with GFE methods. We validate the efficacy of our methods through an extensive series of experiments employing various representations across diverse scenarios",
    "checked": true,
    "id": "eb5b200e0dcb87c4e5d269a9c86ecbf8ddf5d1ea",
    "semantic_title": "g2fr: frequency regularization in grid-based feature encoding neural radiance fields",
    "citation_count": 0,
    "authors": [
      "Shuxiang Xie*",
      "Shuyi Zhou",
      "Ken Sakurada",
      "Ryoichi Ishikawa",
      "Masaki Onishi",
      "Takeshi Oishi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3260_ECCV_2024_paper.php": {
    "title": "Getting it Right: Improving Spatial Consistency in Text-to-Image Models",
    "volume": "main",
    "abstract": "One of the key shortcomings in current text-to-image (T2I) models is their inability to consistently generate images which faithfully follow the spatial relationships specified in the text prompt. In this paper, we offer a comprehensive investigation of this limitation, while also developing datasets and methods that support algorithmic solutions to improve spatial reasoning in T2I models. We find that spatial relationships are under-represented in the image descriptions found in current vision-language datasets. To alleviate this data bottleneck, we create SPRIGHT, the first spatially focused, large-scale dataset, by re-captioning 6 million images from 4 widely used vision datasets and through a 3-fold evaluation and analysis pipeline, show that SPRIGHT improves the proportion of spatial relationships in existing datasets. We show the efficacy of SPRIGHT data by showing that using only ∼0.25% of SPRIGHT results in a 22% improvement in generating spatially accurate images while also improving FID and CMMD scores. We also find that training on images containing a larger number of objects leads to substantial improvements in spatial consistency, including state-of-the-art results on T2I-CompBench with a spatial score of 0.2133, by fine-tuning on ¡500 images. Through a set of controlled experiments and ablations, we document additional findings that could support future work that seeks to understand factors that affect spatial consistency in text-to-image models. Project page : https://spright-t2i.github.io/",
    "checked": true,
    "id": "1c7fb5c8a8a866b84df46e42ba4a51c085a4c17c",
    "semantic_title": "getting it right: improving spatial consistency in text-to-image models",
    "citation_count": 7,
    "authors": [
      "Agneet Chatterjee*",
      "Gabriela Ben Melech Stan",
      "Estelle Guez Aflalo",
      "Sayak Paul",
      "Dhruba Ghosh",
      "Tejas Gokhale",
      "Ludwig Schmidt",
      "Hanna Hajishirzi",
      "Vasudev Lal",
      "Chitta R Baral",
      "Yezhou Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3261_ECCV_2024_paper.php": {
    "title": "Generating 3D House Wireframes with Semantics",
    "volume": "main",
    "abstract": "We present a new approach for generating 3D house wireframes with semantic enrichment using an autoregressive model. Unlike conventional generative models that independently process vertices, edges, and faces, our approach employs a unified wire-based representation for improved coherence in learning 3D wireframe structures. By re-ordering wire sequences based on semantic meanings, we facilitate seamless semantic integration during sequence generation. Our two-phase technique merges a graph-based autoencoder with a transformer-based decoder to learn latent geometric tokens and generate semantic-aware wireframes. Through iterative prediction and decoding during inference, our model produces detailed wireframes that can be easily segmented into distinct components, such as walls, roofs, and rooms, reflecting the semantic essence of the shape. Empirical results on a comprehensive house dataset validate the superior accuracy, novelty, and semantic fidelity of our model compared to existing generative models. More results and details can be found on https://vcc.tech/research/2024/3DWire",
    "checked": true,
    "id": "bcc2b2127f56427659c390e6aa417310ce06178d",
    "semantic_title": "generating 3d house wireframes with semantics",
    "citation_count": 0,
    "authors": [
      "Xueqi Ma",
      "Yilin Liu",
      "Wenjun Zhou",
      "Ruowei Wang",
      "Hui Huang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3265_ECCV_2024_paper.php": {
    "title": "GeoWizard: Unleashing the Diffusion Priors for 3D Geometry Estimation from a Single Image",
    "volume": "main",
    "abstract": "∗ Equal contributionWe introduce GeoWizard, a new generative foundation model designed for estimating geometric attributes, , depth and normals, from single images. While significant research has already been conducted in this area, the progress has been substantially limited by the low diversity and poor quality of publicly available datasets. As a result, the prior works either are constrained to limited scenarios or suffer from the inability to capture geometric details. In this paper, we demonstrate that generative models, as opposed to traditional discriminative models (, CNNs and Transformers), can effectively address the inherently ill-posed problem. We further show that leveraging diffusion priors can markedly improve generalization, detail preservation, and efficiency in resource usage. Specifically, we extend the original stable diffusion model to jointly predict depth and normal, allowing mutual information exchange and high consistency between the two representations. More importantly, we propose a simple yet effective strategy to segregate the complex data distribution of various scenes into distinct sub-distributions. This strategy enables our model to recognize different scene layouts, capturing 3D geometry with remarkable fidelity. GeoWizard sets new benchmarks for zero-shot depth and normal prediction, significantly enhancing many downstream applications such as 3D reconstruction, 2D content creation, and novel viewpoint synthesis",
    "checked": true,
    "id": "8092f6a18c525f7c6eacee2a842e80985b177e74",
    "semantic_title": "geowizard: unleashing the diffusion priors for 3d geometry estimation from a single image",
    "citation_count": 28,
    "authors": [
      "Xiao Fu*",
      "Wei Yin",
      "Mu Hu",
      "Kaixuan Wang",
      "Yuexin Ma",
      "Ping Tan",
      "Shaojie Shen",
      "Dahua Lin",
      "Xiaoxiao Long"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3268_ECCV_2024_paper.php": {
    "title": "Shape-guided Configuration-aware Learning for Endoscopic-image-based Pose Estimation of Flexible Robotic Instruments",
    "volume": "main",
    "abstract": "Accurate estimation of both the external orientation and internal bending angle is crucial for understanding a flexible robot state within its environment. However, existing sensor-based methods face limitations in cost, environmental constraints, and integration issues. Conventional image-based methods struggle with the shape complexity of flexible robots. In this paper, we propose a novel shape-guided configuration-aware learning framework for image-based flexible robot pose estimation. Inspired by the recent advances in 2D-3D joint representation learning, we leverage the 3D shape prior of the flexible robot to enhance its image-based shape representation. We first extract the part-level geometry representation of the 3D shape prior, then adapt this representation to the image by querying the image features corresponding to different robot parts. Furthermore, we present an effective mechanism to dynamically deform the shape prior. It aims to mitigate the shape difference between the adopted shape prior and the flexible robot depicted in the image. This more expressive shape guidance boosts the image-based robot representation and can be effectively used for flexible robot pose refinement. Extensive experiments on a general flexible robot designed for endoluminal surgery demonstrate the advantages of our method over a series of keypoint-based, skeleton-based and direct regression-based methods. Project homepage: https://poseflex.github.io/",
    "checked": true,
    "id": "4106f76774776ec9413075821d2710c60df9d043",
    "semantic_title": "shape-guided configuration-aware learning for endoscopic-image-based pose estimation of flexible robotic instruments",
    "citation_count": 0,
    "authors": [
      "Yiyao Ma*",
      "Kai Chen*",
      "Hon-Sing Tong",
      "Ruofeng Wei",
      "Yui-Lun Ng",
      "Ka-Wai Kwok*",
      "Qi Dou*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3272_ECCV_2024_paper.php": {
    "title": "Nonverbal Interaction Detection",
    "volume": "main",
    "abstract": "This work addresses a new challenge of understanding human nonverbal interaction in social contexts. Nonverbal signals pervade virtually every communicative act. Our gestures, facial expressions, postures, gaze, even physical appearance all convey messages, without anything being said. Despite their critical role in social life, nonverbal signals receive very limited attention as compared to the linguistic counterparts, and existing solutions typically examine nonverbal cues in isolation. Our study marks the first systematic effort to enhance the interpretation of multifaceted nonverbal signals. First, we contribute a novel large-scale dataset, called , which is meticulously annotated to include bounding boxes for humans and corresponding social groups, along with 22 atomic-level nonverbal behaviors under five broad interaction types. Second, we establish a new task for nonverbal interaction detection, which is formalized as identifying triplets in the form ⟨individual, group, interaction⟩ from images. Third, we propose a nonverbal interaction detection hypergraph (), a new approach that explicitly models high-order nonverbal interactions using hypergraphs. Central to the model is a dual multi-scale hypergraph that adeptly addresses individual-to-individual and group-to-group correlations across varying scales, facilitating interactional feature learning and eventually improving interaction prediction. Extensive experiments on show that improves various baselines significantly in . It also exhibits leading performance on HOI-DET, confirming its versatility in supporting related tasks and strong generalization ability. We hope that our study will offer the community new avenues to explore nonverbal signals in more depth",
    "checked": true,
    "id": "c87e2e6cc3c8760fb5398badfa30de5f01aa8274",
    "semantic_title": "nonverbal interaction detection",
    "citation_count": 2,
    "authors": [
      "Jianan Wei",
      "Tianfei Zhou",
      "Yi Yang",
      "Wenguan Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3274_ECCV_2024_paper.php": {
    "title": "UniM2AE: Multi-modal Masked Autoencoders with Unified 3D Representation for 3D Perception in Autonomous Driving",
    "volume": "main",
    "abstract": "Masked Autoencoders (MAE) play a pivotal role in learning potent representations, delivering outstanding results across various 3D perception tasks essential for autonomous driving. In real-world driving scenarios, it's commonplace to deploy multiple sensors for comprehensive environment perception. Despite integrating multi-modal features from these sensors can produce rich and powerful features, there is a noticeable challenge in MAE methods addressing this integration due to the substantial disparity between the different modalities. This research delves into multi-modal Masked Autoencoders tailored for a unified representation space in autonomous driving, aiming to pioneer a more efficient fusion of two distinct modalities. To intricately marry the semantics inherent in images with the geometric intricacies of LiDAR point clouds, we propose UniM2 AE. This model stands as a potent yet straightforward, multi-modal self-supervised pre-training framework, mainly consisting of two designs. First, it projects the features from both modalities into a cohesive 3D volume space to intricately marry the bird's eye view (BEV) with the height dimension. The extension allows for a precise representation of objects and reduces information loss when aligning multi-modal features. Second, the Multi-modal 3D Interactive Module (MMIM) is invoked to facilitate the efficient inter-modal interaction during the interaction process. Extensive experiments conducted on the nuScenes Dataset attest to the efficacy of UniM2 AE, indicating enhancements in 3D object detection and BEV map segmentation by 1.2% NDS and 6.5% mIoU, respectively. The code is available at https://github.com/hollow-503/UniM2AE",
    "checked": true,
    "id": "23698ecdb7fc23f222d6cfed41447bddf3317dd3",
    "semantic_title": "unim2ae: multi-modal masked autoencoders with unified 3d representation for 3d perception in autonomous driving",
    "citation_count": 3,
    "authors": [
      "Jian Zou",
      "Tianyu Huang",
      "Guanglei Yang*",
      "Zhenhua Guo",
      "Tao Luo*",
      "Chun-Mei Feng",
      "Wangmeng Zuo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3276_ECCV_2024_paper.php": {
    "title": "Responsible Visual Editing",
    "volume": "main",
    "abstract": "With the recent advancements in visual synthesis, there is a growing risk of encountering synthesized images with detrimental effects, such as hate, discrimination, and privacy violations. Unfortunately, it remains unexplored on how to avoid synthesizing harmful images and convert them into responsible ones. In this paper, we present responsible visual editing, which edits risky concepts within an image to more responsible ones with minimal content changes. However, the concepts that need to be edited are often abstract, making them hard to be located and edited. To tackle these challenges, we propose a Cognitive Editor (CoEditor) by harnessing the large multimodal models through a two-stage cognitive process: (1) a perceptual cognitive process to locate what to be edited and (2) a behavioral cognitive process to strategize how to edit. To mitigate the negative implications of harmful images on research, we build a transparent and public dataset, namely AltBear, which expresses harmful information using teddy bears instead of humans. Experiments demonstrate that CoEditor can effectively comprehend abstract concepts in complex scenes, significantly surpassing the baseline models for responsible visual editing. Moreover, we find that the AltBear dataset corresponds well to the harmful content found in real images, providing a safe and effective benchmark for future research. Our source code and dataset can be found at https://github.com/kodenii/ Responsible-Visual-Editing",
    "checked": true,
    "id": "ffb723fad01a2ff3dbe03bf2ecccd19f6dc03a40",
    "semantic_title": "responsible visual editing",
    "citation_count": 0,
    "authors": [
      "Minheng Ni",
      "Yeli Shen",
      "Lei Zhang*",
      "Wangmeng Zuo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3277_ECCV_2024_paper.php": {
    "title": "Drag Anything: Motion Control for Anything using Entity Representation",
    "volume": "main",
    "abstract": "We introduce , which utilizes a entity representation to achieve motion control for any object in controllable video generation. Comparison to existing motion control methods, offers several advantages. Firstly, trajectory-based is more user-friendly for interaction, when acquiring other guidance signals ( masks, depth maps) is labor-intensive. Users only need to draw a line (trajectory) during interaction. Secondly, our entity representation serves as an open-domain embedding capable of representing any object, enabling the control of motion for diverse entities, including background. Lastly, our entity representation allows simultaneous and distinct motion control for multiple objects. Extensive experiments demonstrate that our achieves state-of-the-art performance for FVD, FID, and User Study, particularly in terms of object motion control, where our method surpasses the previous methods ( DragNUWA) by 26% in human voting. The project website is at: blueDragAnything",
    "checked": false,
    "id": "e4cceb859cb9b29f816f04d876cef9a1a3f6b18a",
    "semantic_title": "draganything: motion control for anything using entity representation",
    "citation_count": 9,
    "authors": [
      "Weijia Wu ",
      "Zhuang Li",
      "Yuchao Gu",
      "Rui Zhao",
      "Yefei He",
      "David Junhao Zhang",
      "Mike Zheng Shou*",
      "Yan Li",
      "Tingting Gao",
      "Zhang Di"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3280_ECCV_2024_paper.php": {
    "title": "SegPoint: Segment Any Point Cloud via Large Language Model",
    "volume": "main",
    "abstract": "Despite significant progress in 3D point cloud segmentation, existing methods primarily address specific tasks and depend on explicit instructions to identify targets, lacking the capability to infer and understand implicit user intentions in a unified framework. In this work, we propose a model, called , that leverages the reasoning capabilities of a multi-modal Large Language Model (LLM) to produce point-wise segmentation masks across a diverse range of tasks: 1) 3D instruction segmentation, 2) 3D referring segmentation, 3) 3D semantic segmentation, and 4) 3D open-vocabulary semantic segmentation. To advance 3D instruction research, we introduce a new benchmark, , designed to evaluate segmentation performance from complex and implicit instructional texts, featuring point cloud-instruction pairs. Our experimental results demonstrate that achieves competitive performance on established benchmarks such as ScanRefer for referring segmentation and ScanNet for semantic segmentation, while delivering outstanding outcomes on the dataset. To our knowledge, is the first model to address these varied segmentation tasks within a single framework, achieving satisfactory performance",
    "checked": true,
    "id": "5d637c654deec76faba6ecb8b11b0617ac51c459",
    "semantic_title": "segpoint: segment any point cloud via large language model",
    "citation_count": 2,
    "authors": [
      "Shuting He",
      "Henghui Ding",
      "Xudong Jiang",
      "Bihan Wen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3285_ECCV_2024_paper.php": {
    "title": "Navigation Instruction Generation with BEV Perception and Large Language Models",
    "volume": "main",
    "abstract": "Navigation instruction generation, which requires embodied agents to describe the navigation routes, has been of great interest in robotics and human-computer interaction. Existing studies directly map the sequence of 2D perspective observations to route descriptions. Though straightforward, they overlook the geometric information and object semantics of the 3D environment. To address these challenges, we propose BEVInstructor, which incorporates Bird's Eye View (BEV) features into Multi-Modal Large Language Models (MLLMs) for instruction generation. Specifically, BEVInstructor constructs a Perspective-BEV Visual Encoder for the comprehension of 3D environments through fusing BEV and perspective features. To leverage the powerful language capabilities of MLLMs, the fused representations are used as visual prom-pts for MLLMs, and perspective-BEV prompt tuning is proposed for parameter-efficient updating. Based on the perspective-BEV prompts, BEVInstructor further adopts an instance-guided iterative refinement pipeline, which improves the instructions in a progressive manner. BEVInstructor achieves impressive performance across diverse datasets (, R2R, REVERIE, and UrbanWalk)",
    "checked": true,
    "id": "c65c7ed27008ebe64d84d301257b0ced5c416609",
    "semantic_title": "navigation instruction generation with bev perception and large language models",
    "citation_count": 2,
    "authors": [
      "Sheng Fan",
      "Rui Liu",
      "Wenguan Wang*",
      "Yi Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3287_ECCV_2024_paper.php": {
    "title": "Rebalancing Using Estimated Class Distribution for Imbalanced Semi-Supervised Learning under Class Distribution Mismatch",
    "volume": "main",
    "abstract": "Despite significant advancements in class-imbalanced semi-supervised learning (CISSL), many existing algorithms explicitly or implicitly assume that the class distribution of unlabeled data matches that of labeled data. However, when this assumption fails in practice, the classification performance of such algorithms may degrade due to incorrectly assigned weight to each class during training. We propose a novel CISSL algorithm called Rebalancing Using Estimated Class Distribution (RECD). RECD estimates the unknown class distribution of unlabeled data through Monte Carlo approximation, leveraging predicted class probabilities for unlabeled samples, and subsequently rebalances the classifier based on the estimated class distribution. Additionally, we propose an extension of feature clusters compression in the context of CISSL to mitigate feature map imbalance by densifying minority class clusters. Experimental results on four benchmark datasets demonstrate that RECD achieves state-of-the-art classification performance in CISSL",
    "checked": true,
    "id": "3e7761c0d347ece2f0a86b95e1cf60efed5316d0",
    "semantic_title": "rebalancing using estimated class distribution for imbalanced semi-supervised learning under class distribution mismatch",
    "citation_count": 0,
    "authors": [
      "Taemin Park",
      "Hyuck Lee",
      "Heeyoung Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3293_ECCV_2024_paper.php": {
    "title": "Vista3D: unravel the 3d darkside of a single image",
    "volume": "main",
    "abstract": "We embark on the age-old quest: unveiling the hidden dimensions of objects from mere glimpses of their visible parts. To address this, we present Vista3D, a framework that realizes swift and consistent 3D generation within a mere 5 minutes. At the heart of Vista3D lies a two-phase approach: the coarse phase and the fine phase. In the coarse phase, we rapidly generate initial geometry with Gaussian Splatting from a single image. In the fine phase, we extract a Signed Distance Function (SDF) directly from learned Gaussian Splatting, optimizing it with a differentiable isosurface representation. Furthermore, it elevates the quality of generation by using a disentangled representation with two independent implicit functions to capture both visible and obscured aspects of objects. Additionally, it harmonizes gradients from 2D diffusion prior with 3D-aware diffusion priors by angular diffusion prior composition. Through extensive evaluation, we demonstrate that Vista3D effectively sustains a balance between the consistency and diversity of the generated 3D objects. Demos and code will be available at https://github.com/florinshen/Vista3D",
    "checked": true,
    "id": "224f213abce79dc6fd631421abd88b8180704eef",
    "semantic_title": "vista3d: unravel the 3d darkside of a single image",
    "citation_count": 0,
    "authors": [
      "Qiuhong Shen",
      "Xingyi Yang",
      "Michael Bi Mi",
      "Xinchao Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3294_ECCV_2024_paper.php": {
    "title": "The Fabrication of Reality and Fantasy: Scene Generation with LLM-Assisted Prompt Interpretation",
    "volume": "main",
    "abstract": "In spite of recent advancements in text-to-image generation, limitations persist in handling complex and imaginative prompts due to the restricted diversity and complexity of training data. This work explores how diffusion models can generate images from prompts requiring artistic creativity or specialized knowledge. We introduce the Realistic-Fantasy Benchmark (RFBench), a novel evaluation framework blending realistic and fantastical scenarios. To address these challenges, we propose the Realistic-Fantasy Network (RFNet), a training-free approach integrating diffusion models with LLMs. Extensive human evaluations and GPT-based compositional assessments demonstrate our approach's superiority over state-of-the-art methods. Our code and dataset is available at https: //leo81005.github.io/Reality-and-Fantasy/",
    "checked": true,
    "id": "9ed9ee4b06249147adc5bee1aabaf86cb309bdc4",
    "semantic_title": "the fabrication of reality and fantasy: scene generation with llm-assisted prompt interpretation",
    "citation_count": 1,
    "authors": [
      "Yi Yao",
      "Chan-Feng Hsu*",
      "Jhe-Hao Lin",
      "Hongxia Xie",
      "Terence Lin",
      "Yi-Ning Huang",
      "Hong-Han Shuai*",
      "Wen-Huang Cheng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3298_ECCV_2024_paper.php": {
    "title": "Detecting As Labeling: Rethinking LiDAR-camera Fusion in 3D Object Detection",
    "volume": "main",
    "abstract": "3D object Detection with LiDAR-camera encounters overfitting in algorithm development derived from violating some fundamental rules. We refer to the data annotation in dataset construction for theoretical optimization and argue that the regression task prediction should not involve the feature from the camera branch. Following the cutting-edge perspective of 'Detecting As Labeling', we propose a novel paradigm dubbed DAL. With the most classical elementary algorithms, a simple predicting pipeline is constructed by imitating the data annotation process. Then we train it in the simplest way to minimize its dependency and strengthen its portability. Though simple in construction and training, the proposed DAL paradigm not only substantially pushes the performance boundary but also provides a superior trade-off between speed and accuracy among all existing methods. With comprehensive superiority, DAL is an ideal baseline for both future work development and practical deployment. The code has been released to facilitate future work https://github.com/ HuangJunJie2017/BEVDet",
    "checked": true,
    "id": "61836a372e3ba027ac35c762cda265c3c8e11f29",
    "semantic_title": "detecting as labeling: rethinking lidar-camera fusion in 3d object detection",
    "citation_count": 9,
    "authors": [
      "Junjie Huang*",
      "Yun Ye",
      "Zhujin Liang",
      "Yi Shan",
      "Dalong Du"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3300_ECCV_2024_paper.php": {
    "title": "FlashSplat: 2D to 3D Gaussian Splatting Segmentation Solved Optimally",
    "volume": "main",
    "abstract": "This study addresses the challenge of accurately segmenting 3D Gaussian Splatting (3D-GS) from 2D masks. Conventional methods often rely on iterative gradient descent to assign each Gaussian a unique label, leading to lengthy optimization and sub-optimal solutions. Instead, we propose a straightforward yet globally optimal solver for 3D-GS segmentation. The core insight of our method is that, with a reconstructed 3D-GS scene , the rendering of the 2D masks is essentially a linear function with respect to the labels of each Gaussian. As such, the optimal label assignment can be solved via linear programming in closed form. This solution capitalizes on the alpha blending characteristic of the splatting process for single step optimization. By incorporating the background bias in our objective function, our method shows superior robustness in 3D segmentation against noises. Remarkably, our optimization completes within 30 seconds, about 50× faster than the best existing methods. Extensive experiments demonstrate our method's efficiency and robustness in segmenting various scenes, and its superior performance in downstream tasks such as object removal and inpainting. Demos and code will be available at https://github.com/florinshen/FlashSplat",
    "checked": true,
    "id": "c9077bab68d54ae613f686a4b5b814b715e49e91",
    "semantic_title": "flashsplat: 2d to 3d gaussian splatting segmentation solved optimally",
    "citation_count": 1,
    "authors": [
      "Qiuhong Shen",
      "Xingyi Yang",
      "Xinchao Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3302_ECCV_2024_paper.php": {
    "title": "Exploiting Dual-Correlation for Multi-frame Time-of-Flight Denoising",
    "volume": "main",
    "abstract": "Recent advancements in Time-of-Flight (ToF) depth denoising have achieved impressive results in removing Multi-Path Interference (MPI) and shot noise. However, existing methods only utilize a single frame of ToF data, neglecting the correlation between frames. In this paper, we propose the first learning-based framework for multi-frame ToF denoising. Different from existing methods, our framework leverages the correlation between neighboring frames to guide ToF noise removal with a confidence map. Specifically, we introduce a Dual-Correlation Estimation Module, which exploits both intra- and inter-correlation. The intra-correlation explicitly establishes the relevance between the spatial positions of geometric objects within the scene, aiding in depth residual initialization. The inter-correlation discerns variations in ToF noise distribution across different frames, thereby locating the regions with strong ToF noise. To further leverage dual-correlation, we introduce a Confidence-guided Residual Regression Module to predict a confidence map, which guides the residual regression to prioritize the regions with strong ToF noise. The experimental evaluations have consistently shown that our framework outperforms existing ToF denoising methods, highlighting its superior performance in effectively reducing strong ToF noise. The source code is available at https://github.com/gtdong-ustc/multi-frame-tof-denoising",
    "checked": true,
    "id": "5ca7a5194be0ed2e7c1f0afb7eda265364384379",
    "semantic_title": "exploiting dual-correlation for multi-frame time-of-flight denoising",
    "citation_count": 0,
    "authors": [
      "Guanting Dong*",
      "Yueyi Zhang*",
      "Xiaoyan Sun",
      "Zhiwei Xiong"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3307_ECCV_2024_paper.php": {
    "title": "Weak-to-Strong Compositional Learning from Generative Models for Language-based Object Detection",
    "volume": "main",
    "abstract": "Vision-language (VL) models often exhibit a limited understanding of complex expressions of visual objects (, attributes, shapes, and their relations), given complex and diverse language queries. Traditional approaches attempt to improve VL models using hard negative synthetic text, but their effectiveness is limited. In this paper, we harness the exceptional compositional understanding capabilities of generative foundational models. We introduce a novel method for structured synthetic data generation aimed at enhancing the compositional understanding of VL models in language-based object detection. Our framework generates densely paired positive and negative triplets (image, text descriptions, and bounding boxes) in both image and text domains. By leveraging these synthetic triplets, we transform ‘weaker' VL models into ‘stronger' models in terms of compositional understanding, a process we call \"Weak-to-Strong Compositional Learning\" (WSCL). To achieve this, we propose a new compositional contrastive learning formulation that discovers semantics and structures in complex descriptions from synthetic triplets. As a result, VL models trained with our synthetic data generation exhibit a significant performance boost in the Omnilabel benchmark by up to +5AP and the D3 benchmark by +6.9AP upon existing baselines",
    "checked": true,
    "id": "e1bc831f24850bac1236b6cbaf79c14c4e3fcce8",
    "semantic_title": "weak-to-strong compositional learning from generative models for language-based object detection",
    "citation_count": 0,
    "authors": [
      "Kwanyong Park",
      "Kuniaki Saito",
      "Donghyun Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3309_ECCV_2024_paper.php": {
    "title": "Domesticating SAM for Breast Ultrasound Image Segmentation via Spatial-frequency Fusion and Uncertainty Correction",
    "volume": "main",
    "abstract": "Breast ultrasound image segmentation is a challenging task due to the low contrast and blurred boundary between the breast mass and the background. Our goal is to utilize the powerful feature extraction capability of segment anything model (SAM) and make out-of-domain tuning to help SAM distinguish breast masses from background. To this end, we propose a novel model called SF RecSAM , which inherits the model architecture of SAM but makes improvements to adapt to breast ultrasound image segmentation. First, we propose a spatial-frequency feature fusion module, which utilizes the fused spatial-frequency features to obtain a more comprehensive feature representation. This fusion feature is used to make up for the shortcomings of SAM's ViT image encoder in extracting low-level feature of masses. It complements the texture details and boundary structure information of masses to better segment targets in low contrast ultrasound images. Second, we propose a dual false corrector, which identifies and corrects false positive and false negative regions using uncertainty estimation, to further improve the segmentation accuracy. Extensive experiments demonstrate that the proposed method significantly outperforms state-of-the-art methods on two representative public breast ultrasound datasets: BUSI and UDIAT. Codes is available at https://github.com/dodooo1/SFRecSAM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wanting Zhang",
      "Huisi Wu*",
      "Jing Qin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3322_ECCV_2024_paper.php": {
    "title": "CanonicalFusion: Generating Drivable 3D Human Avatars from Multiple Images",
    "volume": "main",
    "abstract": "We present a novel framework for reconstructing animatable human avatars from multiple images, termed CanonicalFusion. Our central concept involves integrating individual reconstruction results into the canonical space. To be specific, we first predict Linear Blend Skinning (LBS) weight maps and depth maps using a shared-encoder-dual-decoder network, enabling direct canonicalization of the 3D mesh from the predicted depth maps. Here, instead of predicting high-dimensional skinning weights, we infer compressed skinning weights, i.e., 3-dimensional vector, with the aid of pre-trained MLP networks. We also introduce a forward skinning-based differentiable rendering scheme to merge the reconstructed results from multiple images. This scheme refines the initial mesh by reposing the canonical mesh via the forward skinning and by minimizing photometric and geometric errors between the rendered and the predicted results. Our optimization scheme considers the position and color of vertices as well as the joint angles for each image, thereby mitigating the negative effects of pose errors. We conduct extensive experiments to demonstrate the effectiveness of our method and compare our CanonicalFusion with state-of-the-art methods. Our source codes are available at https://github.com/jsshin98/CanonicalFusion",
    "checked": true,
    "id": "e9d52c2dc667cac59093324baaa619c49cadd151",
    "semantic_title": "canonicalfusion: generating drivable 3d human avatars from multiple images",
    "citation_count": 0,
    "authors": [
      "Jisu Shin",
      "Junmyeong Lee",
      "Seongmin Lee",
      "Min-Gyu Park",
      "Jumi Kang",
      "Ju Hong Yoon",
      "Hae-Gon Jeon*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3328_ECCV_2024_paper.php": {
    "title": "Camera Height Doesn't Change: Unsupervised Training for Metric Monocular Road-Scene Depth Estimation",
    "volume": "main",
    "abstract": "In this paper, we introduce a novel training method for making any monocular depth network learn absolute scale and estimate metric road-scene depth just from regular training data, , driving videos. We refer to this training framework as FUMET. The key idea is to leverage cars found on the road as sources of scale supervision and to incorporate them in network training robustly. FUMET detects and estimates the sizes of cars in a frame and aggregates scale information extracted from them into an estimate of the camera height whose consistency across the entire video sequence is enforced as scale supervision. This realizes robust unsupervised training of any, otherwise scale-oblivious, monocular depth network so that they become not only scale-aware but also metric-accurate without the need for auxiliary sensors and extra supervision. Extensive experiments on the KITTI and the Cityscapes datasets show the effectiveness of FUMET, which achieves state-of-the-art accuracy. We also show that FUMET enables training on mixed datasets of different camera heights, which leads to larger-scale training and better generalization. Metric depth reconstruction is essential in any road-scene visual modeling, and FUMET democratizes its deployment by establishing the means to convert any model into a metric depth estimator",
    "checked": true,
    "id": "a8321c6090ed84b56ea01e1d60fdf3b224b2656d",
    "semantic_title": "camera height doesn't change: unsupervised training for metric monocular road-scene depth estimation",
    "citation_count": 0,
    "authors": [
      "Genki Kinoshita*",
      "Ko Nishino"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3330_ECCV_2024_paper.php": {
    "title": "Uni3DL: A Unified Model for 3D Vision-Language Understanding",
    "volume": "main",
    "abstract": "We present Uni3DL, a unified model for 3D Vision-Language understanding. Distinct from existing unified 3D vision-language models that mostly rely on projected multi-view images and support limited tasks, Uni3DL operates directly on point clouds and significantly broadens the spectrum of tasks in the 3D domain, encompassing both vision and vision-language tasks. At the core of Uni3DL, a query transformer is designed to learn task-agnostic semantic and mask outputs by attending to 3D visual features, and a task router is employed to selectively produce task-specific outputs required for diverse tasks. With a unified architecture, our Uni3DL model enjoys seamless task decomposition and substantial parameter sharing across tasks. Uni3DL has been rigorously evaluated across diverse 3D vision-language understanding tasks, including semantic segmentation, object detection, instance segmentation, visual grounding, 3D captioning, and text-3D cross-modal retrieval. It demonstrates performance on par with or surpassing state-of-the-art (SOTA) task-specific models. We hope our benchmark and Uni3DL model will serve as a solid step to ease future research in unified models in the realm of 3D vision-language understanding. Project page: https://uni3dl.github.io/",
    "checked": false,
    "id": "574d3ea2e24c1857fb2bec0a4cec9a925800cf36",
    "semantic_title": "uni3dl: unified model for 3d and language understanding",
    "citation_count": 3,
    "authors": [
      "Xiang Li*",
      "Jian Ding",
      "Zhaoyang Chen",
      "Mohamed Elhoseiny"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3338_ECCV_2024_paper.php": {
    "title": "Object-Aware NIR-to-Visible Translation",
    "volume": "main",
    "abstract": "While near-infrared (NIR) imaging is essential for assisted driving and safety monitoring systems, its monochromatic nature hinders its broader application, which prompts the development of NIR-to-visible translation tasks. However, the performance of existing translation methods is limited by the neglected disparities between NIR and visible imaging and the lack of paired training data. To address these challenges, we propose a novel object-aware framework for NIR-to-visible translation. Our approach decomposes the visible image recovery into object-independent luminance sources and object-specific reflective components, processing them separately to bridge the gap between NIR and visible imaging under various lighting conditions. Leveraging prior segmentation knowledge enhances our model's ability to identify and understand the separated object reflection. We also collect the Fully Aligned NIR-Visible Image Dataset, a large-scale dataset comprising fully matched pairs of NIR and visible images captured with a multi-sensor coaxial camera. Empirical evaluations demonstrate our method's superiority over existing methods, producing visually compelling results on mainstream datasets. Code is accessible at: https://github.com/Yiiclass/Sherry",
    "checked": false,
    "id": "bfae1d9b4c4f99ee0c3e4a6e4dee776d48cd0aa9",
    "semantic_title": "tirdet: mono-modality thermal infrared object detection based on prior thermal-to-visible translation",
    "citation_count": 4,
    "authors": [
      "Yunyi Gao",
      "Lin Gu",
      "Qiankun Liu",
      "Ying Fu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3341_ECCV_2024_paper.php": {
    "title": "PaPr: Training-Free One-Step Patch Pruning with Lightweight ConvNets for Faster Inference",
    "volume": "main",
    "abstract": "As deep neural networks evolve from convolutional neural networks (ConvNets) to advanced vision transformers (ViTs), there is an increased need to eliminate redundant data for faster processing without compromising accuracy. Previous methods are often architecture-specific or necessitate re-training, restricting their applicability with frequent model updates. To solve this, we first introduce a novel property of lightweight ConvNets: their ability to identify key discriminative patch regions in images, irrespective of model's final accuracy or size. We demonstrate that fully-connected layers are the primary bottleneck for ConvNets performance, and their suppression with simple weight recalibration markedly enhances discriminative patch localization performance. Using this insight, we introduce PaPr, a method for substantially pruning redundant patches with minimal accuracy loss using lightweight ConvNets across a variety of deep learning architectures, including ViTs, ConvNets, and hybrid transformers, without any re-training. Moreover, the simple early-stage one-step patch pruning with PaPr enhances existing patch reduction methods. Through extensive testing on diverse architectures, PaPr achieves significantly higher accuracy over state-of-the-art patch reduction methods with similar FLOP count reduction. More specifically, PaPr reduces about 70% of redundant patches in videos with less than 0.8% drop in accuracy, and up to 3.7× FLOPs reduction, which is a 15% more reduction with 2.5% higher accuracy. Code is available at https://github.com/tanvir-utexas/PaPr",
    "checked": true,
    "id": "83ef4430cc8f4d825502aa157a31d45c5e49e1dd",
    "semantic_title": "papr: training-free one-step patch pruning with lightweight convnets for faster inference",
    "citation_count": 2,
    "authors": [
      "Tanvir Mahmud*",
      "Burhaneddin Yaman",
      "Chun-Hao Liu",
      "Diana Marculescu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3355_ECCV_2024_paper.php": {
    "title": "GENIXER: Empowering Multimodal Large Language Models as a Powerful Data Generator",
    "volume": "main",
    "abstract": "Multimodal Large Language Models (MLLMs) demonstrate exceptional problem-solving capabilities, but few research studies aim to gauge the ability to generate visual instruction tuning data. This paper proposes to explore the potential of empowering MLLMs to generate data independently without relying on GPT-4. We introduce , a comprehensive data generation pipeline consisting of four key steps: (i) instruction data collection, (ii) instruction template design, (iii) empowering MLLMs, and (iv) data generation and filtering. Additionally, we outline two modes of data generation: task-agnostic and task-specific, enabling controllable output. We demonstrate that a synthetic VQA-like dataset trained with LLaVA1.5 enhances performance on 10 out of 12 multimodal benchmarks. Additionally, the grounding MLLM Shikra, when trained with a REC-like synthetic dataset, shows improvements on 7 out of 8 REC datasets. Through experiments and synthetic data analysis, our findings are: (1) current MLLMs can serve as robust data generators without assistance from GPT-4V; (2) MLLMs trained with task-specific datasets can surpass GPT-4V in generating complex instruction tuning data; (3) synthetic datasets enhance performance across various multimodal benchmarks and help mitigate model hallucinations. The code can be found at https://github.com/zhaohengyuan1/Genixer",
    "checked": true,
    "id": "cb2295766b2f8f35524f6a9f93ae39d948d50bd4",
    "semantic_title": "genixer: empowering multimodal large language models as a powerful data generator",
    "citation_count": 3,
    "authors": [
      "Henry Hengyuan Zhao*",
      "Pan Zhou*",
      "Mike Zheng Shou*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3356_ECCV_2024_paper.php": {
    "title": "BLINK: Multimodal Large Language Models Can See but Not Perceive",
    "volume": "main",
    "abstract": "We introduce , a new benchmark for multimodal language models (LLMs) that focuses on core visual perception abilities not found in other evaluations. Most of the tasks can be solved by humans \"within a blink\" (, relative depth estimation, visual correspondence, forensics detection, and multi-view reasoning). However, we find these perception-demanding tasks cast significant challenges for current multimodal LLMs because they resist mediation through natural language. reformats 14 classic computer vision tasks into 3,807 multiple-choice questions, paired with single or multiple images and visual prompting. While humans get 95.70% accuracy on average, is surprisingly challenging for existing multimodal LLMs: even the best-performing GPT-4V and Gemini achieve accuracies of 51.26% and 45.72%, only 13.17% and 7.63% higher than random guessing, indicating that such perception abilities have not \"emerged\" yet in recent multimodal LLMs. Our analysis also highlights that specialist CV models could solve these problems much better, suggesting potential pathways for future improvements. We believe will stimulate the community to help multimodal LLMs catch up with human-level visual perception",
    "checked": true,
    "id": "09ff7cdc583b28ec63dfc85c7142f2cb0e5e4eef",
    "semantic_title": "blink: multimodal large language models can see but not perceive",
    "citation_count": 54,
    "authors": [
      "Xingyu Fu*",
      "Yushi Hu*",
      "Bangzheng Li",
      "Yu Feng",
      "Haoyu Wang",
      "Xudong Lin",
      "Dan Roth",
      "Noah A Smith",
      "Wei-Chiu Ma",
      "Ranjay Krishna"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3361_ECCV_2024_paper.php": {
    "title": "AFF-ttention! Affordances and Attention models for Short-Term Object Interaction Anticipation",
    "volume": "main",
    "abstract": "Short-Term object-interaction Anticipation (STA) consists of detecting the location of the next-active objects, the noun and verb categories of the interaction, and the time to contact from the observation of egocentric video. This ability is fundamental for wearable assistants or human-robot interaction to understand the user's goals, but there is still room for improvement to perform STA in a precise and reliable way. In this work, we improve the performance of STA predictions with two contributions: 1) We propose STAformer, a novel attention-based architecture integrating frame-guided temporal pooling, dual image-video attention, and multiscale feature fusion to support STA predictions from an image-input video pair; 2) We introduce two novel modules to ground STA predictions on human behavior by modeling affordances. First, we integrate an environment affordance model which acts as a persistent memory of interactions that can take place in a given physical scene. Second, we predict interaction hotspots from the observation of hands and object trajectories, increasing confidence in STA predictions localized around the hotspot. Our results show significant relative Overall Top-5 mAP improvements of up to +45% on Ego4D and +42% on a novel set of curated EPIC-Kitchens STA labels. We will release the code, annotations, and pre-extracted affordances on Ego4D and EPIC-Kitchens to encourage future research in this area",
    "checked": true,
    "id": "37488d9c5c37e2a504afab41aefef26f028f3e1b",
    "semantic_title": "aff-ttention! affordances and attention models for short-term object interaction anticipation",
    "citation_count": 2,
    "authors": [
      "Lorenzo Mur-Labadia*",
      "Ruben Martinez-Cantin",
      "Jose J Guerrero",
      "Giovanni Maria Farinella",
      "Antonino Furnari"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3363_ECCV_2024_paper.php": {
    "title": "PreLAR: World Model Pre-training with Learnable Action Representation",
    "volume": "main",
    "abstract": "The recent technique of Model-Based Reinforcement Learning learns to make decisions by building a world model about the dynamics of the environment. The world model learning requires extensive interactions with the real environment. Therefore, several innovative approaches such as APV proposed to unsupervised pre-train the world model from large-scale videos, allowing fewer interactions to fine-tune the world model. However, these methods only pre-train the world model as a video predictive model without action conditions, while the final world model is action-conditional. This gap limits the effectiveness of unsupervised pre-training in enhancing the world model's capabilities. To further release the potential of unsupervised pre-training, we introduce an approach that Pre-trains the world model from action-free videos but with Learnable Action Representation (PreLAR). Specifically, the observations of two adjacent time steps are encoded as an implicit action representation, with which the world model is pre-trained as action conditional. To make the implicit action representation closer to the real action, an action-state consistency loss is designed to self-supervise its optimization. During fine-tuning, the real actions are encoded as the action representation to train the overall world model for downstream tasks. The proposed method is evaluated on various visual control tasks from the Meta-world simulation environment. The results show that the proposed PreLAR significantly improves the sample efficiency in world model learning, demonstrating the necessity of incorporating action in the world model pre-training. Codes can be found at https://github.com/zhanglixuan0720/PreLAR",
    "checked": false,
    "id": "02b5acd94cbbbf7b3c4b6c4d56d590104f4a4fc6",
    "semantic_title": "robouniview: visual-language model with unified view representation for robotic manipulaiton",
    "citation_count": 3,
    "authors": [
      "Lixuan Zhang",
      "Meina Kan",
      "Shiguang Shan",
      "Xilin Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3364_ECCV_2024_paper.php": {
    "title": "Multi-HMR: Multi-Person Whole-Body Human Mesh Recovery in a Single Shot",
    "volume": "main",
    "abstract": "We present , a strong model for multi-person 3D human mesh recovery from a single RGB image. Predictions encompass the whole body, , including hands and facial expressions, using the SMPL-X parametric model and 3D location in the camera coordinate system. Our model detects people by predicting coarse 2D heatmaps of person locations, using features produced by a standard Vision Transformer (ViT) backbone. It then predicts their whole-body pose, shape and 3D location using a new cross-attention module called the Human Prediction Head (HPH), with one query attending to the entire set of features for each detected person. As direct prediction of fine-grained hands and facial poses in a single shot, , without relying on explicit crops around body parts, is hard to learn from existing data, we introduce , the dataset, containing humans close to the camera with diverse hand poses. We show that incorporating it into the training data further enhances predictions, particularly for hands. also optionally accounts for camera intrinsics, if available, by encoding camera ray directions for each image token. This simple design achieves strong performance on whole-body and body-only benchmarks simultaneously: a ViT-S backbone on 448×448 images already yields a fast and competitive model, while larger models and higher resolutions obtain state-of-the-art results",
    "checked": true,
    "id": "4b7074f542f829a586267e160dcb8b3421b4abe5",
    "semantic_title": "multi-hmr: multi-person whole-body human mesh recovery in a single shot",
    "citation_count": 6,
    "authors": [
      "Fabien Baradel*",
      "Thomas LUCAS",
      "Matthieu Armando",
      "Salma Galaaoui",
      "Romain Brégier",
      "Philippe Weinzaepfel",
      "Gregory Rogez"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3367_ECCV_2024_paper.php": {
    "title": "De-confounded Gaze Estimation",
    "volume": "main",
    "abstract": "Deep-learning based gaze estimation methods suffer from sever performance degradation in cross-domain settings. One of the primary reason is that the gaze estimation model is confounded by gaze-irrelevant factor during estimation, such as identity and illumination. In this paper, we propose to tackle this problem by causal intervention, an analytical tool that alleviates the impact of confounding factors by using intervening the distribution of confounding factors. Concretely, we propose the Feature-Separation-based Causal Intervention (FSCI) framework for generalizable gaze estimation. The FSCI framework first separates gaze features from gaze-irrelevant features. To alleviate the impact of gaze-irrelevant factors during training, the FSCI framework further implements causal intervention by averaging gaze-irrelevant features using the proposed Dynamic Confounder Bank strategy. Experiments show that the proposed FSCI framework outperforms SOTA gaze estimation methods in varies cross-domain settings, improving cross-domain accuracies by up to 36.2% over the baseline and 11.5% over SOTA methods, respectively, without touching target domain data",
    "checked": false,
    "id": "bca0f8a861655de495e4cd50b75e3dbd6364b0cb",
    "semantic_title": "context de-confounded emotion recognition",
    "citation_count": 33,
    "authors": [
      "Ziyang Liang",
      "Yiwei Bao",
      "Feng Lu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3381_ECCV_2024_paper.php": {
    "title": "Diffusion Models for Monocular Depth Estimation: Overcoming Challenging Conditions",
    "volume": "main",
    "abstract": "We present a novel approach designed to address the complexities posed by challenging, out-of-distribution data in the single-image depth estimation task. Starting with images that facilitate depth prediction due to the absence of unfavorable factors, we systematically generate new, user-defined scenes with a comprehensive set of challenges and associated depth information. This is achieved by leveraging cutting-edge text-to-image diffusion models with depth-aware control, known for synthesizing high-quality image content from textual prompts while preserving the coherence of 3D structure between generated and source imagery. Subsequent fine-tuning of any monocular depth network is carried out through a self-distillation protocol that takes into account images generated using our strategy and its own depth predictions on simple, unchallenging scenes. Experiments on benchmarks tailored for our purposes demonstrate the effectiveness and versatility of our proposal",
    "checked": true,
    "id": "e9904e86380e09d4db4aa16fae1079afb876e926",
    "semantic_title": "diffusion models for monocular depth estimation: overcoming challenging conditions",
    "citation_count": 1,
    "authors": [
      "Fabio Tosi",
      "Pierluigi Zama Ramirez",
      "Matteo Poggi*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3384_ECCV_2024_paper.php": {
    "title": "FreestyleRet: Retrieving Images from Style-Diversified Queries",
    "volume": "main",
    "abstract": "Image Retrieval aims to retrieve corresponding images based on a given query. In application scenarios, users intend to express their retrieval intent through various query styles. However, current retrieval tasks predominantly focus on text-query retrieval exploration, leading to limited retrieval query options and potential ambiguity or bias in user intention. In this paper, we propose the Style-Diversified Query-Based Image Retrieval task, which enables retrieval based on various query styles. To facilitate the novel setting, we propose the first Diverse-Style Retrieval dataset, encompassing diverse query styles including text, sketch, low-resolution, and art. We also propose a light-weighted style-diversified retrieval framework. For various query style inputs, we apply the Gram Matrix to extract the query's textural features and cluster them into a style space with style-specific bases. Then we employ the style-init prompt learning module to enable the visual encoder to comprehend the texture and style information of the query. Experiments demonstrate that our model outperforms existing retrieval models on the style-diversified retrieval task. Moreover, style-diversified queries (sketch+text, art+text, etc) can be simultaneously retrieved in our model. The auxiliary information from other queries enhances the performance within the respective query, which may hold potential significance for the community. 1 1† corresponding author. ∗ equal contribution. Code and Dataset available in here",
    "checked": true,
    "id": "88b263edd3fda625887e608972b276984637c7b2",
    "semantic_title": "freestyleret: retrieving images from style-diversified queries",
    "citation_count": 2,
    "authors": [
      "Hao Li*",
      "Yanhao Jia",
      "Peng Jin",
      "Zesen Cheng",
      "Kehan Li",
      "Jialu Sui",
      "Chang Liu",
      "Li Yuan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3388_ECCV_2024_paper.php": {
    "title": "ReGround: Improving Textual and Spatial Grounding at No Cost",
    "volume": "main",
    "abstract": "When an image generation process is guided by both a text prompt and spatial cues, such as a set of bounding boxes, do these elements work in harmony, or does one dominate the other? Our analysis of a pretrained image diffusion model that integrates gated self-attention into the U-Net reveals that spatial grounding often outweighs textual grounding due to the sequential flow from gated self-attention to cross-attention. We demonstrate that such bias can be significantly mitigated without sacrificing accuracy in either grounding by simply rewiring the network architecture, changing from sequential to parallel for gated self-attention and cross-attention. This surprisingly simple yet effective solution does not require any fine-tuning of the network but substantially reduces the trade-off between the two groundings. Our experiments demonstrate significant improvements from the original GLIGEN to the rewired version in the trade-off between textual grounding and spatial grounding. The project webpage is at https://re-ground.github.io",
    "checked": true,
    "id": "b01be06da97d5c85da676e44b828a29eee05ff4b",
    "semantic_title": "reground: improving textual and spatial grounding at no cost",
    "citation_count": 1,
    "authors": [
      "Phillip Y. Lee",
      "Minhyuk Sung*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3391_ECCV_2024_paper.php": {
    "title": "CardiacNet: Learning to Reconstruct Abnormalities for Cardiac Disease Assessment from Echocardiogram Videos",
    "volume": "main",
    "abstract": "Echocardiogram video plays a crucial role in analysing cardiac function and diagnosing cardiac diseases. Current deep neural network methods primarily aim to enhance diagnosis accuracy by incorporating prior knowledge, such as segmenting cardiac structures or lesions annotated by human experts. However, diagnosing the inconsistent behaviours of the heart, which exist across both spatial and temporal dimensions, remains extremely challenging. For instance, the analysis of cardiac motion acquires both spatial and temporal information from the heartbeat cycle. To address this issue, we propose a novel reconstruction-based approach named CardiacNet to learn a better representation of local cardiac structures and motion abnormalities through echocardiogram videos. CardiacNet accompanied by the Consistency Deformation Codebook (CDC) and the Consistency Deformed-Discriminator (CDD) to learn the commonalities across abnormal and normal samples by incorporating cardiac prior knowledge. In addition, we propose benchmark datasets named CardiacNet-PAH and CardiacNet-ASD for evaluating the effectiveness of cardiac disease assessment. In experiments, our CardiacNet can achieve state-of-the-art results in three different cardiac disease assessment tasks on public datasets CAMUS, EchoNet, and our datasets. The code and dataset are available at: https://github.com/xmed-lab/CardiacNet",
    "checked": true,
    "id": "721ffef5dc4d97914579c8179ef8d0a73f672093",
    "semantic_title": "cardiacnet: learning to reconstruct abnormalities for cardiac disease assessment from echocardiogram videos",
    "citation_count": 1,
    "authors": [
      "Jiewen Yang*",
      "Yiqun Lin",
      "Bin Pu",
      "Jiarong GUO",
      "Xiaowei Xu*",
      "Xiaomeng Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3396_ECCV_2024_paper.php": {
    "title": "LaMI-DETR: Open-Vocabulary Detection with Language Model Instruction",
    "volume": "main",
    "abstract": "Existing methods enhance open-vocabulary object detection by leveraging the robust open-vocabulary recognition capabilities of Vision-Language Models (VLMs), such as CLIP. However, two main challenges emerge: (1) A deficiency in concept representation, where the category names in CLIP's text space lack textual and visual knowledge. (2) An overfitting tendency towards base categories, with the open vocabulary knowledge biased towards base categories during the transfer from VLMs to detectors. To address these challenges, we propose the Language Model Instruction (LaMI) strategy, which leverages the relationships between visual concepts and applies them within a simple yet effective DETR-like detector, termed LaMI-DETR. LaMI utilizes GPT to construct visual concepts and employs T5 to investigate visual similarities across categories. These inter-category relationships refine concept representation and avoid overfitting to base categories. Comprehensive experiments validate our approach's superior performance over existing methods in the same rigorous setting without reliance on external training resources. LaMI-DETR achieves a rare box AP of 43.4 on OV-LVIS, surpassing the previous best by 7.8 rare box AP",
    "checked": true,
    "id": "65f437121a2cb2a136b0c1e398f39e63edf85cec",
    "semantic_title": "lami-detr: open-vocabulary detection with language model instruction",
    "citation_count": 0,
    "authors": [
      "Penghui Du",
      "Yu Wang",
      "Yifan Sun",
      "Luting Wang",
      "Yue Liao",
      "gang zhang",
      "Errui Ding",
      "Yan Wang*",
      "Jingdong Wang",
      "Si Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3398_ECCV_2024_paper.php": {
    "title": "Unrolled Decomposed Unpaired Learning for Controllable Low-Light Video Enhancement",
    "volume": "main",
    "abstract": "Obtaining pairs of low/normal-light videos, with motions, is more challenging than still images, which raises technical issues and poses the technical route of unpaired learning as a critical role. This paper makes endeavors in the direction of learning for low-light video enhancement without using paired ground truth. Compared to low-light image enhancement, enhancing low-light videos is more difficult due to the intertwined effects of noise, exposure, and contrast in the spatial domain, jointly with the need for temporal coherence. To address the above challenge, we propose the Unrolled Decomposed Unpaired Network (UDU-Net) for enhancing low-light videos by unrolling the optimization functions into a deep network to decompose the signal into spatial and temporal-related factors, which are updated iteratively. Firstly, we formulate low-light video enhancement as a Maximum A Posteriori estimation (MAP) problem with carefully designed spatial and temporal visual regularization. Then, via unrolling the problem, the optimization of the spatial and temporal constraints can be decomposed into different steps and updated in a stage-wise manner. From the spatial perspective, the designed Intra subnet leverages unpair prior information from expert photography retouched skills to adjust the statistical distribution. Additionally, we introduce a novel mechanism that integrates human perception feedback to guide network optimization, suppressing over/under-exposure conditions. Meanwhile, to address the issue from the temporal perspective, the designed Inter subnet fully exploits temporal cues in progressive optimization, which helps achieve improved temporal consistency in enhancement results. Consequently, the proposed method achieves superior performance to state-of-the-art methods in video illumination, noise suppression, and temporal consistency across outdoor and indoor scenes. Our code is available at https://github.com/lingyzhu0101/ UDU.git",
    "checked": true,
    "id": "3ced108a14812f54d17b2bd773e59fde6cb1bac5",
    "semantic_title": "unrolled decomposed unpaired learning for controllable low-light video enhancement",
    "citation_count": 0,
    "authors": [
      "Lingyu Zhu",
      "Wenhan Yang",
      "Baoliang Chen",
      "Hanwei Zhu",
      "Zhangkai Ni",
      "Qi Mao",
      "Shiqi Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3399_ECCV_2024_paper.php": {
    "title": "Efficient Image Pre-Training with Siamese Cropped Masked Autoencoders",
    "volume": "main",
    "abstract": "Self-supervised pre-training of image encoders is omnipresent in the literature, particularly following the introduction of Masked autoencoders (MAE). Current efforts attempt to learn object-centric representations from motion in videos. In particular, SiamMAE recently introduced a Siamese network, training a shared-weight encoder from two frames of a video with a high asymmetric masking ratio (95%). In this work, we propose CropMAE, an alternative approach to the Siamese pre-training introduced by SiamMAE. Our method specifically differs by exclusively considering pairs of cropped images sourced from the same image but cropped differently, deviating from the conventional pairs of frames extracted from a video. CropMAE therefore alleviates the need for video datasets, while maintaining competitive performances and drastically reducing pre-training and learning time. Furthermore, we demonstrate that CropMAE learns similar object-centric representations without explicit motion, showing that current self-supervised learning methods do not learn such representations from explicit object motion, but rather thanks to the implicit image transformations that occur between the two views. Finally, CropMAE achieves the highest masking ratio to date (98.5%), enabling the reconstruction of images using only two visible patches. Our code is available at https://github.com/alexandre-eymael/ CropMAE",
    "checked": true,
    "id": "351c31c31befb4f056b4c15e318d5e7f7c46795c",
    "semantic_title": "efficient image pre-training with siamese cropped masked autoencoders",
    "citation_count": 0,
    "authors": [
      "Alexandre Eymaël",
      "Renaud Vandeghen*",
      "Anthony Cioppa",
      "Silvio Giancola",
      "Bernard Ghanem",
      "Marc Van Droogenbroeck"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3403_ECCV_2024_paper.php": {
    "title": "VP-SAM: Taming Segment Anything Model for Video Polyp Segmentation via Disentanglement and Spatio-temporal Side Network",
    "volume": "main",
    "abstract": "We propose a novel model (VP-SAM) adapted from segment anything model (SAM) for video polyp segmentation (VPS), which is a challenging task due to (1) the low contrast between polyps and background and (2) the large frame-to-frame variations of polyp size, position, and shape. Our aim is to take advantage of the powerful representation capability of SAM while enabling SAM to effectively harness temporal information of colonoscopic videos and disentangle polyps from background with similar appearances. To achieve this, we propose two new techniques. First, we propose a new semantic disentanglement adapter (SDA) by exploiting amplitude information of the Fourier spectrum to facilitate SAM in more effectively differentiating polyps from background. Second, we propose an innovative spatio-temporal side network (STSN) to provide SAM with spatio-temporal information of videos, thus facilitating SAM in effectively tracking the motion status of polyps. Extensive experiments on SUN-SEG, CVC-612, and CVC-300 demonstrate that our method outperforms state-of-the-art methods. While this work focuses on colonoscopic videos, the proposed method is general enough to be used to analyze other medical videos with similar challenges. Code is available at https://github.com/zhixue-fang/ VPSAM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhixue Fang",
      "Yuzhi Liu",
      "Huisi Wu*",
      "Jing Qin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3404_ECCV_2024_paper.php": {
    "title": "Dataset Enhancement with Instance-Level Augmentations",
    "volume": "main",
    "abstract": "We present a method for expanding a dataset by incorporating knowledge from the wide distribution of pre-trained latent diffusion models. Data augmentations typically incorporate inductive biases about the image formation process into the training (e.g. translation, scaling, colour changes, etc.). Here, we go beyond simple pixel transformations and introduce the concept of instance-level data augmentation by repainting parts of the image at the level of object instances. The method combines a conditional diffusion model with depth and edge maps control conditioning to seamlessly repaint individual objects inside the scene, being applicable to any segmentation or detection dataset. Used as a data augmentation method, it improves the performance and generalization of the state-of-the-art salient object detection, semantic segmentation and object detection models. By redrawing all privacy-sensitive instances (people, license plates, etc.), the method is also applicable for data anonymization. We also release fully synthetic and anonymized expansions for popular datasets: COCO, Pascal VOC and DUTS. The project page is available here",
    "checked": true,
    "id": "6442dc909d60d6f9bcd93b2ce1675931c8d3915c",
    "semantic_title": "dataset enhancement with instance-level augmentations",
    "citation_count": 2,
    "authors": [
      "Orest Kupyn*",
      "Christian Rupprecht"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3408_ECCV_2024_paper.php": {
    "title": "FreeMotion: MoCap-Free Human Motion Synthesis with Multimodal Large Language Models",
    "volume": "main",
    "abstract": "Human motion synthesis is a fundamental task in computer animation. Despite recent progress in this field utilizing deep learning and motion capture data, existing methods are always limited to specific motion categories, environments, and styles. This poor generalizability can be partially attributed to the difficulty and expense of collecting large-scale and high-quality motion data. At the same time, foundation models trained with internet-scale image and text data have demonstrated surprising world knowledge and reasoning ability for various downstream tasks. Utilizing these foundation models may help with human motion synthesis, which some recent works have superficially explored. However, these methods didn't fully unveil the foundation models' potential for this task and only support several simple actions and environments. In this paper, we for the first time, without any motion data, explore open-set human motion synthesis using natural language instructions as user control signals based on MLLMs across any motion task and environment. Our framework can be split into two stages: 1) sequential keyframe generation by utilizing MLLMs as a keyframe designer and animator; 2) motion filling between keyframes through interpolation and motion tracking. Our method can achieve general human motion synthesis for many downstream tasks. The promising results demonstrate the worth of mocap-free human motion synthesis aided by MLLMs and pave the way for future research",
    "checked": true,
    "id": "39f1bbc507f9f11a584b6b0bcb5828a1a4eacd56",
    "semantic_title": "freemotion: mocap-free human motion synthesis with multimodal large language models",
    "citation_count": 1,
    "authors": [
      "Zhikai Zhang",
      "Yitang Li",
      "Haofeng Huang",
      "Mingxian Lin",
      "Li Yi*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3409_ECCV_2024_paper.php": {
    "title": "Chameleon: A Data-Efficient Generalist for Dense Visual Prediction in the Wild",
    "volume": "main",
    "abstract": "Despite the success in large language models, constructing a data-efficient generalist for dense visual prediction presents a distinct challenge due to the variation in label structures across different tasks. In this study, we explore a universal model that can flexibly adapt to unseen dense label structures with a few examples, enabling it to serve as a data-efficient vision generalist in diverse real-world scenarios. To this end, we base our method on a powerful meta-learning framework and explore several axes to improve its performance and versatility for real-world problems, such as flexible adaptation mechanisms and scalability. We evaluate our model across a spectrum of unseen real-world scenarios where low-shot learning is desirable, including video, 3D, medical, biological, and user-interactive tasks. Equipped with a generic architecture and an effective adaptation mechanism, our model flexibly adapts to all of these tasks with at most 50 labeled images, showcasing a significant advancement over existing data-efficient generalist approaches. Codes are available at https: //github.com/GitGyun/chameleon",
    "checked": true,
    "id": "9459267d7627529cfc2397316afd2a4a033677dc",
    "semantic_title": "chameleon: a data-efficient generalist for dense visual prediction in the wild",
    "citation_count": 0,
    "authors": [
      "Donggyun Kim",
      "Seongwoong Cho",
      "Semin Kim",
      "Chong Luo",
      "Seunghoon Hong*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3410_ECCV_2024_paper.php": {
    "title": "Reliability in Semantic Segmentation: Can We Use Synthetic Data?",
    "volume": "main",
    "abstract": "Assessing the robustness of perception models to covariate shifts and their ability to detect out-of-distribution (OOD) inputs is crucial for safety-critical applications such as autonomous vehicles. By nature of such applications, however, the relevant data is difficult to collect and annotate. In this paper, we show for the first time how synthetic data can be specifically generated to assess comprehensively the real-world reliability of semantic segmentation models. By fine-tuning Stable Diffusion [?] with only in-domain data, we perform zero-shot generation of visual scenes in OOD domains or inpainted with OOD objects. This synthetic data is employed to evaluate the robustness of pretrained segmenters, thereby offering insights into their performance when confronted with real edge cases. Through extensive experiments, we demonstrate a high correlation between the performance of models when evaluated on our synthetic OOD data and when evaluated on real OOD inputs, showing the relevance of such virtual testing. Furthermore, we demonstrate how our approach can be utilized to enhance the calibration and OOD detection capabilities of segmenters. Code and data are made public",
    "checked": true,
    "id": "9ebded25f62775dddca52d8ecf44df6bc74b8e33",
    "semantic_title": "reliability in semantic segmentation: can we use synthetic data?",
    "citation_count": 4,
    "authors": [
      "Thibaut Loiseau",
      "Tuan-Hung Vu*",
      "Mickael Chen",
      "Patrick Pérez",
      "Matthieu Cord"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3420_ECCV_2024_paper.php": {
    "title": "SCPNet: Unsupervised Cross-modal Homography Estimation via Intra-modal Self-supervised Learning",
    "volume": "main",
    "abstract": "We propose a novel unsupervised cross-modal homography estimation framework based on intra-modal Self-supervised learning, Correlation, and consistent feature map Projection, namely SCPNet. The concept of intra-modal self-supervised learning is first presented to facilitate the unsupervised cross-modal homography estimation. The correlation-based homography estimation network and the consistent feature map projection are combined to form the learnable architecture of SCPNet, boosting the unsupervised learning framework. SCPNet is the first to achieve effective unsupervised homography estimation on the satellite-map image pair cross-modal dataset, GoogleMap, under [-32,+32] offset on a 128 × 128 image, leading the supervised approach MHN by 14.0% of mean average corner error (MACE). We further conduct extensive experiments on several cross-modal/spectral and manually-made inconsistent datasets, on which SCPNet achieves the state-of-the-art (SOTA) performance among unsupervised approaches, and owns 49.0%, 25.2%, 36.4%, and 10.7% lower MACEs than the supervised approach MHN. Source code is available at https://github.com/RM-Zhang/ SCPNet",
    "checked": true,
    "id": "88e8faed04dab5747b96f5ddc67ae0cb64280e94",
    "semantic_title": "scpnet: unsupervised cross-modal homography estimation via intra-modal self-supervised learning",
    "citation_count": 0,
    "authors": [
      "Runmin Zhang*",
      "Jun Ma",
      "Lun Luo",
      "Beinan Yu",
      "Shu-Jie Chen",
      "Junwei Li",
      "Hui-Liang Shen",
      "Si-Yuan Cao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3423_ECCV_2024_paper.php": {
    "title": "SCAPE: A Simple and Strong Category-Agnostic Pose Estimator",
    "volume": "main",
    "abstract": "simplify the pipeline by discarding previous matching blocks, relying only pure self-attention for matching result in using only an MLP can directly regress the coordinates from the support keypoint. Category-Agnostic Pose Estimation (CAPE) aims to localize keypoints on an object of any category given few exemplars in an in-context manner. Prior arts involve sophisticated designs, e.g., sundry modules for similarity calculation and a two-stage framework, or takes in extra heatmap generation and supervision. We notice that CAPE is essentially a task about feature matching, which can be solved within the attention process. Therefore we first streamline the architecture into a simple baseline consisting of several pure self-attention layers and an MLP regression head—this simplification means that one only needs to consider the attention quality to boost the performance of CAPE. Towards an effective attention process for CAPE, we further introduce two key modules: i) a global keypoint feature perceptor to inject global semantic information into support keypoints, and ii) a keypoint attention refiner to enhance inter-node correlation between keypoints. They jointly form a Simple and strong Category-Agnostic Pose Estimator (SCAPE). Experimental results show that SCAPE outperforms prior arts by 2.2 and 1.3 PCK under 1-shot and 5-shot settings with faster inference speed and lighter model capacity, excelling in both accuracy and efficiency. Code and models are available at github.com/tiny-smart/SCAPE",
    "checked": true,
    "id": "60ee316071b8d6e3fc9c8de2be7361eeac2bb290",
    "semantic_title": "scape: a simple and strong category-agnostic pose estimator",
    "citation_count": 0,
    "authors": [
      "Yujia Liang",
      "Zixuan Ye",
      "Wenze Liu",
      "Hao Lu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3426_ECCV_2024_paper.php": {
    "title": "Elevating All Zero-Shot Sketch-Based Image Retrieval Through Multimodal Prompt Learning",
    "volume": "main",
    "abstract": "We address the challenges inherent in sketch-based image retrieval (SBIR) across various settings, including zero-shot SBIR, generalized zero-shot SBIR, and fine-grained zero-shot SBIR, by leveraging the vision-language foundation model CLIP. While recent endeavors have employed CLIP to enhance SBIR, these approaches predominantly follow uni-modal prompt processing and overlook to exploit CLIP's integrated visual and textual capabilities fully. To bridge this gap, we introduce SpLIP, a novel multi-modal prompt learning scheme designed to operate effectively with frozen CLIP backbones. We diverge from existing multi-modal prompting methods that treat visual and textual prompts independently or integrate them in a limited fashion, leading to suboptimal generalization. SpLIP implements a bi-directional prompt-sharing strategy that enables mutual knowledge exchange between CLIP's visual and textual encoders, fostering a more cohesive and synergistic prompt processing mechanism that significantly reduces the semantic gap between the sketch and photo embeddings. In addition to pioneering multi-modal prompt learning, we propose two innovative strategies for further refining the embedding space. The first is an adaptive margin generation for the sketch-photo triplet loss, regulated by CLIP's class textual embeddings. The second introduces a novel task, termed conditional cross-modal jigsaw, aimed at enhancing fine-grained sketch-photo alignment by implicitly modeling sketches' viable patch arrangement using knowledge of unshuffled photos. Our comprehensive experimental evaluations across multiple benchmarks demonstrate the superior performance of SpLIP in all three SBIR scenarios. Project page: https://mainaksingha01.github.io/SpLIP/",
    "checked": true,
    "id": "6292aa9cd3246dd888d5180e5bfcaa47291369a1",
    "semantic_title": "elevating all zero-shot sketch-based image retrieval through multimodal prompt learning",
    "citation_count": 0,
    "authors": [
      "Mainak Singha*",
      "Ankit Jha",
      "Divyam Gupta",
      "Pranav Singla",
      "Biplab Banerjee"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3432_ECCV_2024_paper.php": {
    "title": "Improving Knowledge Distillation via Regularizing Feature Direction and Norm",
    "volume": "main",
    "abstract": "Knowledge distillation (KD) is a particular technique of model compression that exploits a large well-trained teacher neural network to train a small student network . Treating teacher's feature as knowledge, prevailing methods train student by aligning its features with the teacher's, e.g., by minimizing the KL-divergence or L2-distance between their (logits) features. While it is natural to assume that better feature alignment helps distill teacher's knowledge, simply forcing this alignment does not directly contribute to the student's performance, e.g., classification accuracy. For example, minimizing the L2 distance between the penultimate-layer features (used to compute logits for classification) does not necessarily help learn a better student classifier. We are motivated to regularize student features at the penultimate layer using teacher towards training a better student classifier. Specifically, we present a rather simple method that uses teacher's class-mean features to align student features w.r.t their direction. Experiments show that this significantly improves KD performance. Moreover, we empirically find that student produces features that have notably smaller norms than teacher's, motivating us to regularize student to produce large-norm features. Experiments show that doing so also yields better performance. Finally, we present a simple loss as our main technical contribution that regularizes student by simultaneously (1) aligning the direction of its features with the teacher class-mean feature, and (2) encouraging it to produce large-norm features. Experiments on standard benchmarks demonstrate that adopting our technique remarkably improves existing KD methods, achieving the state-of-the-art KD performance through the lens of image classification (on ImageNet and CIFAR100 datasets) and object detection (on the COCO dataset)",
    "checked": false,
    "id": "45d9f9494b13dfcf353bec69f1afa01b1d6c753d",
    "semantic_title": "improving knowledge distillation via regularizing feature norm and direction",
    "citation_count": 10,
    "authors": [
      "Yuzhu Wang",
      "Lechao Cheng*",
      "Manni Duan",
      "Yongheng Wang",
      "Zunlei Feng",
      "Shu Kong"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3433_ECCV_2024_paper.php": {
    "title": "3DFG-PIFu: 3D Feature Grids for Human Digitization from Sparse Views",
    "volume": "main",
    "abstract": "Pixel-aligned implicit models, such as Multi-view PIFu, DeepMultiCap, DoubleField, and SeSDF, are well-established methods for reconstructing a clothed human from sparse views. However, given V images, these models would only combine features from these images in a point-wise and localized manner. In other words, the V images are processed individually and are only combined in a very narrow fashion at the end of the pipeline. To a large extent, this defeats the purpose of having multi-view information since the multi-view task in question is predominantly treated as a single-view task. To resolve this, we introduce 3DFG-PIFu, a pixel-aligned implicit model that exploits multi-view information right from the start and all the way to the end of the pipeline. Our 3DFG-PIFu makes use of 3D Feature Grids to combine features from V images in a global manner (rather than point-wise or localized) and throughout the pipeline. Other than the 3D Feature Grids, 3DFG-PIFu also proposes an iterative mechanism that refines and updates an existing output human mesh using the different views. Moreover, 3DFG-PIFu introduces SDF-based SMPL-X features, which is a new method of incorporating a SMPL-X mesh into a pixel-aligned implicit model. Our experiments show that 3DFG-PIFu significantly outperforms SOTA models. Our code is released at https://github.com/kcyt/3DFG-PIFu",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kennard Yanting Chan*",
      "Fayao Liu",
      "Guosheng Lin",
      "Chuan Sheng Foo",
      "Weisi Lin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3436_ECCV_2024_paper.php": {
    "title": "Lazy Diffusion Transformer for Interactive Image Editing",
    "volume": "main",
    "abstract": "We introduce a novel diffusion transformer, , that generates partial image updates efficiently. Our approach targets interactive image editing applications in which, starting from a blank canvas or an image, a user specifies a sequence of localized image modifications using binary masks and text prompts. Our generator operates in two phases. First, a context encoder processes the current canvas and user mask to produce a compact global context tailored to the region to generate. Second, conditioned on this context, a diffusion-based transformer decoder synthesizes the masked pixels in a \"lazy\" fashion, i.e., it only generates the masked region. This contrasts with previous works that either regenerate the full canvas, wasting time and computation, or confine processing to a tight rectangular crop around the mask, ignoring the global image context altogether. Our decoder's runtime scales with the mask size, which is typically small, while our encoder introduces negligible overhead. We demonstrate that our approach is competitive with state-of-the-art inpainting methods in terms of quality and fidelity while providing a 10× speedup for typical user interactions, where the editing mask represents 10% of the image",
    "checked": true,
    "id": "7a062c6b1c63c1bc933165ea74a04de16952aa94",
    "semantic_title": "lazy diffusion transformer for interactive image editing",
    "citation_count": 3,
    "authors": [
      "Yotam Nitzan*",
      "Zongze Wu",
      "Richard Zhang",
      "Eli Shechtman",
      "Danny Cohen-Or",
      "Taesung Park",
      "Michaël Gharbi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3438_ECCV_2024_paper.php": {
    "title": "Non-parametric Sensor Noise Modeling and Synthesis",
    "volume": "main",
    "abstract": "We introduce a novel non-parametric sensor noise model that directly constructs probability mass functions per intensity level from captured images. We show that our noise model provides a more accurate fit to real sensor noise than existing models. We detail the capture procedure for deriving our non-parametric noise model and introduce an interpolation method that reduces the number of ISOs levels that need to be captured. In addition, we propose a method to synthesize noise on existing noisy images when noise-free images are not available. Our noise model is straightforward to calibrate and provides notable improvements over competing noise models on downstream tasks",
    "checked": false,
    "id": "3dd986040f6a9790f360aa9fb3edf1fc9668a165",
    "semantic_title": "practical blind denoising via swin-conv-unet and data synthesis",
    "citation_count": 56,
    "authors": [
      "Ali Mosleh*",
      "Luxi Zhao",
      "Atin Vikram Singh",
      "Jaeduk Han",
      "Abhijith Punnappurath",
      "Marcus A Brubaker",
      "Jihwan Choe",
      "Michael S Brown"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3451_ECCV_2024_paper.php": {
    "title": "Stripe Observation Guided Inference Cost-free Attention Mechanism",
    "volume": "main",
    "abstract": "Structural re-parameterization (SRP) is a novel technique series that boosts neural networks without introducing any computational costs in inference stage. The existing SRP methods have successfully considered many architectures, such as normalizations, convolutions, etc. However, the widely used but computationally expensive attention modules cannot be directly implemented by SRP due to the inherent multiplicative manner and the modules' output is input-dependent during inference. In this paper, we statistically discover a counter-intuitive phenomenon Stripe Observation in various settings, which reveals that channel attention values consistently approach some constant vectors during training. It inspires us to propose a novel attention-alike SRP, called ASR, that allows us to achieve SRP for a given network while enjoying the effectiveness of the attention mechanism. Extensive experiments conducted on several standard benchmarks show the effectiveness of ASR in generally improving the performance of various scenarios without any elaborated model crafting. We also provide experimental evidence for how the proposed ASR can enhance model performance. https: //github.com/zhongshsh/ASR",
    "checked": false,
    "id": "27e11e364d69238b09bbd64f0c08cb28479a024d",
    "semantic_title": "mmginpainting: multi-modality guided image inpainting based on diffusion models",
    "citation_count": 4,
    "authors": [
      "Zhongzhan Huang*",
      "Shanshan Zhong",
      "Wushao Wen",
      "Jinghui Qin",
      "Liang Lin*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3456_ECCV_2024_paper.php": {
    "title": "The Nerfect Match: Exploring NeRF Features for Visual Localization",
    "volume": "main",
    "abstract": "In this work, we propose the use of Neural Radiance Fields () as a scene representation for visual localization. Recently, has been employed to enhance pose regression and scene coordinate regression models by augmenting the training database, providing auxiliary supervision through rendered images, or serving as an iterative refinement module. We extend its recognized advantages – its ability to provide a compact scene representation with realistic appearances and accurate geometry – by exploring the potential of 's internal features in establishing precise 2D-3D matches for localization. To this end, we conduct a comprehensive examination of 's implicit knowledge, acquired through view synthesis, for matching under various conditions. This includes exploring different matching network architectures, extracting encoder features at multiple layers, and varying training configurations. Significantly, we introduce , an advanced 2D-3D matching function that capitalizes on the internal knowledge of learned via view synthesis. Our evaluation of on standard localization benchmarks, within a structure-based pipeline, achieves competitive results for localization performance on Cambridge Landmarks. We will release all models and code",
    "checked": true,
    "id": "c67143ffe1ddcfbe5b5649c485e12a0ffbd99a2e",
    "semantic_title": "the nerfect match: exploring nerf features for visual localization",
    "citation_count": 7,
    "authors": [
      "Qunjie Zhou*",
      "Maxim Maximov",
      "Or Litany",
      "Laura Leal-Taixé"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3457_ECCV_2024_paper.php": {
    "title": "ComboVerse: Compositional 3D Assets Creation Using Spatially-Aware Diffusion Guidance",
    "volume": "main",
    "abstract": "Generating high-quality 3D assets from a given image is highly desirable in various applications such as AR/VR. Recent advances in single-image 3D generation explore feed-forward models that learn to infer the 3D model of an object without optimization. Though promising results have been achieved in single object generation, these methods often struggle to model complex 3D assets that inherently contain multiple objects. In this work, we present ComboVerse, a 3D generation framework that produces high-quality 3D assets with complex compositions by learning to combine multiple models. 1) We first perform an in-depth analysis of this \"multi-object gap\" from both model and data perspectives. 2) Next, with reconstructed 3D models of different objects, we seek to adjust their sizes, rotation angles, and locations to create a 3D asset that matches the given image. 3) To automate this process, we apply spatially-aware score distillation sampling (SSDS) from pretrained diffusion models to guide the positioning of objects. Our proposed framework emphasizes spatial alignment of objects, compared with standard score distillation sampling, and thus achieves more accurate results. Extensive experiments validate ComboVerse achieves clear improvements over existing methods in generating compositional 3D assets",
    "checked": true,
    "id": "55a4579dd104978c48b048225d0a2b5db3e9defd",
    "semantic_title": "comboverse: compositional 3d assets creation using spatially-aware diffusion guidance",
    "citation_count": 13,
    "authors": [
      "Yongwei Chen",
      "Tengfei Wang",
      "Tong Wu",
      "Xingang Pan",
      "Kui Jia*",
      "Ziwei Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3461_ECCV_2024_paper.php": {
    "title": "Robust Calibration of Large Vision-Language Adapters",
    "volume": "main",
    "abstract": "This paper addresses the critical issue of miscalibration in CLIP-based model adaptation, particularly in the challenging scenario of out-of-distribution (OOD) samples, which has been overlooked in the existing literature on CLIP adaptation. We empirically demonstrate that popular CLIP adaptation approaches, such as Adapters, Prompt Learning, and Test-Time Adaptation, substantially degrade the calibration capabilities of the zero-shot baseline in the presence of distributional drift. We identify the increase in logit ranges as the underlying cause of miscalibration of CLIP adaptation methods, contrasting with previous work on calibrating fully-supervised models. Motivated by these observations, we present a simple and model-agnostic solution to mitigate miscalibration, by scaling the logit range of each sample to its zero-shot prediction logits. We explore three different alternatives to achieve this, which can be either integrated during adaptation or directly used at inference time. Comprehensive experiments on popular OOD classification benchmarks demonstrate the effectiveness of the proposed approaches in mitigating miscalibration while maintaining discriminative performance, whose improvements are consistent across the three families of these increasingly popular approaches. The code is publicly available at: https://github.com/ Bala93/CLIPCalib",
    "checked": true,
    "id": "00eba5b19549eabe777a582c0faf22be91a1c5cf",
    "semantic_title": "robust calibration of large vision-language adapters",
    "citation_count": 0,
    "authors": [
      "Balamurali Murugesan*",
      "Julio Silva-Rodríguez",
      "Ismail Ben Ayed",
      "Jose Dolz"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3463_ECCV_2024_paper.php": {
    "title": "Leveraging Hierarchical Feature Sharing for Efficient Dataset Condensation",
    "volume": "main",
    "abstract": "Given a real-world dataset, data condensation (DC) aims to synthesize a small synthetic dataset that captures the knowledge of a natural dataset while being usable for training models with comparable accuracy. Recent works propose to enhance DC with data parameterization, which condenses data into very compact parameterized data containers instead of images. The intuition behind data parameterization is to encode shared features of images to avoid additional storage costs. In this paper, we recognize that images share common features in a hierarchical way due to the inherent hierarchical structure of the classification system, which is overlooked by current data parameterization methods. To better align DC with this hierarchical nature and encourage more efficient information sharing inside data containers, we propose a novel data parameterization architecture, Hierarchical Memory Network (HMN). HMN stores condensed data in a three-tier structure, representing the dataset-level, class-level, and instance-level features. Another helpful property of the hierarchical architecture is that HMN naturally ensures good independence among images despite achieving information sharing. This enables instance-level pruning for HMN to reduce redundant information, thereby further minimizing redundancy and enhancing performance. We evaluate HMN on five public datasets and show that our proposed method outperforms all baselines",
    "checked": true,
    "id": "88f1cfdeac60364f4a03d83cabf6eb499a8d330f",
    "semantic_title": "leveraging hierarchical feature sharing for efficient dataset condensation",
    "citation_count": 0,
    "authors": [
      "Haizhong Zheng*",
      "Jiachen Sun",
      "Shutong Wu",
      "Bhavya Kailkhura",
      "Zhuoqing Morley Mao",
      "Chaowei Xiao*",
      "Atul Prakash*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3467_ECCV_2024_paper.php": {
    "title": "Improving Domain Generalization in Self-Supervised Monocular Depth Estimation via Stabilized Adversarial Training",
    "volume": "main",
    "abstract": "Learning a self-supervised Monocular Depth Estimation (MDE) model with great generalization remains significantly challenging. Despite the success of adversarial augmentation in the supervised learning generalization, naively incorporating it into self-supervised MDE models potentially causes over-regularization, suffering from severe performance degradation. In this paper, we conduct qualitative analysis and illuminate the main causes: (i) inherent sensitivity in the UNet-alike depth network and (ii) dual optimization conflict caused by over-regularization. To tackle these issues, we propose a general adversarial training framework, named Stabilized Conflict-optimization Adversarial Training (SCAT), integrating adversarial data augmentation into self-supervised MDE methods to achieve a balance between stability and generalization. Specifically, we devise an effective scaling depth network that tunes the coefficients of long skip connection and effectively stabilizes the training process. Then, we propose a conflict gradient surgery strategy, which progressively integrates the adversarial gradient and optimizes the model toward a conflict-free direction. Extensive experiments on five benchmarks demonstrate that SCAT can achieve state-of-the-art performance and significantly improve the generalization capability of existing self-supervised MDE methods",
    "checked": false,
    "id": "d4838211d7f65628f56b9f6faab30a95ff7b51f8",
    "semantic_title": "for prediction city region re-weighting",
    "citation_count": 0,
    "authors": [
      "Yuanqi Yao*",
      "Gang Wu",
      "Kui Jiang",
      "Siao Liu",
      "Jian Kuai",
      "Xianming Liu",
      "Junjun Jiang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3470_ECCV_2024_paper.php": {
    "title": "milliFlow: Scene Flow Estimation on mmWave Radar Point Cloud for Human Motion Sensing",
    "volume": "main",
    "abstract": "Human motion sensing plays a crucial role in smart systems for decision-making, user interaction, and personalized services. Extensive research that has been conducted is predominantly based on cameras, whose intrusive nature limits their use in smart home applications. To address this, mmWave radars have gained popularity due to their privacy-friendly features. In this work, we propose milliFlow, a novel deep learning approach to estimate scene flow as complementary motion information for mmWave point cloud, serving as an intermediate level of features and directly benefiting downstream human motion sensing tasks. Experimental results demonstrate the superior performance of our method when compared with the competing approaches. Furthermore, by incorporating scene flow information, we achieve remarkable improvements in human activity recognition and human parsing and support human body part tracking. Code and dataset are available at https://github.com/Toytiny/milliFlow",
    "checked": true,
    "id": "dc0448b455d74628b13e89a6b8872d18772b5e98",
    "semantic_title": "milliflow: scene flow estimation on mmwave radar point cloud for human motion sensing",
    "citation_count": 5,
    "authors": [
      "Fangqiang Ding*",
      "Zhen Luo",
      "Peijun Zhao",
      "Chris Xiaoxuan Lu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3482_ECCV_2024_paper.php": {
    "title": "denoiSplit: a method for joint microscopy image splitting and unsupervised denoising",
    "volume": "main",
    "abstract": "In this work, we present , a method to tackle a new analysis task, the challenge of joint semantic image splitting and unsupervised denoising. This dual approach has important applications in fluorescence microscopy, where semantic image splitting has important applications but noise does generally hinder the downstream analysis of image content. Image splitting involves dissecting an image into its distinguishable semantic structures. We show that the current state-of-the-art method for this task struggles in the presence of image noise, inadvertently also distributing the noise across the predicted outputs. The method we present here can deal with image noise by integrating an unsupervised denoising subtask. This integration results in improved semantic image unmixing, even in the presence of notable and realistic levels of imaging noise. A key innovation in is the use of specifically formulated noise models and the suitable adjustment of KL-divergence loss for the high-dimensional hierarchical latent space we are training. We showcase the performance of across multiple tasks on real-world microscopy images. Additionally, we perform qualitative and quantitative evaluations and compare the results to existing benchmarks, demonstrating the effectiveness of using : a single Variational Splitting Encoder-Decoder (VSE) Network using two suitable noise models to jointly perform semantic splitting and denoising",
    "checked": true,
    "id": "764f99fe0d3ef75126831fcf0004521b1767f460",
    "semantic_title": "denoisplit: a method for joint microscopy image splitting and unsupervised denoising",
    "citation_count": 1,
    "authors": [
      "Ashesh Ashesh*",
      "Florian Jug*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3484_ECCV_2024_paper.php": {
    "title": "AugDETR: Improving Multi-scale Learning for Detection Transformer",
    "volume": "main",
    "abstract": "Current end-to-end detectors typically exploit transformers to detect objects and show promising performance. Among them, Deformable DETR is a representative paradigm that effectively exploits multi-scale features. However, small local receptive fields and limited query-encoder interactions weaken multi-scale learning. In this paper, we analyze local feature enhancement and multi-level encoder exploitation for improved multi-scale learning and construct a novel detection transformer detector named Augmented DETR (AugDETR) to realize them. Specifically, AugDETR consists of two components: Hybrid Attention Encoder and Encoder-Mixing Cross-Attention. Hybrid Attention Encoder enlarges the receptive field of the deformable encoder and introduces global context features to enhance feature representation. Encoder-Mixing Cross-Attention adaptively leverages multi-level encoders based on query features for more discriminative object features and faster convergence. By combining AugDETR with DETR-based detectors such as DINO, AlignDETR, DDQ, our models achieve performance improvements of 1.2, 1.1, and 1.0 AP in the COCO under the ResNet-50-4scale and 12 epochs setting, respectively",
    "checked": false,
    "id": "fa6f8ce7e7cfe0b11b60c15fc724637418750a32",
    "semantic_title": "a unified multi task learning architecture for hate detection leveraging user-based information",
    "citation_count": 0,
    "authors": [
      "Jinpeng Dong",
      "Yutong Lin",
      "Chen Li",
      "Sanping Zhou",
      "Nanning Zheng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3489_ECCV_2024_paper.php": {
    "title": "Spherical World-Locking for Audio-Visual Localization in Egocentric Videos",
    "volume": "main",
    "abstract": "Egocentric videos provide comprehensive contexts for user and scene understanding, spanning multisensory perception to behavioral interaction. We propose Spherical World-Locking (SWL) as a general framework for egocentric scene representation, which implicitly transforms multisensory streams with respect to measurements of head orientation. Compared to conventional head-locked egocentric representations with a 2D planar field-of-view, SWL effectively offsets challenges posed by self-motion, allowing for improved spatial synchronization between input modalities. Using a set of multisensory embeddings on a world-locked sphere, we design a unified encoder-decoder transformer architecture that preserves the spherical structure of the scene representation, without requiring expensive projections between image and world coordinate systems. We evaluate the effectiveness of the proposed framework on multiple benchmark tasks for egocentric video understanding, including audio-visual active speaker localization, auditory spherical source localization, and behavior anticipation in everyday activities",
    "checked": true,
    "id": "620c582bf38f25967bb7a84f8ab4498ef52cce36",
    "semantic_title": "spherical world-locking for audio-visual localization in egocentric videos",
    "citation_count": 1,
    "authors": [
      "Heeseung Yun*",
      "Ruohan Gao",
      "Ishwarya Ananthabhotla",
      "Anurag Kumar",
      "Jacob Donley",
      "Chao Li",
      "Gunhee Kim",
      "Vamsi Krishna Ithapu",
      "Calvin Murdock*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3498_ECCV_2024_paper.php": {
    "title": "SPIN: Hierarchical Segmentation with Subpart Granularity in Natural Images",
    "volume": "main",
    "abstract": "Hierarchical segmentation entails creating segmentations at varying levels of granularity. We introduce the first hierarchical semantic segmentation dataset with subpart annotations for natural images, which we call SPIN (SubPartImageNet). We also introduce two novel evaluation metrics to evaluate how well algorithms capture spatial and semantic relationships across hierarchical levels. We benchmark modern models across three different tasks and analyze their strengths and weaknesses across objects, parts, and subparts. To facilitate community-wide progress, we publicly release our dataset at https://joshmyersdean.github. io/spin/index.html",
    "checked": true,
    "id": "e8663f61ac3dbe7c1845ea125e3d3f1b2c6dc9ef",
    "semantic_title": "spin: hierarchical segmentation with subpart granularity in natural images",
    "citation_count": 0,
    "authors": [
      "Josh David Myers-Dean*",
      "Jarek T Reynolds",
      "Brian Price",
      "Yifei Fan",
      "Danna Gurari"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3506_ECCV_2024_paper.php": {
    "title": "SIGMA: Sinkhorn-Guided Masked Video Modeling",
    "volume": "main",
    "abstract": "Video-based pretraining offers immense potential for learning strong visual representations on an unprecedented scale. Recently, masked video modeling methods have shown promising scalability, yet fall short in capturing higher-level semantics due to reconstructing predefined low-level targets such as pixels. To tackle this, we present Sinkhorn-guided Masked Video Modelling (), a novel video pretraining method that jointly learns the video model in addition to a target feature space using a projection network. However, this simple modification means that the regular L2 reconstruction loss will lead to trivial solutions as both networks are jointly optimized. As a solution, we distribute features of space-time tubes evenly across a limited number of learnable clusters. By posing this as an optimal transport problem, we enforce high entropy in the generated features across the batch, infusing semantic and temporal meaning into the feature space. The resulting cluster assignments are used as targets for a symmetric prediction task where the video model predicts cluster assignment of the projection network and vice versa. Experimental results on ten datasets across three benchmarks validate the effectiveness of in learning more performant, temporally-aware, and robust video representations improving upon state-of-the-art methods. Our project website with code is available at: https://quva-lab. github.io/SIGMA",
    "checked": true,
    "id": "420dc9dc7dc17edb25df5f82c38717b5c451828a",
    "semantic_title": "sigma: sinkhorn-guided masked video modeling",
    "citation_count": 0,
    "authors": [
      "Mohammadreza Salehi*",
      "Michael Dorkenwald*",
      "Fida Mohammad Thoker",
      "Efstratios Gavves",
      "Cees Snoek",
      "Yuki M Asano"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3510_ECCV_2024_paper.php": {
    "title": "Generative Camera Dolly: Extreme Monocular Dynamic Novel View Synthesis",
    "volume": "main",
    "abstract": "Accurate reconstruction of complex dynamic scenes from just a single viewpoint continues to be a challenging task in computer vision. Current dynamic novel view synthesis methods typically require videos from many different camera viewpoints, necessitating careful recording setups, and significantly restricting their utility in the wild as well as in terms of embodied AI applications. In this paper, we propose GCD, a controllable monocular dynamic view synthesis pipeline that leverages large-scale diffusion priors to, given a video of any scene, generate a synchronous video from any other chosen perspective, conditioned on a set of relative camera pose parameters. Our model does not require depth as input, and does not explicitly model 3D scene geometry, instead performing end-to-end video-to-video translation in order to achieve its goal efficiently. Despite being trained on synthetic multi-view video data only, zero-shot real-world generalization experiments show promising results in multiple domains, including robotics, object permanence, and driving environments. We believe our framework can potentially unlock powerful applications in rich dynamic scene understanding, perception for robotics, and interactive 3D video viewing experiences for virtual reality",
    "checked": true,
    "id": "0b7d3bf45a62d0e636e6181ed543c26182b8982b",
    "semantic_title": "generative camera dolly: extreme monocular dynamic novel view synthesis",
    "citation_count": 5,
    "authors": [
      "Basile Van Hoorick*",
      "Rundi Wu",
      "Ege Ozguroglu",
      "Kyle Sargent",
      "Ruoshi Liu",
      "Pavel Tokmakov",
      "Achal Dave",
      "Changxi Zheng",
      "Carl Vondrick"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3521_ECCV_2024_paper.php": {
    "title": "Distribution Alignment for Fully Test-Time Adaptation with Dynamic Online Data Streams",
    "volume": "main",
    "abstract": "Given a model trained on source data, Test-Time Adaptation (TTA) enables adaptation and inference in test data streams with domain shifts from the source. Current methods predominantly optimize the model for each incoming test data batch using self-training loss. While these methods yield commendable results in ideal test data streams, where batches are independently and identically sampled from the target distribution, they falter under more practical test data streams that are not independent and identically distributed (non-i.i.d.). The data batches in a non-i.i.d. stream display prominent label shifts relative to each other. It leads to conflicting optimization objectives among batches during the TTA process. Given the inherent risks of adapting the source model to unpredictable test-time distributions, we reverse the adaptation process and propose a novel Distribution Alignment loss for TTA. This loss guides the distributions of test-time features back towards the source distributions, which ensures compatibility with the well-trained source model and eliminates the pitfalls associated with conflicting optimization objectives. Moreover, we devise a domain shift detection mechanism to extend the success of our proposed TTA method in the continual domain shift scenarios. Our extensive experiments validate the logic and efficacy of our method. On six benchmark datasets, we surpass existing methods in non-i.i.d. scenarios and maintain competitive performance under the ideal i.i.d. assumption",
    "checked": true,
    "id": "a2e9035effd82d90adf47d17f740edfeded52c07",
    "semantic_title": "distribution alignment for fully test-time adaptation with dynamic online data streams",
    "citation_count": 0,
    "authors": [
      "Ziqiang Wang*",
      "Zhixiang Chi",
      "Yanan Wu",
      "Li Gu",
      "Zhi Liu*",
      "Konstantinos N Plataniotis*",
      "Yang Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3522_ECCV_2024_paper.php": {
    "title": "Divide and Fuse: Body Part Mesh Recovery from Partially Visible Human Images",
    "volume": "main",
    "abstract": "We introduce a novel bottom-up approach for human body mesh reconstruction, specifically designed to address the challenges posed by partial visibility and occlusion in input images. Traditional top-down methods, relying on whole-body parametric models like SMPL, falter when only a small part of the human is visible, as they require visibility of most of the human body for accurate mesh reconstruction. To overcome this limitation, our method employs a \" ()\" strategy, reconstructing human body parts independently before fusing them, thereby ensuring robustness against occlusions. We design () that independently reconstruct the mesh from a few shape and global-location parameters, without inter-part dependency. A specially designed fusion module then seamlessly integrates the reconstructed parts, even when only a few are visible. We harness a large volume of ground-truth SMPL data to train our parametric mesh models. To facilitate the training and evaluation of our method, we have established benchmark datasets featuring images of partially visible humans with annotations. Our experiments, conducted on these benchmark datasets, demonstrate the effectiveness of our method, particularly in scenarios with substantial invisibility, where traditional approaches struggle to maintain reconstruction quality",
    "checked": true,
    "id": "d9f4502a6ebae99e772b4e632c5c44bbcbb33bfa",
    "semantic_title": "divide and fuse: body part mesh recovery from partially visible human images",
    "citation_count": 1,
    "authors": [
      "Tianyu Luan",
      "Zhongpai Gao",
      "Luyuan Xie",
      "Abhishek Sharma",
      "Hao Ding",
      "Benjamin Planche",
      "Meng Zheng",
      "Ange Lou",
      "Terrence Chen",
      "Junsong Yuan",
      "Ziyan Wu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3523_ECCV_2024_paper.php": {
    "title": "Understanding Physical Dynamics with Counterfactual World Modeling",
    "volume": "main",
    "abstract": "The ability to understand physical dynamics is critical for agents to act in the world. Here, we use Counterfactual World Modeling (CWM) to extract vision structures for dynamics understanding. CWM uses a temporally-factored masking policy for masked prediction of video data without annotations. This policy enables highly effective \"counterfactual prompting\" of the predictor, allowing a spectrum of visual structures to be extracted from a single pre-trained predictor without finetuning on annotated datasets. We demonstrate that these structures are useful for physical dynamics understanding, allowing CWM to achieve the state-of-the-art performance on the Physion benchmark. Code is available at https://neuroailab.github.io/cwm-physics/",
    "checked": true,
    "id": "de62344bbec31df7ccb71521d6f25d0f04201338",
    "semantic_title": "understanding physical dynamics with counterfactual world modeling",
    "citation_count": 0,
    "authors": [
      "Rahul Venkatesh*",
      "Honglin Chen*",
      "Kevin Feigelis",
      "Daniel M Bear",
      "Khaled Jedoui",
      "Klemen Kotar",
      "Felix J Binder",
      "Wanhee Lee",
      "Sherry Liu",
      "Kevin Smith",
      "Judith E. Fan",
      "Daniel Yamins"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3525_ECCV_2024_paper.php": {
    "title": "MIGS: Multi-Identity Gaussian Splatting via Tensor Decomposition",
    "volume": "main",
    "abstract": "We introduce (Multi-Identity Gaussian Splatting), a novel method that learns a single neural representation for multiple identities, using only monocular videos. Recent 3D Gaussian Splatting (3DGS) approaches for human avatars require per-identity optimization. However, learning a multi-identity representation presents advantages in robustly animating humans under arbitrary poses. We propose to construct a high-order tensor that combines all the learnable 3DGS parameters for all the training identities. By assuming a low-rank structure and factorizing the tensor, we model the complex rigid and non-rigid deformations of multiple subjects in a unified network, significantly reducing the total number of parameters. Our proposed approach leverages information from all the training identities and enables robust animation under challenging unseen poses, outperforming existing approaches. It can also be extended to learn unseen identities. Project page: https://aggelinacha.github.io/MIGS/",
    "checked": true,
    "id": "adee591ff3410cddb30960f23fd7a9b3efc95429",
    "semantic_title": "migs: multi-identity gaussian splatting via tensor decomposition",
    "citation_count": 0,
    "authors": [
      "Aggelina Chatziagapi*",
      "Grigorios Chrysos",
      "Dimitris Samaras"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3536_ECCV_2024_paper.php": {
    "title": "4Diff: 3D-Aware Diffusion Model for Third-to-First Viewpoint Translation",
    "volume": "main",
    "abstract": "We present , a 3D-aware diffusion model addressing the exo-to-ego viewpoint translation task — generating first-person (egocentric) view images from the corresponding third-person (exocentric) images. Building on the diffusion model's ability to generate photorealistic images, we propose a transformer-based diffusion model that incorporates geometry priors through two mechanisms: (i) egocentric point cloud rasterization and (ii) 3D-aware rotary cross-attention. Egocentric point cloud rasterization converts the input exocentric image into an egocentric layout, which is subsequently used by a diffusion image transformer. As a component of the diffusion transformer's denoiser block, the 3D-aware rotary cross-attention further incorporates 3D information and semantic features from the source exocentric view. Our achieves state-of-the-art results on the challenging and diverse Ego-Exo4D multiview dataset and exhibits robust generalization to novel environments not encountered during training. Our code, processed data, and pretrained models are publicly available at https://klauscc.github.io/ 4diff",
    "checked": false,
    "id": "312c374518d12ec2cd40266b99bc8424b8694af8",
    "semantic_title": "4diff : 3d-aware diffusion model for third-to-first viewpoint translation",
    "citation_count": 0,
    "authors": [
      "Feng Cheng*",
      "Mi Luo*",
      "Huiyu Wang",
      "Alex Dimakis",
      "Lorenzo Torresani",
      "Gedas Bertasius",
      "Kristen Grauman"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3537_ECCV_2024_paper.php": {
    "title": "Improving Point-based Crowd Counting and Localization Based on Auxiliary Point Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "I-HSIANG CHEN",
      "Wei-Ting Chen",
      "Yu-Wei Liu",
      "Ming-Hsuan Yang",
      "Sy-Yen Kuo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3541_ECCV_2024_paper.php": {
    "title": "Nymeria: A Massive Collection of Egocentric Multi-modal Human Motion in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingni Ma*",
      "Yuting Ye",
      "Rowan Postyeni",
      "Alexander J Gamino",
      "Vijay Baiyya",
      "Luis Pesqueira",
      "Kevin M Bailey",
      "David Soriano Fosas",
      "Fangzhou Hong",
      "Vladimir Guzov",
      "Yifeng Jiang",
      "Hyo Jin Kim",
      "Jakob Engel",
      "Karen Liu",
      "Ziwei Liu",
      "Renzo De Nardi",
      "Richard Newcombe"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3543_ECCV_2024_paper.php": {
    "title": "DreamStruct: Understanding Slides and User Interfaces via Synthetic Data Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi-Hao Peng*",
      "Faria Huq",
      "Yue Jiang",
      "Jason Wu",
      "Xin Yue Li",
      "Jeffrey Bigham",
      "Amy Pavel"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3555_ECCV_2024_paper.php": {
    "title": "SemTrack: A Large-scale Dataset for Semantic Tracking in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengfei Wang",
      "Xiaofei Hui",
      "Jing Wu",
      "Zile Yang",
      "Kian Eng Ong",
      "Xinge Zhao",
      "Beijia Lu",
      "Dezhao Huang",
      "Evan Ling",
      "Weiling Chen",
      "Keng Teck Ma",
      "Minhoe Hur",
      "Jun Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3565_ECCV_2024_paper.php": {
    "title": "VideoMamba: Spatio-Temporal Selective State Space Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinyoung Park*",
      "Hee-Seon Kim",
      "Kangwook Ko",
      "Minbeom Kim",
      "Changick Kim"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3566_ECCV_2024_paper.php": {
    "title": "Text to Layer-wise 3D Clothed Human Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junting Dong*",
      "Qi Fang",
      "Zehuan Huang",
      "Xudong XU",
      "Jingbo Wang",
      "Sida Peng",
      "Bo Dai"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3567_ECCV_2024_paper.php": {
    "title": "Texture-GS: Disentangle the Geometry and Texture for 3D Gaussian Splatting Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianxing Xu*",
      "Wenbo Hu",
      "Yu-Kun Lai",
      "Ying Shan",
      "Song-Hai Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3570_ECCV_2024_paper.php": {
    "title": "Fully Sparse 3D Occupancy Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haisong Liu",
      "Yang Chen",
      "Haiguang Wang",
      "Zetong Yang",
      "Tianyu Li",
      "Jia Zeng",
      "Li Chen",
      "Hongyang Li",
      "Limin Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3572_ECCV_2024_paper.php": {
    "title": "Is user feedback always informative? Retrieval Latent Defending for Semi-Supervised Domain Adaptation without Source Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junha Song*",
      "Tae Soo Kim",
      "Junha Kim",
      "Gunhee Nam",
      "Thijs Kooi",
      "Jaegul Choo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3580_ECCV_2024_paper.php": {
    "title": "CG-SLAM: Efficient Dense RGB-D SLAM in a Consistent Uncertainty-aware 3D Gaussian Field",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiarui Hu",
      "Xianhao Chen",
      "Boyin Feng",
      "Guanglin Li",
      "Liangjing Yang",
      "Hujun Bao",
      "Guofeng Zhang",
      "Zhaopeng Cui*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3594_ECCV_2024_paper.php": {
    "title": "Shifted Autoencoders for Point Annotation Restoration in Object Counting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuda Zou",
      "Xin Xiao",
      "Peilin Zhou",
      "Zhichao Sun",
      "Bo Du",
      "Yongchao Xu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3601_ECCV_2024_paper.php": {
    "title": "PointLLM: Empowering Large Language Models to Understand Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runsen Xu*",
      "Xiaolong Wang",
      "Tai Wang*",
      "Yilun Chen",
      "Jiangmiao Pang*",
      "Dahua Lin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3622_ECCV_2024_paper.php": {
    "title": "GarmentAligner: Text-to-Garment Generation via Retrieval-augmented Multi-level Corrections",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiyue Zhang",
      "Zheng Chong",
      "Xujie Zhang",
      "Hanhui Li",
      "Yuhao Cheng",
      "yiqiang yan",
      "Xiaodan Liang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3623_ECCV_2024_paper.php": {
    "title": "Improving Agent Behaviors with RL Fine-tuning for Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenghao Peng",
      "Wenjie Luo",
      "Yiren Lu*",
      "Tianyi Shen",
      "Cole Gulino",
      "Ari Seff",
      "Justin Fu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3625_ECCV_2024_paper.php": {
    "title": "Enhancing Diffusion Models with Text-Encoder Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaofeng Chen*",
      "Annan Wang",
      "Haoning Wu",
      "Liang Liao",
      "Wenxiu Sun",
      "Qiong Yan",
      "Weisi Lin*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3627_ECCV_2024_paper.php": {
    "title": "Asymmetric Mask Scheme for Self-Supervised Real Image Denoising",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangyu Liao*",
      "Tianheng Zheng",
      "Jiayu Zhong",
      "Pingping Zhang",
      "Chao Ren*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3632_ECCV_2024_paper.php": {
    "title": "Omni6D: Large-Vocabulary 3D Object Dataset for Category-Level 6D Object Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengchen Zhang*",
      "Tong Wu",
      "Tai Wang",
      "Tengfei Wang",
      "Ziwei Liu",
      "Dahua Lin*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3633_ECCV_2024_paper.php": {
    "title": "BAD-Gaussians: Bundle Adjusted Deblur Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingzhe Zhao",
      "Peng Wang",
      "Peidong Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3638_ECCV_2024_paper.php": {
    "title": "Forest2Seq: Revitalizing Order Prior for Sequential Indoor Scene Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Sun*",
      "Hang Zhou",
      "Wengang Zhou",
      "Li Li",
      "Houqiang Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3640_ECCV_2024_paper.php": {
    "title": "BaSIC: BayesNet Structure Learning for Computational Scalable Neural Image Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufeng Zhang",
      "Hang Yu",
      "Shizhan Liu",
      "Wenrui Dai",
      "Weiyao Lin*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3642_ECCV_2024_paper.php": {
    "title": "FlexAttention for Efficient High-Resolution Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyan Li*",
      "Delin Chen",
      "Tianle Cai",
      "Peihao Chen",
      "Yining Hong",
      "Zhenfang Chen",
      "Yikang Shen",
      "Chuang Gan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3643_ECCV_2024_paper.php": {
    "title": "Repaint123: Fast and High-quality One Image to 3D Generation with Progressive Controllable Repainting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junwu Zhang*",
      "Zhenyu Tang",
      "Yatian Pang",
      "Xinhua Cheng",
      "Peng Jin",
      "Yida Wei",
      "xing zhou",
      "munan ning",
      "Li Yuan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3644_ECCV_2024_paper.php": {
    "title": "AnimatableDreamer: Text-Guided Non-rigid 3D Model Generation and Reconstruction with Canonical Score Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinzhou Wang",
      "Yikai Wang*",
      "Junliang Ye",
      "Fuchun Sun*",
      "Zhengyi Wang",
      "Ling Wang",
      "Pengkun Liu",
      "Kai Sun",
      "Xintong Wang",
      "Xie wende",
      "Fangfu Liu",
      "Bin He"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3655_ECCV_2024_paper.php": {
    "title": "Spatially-Variant Degradation Model for Dataset-free Super-resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "SHAOJIE GUO",
      "Haofei Song",
      "Qingli Li",
      "Yan Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3660_ECCV_2024_paper.php": {
    "title": "DreamView: Injecting View-specific Text Guidance into Text-to-3D Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junkai Yan",
      "Yipeng Gao",
      "Qize Yang",
      "Xihan Wei",
      "Xuansong Xie",
      "Ancong Wu*",
      "WEI-SHI ZHENG*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3662_ECCV_2024_paper.php": {
    "title": "Learning Exhaustive Correlation for Spectral Super-Resolution: Where Spatial-Spectral Attention Meets Linear Dependence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyuan Wang",
      "Lizhi Wang*",
      "Jiang Xu",
      "Chang Chen",
      "Xue Hu",
      "Fenglong Song",
      "Youliang Yan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3669_ECCV_2024_paper.php": {
    "title": "Local Action-Guided Motion Diffusion Model for Text-to-Motion Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peng Jin*",
      "Hao Li",
      "Zesen Cheng",
      "Kehan Li",
      "Runyi Yu",
      "Chang Liu*",
      "Xiangyang Ji",
      "Li Yuan*",
      "Jie Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3678_ECCV_2024_paper.php": {
    "title": "EAFormer: Scene Text Segmentation with Edge-Aware Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiyang Yu",
      "Teng Fu",
      "Bin Li*",
      "Xiangyang Xue"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3682_ECCV_2024_paper.php": {
    "title": "Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zicong Fan",
      "Takehiko Ohkawa*",
      "Linlin Yang",
      "Nie Lin",
      "Zhishan Zhou",
      "Shihao Zhou",
      "Jiajun Liang",
      "Zhong Gao",
      "Xuanyang Zhang",
      "Xue Zhang",
      "Fei Li",
      "Liu Zheng",
      "Feng Lu",
      "Karim Abou Zeid",
      "Bastian Leibe",
      "Jeongwan On",
      "Seungryul Baek",
      "Aditya Prakash",
      "Saurabh Gupta",
      "Kun He",
      "Yoichi Sato",
      "Otmar Hilliges",
      "Hyung Jin Chang",
      "Angela Yao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3694_ECCV_2024_paper.php": {
    "title": "DetailSemNet: Elevating Signature Verification through Detail-Semantic Integration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meng-Cheng Shih*",
      "Tsai-Ling Huang",
      "Yu-Heng Shih",
      "Hong-Han Shuai",
      "Hsuan-Tung Liu",
      "Yi-Ren Yeh",
      "Ching-Chun Huang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3696_ECCV_2024_paper.php": {
    "title": "LaPose: Laplacian Mixture Shape Modeling for RGB-Based Category-Level Object Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruida Zhang",
      "Ziqin Huang",
      "Gu Wang",
      "Chenyangguang Zhang",
      "Yan Di",
      "Xingxing Zuo",
      "Jiwen Tang",
      "Xiangyang Ji*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3697_ECCV_2024_paper.php": {
    "title": "Upper-body Hierarchical Graph for Skeleton Based Emotion Recognition in Assistive Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiehui Wu",
      "Jiansheng Chen*",
      "Qifeng Luo",
      "Siqi Liu",
      "Youze Xue",
      "Huimin Ma"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3698_ECCV_2024_paper.php": {
    "title": "Fine-Grained Scene Graph Generation via Sample-Level Bias Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yansheng Li",
      "Tingzhu Wang*",
      "Kang Wu",
      "Linlin Wang",
      "Xin Guo",
      "Wenbin Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3700_ECCV_2024_paper.php": {
    "title": "Exploring Guided Sampling of Conditional GANs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei Zhang*",
      "Mengfei Xia",
      "Yujun Shen",
      "Jiapeng Zhu",
      "Ceyuan Yang",
      "Kecheng Zheng",
      "Lianghua Huang",
      "Yu Liu",
      "Fan Cheng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3701_ECCV_2024_paper.php": {
    "title": "MotionChain: Conversational Motion Controllers via Multimodal Prompts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Biao Jiang",
      "Xin Chen",
      "Chi Zhang",
      "Fukun Yin",
      "Zhuoyuan Li",
      "Gang Yu",
      "Jiayuan Fan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3717_ECCV_2024_paper.php": {
    "title": "Idempotent Unsupervised Representation Learning for Skeleton-Based Action Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lilang Lin",
      "Lehong Wu",
      "Jiahang Zhang",
      "Jiaying Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3726_ECCV_2024_paper.php": {
    "title": "Latent Guard: a Safety Framework for Text-to-image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runtao Liu*",
      "Ashkan Khakzar",
      "Jindong Gu",
      "Qifeng Chen*",
      "Philip Torr",
      "Fabio Pizzati*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3727_ECCV_2024_paper.php": {
    "title": "MacDiff: Unified Skeleton Modeling with Masked Conditional Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lehong Wu*",
      "Lilang Lin",
      "Jiahang Zhang",
      "Yiyang Ma",
      "Jiaying Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3732_ECCV_2024_paper.php": {
    "title": "TCC-Det: Temporarily consistent cues for weakly-supervised 3D detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jan Skvrna*",
      "Lukáš Neumann"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3741_ECCV_2024_paper.php": {
    "title": "OPEN: Object-wise Position Embedding for Multi-view 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinghua Hou",
      "Tong Wang",
      "Xiaoqing Ye",
      "Zhe Liu",
      "Shi Gong",
      "Xiao Tan",
      "Errui Ding",
      "Jingdong Wang",
      "Xiang Bai*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3742_ECCV_2024_paper.php": {
    "title": "FoundPose: Unseen Object Pose Estimation with Foundation Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Evin Pınar Örnek*",
      "Yann Labbé",
      "Bugra Tekin",
      "Lingni Ma",
      "Cem Keskin",
      "Christian Forster",
      "Tomas Hodan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3745_ECCV_2024_paper.php": {
    "title": "Early Preparation Pays Off: New Classifier Pre-tuning for Class Incremental Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengyuan Xie",
      "Haiquan Lu",
      "Jia-wen Xiao",
      "Enguang Wang",
      "Le Zhang",
      "Xialei Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3752_ECCV_2024_paper.php": {
    "title": "Kalman-Inspired Feature Propagation for Video Face Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruicheng Feng",
      "Chongyi Li",
      "Chen Change Loy*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3759_ECCV_2024_paper.php": {
    "title": "Select and Distill: Selective Dual-Teacher Knowledge Transfer for Continual Learning on Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu-Chu Yu*",
      "Chi-Pin Huang",
      "Jr-Jen Chen",
      "Kai-Po Chang",
      "Yung-Hsuan Lai",
      "Fu-En Yang",
      "Yu-Chiang Frank Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3773_ECCV_2024_paper.php": {
    "title": "VideoMamba: State Space Model for Efficient Video Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kunchang Li*",
      "Xinhao Li",
      "Yi Wang*",
      "Yinan He",
      "Yali Wang*",
      "Limin Wang*",
      "Yu Qiao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3782_ECCV_2024_paper.php": {
    "title": "SAFNet: Selective Alignment Fusion Network for Efficient HDR Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingtong Kong*",
      "Bo Li",
      "Yike Xiong",
      "Hao Zhang",
      "Hong Gu",
      "Jinwei Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3785_ECCV_2024_paper.php": {
    "title": "Heterogeneous Graph Learning for Scene Graph Prediction in 3D Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanni Ma",
      "Hao Liu",
      "Yun Pei",
      "Yulan Guo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3786_ECCV_2024_paper.php": {
    "title": "Reason2Drive: Towards Interpretable and Chain-based Reasoning for Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ming Nie",
      "Renyuan Peng",
      "Chunwei Wang",
      "Xinyue Cai",
      "Jianhua Han",
      "Hang Xu*",
      "Li Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3791_ECCV_2024_paper.php": {
    "title": "Omniview-Tuning: Boosting Viewpoint Invariance of Vision-Language Pre-training Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shouwei Ruan*",
      "Yinpeng Dong",
      "Liu Hanqing",
      "Yao Huang",
      "Hang Su",
      "Xingxing Wei*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3794_ECCV_2024_paper.php": {
    "title": "Deep Cost Ray Fusion for Sparse Depth Video Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jungeon Kim",
      "Soongjin Kim",
      "Jaesik Park",
      "Seungyong Lee*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3796_ECCV_2024_paper.php": {
    "title": "GraphBEV: Towards Robust BEV Feature Alignment for Multi-Modal 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziying Song",
      "Lei Yang",
      "Shaoqing Xu",
      "Lin Liu",
      "Dongyang Xu",
      "Caiyan Jia*",
      "Feiyang Jia",
      "Li Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3799_ECCV_2024_paper.php": {
    "title": "DINO-Tracker: Taming DINO for Self-Supervised Point Tracking in a Single Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Narek Tumanyan*",
      "Assaf Singer",
      "Shai Bagon",
      "Tali Dekel"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3801_ECCV_2024_paper.php": {
    "title": "GraspXL: Generating Grasping Motions for Diverse Objects at Scale",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hui Zhang*",
      "Sammy Christen",
      "Zicong Fan",
      "Otmar Hilliges",
      "Jie Song"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3804_ECCV_2024_paper.php": {
    "title": "Source Prompt Disentangled Inversion for Boosting Image Editability with Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruibin Li*",
      "Ruihuang Li",
      "Song Guo",
      "Lei Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3805_ECCV_2024_paper.php": {
    "title": "Improving Intervention Efficacy via Concept Realignment in Concept Bottleneck Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nishad Singhi*",
      "Jae Myung Kim",
      "Karsten Roth",
      "Zeynep Akata"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3809_ECCV_2024_paper.php": {
    "title": "JointDreamer: Ensuring Geometry Consistency and Text Congruence in Text-to-3D Generation via Joint Score Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "ChenHan Jiang*",
      "Yihan Zeng",
      "Tianyang Hu",
      "Songcen Xu",
      "Wei Zhang",
      "Hang Xu",
      "Dit-Yan Yeung"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3816_ECCV_2024_paper.php": {
    "title": "Brain Netflix: Scaling Data to Reconstruct Videos from Brain Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Camilo L Fosco*",
      "Benjamin Lahner",
      "Bowen Pan",
      "Alex Andonian",
      "Emilie L Josephs",
      "Alex Lascelles",
      "Aude Oliva"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3831_ECCV_2024_paper.php": {
    "title": "Equivariant Spatio-Temporal Self-Supervision for LiDAR Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Deepti Hegde",
      "Suhas Lohit*",
      "Kuan-Chuan Peng*",
      "Michael J. Jones",
      "Vishal M. Patel"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3832_ECCV_2024_paper.php": {
    "title": "SLAck: Semantic, Location, and Appearance Aware Open-Vocabulary Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyuan Li*",
      "Lei Ke",
      "Yung-Hsu Yang",
      "Luigi Piccinelli",
      "Mattia Segù",
      "Martin Danelljan",
      "Luc Van Gool"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3843_ECCV_2024_paper.php": {
    "title": "Tensorial template matching for fast cross-correlation with rotations and its application for tomography",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antonio Martinez-Sanchez*",
      "Ulrike Homberg",
      "J. M. Almira",
      "Harold Phelippeau"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3848_ECCV_2024_paper.php": {
    "title": "FreeAugment: Data Augmentation Search Across All Degrees of Freedom",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tom Bekor*",
      "Niv Nayman",
      "Lihi Zelnik-Manor"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3849_ECCV_2024_paper.php": {
    "title": "Learning Representations of Satellite Images From Metadata Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jules Bourcier*",
      "Gohar Dashyan",
      "Karteek Alahari",
      "Jocelyn Chanussot"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3857_ECCV_2024_paper.php": {
    "title": "I2-SLAM: Inverting Imaging Process for Robust Photorealistic Dense SLAM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gwangtak Bae",
      "Changwoon Choi",
      "Hyeongjun Heo",
      "Sang Min Kim",
      "Young Min Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3874_ECCV_2024_paper.php": {
    "title": "FlashTex: Fast Relightable Mesh Texturing with LightControlNet",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kangle Deng*",
      "Timothy Omernick",
      "Alexander B Weiss",
      "Deva Ramanan",
      "Jun-Yan Zhu",
      "Tinghui Zhou",
      "Maneesh Agrawala"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3878_ECCV_2024_paper.php": {
    "title": "GS-Pose: Category-Level Object Pose Estimation via Geometric and Semantic Correspondence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengyuan Wang*",
      "Takuya Ikeda",
      "Robert Lee",
      "Koichi Nishiwaki"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3880_ECCV_2024_paper.php": {
    "title": "ArtVLM: Attribute Recognition Through Vision-Based Prefix Language Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "William Yicheng Zhu*",
      "Keren Ye*",
      "Junjie Ke",
      "Jiahui Yu",
      "Leonidas Guibas",
      "Peyman Milanfar",
      "Feng Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3887_ECCV_2024_paper.php": {
    "title": "PanoFree: Tuning-Free Holistic Multi-view Image Generation with Cross-view Self-Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aoming Liu*",
      "Zhong Li*",
      "Zhang Chen*",
      "Nannan Li",
      "Yi Xu",
      "Bryan Plummer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3895_ECCV_2024_paper.php": {
    "title": "SOS: Segment Object System for Open-World Instance Segmentation With Object Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christian Wilms*",
      "Tim Rolff",
      "Maris N Hillemann",
      "Robert Johanson",
      "Simone Frintrop"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3896_ECCV_2024_paper.php": {
    "title": "Lagrangian Hashing for Compressed Neural Field Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shrisudhan Govindarajan*",
      "Zeno Sambugaro",
      "Akhmedkhan Shabanov",
      "Towaki Takikawa",
      "Weiwei Sun",
      "Daniel Rebain",
      "Nicola Conci",
      "Kwang Moo Yi",
      "Andrea Tagliasacchi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3905_ECCV_2024_paper.php": {
    "title": "EDformer: Transformer-Based Event Denoising Across Varied Noise Levels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bin Jiang",
      "Bo Xiong",
      "Bohan Qu",
      "M. Salman Asif",
      "You Zhou*",
      "Zhan Ma*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3908_ECCV_2024_paper.php": {
    "title": "Foster Adaptivity and Balance in Learning with Noisy Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengmeng Sheng",
      "Zeren Sun*",
      "Tao Chen",
      "Shuchao Pang",
      "yucheng wang",
      "Yazhou Yao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3914_ECCV_2024_paper.php": {
    "title": "MetaAug: Meta-Data Augmentation for Post-Training Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cuong Van Pham*",
      "Hoang Anh Dung",
      "Cuong Cao Nguyen",
      "Trung Le",
      "Dinh Phung",
      "Gustavo Carneiro",
      "Thanh-Toan Do"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3916_ECCV_2024_paper.php": {
    "title": "Thermal3D-GS: Physics-induced 3D Gaussians for Thermal Infrared Novel-view Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qian Chen",
      "Shihao Shu",
      "Xiangzhi Bai*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3918_ECCV_2024_paper.php": {
    "title": "Cross-Platform Video Person ReID: A New Benchmark Dataset and Adaptation Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shizhou Zhang",
      "Wenlong Luo",
      "De Cheng*",
      "Qingchun Yang",
      "Lingyan Ran",
      "Yinghui Xing",
      "Yanning Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3925_ECCV_2024_paper.php": {
    "title": "Unleashing the Power of Prompt-driven Nucleus Instance Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongyi Shui*",
      "Yunlong Zhang",
      "Kai Yao",
      "Chenglu Zhu",
      "Sunyi Zheng",
      "Jingxiong Li",
      "Honglin Li",
      "YUXUAN SUN",
      "Ruizhe Guo",
      "Lin Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3933_ECCV_2024_paper.php": {
    "title": "Gaze Target Detection Based on Head-Local-Global Coordination",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaokun Yang",
      "Feng Lu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3937_ECCV_2024_paper.php": {
    "title": "3DSA:Multi-View 3D Human Pose Estimation With 3D Space Attention Mechanisms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Po Han Chen",
      "Chia-Chi Tsai*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3942_ECCV_2024_paper.php": {
    "title": "Toward Tiny and High-quality Facial Makeup with Data Amplify Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiaoqiao Jin",
      "Xuanhong Chen",
      "Meiguang Jin",
      "Ying Chen",
      "Rui Shi",
      "Yucheng Zheng",
      "Yupeng Zhu",
      "Bingbing Ni*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3946_ECCV_2024_paper.php": {
    "title": "An Economic Framework for 6-DoF Grasp Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao-Ming Wu*",
      "Jia-Feng Cai",
      "Jian-Jian Jiang",
      "Dian Zheng",
      "Yi-Lin Wei",
      "Wei-Shi Zheng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3958_ECCV_2024_paper.php": {
    "title": "GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanhui Huang",
      "Wenzhao Zheng",
      "Yunpeng Zhang",
      "Jie Zhou",
      "Jiwen Lu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3967_ECCV_2024_paper.php": {
    "title": "Powerful and Flexible: Personalized Text-to-Image Generation via Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fanyue Wei",
      "Wei Zeng",
      "Zhenyang Li",
      "Dawei Yin",
      "Lixin Duan",
      "Wen Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3969_ECCV_2024_paper.php": {
    "title": "AdaLog: Post-Training Quantization for Vision Transformers with Adaptive Logarithm Quantizer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuguanyu Wu",
      "Jiaxin Chen*",
      "Hanwen Zhong",
      "Di Huang",
      "Yunhong Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3972_ECCV_2024_paper.php": {
    "title": "Multi-Label Cluster Discrimination for Visual Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang An",
      "Kaicheng Yang",
      "Xiangzi Dai",
      "Ziyong Feng",
      "Jiankang Deng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3983_ECCV_2024_paper.php": {
    "title": "Plan, Posture and Go: Towards Open-vocabulary Text-to-Motion Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinpeng Liu",
      "Wenxun Dai",
      "Chunyu Wang",
      "Yiji Cheng",
      "Yansong Tang*",
      "Xin Tong"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3984_ECCV_2024_paper.php": {
    "title": "DAMSDet: Dynamic Adaptive Multispectral Detection Transformer with Competitive Query Selection and Adaptive Feature Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junjie Guo*",
      "Chenqiang Gao*",
      "Fangcen Liu",
      "Deyu Meng",
      "Xinbo Gao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3990_ECCV_2024_paper.php": {
    "title": "CLIP-Guided Generative Networks for Transferable Targeted Adversarial Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Fang",
      "Jiawei Kong",
      "Bin Chen*",
      "Tao Dai",
      "Hao Wu",
      "Shu-Tao Xia"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3991_ECCV_2024_paper.php": {
    "title": "Flash Cache: Reducing Bias in Radiance Cache Based Inverse Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Attal*",
      "Dor Verbin",
      "Ben Mildenhall",
      "Peter Hedman",
      "Jonathan T Barron",
      "Matthew O'Toole",
      "Pratul Srinivasan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4012_ECCV_2024_paper.php": {
    "title": "Progressive Classifier and Feature Extractor Adaptation for Unsupervised Domain Adaptation on Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zicheng Wang",
      "Zhen Zhao",
      "Yiming Wu",
      "Luping Zhou*",
      "Dong Xu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4014_ECCV_2024_paper.php": {
    "title": "A New Dataset and Framework for Real-World Blurred Images Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Qin",
      "Ming Sun",
      "Chao Zhou",
      "Bin Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4016_ECCV_2024_paper.php": {
    "title": "AddressCLIP: Empowering Vision-Language Models for City-wide Image Address Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shixiong Xu",
      "Chenghao Zhang",
      "Lubin Fan*",
      "Gaofeng Meng*",
      "SHIMING XIANG",
      "Jieping Ye"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4024_ECCV_2024_paper.php": {
    "title": "RISurConv: Rotation Invariant Surface Attention-Augmented Convolutions for 3D Point Cloud Classification and Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyuan Zhang*",
      "Licheng Yang",
      "Zhiyu Xiang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4025_ECCV_2024_paper.php": {
    "title": "StyleTokenizer: Defining Image Style by a Single Instance for Controlling Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wen Li*",
      "Muyuan Fang",
      "Cheng Zou",
      "Biao Gong",
      "Ruobing Zheng",
      "Meng Wang",
      "Jingdong Chen",
      "Ming Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4026_ECCV_2024_paper.php": {
    "title": "Bidirectional Uncertainty-Based Active Learning for Open-Set Annotation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen-Chen Zong",
      "Ye-Wen Wang",
      "Kun-Peng Ning",
      "Hai-Bo Ye",
      "Sheng-Jun Huang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4040_ECCV_2024_paper.php": {
    "title": "Preventing Catastrophic Overfitting in Fast Adversarial Training: A Bi-level Optimization Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoxin Wang*",
      "Handing Wang*",
      "Cong Tian",
      "Yaochu Jin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4051_ECCV_2024_paper.php": {
    "title": "Projecting Points to Axes: Oriented Object Detection via Point-Axis Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyang Zhao",
      "Qilong Xue",
      "Yifan Bai",
      "Yuhang He",
      "Xing Wei*",
      "Yihong Gong"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4067_ECCV_2024_paper.php": {
    "title": "SeiT++: Masked Token Modeling Improves Storage-efficient Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minhyun Lee",
      "Song Park",
      "Byeongho Heo",
      "Dongyoon Han",
      "Hyunjung Shim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4069_ECCV_2024_paper.php": {
    "title": "Rectify the Regression Bias in Long-Tailed Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ke Zhu",
      "Minghao Fu",
      "Jie Shao",
      "Tianyu Liu",
      "Jianxin Wu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4070_ECCV_2024_paper.php": {
    "title": "MagicEraser: Erasing Any Objects via Semantics-Aware Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan Li*",
      "Zixiao Zhang",
      "Yi Huang",
      "Jianzhuang Liu",
      "Renjing Pei",
      "Bin Shao",
      "Songcen Xu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4076_ECCV_2024_paper.php": {
    "title": "Reliable Spatial-Temporal Voxels For Multi-Modal Test-Time Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haozhi Cao",
      "Yuecong Xu",
      "Jianfei Yang*",
      "Pengyu Yin",
      "Xingyu Ji",
      "Shenghai Yuan",
      "Lihua Xie"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4077_ECCV_2024_paper.php": {
    "title": "Stable Preference: Redefining training paradigm of human preference model for Text-to-Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanting Li",
      "Hongjing Niu",
      "Feng Zhao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4088_ECCV_2024_paper.php": {
    "title": "SparseSSP: 3D Subcellular Structure Prediction from Sparse-View Transmitted Light Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jintu Zheng",
      "Yi Ding",
      "Qizhe Liu",
      "Yuehui Chen",
      "Yi Cao",
      "Ying Hu",
      "Zenan Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4090_ECCV_2024_paper.php": {
    "title": "NL2Contact: Natural Language Guided 3D Hand-Object Contact Modeling with Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongqun Zhang*",
      "Hengfei Wang",
      "Ziwei Yu",
      "Yihua Cheng*",
      "Angela Yao",
      "Hyung Jin Chang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4094_ECCV_2024_paper.php": {
    "title": "Self-Adapting Large Visual-Language Models to Edge Devices across Visual Modalities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaiwen Cai",
      "ZheKai Duan",
      "Gaowen Liu",
      "Charles Fleming",
      "Chris Xiaoxuan Lu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4096_ECCV_2024_paper.php": {
    "title": "Diff-Tracker: Text-to-Image Diffusion Models are Unsupervised Trackers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengbo Zhang*",
      "Li Xu",
      "Duo Peng",
      "Hossein Rahmani",
      "Jun Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4104_ECCV_2024_paper.php": {
    "title": "Rethinking Tree-Ring Watermarking for Enhanced Multi-Key Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hai Ci*",
      "Pei Yang",
      "Yiren Song",
      "Mike Zheng Shou*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4105_ECCV_2024_paper.php": {
    "title": "3D Small Object Detection with Dynamic Spatial Pruning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihao Sun",
      "Ziwei Wang",
      "Hongmin Liu",
      "Jie Zhou",
      "Jiwen Lu*",
      "Xiuwei Xu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4106_ECCV_2024_paper.php": {
    "title": "STSP: Spatial-Temporal Subspace Projection for Video Class-incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Cheng",
      "SIYUAN YANG",
      "Chong Wang",
      "Joey Tianyi Zhou",
      "Alex Kot",
      "Bihan Wen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4117_ECCV_2024_paper.php": {
    "title": "Transferable 3D Adversarial Shape Completion using Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuelong Dai*",
      "Bin Xiao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4127_ECCV_2024_paper.php": {
    "title": "OmniSat: Self-Supervised Modality Fusion for Earth Observation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guillaume Astruc*",
      "Nicolas Gonthier",
      "Clement Mallet",
      "Loic Landrieu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4129_ECCV_2024_paper.php": {
    "title": "Distilling Diffusion Models into Conditional GANs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "MinGuk Kang*",
      "Richard Zhang",
      "Connelly Barnes",
      "Sylvain Paris",
      "Suha Kwak",
      "Jaesik Park",
      "Eli Shechtman",
      "Jun-Yan Zhu",
      "Taesung Park*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4140_ECCV_2024_paper.php": {
    "title": "Semantically Guided Representation Learning For Action Anticipation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anxhelo Diko*",
      "Danilo Avola",
      "Bardh Prenkaj",
      "Federico Fontana",
      "Luigi Cinque"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4143_ECCV_2024_paper.php": {
    "title": "MemBN: Robust Test-Time Adaptation via Batch Norm with Statistics Memory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juwon Kang*",
      "Nayeong Kim",
      "Jungseul Ok",
      "Suha Kwak*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4145_ECCV_2024_paper.php": {
    "title": "FREST: Feature RESToration for Semantic Segmentation under Multiple Adverse Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sohyun Lee",
      "Namyup Kim",
      "Sungyeon Kim",
      "Suha Kwak*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4147_ECCV_2024_paper.php": {
    "title": "ScanTalk: 3D Talking Heads from Unregistered Scans",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Federico Nocentini*",
      "Thomas Besnier",
      "Claudio Ferrari",
      "Sylvain Arguillere",
      "Stefano Berretti",
      "Mohamed Daoudi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4155_ECCV_2024_paper.php": {
    "title": "Controllable Navigation Instruction Generation with Chain of Thought Prompting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xianghao Kong",
      "Jinyu Chen",
      "Wenguan Wang*",
      "Hang Su",
      "Xiaolin Hu",
      "Yi Yang",
      "Si Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4158_ECCV_2024_paper.php": {
    "title": "GiT: Towards Generalist Vision Transformer through Universal Language Interface",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiyang Wang*",
      "Hao Tang",
      "Li Jiang",
      "Shaoshuai Shi",
      "Muhammad Ferjad Naeem",
      "Hongsheng Li",
      "Bernt Schiele",
      "Liwei Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4159_ECCV_2024_paper.php": {
    "title": "ScatterFormer: Efficient Voxel Transformer with Scattered Linear Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenhang He*",
      "Ruihuang Li",
      "Guowen Zhang",
      "Lei Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4167_ECCV_2024_paper.php": {
    "title": "A Cephalometric Landmark Regression Method based on Dual-encoder for High-resolution X-ray Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Dai",
      "yang wang*",
      "Chaolin Huang",
      "zhou jiakai",
      "Qilin Xu",
      "Minpeng Xu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4168_ECCV_2024_paper.php": {
    "title": "Exploring the Feature Extraction and Relation Modeling For Light-Weight Transformer Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jikai Zheng",
      "Mingjiang Liang",
      "Shaoli Huang",
      "Jifeng Ning*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4179_ECCV_2024_paper.php": {
    "title": "LiveHPS++: Robust and Coherent Motion Capture in Dynamic Free Environment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiming Ren",
      "Xiao Han",
      "Yichen Yao",
      "Xiaoxiao Long",
      "Yujing Sun*",
      "Yuexin Ma*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4181_ECCV_2024_paper.php": {
    "title": "You Only Need One Step: Fast Super-Resolution with Stable Diffusion via Scale Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mehdi Noroozi*",
      "Isma Hadji*",
      "Brais Martinez*",
      "Adrian Bulat*",
      "Georgios Tzimiropoulos*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4195_ECCV_2024_paper.php": {
    "title": "Gaussian Grouping: Segment and Edit Anything in 3D Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingqiao Ye",
      "Martin Danelljan",
      "Fisher Yu",
      "Lei Ke*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4203_ECCV_2024_paper.php": {
    "title": "CoMo: Controllable Motion Generation through Language Guided Pose Code Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiming Huang*",
      "Weilin Wan",
      "Yue Yang",
      "Chris Callison-Burch",
      "Mark Yatskar",
      "Lingjie Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4207_ECCV_2024_paper.php": {
    "title": "MegaScenes: Scene-Level View Synthesis at Scale",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joseph Tung",
      "Gene Chou*",
      "Ruojin Cai",
      "Guandao Yang",
      "Kai Zhang",
      "Gordon Wetzstein",
      "Bharath Hariharan",
      "Noah Snavely"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4210_ECCV_2024_paper.php": {
    "title": "SuperGaussian: Repurposing Video Models for 3D Super Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Shen*",
      "Duygu Ceylan*",
      "Paul Guerrero",
      "Zexiang Xu",
      "Niloy J. Mitra",
      "Shenlong Wang",
      "Anna Fruehstueck*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4212_ECCV_2024_paper.php": {
    "title": "Towards Model-Agnostic Dataset Condensation by Heterogeneous Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun-Yeong Moon",
      "Jung Uk Kim*",
      "Gyeong-Moon Park*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4213_ECCV_2024_paper.php": {
    "title": "Goldfish: Vision-Language Understanding of Arbitrarily Long Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kirolos Ataallah*",
      "Xiaoqian shen",
      "Eslam mohamed abdelrahman*",
      "Essam Sleiman",
      "Mingchen Zhuge",
      "Jian Ding",
      "Deyao Zhu",
      "Jürgen Schmidhuber",
      "Mohamed Elhoseiny"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4230_ECCV_2024_paper.php": {
    "title": "MeshFeat: Multi-Resolution Features for Neural Fields on Meshes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mihir Mahajan*",
      "Florian Hofherr*",
      "Daniel Cremers"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4236_ECCV_2024_paper.php": {
    "title": "Decoupling Common and Unique Representations for Multimodal Self-supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Wang*",
      "Conrad M Albrecht",
      "Nassim Ait Ali Braham",
      "Chenying Liu",
      "Zhitong Xiong",
      "Xiao Xiang Zhu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4237_ECCV_2024_paper.php": {
    "title": "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brandon McKinzie",
      "Zhe Gan",
      "Jean-Philippe Fauconnier",
      "Samuel Dodge",
      "Bowen Zhang",
      "Philipp Dufter",
      "Dhruti Shah",
      "Futang Peng",
      "Anton Belyi",
      "Max A Schwarzer",
      "Hongyu Hè",
      "Xianzhi Du",
      "Haotian Zhang",
      "Karanjeet Singh",
      "Doug Kang",
      "Tom Gunter",
      "Xiang Kong",
      "Aonan Zhang",
      "Jianyu Wang",
      "Chong Wang",
      "Nan Du",
      "Tao Lei",
      "Sam Wiseman",
      "Mark Lee",
      "Zirui Wang",
      "Ruoming Pang",
      "Peter Grasch",
      "Alexander Toshev*",
      "Yinfei Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4239_ECCV_2024_paper.php": {
    "title": "Optimizing Diffusion Models for Joint Trajectory Prediction and Controllable Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixiao Wang*",
      "Chen Tang",
      "Lingfeng Sun",
      "Simone Rossi",
      "Yichen Xie",
      "Chensheng Peng",
      "Thomas Hannagan",
      "Stefano Sabatini",
      "Nicola Poerio",
      "Masayoshi TOMIZUKA",
      "Wei Zhan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4249_ECCV_2024_paper.php": {
    "title": "2S-ODIS: Two-Stage Omni-Directional Image Synthesis by Geometric Distortion Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Atsuya Nakata*",
      "Takao Yamanaka*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4252_ECCV_2024_paper.php": {
    "title": "Open-Vocabulary 3D Semantic Segmentation with Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyu Zhu*",
      "Hao Zhou",
      "Pengfei Xing",
      "Long Zhao",
      "Hao Xu",
      "Junwei Liang",
      "Alexander G. Hauptmann",
      "Ting Liu",
      "Andrew Gallagher"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4261_ECCV_2024_paper.php": {
    "title": "D-SCo: Dual-Stream Conditional Diffusion for Monocular Hand-Held Object Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowen Fu*",
      "Gu Wang*",
      "Chenyangguang Zhang",
      "Yan Di",
      "Ziqin Huang",
      "Zhiying Leng",
      "Fabian Manhardt",
      "Xiangyang Ji*",
      "Federico Tombari*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4272_ECCV_2024_paper.php": {
    "title": "Combining Generative and Geometry Priors for Wide-Angle Portrait Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lan Yao",
      "Chaofeng Chen",
      "Xiaoming Li*",
      "Zifei Yan",
      "Wangmeng Zuo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4277_ECCV_2024_paper.php": {
    "title": "RealViformer: Investigating Attention for Real-World Video Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuehan Zhang*",
      "Angela Yao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4278_ECCV_2024_paper.php": {
    "title": "Pairwise Distance Distillation for Unsupervised Real-World Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuehan Zhang*",
      "Seungjun Lee",
      "Angela Yao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4294_ECCV_2024_paper.php": {
    "title": "Decomposed Vector-Quantized Variational Autoencoder for Human Grasp Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "zhao zhe*",
      "Mengshi Qi",
      "Huadong Ma"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4298_ECCV_2024_paper.php": {
    "title": "UniFS: Universal Few-shot Instance Perception with Point Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheng Jin*",
      "Ruijie Yao",
      "Lumin Xu",
      "Wentao Liu*",
      "Chen Qian",
      "Ji Wu",
      "Ping Luo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4301_ECCV_2024_paper.php": {
    "title": "SemanticHuman-HD: High Resolution Semantic disentangled 3D Human Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peng Zheng",
      "Tao Liu",
      "Zili Yi",
      "Rui Ma*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4306_ECCV_2024_paper.php": {
    "title": "CoherentGS: Sparse Novel View Synthesis with Coherent 3D Gaussians",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Avinash Paliwal*",
      "Wei Ye",
      "Jinhui Xiong",
      "Dmytro Kotovenko",
      "Rakesh Ranjan",
      "Vikas Chandra",
      "Nima Khademi Kalantari"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4310_ECCV_2024_paper.php": {
    "title": "Monocular Occupancy Prediction for Scalable Indoor Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongxiao Yu",
      "Yuqi Wang",
      "Yuntao Chen",
      "Zhaoxiang Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4314_ECCV_2024_paper.php": {
    "title": "Visual Grounding for Object-Level Generalization in Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haobin Jiang",
      "Zongqing Lu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4320_ECCV_2024_paper.php": {
    "title": "3DEgo: 3D Editing on the Go!",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Umar Khalid*",
      "Hasan Iqbal*",
      "Azib Farooq",
      "Jing Hua",
      "Chen Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4325_ECCV_2024_paper.php": {
    "title": "Efficient Depth-Guided Urban View Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "sheng miao*",
      "Jiaxin Huang",
      "Dongfeng Bai",
      "Weichao Qiu",
      "Liu Bingbing",
      "Andreas Geiger",
      "Yiyi Liao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4326_ECCV_2024_paper.php": {
    "title": "Probabilistic Weather Forecasting with Deterministic Guidance-based Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Donggeun Yoon",
      "Minseok Seo",
      "Doyi Kim",
      "Yeji Choi",
      "Donghyeon Cho*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4329_ECCV_2024_paper.php": {
    "title": "Domain-adaptive Video Deblurring via Test-time Blurring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jin-Ting He*",
      "Fu-Jen Tsai",
      "Jia-Hao Wu",
      "Yan-Tsung Peng",
      "Chung-Chi Tsai",
      "Chia-Wen Lin",
      "Yen-Yu Lin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4334_ECCV_2024_paper.php": {
    "title": "Representing Topological Self-Similarity Using Fractal Feature Maps for Accurate Segmentation of Tubular Structures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxing Huang",
      "Yanfeng Zhou",
      "Yaoru Luo",
      "Guole Liu",
      "Heng Guo",
      "Ge Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4335_ECCV_2024_paper.php": {
    "title": "NeuroNCAP: Photorealistic Closed-loop Safety Testing for Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "William Ljungbergh*",
      "Adam Tonderski",
      "Joakim Johnander",
      "Holger Caesar",
      "Kalle Åström",
      "Michael Felsberg",
      "Christoffer Petersson"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4338_ECCV_2024_paper.php": {
    "title": "OLAF: A Plug-and-Play Framework for Enhanced Multi-object Multi-part Scene Parsing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pranav Gupta*",
      "Rishubh Singh",
      "Pradeep Shenoy",
      "Ravi Kiran Sarvadevabhatla*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4345_ECCV_2024_paper.php": {
    "title": "Progressive Pretext Task Learning for Human Trajectory Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaotong Lin",
      "Tianming Liang",
      "Jianhuang Lai",
      "Jian-Fang Hu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4347_ECCV_2024_paper.php": {
    "title": "Hyperion – A fast, versatile symbolic Gaussian Belief Propagation framework for Continuous-Time SLAM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Hug*",
      "Ignacio Alzugaray",
      "Margarita Chli"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4364_ECCV_2024_paper.php": {
    "title": "Isomorphic Pruning for Vision Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gongfan Fang*",
      "Xinyin Ma",
      "Michael Bi Mi",
      "Xinchao Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4374_ECCV_2024_paper.php": {
    "title": "Attention Prompting on Image for Large Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runpeng Yu*",
      "Weihao Yu*",
      "Xinchao Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4377_ECCV_2024_paper.php": {
    "title": "Learning Cross-hand Policies of High-DOF Reaching and Grasping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qijin She",
      "Shishun Zhang",
      "Yunfan Ye",
      "Ruizhen Hu",
      "Kai Xu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4383_ECCV_2024_paper.php": {
    "title": "Reprojection Errors as Prompts for Efficient Scene Coordinate Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ting-Ru Liu*",
      "Hsuan-Kung Yang",
      "Jou-Min Liu",
      "Chun-Wei Huang",
      "Tsung-Chih Chiang",
      "Quan Kong",
      "Norimasa Kobori",
      "Chun-Yi Lee"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4384_ECCV_2024_paper.php": {
    "title": "Diffusion-Driven Data Replay: A Novel Approach to Combat Forgetting in Federated Class Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinglin Liang",
      "Jin Zhong",
      "Hanlin Gu",
      "Zhongqi Lu",
      "Xingxing Tang",
      "Gang Dai",
      "Shuangping Huang*",
      "Lixin Fan",
      "Qiang Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4389_ECCV_2024_paper.php": {
    "title": "Long-Tail Temporal Action Segmentation with Group-wise Temporal Logit Adjustment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhanzhong Pang*",
      "Fadime Sener",
      "Shrinivas Ramasubramanian",
      "Angela Yao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4416_ECCV_2024_paper.php": {
    "title": "REVISION: Rendering Tools Enable Spatial Fidelity in Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Agneet Chatterjee*",
      "Yiran Luo",
      "Tejas Gokhale",
      "Yezhou Yang",
      "Chitta R Baral"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4429_ECCV_2024_paper.php": {
    "title": "DreamMotion: Space-Time Self-Similar Score Distillation for Zero-Shot Video Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyeonho Jeong",
      "Jinho Chang",
      "Geon Yeong Park",
      "Jong Chul Ye*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4432_ECCV_2024_paper.php": {
    "title": "VideoClusterNet: Self-Supervised and Adaptive Face Clustering for Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Devesh Walawalkar*",
      "Pablo Garrido"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4439_ECCV_2024_paper.php": {
    "title": "Unveiling Privacy Risks in Stochastic Neural Networks Training: Effective Image Reconstruction from Gradients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiming Chen*",
      "Xiangyu Yang",
      "Nikos Deligiannis"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4442_ECCV_2024_paper.php": {
    "title": "Controlling the World by Sleight of Hand",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sruthi Sudhakar*",
      "Ruoshi Liu",
      "Basile Van Hoorick",
      "Carl Vondrick",
      "Richard Zemel"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4444_ECCV_2024_paper.php": {
    "title": "Hiding Imperceptible Noise in Curvature-Aware Patches for 3D Point Cloud Attack",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingyu Yang*",
      "Daizong Liu",
      "Keke Tang",
      "Pan Zhou",
      "Lixing Chen",
      "Junyang Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4450_ECCV_2024_paper.php": {
    "title": "Interleaving One-Class and Weakly-Supervised Models with Adaptive Thresholding for Unsupervised Video Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongwei Nie",
      "Hao Huang",
      "Chengjiang Long",
      "Qing Zhang",
      "Pradipta Maji",
      "Hongmin Cai*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4459_ECCV_2024_paper.php": {
    "title": "Cross-Domain Learning for Video Anomaly Detection with Limited Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yashika Jain",
      "Ali Dabouei*",
      "Min Xu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4462_ECCV_2024_paper.php": {
    "title": "YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chien-Yao Wang*",
      "I-Hau Yeh",
      "Hong-Yuan Mark Liao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4467_ECCV_2024_paper.php": {
    "title": "Unsupervised Multi-modal Medical Image Registration via Invertible Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengjie Guo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4468_ECCV_2024_paper.php": {
    "title": "Functional Transform-Based Low-Rank Tensor Factorization for Multi-Dimensional Data Recovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian-Li Wang",
      "Xi-Le Zhao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4484_ECCV_2024_paper.php": {
    "title": "CRM: Single Image to 3D Textured Mesh with Convolutional Reconstruction Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengyi Wang*",
      "Yikai Wang",
      "Yifei Chen",
      "Chendong Xiang",
      "Shuo Chen",
      "Dajiang Yu",
      "Chongxuan Li",
      "Hang Su",
      "Jun Zhu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4491_ECCV_2024_paper.php": {
    "title": "Domain Reduction Strategy for Non-Line-of-Sight Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyunbo Shim",
      "In Cho",
      "Daekyu Kwon",
      "Seon Joo Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4496_ECCV_2024_paper.php": {
    "title": "HPE-Li: WiFi-enabled Lightweight Dual Selective Kernel Convolution for Human Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Toan D. Gian",
      "Tien Dac Lai",
      "Thien Van Luong",
      "Kok-Seng Wong",
      "Van-Dinh Nguyen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4501_ECCV_2024_paper.php": {
    "title": "Cut out the Middleman: Revisiting Pose-based Gait Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Fu",
      "Saihui Hou*",
      "Shibei Meng",
      "Xuecai Hu*",
      "Chunshui Cao",
      "Xu Liu",
      "Yongzhen Huang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4514_ECCV_2024_paper.php": {
    "title": "HiEI: A Universal Framework for Generating High-quality Emerging Images from Natural Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingmeng Li",
      "Lukang Fu",
      "Surun Yang",
      "Hui Wei*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4515_ECCV_2024_paper.php": {
    "title": "High-Precision Self-Supervised Monocular Depth Estimation with Rich-Resource Prior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianbing Shen*",
      "Wencheng Han"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4516_ECCV_2024_paper.php": {
    "title": "SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingrui Li",
      "Shuhong Liu",
      "Heng Zhou",
      "Guohao Zhu",
      "Na Cheng",
      "Tianchen Deng",
      "Hongyu Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4528_ECCV_2024_paper.php": {
    "title": "View Selection for 3D Captioning via Diffusion Ranking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tiange Luo*",
      "Justin Johnson",
      "Honglak Lee"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4529_ECCV_2024_paper.php": {
    "title": "OmniSSR: Zero-shot Omnidirectional Image Super-Resolution using Stable Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runyi Li*",
      "Xuhan Sheng",
      "Weiqi Li",
      "Jian Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4534_ECCV_2024_paper.php": {
    "title": "UDiffText: A Unified Framework for High-quality Text Synthesis in Arbitrary Images via Character-aware Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiming Zhao*",
      "Zhouhui Lian*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4537_ECCV_2024_paper.php": {
    "title": "Confidence Self-Calibration for Multi-Label Class-Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaile Du*",
      "Yifan Zhou",
      "Fan Lyu",
      "Yuyang Li",
      "Chen Lu",
      "Guangcan Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4539_ECCV_2024_paper.php": {
    "title": "OMG: Occlusion-friendly Personalized Multi-concept Generation in Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhe Kong*",
      "Yong Zhang*",
      "Tianyu Yang",
      "Tao Wang",
      "Kaihao Zhang",
      "Bizhu Wu",
      "Guanying Chen",
      "Wei Liu",
      "Wenhan Luo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4550_ECCV_2024_paper.php": {
    "title": "Versatile Incremental Learning: Towards Class and Domain-Agnostic Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Min-Yeong Park",
      "Jae-Ho Lee",
      "Gyeong-Moon Park*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4553_ECCV_2024_paper.php": {
    "title": "WeCromCL: Weakly Supervised Cross-Modality Contrastive Learning for Transcription-only Supervised Text Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingjing Wu",
      "Zhengyao Fang",
      "Pengyuan Lyu",
      "Chengquan Zhang",
      "Fanglin Chen",
      "Guangming Lu",
      "Wenjie Pei*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4569_ECCV_2024_paper.php": {
    "title": "An Incremental Unified Framework for Small Defect Inspection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqi Tang",
      "Hao Lu",
      "Xiaogang Xu",
      "Ruizheng Wu",
      "Sixing Hu",
      "Tong Zhang",
      "Tsz Wa Cheng",
      "Ming Ge",
      "Ying-Cong Chen*",
      "Fugee Tsung"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4573_ECCV_2024_paper.php": {
    "title": "Enhancing Optimization Robustness in 1-bit Neural Networks through Stochastic Sign Descent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "NianHui Guo*",
      "Hong Guo",
      "Christoph Meinel",
      "Haojin Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4579_ECCV_2024_paper.php": {
    "title": "Temporally Consistent Stereo Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxi Zeng*",
      "Chengtang Yao",
      "Yuwei Wu*",
      "Yunde Jia"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4582_ECCV_2024_paper.php": {
    "title": "A Rotation-invariant Texture ViT for Fine-Grained Recognition of Esophageal Cancer Endoscopic Ultrasound Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyi Liu",
      "Shuaishuai S Zhuang",
      "Jiacheng Nie",
      "Geng Chen ",
      "Yusheng Guo",
      "Guangquan Zhou*",
      "Jean-Louis Coatrieux",
      "Yang Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4591_ECCV_2024_paper.php": {
    "title": "BI-MDRG: Bridging Image History in Multimodal Dialogue Response Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hee Suk Yoon",
      "Eunseop Yoon",
      "Joshua Tian Jin Tee",
      "Kang Zhang",
      "Yu-Jung Heo",
      "Du-Seong Chang",
      "Chang D. Yoo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4592_ECCV_2024_paper.php": {
    "title": "Adapting Fine-Grained Cross-View Localization to Areas without Fine Ground Truth",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zimin Xia*",
      "Yujiao Shi",
      "Hongdong Li",
      "Julian F. P. Kooij"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4596_ECCV_2024_paper.php": {
    "title": "BeNeRF:Neural Radiance Fields from a Single Blurry Image and Event Stream",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenpu Li",
      "Pian Wan",
      "Peng Wang",
      "Jinghang Li",
      "Yi Zhou",
      "Peidong Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4599_ECCV_2024_paper.php": {
    "title": "Human Motion Forecasting in Dynamic Domain Shifts: A Homeostatic Continual Test-time Adaptation Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiongjie Cui*",
      "Huaijiang Sun",
      "Bin Li",
      "Jianfeng Lu",
      "Weiqing Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4603_ECCV_2024_paper.php": {
    "title": "CloudFixer: Test-Time Adaptation for 3D Point Clouds via Diffusion-Guided Geometric Transformation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hajin Shim",
      "Changhun Kim",
      "Eunho Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4605_ECCV_2024_paper.php": {
    "title": "DreamDiffusion: High-Quality EEG-to-Image Generation with Temporal Masked Signal Modeling and CLIP Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunpeng Bai*",
      "Xintao Wang",
      "Yan-Pei Cao",
      "Yixiao Ge",
      "Chun Yuan",
      "Ying Shan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4606_ECCV_2024_paper.php": {
    "title": "FRI-Net: Floorplan Reconstruction via Room-wise Implicit Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Honghao Xu",
      "Juzhan Xu",
      "Zeyu Huang",
      "Pengfei Xu",
      "Hui Huang",
      "Ruizhen Hu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4613_ECCV_2024_paper.php": {
    "title": "BugNIST - a Large Volumetric Dataset for Detection under Domain Shift",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Patrick M Jensen",
      "Vedrana A Dahl",
      "Rebecca Engberg",
      "Carsten Gundlach",
      "Hans Martin Kjer",
      "Anders B Dahl*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4623_ECCV_2024_paper.php": {
    "title": "SCP-Diff: Spatial-Categorical Joint Prior for Diffusion Based Semantic Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huan-ang Gao",
      "Mingju Gao",
      "Jiaju Li",
      "Wenyi Li",
      "Rong Zhi",
      "Hao Tang",
      "Hao Zhao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4630_ECCV_2024_paper.php": {
    "title": "PoseAugment: Generative Human Pose Data Augmentation with Physical Plausibility for IMU-based Motion Capture",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuojun Li*",
      "Chun Yu*",
      "Chen Liang",
      "Yuanchun Shi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4633_ECCV_2024_paper.php": {
    "title": "PixArt-Sigma: Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junsong Chen",
      "Chongjian GE",
      "Enze Xie*",
      "Yue Wu",
      "Lewei Yao",
      "Xiaozhe Ren",
      "Zhongdao Wang",
      "Ping Luo",
      "Huchuan Lu",
      "Zhenguo Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4634_ECCV_2024_paper.php": {
    "title": "Hierarchical Gaussian Mixture Normalizing Flow Modeling for Unified Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xincheng Yao*",
      "Ruoqi Li",
      "Zefeng Qian",
      "lu wang",
      "Chongyang Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4642_ECCV_2024_paper.php": {
    "title": "A Closer Look at GAN Priors: Exploiting Intermediate Features for Enhanced Model Inversion Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixiang Qiu*",
      "Hao Fang",
      "Hongyao Yu",
      "Bin Chen*",
      "Meikang Qiu",
      "Shu-Tao Xia"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4650_ECCV_2024_paper.php": {
    "title": "Improving Unsupervised Domain Adaptation: A Pseudo-Candidate Set Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aveen Dayal*",
      "Rishabh Lalla",
      "Linga Reddy Cenkeramaddi",
      "C. Krishna Mohan",
      "Abhinav Kumar",
      "Vineeth N Balasubramanian"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4681_ECCV_2024_paper.php": {
    "title": "HeadStudio: Text to Animatable Head Avatars with 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenglin Zhou*",
      "Fan Ma",
      "Hehe Fan",
      "Zongxin Yang",
      "Yi Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4682_ECCV_2024_paper.php": {
    "title": "DetToolChain: A New Prompting Paradigm to Unleash Detection Ability of MLLM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixuan Wu*",
      "Yizhou Wang",
      "Shixiang Tang",
      "Wenhao Wu",
      "Tong He",
      "Wanli Ouyang",
      "Philip Torr",
      "Jian Wu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4691_ECCV_2024_paper.php": {
    "title": "Surface-Centric Modeling for High-Fidelity Generalizable Neural Surface Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Peng",
      "Shihe Shen",
      "Kaiqiang Xiong",
      "Huachen Gao",
      "Jianbo Jiao",
      "Xiaodong Gu",
      "Ronggang Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4696_ECCV_2024_paper.php": {
    "title": "HumanRefiner: Benchmarking Abnormal Human Generation and Refining with Coarse-to-fine Pose-Reversible Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guian Fang*",
      "Wenbiao Yan",
      "Yuanfan Guo",
      "Jianhua Han",
      "Zutao Jiang",
      "Hang Xu",
      "Shengcai Liao",
      "Xiaodan Liang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4702_ECCV_2024_paper.php": {
    "title": "Multiscale Graph Texture Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ravishankar Evani*",
      "Deepu Rajan",
      "Shangbo Mao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4703_ECCV_2024_paper.php": {
    "title": "HyTAS: A Hyperspectral Image Transformer Architecture Search Benchmark and Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangqin Zhou*",
      "Mert Kilickaya",
      "Joaquin Vanschoren",
      "Ran Piao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4704_ECCV_2024_paper.php": {
    "title": "Integer-Valued Training and Spike-driven Inference Spiking Neural Network for High-performance and Energy-efficient Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinhao Luo",
      "Man Yao",
      "Yuhong Chou",
      "Bo Xu",
      "Guoqi Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4706_ECCV_2024_paper.php": {
    "title": "RepVF: A Unified Vector Fields Representation for Multi-task 3D Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianbing Shen",
      "Chunliang Li",
      "Wencheng Han",
      "Junbo Yin",
      "Sanyuan Zhao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4729_ECCV_2024_paper.php": {
    "title": "Phase Concentration and Shortcut Suppression for Weakly Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hoyong Kwon",
      "Jaeseok Jeong",
      "Sung-Hoon Yoon",
      "Kuk-Jin Yoon*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4734_ECCV_2024_paper.php": {
    "title": "Group Testing for Accurate and Efficient Range-Based Near Neighbor Search for Plagiarism Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Harsh Shah*",
      "Kashish Mittal",
      "Ajit Rajwade*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4735_ECCV_2024_paper.php": {
    "title": "CompGS: Smaller and Faster Gaussian Splatting with Vector Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "K L Navaneet*",
      "Kossar Pourahmadi Meibodi",
      "Soroush Abbasi Koohpayegani",
      "Hamed Pirsiavash"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4743_ECCV_2024_paper.php": {
    "title": "SMILe: Leveraging Submodular Mutual Information For Robust Few-Shot Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anay Majee*",
      "Ryan X Sharp",
      "Rishabh Iyer*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4746_ECCV_2024_paper.php": {
    "title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixuan Ren*",
      "Yang Zhou",
      "Jimei Yang",
      "Jing Shi",
      "Difan Liu",
      "Feng Liu",
      "Mingi Kwon",
      "Abhinav Shrivastava"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4755_ECCV_2024_paper.php": {
    "title": "S-JEPA: A Joint Embedding Predictive Architecture for Skeletal Action Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohamed Abdelfattah*",
      "Alexandre Alahi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4761_ECCV_2024_paper.php": {
    "title": "∞-Brush: Controllable Large Image Synthesis with Diffusion Models in Infinite Dimensions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minh-Quan Le*",
      "Alexandros Graikos",
      "Srikar Yellapragada",
      "Rajarsi Gupta",
      "Joel Saltz",
      "Dimitris Samaras"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4768_ECCV_2024_paper.php": {
    "title": "SwapAnything: Enabling Arbitrary Object Swapping in Personalized Image Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Gu*",
      "Nanxuan Zhao",
      "Wei Xiong",
      "Qing Liu",
      "Zhifei Zhang",
      "He Zhang",
      "Jianming Zhang",
      "HyunJoon Jung",
      "Yilin Wang*",
      "Xin Eric Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4769_ECCV_2024_paper.php": {
    "title": "Interaction-centric Spatio-Temporal Context Reasoning for Multi-Person Video HOI Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yisong Wang",
      "Nan Xi*",
      "Jingjing Meng",
      "Junsong Yuan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4780_ECCV_2024_paper.php": {
    "title": "Efficient Unsupervised Visual Representation Learning with Explicit Cluster Balancing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ioannis Maniadis Metaxas*",
      "Georgios Tzimiropoulos",
      "Ioannis Patras"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4783_ECCV_2024_paper.php": {
    "title": "ProTIP: Probabilistic Robustness Verification on Text-to-Image Diffusion Models against Stochastic Perturbation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Zhang",
      "Yun Tang",
      "Wenjie Ruan",
      "Xiaowei Huang",
      "Siddartha Khastgir",
      "Paul A Jennings",
      "Xingyu Zhao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4784_ECCV_2024_paper.php": {
    "title": "Leveraging Near-Field Lighting for Monocular Depth Estimation from Endoscopy Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akshay Paruchuri*",
      "Samuel Ehrenstein",
      "Shuxian Wang",
      "Inbar Fried",
      "Stephen Pizer",
      "Marc Niethammer",
      "Roni Sengupta"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4787_ECCV_2024_paper.php": {
    "title": "OvSW: Overcoming Silent Weights for Accurate Binary Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "jingyang xiang*",
      "Zuohui Chen",
      "Siqi Li",
      "Qing Wu",
      "Yong Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4788_ECCV_2024_paper.php": {
    "title": "Multistain Pretraining for Slide Representation Learning in Pathology",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guillaume Jaume*",
      "Anurag J Vaidya*",
      "Andrew Zhang",
      "Andrew Song",
      "Richard J Chen",
      "Sharifa Sahai",
      "Dandan Mo",
      "Emilio Madrigal",
      "Long P Le",
      "Faisal Mahmood*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4804_ECCV_2024_paper.php": {
    "title": "T-Rex2: Towards Generic Object Detection via Text-Visual Prompt Synergy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qing Jiang*",
      "Feng Li",
      "Zhaoyang Zeng",
      "Shilong Liu",
      "Tianhe Ren",
      "Lei Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4805_ECCV_2024_paper.php": {
    "title": "Harmonizing knowledge Transfer in Neural Network with Unified Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "yaomin huang",
      "Faming Fang",
      "Zaoming Yan",
      "Chaomin Shen",
      "Guixu Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4811_ECCV_2024_paper.php": {
    "title": "Mamba-ND: Selective State Space Modeling for Multi-Dimensional Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shufan Li*",
      "Aditya Grover",
      "Harkanwar Singh"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4816_ECCV_2024_paper.php": {
    "title": "Click Prompt Learning with Optimal Transport for Interactive Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Liu*",
      "Haochen wang",
      "Wenzhe Yin",
      "Jan-Jakob Sonke",
      "Efstratios Gavves"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4820_ECCV_2024_paper.php": {
    "title": "3D Human Pose Estimation via Non-Causal Retentive Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaili Zheng",
      "Feixiang Lu",
      "Yihao Lv",
      "Liangjun Zhang",
      "Chenyi Guo*",
      "Ji Wu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4822_ECCV_2024_paper.php": {
    "title": "OMR: Occlusion-Aware Memory-Based Refinement for Video Lane Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongkwon Jin",
      "Chang-Su Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4824_ECCV_2024_paper.php": {
    "title": "6DoF Head Pose Estimation through Explicit Bidirectional Interaction with Face Geometry",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sungho Chun",
      "Ju Yong Chang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4825_ECCV_2024_paper.php": {
    "title": "Latent Diffusion Prior Enhanced Deep Unfolding for Snapshot Spectral Compressive Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zongliang Wu*",
      "Ruiying Lu",
      "Ying Fu",
      "Xin Yuan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4830_ECCV_2024_paper.php": {
    "title": "Multimodal Cross-Domain Few-Shot Learning for Egocentric Action Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Masashi Hatano*",
      "Ryo Hachiuma",
      "Ryo Fujii",
      "Hideo Saito"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4834_ECCV_2024_paper.php": {
    "title": "Enhancing Tampered Text Detection through Frequency Feature Fusion and Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongxi Chen",
      "Shen Chen",
      "Taiping Yao*",
      "Ke Sun",
      "Shouhong Ding",
      "Xianming Lin*",
      "Liujuan Cao",
      "Rongrong Ji"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4855_ECCV_2024_paper.php": {
    "title": "Modeling Label Correlations with Latent Context for Multi-Label Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaomin Chen*",
      "Quan Cui",
      "Ruoxi Deng",
      "Jie Hu",
      "Guodao Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4858_ECCV_2024_paper.php": {
    "title": "LLM as Dataset Analyst: Subpopulation Structure Discovery with Large Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yulin Luo",
      "Ruichuan An",
      "Bocheng Zou",
      "Yiming Tang",
      "Jiaming Liu",
      "Shanghang Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4872_ECCV_2024_paper.php": {
    "title": "Finding a needle in a haystack: A Black-Box Approach to Invisible Watermark Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minzhou Pan*",
      "Zhenting Wang",
      "Xin Dong",
      "Vikash Sehwag",
      "Lingjuan Lyu",
      "Xue Lin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4886_ECCV_2024_paper.php": {
    "title": "DynoSurf: Neural Deformation-based Temporally Consistent Dynamic Surface Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxin Yao",
      "Siyu Ren",
      "Junhui Hou*",
      "Zhi Deng",
      "Juyong Zhang",
      "Wenping Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4888_ECCV_2024_paper.php": {
    "title": "MOD-UV: Learning Mobile Object Detectors from Unlabeled Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihong Sun*",
      "Bharath Hariharan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4890_ECCV_2024_paper.php": {
    "title": "ARoFace: Alignment Robustness to Improve Low-quality Face Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Saeed Ebrahimi Saadabadi*",
      "Sahar Rahimi Malakshan",
      "Ali Dabouei",
      "Nasser Nasrabadi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4907_ECCV_2024_paper.php": {
    "title": "Learning Diffusion Models for Multi-View Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chieh Liu*",
      "Yu-Min Chu*",
      "Ting-I Hsieh*",
      "Hwann-Tzong Chen*",
      "Tyng-Luh Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4908_ECCV_2024_paper.php": {
    "title": "Clearer Frames, Anytime: Resolving Velocity Ambiguity in Video Frame Interpolation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihang Zhong",
      "Gurunandan Krishnan",
      "Xiao Sun",
      "Yu Qiao",
      "Sizhuo Ma*",
      "Jian Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4910_ECCV_2024_paper.php": {
    "title": "Multi-modal Relation Distillation for Unified 3D Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huiqun Wang",
      "Yiping Bao",
      "Panwang Pan",
      "Zeming Li",
      "Xiao Liu",
      "Ruijie Yang",
      "Di Huang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4914_ECCV_2024_paper.php": {
    "title": "Strengthening Multimodal Large Language Model with Bootstrapped Preference Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Renjie Pi*",
      "Tianyang Han",
      "Wei Xiong",
      "Jipeng ZHANG",
      "Runtao Liu",
      "Rui Pan",
      "Tong Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4917_ECCV_2024_paper.php": {
    "title": "Collaborative Vision-Text Representation Optimizing for Open-Vocabulary Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyu Jiao*",
      "hongguang Zhu",
      "Yunchao Wei",
      "Yao Zhao*",
      "Jiannan Huang",
      "Humphrey Shi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4926_ECCV_2024_paper.php": {
    "title": "Distributionally Robust Loss for Long-Tailed Multi-Label Image Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dekun Lin*",
      "Zhe Cui",
      "Rui Chen",
      "Tailai Peng",
      "xinran xie",
      "Xiaolin Qin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4927_ECCV_2024_paper.php": {
    "title": "MesonGS: Post-training Compression of 3D Gaussians via Efficient Attribute Transformation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuzhao Xie*",
      "Weixiang Zhang",
      "Chen Tang",
      "Yunpeng Bai",
      "Rongwei Lu",
      "Shjia Ge",
      "Zhi Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4936_ECCV_2024_paper.php": {
    "title": "LongVLM: Efficient Long Video Understanding via Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuetian Weng",
      "Mingfei Han",
      "Haoyu He",
      "Xiaojun Chang",
      "Bohan Zhuang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4939_ECCV_2024_paper.php": {
    "title": "The All-Seeing Project V2: Towards General Relation Comprehension of the Open World",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiyun Wang",
      "yiming ren",
      "Haowen Luo",
      "Tiantong Li",
      "Chenxiang Yan",
      "Zhe Chen",
      "Wenhai Wang",
      "Qingyun Li",
      "Lewei Lu",
      "Xizhou Zhu",
      "Yu Qiao",
      "Jifeng Dai*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4943_ECCV_2024_paper.php": {
    "title": "Neural Metamorphosis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyi Yang*",
      "Xinchao Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4945_ECCV_2024_paper.php": {
    "title": "WHAC: World-grounded Humans and Cameras",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wanqi Yin",
      "Zhongang Cai",
      "Chen Wei",
      "Fanzhou Wang",
      "Ruisi Wang",
      "Haiyi Mei",
      "Weiye Xiao",
      "Zhitao Yang",
      "Qingping Sun",
      "Atsushi Yamashita",
      "Ziwei Liu",
      "Lei Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4952_ECCV_2024_paper.php": {
    "title": "Federated Learning with Local Openset Noisy Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zonglin Di*",
      "Zhaowei Zhu",
      "Xiaoxiao Li",
      "Yang Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4960_ECCV_2024_paper.php": {
    "title": "Diff3DETR: Agent-based Diffusion Model for Semi-supervised 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiacheng Deng*",
      "Jiahao Lu",
      "Tianzhu Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4964_ECCV_2024_paper.php": {
    "title": "PSALM: Pixelwise Segmentation with Large Multi-modal Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheng Zhang",
      "yeyao ma",
      "Enming Zhang",
      "Xiang Bai*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4969_ECCV_2024_paper.php": {
    "title": "Layout-Corrector: Alleviating Layout Sticking Phenomenon in Discrete Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shoma Iwai*",
      "Atsuki Osanai",
      "Shunsuke Kitada",
      "Shinichiro Omachi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4970_ECCV_2024_paper.php": {
    "title": "Active Coarse-to-Fine Segmentation of Moveable Parts from Real Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruiqi Wang*",
      "Akshay Gadi Patil",
      "Fenggen Yu",
      "Hao Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4975_ECCV_2024_paper.php": {
    "title": "Topo4D: Topology-Preserving Gaussian Splatting for High-Fidelity 4D Head Capture",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuanchen Li",
      "Yuhao Cheng",
      "Xingyu Ren",
      "Haozhe Jia",
      "Di Xu",
      "Wenhan Zhu",
      "Yichao Yan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4978_ECCV_2024_paper.php": {
    "title": "Learning Modality-agnostic Representation for Semantic Segmentation from Any Modalities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xu Zheng*",
      "Yuanhuiyi Lyu",
      "Lin Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5000_ECCV_2024_paper.php": {
    "title": "Kinetic Typography Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seonmi Park",
      "Inhwan Bae",
      "Seunghyun Shin",
      "Hae-Gon Jeon*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5008_ECCV_2024_paper.php": {
    "title": "Refine, Discriminate and Align: Stealing Encoders via Sample-Wise Prototypes and Multi-Relational Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuchi Wu*",
      "Chuan Ma*",
      "Kang Wei*",
      "Xiaogang XU",
      "Ming Ding",
      "Yuwen Qian",
      "Di Xiao",
      "Tao Xiang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5009_ECCV_2024_paper.php": {
    "title": "Light-in-Flight for a World-in-Motion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jongho Lee*",
      "Ryan J Suess",
      "Mohit Gupta"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5013_ECCV_2024_paper.php": {
    "title": "GroupDiff: Diffusion-based Group Portrait Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuming Jiang",
      "Nanxuan Zhao*",
      "Qing Liu",
      "Krishna Kumar Singh",
      "Shuai Yang",
      "Chen Change Loy",
      "Ziwei Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5019_ECCV_2024_paper.php": {
    "title": "Faceptor: A Generalist Model for Face Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lixiong Qin*",
      "Mei Wang",
      "Xuannan Liu",
      "Yuhang Zhang",
      "Wei Deng",
      "Xiaoshuai Song",
      "Weiran Xu*",
      "Weihong Deng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5026_ECCV_2024_paper.php": {
    "title": "Inter-Class Topology Alignment for Efficient Black-Box Substitute Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingzhuang Meng",
      "Mingwen Shao*",
      "Yuanjian Qiao",
      "Wenjie Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5027_ECCV_2024_paper.php": {
    "title": "Segment3D: Learning Fine-Grained Class-Agnostic 3D Segmentation without Manual Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Huang",
      "Songyou Peng",
      "Ayca Takmaz",
      "Federico Tombari",
      "Marc Pollefeys",
      "Shiji Song",
      "Gao Huang*",
      "Francis Engelmann"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5030_ECCV_2024_paper.php": {
    "title": "InsMapper: Exploring Inner-instance Information for Vectorized HD Mapping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "zhenhua xu*",
      "Kwan-Yee K. Wong",
      "Hengshuang Zhao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5033_ECCV_2024_paper.php": {
    "title": "KDProR: A Knowledge-Decoupling Probabilistic Framework for Video-Text Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xianwei Zhuang*",
      "Hongxiang Li",
      "Xuxin Cheng",
      "Zhihong Zhu",
      "Yuxin Xie",
      "Yuexian Zou"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5039_ECCV_2024_paper.php": {
    "title": "Category-level Object Detection, Pose Estimation and Reconstruction from Stereo Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuanrui Zhang*",
      "Yonggen Ling*",
      "Minglei Lu",
      "Minghan Qin",
      "Haoqian Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5042_ECCV_2024_paper.php": {
    "title": "Learning with Unmasked Tokens Drives Stronger Vision Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taekyung Kim*",
      "Sanghyuk Chun",
      "Byeongho Heo",
      "Dongyoon Han*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5047_ECCV_2024_paper.php": {
    "title": "Dual-stage Hyperspectral Image Classification Model with Spectral Supertoken",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peifu Liu",
      "Tingfa Xu*",
      "Jie Wang",
      "Huan Chen",
      "Huiyan Bai",
      "Jianan Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5059_ECCV_2024_paper.php": {
    "title": "Multi-Task Domain Adaptation for Language Grounding with 3D Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Penglei Sun",
      "Yaoxian Song",
      "Xinglin Pan",
      "Peijie Dong",
      "Xiaofei Yang",
      "Qiang Wang*",
      "Zhixu Li",
      "Tiefeng Li",
      "Xiaowen Chu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5060_ECCV_2024_paper.php": {
    "title": "Efficient Active Domain Adaptation for Semantic Segmentation by Selecting Information-rich Superpixels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Gao",
      "Zilei Wang*",
      "Yixin Zhang",
      "Bohai Tu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5068_ECCV_2024_paper.php": {
    "title": "Efficient Training of Spiking Neural Networks with Multi-Parallel Implicit Stream Architecture",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhigao Cao",
      "Meng Li",
      "Xiashuang Wang",
      "Haoyu Wang",
      "Fan Wang",
      "Youjun Li",
      "Zigang Huang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5072_ECCV_2024_paper.php": {
    "title": "Camera-LiDAR Cross-modality Gait Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenxuan Guo*",
      "Yingping Liang",
      "Zhiyu Pan",
      "Ziheng Xi",
      "Jianjiang Feng",
      "Jie Zhou"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5077_ECCV_2024_paper.php": {
    "title": "LiteSAM is Actually what you Need for segment Everything",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianhai Fu",
      "Yuanjie Yu",
      "Ningchuan Li*",
      "Yi Zhang",
      "Qichao Chen",
      "Jianping Xiong",
      "Jun Yin",
      "Zhiyu Xiang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5081_ECCV_2024_paper.php": {
    "title": "IGNORE: Information Gap-based False Negative Loss Rejection for Single Positive Multi-Label Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gyeong Ryeol Song",
      "Noo-ri Kim",
      "Jin-Seop Lee",
      "Jee-Hyong Lee*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5082_ECCV_2024_paper.php": {
    "title": "Visual Prompting via Partial Optimal Transport",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengyu Zheng*",
      "Zhiwei Hao",
      "Yehui Tang",
      "Chang Xu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5085_ECCV_2024_paper.php": {
    "title": "Modelling Competitive Behaviors in Autonomous Driving Under Generative World Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guanren Qiao",
      "Guiliang Liu*",
      "Guorui Quan",
      "Rongxiao Qu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5086_ECCV_2024_paper.php": {
    "title": "Tendency-driven Mutual Exclusivity for Weakly Supervised Incremental Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chongjie Si",
      "Xuehui Wang",
      "Xiaokang Yang",
      "Wei Shen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5096_ECCV_2024_paper.php": {
    "title": "AdaCLIP: Adapting CLIP with Hybrid Learnable Prompts for Zero-Shot Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunkang Cao*",
      "Jiangning Zhang",
      "Luca Frittoli",
      "Yuqi Cheng",
      "Weiming Shen*",
      "Giacomo Boracchi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5104_ECCV_2024_paper.php": {
    "title": "Pathformer3D: A 3D Scanpath Transformer for 360° Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rong Quan",
      "yantao Lai",
      "Mengyu Qiu",
      "Dong Liang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5117_ECCV_2024_paper.php": {
    "title": "TransFusion -- A Transparency-Based Diffusion Model for Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matic Fučka*",
      "Vitjan Zavrtanik",
      "Danijel Skočaj"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5130_ECCV_2024_paper.php": {
    "title": "SparseLIF: High-Performance Sparse LiDAR-Camera Fusion for 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongcheng Zhang",
      "Liu Liang",
      "Pengxin Zeng*",
      "Xiao Song",
      "Zhe Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5133_ECCV_2024_paper.php": {
    "title": "3D Gaussian Parametric Head Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuelang Xu",
      "Lizhen Wang",
      "Zerong Zheng",
      "Zhaoqi Su",
      "Yebin Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5141_ECCV_2024_paper.php": {
    "title": "RING-NeRF : Rethinking Inductive Biases for Versatile and Efficient Neural Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Doriand Petit*",
      "Steve Bourgeois",
      "Dumitru Pavel",
      "Vincent Gay-Bellile",
      "Florian Chabot",
      "Loïc Barthe"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5143_ECCV_2024_paper.php": {
    "title": "Platypus: A Generalized Specialist Model for Reading Text in Various Forms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peng Wang",
      "Zhaohai Li",
      "Jun Tang",
      "Humen Zhong",
      "Fei Huang",
      "Zhibo Yang*",
      "Cong Yao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5154_ECCV_2024_paper.php": {
    "title": "Structured-NeRF: Hierarchical Scene Graph with Neural Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhide Zhong",
      "Jiakai Cao",
      "songen gu",
      "Sirui Xie",
      "Liyi Luo",
      "Hao Zhao",
      "Guyue Zhou",
      "Haoang Li",
      "Zike Yan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5155_ECCV_2024_paper.php": {
    "title": "EGIC: Enhanced Low-Bit-Rate Generative Image Compression Guided by Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikolai Körber*",
      "Eduard Kromer",
      "Andreas Siebert",
      "Sascha Hauke",
      "Daniel Mueller-Gritschneder",
      "Björn Schuller"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5164_ECCV_2024_paper.php": {
    "title": "Plug-and-Play Learned Proximal Trajectory for 3D Sparse-View X-Ray Computed Tomography",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Romain Vo*",
      "Julie Escoda",
      "Caroline Vienne",
      "Etienne Decenciere"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5165_ECCV_2024_paper.php": {
    "title": "PPAD: Iterative Interactions of Prediction and Planning for End-to-end Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhili Chen",
      "Maosheng Ye",
      "Shuangjie Xu",
      "Tongyi Cao",
      "Qifeng Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5175_ECCV_2024_paper.php": {
    "title": "Test-Time Stain Adaptation with Diffusion Models for Histopathology Image Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng-Chang Tsai*",
      "Yuan-Chih Chen",
      "Chun-Shien Lu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5182_ECCV_2024_paper.php": {
    "title": "Beyond MOT: Semantic Multi-Object Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunhao Li",
      "Qin Li",
      "Hao Wang",
      "Xue Ma",
      "Jiali Yao",
      "Shaohua Dong",
      "Heng Fan",
      "Libo Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5185_ECCV_2024_paper.php": {
    "title": "Temporal Event Stereo via Joint Learning with Stereoscopic Flow",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hoonhee Cho",
      "Jae-Young Kang",
      "Kuk-Jin Yoon*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5187_ECCV_2024_paper.php": {
    "title": "SAM-COD: SAM-guided Unified Framework for Weakly-Supervised Camouflaged Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huafeng Chen",
      "Pengxu Wei",
      "Guangqian Guo",
      "Shan Gao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5190_ECCV_2024_paper.php": {
    "title": "Just a Hint: Point-Supervised Camouflaged Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huafeng Chen",
      "Dian SHAO*",
      "Guangqian Guo",
      "shan gao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5194_ECCV_2024_paper.php": {
    "title": "ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guanxing Lu",
      "Shiyi Zhang",
      "Ziwei Wang*",
      "Changliu Liu",
      "Jiwen Lu",
      "Yansong Tang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5197_ECCV_2024_paper.php": {
    "title": "Global-Local Collaborative Inference with LLM for Lidar-Based Open-Vocabulary Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyu Peng",
      "Yan Bai",
      "Chen Gao",
      "Lirong Yang",
      "Fei Xia",
      "Beipeng Mu",
      "Xiaofei Wang",
      "Si Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5200_ECCV_2024_paper.php": {
    "title": "Learning High-resolution Vector Representation from Multi-Camera Images for 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhili Chen",
      "Shuangjie Xu",
      "Maosheng Ye",
      "Zian Qian",
      "Xiaoyi Zou",
      "Dit-Yan Yeung",
      "Qifeng Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5210_ECCV_2024_paper.php": {
    "title": "View-Consistent 3D Editing with Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Wang*",
      "Xuanyu Yi",
      "Zike Wu",
      "Na Zhao",
      "Long Chen",
      "Hanwang Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5216_ECCV_2024_paper.php": {
    "title": "E3V-K5: An Authentic Benchmark for Redefining Video-Based Energy Expenditure Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengxuming Zhang",
      "Lei Jin",
      "Yifan Wang",
      "Xinyu Wang",
      "Xu Wen",
      "Zunlei Feng*",
      "Mingli Song"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5218_ECCV_2024_paper.php": {
    "title": "GeoGaussian: Geometry-aware Gaussian Splatting for Scene Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanyan Li*",
      "Chenyu Lyu",
      "Yan Di",
      "Guangyao Zhai",
      "Gim Hee Lee",
      "Federico Tombari"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5236_ECCV_2024_paper.php": {
    "title": "URS-NeRF: Unordered Rolling Shutter Bundle Adjustment for Neural Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Xu*",
      "Liu Ziao",
      "Mengqi Guo",
      "jiancheng Li",
      "Gim Hee Lee"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5238_ECCV_2024_paper.php": {
    "title": "InstructIR: High-Quality Image Restoration Following Human Instructions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marcos V. Conde*",
      "Gregor Geigle",
      "Radu Timofte"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5239_ECCV_2024_paper.php": {
    "title": "Asynchronous Large Language Model Enhanced Planner for Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Chen",
      "Zi-han Ding",
      "Ziqin Wang",
      "Yan Wang*",
      "Lijun Zhang",
      "Si Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5243_ECCV_2024_paper.php": {
    "title": "Make a Cheap Scaling: A Self-Cascade Diffusion Model for Higher-Resolution Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lanqing Guo",
      "Yingqing HE",
      "Haoxin Chen",
      "Menghan Xia",
      "Xiaodong Cun",
      "Yufei Wang",
      "Siyu Huang",
      "Yong Zhang",
      "Xintao Wang",
      "Qifeng Chen",
      "Ying Shan",
      "Bihan Wen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5246_ECCV_2024_paper.php": {
    "title": "LayoutFlow: Flow Matching for Layout Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julian Jorge Andrade Guerreiro*",
      "Naoto Inoue*",
      "Kento Masui",
      "Mayu Otani",
      "Hideki Nakayama"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5259_ECCV_2024_paper.php": {
    "title": "Making Large Language Models Better Planners with Reasoning-Decision Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhijian Huang",
      "Tao Tang",
      "Shaoxiang Chen",
      "Sihao Lin",
      "Zequn Jie",
      "Lin Ma",
      "Guangrun Wang",
      "Xiaodan Liang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5272_ECCV_2024_paper.php": {
    "title": "R3D-AD: Reconstruction via Diffusion for 3D Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheyuan Zhou",
      "Le Wang",
      "Naiyu Fang",
      "Zili Wang",
      "Lemiao Qiu*",
      "Shuyou Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5278_ECCV_2024_paper.php": {
    "title": "Representation Enhancement-Stabilization: Reducing Bias-Variance of Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Huang*",
      "Yilei Shi",
      "Zhitong Xiong",
      "Xiao Xiang Zhu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5279_ECCV_2024_paper.php": {
    "title": "Continual Learning for Remote Physiological Measurement: Minimize Forgetting and Simplify Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qian Liang",
      "Yan Chen",
      "Yang Hu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5282_ECCV_2024_paper.php": {
    "title": "An Optimization Framework to Enforce Multi-View Consistency for Texturing 3D Meshes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengyi Zhao",
      "Chen Song",
      "Xiaodong Gu",
      "Yuan Dong",
      "Qi Zuo",
      "Weihao Yuan",
      "Zilong Dong*",
      "Liefeng Bo",
      "Qixing Huang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5288_ECCV_2024_paper.php": {
    "title": "STAG4D: Spatial-Temporal Anchored Generative 4D Gaussians",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei Zeng",
      "Yanqin Jiang",
      "Siyu Zhu",
      "Yuanxun Lu",
      "Youtian Lin",
      "Hao Zhu",
      "Weiming Hu",
      "Xun Cao",
      "Yao Yao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5296_ECCV_2024_paper.php": {
    "title": "RGBD GS-ICP SLAM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seongbo Ha",
      "Jiung Yeon",
      "Hyeonwoo Yu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5300_ECCV_2024_paper.php": {
    "title": "Efficient NeRF Optimization - Not All Samples Remain Equally Hard",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juuso Korhonen*",
      "Goutham Rangu",
      "Hamed Rezazadegan Tavakoli",
      "Juho Kannala"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5302_ECCV_2024_paper.php": {
    "title": "Revisiting Calibration of Wide-Angle Radially Symmetric Cameras",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrea Porfiri Dal Cin*",
      "Francesco Azzoni",
      "Giacomo Boracchi",
      "Luca Magri*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5305_ECCV_2024_paper.php": {
    "title": "Rawformer: Unpaired Raw-to-Raw Translation for Learnable Camera ISPs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Georgy Perevozchikov*",
      "Nancy Mehta*",
      "Mahmoud Afifi*",
      "Radu Timofte*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5311_ECCV_2024_paper.php": {
    "title": "Robust Incremental Structure-from-Motion with Hybrid Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaohui Liu*",
      "Yidan Gao",
      "Tianyi Zhang",
      "Rémi Pautrat",
      "Johannes L Schönberger",
      "Viktor Larsson",
      "Marc Pollefeys"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5312_ECCV_2024_paper.php": {
    "title": "Revisiting Domain-Adaptive Object Detection in Adverse Weather by the Generation and Composition of High-Quality Pseudo-Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Zhao",
      "Huibin Yan",
      "Shuoyao Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5316_ECCV_2024_paper.php": {
    "title": "Prediction Exposes Your Face: Black-box Model Inversion via Prediction Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufan Liu*",
      "Wanqian Zhang",
      "Dayan Wu",
      "Zheng Lin",
      "jingzi Gu",
      "Weiping Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5324_ECCV_2024_paper.php": {
    "title": "Noise Calibration: Plug-and-play Content-Preserving Video Enhancement using Pre-trained Video Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qinyu Yang",
      "Haoxin Chen",
      "Yong Zhang*",
      "Menghan Xia",
      "Xiaodong Cun",
      "Zhixun Su*",
      "Ying Shan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5325_ECCV_2024_paper.php": {
    "title": "UniCal: Unified Neural Sensor Calibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ze Yang*",
      "George G Chen",
      "Haowei Zhang",
      "Kevin Ta",
      "Ioan Andrei Bârsan",
      "Daniel Murphy",
      "Sivabalan Manivasagam*",
      "Raquel Urtasun*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5329_ECCV_2024_paper.php": {
    "title": "Mind the Interference: Retaining Pre-trained Knowledge in Parameter Efficient Continual Learning of Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Longxiang Tang*",
      "Zhuotao Tian",
      "Kai Li",
      "Chunming He",
      "Hantao Zhou",
      "Hengshuang Zhao",
      "Xiu Li",
      "Jiaya Jia"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5333_ECCV_2024_paper.php": {
    "title": "Urban Waterlogging Detection: A Challenging Benchmark and Large-Small Model Co-Adapter",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suqi Song",
      "Chenxu Zhang",
      "Peng Zhang",
      "Pengkun Li",
      "Fenglong Song",
      "Lei Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5346_ECCV_2024_paper.php": {
    "title": "Pseudo-Embedding for Generalized Few-Shot Point Cloud Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chih-Jung Tsai",
      "Hwann-Tzong Chen*",
      "Tyng-Luh Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5355_ECCV_2024_paper.php": {
    "title": "WSI-VQA: Interpreting Whole Slide Images by Generative Visual Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pingyi Chen*",
      "Chenglu Zhu",
      "Sunyi Zheng",
      "Honglin Li",
      "Lin Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5358_ECCV_2024_paper.php": {
    "title": "ReMoS: 3D Motion-Conditioned Reaction Synthesis for Two-Person Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anindita Ghosh*",
      "Rishabh Dabral",
      "Vladislav Golyanik",
      "Christian Theobalt",
      "Philipp Slusallek"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5359_ECCV_2024_paper.php": {
    "title": "Statewide Visual Geolocalization in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Florian Fervers*",
      "Sebastian Bullinger",
      "Christoph Bodensteiner",
      "Michael Arens",
      "Rainer Stiefelhagen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5360_ECCV_2024_paper.php": {
    "title": "Any2Point: Empowering Any-modality Transformers for Efficient 3D Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiwen Tang",
      "Ray Zhang",
      "Jiaming Liu",
      "Zoey Guo",
      "Bin Zhao*",
      "Zhigang Wang",
      "Dong Wang*",
      "Peng Gao",
      "Hongsheng Li",
      "Xuelong Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5362_ECCV_2024_paper.php": {
    "title": "Trajectory-aligned Space-time Tokens for Few-shot Action Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pulkit Kumar*",
      "Namitha Padmanabhan",
      "Luke Luo",
      "Sai Saketh Rambhatla",
      "Abhinav Shrivastava"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5363_ECCV_2024_paper.php": {
    "title": "EgoCVR: An Egocentric Benchmark for Fine-Grained Composed Video Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Hummel*",
      "Shyamgopal Karthik",
      "Mariana-Iuliana Georgescu",
      "Zeynep Akata"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5369_ECCV_2024_paper.php": {
    "title": "Synchronization of Projective Transformations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rakshith Madhavan*",
      "Andrea Fusiello",
      "Federica Arrigoni"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5370_ECCV_2024_paper.php": {
    "title": "TLControl: Trajectory and Language Control for Human Motion Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weilin Wan*",
      "Zhiyang Dou",
      "Taku Komura",
      "Wenping Wang",
      "Dinesh Jayaraman",
      "Lingjie Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5373_ECCV_2024_paper.php": {
    "title": "Insect Identification in the Wild: The AMI Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditya Jain*",
      "Fagner Cunha",
      "Michael J Bunsen",
      "Juan Sebastián Cañas",
      "Léonard Pasi",
      "Nathan Pinoy",
      "Flemming Helsing",
      "JoAnne Russo",
      "Marc S Botham",
      "Michael Sabourin",
      "Jonathan Fréchette",
      "Alexandre Anctil",
      "Yacksecari Lopez",
      "Eduardo Navarro",
      "Filonila Pérez",
      "Ana C Zamora",
      "Jose Alejandro Ramirez-Silva",
      "Jonathan Gagnon",
      "Tom A August",
      "Kim Bjerge",
      "Alba Gomez Segura",
      "Marc Belisle",
      "Yves Basset",
      "Kent P McFarland",
      "David B Roy",
      "Toke T Høye",
      "Maxim Larrivee",
      "David Rolnick"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5379_ECCV_2024_paper.php": {
    "title": "Cross-view image geo-localization with Panorama-BEV Co-Retrieval Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyan Ye",
      "Zhutao Lv",
      "Weijia Li*",
      "Jinhua Yu",
      "Haote Yang",
      "Huaping Zhong",
      "Conghui He*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5386_ECCV_2024_paper.php": {
    "title": "F-HOI: Toward Fine-grained Semantic-Aligned 3D Human-Object Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Yang",
      "Xuesong Niu",
      "Nan Jiang",
      "Ruimao Zhang*",
      "Siyuan Huang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5391_ECCV_2024_paper.php": {
    "title": "Test-time Model Adaptation for Image Reconstruction Using Self-supervised Adaptive Layers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yutian Zhao",
      "Tianjing Zhang",
      "Hui Ji*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5400_ECCV_2024_paper.php": {
    "title": "SHIC: Shape-Image Correspondences with no Keypoint Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aleksandar Shtedritski*",
      "Christian Rupprecht",
      "Andrea Vedaldi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5406_ECCV_2024_paper.php": {
    "title": "GenRC: Generative 3D Room Completion from Sparse Image Collections",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ming-Feng Li*",
      "Yueh-Feng Ku",
      "Hong-Xuan Yen",
      "Chi Liu",
      "Yu-Lun Liu",
      "Albert Y Chen",
      "Cheng-Hao Kuo",
      "Min Sun"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5407_ECCV_2024_paper.php": {
    "title": "A Probability-guided Sampler for Neural Implicit Surface Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gonçalo José Dias Pais",
      "Valter André Piedade",
      "Moitreya Chatterjee",
      "Marcus Greiff",
      "Pedro Miraldo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5411_ECCV_2024_paper.php": {
    "title": "ReMatching: Low-Resolution Representations for Scalable Shape Correspondence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Filippo Maggioli*",
      "Daniele Baieri",
      "Emanuele Rodola",
      "Simone Melzi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5418_ECCV_2024_paper.php": {
    "title": "Where am I? Scene Retrieval with Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqi Chen*",
      "Daniel Barath",
      "Iro Armeni",
      "Marc Pollefeys",
      "Hermann Blum"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5439_ECCV_2024_paper.php": {
    "title": "This Probably Looks Exactly Like That: An Invertible Prototypical Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zachariah Carmichael*",
      "Timothy P Redgrave",
      "Daniel Gonzalez Cedre",
      "Walter Scheirer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5442_ECCV_2024_paper.php": {
    "title": "Arc2Face: A Foundation Model for ID-Consistent Human Faces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Foivos Paraperas Papantoniou*",
      "Alexandros Lattas",
      "Stylianos Moschoglou",
      "Jiankang Deng",
      "Bernhard Kainz",
      "Stefanos Zafeiriou"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5446_ECCV_2024_paper.php": {
    "title": "PhysAvatar: Learning the Physics of Dressed 3D Avatars from Visual Observations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Zheng*",
      "Qingqing Zhao",
      "Guandao Yang",
      "Wang Yifan",
      "Donglai Xiang",
      "Florian Dubost",
      "Dmitry Lagun",
      "Thabo Beeler",
      "Federico Tombari",
      "Leonidas Guibas",
      "Gordon Wetzstein"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5452_ECCV_2024_paper.php": {
    "title": "Revisiting Feature Disentanglement Strategy in Diffusion Training and Breaking Conditional Independence Assumption in Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wonwoong Cho*",
      "Hareesh Ravi*",
      "Midhun Harikumar",
      "Vinh Khuc",
      "Krishna Kumar Singh",
      "Jingwan Lu",
      "David Iseri Inouye*",
      "Ajinkya Kale*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5454_ECCV_2024_paper.php": {
    "title": "SweepNet: Unsupervised Learning Shape Abstraction via Neural Sweepers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingrui Zhao*",
      "Yizhi Wang",
      "Fenggen Yu",
      "Changqing Zou",
      "Ali Mahdavi-Amiri"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5459_ECCV_2024_paper.php": {
    "title": "Leveraging Thermal Modality to Enhance Reconstruction in Low-Light Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiacong Xu*",
      "Mingqian Liao",
      "Ram Prabhakar Kathirvel",
      "Vishal Patel"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5462_ECCV_2024_paper.php": {
    "title": "On the Viability of Monocular Depth Pre-training for Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dong Lao*",
      "Fengyu Yang",
      "Daniel Wang",
      "Hyoungseob Park",
      "Samuel Lu",
      "Alex Wong",
      "Stefano Soatto"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5464_ECCV_2024_paper.php": {
    "title": "Fairness-aware Vision Transformer via Debiased Self-Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yao Qiang",
      "Chengyin Li",
      "Prashant Khanduri",
      "Dongxiao Zhu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5469_ECCV_2024_paper.php": {
    "title": "EgoPet: Egomotion and Interaction Data from an Animal's Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amir Bar*",
      "Arya Bakhtiar",
      "Danny L Tran",
      "Antonio Loquercio",
      "Jathushan Rajasegaran",
      "yann lecun",
      "Amir Globerson",
      "Trevor Darrell"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5472_ECCV_2024_paper.php": {
    "title": "Deep Companion Learning: Enhancing Generalization Through Historical Consistency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruizhao Zhu*",
      "Venkatesh Saligrama*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5476_ECCV_2024_paper.php": {
    "title": "Neural graphics texture compression supporting random access",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Farzad Farhadzadeh*",
      "Qiqi Hou",
      "Hoang Le",
      "Amir Said",
      "Randall R Rauwendaal",
      "Alex Bourd",
      "Fatih Porikli"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5479_ECCV_2024_paper.php": {
    "title": "Contrastive Learning with Synthetic Positives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dewen Zeng*",
      "Xinrong Hu",
      "Yawen Wu",
      "Xiaowei Xu",
      "Yiyu Shi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5481_ECCV_2024_paper.php": {
    "title": "GeneralAD: Anomaly Detection Across Domains by Attending to Distorted Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luc P.J. Sträter*",
      "Mohammadreza Salehi",
      "Efstratios Gavves",
      "Cees G.M. Snoek",
      "Yuki M. Asano"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5486_ECCV_2024_paper.php": {
    "title": "Interpretability-Guided Test-Time Adversarial Defense",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akshay Kulkarni*",
      "Tsui-Wei Weng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5491_ECCV_2024_paper.php": {
    "title": "DIM: Dyadic Interaction Modeling for Social Behavior Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minh Tran*",
      "Di Chang",
      "Maksim Siniukov",
      "Mohammad Soleymani"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5492_ECCV_2024_paper.php": {
    "title": "Tri^{2}-plane: Thinking Head Avatar via Feature Pyramid",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luchuan Song*",
      "Pinxin Liu",
      "Lele Chen",
      "Guojun Yin",
      "Chenliang Xu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5494_ECCV_2024_paper.php": {
    "title": "ControlCap: Controllable Region-level Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuzhong Zhao",
      "Liu Yue",
      "Zonghao Guo",
      "weijia wu",
      "Chen Gong",
      "Qixiang Ye",
      "Fang Wan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5495_ECCV_2024_paper.php": {
    "title": "Free Lunch for Gait Recognition: A Novel Relation Descriptor",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jilong Wang*",
      "Saihui Hou",
      "Yan Huang",
      "Chunshui Cao",
      "Xu Liu",
      "Yongzhen Huang",
      "Tianzhu Zhang",
      "Liang Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5498_ECCV_2024_paper.php": {
    "title": "SegVG: Transferring Object Bounding Box to Segmentation for Visual Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weitai Kang*",
      "Gaowen Liu",
      "Mubarak Shah",
      "Yan Yan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5502_ECCV_2024_paper.php": {
    "title": "Adaptive Correspondence Scoring for Unsupervised Medical Image Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoran Zhang*",
      "John C. Stendahl",
      "Lawrence H. Staib",
      "Albert J. Sinusas",
      "Alex Wong",
      "James S. Duncan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5506_ECCV_2024_paper.php": {
    "title": "MaxFusion: Plug&Play Multi-Modal Generation in Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nithin Gopalakrishnan Nair*",
      "Jeya Maria Jose Valanarasu",
      "Vishal Patel"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5512_ECCV_2024_paper.php": {
    "title": "Watch Your Steps: Local Image and Scene Editing by Text Instructions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashkan Mirzaei*",
      "Tristan T Aumentado-Armstrong",
      "Marcus A Brubaker",
      "Jonathan Kelly",
      "Alex Levinshtein",
      "Konstantinos G Derpanis",
      "Igor Gilitschenski"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5513_ECCV_2024_paper.php": {
    "title": "Forget More to Learn More: Domain-specific Feature Unlearning for Semi-supervised and Unsupervised Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hritam Basak*",
      "Zhaozheng Yin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5514_ECCV_2024_paper.php": {
    "title": "3x2: 3D Object Part Segmentation by 2D Semantic Correspondences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anh Thai*",
      "Weiyao Wang",
      "Hao Tang",
      "Stefan Stojanov",
      "James M Rehg",
      "Matt Feiszli"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5515_ECCV_2024_paper.php": {
    "title": "Idea2Img: Iterative Self-Refinement with GPT-4V for Automatic Image Design and Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengyuan Yang*",
      "Jianfeng Wang",
      "Linjie Li",
      "Kevin Lin",
      "Chung-Ching Lin",
      "Zicheng Liu",
      "Lijuan Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5518_ECCV_2024_paper.php": {
    "title": "Human-in-the-Loop Visual Re-ID for Population Size Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gustavo Perez*",
      "Daniel Sheldon",
      "Grant Van Horn",
      "Subhransu Maji"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5519_ECCV_2024_paper.php": {
    "title": "SEGIC: Unleashing the Emergent Correspondence for In-Context Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingchen Meng",
      "Shiyi Lan",
      "Hengduo Li",
      "Jose M Alvarez",
      "Zuxuan Wu*",
      "Yu-Gang Jiang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5521_ECCV_2024_paper.php": {
    "title": "PointNeRF++: A multi-scale, point-based Neural Radiance Field",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiwei Sun",
      "Eduard Trulls",
      "Yang-Che Tseng",
      "Sneha Sambandam",
      "Gopal Sharma",
      "Andrea Tagliasacchi",
      "Kwang Moo Yi*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5527_ECCV_2024_paper.php": {
    "title": "A Semantic Space is Worth 256 Language Descriptions: Make Stronger Segmentation Models with Descriptive Properties",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junfei Xiao",
      "Ziqi Zhou",
      "Wenxuan Li",
      "Shiyi Lan",
      "Jieru Mei",
      "Zhiding Yu",
      "Bingchen Zhao",
      "Alan Yuille",
      "Yuyin Zhou",
      "Cihang Xie*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5528_ECCV_2024_paper.php": {
    "title": "UMG-CLIP: A Unified Multi-Granularity Vision Generalist for Open-World Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowen Shi",
      "Peisen Zhao",
      "Zichen Wang",
      "Yuhang Zhang",
      "Yaoming Wang",
      "Jin Li",
      "Wenrui Dai",
      "Junni Zou",
      "Hongkai Xiong",
      "Qi Tian",
      "Xiaopeng Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5533_ECCV_2024_paper.php": {
    "title": "Fast View Synthesis of Casual Videos with Soup-of-Planes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yao-Chih Lee*",
      "Zhoutong Zhang",
      "Kevin Blackburn-Matzen",
      "Simon Niklaus",
      "Jianming Zhang",
      "Jia-Bin Huang",
      "Feng Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5542_ECCV_2024_paper.php": {
    "title": "Adaptive Human Trajectory Prediction via Latent Corridors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Neerja Thakkar*",
      "Karttikeya Mangalam",
      "Andrea Bajcsy",
      "Jitendra Malik"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5543_ECCV_2024_paper.php": {
    "title": "Video Question Answering with Procedural Programs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rohan Choudhury*",
      "Koichiro Niinuma",
      "Kris Kitani",
      "Laszlo A Jeni"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5548_ECCV_2024_paper.php": {
    "title": "DGR-MIL: Exploring Diverse Global Representation in Multiple Instance Learning for Whole Slide Image Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhui Zhu*",
      "Xiwen Chen",
      "Peijie Qiu",
      "Aristeidis Sotiras",
      "Abolfazl Razi",
      "Yalin Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5550_ECCV_2024_paper.php": {
    "title": "TexGen: Text-Guided 3D Texture Generation with Multi-view Sampling and Resampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dong Huo*",
      "Zixin Guo",
      "Xinxin Zuo",
      "Zhihao Shi",
      "Juwei Lu",
      "Peng Dai",
      "Songcen Xu",
      "Li Cheng",
      "Yee-Hong Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5552_ECCV_2024_paper.php": {
    "title": "C2C: Component-to-Composition Learning for Zero-Shot Compositional Action Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rongchang Li",
      "Zhenhua Feng",
      "Tianyang Xu",
      "Linze Li",
      "Xiao-Jun Wu*",
      "Muhammad Awais",
      "Sara Atito",
      "Josef Kittler"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5554_ECCV_2024_paper.php": {
    "title": "LLMGA: Multimodal Large Language Model based Generation Assistant",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "bin xia*",
      "Shiyin Wang",
      "Yingfan Tao",
      "Yitong Wang",
      "Jiaya Jia"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5558_ECCV_2024_paper.php": {
    "title": "Put Myself in Your Shoes: Lifting the Egocentric Perspective from Exocentric Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mi Luo*",
      "Zihui Xue",
      "Alex Dimakis",
      "Kristen Grauman"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5559_ECCV_2024_paper.php": {
    "title": "Shape from Heat Conduction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sriram Narayanan*",
      "Mani Ramanagopal",
      "Mark Sheinin",
      "Aswin C. Sankaranarayanan",
      "Srinivasa G. Narasimhan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5560_ECCV_2024_paper.php": {
    "title": "An Adaptive Screen-Space Meshing Approach for Normal Integration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Moritz Heep*",
      "Eduard Zell"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5562_ECCV_2024_paper.php": {
    "title": "Parrot: Pareto-optimal Multi-Reward Reinforcement Learning Framework for Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seung Hyun Lee*",
      "Yinxiao Li",
      "Junjie Ke",
      "Innfarn Yoo",
      "Han Zhang",
      "Jiahui Yu",
      "Qifei Wang",
      "Fei Deng",
      "Glenn Entis",
      "Junfeng He",
      "Gang Li",
      "Sangpil Kim",
      "Irfan Essa",
      "Feng Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5563_ECCV_2024_paper.php": {
    "title": "HandDGP: Camera-Space Hand Mesh Prediction with Differentiable Global Positioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eugene Valassakis",
      "Guillermo Garcia-Hernando*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5568_ECCV_2024_paper.php": {
    "title": "Towards Latent Masked Image Modeling for Self-Supervised Visual Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yibing Wei*",
      "Abhinav Gupta",
      "Pedro Morgado*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5571_ECCV_2024_paper.php": {
    "title": "Nuvo: Neural UV Mapping for Unruly 3D Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pratul Srinivasan*",
      "Stephan J Garbin",
      "Dor Verbin",
      "Jonathan T Barron",
      "Ben Mildenhall"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5572_ECCV_2024_paper.php": {
    "title": "Towards High-Quality 3D Motion Transfer with Realistic Apparel Animation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rong Wang*",
      "Wei Mao",
      "Changsheng Lu",
      "HONGDONG LI"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5573_ECCV_2024_paper.php": {
    "title": "AnyHome: Open-Vocabulary Large-Scale Indoor Scene Generation with First-Person View Exploration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rao Fu*",
      "Zehao Wen",
      "Zichen Liu ",
      "Srinath Sridhar"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5575_ECCV_2024_paper.php": {
    "title": "Better Call SAL: Towards Learning to Segment Anything in Lidar",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aljosa Osep*",
      "Tim Meinhardt",
      "Francesco Ferroni",
      "Neehar Peri",
      "Deva Ramanan",
      "Laura Leal-Taixé"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5579_ECCV_2024_paper.php": {
    "title": "DGInStyle: Domain-Generalizable Semantic Segmentation with Image Diffusion Models and Stylized Semantic Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuru Jia",
      "Lukas Hoyer",
      "Shengyu Huang",
      "Tianfu Wang",
      "Luc Van Gool",
      "Konrad Schindler",
      "Anton Obukhov*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5580_ECCV_2024_paper.php": {
    "title": "DECOLLAGE: 3D Detailization by Controllable, Localized, and Learned Geometry Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qimin Chen*",
      "Zhiqin Chen",
      "Vladimir G. Kim",
      "Noam Aigerman",
      "Hao Zhang",
      "Siddhartha Chaudhuri"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5581_ECCV_2024_paper.php": {
    "title": "Scene-aware Human Motion Forecasting via Mutual Distance Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaoyue Xing*",
      "Wei Mao",
      "Miaomiao Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5583_ECCV_2024_paper.php": {
    "title": "FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zehao Zhu",
      "Zhiwen Fan*",
      "Yifan Jiang",
      "Zhangyang Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5584_ECCV_2024_paper.php": {
    "title": "Open Panoramic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junwei Zheng",
      "Ruiping Liu",
      "Yufan Chen",
      "Kunyu Peng",
      "Chengzhi Wu",
      "Kailun Yang",
      "Jiaming Zhang*",
      "Rainer Stiefelhagen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5586_ECCV_2024_paper.php": {
    "title": "iMatching: Imperative Correspondence Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zitong Zhan*",
      "Dasong Gao",
      "Yun-Jou Lin",
      "Youjie Xia",
      "Chen Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5589_ECCV_2024_paper.php": {
    "title": "COSMU: Complete 3D human shape from monocular unconstrained images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marco Pesavento*",
      "Marco Volino",
      "Adrian Hilton"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5590_ECCV_2024_paper.php": {
    "title": "MAP-ADAPT: Real-Time Quality-Adaptive Semantic 3D Maps",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianhao Zheng*",
      "Daniel Barath",
      "Marc Pollefeys",
      "Iro Armeni*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5594_ECCV_2024_paper.php": {
    "title": "Appearance-based Refinement for Object-Centric Motion Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyu Xie*",
      "Weidi Xie",
      "Andrew Zisserman"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5598_ECCV_2024_paper.php": {
    "title": "SemiVL: Semi-Supervised Semantic Segmentation with Vision-Language Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lukas Hoyer*",
      "David Joseph Tan",
      "Muhammad Ferjad Naeem",
      "Luc Van Gool",
      "Federico Tombari"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5599_ECCV_2024_paper.php": {
    "title": "Open Vocabulary Multi-Label Video Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rohit Gupta*",
      "Mamshad Nayeem Rizve",
      "Jayakrishnan Unnikrishnan",
      "Ashish Tawari",
      "Son Tran",
      "Mubarak Shah",
      "Benjamin Yao",
      "Trishul A Chilimbi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5600_ECCV_2024_paper.php": {
    "title": "Optimal Transport of Diverse Unsupervised Tasks for Robust Learning from Noisy Few-Shot Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaofan Que",
      "Qi Yu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5607_ECCV_2024_paper.php": {
    "title": "Regularizing Dynamic Radiance Fields with Kinematic Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Woobin Im",
      "Geonho Cha",
      "Sebin Lee",
      "Jumin Lee",
      "Juhyeong Seon",
      "Dongyoon Wee",
      "Sungeui Yoon*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5609_ECCV_2024_paper.php": {
    "title": "MICDrop: Masking Image and Depth Features via Complementary Dropout for Domain-Adaptive Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linyan Yang*",
      "Lukas Hoyer*",
      "Mark Weber",
      "Tobias Fischer",
      "Dengxin Dai",
      "Laura Leal-Taixé",
      "Daniel Cremers",
      "Marc Pollefeys",
      "Luc Van Gool"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5611_ECCV_2024_paper.php": {
    "title": "Efficient Pre-training for Localized Instruction Generation of Procedural Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anil Batra*",
      "Davide Moltisanti",
      "Laura Sevilla-Lara",
      "Marcus Rohrbach",
      "Frank Keller"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5614_ECCV_2024_paper.php": {
    "title": "MTKD: Multi-Teacher Knowledge Distillation for Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Jiang*",
      "Chen Feng",
      "Fan Zhang",
      "David Bull"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5615_ECCV_2024_paper.php": {
    "title": "DEAL: Disentangle and Localize Concept-level Explanations for VLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tang Li*",
      "Mengmeng Ma",
      "Xi Peng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5618_ECCV_2024_paper.php": {
    "title": "Fast Encoding and Decoding for Implicit Video Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Chen*",
      "Saining Xie",
      "Ser-Nam Lim",
      "Abhinav Shrivastava"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5619_ECCV_2024_paper.php": {
    "title": "Surf-D: Generating High-Quality Surfaces of Arbitrary Topologies Using Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengming Yu*",
      "Zhiyang Dou",
      "Xiaoxiao Long",
      "Cheng Lin",
      "Zekun Li",
      "Yuan Liu",
      "Norman Müller",
      "Taku Komura",
      "Marc Habermann",
      "Christian Theobalt",
      "Xin Li",
      "Wenping Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5621_ECCV_2024_paper.php": {
    "title": "Diffusion-Refined VQA Annotations for Semi-Supervised Gaze Following",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiaomu Miao*",
      "Alexandros Graikos",
      "Jingwei Zhang",
      "Sounak Mondal",
      "Minh Hoai",
      "Dimitris Samaras"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5625_ECCV_2024_paper.php": {
    "title": "IMMA: Immunizing text-to-image Models against Malicious Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amber Yijia Zheng*",
      "Raymond A. Yeh"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5635_ECCV_2024_paper.php": {
    "title": "Motion-Oriented Compositional Neural Radiance Fields for Monocular Dynamic Human Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaehyeok Kim",
      "Dongyoon Wee",
      "Dan Xu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5636_ECCV_2024_paper.php": {
    "title": "GeoCalib: Learning Single-image Calibration with Geometric Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Veicht*",
      "Paul-Edouard Sarlin*",
      "Philipp Lindenberger",
      "Marc Pollefeys"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5642_ECCV_2024_paper.php": {
    "title": "3D Open-Vocabulary Panoptic Segmentation with 2D-3D Vision-Language Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihao Xiao*",
      "Longlong Jing",
      "Shangxuan Wu",
      "Alex Zihao Zhu",
      "Jingwei Ji",
      "Chiyu Max Jiang",
      "Wei-Chih Hung",
      "Thomas Funkhouser",
      "Weicheng Kuo",
      "Anelia Angelova",
      "Yin Zhou",
      "Shiwei Sheng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5645_ECCV_2024_paper.php": {
    "title": "Semicalibrated Relative Pose from an Affine Correspondence and Monodepth",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Petr Hruby*",
      "Marc Pollefeys",
      "Daniel Barath"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5646_ECCV_2024_paper.php": {
    "title": "Global Structure-from-Motion Revisited",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linfei Pan*",
      "Daniel Barath",
      "Marc Pollefeys",
      "Johannes L Schönberger"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5647_ECCV_2024_paper.php": {
    "title": "MobileNetV4: Universal Models for the Mobile Ecosystem",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Danfeng Qin*",
      "Chas H Leichner",
      "Manolis Delakis",
      "Marco Fornoni",
      "Shixin Luo",
      "Fan Yang",
      "Weijun Wang",
      "Colby Banbury",
      "Chengxi Ye",
      "Berkin Akin",
      "Vaibhav Aggarwal",
      "Tenghui Zhu",
      "Daniele Moro",
      "Andrew Howard"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5651_ECCV_2024_paper.php": {
    "title": "Gravity-aligned Rotation Averaging with Circular Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linfei Pan*",
      "Marc Pollefeys",
      "Daniel Barath"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5653_ECCV_2024_paper.php": {
    "title": "MoMA: Multimodal LLM Adapter for Fast Personalized Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kunpeng Song*",
      "Yizhe Zhu*",
      "Bingchen Liu*",
      "Qing Yan*",
      "Ahmed Elgammal*",
      "Xiao Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5654_ECCV_2024_paper.php": {
    "title": "Find n' Propagate: Open-Vocabulary 3D Object Detection in Urban Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Djamahl Etchegaray*",
      "Zi Helen Huang",
      "Tatsuya Harada",
      "Yadan Luo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5655_ECCV_2024_paper.php": {
    "title": "Quanta Video Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prateek Chennuri*",
      "Yiheng Chi",
      "Enze Jiang",
      "GM Dilshan Godaliyadda*",
      "Abhiram Gnanasambandam*",
      "Hamid R Sheikh",
      "Istvan Gyongy",
      "Stanley H Chan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5660_ECCV_2024_paper.php": {
    "title": "Concept Sliders: LoRA Adaptors for Precise Control in Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rohit Gandikota*",
      "Joanna Materzynska",
      "Tingrui Zhou",
      "Antonio Torralba",
      "David Bau"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5662_ECCV_2024_paper.php": {
    "title": "CAT-SAM: Conditional Tuning for Few-Shot Adaptation of Segment Anything Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aoran Xiao",
      "Weihao Xuan",
      "Heli Qi",
      "Yun Xing",
      "Ruijie Ren",
      "Xiaoqin Zhang",
      "Ling Shao",
      "Shijian Lu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5664_ECCV_2024_paper.php": {
    "title": "ScribblePrompt: Fast and Flexible Interactive Segmentation for Any Biomedical Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hallee E. Wong*",
      "Marianne Rakic",
      "John Guttag",
      "Adrian V. Dalca"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5670_ECCV_2024_paper.php": {
    "title": "POCA: Post-training Quantization with Temporal Alignment for Codec Avatars",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Meng*",
      "Yuecheng Li*",
      "Leo (Chenghui) Li",
      "Syed Shakib Sarwar",
      "Dilin Wang",
      "Jae-sun Seo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5671_ECCV_2024_paper.php": {
    "title": "HYPE: Hyperbolic Entailment Filtering for Underspecified Images and Texts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wonjae Kim*",
      "Sanghyuk Chun",
      "Taekyung Kim",
      "Dongyoon Han",
      "Sangdoo Yun"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5674_ECCV_2024_paper.php": {
    "title": "Finding Meaning in Points: Weakly Supervised Semantic Segmentation for Event Cameras",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hoonhee Cho",
      "Sung-Hoon Yoon",
      "Hyeokjun Kweon",
      "Kuk-Jin Yoon*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5675_ECCV_2024_paper.php": {
    "title": "Unsupervised Dense Prediction using Differentiable Normalized Cuts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanbin Liu*",
      "Stephen Gould"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5678_ECCV_2024_paper.php": {
    "title": "Boosting the Power of Small Multimodal Reasoning Models to Match Larger Models with Self-Consistency Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Tan*",
      "Jingxuan Wei*",
      "Zhangyang Gao",
      "Linzhuang Sun",
      "Siyuan Li",
      "Ruifeng Guo",
      "BiHui Yu",
      "Stan Z. Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5680_ECCV_2024_paper.php": {
    "title": "Scaling Up Personalized Image Aesthetic Assessment via Task Vector Customization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jooyeol Yun*",
      "Jaegul Choo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5684_ECCV_2024_paper.php": {
    "title": "AutoDIR: Automatic All-in-One Image Restoration with Latent Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yitong Jiang*",
      "Zhaoyang Zhang",
      "Tianfan Xue",
      "Jinwei Gu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5685_ECCV_2024_paper.php": {
    "title": "Receler: Reliable Concept Erasing of Text-to-Image Diffusion Models via Lightweight Erasers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chi-Pin Huang*",
      "Kai-Po Chang",
      "Chung-Ting Tsai",
      "Yung-Hsuan Lai",
      "Fu-En Yang",
      "Yu-Chiang Frank Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5687_ECCV_2024_paper.php": {
    "title": "EINet: Point Cloud Completion via Extrapolation and Interpolation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pingping Cai*",
      "Canyu Zhang",
      "LINGJIA SHI",
      "Lili Wang",
      "Nasrin Imanpour",
      "Song Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5689_ECCV_2024_paper.php": {
    "title": "Personalized Video Relighting With an At-Home Light Stage",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Myeong Choi*",
      "Max Christman",
      "Roni Sengupta"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5690_ECCV_2024_paper.php": {
    "title": "Temporal Residual Guided Diffusion Framework for Event-Driven Video Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lin Zhu*",
      "Yunlong Zheng",
      "Yijun Zhang",
      "Xiao Wang",
      "Lizhi Wang",
      "Hua Huang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5695_ECCV_2024_paper.php": {
    "title": "A Secure Image Watermarking Framework with Statistical Guarantees via Adversarial Attacks on Secret Key Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feiyu CHEN*",
      "Wei Lin",
      "Ziquan Liu",
      "Antoni Chan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5700_ECCV_2024_paper.php": {
    "title": "SPIRE: Semantic Prompt-Driven Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenyang QI*",
      "Zhengzhong Tu",
      "Keren Ye",
      "Mauricio Delbracio",
      "Peyman Milanfar",
      "Qifeng Chen",
      "Hossein Talebi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5708_ECCV_2024_paper.php": {
    "title": "Free-ATM: Harnessing Free Attention Masks for Representation Learning on Diffusion-Generated Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Junhao Zhang*",
      "Mutian Xu",
      "Jay Zhangjie Wu",
      "Chuhui Xue",
      "Wenqing Zhang",
      "Xiaoguang Han",
      "Song Bai",
      "Mike Zheng Shou*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5709_ECCV_2024_paper.php": {
    "title": "HiT-SR: Hierarchical Transformer for Efficient Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "XIANG ZHANG*",
      "Yulun Zhang",
      "Fisher Yu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5712_ECCV_2024_paper.php": {
    "title": "Audio-Synchronized Visual Animation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lin Zhang",
      "Shentong Mo",
      "Yijing Zhang",
      "Pedro Morgado*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5715_ECCV_2024_paper.php": {
    "title": "Expressive Whole-Body 3D Gaussian Avatar",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gyeongsik Moon*",
      "Takaaki Shiratori",
      "Shunsuke Saito"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5717_ECCV_2024_paper.php": {
    "title": "Canonical Shape Projection is All You Need for 3D Few-shot Class Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali Cheraghian*",
      "Zeeshan Hayder",
      "Sameeea Ramasinghe",
      "Shafin Rahman",
      "Javad Jafaryahya",
      "Lars Petersson",
      "Mehrtash Harandi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5720_ECCV_2024_paper.php": {
    "title": "Controllable Human-Object Interaction Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaman Li*",
      "Alexander Clegg",
      "Roozbeh Mottaghi",
      "Jiajun Wu",
      "Xavier Puig",
      "C. Karen Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5728_ECCV_2024_paper.php": {
    "title": "High-Fidelity and Transferable NeRF Editing by Frequency Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yisheng He*",
      "Weihao Yuan*",
      "Siyu Zhu",
      "Zilong Dong",
      "Liefeng Bo",
      "Qixing Huang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5729_ECCV_2024_paper.php": {
    "title": "DoughNet: A Visual Predictive Model for Topological Manipulation of Deformable Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dominik Bauer*",
      "Zhenjia Xu",
      "Shuran Song"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5730_ECCV_2024_paper.php": {
    "title": "PAV: Personalized Head Avatar from Unstructured Video Collection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akin Caliskan*",
      "Berkay Kicanaoglu",
      "Hyeongwoo Kim"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5733_ECCV_2024_paper.php": {
    "title": "Strike a Balance in Continual Panoptic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinpeng Chen",
      "Runmin Cong*",
      "Yuxuan Luo",
      "Horace Ho Shing Ip",
      "Sam Kwong*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5739_ECCV_2024_paper.php": {
    "title": "In Defense of Lazy Visual Grounding for Open-Vocabulary Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dahyun Kang",
      "Minsu Cho*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5743_ECCV_2024_paper.php": {
    "title": "MultiDelete for Multimodal Machine Unlearning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiali Cheng*",
      "Hadi Amiri"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5744_ECCV_2024_paper.php": {
    "title": "Unified Local-Cloud Decision-Making via Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kathakoli Sengupta",
      "Zhongkai Shangguan",
      "Sandesh Bharadwaj",
      "Sanjay Arora",
      "Eshed Ohn-Bar*",
      "Renato Mancuso"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5747_ECCV_2024_paper.php": {
    "title": "UniTalker: Scaling up Audio-Driven 3D Facial Animation through A Unified Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangyu Fan*",
      "Jiaqi Li",
      "Zhiqian Lin",
      "Weiye Xiao",
      "Lei Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5748_ECCV_2024_paper.php": {
    "title": "Robo-ABC: Affordance Generalization Beyond Categories via Semantic Correspondence for Robot Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanchen Ju",
      "Kaizhe Hu",
      "Guowei Zhang",
      "Gu Zhang",
      "Mingrun Jiang",
      "Huazhe Xu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5751_ECCV_2024_paper.php": {
    "title": "Efficient Frequency-Domain Image Deraining with Contrastive Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ning Gao",
      "Xingyu Jiang",
      "Xiuhui Zhang",
      "Yue Deng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5757_ECCV_2024_paper.php": {
    "title": "Stitched ViTs are Flexible Vision Backbones",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zizheng Pan*",
      "Jing Liu",
      "Haoyu He",
      "Jianfei Cai",
      "Bohan Zhuang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5758_ECCV_2024_paper.php": {
    "title": "TrajPrompt: Aligning Color Trajectory with Vision-Language Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li-Wu Tsao*",
      "Hao-Tang Tsui",
      "Yu-Rou Tuan",
      "Pei-Chi Chen",
      "Kuan-Lin Wang",
      "Jhih-Ciang Wu",
      "Hong-Han Shuai*",
      "Wen-Huang Cheng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5759_ECCV_2024_paper.php": {
    "title": "SemReg: Semantics Constrained Point Cloud Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheldon Fung",
      "Xuequan Lu*",
      "Dasith de Silva Edirimuni",
      "Wei Pan",
      "Xiao Liu",
      "HONGDONG LI"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5774_ECCV_2024_paper.php": {
    "title": "Cascade-Zero123: One Image to Highly Consistent 3D with Self-Prompted Nearby Views",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yabo Chen",
      "Jiemin Fang",
      "Yuyang Huang",
      "Taoran Yi",
      "Xiaopeng Zhang*",
      "Lingxi Xie",
      "Xinggang Wang",
      "Wenrui Dai*",
      "Hongkai Xiong",
      "Qi Tian"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5780_ECCV_2024_paper.php": {
    "title": "RoScenes: A Large-scale Multi-view 3D Dataset for Roadside Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaosu Zhu",
      "Hualian Sheng",
      "Sijia Cai",
      "Bing Deng",
      "Shaopeng Yang",
      "Qiao Liang",
      "Ken Chen",
      "Lianli Gao",
      "Jingkuan Song*",
      "Jieping Ye*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5783_ECCV_2024_paper.php": {
    "title": "ReSyncer: Rewiring Style-based Generator for Unified Audio-Visually Synced Facial Performer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiazhi Guan*",
      "Zhiliang Xu",
      "Hang Zhou",
      "Kaisiyuan Wang",
      "Shengyi He",
      "Zhanwang Zhang",
      "Borong Liang",
      "Haocheng Feng",
      "Errui Ding",
      "Jingtuo Liu",
      "Jingdong Wang",
      "Youjian Zhao",
      "Ziwei Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5788_ECCV_2024_paper.php": {
    "title": "Language-Driven Physics-Based Scene Synthesis and Editing via Feature Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ri-Zhao Qiu*",
      "Ge Yang",
      "Weijia Zeng",
      "Xiaolong Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5794_ECCV_2024_paper.php": {
    "title": "AlignDiff: Aligning Diffusion Models for General Few-Shot Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ri-Zhao Qiu*",
      "Yu-Xiong Wang",
      "Kris Hauser"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5796_ECCV_2024_paper.php": {
    "title": "SkateFormer: Skeletal-Temporal Transformer for Human Action Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeonghyeok Do",
      "Munchurl Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5800_ECCV_2024_paper.php": {
    "title": "R^2-Tuning: Efficient Image-to-Video Transfer Learning for Video Temporal Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ye Liu",
      "Jixuan He",
      "Wanhua Li*",
      "Junsik Kim",
      "Donglai Wei",
      "Hanspeter Pfister",
      "Chang Wen Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5802_ECCV_2024_paper.php": {
    "title": "Tree-D Fusion: Simulation-Ready Tree Dataset from Single Images with Diffusion Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jae Joong Lee",
      "Bosheng Li",
      "Sara M Beery",
      "Jonathan Huang",
      "Songlin Fei",
      "Raymond A. Yeh",
      "Bedrich Benes*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5803_ECCV_2024_paper.php": {
    "title": "Parameterization-driven Neural Surface Reconstruction for Object-oriented Editing in Neural Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baixin Xu",
      "Jiangbei Hu",
      "Fei Hou",
      "Kwan-Yee Lin",
      "Wayne Wu",
      "Chen Qian",
      "Ying He*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5806_ECCV_2024_paper.php": {
    "title": "DomainFusion: Generalizing To Unseen Domains with Latent Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuyang Huang",
      "Yabo Chen",
      "Yuchen Liu",
      "xiaopeng zhang*",
      "Wenrui Dai*",
      "Hongkai Xiong",
      "Qi Tian"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5808_ECCV_2024_paper.php": {
    "title": "Open-Set Recognition in the Age of Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dimity Miller*",
      "Niko Suenderhauf",
      "Alex Kenna",
      "Keita Mason"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5810_ECCV_2024_paper.php": {
    "title": "Unsqueeze [CLS] Bottleneck to Learn Rich Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qing Su*",
      "Shihao Ji"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5817_ECCV_2024_paper.php": {
    "title": "Robust Multimodal Learning via Representation Decoupling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shicai Wei",
      "Yang Luo",
      "Yuji Wang",
      "Chunbo Luo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5822_ECCV_2024_paper.php": {
    "title": "Object-Conditioned Energy-Based Attention Map Alignment in Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yasi Zhang*",
      "Peiyu Yu",
      "Ying Nian Wu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5826_ECCV_2024_paper.php": {
    "title": "WiMANS: A Benchmark Dataset for WiFi-based Multi-user Activity Sensing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuokang Huang*",
      "Kaihan Li",
      "Di You",
      "Yichong Chen",
      "Arvin Lin",
      "Siying Liu",
      "Xiaohui Li",
      "Julie A. McCann*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5831_ECCV_2024_paper.php": {
    "title": "Embedding-Free Transformer with Inference Spatial Reduction for Efficient Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyunwoo Yu",
      "Yubin Cho",
      "Beoungwoo Kang",
      "Seunghun Moon",
      "Kyeongbo Kong",
      "Suk-Ju Kang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5836_ECCV_2024_paper.php": {
    "title": "VeCLIP: Improving CLIP Training via Visual-enriched Captions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengfeng Lai*",
      "Haotian Zhang",
      "Bowen Zhang",
      "Wentao Wu",
      "Haoping Bai",
      "Aleksei Timofeev",
      "Xianzhi Du",
      "Zhe Gan",
      "Jiulong Shan",
      "Chen-Nee Chuah",
      "Yinfei Yang",
      "Meng Cao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5837_ECCV_2024_paper.php": {
    "title": "Three Things We Need to Know About Transferring Stable Diffusion to Visual Dense Prediciton Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manyuan Zhang*",
      "Guanglu Song",
      "Xiaoyu Shi",
      "Yu Liu",
      "Hongsheng Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5841_ECCV_2024_paper.php": {
    "title": "Learning Representations from Foundation Models for Domain Generalized Stereo Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongjian Zhang",
      "Longguang Wang",
      "Kunhong Li",
      "WANG Yun",
      "Yulan Guo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5843_ECCV_2024_paper.php": {
    "title": "Spike-Temporal Latent Representation for Energy-Efficient Event-to-Video Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianxiong Tang*",
      "Jian-Huang Lai*",
      "Lingxiao Yang",
      "Xiaohua Xie"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5850_ECCV_2024_paper.php": {
    "title": "Effective Lymph Nodes Detection in CT Scans Using Location Debiased Query Selection and Contrastive Query Representation in Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qinji Yu*",
      "Yirui Wang*",
      "Ke Yan",
      "Haoshen Li",
      "Dazhou Guo",
      "Li Zhang",
      "Na Shen",
      "Qifeng Wang",
      "Xiaowei Ding",
      "Le Lu",
      "Xianghua Ye*",
      "Dakai Jin*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5851_ECCV_2024_paper.php": {
    "title": "Chat-Edit-3D: Interactive 3D Scene Editing via Text Prompts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "shuangkang fang*",
      "Yufeng Wang*",
      "Yi-Hsuan Tsai",
      "Yi Yang",
      "Wenrui Ding",
      "Shuchang Zhou",
      "Ming-Hsuan Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5857_ECCV_2024_paper.php": {
    "title": "Event-Adapted Video Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyu Xiao",
      "Dachun Kai",
      "Yueyi Zhang",
      "Zheng-Jun Zha",
      "Xiaoyan Sun",
      "Zhiwei Xiong*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5859_ECCV_2024_paper.php": {
    "title": "Look Hear: Gaze Prediction for Speech-directed Human Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sounak Mondal*",
      "Seoyoung Ahn",
      "Zhibo Yang",
      "Niranjan Balasubramanian",
      "Dimitris Samaras",
      "Gregory Zelinsky",
      "Minh Hoai"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5862_ECCV_2024_paper.php": {
    "title": "Raising the Ceiling: Conflict-Free Local Feature Matching with Dynamic View Switching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyong Lu*",
      "Songlin Du*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5863_ECCV_2024_paper.php": {
    "title": "Q&A Prompts: Discovering Rich Visual Clues through Mining Question-Answer Prompts for VQA requiring Diverse World Knowledge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haibo Wang*",
      "Weifeng Ge*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5875_ECCV_2024_paper.php": {
    "title": "Catastrophic Overfitting: A Potential Blessing in Disguise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "MN Zhao",
      "Lihe Zhang*",
      "Yuqiu Kong",
      "Baocai Yin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5881_ECCV_2024_paper.php": {
    "title": "Long-range Turbulence Mitigation: A Large-scale Dataset and A Coarse-to-fine Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengqi Xu",
      "Run Sun",
      "Yi Chang*",
      "Shuning Cao",
      "Xueyao Xiao",
      "Luxin Yan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5885_ECCV_2024_paper.php": {
    "title": "SparseCtrl: Adding Sparse Controls to Text-to-Video Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuwei Guo",
      "Ceyuan Yang*",
      "Anyi Rao",
      "Maneesh Agrawala",
      "Dahua Lin*",
      "Bo Dai*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5894_ECCV_2024_paper.php": {
    "title": "Visual Alignment Pre-training for Sign Language Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peiqi Jiao",
      "Yuecong Min",
      "Xilin Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5898_ECCV_2024_paper.php": {
    "title": "Parrot Captions Teach CLIP to Spot Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiqi Lin",
      "Conghui He*",
      "Alex Jinpeng Wang",
      "Bin Wang",
      "Weijia Li",
      "Mike Zheng Shou"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5902_ECCV_2024_paper.php": {
    "title": "Solving Motion Planning Tasks with a Scalable Generative Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihan Hu*",
      "Siqi Chai",
      "Zhening Yang",
      "Jingyu Qian",
      "Kun Li",
      "Wenxin Shao",
      "Haichao Zhang",
      "Wei Xu",
      "Qiang Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5904_ECCV_2024_paper.php": {
    "title": "Griffon: Spelling out All Object Locations at Any Granularity with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufei Zhan",
      "Yousong Zhu*",
      "Zhiyang Chen",
      "Fan Yang",
      "Ming Tang",
      "Jinqiao Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5909_ECCV_2024_paper.php": {
    "title": "Vision-Language Action Knowledge Learning for Semantic-Aware Action Quality Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huangbiao Xu",
      "Xiao Ke*",
      "Yuezhou Li",
      "Rui Xu",
      "Huanqi Wu",
      "Xiaofeng Lin",
      "Wenzhong Guo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5912_ECCV_2024_paper.php": {
    "title": "Knowledge Transfer with Simulated Inter-Image Erasing for Weakly Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Chen*",
      "Xiruo Jiang",
      "Gensheng Pei",
      "Zeren Sun",
      "Yucheng Wang",
      "Yazhou Yao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5913_ECCV_2024_paper.php": {
    "title": "BurstM: Deep Burst Multi-scale SR using Fourier Space with Optical Flow",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "EungGu Kang*",
      "Byeonghun Lee",
      "Sunghoon Im",
      "Kyong Hwan Jin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5914_ECCV_2024_paper.php": {
    "title": "Diffusion Reward: Learning Rewards via Conditional Video Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Huang*",
      "Guangqi Jiang",
      "Yanjie Ze",
      "Huazhe Xu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5917_ECCV_2024_paper.php": {
    "title": "Recursive Visual Programming",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxin Ge*",
      "Sanjay Subramanian",
      "Baifeng Shi",
      "Roei Herzig",
      "Trevor Darrell"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5918_ECCV_2024_paper.php": {
    "title": "LLaVA-Grounding: Grounded Visual Chat with Large Multimodal Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Zhang*",
      "Hongyang Li",
      "Feng Li",
      "Tianhe Ren",
      "Xueyan Zou",
      "Shilong Liu",
      "Shijia Huang",
      "Jianfeng Gao",
      "Lei Zhang",
      "Chunyuan Li",
      "Jianwei Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5923_ECCV_2024_paper.php": {
    "title": "Prompt-Driven Contrastive Learning for Transferable Adversarial Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hunmin Yang",
      "Jongoh Jeong",
      "Kuk-Jin Yoon*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5934_ECCV_2024_paper.php": {
    "title": "Learning to Adapt SAM for Segmenting Cross-domain Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xidong Peng",
      "Runnan Chen",
      "Feng Qiao",
      "Lingdong Kong",
      "Youquan Liu",
      "Yujing Sun",
      "Tai Wang",
      "Xinge Zhu*",
      "Yuexin Ma*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5939_ECCV_2024_paper.php": {
    "title": "Learning to Enhance Aperture Phasor Field for Non-Line-of-Sight Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "In Cho",
      "Hyunbo Shim",
      "Seon Joo Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5951_ECCV_2024_paper.php": {
    "title": "ViewFormer: Exploring Spatiotemporal Modeling for Multi-View 3D Occupancy Perception via View-Guided Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinke Li*",
      "Xiao He*",
      "Chonghua Zhou",
      "Xiaoqiang Cheng",
      "Yang Wen",
      "Dan Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5952_ECCV_2024_paper.php": {
    "title": "Fine-grained Dynamic Network for Generic Event Boundary Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziwei Zheng",
      "Lijun He",
      "Le Yang",
      "Fan Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5955_ECCV_2024_paper.php": {
    "title": "Take A Step Back: Rethinking the Two Stages in Visual Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingyu Zhang",
      "Jiting Cai",
      "Mingyu Liu",
      "Yue Xu",
      "Cewu Lu",
      "Yong-Lu Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5957_ECCV_2024_paper.php": {
    "title": "AlignZeg: Mitigating Objective Misalignment for Zero-shot Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiannan Ge*",
      "Lingxi Xie",
      "Hongtao Xie",
      "Pandeng Li",
      "Xiaopeng Zhang",
      "Yongdong Zhang",
      "Qi Tian"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5958_ECCV_2024_paper.php": {
    "title": "Learning with Counterfactual Explanations for Radiology Report Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingjie Li*",
      "Haokun Lin",
      "Liang Qiu",
      "Xiaodan Liang*",
      "Ling Chen",
      "Abdulmotaleb Elsaddik",
      "Xiaojun Chang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5967_ECCV_2024_paper.php": {
    "title": "SpeedUpNet: A Plug-and-Play Adapter Network for Accelerating Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weilong Chai*",
      "Dandan Zheng",
      "Jiajiong Cao",
      "Zhiquan Chen",
      "Changbao Wang",
      "Chenguang Ma"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5971_ECCV_2024_paper.php": {
    "title": "Better Regression Makes Better Test-time Adaptive 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiakang Yuan",
      "Bo Zhang",
      "Kaixiong Gong",
      "Xiangyu Yue",
      "Botian Shi",
      "Yu Qiao",
      "Tao Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5974_ECCV_2024_paper.php": {
    "title": "ShapeLLM: Universal 3D Object Understanding for Embodied Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zekun Qi",
      "Runpei Dong",
      "Shaochen Zhang",
      "Haoran Geng",
      "Chunrui Han",
      "Zheng Ge",
      "Li Yi*",
      "Kaisheng Ma*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5976_ECCV_2024_paper.php": {
    "title": "Content-Aware Radiance Fields: Aligning Model Complexity with Scene Intricacy Through Learned Bitwidth Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weihang Liu",
      "Xue Xian Zheng",
      "Jingyi Yu",
      "Xin Lou*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5978_ECCV_2024_paper.php": {
    "title": "Finding Visual Task Vectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alberto Hojel*",
      "Yutong Bai",
      "Trevor Darrell",
      "Amir Globerson",
      "Amir Bar*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5982_ECCV_2024_paper.php": {
    "title": "Connecting Consistency Distillation to Score Distillation for Text-to-3D Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zongrui Li*",
      "Minghui Hu",
      "Qian Zheng*",
      "Xudong Jiang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5986_ECCV_2024_paper.php": {
    "title": "Event Camera Data Dense Pre-training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Yang",
      "Liyuan Pan*",
      "Liu liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5989_ECCV_2024_paper.php": {
    "title": "Distractors-Immune Representation Learning with Cross-modal Contrastive Regularization for Change Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunbin Tu*",
      "Liang Li",
      "Li Su",
      "Chenggang Yan",
      "Qingming Huang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5996_ECCV_2024_paper.php": {
    "title": "Rethinking Image-to-Video Adaptation: An Object-centric Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Qian*",
      "Shuangrui Ding",
      "Dahua Lin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5997_ECCV_2024_paper.php": {
    "title": "Layer-Wise Relevance Propagation with Conservation Property for ResNet",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seitaro Otsuki*",
      "Tsumugi Iida*",
      "Félix Doublet*",
      "Tsubasa Hirakawa*",
      "Takayoshi Yamashita*",
      "Hironobu Fujiyoshi*",
      "Komei Sugiura*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6001_ECCV_2024_paper.php": {
    "title": "DECap: Towards Generalized Explicit Caption Editing via Diffusion Mechanism",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Wang",
      "Xinyun Jiang",
      "Jun Xiao",
      "Tao Chen",
      "Long Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6006_ECCV_2024_paper.php": {
    "title": "EgoLifter: Open-world 3D Segmentation for Egocentric Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiao Gu*",
      "Zhaoyang Lv*",
      "Duncan Frost",
      "Simon Green",
      "Julian Straub",
      "Chris Sweeney*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6012_ECCV_2024_paper.php": {
    "title": "MEVG : Multi-event Video Generation with Text-to-Video Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gyeongrok Oh*",
      "Jaehwan Jeong",
      "Sieun Kim",
      "Wonmin Byeon",
      "Jinkyu Kim",
      "Sungwoong Kim",
      "Sangpil Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6014_ECCV_2024_paper.php": {
    "title": "Open-Vocabulary SAM: Segment and Recognize Twenty-thousand Classes Interactively",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haobo Yuan",
      "Xiangtai Li*",
      "Chong Zhou",
      "Yining Li",
      "Kai Chen",
      "Chen Change Loy"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6020_ECCV_2024_paper.php": {
    "title": "Data-to-Model Distillation: Data-Efficient Learning Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ahmad Sajedi*",
      "Samir Khaki",
      "Lucy Z. Liu",
      "Ehsan Amjadian",
      "Yuri A. Lawryshyn",
      "Konstantinos N. Plataniotis"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6022_ECCV_2024_paper.php": {
    "title": "DiffuX2CT: Diffusion Learning to Reconstruct CT Images from Biplanar X-Rays",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuhui Liu",
      "Zhi Qiao",
      "Runkun Liu",
      "Hong Li",
      "Xiantong Zhen*",
      "Zhen Qian",
      "Juan Zhang*",
      "Baochang Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6023_ECCV_2024_paper.php": {
    "title": "AdaIFL: Adaptive Image Forgery Localization via a Dynamic and Importance-aware Transformer Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxi Li*",
      "Fuyuan Cheng",
      "Wangbo Yu",
      "Guangshuo Wang",
      "Guibo Luo*",
      "Yuesheng Zhu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6025_ECCV_2024_paper.php": {
    "title": "ComFusion: Enhancing Personalized Generation by Instance-Scene Compositing and Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Hong*",
      "Yuxuan Duan",
      "Bo Zhang",
      "Haoxing Chen",
      "Jun Lan",
      "Huijia Zhu",
      "Weiqiang Wang",
      "Jianfu Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6026_ECCV_2024_paper.php": {
    "title": "ML-SemReg: Boosting Point Cloud Registration with Multi-level Semantic Consistency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaocheng Yan",
      "Pengcheng Shi",
      "Jiayuan Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6027_ECCV_2024_paper.php": {
    "title": "Mask as Supervision: Leveraging Unified Mask Information for Unsupervised 3D Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchen Yang",
      "Yu Qiao",
      "Xiao Sun*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6030_ECCV_2024_paper.php": {
    "title": "MoVideo: Motion-Aware Video Generation with Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyun Liang*",
      "Yuchen Fan",
      "Kai Zhang*",
      "Radu Timofte",
      "Luc Van Gool",
      "Rakesh Ranjan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6031_ECCV_2024_paper.php": {
    "title": "SHERL: Synthesizing High Accuracy and Efficient Memory for Resource-Limited Transfer Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiwen Diao*",
      "Bo Wan",
      "Xu Jia",
      "Yunzhi Zhuge",
      "Ying Zhang",
      "Huchuan Lu*",
      "Long Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6035_ECCV_2024_paper.php": {
    "title": "MonoTTA: Fully Test-Time Adaptation for Monocular 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongbin Lin",
      "Yifan Zhang",
      "Shuaicheng Niu",
      "Shuguang Cui",
      "Zhen Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6037_ECCV_2024_paper.php": {
    "title": "RangeLDM: Fast Realistic LiDAR Point Cloud Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qianjiang Hu",
      "Zhimin Zhang",
      "Wei Hu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6039_ECCV_2024_paper.php": {
    "title": "Learn to Optimize Denoising Scores: A Unified and Improved Diffusion Prior for 3D Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaofeng Yang*",
      "Yiwen Chen",
      "Cheng Chen",
      "Chi Zhang",
      "Yi Xu",
      "Xulei Yang",
      "Fayao Liu",
      "Guosheng Lin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6040_ECCV_2024_paper.php": {
    "title": "Be-Your-Outpainter: Mastering Video Outpainting through Input-Specific Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fu-Yun Wang*",
      "Xiaoshi Wu",
      "Zhaoyang Huang",
      "Xiaoyu Shi",
      "Dazhong Shen",
      "Guanglu Song",
      "Yu Liu",
      "Hongsheng Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6042_ECCV_2024_paper.php": {
    "title": "Physically Plausible Color Correction for Neural Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Zhang*",
      "Ying Feng",
      "HONGDONG LI*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6043_ECCV_2024_paper.php": {
    "title": "Unifying 3D Vision-Language Understanding via Promptable Queries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "ziyu zhu*",
      "Zhuofan Zhang",
      "Xiaojian Ma",
      "Xuesong Niu",
      "Yixin Chen",
      "Baoxiong Jia",
      "Zhidong Deng*",
      "Siyuan Huang*",
      "Qing Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6044_ECCV_2024_paper.php": {
    "title": "Model Stock: All we need is just a few fine-tuned models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dong-Hwan Jang",
      "Sangdoo Yun",
      "Dongyoon Han*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6048_ECCV_2024_paper.php": {
    "title": "Motion-Guided Latent Diffusion for Temporally Consistent Real-world Video Super-resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xi Yang*",
      "Chenhang He",
      "Jianqi Ma",
      "Lei Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6050_ECCV_2024_paper.php": {
    "title": "PoseCrafter: One-Shot Personalized Video Synthesis Following Flexible Pose Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yong Zhong",
      "Min Zhao",
      "Zebin You",
      "Xiaofeng Yu",
      "Changwang Zhang",
      "Chongxuan Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6052_ECCV_2024_paper.php": {
    "title": "MAD-DR: Map Compression for Visual Localization with Matchness Aware Descriptor Dimension Reduction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiang Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6053_ECCV_2024_paper.php": {
    "title": "Benchmarking Object Detectors with COCO: A New Path Forward",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shweta Singh",
      "Aayan Yadav",
      "Jitesh Jain",
      "Humphrey Shi",
      "Justin Johnson",
      "Karan Desai*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6054_ECCV_2024_paper.php": {
    "title": "Adaptive High-Frequency Transformer for Diverse Wildlife Re-Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenyue Li",
      "Shuoyi Chen",
      "Mang Ye*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6063_ECCV_2024_paper.php": {
    "title": "WPS-SAM: Towards Weakly-Supervised Part Segmentation with Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin-Jian Wu*",
      "Ruisong Zhang",
      "Jie Qin",
      "Shijie Ma",
      "Cheng-Lin Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6068_ECCV_2024_paper.php": {
    "title": "Lane Graph as Path: Continuity-preserving Path-wise Modeling for Online Lane Graph Construction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bencheng Liao",
      "Shaoyu Chen",
      "Bo Jiang",
      "Tianheng Cheng",
      "Qian Zhang",
      "Wenyu Liu",
      "Chang Huang",
      "Xinggang Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6071_ECCV_2024_paper.php": {
    "title": "DeCo: Decoupled Human-Centered Diffusion Video Editing with Motion Consistency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaojing Zhong",
      "Xinyi Huang",
      "Xiaofeng Yang",
      "Guosheng Lin*",
      "Qingyao Wu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6072_ECCV_2024_paper.php": {
    "title": "Unleashing the Potential of the Semantic Latent Space in Diffusion Models for Image Dehazing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zizheng Yang",
      "Hu Yu",
      "Bing Li",
      "Jinghao Zhang",
      "Jie Huang",
      "Feng Zhao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6074_ECCV_2024_paper.php": {
    "title": "Uncertainty-aware sign language video retrieval with probability distribution modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan Wu*",
      "Hongxiang Li",
      "yuanjiang luo",
      "Xuxin Cheng",
      "Xianwei Zhuang",
      "Meng Cao",
      "Keren Fu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6076_ECCV_2024_paper.php": {
    "title": "NeRMo: Learning Implicit Neural Representations for 3D Human Motion Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dong Wei",
      "Huaijiang Sun",
      "Xiaoning Sun*",
      "Shengxiang Hu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6085_ECCV_2024_paper.php": {
    "title": "Bridging Synthetic and Real Worlds for Pre-training Scene Text Detectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tongkun Guan",
      "Wei Shen*",
      "Xue Yang",
      "Xuehui Wang",
      "Xiaokang Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6086_ECCV_2024_paper.php": {
    "title": "VLAD-BuFF: Burst-aware Fast Feature Aggregation for Visual Place Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ahmad Khaliq",
      "Ming Xu",
      "Stephen Hausler",
      "Michael J Milford",
      "Sourav Garg*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6090_ECCV_2024_paper.php": {
    "title": "DSA: Discriminative Scatter Analysis for Early Smoke Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lujian Yao*",
      "Haitao Zhao*",
      "Jingchao Peng",
      "Zhongze Wang",
      "Kaijie Zhao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6102_ECCV_2024_paper.php": {
    "title": "SAFARI: Adaptive Sequence Transformer for Weakly Supervised Referring Expression Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sayan Nag*",
      "Koustava Goswami",
      "Srikrishna Karanam"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6103_ECCV_2024_paper.php": {
    "title": "KFD-NeRF: Rethinking Dynamic NeRF with Kalman Filter",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Zhan",
      "Zhuoxiao Li",
      "Muyao Niu",
      "Zhihang Zhong",
      "Shohei Nobuhara",
      "Ko Nishino",
      "Yinqiang Zheng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6110_ECCV_2024_paper.php": {
    "title": "Physical-Based Event Camera Simulator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiqian Han",
      "Jiacheng Lyu",
      "Jianing Li*",
      "Henglu Wei",
      "Cheng Li",
      "Yajing Wei",
      "SHU CHEN",
      "Xiangyang Ji*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6117_ECCV_2024_paper.php": {
    "title": "V-IRL: Grounding Virtual Intelligence in Real Life",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jihan Yang*",
      "Runyu Ding",
      "Ellis L Brown",
      "Xiaojuan Qi",
      "Saining Xie"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6118_ECCV_2024_paper.php": {
    "title": "Adversarial Prompt Tuning for Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaming Zhang",
      "Xingjun Ma*",
      "Xin Wang",
      "Lingyu Qiu",
      "Jiaqi Wang",
      "Yu-Gang Jiang",
      "Jitao Sang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6121_ECCV_2024_paper.php": {
    "title": "Relightable 3D Gaussians: Realistic Point Cloud Relighting with BRDF Decomposition and Ray Tracing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Gao",
      "chun gu",
      "Youtian Lin",
      "Zhihao Li",
      "Hao Zhu",
      "Xun Cao",
      "Li Zhang*",
      "Yao Yao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6123_ECCV_2024_paper.php": {
    "title": "Mono-ViFI: A Unified Learning Framework for Self-supervised Single- and Multi-frame Monocular Depth Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinfeng Liu*",
      "Lingtong Kong",
      "Bo Li",
      "Zerong Wang",
      "Hong Gu",
      "Jinwei Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6124_ECCV_2024_paper.php": {
    "title": "CC-SAM: Enhancing SAM with Cross-feature Attention and Context for Ultrasound Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shreyank N Gowda*",
      "David A Clifton"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6128_ECCV_2024_paper.php": {
    "title": "An Efficient and Effective Transformer Decoder-Based Framework for Multi-Task Visual Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Chen",
      "Long Chen",
      "Yu Wu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6129_ECCV_2024_paper.php": {
    "title": "Think2Drive: Efficient Reinforcement Learning by Thinking with Latent World Model for Autonomous Driving (in CARLA-v2)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qifeng Li*",
      "Xiaosong Jia",
      "Shaobo Wang",
      "Junchi Yan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6138_ECCV_2024_paper.php": {
    "title": "PanGu-Draw: Advancing Resource-Efficient Text-to-Image Synthesis with Time-Decoupled Training and Reusable Coop-Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guansong Lu*",
      "Yuanfan Guo",
      "Jianhua Han",
      "Minzhe Niu",
      "Yihan Zeng",
      "Songcen Xu",
      "Zeyi Huang",
      "Zhao Zhong",
      "Wei Zhang",
      "Hang Xu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6140_ECCV_2024_paper.php": {
    "title": "X-InstructBLIP: A Framework for Aligning Image, 3D, Audio, Video to LLMs and its Emergent Cross-modal Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Artemis Panagopoulou*",
      "Le Xue",
      "Ning Yu",
      "LI JUNNAN",
      "DONGXU LI",
      "Shafiq Joty",
      "Ran Xu",
      "Silvio Savarese",
      "Caiming Xiong",
      "Juan Carlos Niebles"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6141_ECCV_2024_paper.php": {
    "title": "Learning Neural Volumetric Pose Features for Camera Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyu Lin",
      "Jiaqi Gu",
      "Bojian Wu",
      "Lubin Fan*",
      "Renjie Chen*",
      "Ligang Liu",
      "Jieping Ye"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6142_ECCV_2024_paper.php": {
    "title": "Betrayed by Attention: A Simple yet Effective Approach for Self-supervised Video Object Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuangrui Ding*",
      "Rui Qian",
      "Haohang Xu",
      "Dahua Lin",
      "Hongkai Xiong"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6148_ECCV_2024_paper.php": {
    "title": "REFRAME: Reflective Surface Real-Time Rendering for Mobile Devices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaojie Ji*",
      "Yufeng Li",
      "Yiyi Liao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6150_ECCV_2024_paper.php": {
    "title": "Self-Training Room Layout via Geometry-aware Ray-casting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bolivar Solarte*",
      "Chin-Hsuan Wu*",
      "Jin-Cheng Jhang*",
      "Jonathan Lee*",
      "Yi-Hsuan Tsai*",
      "Min Sun*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6162_ECCV_2024_paper.php": {
    "title": "Closed-Loop Unsupervised Representation Disentanglement with $\\\\beta$-VAE Distillation and Diffusion Probabilistic Feedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Jin*",
      "Bohan Li*",
      "Baao Xie",
      "Wenyao Zhang",
      "Jinming Liu",
      "Ziqiang Li",
      "Tao Yang",
      "Wenjun Zeng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6164_ECCV_2024_paper.php": {
    "title": "Rethinking Weakly-supervised Video Temporal Grounding From a Game Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Fang",
      "Zeyu Xiong",
      "Wanlong Fang",
      "Xiaoye Qu",
      "Chen Chen",
      "Jianfeng Dong",
      "Keke Tang",
      "Pan Zhou*",
      "Yu Cheng",
      "Daizong Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6171_ECCV_2024_paper.php": {
    "title": "Every Pixel Has its Moments: Ultra-High-Resolution Unpaired Image-to-Image Translation via Dense Normalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ming-Yang Ho",
      "Che-Ming Wu",
      "Min-Sheng Wu",
      "‪Yufeng Jane Tseng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6174_ECCV_2024_paper.php": {
    "title": "ZoLA: Zero-Shot Creative Long Animation Generation with Short Video Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fu-Yun Wang*",
      "Zhaoyang Huang*",
      "Qiang Ma",
      "Guanglu Song",
      "Xudong LU",
      "Weikang Bian",
      "Yijin Li",
      "Yu Liu",
      "Hongsheng Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6176_ECCV_2024_paper.php": {
    "title": "Parameter-Efficient and Memory-Efficient Tuning for Vision Transformer: A Disentangled Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taolin Zhang",
      "Jiawang Bai",
      "Zhihe Lu",
      "Dongze Lian",
      "genping wang*",
      "Xinchao Wang*",
      "Shu-Tao Xia"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6185_ECCV_2024_paper.php": {
    "title": "Restore Anything with Masks: Leveraging Mask Image Modeling for Blind All-in-One Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chujie Qin",
      "Ruiqi Wu",
      "Zikun Liu",
      "Xin Lin",
      "Chun-Le Guo",
      "Hyun Hee Park",
      "Chongyi Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6190_ECCV_2024_paper.php": {
    "title": "When Fast Fourier Transform Meets Transformer for Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyu Jiang",
      "Xiuhui Zhang",
      "Ning Gao",
      "Yue Deng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6203_ECCV_2024_paper.php": {
    "title": "Dolphins: Multimodal Language Model for Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingzi Ma",
      "Yulong Cao",
      "Jiachen Sun",
      "Marco Pavone",
      "Chaowei Xiao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6210_ECCV_2024_paper.php": {
    "title": "Rethinking Video Deblurring with Wavelet-Aware Dynamic Transformer and Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Rao",
      "Guangyuan Li",
      "Zehua Lan",
      "Jiakai Sun",
      "Junsheng Luan",
      "Wei Xing*",
      "Lei Zhao*",
      "Huaizhong Lin*",
      "Jianfeng Dong",
      "Dalong Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6215_ECCV_2024_paper.php": {
    "title": "CamoTeacher: Dual-Rotation Consistency Learning for Semi-Supervised Camouflaged Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "xunfa lai",
      "Zhiyu Yang",
      "Jie Hu",
      "ShengChuan Zhang*",
      "Liujuan Cao",
      "Guannan Jiang",
      "Songan Zhang",
      "zhiyu wang",
      "Rongrong Ji"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6218_ECCV_2024_paper.php": {
    "title": "Placing Objects in Context via Inpainting for Out-of-distribution Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pau de Jorge Aranda*",
      "Riccardo Volpi",
      "Puneet Dokania",
      "Philip Torr",
      "Gregory Rogez"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6219_ECCV_2024_paper.php": {
    "title": "Textual Grounding for Open-vocabulary Visual Information Extraction in Layout-diversified Documents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengjun Cheng",
      "Chengquan Zhang",
      "Chang Liu*",
      "Yuke Li",
      "Bohan Li",
      "Kun Yao",
      "Xiawu Zheng",
      "Rongrong Ji",
      "Jie Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6222_ECCV_2024_paper.php": {
    "title": "Teddy: Efficient Large-Scale Dataset Distillation via Taylor-Approximated Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruonan Yu",
      "Songhua Liu",
      "Jingwen Ye",
      "Xinchao Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6227_ECCV_2024_paper.php": {
    "title": "Rethinking and Improving Visual Prompt Selection for In-Context Learning Segmentation Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Suo",
      "Lanqing Lai",
      "Mengyang Sun",
      "Hanwang Zhang",
      "Peng Wang*",
      "Yanning Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6231_ECCV_2024_paper.php": {
    "title": "D4-VTON: Dynamic Semantics Disentangling for Differential Diffusion based Virtual Try-On",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaotong Yang",
      "Zicheng Jiang",
      "Xinzhe Li",
      "Huiyu Zhou",
      "Junyu Dong",
      "Huaidong Zhang",
      "Yong Du*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6234_ECCV_2024_paper.php": {
    "title": "TC4D: Trajectory-Conditioned Text-to-4D Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sherwin Bahmani*",
      "Xian Liu",
      "Wang Yifan",
      "Ivan Skorokhodov",
      "Victor Rong",
      "Ziwei Liu",
      "Xihui Liu",
      "Jeong Joon Park",
      "Sergey Tulyakov",
      "Gordon Wetzstein",
      "Andrea Tagliasacchi",
      "David B Lindell"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6235_ECCV_2024_paper.php": {
    "title": "Blind Image Deconvolution by Generative-based Kernel Prior and Initializer via Latent Encoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiangtao Zhang",
      "Zongsheng Yue*",
      "Hui Wang",
      "Qian Zhao*",
      "Deyu Meng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6243_ECCV_2024_paper.php": {
    "title": "AdvDiff: Generating Unrestricted Adversarial Examples using Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuelong Dai*",
      "Kaisheng Liang",
      "Bin Xiao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6251_ECCV_2024_paper.php": {
    "title": "Improving Text-guided Object Inpainting with Semantic Pre-inpainting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifu Chen",
      "Jingwen Chen",
      "Yingwei Pan*",
      "Yehao Li",
      "Ting Yao",
      "Zhineng Chen",
      "Tao Mei"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6262_ECCV_2024_paper.php": {
    "title": "Personalized Federated Domain-Incremental Learning based on Adaptive Knowledge Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yichen Li",
      "Wenchao Xu",
      "Haozhao Wang*",
      "Yining Qi*",
      "Jingcai Guo",
      "Ruixuan Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6265_ECCV_2024_paper.php": {
    "title": "ST-LDM: A Universal Framework for Text-Grounded Object Generation in Real Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangtian Xue",
      "Jiasong Wu*",
      "Youyong Kong",
      "Lotfi Senhadji",
      "Huazhong Shu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6268_ECCV_2024_paper.php": {
    "title": "RS-NeRF: Neural Radiance Fields from Rolling Shutter Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muyao Niu",
      "Tong Chen",
      "Yifan Zhan",
      "Zhuoxiao Li",
      "Xiang Ji",
      "Yinqiang Zheng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6270_ECCV_2024_paper.php": {
    "title": "Region-Adaptive Transform with Segmentation Prior for Image Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxi Liu*",
      "Wenhan Yang",
      "Huihui Bai",
      "Yunchao Wei",
      "Yao Zhao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6271_ECCV_2024_paper.php": {
    "title": "Enhancing Tracking Robustness with Auxiliary Adversarial Defense Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhewei Wu",
      "Ruilong Yu",
      "Qihe Liu*",
      "Shuying Cheng",
      "Shilin Qiu",
      "Shijie Zhou"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6273_ECCV_2024_paper.php": {
    "title": "SLIM: Spuriousness Mitigation with Minimal Human Annotations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiwei Xuan*",
      "Ziquan Deng",
      "Hsuan-Tien Lin",
      "Kwan-Liu Ma"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6275_ECCV_2024_paper.php": {
    "title": "Uncertainty Calibration with Energy Based Instance-wise Scaling in the Wild Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mijoo Kim",
      "Junseok Kwon*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6277_ECCV_2024_paper.php": {
    "title": "X-Pose: Detecting Any Keypoints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Yang",
      "Ailing Zeng*",
      "Ruimao Zhang*",
      "Lei Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6281_ECCV_2024_paper.php": {
    "title": "M^2Depth: Self-supervised Two-Frame Multi-camera Metric Depth Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingshuang Zou*",
      "Yikang Ding",
      "Xi Qiu",
      "Haoqian Wang*",
      "Haotian Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6283_ECCV_2024_paper.php": {
    "title": "UniMD: Towards Unifying Moment Retrieval and Temporal Action Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingsen Zeng",
      "Yujie Zhong*",
      "Chengjian Feng",
      "Lin Ma"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6288_ECCV_2024_paper.php": {
    "title": "DyFADet: Dynamic Feature Aggregation for Temporal Action Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Le Yang*",
      "Ziwei Zheng",
      "Yizeng Han",
      "Hao Cheng",
      "Shiji Song",
      "Gao Huang",
      "Fan Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6290_ECCV_2024_paper.php": {
    "title": "LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanwei Li*",
      "Chengyao Wang",
      "Jiaya Jia"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6292_ECCV_2024_paper.php": {
    "title": "MetaCap: Meta-learning Priors from Multi-View Imagery for Sparse-view Human Performance Capture and Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guoxing Sun*",
      "Rishabh Dabral",
      "Pascal Fua",
      "Christian Theobalt",
      "Marc Habermann"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6294_ECCV_2024_paper.php": {
    "title": "DiffPMAE: Diffusion Masked Autoencoders for Point Cloud Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanlong LI*",
      "Chamara Madarasingha",
      "Kanchana Thilakarathna"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6296_ECCV_2024_paper.php": {
    "title": "Multi-branch Collaborative Learning Network for 3D Visual Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhipeng Qian",
      "Yiwei Ma",
      "Zhekai Lin",
      "Jiayi Ji",
      "Xiawu Zheng",
      "Xiaoshuai Sun*",
      "Rongrong Ji"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6298_ECCV_2024_paper.php": {
    "title": "DynamiCrafter: Animating Open-domain Images with Video Diffusion Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinbo Xing*",
      "Menghan Xia",
      "Yong Zhang",
      "Haoxin Chen",
      "Wangbo Yu",
      "Hanyuan Liu",
      "Gongye Liu",
      "Xintao Wang",
      "Ying Shan",
      "Tien-Tsin Wong"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6299_ECCV_2024_paper.php": {
    "title": "Motion Aware Event Representation-driven Image Deblurring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhijing Sun",
      "Xueyang Fu",
      "Longzhuo Huang",
      "Aiping Liu",
      "Zheng-Jun Zha*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6301_ECCV_2024_paper.php": {
    "title": "Turbo: Informativity-Driven Acceleration Plug-In for Vision-Language Large Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Ju*",
      "Haicheng Wang",
      "Haozhe Cheng",
      "Xu Chen",
      "Zhonghua Zhai",
      "Weilin Huang",
      "Jinsong Lan",
      "Shuai Xiao*",
      "Bo Zheng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6306_ECCV_2024_paper.php": {
    "title": "WildRefer: 3D Object Localization in Large-scale Dynamic Scenes with Multi-modal Visual Data and Natural Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenxiang Lin",
      "Xidong Peng",
      "Peishan Cong",
      "Ge Zheng",
      "Yujing Sun",
      "Yuenan HOU",
      "Xinge Zhu",
      "Sibei Yang",
      "Yuexin Ma*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6307_ECCV_2024_paper.php": {
    "title": "RCS-Prompt: Learning Prompt to Rearrange Class Space for Prompt-based Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Longrong Yang",
      "Hanbin Zhao",
      "Yunlong Yu*",
      "Xiaodong Zeng",
      "Xi Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6313_ECCV_2024_paper.php": {
    "title": "Text-Anchored Score Composition: Tackling Condition Misalignment in Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luozhou Wang*",
      "Guibao Shen",
      "Wenhang Ge",
      "Guangyong Chen",
      "Yijun Li",
      "Yingcong Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6319_ECCV_2024_paper.php": {
    "title": "Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shilong Liu*",
      "Zhaoyang Zeng",
      "Tianhe Ren",
      "Feng Li",
      "Hao Zhang",
      "Jie Yang",
      "Qing Jiang",
      "Chunyuan Li",
      "Jianwei Yang",
      "Hang Su",
      "Jun Zhu",
      "Lei Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6320_ECCV_2024_paper.php": {
    "title": "Make Your ViT-based Multi-view 3D Detectors Faster via Token Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dingyuan Zhang",
      "Dingkang Liang*",
      "Zichang Tan",
      "Xiaoqing Ye",
      "Cheng Zhang",
      "Jingdong Wang",
      "Xiang Bai*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6321_ECCV_2024_paper.php": {
    "title": "OV-Uni3DETR: Towards Unified Open-Vocabulary 3D Object Detection via Cycle-Modality Propagation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenyu Wang*",
      "Ya-Li Li",
      "TAICHI LIU",
      "Hengshuang Zhao",
      "Shengjin Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6326_ECCV_2024_paper.php": {
    "title": "CatchBackdoor: Backdoor Detection via Critical Trojan Neural Path Fuzzing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haibo Jin",
      "Ruoxi Chen",
      "Jinyin Chen",
      "Haibin Zheng",
      "Yang Zhang",
      "Haohan Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6331_ECCV_2024_paper.php": {
    "title": "UCIP: A Universal Framework for Compressed Image Super-Resolution using Dynamic Prompt",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Li*",
      "Bingchen Li",
      "Yeying Jin",
      "Cuiling Lan",
      "Hanxin Zhu",
      "Yulin Ren",
      "Zhibo Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6341_ECCV_2024_paper.php": {
    "title": "LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shilong Liu*",
      "Hao Cheng",
      "Haotian Liu",
      "Hao Zhang",
      "Feng Li",
      "Tianhe Ren",
      "Xueyan Zou",
      "Jianwei Yang",
      "Hang Su",
      "Jun Zhu",
      "Lei Zhang",
      "Jianfeng Gao",
      "Chunyuan Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6346_ECCV_2024_paper.php": {
    "title": "ClearCLIP: Decomposing CLIP Representations for Dense Vision-Language Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengcheng Lan",
      "Chaofeng Chen",
      "Yiping Ke",
      "Xinjiang Wang",
      "Litong Feng*",
      "Wayne Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6348_ECCV_2024_paper.php": {
    "title": "Two-Stage Active Learning for Efficient Temporal Action Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhao Su",
      "Ehsan Elhamifar*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6349_ECCV_2024_paper.php": {
    "title": "TexDreamer: Towards Zero-Shot High-Fidelity 3D Human Texture Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufei Liu",
      "Junwei Zhu",
      "Junshu Tang",
      "Shijie Zhang",
      "Jiangning Zhang",
      "Weijian Cao",
      "Chengjie Wang",
      "Yunsheng Wu",
      "Dongjin Huang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6350_ECCV_2024_paper.php": {
    "title": "MVPGS: Excavating Multi-view Priors for Gaussian Splatting from Sparse Input Views",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wangze Xu",
      "Huachen Gao",
      "Shihe Shen",
      "Rui Peng",
      "Jianbo Jiao",
      "Ronggang Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6352_ECCV_2024_paper.php": {
    "title": "Domain-Adaptive 2D Human Pose Estimation via Dual Teachers in Extremely Low-Light Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihao Ai*",
      "Yifei Qi",
      "Bo Wang",
      "Yu Cheng",
      "Xinchao Wang",
      "Robby T. Tan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6355_ECCV_2024_paper.php": {
    "title": "Towards More Practical Group Activity Detection: A New Benchmark and Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongkeun Kim",
      "Youngkil Song",
      "Minsu Cho",
      "Suha Kwak*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6356_ECCV_2024_paper.php": {
    "title": "Depicting Beyond Scores: Advancing Image Quality Assessment through Multi-modal Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyuan You*",
      "Zheyuan Li",
      "Jinjin Gu*",
      "Zhenfei Yin",
      "Tianfan Xue*",
      "Chao Dong*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6357_ECCV_2024_paper.php": {
    "title": "Zero-Shot Image Feature Consensus with Deep Functional Maps",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinle Cheng",
      "Congyue Deng*",
      "Adam Harley",
      "Yixin Zhu*",
      "Leonidas Guibas*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6362_ECCV_2024_paper.php": {
    "title": "WindPoly: Polygonal Mesh Reconstruction via Winding Numbers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin He",
      "Chenlei Lv",
      "Pengdi Huang",
      "Hui Huang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6365_ECCV_2024_paper.php": {
    "title": "MinD-3D: Reconstruct High-quality 3D objects in Human Brain",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianxiong Gao",
      "Yuqian Fu",
      "Yun Wang",
      "Xuelin Qian",
      "Jianfeng Feng",
      "Yanwei Fu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6366_ECCV_2024_paper.php": {
    "title": "Tokenize Anything via Prompting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ting Pan*",
      "Lulu Tang",
      "Xinlong Wang*",
      "Shiguang Shan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6376_ECCV_2024_paper.php": {
    "title": "Geospecific View Generation - Geometry-Context Aware High-resolution Ground View Inference from Satellite Views",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ningli Xu",
      "Rongjun Qin*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6379_ECCV_2024_paper.php": {
    "title": "Scissorhands: Scrub Data Influence via Connection Sensitivity in Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Wu*",
      "Mehrtash Harandi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6388_ECCV_2024_paper.php": {
    "title": "City-on-Web: Real-time Neural Rendering of Large-scale Scenes on the Web",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaiwen Song",
      "Xiaoyi Zeng",
      "Chenqu Ren",
      "Juyong Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6392_ECCV_2024_paper.php": {
    "title": "GRAPE: Generalizable and Robust Multi-view Facial Capture",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Li",
      "Di Kang",
      "Zhenyu He*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6399_ECCV_2024_paper.php": {
    "title": "Training-Free Model Merging for Multi-target Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenyi Li",
      "Huan-ang Gao",
      "Mingju Gao",
      "Beiwen Tian",
      "Rong Zhi",
      "Hao Zhao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6405_ECCV_2024_paper.php": {
    "title": "Multi-RoI Human Mesh Recovery with Camera Consistency and Contrastive Losses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongwei Nie",
      "Changzhen Liu",
      "Chengjiang Long",
      "Qing Zhang",
      "Guiqing Li",
      "Hongmin Cai*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6407_ECCV_2024_paper.php": {
    "title": "Co-Student: Collaborating Strong and Weak Students for Sparsely Annotated Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lianjun Wu",
      "Jiangxiao Han",
      "Zengqiang Zheng",
      "Xinggang Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6409_ECCV_2024_paper.php": {
    "title": "Open-Vocabulary Camouflaged Object Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youwei Pang",
      "Xiaoqi Zhao",
      "JiaMing Zuo",
      "Lihe Zhang*",
      "Huchuan Lu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6412_ECCV_2024_paper.php": {
    "title": "SmartControl: Enhancing ControlNet for Handling Rough Visual Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyu Liu",
      "Yuxiang Wei",
      "Ming Liu*",
      "Xianhui Lin",
      "Peiran Ren",
      "xuansong xie",
      "Wangmeng Zuo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6414_ECCV_2024_paper.php": {
    "title": "InterFusion: Text-Driven Generation of 3D Human-Object Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sisi Dai",
      "Wenhao Li",
      "Haowen Sun",
      "Haibin Huang",
      "Chongyang Ma",
      "Hui Huang",
      "Kai Xu*",
      "Ruizhen Hu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6415_ECCV_2024_paper.php": {
    "title": "GLARE: Low Light Image Enhancement via Generative Latent Feature based Codebook Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Zhou",
      "Wei Dong",
      "Xiaohong Liu*",
      "Shuaicheng Liu",
      "Xiongkuo Min",
      "Guangtao Zhai",
      "Jun Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6416_ECCV_2024_paper.php": {
    "title": "DriveDreamer: Towards Real-world-driven World Models for Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaofeng Wang*",
      "Zheng Zhu",
      "Guan Huang",
      "Chen Xinze",
      "Jiagang Zhu",
      "Jiwen Lu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6418_ECCV_2024_paper.php": {
    "title": "Flow-Assisted Motion Learning Network for Weakly-Supervised Group Activity Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Adi Nugroho*",
      "Sangmin Woo",
      "Sumin Lee",
      "Jinyoung Park",
      "Yooseung Wang",
      "Donguk Kim",
      "Changick Kim"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6424_ECCV_2024_paper.php": {
    "title": "NeRF-XL: NeRF at Any Scale with Multi-GPU",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruilong Li*",
      "Sanja Fidler",
      "Angjoo Kanazawa",
      "Francis Williams"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6425_ECCV_2024_paper.php": {
    "title": "CoSIGN: Few-Step Guidance of ConSIstency Model to Solve General INverse Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiankun Zhao",
      "Bowen Song",
      "Liyue Shen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6429_ECCV_2024_paper.php": {
    "title": "The First to Know: How Token Distributions Reveal Hidden Knowledge in Large Vision-Language Models?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qinyu Zhao*",
      "Ming Xu",
      "Kartik Gupta",
      "Akshay Asthana",
      "Liang Zheng",
      "Stephen Gould"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6434_ECCV_2024_paper.php": {
    "title": "Compositional Substitutivity of Visual Reasoning for Visual Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuanhao Li",
      "Zhen Li",
      "Chenchen Jing*",
      "Yuwei Wu*",
      "Mingliang Zhai",
      "Yunde Jia"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6440_ECCV_2024_paper.php": {
    "title": "LightenDiffusion: Unsupervised Low-Light Image Enhancement with Latent-Retinex Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hai Jiang",
      "Ao Luo",
      "Xiaohong Liu",
      "Songchen Han",
      "Shuaicheng Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6442_ECCV_2024_paper.php": {
    "title": "DNI: Dilutional Noise Initialization for Diffusion Video Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sunjae Yoon",
      "Gwanhyeong Koo",
      "Ji Woo Hong",
      "Chang D. Yoo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6444_ECCV_2024_paper.php": {
    "title": "Two-Stage Video Shadow Detection via Temporal-Spatial Adaption",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Duan",
      "Yu Cao",
      "Lei Zhu",
      "Gang Fu",
      "Xin Wang",
      "Renjie ZHANG",
      "Ping Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6445_ECCV_2024_paper.php": {
    "title": "Towards Physical World Backdoor Attacks against Skeleton Action Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qichen Zheng",
      "Yi Yu",
      "SIYUAN YANG*",
      "Jun Liu",
      "Kwok-Yan Lam",
      "Alex Kot"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6451_ECCV_2024_paper.php": {
    "title": "SAM-guided Graph Cut for 3D Instance Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu Guo*",
      "He Zhu",
      "Sida Peng",
      "Yuang Wang",
      "Yujun Shen",
      "Ruizhen Hu*",
      "Xiaowei Zhou*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6457_ECCV_2024_paper.php": {
    "title": "Fully Authentic Visual Question Answering Dataset from Online Communities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chongyan Chen*",
      "Mengchen Liu",
      "Noel C Codella",
      "Yunsheng Li",
      "Lu Yuan",
      "Danna Gurari"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6459_ECCV_2024_paper.php": {
    "title": "Active Generation for Image Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Huang",
      "Jiaqi Liu",
      "Shan You*",
      "Chang Xu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6460_ECCV_2024_paper.php": {
    "title": "FuseTeacher: Modality-fused Encoders are Strong Vision Supervisors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen-Wei Xie*",
      "Siyang Sun",
      "Liming Zhao",
      "Pandeng Li",
      "Shuailei Ma",
      "Yun Zheng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6463_ECCV_2024_paper.php": {
    "title": "Learning Local Pattern Modularization for Point Cloud Reconstruction from Unseen Classes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Chen",
      "Yu-Shen Liu*",
      "Zhizhong Han"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6472_ECCV_2024_paper.php": {
    "title": "Understanding Multi-compositional learning in Vision and Language models via Category Theory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sotirios Panagiotis Chytas*",
      "Hyunwoo J Kim",
      "Vikas Singh"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6476_ECCV_2024_paper.php": {
    "title": "FedRA: A Random Allocation Strategy for Federated Tuning to Unleash the Power of Heterogeneous Clients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shangchao Su",
      "Bin Li*",
      "Xiangyang Xue"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6479_ECCV_2024_paper.php": {
    "title": "Panel-Specific Degradation Representation for Raw Under-Display Camera Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youngjin Oh*",
      "Keuntek Lee",
      "Jooyoung Lee",
      "Dae-Hyun Lee",
      "Nam Ik Cho"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6480_ECCV_2024_paper.php": {
    "title": "Unlocking Textual and Visual Wisdom: Open-Vocabulary 3D Object Detection Enhanced by Comprehensive Guidance from Text and Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengkun Jiao*",
      "Na Zhao*",
      "Jingjing Chen",
      "Yu-Gang Jiang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6482_ECCV_2024_paper.php": {
    "title": "Diffusion-Guided Weakly Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sung-Hoon Yoon",
      "Hoyong Kwon",
      "Jaeseok Jeong",
      "Daehee Park",
      "Kuk-Jin Yoon*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6485_ECCV_2024_paper.php": {
    "title": "Weakly-Supervised Spatio-Temporal Video Grounding with Variational Cross-Modal Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Jin*",
      "Yadong Mu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6487_ECCV_2024_paper.php": {
    "title": "When Pedestrian Detection Meets Multi-Modal Learning: Generalist Model and Benchmark Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Zhang",
      "Wang Zeng",
      "Sheng Jin",
      "Chen Qian*",
      "Ping Luo",
      "Wentao Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6489_ECCV_2024_paper.php": {
    "title": "NVS-Adapter: Plug-and-Play Novel View Synthesis from a Single Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yoonwoo Jeong",
      "Jinwoo Lee",
      "Chiheon Kim",
      "Minsu Cho*",
      "Doyup Lee*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6495_ECCV_2024_paper.php": {
    "title": "Segment and Recognize Anything at Any Granularity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feng Li*",
      "Hao Zhang",
      "Peize Sun",
      "Xueyan Zou",
      "Shilong Liu",
      "Chunyuan Li",
      "Jianwei Yang",
      "Lei Zhang*",
      "Jianfeng Gao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6496_ECCV_2024_paper.php": {
    "title": "Real-time Holistic Robot Pose Estimation with Unknown States",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shikun Ban",
      "Juling Fan",
      "Xiaoxuan Ma",
      "Wentao Zhu*",
      "Yu QIAO*",
      "Yizhou Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6497_ECCV_2024_paper.php": {
    "title": "CLOSER: Towards Better Representation Learning for Few-Shot Class-Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junghun Oh",
      "Sungyong Baik",
      "Kyoung Mu Lee*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6499_ECCV_2024_paper.php": {
    "title": "A Simple Baseline for Spoken Language to Sign Language Translation with 3D Avatars",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ronglai Zuo",
      "Fangyun Wei*",
      "Zenggui Chen",
      "Brian Mak",
      "Jiaolong Yang",
      "Xin Tong"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6511_ECCV_2024_paper.php": {
    "title": "An accurate detection is not all you need to combat label noise in web-noisy datasets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paul Albert*",
      "Kevin McGuinness",
      "Eric Arazo",
      "Tarun Krishna",
      "Noel O Connor",
      "Jack Valmadre"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6513_ECCV_2024_paper.php": {
    "title": "Online Vectorized HD Map Construction using Geometry",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhixin Zhang",
      "Yiyuan Zhang",
      "Xiaohan Ding",
      "Fusheng Jin*",
      "Xiangyu Yue"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6517_ECCV_2024_paper.php": {
    "title": "Image-adaptive 3D Lookup Tables for Real-time Image Enhancement with Bilateral Grids",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wontae Kim*",
      "Nam Ik Cho*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6526_ECCV_2024_paper.php": {
    "title": "Learned HDR Image Compression for Perceptually Optimal Storage and Display",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peibei Cao",
      "HAOYU CHEN",
      "Jingzhe Ma",
      "Yu-Chieh Yuan",
      "Zhiyong Xie",
      "Xin Xie",
      "Haiqing Bai",
      "Kede Ma*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6528_ECCV_2024_paper.php": {
    "title": "Sparse Beats Dense: Rethinking Supervision in Radar-Camera Depth Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huadong Li",
      "Minhao Jing",
      "Jin Wang",
      "Shichao Dong",
      "Jiajun Liang",
      "Haoqiang Fan",
      "Renhe Ji*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6534_ECCV_2024_paper.php": {
    "title": "Non-Exemplar Domain Incremental Learning via Cross-Domain Concept Integration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiang Wang*",
      "Yuhang He",
      "Songlin Dong",
      "Xinyuan Gao",
      "Shaokun Wang",
      "Yihong Gong"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6543_ECCV_2024_paper.php": {
    "title": "Free-VSC: Free Semantics from Visual Foundation Models for Unsupervised Video Semantic Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Tian*",
      "Guo Lu*",
      "Guangtao Zhai*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6545_ECCV_2024_paper.php": {
    "title": "Improving Virtual Try-On with Garment-focused Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siqi Wan",
      "Yehao Li",
      "Jingwen Chen",
      "Yingwei Pan*",
      "Ting Yao",
      "Yang Cao",
      "Tao Mei"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6549_ECCV_2024_paper.php": {
    "title": "Ray Denoising: Depth-aware Hard Negative Sampling for Multi-view 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feng Liu*",
      "Tengteng Huang",
      "Qianjing Zhang",
      "Haotian Yao",
      "Chi Zhang",
      "Fang Wan",
      "Qixiang Ye",
      "Yanzhao Zhou*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6554_ECCV_2024_paper.php": {
    "title": "Disentangled Generation and Aggregation for Robust Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shihe Shen",
      "Huachen Gao",
      "Wangze Xu",
      "Rui Peng",
      "Luyang Tang",
      "Kaiqiang Xiong",
      "Jianbo Jiao",
      "Ronggang Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6561_ECCV_2024_paper.php": {
    "title": "UNIKD: UNcertainty-Filtered Incremental Knowledge Distillation for Neural Implicit Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengqi Guo*",
      "Chen Li",
      "Hanlin Chen",
      "Gim Hee Lee"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6562_ECCV_2024_paper.php": {
    "title": "Subspace Prototype Guidance for Mitigating Class Imbalance in Point Cloud Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawei Han",
      "Kaiqi Liu*",
      "Wei Li",
      "Guangzhi Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6579_ECCV_2024_paper.php": {
    "title": "MoAI: Mixture of All Intelligence for Large Language and Vision Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Byung-Kwan Lee",
      "Beomchan Park",
      "Chae Won Kim",
      "Yong Man Ro*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6583_ECCV_2024_paper.php": {
    "title": "Semantic-guided Robustness Tuning for Few-Shot Transfer Across Extreme Domain Shift",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "kangyu xiao*",
      "Zilei Wang",
      "junjie li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6586_ECCV_2024_paper.php": {
    "title": "Revisit Event Generation Model: Self-Supervised Learning of Event-to-Video Reconstruction with Implicit Neural Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zipeng Wang*",
      "yunfan lu",
      "Lin Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6593_ECCV_2024_paper.php": {
    "title": "SDPT: Synchronous Dual Prompt Tuning for Fusion-based Visual-Language Pre-trained Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Zhou*",
      "Yongjian Wu",
      "Jiya Saiyin",
      "Bingzheng Wei",
      "Maode Lai",
      "Eric I Chang",
      "Yan Xu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6595_ECCV_2024_paper.php": {
    "title": "Open-World Dynamic Prompt and Continual Visual Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youngeun Kim",
      "Jun Fang*",
      "Qin Zhang",
      "Zhaowei Cai",
      "Yantao Shen",
      "Rahul Duggal",
      "Dripta S. Raychaudhuri",
      "Zhuowen Tu",
      "Yifan Xing",
      "Onkar Dabeer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6601_ECCV_2024_paper.php": {
    "title": "Learning Video Context as Interleaved Multimodal Sequences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kevin Qinghong Lin",
      "Pengchuan Zhang",
      "Difei Gao",
      "Xide Xia",
      "Joya Chen",
      "Ziteng Gao",
      "Jinheng Xie",
      "Xuhong Xiao",
      "Mike Zheng Shou*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6611_ECCV_2024_paper.php": {
    "title": "Learning Unsigned Distance Functions from Multi-view Images with Volume Rendering Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenyuan Zhang",
      "Kanle Shi",
      "Yu-Shen Liu*",
      "Zhizhong Han"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6612_ECCV_2024_paper.php": {
    "title": "Dense Multimodal Alignment for Open-Vocabulary 3D Scene Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruihuang Li*",
      "Zhengqiang ZHANG",
      "Chenhang He",
      "Zhiyuan Ma",
      "Vishal Patel",
      "Lei Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6613_ECCV_2024_paper.php": {
    "title": "Deep Feature Surgery: Towards Accurate and Efficient Multi-Exit Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Gong",
      "Yao Chen*",
      "Qiuyang Luo",
      "Ye Lu",
      "Tao Li",
      "Yuzhi Zhang",
      "Yufei Sun*",
      "Le Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6619_ECCV_2024_paper.php": {
    "title": "Multi-scale Cross Distillation for Object Detection in Aerial Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kun Wang",
      "Zi Wang",
      "Zhang Li*",
      "Xichao Teng",
      "Yang Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6621_ECCV_2024_paper.php": {
    "title": "Progressive Proxy Anchor Propagation for Unsupervised Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyun Seok Seong",
      "WonJun Moon",
      "SuBeen Lee",
      "Jae-Pil Heo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6624_ECCV_2024_paper.php": {
    "title": "Within the Dynamic Context: Inertia-aware 3D Human Modeling with Pose Sequence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yutong Chen",
      "Yifan Zhan",
      "Zhihang Zhong*",
      "Wei Wang",
      "Xiao Sun*",
      "Yu Qiao",
      "Yinqiang Zheng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6630_ECCV_2024_paper.php": {
    "title": "Revisit Human-Scene Interaction via Space Occupancy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinpeng Liu",
      "Haowen Hou",
      "Yanchao Yang",
      "Yong-Lu Li*",
      "Cewu Lu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6634_ECCV_2024_paper.php": {
    "title": "Face Adapter for Pre-Trained Diffusion Models with Fine-Grained ID and Attribute Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Han*",
      "Junwei Zhu",
      "Keke He",
      "Xu Chen",
      "Yanhao Ge",
      "Wei Li",
      "Xiangtai Li",
      "Jiangning Zhang",
      "Chengjie Wang",
      "Yong Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6635_ECCV_2024_paper.php": {
    "title": "WeConvene: Learned Image Compression with Wavelet-Domain Convolution and Entropy Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haisheng Fu*",
      "Jie Liang",
      "Zhenman Fang",
      "Jingning Han",
      "Feng Liang",
      "Guohe Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6640_ECCV_2024_paper.php": {
    "title": "Grid-Attention: Enhancing Computational Efficiency of Large Vision Models without Fine-Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengyu Li*",
      "biao wang",
      "Tianchu Guo",
      "Xian-Sheng Hua"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6641_ECCV_2024_paper.php": {
    "title": "Mitigating Background Shift in Class-Incremental Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gilhan Park",
      "WonJun Moon",
      "SuBeen Lee",
      "Tae-Young Kim",
      "Jae-Pil Heo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6646_ECCV_2024_paper.php": {
    "title": "Relation DETR: Exploring Explicit Position Relation Prior for Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiuquan Hou",
      "Meiqin Liu*",
      "Senlin Zhang",
      "Ping Wei",
      "Badong Chen",
      "Xuguang Lan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6649_ECCV_2024_paper.php": {
    "title": "BKDSNN: Enhancing the Performance of Learning-based Spiking Neural Networks Training with Blurred Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zekai Xu",
      "Kang You",
      "Qinghai Guo",
      "Xiang Wang",
      "Zhezhi He*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6652_ECCV_2024_paper.php": {
    "title": "Agent Attention: On the Integration of Softmax and Linear Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongchen Han",
      "Tianzhu Ye",
      "Yizeng Han",
      "Zhuofan Xia",
      "Siyuan Pan",
      "Pengfei Wan",
      "Shiji Song",
      "Gao Huang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6656_ECCV_2024_paper.php": {
    "title": "Learning by Aligning 2D Skeleton Sequences and Multi-Modality Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quoc-Huy Tran*",
      "Muhammad Ahmed",
      "Murad Popattia",
      "Muhammad Hassan Ahmed",
      "Andrey Konin",
      "Zeeshan Zia"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6658_ECCV_2024_paper.php": {
    "title": "Resolving Scale Ambiguity in Multi-view 3D Reconstruction using Dual-Pixel Sensors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kohei Ashida*",
      "Hiroaki Santo",
      "Fumio Okura",
      "Yasuyuki Matsushita"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6669_ECCV_2024_paper.php": {
    "title": "Object-Oriented Anchoring and Modal Alignment in Multimodal Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shibin Mei",
      "Bingbing Ni*",
      "Hang Wang",
      "Chenglong Zhao",
      "fengfa hu",
      "Zhiming Pi",
      "BiLian Ke"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6670_ECCV_2024_paper.php": {
    "title": "Towards Stable 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiabao Wang",
      "Qiang Meng",
      "Guochao Liu",
      "Liujiang Yan",
      "Ke Wang",
      "Ming-Ming Cheng",
      "Qibin Hou*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6675_ECCV_2024_paper.php": {
    "title": "FYI: Flip Your Images for Dataset Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Byunggwan Son*",
      "Youngmin Oh",
      "Donghyeon Baek",
      "Bumsub Ham*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6676_ECCV_2024_paper.php": {
    "title": "On-the-fly Category Discovery for LiDAR Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyeonseong Kim",
      "Sung-Hoon Yoon",
      "Minseok Kim",
      "Kuk-Jin Yoon*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6678_ECCV_2024_paper.php": {
    "title": "Dual-Camera Smooth Zoom on Mobile Phones",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Renlong Wu",
      "Zhilu Zhang*",
      "Yu Yang",
      "Wangmeng Zuo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6685_ECCV_2024_paper.php": {
    "title": "ProtoComp: Diverse Point Cloud Completion with Controllable Prototype",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xumin Yu",
      "Yanbo Wang",
      "Jie Zhou",
      "Jiwen Lu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6695_ECCV_2024_paper.php": {
    "title": "CONDA: Condensed Deep Association Learning for Co-Salient Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Long Li",
      "Nian Liu*",
      "Dingwen Zhang",
      "Zhongyu Li",
      "Salman Khan",
      "Rao Anwer",
      "Hisham Cholakkal",
      "Junwei Han*",
      "Fahad Shahbaz Khan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6701_ECCV_2024_paper.php": {
    "title": "Cascade Prompt Learning for Visual-Language Model Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ge Wu",
      "Xin Zhang",
      "Zheng Li",
      "Zhaowei Chen",
      "Jiajun Liang",
      "Jian Yang",
      "Xiang Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6708_ECCV_2024_paper.php": {
    "title": "PolyRoom: Room-aware Transformer for Floorplan Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuzhou Liu",
      "Lingjie Zhu",
      "Xiaodong Ma",
      "Hanqiao Ye",
      "Xiang Gao",
      "Xianwei Zheng",
      "Shuhan Shen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6710_ECCV_2024_paper.php": {
    "title": "BenchLMM: Benchmarking Cross-style Visual Capability of Large Multimodal Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rizhao Cai*",
      "Zirui Song",
      "Dayan Guan*",
      "Zhenhao Chen",
      "Yaohang Li",
      "Xing Luo",
      "Chenyu Yi",
      "Alex Kot"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6713_ECCV_2024_paper.php": {
    "title": "SMFANet: A Lightweight Self-Modulation Feature Aggregation Network for Efficient Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "mingjun zheng",
      "Long Sun",
      "Jiangxin Dong",
      "Jinshan Pan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6715_ECCV_2024_paper.php": {
    "title": "HENet: Hybrid Encoding for End-to-end Multi-task 3D Perception from Multi-view Cameras",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongyu Xia",
      "ZhiWei Lin",
      "Xinhao Wang",
      "Yongtao Wang*",
      "Yun Xing",
      "Shengxiang Qi",
      "Nan Dong",
      "Ming-Hsuan Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6726_ECCV_2024_paper.php": {
    "title": "Hierarchical Unsupervised Relation Distillation for Source Free Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowei Xing*",
      "Xianghua Ying",
      "Ruibin Wang",
      "Ruohao Guo",
      "Ji Shi",
      "Wenzhen Yue"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6727_ECCV_2024_paper.php": {
    "title": "Customized Generation Reimagined: Fidelity and Editability Harmonized",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Jin",
      "Yang Shen",
      "Zhenyong Fu*",
      "Jian Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6729_ECCV_2024_paper.php": {
    "title": "AUFormer: Vision Transformers are Parameter-Efficient Facial Action Unit Detectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaishen Yuan",
      "Zitong Yu*",
      "Xin Liu*",
      "Weicheng Xie",
      "Huanjing Yue",
      "Jingyu Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6733_ECCV_2024_paper.php": {
    "title": "Improving Video Segmentation via Dynamic Anchor Queries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yikang Zhou",
      "Tao Zhang*",
      "Xiangtai Li*",
      "Shunping Ji*",
      "Shuicheng Yan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6734_ECCV_2024_paper.php": {
    "title": "Controllable Contextualized Image Captioning: Directing the Visual Narrative through User-Defined Highlights",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shunqi Mao*",
      "Chaoyi Zhang",
      "Hang Su",
      "Hwanjun Song",
      "Igor Shalyminov",
      "Weidong Cai"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6735_ECCV_2024_paper.php": {
    "title": "Diffusion Models as Optimizers for Efficient Planning in Offline RL",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Renming Huang",
      "Yunqiang Pei",
      "Guoqing Wang*",
      "Yangming Zhang",
      "Yang Yang",
      "Peng Wang",
      "Heng Tao Shen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6736_ECCV_2024_paper.php": {
    "title": "Enhanced Sparsification via Stimulative Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengji Tang",
      "Weihao Lin",
      "Hancheng Ye",
      "Peng Ye",
      "Chong Yu",
      "Baopu Li",
      "Tao Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6741_ECCV_2024_paper.php": {
    "title": "How Many Unicorns Are in This Image? A Safety Evaluation Benchmark for Vision LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoqin Tu*",
      "Chenhang Cui",
      "Zijun Wang",
      "Yiyang Zhou",
      "Bingchen Zhao",
      "Junlin Han",
      "Wangchunshu Zhou",
      "Huaxiu Yao",
      "Cihang Xie*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6746_ECCV_2024_paper.php": {
    "title": "NeuroPictor: Refining fMRI-to-Image Reconstruction via Multi-individual Pretraining and Multi-level Modulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyang Huo",
      "Yikai Wang",
      "Yanwei Fu*",
      "Xuelin Qian",
      "Chong Li",
      "Yun Wang",
      "Jianfeng Feng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6748_ECCV_2024_paper.php": {
    "title": "Coarse-to-Fine Implicit Representation Learning for 3D Hand-Object Reconstruction from a Single RGB-D Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyu Liu",
      "Pengfei Ren",
      "Jingyu Wang*",
      "Qi Qi",
      "Haifeng Sun",
      "Zirui Zhuang*",
      "Jianxin Liao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6750_ECCV_2024_paper.php": {
    "title": "Efficient Snapshot Spectral Imaging: Calibration-Free Parallel Structure with Aperture Diffraction Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Lv*",
      "Lihao Hu",
      "Shiqiao Li",
      "Chenglong Huang",
      "Xun Cao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6751_ECCV_2024_paper.php": {
    "title": "Enhancing Recipe Retrieval with Foundation Models: A Data Augmentation Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangzhou Song",
      "Bin Zhu",
      "Yanbin Hao*",
      "Shuo Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6758_ECCV_2024_paper.php": {
    "title": "PapMOT: Exploring Adversarial Patch Attack against Multiple Object Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahuan Long*",
      "Tingsong Jiang*",
      "Wen Yao*",
      "Shuai Jia*",
      "Weijia Zhang*",
      "Weien Zhou*",
      "Chao Ma*",
      "Xiaoqian Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6764_ECCV_2024_paper.php": {
    "title": "HiDiffusion: Unlocking Higher-Resolution Creativity and Efficiency in Pretrained Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shen Zhang",
      "Zhaowei CHEN",
      "Zhenyu Zhao",
      "Yuhao Chen",
      "Yao Tang",
      "Jiajun Liang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6766_ECCV_2024_paper.php": {
    "title": "On the Approximation Risk of Few-Shot Class-Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan Wang",
      "Zhong Ji*",
      "Xiyao Liu",
      "Yanwei Pang",
      "Jungong Han"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6768_ECCV_2024_paper.php": {
    "title": "Syn-to-Real Domain Adaptation for Point Cloud Completion via Part-based Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunseo Yang",
      "Jihun Kim",
      "Kuk-Jin Yoon*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6773_ECCV_2024_paper.php": {
    "title": "Learn to Preserve and Diversify: Parameter-Efficient Group with Orthogonal Regularization for Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiajun Hu",
      "Jian Zhang",
      "Lei Qi*",
      "Yinghuan Shi*",
      "Yang Gao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6776_ECCV_2024_paper.php": {
    "title": "SCOMatch: Alleviating Overtrusting in Open-set Semi-supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zerun Wang*",
      "Liuyu Xiang",
      "Lang Huang",
      "Jiafeng Mao",
      "Ling Xiao",
      "Toshihiko Yamasaki"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6781_ECCV_2024_paper.php": {
    "title": "Region-aware Distribution Contrast: A Novel Approach to Multi-Task Partially Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meixuan Li",
      "Tianyu Li",
      "Guoqing Wang*",
      "Peng Wang",
      "Yang Yang",
      "Jie Zou"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6786_ECCV_2024_paper.php": {
    "title": "MasterWeaver: Taming Editability and Face Identity for Personalized Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxiang Wei",
      "Zhilong Ji",
      "Jinfeng Bai",
      "Hongzhi Zhang",
      "Lei Zhang*",
      "Wangmeng Zuo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6787_ECCV_2024_paper.php": {
    "title": "PointRegGPT: Boosting 3D Point Cloud Registration using Generative Point-Cloud Pairs for Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suyi Chen",
      "Hao Xu",
      "Haipeng Li",
      "Kunming Luo",
      "Guanghui Liu",
      "Chi-Wing Fu",
      "Ping Tan",
      "Shuaicheng Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6788_ECCV_2024_paper.php": {
    "title": "General Geometry-aware Weakly Supervised 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guowen Zhang*",
      "Junsong Fan",
      "Liyi Chen",
      "Zhaoxiang Zhang",
      "Zhen Lei",
      "Lei Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6793_ECCV_2024_paper.php": {
    "title": "Long-CLIP: Unlocking the Long-Text Capability of CLIP",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Beichen Zhang*",
      "Pan Zhang",
      "Xiaoyi Dong*",
      "Yuhang Zang",
      "Jiaqi Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6812_ECCV_2024_paper.php": {
    "title": "Dolfin: Diffusion Layout Transformers without Autoencoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yilin Wang",
      "Zeyuan Chen",
      "Liangjun Zhong",
      "Zheng Ding",
      "Zhuowen Tu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6814_ECCV_2024_paper.php": {
    "title": "Real-time 3D-aware Portrait Editing from a Single Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingyan Bai*",
      "Zifan Shi",
      "Yinghao Xu",
      "Hao Ouyang",
      "Qiuyu Wang",
      "Ceyuan Yang",
      "Xuan Wang",
      "Gordon Wetzstein",
      "Yujun Shen*",
      "Qifeng Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6815_ECCV_2024_paper.php": {
    "title": "StructLDM: Structured Latent Diffusion for 3D Human Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Hu",
      "Fangzhou Hong",
      "Ziwei Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6823_ECCV_2024_paper.php": {
    "title": "Image Compression for Machine and Human Vision With Spatial-Frequency Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Li*",
      "Shaohui Li*",
      "Shuangrui Ding",
      "Wenrui Dai*",
      "Maida Cao",
      "Chenglin Li",
      "Junni Zou",
      "Hongkai Xiong"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6824_ECCV_2024_paper.php": {
    "title": "Beyond the Contact: Discovering Comprehensive Affordance for 3D Objects from Pre-trained 2D Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyeonwoo Kim",
      "Sookwan Han",
      "Patrick Kwon",
      "Hanbyul Joo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6825_ECCV_2024_paper.php": {
    "title": "Norma: A Noise Robust Memory-Augmented Framework for Whole Slide Image Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Bai",
      "Bo Zhang*",
      "Zheng Zhang",
      "Shuo Yan",
      "Zibo Ma",
      "Wu Liu",
      "Xiuzhuang Zhou",
      "Xiangyang Gong",
      "Wendong Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6826_ECCV_2024_paper.php": {
    "title": "Continuous Memory Representation for Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joo Chan Lee*",
      "Taejune Kim",
      "Eunbyung Park*",
      "Simon S Woo*",
      "Jong Hwan Ko*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6828_ECCV_2024_paper.php": {
    "title": "InstaStyle: Inversion Noise of a Stylized Image is Secretly a Style Adviser",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xing Cui",
      "Zekun Li",
      "Peipei Li*",
      "Huaibo Huang",
      "Xuannan Liu",
      "Zhaofeng He"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6837_ECCV_2024_paper.php": {
    "title": "PACE: Pose Annotations in Cluttered Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang You*",
      "kai xiong",
      "Zhening Yang",
      "Zhengxiang Huang",
      "Junwei Zhou",
      "Ruoxi Shi",
      "Zhou FANG",
      "Adam Harley",
      "Leonidas Guibas",
      "Cewu Lu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6838_ECCV_2024_paper.php": {
    "title": "CMTA: Cross-Modal Temporal Alignment for Event-guided Video Deblurring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taewoo Kim",
      "Hoonhee Cho",
      "Kuk-Jin Yoon*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6839_ECCV_2024_paper.php": {
    "title": "CountFormer: Multi-View Crowd Counting Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hong Mo*",
      "Xiong Zhang*",
      "Jianchao Tan",
      "Cheng Yang",
      "Qiong Gu",
      "Bo Hang",
      "Wenqi Ren"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6840_ECCV_2024_paper.php": {
    "title": "Textual Knowledge Matters: Cross-Modality Co-Teaching for Generalized Visual Class Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiyang Zheng",
      "Nan Pu",
      "Wenjing Li*",
      "Nicu Sebe",
      "Zhun Zhong*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6843_ECCV_2024_paper.php": {
    "title": "Continuous SO(3) Equivariant Convolution for 3D Point Cloud Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaein Kim",
      "HEE BIN YOO",
      "Dong-Sig Han",
      "Yeon-Ji Song",
      "Byoung-Tak Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6845_ECCV_2024_paper.php": {
    "title": "EA-VTR: Event-Aware Video-Text Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zongyang Ma*",
      "Ziqi Zhang",
      "Yuxin Chen",
      "Zhongang Qi",
      "Chunfeng Yuan",
      "Bing Li",
      "Yingmin Luo",
      "Xu LI",
      "Xiaojuan Qi",
      "Ying Shan",
      "Weiming Hu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6850_ECCV_2024_paper.php": {
    "title": "Privacy-Preserving Adaptive Re-Identification without Image Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hamza Rami*",
      "Jhony H. Giraldo",
      "Nicolas Winckler",
      "Stéphane Lathuilière"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6851_ECCV_2024_paper.php": {
    "title": "A Simple Low-bit Quantization Framework for Video Snapshot Compressive Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miao Cao*",
      "Lishun Wang",
      "Huan Wang",
      "Xin Yuan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6852_ECCV_2024_paper.php": {
    "title": "DIFFender: Diffusion-Based Adversarial Defense against Patch Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Caixin Kang*",
      "Yinpeng Dong",
      "Zhengyi Wang",
      "Shouwei Ruan",
      "Yubo Chen",
      "Hang Su*",
      "Xingxing Wei*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6854_ECCV_2024_paper.php": {
    "title": "Hybrid Video Diffusion Models with 2D Triplane and 3D Wavelet Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kihong Kim",
      "Haneol Lee",
      "Jihye Park",
      "Seyeon Kim",
      "Kwang Hee Lee",
      "Seungryong Kim*",
      "Jaejun Yoo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6856_ECCV_2024_paper.php": {
    "title": "Background Adaptation with Residual Modeling for Exemplar-Free Class-Incremental Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anqi Zhang",
      "Guangyu Gao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6858_ECCV_2024_paper.php": {
    "title": "Efficient Diffusion-Driven Corruption Editor for Test-Time Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yeongtak Oh",
      "Jonghyun Lee",
      "Jooyoung Choi",
      "Dahuin Jung",
      "Uiwon Hwang*",
      "Sungroh Yoon*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6863_ECCV_2024_paper.php": {
    "title": "Learning to Unlearn for Robust Machine Unlearning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mark He Huang*",
      "Lin Geng Foo",
      "Jun Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6864_ECCV_2024_paper.php": {
    "title": "Emergent Visual-Semantic Hierarchies in Image-Text Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Morris Alper*",
      "Hadar Averbuch-Elor"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6867_ECCV_2024_paper.php": {
    "title": "Context-Guided Spatial Feature Reconstruction for Efficient Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenliang Ni",
      "Xinghao Chen*",
      "Yingjie Zhai",
      "Yehui Tang",
      "Yunhe Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6870_ECCV_2024_paper.php": {
    "title": "DriveLM: Driving with Graph Visual Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chonghao Sima*",
      "Katrin Renz",
      "Kashyap Chitta",
      "Li Chen",
      "Zhang Hanxue",
      "Chengen Xie",
      "Jens Beißwenger",
      "Ping Luo",
      "Andreas Geiger",
      "Hongyang Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6871_ECCV_2024_paper.php": {
    "title": "Neural Spectral Decomposition for Dataset Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaolei Yang",
      "Shen Cheng",
      "Mingbo Hong",
      "Haoqiang Fan",
      "Xing Wei",
      "Shuaicheng Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6872_ECCV_2024_paper.php": {
    "title": "Beyond Viewpoint: Robust 3D Object Recognition under Arbitrary Views through Joint Multi-Part Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linlong Fan",
      "Ye Huang*",
      "Yanqi Ge",
      "Wen Li",
      "Lixin Duan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6873_ECCV_2024_paper.php": {
    "title": "Learning Non-Linear Invariants for Unsupervised Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lars Doorenbos*",
      "Raphael Sznitman",
      "Pablo Márquez Neila"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6893_ECCV_2024_paper.php": {
    "title": "Dynamic Retraining-Updating Mean Teacher for Source-Free Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Trinh Le Ba Khanh*",
      "Huy-Hung Nguyen",
      "Long Hoang Pham",
      "Duong Nguyen-Ngoc Tran",
      "Jae Wook Jeon*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6901_ECCV_2024_paper.php": {
    "title": "Knowledge-enhanced Visual-Language Pretraining for Computational Pathology",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Zhou",
      "Xiaoman Zhang",
      "Chaoyi Wu",
      "Ya Zhang",
      "Weidi Xie",
      "Yan-Feng Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6903_ECCV_2024_paper.php": {
    "title": "Adaptive Multi-modal Fusion of Spatially Variant Kernel Refinement with Diffusion Model for Blind Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junxiong Lin*",
      "Yan Wang",
      "Zeng Tao",
      "Boyang Wang",
      "Qing Zhao",
      "Haoran Wang",
      "Xuan Tong",
      "Xinji Mai",
      "Yuxuan Lin",
      "Wei Song",
      "Jiawen Yu",
      "Shaoqi Yan",
      "Wenqiang Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6908_ECCV_2024_paper.php": {
    "title": "Disentangled Clothed Avatar Generation from Text Descriptions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jionghao Wang*",
      "Yuan Liu",
      "Zhiyang Dou",
      "Zhengming Yu",
      "Yongqing Liang",
      "Cheng Lin",
      "Rong Xie",
      "Li Song*",
      "Xin Li",
      "Wenping Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6913_ECCV_2024_paper.php": {
    "title": "Real Appearance Modeling for More General Deepfake Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahe Tian",
      "Cai Yu",
      "Xi Wang",
      "Peng Chen",
      "Zihao Xiao",
      "Jiao Dai",
      "Yesheng Chai*",
      "Jizhong Han"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6914_ECCV_2024_paper.php": {
    "title": "6DGS: 6D Pose Estimation from a Single Image and a 3D Gaussian Splatting Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matteo Bortolon*",
      "Theodore Tsesmelis",
      "Stuart James",
      "Fabio Poiesi",
      "Alessio Del Bue"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6917_ECCV_2024_paper.php": {
    "title": "Dual-Decoupling Learning and Metric-Adaptive Thresholding for Semi-Supervised Multi-Label Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jia-Hao Xiao",
      "Ming-Kun Xie",
      "Heng-Bo Fan",
      "Gang Niu",
      "Masashi Sugiyama",
      "Sheng-Jun Huang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6926_ECCV_2024_paper.php": {
    "title": "V2X-Real: a Largs-Scale Dataset for Vehicle-to-Everything Cooperative Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Xiang",
      "Xin Xia",
      "Zhaoliang Zheng",
      "Runsheng Xu",
      "Letian Gao",
      "Zewei Zhou",
      "xu han",
      "Xinkai Ji",
      "Mingxi Li",
      "Zonglin Meng",
      "Li Jin",
      "Mingyue Lei",
      "Zhaoyang Ma",
      "Zihang He",
      "Haoxuan Ma",
      "Yunshuang Yuan",
      "Yingqian Zhao",
      "Jiaqi Ma*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6929_ECCV_2024_paper.php": {
    "title": "VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guénolé Fiche*",
      "Simon Leglaive",
      "Xavier Alameda-Pineda",
      "Antonio Agudo",
      "Francesc Moreno"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6931_ECCV_2024_paper.php": {
    "title": "Attention Beats Linear for Fast Implicit Neural Representation Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuyi Zhang",
      "Ke Liu",
      "Jingjun Gu",
      "Xiaoxu Cai",
      "Zhihua Wang",
      "Jiajun Bu",
      "Haishuai Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6938_ECCV_2024_paper.php": {
    "title": "HARIVO: Harnessing Text-to-Image Models for Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingi Kwon",
      "Seoung Wug Oh",
      "Yang Zhou",
      "Joon-Young Lee",
      "Difan Liu",
      "Haoran Cai",
      "Baqiao Liu",
      "Feng Liu",
      "Youngjung Uh*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6944_ECCV_2024_paper.php": {
    "title": "Deep Online Probability Aggregation Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Yan",
      "Na Lu*",
      "Ruofan Yan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6947_ECCV_2024_paper.php": {
    "title": "WRIM-Net: Wide-Ranging Information Mining Network for Visible-Infrared Person Re-Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yonggan Wu",
      "Ling-Chao Meng*",
      "Yuan Zichao",
      "Sixian Chan",
      "Hong-Qiang Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6950_ECCV_2024_paper.php": {
    "title": "Reliable and Efficient Concept Erasure of Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Gong*",
      "Kai Chen",
      "Zhipeng Wei",
      "Jingjing Chen*",
      "Yu-Gang Jiang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6951_ECCV_2024_paper.php": {
    "title": "Visual Text Generation in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanzhi Zhu",
      "Jiawei Liu",
      "Feiyu Gao",
      "Wenyu Liu*",
      "Xinggang Wang",
      "Peng Wang",
      "Fei Huang",
      "Cong Yao",
      "Zhibo Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6952_ECCV_2024_paper.php": {
    "title": "Length-Aware Motion Synthesis via Latent Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alessio Sampieri*",
      "Alessio Palma",
      "Indro Spinelli",
      "Fabio Galasso"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6954_ECCV_2024_paper.php": {
    "title": "Attention-Challenging Multiple Instance Learning for Whole Slide Image Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunlong Zhang*",
      "Honglin Li",
      "YUXUAN SUN",
      "Chenglu Zhu",
      "Sunyi Zheng",
      "Lin Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6958_ECCV_2024_paper.php": {
    "title": "An Optimal Control View of LoRA and Binary Controller Design for Vision Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chi Zhang*",
      "Jingpu Cheng",
      "Qianxiao Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6963_ECCV_2024_paper.php": {
    "title": "Exploring Phrase-Level Grounding with Text-to-Image Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Danni Yang",
      "Ruohan Dong",
      "Jiayi Ji",
      "Yiwei Ma",
      "Haowei Wang",
      "Xiaoshuai Sun*",
      "Rongrong Ji"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6966_ECCV_2024_paper.php": {
    "title": "FocusDiffuser: Perceiving Local Disparities for Camouflaged Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianwei Zhao*",
      "Xin Li",
      "Fan Yang",
      "Qiang Zhai*",
      "Ao Luo",
      "Zhicheng Jiao",
      "Hong Cheng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6969_ECCV_2024_paper.php": {
    "title": "Improving image synthesis with diffusion-negative sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alakh Desai*",
      "Nuno Vasconcelos"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6979_ECCV_2024_paper.php": {
    "title": "AvatarPose: Avatar-guided 3D Pose Estimation of Close Human Interaction from Sparse Multi-view Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feichi Lu*",
      "Zijian Dong*",
      "Jie Song",
      "Otmar Hilliges"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6981_ECCV_2024_paper.php": {
    "title": "FedVAD: Enhancing Federated Video Anomaly Detection with GPT-Driven Semantic Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan Qi*",
      "Ruijie Pan",
      "Huaiwen Zhang",
      "Changsheng Xu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6988_ECCV_2024_paper.php": {
    "title": "SignGen: End-to-End Sign Language Video Generation with Latent Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan Qi*",
      "Yu Duan",
      "Changsheng Xu",
      "Huaiwen Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6989_ECCV_2024_paper.php": {
    "title": "Idling Neurons, Appropriately Lenient Workload During Fine-tuning Leads to Better Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongjing Niu*",
      "Hanting Li",
      "Bin Li",
      "Feng Zhao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6993_ECCV_2024_paper.php": {
    "title": "Diffusion Prior-Based Amortized Variational Inference for Noisy Inverse Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sojin Lee",
      "Dogyun Park",
      "Inho Kong",
      "Hyunwoo J. Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7003_ECCV_2024_paper.php": {
    "title": "The Gaussian Discriminant Variational Autoencoder (GdVAE): A Self-Explainable Model with Counterfactual Explanations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anselm Haselhoff*",
      "Kevin Trelenberg",
      "Fabian Küppers",
      "Jonas Schneider"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7004_ECCV_2024_paper.php": {
    "title": "Accelerating Image Generation with Sub-path Linear Approximation Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Xu",
      "Tianhui Song",
      "Weixin Feng",
      "Xubin Li",
      "Tiezheng Ge",
      "Bo Zheng",
      "Limin Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7009_ECCV_2024_paper.php": {
    "title": "Safe-CLIP: Removing NSFW Concepts from Vision-and-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuele Poppi*",
      "Tobia Poppi*",
      "Federico Cocchi",
      "Marcella Cornia",
      "Lorenzo Baraldi",
      "Rita Cucchiara"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7010_ECCV_2024_paper.php": {
    "title": "TetraDiffusion: Tetrahedral Diffusion Models for 3D Shape Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikolai Kalischek*",
      "Torben Peters",
      "Jan Dirk Wegner",
      "Konrad Schindler"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7011_ECCV_2024_paper.php": {
    "title": "Camera Calibration using a Collimator System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shunkun Liang",
      "Banglei Guan*",
      "Zhenbao Yu",
      "Pengju Sun",
      "Yang Shang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7015_ECCV_2024_paper.php": {
    "title": "Label-free Neural Semantic Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayi Wang*",
      "Kevin A Laube",
      "Yumeng Li",
      "Jan Hendrik Metzen",
      "Shin-I Cheng",
      "Julio Borges",
      "Anna Khoreva"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7022_ECCV_2024_paper.php": {
    "title": "Exploring Reliable Matching with Phase Enhancement for Night-time Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuwen Pan*",
      "Rui Sun",
      "Naisong Luo",
      "Tianzhu Zhang",
      "Yongdong Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7025_ECCV_2024_paper.php": {
    "title": "Multiscale Sliced Wasserstein Distances as Perceptual Color Difference Measures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqi He",
      "Zhihua Wang",
      "Leon Wang",
      "Tsein-I Liu",
      "Yuming Fang",
      "Qilin Sun*",
      "Kede Ma"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7026_ECCV_2024_paper.php": {
    "title": "DiscoMatch: Fast Discrete Optimisation for Geometrically Consistent 3D Shape Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paul Roetzer*",
      "Ahmed Abbas*",
      "Dongliang Cao",
      "Florian Bernard",
      "Paul Swoboda"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7035_ECCV_2024_paper.php": {
    "title": "Switch Diffusion Transformer: Synergizing Denoising Tasks with Sparse Mixture-of-Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Byeongjun Park",
      "Hyojun Go",
      "Jin-Young Kim",
      "Sangmin Woo",
      "Seokil Ham",
      "Changick Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7037_ECCV_2024_paper.php": {
    "title": "FARSE-CNN: Fully Asynchronous, Recurrent and Sparse Event-Based CNN",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Riccardo Santambrogio*",
      "Marco Cannici",
      "Matteo Matteucci"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7038_ECCV_2024_paper.php": {
    "title": "ConDense: Consistent 2D-3D Pre-training for Dense and Sparse Features from Multi-View Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoshuai Zhang*",
      "Zhicheng Wang",
      "Howard Zhou",
      "Soham Ghosh",
      "Danushen L Gnanapragasam",
      "Varun Jampani",
      "Hao Su",
      "Leonidas Guibas"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7040_ECCV_2024_paper.php": {
    "title": "MTA-CLIP: Language-Guided Semantic Segmentation with Mask-Text Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anurag Das*",
      "Xinting Hu",
      "Li Jiang",
      "Bernt Schiele"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7043_ECCV_2024_paper.php": {
    "title": "Event-Aided Time-To-Collision Estimation for Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinghang Li",
      "Bangyan Liao",
      "Xiuyuan Lu",
      "Peidong Liu",
      "Shaojie Shen",
      "Yi Zhou*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7047_ECCV_2024_paper.php": {
    "title": "The Devil is in the Statistics: Mitigating and Exploiting Statistics Difference for Generalizable Semi-supervised Medical Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muyang Qiu",
      "Jian Zhang",
      "Lei Qi",
      "Qian Yu",
      "Yinghuan Shi*",
      "Yang Gao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7051_ECCV_2024_paper.php": {
    "title": "VEON: Vocabulary-Enhanced Occupancy Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jilai Zheng",
      "Pin Tang",
      "Zhongdao Wang",
      "Guoqing Wang",
      "Xiangxuan Ren",
      "Bailan Feng",
      "Chao Ma*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7052_ECCV_2024_paper.php": {
    "title": "Adapt without Forgetting: Distill Proximity from Dual Teachers in Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengyu Zheng*",
      "Yehui Tang",
      "Zhiwei Hao",
      "Kai Han",
      "Yunhe Wang",
      "Chang Xu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7067_ECCV_2024_paper.php": {
    "title": "The Sky's the Limit: Relightable Outdoor Scenes via a Sky-pixel Constrained Illumination Prior and Outside-In Visibility",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "James A D Gardner*",
      "Evgenii Kashin",
      "Bernhard Egger",
      "William Smith"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7068_ECCV_2024_paper.php": {
    "title": "DiffFAS: Face Anti-Spoofing via Generative Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinxu Ge",
      "Xin Liu*",
      "Zitong Yu*",
      "Jingang Shi",
      "Chun Qi",
      "Jie Li",
      "Heikki Kälviäinen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7071_ECCV_2024_paper.php": {
    "title": "Hetecooper: Feature Collaboration Graph for Heterogeneous Collaborative Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Congzhang Shao",
      "Guiyang Luo*",
      "Quan Yuan*",
      "Yifu Chen",
      "Yilin Liu",
      "Gong Kexin",
      "Jinglin Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7072_ECCV_2024_paper.php": {
    "title": "Learning-based Axial Video Motion Magnification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kwon Byung-Ki",
      "Oh Hyun-Bin",
      "Kim Jun-Seong",
      "Hyunwoo Ha",
      "Tae-Hyun Oh*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7083_ECCV_2024_paper.php": {
    "title": "Simplifying Source-Free Domain Adaptation for Object Detection: Effective Self-Training Strategies and Performance Insights",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Hao",
      "Florent Forest*",
      "Olga Fink"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7092_ECCV_2024_paper.php": {
    "title": "Class-Incremental Learning with CLIP: Adaptive Representation Adjustment and Parameter Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linlan Huang",
      "Xusheng Cao",
      "Haori Lu",
      "Xialei Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7100_ECCV_2024_paper.php": {
    "title": "cDP-MIL: Robust Multiple Instance Learning via Cascaded Dirichlet Process",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihang Chen",
      "Tsai Hor Chan",
      "Guosheng Yin",
      "Yuming Jiang",
      "Lequan Yu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7107_ECCV_2024_paper.php": {
    "title": "Causality-inspired Discriminative Feature Learning in Triple Domains for Gait Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haijun Xiong",
      "Bin Feng*",
      "Xinggang Wang",
      "Wenyu Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7118_ECCV_2024_paper.php": {
    "title": "Retargeting Visual Data with Deformation Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tim Elsner*",
      "Julia Berger",
      "Tong Wu",
      "Victor Czech",
      "Lin Gao",
      "Leif Kobbelt"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7122_ECCV_2024_paper.php": {
    "title": "Delving Deep into Engagement Prediction of Short Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "dasong Li",
      "Wenjie Li",
      "Baili Lu",
      "Hongsheng Li",
      "Sizhuo Ma",
      "Gurunandan Krishnan",
      "Jian Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7132_ECCV_2024_paper.php": {
    "title": "Flexible Distribution Alignment: Towards Long-tailed Semi-supervised Learning with Proper Calibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emanuel Sanchez Aimar*",
      "Nathaniel D Helgesen",
      "Yonghao Xu",
      "Marco Kuhlmann",
      "Michael Felsberg"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7133_ECCV_2024_paper.php": {
    "title": "CLEO: Continual Learning of Evolving Ontologies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shishir Muralidhara*",
      "Saqib Bukhari",
      "Georg Dr. Schneider",
      "Didier Stricker",
      "René Schuster"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7134_ECCV_2024_paper.php": {
    "title": "SpecFormer: Guarding Vision Transformer Robustness via Maximum Singular Value Penalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xixu Hu",
      "Runkai Zheng",
      "Jindong Wang*",
      "Cheuk Hang Leung",
      "Qi Wu*",
      "Xing Xie"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7137_ECCV_2024_paper.php": {
    "title": "Wavelet Convolutions for Large Receptive Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shahaf E Finder*",
      "Roy Amoyal",
      "Eran Treister",
      "Oren Freifeld*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7138_ECCV_2024_paper.php": {
    "title": "BK-SDM: A Lightweight, Fast, and Cheap Version of Stable Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo-Kyeong Kim*",
      "Hyoung-Kyu Song",
      "Thibault Castells",
      "Shinkook Choi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7145_ECCV_2024_paper.php": {
    "title": "Language-Assisted Skeleton Action Understanding for Skeleton-Based Temporal Action Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu Ji",
      "Bowen Chen",
      "Xinglong Xu",
      "Weihong Ren",
      "Zhiyong Wang*",
      "Honghai Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7149_ECCV_2024_paper.php": {
    "title": "Leveraging scale- and orientation-covariant features for planar motion estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marcus Valtonen Örnhag*",
      "Alberto Jaenal"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7150_ECCV_2024_paper.php": {
    "title": "Understanding and Mitigating Human-Labelling Errors in Supervised Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijun Long*",
      "Lipeng Zhuang",
      "George W Killick",
      "Richard Mccreadie",
      "Gerardo Aragon-Camarasa",
      "Paul Henderson"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7153_ECCV_2024_paper.php": {
    "title": "Adaptive Parametric Activation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Konstantinos P Alexandridis*",
      "Jiankang Deng",
      "Anh Nguyen",
      "Shan Luo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7158_ECCV_2024_paper.php": {
    "title": "Distractor-Free Novel View Synthesis via Exploiting Memorization Effect in Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yukun Wang*",
      "Kunhong Li",
      "Minglin Chen",
      "Longguang Wang",
      "Shunbo Zhou",
      "Kaiwen Xue",
      "Yulan Guo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7159_ECCV_2024_paper.php": {
    "title": "VEGS: View Extrapolation of Urban Scenes in 3D Gaussian Splatting using Learned Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sungwon Hwang",
      "Min-Jung Kim",
      "Taewoong Kang",
      "Jayeon Kang",
      "Jaegul Choo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7166_ECCV_2024_paper.php": {
    "title": "HGL: Hierarchical Geometry Learning for Test-time Adaptation in 3D Point Cloud Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianpei Zou",
      "Sanqing Qu",
      "Zhijun Li",
      "Alois C. Knoll",
      "何 良华",
      "Guang Chen*",
      "Changjun Jiang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7170_ECCV_2024_paper.php": {
    "title": "SWinGS: Sliding Windows for Dynamic 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Richard Shaw*",
      "Michal Nazarczuk",
      "Jifei Song",
      "Arthur Moreau",
      "Sibi Catley-Chandar",
      "Helisa Dhamo",
      "Eduardo Pérez Pellitero"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7171_ECCV_2024_paper.php": {
    "title": "Temporal-Mapping Photography for Event Cameras",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhan Bao",
      "Lei Sun*",
      "Yuqin Ma",
      "Kaiwei Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7173_ECCV_2024_paper.php": {
    "title": "Shape2Scene: 3D Scene Representation Learning Through Pre-training on Shape Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tuo Feng",
      "Wenguan Wang",
      "Ruijie Quan",
      "Yi Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7181_ECCV_2024_paper.php": {
    "title": "LineFit: A Geometric Approach for Fitting Line Segments in Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marion Boyer",
      "David Youssefi",
      "Florent Lafarge*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7183_ECCV_2024_paper.php": {
    "title": "Six-Point Method for Multi-Camera Systems with Reduced Solution Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Banglei Guan",
      "Ji Zhao*",
      "Laurent Kneip"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7189_ECCV_2024_paper.php": {
    "title": "Mew: Multiplexed Immunofluorescence Image Analysis through an Efficient Multiplex Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sukwon Yun",
      "Jie Peng",
      "Alexandro E Trevino",
      "Chanyoung Park",
      "Tianlong Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7192_ECCV_2024_paper.php": {
    "title": "Champ: Controllable and Consistent Human Image Animation with 3D Parametric Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shenhao Zhu",
      "Junming Leo Chen",
      "Zuozhuo Dai",
      "Zilong Dong",
      "Yinghui Xu",
      "Xun Cao",
      "Yao Yao",
      "Hao Zhu*",
      "Siyu Zhu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7206_ECCV_2024_paper.php": {
    "title": "AdaDistill: Adaptive Knowledge Distillation for Deep Face Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fadi Boutros*",
      "Vitomir Struc",
      "Naser Damer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7209_ECCV_2024_paper.php": {
    "title": "HERGen: Elevating Radiology Report Generation with Longitudinal Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fuying Wang",
      "Shenghui Du",
      "Lequan Yu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7212_ECCV_2024_paper.php": {
    "title": "Labeled Data Selection for Category Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bingchen Zhao*",
      "Nico Lang",
      "Serge Belongie",
      "Oisin Mac Aodha*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7216_ECCV_2024_paper.php": {
    "title": "Dependency-aware Differentiable Neural Architecture Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Buang Zhang*",
      "Xinle Wu",
      "Hao Miao",
      "Bin Yang",
      "Chenjuan Guo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7220_ECCV_2024_paper.php": {
    "title": "WAS: Dataset and Methods for Artistic Text Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xudong Xie",
      "Yuzhe Li",
      "Yang Liu",
      "Zhifei Zhang",
      "Zhaowen Wang",
      "Wei Xiong",
      "Xiang Bai*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7221_ECCV_2024_paper.php": {
    "title": "CLIFF: Continual Latent Diffusion for Open-Vocabulary Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wuyang Li",
      "Xinyu Liu",
      "Jiayi Ma",
      "Yixuan Yuan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7223_ECCV_2024_paper.php": {
    "title": "GMT: Enhancing Generalizable Neural Rendering via Geometry-Driven Multi-Reference Texture Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youngho Yoon",
      "Hyun-Kurl Jang",
      "Kuk-Jin Yoon*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7224_ECCV_2024_paper.php": {
    "title": "Norface: Improving Facial Expression Analysis by Identity Normalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanwei Liu*",
      "Rudong An",
      "Zhimeng Zhang",
      "Bowen Ma",
      "Wei Zhang",
      "Yan Song",
      "Yujing Hu",
      "Chen Wei",
      "Yu Ding*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7226_ECCV_2024_paper.php": {
    "title": "Unlocking Attributes' Contribution to Successful Camouflage: A Combined Textual and Visual Analysis Strategy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hong Zhang",
      "Yixuan Lyu",
      "Qian Yu",
      "Hanyang Liu",
      "Huimin Ma",
      "Yuan Ding",
      "Yifan Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7231_ECCV_2024_paper.php": {
    "title": "SNeRV: Spectra-preserving Neural Representation for Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jina Kim*",
      "Jihoo Lee*",
      "Jewon Kang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7232_ECCV_2024_paper.php": {
    "title": "COMO: Compact Mapping and Odometry",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eric Dexheimer*",
      "Andrew Davison"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7236_ECCV_2024_paper.php": {
    "title": "OAT: Object-Level Attention Transformer for Gaze Scanpath Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yini Fang*",
      "Jingling Yu",
      "Haozheng Zhang",
      "Ralf van der Lans",
      "Bertram E Shi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7239_ECCV_2024_paper.php": {
    "title": "SelfSwapper: Self-Supervised Face Swapping via Shape Agnostic Masked AutoEncoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaeseong Lee*",
      "Junha Hyung*",
      "Sohyun Jeong",
      "Jaegul Choo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7241_ECCV_2024_paper.php": {
    "title": "EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenhongyi Yang*",
      "Anastasia Tkach",
      "Shreyas Hampali",
      "Linguang Zhang",
      "Elliot J Crowley",
      "Cem Keskin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7242_ECCV_2024_paper.php": {
    "title": "An Information Theoretical View for Out-Of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hu Jinjing",
      "Wenrui Liu",
      "Hong Chang*",
      "Bingpeng MA",
      "Shiguang Shan",
      "Xilin Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7243_ECCV_2024_paper.php": {
    "title": "DMiT: Deformable Mipmapped Tri-Plane Representation for Dynamic Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing-Wen Yang",
      "Jia-Mu Sun",
      "Yong-Liang Yang",
      "Jie Yang",
      "Ying Shan",
      "Yan-Pei Cao",
      "Lin Gao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7244_ECCV_2024_paper.php": {
    "title": "Gated Temporal Diffusion for Stochastic Long-term Dense Anticipation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Olga Zatsarynna*",
      "Emad Bahrami*",
      "Yazan Abu Farha",
      "Gianpiero Francesca",
      "Jürgen Gall*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7246_ECCV_2024_paper.php": {
    "title": "Gradient-Aware for Class-Imbalanced Semi-supervised Medical Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenbo Qi",
      "Jiafei Wu*",
      "S. C. Chan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7249_ECCV_2024_paper.php": {
    "title": "HowToCaption: Prompting LLMs to Transform Video Annotations at Scale",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nina Shvetsova*",
      "Anna Kukleva",
      "Xudong Hong",
      "Christian Rupprecht",
      "Bernt Schiele",
      "Hilde Kuehne"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7250_ECCV_2024_paper.php": {
    "title": "LabelDistill: Label-guided Cross-modal Knowledge Distillation for Camera-based 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanmin Kim",
      "Youngseok Kim",
      "Sihwan Hwang",
      "Hyeonjun Jeong",
      "Dongsuk Kum*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7256_ECCV_2024_paper.php": {
    "title": "Beyond the Data Imbalance: Employing the Heterogeneous Datasets for Vehicle Maneuver Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyeongseok Jeon",
      "Sanmin Kim",
      "Abi Rahman Syamil",
      "Junsoo Kim",
      "Dongsuk Kum*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7257_ECCV_2024_paper.php": {
    "title": "On Pretraining Data Diversity for Self-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hasan Abed Al Kader Hammoud*",
      "Tuhin Das",
      "Fabio Pizzati*",
      "Philip Torr",
      "Adel Bibi",
      "Bernard Ghanem"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7259_ECCV_2024_paper.php": {
    "title": "Look Around and Learn: Self-Training Object Detection by Exploration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gianluca Scarpellini*",
      "Stefano Rosa*",
      "Pietro Morerio",
      "Lorenzo Natale",
      "Alessio Del Bue"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7260_ECCV_2024_paper.php": {
    "title": "Bayesian Self-Training for Semi-Supervised 3D Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ozan Unal*",
      "Christos Sakaridis",
      "Luc Van Gool"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7261_ECCV_2024_paper.php": {
    "title": "Motion and Structure from Event-based Normal Flow",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongyang Ren",
      "Bangyan Liao",
      "Delei Kong",
      "Jinghang Li",
      "Peidong Liu",
      "Laurent Kneip",
      "Guillermo Gallego",
      "Yi Zhou*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7265_ECCV_2024_paper.php": {
    "title": "ParCo: Part-Coordinating Text-to-Motion Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiran Zou",
      "Shangyuan Yuan",
      "Shian Du",
      "Yu Wang",
      "Chang Liu",
      "Yi Xu",
      "Jie Chen",
      "Xiangyang Ji*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7269_ECCV_2024_paper.php": {
    "title": "Learning to Complement and to Defer to Multiple Users",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheng Zhang",
      "Wenjie Ai",
      "Kevin Wells",
      "David M Rosewarne",
      "Thanh-Toan Do",
      "Gustavo Carneiro*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7282_ECCV_2024_paper.php": {
    "title": "Tiny Models are the Computational Saver for Large Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingyuan Wang*",
      "Barry Cardiff",
      "Antoine Frappé",
      "Benoit Larras",
      "Deepu John*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7284_ECCV_2024_paper.php": {
    "title": "DragVideo: Interactive Drag-style Video Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufan Deng",
      "Ruida WANG",
      "Yuhao ZHANG",
      "Yu-Wing Tai*",
      "Chi-Keung Tang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7285_ECCV_2024_paper.php": {
    "title": "Multi-Sentence Grounding for Long-term Instructional Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeqian Li",
      "Qirui Chen",
      "Tengda Han",
      "Ya Zhang",
      "Yan-Feng Wang",
      "Weidi Xie*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7293_ECCV_2024_paper.php": {
    "title": "Do Generalised Classifiers really work on Human Drawn Sketches?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hmrishav Bandyopadhyay*",
      "Pinaki Nath Chowdhury",
      "Aneeshan Sain",
      "Subhadeep Koley",
      "Tao Xiang",
      "Ayan Kumar Bhunia",
      "Yi-Zhe Song"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7316_ECCV_2024_paper.php": {
    "title": "KMTalk: Speech-Driven 3D Facial Animation with Key Motion Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihao Xu",
      "Shengjie Gong",
      "Jiapeng Tang",
      "Lingyu Liang",
      "Yining Huang",
      "Haojie Li",
      "Shuangping Huang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7318_ECCV_2024_paper.php": {
    "title": "Head360: Learning a Parametric 3D Full-Head for Free-View Synthesis in 360°",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxiao He",
      "Yiyu Zhuang",
      "Yanwen Wang",
      "Yao Yao",
      "Siyu Zhu",
      "Xiaoyu Li",
      "Qi Zhang",
      "Xun Cao",
      "Hao Zhu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7327_ECCV_2024_paper.php": {
    "title": "MotionDirector: Motion Customization of Text-to-Video Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Zhao",
      "Yuchao Gu",
      "Jay Zhangjie Wu",
      "David Junhao Zhang",
      "Jia-Wei Liu",
      "weijia wu",
      "Jussi Keppo",
      "Mike Zheng Shou*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7328_ECCV_2024_paper.php": {
    "title": "Text2LiDAR: Text-guided LiDAR Point Clouds Generation via Equirectangular Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Wu*",
      "Kaihua Zhang",
      "Jianjun Qian",
      "Jin Xie*",
      "Jian Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7336_ECCV_2024_paper.php": {
    "title": "Enhanced Motion Forecasting with Visual Relation Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sungjune Kim",
      "Hadam Baek",
      "Seunggwan Lee",
      "Hyung-gun Chi",
      "Hyerin Lim",
      "Jinkyu Kim*",
      "Sangpil Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7338_ECCV_2024_paper.php": {
    "title": "Rate-Distortion-Cognition Controllable Versatile Neural Image Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinming Liu*",
      "Ruoyu Feng",
      "Yunpeng Qi",
      "Qiuyu Chen",
      "Zhibo Chen",
      "Wenjun Zeng",
      "Xin Jin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7344_ECCV_2024_paper.php": {
    "title": "Temporal As a Plugin: Unsupervised Video Denoising with Pre-Trained Image Denoisers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixuan Fu*",
      "Lanqing Guo",
      "Chong Wang",
      "Yufei Wang",
      "Zhihao Li",
      "Bihan Wen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7349_ECCV_2024_paper.php": {
    "title": "LiDAR-based All-weather 3D Object Detection via Prompting and Distilling 4D Radar",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujeong Chae",
      "Hyeonseong Kim",
      "Changgyoon Oh",
      "Minseok Kim",
      "Kuk-Jin Yoon*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7350_ECCV_2024_paper.php": {
    "title": "MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Liu*",
      "Yichen Zhu",
      "Jindong Gu",
      "Yunshi Lan",
      "Chao Yang",
      "Yu Qiao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7353_ECCV_2024_paper.php": {
    "title": "Post-training Quantization with Progressive Calibration and Activation Relaxing for Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siao Tang",
      "Xin Wang*",
      "Hong Chen",
      "Chaoyu Guan",
      "Zewen Wu",
      "Yansong Tang",
      "Wenwu Zhu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7356_ECCV_2024_paper.php": {
    "title": "Scene Coordinate Reconstruction: Posing of Image Collections via Incremental Learning of a Relocalizer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eric Brachmann*",
      "Jamie Wynn",
      "Shuai Chen",
      "Tommaso Cavallari",
      "Aron Monszpart",
      "Daniyar Turmukhambetov",
      "Victor Adrian Prisacariu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7357_ECCV_2024_paper.php": {
    "title": "Diffusion Models are Geometry Critics: Single Image 3D Editing Using Pre-Trained Diffusion Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruicheng Wang*",
      "Jianfeng Xiang",
      "Jiaolong Yang",
      "Xin Tong"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7359_ECCV_2024_paper.php": {
    "title": "Weakly Supervised Co-training with Swapping Assignments for Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyu Yang*",
      "Hossein Rahmani",
      "Dame S Black",
      "Bryan M Williams"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7360_ECCV_2024_paper.php": {
    "title": "StoryImager: A Unified and Efficient Framework for Coherent Story Visualization and Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ming Tao*",
      "Bingkun Bao*",
      "Hao Tang",
      "Yaowei Wang",
      "Changsheng Xu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7364_ECCV_2024_paper.php": {
    "title": "ST-LLM: Large Language Models Are Effective Temporal Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruyang Liu",
      "Chen Li",
      "Haoran Tang",
      "Yixiao Ge",
      "Ying Shan",
      "Ge Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7377_ECCV_2024_paper.php": {
    "title": "Exact Diffusion Inversion via Bidirectional Integration Approximation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guoqiang Zhang*",
      "j.p. lewis",
      "W. Bastiaan Kleijn"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7384_ECCV_2024_paper.php": {
    "title": "Textual Query-Driven Mask Transformer for Domain Generalized Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Byeonghyun Pak",
      "Byeongju Woo",
      "Sunghwan Kim",
      "Dae-hwan Kim",
      "Hoseong Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7387_ECCV_2024_paper.php": {
    "title": "EmoTalk3D: High-Fidelity Free-View Synthesis of Emotional 3D Talking Head",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qianyun He",
      "Xinya Ji",
      "Yicheng Gong",
      "Yuanxun Lu",
      "Zhengyu Diao",
      "Linjia Huang",
      "Yao Yao",
      "Siyu Zhu",
      "Zhan Ma",
      "Songcen Xu",
      "Xiaofei Wu",
      "Zixiao Zhang",
      "Xun Cao",
      "Hao Zhu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7392_ECCV_2024_paper.php": {
    "title": "Arbitrary-Scale Video Super-Resolution with Structural and Textural Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Shang*",
      "Dongwei Ren*",
      "Wanying Zhang",
      "Yuming Fang",
      "Wangmeng Zuo",
      "Kede Ma"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7396_ECCV_2024_paper.php": {
    "title": "Object-Centric Diffusion for Efficient Video Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kumara Kahatapitiya*",
      "Adil Karjauv",
      "Davide Abati*",
      "Fatih Porikli",
      "Yuki M Asano",
      "Amirhossein Habibian"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7404_ECCV_2024_paper.php": {
    "title": "Single-Mask Inpainting for Voxel-based Neural Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiafu Chen*",
      "Tianyi Chu",
      "Jiakai Sun",
      "Wei Xing",
      "Lei Zhao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7408_ECCV_2024_paper.php": {
    "title": "McGrids: Monte Carlo-Driven Adaptive Grids for Iso-Surface Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daxuan Ren*",
      "Hezi Shi",
      "Jianmin Zheng",
      "Jianfei Cai"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7423_ECCV_2024_paper.php": {
    "title": "Freeview Sketching: View-Aware Fine-Grained Sketch-Based Image Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aneeshan Sain*",
      "Pinaki Nath Chowdhury",
      "Subhadeep Koley",
      "Ayan Kumar Bhunia",
      "Yi-Zhe Song"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7430_ECCV_2024_paper.php": {
    "title": "Adapt2Reward: Adapting Video-Language Models to Generalizable Robotic Rewards via Failure Prompts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanting Yang",
      "Minghao Chen*",
      "Qibo Qiu",
      "Jiahao WU",
      "Wenxiao Wang",
      "Binbin Lin",
      "Ziyu Guan",
      "Xiaofei He"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7436_ECCV_2024_paper.php": {
    "title": "Diffusion for Natural Image Matting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihan Hu*",
      "Yiheng Lin",
      "Wei Wang",
      "Yao Zhao",
      "Yunchao Wei*",
      "Humphrey Shi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7441_ECCV_2024_paper.php": {
    "title": "Agglomerative Token Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joakim Bruslund Haurum*",
      "Sergio Escalera",
      "Graham W. Taylor*",
      "Thomas B. Moeslund"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7443_ECCV_2024_paper.php": {
    "title": "CMD: A Cross Mechanism Domain Adaptation Dataset for 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinhao Deng",
      "Wei Ye",
      "Hai Wu",
      "Qiming Xia",
      "Xun Huang",
      "Xin Li",
      "Jin Fang",
      "Wei Li*",
      "Chenglu Wen*",
      "Cheng Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7445_ECCV_2024_paper.php": {
    "title": "Unleashing Text-to-Image Diffusion Prior for Zero-Shot Image Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianjie Luo",
      "Jingwen Chen",
      "Yehao Li",
      "Yingwei Pan*",
      "Jianlin Feng",
      "Hongyang Chao",
      "Ting Yao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7447_ECCV_2024_paper.php": {
    "title": "ClusteringSDF: Self-Organized Neural Implicit Surfaces for 3D Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianhao Wu*",
      "Chuanxia Zheng",
      "Qianyi Wu",
      "Tat-Jen Cham"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7448_ECCV_2024_paper.php": {
    "title": "NAMER: Non-Autoregressive Modeling for Handwritten Mathematical Expression Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenyu Liu",
      "Jia Pan",
      "Jinshui Hu",
      "Baocai Yin",
      "Bing Yin",
      "Mingjun Chen",
      "Cong Liu",
      "Jun Du*",
      "Qingfeng Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7451_ECCV_2024_paper.php": {
    "title": "GIVT: Generative Infinite-Vocabulary Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Tschannen*",
      "Cian Eastwood",
      "Fabian Mentzer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7453_ECCV_2024_paper.php": {
    "title": "Mismatch Quest: Visual and Textual Feedback for Image-Text Misalignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brian Gordon*",
      "Yonatan Bitton*",
      "Yonatan Shafir",
      "Roopal Garg",
      "Xi Chen",
      "Dani Lischinski",
      "Daniel Cohen-Or",
      "Idan Szpektor"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7456_ECCV_2024_paper.php": {
    "title": "Regulating Model Reliance on Non-Robust Features by Smoothing Input Marginal Density",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peiyu Yang*",
      "Naveed Akhtar",
      "Mubarak Shah",
      "Ajmal Mian"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7457_ECCV_2024_paper.php": {
    "title": "Multi-Modal Video Dialog State Tracking in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adnen Abdessaied*",
      "Lei Shi",
      "Andreas Bulling"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7469_ECCV_2024_paper.php": {
    "title": "Factorized Diffusion: Perceptual Illusions by Noise Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Geng*",
      "Inbum Park",
      "Andrew Owens"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7479_ECCV_2024_paper.php": {
    "title": "To Generate or Not? Safety-Driven Unlearned Diffusion Models Are Still Easy To Generate Unsafe Images ... For Now",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yimeng Zhang*",
      "jinghan jia",
      "Xin Chen",
      "Aochuan Chen",
      "Yihua Zhang",
      "Jiancheng Liu",
      "Ke Ding",
      "Sijia Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7483_ECCV_2024_paper.php": {
    "title": "Dissecting Dissonance: Benchmarking Large Multimodal Models Against Self-Contradictory Instructions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jin Gao",
      "Lei Gan",
      "Yuankai Li",
      "Yixin Ye",
      "Dequan Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7485_ECCV_2024_paper.php": {
    "title": "StereoGlue: Joint Feature Matching and Robust Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Barath*",
      "Dmytro Mishkin",
      "Luca Cavalli",
      "Paul-Edouard Sarlin",
      "Petr Hruby",
      "Marc Pollefeys"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7488_ECCV_2024_paper.php": {
    "title": "Boosting Transferability in Vision-Language Attacks via Diversification along the Intersection Region of Adversarial Trajectory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sensen Gao",
      "Xiaojun Jia*",
      "Xuhong Ren",
      "Ivor Tsang",
      "Qing Guo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7492_ECCV_2024_paper.php": {
    "title": "Leveraging Enhanced Queries of Point Sets for Vectorized Map Construction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihao Liu",
      "Xiaoyu Zhang",
      "Guangwei Liu",
      "Ji Zhao*",
      "Ningyi Xu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7495_ECCV_2024_paper.php": {
    "title": "Robust Zero-Shot Crowd Counting and Localization with Adaptive Resolution SAM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jia Wan*",
      "Qiangqiang Wu",
      "Wei Lin",
      "Antoni Chan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7496_ECCV_2024_paper.php": {
    "title": "AWOL: Analysis WithOut synthesis using Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Silvia Zuffi*",
      "Michael J. Black"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7505_ECCV_2024_paper.php": {
    "title": "OneVOS: Unifying Video Object Segmentation with All-in-One Transformer Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wanyun Li",
      "Pinxue Guo",
      "Xinyu Zhou",
      "Lingyi Hong",
      "Yangji He",
      "Xiangyu Zheng",
      "Wei Zhang*",
      "Wenqiang Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7509_ECCV_2024_paper.php": {
    "title": "M3DBench: Towards Omni 3D Assistant with Interleaved Multi-modal Instructions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingsheng Li",
      "Xin Chen",
      "Chi Zhang",
      "Sijin Chen",
      "Hongyuan Zhu",
      "Fukun Yin",
      "Zhuoyuan Li",
      "Gang Yu",
      "Tao Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7514_ECCV_2024_paper.php": {
    "title": "MSD: A Benchmark Dataset for Floor Plan Generation of Building Complexes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Casper van Engelenburg*",
      "Fatemeh Mostafavi",
      "Emanuel Kuhn",
      "Yuntae Jeon",
      "Michael Franzen",
      "Matthias Standfest",
      "Jan van Gemert",
      "Seyran Khademi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7526_ECCV_2024_paper.php": {
    "title": "End-to-End Rate-Distortion Optimized 3D Gaussian Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Henan Wang*",
      "Hanxin Zhu",
      "Tianyu He",
      "Runsen Feng",
      "Jiajun Deng",
      "Jiang Bian",
      "Zhibo Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7531_ECCV_2024_paper.php": {
    "title": "Temporal Residual Jacobians for Rig-free Motion Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanjeev Muralikrishnan*",
      "Niladri Shekhar Dutt",
      "Siddhartha Chaudhuri",
      "Noam Aigerman",
      "Vladimir Kim",
      "Matthew Fisher",
      "Niloy Mitra"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7537_ECCV_2024_paper.php": {
    "title": "LetsMap: Unsupervised Representation Learning for Label-Efficient Semantic BEV Mapping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikhil Gosala*",
      "Kürsat Petek",
      "B Ravi Kiran",
      "Senthil Yogamani",
      "Paulo L. J. Drews-Jr",
      "Wolfram Burgard",
      "Abhinav Valada"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7539_ECCV_2024_paper.php": {
    "title": "Deblurring 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Byeonghyeon Lee*",
      "Howoong Lee",
      "Xiangyu Sun",
      "Usman Ali",
      "Eunbyung Park*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7541_ECCV_2024_paper.php": {
    "title": "Taming Lookup Tables for Efficient Image Retouching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sidi Yang",
      "Binxiao Huang",
      "Mingdeng Cao",
      "Yatai Ji",
      "Hanzhong Guo",
      "Ngai Wong",
      "Yujiu Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7547_ECCV_2024_paper.php": {
    "title": "DualDn: Dual-domain Denoising via Differentiable ISP",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruikang Li",
      "Yujin Wang*",
      "Shiqi Chen",
      "Fan Zhang",
      "Jinwei Gu",
      "Tianfan Xue"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7548_ECCV_2024_paper.php": {
    "title": "Quantization-Friendly Winograd Transformations for Convolutional Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vladimir Protsenko*",
      "Vladimir Kryzhanovskiy",
      "Alexander Filippov"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7554_ECCV_2024_paper.php": {
    "title": "A Task is Worth One Word: Learning with Task Prompts for High-Quality Versatile Image Inpainting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junhao Zhuang",
      "Yanhong Zeng",
      "WENRAN LIU",
      "Chun Yuan*",
      "Kai Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7555_ECCV_2024_paper.php": {
    "title": "Self-supervised Shape Completion via Involution and Implicit Correspondences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengya Liu*",
      "Ajad Chhatkuli",
      "Janis Postels",
      "Luc Van Gool",
      "Federico Tombari"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7557_ECCV_2024_paper.php": {
    "title": "From Fake to Real: Pretraining on Balanced Synthetic Images to Prevent Spurious Correlations in Image Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maan Qraitem*",
      "Kate Saenko",
      "Bryan A. Plummer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7562_ECCV_2024_paper.php": {
    "title": "Cross-Domain Few-Shot Object Detection via Enhanced Open-Set Object Detector",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuqian Fu*",
      "Yu Wang",
      "Yixuan Pan",
      "Xingyu Qiu",
      "Lian Huai",
      "Zeyu Shangguan",
      "Tong Liu",
      "Yanwei Fu",
      "Luc Van Gool",
      "Xingqun Jiang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7564_ECCV_2024_paper.php": {
    "title": "NICP: Neural ICP for 3D Human Registration at Scale",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Riccardo Marin*",
      "Enric Corona",
      "Gerard Pons-Moll"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7567_ECCV_2024_paper.php": {
    "title": "PredBench: Benchmarking Spatio-Temporal Prediction across Diverse Disciplines",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "ZiDong Wang*",
      "Zeyu Lu*",
      "Di Huang*",
      "Tong He",
      "Xihui Liu",
      "Wanli Ouyang",
      "Lei Bai*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7569_ECCV_2024_paper.php": {
    "title": "FontStudio: Shape-Adaptive Diffusion Model for Coherent and Consistent Font Effect Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinzhi Mu*",
      "Li Chen",
      "Bohan CHEN",
      "Shuyang Gu",
      "Jianmin Bao",
      "Dong Chen",
      "Ji Li",
      "Yuhui Yuan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7570_ECCV_2024_paper.php": {
    "title": "Chronologically Accurate Retrieval for Temporal Grounding of Motion-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kent Fujiwara*",
      "Mikihiro Tanaka",
      "Qing Yu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7572_ECCV_2024_paper.php": {
    "title": "StableDrag: Stable Dragging for Point-based Image Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yutao Cui",
      "Xiaotong Zhao",
      "Guozhen Zhang",
      "Shengming Cao",
      "Kai Ma",
      "Limin Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7582_ECCV_2024_paper.php": {
    "title": "Improving Feature Stability during Upsampling -- Spectral Artifacts and the Importance of Spatial Context",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shashank Agnihotri*",
      "Julia Grabinski",
      "Margret Keuper"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7587_ECCV_2024_paper.php": {
    "title": "Dynamic Data Selection for Efficient SSL via Coarse-to-Fine Refinement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditay Tripathi*",
      "Pradeep Shenoy",
      "Anirban Chakraborty"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7593_ECCV_2024_paper.php": {
    "title": "Neural Surface Detection for Unsigned Distance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Federico Stella*",
      "Nicolas Talabot",
      "Hieu Le",
      "Pascal Fua"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7598_ECCV_2024_paper.php": {
    "title": "One-Shot Diffusion Mimicker for Handwritten Text Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gang Dai",
      "Yifan Zhang",
      "Quhui Ke",
      "Qiangya Guo",
      "Shuangping Huang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7601_ECCV_2024_paper.php": {
    "title": "Event-Based Motion Magnification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yutian Chen",
      "Shi Guo*",
      "Yu Fangzheng",
      "Feng Zhang",
      "Jinwei Gu",
      "Tianfan Xue"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7603_ECCV_2024_paper.php": {
    "title": "Improving Neural Surface Reconstruction with Feature Priors from Multi-View Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinlin Ren*",
      "Chenjie Cao",
      "Yanwei Fu*",
      "Xiangyang Xue"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7606_ECCV_2024_paper.php": {
    "title": "Towards Multimodal Sentiment Analysis Debiasing via Bias Purification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dingkang Yang",
      "Mingcheng Li",
      "Dongling Xiao",
      "Yang Liu",
      "Kun Yang",
      "Zhaoyu Chen",
      "Yuzheng Wang",
      "Peng Zhai*",
      "Ke Li",
      "Lihua Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7611_ECCV_2024_paper.php": {
    "title": "Kernel Diffusion: An Alternate Approach to Blind Deconvolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yash Sanghvi*",
      "Yiheng Chi",
      "Stanley Chan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7626_ECCV_2024_paper.php": {
    "title": "MUSES: The Multi-Sensor Semantic Perception Dataset for Driving under Uncertainty",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tim Broedermann*",
      "David Brüggemann",
      "Christos Sakaridis",
      "Kevin Ta",
      "Odysseas Liagouris",
      "Jason Corkill",
      "Luc Van Gool"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7629_ECCV_2024_paper.php": {
    "title": "Discovering Novel Actions from Open World Egocentric Videos with Object-Grounded Visual Commonsense Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanjoy Kundu",
      "Shubham Trehan",
      "Sathyanarayanan N Aakur*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7631_ECCV_2024_paper.php": {
    "title": "Bidirectional Progressive Transformer for Interaction Intention Anticipation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zichen Zhang*",
      "Hongchen Luo",
      "Wei Zhai*",
      "Yu Kang",
      "Yang Cao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7633_ECCV_2024_paper.php": {
    "title": "Reinforcement Learning Meets Visual Odometry",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nico Messikommer*",
      "Giovanni Cioffi",
      "Mathias Gehrig",
      "Davide Scaramuzza"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7634_ECCV_2024_paper.php": {
    "title": "Bucketed Ranking-based Losses for Efficient Training of Object Detectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feyza Yavuz*",
      "Baris Can Cam",
      "Adnan Harun Dogan",
      "Kemal Oksuz",
      "Emre Akbas",
      "Sinan Kalkan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7642_ECCV_2024_paper.php": {
    "title": "Robustness Tokens: Towards Adversarial Robustness of Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brian Pulfer*",
      "Yury Belousov",
      "Slava Voloshynovskiy"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7643_ECCV_2024_paper.php": {
    "title": "RSL-BA: Rolling Shutter Line Bundle Adjustment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongcong Zhang",
      "Bangyan Liao",
      "Yifei Xue",
      "Lu Chen",
      "Peidong Liu",
      "Yizhen Lao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7644_ECCV_2024_paper.php": {
    "title": "DecentNeRFs: Decentralized Neural Radiance Fields from Crowdsourced Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zaid Tasneem*",
      "Akshat Dave",
      "Abhishek Singh",
      "Kushagra Tiwary",
      "Praneeth Vepakomma",
      "Ashok Veeraraghavan",
      "Ramesh Raskar"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7646_ECCV_2024_paper.php": {
    "title": "DreamMesh: Jointly Manipulating and Texturing Triangle Meshes for Text-to-3D Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haibo Yang",
      "Yang Chen",
      "Yingwei Pan*",
      "Ting Yao",
      "Zhineng Chen",
      "Zuxuan Wu",
      "Yu-Gang Jiang",
      "Tao Mei"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7650_ECCV_2024_paper.php": {
    "title": "Unveiling Typographic Deceptions: Insights of the Typographic Vulnerability in Large Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Cheng",
      "Erjia Xiao",
      "Jindong Gu",
      "Le Yang",
      "Jinhao Duan",
      "Jize Zhang",
      "Jiahang Cao",
      "Kaidi Xu",
      "Renjing Xu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7652_ECCV_2024_paper.php": {
    "title": "N2F2: Hierarchical Scene Understanding with Nested Neural Feature Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yash Bhalgat*",
      "Iro Laina",
      "Joao F Henriques",
      "Andrew Zisserman",
      "Andrea Vedaldi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7654_ECCV_2024_paper.php": {
    "title": "ConceptExpress: Harnessing Diffusion Models for Single-image Unsupervised Concept Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaozhe Hao*",
      "Kai Han*",
      "Zhengyao Lv",
      "Shihao Zhao",
      "Kwan-Yee K. Wong*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7655_ECCV_2024_paper.php": {
    "title": "PairingNet: A Learning-based Pair-searching and -matching Network for Image Fragments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rixin Zhou*",
      "Ding Xia",
      "YI ZHANG",
      "honglin pang",
      "Xi Yang",
      "chuntao li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7657_ECCV_2024_paper.php": {
    "title": "Skeleton-based Group Activity Recognition via Spatial-Temporal Panoramic Graph",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengcen Li",
      "Xinle Chang",
      "Yueran Li",
      "Jingyong Su*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7658_ECCV_2024_paper.php": {
    "title": "Towards Multimodal Open-Set Domain Generalization and Adaptation through Self-supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Dong*",
      "Eleni Chatzi*",
      "Olga Fink*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7666_ECCV_2024_paper.php": {
    "title": "ReCON: Training-Free Acceleration for Text-to-Image Synthesis with Retrieval of Concept Prompt Trajectories",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen-Yi Lu*",
      "Shubham Agarwal",
      "Md Mehrab Tanjim",
      "Kanak Mahadik",
      "Anup Rao",
      "Subrata Mitra",
      "Shiv K Saini",
      "Saurabh Bagchi",
      "Somali Chaterji"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7670_ECCV_2024_paper.php": {
    "title": "AMES: Asymmetric and Memory-Efficient Similarity Estimation for Instance-level Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pavel Suma*",
      "Giorgos Kordopatis-Zilos",
      "Ahmet Iscen",
      "Giorgos Tolias"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7673_ECCV_2024_paper.php": {
    "title": "TCAN: Animating Human Images with Temporally Consistent Pose Guidance using Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeongho Kim*",
      "Min-Jung Kim*",
      "Junsoo Lee",
      "Jaegul Choo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7674_ECCV_2024_paper.php": {
    "title": "3D Hand Sequence Recovery from Real Blurry Images and Event Stream",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "JoonKyu Park",
      "Gyeongsik Moon",
      "Weipeng Xu",
      "Evan Kaseman",
      "Takaaki Shiratori",
      "Kyoung Mu Lee*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7675_ECCV_2024_paper.php": {
    "title": "GlobalPointer: Large-Scale Plane Adjustment with Bi-Convex Relaxation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bangyan Liao",
      "Zhenjun Zhao",
      "Lu Chen",
      "Haoang Li",
      "Daniel Cremers",
      "Peidong Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7681_ECCV_2024_paper.php": {
    "title": "Dissolving Is Amplifying: Towards Fine-Grained Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Shi*",
      "Pengyi Zhang",
      "Ni Zhang",
      "Hakim Ghazzai",
      "Peter Wonka"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7682_ECCV_2024_paper.php": {
    "title": "StyleCity: Large-Scale 3D Urban Scenes Stylization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingshu Chen",
      "Huajian Huang*",
      "Tuan-Anh Vu",
      "Ka Chun Shum",
      "Sai-Kit Yeung"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7685_ECCV_2024_paper.php": {
    "title": "ViG-Bias: Visually Grounded Bias Discovery and Mitigation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Badr-Eddine Marani*",
      "Mohamed Hanini",
      "Nihitha Malayarukil",
      "Stergios Christodoulidis",
      "Maria Vakalopoulou",
      "Enzo Ferrante"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7690_ECCV_2024_paper.php": {
    "title": "DiffBIR: Toward Blind Image Restoration with Generative Diffusion Prior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinqi Lin*",
      "Jingwen He",
      "Ziyan Chen",
      "Zhaoyang Lyu",
      "Bo Dai",
      "Fanghua Yu",
      "Yu Qiao",
      "Wanli Ouyang",
      "Chao Dong*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7691_ECCV_2024_paper.php": {
    "title": "Assessing Sample Quality via the Latent Space of Generative Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyi Xu*",
      "Hieu Le",
      "Dimitris Samaras"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7695_ECCV_2024_paper.php": {
    "title": "Relightable Neural Actor with Intrinsic Decomposition and Pose Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Diogo Carbonera Luvizon*",
      "Vladislav Golyanik",
      "Adam Kortylewski",
      "Marc Habermann",
      "Christian Theobalt"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7696_ECCV_2024_paper.php": {
    "title": "Sur^2f: A Hybrid Representation for High-Quality and Efficient Surface Reconstruction from Multi-view Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhangjin Huang*",
      "Zhihao Liang",
      "Kui Jia*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7707_ECCV_2024_paper.php": {
    "title": "HO-Gaussian: Hybrid Optimization of 3D Gaussian Splatting for Urban Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuopeng Li*",
      "Yilin Zhang",
      "Chenming Wu",
      "Jianke Zhu*",
      "Liangjun Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7708_ECCV_2024_paper.php": {
    "title": "Pseudo-keypoint RKHS Learning for Self-supervised 6DoF Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yangzheng Wu*",
      "Michael Alan Greenspan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7710_ECCV_2024_paper.php": {
    "title": "Consistent 3D Line Mapping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xulong Bai",
      "Hainan Cui*",
      "Shuhan Shen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7717_ECCV_2024_paper.php": {
    "title": "Distributed Active Client Selection With Noisy Clients Using Model Association Scores",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kwang In Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7720_ECCV_2024_paper.php": {
    "title": "PixOOD: Pixel-Level Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tomas Vojir*",
      "Jan Sochman",
      "Jiri Matas"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7721_ECCV_2024_paper.php": {
    "title": "GarmentCodeData: A Dataset of 3D Made-to-Measure Garments With Sewing Patterns",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maria Korosteleva*",
      "Timur Levent Kesdogan",
      "Fabian Kemper",
      "Stephan Wenninger",
      "Jasmin Koller",
      "Yuhan Zhang",
      "Mario Botsch",
      "Olga Sorkine-Hornung"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7726_ECCV_2024_paper.php": {
    "title": "Towards a Density Preserving Objective Function for Learning on Point Sets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haritha Jayasinghe*",
      "Ioannis Brilakis"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7731_ECCV_2024_paper.php": {
    "title": "AnatoMask: Enhancing Medical Image Segmentation with Reconstruction-guided Self-masking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuheng Li",
      "Tianyu Luan",
      "Yizhou Wu",
      "Shaoyan Pan",
      "Yenho Chen",
      "Xiaofeng Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7733_ECCV_2024_paper.php": {
    "title": "VF-NeRF: Viewshed Fields for Rigid NeRF Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leo Segre*",
      "Shai Avidan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7734_ECCV_2024_paper.php": {
    "title": "Task-Driven Uncertainty Quantification in Inverse Problems via Conformal Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeffrey Wen*",
      "Rizwan Ahmad",
      "Phillip Schniter"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7743_ECCV_2024_paper.php": {
    "title": "Trainable Highly-expressive Activation Functions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Irit Chelly*",
      "Shahaf E. Finder",
      "Shira Ifergane",
      "Oren Freifeld"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7745_ECCV_2024_paper.php": {
    "title": "Region-Aware Sequence-to-Sequence Learning for Hyperspectral Denoising",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "JiaHua Xiao",
      "Yang Liu",
      "Xing Wei*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7748_ECCV_2024_paper.php": {
    "title": "Self-Supervised Representation Learning for Adversarial Attack Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Li*",
      "Plamen Angelov",
      "Neeraj Suri"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7757_ECCV_2024_paper.php": {
    "title": "Do text-free diffusion models learn discriminative visual representations?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soumik Mukhopadhyay*",
      "Matthew A Gwilliam*",
      "Yosuke Yamaguchi",
      "Vatsal Agarwal",
      "Namitha Padmanabhan",
      "Archana Swaminathan",
      "Tianyi Zhou",
      "Jun Ohya",
      "Abhinav Shrivastava"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7761_ECCV_2024_paper.php": {
    "title": "Clean & Compact: Efficient Data-Free Backdoor Defense with Model Compactness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huy Phan*",
      "Jinqi Xiao",
      "Yang Sui",
      "Tianfang Zhang",
      "Zijie Tang",
      "Cong Shi",
      "Yan Wang",
      "Yingying Chen",
      "Bo Yuan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7764_ECCV_2024_paper.php": {
    "title": "DOCCI: Descriptions of Connected and Contrasting Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yasumasa Onoe*",
      "Sunayana Rane",
      "Zachary E Berger",
      "Yonatan Bitton",
      "Jaemin Cho",
      "Roopal Garg",
      "Alexander Ku",
      "Zarana Parekh",
      "Jordi Pont-Tuset",
      "Garrett Tanzer",
      "Su Wang",
      "Jason M Baldridge"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7766_ECCV_2024_paper.php": {
    "title": "EAS-SNN: End-to-End Adaptive Sampling and Representation for Event-based Detection with Recurrent Spiking Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziming Wang",
      "Ziling Wang",
      "Huaning Li",
      "Lang Qin",
      "Runhao Jiang",
      "De Ma*",
      "Huajin Tang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7767_ECCV_2024_paper.php": {
    "title": "AttentionHand: Text-driven Controllable Hand Image Generation for 3D Hand Reconstruction in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junho Park",
      "Kyeongbo Kong",
      "Suk-Ju Kang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7772_ECCV_2024_paper.php": {
    "title": "Dataset Quantization with Active Learning based Adaptive Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenghao Zhao*",
      "Yuzhang Shang",
      "Junyi Wu",
      "Yan Yan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7775_ECCV_2024_paper.php": {
    "title": "LogoSticker: Inserting Logos into Diffusion Models for Customized Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingkang Zhu",
      "Xi CHEN",
      "Zhongdao Wang",
      "Hengshuang Zhao*",
      "Jiaya Jia*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7776_ECCV_2024_paper.php": {
    "title": "LEROjD: Lidar Extended Radar-Only Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Patrick Palmer*",
      "Martin Krüger",
      "Stefan Schütte",
      "Richard Altendorfer",
      "Ganesh Adam",
      "Torsten Bertram"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7778_ECCV_2024_paper.php": {
    "title": "ProCreate, Don't Reproduce! Propulsive Energy Diffusion for Creative Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jack Lu*",
      "Ryan Teehan*",
      "Mengye Ren*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7780_ECCV_2024_paper.php": {
    "title": "Match-Stereo-Videos: Bidirectional Alignment for Consistent Dynamic Stereo Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junpeng Jing*",
      "Ye Mao",
      "Krystian Mikolajczyk*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7781_ECCV_2024_paper.php": {
    "title": "Probabilistic Image-Driven Traffic Modeling via Remote Sensing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Scott Workman*",
      "Armin Hadzic"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7782_ECCV_2024_paper.php": {
    "title": "IntrinsicAnything: Learning Diffusion Priors for Inverse Rendering Under Unknown Illumination",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xi Chen*",
      "Sida Peng",
      "Dongchen Yang",
      "Yuan Liu",
      "Bowen Pan",
      "Chengfei Lyu",
      "Xiaowei Zhou*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7783_ECCV_2024_paper.php": {
    "title": "VideoStudio: Generating Consistent-Content and Multi-Scene Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fuchen Long",
      "Zhaofan Qiu*",
      "Ting Yao",
      "Tao Mei"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7790_ECCV_2024_paper.php": {
    "title": "Semantic Residual Prompts for Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Martin Menabue*",
      "Emanuele Frascaroli",
      "Matteo Boschini",
      "Enver Sangineto",
      "Lorenzo Bonicelli",
      "Angelo Porrello*",
      "SIMONE CALDERARA"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7791_ECCV_2024_paper.php": {
    "title": "TransCAD: A Hierarchical Transformer for CAD Sequence Inference from Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elona Dupont*",
      "Kseniya Cherenkova",
      "Dimitrios Mallis",
      "Gleb A Gusev",
      "Anis Kacem",
      "Djamila Aouada"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7792_ECCV_2024_paper.php": {
    "title": "ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siming Yan*",
      "Min Bai",
      "Weifeng Chen",
      "Xiong Zhou",
      "Qixing Huang",
      "Li Erran Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7799_ECCV_2024_paper.php": {
    "title": "Mixture of Efficient Diffusion Experts Through Automatic Interval and Sub-Network Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alireza Ganjdanesh*",
      "Yan Kang",
      "Yuchen Liu",
      "Richard Zhang",
      "Zhe Lin",
      "Heng Huang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7801_ECCV_2024_paper.php": {
    "title": "Occupancy as Set of Points",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiang Shi",
      "Tianheng Cheng",
      "Qian Zhang",
      "Wenyu Liu",
      "Xinggang Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7802_ECCV_2024_paper.php": {
    "title": "UAV First-Person Viewers Are Radiance Field Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liqi Yan*",
      "Qifan Wang",
      "Junhan Zhao",
      "Qiang Guan",
      "Zheng Tang",
      "Jianhui Zhang",
      "Dongfang Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7806_ECCV_2024_paper.php": {
    "title": "Rethinking Few-shot Class-incremental Learning: Learning from Yourself",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu-Ming Tang",
      "Yi-Xing Peng",
      "Jingke Meng*",
      "Wei-Shi Zheng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7811_ECCV_2024_paper.php": {
    "title": "ProSub: Probabilistic Open-Set Semi-Supervised Learning with Subspace-Based Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Erik Wallin*",
      "Lennart Svensson",
      "Fredrik Kahl",
      "Lars Hammarstrand"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7817_ECCV_2024_paper.php": {
    "title": "A Fair Ranking and New Model for Panoptic Scene Graph Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julian Lorenz*",
      "Alexander Pest",
      "Daniel Kienzle",
      "Katja Ludwig",
      "Rainer Lienhart"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7822_ECCV_2024_paper.php": {
    "title": "Pick-a-back: Selective Device-to-Device Knowledge Transfer in Federated Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "HyungJune Lee*",
      "JinYi Yoon"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7823_ECCV_2024_paper.php": {
    "title": "Compensation Sampling for Improved Convergence in Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hui Lu*",
      "Albert Ali Salah",
      "Ronald Poppe"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7826_ECCV_2024_paper.php": {
    "title": "Situated Instruction Following",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "So Yeon Min*",
      "Xavier Puig",
      "Devendra Singh Chaplot",
      "Tsung-Yen Yang",
      "Priyam Parashar",
      "Akshara Rai",
      "Ruslan Salakhutdinov",
      "Yonatan Bisk",
      "Roozbeh Mottaghi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7832_ECCV_2024_paper.php": {
    "title": "Holodepth: Programmable Depth-Varying Projection via Computer-Generated Holography",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dorian Chan*",
      "Matthew O'Toole",
      "Sizhuo Ma",
      "Jian Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7833_ECCV_2024_paper.php": {
    "title": "SceneScript: Reconstructing Scenes With An Autoregressive Structured Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Armen Avetisyan*",
      "Christopher Xie",
      "Henry Howard-Jenkins",
      "Tsun-Yi Yang",
      "Samir Aroudj",
      "Suvam Patra",
      "Fuyang Zhang",
      "Luke Holland",
      "Duncan Frost",
      "Campbell Orme",
      "Jakob Engel",
      "Edward Miller",
      "Richard Newcombe",
      "Vasileios Balntas"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7834_ECCV_2024_paper.php": {
    "title": "GalLop: Learning global and local prompts for vision-language models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marc Lafon*",
      "Elias Ramzi*",
      "Clément Rambour",
      "Nicolas Audebert",
      "Nicolas Thome"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7836_ECCV_2024_paper.php": {
    "title": "Depth on Demand: Streaming Dense Depth from a Low Frame Rate Active Sensor",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrea Conti*",
      "Matteo Poggi",
      "Valerio Cambareri",
      "Stefano Mattoccia"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7844_ECCV_2024_paper.php": {
    "title": "Lossy Image Compression with Foundation Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucas Relic*",
      "Roberto Azevedo",
      "Markus Gross",
      "Christopher Schroers*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7850_ECCV_2024_paper.php": {
    "title": "CLIP-DINOiser: Teaching CLIP a few DINO tricks for open-vocabulary semantic segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Monika Wysoczańska*",
      "Oriane Siméoni",
      "Michaël Ramamonjisoa",
      "Andrei Bursuc",
      "Tomasz Trzciński",
      "Patrick Pérez"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7855_ECCV_2024_paper.php": {
    "title": "FMBoost: Boosting Latent Diffusion with Flow Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Johannes S Fischer*",
      "Ming Gui",
      "Pingchuan Ma",
      "Nick Stracke",
      "Stefan Andreas Baumann",
      "Vincent Tao Hu",
      "Björn Ommer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7860_ECCV_2024_paper.php": {
    "title": "COMPOSE: Comprehensive Portrait Shadow Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrew Z Hou*",
      "Zhixin Shu",
      "Xuaner Zhang",
      "He Zhang",
      "Yannick Hold-Geoffroy",
      "Jae Shin Yoon",
      "Xiaoming Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7862_ECCV_2024_paper.php": {
    "title": "LNL+K: Enhancing Learning with Noisy Labels Through Noise Source Knowledge Integration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siqi Wang*",
      "Bryan Plummer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7865_ECCV_2024_paper.php": {
    "title": "Diffusion Models as Data Mining Tools",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ioannis Siglidis*",
      "Aleksander Holynski",
      "Alexei A. Efros",
      "Mathieu Aubry",
      "Shiry Ginosar"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7866_ECCV_2024_paper.php": {
    "title": "Graph Neural Network Causal Explanation via Neural Causal Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arman Behnam*",
      "Binghui Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7868_ECCV_2024_paper.php": {
    "title": "Unsupervised, Online and On-The-Fly Anomaly Detection For Non-Stationary Image Distributions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Declan GD McIntosh*",
      "Alexandra Branzan Albu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7871_ECCV_2024_paper.php": {
    "title": "Photorealistic Object Insertion with Diffusion-Guided Inverse Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruofan Liang",
      "Zan Gojcic",
      "Merlin Nimier-David",
      "David Acuna",
      "Nandita Vijaykumar",
      "Sanja Fidler",
      "Zian Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7875_ECCV_2024_paper.php": {
    "title": "GAReT: Cross-view Video Geolocalization with Adapters and Auto-Regressive Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manu S Pillai*",
      "Mamshad Nayeem Rizve",
      "Mubarak Shah"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7882_ECCV_2024_paper.php": {
    "title": "SAMFusion: Sensor-Adaptive Multimodal Fusion for 3D Object Detection in Adverse Weather",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Edoardo Palladin*",
      "Roland Dietze*",
      "Praveen Narayanan",
      "Mario Bijelic",
      "Felix Heide"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7885_ECCV_2024_paper.php": {
    "title": "Generating Physically Realistic and Directable Human Motions from Multi-Modal Inputs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aayam Shrestha",
      "Pan Liu*",
      "German Ros",
      "Kai Yuan*",
      "Alan Fern"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7890_ECCV_2024_paper.php": {
    "title": "CoTracker: It is Better to Track Together",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikita Karaev*",
      "Ignacio Rocco",
      "Ben Graham",
      "Natalia Neverova",
      "Andrea Vedaldi",
      "Christian Rupprecht"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7894_ECCV_2024_paper.php": {
    "title": "SPHINX: A Mixer of Weights, Visual Embeddings and Image Scales for Multi-modal Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyi Lin",
      "Dongyang Liu",
      "Renrui Zhang",
      "Peng Gao*",
      "Longtian Qiu",
      "Han Xiao",
      "Han Qiu",
      "Wenqi Shao",
      "Keqin Chen",
      "Jiaming Han",
      "Siyuan Huang",
      "Yichi Zhang",
      "Xuming He",
      "Yu Qiao*",
      "Hongsheng Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7899_ECCV_2024_paper.php": {
    "title": "PathMMU: A Massive Multimodal Expert-Level Benchmark for Understanding and Reasoning in Pathology",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Sun*",
      "Hao Wu",
      "Chenglu Zhu",
      "Sunyi Zheng",
      "Qizi Chen",
      "Kai Zhang",
      "Yunlong Zhang",
      "Dan Wan",
      "Xiaoxiao Lan",
      "Mengyue Zheng",
      "Jingxiong Li",
      "Xinheng Lyu",
      "Tao Lin*",
      "Lin Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7902_ECCV_2024_paper.php": {
    "title": "Improving Adversarial Transferability via Model Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Avery Ma*",
      "Amir-massoud Farahmand",
      "Yangchen Pan",
      "Philip Torr",
      "Jindong Gu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7903_ECCV_2024_paper.php": {
    "title": "RealGen: Retrieval Augmented Generation for Controllable Traffic Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhao Ding*",
      "Yulong Cao",
      "DING ZHAO",
      "Chaowei Xiao",
      "Marco Pavone"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7904_ECCV_2024_paper.php": {
    "title": "ADen: Adaptive Density Representations for Sparse-view Camera Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Tang",
      "Weiyao Wang",
      "Pierre Gleize",
      "Matt Feiszli*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7908_ECCV_2024_paper.php": {
    "title": "Embodied Understanding of Driving Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunsong Zhou*",
      "Linyan Huang",
      "Qingwen Bu",
      "Jia Zeng",
      "Tianyu Li",
      "Hang Qiu",
      "Hongzi Zhu",
      "Minyi Guo",
      "Yu Qiao",
      "Hongyang Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7910_ECCV_2024_paper.php": {
    "title": "Learning to Drive via Asymmetric Self-Play",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chris Zhang*",
      "Sourav Biswas",
      "Kelvin Wong",
      "Kion Fallah",
      "Lunjun Zhang",
      "Dian Chen",
      "Sergio Casas",
      "Raquel Urtasun"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7914_ECCV_2024_paper.php": {
    "title": "OpenIns3D: Snap and Lookup for 3D Open-vocabulary Instance Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhening Huang",
      "Xiaoyang Wu",
      "Xi Chen",
      "Hengshuang Zhao*",
      "Lei Zhu",
      "Joan Lasenby*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7919_ECCV_2024_paper.php": {
    "title": "ViLA: Efficient Video-Language Alignment for Video Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xijun Wang*",
      "Junbang Liang",
      "Chun-Kai Wang",
      "Kenan Deng",
      "Yu Lou",
      "Ming C Lin",
      "Shan Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7921_ECCV_2024_paper.php": {
    "title": "Factorizing Text-to-Video Generation by Explicit Image Conditioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rohit Girdhar*",
      "Mannat Singh",
      "Andrew Brown",
      "Quentin Duval",
      "Samaneh Azadi",
      "Sai Saketh Rambhatla",
      "Mian Akbar Shah",
      "Xi Yin",
      "Devi Parikh",
      "Ishan Misra"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7923_ECCV_2024_paper.php": {
    "title": "MobileDiffusion: Instant Text-to-Image Generation on Mobile Devices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Zhao*",
      "Zhisheng Xiao*",
      "Yanwu Xu",
      "Haolin Jia",
      "Tingbo Hou"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7925_ECCV_2024_paper.php": {
    "title": "Open-Set Biometrics: Beyond Good Closed-Set Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiyang Su",
      "Minchul Kim",
      "Feng Liu",
      "Anil Jain",
      "Xiaoming Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7926_ECCV_2024_paper.php": {
    "title": "UNIT: Backdoor Mitigation via Automated Neural Distribution Tightening",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyuan Cheng*",
      "Guangyu Shen",
      "Kaiyuan Zhang",
      "Guanhong Tao",
      "Shengwei An",
      "Hanxi Guo",
      "Shiqing Ma",
      "Xiangyu Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7930_ECCV_2024_paper.php": {
    "title": "Which Model Generated This Image? A Model-Agnostic Approach for Origin Attribution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fengyuan Liu",
      "Haochen Luo",
      "Yiming Li",
      "Philip Torr",
      "Jindong Gu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7932_ECCV_2024_paper.php": {
    "title": "Osmosis: RGBD Diffusion Prior for Underwater Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Opher Bar Nathan*",
      "Deborah Levy",
      "Tali Treibitz",
      "Dan Rosenbaum"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7935_ECCV_2024_paper.php": {
    "title": "Towards Adaptive Pseudo-label Learning for Semi-Supervised Temporal Action Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feixiang Zhou",
      "Bryan Williams",
      "Hossein Rahmani*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7936_ECCV_2024_paper.php": {
    "title": "Computing the Lipschitz constant needed for fast scene recovery from CASSI measurements",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Niels Chr Overgaard*",
      "Anders Holst"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7937_ECCV_2024_paper.php": {
    "title": "DatasetNeRF: Efficient 3D-aware Data Factory with Generative Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Chi*",
      "Fangneng Zhan",
      "Sibo Wu",
      "Christian Theobalt",
      "Adam Kortylewski"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7941_ECCV_2024_paper.php": {
    "title": "Flowed Time of Flight Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mikhail Okunev*",
      "Marc Mapeke",
      "Benjamin Attal",
      "Christian Richardt",
      "Matthew O'Toole",
      "James Tompkin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7943_ECCV_2024_paper.php": {
    "title": "3D-GOI: 3D GAN Omni-Inversion for Multifaceted and Multi-object Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoran Li",
      "Long Ma",
      "Haolin Shi",
      "Yanbin Hao",
      "Yong Liao*",
      "Lechao Cheng",
      "Peng Yuan Zhou*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7945_ECCV_2024_paper.php": {
    "title": "Fast Registration of Photorealistic Avatars for VR Facial Animation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaitanya Patel*",
      "Shaojie Bai",
      "Te-Li Wang",
      "Jason Saragih",
      "Shih-En Wei"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7950_ECCV_2024_paper.php": {
    "title": "CoPT: Unsupervised Domain Adaptive Segmentation using Domain-Agnostic Text Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cristina Mata*",
      "Kanchana N Ranasinghe",
      "Michael S Ryoo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7957_ECCV_2024_paper.php": {
    "title": "HiFi-Score: Fine-grained Image Description Evaluation with Hierarchical Parsing Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziwei Yao",
      "Ruiping Wang*",
      "Xilin Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7960_ECCV_2024_paper.php": {
    "title": "Image-to-Lidar Relational Distillation for Autonomous Driving Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anas Mahmoud*",
      "Ali Harakeh",
      "Steven Waslander"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7965_ECCV_2024_paper.php": {
    "title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gemma Canet Tarrés*",
      "Zhe Lin",
      "Zhifei Zhang",
      "Jianming Zhang",
      "Yizhi Song",
      "Dan Ruta",
      "Andrew Gilbert",
      "John Collomosse",
      "Soo Ye Kim"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7968_ECCV_2024_paper.php": {
    "title": "Large-scale Reinforcement Learning for Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinan Zhang*",
      "Eric Tzeng",
      "Yilun Du",
      "Dmitry Kislyuk*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7969_ECCV_2024_paper.php": {
    "title": "CoMusion: Towards Consistent Stochastic Human Motion Prediction via Motion Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiarui Sun*",
      "Girish Chowdhary*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7971_ECCV_2024_paper.php": {
    "title": "FedHARM: Harmonizing Model Architectural Diversity in Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anestis Kastellos*",
      "Athanasios Psaltis",
      "Charalampos Z Patrikakis",
      "Petros Daras"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7976_ECCV_2024_paper.php": {
    "title": "EAGLES: Efficient Accelerated 3D Gaussians with Lightweight EncodingS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sharath Girish*",
      "Kamal Gupta",
      "Abhinav Shrivastava"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7978_ECCV_2024_paper.php": {
    "title": "Global Counterfactual Directions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bartłomiej Sobieski*",
      "Przemyslaw Biecek*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7983_ECCV_2024_paper.php": {
    "title": "TCLC-GS: Tightly Coupled LiDAR-Camera Gaussian Splatting for Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Zhao*",
      "su sun",
      "Ruoyu Wang",
      "Yuliang Guo",
      "Jun-Jun Wan",
      "Zhou Huang",
      "Xinyu Huang",
      "Yingjie Victor Chen",
      "Liu Ren"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7989_ECCV_2024_paper.php": {
    "title": "RT-Pose: A 4D Radar-Tensor based 3D Human Pose Estimation and Localization Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan-Hao Ho",
      "Jen-Hao Cheng",
      "Sheng Yao Kuan",
      "Zhongyu Jiang",
      "Wenhao Chai",
      "Hsiang-Wei Huang",
      "Chih-Lung Lin",
      "Jenq-Neng Hwang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7991_ECCV_2024_paper.php": {
    "title": "EditShield: Protecting Unauthorized Image Editing by Instruction-guided Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruoxi Chen",
      "Haibo Jin",
      "Yixin Liu",
      "Jinyin Chen*",
      "Haohan Wang",
      "Lichao Sun"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8001_ECCV_2024_paper.php": {
    "title": "RICA^2: Rubric-Informed, Calibrated Assessment of Actions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abrar Majeedi",
      "Viswanatha Reddy Gajjala",
      "Satya Sai Srinath Namburi GNVV",
      "Yin Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8007_ECCV_2024_paper.php": {
    "title": "Region-centric Image-Language Pretraining for Open-Vocabulary Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dahun Kim*",
      "Anelia Angelova",
      "Weicheng Kuo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8008_ECCV_2024_paper.php": {
    "title": "Commonly Interesting Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fitim Abdullahu*",
      "Helmut Grabner*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8009_ECCV_2024_paper.php": {
    "title": "Contrasting Deepfakes Diffusion via Contrastive Learning and Global-Local Similarities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lorenzo Baraldi*",
      "Federico Cocchi",
      "Marcella Cornia",
      "Lorenzo Baraldi",
      "Alessandro Nicolosi",
      "Rita Cucchiara"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8023_ECCV_2024_paper.php": {
    "title": "CriSp: Leveraging Tread Depth Maps for Enhanced Crime-Scene Shoeprint Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samia Shafique*",
      "Shu Kong",
      "Charless Fowlkes"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8026_ECCV_2024_paper.php": {
    "title": "Caltech Aerial RGB-Thermal Dataset in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Connor Lee*",
      "Matthew Anderson",
      "Nikhil Ranganathan",
      "Xingxing Zuo",
      "Kevin T Do",
      "Georgia Gkioxari",
      "Soon-Jo Chung"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8027_ECCV_2024_paper.php": {
    "title": "Diffusion Soup: Model Merging for Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin J Biggs*",
      "Arjun Seshadri",
      "Yang Zou",
      "Achin Jain",
      "Aditya Golatkar",
      "Yusheng Xie",
      "Alessandro Achille",
      "Ashwin Swaminathan",
      "Stefano Soatto"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8028_ECCV_2024_paper.php": {
    "title": "Volumetric Rendering with Baked Quadrature Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gopal Sharma*",
      "Daniel Rebain",
      "Kwang Moo Yi",
      "Andrea Tagliasacchi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8031_ECCV_2024_paper.php": {
    "title": "CityGuessr: City-Level Video Geo-Localization on a Global Scale",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Parth Parag Kulkarni*",
      "Gaurav Kumar Nayak",
      "Mubarak Shah"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8036_ECCV_2024_paper.php": {
    "title": "Pseudo-Labelling Should Be Aware of Disguising Channel Activations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changrui Chen",
      "Kurt Debattista",
      "Jungong Han*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8038_ECCV_2024_paper.php": {
    "title": "Bayesian Detector Combination for Object Detection with Crowdsourced Annotations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhi Qin Tan*",
      "Olga Isupova",
      "Gustavo Carneiro",
      "Xiatian Zhu",
      "Yunpeng Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8041_ECCV_2024_paper.php": {
    "title": "Revising Densification in Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuel Rota Bulò*",
      "Lorenzo Porzi",
      "Peter Kontschieder"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8042_ECCV_2024_paper.php": {
    "title": "FlexiEdit: Frequency-Aware Latent Refinement for Enhanced Non-Rigid Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gwanhyeong Koo",
      "Sunjae Yoon",
      "Ji Woo Hong",
      "Chang D. Yoo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8045_ECCV_2024_paper.php": {
    "title": "Smoothness, Synthesis, and Sampling: Re-thinking Unsupervised Multi-View Stereo with DIV Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alex Rich*",
      "Noah Stier",
      "Pradeep Sen",
      "Tobias Hollerer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8050_ECCV_2024_paper.php": {
    "title": "Text Motion Translator: A Bi-Directional Model for Enhanced 3D Human Motion Generation from Open-Vocabulary Descriptions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yijun Qian*",
      "Jack Urbanek",
      "Alexander Hauptmann",
      "Jungdam Won"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8054_ECCV_2024_paper.php": {
    "title": "UL-VIO: Ultra-lightweight Visual-Inertial Odometry with Noise Robust Test-time Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinho Park*",
      "Se Young Chun",
      "Mingoo Seok"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8055_ECCV_2024_paper.php": {
    "title": "PolyOculus: Simultaneous Multi-view Image-based Novel View Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jason J. Yu*",
      "Tristan Aumentado-Armstrong",
      "Fereshteh Forghani",
      "Konstantinos G. Derpanis",
      "Marcus A. Brubaker"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8056_ECCV_2024_paper.php": {
    "title": "R3DS: Reality-linked 3D Scenes for Panoramic Scene Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qirui Wu*",
      "Sonia Raychaudhuri",
      "Daniel Ritchie",
      "Manolis Savva",
      "Angel X Chang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8058_ECCV_2024_paper.php": {
    "title": "A Graph-Based Approach for Category-Agnostic Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Or Hirschorn*",
      "Shai Avidan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8062_ECCV_2024_paper.php": {
    "title": "Depth-guided NeRF Training via Earth Mover's Distance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anita Rau*",
      "Josiah Aklilu",
      "Floyd C Holsinger",
      "Serena Yeung-Levy"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8064_ECCV_2024_paper.php": {
    "title": "INTRA: Interaction Relationship-aware Weakly Supervised Affordance Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ji Ha Jang",
      "Hoigi Seo",
      "Se Young Chun*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8067_ECCV_2024_paper.php": {
    "title": "DEPICT: Diffusion-Enabled Permutation Importance for Image Classification Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sarah Jabbour*",
      "Gregory Kondas",
      "Ella Kazerooni",
      "Michael Sjoding",
      "David Fouhey",
      "Jenna Wiens"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8071_ECCV_2024_paper.php": {
    "title": "Meerkat: Audio-Visual Large Language Model for Grounding in Space and Time",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanjoy Chowdhury*",
      "Sayan Nag",
      "Subhrajyoti Dasgupta",
      "Jun Chen",
      "Mohamed Elhoseiny",
      "Ruohan Gao",
      "Dinesh Manocha"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8072_ECCV_2024_paper.php": {
    "title": "Diagnosing and Re-learning for Balanced Multimodal Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yake Wei",
      "Siwei Li",
      "Ruoxuan Feng",
      "Di Hu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8074_ECCV_2024_paper.php": {
    "title": "Contribution-based Low-Rank Adaptation with Pre-training Model for Real Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongwon Park",
      "Hayeon Kim",
      "Se Young Chun*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8076_ECCV_2024_paper.php": {
    "title": "Elucidating the Hierarchical Nature of Behavior with Masked Autoencoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucas Stoffl",
      "Andy Bonnetto",
      "Stéphane D'Ascoli",
      "Alexander Mathis*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8077_ECCV_2024_paper.php": {
    "title": "BeyondScene: Higher-Resolution Human-Centric Scene Generation With Pretrained Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gwanghyun Kim",
      "Hayeon Kim",
      "Hoigi Seo",
      "Dong Un Kang",
      "Se Young Chun*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8084_ECCV_2024_paper.php": {
    "title": "SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Xu",
      "Ang Li",
      "Linghao Chen",
      "Yulin Liu",
      "Ruoxi Shi",
      "Hao Su*",
      "Minghua Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8085_ECCV_2024_paper.php": {
    "title": "MMEarth: Exploring Multi-Modal Pretext Tasks For Geospatial Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vishal Nedungadi",
      "Ankit Kariryaa",
      "Stefan Oehmcke",
      "Serge Belongie",
      "Christian Igel",
      "Nico Lang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8086_ECCV_2024_paper.php": {
    "title": "Discovering Unwritten Visual Classifiers with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mia Chiquier*",
      "Utkarsh Mall",
      "Carl Vondrick"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8090_ECCV_2024_paper.php": {
    "title": "LITA: Language Instructed Temporal-Localization Assistant",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "De-An Huang*",
      "Shijia Liao",
      "Subhashree Radhakrishnan",
      "Hongxu Yin",
      "Pavlo Molchanov",
      "Zhiding Yu",
      "Jan Kautz"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8092_ECCV_2024_paper.php": {
    "title": "MARs: Multi-view Attention Regularizations for Patch-based Feature Recognition of Space Terrain",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Timothy Chase Jr*",
      "Karthik Dantu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8095_ECCV_2024_paper.php": {
    "title": "Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keen You*",
      "Haotian Zhang",
      "Eldon Schoop",
      "Floris Weers",
      "Amanda Swearngin",
      "Jeff Nichols",
      "Yinfei Yang",
      "Zhe Gan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8099_ECCV_2024_paper.php": {
    "title": "Bridging the Pathology Domain Gap: Efficiently Adapting CLIP for Pathology Image Analysis with Limited Labeled Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengfeng Lai*",
      "Joohi Chauhan",
      "Brittany N. Dugger",
      "Chen-Nee Chuah"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8100_ECCV_2024_paper.php": {
    "title": "AugUndo: Scaling Up Augmentations for Monocular Depth Completion and Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yangchao Wu*",
      "Tian Yu Liu",
      "Hyoungseob Park",
      "Stefano Soatto",
      "Dong Lao",
      "Alex Wong"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8101_ECCV_2024_paper.php": {
    "title": "CARB-Net: Camera-Assisted Radar-Based Network for Vulnerable Road User Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei-Yu Lee*",
      "Martin Dimitrievski",
      "David Van Hamme",
      "Jan Aelterman",
      "Ljubomir Jovanov",
      "Wilfried Philips"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8105_ECCV_2024_paper.php": {
    "title": "SAH-SCI: Self-Supervised Adapter for Efficient Hyperspectral Snapshot Compressive Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haijin Zeng",
      "Yuxi Liu",
      "Yongyong Chen*",
      "Youfa Liu",
      "Chong Peng",
      "Jingyong Su"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8113_ECCV_2024_paper.php": {
    "title": "Minimalist Vision with Freeform Pixels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeremy Klotz*",
      "Shree Nayar"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8116_ECCV_2024_paper.php": {
    "title": "All You Need is Your Voice: Emotional Face Representation with Audio Perspective for Emotional Talking Face Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seongho Kim",
      "Byung Cheol Song*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8118_ECCV_2024_paper.php": {
    "title": "LatentEditor: Text Driven Local Editing of 3D Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Umar Khalid*",
      "Hasan Iqbal",
      "Muhammad Tayyab",
      "Md Nazmul Karim",
      "Jing Hua",
      "Chen Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8121_ECCV_2024_paper.php": {
    "title": "Single-Photon 3D Imaging with Equi-Depth Photon Histograms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaustubh Sadekar*",
      "David Maier",
      "Atul Ingle"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8133_ECCV_2024_paper.php": {
    "title": "Asynchronous Bioplausible Neuron for Spiking Neural Networks for Event-Based Vision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hussain Sajwani",
      "Dimitrios Makris",
      "Yahya Prof. Zweiri",
      "Fariborz Baghaei Naeini",
      "Sanket Mr Kachole*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8139_ECCV_2024_paper.php": {
    "title": "Viewpoint textual inversion: discovering scene representations and 3D view control in 2D diffusion models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "James Burgess*",
      "Kuan-Chieh Wang",
      "Serena Yeung-Levy"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8141_ECCV_2024_paper.php": {
    "title": "POET: Prompt Offset Tuning for Continual Human Action Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prachi Garg*",
      "Joseph K J",
      "Vineeth N Balasubramanian",
      "Necati Cihan Camgoz",
      "Chengde Wan",
      "Kenrick Kin",
      "Weiguang Si",
      "Shugao Ma",
      "Fernando de la Torre"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8142_ECCV_2024_paper.php": {
    "title": "Domain Generalization of 3D Object Detection by Density-Resampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuangzhi Li",
      "Lei Ma",
      "Xingyu Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8150_ECCV_2024_paper.php": {
    "title": "IG Captioner: Information Gain Captioners are Strong Zero-shot Classifiers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenglin Yang*",
      "Siyuan Qiao",
      "Yuan Cao",
      "Yu Zhang",
      "Tao Zhu",
      "Alan Yuille",
      "Jiahui Yu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8153_ECCV_2024_paper.php": {
    "title": "MRSP: Learn Multi-Representations of Single Primitive for Compositional Zero-Shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongyao Jiang",
      "Hui Chen",
      "Haodong Jing",
      "Yongqiang Ma",
      "Nanning Zheng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8161_ECCV_2024_paper.php": {
    "title": "Cross-Domain Semantic Segmentation on Inconsistent Taxonomy using VLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeongkee Lim",
      "Yusung Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8163_ECCV_2024_paper.php": {
    "title": "TrafficNight : An Aerial Multimodal Benchmark For Nighttime Vehicle Surveillance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guoxing Zhang",
      "Yiming Liu",
      "xiaoyu yang",
      "Chao Huang*",
      "HUANG Hailong"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8166_ECCV_2024_paper.php": {
    "title": "Loc3Diff: Local Diffusion for 3D Human Head Synthesis and Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yushi Lan*",
      "Feitong Tan",
      "Qiangeng Xu",
      "Di Qiu",
      "Kyle Genova",
      "Zeng Huang",
      "Rohit Pandey",
      "Sean Fanello",
      "Thomas Funkhouser",
      "Chen Change Loy",
      "Yinda Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8171_ECCV_2024_paper.php": {
    "title": "Towards Open Domain Text-Driven Synthesis of Multi-Person Motions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengyi Shan",
      "Lu Dong",
      "Yutao Han",
      "Yuan Yao",
      "Tao Liu",
      "Ifeoma Nwogu",
      "Guo-Jun Qi",
      "Mitchell K Hill*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8174_ECCV_2024_paper.php": {
    "title": "Generative End-to-End Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenzhao Zheng",
      "Ruiqi Song",
      "Xianda Guo*",
      "Chenming Zhang",
      "Long Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8177_ECCV_2024_paper.php": {
    "title": "Learning to Distinguish Samples for Generalized Category Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fengxiang Yang",
      "Nan Pu",
      "Wenjing Li",
      "Zhiming Luo*",
      "Shaozi Li",
      "Nicu Sebe",
      "Zhun Zhong*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8183_ECCV_2024_paper.php": {
    "title": "COM Kitchens: An Unedited Overhead-view Procedural Videos Dataset a Vision-Language Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Atsushi Hashimoto*",
      "Koki Maeda",
      "Tosho Hirasawa",
      "Jun Harashima",
      "Leszek Rybicki",
      "Yusuke Fukasawa",
      "Yoshitaka Ushiku"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8185_ECCV_2024_paper.php": {
    "title": "PILoRA: Prototype Guided Incremental LoRA for Federated Class-Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiyang Guo*",
      "Fei Zhu",
      "Wenzhuo Liu",
      "Xu-Yao Zhang*",
      "Cheng-Lin Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8193_ECCV_2024_paper.php": {
    "title": "Diff-Reg: Diffusion Model in Doubly Stochastic Matrix Space for Registration Problem",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qianliang Wu*",
      "Haobo Jiang*",
      "Lei Luo",
      "Jun Li",
      "Yaqing Ding*",
      "Jin Xie*",
      "Jian Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8195_ECCV_2024_paper.php": {
    "title": "WBP: Training-time Backdoor Attacks through Hardware-based Weight Bit Poisoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kunbei Cai*",
      "Zhenkai Zhang",
      "Qian Lou",
      "Fan Yao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8206_ECCV_2024_paper.php": {
    "title": "Towards Dual Transparent Liquid Level Estimation in Biomedical Lab: Dataset, Methods and Practice",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiayu Wang",
      "Ke Ma",
      "Ruiyun Zhong",
      "Xinggang Wang",
      "Yi Fang",
      "Yang Xiao",
      "Tian Xia*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8207_ECCV_2024_paper.php": {
    "title": "Encapsulating Knowledge in One Prompt",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Li*",
      "Runpeng Yu*",
      "Xinchao Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8210_ECCV_2024_paper.php": {
    "title": "Cross-Input Certified Training for Universal Perturbations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changming Xu*",
      "Gagandeep Singh"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8217_ECCV_2024_paper.php": {
    "title": "Visual Relationship Transformation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyu Xu*",
      "Jiayan Qiu",
      "Baosheng Yu",
      "Zhou Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8221_ECCV_2024_paper.php": {
    "title": "Not Just Change the Labels, Learn the Features: Watermarking Deep Neural Networks with Multi-View Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Li",
      "Sarthak Kumar Maharana",
      "Yunhui Guo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8224_ECCV_2024_paper.php": {
    "title": "Delving into Adversarial Robustness on Document Tampering Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huiru Shao",
      "Zhuang Qian",
      "Kaizhu Huang",
      "Wei Wang",
      "Xiaowei Huang",
      "Qiufeng Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8236_ECCV_2024_paper.php": {
    "title": "Adaptive Selection of Sampling-Reconstruction in Fourier Compressed Sensing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seongmin Hong",
      "Jaehyeok Bae",
      "Jongho Lee*",
      "Se Young Chun*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8254_ECCV_2024_paper.php": {
    "title": "Confidence-Based Iterative Generation for Real-World Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jialun Peng",
      "Xin Luo",
      "Jingjing Fu*",
      "Dong Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8257_ECCV_2024_paper.php": {
    "title": "Learning Scalable Model Soup on a Single GPU: An Efficient Subspace Training Strategy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Li*",
      "Weisen Jiang",
      "Fanghui Liu",
      "Xiaolin Huang",
      "James Kwok"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8264_ECCV_2024_paper.php": {
    "title": "Correspondences of the Third Kind: Camera Pose Estimation from Object Reflection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kohei Yamashita*",
      "Vincent Lepetit",
      "Ko Nishino"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8266_ECCV_2024_paper.php": {
    "title": "Seeing Faces in Things: A Model and Dataset for Pareidolia",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mark T Hamilton*",
      "Simon Stent",
      "Vasha G DuTell",
      "Anne Harrington",
      "Jennifer E Corbett",
      "Ruth Rosenholtz",
      "William T. Freeman"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8267_ECCV_2024_paper.php": {
    "title": "Cocktail Universal Adversarial Attack on Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaoxin Li*",
      "Xiaofeng Liao",
      "Xin Che",
      "Xintong Li",
      "Yong Zhang",
      "Lingyang Chu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8270_ECCV_2024_paper.php": {
    "title": "Gaussian Frosting: Editable Complex Radiance Fields with Real-Time Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antoine Guédon*",
      "Vincent Lepetit"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8272_ECCV_2024_paper.php": {
    "title": "AMD: Automatic Multi-step Distillation of Large-scale Vision Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Han",
      "Qifan Wang",
      "Sohail A Dianat",
      "Majid Rabbani",
      "Raghuveer Rao",
      "Yi Fang",
      "Qiang Guan",
      "Lifu Huang",
      "Dongfang Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8276_ECCV_2024_paper.php": {
    "title": "FairViT: Fair Vision Transformer via Adaptive Masking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowei Tian",
      "Ruijie Du",
      "Yanning Shen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8278_ECCV_2024_paper.php": {
    "title": "TrojVLM: Backdoor Attack Against Vision Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weimin Lyu*",
      "Lu Pang",
      "Tengfei Ma",
      "Haibin Ling",
      "Chao Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8280_ECCV_2024_paper.php": {
    "title": "VisionLLaMA: A Unified LLaMA Backbone for Vision Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangxiang Chu*",
      "Jianlin Su",
      "Bo Zhang*",
      "Chunhua Shen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8282_ECCV_2024_paper.php": {
    "title": "Frugal 3D Point Cloud Model Training via Progressive Near Point Filtering and Fused Aggregation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Donghyun Lee",
      "Yejin Lee",
      "Jae W. Lee*",
      "Hongil Yoon*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8284_ECCV_2024_paper.php": {
    "title": "HVCLIP: High-dimensional Vector in CLIP for Unsupervised Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Noranart Vesdapunt*",
      "Kah Kuen Fu",
      "Yue Wu",
      "Xu Zhang",
      "Pradeep Natarajan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8286_ECCV_2024_paper.php": {
    "title": "Improving 3D Semi-supervised Learning by Effectively Utilizing All Unlabelled Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sneha Paul*",
      "Zachary Patterson",
      "Nizar Bouguila"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8287_ECCV_2024_paper.php": {
    "title": "PRET: Planning with Directed Fidelity Trajectory for Vision and Language Navigation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Renjie Lu",
      "Jingke Meng*",
      "WEI-SHI ZHENG"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8288_ECCV_2024_paper.php": {
    "title": "MART: MultiscAle Relational Transformer Networks for Multi-agent Trajectory Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seongju Lee",
      "Junseok Lee",
      "Yeonguk Yu",
      "Taeri Kim",
      "Kyoobin Lee*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8290_ECCV_2024_paper.php": {
    "title": "Expanding Scene Graph Boundaries: Fully Open-vocabulary Scene Graph Generation via Visual-Concept Alignment and Retention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zuyao Chen",
      "Jinlin Wu",
      "Zhen Lei",
      "Zhaoxiang Zhang",
      "Chang Wen Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8292_ECCV_2024_paper.php": {
    "title": "Few-shot NeRF by Adaptive Rendering Loss Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingshan Xu*",
      "Xuanyu Yi",
      "Jianyao Xu",
      "Wenbing Tao",
      "Yew Soon Ong",
      "Hanwang Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8294_ECCV_2024_paper.php": {
    "title": "Investigating Style Similarity in Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gowthami Somepalli*",
      "Anubhav Gupta",
      "Kamal Gupta",
      "Shramay Palta",
      "Micah Goldblum",
      "Jonas A. Geiping",
      "Abhinav Shrivastava",
      "Tom Goldstein"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8296_ECCV_2024_paper.php": {
    "title": "JDT3D: Addressing the Gaps in LiDAR-Based Tracking-by-Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brian Cheong*",
      "Jiachen Zhou*",
      "Steven L Waslander*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8299_ECCV_2024_paper.php": {
    "title": "MagicMirror: Fast and High-Quality Avatar Generation with Constrained Search Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Armand Comas",
      "Di Qiu*",
      "Menglei Chai",
      "Marcel C. Bühler",
      "Amit Raj",
      "Ruiqi Gao",
      "Qiangeng Xu",
      "Mark J Matthews",
      "Paulo Gotardo",
      "Sergio Orts-Escolano",
      "Thabo Beeler"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8301_ECCV_2024_paper.php": {
    "title": "EntAugment: Entropy-Driven Adaptive Data Augmentation Framework for Image Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suorong Yang*",
      "Furao Shen*",
      "Jian Zhao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8312_ECCV_2024_paper.php": {
    "title": "Timestep-Aware Correction for Quantized Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuzhe Yao",
      "Feng Tian",
      "Jun Chen*",
      "Haonan Lin",
      "Guang Dai",
      "Yong Liu",
      "Jingdong Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8320_ECCV_2024_paper.php": {
    "title": "SPARO: Selective Attention for Robust and Compositional Transformer Encodings for Vision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ankit Vani*",
      "Bac Nguyen",
      "Samuel Lavoie",
      "Ranjay Krishna",
      "Aaron Courville"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8321_ECCV_2024_paper.php": {
    "title": "Towards compact reversible image representations for neural style transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiyao Liu",
      "Siyu Yang",
      "Jian Zhang*",
      "Gerald Schaefer",
      "Jiya Li",
      "Xunli FAN",
      "Songtao Wu",
      "Hui Fang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8331_ECCV_2024_paper.php": {
    "title": "Out-of-Bounding-Box Triggers: A Stealthy Approach to Cheat Object Detectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Lin*",
      "lijia Yu*",
      "Gaojie Jin*",
      "Renjue Li*",
      "Peng Wu*",
      "Lijun Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8341_ECCV_2024_paper.php": {
    "title": "GTMS: A Gradient-driven Tree-guided Mask-free Referring Image Segmentation Method",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoxin Lv",
      "Tianxiong Zhong",
      "Sanyuan Zhao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8346_ECCV_2024_paper.php": {
    "title": "Long-term Temporal Context Gathering for Neural Video Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linfeng Qi",
      "Zhaoyang Jia",
      "Jiahao Li",
      "Bin Li",
      "Houqiang Li",
      "Yan Lu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8347_ECCV_2024_paper.php": {
    "title": "VQA-Diff: Exploiting VQA and Diffusion for Zero-Shot Image-to-3D Vehicle Asset Generation in Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "YIBO LIU*",
      "Zheyuan Yang",
      "Guile Wu",
      "Yuan Ren",
      "Kejian Lin",
      "Liu Bingbing",
      "Yang Liu",
      "JINJUN SHAN"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8356_ECCV_2024_paper.php": {
    "title": "From Pixels to Objects: A Hierarchical Approach for Part and Object Segmentation Using Local and Global Aggregation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunfei Xie*",
      "Cihang Xie",
      "Alan Yuille",
      "Jieru Mei"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8360_ECCV_2024_paper.php": {
    "title": "Leveraging Text Localization for Scene Text Removal via Text-aware Masked Image Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixiao Wang*",
      "Hongtao Xie",
      "YuXin Wang",
      "Yadong Qu",
      "Fengjun Guo",
      "Pengwei Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8365_ECCV_2024_paper.php": {
    "title": "Unmasking Bias in Diffusion Model Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hu Yu",
      "Li Shen",
      "Jie Huang",
      "Hongsheng Li",
      "Feng Zhao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8369_ECCV_2024_paper.php": {
    "title": "Multimodal Label Relevance Ranking via Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taian Guo",
      "Taolin Zhang",
      "Haoqian Wu",
      "Hanjun Li",
      "Ruizhi Qiao*",
      "Xing Sun"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8371_ECCV_2024_paper.php": {
    "title": "Animate Your Motion: Turning Still Images into Dynamic Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingxiao Li*",
      "Bo Wan*",
      "Sien Moens",
      "Tinne Tuytelaars"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8372_ECCV_2024_paper.php": {
    "title": "Layered Rendering Diffusion Model for Controllable Zero-Shot Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zipeng Qi",
      "Guoxi Huang*",
      "Chenyang Liu",
      "Fei Ye"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8373_ECCV_2024_paper.php": {
    "title": "CIC-BART-SSA: : Controllable Image Captioning with Structured Semantic Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kalliopi Basioti*",
      "Mohamed A Abdelsalam*",
      "Federico Fancellu*",
      "Vladimir Pavlovic*",
      "Afsaneh Fazly*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8374_ECCV_2024_paper.php": {
    "title": "A Simple Background Augmentation Method for Object Detection with Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhang Li",
      "Xin Dong",
      "Chen Chen",
      "Weiming Zhuang",
      "Lingjuan Lyu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8380_ECCV_2024_paper.php": {
    "title": "Echoes of the Past: Boosting Long-tail Recognition via Reflective Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qihao Zhao",
      "Yalun Dai",
      "Shen Lin",
      "Wei Hu",
      "Fan Zhang*",
      "Jun Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8381_ECCV_2024_paper.php": {
    "title": "BlinkVision: A Benchmark for Optical Flow, Scene Flow and Point Tracking Estimation using RGB Frames and Events",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yijin Li",
      "Yichen Shen",
      "Zhaoyang Huang",
      "Shuo Chen",
      "Weikang Bian",
      "Xiaoyu Shi",
      "Fu-Yun Wang",
      "Keqiang Sun",
      "Hujun Bao",
      "Zhaopeng Cui",
      "Guofeng Zhang*",
      "Hongsheng Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8382_ECCV_2024_paper.php": {
    "title": "A Unified Anomaly Synthesis Strategy with Gradient Ascent for Industrial Anomaly Detection and Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiyu Chen",
      "Huiyuan Luo",
      "Chengkan Lv*",
      "Zhengtao Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8385_ECCV_2024_paper.php": {
    "title": "Deep Polarization Cues for Single-shot Shape and Subsurface Scattering Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenhao Li*",
      "Trung Thanh Ngo",
      "Hajime Nagahara"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8386_ECCV_2024_paper.php": {
    "title": "Rethinking Features-Fused-Pyramid-Neck for Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hulin Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8391_ECCV_2024_paper.php": {
    "title": "Spatial-Temporal Multi-level Association for Video Object Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Deshui Miao",
      "Xin Li",
      "Zhenyu He*",
      "Huchuan Lu",
      "Ming-Hsuan Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8392_ECCV_2024_paper.php": {
    "title": "Sparse Refinement for Efficient High-Resolution Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhijian Liu",
      "Zhuoyang Zhang",
      "Samir Khaki",
      "Shang Yang",
      "Haotian Tang",
      "Chenfeng Xu",
      "Kurt Keutzer",
      "Song Han*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8393_ECCV_2024_paper.php": {
    "title": "Safeguard Text-to-Image Diffusion Models with Human Feedback Inversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanghyun Kim*",
      "Seohyeon Jung",
      "Balhae Kim",
      "Moonseok Choi",
      "Jinwoo Shin",
      "Juho Lee*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8395_ECCV_2024_paper.php": {
    "title": "An Explainable Vision Question Answer Model via Diffusion Chain-of-Thought",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chunhao LU",
      "Qiang Lu*",
      "Jake Luo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8396_ECCV_2024_paper.php": {
    "title": "RaFE: Generative Radiance Fields Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongkai Wu",
      "Ziyu Wan",
      "Jing Zhang*",
      "Jing Liao",
      "Dong Xu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8399_ECCV_2024_paper.php": {
    "title": "UniProcessor: A Text-induced Unified Low-level Image Processor",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huiyu Duan*",
      "Xiongkuo Min",
      "Sijing Wu",
      "Wei Shen",
      "Guangtao Zhai"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8402_ECCV_2024_paper.php": {
    "title": "Fast Sprite Decomposition from Animated Graphics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tomoyuki Suzuki*",
      "Kotaro Kikuchi",
      "Kota Yamaguchi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8405_ECCV_2024_paper.php": {
    "title": "Learning Unified Reference Representation for Unsupervised Multi-class Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liren He",
      "Zhengkai Jiang",
      "Jinlong Peng",
      "Wenbing Zhu",
      "Liang Liu",
      "Qiangang Du",
      "Xiaobin Hu",
      "Mingmin Chi*",
      "Yabiao Wang*",
      "Chengjie Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8408_ECCV_2024_paper.php": {
    "title": "IRSAM: Advancing Segment Anything Model for Infrared Small Target Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingjin Zhang",
      "Yuchun Wang*",
      "Jie Guo*",
      "Yunsong Li",
      "Xinbo Gao",
      "Jing Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8418_ECCV_2024_paper.php": {
    "title": "PatchRefiner: Leveraging Synthetic Data for Real-Domain High-Resolution Monocular Metric Depth Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenyu Li*",
      "Shariq Farooq Bhat",
      "Peter Wonka"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8419_ECCV_2024_paper.php": {
    "title": "A Geometric Distortion Immunized Deep Watermarking Framework with Robustness Generalizability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linfeng Ma",
      "Han Fang*",
      "Tianyi Wei",
      "Zijin Yang",
      "Zehua Ma*",
      "Weiming Zhang",
      "Nenghai Yu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8432_ECCV_2024_paper.php": {
    "title": "Towards Robust Event-based Networks for Nighttime via Unpaired Day-to-Night Event Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhwan Jeong",
      "Hoonhee Cho",
      "Kuk-Jin Yoon*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8434_ECCV_2024_paper.php": {
    "title": "CLAMP-ViT: Contrastive Data-Free Learning for Adaptive Post-Training Quantization of ViTs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akshat Ramachandran*",
      "Souvik Kundu*",
      "Tushar Krishna*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8440_ECCV_2024_paper.php": {
    "title": "A Riemannian Approach for Spatiotemporal Analysis and Generation of 4D Tree-shaped Structures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tahmina Khanam",
      "Mohammed Bennamoun",
      "Guan Wang",
      "Guanjin Wang",
      "Ferdous Sohel",
      "Farid Boussaid",
      "Anuj Srivastava",
      "Hamid Laga*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8443_ECCV_2024_paper.php": {
    "title": "Dual-Path Adversarial Lifting for Domain Shift Correction in Online Test-time Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yushun Tang",
      "Shuoshuo Chen",
      "Zhihe Lu",
      "Xinchao Wang",
      "Zhihai He*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8448_ECCV_2024_paper.php": {
    "title": "Data Overfitting for On-Device Super-Resolution with Dynamic Algorithm and Compiler Co-Design",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gen Li*",
      "zhihao shu",
      "Jie Ji",
      "Minghai Qin",
      "Fatemeh Afghah",
      "Wei Niu",
      "Xiaolong Ma*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8449_ECCV_2024_paper.php": {
    "title": "The Role of Masking for Efficient Supervised Knowledge Distillation of Vision Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seungwoo Son*",
      "Jegwang Ryu",
      "Namhoon Lee",
      "Jaeho Lee*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8451_ECCV_2024_paper.php": {
    "title": "Training A Small Emotional Vision Language Model for Visual Art Comprehension",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Zhang",
      "Liang Zheng*",
      "Meng Wang",
      "Dan Guo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8453_ECCV_2024_paper.php": {
    "title": "UGG: Unified Generative Grasping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxin Lu",
      "Hao Kang",
      "Haoxiang Li",
      "Bo Liu",
      "Yiding Yang",
      "Qixing Huang",
      "Gang Hua*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8460_ECCV_2024_paper.php": {
    "title": "FrePolad: Frequency-Rectified Point Latent Diffusion for Point Cloud Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenliang Zhou*",
      "Fangcheng Zhong",
      "Param Hanji",
      "Zhilin Guo",
      "Kyle Thomas Fogarty",
      "Alejandro Sztrajman",
      "Hongyun Gao",
      "A. Cengiz Oztireli"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8462_ECCV_2024_paper.php": {
    "title": "Learning to Detect Multi-class Anomalies with Just One Normal Image Prompt",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bin-Bin Gao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8466_ECCV_2024_paper.php": {
    "title": "GAMMA-FACE: GAussian Mixture Models Amend Diffusion Models for Bias Mitigation in Face Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Basudha Pal*",
      "Arunkumar Kannan*",
      "Ram Prabhakar Kathirvel",
      "Alice O'Toole",
      "Rama Chellappa"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8467_ECCV_2024_paper.php": {
    "title": "Reinforcement Learning Friendly Vision-Language Model for Minecraft",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haobin Jiang",
      "Junpeng Yue",
      "Hao Luo",
      "Ziluo Ding",
      "Zongqing Lu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8471_ECCV_2024_paper.php": {
    "title": "Pseudo-RIS: Distinctive Pseudo-supervision Generation for Referring Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seonghoon Yu",
      "Paul Hongsuck Seo*",
      "Jeany Son*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8472_ECCV_2024_paper.php": {
    "title": "Training-free Composite Scene Generation for Layout-to-Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqi Liu*",
      "Tao Huang",
      "Chang Xu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8486_ECCV_2024_paper.php": {
    "title": "Robustness Preserving Fine-tuning using Neuron Importance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangrui Li",
      "Rahul Duggal*",
      "Aaditya Singh",
      "Kaustav Kundu",
      "Bing Shuai",
      "Jonathan Wu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8490_ECCV_2024_paper.php": {
    "title": "ProxyCLIP: Proxy Attention Improves CLIP for Open-Vocabulary Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengcheng Lan",
      "Chaofeng Chen",
      "Yiping Ke",
      "Xinjiang Wang",
      "Litong Feng",
      "Wayne Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8492_ECCV_2024_paper.php": {
    "title": "PEA-Diffusion: Parameter-Efficient Adapter with Knowledge Distillation in non-English Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "jian ma",
      "Chen Chen*",
      "Qingsong Xie",
      "Haonan Lu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8514_ECCV_2024_paper.php": {
    "title": "Similarity of Neural Architectures using Adversarial Attack Transferability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaehui Hwang",
      "Dongyoon Han",
      "Byeongho Heo",
      "Song Park",
      "Sanghyuk Chun*",
      "Jong-Seok Lee"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8521_ECCV_2024_paper.php": {
    "title": "Dual-Rain: Video Rain Removal using Assertive and Gentle Teachers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tingting Chen*",
      "Beibei Lin",
      "Yeying Jin",
      "Wending Yan",
      "WEI YE",
      "Yuan Yuan",
      "Robby T. Tan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8535_ECCV_2024_paper.php": {
    "title": "PMT: Progressive Mean Teacher via Exploring Temporal Consistency for Semi-Supervised Medical Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ning Gao",
      "Sanping Zhou*",
      "Le Wang",
      "Nanning Zheng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8538_ECCV_2024_paper.php": {
    "title": "OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raghav Kapoor*",
      "Yash Parag Butala*",
      "Melisa A Russak",
      "Jing Yu Koh",
      "Kiran Kamble",
      "Waseem AlShikh",
      "Ruslan Salakhutdinov"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8542_ECCV_2024_paper.php": {
    "title": "AutoEval-Video: An Automatic Benchmark for Assessing Large Vision Language Models in Open-Ended Video Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiuyuan Chen",
      "Yuan Lin*",
      "Yuchen Zhang*",
      "Weiran Huang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8550_ECCV_2024_paper.php": {
    "title": "Reflective Instruction Tuning: Mitigating Hallucinations in Large Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinrui Zhang",
      "Teng Wang",
      "Haigang Zhang",
      "Ping Lu",
      "Feng Zheng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8556_ECCV_2024_paper.php": {
    "title": "Unsupervised Variational Translator for Bridging Image Restoration and High-Level Vision Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawei Wu",
      "Zhi Jin*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8562_ECCV_2024_paper.php": {
    "title": "Diffusion Model for Robust Multi-Sensor Fusion in 3D Object Detection and BEV Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Duy Tho Le*",
      "Hengcan Shi*",
      "Jianfei Cai",
      "Hamid Rezatofighi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8563_ECCV_2024_paper.php": {
    "title": "MeshAvatar: Learning High-quality Triangular Human Avatars from Multi-view Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yushuo Chen*",
      "Zerong Zheng",
      "Zhe Li",
      "Chao Xu",
      "Yebin Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8568_ECCV_2024_paper.php": {
    "title": "Fast Point Cloud Geometry Compression with Context-based Residual Coding and INR-based Refinement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Xu*",
      "Xi Zhang",
      "Xiaolin Wu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8589_ECCV_2024_paper.php": {
    "title": "Scene-Conditional 3D Object Stylization and Composition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinghao Zhou*",
      "Tomas Jakab",
      "Philip Torr",
      "Christian Rupprecht"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8591_ECCV_2024_paper.php": {
    "title": "GenView: Enhancing View Quality with Pretrained Generative Model for Self-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaojie Li",
      "Yibo Yang*",
      "Xiangtai Li",
      "Jianlong Wu*",
      "Yue Yu",
      "Bernard Ghanem",
      "Min Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8592_ECCV_2024_paper.php": {
    "title": "Revisit Anything: Visual Place Recognition via Image Segment Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kartik Garg",
      "Sai Shubodh",
      "Shishir N Y Kolathaya",
      "Madhava Krishna",
      "Sourav Garg*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8613_ECCV_2024_paper.php": {
    "title": "EcoMatcher: Efficient Clustering Oriented Matcher for Detector-free Image Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peiqi Chen*",
      "Lei Yu",
      "Yi Wan*",
      "Yongjun Zhang*",
      "Jian Wang",
      "Liheng Zhong",
      "Jingdong Chen",
      "Ming Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8614_ECCV_2024_paper.php": {
    "title": "DGD: Dynamic 3D Gaussians Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Isaac Labe",
      "Noam Issachar",
      "Itai Lang",
      "Sagie Benaim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8616_ECCV_2024_paper.php": {
    "title": "Semantic Diversity-aware Prototype-based Learning for Unbiased Scene Graph Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaehyeong Jeon*",
      "Kibum Kim",
      "Kanghoon Yoon",
      "Chanyoung Park"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8619_ECCV_2024_paper.php": {
    "title": "DiffuMatting: Synthesizing Arbitrary Objects with Matting-level Annotation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaobin Hu",
      "Xu Peng",
      "Donghao Luo*",
      "Xiaozhong Ji",
      "Jinlong Peng",
      "ZhengKai Jiang",
      "Jiangning Zhang",
      "Taisong Jin*",
      "Chengjie Wang",
      "Rongrong Ji"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8641_ECCV_2024_paper.php": {
    "title": "Self-Guided Generation of Minority Samples Using Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soobin Um",
      "Jong Chul Ye*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8652_ECCV_2024_paper.php": {
    "title": "DEVIAS: Learning Disentangled Video Representations of Action and Scene",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kyungho Bae",
      "Youngrae Kim",
      "Geo Ahn",
      "Jinwoo Choi*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8661_ECCV_2024_paper.php": {
    "title": "AD3: Introducing a score for Anomaly Detection Dataset Difficulty assessment using VIADUCT dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jan D Lehr*",
      "Jan H Philipps",
      "Alik Sargsyan",
      "Martin Pape",
      "Jörg Krüger"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8662_ECCV_2024_paper.php": {
    "title": "RoomTex: Texturing Compositional Indoor Scenes via Iterative Inpainting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi WANG*",
      "Ruijie Lu",
      "Xudong XU",
      "Jingbo Wang",
      "Michael Yu Wang",
      "Bo Dai",
      "Gang Zeng",
      "Dan Xu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8663_ECCV_2024_paper.php": {
    "title": "Class-Agnostic Object Counting with Text-to-Image Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaofei Hui",
      "Qian Wu",
      "Hossein Rahmani",
      "Jun Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8664_ECCV_2024_paper.php": {
    "title": "Mask2Map: Vectorized HD Map Construction Using Bird's Eye View Segmentation Masks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sehwan Choi*",
      "Jun Won Choi",
      "Jungho Kim",
      "Hongjae Shin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8665_ECCV_2024_paper.php": {
    "title": "SUP-NeRF: A Streamlined Unification of Pose Estimation and NeRF for Monocular 3D Object Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuliang Guo*",
      "Abhinav Kumar",
      "Cheng Zhao",
      "Ruoyu Wang",
      "Xinyu Huang",
      "Liu Ren"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8668_ECCV_2024_paper.php": {
    "title": "Forbes: Face Obfuscation Rendering via Backpropagation Refinement Scheme",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jintae Kim",
      "Seungwon Yang",
      "Seong-Gyun Jeong",
      "Chang-Su Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8675_ECCV_2024_paper.php": {
    "title": "Pyramid Diffusion for Fine 3D Large Scene Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuheng Liu*",
      "Xinke Li",
      "Xueting Li",
      "Lu Qi*",
      "Chongshou Li",
      "Ming-Hsuan Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8688_ECCV_2024_paper.php": {
    "title": "ShoeModel: Learning to Wear on the User-specified Shoes via Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenyu Li*",
      "Binghui Chen",
      "Yifeng Geng",
      "Xuansong Xie",
      "Wangmeng Zuo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8694_ECCV_2024_paper.php": {
    "title": "A Watermark-Conditioned Diffusion Model for IP Protection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Min*",
      "Sen Li*",
      "Hongyang Chen*",
      "Minhao Cheng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8696_ECCV_2024_paper.php": {
    "title": "Finding NeMo: Negative-mined Mosaic Augmentation for Referring Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seongsu Ha",
      "Chaeyun Kim",
      "Donghwa Kim",
      "Junho Lee",
      "Sangho Lee",
      "Joonseok Lee*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8701_ECCV_2024_paper.php": {
    "title": "SAFT: Towards Out-of-Distribution Generalization in Fine-Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bac Nguyen*",
      "Stefan Uhlich",
      "Fabien Cardinaux",
      "Lukas Mauch",
      "Marzieh Edraki",
      "Aaron Courville"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8702_ECCV_2024_paper.php": {
    "title": "FTBC: Forward Temporal Bias Correction for Optimizing ANN-SNN Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaofeng Wu*",
      "Velibor Bojkovic",
      "Bin Gu*",
      "Kun Suo",
      "Kai Zou"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8703_ECCV_2024_paper.php": {
    "title": "Improving Vision and Language Concepts Understanding with Multimodal Counterfactual Samples",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengen Lai",
      "Shengli Song*",
      "Sitong Yan",
      "Guangneng Hu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8704_ECCV_2024_paper.php": {
    "title": "Centering the Value of Every Modality: Towards Efficient and Resilient Modality-agnostic Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xu Zheng*",
      "Yuanhuiyi Lyu",
      "jiazhou zhou",
      "Lin Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8711_ECCV_2024_paper.php": {
    "title": "GTPT: Group-based Token Pruning Transformer for Efficient Human Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haonan Wang",
      "Jie Liu*",
      "Jie Tang",
      "Gangshan Wu",
      "Bo Xu",
      "Yanbing Chou",
      "Yong Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8713_ECCV_2024_paper.php": {
    "title": "Lost in Translation: Modern Neural Networks Still Struggle With Small Realistic Image Transformations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ofir Shifman*",
      "Yair Weiss"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8737_ECCV_2024_paper.php": {
    "title": "DIAL: Dense Image-text ALignment for Weakly Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soojin Jang",
      "JungMin Yun",
      "JuneHyoung Kwon",
      "Eunju Lee",
      "YoungBin Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8753_ECCV_2024_paper.php": {
    "title": "Rethinking Normalization Layers for Domain Generalizable Person Re-identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ren Nie",
      "Jin Ding",
      "Xue Zhou*",
      "Xi Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8759_ECCV_2024_paper.php": {
    "title": "Generalizing to Unseen Domains via Text-guided Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daiqing Qi*",
      "Handong Zhao",
      "Aidong Zhang",
      "Sheng Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8761_ECCV_2024_paper.php": {
    "title": "VCP-CLIP: A visual context prompting model for zero-shot anomaly segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Qu",
      "Xian Tao*",
      "Mukesh Prasad",
      "Fei Shen",
      "Zhengtao Zhang",
      "Xinyi Gong",
      "Guiguang Ding"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8763_ECCV_2024_paper.php": {
    "title": "Lost in Translation: Latent Concept Misalignment in Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juntu Zhao",
      "Junyu Deng",
      "Yixin Ye",
      "Chongxuan Li",
      "Zhijie Deng*",
      "Dequan Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8766_ECCV_2024_paper.php": {
    "title": "Crowd-SAM:SAM as a smart annotator for object detection in crowded scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhi Cai",
      "Yingjie Gao",
      "Yaoyan Zheng",
      "Nan Zhou",
      "Di Huang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8774_ECCV_2024_paper.php": {
    "title": "Zero-shot Text-guided Infinite Image Synthesis with LLM guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soyeong Kwon",
      "Taegyeong Lee",
      "Taehwan Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8780_ECCV_2024_paper.php": {
    "title": "Learning Dual-Level Deformable Implicit Representation for Real-World Scale Arbitrary Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiheng Li",
      "Muheng Li",
      "Jixuan Fan",
      "Lei Chen*",
      "Yansong Tang",
      "Jiwen Lu",
      "Jie Zhou"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8783_ECCV_2024_paper.php": {
    "title": "Boosting Gaze Object Prediction via Pixel-level Supervision from Vision Foundation Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Jin",
      "Lei Zhang",
      "Shi Yan",
      "Bin Fan",
      "Binglu Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8795_ECCV_2024_paper.php": {
    "title": "Pro2SAM: Mask Prompt to SAM with Grid Points for Weakly Supervised Object Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xi Yang",
      "Songsong Duan*",
      "Nannan Wang",
      "Xinbo Gao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8798_ECCV_2024_paper.php": {
    "title": "Adaptive Multi-head Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Wang*",
      "Piotr Koniusz",
      "Tom Gedeon",
      "Liang Zheng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8803_ECCV_2024_paper.php": {
    "title": "Rotated Orthographic Projection for Self-Supervised 3D Human Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "YAO YAO",
      "Yixuan Pan",
      "Wenjun Shi",
      "Dongchen Zhu",
      "Lei Wang",
      "Jiamao Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8816_ECCV_2024_paper.php": {
    "title": "Easing 3D Pattern Reasoning with Side-view Features for Semantic Scene Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linxi Huan",
      "Mingyue Dong",
      "Linwei Yue",
      "Shuhan Shen",
      "Xianwei Zheng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8824_ECCV_2024_paper.php": {
    "title": "DSMix: Distortion-Induced Saliency Map Based Pre-training for No-Reference Image Quality Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinsong Shi",
      "Pan Gao*",
      "Xiaojiang Peng",
      "Jie Qin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8825_ECCV_2024_paper.php": {
    "title": "MO-EMT-NAS: Multi-Objective Continuous Transfer of Architectural Knowledge Between Tasks from Different Datasets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "PENG LIAO*",
      "Xilu Wang*",
      "Yaochu Jin*",
      "Wenli Du*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8828_ECCV_2024_paper.php": {
    "title": "Text-to-Sticker: Style Tailoring Latent Diffusion Models for Human Expression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Animesh Sinha*",
      "Bo Sun",
      "Anmol Kalia",
      "Arantxa Casanova",
      "Elliot Blanchard",
      "David Yan",
      "Winnie Zhang",
      "Tony Nelli",
      "Jiahui Chen",
      "Hardik Shah",
      "Licheng Yu",
      "Mitesh Kumar Singh",
      "Ankit Ramchandani",
      "Maziar Sanjabi",
      "Sonal Gupta",
      "Amy L Bearman",
      "Dhruv Mahajan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8834_ECCV_2024_paper.php": {
    "title": "Adaptive Annealing for Robust Averaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sidhartha Chitturi*",
      "Venu Madhav Govindu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8838_ECCV_2024_paper.php": {
    "title": "GRIDS: Grouped Multiple-Degradation Restoration with Image Degradation Similarity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuo Cao",
      "Yihao Liu",
      "Wenlong Zhang",
      "Yu Qiao",
      "Chao Dong*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8845_ECCV_2024_paper.php": {
    "title": "MaxMI: A Maximal Mutual Information Criterion for Manipulation Concept Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pei Zhou",
      "Yanchao Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8847_ECCV_2024_paper.php": {
    "title": "High-Quality Mesh Blendshape Generation from Face Videos via Neural Inverse Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Ming",
      "Jiawei Li",
      "Jingwang Ling",
      "Libo Zhang",
      "Feng Xu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8861_ECCV_2024_paper.php": {
    "title": "Disentangling Masked Autoencoders for Unsupervised Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "An Zhang*",
      "Han Wang",
      "Xiang Wang",
      "Tat-Seng Chua"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8862_ECCV_2024_paper.php": {
    "title": "Early Anticipation of Driving Maneuvers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abdul Wasi Lone",
      "Shankar Gangisetty*",
      "Shyam Nandan Rai",
      "C. V. Jawahar"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8866_ECCV_2024_paper.php": {
    "title": "Bottom-Up Domain Prompt Tuning for Generalized Face Anti-Spoofing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siqi Liu*",
      "Qirui Wang",
      "Pong C. Yuen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8870_ECCV_2024_paper.php": {
    "title": "SG-NeRF: Neural Surface Reconstruction with Scene Graph Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiyang Chen",
      "Siyan Dong*",
      "Xulong Wang",
      "Lulu Cai",
      "Youyi Zheng",
      "Yanchao Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8871_ECCV_2024_paper.php": {
    "title": "On the Evaluation Consistency of Attribution-based Explanations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiarui Duan",
      "Haoling Li",
      "Haofei Zhang",
      "Hao Jiang",
      "Mengqi Xue",
      "Li Sun",
      "Mingli Song",
      "Jie Song*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8881_ECCV_2024_paper.php": {
    "title": "Unified Embedding Alignment for Open-Vocabulary Video Instance Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Fang",
      "Peng Wu",
      "Yawei Li",
      "Xinxin Zhang",
      "Xiankai Lu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8882_ECCV_2024_paper.php": {
    "title": "InfoNorm: Mutual Information Shaping of Normals for Sparse-View Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xulong Wang",
      "Siyan Dong*",
      "Youyi Zheng",
      "Yanchao Yang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8897_ECCV_2024_paper.php": {
    "title": "DreamReward: Aligning Human Preference in Text-to-3D Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junliang Ye",
      "Fangfu Liu",
      "Qixiu Li",
      "Zhengyi Wang",
      "Yikai Wang",
      "Xinzhou Wang",
      "Yueqi Duan*",
      "Jun Zhu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8903_ECCV_2024_paper.php": {
    "title": "Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changan Chen*",
      "Puyuan Peng",
      "Ami Baid",
      "Zihui Xue",
      "Wei-Ning Hsu",
      "David Harwath",
      "Kristen Grauman"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8905_ECCV_2024_paper.php": {
    "title": "Frontier-enhanced Topological Memory with Improved Exploration Awareness for Embodied Visual Navigation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinru Cui",
      "Qiming Liu",
      "Zhe Liu",
      "Hesheng Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8907_ECCV_2024_paper.php": {
    "title": "MTMamba: Enhancing Multi-Task Dense Scene Understanding by Mamba-Based Decoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baijiong Lin*",
      "Weisen Jiang",
      "Pengguang Chen",
      "Yu Zhang",
      "Shu Liu",
      "Yingcong Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8910_ECCV_2024_paper.php": {
    "title": "VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shicheng Li",
      "Lei Li",
      "Yi Liu",
      "Shuhuai Ren",
      "Yuanxin Liu",
      "Rundong Gao",
      "Xu Sun*",
      "Lu Hou"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8911_ECCV_2024_paper.php": {
    "title": "Learning a Dynamic Privacy-preserving Camera Robust to Inversion Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiacheng Cheng*",
      "Xiang Dai",
      "Jia Wan",
      "Nick Antipa",
      "Nuno Vasconcelos"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8913_ECCV_2024_paper.php": {
    "title": "CadVLM: Bridging Language and Vision in the Generation of Parametric CAD Sketches",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sifan Wu*",
      "Amir Hosein Khasahmadi",
      "Mor Katz",
      "Pradeep Kumar Jayaraman",
      "Yewen Pu",
      "Karl D.D. Willis",
      "Bang Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8915_ECCV_2024_paper.php": {
    "title": "Towards Image Ambient Lighting Normalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Florin-Alexandru Vasluianu*",
      "Tim Seizinger",
      "Zongwei WU*",
      "Rakesh Ranjan",
      "Radu Timofte"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8916_ECCV_2024_paper.php": {
    "title": "FedHide: Federated Learning by Hiding in the Neighbors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyunsin Park*",
      "Sungrack Yun"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8921_ECCV_2024_paper.php": {
    "title": "Toward INT4 Fixed-Point Training via Exploring Quantization Error for Gradients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dohyung Kim",
      "Junghyup Lee",
      "Jeimin Jeon",
      "JAEHYEON MOON",
      "Bumsub Ham*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8926_ECCV_2024_paper.php": {
    "title": "SelEx: Self-Expertise in Fine-Grained Generalized Category Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sarah Rastegar*",
      "Mohammadreza Salehi",
      "Yuki M Asano",
      "Hazel Doughty",
      "Cees Snoek"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8927_ECCV_2024_paper.php": {
    "title": "Self-Cooperation Knowledge Distillation for Novel Class Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuzheng Wang*",
      "Zhaoyu Chen",
      "Dingkang Yang",
      "Yunquan Sun",
      "Lizhe Qi*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8931_ECCV_2024_paper.php": {
    "title": "EventBind: Learning a Unified Representation to Bind Them All for Event-based Open-world Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "jiazhou zhou*",
      "Xu Zheng",
      "Yuanhuiyi Lyu",
      "Lin Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8940_ECCV_2024_paper.php": {
    "title": "GLAD: Towards Better Reconstruction with Global and Local Adaptive Diffusion Models for Unsupervised Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hang Yao",
      "Ming Liu*",
      "Zhicun Yin",
      "Zifei Yan",
      "Xiaopeng Hong",
      "Wangmeng Zuo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8947_ECCV_2024_paper.php": {
    "title": "MedRAT: Unpaired Medical Report Generation via Auxiliary Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elad Hirsch*",
      "Gefen Dawidowicz",
      "Ayellet Tal"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8953_ECCV_2024_paper.php": {
    "title": "Are Synthetic Data Useful for Egocentric Hand-Object Interaction Detection?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rosario Leonardi*",
      "Antonino Furnari",
      "Francesco Ragusa",
      "Giovanni Maria Farinella"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8959_ECCV_2024_paper.php": {
    "title": "PoseEmbroider: Towards a 3D, Visual, Semantic-aware Human Pose Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ginger Delmas*",
      "Philippe Weinzaepfel",
      "Francesc Moreno-Noguer",
      "Gregory Rogez"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8961_ECCV_2024_paper.php": {
    "title": "A Comparative Study of Image Restoration Networks for General Backbone Network Design",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangyu Chen*",
      "Zheyuan Li",
      "Yuandong Pu",
      "Yihao Liu",
      "Jiantao Zhou*",
      "Yu Qiao",
      "Chao Dong*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8963_ECCV_2024_paper.php": {
    "title": "Learned Image Enhancement via Color Naming",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Serrano-Lozano*",
      "Luis Herranz",
      "Michael S Brown",
      "Javier Vazquez-Corral"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8964_ECCV_2024_paper.php": {
    "title": "Synthesizing Time-varying BRDFs via Latent Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Takuto Narumoto*",
      "Hiroaki Santo",
      "Fumio Okura"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8965_ECCV_2024_paper.php": {
    "title": "HoloADMM: High-Quality Holographic Complex Field Recovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mazen Mel*",
      "Paul Springer",
      "Pietro Zanuttigh",
      "Haitao Zhou",
      "Alexander Gatto"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8966_ECCV_2024_paper.php": {
    "title": "Fundamental Matrix Estimation Using Relative Depths",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaqing Ding*",
      "Václav Vávra",
      "Snehal Bhayani",
      "Qianliang Wu",
      "Jian Yang",
      "Zuzana Kukelova"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8967_ECCV_2024_paper.php": {
    "title": "Gaussian Splatting on the Move: Blur and Rolling Shutter Compensation for Natural Camera Motion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Otto Seiskari*",
      "Jerry Ylilammi",
      "Valtteri Kaatrasalo",
      "Pekka Rantalankila",
      "Matias Turkulainen",
      "Juho Kannala",
      "Esa Rahtu",
      "Arno Solin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8968_ECCV_2024_paper.php": {
    "title": "MTaDCS: Moving Trace and Feature Density-based Confidence Sample Selection under Label Noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingzheng Huang",
      "Xilin He",
      "Xiaole Xian",
      "Qinliang Lin",
      "Weicheng Xie*",
      "Siyang Song",
      "Linlin Shen",
      "Zitong Yu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8972_ECCV_2024_paper.php": {
    "title": "Towards Open-World Object-based Anomaly Detection via Self-Supervised Outlier Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brian Kostadinov Shalon Isaac-Medina*",
      "Yona Falinie Abdul Gaus*",
      "Neelanjan Bhowmik",
      "Toby P Breckon"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8974_ECCV_2024_paper.php": {
    "title": "GroundUp: Rapid Sketch-Based 3D City Massing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gizem Esra Unlu*",
      "Mohamed Sayed",
      "Yulia Gryaditskaya",
      "Gabriel Brostow"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8987_ECCV_2024_paper.php": {
    "title": "Guide-and-Rescale: Self-Guidance Mechanism for Effective Tuning-Free Real Image Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vadim Titov*",
      "Madina Khalmatova*",
      "Alexandra Ivanova*",
      "Dmitry P Vetrov",
      "Aibek Alanov*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8991_ECCV_2024_paper.php": {
    "title": "DataDream: Few-shot Guided Dataset Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jae Myung Kim*",
      "Jessica Bader",
      "Stephan Alaniz",
      "Cordelia Schmid",
      "Zeynep Akata"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8994_ECCV_2024_paper.php": {
    "title": "LPViT: Low-Power Semi-structured Pruning for Vision Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaixin Xu*",
      "Zhe Wang*",
      "Chunyun Chen",
      "Xue Geng",
      "Jie Lin",
      "Xulei Yang",
      "Min Wu*",
      "Xiaoli Li",
      "Weisi Lin*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8996_ECCV_2024_paper.php": {
    "title": "CipherDM: Secure Three-Party Inference for Diffusion Model Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Zhao",
      "Xiaojun Chen*",
      "Xudong Chen",
      "He Li",
      "Tingyu Fan",
      "Zhendong Zhao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9002_ECCV_2024_paper.php": {
    "title": "Weighted Ensemble Models Are Strong Continual Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Imad Eddine MAROUF*",
      "Subhankar Roy",
      "Enzo Tartaglione",
      "Stéphane Lathuilière"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9006_ECCV_2024_paper.php": {
    "title": "GGRt: Towards Generalizable 3D Gaussians without Pose Priors in Real-Time",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Li",
      "Yuanyuan Gao",
      "Dingwen Zhang*",
      "Chenming Wu",
      "YALUN DAI",
      "Chen Zhao",
      "Haocheng Feng",
      "Errui Ding",
      "Jingdong Wang",
      "Junwei Han"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9009_ECCV_2024_paper.php": {
    "title": "A Unified Image Compression Method for Human Perception and Multiple Vision Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sha Guo",
      "Lin Sui",
      "Chen-Lin Zhang",
      "Zhuo Chen",
      "Wenhan Yang",
      "Lingyu Duan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9015_ECCV_2024_paper.php": {
    "title": "UniVoxel: Fast Inverse Rendering by Unified Voxelization of Scene Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuang Wu",
      "Songlin Tang",
      "Guangming Lu",
      "Jianzhuang Liu",
      "Wenjie Pei*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9025_ECCV_2024_paper.php": {
    "title": "Audio-visual Generalized Zero-shot Learning the Easy Way",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shentong Mo*",
      "Pedro Morgado"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9027_ECCV_2024_paper.php": {
    "title": "PartImageNet++ Dataset: Scaling up Part-based Models for Robust Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Li*",
      "Yining Liu",
      "Na Dong",
      "Sitian Qin",
      "Xiaolin Hu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9030_ECCV_2024_paper.php": {
    "title": "Learning Equilibrium Transformation for Gamut Expansion and Color Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Xiao*",
      "Changjian Shui",
      "Zhi-Song Liu",
      "Qian Ye",
      "Kin-Man Lam"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9032_ECCV_2024_paper.php": {
    "title": "Dyn-Adapter: Towards Disentangled Representation for Efficient Visual Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yurong Zhang*",
      "Honghao Chen",
      "Zhang Xinyu",
      "Xiangxiang Chu",
      "Li Song"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9034_ECCV_2024_paper.php": {
    "title": "Physics-informed Knowledge Transfer for Underwater Monocular Depth Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinghe Yang*",
      "Mingming Gong",
      "Ye Pu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9049_ECCV_2024_paper.php": {
    "title": "Robust Nearest Neighbors for Source-Free Domain Adaptation under Class Distribution Shift",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antonio Tejero-de-Pablos*",
      "Riku Togashi",
      "Mayu Otani",
      "Shin'ichi Satoh"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9056_ECCV_2024_paper.php": {
    "title": "Chains of Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanheng Wei*",
      "Lianghua Huang*",
      "Zhi-Fan Wu",
      "Wei Wang",
      "Yu Liu",
      "Mingda Jia",
      "Shuailei Ma"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9070_ECCV_2024_paper.php": {
    "title": "Time-Efficient and Identity-Consistent Virtual Try-On Using A Variant of Altered Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Phuong Hoang Dam*",
      "Jihoon Jeong*",
      "Anh T Tran*",
      "Daeyoung Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9073_ECCV_2024_paper.php": {
    "title": "Feature Diversification and Adaptation for Federated Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seunghan Yang*",
      "Seokeon Choi",
      "Hyunsin Park",
      "Sungha Choi",
      "Simyung Chang",
      "Sungrack Yun"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9080_ECCV_2024_paper.php": {
    "title": "Grounding Image Matching in 3D with MASt3R",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vincent Leroy*",
      "Yohann Cabon",
      "Jerome Revaud"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9082_ECCV_2024_paper.php": {
    "title": "TP2O: Creative Text Pair-to-Object Generation using Balance Swap-Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Li*",
      "Zedong Zhang",
      "Jian Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9088_ECCV_2024_paper.php": {
    "title": "RoDUS: Robust Decomposition of Static and Dynamic Elements in Urban Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thang-Anh-Quan Nguyen*",
      "Luis G Roldao Jimenez*",
      "Nathan Piasco*",
      "Moussab Bennehar*",
      "Dzmitry Tsishkou*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9090_ECCV_2024_paper.php": {
    "title": "RecurrentBEV: A Long-term Temporal Fusion Framework for Multi-view 3D Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ming Chang",
      "Xishan Zhang*",
      "Rui Zhang",
      "Zhipeng Zhao",
      "Guanhua He",
      "Shaoli Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9097_ECCV_2024_paper.php": {
    "title": "Efficient Bias Mitigation Without Privileged Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mateo Espinosa Zarlenga*",
      "Swami Sankaranarayanan",
      "Jerone T. A. Andrews",
      "Zohreh Shams",
      "Mateja Jamnik",
      "Alice Xiang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9103_ECCV_2024_paper.php": {
    "title": "MC-PanDA: Mask Confidence for Panoptic Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ivan Martinović*",
      "Josip Šarić",
      "Siniša Šegvić"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9107_ECCV_2024_paper.php": {
    "title": "Learning Neural Deformation Representation for 4D Dynamic Shape Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gyojin Han*",
      "Jiwan Hur",
      "Jaehyun Choi",
      "Junmo Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9110_ECCV_2024_paper.php": {
    "title": "Dynamic Guidance Adversarial Distillation with Enhanced Teacher Knowledge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyejin Park",
      "Dongbo Min*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9112_ECCV_2024_paper.php": {
    "title": "Decomposition Betters Tracking Everything Everywhere",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Li",
      "Dong Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9113_ECCV_2024_paper.php": {
    "title": "Straightforward Layer-wise Pruning for More Efficient Visual Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruizi Han*",
      "Jinglei Tang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9116_ECCV_2024_paper.php": {
    "title": "Synchronization is All You Need: Exocentric-to-Egocentric Transfer for Temporal Action Segmentation with Unlabeled Synchronized Video Pairs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Camillo Quattrocchi*",
      "Antonino Furnari",
      "Daniele Di Mauro",
      "Mario Valerio Giuffrida",
      "Giovanni Maria Farinella"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9117_ECCV_2024_paper.php": {
    "title": "LAPT: Label-driven Automated Prompt Tuning for OOD Detection with Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yabin Zhang*",
      "Wenjie Zhu",
      "Chenhang He",
      "Lei Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9119_ECCV_2024_paper.php": {
    "title": "Domain Shifting: A Generalized Solution for Heterogeneous Cross-Modality Person Re-Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Jiang",
      "Xu Cheng*",
      "Hao Yu",
      "Xingyu Liu",
      "Haoyu Chen",
      "Guoying Zhao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9123_ECCV_2024_paper.php": {
    "title": "Self-Supervised Video Desmoking for Laparoscopic Surgery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Renlong Wu",
      "Zhilu Zhang*",
      "Shuohao Zhang",
      "Longfei Gou",
      "Haobin Chen",
      "Lei Zhang",
      "Hao Chen*",
      "Wangmeng Zuo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9133_ECCV_2024_paper.php": {
    "title": "Removing Rows and Columns of Tokens in Vision Transformer enables Faster Dense Prediction without Retraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Diwei Su",
      "cheng fei",
      "Jianxu Luo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9140_ECCV_2024_paper.php": {
    "title": "Continuity Preserving Online CenterLine Graph Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunhui Han",
      "Kun Yu",
      "Zhiwei Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9144_ECCV_2024_paper.php": {
    "title": "Decomposition of Neural Discrete Representations for Large-Scale 3D Mapping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minseong Park",
      "Suhan Woo",
      "Euntai Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9159_ECCV_2024_paper.php": {
    "title": "MirrorGaussian: Reflecting 3D Gaussians for Reconstructing Mirror Reflections",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayue Liu",
      "Xiao Tang",
      "Freeman Cheng",
      "Zihao Yang",
      "Zhihao Li*",
      "Jianzhuang Liu",
      "Yi Huang",
      "Jiaqi Lin",
      "Shiyong Liu",
      "Xiaofei Wu",
      "Songcen Xu",
      "Chun Yuan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9160_ECCV_2024_paper.php": {
    "title": "Leveraging Representations from Intermediate Encoder-blocks for Synthetic Image Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christos Koutlis*",
      "Symeon Papadopoulos"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9164_ECCV_2024_paper.php": {
    "title": "Exploring Vulnerabilities in Spiking Neural Networks: Direct Adversarial Attacks on Raw Event Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanmeng Yao",
      "Xiaohan Zhao",
      "Bin Gu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9167_ECCV_2024_paper.php": {
    "title": "HSR: Holistic 3D Human-Scene Reconstruction from Monocular Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lixin Xue*",
      "Chen Guo",
      "Chengwei Zheng",
      "Fangjinhua Wang",
      "Tianjian Jiang",
      "Hsuan-I Ho",
      "Manuel Kaufmann",
      "Jie Song",
      "Otmar Hilliges"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9175_ECCV_2024_paper.php": {
    "title": "Online Video Quality Enhancement with Spatial-Temporal Look-up Tables",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zefan Qu",
      "Xinyang Jiang*",
      "Yifan Yang",
      "Dongsheng Li",
      "Cairong Zhao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9180_ECCV_2024_paper.php": {
    "title": "PARIS3D: Reasoning-based 3D Part Segmentation Using Large Multimodal Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amrin Kareem*",
      "Jean Lahoud",
      "Hisham Cholakkal*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9184_ECCV_2024_paper.php": {
    "title": "Self-Rectifying Diffusion Sampling with Perturbed-Attention Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Donghoon Ahn",
      "Hyoungwon Cho",
      "Jaewon Min",
      "Jungwoo Kim",
      "Wooseok Jang",
      "SeonHwa Kim",
      "Hyun Hee Park",
      "Kyong Hwan Jin*",
      "Seungryong Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9199_ECCV_2024_paper.php": {
    "title": "Localization and Expansion: A Decoupled Framework for Point Cloud Few-shot Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoyang Li*",
      "Yuan Wang",
      "Wangkai Li",
      "Rui Sun",
      "Tianzhu Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9204_ECCV_2024_paper.php": {
    "title": "Think before Placement: Common Sense Enhanced Transformer for Object Placement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaxuan Qin",
      "Jiayu Xu",
      "Ruiping Wang*",
      "Xilin Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9215_ECCV_2024_paper.php": {
    "title": "Oulu Remote-photoplethysmography Physical Domain Attacks Database (ORPDAD)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marko Savic",
      "Guoying Zhao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9216_ECCV_2024_paper.php": {
    "title": "Leveraging Imperfect Restoration for Data Availability Attack",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "YI HUANG*",
      "Jeremy Styborski*",
      "Mingzhi Lyu*",
      "Fan Wang*",
      "Wai-Kin Adams Kong*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9223_ECCV_2024_paper.php": {
    "title": "3D Weakly Supervised Semantic Segmentation with 2D Vision-Language Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoxu Xu",
      "Yitian Yuan",
      "Jinlong Li",
      "Qiudan Zhang",
      "Zequn Jie",
      "Lin Ma",
      "Hao Tang",
      "Nicu Sebe",
      "Xu Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9228_ECCV_2024_paper.php": {
    "title": "Open-set Domain Adaptation via Joint Error based Multi-class Positive and Unlabeled Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dexuan Zhang*",
      "Thomas Westfechtel",
      "Tatsuya Harada"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9231_ECCV_2024_paper.php": {
    "title": "DoubleTake: Geometry Guided Depth Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohamed Sayed*",
      "Filippo Aleotti",
      "Jamie Watson",
      "Zawar Qureshi",
      "Guillermo Garcia-Hernando",
      "Gabriel Brostow",
      "Sara Vicente",
      "Michael Firman"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9241_ECCV_2024_paper.php": {
    "title": "Empowering Embodied Visual Tracking with Visual Foundation Models and Offline RL",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangwei Zhong*",
      "Kui Wu",
      "Hai Ci",
      "Chu-ran Wang",
      "Hao Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9243_ECCV_2024_paper.php": {
    "title": "Street Gaussians: Modeling Dynamic Urban Scenes with Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunzhi Yan*",
      "Haotong Lin",
      "Chenxu Zhou",
      "Weijie Wang",
      "Haiyang Sun",
      "Kun Zhan",
      "Xianpeng Lang",
      "Xiaowei Zhou",
      "Sida Peng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9265_ECCV_2024_paper.php": {
    "title": "Images are Achilles' Heel of Alignment: Exploiting Visual Vulnerabilities for Jailbreaking Multimodal Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Li*",
      "hangyu guo",
      "Kun Zhou",
      "Wayne Xin Zhao",
      "Ji-Rong Wen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9268_ECCV_2024_paper.php": {
    "title": "Edge-Guided Fusion and Motion Augmentation for Event-Image Stereo",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fengan Zhao*",
      "Qianang Zhou",
      "Junlin Xiong*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9270_ECCV_2024_paper.php": {
    "title": "MetaWeather: Few-Shot Weather-Degraded Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youngrae Kim*",
      "Younggeol Cho",
      "Thanh-Tung Nguyen",
      "Seunghoon Hong",
      "Dongman Lee*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9272_ECCV_2024_paper.php": {
    "title": "CPT-VR: Improving Surface Rendering via Closest Point Transform with View-Reflection Appearance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhipeng Hu",
      "Yongqiang Zhang*",
      "Chen Liu",
      "Lincheng Li*",
      "Sida Peng",
      "Xiaowei Zhou",
      "Changjie Fan",
      "Xin Yu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9275_ECCV_2024_paper.php": {
    "title": "Close, But Not There: Boosting Geographic Distance Sensitivity in Visual Place Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sergio Izquierdo*",
      "Javier Civera*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9283_ECCV_2024_paper.php": {
    "title": "HiFi-123: Towards High-fidelity One Image to 3D Content Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wangbo Yu*",
      "Li Yuan",
      "Yan-Pei Cao",
      "Xiangjun Gao",
      "Xiaoyu Li",
      "Wenbo Hu",
      "Long Quan",
      "Ying Shan",
      "Yonghong Tian"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9285_ECCV_2024_paper.php": {
    "title": "Revisiting Adaptive Cellular Recognition Under Domain Shifts: A Contextual Correspondence View",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianan Fan*",
      "Dongnan Liu",
      "Canran Li",
      "Hang Chang",
      "Heng Huang",
      "Filip Braet",
      "Mei Chen",
      "Weidong Cai*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9286_ECCV_2024_paper.php": {
    "title": "Good Teachers Explain: Explanation-Enhanced Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amin Parchami-Araghi*",
      "Moritz Böhle",
      "Sukrut Rao",
      "Bernt Schiele"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9290_ECCV_2024_paper.php": {
    "title": "Stepping Stones: A Progressive Training Strategy for Audio-Visual Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juncheng Ma",
      "Peiwen Sun",
      "Yaoting Wang",
      "Di Hu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9294_ECCV_2024_paper.php": {
    "title": "FRDiff : Feature Reuse for Universal Training-free Acceleration of Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junhyuk So",
      "Jungwon Lee",
      "Eunhyeok Park*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9297_ECCV_2024_paper.php": {
    "title": "Möbius Transform for Mitigating Perspective Distortions in Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prakash Chandra Chhipa*",
      "Meenakshi Subhash Chippa",
      "Kanjar De",
      "Rajkumar Saini",
      "Marcus Liwicki",
      "Mubarak Shah"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9304_ECCV_2024_paper.php": {
    "title": "TAG: Text Prompt Augmentation for Zero-Shot Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xixi Liu*",
      "Christopher Zach"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9336_ECCV_2024_paper.php": {
    "title": "CVT-Occ: Cost Volume Temporal Fusion for 3D Occupancy Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhangchen Ye",
      "Tao Jiang",
      "Chenfeng Xu",
      "Yiming Li",
      "Hang Zhao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9344_ECCV_2024_paper.php": {
    "title": "SPVLoc: Semantic Panoramic Viewport Matching for 6D Camera Localization in Unseen Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Niklas Gard*",
      "Anna Hilsmann",
      "Peter Eisert"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9349_ECCV_2024_paper.php": {
    "title": "Continual Learning and Unknown Object Discovery in 3D Scenes via Self-Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohamed El Amine Boudjoghra*",
      "Jean Lahoud",
      "Salman Khan",
      "Hisham Cholakkal",
      "Rao M Anwer",
      "Fahad Shahbaz Khan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9350_ECCV_2024_paper.php": {
    "title": "DiffCD: A Symmetric Differentiable Chamfer Distance for Neural Implicit Surface Fitting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linus Härenstam-Nielsen*",
      "Lu Sang",
      "Abhishek Saroha",
      "Nikita Araslanov*",
      "Daniel Cremers*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9351_ECCV_2024_paper.php": {
    "title": "Lost and Found: Overcoming Detector Failures in Online Multi-Object Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lorenzo Vaquero*",
      "Yihong Xu",
      "Xavier Alameda-Pineda",
      "Victor M. Brea",
      "Manuel Mucientes"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9354_ECCV_2024_paper.php": {
    "title": "Local Occupancy-Enhanced Object Grasping with Multiple Triplanar Projection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kangqi Ma*",
      "Hao Dong",
      "Yadong Mu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9359_ECCV_2024_paper.php": {
    "title": "Region-Native Visual Tokenization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengyu Wang*",
      "Yuyao Huang",
      "Henghui Ding",
      "Xinlong Wang",
      "Tiejun Huang",
      "Yao Zhao",
      "Yunchao Wei",
      "Shuicheng Yan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9361_ECCV_2024_paper.php": {
    "title": "SparseCraft: Few-Shot Neural Reconstruction through Stereopsis Guided Geometric Linearization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mae Younes*",
      "Amine Ouasfi",
      "Adnane Boukhayma"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9384_ECCV_2024_paper.php": {
    "title": "Sketch2Vox: Learning 3D Reconstruction from a Single Monocular Sketch Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fei Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9386_ECCV_2024_paper.php": {
    "title": "DGE: Direct Gaussian 3D Editing by Consistent Multi-view Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minghao Chen*",
      "Iro Laina",
      "Andrea Vedaldi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9390_ECCV_2024_paper.php": {
    "title": "The Lottery Ticket Hypothesis in Denoising: Towards Semantic-Driven Initialization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiafeng Mao*",
      "Xueting Wang",
      "Kiyoharu Aizawa"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9395_ECCV_2024_paper.php": {
    "title": "Diffusion for Out-of-Distribution Detection on Road Scenes and Beyond",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Silvio Galesso*",
      "Philipp Schröppel*",
      "Hssan Driss",
      "Thomas Brox"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9398_ECCV_2024_paper.php": {
    "title": "Rethinking Directional Parameterization in Neural Implicit Surface Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijie Jiang*",
      "Tianhan Xu*",
      "Hiroharu Kato"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9400_ECCV_2024_paper.php": {
    "title": "A Comprehensive Study of Multimodal Large Language Models for Image Quality Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianhe Wu",
      "Kede Ma*",
      "Jie Liang",
      "Yujiu Yang*",
      "Lei Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9402_ECCV_2024_paper.php": {
    "title": "Semi-Supervised Teacher-Reference-Student Architecture for Action Quality Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wulian Yun",
      "Mengshi Qi",
      "Fei Peng",
      "Huadong Ma*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9437_ECCV_2024_paper.php": {
    "title": "Efficient Neural Video Representation with Temporally Coherent Modulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seungjun Shin*",
      "Suji Kim*",
      "Dokwan Oh"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9443_ECCV_2024_paper.php": {
    "title": "Ref-AVS: Refer and Segment Objects in Audio-Visual Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaoting Wang",
      "Peiwen Sun",
      "Dongzhan Zhou",
      "Guangyao Li",
      "Honggang Zhang",
      "Di Hu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9444_ECCV_2024_paper.php": {
    "title": "DreamScene: 3D Gaussian-based Text-to-3D Scene Generation via Formation Pattern Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoran Li",
      "Haolin Shi",
      "Wenli Zhang",
      "Wenjun Wu",
      "Yong Liao*",
      "Lin Wang",
      "Lik-Hang Lee",
      "Peng Yuan Zhou*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9445_ECCV_2024_paper.php": {
    "title": "Multi-modal Crowd Counting via a Broker Modality",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoliang Meng",
      "Xiaopeng Hong*",
      "Chenhao Wang",
      "Miao Shang",
      "Wangmeng Zuo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9447_ECCV_2024_paper.php": {
    "title": "FastPCI: Motion-Structure Guided Fast Point Cloud Frame Interpolation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "tianyu zhang",
      "Guocheng Qian",
      "Jin Xie*",
      "Jian Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9454_ECCV_2024_paper.php": {
    "title": "Made to Order: Discovering monotonic temporal changes via self-supervised video ordering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Charig Yang*",
      "Weidi Xie",
      "Andrew Zisserman"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9456_ECCV_2024_paper.php": {
    "title": "PARE-Net: Position-Aware Rotation-Equivariant Networks for Robust Point Cloud Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runzhao Yao",
      "Shaoyi Du*",
      "Wenting Cui",
      "Canhui Tang",
      "Chengwu Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9460_ECCV_2024_paper.php": {
    "title": "Open-Vocabulary RGB-Thermal Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "GuoQiang Zhao",
      "JunJie Huang",
      "Xiaoyun Yan*",
      "Zhaojing Wang",
      "Junwei Tang",
      "Yangjun Ou",
      "Xinrong Hu",
      "Tao Peng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9464_ECCV_2024_paper.php": {
    "title": "MeshVPR: Citywide Visual Place Recognition Using 3D Meshes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gabriele Berton*",
      "Lorenz Junglas",
      "Riccardo Zaccone",
      "Thomas Pollok",
      "Barbara Caputo",
      "Carlo Masone"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9465_ECCV_2024_paper.php": {
    "title": "Can Textual Semantics Mitigate Sounding Object Segmentation Preference?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaoting Wang",
      "Peiwen Sun",
      "Yuanchao Li",
      "Honggang Zhang",
      "Di Hu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9469_ECCV_2024_paper.php": {
    "title": "Concise Plane Arrangements for Low-Poly Surface and Volume Modelling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raphael Sulzer",
      "Florent Lafarge*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9481_ECCV_2024_paper.php": {
    "title": "KeypointDETR: An End-to-End 3D Keypoint Detector",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hairong Jin",
      "Yuefan Shen",
      "Jianwen Lou",
      "Kun Zhou",
      "Youyi Zheng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9492_ECCV_2024_paper.php": {
    "title": "ViPer: Visual Personalization of Generative Models via Individual Preference Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sogand Salehi*",
      "Mahdi Shafiei",
      "Roman Bachmann",
      "Teresa Yeo",
      "Amir Zamir"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9503_ECCV_2024_paper.php": {
    "title": "MLPHand: Real Time Multi-View 3D Hand Reconstruction via MLP Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Yang",
      "Jiakun Li",
      "Guoming Li",
      "Huaiyu Wu",
      "Zhen Shen",
      "Zhaoxin Fan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9508_ECCV_2024_paper.php": {
    "title": "uCAP: An Unsupervised Prompting Method for Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "A. Tuan Nguyen*",
      "Kai Sheng Tai",
      "Bor-Chun Chen",
      "Satya Narayan Shukla",
      "Hanchao Yu",
      "Philip Torr",
      "Tai-Peng Tian",
      "Ser-Nam Lim"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9511_ECCV_2024_paper.php": {
    "title": "LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dilxat Muhtar",
      "Zhenshi Li",
      "Feng Gu",
      "Xueliang Zhang*",
      "Pengfeng Xiao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9512_ECCV_2024_paper.php": {
    "title": "How Far Can a 1-Pixel Camera Go? Solving Vision Tasks using Photoreceptors and Computationally Designed Visual Morphology",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrei Atanov*",
      "Rishubh Singh",
      "Jiawei Fu",
      "Isabella Yu",
      "Andrew Spielberg",
      "Amir Zamir"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9513_ECCV_2024_paper.php": {
    "title": "MONTAGE: Monitoring Training for Attribution of Generative Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonathan Brokman*",
      "Omer Hofman",
      "Roman Vainshtein",
      "Amit Giloni",
      "Toshiya Shimizu",
      "Inderjeet Singh",
      "Oren Rachmil",
      "Alon Zolfi",
      "Asaf Shabtai",
      "Yuki Unno",
      "Hisashi Kojima"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9516_ECCV_2024_paper.php": {
    "title": "Affective Visual Dialog: A Large-Scale Benchmark for Emotional Reasoning Based on Visually Grounded Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kilichbek Haydarov*",
      "Xiaoqian Shen",
      "Avinash Madasu",
      "Mahmoud Salem",
      "Li-Jia Li",
      "Gamaleldin F Elsayed",
      "Mohamed Elhoseiny"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9518_ECCV_2024_paper.php": {
    "title": "Watching it in Dark: A Target-aware Representation Learning Framework for High-Level Vision Tasks in Low Illumination",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunan Li*",
      "Yihao Zhang",
      "Shoude Li",
      "Long Tian",
      "DOU QUAN",
      "Chaoneng Li",
      "Qiguang Miao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9526_ECCV_2024_paper.php": {
    "title": "Self-supervised visual learning from interactions with objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arthur Aubret*",
      "Céline Teulière",
      "Jochen Triesch"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9531_ECCV_2024_paper.php": {
    "title": "OP-Align: Object-level and Part-level Alignment for Self-supervised Category-level Articulated Object Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchen Che*",
      "Ryo Furukawa",
      "Asako Kanezaki"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9553_ECCV_2024_paper.php": {
    "title": "BAFFLE: A Baseline of Backpropagation-Free Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haozhe Feng*",
      "Tianyu Pang*",
      "Chao Du",
      "Wei Chen*",
      "Shuicheng Yan",
      "Min Lin"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9557_ECCV_2024_paper.php": {
    "title": "Sequential Representation Learning via Static-Dynamic Conditional Disentanglement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mathieu Cyrille Simon*",
      "Pascal Frossard",
      "Christophe De Vleeschouwer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9562_ECCV_2024_paper.php": {
    "title": "OmniNOCS: A unified NOCS dataset and model for 3D lifting of 2D objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akshay Krishnan*",
      "Abhijit Kundu*",
      "Kevis-Kokitsi Maninis",
      "James Hays",
      "Matthew Brown"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9564_ECCV_2024_paper.php": {
    "title": "3R-INN: How to be climate friendly while consuming/delivering videos?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "ZOUBIDA AMEUR*",
      "Claire-Helene Demarty",
      "Olivier LE MEUR",
      "Daniel Menard"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9565_ECCV_2024_paper.php": {
    "title": "Rethinking Deep Unrolled Model for Accelerated MRI Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bingyu Xin*",
      "Meng Ye",
      "Leon Axel",
      "Dimitris N. Metaxas"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9567_ECCV_2024_paper.php": {
    "title": "Towards Robust Full Low-bit Quantization of Super Resolution Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Denis S. Makhov*",
      "Irina Zhelavskaya",
      "Ruslan Ostapets",
      "Dehua Song",
      "Kirill Solodskikh"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9574_ECCV_2024_paper.php": {
    "title": "Omni6DPose: A Benchmark and Model for Universal 6D Object Pose Estimation and Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiyao Zhang",
      "Weiyao Huang",
      "Bo Peng",
      "Mingdong Wu",
      "Fei Hu",
      "Zijian Chen",
      "Bo Zhao",
      "Hao Dong*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9575_ECCV_2024_paper.php": {
    "title": "Diverse Text-to-3D Synthesis with Augmented Text Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Uy Dieu Tran*",
      "Minh N. Hoang Luu*",
      "Phong Ha Nguyen*",
      "Khoi Nguyen*",
      "Binh-Son Hua*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9583_ECCV_2024_paper.php": {
    "title": "Style-Extracting Diffusion Models for Semi-Supervised Histopathology Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mathias Öttl*",
      "Frauke Wilm",
      "Jana Steenpass",
      "Jingna Qiu",
      "Matthias Rübner",
      "Prof Arndt Hartmann",
      "Matthias W. Beckmann",
      "Peter Fasching",
      "Andreas K Maier",
      "Ramona Erber",
      "Bernhard Kainz",
      "Katharina Breininger"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9585_ECCV_2024_paper.php": {
    "title": "LLMCO4MR: LLMs-aided Neural Combinatorial Optimization for Ancient Manuscript Restoration from Fragments with Case Studies on Dunhuang",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuqing Zhang",
      "Hangqi Li",
      "Shengyu Zhang*",
      "Runzhong Wang",
      "Baoyi He",
      "Huaiyong Dou",
      "Junchi Yan*",
      "Yongquan Zhang",
      "Fei Wu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9596_ECCV_2024_paper.php": {
    "title": "Model Breadcrumbs: Scaling Multi-Task Model Merging with Sparse Masks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "MohammadReza Davari*",
      "Eugene Belilovsky"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9603_ECCV_2024_paper.php": {
    "title": "AdversariaLeak: External Information Leakage Attack Using Adversarial Samples on Face Recognition Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roye Katzav*",
      "Amit Giloni",
      "Edita Grolman*",
      "Hiroo Saito",
      "Tomoyuki Shibata",
      "Tsukasa Omino",
      "Misaki Komatsu",
      "Yoshikazu Hanatani",
      "Yuval Elovici",
      "Asaf Shabtai"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9607_ECCV_2024_paper.php": {
    "title": "iHuman: Instant Animatable Digital Humans From Monocular Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pramish Paudel*",
      "Anubhav Khanal",
      "Danda Pani Paudel",
      "Jyoti Tandukar",
      "Ajad Chhatkuli"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9612_ECCV_2024_paper.php": {
    "title": "SphereHead: Stable 3D Full-head Synthesis with Spherical Tri-plane Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heyuan Li*",
      "Ce Chen",
      "Tianhao Shi",
      "Yuda Qiu",
      "Sizhe An",
      "Guanying CHEN",
      "Xiaoguang Han*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9617_ECCV_2024_paper.php": {
    "title": "Beyond Pixels: Semi-Supervised Semantic Segmentation with a Multi-scale Patch-based Multi-Label Classifier",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prantik Howlader*",
      "Srijan Das",
      "Hieu Le",
      "Dimitris Samaras"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9622_ECCV_2024_paper.php": {
    "title": "Glyph-ByT5: A Customized Text Encoder for Accurate Visual Text Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyu Liu",
      "Weicong Liang",
      "Zhanhao Liang",
      "Chong Luo",
      "Ji Li",
      "Gao Huang",
      "Yuhui Yuan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9626_ECCV_2024_paper.php": {
    "title": "Solving the inverse problem of microscopy deconvolution with a residual Beylkin-Coifman-Rokhlin neural network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Li",
      "Mikhail Kudryashev",
      "Artur Yakimovich*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9627_ECCV_2024_paper.php": {
    "title": "Face Reconstruction Transfer Attack as Out-of-Distribution Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yoon Gyo Jung*",
      "Jaewoo Park",
      "Xingbo Dong",
      "Hojin Park",
      "Andrew Beng Jin Teoh",
      "Octavia Camps*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9634_ECCV_2024_paper.php": {
    "title": "FreeZe: Training-free zero-shot 6D pose estimation with geometric and vision foundation models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrea Caraffa*",
      "Davide Boscaini",
      "Amir Hamza",
      "Fabio Poiesi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9637_ECCV_2024_paper.php": {
    "title": "Deep Diffusion Image Prior for Efficient OOD Adaptation in 3D Inverse Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyungjin Chung",
      "Jong Chul Ye*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9643_ECCV_2024_paper.php": {
    "title": "Weighting Pseudo-Labels via High-Activation Feature Index Similarity and Object Detection for Semi-Supervised Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prantik Howlader*",
      "Hieu Le",
      "Dimitris Samaras"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9666_ECCV_2024_paper.php": {
    "title": "PartGLEE: A Foundation Model for Recognizing and Parsing Any Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyi Li",
      "Junfeng Wu",
      "Weizhi Zhao",
      "Song Bai",
      "Xiang Bai*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9667_ECCV_2024_paper.php": {
    "title": "WTS: A Pedestrian-Centric Traffic Video Dataset for Fine-grained Spatial-Temporal Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quan Kong*",
      "Yuki Kawana",
      "Rajat Saini",
      "Ashutosh Kumar",
      "Jingjing Pan",
      "Ta Gu",
      "Yohei Ozao",
      "Balazs Opra",
      "Yoichi Sato",
      "Norimasa Kobori"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9680_ECCV_2024_paper.php": {
    "title": "Spiking Wavelet Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuetong Fang",
      "Ziqing Wang",
      "Lingfeng Zhang",
      "Jiahang Cao",
      "Honglei Chen",
      "Renjing Xu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9682_ECCV_2024_paper.php": {
    "title": "WAVE: Warping DDIM Inversion Features for Zero-shot Text-to-Video Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yutang Feng",
      "Sicheng Gao*",
      "Yuxiang Bao",
      "Xiaodi Wang",
      "Shumin Han*",
      "Juan Zhang*",
      "Baochang Zhang",
      "Angela Yao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9702_ECCV_2024_paper.php": {
    "title": "PDT Uav Target Detection Dataset for Pests and Diseases Tree",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingle Zhou",
      "Rui Xing",
      "Delong Han",
      "Zhiyong Qi",
      "Gang Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9712_ECCV_2024_paper.php": {
    "title": "Hypernetworks for Generalizable BRDF Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fazilet Gokbudak*",
      "Alejandro Sztrajman",
      "Chenliang Zhou",
      "Fangcheng Zhong",
      "Rafal Mantiuk",
      "A. Cengiz Oztireli"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9716_ECCV_2024_paper.php": {
    "title": "Photon Inhibition for Energy-Efficient Single-Photon Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucas J Koerner*",
      "Shantanu Gupta",
      "Atul N Ingle",
      "Mohit Gupta"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9729_ECCV_2024_paper.php": {
    "title": "COD: Learning Conditional Invariant Representation for Domain Adaptation Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao-Ran Yang",
      "Chuan-Xian Ren*",
      "You-Wei Luo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9733_ECCV_2024_paper.php": {
    "title": "RANRAC: Robust Neural Scene Representations via Random Ray Consensus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benno Buschmann*",
      "Andreea Dogaru",
      "Elmar Eisemann",
      "Michael Weinmann",
      "Bernhard Egger"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9737_ECCV_2024_paper.php": {
    "title": "LayerDiff: Exploring Text-guided Multi-layered Composable Image Synthesis via Layer-Collaborative Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runhui Huang",
      "Kaixin Cai",
      "Jianhua Han",
      "Xiaodan Liang*",
      "Renjing Pei",
      "Guansong Lu",
      "Songcen Xu",
      "Wei Zhang",
      "Hang Xu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9738_ECCV_2024_paper.php": {
    "title": "Characterizing Model Robustness via Natural Input Gradients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adrian Rodriguez-Munoz*",
      "Tongzhou Wang",
      "Antonio Torralba"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9748_ECCV_2024_paper.php": {
    "title": "UpFusion: Novel View Diffusion from Unposed Sparse View Observations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bharath Raj Nagoor Kani*",
      "Hsin-Ying Lee",
      "Sergey Tulyakov",
      "Shubham Tulsiani"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9749_ECCV_2024_paper.php": {
    "title": "Four Ways to Improve Verbo-visual Fusion for Dense 3D Visual Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ozan Unal*",
      "Christos Sakaridis",
      "Suman Saha",
      "Luc Van Gool"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9762_ECCV_2024_paper.php": {
    "title": "SIMBA: Split Inference - Mechanisms, Benchmarks and Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhishek Singh*",
      "Vivek Sharma",
      "Rohan Sukumaran",
      "John J Mose",
      "Jeffrey K Chiu",
      "Justin Yu",
      "Ramesh Raskar"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9769_ECCV_2024_paper.php": {
    "title": "Tuning-Free Image Customization with Image and Text Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengzhi Li",
      "Qiang Nie",
      "Ying Chen",
      "Xi Jiang",
      "Kai Wu",
      "Yuhuan Lin",
      "Yong Liu",
      "Jinlong Peng",
      "Chengjie Wang",
      "Feng Zheng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9771_ECCV_2024_paper.php": {
    "title": "FairDomain: Achieving Fairness in Cross-Domain Medical Image Segmentation and Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Tian*",
      "Congcong Wen",
      "Min Shi",
      "Muhammad Muneeb Afzal",
      "Hao Huang",
      "Muhammad Osama Khan",
      "Yan Luo",
      "Yi Fang",
      "Mengyu Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9774_ECCV_2024_paper.php": {
    "title": "Emerging Property of Masked Token for Effective Pre-training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyesong Choi",
      "Hunsang Lee",
      "Seyoung Joung",
      "Hyejin Park",
      "Jiyeong Kim",
      "Dongbo Min*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9775_ECCV_2024_paper.php": {
    "title": "DQ-DETR: DETR with Dynamic Query for Tiny Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi-Xin Huang*",
      "Hou-I Liu",
      "Hong-Han Shuai",
      "Wen-Huang Cheng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9800_ECCV_2024_paper.php": {
    "title": "Track2Act: Predicting Point Tracks from Internet Videos enables Generalizable Robot Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Homanga Bharadhwaj*",
      "Roozbeh Mottaghi",
      "Abhinav Gupta",
      "Shubham Tulsiani"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9801_ECCV_2024_paper.php": {
    "title": "SWAG: Splatting in the Wild images with Appearance-conditioned Gaussians",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hiba Dahmani*",
      "Moussab Bennehar",
      "Nathan Piasco",
      "Luis G Roldao Jimenez",
      "Dzmitry Tsishkou"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9802_ECCV_2024_paper.php": {
    "title": "Gaussian in the wild: 3D Gaussian Splatting for Unconstrained Image Collections",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongbin Zhang*",
      "Chuming Wang",
      "Weitao Wang",
      "Peihao Li",
      "Minghan Qin",
      "Haoqian Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9803_ECCV_2024_paper.php": {
    "title": "Few-shot Defect Image Generation based on Consistency Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingfeng Shi",
      "Jing Wei",
      "Fei Shen*",
      "Zhengtao Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9809_ECCV_2024_paper.php": {
    "title": "Taming CLIP for Fine-grained and Structured Visual Understanding of Museum Exhibits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ada-Astrid Balauca*",
      "Danda Pani Paudel",
      "Kristina Toutanova",
      "Luc Van Gool"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9810_ECCV_2024_paper.php": {
    "title": "CLIP-DPO: Vision-Language Models as a Source of Preference for Fixing Hallucinations in LVLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yassine Ouali*",
      "Adrian Bulat*",
      "Brais Martinez",
      "Georgios Tzimiropoulos"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9814_ECCV_2024_paper.php": {
    "title": "Masked Motion Prediction with Semantic Contrast for Point Cloud Sequence Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "yuehui han*",
      "Can Xu",
      "Rui Xu",
      "Jianjun Qian",
      "Jin Xie"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9815_ECCV_2024_paper.php": {
    "title": "Prompt-Based Test-Time Real Image Dehazing: A Novel Pipeline",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixuan Chen",
      "Zewei He*",
      "Ziqian Lu",
      "Xuecheng Sun",
      "Zheming Lu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9816_ECCV_2024_paper.php": {
    "title": "Video Editing via Factorized Diffusion Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Uriel Singer*",
      "Amit Zohar*",
      "Yuval Kirstain",
      "Shelly Sheynin",
      "Adam Polyak",
      "Devi Parikh",
      "Yaniv Taigman"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9819_ECCV_2024_paper.php": {
    "title": "Trackastra: Transformer-based cell tracking for live-cell microscopy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Gallusser",
      "Martin Weigert*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9827_ECCV_2024_paper.php": {
    "title": "CogView3: Finer and Faster Text-to-Image Generation via Relay Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wendi Zheng*",
      "Jiayan Teng",
      "Zhuoyi Yang",
      "Weihan Wang",
      "Jidong Chen",
      "Xiaotao Gu",
      "Yuxiao Dong*",
      "Ming Ding*",
      "Jie Tang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9828_ECCV_2024_paper.php": {
    "title": "SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nanye Ma*",
      "Mark Goldstein",
      "Michael Albergo",
      "Nicholas M Boffi",
      "Eric Vanden-Eijnden*",
      "Saining Xie*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9837_ECCV_2024_paper.php": {
    "title": "Learn to Memorize and to Forget: A Continual Learning Perspective of Dynamic SLAM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baicheng Li*",
      "Zike Yan*",
      "Dong Wu",
      "Hanqing Jiang",
      "Hongbin Zha*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9842_ECCV_2024_paper.php": {
    "title": "Forecasting Future Videos from Novel Views via Disentangled 3D Scene Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sudhir Yarram*",
      "Junsong Yuan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9846_ECCV_2024_paper.php": {
    "title": "GMM-IKRS: Gaussian Mixture Models for Interpretable Keypoint Refinement and Scoring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emanuele Santellani*",
      "Martin Zach",
      "Christian Sormann",
      "Mattia Rossi",
      "Andreas Kuhn",
      "Friedrich Fraundorfer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9856_ECCV_2024_paper.php": {
    "title": "Get Your Embedding Space in Order: Domain-Adaptive Regression for Forest Monitoring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sizhuo Li",
      "Dimitri Gominski*",
      "Martin Brandt",
      "Xiaoye Tong",
      "Philippe Ciais"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9857_ECCV_2024_paper.php": {
    "title": "ObjectDrop: Bootstrapping Counterfactuals for Photorealistic Object Removal and Insertion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Winter*",
      "Matan Cohen",
      "Shlomi Fruchter",
      "Yael Pritch",
      "Alex Rav-Acha",
      "Yedid Hoshen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9859_ECCV_2024_paper.php": {
    "title": "CoDA: Instructive Chain-of-Domain Adaptation with Severity-Aware Visual Prompt Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "ZiYang Gong",
      "FuHao Li",
      "Yupeng Deng",
      "Deblina Bhattacharjee",
      "Xianzheng Ma*",
      "Xiangwei Zhu*",
      "Zhenming Ji*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9862_ECCV_2024_paper.php": {
    "title": "Curved Diffusion: A Generative Model With Optical Geometry Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrey Voynov*",
      "Amir Hertz",
      "Moab Arar",
      "Shlomi Fruchter",
      "Daniel Cohen-Or"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9866_ECCV_2024_paper.php": {
    "title": "Mini-Splatting: Representing Scenes with a Constrained Number of Gaussians",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangchi Fang",
      "Bing Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9881_ECCV_2024_paper.php": {
    "title": "MeshSegmenter: Zero-Shot Mesh Segmentation via Texture Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziming Zhong*",
      "Yanyu Xu",
      "Jing Li",
      "Jiale Xu",
      "Zhengxin Li",
      "Chaohui Yu",
      "Shenghua Gao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9900_ECCV_2024_paper.php": {
    "title": "OTSeg: Multi-prompt Sinkhorn Attention for Zero-Shot Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kwanyoung Kim",
      "Yujin Oh",
      "Jong Chul Ye*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9904_ECCV_2024_paper.php": {
    "title": "Skeleton Recall Loss for Connectivity Conserving and Resource Efficient Segmentation of Thin Tubular Structures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yannick Kirchhoff*",
      "Maximilian R Rokuss*",
      "Saikat Roy*",
      "Balint Kovacs",
      "Constantin Ulrich",
      "Tassilo Wald",
      "Maximilian Zenk",
      "Philipp Vollmuth",
      "Jens Kleesiek",
      "Fabian Isensee",
      "Klaus H. Maier-Hein"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9905_ECCV_2024_paper.php": {
    "title": "Conceptual Codebook Learning for Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Zhang*",
      "Ke Yu",
      "Siqi Wu",
      "Zhihai He*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9911_ECCV_2024_paper.php": {
    "title": "LingoQA: Video Question Answering for Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ana-Maria Marcu*",
      "Long Chen",
      "Jan Hünermann",
      "Alice Karnsund",
      "Benoit Hanotte",
      "Prajwal Chidananda",
      "Saurabh Nair",
      "Vijay Badrinarayanan",
      "Alex Kendall",
      "Jamie Shotton",
      "Elahe Arani",
      "Oleg Sinavski"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9918_ECCV_2024_paper.php": {
    "title": "AnimateMe: 4D Facial Expressions via Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dimitrios Gerogiannis*",
      "Foivos Paraperas Papantoniou",
      "Rolandos Alexandros Potamias",
      "Alexandros Lattas",
      "Stylianos Moschoglou",
      "Stylianos Ploumpis",
      "Stefanos Zafeiriou"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9919_ECCV_2024_paper.php": {
    "title": "HaloQuest: A Visual Hallucination Dataset for Advancing Multimodal Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhecan Wang",
      "Garrett Bingham*",
      "Adams Wei Yu",
      "Quoc V. Le",
      "Thang Luong",
      "Golnaz Ghiasi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9925_ECCV_2024_paper.php": {
    "title": "LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kevin Xie*",
      "Tianshi Cao",
      "Jonathan P Lorraine",
      "Jun Gao",
      "James R Lucas",
      "Antonio Torralba",
      "Sanja Fidler",
      "Xiaohui Zeng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9936_ECCV_2024_paper.php": {
    "title": "PreSight: Enhancing Autonomous Vehicle Perception with City-Scale NeRF Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyuan Yuan*",
      "Yucheng Mao",
      "Jiawei Yang",
      "Yicheng LIU",
      "Yue Wang",
      "Hang Zhao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9937_ECCV_2024_paper.php": {
    "title": "Unveiling and Mitigating Memorization in Text-to-image Diffusion Models through Cross Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Ren*",
      "Yaxin Li",
      "Shenglai Zeng",
      "Han Xu",
      "Lingjuan Lyu",
      "Yue Xing",
      "Jiliang Tang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9941_ECCV_2024_paper.php": {
    "title": "iNeMo: Incremental Neural Mesh Models for Robust Class-Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tom Fischer*",
      "Yaoyao Liu",
      "Artur Jesslen",
      "Noor Ahmed",
      "Prakhar Kaushik",
      "Angtian Wang",
      "Alan Yuille",
      "Adam Kortylewski",
      "Eddy Ilg"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9942_ECCV_2024_paper.php": {
    "title": "Context Diffusion: In-Context Aware Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ivona Najdenkoska*",
      "Animesh Sinha",
      "Abhimanyu Dubey",
      "Dhruv Mahajan",
      "Vignesh Ramanathan",
      "Filip Radenovic"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9947_ECCV_2024_paper.php": {
    "title": "Pose Guided Fine-Grained Sign Language Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tongkai Shi",
      "Lianyu Hu",
      "Fanhua Shang",
      "Jichao Feng",
      "liu peidong",
      "Wei Feng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9950_ECCV_2024_paper.php": {
    "title": "RAP: Retrieval-Augmented Planner for Adaptive Procedure Planning in Instructional Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali Zare*",
      "Yulei Niu",
      "Hammad Ayyubi",
      "Shih-Fu Chang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9971_ECCV_2024_paper.php": {
    "title": "Certifiably Robust Image Watermark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengyuan Jiang*",
      "Moyang Guo",
      "Yuepeng Hu",
      "Jinyuan Jia",
      "Neil Zhenqiang Gong"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9973_ECCV_2024_paper.php": {
    "title": "Discover-then-Name: Task-Agnostic Concept Bottlenecks via Automated Concept Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sukrut Rao*",
      "Sweta Mahajan*",
      "Moritz Böhle",
      "Bernt Schiele"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9976_ECCV_2024_paper.php": {
    "title": "Online Zero-Shot Classification with CLIP",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Qian*",
      "Juhua Hu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9979_ECCV_2024_paper.php": {
    "title": "SeA: Semantic Adversarial Augmentation for Last Layer Features from Unsupervised Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Qian*",
      "Yuanhong Xu",
      "Juhua Hu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9992_ECCV_2024_paper.php": {
    "title": "Unlocking the Potential of Federated Learning: The Symphony of Dataset Distillation via Deep Generative Latents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuqi Jia",
      "Saeed Vahidian*",
      "Jingwei Sun",
      "Jianyi Zhang",
      "Vyacheslav Kungurtsev",
      "Neil Zhenqiang Gong",
      "Yiran Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9996_ECCV_2024_paper.php": {
    "title": "Rethinking Fast Adversarial Training: A Splitting Technique To Overcome Catastrophic Overfitting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Masoumeh Zareapoor",
      "Pourya Shamsolmoali*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9997_ECCV_2024_paper.php": {
    "title": "Quality Assured: Rethinking Annotation Strategies in Imaging AI",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tim Rädsch*",
      "Annika Reinke",
      "Vivienn Weru",
      "Minu D. Tizabi",
      "Nicholas Heller",
      "Fabian Isensee",
      "Annette Kopp-Schneider",
      "Lena Maier-Hein*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10005_ECCV_2024_paper.php": {
    "title": "BRIDGE: Bridging Gaps in Image Captioning Evaluation with Stronger Visual Cues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sara Sarto*",
      "Marcella Cornia",
      "Lorenzo Baraldi",
      "Rita Cucchiara"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10006_ECCV_2024_paper.php": {
    "title": "Enhancing Plausibility Evaluation for Generated Designs with Denoising Autoencoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiajie Fan*",
      "Amal Trigui*",
      "Thomas Bäck",
      "Hao Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10017_ECCV_2024_paper.php": {
    "title": "Weakly-Supervised 3D Hand Reconstruction with Knowledge Prior and Uncertainty Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufei Zhang*",
      "Jeffrey Kephart",
      "Qiang Ji*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10029_ECCV_2024_paper.php": {
    "title": "3D Reconstruction of Objects in Hands without Real World 3D Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditya Prakash*",
      "Matthew Chang",
      "Matthew Jin",
      "Ruisen Tu",
      "Saurabh Gupta"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10031_ECCV_2024_paper.php": {
    "title": "To Supervise or Not to Supervise: Understanding and Addressing the Key Challenges of Point Cloud Transfer Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Souhail Hadgi*",
      "Lei Li",
      "Maks Ovsjanikov"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10032_ECCV_2024_paper.php": {
    "title": "Parameterized Quasi-Physical Simulators for Dexterous Manipulations Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xueyi Liu*",
      "Kangbo Lyu",
      "jieqiong zhang",
      "Tao Du",
      "Li Yi*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10034_ECCV_2024_paper.php": {
    "title": "3D Hand Pose Estimation in Everyday Egocentric Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditya Prakash*",
      "Ruisen Tu",
      "Matthew Chang",
      "Saurabh Gupta"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10036_ECCV_2024_paper.php": {
    "title": "Mitigating Perspective Distortion-induced Shape Ambiguity in Image Crops",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditya Prakash*",
      "Arjun Gupta",
      "Saurabh Gupta"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10038_ECCV_2024_paper.php": {
    "title": "Towards Neuro-Symbolic Video Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minkyu Choi*",
      "Harsh Goel",
      "Mohammad Omama",
      "Yunhao Yang",
      "Sahil Shah",
      "Sandeep Chinchali"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10054_ECCV_2024_paper.php": {
    "title": "Optimization-based Uncertainty Attribution Via Learning Informative Perturbations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanjing Wang*",
      "Bashirul Azam Biswas",
      "Qiang Ji"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10056_ECCV_2024_paper.php": {
    "title": "Context-Aware Action Recognition: Introducing a Comprehensive Dataset for Behavior Contrast",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tatsuya Sasaki*",
      "Yoshiki Ito",
      "Satoshi Kondo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10058_ECCV_2024_paper.php": {
    "title": "Semi-supervised Segmentation of Histopathology Images with Noise-Aware Topological Consistency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meilong Xu*",
      "Xiaoling Hu",
      "Saumya Gupta",
      "Shahira Abousamra",
      "Chao Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10059_ECCV_2024_paper.php": {
    "title": "Adaptive Compressed Sensing with Diffusion-Based Posterior Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Noam Elata*",
      "Tomer Michaeli",
      "Michael Elad"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10061_ECCV_2024_paper.php": {
    "title": "Instant Uncertainty Calibration of NeRFs Using a Meta-Calibrator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Niki Amini-Naieni*",
      "Tomas Jakab",
      "Andrea Vedaldi",
      "Ronald Clark"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10062_ECCV_2024_paper.php": {
    "title": "MetaAT: Active Testing for Label-Efficient Evaluation of Dense Recognition Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanbao Su",
      "Xin Li*",
      "Thang Doan",
      "Sima Behpour",
      "Wenbin He",
      "Liang Gou",
      "Fei Miao",
      "Liu Ren"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10063_ECCV_2024_paper.php": {
    "title": "Salience-Based Adaptive Masking: Revisiting Token Dynamics for Enhanced Pre-training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyesong Choi",
      "Hyejin Park",
      "Kwang Moo Yi",
      "Sungmin Cha",
      "Dongbo Min*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10071_ECCV_2024_paper.php": {
    "title": "Data Augmentation via Latent Diffusion for Saliency Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bahar Aydemir*",
      "Deblina Bhattacharjee",
      "Tong Zhang",
      "Mathieu Salzmann",
      "Sabine Süsstrunk"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10075_ECCV_2024_paper.php": {
    "title": "Explorative Inbetweening of Time and Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiwen Feng*",
      "Zheng Ding",
      "Zhihao Xia",
      "Simon Niklaus",
      "Victoria Fernandez Abrevaya",
      "Michael J. Black",
      "Xuaner Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10081_ECCV_2024_paper.php": {
    "title": "A Diffusion Model for Simulation Ready Coronary Anatomy with Morpho-skeletal Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karim Kadry*",
      "Shreya Gupta",
      "Jonas Sogbadji",
      "Michiel Schaap",
      "Kersten Petersen",
      "Takuya Mizukami",
      "Carlos Collet",
      "Farhad R. Nezami",
      "Elazer R Edelman"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10082_ECCV_2024_paper.php": {
    "title": "Learning to Make Keypoints Sub-Pixel Accurate",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shinjeong Kim*",
      "Marc Pollefeys",
      "Daniel Barath"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10086_ECCV_2024_paper.php": {
    "title": "Imaging with Confidence: Uncertainty Quantification for High-dimensional Undersampled MR Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Frederik Hoppe*",
      "Claudio Mayrink Verdun",
      "Hannah Sophie Laus",
      "Sebastian Endt",
      "Marion Irene Menzel",
      "Felix Krahmer",
      "Holger Rauhut"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10096_ECCV_2024_paper.php": {
    "title": "Generalizable Human Gaussians for Sparse View Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "YoungJoong Kwon*",
      "Baole Fang",
      "Yixing Lu",
      "Haoye Dong",
      "Cheng Zhang",
      "Francisco Vicente Carrasco",
      "Albert Mosella-Montoro",
      "Jianjin Xu",
      "Shingo J Takagi",
      "Daeil Kim",
      "Aayush Prakash",
      "Fernando de la Torre"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10097_ECCV_2024_paper.php": {
    "title": "DrivingDiffusion: Layout-Guided Multi-View Driving Scenarios Video Generation with Latent Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li Xiaofan*",
      "Zhang Yifu*",
      "Ye Xiaoqing*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10111_ECCV_2024_paper.php": {
    "title": "Evaluating the Adversarial Robustness of Semantic Segmentation: Trying Harder Pays Off",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Levente Halmosi",
      "Bálint Mohos",
      "Márk Jelasity*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10113_ECCV_2024_paper.php": {
    "title": "SkyScenes: A Synthetic Dataset for Aerial Scene Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sahil S Khose*",
      "Anisha Pal",
      "Aayushi Agarwal",
      ". Deepanshi",
      "Judy Hoffman",
      "Prithvijit Chattopadhyay"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10114_ECCV_2024_paper.php": {
    "title": "Large-Scale Multi-Hypotheses Cell Tracking Using Ultrametric Contours Maps",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jordão Bragantini*",
      "Merlin Lange",
      "Loïc A Royer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10117_ECCV_2024_paper.php": {
    "title": "GSD: View-Guided Gaussian Splatting Diffusion for 3D Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Mu*",
      "Xinxin Zuo",
      "Chuan Guo",
      "Yilin Wang",
      "Juwei Lu",
      "Xiaofei Wu",
      "Songcen Xu",
      "Peng Dai",
      "Youliang Yan",
      "Li Cheng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10120_ECCV_2024_paper.php": {
    "title": "AdaDiff: Accelerating Diffusion Models through Step-Wise Adaptive Computation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengkun Tang*",
      "Yaqing Wang",
      "Caiwen Ding",
      "Yi Liang",
      "Yao Li",
      "Dongkuan Xu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10133_ECCV_2024_paper.php": {
    "title": "PFedEdit: Personalized Federated Learning via Automated Model Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haolin Yuan*",
      "William Paul",
      "John Aucott",
      "Philippe Burlina",
      "Yinzhi Cao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10138_ECCV_2024_paper.php": {
    "title": "De-Confusing Pseudo-Labels in Source-Free Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Idit Diamant*",
      "Amir Rosenfeld",
      "Idan Achituve",
      "Jacob Goldberger",
      "Arnon Netzer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10147_ECCV_2024_paper.php": {
    "title": "GenerateCT: Text-Conditional Generation of 3D Chest CT Volumes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ibrahim Ethem Hamamci*",
      "Sezgin Er",
      "Anjany Sekuboyina",
      "Enis Simsar",
      "Alperen Tezcan",
      "Ayse Gulnihan Simsek",
      "Sevval Nil Esirgun",
      "Furkan Almas",
      "Irem Dogan",
      "Muhammed Furkan Dasdelen",
      "Chinmay Prabhakar",
      "Hadrien Reynaud",
      "Sarthak Pati",
      "Christian Bluethgen",
      "Mehmet Kemal Ozdemir",
      "Bjoern Menze"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10156_ECCV_2024_paper.php": {
    "title": "EraseDraw : Learning to Insert Objects by Erasing Them from Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alper Canberk*",
      "Maksym Bondarenko",
      "Ege Ozguroglu",
      "Ruoshi Liu",
      "Carl Vondrick"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10165_ECCV_2024_paper.php": {
    "title": "SuperFedNAS: Cost-Efficient Federated Neural Architecture Search for On-Device Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alind Khare*",
      "Animesh Agrawal",
      "Aditya Annavajjala",
      "Payman Behnam",
      "Myungjin Lee",
      "Hugo M Latapie",
      "Alexey Tumanov"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10167_ECCV_2024_paper.php": {
    "title": "Towards Reliable Evaluation and Fast Training of Robust Semantic Segmentation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francesco Croce*",
      "Naman D. Singh",
      "Matthias Hein*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10172_ECCV_2024_paper.php": {
    "title": "Contrastive Region Guidance: Improving Grounding in Vision-Language Models without Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Wan*",
      "Jaemin Cho",
      "Elias Stengel-Eskin",
      "Mohit Bansal"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10180_ECCV_2024_paper.php": {
    "title": "Keypoint Promptable Re-Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vladimir Somers*",
      "Alexandre Alahi",
      "Christophe De Vleeschouwer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10190_ECCV_2024_paper.php": {
    "title": "Merging and Splitting Diffusion Paths for Semantically Coherent Panoramas",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fabio Quattrini*",
      "Vittorio Pippi",
      "Silvia Cascianelli*",
      "Rita Cucchiara"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10191_ECCV_2024_paper.php": {
    "title": "DynMF: Neural Motion Factorization for Real-time Dynamic View Synthesis with 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Angelos Kratimenos*",
      "Jiahui Lei",
      "Kostas Daniilidis"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10197_ECCV_2024_paper.php": {
    "title": "Animal Avatars: Reconstructing Animatable 3D Animals from Casual Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Remy Sabathier*",
      "David Novotny",
      "Niloy Mitra"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10198_ECCV_2024_paper.php": {
    "title": "Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers' Opinion Scores",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucas Goncalves",
      "Prashant Mathur*",
      "Chandrashekhar Lavania",
      "Metehan Cekic",
      "Marcello Federico",
      "Kyu Han"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10205_ECCV_2024_paper.php": {
    "title": "MMVR: Millimeter-wave Multi-View Radar Dataset and Benchmark for Indoor Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Mahbubur Rahman",
      "Ryoma Yataka",
      "Sorachi Kato",
      "Pu Wang*",
      "Peizhao Li",
      "Adriano Cardace",
      "Petros Boufounos"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10236_ECCV_2024_paper.php": {
    "title": "Training A Secure Model against Data-Free Model Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenyi Wang*",
      "Li Shen*",
      "junfeng guo",
      "Tiehang Duan",
      "Siyu Luan",
      "Tongliang Liu",
      "Mingchen Gao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10237_ECCV_2024_paper.php": {
    "title": "EpipolarGAN: Omnidirectional Image Synthesis with Explicit Camera Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christopher May*",
      "Daniel Aliaga"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10248_ECCV_2024_paper.php": {
    "title": "TriNeRFLet: A Wavelet Based Triplane NeRF Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rajaei Khatib*",
      "Raja Giryes*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10261_ECCV_2024_paper.php": {
    "title": "EgoBody3M: Egocentric Body Tracking on a VR Headset using a Diverse Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amy Zhao",
      "Chengcheng Tang",
      "Lezi Wang",
      "Yijing Li",
      "Mihika Dave",
      "Lingling Tao*",
      "Christopher D. Twigg",
      "Robert Y. Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10270_ECCV_2024_paper.php": {
    "title": "Photorealistic Video Generation with Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Agrim Gupta*",
      "Lijun Yu",
      "Kihyuk Sohn",
      "Xiuye Gu",
      "Meera Hahn",
      "Li Fei-Fei",
      "Irfan Essa",
      "Lu Jiang",
      "Jose Lezama"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10278_ECCV_2024_paper.php": {
    "title": "RAVE: Residual Vector Embedding for CLIP-Guided Backlit Image Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tatiana Gaintseva*",
      "Martin Benning",
      "Gregory Slabaugh*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10285_ECCV_2024_paper.php": {
    "title": "TIBET: Identifying and Evaluating Biases in Text-to-Image Generative Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditya Chinchure*",
      "Pushkar Shukla*",
      "Gaurav Bhatt",
      "Kiri Salij",
      "Kartik Hosanagar",
      "Leonid Sigal",
      "Matthew Turk"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10290_ECCV_2024_paper.php": {
    "title": "Object-Aware Query Perturbation for Cross-Modal Image-Text Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Naoya Sogi*",
      "Takashi Shibata*",
      "Makoto Terao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10297_ECCV_2024_paper.php": {
    "title": "DECIDER: Leveraging Foundation Model Priors for Improved Model Failure Detection and Explanation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rakshith Subramanyam*",
      "Kowshik Thopalli*",
      "Vivek Sivaraman Narayanaswamy",
      "Jayaraman J. Thiagarajan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10301_ECCV_2024_paper.php": {
    "title": "Ex2Eg-MAE: A Framework for Adaptation of Exocentric Video Masked Autoencoders for Egocentric Social Role Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minh Tran*",
      "Yelin Kim",
      "Che-Chun Su",
      "Min Sun",
      "Cheng-Hao Kuo",
      "Mohammad Soleymani"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10310_ECCV_2024_paper.php": {
    "title": "Self-Supervised Audio-Visual Soundscape Stylization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tingle Li*",
      "Renhao Wang",
      "Po-Yao Huang",
      "Andrew Owens",
      "Gopala Krishna Anumanchipalli"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10311_ECCV_2024_paper.php": {
    "title": "SAVE: Protagonist Diversification with Structure Agnostic Video Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yeji Song*",
      "Wonsik Shin",
      "Junsoo Lee",
      "Jeesoo Kim",
      "Nojun Kwak*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10325_ECCV_2024_paper.php": {
    "title": "VideoAgent: Long-form Video Understanding with Large Language Model as Agent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaohan Wang*",
      "Yuhui Zhang",
      "Orr Zohar",
      "Serena Yeung-Levy"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10330_ECCV_2024_paper.php": {
    "title": "Meta-optimized Angular Margin Contrastive Framework for Video-Language Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thong Thanh Nguyen*",
      "Yi Bin",
      "Xiaobao Wu",
      "Xinshuai Dong",
      "Zhiyuan Hu",
      "Khoi M Le",
      "Cong-Duy Nguyen",
      "See Kiong Ng",
      "Anh Tuan Luu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10331_ECCV_2024_paper.php": {
    "title": "Source-Free Domain-Invariant Performance Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ekaterina Khramtsova*",
      "Mahsa Baktashmotlagh",
      "Guido Zuccon",
      "Xi Wang",
      "Mathieu Salzmann"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10339_ECCV_2024_paper.php": {
    "title": "Improving Robustness to Model Inversion Attacks via Sparse Coding Architectures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sayanton V. Dibbo*",
      "Adam Breuer",
      "Juston Moore",
      "Michael Teti"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10348_ECCV_2024_paper.php": {
    "title": "Constructing Concept-based Models to Mitigate Spurious Correlations with Minimal Human Effort",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeeyung Kim*",
      "Ze Wang",
      "Qiang Qiu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10350_ECCV_2024_paper.php": {
    "title": "Direct Distillation between Different Domains",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jialiang Tang",
      "Shuo Chen*",
      "Gang Niu",
      "Hongyuan Zhu",
      "Joey Tianyi Zhou",
      "Chen Gong*",
      "Masashi Sugiyama"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10352_ECCV_2024_paper.php": {
    "title": "Contrastive ground-level image and remote sensing pre-training improves representation learning for natural world imagery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andy V Huynh*",
      "Lauren Gillespie",
      "Jael Lopez-Saucedo",
      "Claire Tang",
      "Rohan Sikand",
      "Moisés Expósito-Alonso"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10354_ECCV_2024_paper.php": {
    "title": "V-Trans4Style: Visual Transition Recommendation for Video Production Style Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pooja Guhan*",
      "Tsung-Wei Huang",
      "Guan-Ming Su",
      "Subhadra Gopalakrishnan",
      "Dinesh Manocha"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10356_ECCV_2024_paper.php": {
    "title": "GRiT: A Generative Region-to-text Transformer for Object Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jialian Wu*",
      "Jianfeng Wang",
      "Zhengyuan Yang",
      "Zhe Gan",
      "Zicheng Liu",
      "Junsong Yuan",
      "Lijuan Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10364_ECCV_2024_paper.php": {
    "title": "LRSLAM: Low-rank Representation of Signed Distance Fields in Dense Visual SLAM System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongbeen Park",
      "Minjeong Park",
      "Giljoo Nam",
      "Jinkyu Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10369_ECCV_2024_paper.php": {
    "title": "Learning Representation for Multitask Learning through Self-Supervised Auxiliary Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seokwon Shin",
      "Hyungrok Do",
      "Youngdoo Son*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10378_ECCV_2024_paper.php": {
    "title": "Neural Poisson Solver: A Universal and Continuous Framework for Natural Signal Blending",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Delong Wu",
      "Hao Zhu",
      "Qi Zhang",
      "You Li",
      "Xun Cao*",
      "Zhan Ma*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10383_ECCV_2024_paper.php": {
    "title": "Geometry Fidelity for Spherical Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anders Christensen*",
      "Nooshin Mojab*",
      "Khushman Patel",
      "Karan Ahuja",
      "Zeynep Akata",
      "Ole Winther",
      "Mar Gonzalez Franco",
      "Andrea Colaco"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10394_ECCV_2024_paper.php": {
    "title": "BAGS: Blur Agnostic Gaussian Splatting through Multi-Scale Kernel Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Peng*",
      "Yutao Tang",
      "Yifan Zhou",
      "Nengyu Wang",
      "Xijun Liu",
      "Deming Li",
      "Rama Chellappa"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10398_ECCV_2024_paper.php": {
    "title": "CroMo-Mixup: Augmenting Cross-Model Representations for Continual Self-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Erum Mushtaq*",
      "Duygu Nur Yaldiz",
      "Yavuz Faruk Bakman",
      "Jie Ding",
      "Chenyang Tao",
      "Dimitrios Dimitriadis",
      "Salman Avestimehr"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10410_ECCV_2024_paper.php": {
    "title": "WoVoGen: World Volume-aware Diffusion for Controllable Multi-camera Driving Scene Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiachen Lu",
      "Ze Huang",
      "Zeyu Yang",
      "Zhang Jiahui",
      "Li Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10418_ECCV_2024_paper.php": {
    "title": "Benchmarking Spurious Bias in Few-Shot Image Classifiers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangtao Zheng*",
      "Wenqian Ye",
      "Aidong Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10421_ECCV_2024_paper.php": {
    "title": "TurboEdit: Real-time text-based disentangled real image editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zongze Wu*",
      "Nicholas I Kolkin",
      "Jonathan Brandt",
      "Richard Zhang",
      "Eli Shechtman"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10427_ECCV_2024_paper.php": {
    "title": "Soft Shadow Diffusion (SSD): Physics-inspired Learning for 3D Computational Periscopy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fadlullah A Raji*",
      "John Murray-Bruce*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10431_ECCV_2024_paper.php": {
    "title": "Augmented Neural Fine-tuning for Efficient Backdoor Purification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nazmul Karim*",
      "Abdullah Al Arafat",
      "Umar Khalid",
      "Zhishan Guo",
      "Nazanin Rahnavard"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10433_ECCV_2024_paper.php": {
    "title": "REDIR: Refocus-free Event-based De-occlusion Image Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Guo",
      "Hailong Shi*",
      "Huan Li",
      "Jinsheng Xiao",
      "Xingyu Gao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10441_ECCV_2024_paper.php": {
    "title": "Free-Editor: Zero-shot Text-driven 3D Scene Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nazmul Karim*",
      "Hasan Iqbal",
      "Umar Khalid",
      "Chen Chen",
      "Jing Hua"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10442_ECCV_2024_paper.php": {
    "title": "DPA-Net: Structured 3D Abstraction from Sparse Views via Differentiable Primitive Assembly",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fenggen Yu*",
      "Yiming Qian",
      "Xu Zhang",
      "Francisca Gil-Ureta",
      "Brian Jackson",
      "Eric Bennett",
      "Hao Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10455_ECCV_2024_paper.php": {
    "title": "An Empirical Study and Analysis of Text-to-Image Generation Using Large Language Model-Powered Textual Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyu Tan",
      "Mengping Yang",
      "Luozheng Qin ",
      "Hao Yang",
      "Ye Qian ",
      "Qiang Zhou",
      "Cheng Zhang",
      "Hao Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10470_ECCV_2024_paper.php": {
    "title": "Few-shot Class Incremental Learning with Attention-Aware Self-Adaptive Prompt",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenxi Liu*",
      "Zhenyi Wang",
      "Tianyi Xiong",
      "Ruibo Chen",
      "Yihan Wu",
      "junfeng guo",
      "Heng Huang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10478_ECCV_2024_paper.php": {
    "title": "An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liang Chen",
      "Haozhe Zhao",
      "Tianyu Liu",
      "Shuai Bai",
      "Junyang Lin",
      "Chang Zhou",
      "Baobao Chang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10485_ECCV_2024_paper.php": {
    "title": "Generalizable Symbolic Optimizer Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaotian Song",
      "Peng Zeng",
      "Yanan Sun*",
      "Andy Song"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10490_ECCV_2024_paper.php": {
    "title": "Online Continuous Generalized Category Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keon-Hee Park",
      "Hakyung Lee",
      "Kyungwoo Song*",
      "Gyeong-Moon Park*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10495_ECCV_2024_paper.php": {
    "title": "Bridging Different Language Models and Generative Vision Models for Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shihao Zhao*",
      "Shaozhe Hao",
      "Bojia Zi",
      "Huaizhe Xu",
      "Kwan-Yee K. Wong*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10498_ECCV_2024_paper.php": {
    "title": "Tackling Structural Hallucination in Image Translation with Local Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seunghoi Kim*",
      "Chen Jin",
      "Tom Diethe",
      "Matteo Figini",
      "Henry FJ Tregidgo",
      "Asher Mullokandov",
      "Philip A Teare",
      "Daniel Alexander"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10502_ECCV_2024_paper.php": {
    "title": "Hierarchical Separable Video Transformer for Snapshot Compressive Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ping Wang*",
      "Yulun Zhang",
      "Lishun Wang",
      "Xin Yuan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10510_ECCV_2024_paper.php": {
    "title": "Unified Medical Image Pre-training in Language-Guided Common Semantic Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoxuan He",
      "Yifan Yang",
      "Xinyang Jiang",
      "Xufang Luo*",
      "Haoji Hu",
      "Siyun Zhao",
      "Dongsheng Li",
      "Yuqing Yang",
      "Lili Qiu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10529_ECCV_2024_paper.php": {
    "title": "On the Vulnerability of Skip Connections to Model Inversion Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Hao Koh*",
      "Sy-Tuyen Ho",
      "Ngoc-Bao Nguyen",
      "Ngai-Man Cheung"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10530_ECCV_2024_paper.php": {
    "title": "Adversarial Robustification via Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daewon Choi",
      "Jongheon Jeong",
      "Huiwon Jang",
      "Jinwoo Shin*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10531_ECCV_2024_paper.php": {
    "title": "Overcome Modal Bias in Multi-modal Federated Learning via Balanced Modality Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunfeng FAN*",
      "Wenchao Xu*",
      "Haozhao Wang",
      "Fushuo Huo",
      "Jinyu Chen",
      "Song Guo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10537_ECCV_2024_paper.php": {
    "title": "Comprehensive Attribution: Inherently Explainable Vision Model with Feature Detector",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xianren Zhang",
      "Dongwon Lee",
      "Suhang Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10541_ECCV_2024_paper.php": {
    "title": "Reinforcement Learning via Auxillary Task Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhinav N Harish*",
      "Larry Heck",
      "Josiah P Hanna",
      "Zsolt Kira",
      "Andrew Szot"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10547_ECCV_2024_paper.php": {
    "title": "DHR: Dual Features-Driven Hierarchical Rebalancing in Inter- and Intra-Class Regions for Weakly-Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanghyun Jo",
      "Fei Pan",
      "In-Jae Yu",
      "Kyungsu Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10552_ECCV_2024_paper.php": {
    "title": "Pre-trained Visual Dynamics Representations for Efficient Policy Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Luo*",
      "Bohan Zhou",
      "Zongqing Lu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10555_ECCV_2024_paper.php": {
    "title": "View-Consistent Hierarchical 3D Segmentation Using Ultrametric Feature Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haodi He",
      "Colton Stearns",
      "Adam Harley",
      "Leonidas Guibas*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10564_ECCV_2024_paper.php": {
    "title": "Plug and Play: A Representation Enhanced Domain Adapter for Collaborative Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyou Luo*",
      "Quan Yuan*",
      "Yuchen Xia",
      "Guiyang Luo",
      "Yujia Yang",
      "Jinglin Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10568_ECCV_2024_paper.php": {
    "title": "Follow the Rules: Reasoning for Video Anomaly Detection with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchen Yang*",
      "Kwonjoon Lee",
      "Behzad Dariush",
      "Yinzhi Cao*",
      "Shao-Yuan Lo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10570_ECCV_2024_paper.php": {
    "title": "SAM4MLLM: Enhance Multi-Modal Large Language Model for Referring Expression Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi-Chia Chen",
      "Wei-Hua Li",
      "Cheng Sun",
      "Yu-Chiang Frank Wang",
      "Chu-Song Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10592_ECCV_2024_paper.php": {
    "title": "TTD: Text-Tag Self-Distillation Enhancing Image-Text Alignment in CLIP to Alleviate Single Tag Bias",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanghyun Jo",
      "Soohyun Ryu",
      "Sungyub Kim",
      "Eunho Yang",
      "Kyungsu Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10614_ECCV_2024_paper.php": {
    "title": "Learning Quantized Adaptive Conditions for Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchen Liang*",
      "Yuchuan Tian",
      "Lei Yu",
      "Huaao Tang",
      "Jie Hu",
      "Xiangzhong Fang",
      "Hanting Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10615_ECCV_2024_paper.php": {
    "title": "STAMP: Outlier-Aware Test-Time Adaptation with Stable Memory Replay",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Yongcan",
      "Lijun Sheng",
      "Ran He",
      "Jian Liang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10618_ECCV_2024_paper.php": {
    "title": "Remove Projective LiDAR Depthmap Artifacts via Exploiting Epipolar Geometry",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengjie Zhu*",
      "Girish Chandar Ganesan",
      "Abhinav Kumar",
      "Xiaoming Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10635_ECCV_2024_paper.php": {
    "title": "Accelerating Online Mapping and Behavior Prediction via Direct BEV Feature Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xunjiang Gu",
      "Guanyu Song",
      "Igor Gilitschenski",
      "Marco Pavone",
      "Boris Ivanovic*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10650_ECCV_2024_paper.php": {
    "title": "High-Fidelity Modeling of Generalizable Wrinkle Deformation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingfan Guo",
      "Jae Shin Yoon",
      "Shunsuke Saito",
      "Takaaki Shiratori",
      "Hyun Soo Park*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10652_ECCV_2024_paper.php": {
    "title": "Instruction Tuning-free Visual Token Complement for Multimodal LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongsheng Wang*",
      "Jiequan Cui",
      "Miaoge Li",
      "Wang Lin",
      "Bo Chen",
      "Hanwang Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10677_ECCV_2024_paper.php": {
    "title": "Exploring Conditional Multi-Modal Prompts for Zero-shot HOI Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ting Lei",
      "Shaofeng Yin",
      "Yuxin Peng",
      "Yang Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10687_ECCV_2024_paper.php": {
    "title": "Training-free Video Temporal Grounding using Large-scale Pre-trained Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minghang Zheng",
      "Xinhao Cai",
      "Qingchao Chen",
      "Yuxin Peng",
      "Yang Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10690_ECCV_2024_paper.php": {
    "title": "Revisit Self-supervision with Local Structure-from-Motion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengjie Zhu*",
      "Xiaoming Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10720_ECCV_2024_paper.php": {
    "title": "FAMOUS: High-Fidelity Monocular 3D Human Digitization Using View Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vishnu Mani Hema*",
      "Shubhra Aich",
      "Christian Haene",
      "Jean-Charles Bazin",
      "Fernando de la Torre"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10733_ECCV_2024_paper.php": {
    "title": "Efficient Learning of Event-based Dense Representation using Hierarchical Memories with Adaptive Update",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Uday Kamal*",
      "Saibal Mukhopadhyay"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10737_ECCV_2024_paper.php": {
    "title": "SNP: Structured Neuron-level Pruning to Preserve Attention Scores",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "KyungHwan Shim",
      "Jaewoong Yun",
      "Shinkook Choi*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10738_ECCV_2024_paper.php": {
    "title": "Multi-Granularity Sparse Relationship Matrix Prediction Network for End-to-End Scene Graph Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "lei wang",
      "Zejian Yuan",
      "Badong Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10739_ECCV_2024_paper.php": {
    "title": "Flash-Splat: 3D Reflection Removal with Flash Cues and Gaussian Splats",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingyang Xie*",
      "Haoming Cai",
      "Sachin Shah",
      "Yiran Xu",
      "Brandon Y. Feng",
      "Jia-Bin Huang",
      "Christopher A. Metzler"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10743_ECCV_2024_paper.php": {
    "title": "PALM: Predicting Actions through Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanghwan Kim*",
      "Daoji Huang",
      "Yongqin Xian",
      "Otmar Hilliges",
      "Luc Van Gool",
      "Xi Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10749_ECCV_2024_paper.php": {
    "title": "Motion Keyframe Interpolation for Any Human Skeleton using Point Cloud-based Human Motion Data Homogenisation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Clinton A Mo",
      "Kun Hu*",
      "Chengjiang Long",
      "Dong Yuan",
      "Zhiyong Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10756_ECCV_2024_paper.php": {
    "title": "SwiftBrush v2: Make Your One-step Diffusion Model Better Than Its Teacher",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Trung Tuan Dao*",
      "Thuan Hoang Nguyen",
      "Thanh Van Le",
      "Duc H Vu",
      "Khoi Nguyen",
      "Cuong Pham",
      "Anh T Tran*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10759_ECCV_2024_paper.php": {
    "title": "Learning to Localize Actions in Instructional Videos with LLM-Based Multi-Pathway Text-Video Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxiao Chen*",
      "Kai Li",
      "Wentao Bao",
      "Deep Patel",
      "Yu Kong",
      "Martin Renqiang Min",
      "Dimitris N. Metaxas*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10766_ECCV_2024_paper.php": {
    "title": "Improving Hyperbolic Representations via Gromov-Wasserstein Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei Yang",
      "Wonjun Lee",
      "Dongmian Zou*",
      "Gilad Lerman"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10768_ECCV_2024_paper.php": {
    "title": "VSViG: Real-time Video-based Seizure Detection via Skeleton-based Spatiotemporal ViG",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yankun Xu*",
      "Junzhe Wang",
      "Yun-Hsuan Chen",
      "Jie Yang",
      "Wenjie Ming",
      "Shuang Wang",
      "Mohamad Sawan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10778_ECCV_2024_paper.php": {
    "title": "DiffSurf: A Transformer-based Diffusion Model for Generating and Reconstructing 3D Surfaces in Pose",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yusuke Yoshiyasu*",
      "Leyuan Sun"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10783_ECCV_2024_paper.php": {
    "title": "Exploiting Supervised Poison Vulnerability to Strengthen Self-Supervised Defense",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeremy Styborski*",
      "Mingzhi Lyu*",
      "Yi Huang*",
      "Adams Kong*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10784_ECCV_2024_paper.php": {
    "title": "Dense Hand-Object(HO) GraspNet with Full Grasping Taxonomy and Dynamics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Woojin Cho",
      "Jihyun Lee",
      "Minjae Yi",
      "Minje Kim",
      "Taeyun Woo",
      "Donghwan Kim",
      "Taewook Ha",
      "Hyokeun Lee",
      "Je-Hwan Ryu",
      "Woontack Woo",
      "Tae-Kyun (T-K) Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10791_ECCV_2024_paper.php": {
    "title": "Human Pose Recognition via Occlusion-Preserving Abstract Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saad Manzur*",
      "Wayne B Hayes*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10792_ECCV_2024_paper.php": {
    "title": "DA-BEV: Unsupervised Domain Adaptation for Bird's Eye View Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Jiang*",
      "Jiaxing Huang",
      "Weiying Xie",
      "Jie Lei",
      "Yunsong Li",
      "Ling Shao",
      "Shijian Lu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10822_ECCV_2024_paper.php": {
    "title": "SlimFlow: Training Smaller One-Step Diffusion Models with Rectified Flow",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanzhi Zhu*",
      "Xingchao Liu",
      "Qiang Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10827_ECCV_2024_paper.php": {
    "title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaowei Liu",
      "Zhongzheng Ren",
      "Saurabh Gupta",
      "Shenlong Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10835_ECCV_2024_paper.php": {
    "title": "Depth-Aware Blind Image Decomposition for Real-World Adverse Weather Recovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Wang*",
      "Zhedong Zheng",
      "Ruijie Quan",
      "Yi Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10837_ECCV_2024_paper.php": {
    "title": "DreamSampler: Unifying Diffusion Sampling and Score Distillation for Image Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeongsol Kim",
      "Geon Yeong Park",
      "Jong Chul Ye*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10845_ECCV_2024_paper.php": {
    "title": "Reshaping the Online Data Buffering and Organizing Mechanism for Continual Test-Time Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhilin Zhu*",
      "Xiaopeng Hong*",
      "Zhiheng Ma",
      "Weijun Zhuang",
      "YaoHui Ma",
      "Yong Dai",
      "Yaowei Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10846_ECCV_2024_paper.php": {
    "title": "Personalized Privacy Protection Mask Against Unauthorized Facial Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ka-Ho Chow*",
      "Sihao Hu",
      "Tiansheng Huang",
      "Ling Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10849_ECCV_2024_paper.php": {
    "title": "PosterLlama: Bridging Design Ability of Langauge Model to Content-Aware Layout Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaejung Seol",
      "SeoJun Kim",
      "Jaejun Yoo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10860_ECCV_2024_paper.php": {
    "title": "PreciseControl: Enhancing Text-To-Image Diffusion Models with Fine-Grained Attribute Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rishubh Parihar*",
      "Sachidanand VS",
      "Sabariswaran Mani",
      "Tejan Karmali",
      "Venkatesh Babu RADHAKRISHNAN"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10868_ECCV_2024_paper.php": {
    "title": "LG-Gaze: Learning Geometry-aware Continuous Prompts for Language-Guided Gaze Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengwei Yin*",
      "Jingjing Wang",
      "Guanzhong Zeng",
      "Di Xie",
      "Jiang Zhu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10873_ECCV_2024_paper.php": {
    "title": "Efficient Training with Denoised Neural Weights",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Gong*",
      "Zheng Zhan",
      "Yanyu Li",
      "Yerlan Idelbayev",
      "Andrey Zharkov",
      "Kfir Aberman",
      "Sergey Tulyakov",
      "Yanzhi Wang",
      "Jian Ren"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10882_ECCV_2024_paper.php": {
    "title": "Learning the Unlearned: Mitigating Feature Suppression in Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jihai Zhang",
      "Xiang Lan",
      "Xiaoye Qu",
      "Yu Cheng",
      "Mengling Feng*",
      "Bryan Hooi*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10886_ECCV_2024_paper.php": {
    "title": "Integration of Global and Local Representations for Fine-grained Cross-modal Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seungwan Jin",
      "Hoyoung Choi",
      "Taehyung Noh",
      "Kyungsik Han*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10888_ECCV_2024_paper.php": {
    "title": "Local and Global Flatness for Federated Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Yan",
      "Yuhong Guo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10919_ECCV_2024_paper.php": {
    "title": "SRPose: Two-view Relative Pose Estimation with Sparse Keypoints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Yin",
      "Yulun Zhang",
      "Zherong Pan",
      "Jianjun Zhu",
      "Cheng Wang",
      "Biao Jia*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10922_ECCV_2024_paper.php": {
    "title": "Deep Reward Supervisions for Tuning Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoshi Wu",
      "Yiming Hao",
      "Manyuan Zhang*",
      "Keqiang Sun",
      "Zhaoyang Huang",
      "Guanglu Song",
      "Yu Liu",
      "Hongsheng Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10933_ECCV_2024_paper.php": {
    "title": "Paying More Attention to Images: A Training-Free Method for Alleviating Hallucination in LVLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shi Liu*",
      "Kecheng Zheng*",
      "Wei Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10940_ECCV_2024_paper.php": {
    "title": "Inf-DiT: Upsampling any-resolution image with memory-efficient diffusion transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuoyi Yang*",
      "Heyang Jiang",
      "Wenyi Hong",
      "Jiayan Teng",
      "Wendi Zheng",
      "Yuxiao Dong",
      "Ming Ding",
      "Jie Tang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10941_ECCV_2024_paper.php": {
    "title": "Implicit Neural Models to Extract Heart Rate from Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pradyumna Chari*",
      "Anirudh Bindiganavale Harish",
      "Adnan Armouti",
      "Alexander Vilesov",
      "Sanjit Sarda",
      "Laleh Jalilian",
      "Achuta Kadambi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10961_ECCV_2024_paper.php": {
    "title": "Boost Your NeRF: A Model-Agnostic Mixture of Experts Framework for High Quality and Efficient Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francesco Di Sario*",
      "Riccardo Renzulli",
      "Marco Grangetto",
      "Enzo Tartaglione"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10987_ECCV_2024_paper.php": {
    "title": "PFGS: High Fidelity Point Cloud Rendering via Feature Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxu Wang",
      "Zhang Ziyi",
      "Junhao He",
      "Renjing Xu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11002_ECCV_2024_paper.php": {
    "title": "Few-Shot Anomaly-Driven Generation for Anomaly Classification and Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guan Gui",
      "Bin-Bin Gao*",
      "Jun Liu",
      "Chengjie Wang",
      "Yunsheng Wu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11011_ECCV_2024_paper.php": {
    "title": "E3M: Zero-Shot Spatio-Temporal Video Grounding with Expectation-Maximization Multimodal Modulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peijun Bao*",
      "Zihao Shao",
      "Wenhan Yang",
      "Boon Poh Ng",
      "Alex Kot"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11028_ECCV_2024_paper.php": {
    "title": "EMO: Emote Portrait Alive - Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linrui Tian*",
      "Qi Wang*",
      "Bang Zhang*",
      "Liefeng Bo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11031_ECCV_2024_paper.php": {
    "title": "LMT-GP: Combined Latent Mean-Teacher and Gaussian Process for Semi-supervised Low-light Image Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ye Yu",
      "Fengxin Chen",
      "Jun Yu*",
      "Zhen Kan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11044_ECCV_2024_paper.php": {
    "title": "Veil Privacy on Visual Data: Concealing Privacy for Humans, Unveiling for DNNs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuchao Pang*",
      "Ruhao Ma",
      "Bing Li*",
      "Yongbin Zhou",
      "Yazhou Yao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11047_ECCV_2024_paper.php": {
    "title": "Efficient Vision Transformers with Partial Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan-Thuy Vo*",
      "Duy-Linh Nguyen",
      "Adri Priadana",
      "Kang-Hyun Jo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11051_ECCV_2024_paper.php": {
    "title": "Generalized Coverage for More Robust Low-Budget Active Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wonho Bae",
      "Junhyug Noh",
      "Danica J. Sutherland*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11059_ECCV_2024_paper.php": {
    "title": "Rasterized Edge Gradients: Handling Discontinuities Differentially",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stanislav Pidhorskyi*",
      "Tomas Simon",
      "Gabriel Schwartz",
      "He Wen",
      "Yaser Sheikh",
      "Jason Saragih"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11062_ECCV_2024_paper.php": {
    "title": "Enhancing Cross-Subject fMRI-to-Video Decoding with Global-Local Functional Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chong Li*",
      "Xuelin Qian",
      "Yun Wang",
      "Jingyang Huo",
      "Xiangyang Xue*",
      "Yanwei Fu*",
      "Jianfeng Feng"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11069_ECCV_2024_paper.php": {
    "title": "FedTSA: A Cluster-based Two-Stage Aggregation Method for Model-heterogeneous Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boyu Fan*",
      "Chenrui Wu",
      "Xiang Su",
      "Pan HUI"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11080_ECCV_2024_paper.php": {
    "title": "LLaVA-UHD: an LMM Perceiving any Aspect Ratio and High-Resolution Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zonghao Guo",
      "Ruyi Xu",
      "Yuan Yao*",
      "Junbo Cui",
      "Zanlin Ni",
      "Chunjiang Ge",
      "Tat-Seng Chua",
      "Zhiyuan Liu",
      "Gao Huang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11103_ECCV_2024_paper.php": {
    "title": "Learning Natural Consistency Representation for Face Forgery Video Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daichi Zhang*",
      "Zihao Xiao",
      "Shikun Li",
      "Fanzhao Lin",
      "Jianmin Li",
      "Shiming Ge*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11109_ECCV_2024_paper.php": {
    "title": "ZeroI2V: Zero-Cost Adaptation of Pre-Trained Transformers from Image to Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinhao Li",
      "Yuhan Zhu",
      "Limin Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11114_ECCV_2024_paper.php": {
    "title": "Zero-Shot Adaptation for Approximate Posterior Sampling of Diffusion Models in Inverse Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yasar U Alcalar*",
      "Mehmet Akcakaya"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11117_ECCV_2024_paper.php": {
    "title": "R.A.C.E.: Robust Adversarial Concept Erasure for Secure Text-to-Image Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changhoon Kim*",
      "Kyle Min*",
      "Yezhou Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11118_ECCV_2024_paper.php": {
    "title": "OpenSight: A Simple Open-Vocabulary Framework for LiDAR-Based Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hu Zhang",
      "xu jianhua",
      "Tao Tang",
      "Haiyang Sun",
      "Xin Yu*",
      "Zi Helen Huang*",
      "Kaicheng Yu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11133_ECCV_2024_paper.php": {
    "title": "Few-Shot Image Generation by Conditional Relaxing Diffusion Inversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Cao*",
      "Shaogang Gong"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11142_ECCV_2024_paper.php": {
    "title": "Data Poisoning Quantization Backdoor Attack",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tran Huynh*",
      "Anh Tran",
      "Khoa Doan",
      "Tung Pham"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11182_ECCV_2024_paper.php": {
    "title": "DailyDVS-200: A Comprehensive Benchmark Dataset for Event-Based Action Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Wang",
      "Zhou Xu",
      "Yuming Lin",
      "Jingtao Ye",
      "Hongsheng Li",
      "Guangming Zhu",
      "Syed Afaq Ali Shah",
      "Mohammed Bennamoun",
      "Liang Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11184_ECCV_2024_paper.php": {
    "title": "On the Topology Awareness and Generalization Performance of Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junwei Su*",
      "Chuan Wu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11187_ECCV_2024_paper.php": {
    "title": "T-CorresNet: Template Guided 3D Point Cloud Completion with Correspondence Pooling Query Generation Strategy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan Duan",
      "Jiahao Yu",
      "Li Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11188_ECCV_2024_paper.php": {
    "title": "A high-quality robust diffusion framework for corrupted dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quan Dao*",
      "Binh Ta",
      "Tung Pham",
      "Anh Tran"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11199_ECCV_2024_paper.php": {
    "title": "Efficient 3D-Aware Facial Image Editing via Attribute-Specific Prompt Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amandeep Kumar*",
      "Muhammad Awais",
      "Sanath Narayan",
      "Hisham Cholakkal",
      "Salman Khan",
      "Rao Muhammad Anwer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11200_ECCV_2024_paper.php": {
    "title": "Distilling Knowledge from Large-Scale Image Models for Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gang Li*",
      "Wenhai Wang",
      "Xiang Li",
      "Ziheng Li",
      "Jian Yang",
      "Jifeng Dai",
      "Yu Qiao",
      "Shanshan Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11201_ECCV_2024_paper.php": {
    "title": "Embracing Events and Frames with Hierarchical Feature Refinement Network for Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hu Cao",
      "Zehua Zhang",
      "Yan Xia",
      "Xinyi Li",
      "Jiahao Xia",
      "Guang Chen*",
      "Alois C. Knoll"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11208_ECCV_2024_paper.php": {
    "title": "TimeLens-XL: Real-time Event-based Video Frame Interpolation with Large Motion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shi Guo",
      "Yutian Chen",
      "Tianfan Xue",
      "Jinwei Gu",
      "Yongrui Ma*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11211_ECCV_2024_paper.php": {
    "title": "Scene-Graph ViT: End-to-End Open-Vocabulary Visual Relationship Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tim Salzmann",
      "Markus Ryll",
      "Alex Bewley",
      "Matthias Minderer*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11219_ECCV_2024_paper.php": {
    "title": "Self-Supervised Underwater Caustics Removal and Descattering via Deep Monocular SLAM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonathan Sauder*",
      "Devis Tuia"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11222_ECCV_2024_paper.php": {
    "title": "Enriching Information and Preserving Semantic Consistency in Expanding Curvilinear Object Segmentation Datasets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qin Lei*",
      "Jiang Zhong",
      "Qizhu Dai"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11229_ECCV_2024_paper.php": {
    "title": "Retrieval Robust to Object Motion Blur",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rong Zou",
      "Marc Pollefeys",
      "Denys Rozumnyi*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11230_ECCV_2024_paper.php": {
    "title": "Unsupervised Representation Learning by Balanced Self Attention Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Shalam*",
      "Simon Korman*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11238_ECCV_2024_paper.php": {
    "title": "DualBEV: Unifying Dual View Transformation with Probabilistic Correspondences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peidong Li*",
      "Wancheng Shen",
      "Qihao Huang",
      "Dixiao Cui*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11241_ECCV_2024_paper.php": {
    "title": "Identity-Consistent Diffusion Network for Grading Knee Osteoarthritis Progression in Radiographic Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhua Wu",
      "Kun Hu*",
      "Wenxi Yue",
      "Wei Li",
      "Milena Simic",
      "Changyang Li",
      "Wei Xiang",
      "Zhiyong Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11249_ECCV_2024_paper.php": {
    "title": "Learned Neural Physics Simulation for Articulated 3D Human Pose Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Misha Andriluka*",
      "Baruch Tabanpour",
      "Daniel Freeman",
      "Cristian Sminchisescu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11254_ECCV_2024_paper.php": {
    "title": "Enhancing Source-Free Domain Adaptive Object Detection with Low-confidence Pseudo Label Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ilhoon Yoon",
      "Hyeongjun Kwon",
      "Jin Kim",
      "Junyoung Park",
      "Hyunsung Jang",
      "Kwanghoon Sohn*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11278_ECCV_2024_paper.php": {
    "title": "Fast Training of Diffusion Transformer with Extreme Masking for 3D Point Clouds Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shentong Mo",
      "Enze Xie*",
      "Yue Wu",
      "Junsong Chen",
      "Matthias Niessner",
      "Zhenguo Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11288_ECCV_2024_paper.php": {
    "title": "Make a Strong Teacher with Label Assistance: A Novel Knowledge Distillation Approach for Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shoumeng Qiu",
      "Jie Chen",
      "Xinrun Li",
      "Ru Wan",
      "Xiangyang Xue",
      "Jian Pu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11290_ECCV_2024_paper.php": {
    "title": "Make-Your-3D: Fast and Consistent Subject-Driven 3D Content Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangfu Liu",
      "Hanyang Wang",
      "Weiliang Chen",
      "Haowen Sun",
      "Yueqi Duan*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11297_ECCV_2024_paper.php": {
    "title": "Segment, Lift and Fit: Automatic 3D Shape Labeling from 2D Prompts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianhao Li",
      "Tianyu Sun",
      "Zhongdao Wang*",
      "Enze Xie",
      "Bailan Feng",
      "Hongbo Zhang",
      "Ze Yuan",
      "Ke Xu",
      "Jiaheng Liu*",
      "Ping Luo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11303_ECCV_2024_paper.php": {
    "title": "SCOD: From Heuristics to Theory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vojtech Franc*",
      "Jakub Paplham*",
      "Daniel Prusa*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11330_ECCV_2024_paper.php": {
    "title": "Preventing Catastrophic Forgetting through Memory Networks in Continuous Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gaurav Bhatt*",
      "Leonid Sigal",
      "James Ross"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11332_ECCV_2024_paper.php": {
    "title": "Improving Zero-shot Generalization of Learned Prompts via Unsupervised Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marco Mistretta*",
      "Alberto Baldrati",
      "Marco Bertini",
      "Andrew D. Bagdanov"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11339_ECCV_2024_paper.php": {
    "title": "Teach CLIP to Develop a Number Sense for Ordinal Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yao DU*",
      "Qiang Zhai",
      "Weihang Dai",
      "Xiaomeng Li*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11348_ECCV_2024_paper.php": {
    "title": "Compact 3D Scene Representation via Self-Organizing Gaussian Grids",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wieland Morgenstern*",
      "Florian Barthel",
      "Anna Hilsmann",
      "Peter Eisert"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11350_ECCV_2024_paper.php": {
    "title": "Pix2Gif: Motion-Guided Diffusion for GIF Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hitesh Kandala*",
      "Jianfeng Gao",
      "Jianwei Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11352_ECCV_2024_paper.php": {
    "title": "VETRA: A Dataset for Vehicle Tracking in Aerial Imagery - New Challenges for Multi-Object Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jens Hellekes*",
      "Manuel Mühlhaus",
      "Reza Bahmanyar",
      "Seyed Majid Azimi",
      "Franz Kurz"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11356_ECCV_2024_paper.php": {
    "title": "SelfGeo: Self-supervised and Geodesic-consistent Estimation of Keypoints on Deformable Shapes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Zohaib*",
      "Luca Cosmo",
      "Alessio Del Bue"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11358_ECCV_2024_paper.php": {
    "title": "Beyond Prompt Learning: Continual Adapter for Efficient Rehearsal-Free Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyuan Gao",
      "Songlin Dong",
      "Yuhang He*",
      "Qiang Wang",
      "Yihong Gong"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11361_ECCV_2024_paper.php": {
    "title": "T2IShield: Defending Against Backdoors on Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongqi Wang",
      "Jie Zhang*",
      "Shiguang Shan",
      "Xilin Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11377_ECCV_2024_paper.php": {
    "title": "ExMatch: Self-guided Exploitation for Semi-Supervised Learning with Scarce Labeled Samples",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Noo-ri Kim",
      "Jin-Seop Lee",
      "Jee-Hyong Lee*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11379_ECCV_2024_paper.php": {
    "title": "Towards Certifiably Robust Face Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seunghun Paik",
      "Dongsoo Kim",
      "Chanwoo Hwang",
      "Sunpill Kim",
      "Jae Hong Seo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11380_ECCV_2024_paper.php": {
    "title": "Linking in Style: Understanding learned features in deep learning models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maren Wehrheim*",
      "Pamela Osuna Vargas",
      "Matthias Kaschube"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11389_ECCV_2024_paper.php": {
    "title": "Stable Video Portraits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mirela Ostrek*",
      "Justus Thies"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11391_ECCV_2024_paper.php": {
    "title": "UDA-Bench: Revisiting Common Assumptions in Unsupervised Domain Adaptation Using a Standardized Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tarun Kalluri*",
      "Sreyas Ravichandran",
      "Manmohan Chandraker"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11393_ECCV_2024_paper.php": {
    "title": "CliffPhys: Camera-based Respiratory Measurement using Clifford Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Omar Ghezzi*",
      "Giuseppe Boccignone",
      "Giuliano Grossi",
      "Raffaella Lanzarotti",
      "Alessandro D'Amelio"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11394_ECCV_2024_paper.php": {
    "title": "Learned Rate Control for Frame-Level Adaptive Neural Video Compression via Dynamic Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenhao Zhang",
      "Wei Gao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11397_ECCV_2024_paper.php": {
    "title": "PDiscoFormer: Relaxing Part Discovery Constraints with Vision Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ananthu Aniraj*",
      "Cassio F. Dantas",
      "Dino Ienco",
      "Diego Marcos"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11399_ECCV_2024_paper.php": {
    "title": "Vision-Language Dual-Pattern Matching for Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihan Zhang",
      "Zhuo Xu",
      "Xiang Xiang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11417_ECCV_2024_paper.php": {
    "title": "Synthesizing Environment-Specific People in Photographs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mirela Ostrek*",
      "Carol O'Sullivan",
      "Michael J. Black",
      "Justus Thies"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11424_ECCV_2024_paper.php": {
    "title": "Weight Conditioning for Smooth Optimization of Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hemanth Saratchandran*",
      "Thomas X Wang",
      "Simon Lucey"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11433_ECCV_2024_paper.php": {
    "title": "Energy-Clibrated VAE with Test Time Free Lunch",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihong Luo",
      "Siya Qiu",
      "Xingjian Tao",
      "Yujun Cai",
      "Jing Tang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11465_ECCV_2024_paper.php": {
    "title": "MoEAD: A Parameter-efficient Model for Multi-class Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiyuan Meng",
      "Wenchao Meng*",
      "Qihang Zhou",
      "Shizhong Li",
      "Weiye Hou",
      "Shibo He"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11481_ECCV_2024_paper.php": {
    "title": "SceneTeller: Language-to-3D Scene Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Basak Melis Ocal*",
      "Maxim Tatarchenko",
      "Sezer Karaoglu",
      "Theo Gevers"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11489_ECCV_2024_paper.php": {
    "title": "MagMax: Leveraging Model Merging for Seamless Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Marczak*",
      "Bartlomiej Twardowski*",
      "Tomasz Trzcinski*",
      "Sebastian Cygert*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11491_ECCV_2024_paper.php": {
    "title": "InternVideo2: Scaling Foundation Models for Multimodal Video Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Wang*",
      "Kunchang Li",
      "Xinhao Li",
      "Jiashuo Yu",
      "Yinan He",
      "Guo Chen",
      "Baoqi Pei",
      "Rongkun Zheng",
      "Jilan Xu",
      "Zun Wang",
      "Yansong Shi",
      "Tianxiang Jiang",
      "SongZe Li",
      "hongjie Zhang",
      "Yifei Huang",
      "Yu Qiao*",
      "Yali Wang*",
      "Limin Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11492_ECCV_2024_paper.php": {
    "title": "DiffusionPen: Towards Controlling the Style of Handwritten Text Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Konstantina Nikolaidou*",
      "George Retsinas",
      "Giorgos Sfikas",
      "Marcus Liwicki"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11496_ECCV_2024_paper.php": {
    "title": "Debiasing surgeon: fantastic weights and how to find them",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Remi Nahon",
      "Ivan Luiz De Moura Matos",
      "Van-Tam Nguyen",
      "Enzo Tartaglione*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11504_ECCV_2024_paper.php": {
    "title": "Denoising Vision Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawei Yang*",
      "Katie Z Luo",
      "Jiefeng Li",
      "Congyue Deng",
      "Leonidas Guibas",
      "Dilip Krishnan",
      "Kilian Weinberger",
      "Yonglong Tian",
      "Yue Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11515_ECCV_2024_paper.php": {
    "title": "Differentiable Product Quantization for Memory Efficient Camera Relocalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zakaria Laskar*",
      "Iaroslav Melekhov",
      "Assia Benbihi",
      "Shuzhe Wang",
      "Juho Kannala"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11525_ECCV_2024_paper.php": {
    "title": "Spline-based Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prashanth Chandran*",
      "Agon Serifi*",
      "Markus Gross",
      "Moritz Bächer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11528_ECCV_2024_paper.php": {
    "title": "Learning Pseudo 3D Guidance for View-consistent Texturing with 2D Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kehan Li",
      "Yanbo Fan*",
      "Yang Wu",
      "Zhongqian Sun",
      "Wei Yang",
      "Xiangyang Ji",
      "Li Yuan",
      "Jie Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11535_ECCV_2024_paper.php": {
    "title": "TreeSBA: Tree-Transformer for Self-Supervised Sequential Brick Assembly",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengqi Guo*",
      "Chen Li",
      "Yuyang Zhao",
      "Gim Hee Lee"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11537_ECCV_2024_paper.php": {
    "title": "SparseRadNet: Sparse Perception Neural Network on Subsampled Radar Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jialong Wu*",
      "Mirko Meuter",
      "Markus Schoeler",
      "Matthias Rottmann"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11554_ECCV_2024_paper.php": {
    "title": "Enhancing Semantic Fidelity in Text-to-Image Synthesis: Attention Regulation in Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Zhang*",
      "Tze Tzun Teoh",
      "Wei Hern Lim",
      "Kenji Kawaguchi"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11557_ECCV_2024_paper.php": {
    "title": "Adversarial Diffusion Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Axel Sauer*",
      "Dominik Lorenz",
      "Andreas Blattmann",
      "Robin Rombach"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11581_ECCV_2024_paper.php": {
    "title": "Fake It till You Make It: Curricular Dynamic Forgery Augmentations towards General Deepfake Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuzhen Lin*",
      "Wentang Song",
      "Bin Li*",
      "Yuezun Li",
      "Jiangqun Ni",
      "Han Chen",
      "Qiushi Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11588_ECCV_2024_paper.php": {
    "title": "Explain via Any Concept: Concept Bottleneck Model with Open Vocabulary Concepts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andong Tan",
      "Fengtao Zhou",
      "Hao Chen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11599_ECCV_2024_paper.php": {
    "title": "Explore the Potential of CLIP for Training-Free Open Vocabulary Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tong Shao",
      "Zhuotao Tian*",
      "Hang Zhao",
      "Jingyong Su*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11606_ECCV_2024_paper.php": {
    "title": "A Multimodal Benchmark Dataset and Model for Crop Disease Diagnosis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Liu",
      "Zhaoxiang Liu*",
      "Huan Hu",
      "Zezhou Chen",
      "Kohou Wang",
      "Kai Wang",
      "Shiguo Lian*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11614_ECCV_2024_paper.php": {
    "title": "Missing Modality Prediction for Unpaired Multimodal Learning via Joint Embedding of Unimodal Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taesup Kim*",
      "Donggeun Kim"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11621_ECCV_2024_paper.php": {
    "title": "Learning Where to Look: Self-supervised Viewpoint Selection for Active Localization using Geometrical Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luca Di Giammarino*",
      "Boyang Sun",
      "Giorgio Grisetti",
      "Marc Pollefeys",
      "Hermann Blum",
      "Daniel Barath"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11626_ECCV_2024_paper.php": {
    "title": "Improving Diffusion Models for Authentic Virtual Try-on in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yisol Choi*",
      "Sangkyung Kwak",
      "Kyungmin Lee",
      "Hyungwon Choi",
      "Jinwoo Shin*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11636_ECCV_2024_paper.php": {
    "title": "Exploiting Semantic Reconstruction to Mitigate Hallucinations in Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minchan Kim",
      "Minyeong Kim",
      "Junik Bae",
      "Suhwan Choi",
      "Sungkyung Kim",
      "Buru Chang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11661_ECCV_2024_paper.php": {
    "title": "LISO: Lidar-only Self-Supervised 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stefan Andreas Baur*",
      "Frank Moosmann",
      "Andreas Geiger"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11664_ECCV_2024_paper.php": {
    "title": "Text-Conditioned Resampler For Long Form Video Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bruno Korbar*",
      "Yongqin Xian",
      "Alessio Tonioni",
      "Andrew Zisserman",
      "Federico Tombari"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11671_ECCV_2024_paper.php": {
    "title": "Implicit Steganography Beyond the Constraints of Modality",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sojeong Song*",
      "Seoyun Yang*",
      "Chang D. Yoo*",
      "Junmo Kim*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11672_ECCV_2024_paper.php": {
    "title": "Using My Artistic Style? You Must Obtain My Authorization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiuli Bi",
      "Haowei Liu",
      "Weisheng Li",
      "Bo Liu*",
      "Bin Xiao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11673_ECCV_2024_paper.php": {
    "title": "LookupViT: Compressing visual information to a limited number of tokens",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rajat Koner",
      "Gagan Jain",
      "Sujoy Paul*",
      "Volker Tresp",
      "Prateek Jain"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11674_ECCV_2024_paper.php": {
    "title": "Fast Diffusion-Based Counterfactuals for Shortcut Removal and Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nina Weng*",
      "Paraskevas Pegios",
      "Eike Petersen",
      "Aasa Feragen",
      "Siavash Arjomand Bigdeli"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11688_ECCV_2024_paper.php": {
    "title": "UMERegRobust – Universal Manifold Embedding Compatible Features for Robust Point Cloud Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuval Haitman*",
      "Amit Efraim",
      "Joseph M Francos"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11714_ECCV_2024_paper.php": {
    "title": "Non-transferable Pruning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruyi Ding*",
      "Lili Su",
      "A. Adam Ding",
      "Yunsi Fei"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11720_ECCV_2024_paper.php": {
    "title": "A Compact Dynamic 3D Gaussian Representation for Real-Time Dynamic View Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Katsumata*",
      "Duc Minh Vo",
      "Hideki Nakayama"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11739_ECCV_2024_paper.php": {
    "title": "Fast Context-Based Low-Light Image Enhancement via Neural Implicit Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tomáš Chobola*",
      "Yu Liu",
      "Hanyi Zhang",
      "Julia A Schnabel",
      "Tingying Peng*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11741_ECCV_2024_paper.php": {
    "title": "Toward Open Vocabulary Aerial Object Detection with CLIP-Activated Student-Teacher Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Li",
      "Weiwei Guo*",
      "Xue Yang",
      "Ning Liao",
      "Dunyun He",
      "Jiaqi Zhou",
      "Wenxian Yu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11742_ECCV_2024_paper.php": {
    "title": "Affine steerers for structured keypoint description",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Georg Bökman*",
      "Johan Edstedt",
      "Michael Felsberg",
      "Fredrik Kahl"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11754_ECCV_2024_paper.php": {
    "title": "Score Distillation Sampling with Learned Manifold Corrective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thiemo Alldieck*",
      "Nikos Kolotouros",
      "Cristian Sminchisescu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11758_ECCV_2024_paper.php": {
    "title": "FipTR: A Simple yet Effective Transformer Framework for Future Instance Prediction in Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingtai Gui*",
      "Tengteng Huang",
      "Haonan Shao",
      "Haotian Yao",
      "Chi Zhang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11762_ECCV_2024_paper.php": {
    "title": "Benchmarking the Robustness of Cross-view Geo-localization Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingwang Zhang",
      "Yingying Zhu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11763_ECCV_2024_paper.php": {
    "title": "GroCo: Ground Constraint for Metric Self-Supervised Monocular Depth",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aurélien Cecille*",
      "Stefan Duffner",
      "Franck Davoine",
      "Thibault Neveu",
      "Rémi Agier"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11787_ECCV_2024_paper.php": {
    "title": "SUMix: Mixup with Semantic and Uncertain Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huafeng Qin",
      "Xin Jin*",
      "Hongyu Zhu",
      "Hongchao Liao",
      "Mounim A. El Yacoubi",
      "Xinbo Gao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11789_ECCV_2024_paper.php": {
    "title": "Flatness-aware Sequential Learning Generates Resilient Backdoors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hoang Pham*",
      "The-Anh Ta",
      "Anh T Tran",
      "Khoa D Doan"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11800_ECCV_2024_paper.php": {
    "title": "Iterative Ensemble Training with Anti-Gradient Control for Mitigating Memorization in Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Liu",
      "Xiaoliu Guan",
      "Yu Wu*",
      "Jiaxu Miao*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11802_ECCV_2024_paper.php": {
    "title": "IFTR: An Instance-Level Fusion Transformer for Visual Collaborative Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaohong Wang",
      "Lu Bin",
      "Xinyu Xiao",
      "Zhiyu Xiang",
      "Hangguan Shan",
      "Eryun Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11819_ECCV_2024_paper.php": {
    "title": "DiffClass: Diffusion-Based Class Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zichong Meng",
      "Jie Zhang",
      "Changdi Yang",
      "Zheng Zhan",
      "Pu Zhao*",
      "Yanzhi Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11823_ECCV_2024_paper.php": {
    "title": "Convex Relaxations for Manifold-Valued Markov Random Fields with Approximation Guarantees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Robin Kenis*",
      "Emanuel Laude",
      "Panagiotis Patrinos"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11825_ECCV_2024_paper.php": {
    "title": "Instant 3D Human Avatar Generation using Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikos Kolotouros*",
      "Thiemo Alldieck",
      "Enric Corona",
      "Eduard Gabriel Bazavan",
      "Cristian Sminchisescu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11826_ECCV_2024_paper.php": {
    "title": "PromptFusion: Decoupling Stability and Plasticity for Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoran Chen",
      "Zuxuan Wu*",
      "Xintong Han",
      "Menglin Jia",
      "Yu-Gang Jiang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11844_ECCV_2024_paper.php": {
    "title": "Improving Geo-diversity of Generated Images with Contextualized Vendi Score Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Reyhane Askari Hemmat*",
      "Melissa Hall*",
      "Alicia Yi Sun",
      "Candace Ross",
      "Michal Drozdzal",
      "Adriana Romero-Soriano"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11855_ECCV_2024_paper.php": {
    "title": "Adapting to Shifting Correlations with Unlabeled Data Calibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minh Nguyen*",
      "Alan Q Wang",
      "Heejong Kim",
      "Mert Sabuncu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11856_ECCV_2024_paper.php": {
    "title": "Masked Generative Video-to-Audio Transformers with Enhanced Synchronicity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Santiago Pascual",
      "Chunghsin YEH*",
      "Ioannis Tsiamas",
      "Joan Serrà"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11862_ECCV_2024_paper.php": {
    "title": "Information Bottleneck Based Data Correction in Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuai Chen",
      "mingyi zhang",
      "Junge Zhang*",
      "Kaiqi Huang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11865_ECCV_2024_paper.php": {
    "title": "On Spectral Properties of Gradient-based Explanation Methods",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amir Mehrpanah*",
      "Erik Englesson",
      "Hossein Azizpour"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11870_ECCV_2024_paper.php": {
    "title": "Contextual Correspondence Matters: Bidirectional Graph Matching for Video Summarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunzuo Zhang*",
      "Yameng Liu"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11877_ECCV_2024_paper.php": {
    "title": "O2V-Mapping: Online Open-Vocabulary Mapping with Neural Implicit Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muer Tie",
      "Julong Wei",
      "Zhengjun Wang",
      "Ke Wu",
      "Shanshuai Yuan",
      "Kaizhao Zhang",
      "Jie Jia",
      "Jieru Zhao",
      "Zhongxue Gan*",
      "Wenchao Ding*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11886_ECCV_2024_paper.php": {
    "title": "Dataset Distillation by Automatic Training Trajectories",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dai Liu*",
      "Jindong Gu*",
      "Hu Cao",
      "Carsten Trinitis",
      "Martin Schulz*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11904_ECCV_2024_paper.php": {
    "title": "FAFA: Frequency-Aware Flow-Aided Self-Supervision for Underwater Object Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyi Tang*",
      "Gu Wang",
      "Zeyu Chen",
      "Shengquan Li",
      "Xiu Li*",
      "Xiangyang Ji"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11918_ECCV_2024_paper.php": {
    "title": "EMIE-MAP: Large-Scale Road Surface Reconstruction Based on Explicit Mesh and Implicit Encoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhua Wu",
      "Qi Wang",
      "Guangming Wang",
      "Junping Wang",
      "Tiankun Zhao",
      "Yang Liu",
      "Dongchao Gao",
      "Zhe Liu*",
      "Hesheng Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11927_ECCV_2024_paper.php": {
    "title": "UniIR: Training and Benchmarking Universal Multimodal Information Retrievers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cong Wei*",
      "Yang Chen",
      "Haonan Chen",
      "Hexiang Hu",
      "Ge Zhang",
      "Jie Fu",
      "Alan Ritter",
      "Wenhu Chen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11933_ECCV_2024_paper.php": {
    "title": "SSL-Cleanse: Trojan Detection and Mitigation in Self-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengxin Zheng*",
      "Jiaqi Xue",
      "Zihao Wang",
      "Xun Chen",
      "Qian Lou",
      "Lei Jiang",
      "Xiaofeng Wang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11936_ECCV_2024_paper.php": {
    "title": "Skews in the Phenomenon Space Hinder Generalization in Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingshan Chang*",
      "Yasi Zhang",
      "Zhiyuan Fang",
      "Ying Nian Wu",
      "Yonatan Bisk",
      "Feng Gao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11949_ECCV_2024_paper.php": {
    "title": "Bones Can't Be Triangles: Accurate and Efficient Vertebrae Keypoint Estimation through Collaborative Error Revision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinhee Kim",
      "Taesung Kim",
      "Jaegul Choo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11950_ECCV_2024_paper.php": {
    "title": "latentSplat: Autoencoding Variational Gaussians for Fast Generalizable 3D Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christopher Wewer*",
      "Kevin Raj",
      "Eddy Ilg",
      "Bernt Schiele",
      "Jan E. Lenssen*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11979_ECCV_2024_paper.php": {
    "title": "HyperSpaceX: Radial and Angular Exploration of HyperSpherical Dimensions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chiranjeev Chiranjeev",
      "Muskan Dosi",
      "Kartik Thakral",
      "Mayank Vatsa*",
      "Richa Singh"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12016_ECCV_2024_paper.php": {
    "title": "InstructGIE: Towards Generalizable Image Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zichong Meng",
      "Changdi Yang",
      "Jun Liu",
      "Hao Tang*",
      "Pu Zhao*",
      "Yanzhi Wang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12018_ECCV_2024_paper.php": {
    "title": "HandDAGT: A Denoising Adaptive Graph Transformer for 3D Hand Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "WENCAN CHENG",
      "Eunji Kim",
      "Jong Hwan Ko*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12020_ECCV_2024_paper.php": {
    "title": "Navigating Text-to-Image Generative Bias across Indic Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Surbhi Mittal*",
      "Arnav Sudan",
      "Mayank Vatsa*",
      "Richa Singh",
      "Tamar Glaser",
      "Tal Hassner"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12030_ECCV_2024_paper.php": {
    "title": "Correspondence-Free SE(3) Point Cloud Registration in RKHS via Unsupervised Equivariant Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ray Zhang*",
      "Zheming Zhou",
      "Min Sun",
      "Omid Ghasemalizadeh",
      "Cheng-Hao Kuo",
      "Ryan M. Eustice",
      "Maani Ghaffari Jadidi",
      "Arnie Sen"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12052_ECCV_2024_paper.php": {
    "title": "CTRLorALTer: Conditional LoRAdapter for Efficient 0-Shot Control & Altering of T2I Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nick Stracke*",
      "Stefan Andreas Baumann",
      "Joshua Susskind",
      "Miguel Angel Bautista",
      "Bjorn Ommer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12073_ECCV_2024_paper.php": {
    "title": "Nickel and Diming Your GAN: A Dual-Method Approach to Enhancing GAN Efficiency via Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sangyeop Yeo",
      "Yoojin Jang",
      "Jaejun Yoo*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12075_ECCV_2024_paper.php": {
    "title": "VividDreamer: Invariant Score Distillation for Hyper-Realistic Text-to-3D Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenjie Zhuo*",
      "Fan Ma",
      "Hehe Fan",
      "Yi Yang"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12117_ECCV_2024_paper.php": {
    "title": "A Framework for Efficient Model Evaluation through Stratification, Sampling, and Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Riccardo Fogliato*",
      "Pratik Patil",
      "Mathew Monfort",
      "Pietro Perona"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12130_ECCV_2024_paper.php": {
    "title": "Towards Scene Graph Anticipation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rohith Peddi*",
      "Saksham Singh",
      "Saurabh .",
      "Parag Singla",
      "Vibhav Gogate"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12133_ECCV_2024_paper.php": {
    "title": "Non-Line-of-Sight Estimation of Fast Human Motion with Slow Scanning Imagers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Javier Grau Chopite*",
      "Patrick Hähn",
      "Matthias B Hullin*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12134_ECCV_2024_paper.php": {
    "title": "Distributed Semantic Segmentation with Efficient Joint Source and Task Decoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Danish Nazir*",
      "Timo Bartels",
      "Jan Piewek",
      "Thorsten Bagdonat",
      "Tim Fingscheidt"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12148_ECCV_2024_paper.php": {
    "title": "NePhi: Neural Deformation Fields for Approximately Diffeomorphic Medical Image Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lin Tian*",
      "Thomas H Greer",
      "Raul San Jose Estepar",
      "Roni Sengupta",
      "Marc Niethammer"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12149_ECCV_2024_paper.php": {
    "title": "Aligning Neuronal Coding of Dynamic Visual Scenes with Foundation Vision Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rining Wu*",
      "Feixiang Zhou",
      "Ziwei Yin",
      "Jian Liu*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12164_ECCV_2024_paper.php": {
    "title": "Image Manipulation Detection With Implicit Neural Representation and Limited Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenfei Zhang*",
      "Mingyang Li",
      "Xin Li",
      "Ming-Ching Chang",
      "Jun-Wei Hsieh"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12191_ECCV_2024_paper.php": {
    "title": "Scalar Function Topology Divergence: Comparing Topology of 3D Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ilya Trofimov*",
      "Daria Voronkova",
      "Eduard Tulchinskii",
      "Evgeny Burnaev",
      "Serguei Barannikov"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12192_ECCV_2024_paper.php": {
    "title": "Introducing Routing Functions to Vision-Language Parameter-Efficient Fine-Tuning with Low-Rank Bottlenecks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tingyu Qu*",
      "Tinne Tuytelaars",
      "Marie-Francine Moens"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12206_ECCV_2024_paper.php": {
    "title": "Concept Arithmetics for Circumventing Concept Inhibition in Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vitali Petsiuk*",
      "Kate Saenko"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12207_ECCV_2024_paper.php": {
    "title": "DeTra: A Unified Model for Object Detection and Trajectory Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sergio Casas*",
      "Ben T Agro",
      "Jiageng Mao",
      "Thomas Gilles",
      "ALEXANDER Y CUI",
      "Enxu Li",
      "Raquel Urtasun"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12254_ECCV_2024_paper.php": {
    "title": "ControlNet-XS: Rethinking the Control of Text-to-Image Diffusion Models as Feedback-Control Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Denis Zavadski*",
      "Johann-Friedrich Feiden",
      "Carsten Rother"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12292_ECCV_2024_paper.php": {
    "title": "Adaptive Bounding Box Uncertainties via Two-Step Conformal Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Timans*",
      "Christoph-Nikolas Straehle",
      "Kaspar Sakmann",
      "Eric Nalisnick"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12295_ECCV_2024_paper.php": {
    "title": "Common Sense Reasoning for Deep Fake Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Zhang*",
      "Ben Colman",
      "Xiao Guo",
      "Ali Shahriyari",
      "Gaurav Bharaj*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12305_ECCV_2024_paper.php": {
    "title": "Let the Avatar Talk using Texts without Paired Training Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiuzhe Wu",
      "Yang-Tian Sun",
      "Handi Chen",
      "Hang Zhou",
      "Jingdong Wang",
      "Zhengzhe Liu",
      "Xiaojuan Qi*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12306_ECCV_2024_paper.php": {
    "title": "NeRF-MAE: Masked AutoEncoders for Self-Supervised 3D Representation Learning for Neural Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Zubair Irshad*",
      "Sergey Zakharov",
      "Vitor Guizilini",
      "Adrien Gaidon",
      "Zsolt Kira",
      "Rares Ambrus"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12324_ECCV_2024_paper.php": {
    "title": "GOEmbed: Gradient Origin Embeddings for Representation Agnostic 3D Feature Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Animesh Karnewar*",
      "Roman Shapovalov",
      "Tom Monnier",
      "Andrea Vedaldi",
      "Niloy J. Mitra*",
      "David Novotny*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12325_ECCV_2024_paper.php": {
    "title": "Causal Subgraphs and Information Bottlenecks: Redefining OOD Robustness in Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weizhi An",
      "Wenliang Zhong",
      "Feng Jiang",
      "Hehuan Ma",
      "Junzhou Huang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12349_ECCV_2024_paper.php": {
    "title": "AddBiomechanics Dataset: Capturing the Physics of Human Motion at Scale",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keenon Werling*",
      "Janelle M Kaneda",
      "Tian Tan",
      "Rishi Agarwal",
      "Six Skov",
      "Tom Van Wouwe",
      "Scott Uhlrich",
      "Scott Delp",
      "Karen Liu",
      "Nicholas A Bianco",
      "Carmichael Ong",
      "Antoine Falisse",
      "Shardul Sapkota",
      "Aidan Jai Chandra",
      "Joshua A Carter",
      "Ezio Preatoni",
      "Benjamin J Fregly",
      "Jennifer Hicks"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12363_ECCV_2024_paper.php": {
    "title": "How to Train the Teacher Model for Effective Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shayan Mohajer Hamidi*",
      "Xizhen Deng",
      "Renhao Tan",
      "Linfeng Ye",
      "Ahmed Hussein Salamah"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12373_ECCV_2024_paper.php": {
    "title": "Tight and Efficient Upper Bound on Spectral Norm of Convolutional Layers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ekaterina Grishina*",
      "Mikhail Gorbunov",
      "Maxim Rakhuba"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12388_ECCV_2024_paper.php": {
    "title": "Deciphering the Role of Representation Disentanglement: Investigating Compositional Generalization in CLIP Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Reza Abbasi",
      "Mohammad Rohban",
      "Mahdieh Soleymani Baghshah*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12401_ECCV_2024_paper.php": {
    "title": "Modality Translation for Object Detection Adaptation without forgetting prior knowledge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heitor Rapela Medeiros*",
      "Masih Aminbeidokhti",
      "Fidel A Guerrero Pena",
      "David Latortue",
      "Eric Granger",
      "Marco Pedersoli"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12410_ECCV_2024_paper.php": {
    "title": "FroSSL: Frobenius Norm Minimization for Efficient Multiview Self-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oscar Skean*",
      "Aayush Dhakal",
      "Nathan Jacobs",
      "Luis G Sanchez Giraldo"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12427_ECCV_2024_paper.php": {
    "title": "Learning Multimodal Latent Generative Models with Energy-Based Prior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiyu Yuan*",
      "Jiali Cui",
      "Hanao Li",
      "Tian Han"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12446_ECCV_2024_paper.php": {
    "title": "On Learning Discriminative Features from Synthesized Data for Self-Supervised Fine-Grained Visual Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihu Wang*",
      "Lingqiao Liu",
      "Scott Ricardo Figueroa Weston",
      "Samuel Tian",
      "Peng Li"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12460_ECCV_2024_paper.php": {
    "title": "LaWa: Using Latent Space for In-Generation Image Watermarking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ahmad Rezaei*",
      "Mohammad Akbari*",
      "Saeed Ranjbar Alvar",
      "Arezou Fatemi",
      "Yong Zhang*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12475_ECCV_2024_paper.php": {
    "title": "Hierarchical Conditioning of Diffusion Models Using Tree-of-Life for Studying Species Evolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mridul Khurana*",
      "Arka Daw",
      "M. Maruf",
      "Josef C. Uyeda",
      "Wasila Dahdul",
      "Caleb Charpentier",
      "Yasin Bakış",
      "Henry L. Bart Jr.",
      "Paula M. Mabee",
      "Hilmar Lapp",
      "James P. Balhoff",
      "Wei-Lun Chao",
      "Charles Stewart",
      "Tanya Berger-Wolf",
      "Anuj Karpatne*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12478_ECCV_2024_paper.php": {
    "title": "Markov Knowledge Distillation: Make Nasty Teachers trained by Self-undermining Knowledge Distillation Fully Distillable",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "En-hui Yang",
      "Linfeng Ye*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12483_ECCV_2024_paper.php": {
    "title": "Co-speech Gesture Video Generation with 3D Human Meshes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aniruddha Mahapatra*",
      "Richa Mishra*",
      "Ziyi Chen",
      "Boyang Ding",
      "Renda Li",
      "Shoulei Wang",
      "Jun-Yan Zhu",
      "Peng Chang",
      "Mei Han",
      "Jing Xiao"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12484_ECCV_2024_paper.php": {
    "title": "When and How do negative prompts take effect?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanhao Ban",
      "Ruochen Wang",
      "Tianyi Zhou",
      "Minhao Cheng",
      "Boqing Gong",
      "Cho-Jui Hsieh*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12486_ECCV_2024_paper.php": {
    "title": "GS2Mesh: Surface Reconstruction from Gaussian Splatting via Novel Stereo Views",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaniv Wolf*",
      "Amit Bracha",
      "Ron Kimmel"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12520_ECCV_2024_paper.php": {
    "title": "CARFF: Conditional Auto-encoded Radiance Field for 3D Scene Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiezhi Yang*",
      "Khushi P Desai*",
      "Charles Packer*",
      "Harshil bhatia",
      "Nicholas Rhinehart",
      "Rowan McAllister",
      "Joseph E Gonzalez*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12535_ECCV_2024_paper.php": {
    "title": "Snuffy: Efficient Whole Slide Image Classifier",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hossein Jafarinia*",
      "Alireza Alipanah",
      "Saeed Razavi",
      "Nahal Mirzaie",
      "Mohammad Hossein Rohban*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12536_ECCV_2024_paper.php": {
    "title": "Learning to Build by Building Your Own Instructions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aaron T Walsman*",
      "Muru Zhang",
      "Adam Fishman",
      "Ali Farhadi",
      "Dieter Fox"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12574_ECCV_2024_paper.php": {
    "title": "Exploring Active Learning in Meta-Learning: Enhancing Context Set Labeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wonho Bae",
      "Jing Wang",
      "Danica J. Sutherland*"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12578_ECCV_2024_paper.php": {
    "title": "BlenderAlchemy: Editing 3D Graphics with Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ian Huang*",
      "Guandao Yang",
      "Leonidas Guibas"
    ]
  },
  "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12638_ECCV_2024_paper.php": {
    "title": "DεpS: Delayed ε-Shrinking for Faster Once-For-All Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditya Annavajjala*",
      "Alind Khare*",
      "Animesh Agrawal",
      "Igor Fedorov",
      "Hugo M Latapie",
      "Myungjin Lee",
      "Alexey Tumanov"
    ]
  }
}