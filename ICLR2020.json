{
  "https://openreview.net/forum?id=BkluqlSFDS": {
    "title": "Federated Learning with Matched Averaging",
    "volume": "oral",
    "abstract": "Federated learning allows edge devices to collaboratively learn a shared model while keeping the training data on device, decoupling the ability to do model training from the need to store the data in the cloud. We propose Federated matched averaging (FedMA) algorithm designed for federated learning of modern neural network architectures e.g. convolutional neural networks (CNNs) and LSTMs. FedMA constructs the shared global model in a layer-wise manner by matching and averaging hidden elements (i.e. channels for convolution layers; hidden states for LSTM; neurons for fully connected layers) with similar feature extraction signatures. Our experiments indicate that FedMA not only outperforms popular state-of-the-art federated learning algorithms on deep CNN and LSTM architectures trained on real world datasets, but also reduces the overall communication burden",
    "checked": true,
    "id": "998620638cfc7c6b5d7b95fa8645f75723d78372",
    "semantic_title": "federated learning with matched averaging",
    "citation_count": 1129,
    "authors": []
  },
  "https://openreview.net/forum?id=SJxstlHFPH": {
    "title": "Differentiable Reasoning over a Virtual Knowledge Base",
    "volume": "oral",
    "abstract": "We consider the task of answering complex multi-hop questions using a corpus as a virtual knowledge base (KB). In particular, we describe a neural module, DrKIT, that traverses textual data like a KB, softly following paths of relations between mentions of entities in the corpus. At each step the module uses a combination of sparse-matrix TFIDF indices and a maximum inner product search (MIPS) on a special index of contextual representations of the mentions. This module is differentiable, so the full system can be trained end-to-end using gradient based methods, starting from natural language inputs. We also describe a pretraining scheme for the contextual representation encoder by generating hard negative examples using existing knowledge bases. We show that DrKIT improves accuracy by 9 points on 3-hop questions in the MetaQA dataset, cutting the gap between text-based and KB-based state-of-the-art by 70%. On HotpotQA, DrKIT leads to a 10% improvement over a BERT-based re-ranking approach to retrieving the relevant passages required to answer a question. DrKIT is also very efficient, processing up to 10-100x more queries per second than existing multi-hop systems",
    "checked": true,
    "id": "a9e6222e71dd101d444b7192b3a0636c71edb0a4",
    "semantic_title": "differentiable reasoning over a virtual knowledge base",
    "citation_count": 90,
    "authors": []
  },
  "https://openreview.net/forum?id=SJxSDxrKDr": {
    "title": "Adversarial Training and Provable Defenses: Bridging the Gap",
    "volume": "oral",
    "abstract": "We present COLT, a new method to train neural networks based on a novel combination of adversarial training and provable defenses. The key idea is to model neural network training as a procedure which includes both, the verifier and the adversary. In every iteration, the verifier aims to certify the network using convex relaxation while the adversary tries to find inputs inside that convex relaxation which cause verification to fail. We experimentally show that this training method, named convex layerwise adversarial training (COLT), is promising and achieves the best of both worlds -- it produces a state-of-the-art neural network with certified robustness of 60.5% and accuracy of 78.4% on the challenging CIFAR-10 dataset with a 2/255 L-infinity perturbation. This significantly improves over the best concurrent results of 54.0% certified robustness and 71.5% accuracy",
    "checked": true,
    "id": "71ea8f105803703893b5c2d01f0c9508643b6554",
    "semantic_title": "adversarial training and provable defenses: bridging the gap",
    "citation_count": 171,
    "authors": []
  },
  "https://openreview.net/forum?id=SJeLIgBKPS": {
    "title": "Gradient Descent Maximizes the Margin of Homogeneous Neural Networks",
    "volume": "oral",
    "abstract": "In this paper, we study the implicit regularization of the gradient descent algorithm in homogeneous neural networks, including fully-connected and convolutional neural networks with ReLU or LeakyReLU activations. In particular, we study the gradient descent or gradient flow (i.e., gradient descent with infinitesimal step size) optimizing the logistic loss or cross-entropy loss of any homogeneous model (possibly non-smooth), and show that if the training loss decreases below a certain threshold, then we can define a smoothed version of the normalized margin which increases over time. We also formulate a natural constrained optimization problem related to margin maximization, and prove that both the normalized margin and its smoothed version converge to the objective value at a KKT point of the optimization problem. Our results generalize the previous results for logistic regression with one-layer or multi-layer linear networks, and provide more quantitative convergence results with weaker assumptions than previous results for homogeneous smooth neural networks. We conduct several experiments to justify our theoretical finding on MNIST and CIFAR-10 datasets. Finally, as margin is closely related to robustness, we discuss potential benefits of training longer for improving the robustness of the model",
    "checked": true,
    "id": "3f46ac38812f9f0f99ff1edddd85d71a84da4497",
    "semantic_title": "gradient descent maximizes the margin of homogeneous neural networks",
    "citation_count": 336,
    "authors": []
  },
  "https://openreview.net/forum?id=Skey4eBYPS": {
    "title": "Convolutional Conditional Neural Processes",
    "volume": "oral",
    "abstract": "We introduce the Convolutional Conditional Neural Process (ConvCNP), a new member of the Neural Process family that models translation equivariance in the data. Translation equivariance is an important inductive bias for many learning problems including time series modelling, spatial data, and images. The model embeds data sets into an infinite-dimensional function space, as opposed to finite-dimensional vector spaces. To formalize this notion, we extend the theory of neural representations of sets to include functional representations, and demonstrate that any translation-equivariant embedding can be represented using a convolutional deep-set. We evaluate ConvCNPs in several settings, demonstrating that they achieve state-of-the-art performance compared to existing NPs. We demonstrate that building in translation equivariance enables zero-shot generalization to challenging, out-of-domain tasks",
    "checked": true,
    "id": "ab6418856eced17a5139e757fb9880c9e2df24fc",
    "semantic_title": "convolutional conditional neural processes",
    "citation_count": 168,
    "authors": []
  },
  "https://openreview.net/forum?id=rkeiQlBFPB": {
    "title": "Meta-Learning with Warped Gradient Descent",
    "volume": "oral",
    "abstract": "Learning an efficient update rule from data that promotes rapid learning of new tasks from the same distribution remains an open problem in meta-learning. Typically, previous works have approached this issue either by attempting to train a neural network that directly produces updates or by attempting to learn better initialisations or scaling factors for a gradient-based update rule. Both of these approaches pose challenges. On one hand, directly producing an update forgoes a useful inductive bias and can easily lead to non-converging behaviour. On the other hand, approaches that try to control a gradient-based update rule typically resort to computing gradients through the learning process to obtain their meta-gradients, leading to methods that can not scale beyond few-shot task adaptation. In this work, we propose Warped Gradient Descent (WarpGrad), a method that intersects these approaches to mitigate their limitations. WarpGrad meta-learns an efficiently parameterised preconditioning matrix that facilitates gradient descent across the task distribution. Preconditioning arises by interleaving non-linear layers, referred to as warp-layers, between the layers of a task-learner. Warp-layers are meta-learned without backpropagating through the task training process in a manner similar to methods that learn to directly produce updates. WarpGrad is computationally efficient, easy to implement, and can scale to arbitrarily large meta-learning problems. We provide a geometrical interpretation of the approach and evaluate its effectiveness in a variety of settings, including few-shot, standard supervised, continual and reinforcement learning",
    "checked": true,
    "id": "e13ca1bd60af3325afc64dc09979e3322818e365",
    "semantic_title": "meta-learning with warped gradient descent",
    "citation_count": 210,
    "authors": []
  },
  "https://openreview.net/forum?id=rkgvXlrKwH": {
    "title": "SEED RL: Scalable and Efficient Deep-RL with Accelerated Central Inference",
    "volume": "oral",
    "abstract": "We present a modern scalable reinforcement learning agent called SEED (Scalable, Efficient Deep-RL). By effectively utilizing modern accelerators, we show that it is not only possible to train on millions of frames per second but also to lower the cost. of experiments compared to current methods. We achieve this with a simple architecture that features centralized inference and an optimized communication layer. SEED adopts two state-of-the-art distributed algorithms, IMPALA/V-trace (policy gradients) and R2D2 (Q-learning), and is evaluated on Atari-57, DeepMind Lab and Google Research Football. We improve the state of the art on Football and are able to reach state of the art on Atari-57 twice as fast in wall-time. For the scenarios we consider, a 40% to 80% cost reduction for running experiments is achieved. The implementation along with experiments is open-sourced so results can be reproduced and novel ideas tried out",
    "checked": true,
    "id": "89a6e3796aca72f370a1f1550f5759cea7eef382",
    "semantic_title": "",
    "citation_count": 93,
    "authors": []
  },
  "https://openreview.net/forum?id=r1gfQgSFDr": {
    "title": "High Fidelity Speech Synthesis with Adversarial Networks",
    "volume": "oral",
    "abstract": "Generative adversarial networks have seen rapid development in recent years and have led to remarkable improvements in generative modelling of images. However, their application in the audio domain has received limited attention, and autoregressive models, such as WaveNet, remain the state of the art in generative modelling of audio signals such as human speech. To address this paucity, we introduce GAN-TTS, a Generative Adversarial Network for Text-to-Speech. Our architecture is composed of a conditional feed-forward generator producing raw speech audio, and an ensemble of discriminators which operate on random windows of different sizes. The discriminators analyse the audio both in terms of general realism, as well as how well the audio corresponds to the utterance that should be pronounced. To measure the performance of GAN-TTS, we employ both subjective human evaluation (MOS - Mean Opinion Score), as well as novel quantitative metrics (Fréchet DeepSpeech Distance and Kernel DeepSpeech Distance), which we find to be well correlated with MOS. We show that GAN-TTS is capable of generating high-fidelity speech with naturalness comparable to the state-of-the-art models, and unlike autoregressive models, it is highly parallelisable thanks to an efficient feed-forward generator. Listen to GAN-TTS reading this abstract at https://storage.googleapis.com/deepmind-media/research/abstract.wav",
    "checked": true,
    "id": "85381e0e89e98ea1ed8bd3dfa533d911263757df",
    "semantic_title": "high fidelity speech synthesis with adversarial networks",
    "citation_count": 240,
    "authors": []
  },
  "https://openreview.net/forum?id=Bkl5kxrKDr": {
    "title": "A Generalized Training Approach for Multiagent Learning",
    "volume": "oral",
    "abstract": "This paper investigates a population-based training regime based on game-theoretic principles called Policy-Spaced Response Oracles (PSRO). PSRO is general in the sense that it (1) encompasses well-known algorithms such as fictitious play and double oracle as special cases, and (2) in principle applies to general-sum, many-player games. Despite this, prior studies of PSRO have been focused on two-player zero-sum games, a regime where in Nash equilibria are tractably computable. In moving from two-player zero-sum games to more general settings, computation of Nash equilibria quickly becomes infeasible. Here, we extend the theoretical underpinnings of PSRO by considering an alternative solution concept, α-Rank, which is unique (thus faces no equilibrium selection issues, unlike Nash) and applies readily to general-sum, many-player settings. We establish convergence guarantees in several games classes, and identify links between Nash equilibria and α-Rank. We demonstrate the competitive performance of α-Rank-based PSRO against an exact Nash solver-based PSRO in 2-player Kuhn and Leduc Poker. We then go beyond the reach of prior PSRO applications by considering 3- to 5-player poker games, yielding instances where α-Rank achieves faster convergence than approximate Nash solvers, thus establishing it as a favorable general games solver. We also carry out an initial empirical validation in MuJoCo soccer, illustrating the feasibility of the proposed approach in another complex domain",
    "checked": true,
    "id": "58619863cb2685698fa29d88610c23a7078ae6dd",
    "semantic_title": "a generalized training approach for multiagent learning",
    "citation_count": 93,
    "authors": []
  },
  "https://openreview.net/forum?id=BJgNJgSFPS": {
    "title": "Building Deep Equivariant Capsule Networks",
    "volume": "oral",
    "abstract": "Capsule networks are constrained by the parameter-expensive nature of their layers, and the general lack of provable equivariance guarantees. We present a variation of capsule networks that aims to remedy this. We identify that learning all pair-wise part-whole relationships between capsules of successive layers is inefficient. Further, we also realise that the choice of prediction networks and the routing mechanism are both key to equivariance. Based on these, we propose an alternative framework for capsule networks that learns to projectively encode the manifold of pose-variations, termed the space-of-variation (SOV), for every capsule-type of each layer. This is done using a trainable, equivariant function defined over a grid of group-transformations. Thus, the prediction-phase of routing involves projection into the SOV of a deeper capsule using the corresponding function. As a specific instantiation of this idea, and also in order to reap the benefits of increased parameter-sharing, we use type-homogeneous group-equivariant convolutions of shallower capsules in this phase. We also introduce an equivariant routing mechanism based on degree-centrality. We show that this particular instance of our general model is equivariant, and hence preserves the compositional representation of an input under transformations. We conduct several experiments on standard object-classification datasets that showcase the increased transformation-robustness, as well as general performance, of our model to several capsule baselines",
    "checked": true,
    "id": "9d8039474b4994ffaa28a698ace4696d7c26dd92",
    "semantic_title": "building deep, equivariant capsule networks",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=S1xWh1rYwB": {
    "title": "Restricting the Flow: Information Bottlenecks for Attribution",
    "volume": "oral",
    "abstract": "Attribution methods provide insights into the decision-making of machine learning models like artificial neural networks. For a given input sample, they assign a relevance score to each individual input variable, such as the pixels of an image. In this work, we adopt the information bottleneck concept for attribution. By adding noise to intermediate feature maps, we restrict the flow of information and can quantify (in bits) how much information image regions provide. We compare our method against ten baselines using three different metrics on VGG-16 and ResNet-50, and find that our methods outperform all baselines in five out of six settings. The method's information-theoretic foundation provides an absolute frame of reference for attribution values (bits) and a guarantee that regions scored close to zero are not necessary for the network's decision",
    "checked": true,
    "id": "57d697214af89d8b8f77eeaaf040a43d1a0c651b",
    "semantic_title": "restricting the flow: information bottlenecks for attribution",
    "citation_count": 190,
    "authors": []
  },
  "https://openreview.net/forum?id=rkg6sJHYDr": {
    "title": "Intrinsically Motivated Discovery of Diverse Patterns in Self-Organizing Systems",
    "volume": "oral",
    "abstract": "In many complex dynamical systems, artificial or natural, one can observe self-organization of patterns emerging from local rules. Cellular automata, like the Game of Life (GOL), have been widely used as abstract models enabling the study of various aspects of self-organization and morphogenesis, such as the emergence of spatially localized patterns. However, findings of self-organized patterns in such models have so far relied on manual tuning of parameters and initial states, and on the human eye to identify interesting patterns. In this paper, we formulate the problem of automated discovery of diverse self-organized patterns in such high-dimensional complex dynamical systems, as well as a framework for experimentation and evaluation. Using a continuous GOL as a testbed, we show that recent intrinsically-motivated machine learning algorithms (POP-IMGEPs), initially developed for learning of inverse models in robotics, can be transposed and used in this novel application area. These algorithms combine intrinsically-motivated goal exploration and unsupervised learning of goal space representations. Goal space representations describe the interesting features of patterns for which diverse variations should be discovered. In particular, we compare various approaches to define and learn goal space representations from the perspective of discovering diverse spatially localized patterns. Moreover, we introduce an extension of a state-of-the-art POP-IMGEP algorithm which incrementally learns a goal representation using a deep auto-encoder, and the use of CPPN primitives for generating initialization parameters. We show that it is more efficient than several baselines and equally efficient as a system pre-trained on a hand-made database of patterns identified by human experts",
    "checked": true,
    "id": "b3e9c1fa15bd08afc7bef3e58c0ac2527f409ec1",
    "semantic_title": "intrinsically motivated discovery of diverse patterns in self-organizing systems",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=S1g2skStPB": {
    "title": "Causal Discovery with Reinforcement Learning",
    "volume": "oral",
    "abstract": "Discovering causal structure among a set of variables is a fundamental problem in many empirical sciences. Traditional score-based casual discovery methods rely on various local heuristics to search for a Directed Acyclic Graph (DAG) according to a predefined score function. While these methods, e.g., greedy equivalence search, may have attractive results with infinite samples and certain model assumptions, they are less satisfactory in practice due to finite data and possible violation of assumptions. Motivated by recent advances in neural combinatorial optimization, we propose to use Reinforcement Learning (RL) to search for the DAG with the best scoring. Our encoder-decoder model takes observable data as input and generates graph adjacency matrices that are used to compute rewards. The reward incorporates both the predefined score function and two penalty terms for enforcing acyclicity. In contrast with typical RL applications where the goal is to learn a policy, we use RL as a search strategy and our final output would be the graph, among all graphs generated during training, that achieves the best reward. We conduct experiments on both synthetic and real datasets, and show that the proposed approach not only has an improved search ability but also allows for a flexible score function under the acyclicity constraint",
    "checked": true,
    "id": "f844925066c3de9cd9ad662059e25a9d47a59843",
    "semantic_title": "causal discovery with reinforcement learning",
    "citation_count": 241,
    "authors": []
  },
  "https://openreview.net/forum?id=rklr9kHFDB": {
    "title": "Rotation-invariant clustering of neuronal responses in primary visual cortex",
    "volume": "oral",
    "abstract": "Similar to a convolutional neural network (CNN), the mammalian retina encodes visual information into several dozen nonlinear feature maps, each formed by one ganglion cell type that tiles the visual space in an approximately shift-equivariant manner. Whether such organization into distinct cell types is maintained at the level of cortical image processing is an open question. Predictive models building upon convolutional features have been shown to provide state-of-the-art performance, and have recently been extended to include rotation equivariance in order to account for the orientation selectivity of V1 neurons. However, generally no direct correspondence between CNN feature maps and groups of individual neurons emerges in these models, thus rendering it an open question whether V1 neurons form distinct functional clusters. Here we build upon the rotation-equivariant representation of a CNN-based V1 model and propose a methodology for clustering the representations of neurons in this model to find functional cell types independent of preferred orientations of the neurons. We apply this method to a dataset of 6000 neurons and visualize the preferred stimuli of the resulting clusters. Our results highlight the range of non-linear computations in mouse V1",
    "checked": true,
    "id": "b2a469ca8197a0749d2ebaaa0014e933db747fbc",
    "semantic_title": "rotation-invariant clustering of neuronal responses in primary visual cortex",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=rkgNKkHtvB": {
    "title": "Reformer: The Efficient Transformer",
    "volume": "oral",
    "abstract": "Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($L \\log L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of N times, where N is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences",
    "checked": true,
    "id": "055fd6a9f7293269f1b22c1470e63bd02d8d9500",
    "semantic_title": "reformer: the efficient transformer",
    "citation_count": 2335,
    "authors": []
  },
  "https://openreview.net/forum?id=BygXFkSYDH": {
    "title": "Target-Embedding Autoencoders for Supervised Representation Learning",
    "volume": "oral",
    "abstract": "Autoencoder-based learning has emerged as a staple for disciplining representations in unsupervised and semi-supervised settings. This paper analyzes a framework for improving generalization in a purely supervised setting, where the target space is high-dimensional. We motivate and formalize the general framework of target-embedding autoencoders (TEA) for supervised prediction, learning intermediate latent representations jointly optimized to be both predictable from features as well as predictive of targets---encoding the prior that variations in targets are driven by a compact set of underlying factors. As our theoretical contribution, we provide a guarantee of generalization for linear TEAs by demonstrating uniform stability, interpreting the benefit of the auxiliary reconstruction task as a form of regularization. As our empirical contribution, we extend validation of this approach beyond existing static classification applications to multivariate sequence forecasting, verifying their advantage on both linear and nonlinear recurrent architectures---thereby underscoring the further generality of this framework beyond feedforward instantiations",
    "checked": true,
    "id": "a104a57f206d45c94db179f651e794fe5a33520b",
    "semantic_title": "target-embedding autoencoders for supervised representation learning",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=BJlQtJSKDB": {
    "title": "Watch the Unobserved: A Simple Approach to Parallelizing Monte Carlo Tree Search",
    "volume": "oral",
    "abstract": "Monte Carlo Tree Search (MCTS) algorithms have achieved great success on many challenging benchmarks (e.g., Computer Go). However, they generally require a large number of rollouts, making their applications costly. Furthermore, it is also extremely challenging to parallelize MCTS due to its inherent sequential nature: each rollout heavily relies on the statistics (e.g., node visitation counts) estimated from previous simulations to achieve an effective exploration-exploitation tradeoff. In spite of these difficulties, we develop an algorithm, WU-UCT, to effectively parallelize MCTS, which achieves linear speedup and exhibits only limited performance loss with an increasing number of workers. The key idea in WU-UCT is a set of statistics that we introduce to track the number of on-going yet incomplete simulation queries (named as unobserved samples). These statistics are used to modify the UCT tree policy in the selection steps in a principled manner to retain effective exploration-exploitation tradeoff when we parallelize the most time-consuming expansion and simulation steps. Experiments on a proprietary benchmark and the Atari Game benchmark demonstrate the linear speedup and the superior performance of WU-UCT comparing to existing techniques",
    "checked": true,
    "id": "ec418426d0ac1e4a0f8ff77b042e6447a29bd734",
    "semantic_title": "watch the unobserved: a simple approach to parallelizing monte carlo tree search",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=S1eALyrYDH": {
    "title": "RNA Secondary Structure Prediction By Learning Unrolled Algorithms",
    "volume": "oral",
    "abstract": "In this paper, we propose an end-to-end deep learning model, called E2Efold, for RNA secondary structure prediction which can effectively take into account the inherent constraints in the problem. The key idea of E2Efold is to directly predict the RNA base-pairing matrix, and use an unrolled algorithm for constrained programming as the template for deep architectures to enforce constraints. With comprehensive experiments on benchmark datasets, we demonstrate the superior performance of E2Efold: it predicts significantly better structures compared to previous SOTA (especially for pseudoknotted structures), while being as efficient as the fastest algorithms in terms of inference time",
    "checked": true,
    "id": "cc9dc50dfd216067bbf7b5f3cac6096d1c6128d8",
    "semantic_title": "rna secondary structure prediction by learning unrolled algorithms",
    "citation_count": 119,
    "authors": []
  },
  "https://openreview.net/forum?id=rkeZIJBYvr": {
    "title": "Learning to Balance: Bayesian Meta-Learning for Imbalanced and Out-of-distribution Tasks",
    "volume": "oral",
    "abstract": "While tasks could come with varying the number of instances and classes in realistic settings, the existing meta-learning approaches for few-shot classification assume that number of instances per task and class is fixed. Due to such restriction, they learn to equally utilize the meta-knowledge across all the tasks, even when the number of instances per task and class largely varies. Moreover, they do not consider distributional difference in unseen tasks, on which the meta-knowledge may have less usefulness depending on the task relatedness. To overcome these limitations, we propose a novel meta-learning model that adaptively balances the effect of the meta-learning and task-specific learning within each task. Through the learning of the balancing variables, we can decide whether to obtain a solution by relying on the meta-knowledge or task-specific learning. We formulate this objective into a Bayesian inference framework and tackle it using variational inference. We validate our Bayesian Task-Adaptive Meta-Learning (Bayesian TAML) on two realistic task- and class-imbalanced datasets, on which it significantly outperforms existing meta-learning approaches. Further ablation study confirms the effectiveness of each balancing component and the Bayesian learning framework",
    "checked": true,
    "id": "96f8e5820c5b59e65c2331d577aa738676e8c605",
    "semantic_title": "learning to balance: bayesian meta-learning for imbalanced and out-of-distribution tasks",
    "citation_count": 107,
    "authors": []
  },
  "https://openreview.net/forum?id=BJeAHkrYDS": {
    "title": "Fast Task Inference with Variational Intrinsic Successor Features",
    "volume": "oral",
    "abstract": "It has been established that diverse behaviors spanning the controllable subspace of a Markov decision process can be trained by rewarding a policy for being distinguishable from other policies. However, one limitation of this formulation is the difficulty to generalize beyond the finite set of behaviors being explicitly learned, as may be needed in subsequent tasks. Successor features provide an appealing solution to this generalization problem, but require defining the reward function as linear in some grounded feature space. In this paper, we show that these two techniques can be combined, and that each method solves the other's primary limitation. To do so we introduce Variational Intrinsic Successor FeatuRes (VISR), a novel algorithm which learns controllable features that can be leveraged to provide enhanced generalization and fast task inference through the successor features framework. We empirically validate VISR on the full Atari suite, in a novel setup wherein the rewards are only exposed briefly after a long unsupervised phase. Achieving human-level performance on 12 games and beating all baselines, we believe VISR represents a step towards agents that rapidly learn from limited feedback",
    "checked": true,
    "id": "55203cd25f9c03d7c6b691ba84e95bb82df0bc6f",
    "semantic_title": "fast task inference with variational intrinsic successor features",
    "citation_count": 152,
    "authors": []
  },
  "https://openreview.net/forum?id=r1etN1rtPB": {
    "title": "Implementation Matters in Deep RL: A Case Study on PPO and TRPO",
    "volume": "oral",
    "abstract": "We study the roots of algorithmic progress in deep policy gradient algorithms through a case study on two popular algorithms: Proximal Policy Optimization (PPO) and Trust Region Policy Optimization (TRPO). Specifically, we investigate the consequences of \"code-level optimizations:\" algorithm augmentations found only in implementations or described as auxiliary details to the core algorithm. Seemingly of secondary importance, such optimizations turn out to have a major impact on agent behavior. Our results show that they (a) are responsible for most of PPO's gain in cumulative reward over TRPO, and (b) fundamentally change how RL methods function. These insights show the difficulty, and importance, of attributing performance gains in deep reinforcement learning",
    "checked": true,
    "id": "f1697ce4dddb58533d7d3f937fed74807d46edb8",
    "semantic_title": "implementation matters in deep rl: a case study on ppo and trpo",
    "citation_count": 257,
    "authors": []
  },
  "https://openreview.net/forum?id=ryxdEkHtPS": {
    "title": "A Closer Look at Deep Policy Gradients",
    "volume": "oral",
    "abstract": "We study how the behavior of deep policy gradient algorithms reflects the conceptual framework motivating their development. To this end, we propose a fine-grained analysis of state-of-the-art methods based on key elements of this framework: gradient estimation, value prediction, and optimization landscapes. Our results show that the behavior of deep policy gradient algorithms often deviates from what their motivating framework would predict: surrogate rewards do not match the true reward landscape, learned value estimators fail to fit the true value function, and gradient estimates poorly correlate with the \"true\" gradient. The mismatch between predicted and empirical behavior we uncover highlights our poor understanding of current methods, and indicates the need to move beyond current benchmark-centric evaluation methods",
    "checked": true,
    "id": "ffb90ac625db6c398d019c1d7e5a180581114517",
    "semantic_title": "a closer look at deep policy gradients",
    "citation_count": 86,
    "authors": []
  },
  "https://openreview.net/forum?id=H1gDNyrKDS": {
    "title": "Understanding and Robustifying Differentiable Architecture Search",
    "volume": "oral",
    "abstract": "Differentiable Architecture Search (DARTS) has attracted a lot of attention due to its simplicity and small search costs achieved by a continuous relaxation and an approximation of the resulting bi-level optimization problem. However, DARTS does not work robustly for new problems: we identify a wide range of search spaces for which DARTS yields degenerate architectures with very poor test performance. We study this failure mode and show that, while DARTS successfully minimizes validation loss, the found solutions generalize poorly when they coincide with high validation loss curvature in the architecture space. We show that by adding one of various types of regularization we can robustify DARTS to find solutions with less curvature and better generalization properties. Based on these observations, we propose several simple variations of DARTS that perform substantially more robustly in practice. Our observations are robust across five search spaces on three image classification tasks and also hold for the very different domains of disparity estimation (a dense regression task) and language modelling",
    "checked": true,
    "id": "3242bf8767179c13c7322ccfdbe18c66c1e25a99",
    "semantic_title": "understanding and robustifying differentiable architecture search",
    "citation_count": 375,
    "authors": []
  },
  "https://openreview.net/forum?id=ryghZJBKPS": {
    "title": "Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds",
    "volume": "oral",
    "abstract": "We design a new algorithm for batch active learning with deep neural network models. Our algorithm, Batch Active learning by Diverse Gradient Embeddings (BADGE), samples groups of points that are disparate and high-magnitude when represented in a hallucinated gradient space, a strategy designed to incorporate both predictive uncertainty and sample diversity into every selected batch. Crucially, BADGE trades off between diversity and uncertainty without requiring any hand-tuned hyperparameters. While other approaches sometimes succeed for particular batch sizes or architectures, BADGE consistently performs as well or better, making it a useful option for real world active learning problems",
    "checked": true,
    "id": "cf5a21684aefb1b8db6e0490167636d245396095",
    "semantic_title": "deep batch active learning by diverse, uncertain gradient lower bounds",
    "citation_count": 779,
    "authors": []
  },
  "https://openreview.net/forum?id=rygixkHKDH": {
    "title": "Geometric Analysis of Nonconvex Optimization Landscapes for Overcomplete Learning",
    "volume": "oral",
    "abstract": "Learning overcomplete representations finds many applications in machine learning and data analytics. In the past decade, despite the empirical success of heuristic methods, theoretical understandings and explanations of these algorithms are still far from satisfactory. In this work, we provide new theoretical insights for several important representation learning problems: learning (i) sparsely used overcomplete dictionaries and (ii) convolutional dictionaries. We formulate these problems as $\\ell^4$-norm optimization problems over the sphere and study the geometric properties of their nonconvex optimization landscapes. For both problems, we show the nonconvex objective has benign (global) geometric structures, which enable the development of efficient optimization methods finding the target solutions. Finally, our theoretical results are justified by numerical simulations",
    "checked": true,
    "id": "9f5526fe4553d61a6e093f8d32d0cc4e54c1abfe",
    "semantic_title": "geometric analysis of nonconvex optimization landscapes for overcomplete learning",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=r1eBeyHFDH": {
    "title": "A Theory of Usable Information under Computational Constraints",
    "volume": "oral",
    "abstract": "We propose a new framework for reasoning about information in complex systems. Our foundation is based on a variational extension of Shannon's information theory that takes into account the modeling power and computational constraints of the observer. The resulting predictive V-information encompasses mutual information and other notions of informativeness such as the coefficient of determination. Unlike Shannon's mutual information and in violation of the data processing inequality, V-information can be created through computation. This is consistent with deep neural networks extracting hierarchies of progressively more informative features in representation learning. Additionally, we show that by incorporating computational constraints, V-information can be reliably estimated from data even in high dimensions with PAC-style guarantees. Empirically, we demonstrate predictive V-information is more effective than mutual information for structure learning and fair representation learning. Codes are available at https://github.com/Newbeeer/V-information",
    "checked": true,
    "id": "0e4f7290f9cce44284665ddb399abeea0d72c557",
    "semantic_title": "a theory of usable information under computational constraints",
    "citation_count": 175,
    "authors": []
  },
  "https://openreview.net/forum?id=Ske31kBtPr": {
    "title": "Mathematical Reasoning in Latent Space",
    "volume": "oral",
    "abstract": "We design and conduct a simple experiment to study whether neural networks can perform several steps of approximate reasoning in a fixed dimensional latent space. The set of rewrites (i.e. transformations) that can be successfully performed on a statement represents essential semantic features of the statement. We can compress this information by embedding the formula in a vector space, such that the vector associated with a statement can be used to predict whether a statement can be rewritten by other theorems. Predicting the embedding of a formula generated by some rewrite rule is naturally viewed as approximate reasoning in the latent space. In order to measure the effectiveness of this reasoning, we perform approximate deduction sequences in the latent space and use the resulting embedding to inform the semantic features of the corresponding formal statement (which is obtained by performing the corresponding rewrite sequence using real formulas). Our experiments show that graph neural networks can make non-trivial predictions about the rewrite-success of statements, even when they propagate predicted latent representations for several steps. Since our corpus of mathematical formulas includes a wide variety of mathematical disciplines, this experiment is a strong indicator for the feasibility of deduction in latent space in general",
    "checked": true,
    "id": "84a2a78fbbcde2258af267dd66f8b80c21ff03a7",
    "semantic_title": "mathematical reasoning in latent space",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=SJeD3CEFPH": {
    "title": "Meta-Q-Learning",
    "volume": "oral",
    "abstract": "This paper introduces Meta-Q-Learning (MQL), a new off-policy algorithm for meta-Reinforcement Learning (meta-RL). MQL builds upon three simple ideas. First, we show that Q-learning is competitive with state-of-the-art meta-RL algorithms if given access to a context variable that is a representation of the past trajectory. Second, a multi-task objective to maximize the average reward across the training tasks is an effective method to meta-train RL policies. Third, past data from the meta-training replay buffer can be recycled to adapt the policy on a new task using off-policy updates. MQL draws upon ideas in propensity estimation to do so and thereby amplifies the amount of available data for adaptation. Experiments on standard continuous-control benchmarks suggest that MQL compares favorably with the state of the art in meta-RL",
    "checked": true,
    "id": "405d2cc9cccc035b4c8d950d6cfab4cc1a5d0628",
    "semantic_title": "meta-q-learning",
    "citation_count": 148,
    "authors": []
  },
  "https://openreview.net/forum?id=S1gSj0NKvB": {
    "title": "Comparing Rewinding and Fine-tuning in Neural Network Pruning",
    "volume": "oral",
    "abstract": "Many neural network pruning algorithms proceed in three steps: train the network to completion, remove unwanted structure to compress the network, and retrain the remaining structure to recover lost accuracy. The standard retraining technique, fine-tuning, trains the unpruned weights from their final trained values using a small fixed learning rate. In this paper, we compare fine-tuning to alternative retraining techniques. Weight rewinding (as proposed by Frankle et al., (2019)), rewinds unpruned weights to their values from earlier in training and retrains them from there using the original training schedule. Learning rate rewinding (which we propose) trains the unpruned weights from their final values using the same learning rate schedule as weight rewinding. Both rewinding techniques outperform fine-tuning, forming the basis of a network-agnostic pruning algorithm that matches the accuracy and compression ratios of several more network-specific state-of-the-art techniques",
    "checked": true,
    "id": "850464c9006261bd632c4203f3e630db09a32faf",
    "semantic_title": "comparing rewinding and fine-tuning in neural network pruning",
    "citation_count": 388,
    "authors": []
  },
  "https://openreview.net/forum?id=rklHqRVKvH": {
    "title": "Harnessing Structures for Value-Based Planning and Reinforcement Learning",
    "volume": "oral",
    "abstract": "Value-based methods constitute a fundamental methodology in planning and deep reinforcement learning (RL). In this paper, we propose to exploit the underlying structures of the state-action value function, i.e., Q function, for both planning and deep RL. In particular, if the underlying system dynamics lead to some global structures of the Q function, one should be capable of inferring the function better by leveraging such structures. Specifically, we investigate the low-rank structure, which widely exists for big data matrices. We verify empirically the existence of low-rank Q functions in the context of control and deep RL tasks. As our key contribution, by leveraging Matrix Estimation (ME) techniques, we propose a general framework to exploit the underlying low-rank structure in Q functions. This leads to a more efficient planning procedure for classical control, and additionally, a simple scheme that can be applied to value-based RL techniques to consistently achieve better performance on \"low-rank\" tasks. Extensive experiments on control tasks and Atari games confirm the efficacy of our approach",
    "checked": true,
    "id": "35d319510719e7ab71e341beffb3209c0144f49e",
    "semantic_title": "harnessing structures for value-based planning and reinforcement learning",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=r1lGO0EKDH": {
    "title": "GraphZoom: A Multi-level Spectral Approach for Accurate and Scalable Graph Embedding",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "49df8cfd358b4b55cfd6e2102a37f1c373442760",
    "semantic_title": "graphzoom: a multi-level spectral approach for accurate and scalable graph embedding",
    "citation_count": 106,
    "authors": []
  },
  "https://openreview.net/forum?id=BkgzMCVtPB": {
    "title": "Optimal Strategies Against Generative Attacks",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "b5cd28ee3582114f0c785d37968a413a139a6a65",
    "semantic_title": "optimal strategies against generative attacks",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=HJgLZR4KvH": {
    "title": "Dynamics-Aware Unsupervised Discovery of Skills",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "ffb3886a253ff927bcc46b78e00409893865a68e",
    "semantic_title": "dynamics-aware unsupervised discovery of skills",
    "citation_count": 413,
    "authors": []
  },
  "https://openreview.net/forum?id=Hkxzx0NtDB": {
    "title": "Your classifier is secretly an energy based model and you should treat it like one",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "97cd86d8d8c0f27cd3e64c6ca5cfdeb957ee39f4",
    "semantic_title": "your classifier is secretly an energy based model and you should treat it like one",
    "citation_count": 547,
    "authors": []
  },
  "https://openreview.net/forum?id=rkeS1RVtPS": {
    "title": "Cyclical Stochastic Gradient MCMC for Bayesian Deep Learning",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "079ce2910925dbe4272c36c7e59cfa7ce959bc24",
    "semantic_title": "cyclical stochastic gradient mcmc for bayesian deep learning",
    "citation_count": 279,
    "authors": []
  },
  "https://openreview.net/forum?id=HkxQRTNYPH": {
    "title": "Mirror-Generative Neural Machine Translation",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "f1a387047840aff5a8a92d9838936b64622fd332",
    "semantic_title": "mirror-generative neural machine translation",
    "citation_count": 37,
    "authors": []
  },
  "https://openreview.net/forum?id=B1elCp4KwH": {
    "title": "Learning Hierarchical Discrete Linguistic Units from Visually-Grounded Speech",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "06e6ab9e9333498698ac9e557dc2a0d1dd61d510",
    "semantic_title": "learning hierarchical discrete linguistic units from visually-grounded speech",
    "citation_count": 85,
    "authors": []
  },
  "https://openreview.net/forum?id=SJe5P6EYvS": {
    "title": "Mogrifier LSTM",
    "volume": "oral",
    "abstract": "",
    "checked": false,
    "id": "c8ae6437e6cff3c7f468676de4ee85837c566e99",
    "semantic_title": "mogriﬁer lstm",
    "citation_count": 55,
    "authors": []
  },
  "https://openreview.net/forum?id=Syg-ET4FPS": {
    "title": "Posterior sampling for multi-agent reinforcement learning: solving extensive games with imperfect information",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "d2aa21a692fe0134f599c42cc26ea5cb1e12aa4b",
    "semantic_title": "posterior sampling for multi-agent reinforcement learning: solving extensive games with imperfect information",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=BJgnXpVYwS": {
    "title": "Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "e658741baefd2c4da4742bb43155f69d4cbd79fa",
    "semantic_title": "why gradient clipping accelerates training: a theoretical justification for adaptivity",
    "citation_count": 469,
    "authors": []
  },
  "https://openreview.net/forum?id=B1evfa4tPB": {
    "title": "Neural Network Branching for Neural Network Verification",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "9b09338f77c006f73eaf18fb1f68c1c52e210c9f",
    "semantic_title": "neural network branching for neural network verification",
    "citation_count": 65,
    "authors": []
  },
  "https://openreview.net/forum?id=H1gax6VtDB": {
    "title": "Contrastive Learning of Structured World Models",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "19b924dd9121f01165276a7afb764cf394acb80b",
    "semantic_title": "contrastive learning of structured world models",
    "citation_count": 285,
    "authors": []
  },
  "https://openreview.net/forum?id=S1efxTVYDr": {
    "title": "Data-dependent Gaussian Prior Objective for Language Generation",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "69e4dabf1f140915878a365c1adb86ceb2362ab6",
    "semantic_title": "data-dependent gaussian prior objective for language generation",
    "citation_count": 63,
    "authors": []
  },
  "https://openreview.net/forum?id=HJxNAnVtDS": {
    "title": "On the Convergence of FedAvg on Non-IID Data",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "c802ceb7a9ff904220c48ee44ae9b671be6d6379",
    "semantic_title": "on the convergence of fedavg on non-iid data",
    "citation_count": 2356,
    "authors": []
  },
  "https://openreview.net/forum?id=H1lma24tPB": {
    "title": "Principled Weight Initialization for Hypernetworks",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "5f1e85540af411bdaa183557d5d1272a13f982f5",
    "semantic_title": "principled weight initialization for hypernetworks",
    "citation_count": 77,
    "authors": []
  },
  "https://openreview.net/forum?id=HkxlcnVFwB": {
    "title": "GenDICE: Generalized Offline Estimation of Stationary Values",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "2b0e79ed1340a79344e37b6f57191b76d810962f",
    "semantic_title": "gendice: generalized offline estimation of stationary values",
    "citation_count": 174,
    "authors": []
  },
  "https://openreview.net/forum?id=BJlrF24twB": {
    "title": "BackPACK: Packing more into Backprop",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "55d5edd470907e2c46d6a7b1cf77d125f18e0369",
    "semantic_title": "backpack: packing more into backprop",
    "citation_count": 103,
    "authors": []
  },
  "https://openreview.net/forum?id=HJgzt2VKPB": {
    "title": "CATER: A diagnostic dataset for Compositional Actions & TEmporal Reasoning",
    "volume": "oral",
    "abstract": "Computer vision has undergone a dramatic revolution in performance, driven in large part through deep features trained on large-scale supervised datasets. However, much of these improvements have focused on static image analysis; video understanding has seen rather modest improvements. Even though new datasets and spatiotemporal models have been proposed, simple frame-by-frame classification methods often still remain competitive. We posit that current video datasets are plagued with implicit biases over scene and object structure that can dwarf variations in temporal structure. In this work, we build a video dataset with fully observable and controllable object and scene bias, and which truly requires spatiotemporal understanding in order to be solved. Our dataset, named CATER, is rendered synthetically using a library of standard 3D objects, and tests the ability to recognize compositions of object movements that require long-term reasoning. In addition to being a challenging dataset, CATER also provides a plethora of diagnostic tools to analyze modern spatiotemporal video architectures by being completely observable and controllable. Using CATER, we provide insights into some of the most recent state of the art deep video architectures",
    "checked": true,
    "id": "f696ea202d5f2af57c66b6f9f2783f124399e0dd",
    "semantic_title": "cater: a diagnostic dataset for compositional actions and temporal reasoning",
    "citation_count": 178,
    "authors": []
  },
  "https://openreview.net/forum?id=S1e2agrFvS": {
    "title": "Geom-GCN: Geometric Graph Convolutional Networks",
    "volume": "spotlight",
    "abstract": "Message-passing neural networks (MPNNs) have been successfully applied in a wide variety of applications in the real world. However, two fundamental weaknesses of MPNNs' aggregators limit their ability to represent graph-structured data: losing the structural information of nodes in neighborhoods and lacking the ability to capture long-range dependencies in disassortative graphs. Few studies have noticed the weaknesses from different perspectives. From the observations on classical neural network and network geometry, we propose a novel geometric aggregation scheme for graph neural networks to overcome the two weaknesses. The behind basic idea is the aggregation on a graph can benefit from a continuous space underlying the graph. The proposed aggregation scheme is permutation-invariant and consists of three modules, node embedding, structural neighborhood, and bi-level aggregation. We also present an implementation of the scheme in graph convolutional networks, termed Geom-GCN, to perform transductive learning on graphs. Experimental results show the proposed Geom-GCN achieved state-of-the-art performance on a wide range of open datasets of graphs",
    "checked": true,
    "id": "04f3203f1214063436d81ce0c2ad7623204da488",
    "semantic_title": "geom-gcn: geometric graph convolutional networks",
    "citation_count": 1128,
    "authors": []
  },
  "https://openreview.net/forum?id=rJg8TeSFDH": {
    "title": "An Exponential Learning Rate Schedule for Deep Learning",
    "volume": "spotlight",
    "abstract": "Intriguing empirical evidence exists that deep learning can work well with exotic schedules for varying the learning rate. This paper suggests that the phenomenon may be due to Batch Normalization or BN(Ioffe & Szegedy, 2015), which is ubiq- uitous and provides benefits in optimization and generalization across all standard architectures. The following new results are shown about BN with weight decay and momentum (in other words, the typical use case which was not considered in earlier theoretical analyses of stand-alone BN (Ioffe & Szegedy, 2015; Santurkar et al., 2018; Arora et al., 2018) • Training can be done using SGD with momentum and an exponentially in- creasing learning rate schedule, i.e., learning rate increases by some (1 + α) factor in every epoch for some α > 0. (Precise statement in the paper.) To the best of our knowledge this is the first time such a rate schedule has been successfully used, let alone for highly successful architectures. As ex- pected, such training rapidly blows up network weights, but the net stays well-behaved due to normalization. • Mathematical explanation of the success of the above rate schedule: a rigor- ous proof that it is equivalent to the standard setting of BN + SGD + Standard Rate Tuning + Weight Decay + Momentum. This equivalence holds for other normalization layers as well, Group Normalization(Wu & He, 2018), Layer Normalization(Ba et al., 2016), Instance Norm(Ulyanov et al., 2016), etc. • A worked-out toy example illustrating the above linkage of hyper- parameters. Using either weight decay or BN alone reaches global minimum, but convergence fails when both are used",
    "checked": true,
    "id": "00521e876d109fdcf344e027f218a768499060c8",
    "semantic_title": "an exponential learning rate schedule for deep learning",
    "citation_count": 219,
    "authors": []
  },
  "https://openreview.net/forum?id=SJxpsxrYPS": {
    "title": "PROGRESSIVE LEARNING AND DISENTANGLEMENT OF HIERARCHICAL REPRESENTATIONS",
    "volume": "spotlight",
    "abstract": "Learning rich representation from data is an important task for deep generative models such as variational auto-encoder (VAE). However, by extracting high-level abstractions in the bottom-up inference process, the goal of preserving all factors of variations for top-down generation is compromised. Motivated by the concept of \"starting small\", we present a strategy to progressively learn independent hierarchical representations from high- to low-levels of abstractions. The model starts with learning the most abstract representation, and then progressively grow the network architecture to introduce new representations at different levels of abstraction. We quantitatively demonstrate the ability of the presented model to improve disentanglement in comparison to existing works on two benchmark datasets using three disentanglement metrics, including a new metric we proposed to complement the previously-presented metric of mutual information gap. We further present both qualitative and quantitative evidence on how the progression of learning improves disentangling of hierarchical representations. By drawing on the respective advantage of hierarchical representation learning and progressive learning, this is to our knowledge the first attempt to improve disentanglement by progressively growing the capacity of VAE to learn hierarchical representations",
    "checked": true,
    "id": "21386c52e895805ac0c20d4755ac0193c6576c99",
    "semantic_title": "progressive learning and disentanglement of hierarchical representations",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=SJxUjlBtwB": {
    "title": "Reconstructing continuous distributions of 3D protein structure from cryo-EM images",
    "volume": "spotlight",
    "abstract": "Cryo-electron microscopy (cryo-EM) is a powerful technique for determining the structure of proteins and other macromolecular complexes at near-atomic resolution. In single particle cryo-EM, the central problem is to reconstruct the 3D structure of a macromolecule from $10^{4-7}$ noisy and randomly oriented 2D projection images. However, the imaged protein complexes may exhibit structural variability, which complicates reconstruction and is typically addressed using discrete clustering approaches that fail to capture the full range of protein dynamics. Here, we introduce a novel method for cryo-EM reconstruction that extends naturally to modeling continuous generative factors of structural heterogeneity. This method encodes structures in Fourier space using coordinate-based deep neural networks, and trains these networks from unlabeled 2D cryo-EM images by combining exact inference over image orientation with variational inference for structural heterogeneity. We demonstrate that the proposed method, termed cryoDRGN, can perform ab-initio reconstruction of 3D protein complexes from simulated and real 2D cryo-EM image data. To our knowledge, cryoDRGN is the first neural network-based approach for cryo-EM reconstruction and the first end-to-end method for directly reconstructing continuous ensembles of protein structures from cryo-EM images",
    "checked": true,
    "id": "5305e9fa61d2b942e168651a300ad99afa1db3f1",
    "semantic_title": "reconstructing continuous distributions of 3d protein structure from cryo-em images",
    "citation_count": 109,
    "authors": []
  },
  "https://openreview.net/forum?id=H1gBsgBYwH": {
    "title": "Generalization of Two-layer Neural Networks: An Asymptotic Viewpoint",
    "volume": "spotlight",
    "abstract": "This paper investigates the generalization properties of two-layer neural networks in high-dimensions, i.e. when the number of samples $n$, features $d$, and neurons $h$ tend to infinity at the same rate. Specifically, we derive the exact population risk of the unregularized least squares regression problem with two-layer neural networks when either the first or the second layer is trained using a gradient flow under different initialization setups. When only the second layer coefficients are optimized, we recover the \\textit{double descent} phenomenon: a cusp in the population risk appears at $h\\approx n$ and further overparameterization decreases the risk. In contrast, when the first layer weights are optimized, we highlight how different scales of initialization lead to different inductive bias, and show that the resulting risk is \\textit{independent} of overparameterization. Our theoretical and experimental results suggest that previously studied model setups that provably give rise to \\textit{double descent} might not translate to optimizing two-layer neural networks",
    "checked": true,
    "id": "98c7b0751506f228e8a4eefa283056cccb6650a2",
    "semantic_title": "generalization of two-layer neural networks: an asymptotic viewpoint",
    "citation_count": 74,
    "authors": []
  },
  "https://openreview.net/forum?id=BJe55gBtvH": {
    "title": "Depth-Width Trade-offs for ReLU Networks via Sharkovsky's Theorem",
    "volume": "spotlight",
    "abstract": "Understanding the representational power of Deep Neural Networks (DNNs) and how their structural properties (e.g., depth, width, type of activation unit) affect the functions they can compute, has been an important yet challenging question in deep learning and approximation theory. In a seminal paper, Telgarsky high- lighted the benefits of depth by presenting a family of functions (based on sim- ple triangular waves) for which DNNs achieve zero classification error, whereas shallow networks with fewer than exponentially many nodes incur constant error. Even though Telgarsky's work reveals the limitations of shallow neural networks, it doesn't inform us on why these functions are difficult to represent and in fact he states it as a tantalizing open question to characterize those functions that cannot be well-approximated by smaller depths. In this work, we point to a new connection between DNNs expressivity and Sharkovsky's Theorem from dynamical systems, that enables us to characterize the depth-width trade-offs of ReLU networks for representing functions based on the presence of a generalized notion of fixed points, called periodic points (a fixed point is a point of period 1). Motivated by our observation that the triangle waves used in Telgarsky's work contain points of period 3 – a period that is special in that it implies chaotic behaviour based on the celebrated result by Li-Yorke – we proceed to give general lower bounds for the width needed to represent periodic functions as a function of the depth. Technically, the crux of our approach is based on an eigenvalue analysis of the dynamical systems associated with such functions",
    "checked": true,
    "id": "227456bb36c34b9e5c27f932e0a0ba4e7e1f7220",
    "semantic_title": "depth-width trade-offs for relu networks via sharkovsky's theorem",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=S1e_9xrFvS": {
    "title": "Energy-based models for atomic-resolution protein conformations",
    "volume": "spotlight",
    "abstract": "We propose an energy-based model (EBM) of protein conformations that operates at atomic scale. The model is trained solely on crystallized protein data. By contrast, existing approaches for scoring conformations use energy functions that incorporate knowledge of physical principles and features that are the complex product of several decades of research and tuning. To evaluate the model, we benchmark on the rotamer recovery task, the problem of predicting the conformation of a side chain from its context within a protein structure, which has been used to evaluate energy functions for protein design. The model achieves performance close to that of the Rosetta energy function, a state-of-the-art method widely used in protein structure prediction and design. An investigation of the model's outputs and hidden representations finds that it captures physicochemical properties relevant to protein energy",
    "checked": true,
    "id": "248b58b3d024158607002ef483df7a8701b5a8fb",
    "semantic_title": "energy-based models for atomic-resolution protein conformations",
    "citation_count": 57,
    "authors": []
  },
  "https://openreview.net/forum?id=Syx79eBKwr": {
    "title": "A Mutual Information Maximization Perspective of Language Representation Learning",
    "volume": "spotlight",
    "abstract": "We show state-of-the-art word representation learning methods maximize an objective function that is a lower bound on the mutual information between different parts of a word sequence (i.e., a sentence). Our formulation provides an alternative perspective that unifies classical word embedding models (e.g., Skip-gram) and modern contextual embeddings (e.g., BERT, XLNet). In addition to enhancing our theoretical understanding of these methods, our derivation leads to a principled framework that can be used to construct new self-supervised tasks. We provide an example by drawing inspirations from related methods based on mutual information maximization that have been successful in computer vision, and introduce a simple self-supervised objective that maximizes the mutual information between a global sentence representation and n-grams in the sentence. Our analysis offers a holistic view of representation learning methods to transfer knowledge and translate progress across multiple domains (e.g., natural language processing, computer vision, audio processing)",
    "checked": true,
    "id": "b85d339e49399966d629973c889e8edfca56517c",
    "semantic_title": "a mutual information maximization perspective of language representation learning",
    "citation_count": 167,
    "authors": []
  },
  "https://openreview.net/forum?id=r1eyceSYPr": {
    "title": "Unbiased Contrastive Divergence Algorithm for Training Energy-Based Latent Variable Models",
    "volume": "spotlight",
    "abstract": "The contrastive divergence algorithm is a popular approach to training energy-based latent variable models, which has been widely used in many machine learning models such as the restricted Boltzmann machines and deep belief nets. Despite its empirical success, the contrastive divergence algorithm is also known to have biases that severely affect its convergence. In this article we propose an unbiased version of the contrastive divergence algorithm that completely removes its bias in stochastic gradient methods, based on recent advances on unbiased Markov chain Monte Carlo methods. Rigorous theoretical analysis is developed to justify the proposed algorithm, and numerical experiments show that it significantly improves the existing method. Our findings suggest that the unbiased contrastive divergence algorithm is a promising approach to training general energy-based latent variable models",
    "checked": true,
    "id": "120cc7cdc52992b9769c738c3470d9e1cd8e99be",
    "semantic_title": "unbiased contrastive divergence algorithm for training energy-based latent variable models",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=S1xitgHtvS": {
    "title": "Making Sense of Reinforcement Learning and Probabilistic Inference",
    "volume": "spotlight",
    "abstract": "Reinforcement learning (RL) combines a control problem with statistical estimation: The system dynamics are not known to the agent, but can be learned through experience. A recent line of research casts ‘RL as inference' and suggests a particular framework to generalize the RL problem as probabilistic inference. Our paper surfaces a key shortcoming in that approach, and clarifies the sense in which RL can be coherently cast as an inference problem. In particular, an RL agent must consider the effects of its actions upon future rewards and observations: The exploration-exploitation tradeoff. In all but the most simple settings, the resulting inference is computationally intractable so that practical RL algorithms must resort to approximation. We demonstrate that the popular ‘RL as inference' approximation can perform poorly in even very basic problems. However, we show that with a small modification the framework does yield algorithms that can provably perform well, and we show that the resulting algorithm is equivalent to the recently proposed K-learning, which we further connect with Thompson sampling",
    "checked": true,
    "id": "64b49df12b5e9a27c32b80e6869a95fc5dfad219",
    "semantic_title": "making sense of reinforcement learning and probabilistic inference",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=S1eZYeHFDS": {
    "title": "Deep Learning For Symbolic Mathematics",
    "volume": "spotlight",
    "abstract": "Neural networks have a reputation for being better at solving statistical or approximate problems than at performing calculations or working with symbolic data. In this paper, we show that they can be surprisingly good at more elaborated tasks in mathematics, such as symbolic integration and solving differential equations. We propose a syntax for representing these mathematical problems, and methods for generating large datasets that can be used to train sequence-to-sequence models. We achieve results that outperform commercial Computer Algebra Systems such as Matlab or Mathematica",
    "checked": true,
    "id": "b39eed03d345f5c244eac12fd1315d26eba77d62",
    "semantic_title": "deep learning for symbolic mathematics",
    "citation_count": 414,
    "authors": []
  },
  "https://openreview.net/forum?id=SylkYeHtwr": {
    "title": "SUMO: Unbiased Estimation of Log Marginal Probability for Latent Variable Models",
    "volume": "spotlight",
    "abstract": "Standard variational lower bounds used to train latent variable models produce biased estimates of most quantities of interest. We introduce an unbiased estimator of the log marginal likelihood and its gradients for latent variable models based on randomized truncation of infinite series. If parameterized by an encoder-decoder architecture, the parameters of the encoder can be optimized to minimize its variance of this estimator. We show that models trained using our estimator give better test-set likelihoods than a standard importance-sampling based approach for the same average computational cost. This estimator also allows use of latent variable models for tasks where unbiased estimators, rather than marginal likelihood lower bounds, are preferred, such as minimizing reverse KL divergences and estimating score functions",
    "checked": true,
    "id": "a3d1024c3f57f6d0cf7335b0711089c725de1ccc",
    "semantic_title": "sumo: unbiased estimation of log marginal probability for latent variable models",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=B1e3OlStPB": {
    "title": "DeepSphere: a graph-based spherical CNN",
    "volume": "spotlight",
    "abstract": "Designing a convolution for a spherical neural network requires a delicate tradeoff between efficiency and rotation equivariance. DeepSphere, a method based on a graph representation of the discretized sphere, strikes a controllable balance between these two desiderata. This contribution is twofold. First, we study both theoretically and empirically how equivariance is affected by the underlying graph with respect to the number of pixels and neighbors. Second, we evaluate DeepSphere on relevant problems. Experiments show state-of-the-art performance and demonstrates the efficiency and flexibility of this formulation. Perhaps surprisingly, comparison with previous work suggests that anisotropic filters might be an unnecessary price to pay. Our code is available at https://github.com/deepsphere",
    "checked": true,
    "id": "067c5f3753709b48023678fc12063459b4d82cb6",
    "semantic_title": "deepsphere: a graph-based spherical cnn",
    "citation_count": 87,
    "authors": []
  },
  "https://openreview.net/forum?id=H1gNOeHKPS": {
    "title": "Neural Arithmetic Units",
    "volume": "spotlight",
    "abstract": "Neural networks can approximate complex functions, but they struggle to perform exact arithmetic operations over real numbers. The lack of inductive bias for arithmetic operations leaves neural networks without the underlying logic necessary to extrapolate on tasks such as addition, subtraction, and multiplication. We present two new neural network components: the Neural Addition Unit (NAU), which can learn exact addition and subtraction; and the Neural Multiplication Unit (NMU) that can multiply subsets of a vector. The NMU is, to our knowledge, the first arithmetic neural network component that can learn to multiply elements from a vector, when the hidden size is large. The two new components draw inspiration from a theoretical analysis of recently proposed arithmetic components. We find that careful initialization, restricting parameter space, and regularizing for sparsity is important when optimizing the NAU and NMU. Our proposed units NAU and NMU, compared with previous neural units, converge more consistently, have fewer parameters, learn faster, can converge for larger hidden sizes, obtain sparse and meaningful weights, and can extrapolate to negative and small values",
    "checked": true,
    "id": "4aea918d9bd66440ce0c00abbfbb57b212d76158",
    "semantic_title": "neural arithmetic units",
    "citation_count": 45,
    "authors": []
  },
  "https://openreview.net/forum?id=HyxyIgHFvr": {
    "title": "Truth or backpropaganda? An empirical investigation of deep learning theory",
    "volume": "spotlight",
    "abstract": "We empirically evaluate common assumptions about neural networks that are widely held by practitioners and theorists alike. In this work, we: (1) prove the widespread existence of suboptimal local minima in the loss landscape of neural networks, and we use our theory to find examples; (2) show that small-norm parameters are not optimal for generalization; (3) demonstrate that ResNets do not conform to wide-network theories, such as the neural tangent kernel, and that the interaction between skip connections and batch normalization plays a role; (4) find that rank does not correlate with generalization or robustness in a practical setting",
    "checked": true,
    "id": "5c4c0a74283931d1625045169ae082f836b0d08e",
    "semantic_title": "truth or backpropaganda? an empirical investigation of deep learning theory",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=BJxsrgStvr": {
    "title": "Drawing Early-Bird Tickets: Toward More Efficient Training of Deep Networks",
    "volume": "spotlight",
    "abstract": "(Frankle & Carbin, 2019) shows that there exist winning tickets (small but critical subnetworks) for dense, randomly initialized networks, that can be trained alone to achieve comparable accuracies to the latter in a similar number of iterations. However, the identification of these winning tickets still requires the costly train-prune-retrain process, limiting their practical benefits. In this paper, we discover for the first time that the winning tickets can be identified at the very early training stage, which we term as Early-Bird (EB) tickets, via low-cost training schemes (e.g., early stopping and low-precision training) at large learning rates. Our finding of EB tickets is consistent with recently reported observations that the key connectivity patterns of neural networks emerge early. Furthermore, we propose a mask distance metric that can be used to identify EB tickets with low computational overhead, without needing to know the true winning tickets that emerge after the full training. Finally, we leverage the existence of EB tickets and the proposed mask distance to develop efficient training methods, which are achieved by first identifying EB tickets via low-cost schemes, and then continuing to train merely the EB tickets towards the target accuracy. Experiments based on various deep networks and datasets validate: 1) the existence of EB tickets and the effectiveness of mask distance in efficiently identifying them; and 2) that the proposed efficient training via EB tickets can achieve up to 5.8x ~ 10.7x energy savings while maintaining comparable or even better accuracy as compared to the most competitive state-of-the-art training methods, demonstrating a promising and easily adopted method for tackling cost-prohibitive deep network training",
    "checked": true,
    "id": "de5c34fc56e1aa9aaf8ab58a668c655b942e70c8",
    "semantic_title": "drawing early-bird tickets: towards more efficient training of deep networks",
    "citation_count": 256,
    "authors": []
  },
  "https://openreview.net/forum?id=S1evHerYPr": {
    "title": "Improving Generalization in Meta Reinforcement Learning using Learned Objectives",
    "volume": "spotlight",
    "abstract": "Biological evolution has distilled the experiences of many learners into the general learning algorithms of humans. Our novel meta reinforcement learning algorithm MetaGenRL is inspired by this process. MetaGenRL distills the experiences of many complex agents to meta-learn a low-complexity neural objective function that decides how future individuals will learn. Unlike recent meta-RL algorithms, MetaGenRL can generalize to new environments that are entirely different from those used for meta-training. In some cases, it even outperforms human-engineered RL algorithms. MetaGenRL uses off-policy second-order gradients during meta-training that greatly increase its sample efficiency",
    "checked": true,
    "id": "2c819870111efb9fa70e359c15e2031e992c2b4a",
    "semantic_title": "improving generalization in meta reinforcement learning using learned objectives",
    "citation_count": 119,
    "authors": []
  },
  "https://openreview.net/forum?id=BkgrBgSYDS": {
    "title": "Kaleidoscope: An Efficient, Learnable Representation For All Structured Linear Maps",
    "volume": "spotlight",
    "abstract": "Modern neural network architectures use structured linear transformations, such as low-rank matrices, sparse matrices, permutations, and the Fourier transform, to improve inference speed and reduce memory usage compared to general linear maps. However, choosing which of the myriad structured transformations to use (and its associated parameterization) is a laborious task that requires trading off speed, space, and accuracy. We consider a different approach: we introduce a family of matrices called kaleidoscope matrices (K-matrices) that provably capture any structured matrix with near-optimal space (parameter) and time (arithmetic operation) complexity. We empirically validate that K-matrices can be automatically learned within end-to-end pipelines to replace hand-crafted procedures, in order to improve model quality. For example, replacing channel shuffles in ShuffleNet improves classification accuracy on ImageNet by up to 5%. K-matrices can also simplify hand-engineered pipelines---we replace filter bank feature computation in speech data preprocessing with a learnable kaleidoscope layer, resulting in only 0.4% loss in accuracy on the TIMIT speech recognition task. In addition, K-matrices can capture latent structure in models: for a challenging permuted image classification task, adding a K-matrix to a standard convolutional architecture can enable learning the latent permutation and improve accuracy by over 8 points. We provide a practically efficient implementation of our approach, and use K-matrices in a Transformer network to attain 36% faster end-to-end inference speed on a language translation task",
    "checked": true,
    "id": "a68c3412e60560290400d2707596f82a914b7c00",
    "semantic_title": "kaleidoscope: an efficient, learnable representation for all structured linear maps",
    "citation_count": 49,
    "authors": []
  },
  "https://openreview.net/forum?id=rygeHgSFDH": {
    "title": "Disentanglement by Nonlinear ICA with General Incompressible-flow Networks (GIN)",
    "volume": "spotlight",
    "abstract": "A central question of representation learning asks under which conditions it is possible to reconstruct the true latent variables of an arbitrarily complex generative process. Recent breakthrough work by Khemakhem et al. (2019) on nonlinear ICA has answered this question for a broad class of conditional generative processes. We extend this important result in a direction relevant for application to real-world data. First, we generalize the theory to the case of unknown intrinsic problem dimension and prove that in some special (but not very restrictive) cases, informative latent variables will be automatically separated from noise by an estimating model. Furthermore, the recovered informative latent variables will be in one-to-one correspondence with the true latent variables of the generating process, up to a trivial component-wise transformation. Second, we introduce a modification of the RealNVP invertible neural network architecture (Dinh et al. (2016)) which is particularly suitable for this type of problem: the General Incompressible-flow Network (GIN). Experiments on artificial data and EMNIST demonstrate that theoretical predictions are indeed verified in practice. In particular, we provide a detailed set of exactly 22 informative latent variables extracted from EMNIST",
    "checked": true,
    "id": "326eb3e8a3bef4c3b21d6d7ca993c1166e8f9833",
    "semantic_title": "disentanglement by nonlinear ica with general incompressible-flow networks (gin)",
    "citation_count": 121,
    "authors": []
  },
  "https://openreview.net/forum?id=SJgwNerKvB": {
    "title": "Continual learning with hypernetworks",
    "volume": "spotlight",
    "abstract": "Artificial neural networks suffer from catastrophic forgetting when they are sequentially trained on multiple tasks. To overcome this problem, we present a novel approach based on task-conditioned hypernetworks, i.e., networks that generate the weights of a target model based on task identity. Continual learning (CL) is less difficult for this class of models thanks to a simple key feature: instead of recalling the input-output relations of all previously seen data, task-conditioned hypernetworks only require rehearsing task-specific weight realizations, which can be maintained in memory using a simple regularizer. Besides achieving state-of-the-art performance on standard CL benchmarks, additional experiments on long task sequences reveal that task-conditioned hypernetworks display a very large capacity to retain previous memories. Notably, such long memory lifetimes are achieved in a compressive regime, when the number of trainable hypernetwork weights is comparable or smaller than target network size. We provide insight into the structure of low-dimensional task embedding spaces (the input space of the hypernetwork) and show that task-conditioned hypernetworks demonstrate transfer learning. Finally, forward information transfer is further supported by empirical results on a challenging CL benchmark based on the CIFAR-10/100 image datasets",
    "checked": true,
    "id": "191461705e9961a3b07273c19d89eb214819002c",
    "semantic_title": "continual learning with hypernetworks",
    "citation_count": 363,
    "authors": []
  },
  "https://openreview.net/forum?id=Bkeb7lHtvH": {
    "title": "At Stability's Edge: How to Adjust Hyperparameters to Preserve Minima Selection in Asynchronous Training of Neural Networks?",
    "volume": "spotlight",
    "abstract": "Background: Recent developments have made it possible to accelerate neural networks training significantly using large batch sizes and data parallelism. Training in an asynchronous fashion, where delay occurs, can make training even more scalable. However, asynchronous training has its pitfalls, mainly a degradation in generalization, even after convergence of the algorithm. This gap remains not well understood, as theoretical analysis so far mainly focused on the convergence rate of asynchronous methods. Contributions: We examine asynchronous training from the perspective of dynamical stability. We find that the degree of delay interacts with the learning rate, to change the set of minima accessible by an asynchronous stochastic gradient descent algorithm. We derive closed-form rules on how the learning rate could be changed, while keeping the accessible set the same. Specifically, for high delay values, we find that the learning rate should be kept inversely proportional to the delay. We then extend this analysis to include momentum. We find momentum should be either turned off, or modified to improve training stability. We provide empirical experiments to validate our theoretical findings",
    "checked": true,
    "id": "59773c234418e99a70edcbe9b6ad3327c4e7b0f4",
    "semantic_title": "at stability's edge: how to adjust hyperparameters to preserve minima selection in asynchronous training of neural networks?",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=ByeGzlrKwH": {
    "title": "Compression based bound for non-compressed network: unified generalization error analysis of large compressible deep neural network",
    "volume": "spotlight",
    "abstract": "One of the biggest issues in deep learning theory is the generalization ability of networks with huge model size. The classical learning theory suggests that overparameterized models cause overfitting. However, practically used large deep models avoid overfitting, which is not well explained by the classical approaches. To resolve this issue, several attempts have been made. Among them, the compression based bound is one of the promising approaches. However, the compression based bound can be applied only to a compressed network, and it is not applicable to the non-compressed original network. In this paper, we give a unified frame-work that can convert compression based bounds to those for non-compressed original networks. The bound gives even better rate than the one for the compressed network by improving the bias term. By establishing the unified frame-work, we can obtain a data dependent generalization error bound which gives a tighter evaluation than the data independent ones",
    "checked": true,
    "id": "84d20c13c0d07942c5c0c5c678b1195de3ab305f",
    "semantic_title": "compression based bound for non-compressed network: unified generalization error analysis of large compressible deep neural network",
    "citation_count": 44,
    "authors": []
  },
  "https://openreview.net/forum?id=H1xFWgrFPS": {
    "title": "Explanation by Progressive Exaggeration",
    "volume": "spotlight",
    "abstract": "As machine learning methods see greater adoption and implementation in high stakes applications such as medical image diagnosis, the need for model interpretability and explanation has become more critical. Classical approaches that assess feature importance (eg saliency maps) do not explain how and why a particular region of an image is relevant to the prediction. We propose a method that explains the outcome of a classification black-box by gradually exaggerating the semantic effect of a given class. Given a query input to a classifier, our method produces a progressive set of plausible variations of that query, which gradually change the posterior probability from its original class to its negation. These counter-factually generated samples preserve features unrelated to the classification decision, such that a user can employ our method as a ``tuning knob'' to traverse a data manifold while crossing the decision boundary. Our method is model agnostic and only requires the output value and gradient of the predictor with respect to its input",
    "checked": true,
    "id": "c0072c9c25fb94f6e62bb7b18520add3c2df12a9",
    "semantic_title": "explanation by progressive exaggeration",
    "citation_count": 103,
    "authors": []
  },
  "https://openreview.net/forum?id=B1eWbxStPH": {
    "title": "Directional Message Passing for Molecular Graphs",
    "volume": "spotlight",
    "abstract": "Graph neural networks have recently achieved great successes in predicting quantum mechanical properties of molecules. These models represent a molecule as a graph using only the distance between atoms (nodes). They do not, however, consider the spatial direction from one atom to another, despite directional information playing a central role in empirical potentials for molecules, e.g. in angular potentials. To alleviate this limitation we propose directional message passing, in which we embed the messages passed between atoms instead of the atoms themselves. Each message is associated with a direction in coordinate space. These directional message embeddings are rotationally equivariant since the associated directions rotate with the molecule. We propose a message passing scheme analogous to belief propagation, which uses the directional information by transforming messages based on the angle between them. Additionally, we use spherical Bessel functions and spherical harmonics to construct theoretically well-founded, orthogonal representations that achieve better performance than the currently prevalent Gaussian radial basis representations while using fewer than 1/4 of the parameters. We leverage these innovations to construct the directional message passing neural network (DimeNet). DimeNet outperforms previous GNNs on average by 76% on MD17 and by 31% on QM9. Our implementation is available online",
    "checked": true,
    "id": "6bc4004347cdf76d84597210796f38fcf7a01a80",
    "semantic_title": "directional message passing for molecular graphs",
    "citation_count": 881,
    "authors": []
  },
  "https://openreview.net/forum?id=SkeuexBtDr": {
    "title": "Learning from Rules Generalizing Labeled Exemplars",
    "volume": "spotlight",
    "abstract": "In many applications labeled data is not readily available, and needs to be collected via pain-staking human supervision. We propose a rule-exemplar method for collecting human supervision to combine the efficiency of rules with the quality of instance labels. The supervision is coupled such that it is both natural for humans and synergistic for learning. We propose a training algorithm that jointly denoises rules via latent coverage variables, and trains the model through a soft implication loss over the coverage and label variables. The denoised rules and trained model are used jointly for inference. Empirical evaluation on five different tasks shows that (1) our algorithm is more accurate than several existing methods of learning from a mix of clean and noisy supervision, and (2) the coupled rule-exemplar supervision is effective in denoising rules",
    "checked": true,
    "id": "b863c8216ea6f4edc72b2988d99d8f24243e8bf8",
    "semantic_title": "learning from rules generalizing labeled exemplars",
    "citation_count": 86,
    "authors": []
  },
  "https://openreview.net/forum?id=B1gdkxHFDH": {
    "title": "Training individually fair ML models with sensitive subspace robustness",
    "volume": "spotlight",
    "abstract": "We consider training machine learning models that are fair in the sense that their performance is invariant under certain sensitive perturbations to the inputs. For example, the performance of a resume screening system should be invariant under changes to the gender and/or ethnicity of the applicant. We formalize this notion of algorithmic fairness as a variant of individual fairness and develop a distributionally robust optimization approach to enforce it during training. We also demonstrate the effectiveness of the approach on two ML tasks that are susceptible to gender and racial biases",
    "checked": true,
    "id": "b1e6716d068ee0d98aa213ad496ec189c10f9250",
    "semantic_title": "training individually fair ml models with sensitive subspace robustness",
    "citation_count": 120,
    "authors": []
  },
  "https://openreview.net/forum?id=rJxbJeHFPS": {
    "title": "What Can Neural Networks Reason About?",
    "volume": "spotlight",
    "abstract": "Neural networks have succeeded in many reasoning tasks. Empirically, these tasks require specialized network structures, e.g., Graph Neural Networks (GNNs) perform well on many such tasks, but less structured networks fail. Theoretically, there is limited understanding of why and when a network structure generalizes better than others, although they have equal expressive power. In this paper, we develop a framework to characterize which reasoning tasks a network can learn well, by studying how well its computation structure aligns with the algorithmic structure of the relevant reasoning process. We formally define this algorithmic alignment and derive a sample complexity bound that decreases with better alignment. This framework offers an explanation for the empirical success of popular reasoning models, and suggests their limitations. As an example, we unify seemingly different reasoning tasks, such as intuitive physics, visual question answering, and shortest paths, via the lens of a powerful algorithmic paradigm, dynamic programming (DP). We show that GNNs align with DP and thus are expected to solve these tasks. On several reasoning tasks, our theory is supported by empirical results",
    "checked": true,
    "id": "271b31b41782d11c5176a260f0ecdf2611b21e77",
    "semantic_title": "what can neural networks reason about?",
    "citation_count": 248,
    "authors": []
  },
  "https://openreview.net/forum?id=HkxARkrFwB": {
    "title": "word2ket: Space-efficient Word Embeddings inspired by Quantum Entanglement",
    "volume": "spotlight",
    "abstract": "Deep learning natural language processing models often use vector word embeddings, such as word2vec or GloVe, to represent words. A discrete sequence of words can be much more easily integrated with downstream neural layers if it is represented as a sequence of continuous vectors. Also, semantic relationships between words, learned from a text corpus, can be encoded in the relative configurations of the embedding vectors. However, storing and accessing embedding vectors for all words in a dictionary requires large amount of space, and may stain systems with limited GPU memory. Here, we used approaches inspired by quantum computing to propose two related methods, word2ket and word2ketXS, for storing word embedding matrix during training and inference in a highly efficient way. Our approach achieves a hundred-fold or more reduction in the space required to store the embeddings with almost no relative drop in accuracy in practical natural language processing tasks",
    "checked": true,
    "id": "48988bd2d17ff4fa00654e3e983acf390bb0f110",
    "semantic_title": "word2ket: space-efficient word embeddings inspired by quantum entanglement",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=BkxRRkSKwr": {
    "title": "Towards Hierarchical Importance Attribution: Explaining Compositional Semantics for Neural Sequence Models",
    "volume": "spotlight",
    "abstract": "The impressive performance of neural networks on natural language processing tasks attributes to their ability to model complicated word and phrase compositions. To explain how the model handles semantic compositions, we study hierarchical explanation of neural network predictions. We identify non-additivity and context independent importance attributions within hierarchies as two desirable properties for highlighting word and phrase compositions. We show some prior efforts on hierarchical explanations, e.g. contextual decomposition, do not satisfy the desired properties mathematically, leading to inconsistent explanation quality in different models. In this paper, we start by proposing a formal and general way to quantify the importance of each word and phrase. Following the formulation, we propose Sampling and Contextual Decomposition (SCD) algorithm and Sampling and Occlusion (SOC) algorithm. Human and metrics evaluation on both LSTM models and BERT Transformer models on multiple datasets show that our algorithms outperform prior hierarchical explanation algorithms. Our algorithms help to visualize semantic composition captured by models, extract classification rules and improve human trust of models",
    "checked": true,
    "id": "3ad3146eb5aeef3a7fd50ba3d25741e3525f4220",
    "semantic_title": "towards hierarchical importance attribution: explaining compositional semantics for neural sequence models",
    "citation_count": 50,
    "authors": []
  },
  "https://openreview.net/forum?id=H1l_0JBYwS": {
    "title": "Spectral Embedding of Regularized Block Models",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "e35debf74eab7e3cb113e4553ec0b89a421a34eb",
    "semantic_title": "spectral embedding of regularized block models",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=BJliakStvH": {
    "title": "Maximum Likelihood Constraint Inference for Inverse Reinforcement Learning",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "b54d49cdf57abf7f7e7bcc0e946f450fd807f829",
    "semantic_title": "maximum likelihood constraint inference for inverse reinforcement learning",
    "citation_count": 64,
    "authors": []
  },
  "https://openreview.net/forum?id=ryeYpJSKwr": {
    "title": "Meta-Learning Acquisition Functions for Transfer Learning in Bayesian Optimization",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "86b8ed3f8ff08775f1c49f106bf591415c56cfa4",
    "semantic_title": "meta-learning acquisition functions for transfer learning in bayesian optimization",
    "citation_count": 85,
    "authors": []
  },
  "https://openreview.net/forum?id=rJe2syrtvS": {
    "title": "The Ingredients of Real World Robotic Reinforcement Learning",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "2d85f63a193cd88741c8398ceb98c55e1e89387d",
    "semantic_title": "the ingredients of real-world robotic reinforcement learning",
    "citation_count": 180,
    "authors": []
  },
  "https://openreview.net/forum?id=rJgsskrFwH": {
    "title": "Scaling Autoregressive Video Models",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "e763fdc9ae56826ff799163ea035b29bffd8ea6f",
    "semantic_title": "scaling autoregressive video models",
    "citation_count": 204,
    "authors": []
  },
  "https://openreview.net/forum?id=BkevoJSYPB": {
    "title": "Differentiation of Blackbox Combinatorial Solvers",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "15b0dbc21d0e5875c00bbdda7c3abb4cfc797062",
    "semantic_title": "differentiation of blackbox combinatorial solvers",
    "citation_count": 294,
    "authors": []
  },
  "https://openreview.net/forum?id=rkl8sJBYvH": {
    "title": "Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "740d3706b1b4aae35aaa67d44461d13133124094",
    "semantic_title": "harnessing the power of infinitely wide deep nets on small-data tasks",
    "citation_count": 162,
    "authors": []
  },
  "https://openreview.net/forum?id=S1e4jkSKvB": {
    "title": "The intriguing role of module criticality in the generalization of deep networks",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "0aceab048a0c691615f3f7c76b3800b7d2a79d5a",
    "semantic_title": "the intriguing role of module criticality in the generalization of deep networks",
    "citation_count": 52,
    "authors": []
  },
  "https://openreview.net/forum?id=Hyx-jyBFPr": {
    "title": "Self-labelling via simultaneous clustering and representation learning",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "87045bfc6f8036d032ab6ad1ebeb0377db05da9a",
    "semantic_title": "self-labelling via simultaneous clustering and representation learning",
    "citation_count": 778,
    "authors": []
  },
  "https://openreview.net/forum?id=SklD9yrFPS": {
    "title": "Neural Tangents: Fast and Easy Infinite Neural Networks in Python",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "e7f0317b56288b93ac5d2cd6e5ce0455f5a93831",
    "semantic_title": "neural tangents: fast and easy infinite neural networks in python",
    "citation_count": 230,
    "authors": []
  },
  "https://openreview.net/forum?id=Hke0K1HKwr": {
    "title": "Sequential Latent Knowledge Selection for Knowledge-Grounded Dialogue",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "f866406e6111eece3be2a818706453818ca65579",
    "semantic_title": "sequential latent knowledge selection for knowledge-grounded dialogue",
    "citation_count": 169,
    "authors": []
  },
  "https://openreview.net/forum?id=SJlpYJBKvH": {
    "title": "Measuring the Reliability of Reinforcement Learning Algorithms",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "b06e9bd7094c853d75a6fb7a0e024728c58b1770",
    "semantic_title": "measuring the reliability of reinforcement learning algorithms",
    "citation_count": 84,
    "authors": []
  },
  "https://openreview.net/forum?id=H1enKkrFDB": {
    "title": "Stable Rank Normalization for Improved Generalization in Neural Networks and GANs",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "2ac189d66e5b44d5992c549c22565029e875708a",
    "semantic_title": "stable rank normalization for improved generalization in neural networks and gans",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=rkgbYyHtwB": {
    "title": "Disagreement-Regularized Imitation Learning",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "913e9384a1440dbf860f6d8597953187237fa1b4",
    "semantic_title": "disagreement-regularized imitation learning",
    "citation_count": 103,
    "authors": []
  },
  "https://openreview.net/forum?id=S1xCPJHtDB": {
    "title": "Model Based Reinforcement Learning for Atari",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "1fd4694e7c2d9c872a427d50e81b5475056de6bc",
    "semantic_title": "model-based reinforcement learning for atari",
    "citation_count": 870,
    "authors": []
  },
  "https://openreview.net/forum?id=HyevIJStwH": {
    "title": "Understanding Why Neural Networks Generalize Well Through GSNR of Parameters",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "173ef40f5a1adacc2d3a04fce50f42a81057c357",
    "semantic_title": "understanding why neural networks generalize well through gsnr of parameters",
    "citation_count": 50,
    "authors": []
  },
  "https://openreview.net/forum?id=BJxSI1SKDH": {
    "title": "A Latent Morphology Model for Open-Vocabulary Neural Machine Translation",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "229aff74294275d31ded5c8e6e80f6aa6a88fcfd",
    "semantic_title": "a latent morphology model for open-vocabulary neural machine translation",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=rJehVyrKwH": {
    "title": "And the Bit Goes Down: Revisiting the Quantization of Neural Networks",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "adce96bdd3f3093a90933f288106b67f20e094f6",
    "semantic_title": "and the bit goes down: revisiting the quantization of neural networks",
    "citation_count": 149,
    "authors": []
  },
  "https://openreview.net/forum?id=Hklz71rYvS": {
    "title": "Kernelized Wasserstein Natural Gradient",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "6fe9469d615a5c03fb6ce53fb4afacd68d8fb6b5",
    "semantic_title": "kernelized wasserstein natural gradient",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=BygzbyHFvB": {
    "title": "FreeLB: Enhanced Adversarial Training for Natural Language Understanding",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "d01fa0311e8e15b8b874b376123530c815f52852",
    "semantic_title": "freelb: enhanced adversarial training for natural language understanding",
    "citation_count": 443,
    "authors": []
  },
  "https://openreview.net/forum?id=rygf-kSYwH": {
    "title": "Behaviour Suite for Reinforcement Learning",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "3606c313dccd21f784bedf5759152a3677f5719d",
    "semantic_title": "behaviour suite for reinforcement learning",
    "citation_count": 183,
    "authors": []
  },
  "https://openreview.net/forum?id=HJlWWJSFDH": {
    "title": "Strategies for Pre-training Graph Neural Networks",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "789a7069d1a2d02d784e4821685b216cc63e6ec8",
    "semantic_title": "strategies for pre-training graph neural networks",
    "citation_count": 1417,
    "authors": []
  },
  "https://openreview.net/forum?id=HJxyZkBKDr": {
    "title": "NAS-Bench-201: Extending the Scope of Reproducible Neural Architecture Search",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "69599593f93023e2f91ef6673ee9860f85777d98",
    "semantic_title": "nas-bench-201: extending the scope of reproducible neural architecture search",
    "citation_count": 715,
    "authors": []
  },
  "https://openreview.net/forum?id=SkxpxJBKwS": {
    "title": "Emergent Tool Use From Multi-Agent Autocurricula",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "33a2c0f3b9a0adc452a13d53f950c0c3c4abb11b",
    "semantic_title": "emergent tool use from multi-agent autocurricula",
    "citation_count": 658,
    "authors": []
  },
  "https://openreview.net/forum?id=HJlA0C4tPS": {
    "title": "A Probabilistic Formulation of Unsupervised Text Style Transfer",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "57e4074c588c0e27e4c0bc89f12512ccdb900d79",
    "semantic_title": "a probabilistic formulation of unsupervised text style transfer",
    "citation_count": 132,
    "authors": []
  },
  "https://openreview.net/forum?id=S1lOTC4tDS": {
    "title": "Dream to Control: Learning Behaviors by Latent Imagination",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "0cc956565c7d249d4197eeb1dbab6523c648b2c9",
    "semantic_title": "dream to control: learning behaviors by latent imagination",
    "citation_count": 1374,
    "authors": []
  },
  "https://openreview.net/forum?id=B1lPaCNtPB": {
    "title": "Real or Not Real, that is the Question",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "7a2c9cc48062fcfd77a2e6e97aa48119a21c9f7d",
    "semantic_title": "real or not real, that is the question",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=ryxjnREFwH": {
    "title": "Neural Symbolic Reader: Scalable Integration of Distributed and Symbolic Representations for Reading Comprehension",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "b9a5aa5db8836744ff2073e8368520b7a614049f",
    "semantic_title": "neural symbolic reader: scalable integration of distributed and symbolic representations for reading comprehension",
    "citation_count": 101,
    "authors": []
  },
  "https://openreview.net/forum?id=rkeu30EtvS": {
    "title": "Network Deconvolution",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "ef2e3eaab519ae5fb70b97fed97b4ded4499f9b6",
    "semantic_title": "network deconvolution",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=B1xm3RVtwB": {
    "title": "Simplified Action Decoder for Deep Multi-Agent Reinforcement Learning",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "62ccfc7805e287bfe18450e8a6ba7a7ff407e247",
    "semantic_title": "simplified action decoder for deep multi-agent reinforcement learning",
    "citation_count": 82,
    "authors": []
  },
  "https://openreview.net/forum?id=r1genAVKPB": {
    "title": "Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning?",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "8e98719529f8e029210b6c31d54e7486ee00d0af",
    "semantic_title": "is a good representation sufficient for sample efficient reinforcement learning?",
    "citation_count": 193,
    "authors": []
  },
  "https://openreview.net/forum?id=Sklgs0NFvr": {
    "title": "Learning The Difference That Makes A Difference With Counterfactually-Augmented Data",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "47f1eb0dc42189ba7cf21b76598c8217eb1b6e05",
    "semantic_title": "learning the difference that makes a difference with counterfactually-augmented data",
    "citation_count": 571,
    "authors": []
  },
  "https://openreview.net/forum?id=S1gFvANKDS": {
    "title": "Asymptotics of Wide Networks from Feynman Diagrams",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "468ce1e82b27f14d4927dabf5c2b9308bca49c99",
    "semantic_title": "asymptotics of wide networks from feynman diagrams",
    "citation_count": 115,
    "authors": []
  },
  "https://openreview.net/forum?id=BkgYPREtPr": {
    "title": "Symplectic Recurrent Neural Networks",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "ffb1b305dfd84b81999da356cd8f9790636414df",
    "semantic_title": "symplectic recurrent neural networks",
    "citation_count": 225,
    "authors": []
  },
  "https://openreview.net/forum?id=rJgJDAVKvB": {
    "title": "Learning to Plan in High Dimensions via Neural Exploration-Exploitation Trees",
    "volume": "spotlight",
    "abstract": "We propose a meta path planning algorithm named \\emph{Neural Exploration-Exploitation Trees~(NEXT)} for learning from prior experience for solving new path planning problems in high dimensional continuous state and action spaces. Compared to more classical sampling-based methods like RRT, our approach achieves much better sample efficiency in high-dimensions and can benefit from prior experience of planning in similar environments. More specifically, NEXT exploits a novel neural architecture which can learn promising search directions from problem structures. The learned prior is then integrated into a UCB-type algorithm to achieve an online balance between \\emph{exploration} and \\emph{exploitation} when solving a new problem. We conduct thorough experiments to show that NEXT accomplishes new planning problems with more compact search trees and significantly outperforms state-of-the-art methods on several benchmarks",
    "checked": true,
    "id": "9d32d80f940b4418a0ecf31514b6ff4df0eea690",
    "semantic_title": "learning to plan in high dimensions via neural exploration-exploitation trees",
    "citation_count": 52,
    "authors": []
  },
  "https://openreview.net/forum?id=HJxrVA4FDS": {
    "title": "Disentangling neural mechanisms for perceptual grouping",
    "volume": "spotlight",
    "abstract": "Forming perceptual groups and individuating objects in visual scenes is an essential step towards visual intelligence. This ability is thought to arise in the brain from computations implemented by bottom-up, horizontal, and top-down connections between neurons. However, the relative contributions of these connections to perceptual grouping are poorly understood. We address this question by systematically evaluating neural network architectures featuring combinations bottom-up, horizontal, and top-down connections on two synthetic visual tasks, which stress low-level \"Gestalt\" vs. high-level object cues for perceptual grouping. We show that increasing the difficulty of either task strains learning for networks that rely solely on bottom-up connections. Horizontal connections resolve straining on tasks with Gestalt cues by supporting incremental grouping, whereas top-down connections rescue learning on tasks with high-level object cues by modifying coarse predictions about the position of the target object. Our findings dissociate the computational roles of bottom-up, horizontal and top-down connectivity, and demonstrate how a model featuring all of these interactions can more flexibly learn to form perceptual groups",
    "checked": true,
    "id": "bac7cd6175d838e1a0dea3603226a520516fb33a",
    "semantic_title": "disentangling neural mechanisms for perceptual grouping",
    "citation_count": 54,
    "authors": []
  },
  "https://openreview.net/forum?id=H1eA7AEtvS": {
    "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
    "volume": "spotlight",
    "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT~\\citep{devlin2018bert}. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT",
    "checked": true,
    "id": "7a064df1aeada7e69e5173f7d4c8606f4470365b",
    "semantic_title": "albert: a lite bert for self-supervised learning of language representations",
    "citation_count": 6472,
    "authors": []
  },
  "https://openreview.net/forum?id=r1g87C4KwB": {
    "title": "The Break-Even Point on Optimization Trajectories of Deep Neural Networks",
    "volume": "spotlight",
    "abstract": "The early phase of training of deep neural networks is critical for their final performance. In this work, we study how the hyperparameters of stochastic gradient descent (SGD) used in the early phase of training affect the rest of the optimization trajectory. We argue for the existence of the \"``break-even\" point on this trajectory, beyond which the curvature of the loss surface and noise in the gradient are implicitly regularized by SGD. In particular, we demonstrate on multiple classification tasks that using a large learning rate in the initial phase of training reduces the variance of the gradient, and improves the conditioning of the covariance of gradients. These effects are beneficial from the optimization perspective and become visible after the break-even point. Complementing prior work, we also show that using a low learning rate results in bad conditioning of the loss surface even for a neural network with batch normalization layers. In short, our work shows that key properties of the loss surface are strongly influenced by SGD in the early phase of training. We argue that studying the impact of the identified effects on generalization is a promising future direction",
    "checked": true,
    "id": "77aecf26808e57974db6d4feb5ee1a7f375cb4c9",
    "semantic_title": "the break-even point on optimization trajectories of deep neural networks",
    "citation_count": 164,
    "authors": []
  },
  "https://openreview.net/forum?id=r1lZ7AEKvB": {
    "title": "The Logical Expressiveness of Graph Neural Networks",
    "volume": "spotlight",
    "abstract": "The ability of graph neural networks (GNNs) for distinguishing nodes in graphs has been recently characterized in terms of the Weisfeiler-Lehman (WL) test for checking graph isomorphism. This characterization, however, does not settle the issue of which Boolean node classifiers (i.e., functions classifying nodes in graphs as true or false) can be expressed by GNNs. We tackle this problem by focusing on Boolean classifiers expressible as formulas in the logic FOC2, a well-studied fragment of first order logic. FOC2 is tightly related to the WL test, and hence to GNNs. We start by studying a popular class of GNNs, which we call AC-GNNs, in which the features of each node in the graph are updated, in successive layers, only in terms of the features of its neighbors. We show that this class of GNNs is too weak to capture all FOC2 classifiers, and provide a syntactic characterization of the largest subclass of FOC2 classifiers that can be captured by AC-GNNs. This subclass coincides with a logic heavily used by the knowledge representation community. We then look at what needs to be added to AC-GNNs for capturing all FOC2 classifiers. We show that it suffices to add readout functions, which allow to update the features of a node not only in terms of its neighbors, but also in terms of a global attribute vector. We call GNNs of this kind ACR-GNNs. We experimentally validate our findings showing that, on synthetic data conforming to FOC2 formulas, AC-GNNs struggle to fit the training data while ACR-GNNs can generalize even to graphs of sizes not seen during training",
    "checked": true,
    "id": "6a7c6e9feab24ccf51ee9e53cbc0b42b9b421a7d",
    "semantic_title": "logical expressiveness of graph neural networks",
    "citation_count": 237,
    "authors": []
  },
  "https://openreview.net/forum?id=HkxYzANYDB": {
    "title": "CLEVRER: Collision Events for Video Representation and Reasoning",
    "volume": "spotlight",
    "abstract": "The ability to reason about temporal and causal events from videos lies at the core of human intelligence. Most video reasoning benchmarks, however, focus on pattern recognition from complex visual and language input, instead of on causal structure. We study the complementary problem, exploring the temporal and causal structures behind videos of objects with simple visual appearance. To this end, we introduce the CoLlision Events for Video REpresentation and Reasoning (CLEVRER) dataset, a diagnostic video dataset for systematic evaluation of computational models on a wide range of reasoning tasks. Motivated by the theory of human casual judgment, CLEVRER includes four types of question: descriptive (e.g., ‘what color'), explanatory (‘what's responsible for'), predictive (‘what will happen next'), and counterfactual (‘what if'). We evaluate various state-of-the-art models for visual reasoning on our benchmark. While these models thrive on the perception-based task (descriptive), they perform poorly on the causal tasks (explanatory, predictive and counterfactual), suggesting that a principled approach for causal reasoning should incorporate the capability of both perceiving complex visual and language inputs, and understanding the underlying dynamics and causal relations. We also study an oracle model that explicitly combines these components via symbolic representations",
    "checked": true,
    "id": "80ba19a949eee46af2bf58cd2f24949e009d884f",
    "semantic_title": "clevrer: collision events for video representation and reasoning",
    "citation_count": 475,
    "authors": []
  },
  "https://openreview.net/forum?id=H1ldzA4tPr": {
    "title": "Learning Compositional Koopman Operators for Model-Based Control",
    "volume": "spotlight",
    "abstract": "Finding an embedding space for a linear approximation of a nonlinear dynamical system enables efficient system identification and control synthesis. The Koopman operator theory lays the foundation for identifying the nonlinear-to-linear coordinate transformations with data-driven methods. Recently, researchers have proposed to use deep neural networks as a more expressive class of basis functions for calculating the Koopman operators. These approaches, however, assume a fixed dimensional state space; they are therefore not applicable to scenarios with a variable number of objects. In this paper, we propose to learn compositional Koopman operators, using graph neural networks to encode the state into object-centric embeddings and using a block-wise linear transition matrix to regularize the shared structure across objects. The learned dynamics can quickly adapt to new environments of unknown physical parameters and produce control signals to achieve a specified goal. Our experiments on manipulating ropes and controlling soft robots show that the proposed method has better efficiency and generalization ability than existing baselines",
    "checked": true,
    "id": "6d72353f63dbc9c056deaae8353240d8f2f49c78",
    "semantic_title": "learning compositional koopman operators for model-based control",
    "citation_count": 121,
    "authors": []
  },
  "https://openreview.net/forum?id=S1glGANtDr": {
    "title": "Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation",
    "volume": "spotlight",
    "abstract": "Infinite horizon off-policy policy evaluation is a highly challenging task due to the excessively large variance of typical importance sampling (IS) estimators. Recently, Liu et al. (2018) proposed an approach that significantly reduces the variance of infinite-horizon off-policy evaluation by estimating the stationary density ratio, but at the cost of introducing potentially high risks due to the error in density ratio estimation. In this paper, we develop a bias-reduced augmentation of their method, which can take advantage of a learned value function to obtain higher accuracy. Our method is doubly robust in that the bias vanishes when either the density ratio or value function estimation is perfect. In general, when either of them is accurate, the bias can also be reduced. Both theoretical and empirical results show that our method yields significant advantages over previous methods",
    "checked": true,
    "id": "c2c5d4650beb72a0e00a4c384064f4ab2ddef1dc",
    "semantic_title": "doubly robust bias reduction in infinite horizon off-policy estimation",
    "citation_count": 69,
    "authors": []
  },
  "https://openreview.net/forum?id=SyxrxR4KPS": {
    "title": "Deep neuroethology of a virtual rodent",
    "volume": "spotlight",
    "abstract": "Parallel developments in neuroscience and deep learning have led to mutually productive exchanges, pushing our understanding of real and artificial neural networks in sensory and cognitive systems. However, this interaction between fields is less developed in the study of motor control. In this work, we develop a virtual rodent as a platform for the grounded study of motor activity in artificial models of embodied control. We then use this platform to study motor activity across contexts by training a model to solve four complex tasks. Using methods familiar to neuroscientists, we describe the behavioral representations and algorithms employed by different layers of the network using a neuroethological approach to characterize motor activity relative to the rodent's behavior and goals. We find that the model uses two classes of representations which respectively encode the task-specific behavioral strategies and task-invariant behavioral kinematics. These representations are reflected in the sequential activity and population dynamics of neural subpopulations. Overall, the virtual rodent facilitates grounded collaborations between deep reinforcement learning and motor neuroscience",
    "checked": true,
    "id": "e9fd8b69faa4c909828134278c89d9aafffbcf39",
    "semantic_title": "deep neuroethology of a virtual rodent",
    "citation_count": 69,
    "authors": []
  },
  "https://openreview.net/forum?id=HklSeREtPB": {
    "title": "Emergence of functional and structural properties of the head direction system by optimization of recurrent neural networks",
    "volume": "spotlight",
    "abstract": "Recent work suggests goal-driven training of neural networks can be used to model neural activity in the brain. While response properties of neurons in artificial neural networks bear similarities to those in the brain, the network architectures are often constrained to be different. Here we ask if a neural network can recover both neural representations and, if the architecture is unconstrained and optimized, also the anatomical properties of neural circuits. We demonstrate this in a system where the connectivity and the functional organization have been characterized, namely, the head direction circuit of the rodent and fruit fly. We trained recurrent neural networks (RNNs) to estimate head direction through integration of angular velocity. We found that the two distinct classes of neurons observed in the head direction system, the Compass neurons and the Shifter neurons, emerged naturally in artificial neural networks as a result of training. Furthermore, connectivity analysis and in-silico neurophysiology revealed structural and mechanistic similarities between artificial networks and the head direction system. Overall, our results show that optimization of RNNs in a goal-driven task can recapitulate the structure and function of biological circuits, suggesting that artificial neural networks can be used to study the brain at the level of both neural activity and anatomical organization",
    "checked": true,
    "id": "b9ef4bd9d43d32df3f5f2caac6dc5b4e2f2bada9",
    "semantic_title": "emergence of functional and structural properties of the head direction system by optimization of recurrent neural networks",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=Hkx7xRVYDr": {
    "title": "Duration-of-Stay Storage Assignment under Uncertainty",
    "volume": "spotlight",
    "abstract": "Storage assignment, the act of choosing what goods are placed in what locations in a warehouse, is a central problem of supply chain logistics. Past literature has shown that the optimal method to assign pallets is to arrange them in increasing duration of stay in the warehouse (the Duration-of-Stay, or DoS, method), but the methodology requires perfect prior knowledge of DoS for each pallet, which is unknown and uncertain under realistic conditions. Attempts to predict DoS have largely been unfruitful due to the multi-valuedness nature (every shipment contains multiple identical pallets with different DoS) and data sparsity induced by lack of matching historical conditions. In this paper, we introduce a new framework for storage assignment that provides a solution to the DoS prediction problem through a distributional reformulation and a novel neural network, ParallelNet. Through collaboration with a world-leading cold storage company, we show that the system is able to predict DoS with a MAPE of 29%, a decrease of ~30% compared to a CNN-LSTM model, and suffers less performance decay into the future. The framework is then integrated into a first-of-its-kind Storage Assignment system, which is being deployed in warehouses across United States, with initial results showing up to 21% in labor savings. We also release the first publicly available set of warehousing records to facilitate research into this central problem",
    "checked": true,
    "id": "2c4f864bc0c94fd833de1e72df1e0a9faf6ac4d8",
    "semantic_title": "duration-of-stay storage assignment under uncertainty",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=ByxxgCEYDS": {
    "title": "Inductive Matrix Completion Based on Graph Neural Networks",
    "volume": "spotlight",
    "abstract": "We propose an inductive matrix completion model without using side information. By factorizing the (rating) matrix into the product of low-dimensional latent embeddings of rows (users) and columns (items), a majority of existing matrix completion methods are transductive, since the learned embeddings cannot generalize to unseen rows/columns or to new matrices. To make matrix completion inductive, most previous works use content (side information), such as user's age or movie's genre, to make predictions. However, high-quality content is not always available, and can be hard to extract. Under the extreme setting where not any side information is available other than the matrix to complete, can we still learn an inductive matrix completion model? In this paper, we propose an Inductive Graph-based Matrix Completion (IGMC) model to address this problem. IGMC trains a graph neural network (GNN) based purely on 1-hop subgraphs around (user, item) pairs generated from the rating matrix and maps these subgraphs to their corresponding ratings. It achieves highly competitive performance with state-of-the-art transductive baselines. In addition, IGMC is inductive -- it can generalize to users/items unseen during the training (given that their interactions exist), and can even transfer to new tasks. Our transfer learning experiments show that a model trained out of the MovieLens dataset can be directly used to predict Douban movie ratings with surprisingly good performance. Our work demonstrates that: 1) it is possible to train inductive matrix completion models without using side information while achieving similar or better performances than state-of-the-art transductive methods; 2) local graph patterns around a (user, item) pair are effective predictors of the rating this user gives to the item; and 3) Long-range dependencies might not be necessary for modeling recommender systems",
    "checked": true,
    "id": "19605cad33f79d3070b7c4b24aa49653ab7c90da",
    "semantic_title": "inductive matrix completion based on graph neural networks",
    "citation_count": 238,
    "authors": []
  },
  "https://openreview.net/forum?id=Hkekl0NFPr": {
    "title": "Conditional Learning of Fair Representations",
    "volume": "spotlight",
    "abstract": "We propose a novel algorithm for learning fair representations that can simultaneously mitigate two notions of disparity among different demographic subgroups in the classification setting. Two key components underpinning the design of our algorithm are balanced error rate and conditional alignment of representations. We show how these two components contribute to ensuring accuracy parity and equalized false-positive and false-negative rates across groups without impacting demographic parity. Furthermore, we also demonstrate both in theory and on two real-world experiments that the proposed algorithm leads to a better utility-fairness trade-off on balanced datasets compared with existing algorithms on learning fair representations for classification",
    "checked": true,
    "id": "6383cd6c799aff01c9c7fb1c38199febbd0bd591",
    "semantic_title": "conditional learning of fair representations",
    "citation_count": 109,
    "authors": []
  },
  "https://openreview.net/forum?id=Skep6TVYDB": {
    "title": "Gradientless Descent: High-Dimensional Zeroth-Order Optimization",
    "volume": "spotlight",
    "abstract": "Zeroth-order optimization is the process of minimizing an objective $f(x)$, given oracle access to evaluations at adaptively chosen inputs $x$. In this paper, we present two simple yet powerful GradientLess Descent (GLD) algorithms that do not rely on an underlying gradient estimate and are numerically stable. We analyze our algorithm from a novel geometric perspective and we show that for {\\it any monotone transform} of a smooth and strongly convex objective with latent dimension $k \\ge n$, we present a novel analysis that shows convergence within an $\\epsilon$-ball of the optimum in $O(kQ\\log(n)\\log(R/\\epsilon))$ evaluations, where the input dimension is $n$, $R$ is the diameter of the input space and $Q$ is the condition number. Our rates are the first of its kind to be both 1) poly-logarithmically dependent on dimensionality and 2) invariant under monotone transformations. We further leverage our geometric perspective to show that our analysis is optimal. Both monotone invariance and its ability to utilize a low latent dimensionality are key to the empirical success of our algorithms, as demonstrated on synthetic and MuJoCo benchmarks",
    "checked": true,
    "id": "93b982385d61793e4859728e58d04f0ca6b9f48f",
    "semantic_title": "gradientless descent: high-dimensional zeroth-order optimization",
    "citation_count": 76,
    "authors": []
  },
  "https://openreview.net/forum?id=BJg866NFvB": {
    "title": "Estimating counterfactual treatment outcomes over time through adversarially balanced representations",
    "volume": "spotlight",
    "abstract": "Identifying when to give treatments to patients and how to select among multiple treatments over time are important medical problems with a few existing solutions. In this paper, we introduce the Counterfactual Recurrent Network (CRN), a novel sequence-to-sequence model that leverages the increasingly available patient observational data to estimate treatment effects over time and answer such medical questions. To handle the bias from time-varying confounders, covariates affecting the treatment assignment policy in the observational data, CRN uses domain adversarial training to build balancing representations of the patient history. At each timestep, CRN constructs a treatment invariant representation which removes the association between patient history and treatment assignments and thus can be reliably used for making counterfactual predictions. On a simulated model of tumour growth, with varying degree of time-dependent confounding, we show how our model achieves lower error in estimating counterfactuals and in choosing the correct treatment and timing of treatment than current state-of-the-art methods",
    "checked": true,
    "id": "f0cdd98cfdd9fa4b7fa4dd67b561cbac6654d0cb",
    "semantic_title": "estimating counterfactual treatment outcomes over time through adversarially balanced representations",
    "citation_count": 186,
    "authors": []
  },
  "https://openreview.net/forum?id=SkeyppEFvS": {
    "title": "CoPhy: Counterfactual Learning of Physical Dynamics",
    "volume": "spotlight",
    "abstract": "Understanding causes and effects in mechanical systems is an essential component of reasoning in the physical world. This work poses a new problem of counterfactual learning of object mechanics from visual input. We develop the CoPhy benchmark to assess the capacity of the state-of-the-art models for causal physical reasoning in a synthetic 3D environment and propose a model for learning the physical dynamics in a counterfactual setting. Having observed a mechanical experiment that involves, for example, a falling tower of blocks, a set of bouncing balls or colliding objects, we learn to predict how its outcome is affected by an arbitrary intervention on its initial conditions, such as displacing one of the objects in the scene. The alternative future is predicted given the altered past and a latent representation of the confounders learned by the model in an end-to-end fashion with no supervision. We compare against feedforward video prediction baselines and show how observing alternative experiences allows the network to capture latent physical properties of the environment, which results in significantly more accurate predictions at the level of super human performance",
    "checked": true,
    "id": "17b851cfc672b65ccf16a7bdbe85d5db9527483b",
    "semantic_title": "cophy: counterfactual learning of physical dynamics",
    "citation_count": 97,
    "authors": []
  },
  "https://openreview.net/forum?id=HJenn6VFvB": {
    "title": "Hamiltonian Generative Networks",
    "volume": "spotlight",
    "abstract": "The Hamiltonian formalism plays a central role in classical and quantum physics. Hamiltonians are the main tool for modelling the continuous time evolution of systems with conserved quantities, and they come equipped with many useful properties, like time reversibility and smooth interpolation in time. These properties are important for many machine learning problems - from sequence prediction to reinforcement learning and density modelling - but are not typically provided out of the box by standard tools such as recurrent neural networks. In this paper, we introduce the Hamiltonian Generative Network (HGN), the first approach capable of consistently learning Hamiltonian dynamics from high-dimensional observations (such as images) without restrictive domain assumptions. Once trained, we can use HGN to sample new trajectories, perform rollouts both forward and backward in time, and even speed up or slow down the learned dynamics. We demonstrate how a simple modification of the network architecture turns HGN into a powerful normalising flow model, called Neural Hamiltonian Flow (NHF), that uses Hamiltonian dynamics to model expressive densities. Hence, we hope that our work serves as a first practical demonstration of the value that the Hamiltonian formalism can bring to machine learning. More results and video evaluations are available at: http://tiny.cc/hgn",
    "checked": true,
    "id": "80beec251b5d9f4f78fca2ea2016cf9d763b844c",
    "semantic_title": "hamiltonian generative networks",
    "citation_count": 218,
    "authors": []
  },
  "https://openreview.net/forum?id=rJeB36NKvB": {
    "title": "How much Position Information Do Convolutional Neural Networks Encode?",
    "volume": "spotlight",
    "abstract": "In contrast to fully connected networks, Convolutional Neural Networks (CNNs) achieve efficiency by learning weights associated with local filters with a finite spatial extent. An implication of this is that a filter may know what it is looking at, but not where it is positioned in the image. Information concerning absolute position is inherently useful, and it is reasonable to assume that deep CNNs may implicitly learn to encode this information if there is a means to do so. In this paper, we test this hypothesis revealing the surprising degree of absolute position information that is encoded in commonly used neural networks. A comprehensive set of experiments show the validity of this hypothesis and shed light on how and where this information is represented while offering clues to where positional information is derived from in deep CNNs",
    "checked": true,
    "id": "d14e56568dc5f57ccdae899d84f91e34ad847670",
    "semantic_title": "how much position information do convolutional neural networks encode?",
    "citation_count": 349,
    "authors": []
  },
  "https://openreview.net/forum?id=BJge3TNKwH": {
    "title": "Sliced Cramer Synaptic Consolidation for Preserving Deeply Learned Representations",
    "volume": "spotlight",
    "abstract": "Deep neural networks suffer from the inability to preserve the learned data representation (i.e., catastrophic forgetting) in domains where the input data distribution is non-stationary, and it changes during training. Various selective synaptic plasticity approaches have been recently proposed to preserve network parameters, which are crucial for previously learned tasks while learning new tasks. We explore such selective synaptic plasticity approaches through a unifying lens of memory replay and show the close relationship between methods like Elastic Weight Consolidation (EWC) and Memory-Aware-Synapses (MAS). We then propose a fundamentally different class of preservation methods that aim at preserving the distribution of internal neural representations for previous tasks while learning a new one. We propose the sliced Cram\\'{e}r distance as a suitable choice for such preservation and evaluate our Sliced Cramer Preservation (SCP) algorithm through extensive empirical investigations on various network architectures in both supervised and unsupervised learning settings. We show that SCP consistently utilizes the learning capacity of the network better than online-EWC and MAS methods on various incremental learning tasks",
    "checked": true,
    "id": "aabc3c0ef257a1049aebdfad6d1efbbb98995497",
    "semantic_title": "sliced cramer synaptic consolidation for preserving deeply learned representations",
    "citation_count": 37,
    "authors": []
  },
  "https://openreview.net/forum?id=SJeqs6EFvB": {
    "title": "HOPPITY: LEARNING GRAPH TRANSFORMATIONS TO DETECT AND FIX BUGS IN PROGRAMS",
    "volume": "spotlight",
    "abstract": "We present a learning-based approach to detect and fix a broad range of bugs in Javascript programs. We frame the problem in terms of learning a sequence of graph transformations: given a buggy program modeled by a graph structure, our model makes a sequence of predictions including the position of bug nodes and corresponding graph edits to produce a fix. Unlike previous works that use deep neural networks, our approach targets bugs that are more complex and semantic in nature (i.e.~bugs that require adding or deleting statements to fix). We have realized our approach in a tool called HOPPITY. By training on 290,715 Javascript code change commits on Github, HOPPITY correctly detects and fixes bugs in 9,490 out of 36,361 programs in an end-to-end fashion. Given the bug location and type of the fix, HOPPITY also outperforms the baseline approach by a wide margin",
    "checked": true,
    "id": "6acf0d7b24a93953269a4d52c6b2f0c7d11f9ae4",
    "semantic_title": "hoppity: learning graph transformations to detect and fix bugs in programs",
    "citation_count": 196,
    "authors": []
  },
  "https://openreview.net/forum?id=BJgy96EYvr": {
    "title": "Influence-Based Multi-Agent Exploration",
    "volume": "spotlight",
    "abstract": "Intrinsically motivated reinforcement learning aims to address the exploration challenge for sparse-reward tasks. However, the study of exploration methods in transition-dependent multi-agent settings is largely absent from the literature. We aim to take a step towards solving this problem. We present two exploration methods: exploration via information-theoretic influence (EITI) and exploration via decision-theoretic influence (EDTI), by exploiting the role of interaction in coordinated behaviors of agents. EITI uses mutual information to capture the interdependence between the transition dynamics of agents. EDTI uses a novel intrinsic reward, called Value of Interaction (VoI), to characterize and quantify the influence of one agent's behavior on expected returns of other agents. By optimizing EITI or EDTI objective as a regularizer, agents are encouraged to coordinate their exploration and learn policies to optimize the team performance. We show how to optimize these regularizers so that they can be easily integrated with policy gradient reinforcement learning. The resulting update rule draws a connection between coordinated exploration and intrinsic reward distribution. Finally, we empirically demonstrate the significant strength of our methods in a variety of multi-agent scenarios",
    "checked": true,
    "id": "46106fcd08223540d080f674b26770c1fa8a52ff",
    "semantic_title": "influence-based multi-agent exploration",
    "citation_count": 139,
    "authors": []
  },
  "https://openreview.net/forum?id=BklEFpEYwS": {
    "title": "Meta-Learning without Memorization",
    "volume": "spotlight",
    "abstract": "The ability to learn new concepts with small amounts of data is a critical aspect of intelligence that has proven challenging for deep learning methods. Meta-learning has emerged as a promising technique for leveraging data from previous tasks to enable efficient learning of new tasks. However, most meta-learning algorithms implicitly require that the meta-training tasks be mutually-exclusive, such that no single model can solve all of the tasks at once. For example, when creating tasks for few-shot image classification, prior work uses a per-task random assignment of image classes to N-way classification labels. If this is not done, the meta-learner can ignore the task training data and learn a single model that performs all of the meta-training tasks zero-shot, but does not adapt effectively to new image classes. This requirement means that the user must take great care in designing the tasks, for example by shuffling labels or removing task identifying information from the inputs. In some domains, this makes meta-learning entirely inapplicable. In this paper, we address this challenge by designing a meta-regularization objective using information theory that places precedence on data-driven adaptation. This causes the meta-learner to decide what must be learned from the task training data and what should be inferred from the task testing input. By doing so, our algorithm can successfully use data from non-mutually-exclusive tasks to efficiently adapt to novel tasks. We demonstrate its applicability to both contextual and gradient-based meta-learning algorithms, and apply it in practical settings where applying standard meta-learning has been difficult. Our approach substantially outperforms standard meta-learning algorithms in these settings",
    "checked": true,
    "id": "4bf9f88d438c7d978fb854eba686cf3933879df1",
    "semantic_title": "meta-learning without memorization",
    "citation_count": 188,
    "authors": []
  },
  "https://openreview.net/forum?id=SJgndT4KwB": {
    "title": "Finite Depth and Width Corrections to the Neural Tangent Kernel",
    "volume": "spotlight",
    "abstract": "We prove the precise scaling, at finite depth and width, for the mean and variance of the neural tangent kernel (NTK) in a randomly initialized ReLU network. The standard deviation is exponential in the ratio of network depth to width. Thus, even in the limit of infinite overparameterization, the NTK is not deterministic if depth and width simultaneously tend to infinity. Moreover, we prove that for such deep and wide networks, the NTK has a non-trivial evolution during training by showing that the mean of its first SGD update is also exponential in the ratio of network depth to width. This is sharp contrast to the regime where depth is fixed and network width is very large. Our results suggest that, unlike relatively shallow and wide networks, deep and wide ReLU networks are capable of learning data-dependent features even in the so-called lazy training regime",
    "checked": true,
    "id": "2d8591778f3af1bc98bee3fe9ac932282780abd9",
    "semantic_title": "finite depth and width corrections to the neural tangent kernel",
    "citation_count": 152,
    "authors": []
  },
  "https://openreview.net/forum?id=HklRwaEKwB": {
    "title": "Ridge Regression: Structure, Cross-Validation, and Sketching",
    "volume": "spotlight",
    "abstract": "We study the following three fundamental problems about ridge regression: (1) what is the structure of the estimator? (2) how to correctly use cross-validation to choose the regularization parameter? and (3) how to accelerate computation without losing too much accuracy? We consider the three problems in a unified large-data linear model. We give a precise representation of ridge regression as a covariance matrix-dependent linear combination of the true parameter and the noise. We study the bias of $K$-fold cross-validation for choosing the regularization parameter, and propose a simple bias-correction. We analyze the accuracy of primal and dual sketching for ridge regression, showing they are surprisingly accurate. Our results are illustrated by simulations and by analyzing empirical data",
    "checked": true,
    "id": "1cb99a1c7e7d44ec0efbb1953e0622fb59a1a085",
    "semantic_title": "ridge regression: structure, cross-validation, and sketching",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=SJl5Np4tPr": {
    "title": "Cross-Domain Few-Shot Classification via Learned Feature-Wise Transformation",
    "volume": "spotlight",
    "abstract": "Few-shot classification aims to recognize novel categories with only few labeled images in each class. Existing metric-based few-shot classification algorithms predict categories by comparing the feature embeddings of query images with those from a few labeled images (support examples) using a learned metric function. While promising performance has been demonstrated, these methods often fail to generalize to unseen domains due to large discrepancy of the feature distribution across domains. In this work, we address the problem of few-shot classification under domain shifts for metric-based methods. Our core idea is to use feature-wise transformation layers for augmenting the image features using affine transforms to simulate various feature distributions under different domains in the training stage. To capture variations of the feature distributions under different domains, we further apply a learning-to-learn approach to search for the hyper-parameters of the feature-wise transformation layers. We conduct extensive experiments and ablation studies under the domain generalization setting using five few-shot classification datasets: mini-ImageNet, CUB, Cars, Places, and Plantae. Experimental results demonstrate that the proposed feature-wise transformation layer is applicable to various metric-based models, and provides consistent improvements on the few-shot classification performance under domain shift",
    "checked": true,
    "id": "17b99c60d6b2fdd656af6a7661b8d6af05255792",
    "semantic_title": "cross-domain few-shot classification via learned feature-wise transformation",
    "citation_count": 394,
    "authors": []
  },
  "https://openreview.net/forum?id=B1x1ma4tDr": {
    "title": "DDSP: Differentiable Digital Signal Processing",
    "volume": "spotlight",
    "abstract": "Most generative models of audio directly generate samples in one of two domains: time or frequency. While sufficient to express any signal, these representations are inefficient, as they do not utilize existing knowledge of how sound is generated and perceived. A third approach (vocoders/synthesizers) successfully incorporates strong domain knowledge of signal processing and perception, but has been less actively researched due to limited expressivity and difficulty integrating with modern auto-differentiation-based machine learning methods. In this paper, we introduce the Differentiable Digital Signal Processing (DDSP) library, which enables direct integration of classic signal processing elements with deep learning methods. Focusing on audio synthesis, we achieve high-fidelity generation without the need for large autoregressive models or adversarial losses, demonstrating that DDSP enables utilizing strong inductive biases without losing the expressive power of neural networks. Further, we show that combining interpretable modules permits manipulation of each separate model component, with applications such as independent control of pitch and loudness, realistic extrapolation to pitches not seen during training, blind dereverberation of room acoustics, transfer of extracted room acoustics to new environments, and transformation of timbre between disparate sources. In short, DDSP enables an interpretable and modular approach to generative modeling, without sacrificing the benefits of deep learning. The library will is available at https://github.com/magenta/ddsp and we encourage further contributions from the community and domain experts",
    "checked": true,
    "id": "468aa95cfdf66da9fc3dc6a1b9042a52a6ec99c6",
    "semantic_title": "ddsp: differentiable digital signal processing",
    "citation_count": 381,
    "authors": []
  },
  "https://openreview.net/forum?id=Hke-WTVtwr": {
    "title": "Encoding word order in complex embeddings",
    "volume": "spotlight",
    "abstract": "Sequential word order is important when processing text. Currently, neural networks (NNs) address this by modeling word position using position embeddings. The problem is that position embeddings capture the position of individual words, but not the ordered relationship (e.g., adjacency or precedence) between individual word positions. We present a novel and principled solution for modeling both the global absolute positions of words and their order relationships. Our solution generalizes word embeddings, previously defined as independent vectors, to continuous word functions over a variable (position). The benefit of continuous functions over variable positions is that word representations shift smoothly with increasing positions. Hence, word representations in different positions can correlate with each other in a continuous function. The general solution of these functions can be extended to complex-valued variants. We extend CNN, RNN and Transformer NNs to complex-valued versions to incorporate our complex embedding (we make all code available). Experiments on text classification, machine translation and language modeling show gains over both classical word embeddings and position-enriched word embeddings. To our knowledge, this is the first work in NLP to link imaginary numbers in complex-valued representations to concrete meanings (i.e., word order)",
    "checked": true,
    "id": "d0e28f5dc1feae19e41087a92a87992977fd85af",
    "semantic_title": "encoding word order in complex embeddings",
    "citation_count": 114,
    "authors": []
  },
  "https://openreview.net/forum?id=Skgvy64tvr": {
    "title": "Enhancing Adversarial Defense by k-Winners-Take-All",
    "volume": "spotlight",
    "abstract": "We propose a simple change to existing neural network structures for better defending against gradient-based adversarial attacks. Instead of using popular activation functions (such as ReLU), we advocate the use of k-Winners-Take-All (k-WTA) activation, a C0 discontinuous function that purposely invalidates the neural network model's gradient at densely distributed input data points. The proposed k-WTA activation can be readily used in nearly all existing networks and training methods with no significant overhead. Our proposal is theoretically rationalized. We analyze why the discontinuities in k-WTA networks can largely prevent gradient-based search of adversarial examples and why they at the same time remain innocuous to the network training. This understanding is also empirically backed. We test k-WTA activation on various network structures optimized by a training method, be it adversarial training or not. In all cases, the robustness of k-WTA networks outperforms that of traditional networks under white-box attacks",
    "checked": true,
    "id": "649de559f530aab8f22f6022d40cdfd3bb4e1039",
    "semantic_title": "enhancing adversarial defense by k-winners-take-all",
    "citation_count": 99,
    "authors": []
  },
  "https://openreview.net/forum?id=rkxZyaNtwB": {
    "title": "Online and stochastic optimization beyond Lipschitz continuity: A Riemannian approach",
    "volume": "spotlight",
    "abstract": "Motivated by applications to machine learning and imaging science, we study a class of online and stochastic optimization problems with loss functions that are not Lipschitz continuous; in particular, the loss functions encountered by the optimizer could exhibit gradient singularities or be singular themselves. Drawing on tools and techniques from Riemannian geometry, we examine a Riemann–Lipschitz (RL) continuity condition which is tailored to the singularity landscape of the problem's loss functions. In this way, we are able to tackle cases beyond the Lipschitz framework provided by a global norm, and we derive optimal regret bounds and last iterate convergence results through the use of regularized learning methods (such as online mirror descent). These results are subsequently validated in a class of stochastic Poisson inverse problems that arise in imaging science",
    "checked": true,
    "id": "3df942024fe6b3ace56edfc48f79532e5c03b27f",
    "semantic_title": "online and stochastic optimization beyond lipschitz continuity: a riemannian approach",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=BJlS634tPr": {
    "title": "PC-DARTS: Partial Channel Connections for Memory-Efficient Architecture Search",
    "volume": "spotlight",
    "abstract": "Differentiable architecture search (DARTS) provided a fast solution in finding effective network architectures, but suffered from large memory and computing overheads in jointly training a super-net and searching for an optimal architecture. In this paper, we present a novel approach, namely Partially-Connected DARTS, by sampling a small part of super-net to reduce the redundancy in exploring the network space, thereby performing a more efficient search without comprising the performance. In particular, we perform operation search in a subset of channels while bypassing the held out part in a shortcut. This strategy may suffer from an undesired inconsistency on selecting the edges of super-net caused by sampling different channels. We solve it by introducing edge normalization, which adds a new set of edge-level hyper-parameters to reduce uncertainty in search. Thanks to the reduced memory cost, PC-DARTS can be trained with a larger batch size and, consequently, enjoy both faster speed and higher training stability. Experiment results demonstrate the effectiveness of the proposed method. Specifically, we achieve an error rate of 2.57% on CIFAR10 within merely 0.1 GPU-days for architecture search, and a state-of-the-art top-1 error rate of 24.2% on ImageNet (under the mobile setting) within 3.8 GPU-days for search. Our code has been made available at https://www.dropbox.com/sh/on9lg3rpx1r6dkf/AABG5mt0sMHjnEJyoRnLEYW4a?dl=0",
    "checked": true,
    "id": "4ebce2425e231031f89a4a68dc52a151cd735d03",
    "semantic_title": "pc-darts: partial channel connections for memory-efficient architecture search",
    "citation_count": 612,
    "authors": []
  },
  "https://openreview.net/forum?id=BJeKh3VYDH": {
    "title": "Tranquil Clouds: Neural Networks for Learning Temporally Coherent Features in Point Clouds",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "e8c29e13782b23b9ef364e60cbf2b2e053f1b979",
    "semantic_title": "tranquil clouds: neural networks for learning temporally coherent features in point clouds",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=Byl8hhNYPS": {
    "title": "Neural Machine Translation with Universal Visual Representation",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "834e987e107e251441970a6e5058446f1dd8a005",
    "semantic_title": "neural machine translation with universal visual representation",
    "citation_count": 102,
    "authors": []
  },
  "https://openreview.net/forum?id=H1ebhnEYDH": {
    "title": "White Noise Analysis of Neural Networks",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "6899ed2990d255075ce21c14eb905046713f7f58",
    "semantic_title": "white noise analysis of neural networks",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=BJlRs34Fvr": {
    "title": "Skip Connections Matter: On the Transferability of Adversarial Examples Generated with ResNets",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "2fb43a4c5cab8215d510fc585ca81fb5ee8a3abb",
    "semantic_title": "skip connections matter: on the transferability of adversarial examples generated with resnets",
    "citation_count": 314,
    "authors": []
  },
  "https://openreview.net/forum?id=HJeTo2VFwH": {
    "title": "A Signal Propagation Perspective for Pruning Neural Networks at Initialization",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "e7907e7e7d470a12bdab5e6381ad12c4f832ea49",
    "semantic_title": "a signal propagation perspective for pruning neural networks at initialization",
    "citation_count": 155,
    "authors": []
  },
  "https://openreview.net/forum?id=HygOjhEYDH": {
    "title": "Intensity-Free Learning of Temporal Point Processes",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "55101536cfd5142b7e29a355d7c04f212f31f6d2",
    "semantic_title": "intensity-free learning of temporal point processes",
    "citation_count": 172,
    "authors": []
  },
  "https://openreview.net/forum?id=HyeSin4FPB": {
    "title": "Learning to Control PDEs with Differentiable Physics",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "6a83d327627aaa1abafb9b858110dd4876bfcb34",
    "semantic_title": "learning to control pdes with differentiable physics",
    "citation_count": 189,
    "authors": []
  },
  "https://openreview.net/forum?id=rklEj2EFvB": {
    "title": "Estimating Gradients for Discrete Random Variables by Sampling without Replacement",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "f068c285e7ffe09886b944cc0144c88afe65825b",
    "semantic_title": "estimating gradients for discrete random variables by sampling without replacement",
    "citation_count": 50,
    "authors": []
  },
  "https://openreview.net/forum?id=H1xscnEKDr": {
    "title": "Defending Against Physically Realizable Attacks on Image Classification",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "83473cc2eb5acc2c41b93ef5b3b1d925a12f25a3",
    "semantic_title": "defending against physically realizable attacks on image classification",
    "citation_count": 126,
    "authors": []
  },
  "https://openreview.net/forum?id=B1e9Y2NYvS": {
    "title": "On Robustness of Neural Ordinary Differential Equations",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "00d4f1c5f11ecbbc1a033e6675934703f09016f8",
    "semantic_title": "on robustness of neural ordinary differential equations",
    "citation_count": 141,
    "authors": []
  },
  "https://openreview.net/forum?id=r1lfF2NYvH": {
    "title": "InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "2fb59ebe271d6b007bb0429c1701fd1004782d1b",
    "semantic_title": "infograph: unsupervised and semi-supervised graph-level representation learning via mutual information maximization",
    "citation_count": 866,
    "authors": []
  },
  "https://openreview.net/forum?id=rJljdh4KDH": {
    "title": "Multi-Scale Representation Learning for Spatial Feature Distributions using Grid Cells",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "e96016d5e3c6492230aa172efcf733596dc64b6e",
    "semantic_title": "multi-scale representation learning for spatial feature distributions using grid cells",
    "citation_count": 124,
    "authors": []
  },
  "https://openreview.net/forum?id=S1ldO2EFPr": {
    "title": "Graph Neural Networks Exponentially Lose Expressive Power for Node Classification",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "4ce9c20642dce5eb7930966053a1e3da4ef617f2",
    "semantic_title": "graph neural networks exponentially lose expressive power for node classification",
    "citation_count": 695,
    "authors": []
  },
  "https://openreview.net/forum?id=BygPO2VKPH": {
    "title": "Sparse Coding with Gated Learned ISTA",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "394e5d3490b319deefe5a8e717f2b272c1bd3d46",
    "semantic_title": "sparse coding with gated learned ista",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=BkxUvnEYDH": {
    "title": "Program Guided Agent",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "6f6d6da7b4c6219c55d0da09fd2b1f9809535d6d",
    "semantic_title": "program guided agent",
    "citation_count": 58,
    "authors": []
  },
  "https://openreview.net/forum?id=ryxyCeHtPB": {
    "title": "Pay Attention to Features, Transfer Learn Faster CNNs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bff99fde2382c904c1f5a45a39e52a675ccde5de",
    "semantic_title": "pay attention to features, transfer learn faster cnns",
    "citation_count": 72,
    "authors": []
  },
  "https://openreview.net/forum?id=BkeoaeHKDS": {
    "title": "Gradients as Features for Deep Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a48f470434d172f52f6979858e940cac6c979e13",
    "semantic_title": "gradients as features for deep representation learning",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=Hyg96gBKPS": {
    "title": "Monotonic Multihead Attention",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ee3c825e6bd1cd5bc3a8bb62a3dbab13ff1c5b73",
    "semantic_title": "monotonic multihead attention",
    "citation_count": 140,
    "authors": []
  },
  "https://openreview.net/forum?id=HyeYTgrFPB": {
    "title": "Massively Multilingual Sparse Word Representations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d875d54f523a804f67cb093d7aef8579dc403f58",
    "semantic_title": "massively multilingual sparse word representations",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=Skxd6gSYDS": {
    "title": "Query-efficient Meta Attack to Deep Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "11154f0b587691dfee5bd1484f0be539a853e6f1",
    "semantic_title": "query-efficient meta attack to deep neural networks",
    "citation_count": 84,
    "authors": []
  },
  "https://openreview.net/forum?id=HJxdTxHYvB": {
    "title": "BREAKING CERTIFIED DEFENSES: SEMANTIC ADVERSARIAL EXAMPLES WITH SPOOFED ROBUSTNESS CERTIFICATES",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6e635f131c37d481f5b8778c5823a35201876150",
    "semantic_title": "breaking certified defenses: semantic adversarial examples with spoofed robustness certificates",
    "citation_count": 55,
    "authors": []
  },
  "https://openreview.net/forum?id=B1xSperKvH": {
    "title": "Enabling Deep Spiking Neural Networks with Hybrid Conversion and Spike Timing Dependent Backpropagation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6dca2ee4cda6c75ba18e5efbcf8b1930c7c04ed8",
    "semantic_title": "enabling deep spiking neural networks with hybrid conversion and spike timing dependent backpropagation",
    "citation_count": 304,
    "authors": []
  },
  "https://openreview.net/forum?id=S1erpeBFPB": {
    "title": "How to 0wn the NAS in Your Spare Time",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6f277bd6712e6cafa4bd1e0da098d306864ae0fe",
    "semantic_title": "how to 0wn the nas in your spare time",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=HyebplHYwB": {
    "title": "The Shape of Data: Intrinsic Distance for Data Distributions",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "13219f5ed02b37fee8052133e058117610c073aa",
    "semantic_title": "the shape of data: intrinsic distance for data distributions",
    "citation_count": 54,
    "authors": []
  },
  "https://openreview.net/forum?id=rkgg6xBYDH": {
    "title": "Understanding Generalization in Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7493604c39962bd488480a488e913ac1c49d65f0",
    "semantic_title": "understanding generalization in recurrent neural networks",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=BJlahxHYDS": {
    "title": "Conservative Uncertainty Estimation By Fitting Prior Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5f81689daeda82f0b1e08da37573032b657f0c47",
    "semantic_title": "conservative uncertainty estimation by fitting prior networks",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=SJx9ngStPH": {
    "title": "NAS-Bench-1Shot1: Benchmarking and Dissecting One-shot Neural Architecture Search",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "40744235481391b7d6baf8f9d590dfa32da1f4d4",
    "semantic_title": "nas-bench-1shot1: benchmarking and dissecting one-shot neural architecture search",
    "citation_count": 149,
    "authors": []
  },
  "https://openreview.net/forum?id=ryxB2lBtvH": {
    "title": "Learning to Coordinate Manipulation Skills via Skill Behavior Diversification",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "121cca1bb7cec48fa9080801927f50d99193eae6",
    "semantic_title": "learning to coordinate manipulation skills via skill behavior diversification",
    "citation_count": 82,
    "authors": []
  },
  "https://openreview.net/forum?id=rylb3eBtwr": {
    "title": "Robust Subspace Recovery Layer for Unsupervised Anomaly Detection",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3a2ef5c27e7140f819a4c99444b7d4fd533dca59",
    "semantic_title": "robust subspace recovery layer for unsupervised anomaly detection",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=HJx-3grYDB": {
    "title": "Learning Nearly Decomposable Value Functions Via Communication Minimization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "66ab093b76aa8842e7cc8c66fa8d84359b8ce88e",
    "semantic_title": "learning nearly decomposable value functions via communication minimization",
    "citation_count": 133,
    "authors": []
  },
  "https://openreview.net/forum?id=rJxe3xSYDS": {
    "title": "Extreme Classification via Adversarial Softmax Approximation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3881717a4db3c82fa0d7d7b407e5a1c29d673c44",
    "semantic_title": "extreme classification via adversarial softmax approximation",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=rkg1ngrFPr": {
    "title": "Information Geometry of Orthogonal Initializations and Training",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "84493a1ab6b415d0ecfdd1239b44cfd2f3883561",
    "semantic_title": "information geometry of orthogonal initializations and training",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=Hyx0slrFvH": {
    "title": "Mixed Precision DNNs: All you need is a good parametrization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f31f7e68d662dfe95e1cd26ad7d61a4bffff5493",
    "semantic_title": "mixed precision dnns: all you need is a good parametrization",
    "citation_count": 141,
    "authors": []
  },
  "https://openreview.net/forum?id=r1g6ogrtDr": {
    "title": "Co-Attentive Equivariant Neural Networks: Focusing Equivariance On Transformations Co-Occurring in Data",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1d01105478b8d9ceac4aaa8e08d4792d063c05bb",
    "semantic_title": "co-attentive equivariant neural networks: focusing equivariance on transformations co-occurring in data",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=ryloogSKDS": {
    "title": "Deep Orientation Uncertainty Learning based on a Bingham Loss",
    "volume": "poster",
    "abstract": "Reasoning about uncertain orientations is one of the core problems in many perception tasks such as object pose estimation or motion estimation. In these scenarios, poor illumination conditions, sensor limitations, or appearance invariance may result in highly uncertain estimates. In this work, we propose a novel learning-based representation for orientation uncertainty. By characterizing uncertainty over unit quaternions with the Bingham distribution, we formulate a loss that naturally captures the antipodal symmetry of the representation. We discuss the interpretability of the learned distribution parameters and demonstrate the feasibility of our approach on several challenging real-world pose estimation tasks involving uncertain orientations",
    "checked": true,
    "id": "7bbce51c8e25e52f8f19ed9d1420560368fa35f8",
    "semantic_title": "deep orientation uncertainty learning based on a bingham loss",
    "citation_count": 62,
    "authors": []
  },
  "https://openreview.net/forum?id=rylmoxrFDH": {
    "title": "Critical initialisation in continuous approximations of binary neural networks",
    "volume": "poster",
    "abstract": "The training of stochastic neural network models with binary ($\\pm1$) weights and activations via continuous surrogate networks is investigated. We derive new surrogates using a novel derivation based on writing the stochastic neural network as a Markov chain. This derivation also encompasses existing variants of the surrogates presented in the literature. Following this, we theoretically study the surrogates at initialisation. We derive, using mean field theory, a set of scalar equations describing how input signals propagate through the randomly initialised networks. The equations reveal whether so-called critical initialisations exist for each surrogate network, where the network can be trained to arbitrary depth. Moreover, we predict theoretically and confirm numerically, that common weight initialisation schemes used in standard continuous networks, when applied to the mean values of the stochastic binary weights, yield poor training performance. This study shows that, contrary to common intuition, the means of the stochastic binary weights should be initialised close to $\\pm 1$, for deeper networks to be trainable",
    "checked": true,
    "id": "c1195f168150902238ac6ced054c7ebaf676e2e9",
    "semantic_title": "critical initialisation in continuous approximations of binary neural networks",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=ByeWogStDS": {
    "title": "Sub-policy Adaptation for Hierarchical Reinforcement Learning",
    "volume": "poster",
    "abstract": "Hierarchical reinforcement learning is a promising approach to tackle long-horizon decision-making problems with sparse rewards. Unfortunately, most methods still decouple the lower-level skill acquisition process and the training of a higher level that controls the skills in a new task. Leaving the skills fixed can lead to significant sub-optimality in the transfer setting. In this work, we propose a novel algorithm to discover a set of skills, and continuously adapt them along with the higher level even when training on a new task. Our main contributions are two-fold. First, we derive a new hierarchical policy gradient with an unbiased latent-dependent baseline, and we introduce Hierarchical Proximal Policy Optimization (HiPPO), an on-policy method to efficiently train all levels of the hierarchy jointly. Second, we propose a method of training time-abstractions that improves the robustness of the obtained skills to environment changes. Code and videos are available at sites.google.com/view/hippo-rl",
    "checked": true,
    "id": "2fed116dea9c36914b52b55e0f9688ccf641ee07",
    "semantic_title": "sub-policy adaptation for hierarchical reinforcement learning",
    "citation_count": 74,
    "authors": []
  },
  "https://openreview.net/forum?id=HkxjqxBYDB": {
    "title": "Episodic Reinforcement Learning with Associative Memory",
    "volume": "poster",
    "abstract": "Sample efficiency has been one of the major challenges for deep reinforcement learning. Non-parametric episodic control has been proposed to speed up parametric reinforcement learning by rapidly latching on previously successful policies. However, previous work on episodic reinforcement learning neglects the relationship between states and only stored the experiences as unrelated items. To improve sample efficiency of reinforcement learning, we propose a novel framework, called Episodic Reinforcement Learning with Associative Memory (ERLAM), which associates related experience trajectories to enable reasoning effective strategies. We build a graph on top of states in memory based on state transitions and develop a reverse-trajectory propagation strategy to allow rapid value propagation through the graph. We use the non-parametric associative memory as early guidance for a parametric reinforcement learning model. Results on navigation domain and Atari games show our framework achieves significantly higher sample efficiency than state-of-the-art episodic reinforcement learning models",
    "checked": true,
    "id": "8c3d0de141260c3344624aeef397af00c926b8f2",
    "semantic_title": "episodic reinforcement learning with associative memory",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=BJxI5gHKDr": {
    "title": "Pitfalls of In-Domain Uncertainty Estimation and Ensembling in Deep Learning",
    "volume": "poster",
    "abstract": "Uncertainty estimation and ensembling methods go hand-in-hand. Uncertainty estimation is one of the main benchmarks for assessment of ensembling performance. At the same time, deep learning ensembles have provided state-of-the-art results in uncertainty estimation. In this work, we focus on in-domain uncertainty for image classification. We explore the standards for its quantification and point out pitfalls of existing metrics. Avoiding these pitfalls, we perform a broad study of different ensembling techniques. To provide more insight in this study, we introduce the deep ensemble equivalent score (DEE) and show that many sophisticated ensembling techniques are equivalent to an ensemble of only few independently trained networks in terms of test performance",
    "checked": true,
    "id": "d12fd94337ac804470dc78911e74b5b6480eef8e",
    "semantic_title": "pitfalls of in-domain uncertainty estimation and ensembling in deep learning",
    "citation_count": 320,
    "authors": []
  },
  "https://openreview.net/forum?id=B1eB5xSFvr": {
    "title": "DiffTaichi: Differentiable Programming for Physical Simulation",
    "volume": "poster",
    "abstract": "We present DiffTaichi, a new differentiable programming language tailored for building high-performance differentiable physical simulators. Based on an imperative programming language, DiffTaichi generates gradients of simulation steps using source code transformations that preserve arithmetic intensity and parallelism. A light-weight tape is used to record the whole simulation program structure and replay the gradient kernels in a reversed order, for end-to-end backpropagation. We demonstrate the performance and productivity of our language in gradient-based learning and optimization tasks on 10 different physical simulators. For example, a differentiable elastic object simulator written in our language is 4.2x shorter than the hand-engineered CUDA version yet runs as fast, and is 188x faster than the TensorFlow implementation. Using our differentiable programs, neural network controllers are typically optimized within only tens of iterations",
    "checked": true,
    "id": "666aaf80f647faf52d5058ff951e2d4b9a8844f5",
    "semantic_title": "difftaichi: differentiable programming for physical simulation",
    "citation_count": 388,
    "authors": []
  },
  "https://openreview.net/forum?id=rJxycxHKDS": {
    "title": "Domain Adaptive Multibranch Networks",
    "volume": "poster",
    "abstract": "We tackle unsupervised domain adaptation by accounting for the fact that different domains may need to be processed differently to arrive to a common feature representation effective for recognition. To this end, we introduce a deep learning framework where each domain undergoes a different sequence of operations, allowing some, possibly more complex, domains to go through more computations than others. This contrasts with state-of-the-art domain adaptation techniques that force all domains to be processed with the same series of operations, even when using multi-stream architectures whose parameters are not shared. As evidenced by our experiments, the greater flexibility of our method translates to higher accuracy. Furthermore, it allows us to handle any number of domains simultaneously",
    "checked": true,
    "id": "57fb8c7a0fbe2ac66ea4410bda2b64e57e5ef2c8",
    "semantic_title": "domain adaptive multibranch networks",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=SyevYxHtDB": {
    "title": "Prediction Poisoning: Towards Defenses Against DNN Model Stealing Attacks",
    "volume": "poster",
    "abstract": "High-performance Deep Neural Networks (DNNs) are increasingly deployed in many real-world applications e.g., cloud prediction APIs. Recent advances in model functionality stealing attacks via black-box access (i.e., inputs in, predictions out) threaten the business model of such applications, which require a lot of time, money, and effort to develop. Existing defenses take a passive role against stealing attacks, such as by truncating predicted information. We find such passive defenses ineffective against DNN stealing attacks. In this paper, we propose the first defense which actively perturbs predictions targeted at poisoning the training objective of the attacker. We find our defense effective across a wide range of challenging datasets and DNN model stealing attacks, and additionally outperforms existing defenses. Our defense is the first that can withstand highly accurate model stealing attacks for tens of thousands of queries, amplifying the attacker's error rate up to a factor of 85$\\times$ with minimal impact on the utility for benign users",
    "checked": true,
    "id": "da10f79f983fd4fbd589ed7ffa68d33964841443",
    "semantic_title": "prediction poisoning: towards defenses against dnn model stealing attacks",
    "citation_count": 166,
    "authors": []
  },
  "https://openreview.net/forum?id=SJxrKgStDH": {
    "title": "SCALOR: Generative World Models with Scalable Object Representations",
    "volume": "poster",
    "abstract": "Scalability in terms of object density in a scene is a primary challenge in unsupervised sequential object-oriented representation learning. Most of the previous models have been shown to work only on scenes with a few objects. In this paper, we propose SCALOR, a probabilistic generative world model for learning SCALable Object-oriented Representation of a video. With the proposed spatially parallel attention and proposal-rejection mechanisms, SCALOR can deal with orders of magnitude larger numbers of objects compared to the previous state-of-the-art models. Additionally, we introduce a background module that allows SCALOR to model complex dynamic backgrounds as well as many foreground objects in the scene. We demonstrate that SCALOR can deal with crowded scenes containing up to a hundred objects while jointly modeling complex dynamic backgrounds. Importantly, SCALOR is the ﬁrst unsupervised object representation model shown to work for natural scenes containing several tens of moving objects",
    "checked": true,
    "id": "315933cc91d0a395e9cecfe2241aa8d754845b17",
    "semantic_title": "scalor: generative world models with scalable object representations",
    "citation_count": 133,
    "authors": []
  },
  "https://openreview.net/forum?id=HklQYxBKwS": {
    "title": "Neural tangent kernels, transportation mappings, and universal approximation",
    "volume": "poster",
    "abstract": "This paper establishes rates of universal approximation for the shallow neural tangent kernel (NTK): network weights are only allowed microscopic changes from random initialization, which entails that activations are mostly unchanged, and the network is nearly equivalent to its linearization. Concretely, the paper has two main contributions: a generic scheme to approximate functions with the NTK by sampling from transport mappings between the initial weights and their desired values, and the construction of transport mappings via Fourier transforms. Regarding the first contribution, the proof scheme provides another perspective on how the NTK regime arises from rescaling: redundancy in the weights due to resampling allows individual weights to be scaled down. Regarding the second contribution, the most notable transport mapping asserts that roughly $1 / \\delta^{10d}$ nodes are sufficient to approximate continuous functions, where $\\delta$ depends on the continuity properties of the target function. By contrast, nearly the same proof yields a bound of $1 / \\delta^{2d}$ for shallow ReLU networks; this gap suggests a tantalizing direction for future work, separating shallow ReLU networks and their linearization",
    "checked": true,
    "id": "2fb30bc990e08b78f2a2856e799a12eb04dd0595",
    "semantic_title": "neural tangent kernels, transportation mappings, and universal approximation",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=BJgMFxrYPB": {
    "title": "Learning to Move with Affordance Maps",
    "volume": "poster",
    "abstract": "The ability to autonomously explore and navigate a physical space is a fundamental requirement for virtually any mobile autonomous agent, from household robotic vacuums to autonomous vehicles. Traditional SLAM-based approaches for exploration and navigation largely focus on leveraging scene geometry, but fail to model dynamic objects (such as other agents) or semantic constraints (such as wet floors or doorways). Learning-based RL agents are an attractive alternative because they can incorporate both semantic and geometric information, but are notoriously sample inefficient, difficult to generalize to novel settings, and are difficult to interpret. In this paper, we combine the best of both worlds with a modular approach that {\\em learns} a spatial representation of a scene that is trained to be effective when coupled with traditional geometric planners. Specifically, we design an agent that learns to predict a spatial affordance map that elucidates what parts of a scene are navigable through active self-supervised experience gathering. In contrast to most simulation environments that assume a static world, we evaluate our approach in the VizDoom simulator, using large-scale randomly-generated maps containing a variety of dynamic actors and hazards. We show that learned affordance maps can be used to augment traditional approaches for both exploration and navigation, providing significant improvements in performance",
    "checked": true,
    "id": "15e3e5c8c7abab47ea02aca906f0b29358d7d612",
    "semantic_title": "learning to move with affordance maps",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=rJleKgrKwS": {
    "title": "Differentiable learning of numerical rules in knowledge graphs",
    "volume": "poster",
    "abstract": "Rules over a knowledge graph (KG) capture interpretable patterns in data and can be used for KG cleaning and completion. Inspired by the TensorLog differentiable logic framework, which compiles rule inference into a sequence of differentiable operations, recently a method called Neural LP has been proposed for learning the parameters as well as the structure of rules. However, it is limited with respect to the treatment of numerical features like age, weight or scientific measurements. We address this limitation by extending Neural LP to learn rules with numerical values, e.g., \"People younger than 18 typically live with their parents\". We demonstrate how dynamic programming and cumulative sum operations can be exploited to ensure efficiency of such extension. Our novel approach allows us to extract more expressive rules with aggregates, which are of higher quality and yield more accurate predictions compared to rules learned by the state-of-the-art methods, as shown by our experiments on synthetic and real-world datasets",
    "checked": true,
    "id": "4e36251b223b86066aa280d6ea8a277c51775281",
    "semantic_title": "differentiable learning of numerical rules in knowledge graphs",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=S1lxKlSKPH": {
    "title": "Consistency Regularization for Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "Generative Adversarial Networks (GANs) are known to be difficult to train, despite considerable research effort. Several regularization techniques for stabilizing training have been proposed, but they introduce non-trivial computational overheads and interact poorly with existing techniques like spectral normalization. In this work, we propose a simple, effective training stabilizer based on the notion of consistency regularization—a popular technique in the semi-supervised learning literature. In particular, we augment data passing into the GAN discriminator and penalize the sensitivity of the discriminator to these augmentations. We conduct a series of experiments to demonstrate that consistency regularization works effectively with spectral normalization and various GAN architectures, loss functions and optimizer settings. Our method achieves the best FID scores for unconditional image generation compared to other regularization methods on CIFAR-10 and CelebA. Moreover, Our consistency regularized GAN (CR-GAN) improves state of-the-art FID scores for conditional generation from 14.73 to 11.48 on CIFAR-10 and from 8.73 to 6.66 on ImageNet-2012",
    "checked": true,
    "id": "63470afe06145e08c3b851491450f68c83cc938f",
    "semantic_title": "consistency regularization for generative adversarial networks",
    "citation_count": 285,
    "authors": []
  },
  "https://openreview.net/forum?id=SkxxtgHKPS": {
    "title": "On Generalization Error Bounds of Noisy Gradient Methods for Non-Convex Learning",
    "volume": "poster",
    "abstract": "Generalization error (also known as the out-of-sample error) measures how well the hypothesis learned from training data generalizes to previously unseen data. Proving tight generalization error bounds is a central question in statistical learning theory. In this paper, we obtain generalization error bounds for learning general non-convex objectives, which has attracted significant attention in recent years. We develop a new framework, termed Bayes-Stability, for proving algorithm-dependent generalization error bounds. The new framework combines ideas from both the PAC-Bayesian theory and the notion of algorithmic stability. Applying the Bayes-Stability method, we obtain new data-dependent generalization bounds for stochastic gradient Langevin dynamics (SGLD) and several other noisy gradient methods (e.g., with momentum, mini-batch and acceleration, Entropy-SGD). Our result recovers (and is typically tighter than) a recent result in Mou et al. (2018) and improves upon the results in Pensia et al. (2018). Our experiments demonstrate that our data-dependent bounds can distinguish randomly labelled data from normal data, which provides an explanation to the intriguing phenomena observed in Zhang et al. (2017a). We also study the setting where the total loss is the sum of a bounded loss and an additiona l`2 regularization term. We obtain new generalization bounds for the continuous Langevin dynamic in this setting by developing a new Log-Sobolev inequality for the parameter distribution at any time. Our new bounds are more desirable when the noise level of the processis not very small, and do not become vacuous even when T tends to infinity",
    "checked": true,
    "id": "014e1751da7cec01119a9ea579e3f7c20c1dbad2",
    "semantic_title": "on generalization error bounds of noisy gradient methods for non-convex learning",
    "citation_count": 89,
    "authors": []
  },
  "https://openreview.net/forum?id=HJgpugrKPS": {
    "title": "Scale-Equivariant Steerable Networks",
    "volume": "poster",
    "abstract": "The effectiveness of Convolutional Neural Networks (CNNs) has been substantially attributed to their built-in property of translation equivariance. However, CNNs do not have embedded mechanisms to handle other types of transformations. In this work, we pay attention to scale changes, which regularly appear in various tasks due to the changing distances between the objects and the camera. First, we introduce the general theory for building scale-equivariant convolutional networks with steerable filters. We develop scale-convolution and generalize other common blocks to be scale-equivariant. We demonstrate the computational efficiency and numerical stability of the proposed method. We compare the proposed models to the previously developed methods for scale equivariance and local scale invariance. We demonstrate state-of-the-art results on the MNIST-scale dataset and on the STL-10 dataset in the supervised learning setting",
    "checked": true,
    "id": "44904a561644c3031af749d33328935dc8d3b366",
    "semantic_title": "scale-equivariant steerable networks",
    "citation_count": 149,
    "authors": []
  },
  "https://openreview.net/forum?id=H1lK_lBtvS": {
    "title": "Classification-Based Anomaly Detection for General Data",
    "volume": "poster",
    "abstract": "Anomaly detection, finding patterns that substantially deviate from those seen previously, is one of the fundamental problems of artificial intelligence. Recently, classification-based methods were shown to achieve superior results on this task. In this work, we present a unifying view and propose an open-set method, GOAD, to relax current generalization assumptions. Furthermore, we extend the applicability of transformation-based methods to non-image data using random affine transformations. Our method is shown to obtain state-of-the-art accuracy and is applicable to broad data types. The strong performance of our method is extensively validated on multiple datasets from different domains",
    "checked": true,
    "id": "04513c7c0b3a63fde81a996dae064a28d453c17a",
    "semantic_title": "classification-based anomaly detection for general data",
    "citation_count": 351,
    "authors": []
  },
  "https://openreview.net/forum?id=Sye_OgHFwH": {
    "title": "Unrestricted Adversarial Examples via Semantic Manipulation",
    "volume": "poster",
    "abstract": "Machine learning models, especially deep neural networks (DNNs), have been shown to be vulnerable against adversarial examples which are carefully crafted samples with a small magnitude of the perturbation. Such adversarial perturbations are usually restricted by bounding their $\\mathcal{L}_p$ norm such that they are imperceptible, and thus many current defenses can exploit this property to reduce their adversarial impact. In this paper, we instead introduce \"unrestricted\" perturbations that manipulate semantically meaningful image-based visual descriptors - color and texture - in order to generate effective and photorealistic adversarial examples. We show that these semantically aware perturbations are effective against JPEG compression, feature squeezing and adversarially trained model. We also show that the proposed methods can effectively be applied to both image classification and image captioning tasks on complex datasets such as ImageNet and MSCOCO. In addition, we conduct comprehensive user studies to show that our generated semantic adversarial examples are photorealistic to humans despite large magnitude perturbations when compared to other attacks",
    "checked": true,
    "id": "d66c7ec5cdbf4df77789748d9173e2c4775933f0",
    "semantic_title": "unrestricted adversarial examples via semantic manipulation",
    "citation_count": 153,
    "authors": []
  },
  "https://openreview.net/forum?id=HJl8_eHYvS": {
    "title": "Discriminative Particle Filter Reinforcement Learning for Complex Partial observations",
    "volume": "poster",
    "abstract": "Deep reinforcement learning is successful in decision making for sophisticated games, such as Atari, Go, etc. However, real-world decision making often requires reasoning with partial information extracted from complex visual observations. This paper presents Discriminative Particle Filter Reinforcement Learning (DPFRL), a new reinforcement learning framework for complex partial observations. DPFRL encodes a differentiable particle filter in the neural network policy for explicit reasoning with partial observations over time. The particle filter maintains a belief using learned discriminative update, which is trained end-to-end for decision making. We show that using the discriminative update instead of standard generative models results in significantly improved performance, especially for tasks with complex visual observations, because they circumvent the difficulty of modeling complex observations that are irrelevant to decision making. In addition, to extract features from the particle belief, we propose a new type of belief feature based on the moment generating function. DPFRL outperforms state-of-the-art POMDP RL models in Flickering Atari Games, an existing POMDP RL benchmark, and in Natural Flickering Atari Games, a new, more challenging POMDP RL benchmark introduced in this paper. Further, DPFRL performs well for visual navigation with real-world data in the Habitat environment",
    "checked": true,
    "id": "dc0276b62e8e92dc475bae5e107524749b948873",
    "semantic_title": "discriminative particle filter reinforcement learning for complex partial observations",
    "citation_count": 43,
    "authors": []
  },
  "https://openreview.net/forum?id=rkl8dlHYvB": {
    "title": "Learning to Group: A Bottom-Up Framework for 3D Part Discovery in Unseen Categories",
    "volume": "poster",
    "abstract": "We address the problem of learning to discover 3D parts for objects in unseen categories. Being able to learn the geometry prior of parts and transfer this prior to unseen categories pose fundamental challenges on data-driven shape segmentation approaches. Formulated as a contextual bandit problem, we propose a learning-based iterative grouping framework which learns a grouping policy to progressively merge small part proposals into bigger ones in a bottom-up fashion. At the core of our approach is to restrict the local context for extracting part-level features, which encourages the generalizability to novel categories. On a recently proposed large-scale fine-grained 3D part dataset, PartNet, we demonstrate that our method can transfer knowledge of parts learned from 3 training categories to 21 unseen testing categories without seeing any annotated samples. Quantitative comparisons against four strong shape segmentation baselines show that we achieve the state-of-the-art performance",
    "checked": true,
    "id": "32d8cfbd19702f894b94ae357d7d063d94a376bc",
    "semantic_title": "learning to group: a bottom-up framework for 3d part discovery in unseen categories",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=rylrdxHFDr": {
    "title": "State Alignment-based Imitation Learning",
    "volume": "poster",
    "abstract": "Consider an imitation learning problem that the imitator and the expert have different dynamics models. Most of existing imitation learning methods fail because they focus on the imitation of actions. We propose a novel state alignment-based imitation learning method to train the imitator by following the state sequences in the expert demonstrations as much as possible. The alignment of states comes from both local and global perspectives. We combine them into a reinforcement learning framework by a regularized policy update objective. We show the superiority of our method on standard imitation learning settings as well as the challenging settings in which the expert and the imitator have different dynamics models",
    "checked": true,
    "id": "1c1081922f849ced90745a5bb699ad0a93625be4",
    "semantic_title": "state alignment-based imitation learning",
    "citation_count": 93,
    "authors": []
  },
  "https://openreview.net/forum?id=rJe4_xSFDB": {
    "title": "Lipschitz constant estimation of Neural Networks via sparse polynomial optimization",
    "volume": "poster",
    "abstract": "We introduce LiPopt, a polynomial optimization framework for computing increasingly tighter upper bound on the Lipschitz constant of neural networks. The underlying optimization problems boil down to either linear (LP) or semidefinite (SDP) programming. We show how to use the sparse connectivity of a network, to significantly reduce the complexity of computation. This is specially useful for convolutional as well as pruned neural networks. We conduct experiments on networks with random weights as well as networks trained on MNIST, showing that in the particular case of the $\\ell_\\infty$-Lipschitz constant, our approach yields superior estimates as compared to other baselines available in the literature",
    "checked": true,
    "id": "5f80240a91e751565b1a7d28de80700b8126258c",
    "semantic_title": "lipschitz constant estimation of neural networks via sparse polynomial optimization",
    "citation_count": 139,
    "authors": []
  },
  "https://openreview.net/forum?id=rkgfdeBYvH": {
    "title": "Effect of Activation Functions on the Training of Overparametrized Neural Nets",
    "volume": "poster",
    "abstract": "It is well-known that overparametrized neural networks trained using gradient based methods quickly achieve small training error with appropriate hyperparameter settings. Recent papers have proved this statement theoretically for highly overparametrized networks under reasonable assumptions. These results either assume that the activation function is ReLU or they depend on the minimum eigenvalue of a certain Gram matrix. In the latter case, existing works only prove that this minimum eigenvalue is non-zero and do not provide quantitative bounds which require that this eigenvalue be large. Empirically, a number of alternative activation functions have been proposed which tend to perform better than ReLU at least in some settings but no clear understanding has emerged. This state of affairs underscores the importance of theoretically understanding the impact of activation functions on training. In the present paper, we provide theoretical results about the effect of activation function on the training of highly overparametrized 2-layer neural networks. A crucial property that governs the performance of an activation is whether or not it is smooth: • For non-smooth activations such as ReLU, SELU, ELU, which are not smooth because there is a point where either the ﬁrst order or second order derivative is discontinuous, all eigenvalues of the associated Gram matrix are large under minimal assumptions on the data. • For smooth activations such as tanh, swish, polynomial, which have derivatives of all orders at all points, the situation is more complex: if the subspace spanned by the data has small dimension then the minimum eigenvalue of the Gram matrix can be small leading to slow training. But if the dimension is large and the data satisﬁes another mild condition, then the eigenvalues are large. If we allow deep networks, then the small data dimension is not a limitation provided that the depth is sufﬁcient. We discuss a number of extensions and applications of these results",
    "checked": true,
    "id": "bf6529305a9640b8878c7728f8ac3c36134d60e2",
    "semantic_title": "effect of activation functions on the training of overparametrized neural nets",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=BJxkOlSYDH": {
    "title": "Provable Filter Pruning for Efficient Neural Networks",
    "volume": "poster",
    "abstract": "We present a provable, sampling-based approach for generating compact Convolutional Neural Networks (CNNs) by identifying and removing redundant filters from an over-parameterized network. Our algorithm uses a small batch of input data points to assign a saliency score to each filter and constructs an importance sampling distribution where filters that highly affect the output are sampled with correspondingly high probability. In contrast to existing filter pruning approaches, our method is simultaneously data-informed, exhibits provable guarantees on the size and performance of the pruned network, and is widely applicable to varying network architectures and data sets. Our analytical bounds bridge the notions of compressibility and importance of network structures, which gives rise to a fully-automated procedure for identifying and preserving filters in layers that are essential to the network's performance. Our experimental evaluations on popular architectures and data sets show that our algorithm consistently generates sparser and more efficient models than those constructed by existing filter pruning approaches",
    "checked": true,
    "id": "c44f236d7ac551f3d18f1e024f2502d3fb6e1228",
    "semantic_title": "provable filter pruning for efficient neural networks",
    "citation_count": 141,
    "authors": []
  },
  "https://openreview.net/forum?id=rkxawlHKDr": {
    "title": "End to End Trainable Active Contours via Differentiable Rendering",
    "volume": "poster",
    "abstract": "We present an image segmentation method that iteratively evolves a polygon. At each iteration, the vertices of the polygon are displaced based on the local value of a 2D shift map that is inferred from the input image via an encoder-decoder architecture. The main training loss that is used is the difference between the polygon shape and the ground truth segmentation mask. The network employs a neural renderer to create the polygon from its vertices, making the process fully differentiable. We demonstrate that our method outperforms the state of the art segmentation networks and deep active contour solutions in a variety of benchmarks, including medical imaging and aerial images",
    "checked": true,
    "id": "c0d2a94cf3b23df1e479349c76ca909d4887ab3b",
    "semantic_title": "end to end trainable active contours via differentiable rendering",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=rklnDgHtDS": {
    "title": "Compositional Language Continual Learning",
    "volume": "poster",
    "abstract": "Motivated by the human's ability to continually learn and gain knowledge over time, several research efforts have been pushing the limits of machines to constantly learn while alleviating catastrophic forgetting. Most of the existing methods have been focusing on continual learning of label prediction tasks, which have fixed input and output sizes. In this paper, we propose a new scenario of continual learning which handles sequence-to-sequence tasks common in language learning. We further propose an approach to use label prediction continual learning algorithm for sequence-to-sequence continual learning by leveraging compositionality. Experimental results show that the proposed method has significant improvement over state-of-the-art methods. It enables knowledge transfer and prevents catastrophic forgetting, resulting in more than 85% accuracy up to 100 stages, compared with less than 50% accuracy for baselines in instruction learning task. It also shows significant improvement in machine translation task. This is the first work to combine continual learning and compositionality for language learning, and we hope this work will make machines more helpful in various tasks",
    "checked": true,
    "id": "daaf8b821c07dc2de2b8d9e8bbb5cb41cab15f94",
    "semantic_title": "compositional language continual learning",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=Bke_DertPB": {
    "title": "Adversarial Lipschitz Regularization",
    "volume": "poster",
    "abstract": "Generative adversarial networks (GANs) are one of the most popular approaches when it comes to training generative models, among which variants of Wasserstein GANs are considered superior to the standard GAN formulation in terms of learning stability and sample quality. However, Wasserstein GANs require the critic to be 1-Lipschitz, which is often enforced implicitly by penalizing the norm of its gradient, or by globally restricting its Lipschitz constant via weight normalization techniques. Training with a regularization term penalizing the violation of the Lipschitz constraint explicitly, instead of through the norm of the gradient, was found to be practically infeasible in most situations. Inspired by Virtual Adversarial Training, we propose a method called Adversarial Lipschitz Regularization, and show that using an explicit Lipschitz penalty is indeed viable and leads to competitive performance when applied to Wasserstein GANs, highlighting an important connection between Lipschitz regularization and adversarial training",
    "checked": true,
    "id": "4538789bb5a4b3fb67123d905692800908061724",
    "semantic_title": "adversarial lipschitz regularization",
    "citation_count": 52,
    "authors": []
  },
  "https://openreview.net/forum?id=H1lNPxHKDH": {
    "title": "A Function Space View of Bounded Norm Infinite Width ReLU Nets: The Multivariate Case",
    "volume": "poster",
    "abstract": "We give a tight characterization of the (vectorized Euclidean) norm of weights required to realize a function $f:\\mathbb{R}\\rightarrow \\mathbb{R}^d$ as a single hidden-layer ReLU network with an unbounded number of units (infinite width), extending the univariate characterization of Savarese et al. (2019) to the multivariate case",
    "checked": true,
    "id": "f03b51022418bc6fd600e231519c4b9d06fd14e8",
    "semantic_title": "a function space view of bounded norm infinite width relu nets: the multivariate case",
    "citation_count": 161,
    "authors": []
  },
  "https://openreview.net/forum?id=ByeMPlHKPH": {
    "title": "Lite Transformer with Long-Short Range Attention",
    "volume": "poster",
    "abstract": "Transformer has become ubiquitous in natural language processing (e.g., machine translation, question answering); however, it requires enormous amount of computations to achieve high performance, which makes it not suitable for mobile applications that are tightly constrained by the hardware resources and battery. In this paper, we present an efficient mobile NLP architecture, Lite Transformer to facilitate deploying mobile NLP applications on edge devices. The key primitive is the Long-Short Range Attention (LSRA), where one group of heads specializes in the local context modeling (by convolution) while another group specializes in the long-distance relationship modeling (by attention). Such specialization brings consistent improvement over the vanilla transformer on three well-established language tasks: machine translation, abstractive summarization, and language modeling. Under constrained resources (500M/100M MACs), Lite Transformer outperforms transformer on WMT'14 English-French by 1.2/1.7 BLEU, respectively. Lite Transformer reduces the computation of transformer base model by 2.5x with 0.3 BLEU score degradation. Combining with pruning and quantization, we further compressed the model size of Lite Transformer by 18.2x. For language modeling, Lite Transformer achieves 1.8 lower perplexity than the transformer at around 500M MACs. Notably, Lite Transformer outperforms the AutoML-based Evolved Transformer by 0.5 higher BLEU for the mobile NLP setting without the costly architecture search that requires more than 250 GPU years. Code has been made available at https://github.com/mit-han-lab/lite-transformer",
    "checked": true,
    "id": "8af925f4edf45131b5b6fed8aa655089d58692fa",
    "semantic_title": "lite transformer with long-short range attention",
    "citation_count": 323,
    "authors": []
  },
  "https://openreview.net/forum?id=ByxaUgrFvH": {
    "title": "Mutual Information Gradient Estimation for Representation Learning",
    "volume": "poster",
    "abstract": "Mutual Information (MI) plays an important role in representation learning. However, MI is unfortunately intractable in continuous and high-dimensional settings. Recent advances establish tractable and scalable MI estimators to discover useful representation. However, most of the existing methods are not capable of providing an accurate estimation of MI with low-variance when the MI is large. We argue that directly estimating the gradients of MI is more appealing for representation learning than estimating MI in itself. To this end, we propose the Mutual Information Gradient Estimator (MIGE) for representation learning based on the score estimation of implicit distributions. MIGE exhibits a tight and smooth gradient estimation of MI in the high-dimensional and large-MI settings. We expand the applications of MIGE in both unsupervised learning of deep representations based on InfoMax and the Information Bottleneck method. Experimental results have indicated significant performance improvement in learning useful representation",
    "checked": true,
    "id": "f386bd5a2b73961740232dd5118f81081714935d",
    "semantic_title": "mutual information gradient estimation for representation learning",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=rygwLgrYPB": {
    "title": "Regularizing activations in neural networks via distribution matching with the Wasserstein metric",
    "volume": "poster",
    "abstract": "Regularization and normalization have become indispensable components in training deep neural networks, resulting in faster training and improved generalization performance. We propose the projected error function regularization loss (PER) that encourages activations to follow the standard normal distribution. PER randomly projects activations onto one-dimensional space and computes the regularization loss in the projected space. PER is similar to the Pseudo-Huber loss in the projected space, thus taking advantage of both $L^1$ and $L^2$ regularization losses. Besides, PER can capture the interaction between hidden units by projection vector drawn from a unit sphere. By doing so, PER minimizes the upper bound of the Wasserstein distance of order one between an empirical distribution of activations and the standard normal distribution. To the best of the authors' knowledge, this is the first work to regularize activations via distribution matching in the probability distribution space. We evaluate the proposed method on the image classification task and the word-level language modeling task",
    "checked": true,
    "id": "84c5bac12a4e9d09f07c7996a468cfc3685d8284",
    "semantic_title": "regularizing activations in neural networks via distribution matching with the wasserstein metric",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=S1gEIerYwH": {
    "title": "Transferring Optimality Across Data Distributions via Homotopy Methods",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "74a26773ea750739822e3a686be7051ceae9e6f4",
    "semantic_title": "transferring optimality across data distributions via homotopy methods",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=SJxE8erKDH": {
    "title": "Latent Normalizing Flows for Many-to-Many Cross-Domain Mappings",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "87415957ec3aa3ae6756e3e7d22873cd9fc74c51",
    "semantic_title": "latent normalizing flows for many-to-many cross-domain mappings",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=SJem8lSFwB": {
    "title": "Dynamic Model Pruning with Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c71da533053d79ad267b1d74814a43dda7c584fb",
    "semantic_title": "dynamic model pruning with feedback",
    "citation_count": 204,
    "authors": []
  },
  "https://openreview.net/forum?id=rJxGLlBtwH": {
    "title": "On the interaction between supervision and self-play in emergent communication",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "28ac7f784a766dc24c26eda4a5ad2cb6351826db",
    "semantic_title": "",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=ryxWIgBFPS": {
    "title": "A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "492ba3ad3f0cb85f0636bc275fecd7e7960709da",
    "semantic_title": "a meta-transfer objective for learning to disentangle causal mechanisms",
    "citation_count": 335,
    "authors": []
  },
  "https://openreview.net/forum?id=ByglLlHFDS": {
    "title": "Expected Information Maximization: Using the I-Projection for Mixture Density Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0fa6b219edfa380a4b598d97732c0158220ba1b4",
    "semantic_title": "expected information maximization: using the i-projection for mixture density estimation",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=rygjHxrYDB": {
    "title": "Deep Audio Priors Emerge From Harmonic Convolutional Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "117a0cf439519b30abccc4d031e43edabb26d4d9",
    "semantic_title": "deep audio priors emerge from harmonic convolutional networks",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=rkevSgrtPr": {
    "title": "A closer look at the approximation capabilities of neural networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "141758efb70c1e90e9b985e539fc18ea32b21799",
    "semantic_title": "a closer look at the approximation capabilities of neural networks",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=B1l4SgHKDH": {
    "title": "Residual Energy-Based Models for Text Generation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "68f86237dadcf2f570f0cd5b5e56161693619a74",
    "semantic_title": "residual energy-based models for text generation",
    "citation_count": 133,
    "authors": []
  },
  "https://openreview.net/forum?id=BylQSxHFwr": {
    "title": "AtomNAS: Fine-Grained End-to-End Neural Architecture Search",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f5f35340893d550bd5d1a2711f04308525c6dcd2",
    "semantic_title": "atomnas: fine-grained end-to-end neural architecture search",
    "citation_count": 107,
    "authors": []
  },
  "https://openreview.net/forum?id=S1gmrxHFvB": {
    "title": "AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "02b1607af35b48f0bd716367caf6a7428b969369",
    "semantic_title": "augmix: a simple data processing method to improve robustness and uncertainty",
    "citation_count": 1309,
    "authors": []
  },
  "https://openreview.net/forum?id=r1laNeBYPB": {
    "title": "Memory-Based Graph Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "47f01fd4f0c9c77058a966d3f17dbc09cf7ef42a",
    "semantic_title": "memory-based graph networks",
    "citation_count": 91,
    "authors": []
  },
  "https://openreview.net/forum?id=HkejNgBtPB": {
    "title": "Variational Template Machine for Data-to-Text Generation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "54b64cb034f86c25c9ce877be62ee90fa426cbb4",
    "semantic_title": "variational template machine for data-to-text generation",
    "citation_count": 45,
    "authors": []
  },
  "https://openreview.net/forum?id=HJloElBYvB": {
    "title": "Phase Transitions for the Information Bottleneck in Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "27485fa549cd72dc73da6b187661f5b1f403d9c1",
    "semantic_title": "phase transitions for the information bottleneck in representation learning",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=SylVNerFvr": {
    "title": "Permutation Equivariant Models for Compositional Generalization in Language",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3249dec80e963cbc86d941b819c549a325613f8c",
    "semantic_title": "permutation equivariant models for compositional generalization in language",
    "citation_count": 108,
    "authors": []
  },
  "https://openreview.net/forum?id=BJg4NgBKvH": {
    "title": "Training binary neural networks with real-to-binary convolutions",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ed9aef35fd6b96f0f357d703143c0941331b5968",
    "semantic_title": "training binary neural networks with real-to-binary convolutions",
    "citation_count": 229,
    "authors": []
  },
  "https://openreview.net/forum?id=BJgQ4lSFPH": {
    "title": "StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d56c1fc337fb07ec004dc846f80582c327af717c",
    "semantic_title": "structbert: incorporating language structures into pre-training for deep language understanding",
    "citation_count": 264,
    "authors": []
  },
  "https://openreview.net/forum?id=B1xMEerYvB": {
    "title": "Smooth markets: A basic mechanism for organizing gradient-based learners",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "68295688fe3e7fb44323880e5a76d11821602b17",
    "semantic_title": "smooth markets: a basic mechanism for organizing gradient-based learners",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=ByexElSYDr": {
    "title": "Fair Resource Allocation in Federated Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "90fdbe550c6c04b4bd082e4f5714ed40d61738a6",
    "semantic_title": "fair resource allocation in federated learning",
    "citation_count": 804,
    "authors": []
  },
  "https://openreview.net/forum?id=Sye57xStvB": {
    "title": "Never Give Up: Learning Directed Exploration Strategies",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "086159600bede14e00f96043c733d4f3b45855aa",
    "semantic_title": "never give up: learning directed exploration strategies",
    "citation_count": 300,
    "authors": []
  },
  "https://openreview.net/forum?id=H1eqQeHFDS": {
    "title": "AdvectiveNet: An Eulerian-Lagrangian Fluidic Reservoir for Point Cloud Processing",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8214a00679a1616b520f6bfa2c44bcd06df3f2e1",
    "semantic_title": "advectivenet: an eulerian-lagrangian fluidic reservoir for point cloud processing",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=BkxSmlBFvr": {
    "title": "You CAN Teach an Old Dog New Tricks! On Training Knowledge Graph Embeddings",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "829af9ef30d654c697c0309036a9087666b2f3e3",
    "semantic_title": "you can teach an old dog new tricks! on training knowledge graph embeddings",
    "citation_count": 216,
    "authors": []
  },
  "https://openreview.net/forum?id=HkxCzeHFDB": {
    "title": "Functional Regularisation for Continual Learning with Gaussian Processes",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0a6bd4bdfe9189cd7e7138c20844714cd975d27b",
    "semantic_title": "functional regularisation for continual learning using gaussian processes",
    "citation_count": 186,
    "authors": []
  },
  "https://openreview.net/forum?id=BJgZGeHFPH": {
    "title": "Dynamics-Aware Embeddings",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5ab999687734ddf8c480315bde537e76ac358a80",
    "semantic_title": "dynamics-aware embeddings",
    "citation_count": 53,
    "authors": []
  },
  "https://openreview.net/forum?id=HkgeGeBYDB": {
    "title": "RaPP: Novelty Detection with Reconstruction along Projection Pathway",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "964465406626125ce235faaccf25e265c56f501b",
    "semantic_title": "rapp: novelty detection with reconstruction along projection pathway",
    "citation_count": 81,
    "authors": []
  },
  "https://openreview.net/forum?id=ryx6WgStPB": {
    "title": "Hypermodels for Exploration",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "69efbf9ddee489b3bdc10a278d3cd375abbb0d54",
    "semantic_title": "hypermodels for exploration",
    "citation_count": 46,
    "authors": []
  },
  "https://openreview.net/forum?id=HkgsWxrtPB": {
    "title": "Meta Reinforcement Learning with Autonomous Inference of Subtask Dependencies",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4f07310856b3ebbe17f021ce142a7bb3631ff559",
    "semantic_title": "meta reinforcement learning with autonomous inference of subtask dependencies",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=Hkem-lrtvH": {
    "title": "BayesOpt Adversarial Attack",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "11c919ca13ffb2204f92c3974db1ae4c0ee051b5",
    "semantic_title": "bayesopt adversarial attack",
    "citation_count": 79,
    "authors": []
  },
  "https://openreview.net/forum?id=HklxbgBKvr": {
    "title": "Model-based reinforcement learning for biological sequence design",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "821400c84cf2d2f08d12f2fcbba4bcd881cb2088",
    "semantic_title": "model-based reinforcement learning for biological sequence design",
    "citation_count": 135,
    "authors": []
  },
  "https://openreview.net/forum?id=r1x0lxrFPS": {
    "title": "BinaryDuo: Reducing Gradient Mismatch in Binary Activation Network by Coupling Binary Activations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "eb27f938ec634b18dddb42ed3f1458fc0df3bb73",
    "semantic_title": "binaryduo: reducing gradient mismatch in binary activation network by coupling binary activations",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=S1g6xeSKDS": {
    "title": "Mixed-curvature Variational Autoencoders",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b20e90396af78f2dcd73994267d53f0a5bd3707a",
    "semantic_title": "mixed-curvature variational autoencoders",
    "citation_count": 102,
    "authors": []
  },
  "https://openreview.net/forum?id=Hyl9xxHYPr": {
    "title": "Demystifying Inter-Class Disentanglement",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "64ba28a03d0d14072fdf85ca8ec7b866c4c7869f",
    "semantic_title": "demystifying inter-class disentanglement",
    "citation_count": 56,
    "authors": []
  },
  "https://openreview.net/forum?id=r1lPleBFvH": {
    "title": "Understanding the Limitations of Conditional Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2cfb6271a5b9e47951ccd9a79ddd74b1cf457fef",
    "semantic_title": "understanding the limitations of conditional generative models",
    "citation_count": 54,
    "authors": []
  },
  "https://openreview.net/forum?id=rke7geHtwH": {
    "title": "Keep Doing What Worked: Behavior Modelling Priors for Offline Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0881655dcdf891f529ebe7ac18301e138a5e265b",
    "semantic_title": "keep doing what worked: behavioral modelling priors for offline reinforcement learning",
    "citation_count": 283,
    "authors": []
  },
  "https://openreview.net/forum?id=Hkg-xgrYvH": {
    "title": "Empirical Bayes Transductive Meta-Learning with Synthetic Gradients",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "96b279f5e8ef539a7ef01fd1fdfa6bc10f36a07c",
    "semantic_title": "empirical bayes transductive meta-learning with synthetic gradients",
    "citation_count": 125,
    "authors": []
  },
  "https://openreview.net/forum?id=rJxWxxSYvB": {
    "title": "Spike-based causal inference for weight alignment",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "10cdc548b6f1f601a6c2dee0af6f44b337760149",
    "semantic_title": "spike-based causal inference for weight alignment",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=ryl3ygHYDB": {
    "title": "Lookahead: A Far-sighted Alternative of Magnitude-based Pruning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5a6d6640e34e8328a3627200bca9a3d3972a2247",
    "semantic_title": "lookahead: a far-sighted alternative of magnitude-based pruning",
    "citation_count": 93,
    "authors": []
  },
  "https://openreview.net/forum?id=Hkl9JlBYvr": {
    "title": "VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "361e953f792a585496834ee14216b94d0ce9ae74",
    "semantic_title": "varibad: a very good method for bayes-adaptive deep rl via meta-learning",
    "citation_count": 276,
    "authors": []
  },
  "https://openreview.net/forum?id=SygKyeHKDH": {
    "title": "Making Efficient Use of Demonstrations to Solve Hard Exploration Problems",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "428e2bf31b1e6ef6b5f2e982d39575158bded349",
    "semantic_title": "making efficient use of demonstrations to solve hard exploration problems",
    "citation_count": 82,
    "authors": []
  },
  "https://openreview.net/forum?id=BygdyxHFDS": {
    "title": "Meta-learning curiosity algorithms",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "11236ae4f31b428b6313559fb99300643c172cf9",
    "semantic_title": "meta-learning curiosity algorithms",
    "citation_count": 63,
    "authors": []
  },
  "https://openreview.net/forum?id=rylwJxrYDS": {
    "title": "vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "84913d6f08942ddf8dd51418820537abfaa5ae19",
    "semantic_title": "vq-wav2vec: self-supervised learning of discrete speech representations",
    "citation_count": 666,
    "authors": []
  },
  "https://openreview.net/forum?id=rkgU1gHtvr": {
    "title": "Infinite-horizon Off-Policy Policy Evaluation with Multiple Behavior Policies",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f68d160d9d782d5ee614b478aa42466e95a96574",
    "semantic_title": "infinite-horizon off-policy policy evaluation with multiple behavior policies",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=HkeryxBtPB": {
    "title": "MMA Training: Direct Input Space Margin Maximization through Adversarial Training",
    "volume": "poster",
    "abstract": "We study adversarial robustness of neural networks from a margin maximization perspective, where margins are defined as the distances from inputs to a classifier's decision boundary. Our study shows that maximizing margins can be achieved by minimizing the adversarial loss on the decision boundary at the \"shortest successful perturbation\", demonstrating a close connection between adversarial losses and the margins. We propose Max-Margin Adversarial (MMA) training to directly maximize the margins to achieve adversarial robustness. Instead of adversarial training with a fixed $\\epsilon$, MMA offers an improvement by enabling adaptive selection of the \"correct\" $\\epsilon$ as the margin individually for each datapoint. In addition, we rigorously analyze adversarial training with the perspective of margin maximization, and provide an alternative interpretation for adversarial training, maximizing either a lower or an upper bound of the margins. Our experiments empirically confirm our theory and demonstrate MMA training's efficacy on the MNIST and CIFAR10 datasets w.r.t. $\\ell_\\infty$ and $\\ell_2$ robustness",
    "checked": true,
    "id": "73cb515ea6d15baf73b8cf3f30773f5ebad6b991",
    "semantic_title": "mma training: direct input space margin maximization through adversarial training",
    "citation_count": 273,
    "authors": []
  },
  "https://openreview.net/forum?id=Hyl7ygStwB": {
    "title": "Incorporating BERT into Neural Machine Translation",
    "volume": "poster",
    "abstract": "The recently proposed BERT (Devlin et al., 2019) has shown great power on a variety of natural language understanding tasks, such as text classification, reading comprehension, etc. However, how to effectively apply BERT to neural machine translation (NMT) lacks enough exploration. While BERT is more commonly used as fine-tuning instead of contextual embedding for downstream language understanding tasks, in NMT, our preliminary exploration of using BERT as contextual embedding is better than using for fine-tuning. This motivates us to think how to better leverage BERT for NMT along this direction. We propose a new algorithm named BERT-fused model, in which we first use BERT to extract representations for an input sequence, and then the representations are fused with each layer of the encoder and decoder of the NMT model through attention mechanisms. We conduct experiments on supervised (including sentence-level and document-level translations), semi-supervised and unsupervised machine translation, and achieve state-of-the-art results on seven benchmark datasets. Our code is available at https://github.com/bert-nmt/bert-nmt",
    "checked": true,
    "id": "dc373d5e108a90a70f55285a852a32706adbeb45",
    "semantic_title": "incorporating bert into neural machine translation",
    "citation_count": 359,
    "authors": []
  },
  "https://openreview.net/forum?id=B1e-kxSKDH": {
    "title": "Structured Object-Aware Physics Prediction for Video Modeling and Planning",
    "volume": "poster",
    "abstract": "When humans observe a physical system, they can easily locate components, understand their interactions, and anticipate future behavior, even in settings with complicated and previously unseen interactions. For computers, however, learning such models from videos in an unsupervised fashion is an unsolved research problem. In this paper, we present STOVE, a novel state-space model for videos, which explicitly reasons about objects and their positions, velocities, and interactions. It is constructed by combining an image model and a dynamics model in compositional manner and improves on previous work by reusing the dynamics model for inference, accelerating and regularizing training. STOVE predicts videos with convincing physical behavior over hundreds of timesteps, outperforms previous unsupervised models, and even approaches the performance of supervised baselines. We further demonstrate the strength of our model as a simulator for sample efficient model-based control, in a task with heavily interacting objects",
    "checked": true,
    "id": "f87c63cd336f859007a2c07744fb24f8ec466932",
    "semantic_title": "structured object-aware physics prediction for video modeling and planning",
    "citation_count": 70,
    "authors": []
  },
  "https://openreview.net/forum?id=HyxJ1xBYDH": {
    "title": "Learning-Augmented Data Stream Algorithms",
    "volume": "poster",
    "abstract": "The data stream model is a fundamental model for processing massive data sets with limited memory and fast processing time. Recently Hsu et al. (2019) incorporated machine learning techniques into the data stream model in order to learn relevant patterns in the input data. Such techniques were encapsulated by training an oracle to predict item frequencies in the streaming model. In this paper we explore the full power of such an oracle, showing that it can be applied to a wide array of problems in data streams, sometimes resulting in the first optimal bounds for such problems. Namely, we apply the oracle to counting distinct elements on the difference of streams, estimating frequency moments, estimating cascaded aggregates, and estimating moments of geometric data streams. For the distinct elements problem, we obtain the first memory-optimal algorithms. For estimating the $p$-th frequency moment for $0 < p < 2$ we obtain the first algorithms with optimal update time. For estimating the $p$-the frequency moment for $p > 2$ we obtain a quadratic saving in memory. We empirically validate our results, demonstrating also our improvements in practice",
    "checked": true,
    "id": "1c0f34c283c009cb78f86ff55d35d15eb5fc7ded",
    "semantic_title": "learning-augmented data stream algorithms",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=HJlnC1rKPB": {
    "title": "On the Relationship between Self-Attention and Convolutional Layers",
    "volume": "poster",
    "abstract": "Recent trends of incorporating attention mechanisms in vision have led researchers to reconsider the supremacy of convolutional layers as a primary building block. Beyond helping CNNs to handle long-range dependencies, Ramachandran et al. (2019) showed that attention can completely replace convolution and achieve state-of-the-art performance on vision tasks. This raises the question: do learned attention layers operate similarly to convolutional layers? This work provides evidence that attention layers can perform convolution and, indeed, they often learn to do so in practice. Specifically, we prove that a multi-head self-attention layer with sufficient number of heads is at least as expressive as any convolutional layer. Our numerical experiments then show that self-attention layers attend to pixel-grid patterns similarly to CNN layers, corroborating our analysis. Our code is publicly available",
    "checked": true,
    "id": "bb713d56a39a040b35e4f9e036fb4422f543e614",
    "semantic_title": "on the relationship between self-attention and convolutional layers",
    "citation_count": 534,
    "authors": []
  },
  "https://openreview.net/forum?id=rkxs0yHFPH": {
    "title": "SpikeGrad: An ANN-equivalent Computation Model for Implementing Backpropagation with Spikes",
    "volume": "poster",
    "abstract": "Event-based neuromorphic systems promise to reduce the energy consumption of deep neural networks by replacing expensive floating point operations on dense matrices by low energy, sparse operations on spike events. While these systems can be trained increasingly well using approximations of the backpropagation algorithm, this usually requires high precision errors and is therefore incompatible with the typical communication infrastructure of neuromorphic circuits. In this work, we analyze how the gradient can be discretized into spike events when training a spiking neural network. To accelerate our simulation, we show that using a special implementation of the integrate-and-fire neuron allows us to describe the accumulated activations and errors of the spiking neural network in terms of an equivalent artificial neural network, allowing us to largely speed up training compared to an explicit simulation of all spike events. This way we are able to demonstrate that even for deep networks, the gradients can be discretized sufficiently well with spikes if the gradient is properly rescaled. This form of spike-based backpropagation enables us to achieve equivalent or better accuracies on the MNIST and CIFAR10 datasets than comparable state-of-the-art spiking neural networks trained with full precision gradients. The algorithm, which we call SpikeGrad, is based on only accumulation and comparison operations and can naturally exploit sparsity in the gradient computation, which makes it an interesting choice for a spiking neuromorphic systems with on-chip learning capacities",
    "checked": true,
    "id": "e86a97610a449d8fd46b1af9ff48a1b170e07306",
    "semantic_title": "spikegrad: an ann-equivalent computation model for implementing backpropagation with spikes",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=ryxK0JBtPr": {
    "title": "Gradient ℓ 1 Regularization for Quantization Robustness",
    "volume": "poster",
    "abstract": "We analyze the effect of quantizing weights and activations of neural networks on their loss and derive a simple regularization scheme that improves robustness against post-training quantization. By training quantization-ready networks, our approach enables storing a single set of weights that can be quantized on-demand to different bit-widths as energy and memory requirements of the application change. Unlike quantization-aware training using the straight-through estimator that only targets a specific bit-width and requires access to training data and pipeline, our regularization-based method paves the way for ``on the fly'' post-training quantization to various bit-widths. We show that by modeling quantization as a $\\ell_\\infty$-bounded perturbation, the first-order term in the loss expansion can be regularized using the $\\ell_1$-norm of gradients. We experimentally validate our method on different vision architectures on CIFAR-10 and ImageNet datasets and show that the regularization of a neural network using our method improves robustness against quantization noise",
    "checked": true,
    "id": "8085bf032d71856bdc68fdb02cbc730875ba6ce5",
    "semantic_title": "gradient 𝓁1 regularization for quantization robustness",
    "citation_count": 49,
    "authors": []
  },
  "https://openreview.net/forum?id=SylL0krYPS": {
    "title": "Toward Evaluating Robustness of Deep Reinforcement Learning with Continuous Control",
    "volume": "poster",
    "abstract": "Deep reinforcement learning has achieved great success in many previously difficult reinforcement learning tasks, yet recent studies show that deep RL agents are also unavoidably susceptible to adversarial perturbations, similar to deep neural networks in classification tasks. Prior works mostly focus on model-free adversarial attacks and agents with discrete actions. In this work, we study the problem of continuous control agents in deep RL with adversarial attacks and propose the first two-step algorithm based on learned model dynamics. Extensive experiments on various MuJoCo domains (Cartpole, Fish, Walker, Humanoid) demonstrate that our proposed framework is much more effective and efficient than model-free based attacks baselines in degrading agent performance as well as driving agents to unsafe states",
    "checked": true,
    "id": "6113062a0d646080aa8ec726908fba9406f4eebb",
    "semantic_title": "toward evaluating robustness of deep reinforcement learning with continuous control",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=SkgGCkrKvH": {
    "title": "Decentralized Deep Learning with Arbitrary Communication Compression",
    "volume": "poster",
    "abstract": "Decentralized training of deep learning models is a key element for enabling data privacy and on-device learning over networks, as well as for efficient scaling to large compute clusters. As current approaches are limited by network bandwidth, we propose the use of communication compression in the decentralized training context. We show that Choco-SGD achieves linear speedup in the number of workers for arbitrary high compression ratios on general non-convex functions, and non-IID training data. We demonstrate the practical performance of the algorithm in two key scenarios: the training of deep learning models (i) over decentralized user devices, connected by a peer-to-peer network and (ii) in a datacenter",
    "checked": true,
    "id": "6105fc6d1058cd883037ed89f42332c56eef8160",
    "semantic_title": "decentralized deep learning with arbitrary communication compression",
    "citation_count": 235,
    "authors": []
  },
  "https://openreview.net/forum?id=Hye1RJHKwB": {
    "title": "Training Generative Adversarial Networks from Incomplete Observations using Factorised Discriminators",
    "volume": "poster",
    "abstract": "Generative adversarial networks (GANs) have shown great success in applications such as image generation and inpainting. However, they typically require large datasets, which are often not available, especially in the context of prediction tasks such as image segmentation that require labels. Therefore, methods such as the CycleGAN use more easily available unlabelled data, but do not offer a way to leverage additional labelled data for improved performance. To address this shortcoming, we show how to factorise the joint data distribution into a set of lower-dimensional distributions along with their dependencies. This allows splitting the discriminator in a GAN into multiple \"sub-discriminators\" that can be independently trained from incomplete observations. Their outputs can be combined to estimate the density ratio between the joint real and the generator distribution, which enables training generators as in the original GAN framework. We apply our method to image generation, image segmentation and audio source separation, and obtain improved performance over a standard GAN when additional incomplete training examples are available. For the Cityscapes segmentation task in particular, our method also improves accuracy by an absolute 14.9% over CycleGAN while using only 25 additional paired examples",
    "checked": true,
    "id": "c2ce9df9888a167612684b60bd7f3d7407bb2a73",
    "semantic_title": "training generative adversarial networks from incomplete observations using factorised discriminators",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=SkeAaJrKDS": {
    "title": "Combining Q-Learning and Search with Amortized Value Estimates",
    "volume": "poster",
    "abstract": "We introduce \"Search with Amortized Value Estimates\" (SAVE), an approach for combining model-free Q-learning with model-based Monte-Carlo Tree Search (MCTS). In SAVE, a learned prior over state-action values is used to guide MCTS, which estimates an improved set of state-action values. The new Q-estimates are then used in combination with real experience to update the prior. This effectively amortizes the value computation performed by MCTS, resulting in a cooperative relationship between model-free learning and model-based search. SAVE can be implemented on top of any Q-learning agent with access to a model, which we demonstrate by incorporating it into agents that perform challenging physical reasoning tasks and Atari. SAVE consistently achieves higher rewards with fewer training steps, and---in contrast to typical model-based search approaches---yields strong performance with very small search budgets. By combining real experience with information computed during search, SAVE demonstrates that it is possible to improve on both the performance of model-free learning and the computational cost of planning",
    "checked": true,
    "id": "8d6d37f3f01b6aa7e42b1b76bffad1d14d934d85",
    "semantic_title": "combining q-learning and search with amortized value estimates",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=ryxC6kSYPr": {
    "title": "Infinite-Horizon Differentiable Model Predictive Control",
    "volume": "poster",
    "abstract": "This paper proposes a differentiable linear quadratic Model Predictive Control (MPC) framework for safe imitation learning. The infinite-horizon cost is enforced using a terminal cost function obtained from the discrete-time algebraic Riccati equation (DARE), so that the learned controller can be proven to be stabilizing in closed-loop. A central contribution is the derivation of the analytical derivative of the solution of the DARE, thereby allowing the use of differentiation-based learning methods. A further contribution is the structure of the MPC optimization problem: an augmented Lagrangian method ensures that the MPC optimization is feasible throughout training whilst enforcing hard constraints on state and input, and a pre-stabilizing controller ensures that the MPC solution and derivatives are accurate at each iteration. The learning capabilities of the framework are demonstrated in a set of numerical studies",
    "checked": true,
    "id": "40d6a919b377e1d662f5990cdbe5aea780d6076f",
    "semantic_title": "infinite-horizon differentiable model predictive control",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=rke3TJrtPS": {
    "title": "Projection-Based Constrained Policy Optimization",
    "volume": "poster",
    "abstract": "We consider the problem of learning control policies that optimize a reward function while satisfying constraints due to considerations of safety, fairness, or other costs. We propose a new algorithm - Projection-Based Constrained Policy Optimization (PCPO), an iterative method for optimizing policies in a two-step process - the first step performs an unconstrained update while the second step reconciles the constraint violation by projecting the policy back onto the constraint set. We theoretically analyze PCPO and provide a lower bound on reward improvement, as well as an upper bound on constraint violation for each policy update. We further characterize the convergence of PCPO with projection based on two different metrics - L2 norm and Kullback-Leibler divergence. Our empirical results over several control tasks demonstrate that our algorithm achieves superior performance, averaging more than 3.5 times less constraint violation and around 15% higher reward compared to state-of-the-art methods",
    "checked": true,
    "id": "9ec0bd60df62b997bb371e65d254434f8851ea9b",
    "semantic_title": "projection-based constrained policy optimization",
    "citation_count": 239,
    "authors": []
  },
  "https://openreview.net/forum?id=HyxY6JHKwr": {
    "title": "You Only Train Once: Loss-Conditional Training of Deep Networks",
    "volume": "poster",
    "abstract": "In many machine learning problems, loss functions are weighted sums of several terms. A typical approach to dealing with these is to train multiple separate models with different selections of weights and then either choose the best one according to some criterion or keep multiple models if it is desirable to maintain a diverse set of solutions. This is inefficient both at training and at inference time. We propose a method that allows replacing multiple models trained on one loss function each by a single model trained on a distribution of losses. At test time a model trained this way can be conditioned to generate outputs corresponding to any loss from the training distribution of losses. We demonstrate this approach on three tasks with parametrized losses: beta-VAE, learned image compression, and fast style transfer",
    "checked": true,
    "id": "58a1a06d505526b08163ca36c3a130c043fee54c",
    "semantic_title": "you only train once: loss-conditional training of deep networks",
    "citation_count": 80,
    "authors": []
  },
  "https://openreview.net/forum?id=BJe8pkHFwS": {
    "title": "GraphSAINT: Graph Sampling Based Inductive Learning Method",
    "volume": "poster",
    "abstract": "Graph Convolutional Networks (GCNs) are powerful models for learning representations of attributed graphs. To scale GCNs to large graphs, state-of-the-art methods use various layer sampling techniques to alleviate the \"neighbor explosion\" problem during minibatch training. We propose GraphSAINT, a graph sampling based inductive learning method that improves training efficiency and accuracy in a fundamentally different way. By changing perspective, GraphSAINT constructs minibatches by sampling the training graph, rather than the nodes or edges across GCN layers. Each iteration, a complete GCN is built from the properly sampled subgraph. Thus, we ensure fixed number of well-connected nodes in all layers. We further propose normalization technique to eliminate bias, and sampling algorithms for variance reduction. Importantly, we can decouple the sampling from the forward and backward propagation, and extend GraphSAINT with many architecture variants (e.g., graph attention, jumping connection). GraphSAINT demonstrates superior performance in both accuracy and training time on five large graphs, and achieves new state-of-the-art F1 scores for PPI (0.995) and Reddit (0.970)",
    "checked": true,
    "id": "3345443925cec95381c2cf2f0b029c411945bfef",
    "semantic_title": "graphsaint: graph sampling based inductive learning method",
    "citation_count": 205,
    "authors": []
  },
  "https://openreview.net/forum?id=rJg76kStwH": {
    "title": "Efficient Probabilistic Logic Reasoning with Graph Neural Networks",
    "volume": "poster",
    "abstract": "Markov Logic Networks (MLNs), which elegantly combine logic rules and probabilistic graphical models, can be used to address many knowledge graph problems. However, inference in MLN is computationally intensive, making the industrial-scale application of MLN very difficult. In recent years, graph neural networks (GNNs) have emerged as efficient and effective tools for large-scale graph problems. Nevertheless, GNNs do not explicitly incorporate prior logic rules into the models, and may require many labeled examples for a target task. In this paper, we explore the combination of MLNs and GNNs, and use graph neural networks for variational inference in MLN. We propose a GNN variant, named ExpressGNN, which strikes a nice balance between the representation power and the simplicity of the model. Our extensive experiments on several benchmark datasets demonstrate that ExpressGNN leads to effective and efficient probabilistic logic reasoning",
    "checked": true,
    "id": "c77faa8577817b26c1a2e81914f19eecbe13ee1f",
    "semantic_title": "efficient probabilistic logic reasoning with graph neural networks",
    "citation_count": 111,
    "authors": []
  },
  "https://openreview.net/forum?id=SkxQp1StDH": {
    "title": "Low-dimensional statistical manifold embedding of directed graphs",
    "volume": "poster",
    "abstract": "We propose a novel node embedding of directed graphs to statistical manifolds, which is based on a global minimization of pairwise relative entropy and graph geodesics in a non-linear way. Each node is encoded with a probability density function over a measurable space. Furthermore, we analyze the connection of the geometrical properties of such embedding and their efficient learning procedure. Extensive experiments show that our proposed embedding is better preserving the global geodesic information of graphs, as well as outperforming existing embedding models on directed graphs in a variety of evaluation metrics, in an unsupervised setting",
    "checked": true,
    "id": "9ae8111aadef1ff4c894809faa4d7430c3dddd3d",
    "semantic_title": "low-dimensional statistical manifold embedding of directed graphs",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=rkg-TJBFPB": {
    "title": "RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments",
    "volume": "poster",
    "abstract": "Exploration in sparse reward environments remains one of the key challenges of model-free reinforcement learning. Instead of solely relying on extrinsic rewards provided by the environment, many state-of-the-art methods use intrinsic rewards to encourage exploration. However, we show that existing methods fall short in procedurally-generated environments where an agent is unlikely to visit a state more than once. We propose a novel type of intrinsic reward which encourages the agent to take actions that lead to significant changes in its learned state representation. We evaluate our method on multiple challenging procedurally-generated tasks in MiniGrid, as well as on tasks with high-dimensional observations used in prior work. Our experiments demonstrate that this approach is more sample efficient than existing exploration methods, particularly for procedurally-generated MiniGrid environments. Furthermore, we analyze the learned behavior as well as the intrinsic reward received by our agent. In contrast to previous approaches, our intrinsic reward does not diminish during the course of training and it rewards the agent substantially more for interacting with objects that it can control",
    "checked": true,
    "id": "bebe8ffb0c357ac0c7eea2556f817b03ee22b570",
    "semantic_title": "ride: rewarding impact-driven exploration for procedurally-generated environments",
    "citation_count": 172,
    "authors": []
  },
  "https://openreview.net/forum?id=rkl03ySYDH": {
    "title": "SPACE: Unsupervised Object-Oriented Scene Representation via Spatial Attention and Decomposition",
    "volume": "poster",
    "abstract": "The ability to decompose complex multi-object scenes into meaningful abstractions like objects is fundamental to achieve higher-level cognition. Previous approaches for unsupervised object-oriented scene representation learning are either based on spatial-attention or scene-mixture approaches and limited in scalability which is a main obstacle towards modeling real-world scenes. In this paper, we propose a generative latent variable model, called SPACE, that provides a uniﬁed probabilistic modeling framework that combines the best of spatial-attention and scene-mixture approaches. SPACE can explicitly provide factorized object representations for foreground objects while also decomposing background segments of complex morphology. Previous models are good at either of these, but not both. SPACE also resolves the scalability problems of previous methods by incorporating parallel spatial-attention and thus is applicable to scenes with a large number of objects without performance degradations. We show through experiments on Atari and 3D-Rooms that SPACE achieves the above properties consistently in comparison to SPAIR, IODINE, and GENESIS. Results of our experiments can be found on our project website: https://sites.google.com/view/space-project-page",
    "checked": true,
    "id": "5a3a1aed872ef687e91bc98ea5acf5041fd342ae",
    "semantic_title": "space: unsupervised object-oriented scene representation via spatial attention and decomposition",
    "citation_count": 250,
    "authors": []
  },
  "https://openreview.net/forum?id=HJeT3yrtDr": {
    "title": "Cross-Lingual Ability of Multilingual BERT: An Empirical Study",
    "volume": "poster",
    "abstract": "Recent work has exhibited the surprising cross-lingual abilities of multilingual BERT (M-BERT) -- surprising since it is trained without any cross-lingual objective and with no aligned data. In this work, we provide a comprehensive study of the contribution of different components in M-BERT to its cross-lingual ability. We study the impact of linguistic properties of the languages, the architecture of the model, and the learning objectives. The experimental study is done in the context of three typologically different languages -- Spanish, Hindi, and Russian -- and using two conceptually different NLP tasks, textual entailment and named entity recognition. Among our key conclusions is the fact that the lexical overlap between languages plays a negligible role in the cross-lingual success, while the depth of the network is an integral part of it. All our models and implementations can be found on our project page: http://cogcomp.org/page/publication_view/900",
    "checked": true,
    "id": "3b2538f84812f434c740115c185be3e5e216c526",
    "semantic_title": "cross-lingual ability of multilingual bert: an empirical study",
    "citation_count": 339,
    "authors": []
  },
  "https://openreview.net/forum?id=SylO2yStDr": {
    "title": "Reducing Transformer Depth on Demand with Structured Dropout",
    "volume": "poster",
    "abstract": "Overparametrized transformer networks have obtained state of the art results in various natural language processing tasks, such as machine translation, language modeling, and question answering. These models contain hundreds of millions of parameters, necessitating a large amount of computation and making them prone to overfitting. In this work, we explore LayerDrop, a form of structured dropout, which has a regularization effect during training and allows for efficient pruning at inference time. In particular, we show that it is possible to select sub-networks of any depth from one large network without having to finetune them and with limited impact on performance. We demonstrate the effectiveness of our approach by improving the state of the art on machine translation, language modeling, summarization, question answering, and language understanding benchmarks. Moreover, we show that our approach leads to small BERT-like models of higher quality than when training from scratch or using distillation",
    "checked": true,
    "id": "f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1",
    "semantic_title": "reducing transformer depth on demand with structured dropout",
    "citation_count": 591,
    "authors": []
  },
  "https://openreview.net/forum?id=Skx82ySYPH": {
    "title": "Neural Outlier Rejection for Self-Supervised Keypoint Learning",
    "volume": "poster",
    "abstract": "Identifying salient points in images is a crucial component for visual odometry, Structure-from-Motion or SLAM algorithms. Recently, several learned keypoint methods have demonstrated compelling performance on challenging benchmarks. However, generating consistent and accurate training data for interest-point detection in natural images still remains challenging, especially for human annotators. We introduce IO-Net (i.e. InlierOutlierNet), a novel proxy task for the self-supervision of keypoint detection, description and matching. By making the sampling of inlier-outlier sets from point-pair correspondences fully differentiable within the keypoint learning framework, we show that are able to simultaneously self-supervise keypoint description and improve keypoint matching. Second, we introduce KeyPointNet, a keypoint-network architecture that is especially amenable to robust keypoint detection and description. We design the network to allow local keypoint aggregation to avoid artifacts due to spatial discretizations commonly used for this task, and we improve fine-grained keypoint descriptor performance by taking advantage of efficient sub-pixel convolutions to upsample the descriptor feature-maps to a higher operating resolution. Through extensive experiments and ablative analysis, we show that the proposed self-supervised keypoint learning method greatly improves the quality of feature matching and homography estimation on challenging benchmarks over the state-of-the-art",
    "checked": true,
    "id": "6e5da19cdd3c8933e4745e376f5ced7ace347167",
    "semantic_title": "neural outlier rejection for self-supervised keypoint learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H1gBhkBFDH": {
    "title": "B-Spline CNNs on Lie groups",
    "volume": "poster",
    "abstract": "Group convolutional neural networks (G-CNNs) can be used to improve classical CNNs by equipping them with the geometric structure of groups. Central in the success of G-CNNs is the lifting of feature maps to higher dimensional disentangled representations, in which data characteristics are effectively learned, geometric data-augmentations are made obsolete, and predictable behavior under geometric transformations (equivariance) is guaranteed via group theory. Currently, however, the practical implementations of G-CNNs are limited to either discrete groups (that leave the grid intact) or continuous compact groups such as rotations (that enable the use of Fourier theory). In this paper we lift these limitations and propose a modular framework for the design and implementation of G-CNNs for arbitrary Lie groups. In our approach the differential structure of Lie groups is used to expand convolution kernels in a generic basis of B-splines that is defined on the Lie algebra. This leads to a flexible framework that enables localized, atrous, and deformable convolutions in G-CNNs by means of respectively localized, sparse and non-uniform B-spline expansions. The impact and potential of our approach is studied on two benchmark datasets: cancer detection in histopathology slides (PCam dataset) in which rotation equivariance plays a key role and facial landmark localization (CelebA dataset) in which scale equivariance is important. In both cases, G-CNN architectures outperform their classical 2D counterparts and the added value of atrous and localized group convolutions is studied in detail",
    "checked": true,
    "id": "467fe8190f9ffbbd860beec735f3b8154984d9e7",
    "semantic_title": "b-spline cnns on lie groups",
    "citation_count": 130,
    "authors": []
  },
  "https://openreview.net/forum?id=rkxNh1Stvr": {
    "title": "Quantifying Point-Prediction Uncertainty in Neural Networks via Residual Estimation with an I/O Kernel",
    "volume": "poster",
    "abstract": "Neural Networks (NNs) have been extensively used for a wide spectrum of real-world regression tasks, where the goal is to predict a numerical outcome such as revenue, effectiveness, or a quantitative result. In many such tasks, the point prediction is not enough: the uncertainty (i.e. risk or confidence) of that prediction must also be estimated. Standard NNs, which are most often used in such tasks, do not provide uncertainty information. Existing approaches address this issue by combining Bayesian models with NNs, but these models are hard to implement, more expensive to train, and usually do not predict as accurately as standard NNs. In this paper, a new framework (RIO) is developed that makes it possible to estimate uncertainty in any pretrained standard NN. The behavior of the NN is captured by modeling its prediction residuals with a Gaussian Process, whose kernel includes both the NN's input and its output. The framework is justified theoretically and evaluated in twelve real-world datasets, where it is found to (1) provide reliable estimates of uncertainty, (2) reduce the error of the point predictions, and (3) scale well to large datasets. Given that RIO can be applied to any standard NN without modifications to model architecture or training pipeline, it provides an important ingredient for building real-world NN applications",
    "checked": true,
    "id": "63b8e0bbf42cee7fdbed1de3557d94a56b41cf18",
    "semantic_title": "quantifying point-prediction uncertainty in neural networks via residual estimation with an i/o kernel",
    "citation_count": 53,
    "authors": []
  },
  "https://openreview.net/forum?id=HJem3yHKwH": {
    "title": "EMPIR: Ensembles of Mixed Precision Deep Networks for Increased Robustness Against Adversarial Attacks",
    "volume": "poster",
    "abstract": "Ensuring robustness of Deep Neural Networks (DNNs) is crucial to their adoption in safety-critical applications such as self-driving cars, drones, and healthcare. Notably, DNNs are vulnerable to adversarial attacks in which small input perturbations can produce catastrophic misclassifications. In this work, we propose EMPIR, ensembles of quantized DNN models with different numerical precisions, as a new approach to increase robustness against adversarial attacks. EMPIR is based on the observation that quantized neural networks often demonstrate much higher robustness to adversarial attacks than full precision networks, but at the cost of a substantial loss in accuracy on the original (unperturbed) inputs. EMPIR overcomes this limitation to achieve the ``best of both worlds\", i.e., the higher unperturbed accuracies of the full precision models combined with the higher robustness of the low precision models, by composing them in an ensemble. Further, as low precision DNN models have significantly lower computational and storage requirements than full precision models, EMPIR models only incur modest compute and memory overheads compared to a single full-precision model (<25% in our evaluations). We evaluate EMPIR across a suite of DNNs for 3 different image recognition tasks (MNIST, CIFAR-10 and ImageNet) and under 4 different adversarial attacks. Our results indicate that EMPIR boosts the average adversarial accuracies by 42.6%, 15.2% and 10.5% for the DNN models trained on the MNIST, CIFAR-10 and ImageNet datasets respectively, when compared to single full-precision models, without sacrificing accuracy on the unperturbed inputs",
    "checked": true,
    "id": "2a88cc8cc9562b4addec03ba16b35cb4d3baaa43",
    "semantic_title": "empir: ensembles of mixed precision deep networks for increased robustness against adversarial attacks",
    "citation_count": 63,
    "authors": []
  },
  "https://openreview.net/forum?id=HklXn1BKDH": {
    "title": "Learning To Explore Using Active Neural SLAM",
    "volume": "poster",
    "abstract": "This work presents a modular and hierarchical approach to learn policies for exploring 3D environments, called `Active Neural SLAM'. Our approach leverages the strengths of both classical and learning-based methods, by using analytical path planners with learned SLAM module, and global and local policies. The use of learning provides flexibility with respect to input modalities (in the SLAM module), leverages structural regularities of the world (in global policies), and provides robustness to errors in state estimation (in local policies). Such use of learning within each module retains its benefits, while at the same time, hierarchical decomposition and modular training allow us to sidestep the high sample complexities associated with training end-to-end policies. Our experiments in visually and physically realistic simulated 3D environments demonstrate the effectiveness of our approach over past learning and geometry-based approaches. The proposed model can also be easily transferred to the PointGoal task and was the winning entry of the CVPR 2019 Habitat PointGoal Navigation Challenge",
    "checked": true,
    "id": "6c5f199f7e2cc1fd93240a21719498a3f540dcbe",
    "semantic_title": "learning to explore using active neural slam",
    "citation_count": 513,
    "authors": []
  },
  "https://openreview.net/forum?id=SylzhkBtDB": {
    "title": "Understanding and Improving Information Transfer in Multi-Task Learning",
    "volume": "poster",
    "abstract": "We investigate multi-task learning approaches that use a shared feature representation for all tasks. To better understand the transfer of task information, we study an architecture with a shared module for all tasks and a separate output module for each task. We study the theory of this setting on linear and ReLU-activated models. Our key observation is that whether or not tasks' data are well-aligned can significantly affect the performance of multi-task learning. We show that misalignment between task data can cause negative transfer (or hurt performance) and provide sufficient conditions for positive transfer. Inspired by the theoretical insights, we show that aligning tasks' embedding layers leads to performance gains for multi-task training and transfer learning on the GLUE benchmark and sentiment analysis tasks; for example, we obtained a 2.35% GLUE score average improvement on 5 GLUE tasks over BERT LARGE using our alignment method. We also design an SVD-based task re-weighting scheme and show that it improves the robustness of multi-task training on a multi-label image dataset",
    "checked": true,
    "id": "7438c34a128c13e811aca2e599028bb3760e1816",
    "semantic_title": "understanding and improving information transfer in multi-task learning",
    "citation_count": 156,
    "authors": []
  },
  "https://openreview.net/forum?id=HylAoJSKvH": {
    "title": "A Stochastic Derivative Free Optimization Method with Momentum",
    "volume": "poster",
    "abstract": "We consider the problem of unconstrained minimization of a smooth objective function in $\\mathbb{R}^d$ in setting where only function evaluations are possible. We propose and analyze stochastic zeroth-order method with heavy ball momentum. In particular, we propose, SMTP, a momentum version of the stochastic three-point method (STP) Bergou et al. (2019). We show new complexity results for non-convex, convex and strongly convex functions. We test our method on a collection of learning to continuous control tasks on several MuJoCo Todorov et al. (2012) environments with varying difficulty and compare against STP, other state-of-the-art derivative-free optimization algorithms and against policy gradient methods. SMTP significantly outperforms STP and all other methods that we considered in our numerical experiments. Our second contribution is SMTP with importance sampling which we call SMTP_IS. We provide convergence analysis of this method for non-convex, convex and strongly convex objectives",
    "checked": true,
    "id": "1bd7d16340642948142d7608ef8f085d934d94a3",
    "semantic_title": "a stochastic derivative free optimization method with momentum",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=SylKikSYDH": {
    "title": "Compressive Transformers for Long-Range Sequence Modelling",
    "volume": "poster",
    "abstract": "We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19",
    "checked": true,
    "id": "f51497f463566581874c941353dd9d80069c5b77",
    "semantic_title": "compressive transformers for long-range sequence modelling",
    "citation_count": 646,
    "authors": []
  },
  "https://openreview.net/forum?id=rkxDoJBYPB": {
    "title": "Reinforced Genetic Algorithm Learning for Optimizing Computation Graphs",
    "volume": "poster",
    "abstract": "We present a deep reinforcement learning approach to minimizing the execution cost of neural network computation graphs in an optimizing compiler. Unlike earlier learning-based works that require training the optimizer on the same graph to be optimized, we propose a learning approach that trains an optimizer offline and then generalizes to previously unseen graphs without further training. This allows our approach to produce high-quality execution decisions on real-world TensorFlow graphs in seconds instead of hours. We consider two optimization tasks for computation graphs: minimizing running time and peak memory usage. In comparison to an extensive set of baselines, our approach achieves significant improvements over classical and other learning-based methods on these two tasks",
    "checked": true,
    "id": "ddd73601e95d70d0b7dbabb6fe455d09144d2253",
    "semantic_title": "reinforced genetic algorithm learning for optimizing computation graphs",
    "citation_count": 67,
    "authors": []
  },
  "https://openreview.net/forum?id=B1lDoJSYDH": {
    "title": "Lagrangian Fluid Simulation with Continuous Convolutions",
    "volume": "poster",
    "abstract": "We present an approach to Lagrangian fluid simulation with a new type of convolutional network. Our networks process sets of moving particles, which describe fluids in space and time. Unlike previous approaches, we do not build an explicit graph structure to connect the particles but use spatial convolutions as the main differentiable operation that relates particles to their neighbors. To this end we present a simple, novel, and effective extension of N-D convolutions to the continuous domain. We show that our network architecture can simulate different materials, generalizes to arbitrary collision geometries, and can be used for inverse problems. In addition, we demonstrate that our continuous convolutions outperform prior formulations in terms of accuracy and speed",
    "checked": true,
    "id": "46072c941f2c5308ba4d378f0af3c9e523a8dc79",
    "semantic_title": "lagrangian fluid simulation with continuous convolutions",
    "citation_count": 176,
    "authors": []
  },
  "https://openreview.net/forum?id=B1gHokBKwS": {
    "title": "Learning to Guide Random Search",
    "volume": "poster",
    "abstract": "We are interested in derivative-free optimization of high-dimensional functions. The sample complexity of existing methods is high and depends on problem dimensionality, unlike the dimensionality-independent rates of first-order methods. The recent success of deep learning suggests that many datasets lie on low-dimensional manifolds that can be represented by deep nonlinear models. We therefore consider derivative-free optimization of a high-dimensional function that lies on a latent low-dimensional manifold. We develop an online learning approach that learns this manifold while performing the optimization. In other words, we jointly learn the manifold and optimize the function. Our analysis suggests that the presented method significantly reduces sample complexity. We empirically evaluate the method on continuous optimization benchmarks and high-dimensional continuous control problems. Our method achieves significantly lower sample complexity than Augmented Random Search, Bayesian optimization, covariance matrix adaptation (CMA-ES), and other derivative-free optimization algorithms",
    "checked": true,
    "id": "8ec2720501d862a3a0d96dcca60d54176005416c",
    "semantic_title": "learning to guide random search",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=SJx0q1rtvS": {
    "title": "Robust anomaly detection and backdoor attack detection via differential privacy",
    "volume": "poster",
    "abstract": "Outlier detection and novelty detection are two important topics for anomaly detection. Suppose the majority of a dataset are drawn from a certain distribution, outlier detection and novelty detection both aim to detect data samples that do not fit the distribution. Outliers refer to data samples within this dataset, while novelties refer to new samples. In the meantime, backdoor poisoning attacks for machine learning models are achieved through injecting poisoning samples into the training dataset, which could be regarded as \"outliers\" that are intentionally added by attackers. Differential privacy has been proposed to avoid leaking any individual's information, when aggregated analysis is performed on a given dataset. It is typically achieved by adding random noise, either directly to the input dataset, or to intermediate results of the aggregation mechanism. In this paper, we demonstrate that applying differential privacy could improve the utility of outlier detection and novelty detection, with an extension to detect poisoning samples in backdoor attacks. We first present a theoretical analysis on how differential privacy helps with the detection, and then conduct extensive experiments to validate the effectiveness of differential privacy in improving outlier detection, novelty detection, and backdoor attack detection",
    "checked": true,
    "id": "7ad9822c1de2b61708b803e8a0a548718cefdeb5",
    "semantic_title": "robust anomaly detection and backdoor attack detection via differential privacy",
    "citation_count": 176,
    "authors": []
  },
  "https://openreview.net/forum?id=SJeq9JBFvH": {
    "title": "Deep probabilistic subsampling for task-adaptive compressed sensing",
    "volume": "poster",
    "abstract": "The field of deep learning is commonly concerned with optimizing predictive models using large pre-acquired datasets of densely sampled datapoints or signals. In this work, we demonstrate that the deep learning paradigm can be extended to incorporate a subsampling scheme that is jointly optimized under a desired minimum sample rate. We present Deep Probabilistic Subsampling (DPS), a widely applicable framework for task-adaptive compressed sensing that enables end-to end optimization of an optimal subset of signal samples with a subsequent model that performs a required task. We demonstrate strong performance on reconstruction and classification tasks of a toy dataset, MNIST, and CIFAR10 under stringent subsampling rates in both the pixel and the spatial frequency domain. Due to the task-agnostic nature of the framework, DPS is directly applicable to all real-world domains that benefit from sample rate reduction",
    "checked": true,
    "id": "36e3339efb1a0a548e3469f4332d2c0036bb543c",
    "semantic_title": "deep probabilistic subsampling for task-adaptive compressed sensing",
    "citation_count": 42,
    "authors": []
  },
  "https://openreview.net/forum?id=B1xwcyHFDr": {
    "title": "Learning Robust Representations via Multi-View Information Bottleneck",
    "volume": "poster",
    "abstract": "The information bottleneck principle provides an information-theoretic method for representation learning, by training an encoder to retain all information which is relevant for predicting the label while minimizing the amount of other, excess information in the representation. The original formulation, however, requires labeled data to identify the superfluous information. In this work, we extend this ability to the multi-view unsupervised setting, where two views of the same underlying entity are provided but the label is unknown. This enables us to identify superfluous information as that not shared by both views. A theoretical analysis leads to the definition of a new multi-view model that produces state-of-the-art results on the Sketchy dataset and label-limited versions of the MIR-Flickr dataset. We also extend our theory to the single-view setting by taking advantage of standard data augmentation techniques, empirically showing better generalization capabilities when compared to common unsupervised approaches for representation learning",
    "checked": true,
    "id": "4bf832104a47c380eb4413b20a9d5bf06649684f",
    "semantic_title": "learning robust representations via multi-view information bottleneck",
    "citation_count": 191,
    "authors": []
  },
  "https://openreview.net/forum?id=Bke89JBtvB": {
    "title": "Batch-shaping for learning conditional channel gated networks",
    "volume": "poster",
    "abstract": "We present a method that trains large capacity neural networks with significantly improved accuracy and lower dynamic computational cost. This is achieved by gating the deep-learning architecture on a fine-grained-level. Individual convolutional maps are turned on/off conditionally on features in the network. To achieve this, we introduce a new residual block architecture that gates convolutional channels in a fine-grained manner. We also introduce a generally applicable tool batch-shaping that matches the marginal aggregate posteriors of features in a neural network to a pre-specified prior distribution. We use this novel technique to force gates to be more conditional on the data. We present results on CIFAR-10 and ImageNet datasets for image classification, and Cityscapes for semantic segmentation. Our results show that our method can slim down large architectures conditionally, such that the average computational cost on the data is on par with a smaller architecture, but with higher accuracy. In particular, on ImageNet, our ResNet50 and ResNet34 gated networks obtain 74.60% and 72.55% top-1 accuracy compared to the 69.76% accuracy of the baseline ResNet18 model, for similar complexity. We also show that the resulting networks automatically learn to use more features for difficult examples and fewer features for simple examples",
    "checked": true,
    "id": "e3d6daa6b562096d2cf2e48a4b16598ad175b0c4",
    "semantic_title": "batch-shaping for learning conditional channel gated networks",
    "citation_count": 76,
    "authors": []
  },
  "https://openreview.net/forum?id=rkem91rtDB": {
    "title": "Inductive and Unsupervised Representation Learning on Graph Structured Objects",
    "volume": "poster",
    "abstract": "Inductive and unsupervised graph learning is a critical technique for predictive or information retrieval tasks where label information is difficult to obtain. It is also challenging to make graph learning inductive and unsupervised at the same time, as learning processes guided by reconstruction error based loss functions inevitably demand graph similarity evaluation that is usually computationally intractable. In this paper, we propose a general framework SEED (Sampling, Encoding, and Embedding Distributions) for inductive and unsupervised representation learning on graph structured objects. Instead of directly dealing with the computational challenges raised by graph similarity evaluation, given an input graph, the SEED framework samples a number of subgraphs whose reconstruction errors could be efficiently evaluated, encodes the subgraph samples into a collection of subgraph vectors, and employs the embedding of the subgraph vector distribution as the output vector representation for the input graph. By theoretical analysis, we demonstrate the close connection between SEED and graph isomorphism. Using public benchmark datasets, our empirical study suggests the proposed SEED framework is able to achieve up to 10% improvement, compared with competitive baseline methods",
    "checked": true,
    "id": "6154bcae4a3df2a2520b13655ddc385488a9e17f",
    "semantic_title": "inductive and unsupervised representation learning on graph structured objects",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=BJlZ5ySKPH": {
    "title": "U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation",
    "volume": "poster",
    "abstract": "We propose a novel method for unsupervised image-to-image translation, which incorporates a new attention module and a new learnable normalization function in an end-to-end manner. The attention module guides our model to focus on more important regions distinguishing between source and target domains based on the attention map obtained by the auxiliary classifier. Unlike previous attention-based method which cannot handle the geometric changes between domains, our model can translate both images requiring holistic changes and images requiring large shape changes. Moreover, our new AdaLIN (Adaptive Layer-Instance Normalization) function helps our attention-guided model to flexibly control the amount of change in shape and texture by learned parameters depending on datasets. Experimental results show the superiority of the proposed method compared to the existing state-of-the-art models with a fixed network architecture and hyper-parameters",
    "checked": true,
    "id": "2f5cc15bee412176356ea6e068ff775140a02f8f",
    "semantic_title": "u-gat-it: unsupervised generative attentional networks with adaptive layer-instance normalization for image-to-image translation",
    "citation_count": 470,
    "authors": []
  },
  "https://openreview.net/forum?id=BJe-91BtvH": {
    "title": "Masked Based Unsupervised Content Transfer",
    "volume": "poster",
    "abstract": "We consider the problem of translating, in an unsupervised manner, between two domains where one contains some additional information compared to the other. The proposed method disentangles the common and separate parts of these domains and, through the generation of a mask, focuses the attention of the underlying network to the desired augmentation alone, without wastefully reconstructing the entire target. This enables state-of-the-art quality and variety of content translation, as demonstrated through extensive quantitative and qualitative evaluation. Our method is also capable of adding the separate content of different guide images and domains as well as remove existing separate content. Furthermore, our method enables weakly-supervised semantic segmentation of the separate part of each domain, where only class labels are provided. Our code is available at https://github.com/rmokady/mbu-content-tansfer",
    "checked": true,
    "id": "ffdc2cc5ae2e0ff6b4165e3ee70a1405b1471f7a",
    "semantic_title": "mask based unsupervised content transfer",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=Hkx1qkrKPr": {
    "title": "DropEdge: Towards Deep Graph Convolutional Networks on Node Classification",
    "volume": "poster",
    "abstract": "Over-fitting and over-smoothing are two main obstacles of developing deep Graph Convolutional Networks (GCNs) for node classification. In particular, over-fitting weakens the generalization ability on small dataset, while over-smoothing impedes model training by isolating output representations from the input features with the increase in network depth. This paper proposes DropEdge, a novel and flexible technique to alleviate both issues. At its core, DropEdge randomly removes a certain number of edges from the input graph at each training epoch, acting like a data augmenter and also a message passing reducer. Furthermore, we theoretically demonstrate that DropEdge either reduces the convergence speed of over-smoothing or relieves the information loss caused by it. More importantly, our DropEdge is a general skill that can be equipped with many other backbone models (e.g. GCN, ResGCN, GraphSAGE, and JKNet) for enhanced performance. Extensive experiments on several benchmarks verify that DropEdge consistently improves the performance on a variety of both shallow and deep GCNs. The effect of DropEdge on preventing over-smoothing is empirically visualized and validated as well. Codes are released on~https://github.com/DropEdge/DropEdge",
    "checked": true,
    "id": "0a6a9e6d4e3efd7c69357769305b70097281655f",
    "semantic_title": "dropedge: towards deep graph convolutional networks on node classification",
    "citation_count": 1040,
    "authors": []
  },
  "https://openreview.net/forum?id=BylsKkHYvH": {
    "title": "Why Not to Use Zero Imputation? Correcting Sparsity Bias in Training Neural Networks",
    "volume": "poster",
    "abstract": "Handling missing data is one of the most fundamental problems in machine learning. Among many approaches, the simplest and most intuitive way is zero imputation, which treats the value of a missing entry simply as zero. However, many studies have experimentally confirmed that zero imputation results in suboptimal performances in training neural networks. Yet, none of the existing work has explained what brings such performance degradations. In this paper, we introduce the variable sparsity problem (VSP), which describes a phenomenon where the output of a predictive model largely varies with respect to the rate of missingness in the given input, and show that it adversarially affects the model performance. We first theoretically analyze this phenomenon and propose a simple yet effective technique to handle missingness, which we refer to as Sparsity Normalization (SN), that directly targets and resolves the VSP. We further experimentally validate SN on diverse benchmark datasets, to show that debiasing the effect of input-level sparsity improves the performance and stabilizes the training of neural networks",
    "checked": true,
    "id": "0dcb1f803eada9fbd414c24d7b804e23dda50464",
    "semantic_title": "why not to use zero imputation? correcting sparsity bias in training neural networks",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=S1g8K1BFwS": {
    "title": "Probability Calibration for Knowledge Graph Embedding Models",
    "volume": "poster",
    "abstract": "Knowledge graph embedding research has overlooked the problem of probability calibration. We show popular embedding models are indeed uncalibrated. That means probability estimates associated to predicted triples are unreliable. We present a novel method to calibrate a model when ground truth negatives are not available, which is the usual case in knowledge graphs. We propose to use Platt scaling and isotonic regression alongside our method. Experiments on three datasets with ground truth negatives show our contribution leads to well calibrated models when compared to the gold standard of using negatives. We get significantly better results than the uncalibrated models from all calibration methods. We show isotonic regression offers the best the performance overall, not without trade-offs. We also show that calibrated models reach state-of-the-art accuracy without the need to define relation-specific decision thresholds",
    "checked": true,
    "id": "19a672bdf29367b7509586a4be27c6843af903b1",
    "semantic_title": "probability calibration for knowledge graph embedding models",
    "citation_count": 46,
    "authors": []
  },
  "https://openreview.net/forum?id=SJxzFySKwH": {
    "title": "On the Equivalence between Positional Node Embeddings and Structural Graph Representations",
    "volume": "poster",
    "abstract": "This work provides the first unifying theoretical framework for node (positional) embeddings and structural graph representations, bridging methods like matrix factorization and graph neural networks. Using invariant theory, we show that relationship between structural representations and node embeddings is analogous to that of a distribution and its samples. We prove that all tasks that can be performed by node embeddings can also be performed by structural representations and vice-versa. We also show that the concept of transductive and inductive learning is unrelated to node embeddings and graph representations, clearing another source of confusion in the literature. Finally, we introduce new practical guidelines to generating and using node embeddings, which further augments standard operating procedures used today",
    "checked": true,
    "id": "cc9b137697046c29363b971e06b41cd5412b1831",
    "semantic_title": "on the equivalence between positional node embeddings and structural graph representations",
    "citation_count": 121,
    "authors": []
  },
  "https://openreview.net/forum?id=HyxjOyrKvr": {
    "title": "Neural Epitome Search for Architecture-Agnostic Network Compression",
    "volume": "poster",
    "abstract": "Traditional compression methods including network pruning, quantization, low rank factorization and knowledge distillation all assume that network architectures and parameters should be hardwired. In this work, we propose a new perspective on network compression, i.e., network parameters can be disentangled from the architectures. From this viewpoint, we present the Neural Epitome Search (NES), a new neural network compression approach that learns to find compact yet expressive epitomes for weight parameters of a specified network architecture end-to-end. The complete network to compress can be generated from the learned epitome via a novel transformation method that adaptively transforms the epitomes to match shapes of the given architecture. Compared with existing compression methods, NES allows the weight tensors to be independent of the architecture design and hence can achieve a good trade-off between model compression rate and performance given a specific model size constraint. Experiments demonstrate that, on ImageNet, when taking MobileNetV2 as backbone, our approach improves the full-model baseline by 1.47% in top-1 accuracy with 25% MAdd reduction and AutoML for Model Compression (AMC) by 2.5% with nearly the same compression ratio. Moreover, taking EfficientNet-B0 as baseline, our NES yields an improvement of 1.2% but are with 10% less MAdd. In particular, our method achieves a new state-of-the-art results of 77.5% under mobile settings (<350M MAdd). Code will be made publicly available",
    "checked": true,
    "id": "d24047d1a4a88de5139adc22f8e13601aba98d2b",
    "semantic_title": "neural epitome search for architecture-agnostic network compression",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=ryeHuJBtPH": {
    "title": "Hyper-SAGNN: a self-attention based graph neural network for hypergraphs",
    "volume": "poster",
    "abstract": "Graph representation learning for hypergraphs can be utilized to extract patterns among higher-order interactions that are critically important in many real world problems. Current approaches designed for hypergraphs, however, are unable to handle different types of hypergraphs and are typically not generic for various learning tasks. Indeed, models that can predict variable-sized heterogeneous hyperedges have not been available. Here we develop a new self-attention based graph neural network called Hyper-SAGNN applicable to homogeneous and heterogeneous hypergraphs with variable hyperedge sizes. We perform extensive evaluations on multiple datasets, including four benchmark network datasets and two single-cell Hi-C datasets in genomics. We demonstrate that Hyper-SAGNN significantly outperforms state-of-the-art methods on traditional tasks while also achieving great performance on a new task called outsider identification. We believe that Hyper-SAGNN will be useful for graph representation learning to uncover complex higher-order interactions in different applications",
    "checked": true,
    "id": "38b8c34c31f98fe43b0a478af24868365035cbb8",
    "semantic_title": "hyper-sagnn: a self-attention based graph neural network for hypergraphs",
    "citation_count": 194,
    "authors": []
  },
  "https://openreview.net/forum?id=SJxSOJStPr": {
    "title": "A Neural Dirichlet Process Mixture Model for Task-Free Continual Learning",
    "volume": "poster",
    "abstract": "Despite the growing interest in continual learning, most of its contemporary works have been studied in a rather restricted setting where tasks are clearly distinguishable, and task boundaries are known during training. However, if our goal is to develop an algorithm that learns as humans do, this setting is far from realistic, and it is essential to develop a methodology that works in a task-free manner. Meanwhile, among several branches of continual learning, expansion-based methods have the advantage of eliminating catastrophic forgetting by allocating new resources to learn new data. In this work, we propose an expansion-based approach for task-free continual learning. Our model, named Continual Neural Dirichlet Process Mixture (CN-DPM), consists of a set of neural network experts that are in charge of a subset of the data. CN-DPM expands the number of experts in a principled way under the Bayesian nonparametric framework. With extensive experiments, we show that our model successfully performs task-free continual learning for both discriminative and generative tasks such as image classification and image generation",
    "checked": true,
    "id": "8904e9986ea256f37da535f282c7cd727736db4a",
    "semantic_title": "a neural dirichlet process mixture model for task-free continual learning",
    "citation_count": 176,
    "authors": []
  },
  "https://openreview.net/forum?id=Hkx7_1rKwS": {
    "title": "On Solving Minimax Optimization Locally: A Follow-the-Ridge Approach",
    "volume": "poster",
    "abstract": "Many tasks in modern machine learning can be formulated as finding equilibria in sequential games. In particular, two-player zero-sum sequential games, also known as minimax optimization, have received growing interest. It is tempting to apply gradient descent to solve minimax optimization given its popularity and success in supervised learning. However, it has been noted that naive application of gradient descent fails to find some local minimax and can converge to non-local-minimax points. In this paper, we propose Follow-the-Ridge (FR), a novel algorithm that provably converges to and only converges to local minimax. We show theoretically that the algorithm addresses the notorious rotational behaviour of gradient dynamics, and is compatible with preconditioning and positive momentum. Empirically, FR solves toy minimax problems and improves the convergence of GAN training compared to the recent minimax optimization algorithms",
    "checked": true,
    "id": "f48050ebf0b4f8fbb77d6a8a45e546b062320421",
    "semantic_title": "on solving minimax optimization locally: a follow-the-ridge approach",
    "citation_count": 101,
    "authors": []
  },
  "https://openreview.net/forum?id=ryxGuJrFvS": {
    "title": "Distributionally Robust Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5d0e2635a1ebe2c9347529975bc876d4286c9ab7",
    "semantic_title": "distributionally robust neural networks",
    "citation_count": 577,
    "authors": []
  },
  "https://openreview.net/forum?id=B1eWOJHKvB": {
    "title": "Kernel of CycleGAN as a principal homogeneous space",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8f938d6b54c38e469997969380383de635e654aa",
    "semantic_title": "kernel of cyclegan as a principal homogeneous space",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=B1eyO1BFPr": {
    "title": "Don't Use Large Mini-batches, Use Local SGD",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "93ef5b740fa1b54929ead6eb177e0698d7f19719",
    "semantic_title": "don't use large mini-batches, use local sgd",
    "citation_count": 433,
    "authors": []
  },
  "https://openreview.net/forum?id=rklk_ySYPB": {
    "title": "Provable robustness against all adversarial l p -perturbations for p ≥ 1",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b9ad22650772ff791e687cf67b73b00b9a22203d",
    "semantic_title": "provable robustness against all adversarial lp-perturbations for p≥1",
    "citation_count": 75,
    "authors": []
  },
  "https://openreview.net/forum?id=HkxTwkrKDB": {
    "title": "On Universal Equivariant Set Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5d8b0520b61bc77975e81d4636490dbe3f22238f",
    "semantic_title": "on universal equivariant set networks",
    "citation_count": 64,
    "authors": []
  },
  "https://openreview.net/forum?id=rke2P1BFwS": {
    "title": "Tensor Decompositions for Temporal Knowledge Base Completion",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b6263f7ab737c7a790dbf818a1bb2255449db6e8",
    "semantic_title": "tensor decompositions for temporal knowledge base completion",
    "citation_count": 259,
    "authors": []
  },
  "https://openreview.net/forum?id=HJgcvJBFvB": {
    "title": "Network Randomization: A Simple Technique for Generalization in Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "dc05886db1e6f17f4489d867477b38fe13e31783",
    "semantic_title": "network randomization: a simple technique for generalization in deep reinforcement learning",
    "citation_count": 174,
    "authors": []
  },
  "https://openreview.net/forum?id=BJxwPJHFwS": {
    "title": "Robustness Verification for Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6189bf5f4c851ad0217a782509f8818aca4c7ff4",
    "semantic_title": "robustness verification for transformers",
    "citation_count": 107,
    "authors": []
  },
  "https://openreview.net/forum?id=SJgIPJBFvH": {
    "title": "Fantastic Generalization Measures and Where to Find Them",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8f18c9da3d1763723c6ef8c3734d74db005d0cff",
    "semantic_title": "fantastic generalization measures and where to find them",
    "citation_count": 492,
    "authors": []
  },
  "https://openreview.net/forum?id=SJlHwkBYDH": {
    "title": "Nesterov Accelerated Gradient and Scale Invariance for Adversarial Attacks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "fb0568dcb546bbb4d7d1ed71ce395f4b66691003",
    "semantic_title": "nesterov accelerated gradient and scale invariance for adversarial attacks",
    "citation_count": 566,
    "authors": []
  },
  "https://openreview.net/forum?id=HJgSwyBKvr": {
    "title": "Weakly Supervised Disentanglement with Guarantees",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3185fd6ca0664a06ad0db58ea92142f0e97cb89e",
    "semantic_title": "weakly supervised disentanglement with guarantees",
    "citation_count": 137,
    "authors": []
  },
  "https://openreview.net/forum?id=Byg-wJSYDS": {
    "title": "Discrepancy Ratio: Evaluating Model Performance When Even Experts Disagree on the Truth",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a1748d9242809126934b038157b690859712cffd",
    "semantic_title": "discrepancy ratio: evaluating model performance when even experts disagree on the truth",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=Byg1v1HKDB": {
    "title": "Abductive Commonsense Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a550f576ff20b8cce98f3ddad0043d3783fbc9b4",
    "semantic_title": "abductive commonsense reasoning",
    "citation_count": 460,
    "authors": []
  },
  "https://openreview.net/forum?id=Syx1DkSYwB": {
    "title": "Variance Reduction With Sparse Gradients",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a85e4d8ea8792f4d4eb6a9ef74fcbcbd57200eaf",
    "semantic_title": "variance reduction with sparse gradients",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=SklkDkSFPB": {
    "title": "BlockSwap: Fisher-guided Block Substitution for Network Compression on a Budget",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "cb084a60bfd1fac8efac916a43c2c2e958be390c",
    "semantic_title": "blockswap: fisher-guided block substitution for network compression",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SJlRUkrFPS": {
    "title": "Learning transport cost from subset correspondence",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2c21457a9880a2b8c3dcedd22db6e3a35edf3d5c",
    "semantic_title": "learning transport cost from subset correspondence",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=HkgsUJrtDB": {
    "title": "Rényi Fair Inference",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1eb7b1cafe1891712a4f764c78399a53182cdcd1",
    "semantic_title": "rényi fair inference",
    "citation_count": 67,
    "authors": []
  },
  "https://openreview.net/forum?id=BJgd81SYwr": {
    "title": "Meta Dropout: Learning to Perturb Latent Features for Generalization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1518d8af2eca9829093bcbb6f71fb4afcfd4e2ba",
    "semantic_title": "meta dropout: learning to perturb latent features for generalization",
    "citation_count": 56,
    "authors": []
  },
  "https://openreview.net/forum?id=ByxdUySKvS": {
    "title": "Adversarial AutoAugment",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7d668ed2d5b383c7ff9641974b2bc0c84f43e251",
    "semantic_title": "adversarial autoaugment",
    "citation_count": 197,
    "authors": []
  },
  "https://openreview.net/forum?id=HJgLLyrYwB": {
    "title": "State-only Imitation with Transition Dynamics Mismatch",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "34601270eb6274c5331c634be82f701ef394eacf",
    "semantic_title": "state-only imitation with transition dynamics mismatch",
    "citation_count": 49,
    "authors": []
  },
  "https://openreview.net/forum?id=rkeIIkHKvS": {
    "title": "Measuring and Improving the Use of Graph Information in Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "dc5421813f564c2b39e02afc0e1c93aabb03154e",
    "semantic_title": "measuring and improving the use of graph information in graph neural networks",
    "citation_count": 136,
    "authors": []
  },
  "https://openreview.net/forum?id=B1gX8kBtPr": {
    "title": "Universal Approximation with Certified Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e91b3347c8b411211a68ed68556298d12406cc83",
    "semantic_title": "universal approximation with certified networks",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=SJgzLkBKPB": {
    "title": "Explain Your Move: Understanding Agent Actions Using Specific and Relevant Feature Attribution",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "dd7d5d545e91882df043022b8965c147cf28af23",
    "semantic_title": "explain your move: understanding agent actions using specific and relevant feature attribution",
    "citation_count": 76,
    "authors": []
  },
  "https://openreview.net/forum?id=r1egIyBFPS": {
    "title": "Deep Symbolic Superoptimization Without Human Knowledge",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6aa41f4389de6761f7c600285be9b5518271fc32",
    "semantic_title": "deep symbolic superoptimization without human knowledge",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=HJlxIJBFDr": {
    "title": "Sample Efficient Policy Gradient Methods with Recursive Variance Reduction",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "da77c149fdb32d99e4dbb50597b905f30ace2323",
    "semantic_title": "sample efficient policy gradient methods with recursive variance reduction",
    "citation_count": 88,
    "authors": []
  },
  "https://openreview.net/forum?id=HyeaSkrYPH": {
    "title": "Certified Defenses for Adversarial Patches",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4c9ee8358d82afa960708391e2b8e83c4a737ae9",
    "semantic_title": "certified defenses for adversarial patches",
    "citation_count": 171,
    "authors": []
  },
  "https://openreview.net/forum?id=SkgpBJrtvS": {
    "title": "Contrastive Representation Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "197498cb2ad787e4f71c05098cee6b10d9d067bd",
    "semantic_title": "contrastive representation distillation",
    "citation_count": 1045,
    "authors": []
  },
  "https://openreview.net/forum?id=SJlKrkSFPH": {
    "title": "A FRAMEWORK FOR ROBUSTNESS CERTIFICATION OF SMOOTHED CLASSIFIERS USING F-DIVERGENCES",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e2d09159e54a53bb3b02e9265c0e63631d8d7d8a",
    "semantic_title": "a framework for robustness certification of smoothed classifiers using f-divergences",
    "citation_count": 70,
    "authors": []
  },
  "https://openreview.net/forum?id=BJlBSkHtDS": {
    "title": "Padé Activation Units: End-to-end Learning of Flexible Activation Functions in Deep Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a613a3dc78bcd2b52826cab0646c74fd87f1a7d1",
    "semantic_title": "padé activation units: end-to-end learning of flexible activation functions in deep networks",
    "citation_count": 80,
    "authors": []
  },
  "https://openreview.net/forum?id=SJgVHkrYDH": {
    "title": "Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "521b4e26df0f1cf5763dece14cbb218df152dc59",
    "semantic_title": "learning to retrieve reasoning paths over wikipedia graph for question answering",
    "citation_count": 288,
    "authors": []
  },
  "https://openreview.net/forum?id=rylXBkrYDS": {
    "title": "A Baseline for Few-Shot Image Classification",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ca2b1b74d61a037e3e67804e28b80ae36c64269e",
    "semantic_title": "a baseline for few-shot image classification",
    "citation_count": 579,
    "authors": []
  },
  "https://openreview.net/forum?id=ByxQB1BKwH": {
    "title": "Abstract Diagrammatic Reasoning with Multiplex Graph Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b366ad7c410409933256898065e3b9101f187f94",
    "semantic_title": "abstract diagrammatic reasoning with multiplex graph networks",
    "citation_count": 66,
    "authors": []
  },
  "https://openreview.net/forum?id=SklGryBtwr": {
    "title": "Environmental drivers of systematicity and generalization in a situated agent",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ae3501afbe8f3f5cf1c5270fa00d0b65fc1c9484",
    "semantic_title": "environmental drivers of systematicity and generalization in a situated agent",
    "citation_count": 107,
    "authors": []
  },
  "https://openreview.net/forum?id=r1xMH1BtvB": {
    "title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "756810258e3419af76aff38c895c20343b0602d0",
    "semantic_title": "",
    "citation_count": 1640,
    "authors": []
  },
  "https://openreview.net/forum?id=SJxbHkrKDH": {
    "title": "Evolutionary Population Curriculum for Scaling Multi-Agent Reinforcement Learning",
    "volume": "poster",
    "abstract": "In multi-agent games, the complexity of the environment can grow exponentially as the number of agents increases, so it is particularly challenging to learn good policies when the agent population is large. In this paper, we introduce Evolutionary Population Curriculum (EPC), a curriculum learning paradigm that scales up Multi-Agent Reinforcement Learning (MARL) by progressively increasing the population of training agents in a stage-wise manner. Furthermore, EPC uses an evolutionary approach to fix an objective misalignment issue throughout the curriculum: agents successfully trained in an early stage with a small population are not necessarily the best candidates for adapting to later stages with scaled populations. Concretely, EPC maintains multiple sets of agents in each stage, performs mix-and-match and fine-tuning over these sets and promotes the sets of agents with the best adaptability to the next stage. We implement EPC on a popular MARL algorithm, MADDPG, and empirically show that our approach consistently outperforms baselines by a large margin as the number of agents grows exponentially. The source code and videos can be found at https://sites.google.com/view/epciclr2020",
    "checked": true,
    "id": "707eb919d3daa087e63d48930c8630b06c43d24f",
    "semantic_title": "evolutionary population curriculum for scaling multi-agent reinforcement learning",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=SJexHkSFPS": {
    "title": "Thinking While Moving: Deep Reinforcement Learning with Concurrent Control",
    "volume": "poster",
    "abstract": "We study reinforcement learning in settings where sampling an action from the policy must be done concurrently with the time evolution of the controlled system, such as when a robot must decide on the next action while still performing the previous action. Much like a person or an animal, the robot must think and move at the same time, deciding on its next action before the previous one has completed. In order to develop an algorithmic framework for such concurrent control problems, we start with a continuous-time formulation of the Bellman equations, and then discretize them in a way that is aware of system delays. We instantiate this new class of approximate dynamic programming methods via a simple architectural extension to existing value-based deep reinforcement learning algorithms. We evaluate our methods on simulated benchmark tasks and a large-scale robotic grasping task where the robot must \"think while moving",
    "checked": true,
    "id": "8c7bbdc11225ac234f0117d317c823f61cec61b9",
    "semantic_title": "thinking while moving: deep reinforcement learning with concurrent control",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=Hke0V1rKPS": {
    "title": "Jacobian Adversarially Regularized Networks for Robustness",
    "volume": "poster",
    "abstract": "Adversarial examples are crafted with imperceptible perturbations with the intent to fool neural networks. Against such attacks, adversarial training and its variants stand as the strongest defense to date. Previous studies have pointed out that robust models that have undergone adversarial training tend to produce more salient and interpretable Jacobian matrices than their non-robust counterparts. A natural question is whether a model trained with an objective to produce salient Jacobian can result in better robustness. This paper answers this question with affirmative empirical results. We propose Jacobian Adversarially Regularized Networks (JARN) as a method to optimize the saliency of a classifier's Jacobian by adversarially regularizing the model's Jacobian to resemble natural training images. Image classifiers trained with JARN show improved robust accuracy compared to standard models on the MNIST, SVHN and CIFAR-10 datasets, uncovering a new angle to boost robustness without using adversarial training",
    "checked": true,
    "id": "000829516229b87cafff052fe310604c85236671",
    "semantic_title": "jacobian adversarially regularized networks for robustness",
    "citation_count": 75,
    "authors": []
  },
  "https://openreview.net/forum?id=SyxhVkrYvr": {
    "title": "Towards Verified Robustness under Text Deletion Interventions",
    "volume": "poster",
    "abstract": "Neural networks are widely used in Natural Language Processing, yet despite their empirical successes, their behaviour is brittle: they are both over-sensitive to small input changes, and under-sensitive to deletions of large fractions of input text. This paper aims to tackle under-sensitivity in the context of natural language inference by ensuring that models do not become more confident in their predictions as arbitrary subsets of words from the input text are deleted. We develop a novel technique for formal verification of this specification for models based on the popular decomposable attention mechanism by employing the efficient yet effective interval bound propagation (IBP) approach. Using this method we can efficiently prove, given a model, whether a particular sample is free from the under-sensitivity problem. We compare different training methods to address under-sensitivity, and compare metrics to measure it. In our experiments on the SNLI and MNLI datasets, we observe that IBP training leads to a significantly improved verified accuracy. On the SNLI test set, we can verify 18.4% of samples, a substantial improvement over only 2.8% using standard training",
    "checked": true,
    "id": "f9512b7c6129e0243726742cd833532482b2b11b",
    "semantic_title": "towards verified robustness under text deletion interventions",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=HyxjNyrtPr": {
    "title": "RGBD-GAN: Unsupervised 3D Representation Learning From Natural Image Datasets via RGBD Image Synthesis",
    "volume": "poster",
    "abstract": "Understanding three-dimensional (3D) geometries from two-dimensional (2D) images without any labeled information is promising for understanding the real world without incurring annotation cost. We herein propose a novel generative model, RGBD-GAN, which achieves unsupervised 3D representation learning from 2D images. The proposed method enables camera parameter--conditional image generation and depth image generation without any 3D annotations, such as camera poses or depth. We use an explicit 3D consistency loss for two RGBD images generated from different camera parameters, in addition to the ordinal GAN objective. The loss is simple yet effective for any type of image generator such as DCGAN and StyleGAN to be conditioned on camera parameters. Through experiments, we demonstrated that the proposed method could learn 3D representations from 2D images with various generator architectures",
    "checked": true,
    "id": "305ef4e00ee0511aa03809aacbac7c039c5ffc83",
    "semantic_title": "rgbd-gan: unsupervised 3d representation learning from natural image datasets via rgbd image synthesis",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=rkgqN1SYvr": {
    "title": "Provable Benefit of Orthogonal Initialization in Optimizing Deep Linear Networks",
    "volume": "poster",
    "abstract": "The selection of initial parameter values for gradient-based optimization of deep neural networks is one of the most impactful hyperparameter choices in deep learning systems, affecting both convergence times and model performance. Yet despite significant empirical and theoretical analysis, relatively little has been proved about the concrete effects of different initialization schemes. In this work, we analyze the effect of initialization in deep linear networks, and provide for the first time a rigorous proof that drawing the initial weights from the orthogonal group speeds up convergence relative to the standard Gaussian initialization with iid weights. We show that for deep networks, the width needed for efficient convergence to a global minimum with orthogonal initializations is independent of the depth, whereas the width needed for efficient convergence with Gaussian initializations scales linearly in the depth. Our results demonstrate how the benefits of a good initialization can persist throughout learning, suggesting an explanation for the recent empirical successes found by initializing very deep non-linear networks according to the principle of dynamical isometry",
    "checked": true,
    "id": "6cce1d4f64f0fdc4a69e4a108e596c555879083b",
    "semantic_title": "provable benefit of orthogonal initialization in optimizing deep linear networks",
    "citation_count": 113,
    "authors": []
  },
  "https://openreview.net/forum?id=H1edEyBKDS": {
    "title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation",
    "volume": "poster",
    "abstract": "Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper",
    "checked": true,
    "id": "e04a80263d252a3d8a382ba37a249b9345620570",
    "semantic_title": "plug and play language models: a simple approach to controlled text generation",
    "citation_count": 969,
    "authors": []
  },
  "https://openreview.net/forum?id=B1g8VkHFPH": {
    "title": "Rethinking the Hyperparameters for Fine-tuning",
    "volume": "poster",
    "abstract": "Fine-tuning from pre-trained ImageNet models has become the de-facto standard for various computer vision tasks. Current practices for fine-tuning typically involve selecting an ad-hoc choice of hyperparameters and keeping them fixed to values normally used for training from scratch. This paper re-examines several common practices of setting hyperparameters for fine-tuning. Our findings are based on extensive empirical evaluation for fine-tuning on various transfer learning benchmarks. (1) While prior works have thoroughly investigated learning rate and batch size, momentum for fine-tuning is a relatively unexplored parameter. We find that the value of momentum also affects fine-tuning performance and connect it with previous theoretical findings. (2) Optimal hyperparameters for fine-tuning, in particular, the effective learning rate, are not only dataset dependent but also sensitive to the similarity between the source domain and target domain. This is in contrast to hyperparameters for training from scratch. (3) Reference-based regularization that keeps models close to the initial model does not necessarily apply for \"dissimilar\" datasets. Our findings challenge common practices of fine-tuning and encourages deep learning practitioners to rethink the hyperparameters for fine-tuning",
    "checked": true,
    "id": "d1bc9492eabb2cac705c26bcaca98a222772b437",
    "semantic_title": "rethinking the hyperparameters for fine-tuning",
    "citation_count": 128,
    "authors": []
  },
  "https://openreview.net/forum?id=BJgr4kSFDS": {
    "title": "Query2box: Reasoning over Knowledge Graphs in Vector Space Using Box Embeddings",
    "volume": "poster",
    "abstract": "Answering complex logical queries on large-scale incomplete knowledge graphs (KGs) is a fundamental yet challenging task. Recently, a promising approach to this problem has been to embed KG entities as well as the query into a vector space such that entities that answer the query are embedded close to the query. However, prior work models queries as single points in the vector space, which is problematic because a complex query represents a potentially large set of its answer entities, but it is unclear how such a set can be represented as a single point. Furthermore, prior work can only handle queries that use conjunctions ($\\wedge$) and existential quantifiers ($\\exists$). Handling queries with logical disjunctions ($\\vee$) remains an open problem. Here we propose query2box, an embedding-based framework for reasoning over arbitrary queries with $\\wedge$, $\\vee$, and $\\exists$ operators in massive and incomplete KGs. Our main insight is that queries can be embedded as boxes (i.e., hyper-rectangles), where a set of points inside the box corresponds to a set of answer entities of the query. We show that conjunctions can be naturally represented as intersections of boxes and also prove a negative result that handling disjunctions would require embedding with dimension proportional to the number of KG entities. However, we show that by transforming queries into a Disjunctive Normal Form, query2box is capable of handling arbitrary logical queries with $\\wedge$, $\\vee$, $\\exists$ in a scalable manner. We demonstrate the effectiveness of query2box on two large KGs and show that query2box achieves up to 25% relative improvement over the state of the art",
    "checked": true,
    "id": "66f7823be7a0adcd8ed3ecebf00e53daf45bc6bc",
    "semantic_title": "query2box: reasoning over knowledge graphs in vector space using box embeddings",
    "citation_count": 306,
    "authors": []
  },
  "https://openreview.net/forum?id=Byx4NkrtDS": {
    "title": "Implementing Inductive bias for different navigation tasks through diverse RNN attrractors",
    "volume": "poster",
    "abstract": "Navigation is crucial for animal behavior and is assumed to require an internal representation of the external environment, termed a cognitive map. The precise form of this representation is often considered to be a metric representation of space. An internal representation, however, is judged by its contribution to performance on a given task, and may thus vary between different types of navigation tasks. Here we train a recurrent neural network that controls an agent performing several navigation tasks in a simple environment. To focus on internal representations, we split learning into a task-agnostic pre-training stage that modifies internal connectivity and a task-specific Q learning stage that controls the network's output. We show that pre-training shapes the attractor landscape of the networks, leading to either a continuous attractor, discrete attractors or a disordered state. These structures induce bias onto the Q-Learning phase, leading to a performance pattern across the tasks corresponding to metric and topological regularities. Our results show that, in recurrent networks, inductive bias takes the form of attractor landscapes -- which can be shaped by pre-training and analyzed using dynamical systems methods. Furthermore, we demonstrate that non-metric representations are useful for navigation tasks",
    "checked": true,
    "id": "44a440d630970086eacf9c651dfe93052028a292",
    "semantic_title": "implementing inductive bias for different navigation tasks through diverse rnn attrractors",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=BJgWE1SFwS": {
    "title": "PCMC-Net: Feature-based Pairwise Choice Markov Chains",
    "volume": "poster",
    "abstract": "Pairwise Choice Markov Chains (PCMC) have been recently introduced to overcome limitations of choice models based on traditional axioms unable to express empirical observations from modern behavior economics like context effects occurring when a choice between two options is altered by adding a third alternative. The inference approach that estimates the transition rates between each possible pair of alternatives via maximum likelihood suffers when the examples of each alternative are scarce and is inappropriate when new alternatives can be observed at test time. In this work, we propose an amortized inference approach for PCMC by embedding its definition into a neural network that represents transition rates as a function of the alternatives' and individual's features. We apply our construction to the complex case of airline itinerary booking where singletons are common (due to varying prices and individual-specific itineraries), and context effects and behaviors strongly dependent on market segments are observed. Experiments show our network significantly outperforming, in terms of prediction accuracy and logarithmic loss, feature engineered standard and latent class Multinomial Logit models as well as recent machine learning approaches",
    "checked": true,
    "id": "04d640d52063f2c4ab0e26c3395bd26f35f4c0d6",
    "semantic_title": "pcmc-net: feature-based pairwise choice markov chains",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=B1gZV1HYvS": {
    "title": "Multi-Agent Interactions Modeling with Correlated Policies",
    "volume": "poster",
    "abstract": "In multi-agent systems, complex interacting behaviors arise due to the high correlations among agents. However, previous work on modeling multi-agent interactions from demonstrations is primarily constrained by assuming the independence among policies and their reward structures. In this paper, we cast the multi-agent interactions modeling problem into a multi-agent imitation learning framework with explicit modeling of correlated policies by approximating opponents' policies, which can recover agents' policies that can regenerate similar interactions. Consequently, we develop a Decentralized Adversarial Imitation Learning algorithm with Correlated policies (CoDAIL), which allows for decentralized training and execution. Various experiments demonstrate that CoDAIL can better regenerate complex interactions close to the demonstrators and outperforms state-of-the-art multi-agent imitation learning methods. Our code is available at \\url{https://github.com/apexrl/CoDAIL}",
    "checked": true,
    "id": "483579d2c9a8efa4914626b8ec897c47decf17a2",
    "semantic_title": "multi-agent interactions modeling with correlated policies",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=HylxE1HKwS": {
    "title": "Once-for-All: Train One Network and Specialize it for Efficient Deployment",
    "volume": "poster",
    "abstract": "We address the challenging problem of efficient inference across many devices and resource constraints, especially on edge devices. Conventional approaches either manually design or use neural architecture search (NAS) to find a specialized neural network and train it from scratch for each case, which is computationally prohibitive (causing $CO_2$ emission as much as 5 cars' lifetime) thus unscalable. In this work, we propose to train a once-for-all (OFA) network that supports diverse architectural settings by decoupling training and search, to reduce the cost. We can quickly get a specialized sub-network by selecting from the OFA network without additional training. To efficiently train OFA networks, we also propose a novel progressive shrinking algorithm, a generalized pruning method that reduces the model size across many more dimensions than pruning (depth, width, kernel size, and resolution). It can obtain a surprisingly large number of sub-networks ($> 10^{19}$) that can fit different hardware platforms and latency constraints while maintaining the same level of accuracy as training independently. On diverse edge devices, OFA consistently outperforms state-of-the-art (SOTA) NAS methods (up to 4.0% ImageNet top1 accuracy improvement over MobileNetV3, or same accuracy but 1.5x faster than MobileNetV3, 2.6x faster than EfficientNet w.r.t measured latency) while reducing many orders of magnitude GPU hours and $CO_2$ emission. In particular, OFA achieves a new SOTA 80.0% ImageNet top-1 accuracy under the mobile setting ($<$600M MACs). OFA is the winning solution for the 3rd Low Power Computer Vision Challenge (LPCVC), DSP classification track and the 4th LPCVC, both classification track and detection track. Code and 50 pre-trained models (for many devices & many latency constraints) are released at https://github.com/mit-han-lab/once-for-all",
    "checked": true,
    "id": "7823292e5c4b05c47af91ab6ddf671a0da709e82",
    "semantic_title": "once for all: train one network and specialize it for efficient deployment",
    "citation_count": 1278,
    "authors": []
  },
  "https://openreview.net/forum?id=H1lxVyStPH": {
    "title": "Generalized Convolutional Forest Networks for Domain Generalization and Visual Recognition",
    "volume": "poster",
    "abstract": "When constructing random forests, it is of prime importance to ensure high accuracy and low correlation of individual tree classifiers for good performance. Nevertheless, it is typically difficult for existing random forest methods to strike a good balance between these conflicting factors. In this work, we propose a generalized convolutional forest networks to learn a feature space to maximize the strength of individual tree classifiers while minimizing the respective correlation. The feature space is iteratively constructed by a probabilistic triplet sampling method based on the distribution obtained from the splits of the random forest. The sampling process is designed to pull the data of the same label together for higher strength and push away the data frequently falling to the same leaf nodes. We perform extensive experiments on five image classification and two domain generalization datasets with ResNet-50 and DenseNet-161 backbone networks. Experimental results show that the proposed algorithm performs favorably against state-of-the-art methods",
    "checked": true,
    "id": "b3fd1df1e6d8e10dc17726bf9277834d1b235843",
    "semantic_title": "generalized convolutional forest networks for domain generalization and visual recognition",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=Sye0XkBKvS": {
    "title": "SNODE: Spectral Discretization of Neural ODEs for System Identification",
    "volume": "poster",
    "abstract": "This paper proposes the use of spectral element methods \\citep{canuto_spectral_1988} for fast and accurate training of Neural Ordinary Differential Equations (ODE-Nets; \\citealp{Chen2018NeuralOD}) for system identification. This is achieved by expressing their dynamics as a truncated series of Legendre polynomials. The series coefficients, as well as the network weights, are computed by minimizing the weighted sum of the loss function and the violation of the ODE-Net dynamics. The problem is solved by coordinate descent that alternately minimizes, with respect to the coefficients and the weights, two unconstrained sub-problems using standard backpropagation and gradient methods. The resulting optimization scheme is fully time-parallel and results in a low memory footprint. Experimental comparison to standard methods, such as backpropagation through explicit solvers and the adjoint technique \\citep{Chen2018NeuralOD}, on training surrogate models of small and medium-scale dynamical systems shows that it is at least one order of magnitude faster at reaching a comparable value of the loss function. The corresponding testing MSE is one order of magnitude smaller as well, suggesting generalization capabilities increase",
    "checked": true,
    "id": "79a13802beabf29155cc8b554a140fa215d77722",
    "semantic_title": "snode: spectral discretization of neural odes for system identification",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=BJl07ySKvS": {
    "title": "Guiding Program Synthesis by Learning to Generate Examples",
    "volume": "poster",
    "abstract": "A key challenge of existing program synthesizers is ensuring that the synthesized program generalizes well. This can be difficult to achieve as the specification provided by the end user is often limited, containing as few as one or two input-output examples. In this paper we address this challenge via an iterative approach that finds ambiguities in the provided specification and learns to resolve these by generating additional input-output examples. The main insight is to reduce the problem of selecting which program generalizes well to the simpler task of deciding which output is correct. As a result, to train our probabilistic models, we can take advantage of the large amounts of data in the form of program outputs, which are often much easier to obtain than the corresponding ground-truth programs",
    "checked": true,
    "id": "d1b79dc678365c94a8ebdd0015efc154cb482a90",
    "semantic_title": "guiding program synthesis by learning to generate examples",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=rklTmyBKPH": {
    "title": "Fast Neural Network Adaptation via Parameter Remapping and Architecture Search",
    "volume": "poster",
    "abstract": "Deep neural networks achieve remarkable performance in many computer vision tasks. Most state-of-the-art~(SOTA) semantic segmentation and object detection approaches reuse neural network architectures designed for image classification as the backbone, commonly pre-trained on ImageNet. However, performance gains can be achieved by designing network architectures specifically for detection and segmentation, as shown by recent neural architecture search (NAS) research for detection and segmentation. One major challenge though, is that ImageNet pre-training of the search space representation (a.k.a. super network) or the searched networks incurs huge computational cost. In this paper, we propose a Fast Neural Network Adaptation (FNA) method, which can adapt both the architecture and parameters of a seed network (e.g. a high performing manually designed backbone) to become a network with different depth, width, or kernels via a Parameter Remapping technique, making it possible to utilize NAS for detection/segmentation tasks a lot more efficiently. In our experiments, we conduct FNA on MobileNetV2 to obtain new networks for both segmentation and detection that clearly out-perform existing networks designed both manually and by NAS. The total computation cost of FNA is significantly less than SOTA segmentation/detection NAS approaches: 1737$\\times$ less than DPC, 6.8$\\times$ less than Auto-DeepLab and 7.4$\\times$ less than DetNAS. The code is available at https://github.com/JaminFong/FNA",
    "checked": true,
    "id": "25c156c5eb1a32cd3ca0255ace30626812fbc830",
    "semantic_title": "fast neural network adaptation via parameter remapping and architecture search",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=rkl3m1BFDB": {
    "title": "Exploratory Not Explanatory: Counterfactual Analysis of Saliency Maps for Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "Saliency maps are frequently used to support explanations of the behavior of deep reinforcement learning (RL) agents. However, a review of how saliency maps are used in practice indicates that the derived explanations are often unfalsifiable and can be highly subjective. We introduce an empirical approach grounded in counterfactual reasoning to test the hypotheses generated from saliency maps and assess the degree to which they correspond to the semantics of RL environments. We use Atari games, a common benchmark for deep RL, to evaluate three types of saliency maps. Our results show the extent to which existing claims about Atari games can be evaluated and suggest that saliency maps are best viewed as an exploratory tool rather than an explanatory tool",
    "checked": true,
    "id": "61b8b2c8e755c44882c606242d928c33d7db96b6",
    "semantic_title": "exploratory not explanatory: counterfactual analysis of saliency maps for deep reinforcement learning",
    "citation_count": 91,
    "authors": []
  },
  "https://openreview.net/forum?id=SyljQyBFDH": {
    "title": "Meta-Learning Deep Energy-Based Memory Models",
    "volume": "poster",
    "abstract": "We study the problem of learning an associative memory model -- a system which is able to retrieve a remembered pattern based on its distorted or incomplete version. Attractor networks provide a sound model of associative memory: patterns are stored as attractors of the network dynamics and associative retrieval is performed by running the dynamics starting from a query pattern until it converges to an attractor. In such models the dynamics are often implemented as an optimization procedure that minimizes an energy function, such as in the classical Hopfield network. In general it is difficult to derive a writing rule for a given dynamics and energy that is both compressive and fast. Thus, most research in energy-based memory has been limited either to tractable energy models not expressive enough to handle complex high-dimensional objects such as natural images, or to models that do not offer fast writing. We present a novel meta-learning approach to energy-based memory models (EBMM) that allows one to use an arbitrary neural architecture as an energy model and quickly store patterns in its weights. We demonstrate experimentally that our EBMM approach can build compressed memories for synthetic and natural data, and is capable of associative retrieval that outperforms existing memory systems in terms of the reconstruction error and compression rate",
    "checked": true,
    "id": "3839e3b4a64fd60e16861655acb4cb788730b609",
    "semantic_title": "meta-learning deep energy-based memory models",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=HkxdQkSYDB": {
    "title": "Graph Convolutional Reinforcement Learning",
    "volume": "poster",
    "abstract": "Learning to cooperate is crucially important in multi-agent environments. The key is to understand the mutual interplay between agents. However, multi-agent environments are highly dynamic, where agents keep moving and their neighbors change quickly. This makes it hard to learn abstract representations of mutual interplay between agents. To tackle these difficulties, we propose graph convolutional reinforcement learning, where graph convolution adapts to the dynamics of the underlying graph of the multi-agent environment, and relation kernels capture the interplay between agents by their relation representations. Latent features produced by convolutional layers from gradually increased receptive fields are exploited to learn cooperation, and cooperation is further improved by temporal relation regularization for consistency. Empirically, we show that our method substantially outperforms existing methods in a variety of cooperative scenarios",
    "checked": true,
    "id": "1796f6f928079f935748dad59f6ed6fdbba960c8",
    "semantic_title": "graph convolutional reinforcement learning",
    "citation_count": 337,
    "authors": []
  },
  "https://openreview.net/forum?id=rygGQyrFvH": {
    "title": "The Curious Case of Neural Text Degeneration",
    "volume": "poster",
    "abstract": "Despite considerable advances in neural language modeling, it remains an open question what the best decoding strategy is for text generation from a language model (e.g. to generate a story). The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, maximization-based decoding methods such as beam search lead to degeneration — output text that is bland, incoherent, or gets stuck in repetitive loops. To address this we propose Nucleus Sampling, a simple but effective method to draw considerably higher quality text out of neural language models than previous decoding strategies. Our approach avoids text degeneration by truncating the unreliable tail of the probability distribution, sampling from the dynamic nucleus of tokens containing the vast majority of the probability mass. To properly examine current maximization-based and stochastic decoding methods, we compare generations from each of these methods to the distribution of human text along several axes such as likelihood, diversity, and repetition. Our results show that (1) maximization is an inappropriate decoding objective for open-ended text generation, (2) the probability distributions of the best current language models have an unreliable tail which needs to be truncated during generation and (3) Nucleus Sampling is currently the best available decoding strategy for generating long-form text that is both high-quality — as measured by human evaluation — and as diverse as human-written text",
    "checked": true,
    "id": "cf4aa38ae31b43fd07abe13b4ffdb265babb7be1",
    "semantic_title": "the curious case of neural text degeneration",
    "citation_count": 3164,
    "authors": []
  },
  "https://openreview.net/forum?id=r1xCMyBtPS": {
    "title": "Multilingual Alignment of Contextual Word Representations",
    "volume": "poster",
    "abstract": "We propose procedures for evaluating and strengthening contextual embedding alignment and show that they are useful in analyzing and improving multilingual BERT. In particular, after our proposed alignment procedure, BERT exhibits significantly improved zero-shot performance on XNLI compared to the base model, remarkably matching pseudo-fully-supervised translate-train models for Bulgarian and Greek. Further, to measure the degree of alignment, we introduce a contextual version of word retrieval and show that it correlates well with downstream zero-shot transfer. Using this word retrieval task, we also analyze BERT and find that it exhibits systematic deficiencies, e.g. worse alignment for open-class parts-of-speech and word pairs written in different scripts, that are corrected by the alignment procedure. These results support contextual alignment as a useful concept for understanding large multilingual pre-trained models",
    "checked": true,
    "id": "d592007d1c106fe1217604eb35664c7a5f07cb32",
    "semantic_title": "multilingual alignment of contextual word representations",
    "citation_count": 193,
    "authors": []
  },
  "https://openreview.net/forum?id=HyxnMyBKwB": {
    "title": "The Gambler's Problem and Beyond",
    "volume": "poster",
    "abstract": "We analyze the Gambler's problem, a simple reinforcement learning problem where the gambler has the chance to double or lose their bets until the target is reached. This is an early example introduced in the reinforcement learning textbook by Sutton and Barto (2018), where they mention an interesting pattern of the optimal value function with high-frequency components and repeating non-smooth points. It is however without further investigation. We provide the exact formula for the optimal value function for both the discrete and the continuous cases. Though simple as it might seem, the value function is pathological: fractal, self-similar, derivative taking either zero or infinity, not smooth on any interval, and not written as elementary functions. It is in fact one of the generalized Cantor functions, where it holds a complexity that has been uncharted thus far. Our analyses could lead insights into improving value function approximation, gradient-based algorithms, and Q-learning, in real applications and implementations",
    "checked": true,
    "id": "9d2967958f1acafe2fadd7a0524d5752b5bd63dc",
    "semantic_title": "the gambler's problem and beyond",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S1esMkHYPr": {
    "title": "GraphAF: a Flow-based Autoregressive Model for Molecular Graph Generation",
    "volume": "poster",
    "abstract": "Molecular graph generation is a fundamental problem for drug discovery and has been attracting growing attention. The problem is challenging since it requires not only generating chemically valid molecular structures but also optimizing their chemical properties in the meantime. Inspired by the recent progress in deep generative models, in this paper we propose a flow-based autoregressive model for graph generation called GraphAF. GraphAF combines the advantages of both autoregressive and flow-based approaches and enjoys: (1) high model flexibility for data density estimation; (2) efficient parallel computation for training; (3) an iterative sampling process, which allows leveraging chemical domain knowledge for valency checking. Experimental results show that GraphAF is able to generate 68\\% chemically valid molecules even without chemical knowledge rules and 100\\% valid molecules with chemical rules. The training process of GraphAF is two times faster than the existing state-of-the-art approach GCPN. After fine-tuning the model for goal-directed property optimization with reinforcement learning, GraphAF achieves state-of-the-art performance on both chemical property optimization and constrained property optimization",
    "checked": true,
    "id": "036d743c7ca1e513adf0a91594fc8111e03dc30c",
    "semantic_title": "graphaf: a flow-based autoregressive model for molecular graph generation",
    "citation_count": 437,
    "authors": []
  },
  "https://openreview.net/forum?id=ByedzkrKvH": {
    "title": "Double Neural Counterfactual Regret Minimization",
    "volume": "poster",
    "abstract": "Counterfactual regret minimization (CFR) is a fundamental and effective technique for solving Imperfect Information Games (IIG). However, the original CFR algorithm only works for discrete states and action spaces, and the resulting strategy is maintained as a tabular representation. Such tabular representation limits the method from being directly applied to large games. In this paper, we propose a double neural representation for the IIGs, where one neural network represents the cumulative regret, and the other represents the average strategy. Such neural representations allow us to avoid manual game abstraction and carry out end-to-end optimization. To make the learning efficient, we also developed several novel techniques including a robust sampling method and a mini-batch Monte Carlo Counterfactual Regret Minimization (MCCFR) method, which may be of independent interests. Empirically, on games tractable to tabular approaches, neural strategies trained with our algorithm converge comparably to their tabular counterparts, and significantly outperform those based on deep reinforcement learning. On extremely large games with billions of decision nodes, our approach achieved strong performance while using hundreds of times less memory than the tabular CFR. On head-to-head matches of hands-up no-limit texas hold'em, our neural agent beat the strong agent ABS-CFR by $9.8\\pm4.1$ chips per game. It's a successful application of neural CFR in large games",
    "checked": true,
    "id": "b7a30e04aeb0ee7bb1a739d94c5dd785d00f3b25",
    "semantic_title": "double neural counterfactual regret minimization",
    "citation_count": 52,
    "authors": []
  },
  "https://openreview.net/forum?id=BJgQfkSYDS": {
    "title": "Neural Policy Gradient Methods: Global Optimality and Rates of Convergence",
    "volume": "poster",
    "abstract": "Policy gradient methods with actor-critic schemes demonstrate tremendous empirical successes, especially when the actors and critics are parameterized by neural networks. However, it remains less clear whether such \"neural\" policy gradient methods converge to globally optimal policies and whether they even converge at all. We answer both the questions affirmatively in the overparameterized regime. In detail, we prove that neural natural policy gradient converges to a globally optimal policy at a sublinear rate. Also, we show that neural vanilla policy gradient converges sublinearly to a stationary point. Meanwhile, by relating the suboptimality of the stationary points to the~representation power of neural actor and critic classes, we prove the global optimality of all stationary points under mild regularity conditions. Particularly, we show that a key to the global optimality and convergence is the \"compatibility\" between the actor and critic, which is ensured by sharing neural architectures and random initializations across the actor and critic. To the best of our knowledge, our analysis establishes the first global optimality and convergence guarantees for neural policy gradient methods",
    "checked": true,
    "id": "893a16bfc2e14c4eec45470f76083632470fc41c",
    "semantic_title": "neural policy gradient methods: global optimality and rates of convergence",
    "citation_count": 211,
    "authors": []
  },
  "https://openreview.net/forum?id=rJgzzJHtDB": {
    "title": "Triple Wins: Boosting Accuracy, Robustness and Efficiency Together by Enabling Input-Adaptive Inference",
    "volume": "poster",
    "abstract": "Deep networks were recently suggested to face the odds between accuracy (on clean natural images) and robustness (on adversarially perturbed images) (Tsipras et al., 2019). Such a dilemma is shown to be rooted in the inherently higher sample complexity (Schmidt et al., 2018) and/or model capacity (Nakkiran, 2019), for learning a high-accuracy and robust classifier. In view of that, give a classification task, growing the model capacity appears to help draw a win-win between accuracy and robustness, yet at the expense of model size and latency, therefore posing challenges for resource-constrained applications. Is it possible to co-design model accuracy, robustness and efficiency to achieve their triple wins? This paper studies multi-exit networks associated with input-adaptive efficient inference, showing their strong promise in achieving a \"sweet point\" in co-optimizing model accuracy, robustness, and efficiency. Our proposed solution, dubbed Robust Dynamic Inference Networks (RDI-Nets), allows for each input (either clean or adversarial) to adaptively choose one of the multiple output layers (early branches or the final one) to output its prediction. That multi-loss adaptivity adds new variations and flexibility to adversarial attacks and defenses, on which we present a systematical investigation. We show experimentally that by equipping existing backbones with such robust adaptive inference, the resulting RDI-Nets can achieve better accuracy and robustness, yet with over 30% computational savings, compared to the defended original models",
    "checked": true,
    "id": "70a6f1820eec8152f4af826d9adf61f442a24743",
    "semantic_title": "triple wins: boosting accuracy, robustness and efficiency together by enabling input-adaptive inference",
    "citation_count": 84,
    "authors": []
  },
  "https://openreview.net/forum?id=SJlbGJrtDB": {
    "title": "Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers",
    "volume": "poster",
    "abstract": "We present a novel network pruning algorithm called Dynamic Sparse Training that can jointly ﬁnd the optimal network parameters and sparse network structure in a uniﬁed optimization process with trainable pruning thresholds. These thresholds can have ﬁne-grained layer-wise adjustments dynamically via backpropagation. We demonstrate that our dynamic sparse training algorithm can easily train very sparse neural network models with little performance loss using the same training epochs as dense models. Dynamic Sparse Training achieves prior art performance compared with other sparse training algorithms on various network architectures. Additionally, we have several surprising observations that provide strong evidence to the effectiveness and efﬁciency of our algorithm. These observations reveal the underlying problems of traditional three-stage pruning algorithms and present the potential guidance provided by our algorithm to the design of more compact network architectures",
    "checked": true,
    "id": "573cf589caa7e131c17fa5c45fa817f3dce6cfa5",
    "semantic_title": "dynamic sparse training: find efficient sparse network from scratch with trainable masked layers",
    "citation_count": 121,
    "authors": []
  },
  "https://openreview.net/forum?id=rkllGyBFPH": {
    "title": "Beyond Linearization: On Quadratic and Higher-Order Approximation of Wide Neural Networks",
    "volume": "poster",
    "abstract": "Recent theoretical work has established connections between over-parametrized neural networks and linearized models governed by the Neural Tangent Kernels (NTKs). NTK theory leads to concrete convergence and generalization results, yet the empirical performance of neural networks are observed to exceed their linearized models, suggesting insufficiency of this theory. Towards closing this gap, we investigate the training of over-parametrized neural networks that are beyond the NTK regime yet still governed by the Taylor expansion of the network. We bring forward the idea of randomizing the neural networks, which allows them to escape their NTK and couple with quadratic models. We show that the optimization landscape of randomized two-layer networks are nice and amenable to escaping-saddle algorithms. We prove concrete generalization and expressivity results on these randomized networks, which lead to sample complexity bounds (of learning certain simple functions) that match the NTK and can in addition be better by a dimension factor when mild distributional assumptions are present. We demonstrate that our randomization technique can be generalized systematically beyond the quadratic case, by using it to find networks that are coupled with higher-order terms in their Taylor series",
    "checked": true,
    "id": "5cd1eb529e01dc07c4c7844696b4514403264834",
    "semantic_title": "beyond linearization: on quadratic and higher-order approximation of wide neural networks",
    "citation_count": 116,
    "authors": []
  },
  "https://openreview.net/forum?id=HyeJf1HKvS": {
    "title": "Deep Graph Matching Consensus",
    "volume": "poster",
    "abstract": "This work presents a two-stage neural architecture for learning and refining structural correspondences between graphs. First, we use localized node embeddings computed by a graph neural network to obtain an initial ranking of soft correspondences between nodes. Secondly, we employ synchronous message passing networks to iteratively re-rank the soft correspondences to reach a matching consensus in local neighborhoods between graphs. We show, theoretically and empirically, that our message passing scheme computes a well-founded measure of consensus for corresponding neighborhoods, which is then used to guide the iterative re-ranking process. Our purely local and sparsity-aware architecture scales well to large, real-world inputs while still being able to recover global correspondences consistently. We demonstrate the practical effectiveness of our method on real-world tasks from the fields of computer vision and entity alignment between knowledge graphs, on which we improve upon the current state-of-the-art",
    "checked": true,
    "id": "339d6340960fe77bc597cad48eecf45fc237e742",
    "semantic_title": "deep graph matching consensus",
    "citation_count": 210,
    "authors": []
  },
  "https://openreview.net/forum?id=B1lJzyStvS": {
    "title": "Self-Supervised Learning of Appliance Usage",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3ab699bc0208256d01f7cf5cb1290e2632279534",
    "semantic_title": "self-supervised learning of appliance usage",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=Hygab1rKDS": {
    "title": "Quantum Algorithms for Deep Convolutional Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3663fb604d41a45c3592d4deee647dd62b07576b",
    "semantic_title": "quantum algorithms for deep convolutional neural networks",
    "citation_count": 135,
    "authors": []
  },
  "https://openreview.net/forum?id=SJeY-1BKDS": {
    "title": "Understanding l4-based Dictionary Learning: Interpretation, Stability, and Robustness",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f4ebde1ba8944f136a654b0bcd468afec23556f7",
    "semantic_title": "understanding l4-based dictionary learning: interpretation, stability, and robustness",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=ryxmb1rKDS": {
    "title": "Symplectic ODE-Net: Learning Hamiltonian Dynamics with Control",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "329fb2f19fd1cecbb0a4ff317a4dbd833e43f903",
    "semantic_title": "symplectic ode-net: learning hamiltonian dynamics with control",
    "citation_count": 271,
    "authors": []
  },
  "https://openreview.net/forum?id=H1laeJrKDB": {
    "title": "Controlling generative models with continuous factors of variations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7809846c6ab7437976a2ef495204d745aad77d05",
    "semantic_title": "controlling generative models with continuous factors of variations",
    "citation_count": 114,
    "authors": []
  },
  "https://openreview.net/forum?id=Hke3gyHYwH": {
    "title": "Simple and Effective Regularization Methods for Training on Noisily Labeled Data with Generalization Guarantee",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e625dac27fdcd1b81905884c5a88f6c87f2632c6",
    "semantic_title": "simple and effective regularization methods for training on noisily labeled data with generalization guarantee",
    "citation_count": 128,
    "authors": []
  },
  "https://openreview.net/forum?id=rJlnxkSYPS": {
    "title": "Unsupervised Clustering using Pseudo-semi-supervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1d18377dd32d8374e4bce701aefc2dcfe78649f6",
    "semantic_title": "unsupervised clustering using pseudo-semi-supervised learning",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=rkecl1rtwB": {
    "title": "PairNorm: Tackling Oversmoothing in GNNs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "38293873cce681b857b3c1d73f5590f641f3dc42",
    "semantic_title": "pairnorm: tackling oversmoothing in gnns",
    "citation_count": 508,
    "authors": []
  },
  "https://openreview.net/forum?id=S1ltg1rFDS": {
    "title": "Black-box Off-policy Estimation for Infinite-Horizon Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9a08c1e181ae42381ef0bbb8199e4832621ff2e3",
    "semantic_title": "black-box off-policy estimation for infinite-horizon reinforcement learning",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=SkeFl1HKwr": {
    "title": "Empirical Studies on the Properties of Linear Regions in Deep Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bab2962d0873cbf754f072a84b7a5a3a6bd7a342",
    "semantic_title": "empirical studies on the properties of linear regions in deep neural networks",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=rJxtgJBKDr": {
    "title": "SNOW: Subscribing to Knowledge via Channel Pooling for Transfer & Lifelong Learning of Convolutional Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "15a4dce41f61f20ac4b817c2c5b0b928aed6dde9",
    "semantic_title": "snow: subscribing to knowledge via channel pooling for transfer & lifelong learning of convolutional neural networks",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=HJeOekHKwr": {
    "title": "Smoothness and Stability in GANs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "531836a1c3fbbf10eba5375d8558f218cdb9805e",
    "semantic_title": "smoothness and stability in gans",
    "citation_count": 56,
    "authors": []
  },
  "https://openreview.net/forum?id=r1lOgyrKDS": {
    "title": "Adaptive Correlated Monte Carlo for Contextual Categorical Sequence Generation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "aa950171b25c05466d9d3cc58dff3b9ea9882e4a",
    "semantic_title": "adaptive correlated monte carlo for contextual categorical sequence generation",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=BJewlyStDr": {
    "title": "On Bonus Based Exploration Methods In The Arcade Learning Environment",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2b2735cffb0d2321a456363880ff5671e80df4cb",
    "semantic_title": "on bonus based exploration methods in the arcade learning environment",
    "citation_count": 59,
    "authors": []
  },
  "https://openreview.net/forum?id=BJeGlJStPr": {
    "title": "IMPACT: Importance Weighted Asynchronous Architectures with Clipped Target Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "092b08dead33f73b3c9800b1d016eb776c273a7d",
    "semantic_title": "impact: importance weighted asynchronous architectures with clipped target networks",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=r1lZgyBYwS": {
    "title": "HiLLoC: lossless image compression with hierarchical latent variable models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "421271a93517c2c3f7ad9096b8c9455c16ae85a1",
    "semantic_title": "hilloc: lossless image compression with hierarchical latent variable models",
    "citation_count": 56,
    "authors": []
  },
  "https://openreview.net/forum?id=r1gelyrtwH": {
    "title": "Physics-aware Difference Graph Networks for Sparsely-Observed Dynamics",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f3819aee0294502d8705d001066789b493ee3792",
    "semantic_title": "physics-aware difference graph networks for sparsely-observed dynamics",
    "citation_count": 60,
    "authors": []
  },
  "https://openreview.net/forum?id=HygegyrYwH": {
    "title": "Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow ReLU networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0fdf1a213ed08012d5d21067544b860f40c08e8f",
    "semantic_title": "polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow relu networks",
    "citation_count": 177,
    "authors": []
  },
  "https://openreview.net/forum?id=Bke61krFvS": {
    "title": "Learning representations for binary-classification without backpropagation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "022a664b5ab1fcdf8154ea041086bbfc177095ae",
    "semantic_title": "learning representations for binary-classification without backpropagation",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=B1gskyStwr": {
    "title": "Frequency-based Search-control in Dyna",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "eb5f4f0cb8503beefdf8a5efd5061f61cc64764e",
    "semantic_title": "frequency-based search-control in dyna",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=Skxuk1rFwB": {
    "title": "Towards Stable and Efficient Training of Verifiably Robust Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ff22e140a0423f1cf0595d213f36402668084014",
    "semantic_title": "towards stable and efficient training of verifiably robust neural networks",
    "citation_count": 347,
    "authors": []
  },
  "https://openreview.net/forum?id=HJx81ySKwr": {
    "title": "Iterative energy-based projection on a normal data manifold for anomaly localization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d9d7ab13ce305ccee309c989a2341d72b1252070",
    "semantic_title": "iterative energy-based projection on a normal data manifold for anomaly localization",
    "citation_count": 145,
    "authors": []
  },
  "https://openreview.net/forum?id=ByxGkySKwH": {
    "title": "Towards neural networks that provably know when they don't know",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a41efad7ff629b8db586dbd140fe1464532dc406",
    "semantic_title": "towards neural networks that provably know when they don't know",
    "citation_count": 141,
    "authors": []
  },
  "https://openreview.net/forum?id=Sklf1yrYDr": {
    "title": "BatchEnsemble: an Alternative Approach to Efficient Ensemble and Lifelong Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b145ea2049648535d6081407ebd315b072248183",
    "semantic_title": "batchensemble: an alternative approach to efficient ensemble and lifelong learning",
    "citation_count": 491,
    "authors": []
  },
  "https://openreview.net/forum?id=rJeW1yHYwH": {
    "title": "Inductive representation learning on temporal graphs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1f58e8d4c827037d4c2a1afc695a88704e088beb",
    "semantic_title": "inductive representation learning on temporal graphs",
    "citation_count": 622,
    "authors": []
  },
  "https://openreview.net/forum?id=SJgaRA4FPH": {
    "title": "Generative Models for Effective ML on Private, Decentralized Datasets",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7306c2579f49d3ac3a5377d9b594788711cb3f57",
    "semantic_title": "generative models for effective ml on private, decentralized datasets",
    "citation_count": 148,
    "authors": []
  },
  "https://openreview.net/forum?id=SkgsACVKPH": {
    "title": "Picking Winning Tickets Before Training by Preserving Gradient Flow",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e08557d60dc47af675b48688a3524a7b0a6eac84",
    "semantic_title": "picking winning tickets before training by preserving gradient flow",
    "citation_count": 615,
    "authors": []
  },
  "https://openreview.net/forum?id=rkgt0REKwS": {
    "title": "Curriculum Loss: Robust Learning and Generalization against Label Corruption",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "509f652d67a793a614633d4a5c236592d80b6522",
    "semantic_title": "curriculum loss: robust learning and generalization against label corruption",
    "citation_count": 172,
    "authors": []
  },
  "https://openreview.net/forum?id=HklUCCVKDB": {
    "title": "Uncertainty-guided Continual Learning with Bayesian Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c16244f3090ec8bb1d74edf71991b87c9f0ca802",
    "semantic_title": "uncertainty-guided continual learning with bayesian neural networks",
    "citation_count": 197,
    "authors": []
  },
  "https://openreview.net/forum?id=SJgmR0NKPr": {
    "title": "Training Recurrent Neural Networks Online by Learning Explicit State Variables",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0503c6822ca5a5706de16a112783bd8bddd95cc9",
    "semantic_title": "training recurrent neural networks online by learning explicit state variables",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=S1l-C0NtwS": {
    "title": "Cross-lingual Alignment vs Joint Training: A Comparative Study and A Simple Unified Framework",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5884948777dfc003ba49e1513420830616281839",
    "semantic_title": "cross-lingual alignment vs joint training: a comparative study and a simple unified framework",
    "citation_count": 79,
    "authors": []
  },
  "https://openreview.net/forum?id=HJgC60EtwB": {
    "title": "Robust Reinforcement Learning for Continuous Control with Model Misspecification",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6946ae0c23257586c12d01675e05167d74cb89fa",
    "semantic_title": "robust reinforcement learning for continuous control with model misspecification",
    "citation_count": 103,
    "authors": []
  },
  "https://openreview.net/forum?id=r1gRTCVFvB": {
    "title": "Decoupling Representation and Classifier for Long-Tailed Recognition",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "dcc4c760c3f1cb17f953c487190b735030c33b78",
    "semantic_title": "decoupling representation and classifier for long-tailed recognition",
    "citation_count": 1213,
    "authors": []
  },
  "https://openreview.net/forum?id=BJxt60VtPr": {
    "title": "Learning from Unlabelled Videos Using Contrastive Predictive Neural 3D Mapping",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ed98c56f9c5c224b33399b8fdbc379f99467d727",
    "semantic_title": "learning from unlabelled videos using contrastive predictive neural 3d mapping",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=H1guaREYPr": {
    "title": "From Inference to Generation: End-to-end Fully Self-supervised Generation of Human Face from Speech",
    "volume": "poster",
    "abstract": "This work seeks the possibility of generating the human face from voice solely based on the audio-visual data without any human-labeled annotations. To this end, we propose a multi-modal learning framework that links the inference stage and generation stage. First, the inference networks are trained to match the speaker identity between the two different modalities. Then the pre-trained inference networks cooperate with the generation network by giving conditional information about the voice. The proposed method exploits the recent development of GANs techniques and generates the human face directly from the speech waveform making our system fully end-to-end. We analyze the extent to which the network can naturally disentangle two latent factors that contribute to the generation of a face image one that comes directly from a speech signal and the other that is not related to it and explore whether the network can learn to generate natural human face image distribution by modeling these factors. Experimental results show that the proposed network can not only match the relationship between the human face and speech, but can also generate the high-quality human face sample conditioned on its speech. Finally, the correlation between the generated face and the corresponding speech is quantitatively measured to analyze the relationship between the two modalities",
    "checked": true,
    "id": "b44c4d94d2b1ce39ccf94677d69085a3bf5b2ff3",
    "semantic_title": "from inference to generation: end-to-end fully self-supervised generation of human face from speech",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=Hkx6hANtwH": {
    "title": "LambdaNet: Probabilistic Type Inference using Graph Neural Networks",
    "volume": "poster",
    "abstract": "As gradual typing becomes increasingly popular in languages like Python and TypeScript, there is a growing need to infer type annotations automatically. While type annotations help with tasks like code completion and static error catching, these annotations cannot be fully inferred by compilers and are tedious to annotate by hand. This paper proposes a probabilistic type inference scheme for TypeScript based on a graph neural network. Our approach first uses lightweight source code analysis to generate a program abstraction called a type dependency graph, which links type variables with logical constraints as well as name and usage information. Given this program abstraction, we then use a graph neural network to propagate information between related type variables and eventually make type predictions. Our neural architecture can predict both standard types, like number or string, as well as user-defined types that have not been encountered during training. Our experimental results show that our approach outperforms prior work in this space by 14% (absolute) on library types, while having the ability to make type predictions that are out of scope for existing techniques",
    "checked": true,
    "id": "29fc7ff02e68537a86d32978bfd2e8e45258e77d",
    "semantic_title": "lambdanet: probabilistic type inference using graph neural networks",
    "citation_count": 108,
    "authors": []
  },
  "https://openreview.net/forum?id=Skln2A4YDB": {
    "title": "Model-Augmented Actor-Critic: Backpropagating through Paths",
    "volume": "poster",
    "abstract": "Current model-based reinforcement learning approaches use the model simply as a learned black-box simulator to augment the data for policy optimization or value function learning. In this paper, we show how to make more effective use of the model by exploiting its differentiability. We construct a policy optimization algorithm that uses the pathwise derivative of the learned model and policy across future timesteps. Instabilities of learning across many timesteps are prevented by using a terminal value function, learning the policy in an actor-critic fashion. Furthermore, we present a derivation on the monotonic improvement of our objective in terms of the gradient error in the model and value function. We show that our approach (i) is consistently more sample efficient than existing state-of-the-art model-based algorithms, (ii) matches the asymptotic performance of model-free algorithms, and (iii) scales to long horizons, a regime where typically past model-based approaches have struggled",
    "checked": true,
    "id": "c37c2ded09f0e5ac181cdeebb141ec7c8b4641d6",
    "semantic_title": "model-augmented actor-critic: backpropagating through paths",
    "citation_count": 88,
    "authors": []
  },
  "https://openreview.net/forum?id=B1lj20NFDS": {
    "title": "Variational Autoencoders for Highly Multivariate Spatial Point Processes Intensities",
    "volume": "poster",
    "abstract": "Multivariate spatial point process models can describe heterotopic data over space. However, highly multivariate intensities are computationally challenging due to the curse of dimensionality. To bridge this gap, we introduce a declustering based hidden variable model that leads to an efficient inference procedure via a variational autoencoder (VAE). We also prove that this model is a generalization of the VAE-based model for collaborative filtering. This leads to an interesting application of spatial point process models to recommender systems. Experimental results show the method's utility on both synthetic data and real-world data sets",
    "checked": true,
    "id": "5e604b97415e3fce04bb6aaebfe8ac1bc126264e",
    "semantic_title": "variational autoencoders for highly multivariate spatial point processes intensities",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=HJeqhA4YDS": {
    "title": "Denoising and Regularization via Exploiting the Structural Bias of Convolutional Generators",
    "volume": "poster",
    "abstract": "Convolutional Neural Networks (CNNs) have emerged as highly successful tools for image generation, recovery, and restoration. A major contributing factor to this success is that convolutional networks impose strong prior assumptions about natural images. A surprising experiment that highlights this architectural bias towards natural images is that one can remove noise and corruptions from a natural image without using any training data, by simply fitting (via gradient descent) a randomly initialized, over-parameterized convolutional generator to the corrupted image. While this over-parameterized network can fit the corrupted image perfectly, surprisingly after a few iterations of gradient descent it generates an almost uncorrupted image. This intriguing phenomenon enables state-of-the-art CNN-based denoising and regularization of other inverse problems. In this paper, we attribute this effect to a particular architectural choice of convolutional networks, namely convolutions with fixed interpolating filters. We then formally characterize the dynamics of fitting a two-layer convolutional generator to a noisy signal and prove that early-stopped gradient descent denoises/regularizes. Our proof relies on showing that convolutional generators fit the structured part of an image significantly faster than the corrupted portion",
    "checked": true,
    "id": "dda678ae5abcd3b522ff195705a80ecb3f58f043",
    "semantic_title": "denoising and regularization via exploiting the structural bias of convolutional generators",
    "citation_count": 83,
    "authors": []
  },
  "https://openreview.net/forum?id=SJgdnAVKDH": {
    "title": "Revisiting Self-Training for Neural Sequence Generation",
    "volume": "poster",
    "abstract": "Self-training is one of the earliest and simplest semi-supervised methods. The key idea is to augment the original labeled dataset with unlabeled data paired with the model's prediction (i.e. the pseudo-parallel data). While self-training has been extensively studied on classification problems, in complex sequence generation tasks (e.g. machine translation) it is still unclear how self-training works due to the compositionality of the target space. In this work, we first empirically show that self-training is able to decently improve the supervised baseline on neural sequence generation tasks. Through careful examination of the performance gains, we find that the perturbation on the hidden states (i.e. dropout) is critical for self-training to benefit from the pseudo-parallel data, which acts as a regularizer and forces the model to yield close predictions for similar unlabeled inputs. Such effect helps the model correct some incorrect predictions on unlabeled data. To further encourage this mechanism, we propose to inject noise to the input space, resulting in a noisy version of self-training. Empirical study on standard machine translation and text summarization benchmarks shows that noisy self-training is able to effectively utilize unlabeled data and improve the performance of the supervised baseline by a large margin",
    "checked": true,
    "id": "12442420adf1c36887fafd108f4b7f4fc822ae60",
    "semantic_title": "revisiting self-training for neural sequence generation",
    "citation_count": 272,
    "authors": []
  },
  "https://openreview.net/forum?id=Hklr204Fvr": {
    "title": "Towards a Deep Network Architecture for Structured Smoothness",
    "volume": "poster",
    "abstract": "We propose the Fixed Grouping Layer (FGL); a novel feedforward layer designed to incorporate the inductive bias of structured smoothness into a deep learning model. FGL achieves this goal by connecting nodes across layers based on spatial similarity. The use of structured smoothness, as implemented by FGL, is motivated by applications to structured spatial data, which is, in turn, motivated by domain knowledge. The proposed model architecture outperforms conventional neural network architectures across a variety of simulated and real datasets with structured smoothness",
    "checked": true,
    "id": "aed43e2dc7d50e236e04c4d77590661cc4ca8978",
    "semantic_title": "towards a deep network architecture for structured smoothness",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=HJxEhREKDH": {
    "title": "On the Global Convergence of Training Deep Linear ResNets",
    "volume": "poster",
    "abstract": "We study the convergence of gradient descent (GD) and stochastic gradient descent (SGD) for training $L$-hidden-layer linear residual networks (ResNets). We prove that for training deep residual networks with certain linear transformations at input and output layers, which are fixed throughout training, both GD and SGD with zero initialization on all hidden weights can converge to the global minimum of the training loss. Moreover, when specializing to appropriate Gaussian random linear transformations, GD and SGD provably optimize wide enough deep linear ResNets. Compared with the global convergence result of GD for training standard deep linear networks \\citep{du2019width}, our condition on the neural network width is sharper by a factor of $O(\\kappa L)$, where $\\kappa$ denotes the condition number of the covariance matrix of the training data. We further propose a modified identity input and output transformations, and show that a $(d+k)$-wide neural network is sufficient to guarantee the global convergence of GD/SGD, where $d,k$ are the input and output dimensions respectively",
    "checked": true,
    "id": "306a0b7befa12c6352ade07fe8d0c99a7a9dbb60",
    "semantic_title": "on the global convergence of training deep linear resnets",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=HJeVnCEKwH": {
    "title": "A Closer Look at the Optimization Landscapes of Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "Generative adversarial networks have been very successful in generative modeling, however they remain relatively challenging to train compared to standard deep neural networks. In this paper, we propose new visualization techniques for the optimization landscapes of GANs that enable us to study the game vector field resulting from the concatenation of the gradient of both players. Using these visualization techniques we try to bridge the gap between theory and practice by showing empirically that the training of GANs exhibits significant rotations around LSSP, similar to the one predicted by theory on toy examples. Moreover, we provide empirical evidence that GAN training seems to converge to a stable stationary point which is a saddle point for the generator loss, not a minimum, while still achieving excellent performance",
    "checked": true,
    "id": "ec09db517aa4399ebbb33fb3acc342178f2aca90",
    "semantic_title": "a closer look at the optimization landscapes of generative adversarial networks",
    "citation_count": 64,
    "authors": []
  },
  "https://openreview.net/forum?id=r1xGnA4Kvr": {
    "title": "Biologically inspired sleep algorithm for increased generalization and adversarial robustness in deep neural networks",
    "volume": "poster",
    "abstract": "Current artificial neural networks (ANNs) can perform and excel at a variety of tasks ranging from image classification to spam detection through training on large datasets of labeled data. While the trained network may perform well on similar testing data, inputs that differ even slightly from the training data may trigger unpredictable behavior. Due to this limitation, it is possible to design inputs with very small perturbations that can result in misclassification. These adversarial attacks present a security risk to deployed ANNs and indicate a divergence between how ANNs and humans perform classification. Humans are robust at behaving in the presence of noise and are capable of correctly classifying objects that are noisy, blurred, or otherwise distorted. It has been hypothesized that sleep promotes generalization of knowledge and improves robustness against noise in animals and humans. In this work, we utilize a biologically inspired sleep phase in ANNs and demonstrate the benefit of sleep on defending against adversarial attacks as well as in increasing ANN classification robustness. We compare the sleep algorithm's performance on various robustness tasks with two previously proposed adversarial defenses - defensive distillation and fine-tuning. We report an increase in robustness after sleep phase to adversarial attacks as well as to general image distortions for three datasets: MNIST, CUB200, and a toy dataset. Overall, these results demonstrate the potential for biologically inspired solutions to solve existing problems in ANNs and guide the development of more robust, human-like ANNs",
    "checked": true,
    "id": "161446f6c04f0451bf77cfc8550a2e654e056571",
    "semantic_title": "biologically inspired sleep algorithm for increased generalization and adversarial robustness in deep neural networks",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=SJxZnR4YvB": {
    "title": "Distributed Bandit Learning: Near-Optimal Regret with Efficient Communication",
    "volume": "poster",
    "abstract": "We study the problem of regret minimization for distributed bandits learning, in which $M$ agents work collaboratively to minimize their total regret under the coordination of a central server. Our goal is to design communication protocols with near-optimal regret and little communication cost, which is measured by the total amount of transmitted data. For distributed multi-armed bandits, we propose a protocol with near-optimal regret and only $O(M\\log(MK))$ communication cost, where $K$ is the number of arms. The communication cost is independent of the time horizon $T$, has only logarithmic dependence on the number of arms, and matches the lower bound except for a logarithmic factor. For distributed $d$-dimensional linear bandits, we propose a protocol that achieves near-optimal regret and has communication cost of order $O\\left(\\left(Md+d\\log \\log d\\right)\\log T\\right)$, which has only logarithmic dependence on $T$",
    "checked": true,
    "id": "1e8faf0a572c3b6fff00ce052bcdd8d0e2c63f8b",
    "semantic_title": "distributed bandit learning: near-optimal regret with efficient communication",
    "citation_count": 99,
    "authors": []
  },
  "https://openreview.net/forum?id=Bkxe2AVtPS": {
    "title": "Shifted and Squeezed 8-bit Floating Point format for Low-Precision Training of Deep Neural Networks",
    "volume": "poster",
    "abstract": "Training with larger number of parameters while keeping fast iterations is an increasingly adopted strategy and trend for developing better performing Deep Neural Network (DNN) models. This necessitates increased memory footprint and computational requirements for training. Here we introduce a novel methodology for training deep neural networks using 8-bit floating point (FP8) numbers. Reduced bit precision allows for a larger effective memory and increased computational speed. We name this method Shifted and Squeezed FP8 (S2FP8). We show that, unlike previous 8-bit precision training methods, the proposed method works out of the box for representative models: ResNet50, Transformer and NCF. The method can maintain model accuracy without requiring fine-tuning loss scaling parameters or keeping certain layers in single precision. We introduce two learnable statistics of the DNN tensors - shifted and squeezed factors that are used to optimally adjust the range of the tensors in 8-bits, thus minimizing the loss in information due to quantization",
    "checked": true,
    "id": "6b648f92178c3091c66143cc577ff3b5a82aa430",
    "semantic_title": "shifted and squeezed 8-bit floating point format for low-precision training of deep neural networks",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=HyxJhCEFDS": {
    "title": "Intriguing Properties of Adversarial Training at Scale",
    "volume": "poster",
    "abstract": "Adversarial training is one of the main defenses against adversarial attacks. In this paper, we provide the first rigorous study on diagnosing elements of large-scale adversarial training on ImageNet, which reveals two intriguing properties. First, we study the role of normalization. Batch normalization (BN) is a crucial element for achieving state-of-the-art performance on many vision tasks, but we show it may prevent networks from obtaining strong robustness in adversarial training. One unexpected observation is that, for models trained with BN, simply removing clean images from training data largely boosts adversarial robustness, i.e., 18.3%. We relate this phenomenon to the hypothesis that clean images and adversarial images are drawn from two different domains. This two-domain hypothesis may explain the issue of BN when training with a mixture of clean and adversarial images, as estimating normalization statistics of this mixture distribution is challenging. Guided by this two-domain hypothesis, we show disentangling the mixture distribution for normalization, i.e., applying separate BNs to clean and adversarial images for statistics estimation, achieves much stronger robustness. Additionally, we find that enforcing BNs to behave consistently at training and testing can further enhance robustness. Second, we study the role of network capacity. We find our so-called \"deep\" networks are still shallow for the task of adversarial learning. Unlike traditional classification tasks where accuracy is only marginally improved by adding more layers to \"deep\" networks (e.g., ResNet-152), adversarial training exhibits a much stronger demand on deeper networks to achieve higher adversarial robustness. This robustness improvement can be observed substantially and consistently even by pushing the network capacity to an unprecedented scale, i.e., ResNet-638",
    "checked": false,
    "id": "57e62775245a4429bff233b6d1c7a0c17ebd6b9b",
    "semantic_title": "intriguing properties of adversarial training",
    "citation_count": 68,
    "authors": []
  },
  "https://openreview.net/forum?id=B1g5sA4twr": {
    "title": "Deep Double Descent: Where Bigger Models and More Data Hurt",
    "volume": "poster",
    "abstract": "We show that a variety of modern deep learning tasks exhibit a \"double-descent\" phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity, and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance",
    "checked": true,
    "id": "ea415809bf87ef4b99966c6c50de6cb996a02a97",
    "semantic_title": "deep double descent: where bigger models and more data hurt",
    "citation_count": 935,
    "authors": []
  },
  "https://openreview.net/forum?id=HklOo0VFDH": {
    "title": "Decoding As Dynamic Programming For Recurrent Autoregressive Models",
    "volume": "poster",
    "abstract": "Decoding in autoregressive models (ARMs) consists of searching for a high scoring output sequence under the trained model. Standard decoding methods, based on unidirectional greedy algorithm or beam search, are suboptimal due to error propagation and myopic decisions which do not account for future steps in the generation process. In this paper we present a novel decoding approach based on the method of auxiliary coordinates (Carreira-Perpinan & Wang, 2014) to address the aforementioned shortcomings. Our method introduces discrete variables for output tokens, and auxiliary continuous variables representing the states of the underlying ARM. The auxiliary variables lead to a factor graph approximation of the ARM, whose maximum a posteriori (MAP) inference is found exactly using dynamic programming. The MAP inference is then used to recreate an improved factor graph approximation of the ARM via updated auxiliary variables. We then extend our approach to decode in an ensemble of ARMs, possibly with different generation orders, which is out of reach for the standard unidirectional decoding algorithms. Experiments on the text infilling task over SWAG and Daily Dialogue datasets show that our decoding method is superior to strong unidirectional decoding baselines",
    "checked": true,
    "id": "8d354c100b51c458b5de4e7f285486c0019b128b",
    "semantic_title": "decoding as dynamic programming for recurrent autoregressive models",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=S1l8oANFDH": {
    "title": "Synthesizing Programmatic Policies that Inductively Generalize",
    "volume": "poster",
    "abstract": "Deep reinforcement learning has successfully solved a number of challenging control tasks. However, learned policies typically have difficulty generalizing to novel environments. We propose an algorithm for learning programmatic state machine policies that can capture repeating behaviors. By doing so, they have the ability to generalize to instances requiring an arbitrary number of repetitions, a property we call inductive generalization. However, state machine policies are hard to learn since they consist of a combination of continuous and discrete structures. We propose a learning framework called adaptive teaching, which learns a state machine policy by imitating a teacher; in contrast to traditional imitation learning, our teacher adaptively updates itself based on the structure of the student. We show that our algorithm can be used to learn policies that inductively generalize to novel environments, whereas traditional neural network policies fail to do so",
    "checked": true,
    "id": "745aa968dd81c3639b8d765ea63855cf0741ad92",
    "semantic_title": "synthesizing programmatic policies that inductively generalize",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=r1eIiCNYwS": {
    "title": "Transformer-XH: Multi-Evidence Reasoning with eXtra Hop Attention",
    "volume": "poster",
    "abstract": "Transformers have achieved new heights modeling natural language as a sequence of text tokens. However, in many real world scenarios, textual data inherently exhibits structures beyond a linear sequence such as trees and graphs; many tasks require reasoning with evidence scattered across multiple pieces of texts. This paper presents Transformer-XH, which uses eXtra Hop attention to enable intrinsic modeling of structured texts in a fully data-driven way. Its new attention mechanism naturally \"hops\" across the connected text sequences in addition to attending over tokens within each sequence. Thus, Transformer-XH better conducts joint multi-evidence reasoning by propagating information between documents and constructing global contextualized representations. On multi-hop question answering, Transformer-XH leads to a simpler multi-hop QA system which outperforms previous state-of-the-art on the HotpotQA FullWiki setting. On FEVER fact verification, applying Transformer-XH provides state-of-the-art accuracy and excels on claims whose verification requires multiple evidence",
    "checked": true,
    "id": "8382402fe166df3de499dac182e42baa51335926",
    "semantic_title": "transformer-xh: multi-evidence reasoning with extra hop attention",
    "citation_count": 113,
    "authors": []
  },
  "https://openreview.net/forum?id=HklBjCEKvH": {
    "title": "Generalization through Memorization: Nearest Neighbor Language Models",
    "volume": "poster",
    "abstract": "We introduce $k$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a $k$-nearest neighbors ($k$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this transformation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our $k$NN-LM achieves a new state-of-the-art perplexity of 15.79 -- a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail",
    "checked": true,
    "id": "7be8c119dbe065c52125ee7716601751f3116844",
    "semantic_title": "generalization through memorization: nearest neighbor language models",
    "citation_count": 837,
    "authors": []
  },
  "https://openreview.net/forum?id=rJeQoCNYDS": {
    "title": "Single Episode Policy Transfer in Reinforcement Learning",
    "volume": "poster",
    "abstract": "Transfer and adaptation to new unknown environmental dynamics is a key challenge for reinforcement learning (RL). An even greater challenge is performing near-optimally in a single attempt at test time, possibly without access to dense rewards, which is not addressed by current methods that require multiple experience rollouts for adaptation. To achieve single episode transfer in a family of environments with related dynamics, we propose a general algorithm that optimizes a probe and an inference model to rapidly estimate underlying latent variables of test dynamics, which are then immediately used as input to a universal control policy. This modular approach enables integration of state-of-the-art algorithms for variational inference or RL. Moreover, our approach does not require access to rewards at test time, allowing it to perform in settings where existing adaptive approaches cannot. In diverse experimental domains with a single episode test constraint, our method significantly outperforms existing adaptive approaches and shows favorable performance against baselines for robust transfer",
    "checked": true,
    "id": "27e124cc9d630e3a88ef5ca862efa2b982b8d9ef",
    "semantic_title": "single episode policy transfer in reinforcement learning",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=SkgGjRVKDS": {
    "title": "Towards Stabilizing Batch Statistics in Backward Propagation of Batch Normalization",
    "volume": "poster",
    "abstract": "Batch Normalization (BN) is one of the most widely used techniques in Deep Learning field. But its performance can awfully degrade with insufficient batch size. This weakness limits the usage of BN on many computer vision tasks like detection or segmentation, where batch size is usually small due to the constraint of memory consumption. Therefore many modified normalization techniques have been proposed, which either fail to restore the performance of BN completely, or have to introduce additional nonlinear operations in inference procedure and increase huge consumption. In this paper, we reveal that there are two extra batch statistics involved in backward propagation of BN, on which has never been well discussed before. The extra batch statistics associated with gradients also can severely affect the training of deep neural network. Based on our analysis, we propose a novel normalization method, named Moving Average Batch Normalization (MABN). MABN can completely restore the performance of vanilla BN in small batch cases, without introducing any additional nonlinear operations in inference procedure. We prove the benefits of MABN by both theoretical analysis and experiments. Our experiments demonstrate the effectiveness of MABN in multiple computer vision tasks including ImageNet and COCO. The code has been released in https://github.com/megvii-model/MABN",
    "checked": true,
    "id": "2d44d82bacfac3e656f7bff1214eef2363bfd4b4",
    "semantic_title": "towards stabilizing batch statistics in backward propagation of batch normalization",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=ryxgsCVYPr": {
    "title": "NeurQuRI: Neural Question Requirement Inspector for Answerability Prediction in Machine Reading Comprehension",
    "volume": "poster",
    "abstract": "Real-world question answering systems often retrieve potentially relevant documents to a given question through a keyword search, followed by a machine reading comprehension (MRC) step to find the exact answer from them. In this process, it is essential to properly determine whether an answer to the question exists in a given document. This task often becomes complicated when the question involves multiple different conditions or requirements which are to be met in the answer. For example, in a question \"What was the projection of sea level increases in the fourth assessment report?\", the answer should properly satisfy several conditions, such as \"increases\" (but not decreases) and \"fourth\" (but not third). To address this, we propose a neural question requirement inspection model called NeurQuRI that extracts a list of conditions from the question, each of which should be satisfied by the candidate answer generated by an MRC model. To check whether each condition is met, we propose a novel, attention-based loss function. We evaluate our approach on SQuAD 2.0 dataset by integrating the proposed module with various MRC models, demonstrating the consistent performance improvements across a wide range of state-of-the-art methods",
    "checked": true,
    "id": "f402e5990b2d95587a0c733f7c1ec19ee5c81781",
    "semantic_title": "neurquri: neural question requirement inspector for answerability prediction in machine reading comprehension",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=Hkl1iRNFwS": {
    "title": "The Early Phase of Neural Network Training",
    "volume": "poster",
    "abstract": "Recent studies have shown that many important aspects of neural network learning take place within the very earliest iterations or epochs of training. For example, sparse, trainable sub-networks emerge (Frankle et al., 2019), gradient descent moves into a small subspace (Gur-Ari et al., 2018), and the network undergoes a critical period (Achille et al., 2019). Here we examine the changes that deep neural networks undergo during this early phase of training. We perform extensive measurements of the network state and its updates during these early iterations of training, and leverage the framework of Frankle et al. (2019) to quantitatively probe the weight distribution and its reliance on various aspects of the dataset. We find that, within this framework, deep networks are not robust to reinitializing with random weights while maintaining signs, and that weight distributions are highly non-independent even after only a few hundred iterations. Despite this, pre-training with blurred inputs or an auxiliary self-supervised task can approximate the changes in supervised networks, suggesting that these changes are label-agnostic, though labels significantly accelerate this process. Together, these results help to elucidate the network changes occurring during this pivotal initial period of learning",
    "checked": true,
    "id": "57475eef962f45f6b1c249b590b4f894202f8681",
    "semantic_title": "the early phase of neural network training",
    "citation_count": 172,
    "authors": []
  },
  "https://openreview.net/forum?id=HylpqA4FwS": {
    "title": "RNNs Incrementally Evolving on an Equilibrium Manifold: A Panacea for Vanishing and Exploding Gradients?",
    "volume": "poster",
    "abstract": "Recurrent neural networks (RNNs) are particularly well-suited for modeling long-term dependencies in sequential data, but are notoriously hard to train because the error backpropagated in time either vanishes or explodes at an exponential rate. While a number of works attempt to mitigate this effect through gated recurrent units, skip-connections, parametric constraints and design choices, we propose a novel incremental RNN (iRNN), where hidden state vectors keep track of incremental changes, and as such approximate state-vector increments of Rosenblatt's (1962) continuous-time RNNs. iRNN exhibits identity gradients and is able to account for long-term dependencies (LTD). We show that our method is computationally efficient overcoming overheads of many existing methods that attempt to improve RNN training, while suffering no performance degradation. We demonstrate the utility of our approach with extensive experiments and show competitive performance against standard LSTMs on LTD and other non-LTD tasks",
    "checked": true,
    "id": "45571d2bb0838b8a9f8af95541fd5ee5f66467c5",
    "semantic_title": "rnns incrementally evolving on an equilibrium manifold: a panacea for vanishing and exploding gradients?",
    "citation_count": 45,
    "authors": []
  },
  "https://openreview.net/forum?id=SklKcRNYDH": {
    "title": "Extreme Tensoring for Low-Memory Preconditioning",
    "volume": "poster",
    "abstract": "State-of-the-art models are now trained with billions of parameters, reaching hardware limits in terms of memory consumption. This has created a recent demand for memory-efficient optimizers. To this end, we investigate the limits and performance tradeoffs of memory-efficient adaptively preconditioned gradient methods. We propose \\emph{extreme tensoring} for high-dimensional stochastic optimization, showing that an optimizer needs very little memory to benefit from adaptive preconditioning. Our technique applies to arbitrary models (not necessarily with tensor-shaped parameters), and is accompanied by regret and convergence guarantees, which shed light on the tradeoffs between preconditioner quality and expressivity. On a large-scale NLP model, we reduce the optimizer memory overhead by three orders of magnitude, without degrading performance",
    "checked": true,
    "id": "423c0246d016b092b131132dca8249c3faacd000",
    "semantic_title": "extreme tensoring for low-memory preconditioning",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=H1e_cC4twS": {
    "title": "Non-Autoregressive Dialog State Tracking",
    "volume": "poster",
    "abstract": "Recent efforts in Dialogue State Tracking (DST) for task-oriented dialogues have progressed toward open-vocabulary or generation-based approaches where the models can generate slot value candidates from the dialogue history itself. These approaches have shown good performance gain, especially in complicated dialogue domains with dynamic slot values. However, they fall short in two aspects: (1) they do not allow models to explicitly learn signals across domains and slots to detect potential dependencies among \\textit{(domain, slot)} pairs; and (2) existing models follow auto-regressive approaches which incur high time cost when the dialogue evolves over multiple domains and multiple turns. In this paper, we propose a novel framework of Non-Autoregressive Dialog State Tracking (NADST) which can factor in potential dependencies among domains and slots to optimize the models towards better prediction of dialogue states as a complete set rather than separate slots. In particular, the non-autoregressive nature of our method not only enables decoding in parallel to significantly reduce the latency of DST for real-time dialogue response generation, but also detect dependencies among slots at token level in addition to slot and domain level. Our empirical results show that our model achieves the state-of-the-art joint accuracy across all domains on the MultiWOZ 2.1 corpus, and the latency of our model is an order of magnitude lower than the previous state of the art as the dialogue history extends over time",
    "checked": true,
    "id": "22125c5f56fb2504994f1bbe1a9464c23aa17f1a",
    "semantic_title": "non-autoregressive dialog state tracking",
    "citation_count": 52,
    "authors": []
  },
  "https://openreview.net/forum?id=Bkxv90EKPB": {
    "title": "Bayesian Meta Sampling for Fast Uncertainty Adaptation",
    "volume": "poster",
    "abstract": "Meta learning has been making impressive progress for fast model adaptation. However, limited work has been done on learning fast uncertainty adaption for Bayesian modeling. In this paper, we propose to achieve the goal by placing meta learning on the space of probability measures, inducing the concept of meta sampling for fast uncertainty adaption. Specifically, we propose a Bayesian meta sampling framework consisting of two main components: a meta sampler and a sample adapter. The meta sampler is constructed by adopting a neural-inverse-autoregressive-flow (NIAF) structure, a variant of the recently proposed neural autoregressive flows, to efficiently generate meta samples to be adapted. The sample adapter moves meta samples to task-specific samples, based on a newly proposed and general Bayesian sampling technique, called optimal-transport Bayesian sampling. The combination of the two components allows a simple learning procedure for the meta sampler to be developed, which can be efficiently optimized via standard back-propagation. Extensive experimental results demonstrate the efficiency and effectiveness of the proposed framework, obtaining better sample quality and faster uncertainty adaption compared to related methods",
    "checked": true,
    "id": "f51cdd953b49b5e72b0fe9f6a599b1a1b2301314",
    "semantic_title": "bayesian meta sampling for fast uncertainty adaptation",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=SyxV9ANFDH": {
    "title": "Economy Statistical Recurrent Units For Inferring Nonlinear Granger Causality",
    "volume": "poster",
    "abstract": "Granger causality is a widely-used criterion for analyzing interactions in large-scale networks. As most physical interactions are inherently nonlinear, we consider the problem of inferring the existence of pairwise Granger causality between nonlinearly interacting stochastic processes from their time series measurements. Our proposed approach relies on modeling the embedded nonlinearities in the measurements using a component-wise time series prediction model based on Statistical Recurrent Units (SRUs). We make a case that the network topology of Granger causal relations is directly inferrable from a structured sparse estimate of the internal parameters of the SRU networks trained to predict the processes' time series measurements. We propose a variant of SRU, called economy-SRU, which, by design has considerably fewer trainable parameters, and therefore less prone to overfitting. The economy-SRU computes a low-dimensional sketch of its high-dimensional hidden state in the form of random projections to generate the feedback for its recurrent processing. Additionally, the internal weight parameters of the economy-SRU are strategically regularized in a group-wise manner to facilitate the proposed network in extracting meaningful predictive features that are highly time-localized to mimic real-world causal events. Extensive experiments are carried out to demonstrate that the proposed economy-SRU based time series prediction model outperforms the MLP, LSTM and attention-gated CNN-based time series models considered previously for inferring Granger causality",
    "checked": true,
    "id": "352f610a67d778953d252fd8fe7dba1f7341761b",
    "semantic_title": "economy statistical recurrent units for inferring nonlinear granger causality",
    "citation_count": 72,
    "authors": []
  },
  "https://openreview.net/forum?id=rJxlc0EtDr": {
    "title": "MEMO: A Deep Network for Flexible Combination of Episodic Memories",
    "volume": "poster",
    "abstract": "Recent research developing neural network architectures with external memory have often used the benchmark bAbI question and answering dataset which provides a challenging number of tasks requiring reasoning. Here we employed a classic associative inference task from the human neuroscience literature in order to more carefully probe the reasoning capacity of existing memory-augmented architectures. This task is thought to capture the essence of reasoning -- the appreciation of distant relationships among elements distributed across multiple facts or memories. Surprisingly, we found that current architectures struggle to reason over long distance associations. Similar results were obtained on a more complex task involving finding the shortest path between nodes in a path. We therefore developed a novel architecture, MEMO, endowed with the capacity to reason over longer distances. This was accomplished with the addition of two novel components. First, it introduces a separation between memories/facts stored in external memory and the items that comprise these facts in external memory. Second, it makes use of an adaptive retrieval mechanism, allowing a variable number of ‘memory hops' before the answer is produced. MEMO is capable of solving our novel reasoning tasks, as well as all 20 tasks in bAbI",
    "checked": true,
    "id": "0e3eae794a2e693a8571a32f4df01832cec44515",
    "semantic_title": "memo: a deep network for flexible combination of episodic memories",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=HJgCF0VFwr": {
    "title": "Probabilistic Connection Importance Inference and Lossless Compression of Deep Neural Networks",
    "volume": "poster",
    "abstract": "Deep neural networks (DNNs) can be huge in size, requiring a considerable a mount of energy and computational resources to operate, which limits their applications in numerous scenarios. It is thus of interest to compress DNNs while maintaining their performance levels. We here propose a probabilistic importance inference approach for pruning DNNs. Specifically, we test the significance of the relevance of a connection in a DNN to the DNN's outputs using a nonparemtric scoring testand keep only those significant ones. Experimental results show that the proposed approach achieves better lossless compression rates than existing techniques",
    "checked": true,
    "id": "29899cd954c8081c671902f5f015784be109fa2f",
    "semantic_title": "probabilistic connection importance inference and lossless compression of deep neural networks",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=ryeFY0EFwS": {
    "title": "Coherent Gradients: An Approach to Understanding Generalization in Gradient Descent-based Optimization",
    "volume": "poster",
    "abstract": "An open question in the Deep Learning community is why neural networks trained with Gradient Descent generalize well on real datasets even though they are capable of fitting random data. We propose an approach to answering this question based on a hypothesis about the dynamics of gradient descent that we call Coherent Gradients: Gradients from similar examples are similar and so the overall gradient is stronger in certain directions where these reinforce each other. Thus changes to the network parameters during training are biased towards those that (locally) simultaneously benefit many examples when such similarity exists. We support this hypothesis with heuristic arguments and perturbative experiments and outline how this can explain several common empirical observations about Deep Learning. Furthermore, our analysis is not just descriptive, but prescriptive. It suggests a natural modification to gradient descent that can greatly reduce overfitting",
    "checked": true,
    "id": "2858a11c77055f78c67f9a3c89a74d86da8c3772",
    "semantic_title": "coherent gradients: an approach to understanding generalization in gradient descent-based optimization",
    "citation_count": 51,
    "authors": []
  },
  "https://openreview.net/forum?id=Byx_YAVYPH": {
    "title": "Jelly Bean World: A Testbed for Never-Ending Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "665fbb2645d1213e7eb95d870acd2ed75c74d1a5",
    "semantic_title": "jelly bean world: a testbed for never-ending learning",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=rJlUt0EYwS": {
    "title": "Learning from Explanations with Neural Execution Tree",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7b4358d7692353003eae7e39cececa2c2c44c43a",
    "semantic_title": "learning from explanations with neural execution tree",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=rkgHY0NYwr": {
    "title": "Discovering Motor Programs by Recomposing Demonstrations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e90323d515a024be8a6d0465dd90eefd681f9245",
    "semantic_title": "discovering motor programs by recomposing demonstrations",
    "citation_count": 54,
    "authors": []
  },
  "https://openreview.net/forum?id=SJlVY04FwH": {
    "title": "Convergence of Gradient Methods on Bilinear Zero-Sum Games",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0b8e95f455f6220dd4e7775823e5a3cd70786675",
    "semantic_title": "convergence of gradient methods on bilinear zero-sum games",
    "citation_count": 37,
    "authors": []
  },
  "https://openreview.net/forum?id=H1ezFREtwH": {
    "title": "Composing Task-Agnostic Policies with Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "84771e205117b8bdcd0982c35b4fcd514d183afd",
    "semantic_title": "composing task-agnostic policies with deep reinforcement learning",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=HJxMYANtPH": {
    "title": "The Local Elasticity of Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4e77f3b31cfb9a278b57fac20513e06bc4a73750",
    "semantic_title": "the local elasticity of neural networks",
    "citation_count": 45,
    "authors": []
  },
  "https://openreview.net/forum?id=rklbKA4YDS": {
    "title": "Gradient-Based Neural DAG Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ae39e54c451897999032cde7d3e1c33139d7fdc3",
    "semantic_title": "gradient-based neural dag learning",
    "citation_count": 273,
    "authors": []
  },
  "https://openreview.net/forum?id=BylA_C4tPr": {
    "title": "Composition-based Multi-Relational Graph Convolutional Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4b244a6778c95b1df8e9e02332ff8d22e675f628",
    "semantic_title": "composition-based multi-relational graph convolutional networks",
    "citation_count": 841,
    "authors": []
  },
  "https://openreview.net/forum?id=HJe6uANtwH": {
    "title": "Capsules with Inverted Dot-Product Attention Routing",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "16601473f45fe183da46ba078d13b3fde7d48bac",
    "semantic_title": "capsules with inverted dot-product attention routing",
    "citation_count": 81,
    "authors": []
  },
  "https://openreview.net/forum?id=S1xtORNFwH": {
    "title": "FSNet: Compression of Deep Convolutional Neural Networks by Filter Summary",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "575a8d6ed2e177c7328f8a3524a0631e522cf2dc",
    "semantic_title": "fsnet: compression of deep convolutional neural networks by filter summary",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=r1lF_CEYwS": {
    "title": "On the Need for Topology-Aware Generative Models for Manifold-Based Defenses",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "28fcb9763a5d1f2a3f9a58d63b4f9705bf5de605",
    "semantic_title": "on the need for topology-aware generative models for manifold-based defenses",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=SkgKO0EtvS": {
    "title": "Neural Execution of Graph Algorithms",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a4a1a70b7cd477de254f88662787406dcd40d8bb",
    "semantic_title": "neural execution of graph algorithms",
    "citation_count": 167,
    "authors": []
  },
  "https://openreview.net/forum?id=SkeHuCVFDr": {
    "title": "BERTScore: Evaluating Text Generation with BERT",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "295065d942abca0711300b2b4c39829551060578",
    "semantic_title": "bertscore: evaluating text generation with bert",
    "citation_count": 5776,
    "authors": []
  },
  "https://openreview.net/forum?id=ryxQuANKPB": {
    "title": "Augmenting Non-Collaborative Dialog Systems with Explicit Semantic and Strategic Dialog History",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7b622d7a991da4d2091d7bdb420177ca596f13a8",
    "semantic_title": "augmenting non-collaborative dialog systems with explicit semantic and strategic dialog history",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=BJxG_0EtDS": {
    "title": "Prediction, Consistency, Curvature: Representation Learning for Locally-Linear Control",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b613d8203a94e4cb8c98e5acee2e8d2570421b34",
    "semantic_title": "prediction, consistency, curvature: representation learning for locally-linear control",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=B1x6w0EtwH": {
    "title": "Graph Constrained Reinforcement Learning for Natural Language Action Spaces",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "028c1a07ac62bbdb681d11cacf4c7485f9aa3ef7",
    "semantic_title": "graph constrained reinforcement learning for natural language action spaces",
    "citation_count": 129,
    "authors": []
  },
  "https://openreview.net/forum?id=r1eowANFvr": {
    "title": "Towards Fast Adaptation of Neural Architectures with Meta Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "027d05ab0c9e3ad91103eec2fe6ee2d2951a1afa",
    "semantic_title": "towards fast adaptation of neural architectures with meta learning",
    "citation_count": 85,
    "authors": []
  },
  "https://openreview.net/forum?id=H1x5wRVtvS": {
    "title": "Variational Hetero-Encoder Randomized GANs for Joint Image-Text Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "40526674103a0e4d4f6d43e95e5faa10a7e13deb",
    "semantic_title": "variational hetero-encoder randomized gans for joint image-text modeling",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=HJgfDREKDB": {
    "title": "Higher-Order Function Networks for Learning Composable 3D Object Representations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1a93075da059cf1b6660d8b5f8f5d097d4060c8d",
    "semantic_title": "higher-order function networks for learning composable 3d object representations",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=SygWvAVFPr": {
    "title": "Neural Module Networks for Reasoning over Text",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2d08cdc45bbed8cea283d92132b4b88a507a7303",
    "semantic_title": "neural module networks for reasoning over text",
    "citation_count": 131,
    "authors": []
  },
  "https://openreview.net/forum?id=ryx1wRNFvB": {
    "title": "Improved memory in recurrent neural networks with sequential non-normal dynamics",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f73480dabc01b467f62e088cb51e6e52f819f96f",
    "semantic_title": "improved memory in recurrent neural networks with sequential non-normal dynamics",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=SJlh8CEYDB": {
    "title": "Learn to Explain Efficiently via Neural Logic Inductive Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c6145aec02705d44bc81d79b8e514a2f0349d827",
    "semantic_title": "learn to explain efficiently via neural logic inductive learning",
    "citation_count": 77,
    "authors": []
  },
  "https://openreview.net/forum?id=ByxY8CNtvr": {
    "title": "Improving Neural Language Generation with Spectrum Control",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7fed15cc79332f83b7bfe920c02a9c954322ddcc",
    "semantic_title": "improving neural language generation with spectrum control",
    "citation_count": 84,
    "authors": []
  },
  "https://openreview.net/forum?id=B1guLAVFDB": {
    "title": "Span Recovery for Deep Neural Networks with Applications to Input Obfuscation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "84e669282af040155daf26a33e760cff9167a028",
    "semantic_title": "span recovery for deep neural networks with applications to input obfuscation",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=Bke8UR4FPB": {
    "title": "Oblique Decision Trees from Derivatives of ReLU Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "65506f74cf01a94c1dddd684bac56c97a2b4b2b6",
    "semantic_title": "oblique decision trees from derivatives of relu networks",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=SJgVU0EKwS": {
    "title": "Precision Gating: Improving Neural Network Efficiency with Dynamic Dual-Precision Activations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f0f1bd1cc8e7e8fa5cd0ba7d6c09117399c7f233",
    "semantic_title": "precision gating: improving neural network efficiency with dynamic dual-precision activations",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=BJxVI04YvB": {
    "title": "PAC Confidence Sets for Deep Neural Networks via Calibrated Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "92ca1a7b08f242f4682f0ba0126b2976d2bfb5d0",
    "semantic_title": "pac confidence sets for deep neural networks via calibrated prediction",
    "citation_count": 58,
    "authors": []
  },
  "https://openreview.net/forum?id=H1gX8C4YPr": {
    "title": "DD-PPO: Learning Near-Perfect PointGoal Navigators from 2.5 Billion Frames",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b0e95b881add810353b12e78615613a0132be754",
    "semantic_title": "dd-ppo: learning near-perfect pointgoal navigators from 2.5 billion frames",
    "citation_count": 475,
    "authors": []
  },
  "https://openreview.net/forum?id=ryxz8CVYDH": {
    "title": "Learning to Learn by Zeroth-Order Oracle",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1fcb793d3a8268e39000c5ed8eb86052a7652e80",
    "semantic_title": "learning to learn by zeroth-order oracle",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=SJx1URNKwH": {
    "title": "MetaPix: Few-Shot Video Retargeting",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b23d8546b29cf67e61798ca139c4426ed54855e5",
    "semantic_title": "metapix: few-shot video retargeting",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=SkxJ8REYPH": {
    "title": "SlowMo: Improving Communication-Efficient Distributed SGD with Slow Momentum",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e20848622d5145744127b82367f9b671c7ddb08e",
    "semantic_title": "slowmo: improving communication-efficient distributed sgd with slow momentum",
    "citation_count": 202,
    "authors": []
  },
  "https://openreview.net/forum?id=BJedHRVtPB": {
    "title": "Pseudo-LiDAR++: Accurate Depth for 3D Object Detection in Autonomous Driving",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6f1ecfb639b9e89460c4d1f71ced86f9a9b86ee2",
    "semantic_title": "pseudo-lidar++: accurate depth for 3d object detection in autonomous driving",
    "citation_count": 395,
    "authors": []
  },
  "https://openreview.net/forum?id=HJx8HANFDH": {
    "title": "Four Things Everyone Should Know to Improve Batch Normalization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6c6df22e0add771690e33ed7bce91791aa331788",
    "semantic_title": "four things everyone should know to improve batch normalization",
    "citation_count": 52,
    "authors": []
  },
  "https://openreview.net/forum?id=ByeUBANtvB": {
    "title": "Learning to solve the credit assignment problem",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "cabf0c74cb3d0787054d700939b8e0c06ce55546",
    "semantic_title": "learning to solve the credit assignment problem",
    "citation_count": 52,
    "authors": []
  },
  "https://openreview.net/forum?id=rylVHR4FPB": {
    "title": "Sampling-Free Learning of Bayesian Quantized Neural Networks",
    "volume": "poster",
    "abstract": "Bayesian learning of model parameters in neural networks is important in scenarios where estimates with well-calibrated uncertainty are important. In this paper, we propose Bayesian quantized networks (BQNs), quantized neural networks (QNNs) for which we learn a posterior distribution over their discrete parameters. We provide a set of efficient algorithms for learning and prediction in BQNs without the need to sample from their parameters or activations, which not only allows for differentiable learning in quantized models but also reduces the variance in gradients estimation. We evaluate BQNs on MNIST, Fashion-MNIST and KMNIST classification datasets compared against bootstrap ensemble of QNNs (E-QNN). We demonstrate BQNs achieve both lower predictive errors and better-calibrated uncertainties than E-QNN (with less than 20% of the negative log-likelihood)",
    "checked": true,
    "id": "142e472246b7ef1a22e8174dd46672a71a7d30c6",
    "semantic_title": "sampling-free learning of bayesian quantized neural networks",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=rJeXS04FPH": {
    "title": "DeFINE: Deep Factorized Input Token Embeddings for Neural Sequence Modeling",
    "volume": "poster",
    "abstract": "For sequence models with large vocabularies, a majority of network parameters lie in the input and output layers. In this work, we describe a new method, DeFINE, for learning deep token representations efficiently. Our architecture uses a hierarchical structure with novel skip-connections which allows for the use of low dimensional input and output layers, reducing total parameters and training time while delivering similar or better performance versus existing methods. DeFINE can be incorporated easily in new or existing sequence models. Compared to state-of-the-art methods including adaptive input representations, this technique results in a 6% to 20% drop in perplexity. On WikiText-103, DeFINE reduces the total parameters of Transformer-XL by half with minimal impact on performance. On the Penn Treebank, DeFINE improves AWD-LSTM by 4 points with a 17% reduction in parameters, achieving comparable performance to state-of-the-art methods with fewer parameters. For machine translation, DeFINE improves the efficiency of the Transformer model by about 1.4 times while delivering similar performance",
    "checked": true,
    "id": "841d43cf4015042a4ee45745c5b6f2c59c184da5",
    "semantic_title": "define: deep factorized input token embeddings for neural sequence modeling",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=rkgyS0VFvr": {
    "title": "DBA: Distributed Backdoor Attacks against Federated Learning",
    "volume": "poster",
    "abstract": "Backdoor attacks aim to manipulate a subset of training data by injecting adversarial triggers such that machine learning models trained on the tampered dataset will make arbitrarily (targeted) incorrect prediction on the testset with the same trigger embedded. While federated learning (FL) is capable of aggregating information provided by different parties for training a better model, its distributed learning methodology and inherently heterogeneous data distribution across parties may bring new vulnerabilities. In addition to recent centralized backdoor attacks on FL where each party embeds the same global trigger during training, we propose the distributed backdoor attack (DBA) --- a novel threat assessment framework developed by fully exploiting the distributed nature of FL. DBA decomposes a global trigger pattern into separate local patterns and embed them into the training set of different adversarial parties respectively. Compared to standard centralized backdoors, we show that DBA is substantially more persistent and stealthy against FL on diverse datasets such as finance and image data. We conduct extensive experiments to show that the attack success rate of DBA is significantly higher than centralized backdoors under different settings. Moreover, we find that distributed attacks are indeed more insidious, as DBA can evade two state-of-the-art robust FL algorithms against centralized backdoors. We also provide explanations for the effectiveness of DBA via feature visual interpretation and feature importance ranking. To further explore the properties of DBA, we test the attack performance by varying different trigger factors, including local trigger variations (size, gap, and location), scaling factor in FL, data distribution, and poison ratio and interval. Our proposed DBA and thorough evaluation results shed lights on characterizing the robustness of FL",
    "checked": true,
    "id": "4b2df153540a64b293818bbd9b1c00d28a01025c",
    "semantic_title": "dba: distributed backdoor attacks against federated learning",
    "citation_count": 689,
    "authors": []
  },
  "https://openreview.net/forum?id=BJx040EFvH": {
    "title": "Fast is better than free: Revisiting adversarial training",
    "volume": "poster",
    "abstract": "Adversarial training, a method for learning robust deep networks, is typically assumed to be more expensive than traditional training due to the necessity of constructing adversarial examples via a first-order method like projected gradient decent (PGD). In this paper, we make the surprising discovery that it is possible to train empirically robust models using a much weaker and cheaper adversary, an approach that was previously believed to be ineffective, rendering the method no more costly than standard training in practice. Specifically, we show that adversarial training with the fast gradient sign method (FGSM), when combined with random initialization, is as effective as PGD-based training but has significantly lower cost. Furthermore we show that FGSM adversarial training can be further accelerated by using standard techniques for efficient training of deep networks, allowing us to learn a robust CIFAR10 classifier with 45% robust accuracy at epsilon=8/255 in 6 minutes, and a robust ImageNet classifier with 43% robust accuracy at epsilon=2/255 in 12 hours, in comparison to past work based on ``free'' adversarial training which took 10 and 50 hours to reach the same respective thresholds",
    "checked": true,
    "id": "6d4a87759917132913319960389f17fa1fe8b630",
    "semantic_title": "fast is better than free: revisiting adversarial training",
    "citation_count": 1178,
    "authors": []
  },
  "https://openreview.net/forum?id=Byl5NREFDr": {
    "title": "Thieves on Sesame Street! Model Extraction of BERT-based APIs",
    "volume": "poster",
    "abstract": "We study the problem of model extraction in natural language processing, in which an adversary with only query access to a victim model attempts to reconstruct a local copy of that model. Assuming that both the adversary and victim model fine-tune a large pretrained language model such as BERT (Devlin et al., 2019), we show that the adversary does not need any real training data to successfully mount the attack. In fact, the attacker need not even use grammatical or semantically meaningful queries: we show that random sequences of words coupled with task-specific heuristics form effective queries for model extraction on a diverse set of NLP tasks, including natural language inference and question answering. Our work thus highlights an exploit only made feasible by the shift towards transfer learning methods within the NLP community: for a query budget of a few hundred dollars, an attacker can extract a model that performs only slightly worse than the victim model. Finally, we study two defense strategies against model extraction—membership classification and API watermarking—which while successful against some adversaries can also be circumvented by more clever ones",
    "checked": true,
    "id": "ac713aebdcc06f15f8ea61e1140bb360341fdf27",
    "semantic_title": "thieves on sesame street! model extraction of bert-based apis",
    "citation_count": 200,
    "authors": []
  },
  "https://openreview.net/forum?id=BygFVAEKDH": {
    "title": "Understanding Knowledge Distillation in Non-autoregressive Machine Translation",
    "volume": "poster",
    "abstract": "Non-autoregressive machine translation (NAT) systems predict a sequence of output tokens in parallel, achieving substantial improvements in generation speed compared to autoregressive models. Existing NAT models usually rely on the technique of knowledge distillation, which creates the training data from a pretrained autoregressive model for better performance. Knowledge distillation is empirically useful, leading to large gains in accuracy for NAT models, but the reason for this success has, as of yet, been unclear. In this paper, we first design systematic experiments to investigate why knowledge distillation is crucial to NAT training. We find that knowledge distillation can reduce the complexity of data sets and help NAT to model the variations in the output data. Furthermore, a strong correlation is observed between the capacity of an NAT model and the optimal complexity of the distilled data for the best translation quality. Based on these findings, we further propose several approaches that can alter the complexity of data sets to improve the performance of NAT models. We achieve the state-of-the-art performance for the NAT-based models, and close the gap with the autoregressive baseline on WMT14 En-De benchmark",
    "checked": true,
    "id": "1e5b826ddf0754f6e93234ba1260bd939c255e7f",
    "semantic_title": "understanding knowledge distillation in non-autoregressive machine translation",
    "citation_count": 220,
    "authors": []
  },
  "https://openreview.net/forum?id=Hye_V0NKwr": {
    "title": "Locality and Compositionality in Zero-Shot Learning",
    "volume": "poster",
    "abstract": "In this work we study locality and compositionality in the context of learning representations for Zero Shot Learning (ZSL). In order to well-isolate the importance of these properties in learned representations, we impose the additional constraint that, differently from most recent work in ZSL, no pre-training on different datasets (e.g. ImageNet) is performed. The results of our experiment show how locality, in terms of small parts of the input, and compositionality, i.e. how well can the learned representations be expressed as a function of a smaller vocabulary, are both deeply related to generalization and motivate the focus on more local-aware models in future research directions for representation learning",
    "checked": true,
    "id": "b15ab23efa4f260336e0c8dc66179d04c2993533",
    "semantic_title": "locality and compositionality in zero-shot learning",
    "citation_count": 56,
    "authors": []
  },
  "https://openreview.net/forum?id=H1gB4RVKvB": {
    "title": "Recurrent neural circuits for contour detection",
    "volume": "poster",
    "abstract": "We introduce a deep recurrent neural network architecture that approximates visual cortical circuits (Mély et al., 2018). We show that this architecture, which we refer to as the 𝜸-net, learns to solve contour detection tasks with better sample efficiency than state-of-the-art feedforward networks, while also exhibiting a classic perceptual illusion, known as the orientation-tilt illusion. Correcting this illusion significantly reduces \\gnetw contour detection accuracy by driving it to prefer low-level edges over high-level object boundary contours. Overall, our study suggests that the orientation-tilt illusion is a byproduct of neural circuits that help biological visual systems achieve robust and efficient contour detection, and that incorporating these circuits in artificial neural networks can improve computer vision",
    "checked": true,
    "id": "e70164c4d85867a7dc06dcf05e1cd40e52b513fe",
    "semantic_title": "recurrent neural circuits for contour detection",
    "citation_count": 45,
    "authors": []
  },
  "https://openreview.net/forum?id=rygG4AVFvH": {
    "title": "Chameleon: Adaptive Code Optimization for Expedited Deep Neural Network Compilation",
    "volume": "poster",
    "abstract": "Achieving faster execution with shorter compilation time can foster further diversity and innovation in neural networks. However, the current paradigm of executing neural networks either relies on hand-optimized libraries, traditional compilation heuristics, or very recently genetic algorithms and other stochastic methods. These methods suffer from frequent costly hardware measurements rendering them not only too time consuming but also suboptimal. As such, we devise a solution that can learn to quickly adapt to a previously unseen design space for code optimization, both accelerating the search and improving the output performance. This solution dubbed Chameleon leverages reinforcement learning whose solution takes fewer steps to converge, and develops an adaptive sampling algorithm that not only focuses on the costly samples (real hardware measurements) on representative points but also uses a domain-knowledge inspired logic to improve the samples itself. Experimentation with real hardware shows that Chameleon provides 4.45x speed up in optimization time over AutoTVM, while also improving inference time of the modern deep networks by 5.6%",
    "checked": true,
    "id": "e4c965324f9773ec3111686a4abe53ea61c89f7c",
    "semantic_title": "chameleon: adaptive code optimization for expedited deep neural network compilation",
    "citation_count": 82,
    "authors": []
  },
  "https://openreview.net/forum?id=SJleNCNtDH": {
    "title": "Intrinsic Motivation for Encouraging Synergistic Behavior",
    "volume": "poster",
    "abstract": "We study the role of intrinsic motivation as an exploration bias for reinforcement learning in sparse-reward synergistic tasks, which are tasks where multiple agents must work together to achieve a goal they could not individually. Our key idea is that a good guiding principle for intrinsic motivation in synergistic tasks is to take actions which affect the world in ways that would not be achieved if the agents were acting on their own. Thus, we propose to incentivize agents to take (joint) actions whose effects cannot be predicted via a composition of the predicted effect for each individual agent. We study two instantiations of this idea, one based on the true states encountered, and another based on a dynamics model trained concurrently with the policy. While the former is simpler, the latter has the benefit of being analytically differentiable with respect to the action taken. We validate our approach in robotic bimanual manipulation and multi-agent locomotion tasks with sparse rewards; we find that our approach yields more efficient learning than both 1) training with only the sparse reward and 2) using the typical surprise-based formulation of intrinsic motivation, which does not bias toward synergistic behavior. Videos are available on the project webpage: https://sites.google.com/view/iclr2020-synergistic",
    "checked": true,
    "id": "4dc5be46f267e8fa0213c12d374dab5e46377d28",
    "semantic_title": "intrinsic motivation for encouraging synergistic behavior",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=HJxR7R4FvS": {
    "title": "RaCT: Toward Amortized Ranking-Critical Training For Collaborative Filtering",
    "volume": "poster",
    "abstract": "We investigate new methods for training collaborative filtering models based on actor-critic reinforcement learning, to more directly maximize ranking-based objective functions. Specifically, we train a critic network to approximate ranking-based metrics, and then update the actor network to directly optimize against the learned metrics. In contrast to traditional learning-to-rank methods that require re-running the optimization procedure for new lists, our critic-based method amortizes the scoring process with a neural network, and can directly provide the (approximate) ranking scores for new lists. We demonstrate the actor-critic's ability to significantly improve the performance of a variety of prediction models, and achieve better or comparable performance to a variety of strong baselines on three large-scale datasets",
    "checked": true,
    "id": "3d8dbdef39e1c51473ea6e1ed322cd8d8618c335",
    "semantic_title": "ract: toward amortized ranking-critical training for collaborative filtering",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=SklTQCNtvS": {
    "title": "Sign-OPT: A Query-Efficient Hard-label Adversarial Attack",
    "volume": "poster",
    "abstract": "We study the most practical problem setup for evaluating adversarial robustness of a machine learning system with limited access: the hard-label black-box attack setting for generating adversarial examples, where limited model queries are allowed and only the decision is provided to a queried data input. Several algorithms have been proposed for this problem but they typically require huge amount (>20,000) of queries for attacking one example. Among them, one of the state-of-the-art approaches (Cheng et al., 2019) showed that hard-label attack can be modeled as an optimization problem where the objective function can be evaluated by binary search with additional model queries, thereby a zeroth order optimization algorithm can be applied. In this paper, we adopt the same optimization formulation but propose to directly estimate the sign of gradient at any direction instead of the gradient itself, which enjoys the benefit of single query. Using this single query oracle for retrieving sign of directional derivative, we develop a novel query-efficient Sign-OPT approach for hard-label black-box attack. We provide a convergence analysis of the new algorithm and conduct experiments on several models on MNIST, CIFAR-10 and ImageNet. We find that Sign-OPT attack consistently requires 5X to 10X fewer queries when compared to the current state-of-the-art approaches, and usually converges to an adversarial example with smaller perturbation",
    "checked": true,
    "id": "166c962e378eafc5b5b2ad980e222e1b17ee994e",
    "semantic_title": "sign-opt: a query-efficient hard-label adversarial attack",
    "citation_count": 221,
    "authors": []
  },
  "https://openreview.net/forum?id=S1xnXRVFwH": {
    "title": "Playing the lottery with rewards and multiple languages: lottery tickets in RL and NLP",
    "volume": "poster",
    "abstract": "The lottery ticket hypothesis proposes that over-parameterization of deep neural networks (DNNs) aids training by increasing the probability of a \"lucky\" sub-network initialization being present rather than by helping the optimization process (Frankle& Carbin, 2019). Intriguingly, this phenomenon suggests that initialization strategies for DNNs can be improved substantially, but the lottery ticket hypothesis has only previously been tested in the context of supervised learning for natural image tasks. Here, we evaluate whether \"winning ticket\" initializations exist in two different domains: natural language processing (NLP) and reinforcement learning (RL).For NLP, we examined both recurrent LSTM models and large-scale Transformer models (Vaswani et al., 2017). For RL, we analyzed a number of discrete-action space tasks, including both classic control and pixel control. Consistent with workin supervised image classification, we confirm that winning ticket initializations generally outperform parameter-matched random initializations, even at extreme pruning rates for both NLP and RL. Notably, we are able to find winning ticket initializations for Transformers which enable models one-third the size to achieve nearly equivalent performance. Together, these results suggest that the lottery ticket hypothesis is not restricted to supervised learning of natural images, but rather represents a broader phenomenon in DNNs",
    "checked": true,
    "id": "387e0b95d56e9ecec60a1037ddf7cc57b2851835",
    "semantic_title": "playing the lottery with rewards and multiple languages: lottery tickets in rl and nlp",
    "citation_count": 149,
    "authors": []
  },
  "https://openreview.net/forum?id=rkenmREFDr": {
    "title": "Learning Space Partitions for Nearest Neighbor Search",
    "volume": "poster",
    "abstract": "Space partitions of $\\mathbb{R}^d$ underlie a vast and important class of fast nearest neighbor search (NNS) algorithms. Inspired by recent theoretical work on NNS for general metric spaces (Andoni et al. 2018b,c), we develop a new framework for building space partitions reducing the problem to balanced graph partitioning followed by supervised classification. We instantiate this general approach with the KaHIP graph partitioner (Sanders and Schulz 2013) and neural networks, respectively, to obtain a new partitioning procedure called Neural Locality-Sensitive Hashing (Neural LSH). On several standard benchmarks for NNS (Aumuller et al. 2017), our experiments show that the partitions obtained by Neural LSH consistently outperform partitions found by quantization-based and tree-based methods as well as classic, data-oblivious LSH",
    "checked": true,
    "id": "07e92b8cf0d5b60ae0271bdeb82b7c3663108572",
    "semantic_title": "learning space partitions for nearest neighbor search",
    "citation_count": 94,
    "authors": []
  },
  "https://openreview.net/forum?id=HJeO7RNKPr": {
    "title": "DeepV2D: Video to Depth with Differentiable Structure from Motion",
    "volume": "poster",
    "abstract": "We propose DeepV2D, an end-to-end deep learning architecture for predicting depth from video. DeepV2D combines the representation ability of neural networks with the geometric principles governing image formation. We compose a collection of classical geometric algorithms, which are converted into trainable modules and combined into an end-to-end differentiable architecture. DeepV2D interleaves two stages: motion estimation and depth estimation. During inference, motion and depth estimation are alternated and converge to accurate depth",
    "checked": true,
    "id": "dfb22535ada5e35b9ed3523499127003e4afc00d",
    "semantic_title": "deepv2d: video to depth with differentiable structure from motion",
    "citation_count": 247,
    "authors": []
  },
  "https://openreview.net/forum?id=SJxIm0VtwH": {
    "title": "Towards Better Understanding of Adaptive Gradient Algorithms in Generative Adversarial Nets",
    "volume": "poster",
    "abstract": "Adaptive gradient algorithms perform gradient-based updates using the history of gradients and are ubiquitous in training deep neural networks. While adaptive gradient methods theory is well understood for minimization problems, the underlying factors driving their empirical success in min-max problems such as GANs remain unclear. In this paper, we aim at bridging this gap from both theoretical and empirical perspectives. First, we analyze a variant of Optimistic Stochastic Gradient (OSG) proposed in~\\citep{daskalakis2017training} for solving a class of non-convex non-concave min-max problem and establish $O(\\epsilon^{-4})$ complexity for finding $\\epsilon$-first-order stationary point, in which the algorithm only requires invoking one stochastic first-order oracle while enjoying state-of-the-art iteration complexity achieved by stochastic extragradient method by~\\citep{iusem2017extragradient}. Then we propose an adaptive variant of OSG named Optimistic Adagrad (OAdagrad) and reveal an \\emph{improved} adaptive complexity $O\\left(\\epsilon^{-\\frac{2}{1-\\alpha}}\\right)$, where $\\alpha$ characterizes the growth rate of the cumulative stochastic gradient and $0\\leq \\alpha\\leq 1/2$. To the best of our knowledge, this is the first work for establishing adaptive complexity in non-convex non-concave min-max optimization. Empirically, our experiments show that indeed adaptive gradient algorithms outperform their non-adaptive counterparts in GAN training. Moreover, this observation can be explained by the slow growth rate of the cumulative stochastic gradient, as observed empirically",
    "checked": true,
    "id": "16b4c8f533b8ecf9f384684dbf50fed698baa0a6",
    "semantic_title": "towards better understanding of adaptive gradient algorithms in generative adversarial nets",
    "citation_count": 63,
    "authors": []
  },
  "https://openreview.net/forum?id=HJlSmC4FPS": {
    "title": "Robust And Interpretable Blind Image Denoising Via Bias-Free Convolutional Neural Networks",
    "volume": "poster",
    "abstract": "We study the generalization properties of deep convolutional neural networks for image denoising in the presence of varying noise levels. We provide extensive empirical evidence that current state-of-the-art architectures systematically overfit to the noise levels in the training set, performing very poorly at new noise levels. We show that strong generalization can be achieved through a simple architectural modification: removing all additive constants. The resulting \"bias-free\" networks attain state-of-the-art performance over a broad range of noise levels, even when trained over a limited range. They are also locally linear, which enables direct analysis with linear-algebraic tools. We show that the denoising map can be visualized locally as a filter that adapts to both image structure and noise level. In addition, our analysis reveals that deep networks implicitly perform a projection onto an adaptively-selected low-dimensional subspace, with dimensionality inversely proportional to noise level, that captures features of natural images",
    "checked": true,
    "id": "832baf7476fa4c98ad326ae9190d613d78e6eb13",
    "semantic_title": "robust and interpretable blind image denoising via bias-free convolutional neural networks",
    "citation_count": 124,
    "authors": []
  },
  "https://openreview.net/forum?id=S1lEX04tPr": {
    "title": "CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning",
    "volume": "poster",
    "abstract": "A variety of cooperative multi-agent control problems require agents to achieve individual goals while contributing to collective success. This multi-goal multi-agent setting poses difficulties for recent algorithms, which primarily target settings with a single global reward, due to two new challenges: efficient exploration for learning both individual goal attainment and cooperation for others' success, and credit-assignment for interactions between actions and goals of different agents. To address both challenges, we restructure the problem into a novel two-stage curriculum, in which single-agent goal attainment is learned prior to learning multi-agent cooperation, and we derive a new multi-goal multi-agent policy gradient with a credit function for localized credit assignment. We use a function augmentation scheme to bridge value and policy functions across the curriculum. The complete architecture, called CM3, learns significantly faster than direct adaptations of existing algorithms on three challenging multi-goal multi-agent problems: cooperative navigation in difficult formations, negotiating multi-vehicle lane changes in the SUMO traffic simulator, and strategic cooperation in a Checkers environment",
    "checked": true,
    "id": "cf4a1eadf2e1a8e9144446790e563c8a3097f9c3",
    "semantic_title": "cm3: cooperative multi-goal multi-stage multi-agent reinforcement learning",
    "citation_count": 76,
    "authors": []
  },
  "https://openreview.net/forum?id=Skl4mRNYDr": {
    "title": "Deep Imitative Models for Flexible Inference, Planning, and Control",
    "volume": "poster",
    "abstract": "Imitation Learning (IL) is an appealing approach to learn desirable autonomous behavior. However, directing IL to achieve arbitrary goals is difficult. In contrast, planning-based algorithms use dynamics models and reward functions to achieve goals. Yet, reward functions that evoke desirable behavior are often difficult to specify. In this paper, we propose \"Imitative Models\" to combine the benefits of IL and goal-directed planning. Imitative Models are probabilistic predictive models of desirable behavior able to plan interpretable expert-like trajectories to achieve specified goals. We derive families of flexible goal objectives, including constrained goal regions, unconstrained goal sets, and energy-based goals. We show that our method can use these objectives to successfully direct behavior. Our method substantially outperforms six IL approaches and a planning-based approach in a dynamic simulated autonomous driving task, and is efficiently learned from expert demonstrations without online data collection. We also show our approach is robust to poorly-specified goals, such as goals on the wrong side of the road",
    "checked": true,
    "id": "9b114351bf97a66ab8cfef4c3b04a0f70503295e",
    "semantic_title": "deep imitative models for flexible inference, planning, and control",
    "citation_count": 148,
    "authors": []
  },
  "https://openreview.net/forum?id=rkg-mA4FDr": {
    "title": "Pre-training Tasks for Embedding-based Large-scale Retrieval",
    "volume": "poster",
    "abstract": "We consider the large-scale query-document retrieval problem: given a query (e.g., a question), return the set of relevant documents (e.g., paragraphs containing the answer) from a large document corpus. This problem is often solved in two steps. The retrieval phase first reduces the solution space, returning a subset of candidate documents. The scoring phase then re-ranks the documents. Critically, the retrieval algorithm not only desires high recall but also requires to be highly efficient, returning candidates in time sublinear to the number of documents. Unlike the scoring phase witnessing significant advances recently due to the BERT-style pre-training tasks on cross-attention models, the retrieval phase remains less well studied. Most previous works rely on classic Information Retrieval (IR) methods such as BM-25 (token matching + TF-IDF weights). These models only accept sparse handcrafted features and can not be optimized for different downstream tasks of interest. In this paper, we conduct a comprehensive study on the embedding-based retrieval models. We show that the key ingredient of learning a strong embedding-based Transformer model is the set of pre-training tasks. With adequately designed paragraph-level pre-training tasks, the Transformer models can remarkably improve over the widely-used BM-25 as well as embedding models without Transformers. The paragraph-level pre-training tasks we studied are Inverse Cloze Task (ICT), Body First Selection (BFS), Wiki Link Prediction (WLP), and the combination of all three",
    "checked": true,
    "id": "beb21c9f339bb040e8f360b45eda0f9b10b2c95c",
    "semantic_title": "pre-training tasks for embedding-based large-scale retrieval",
    "citation_count": 306,
    "authors": []
  },
  "https://openreview.net/forum?id=ByxRM0Ntvr": {
    "title": "Are Transformers universal approximators of sequence-to-sequence functions?",
    "volume": "poster",
    "abstract": "Despite the widespread adoption of Transformer models for NLP tasks, the expressive power of these models is not well-understood. In this paper, we establish that Transformer models are universal approximators of continuous permutation equivariant sequence-to-sequence functions with compact support, which is quite surprising given the amount of shared parameters in these models. Furthermore, using positional encodings, we circumvent the restriction of permutation equivariance, and show that Transformer models can universally approximate arbitrary continuous sequence-to-sequence functions on a compact domain. Interestingly, our proof techniques clearly highlight the different roles of the self-attention and the feed-forward layers in Transformers. In particular, we prove that fixed width self-attention layers can compute contextual mappings of the input sequences, playing a key role in the universal approximation property of Transformers. Based on this insight from our analysis, we consider other simpler alternatives to self-attention layers and empirically evaluate them",
    "checked": true,
    "id": "509b4661ed74a24c2ffdbf131f9e1c6a1783752d",
    "semantic_title": "are transformers universal approximators of sequence-to-sequence functions?",
    "citation_count": 352,
    "authors": []
  },
  "https://openreview.net/forum?id=rkgAGAVKPr": {
    "title": "Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples",
    "volume": "poster",
    "abstract": "Few-shot classification refers to learning a classifier for new classes given only a few examples. While a plethora of models have emerged to tackle it, we find the procedure and datasets that are used to assess their progress lacking. To address this limitation, we propose Meta-Dataset: a new benchmark for training and evaluating models that is large-scale, consists of diverse datasets, and presents more realistic tasks. We experiment with popular baselines and meta-learners on Meta-Dataset, along with a competitive method that we propose. We analyze performance as a function of various characteristics of test tasks and examine the models' ability to leverage diverse training sources for improving their generalization. We also propose a new set of baselines for quantifying the benefit of meta-learning in Meta-Dataset. Our extensive experimentation has uncovered important research challenges and we hope to inspire work in these directions",
    "checked": true,
    "id": "c5875654fecca84e61a7287b8fbf30d4caccada6",
    "semantic_title": "meta-dataset: a dataset of datasets for learning to learn from few examples",
    "citation_count": 616,
    "authors": []
  },
  "https://openreview.net/forum?id=r1e9GCNKvH": {
    "title": "One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum Evaluation",
    "volume": "poster",
    "abstract": "Recent advances in the sparse neural network literature have made it possible to prune many large feed forward and convolutional networks with only a small quantity of data. Yet, these same techniques often falter when applied to the problem of recovering sparse recurrent networks. These failures are quantitative: when pruned with recent techniques, RNNs typically obtain worse performance than they do under a simple random pruning scheme. The failures are also qualitative: the distribution of active weights in a pruned LSTM or GRU network tend to be concentrated in specific neurons and gates, and not well dispersed across the entire architecture. We seek to rectify both the quantitative and qualitative issues with recurrent network pruning by introducing a new recurrent pruning objective derived from the spectrum of the recurrent Jacobian. Our objective is data efficient (requiring only 64 data points to prune the network), easy to implement, and produces 95 % sparse GRUs that significantly improve on existing baselines. We evaluate on sequential MNIST, Billion Words, and Wikitext",
    "checked": true,
    "id": "7cf7eac260e2f187a2fbe2321dd5478c5747a0a2",
    "semantic_title": "one-shot pruning of recurrent neural networks by jacobian spectrum evaluation",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=rJgqMRVYvr": {
    "title": "Differentially Private Meta-Learning",
    "volume": "poster",
    "abstract": "Parameter-transfer is a well-known and versatile approach for meta-learning, with applications including few-shot learning, federated learning, with personalization, and reinforcement learning. However, parameter-transfer algorithms often require sharing models that have been trained on the samples from specific tasks, thus leaving the task-owners susceptible to breaches of privacy. We conduct the first formal study of privacy in this setting and formalize the notion of task-global differential privacy as a practical relaxation of more commonly studied threat models. We then propose a new differentially private algorithm for gradient-based parameter transfer that not only satisfies this privacy requirement but also retains provable transfer learning guarantees in convex settings. Empirically, we apply our analysis to the problems of federated learning with personalization and few-shot classification, showing that allowing the relaxation to task-global privacy from the more commonly studied notion of local privacy leads to dramatically increased performance in recurrent neural language modeling and image classification",
    "checked": true,
    "id": "eb4fa3f726cf583a5c8ca574ca94543117e7dc97",
    "semantic_title": "differentially private meta-learning",
    "citation_count": 108,
    "authors": []
  },
  "https://openreview.net/forum?id=SJgwzCEKwH": {
    "title": "Bridging Mode Connectivity in Loss Landscapes and Adversarial Robustness",
    "volume": "poster",
    "abstract": "Mode connectivity provides novel geometric insights on analyzing loss landscapes and enables building high-accuracy pathways between well-trained neural networks. In this work, we propose to employ mode connectivity in loss landscapes to study the adversarial robustness of deep neural networks, and provide novel methods for improving this robustness. Our experiments cover various types of adversarial attacks applied to different network architectures and datasets. When network models are tampered with backdoor or error-injection attacks, our results demonstrate that the path connection learned using limited amount of bonafide data can effectively mitigate adversarial effects while maintaining the original accuracy on clean data. Therefore, mode connectivity provides users with the power to repair backdoored or error-injected models. We also use mode connectivity to investigate the loss landscapes of regular and robust models against evasion attacks. Experiments show that there exists a barrier in adversarial robustness loss on the path connecting regular and adversarially-trained models. A high correlation is observed between the adversarial robustness loss and the largest eigenvalue of the input Hessian matrix, for which theoretical justifications are provided. Our results suggest that mode connectivity offers a holistic tool and practical means for evaluating and improving adversarial robustness",
    "checked": true,
    "id": "dc0def651f7ff53d7b3d764924b5c2c28024cdd7",
    "semantic_title": "bridging mode connectivity in loss landscapes and adversarial robustness",
    "citation_count": 188,
    "authors": []
  },
  "https://openreview.net/forum?id=SJeNz04tDS": {
    "title": "Overlearning Reveals Sensitive Attributes",
    "volume": "poster",
    "abstract": "``\"Overlearning'' means that a model trained for a seemingly simple objective implicitly learns to recognize attributes and concepts that are (1) not part of the learning objective, and (2) sensitive from a privacy or bias perspective. For example, a binary gender classifier of facial images also learns to recognize races, even races that are not represented in the training data, and identities. We demonstrate overlearning in several vision and NLP models and analyze its harmful consequences. First, inference-time representations of an overlearned model reveal sensitive attributes of the input, breaking privacy protections such as model partitioning. Second, an overlearned model can be \"`re-purposed'' for a different, privacy-violating task even in the absence of the original training data. We show that overlearning is intrinsic for some tasks and cannot be prevented by censoring unwanted attributes. Finally, we investigate where, when, and why overlearning happens during model training",
    "checked": true,
    "id": "e527f9c52a49b05ab5ccc0c380e46b37b1e80f2b",
    "semantic_title": "overlearning reveals sensitive attributes",
    "citation_count": 148,
    "authors": []
  },
  "https://openreview.net/forum?id=ryebG04YvB": {
    "title": "Adversarially robust transfer learning",
    "volume": "poster",
    "abstract": "Transfer learning, in which a network is trained on one task and re-purposed on another, is often used to produce neural network classifiers when data is scarce or full-scale training is too costly. When the goal is to produce a model that is not only accurate but also adversarially robust, data scarcity and computational limitations become even more cumbersome. We consider robust transfer learning, in which we transfer not only performance but also robustness from a source model to a target domain. We start by observing that robust networks contain robust feature extractors. By training classifiers on top of these feature extractors, we produce new models that inherit the robustness of their parent networks. We then consider the case of \"fine tuning\" a network by re-training end-to-end in the target domain. When using lifelong learning strategies, this process preserves the robustness of the source network while achieving high accuracy. By using such strategies, it is possible to produce accurate and robust models with little data, and without the cost of adversarial training. Additionally, we can improve the generalization of adversarially trained models, while maintaining their robustness",
    "checked": true,
    "id": "0d94293388417458eae73632e33840a772375900",
    "semantic_title": "adversarially robust transfer learning",
    "citation_count": 116,
    "authors": []
  },
  "https://openreview.net/forum?id=S1eRbANtDB": {
    "title": "Learning to Link",
    "volume": "poster",
    "abstract": "Clustering is an important part of many modern data analysis pipelines, including network analysis and data retrieval. There are many different clustering algorithms developed by various communities, and it is often not clear which algorithm will give the best performance on a specific clustering task. Similarly, we often have multiple ways to measure distances between data points, and the best clustering performance might require a non-trivial combination of those metrics. In this work, we study data-driven algorithm selection and metric learning for clustering problems, where the goal is to simultaneously learn the best algorithm and metric for a specific application. The family of clustering algorithms we consider is parameterized linkage based procedures that includes single and complete linkage. The family of distance functions we learn over are convex combinations of base distance functions. We design efficient learning algorithms which receive samples from an application-specific distribution over clustering instances and learn a near-optimal distance and clustering algorithm from these classes. We also carry out a comprehensive empirical evaluation of our techniques showing that they can lead to significantly improved clustering performance on real-world datasets",
    "checked": true,
    "id": "4f6d553265eecdb86691eb7fc225aed434b54ddf",
    "semantic_title": "learning to link",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=BJl6bANtwH": {
    "title": "Detecting Extrapolation with Local Ensembles",
    "volume": "poster",
    "abstract": "We present local ensembles, a method for detecting extrapolation at test time in a pre-trained model. We focus on underdetermination as a key component of extrapolation: we aim to detect when many possible predictions are consistent with the training data and model class. Our method uses local second-order information to approximate the variance of predictions across an ensemble of models from the same class. We compute this approximation by estimating the norm of the component of a test point's gradient that aligns with the low-curvature directions of the Hessian, and provide a tractable method for estimating this quantity. Experimentally, we show that our method is capable of detecting when a pre-trained model is extrapolating on test data, with applications to out-of-distribution detection, detecting spurious correlates, and active learning",
    "checked": true,
    "id": "8e0fbbbd7e269377b2bea40f71472bfd5f3e5d96",
    "semantic_title": "detecting extrapolation with local ensembles",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=B1lnbRNtwr": {
    "title": "Global Relational Models of Source Code",
    "volume": "poster",
    "abstract": "Models of code can learn distributed representations of a program's syntax and semantics to predict many non-trivial properties of a program. Recent state-of-the-art models leverage highly structured representations of programs, such as trees, graphs and paths therein (e.g. data-flow relations), which are precise and abundantly available for code. This provides a strong inductive bias towards semantically meaningful relations, yielding more generalizable representations than classical sequence-based models. Unfortunately, these models primarily rely on graph-based message passing to represent relations in code, which makes them de facto local due to the high cost of message-passing steps, quite in contrast to modern, global sequence-based models, such as the Transformer. In this work, we bridge this divide between global and structured models by introducing two new hybrid model families that are both global and incorporate structural bias: Graph Sandwiches, which wrap traditional (gated) graph message-passing layers in sequential message-passing layers; and Graph Relational Embedding Attention Transformers (GREAT for short), which bias traditional Transformers with relational information from graph edge types. By studying a popular, non-trivial program repair task, variable-misuse identification, we explore the relative merits of traditional and hybrid model families for code representation. Starting with a graph-based model that already improves upon the prior state-of-the-art for this task by 20%, we show that our proposed hybrid models improve an additional 10-15%, while training both faster and using fewer parameters",
    "checked": true,
    "id": "08066c80919620397e8e4e5372ff84caf401e675",
    "semantic_title": "global relational models of source code",
    "citation_count": 202,
    "authors": []
  },
  "https://openreview.net/forum?id=HJg2b0VYDr": {
    "title": "Selection via Proxy: Efficient Data Selection for Deep Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b06d260b504a20e90d07180aa3c4eaecb3b5cde5",
    "semantic_title": "selection via proxy: efficient data selection for deep learning",
    "citation_count": 237,
    "authors": []
  },
  "https://openreview.net/forum?id=Byg5ZANtvH": {
    "title": "Short and Sparse Deconvolution --- A Geometric Approach",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ad489ff2538147dbfcae0362438a100bab466e9a",
    "semantic_title": "short-and-sparse deconvolution - a geometric approach",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=rygFWAEFwS": {
    "title": "Stochastic Weight Averaging in Parallel: Large-Batch Training That Generalizes Well",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8fc7b672ee6dd2ee08cb2315d64be07ff9779e22",
    "semantic_title": "stochastic weight averaging in parallel: large-batch training that generalizes well",
    "citation_count": 58,
    "authors": []
  },
  "https://openreview.net/forum?id=HJe_Z04Yvr": {
    "title": "Adjustable Real-time Style Transfer",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "14506ec513066927c7752c039919e3868dda074a",
    "semantic_title": "adjustable real-time style transfer",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=HkgrZ0EYwB": {
    "title": "Unpaired Point Cloud Completion on Real Scans using Adversarial Training",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5a70224ba2023484bb1860af110e22bc101ccdc8",
    "semantic_title": "unpaired point cloud completion on real scans using adversarial training",
    "citation_count": 127,
    "authors": []
  },
  "https://openreview.net/forum?id=HJxV-ANKDH": {
    "title": "Efficient Riemannian Optimization on the Stiefel Manifold via the Cayley Transform",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6526593e1d5f20dec83b2c23050a53173ed18c66",
    "semantic_title": "efficient riemannian optimization on the stiefel manifold via the cayley transform",
    "citation_count": 105,
    "authors": []
  },
  "https://openreview.net/forum?id=Bkl7bREtDr": {
    "title": "AMRL: Aggregated Memory For Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3222486c25f1c28d6d27cf7c47f78c04ecae7405",
    "semantic_title": "amrl: aggregated memory for reinforcement learning",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=HkgxW0EYDS": {
    "title": "Scalable Model Compression by Entropy Penalized Reparameterization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e79c82249075d9950f9a868fa87e9ad6d4c222b8",
    "semantic_title": "scalable model compression by entropy penalized reparameterization",
    "citation_count": 43,
    "authors": []
  },
  "https://openreview.net/forum?id=SkxybANtDB": {
    "title": "Dynamic Time Lag Regression: Predicting What & When",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0909ecbd252602ba5806689a381adb00e2631adf",
    "semantic_title": "dynamic time lag regression: predicting what & when",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=rJeqeCEtvH": {
    "title": "Semi-Supervised Generative Modeling for Controllable Speech Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7d401cdca689282d9742926bff8d735eef9f5c20",
    "semantic_title": "semi-supervised generative modeling for controllable speech synthesis",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=SJeYe0NtvH": {
    "title": "Neural Text Generation With Unlikelihood Training",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "53a77e8f73f2ca422d6e38fa9ecc490231ac044c",
    "semantic_title": "neural text generation with unlikelihood training",
    "citation_count": 575,
    "authors": []
  },
  "https://openreview.net/forum?id=rkgOlCVYvB": {
    "title": "Pure and Spurious Critical Points: a Geometric Study of Linear Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "74ef8238eecf0efb92bc4dea5cd059776c66d647",
    "semantic_title": "pure and spurious critical points: a geometric study of linear networks",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=BJluxREKDB": {
    "title": "Learning Heuristics for Quantified Boolean Formulas through Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "66c40b2b5e9853632a11e877993ad9efb1e449fd",
    "semantic_title": "learning heuristics for quantified boolean formulas through reinforcement learning",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=BkxXe0Etwr": {
    "title": "CAQL: Continuous Action Q-Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "385057ad5c678b5cd413835b64a813e930c77a4c",
    "semantic_title": "caql: continuous action q-learning",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=BJxWx0NYPr": {
    "title": "Adaptive Structural Fingerprints for Graph Attention Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bc7e9897574ca420066626654b340687f3b33b86",
    "semantic_title": "adaptive structural fingerprints for graph attention networks",
    "citation_count": 81,
    "authors": []
  },
  "https://openreview.net/forum?id=HklkeR4KPB": {
    "title": "ReMixMatch: Semi-Supervised Learning with Distribution Matching and Augmentation Anchoring",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9c5e6395bee6c46273fec3bde416ca07c7b1ad6a",
    "semantic_title": "remixmatch: semi-supervised learning with distribution matching and augmentation anchoring",
    "citation_count": 420,
    "authors": []
  },
  "https://openreview.net/forum?id=B1l6y0VFPr": {
    "title": "Identity Crisis: Memorization and Generalization Under Extreme Overparameterization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8f977a3831132ffaad2f13eea05c1fa46205b8ec",
    "semantic_title": "identity crisis: memorization and generalization under extreme overparameterization",
    "citation_count": 90,
    "authors": []
  },
  "https://openreview.net/forum?id=HJe_yR4Fwr": {
    "title": "Improved Sample Complexities for Deep Neural Networks and Robust Classification via an All-Layer Margin",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "801218f718e04bc98a01ca4eb64fab56107520e8",
    "semantic_title": "improved sample complexities for deep networks and robust classification via an all-layer margin",
    "citation_count": 85,
    "authors": []
  },
  "https://openreview.net/forum?id=H1lmyRNFvr": {
    "title": "Augmenting Genetic Algorithms with Deep Neural Networks for Exploring the Chemical Space",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "cbe6238afd650965a69eb77bc6eb07c183a10351",
    "semantic_title": "augmenting genetic algorithms with deep neural networks for exploring the chemical space",
    "citation_count": 131,
    "authors": []
  },
  "https://openreview.net/forum?id=rkgMkCEtPB": {
    "title": "Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "abf5478c24664a1380b7e213a3ab1c4af54775d0",
    "semantic_title": "rapid learning or feature reuse? towards understanding the effectiveness of maml",
    "citation_count": 644,
    "authors": []
  },
  "https://openreview.net/forum?id=Hyg-JC4FDr": {
    "title": "Imitation Learning via Off-Policy Distribution Matching",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f8ee037f8c82ad8a9b01eec935f024143193bd81",
    "semantic_title": "imitation learning via off-policy distribution matching",
    "citation_count": 204,
    "authors": []
  },
  "https://openreview.net/forum?id=S1ly10EKDS": {
    "title": "Reanalysis of Variance Reduced Temporal Difference Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d72af20c174ed39f45a1538c2fe9444c26686243",
    "semantic_title": "reanalysis of variance reduced temporal difference learning",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=SygpC6Ntvr": {
    "title": "Minimizing FLOPs to Learn Efficient Sparse Representations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9f753f67da834e59f9a5c8cdf9a88ee84c496b2d",
    "semantic_title": "minimizing flops to learn efficient sparse representations",
    "citation_count": 44,
    "authors": []
  },
  "https://openreview.net/forum?id=HyxLRTVKPH": {
    "title": "Budgeted Training: Rethinking Deep Neural Network Training Under Resource Constraints",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6885dc17f1607f1947721ef4c430f1eda22f1228",
    "semantic_title": "budgeted training: rethinking deep neural network training under resource constraints",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=HkgH0TEYwH": {
    "title": "Deep Semi-Supervised Anomaly Detection",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4c1abd8969fc1c360f50373f6552bcfb3cc408b7",
    "semantic_title": "deep semi-supervised anomaly detection",
    "citation_count": 545,
    "authors": []
  },
  "https://openreview.net/forum?id=SygW0TEFwH": {
    "title": "Sign Bits Are All You Need for Black-Box Attacks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9ac39596269d614d63c828e9747b8dba08d87b54",
    "semantic_title": "sign bits are all you need for black-box attacks",
    "citation_count": 78,
    "authors": []
  },
  "https://openreview.net/forum?id=SkgC6TNFvr": {
    "title": "Reinforced active learning for image segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bc69db360d720ebefdbb66dc02409142505e5212",
    "semantic_title": "reinforced active learning for image segmentation",
    "citation_count": 108,
    "authors": []
  },
  "https://openreview.net/forum?id=HylsTT4FvB": {
    "title": "On the \"steerability\" of generative adversarial networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9fac2ad81a561398506ce566d897f7169583b4f8",
    "semantic_title": "on the \"steerability\" of generative adversarial networks",
    "citation_count": 397,
    "authors": []
  },
  "https://openreview.net/forum?id=rkgO66VKDS": {
    "title": "LEARNED STEP SIZE QUANTIZATION",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "dc160709bbe528b506a37ead334f60d258413357",
    "semantic_title": "learned step size quantization",
    "citation_count": 800,
    "authors": []
  },
  "https://openreview.net/forum?id=S1lSapVtwS": {
    "title": "Stochastic Conditional Generative Networks with Basis Decomposition",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "33ac05a1d04b09c393fa130d653680f3ecc4ba1f",
    "semantic_title": "stochastic conditional generative networks with basis decomposition",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=BJgza6VtPB": {
    "title": "Language GANs Falling Short",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BkxfaTVFwH": {
    "title": "GENESIS: Generative Scene Inference and Sampling with Object-Centric Latent Representations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a50b7a45f704f30d7f97dd229d4d53433d5df3b1",
    "semantic_title": "genesis: generative scene inference and sampling with object-centric latent representations",
    "citation_count": 307,
    "authors": []
  },
  "https://openreview.net/forum?id=B1x62TNtDS": {
    "title": "Understanding the Limitations of Variational Mutual Information Estimators",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ab0975b74203fb722614d33081345e75b3cecaae",
    "semantic_title": "understanding the limitations of variational mutual information estimators",
    "citation_count": 165,
    "authors": []
  },
  "https://openreview.net/forum?id=BkgnhTEtDS": {
    "title": "Feature Interaction Interpretability: A Case for Explaining Ad-Recommendation Systems via Neural Interaction Detection",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "dd35d943039d3de7ec38477e3d194e04c70dcf25",
    "semantic_title": "feature interaction interpretability: a case for explaining ad-recommendation systems via neural interaction detection",
    "citation_count": 57,
    "authors": []
  },
  "https://openreview.net/forum?id=SyxL2TNtvr": {
    "title": "Unsupervised Model Selection for Variational Disentangled Representation Learning",
    "volume": "poster",
    "abstract": "Disentangled representations have recently been shown to improve fairness, data efficiency and generalisation in simple supervised and reinforcement learning tasks. To extend the benefits of disentangled representations to more complex domains and practical applications, it is important to enable hyperparameter tuning and model selection of existing unsupervised approaches without requiring access to ground truth attribute labels, which are not available for most datasets. This paper addresses this problem by introducing a simple yet robust and reliable method for unsupervised disentangled model selection. We show that our approach performs comparably to the existing supervised alternatives across 5400 models from six state of the art unsupervised disentangled representation learning model classes. Furthermore, we show that the ranking produced by our approach correlates well with the final task performance on two different domains",
    "checked": true,
    "id": "148870880728bb2059dbf8174a70f97a682b3e58",
    "semantic_title": "unsupervised model selection for variational disentangled representation learning",
    "citation_count": 80,
    "authors": []
  },
  "https://openreview.net/forum?id=HkgB2TNYPS": {
    "title": "A Theoretical Analysis of the Number of Shots in Few-Shot Learning",
    "volume": "poster",
    "abstract": "Few-shot classification is the task of predicting the category of an example from a set of few labeled examples. The number of labeled examples per category is called the number of shots (or shot number). Recent works tackle this task through meta-learning, where a meta-learner extracts information from observed tasks during meta-training to quickly adapt to new tasks during meta-testing. In this formulation, the number of shots exploited during meta-training has an impact on the recognition performance at meta-test time. Generally, the shot number used in meta-training should match the one used in meta-testing to obtain the best performance. We introduce a theoretical analysis of the impact of the shot number on Prototypical Networks, a state-of-the-art few-shot classification method. From our analysis, we propose a simple method that is robust to the choice of shot number used during meta-training, which is a crucial hyperparameter. The performance of our model trained for an arbitrary meta-training shot number shows great performance for different values of meta-testing shot numbers. We experimentally demonstrate our approach on different few-shot classification benchmarks",
    "checked": true,
    "id": "537f0af15dbf5bf91ca653f3d405fd5095d9007b",
    "semantic_title": "a theoretical analysis of the number of shots in few-shot learning",
    "citation_count": 63,
    "authors": []
  },
  "https://openreview.net/forum?id=H1lmhaVtvr": {
    "title": "Dynamical Distance Learning for Semi-Supervised and Unsupervised Skill Discovery",
    "volume": "poster",
    "abstract": "Reinforcement learning requires manual specification of a reward function to learn a task. While in principle this reward function only needs to specify the task goal, in practice reinforcement learning can be very time-consuming or even infeasible unless the reward function is shaped so as to provide a smooth gradient towards a successful outcome. This shaping is difficult to specify by hand, particularly when the task is learned from raw observations, such as images. In this paper, we study how we can automatically learn dynamical distances: a measure of the expected number of time steps to reach a given goal state from any other state. These dynamical distances can be used to provide well-shaped reward functions for reaching new goals, making it possible to learn complex tasks efficiently. We show that dynamical distances can be used in a semi-supervised regime, where unsupervised interaction with the environment is used to learn the dynamical distances, while a small amount of preference supervision is used to determine the task goal, without any manually engineered reward function or goal examples. We evaluate our method both on a real-world robot and in simulation. We show that our method can learn to turn a valve with a real-world 9-DoF hand, using raw image observations and just ten preference labels, without any other supervision. Videos of the learned skills can be found on the project website: https://sites.google.com/view/dynamical-distance-learning",
    "checked": true,
    "id": "294bba666eb44689f21839c3013819c3030eec25",
    "semantic_title": "dynamical distance learning for semi-supervised and unsupervised skill discovery",
    "citation_count": 79,
    "authors": []
  },
  "https://openreview.net/forum?id=rkgz2aEKDr": {
    "title": "On the Variance of the Adaptive Learning Rate and Beyond",
    "volume": "poster",
    "abstract": "The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate -- its variance is problematically large in the early stage, and presume warmup works as a variance reduction technique. We provide both empirical and theoretical evidence to verify our hypothesis. We further propose Rectified Adam (RAdam), a novel variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the efficacy and robustness of RAdam",
    "checked": true,
    "id": "2bf7c350a8280e7c593d46a60127f99b21517121",
    "semantic_title": "on the variance of the adaptive learning rate and beyond",
    "citation_count": 1902,
    "authors": []
  },
  "https://openreview.net/forum?id=HyxG3p4twS": {
    "title": "Quantifying the Cost of Reliable Photo Authentication via High-Performance Learned Lossy Representations",
    "volume": "poster",
    "abstract": "Detection of photo manipulation relies on subtle statistical traces, notoriously removed by aggressive lossy compression employed online. We demonstrate that end-to-end modeling of complex photo dissemination channels allows for codec optimization with explicit provenance objectives. We design a lightweight trainable lossy image codec, that delivers competitive rate-distortion performance, on par with best hand-engineered alternatives, but has lower computational footprint on modern GPU-enabled platforms. Our results show that significant improvements in manipulation detection accuracy are possible at fractional costs in bandwidth/storage. Our codec improved the accuracy from 37% to 86% even at very low bit-rates, well below the practicality of JPEG (QF 20)",
    "checked": true,
    "id": "fe34b8fafe636303750a93330b2414e51dc661bf",
    "semantic_title": "quantifying the cost of reliable photo authentication via high-performance learned lossy representations",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B1gqipNYwH": {
    "title": "Option Discovery using Deep Skill Chaining",
    "volume": "poster",
    "abstract": "Autonomously discovering temporally extended actions, or skills, is a longstanding goal of hierarchical reinforcement learning. We propose a new algorithm that combines skill chaining with deep neural networks to autonomously discover skills in high-dimensional, continuous domains. The resulting algorithm, deep skill chaining, constructs skills with the property that executing one enables the agent to execute another. We demonstrate that deep skill chaining significantly outperforms both non-hierarchical agents and other state-of-the-art skill discovery techniques in challenging continuous control tasks",
    "checked": true,
    "id": "e1a39a6614503546bbb72a8c75aaf0ae93a3ac01",
    "semantic_title": "option discovery using deep skill chaining",
    "citation_count": 111,
    "authors": []
  },
  "https://openreview.net/forum?id=SJeLopEYDH": {
    "title": "V4D: 4D Convolutional Neural Networks for Video-level Representation Learning",
    "volume": "poster",
    "abstract": "Most existing 3D CNN structures for video representation learning are clip-based methods, and do not consider video-level temporal evolution of spatio-temporal features. In this paper, we propose Video-level 4D Convolutional Neural Networks, namely V4D, to model the evolution of long-range spatio-temporal representation with 4D convolutions, as well as preserving 3D spatio-temporal representations with residual connections. We further introduce the training and inference methods for the proposed V4D. Extensive experiments are conducted on three video recognition benchmarks, where V4D achieves excellent results, surpassing recent 3D CNNs by a large margin",
    "checked": true,
    "id": "df2f2591054080d069e563cb9ca4e0592bc6df08",
    "semantic_title": "v4d: 4d convolutional neural networks for video-level representation learning",
    "citation_count": 70,
    "authors": []
  },
  "https://openreview.net/forum?id=rylHspEKPr": {
    "title": "Learning to Represent Programs with Property Signatures",
    "volume": "poster",
    "abstract": "We introduce the notion of property signatures, a representation for programs and program specifications meant for consumption by machine learning algorithms. Given a function with input type τ_in and output type τ_out, a property is a function of type: (τ_in, τ_out) → Bool that (informally) describes some simple property of the function under consideration. For instance, if τ_in and τ_out are both lists of the same type, one property might ask ‘is the input list the same length as the output list?'. If we have a list of such properties, we can evaluate them all for our function to get a list of outputs that we will call the property signature. Crucially, we can ‘guess' the property signature for a function given only a set of input/output pairs meant to specify that function. We discuss several potential applications of property signatures and show experimentally that they can be used to improve over a baseline synthesizer so that it emits twice as many programs in less than one-tenth of the time",
    "checked": true,
    "id": "e41452747ac0674a7b6534e78be33134fe8ef650",
    "semantic_title": "learning to represent programs with property signatures",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=SJg7spEYDS": {
    "title": "Generative Ratio Matching Networks",
    "volume": "poster",
    "abstract": "Deep generative models can learn to generate realistic-looking images, but many of the most effective methods are adversarial and involve a saddlepoint optimization, which requires a careful balancing of training between a generator network and a critic network. Maximum mean discrepancy networks (MMD-nets) avoid this issue by using kernel as a fixed adversary, but unfortunately, they have not on their own been able to match the generative quality of adversarial training. In this work, we take their insight of using kernels as fixed adversaries further and present a novel method for training deep generative models that does not involve saddlepoint optimization. We call our method generative ratio matching or GRAM for short. In GRAM, the generator and the critic networks do not play a zero-sum game against each other, instead, they do so against a fixed kernel. Thus GRAM networks are not only stable to train like MMD-nets but they also match and beat the generative quality of adversarially trained generative networks",
    "checked": true,
    "id": "f22426261987752baa78d0cfe87b82095ec40fb5",
    "semantic_title": "generative ratio matching networks",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=SJx-j64FDr": {
    "title": "In Search for a SAT-friendly Binarized Neural Network Architecture",
    "volume": "poster",
    "abstract": "Analyzing the behavior of neural networks is one of the most pressing challenges in deep learning. Binarized Neural Networks are an important class of networks that allow equivalent representation in Boolean logic and can be analyzed formally with logic-based reasoning tools like SAT solvers. Such tools can be used to answer existential and probabilistic queries about the network, perform explanation generation, etc. However, the main bottleneck for all methods is their ability to reason about large BNNs efficiently. In this work, we analyze architectural design choices of BNNs and discuss how they affect the performance of logic-based reasoners. We propose changes to the BNN architecture and the training procedure to get a simpler network for SAT solvers without sacrificing accuracy on the primary task. Our experimental results demonstrate that our approach scales to larger deep neural networks compared to existing work for existential and probabilistic queries, leading to significant speed ups on all tested datasets",
    "checked": true,
    "id": "bde2a5857047035a48d851c025e6e1df4ea02421",
    "semantic_title": "in search for a sat-friendly binarized neural network architecture",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=H1lhqpEYPr": {
    "title": "Actor-Critic Provably Finds Nash Equilibria of Linear-Quadratic Mean-Field Games",
    "volume": "poster",
    "abstract": "We study discrete-time mean-field Markov games with infinite numbers of agents where each agent aims to minimize its ergodic cost. We consider the setting where the agents have identical linear state transitions and quadratic cost func- tions, while the aggregated effect of the agents is captured by the population mean of their states, namely, the mean-field state. For such a game, based on the Nash certainty equivalence principle, we provide sufficient conditions for the existence and uniqueness of its Nash equilibrium. Moreover, to find the Nash equilibrium, we propose a mean-field actor-critic algorithm with linear function approxima- tion, which does not require knowing the model of dynamics. Specifically, at each iteration of our algorithm, we use the single-agent actor-critic algorithm to approximately obtain the optimal policy of the each agent given the current mean- field state, and then update the mean-field state. In particular, we prove that our algorithm converges to the Nash equilibrium at a linear rate. To the best of our knowledge, this is the first success of applying model-free reinforcement learn- ing with function approximation to discrete-time mean-field Markov games with provable non-asymptotic global convergence guarantees",
    "checked": true,
    "id": "170e6263799707bb301f3b48fd5a73c332807403",
    "semantic_title": "actor-critic provably finds nash equilibria of linear-quadratic mean-field games",
    "citation_count": 54,
    "authors": []
  },
  "https://openreview.net/forum?id=SkgscaNYPS": {
    "title": "The asymptotic spectrum of the Hessian of DNN throughout training",
    "volume": "poster",
    "abstract": "The dynamics of DNNs during gradient descent is described by the so-called Neural Tangent Kernel (NTK). In this article, we show that the NTK allows one to gain precise insight into the Hessian of the cost of DNNs: we obtain a full characterization of the asymptotics of the spectrum of the Hessian, at initialization and during training",
    "checked": true,
    "id": "804ca606c0bbca15a2b6f345da3f43bdeb612423",
    "semantic_title": "the asymptotic spectrum of the hessian of dnn throughout training",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=HJxK5pEYvr": {
    "title": "Tree-Structured Attention with Hierarchical Accumulation",
    "volume": "poster",
    "abstract": "Incorporating hierarchical structures like constituency trees has been shown to be effective for various natural language processing (NLP) tasks. However, it is evident that state-of-the-art (SOTA) sequence-based models like the Transformer struggle to encode such structures inherently. On the other hand, dedicated models like the Tree-LSTM, while explicitly modeling hierarchical structures, do not perform as efficiently as the Transformer. In this paper, we attempt to bridge this gap with Hierarchical Accumulation to encode parse tree structures into self-attention at constant time complexity. Our approach outperforms SOTA methods in four IWSLT translation tasks and the WMT'14 English-German task. It also yields improvements over Transformer and Tree-LSTM on three text classification tasks. We further demonstrate that using hierarchical priors can compensate for data shortage, and that our model prefers phrase-level attentions over token-level attentions",
    "checked": true,
    "id": "4ecf4356c3b451b16780788a3f94e422d4deeda5",
    "semantic_title": "tree-structured attention with hierarchical accumulation",
    "citation_count": 76,
    "authors": []
  },
  "https://openreview.net/forum?id=B1gF56VYPH": {
    "title": "Deep 3D Pan via local adaptive \"t-shaped\" convolutions with global and local adaptive dilations",
    "volume": "poster",
    "abstract": "Recent advances in deep learning have shown promising results in many low-level vision tasks. However, solving the single-image-based view synthesis is still an open problem. In particular, the generation of new images at parallel camera views given a single input image is of great interest, as it enables 3D visualization of the 2D input scenery. We propose a novel network architecture to perform stereoscopic view synthesis at arbitrary camera positions along the X-axis, or \"Deep 3D Pan\", with \"t-shaped\" adaptive kernels equipped with globally and locally adaptive dilations. Our proposed network architecture, the monster-net, is devised with a novel t-shaped adaptive kernel with globally and locally adaptive dilation, which can efficiently incorporate global camera shift into and handle local 3D geometries of the target image's pixels for the synthesis of naturally looking 3D panned views when a 2-D input image is given. Extensive experiments were performed on the KITTI, CityScapes, and our VICLAB_STEREO indoors dataset to prove the efficacy of our method. Our monster-net significantly outperforms the state-of-the-art method (SOTA) by a large margin in all metrics of RMSE, PSNR, and SSIM. Our proposed monster-net is capable of reconstructing more reliable image structures in synthesized images with coherent geometry. Moreover, the disparity information that can be extracted from the \"t-shaped\" kernel is much more reliable than that of the SOTA for the unsupervised monocular depth estimation task, confirming the effectiveness of our method",
    "checked": true,
    "id": "d23c551f8b8f3990077a4b006c0018b8b3b0c56f",
    "semantic_title": "deep 3d pan via local adaptive \"t-shaped\" convolutions with global and local adaptive dilations",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=rJeIcTNtvS": {
    "title": "Low-Resource Knowledge-Grounded Dialogue Generation",
    "volume": "poster",
    "abstract": "Responding with knowledge has been recognized as an important capability for an intelligent conversational agent. Yet knowledge-grounded dialogues, as training data for learning such a response generation model, are difficult to obtain. Motivated by the challenge in practice, we consider knowledge-grounded dialogue generation under a natural assumption that only limited training examples are available. In such a low-resource setting, we devise a disentangled response decoder in order to isolate parameters that depend on knowledge-grounded dialogues from the entire generation model. By this means, the major part of the model can be learned from a large number of ungrounded dialogues and unstructured documents, while the remaining small parameters can be well fitted using the limited training examples. Evaluation results on two benchmarks indicate that with only $1/8$ training data, our model can achieve the state-of-the-art performance and generalize well on out-of-domain knowledge",
    "checked": true,
    "id": "2fe459678943528b6ec391ac9f255fb444c33e33",
    "semantic_title": "low-resource knowledge-grounded dialogue generation",
    "citation_count": 109,
    "authors": []
  },
  "https://openreview.net/forum?id=BylVcTNtDS": {
    "title": "A Target-Agnostic Attack on Deep Models: Exploiting Security Vulnerabilities of Transfer Learning",
    "volume": "poster",
    "abstract": "Due to insufficient training data and the high computational cost to train a deep neural network from scratch, transfer learning has been extensively used in many deep-neural-network-based applications. A commonly used transfer learning approach involves taking a part of a pre-trained model, adding a few layers at the end, and re-training the new layers with a small dataset. This approach, while efficient and widely used, imposes a security vulnerability because the pre-trained model used in transfer learning is usually publicly available, including to potential attackers. In this paper, we show that without any additional knowledge other than the pre-trained model, an attacker can launch an effective and efficient brute force attack that can craft instances of input to trigger each target class with high confidence. We assume that the attacker has no access to any target-specific information, including samples from target classes, re-trained model, and probabilities assigned by Softmax to each class, and thus making the attack target-agnostic. These assumptions render all previous attack models inapplicable, to the best of our knowledge. To evaluate the proposed attack, we perform a set of experiments on face recognition and speech recognition tasks and show the effectiveness of the attack. Our work reveals a fundamental security weakness of the Softmax layer when used in transfer learning settings",
    "checked": true,
    "id": "9286e79ee70745c3065ace3b04deef0fa2885486",
    "semantic_title": "a target-agnostic attack on deep models: exploiting security vulnerabilities of transfer learning",
    "citation_count": 46,
    "authors": []
  },
  "https://openreview.net/forum?id=BJl-5pNKDB": {
    "title": "On Computation and Generalization of Generative Adversarial Imitation Learning",
    "volume": "poster",
    "abstract": "Generative Adversarial Imitation Learning (GAIL) is a powerful and practical approach for learning sequential decision-making policies. Different from Reinforcement Learning (RL), GAIL takes advantage of demonstration data by experts (e.g., human), and learns both the policy and reward function of the unknown environment. Despite the significant empirical progresses, the theory behind GAIL is still largely unknown. The major difficulty comes from the underlying temporal dependency of the demonstration data and the minimax computational formulation of GAIL without convex-concave structure. To bridge such a gap between theory and practice, this paper investigates the theoretical properties of GAIL. Specifically, we show: (1) For GAIL with general reward parameterization, the generalization can be guaranteed as long as the class of the reward functions is properly controlled; (2) For GAIL, where the reward is parameterized as a reproducing kernel function, GAIL can be efficiently solved by stochastic first order optimization algorithms, which attain sublinear convergence to a stationary solution. To the best of our knowledge, these are the first results on statistical and computational guarantees of imitation learning with reward/policy function ap- proximation. Numerical experiments are provided to support our analysis",
    "checked": true,
    "id": "7084faf2844affdebbcabc7f46c36d6e0e4dcb1e",
    "semantic_title": "on computation and generalization of generative adversarial imitation learning",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=Bkeeca4Kvr": {
    "title": "FEW-SHOT LEARNING ON GRAPHS VIA SUPER-CLASSES BASED ON GRAPH SPECTRAL MEASURES",
    "volume": "poster",
    "abstract": "We propose to study the problem of few-shot graph classification in graph neural networks (GNNs) to recognize unseen classes, given limited labeled graph examples. Despite several interesting GNN variants being proposed recently for node and graph classification tasks, when faced with scarce labeled examples in the few-shot setting, these GNNs exhibit significant loss in classification performance. Here, we present an approach where a probability measure is assigned to each graph based on the spectrum of the graph's normalized Laplacian. This enables us to accordingly cluster the graph base-labels associated with each graph into super-classes, where the L^p Wasserstein distance serves as our underlying distance metric. Subsequently, a super-graph constructed based on the super-classes is then fed to our proposed GNN framework which exploits the latent inter-class relationships made explicit by the super-graph to achieve better class label separation among the graphs. We conduct exhaustive empirical evaluations of our proposed method and show that it outperforms both the adaptation of state-of-the-art graph classification methods to few-shot scenario and our naive baseline GNNs. Additionally, we also extend and study the behavior of our method to semi-supervised and active learning scenarios",
    "checked": true,
    "id": "13d1f71036278249ad476eda58d38db48d881db9",
    "semantic_title": "few-shot learning on graphs via super-classes based on graph spectral measures",
    "citation_count": 70,
    "authors": []
  },
  "https://openreview.net/forum?id=rylnK6VtDH": {
    "title": "Multiplicative Interactions and Where to Find Them",
    "volume": "poster",
    "abstract": "We explore the role of multiplicative interaction as a unifying framework to describe a range of classical and modern neural network architectural motifs, such as gating, attention layers, hypernetworks, and dynamic convolutions amongst others. Multiplicative interaction layers as primitive operations have a long-established presence in the literature, though this often not emphasized and thus under-appreciated. We begin by showing that such layers strictly enrich the representable function classes of neural networks. We conjecture that multiplicative interactions offer a particularly powerful inductive bias when fusing multiple streams of information or when conditional computation is required. We therefore argue that they should be considered in many situation where multiple compute or information paths need to be combined, in place of the simple and oft-used concatenation operation. Finally, we back up our claims and demonstrate the potential of multiplicative interactions by applying them in large-scale complex RL and sequence modelling tasks, where their use allows us to deliver state-of-the-art results, and thereby provides new evidence in support of multiplicative interactions playing a more prominent role when designing new neural network architectures",
    "checked": true,
    "id": "caa31faab39d34ecbb8100911640dfaec76f9ee9",
    "semantic_title": "multiplicative interactions and where to find them",
    "citation_count": 131,
    "authors": []
  },
  "https://openreview.net/forum?id=SJlsFpVtDB": {
    "title": "Continual Learning with Bayesian Neural Networks for Non-Stationary Data",
    "volume": "poster",
    "abstract": "This work addresses continual learning for non-stationary data, using Bayesian neural networks and memory-based online variational Bayes. We represent the posterior approximation of the network weights by a diagonal Gaussian distribution and a complementary memory of raw data. This raw data corresponds to likelihood terms that cannot be well approximated by the Gaussian. We introduce a novel method for sequentially updating both components of the posterior approximation. Furthermore, we propose Bayesian forgetting and a Gaussian diffusion process for adapting to non-stationary data. The experimental results show that our update method improves on existing approaches for streaming data. Additionally, the adaptation methods lead to better predictive performance for non-stationary data",
    "checked": true,
    "id": "6c1c394765beb58affe881c22543f52d326c4b9e",
    "semantic_title": "continual learning with bayesian neural networks for non-stationary data",
    "citation_count": 73,
    "authors": []
  },
  "https://openreview.net/forum?id=rye5YaEtPr": {
    "title": "SAdam: A Variant of Adam for Strongly Convex Functions",
    "volume": "poster",
    "abstract": "The Adam algorithm has become extremely popular for large-scale machine learning. Under convexity condition, it has been proved to enjoy a data-dependent $O(\\sqrt{T})$ regret bound where $T$ is the time horizon. However, whether strong convexity can be utilized to further improve the performance remains an open problem. In this paper, we give an affirmative answer by developing a variant of Adam (referred to as SAdam) which achieves a data-dependent $O(\\log T)$ regret bound for strongly convex functions. The essential idea is to maintain a faster decaying yet under controlled step size for exploiting strong convexity. In addition, under a special configuration of hyperparameters, our SAdam reduces to SC-RMSprop, a recently proposed variant of RMSprop for strongly convex functions, for which we provide the first data-dependent logarithmic regret bound. Empirical results on optimizing strongly convex functions and training deep networks demonstrate the effectiveness of our method",
    "checked": true,
    "id": "f47f26e500f190b13f989e0dbe56f49f3bc4c260",
    "semantic_title": "sadam: a variant of adam for strongly convex functions",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=r1e_FpNFDr": {
    "title": "Generalization bounds for deep convolutional neural networks",
    "volume": "poster",
    "abstract": "We prove bounds on the generalization error of convolutional networks. The bounds are in terms of the training loss, the number of parameters, the Lipschitz constant of the loss and the distance from the weights to the initial weights. They are independent of the number of pixels in the input, and the height and width of hidden feature maps. We present experiments using CIFAR-10 with varying hyperparameters of a deep convolutional network, comparing our bounds with practical generalization gaps",
    "checked": true,
    "id": "157605ff0f90b8193bbbcaf2e9d469d9b73be0c2",
    "semantic_title": "generalization bounds for deep convolutional neural networks",
    "citation_count": 90,
    "authors": []
  },
  "https://openreview.net/forum?id=HygDF6NFPB": {
    "title": "A Fair Comparison of Graph Neural Networks for Graph Classification",
    "volume": "poster",
    "abstract": "Experimental reproducibility and replicability are critical topics in machine learning. Authors have often raised concerns about their lack in scientific publications to improve the quality of the field. Recently, the graph representation learning field has attracted the attention of a wide research community, which resulted in a large stream of works. As such, several Graph Neural Network models have been developed to effectively tackle graph classification. However, experimental procedures often lack rigorousness and are hardly reproducible. Motivated by this, we provide an overview of common practices that should be avoided to fairly compare with the state of the art. To counter this troubling trend, we ran more than 47000 experiments in a controlled and uniform framework to re-evaluate five popular models across nine common benchmarks. Moreover, by comparing GNNs with structure-agnostic baselines we provide convincing evidence that, on some datasets, structural information has not been exploited yet. We believe that this work can contribute to the development of the graph learning field, by providing a much needed grounding for rigorous evaluations of graph classification models",
    "checked": true,
    "id": "3db5fcb595492dcd64663c00d56f004dfafa689c",
    "semantic_title": "a fair comparison of graph neural networks for graph classification",
    "citation_count": 445,
    "authors": []
  },
  "https://openreview.net/forum?id=rylvYaNYDH": {
    "title": "Finding and Visualizing Weaknesses of Deep Reinforcement Learning Agents",
    "volume": "poster",
    "abstract": "As deep reinforcement learning driven by visual perception becomes more widely used there is a growing need to better understand and probe the learned agents. Understanding the decision making process and its relationship to visual inputs can be very valuable to identify problems in learned behavior. However, this topic has been relatively under-explored in the research community. In this work we present a method for synthesizing visual inputs of interest for a trained agent. Such inputs or states could be situations in which specific actions are necessary. Further, critical states in which a very high or a very low reward can be achieved are often interesting to understand the situational awareness of the system as they can correspond to risky states. To this end, we learn a generative model over the state space of the environment and use its latent space to optimize a target function for the state of interest. In our experiments we show that this method can generate insights for a variety of environments and reinforcement learning methods. We explore results in the standard Atari benchmark games as well as in an autonomous driving simulator. Based on the efficiency with which we have been able to identify behavioural weaknesses with this technique, we believe this general approach could serve as an important tool for AI safety applications",
    "checked": true,
    "id": "ef58d0447069ef916003f9897502fc78af740609",
    "semantic_title": "finding and visualizing weaknesses of deep reinforcement learning agents",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=SkxLFaNKwB": {
    "title": "Computation Reallocation for Object Detection",
    "volume": "poster",
    "abstract": "The allocation of computation resources in the backbone is a crucial issue in object detection. However, classification allocation pattern is usually adopted directly to object detector, which is proved to be sub-optimal. In order to reallocate the engaged computation resources in a more efficient way, we present CR-NAS (Computation Reallocation Neural Architecture Search) that can learn computation reallocation strategies across different feature resolution and spatial position diectly on the target detection dataset. A two-level reallocation space is proposed for both stage and spatial reallocation. A novel hierarchical search procedure is adopted to cope with the complex search space. We apply CR-NAS to multiple backbones and achieve consistent improvements. Our CR-ResNet50 and CR-MobileNetV2 outperforms the baseline by 1.9% and 1.7% COCO AP respectively without any additional computation budget. The models discovered by CR-NAS can be equiped to other powerful detection neck/head and be easily transferred to other dataset, e.g. PASCAL VOC, and other vision tasks, e.g. instance segmentation. Our CR-NAS can be used as a plugin to improve the performance of various networks, which is demanding",
    "checked": true,
    "id": "8eb458dd33c177f96a97944e6ce977512db0033c",
    "semantic_title": "computation reallocation for object detection",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=S1g7tpEYDS": {
    "title": "From Variational to Deterministic Autoencoders",
    "volume": "poster",
    "abstract": "Variational Autoencoders (VAEs) provide a theoretically-backed and popular framework for deep generative models. However, learning a VAE from data poses still unanswered theoretical questions and considerable practical challenges. In this work, we propose an alternative framework for generative modeling that is simpler, easier to train, and deterministic, yet has many of the advantages of the VAE. We observe that sampling a stochastic encoder in a Gaussian VAE can be interpreted as simply injecting noise into the input of a deterministic decoder. We investigate how substituting this kind of stochasticity, with other explicit and implicit regularization schemes, can lead to an equally smooth and meaningful latent space without having to force it to conform to an arbitrarily chosen prior. To retrieve a generative mechanism to sample new data points, we introduce an ex-post density estimation step that can be readily applied to the proposed framework as well as existing VAEs, improving their sample quality. We show, in a rigorous empirical study, that the proposed regularized deterministic autoencoders are able to generate samples that are comparable to, or better than, those of VAEs and more powerful alternatives when applied to images as well as to structured data such as molecules",
    "checked": true,
    "id": "622e392f8c5da161cf61582af434f6976094dfc4",
    "semantic_title": "from variational to deterministic autoencoders",
    "citation_count": 242,
    "authors": []
  },
  "https://openreview.net/forum?id=H1gfFaEYDS": {
    "title": "Adversarially Robust Representations with Smooth Encoders",
    "volume": "poster",
    "abstract": "This paper studies the undesired phenomena of over-sensitivity of representations learned by deep networks to semantically-irrelevant changes in data. We identify a cause for this shortcoming in the classical Variational Auto-encoder (VAE) objective, the evidence lower bound (ELBO). We show that the ELBO fails to control the behaviour of the encoder out of the support of the empirical data distribution and this behaviour of the VAE can lead to extreme errors in the learned representation. This is a key hurdle in the effective use of representations for data-efficient learning and transfer. To address this problem, we propose to augment the data with specifications that enforce insensitivity of the representation with respect to families of transformations. To incorporate these specifications, we propose a regularization method that is based on a selection mechanism that creates a fictive data point by explicitly perturbing an observed true data point. For certain choices of parameters, our formulation naturally leads to the minimization of the entropy regularized Wasserstein distance between representations. We illustrate our approach on standard datasets and experimentally show that significant improvements in the downstream adversarial accuracy can be achieved by learning robust representations completely in an unsupervised manner, without a reference to a particular downstream task and without a costly supervised adversarial training procedure",
    "checked": true,
    "id": "5ecd39a7797978bf0fd5682f93eddb195ad4a1ea",
    "semantic_title": "adversarially robust representations with smooth encoders",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=SJgMK64Ywr": {
    "title": "AssembleNet: Searching for Multi-Stream Neural Connectivity in Video Architectures",
    "volume": "poster",
    "abstract": "Learning to represent videos is a very challenging task both algorithmically and computationally. Standard video CNN architectures have been designed by directly extending architectures devised for image understanding to include the time dimension, using modules such as 3D convolutions, or by using two-stream design to capture both appearance and motion in videos. We interpret a video CNN as a collection of multi-stream convolutional blocks connected to each other, and propose the approach of automatically finding neural architectures with better connectivity and spatio-temporal interactions for video understanding. This is done by evolving a population of overly-connected architectures guided by connection weight learning. Architectures combining representations that abstract different input types (i.e., RGB and optical flow) at multiple temporal resolutions are searched for, allowing different types or sources of information to interact with each other. Our method, referred to as AssembleNet, outperforms prior approaches on public video datasets, in some cases by a great margin. We obtain 58.6% mAP on Charades and 34.27% accuracy on Moments-in-Time",
    "checked": true,
    "id": "a7afd4fdc9388b94ae5ff6aee062f8c5a9270ec1",
    "semantic_title": "assemblenet: searching for multi-stream neural connectivity in video architectures",
    "citation_count": 91,
    "authors": []
  },
  "https://openreview.net/forum?id=HJgJtT4tvB": {
    "title": "ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning",
    "volume": "poster",
    "abstract": "Recent powerful pre-trained language models have achieved remarkable performance on most of the popular datasets for reading comprehension. It is time to introduce more challenging datasets to push the development of this field towards more comprehensive reasoning of text. In this paper, we introduce a new Reading Comprehension dataset requiring logical reasoning (ReClor) extracted from standardized graduate admission examinations. As earlier studies suggest, human-annotated datasets usually contain biases, which are often exploited by models to achieve high accuracy without truly understanding the text. In order to comprehensively evaluate the logical reasoning ability of models on ReClor, we propose to identify biased data points and separate them into EASY set while the rest as HARD set. Empirical results show that state-of-the-art models have an outstanding ability to capture biases contained in the dataset with high accuracy on EASY set. However, they struggle on HARD set with poor performance near that of random guess, indicating more research is needed to essentially enhance the logical reasoning ability of current models",
    "checked": true,
    "id": "65723a96ff5b15f124bf3b534503950b537b4792",
    "semantic_title": "reclor: a reading comprehension dataset requiring logical reasoning",
    "citation_count": 250,
    "authors": []
  },
  "https://openreview.net/forum?id=HygsuaNFwr": {
    "title": "Order Learning and Its Application to Age Estimation",
    "volume": "poster",
    "abstract": "We propose order learning to determine the order graph of classes, representing ranks or priorities, and classify an object instance into one of the classes. To this end, we design a pairwise comparator to categorize the relationship between two instances into one of three cases: one instance is `greater than,' `similar to,' or `smaller than' the other. Then, by comparing an input instance with reference instances and maximizing the consistency among the comparison results, the class of the input can be estimated reliably. We apply order learning to develop a facial age estimator, which provides the state-of-the-art performance. Moreover, the performance is further improved when the order graph is divided into disjoint chains using gender and ethnic group information or even in an unsupervised manner",
    "checked": true,
    "id": "6bc792035523802f8b601a0ce87bb6dbe908ec9d",
    "semantic_title": "order learning and its application to age estimation",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=B1eY_pVYvB": {
    "title": "Efficient and Information-Preserving Future Frame Prediction and Beyond",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "43fae6940c4ef3f47d80b0749c3447012254a615",
    "semantic_title": "efficient and information-preserving future frame prediction and beyond",
    "citation_count": 113,
    "authors": []
  },
  "https://openreview.net/forum?id=HygrdpVKvr": {
    "title": "NAS evaluation is frustratingly hard",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "645a24296f96f325f4a6fd324cef85661a8987da",
    "semantic_title": "nas evaluation is frustratingly hard",
    "citation_count": 168,
    "authors": []
  },
  "https://openreview.net/forum?id=HJlfuTEtvB": {
    "title": "CLN2INV: Learning Loop Invariants with Continuous Logic Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "eaddaa76aa4c5b1804802d512c7cc1f854b0bda9",
    "semantic_title": "cln2inv: learning loop invariants with continuous logic networks",
    "citation_count": 54,
    "authors": []
  },
  "https://openreview.net/forum?id=BJlguT4YPr": {
    "title": "Scalable Neural Methods for Reasoning With a Symbolic Knowledge Base",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7f0dbd30dc839fd95ea953a9229c879396ca11c0",
    "semantic_title": "scalable neural methods for reasoning with a symbolic knowledge base",
    "citation_count": 62,
    "authors": []
  },
  "https://openreview.net/forum?id=ryenvpEKDr": {
    "title": "A Constructive Prediction of the Generalization Error Across Scales",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d28c18a3c2a0afdc0a8634d18345af8d36e1f948",
    "semantic_title": "a constructive prediction of the generalization error across scales",
    "citation_count": 211,
    "authors": []
  },
  "https://openreview.net/forum?id=HJeiDpVFPr": {
    "title": "An Inductive Bias for Distances: Neural Nets that Respect the Triangle Inequality",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0b09a36a2de74ace4b792a378ce033601fb87e64",
    "semantic_title": "an inductive bias for distances: neural nets that respect the triangle inequality",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=BJeKwTNFvB": {
    "title": "Physics-as-Inverse-Graphics: Unsupervised Physical Parameter Estimation from Video",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "623824d42c226e4024e97fc62a6030a2bf45b2c7",
    "semantic_title": "physics-as-inverse-graphics: unsupervised physical parameter estimation from video",
    "citation_count": 45,
    "authors": []
  },
  "https://openreview.net/forum?id=SJxDDpEKvH": {
    "title": "Counterfactuals uncover the modular structure of deep generative models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "fff34334f4a1a7afe8ddbb27911af52decbac987",
    "semantic_title": "counterfactuals uncover the modular structure of deep generative models",
    "citation_count": 107,
    "authors": []
  },
  "https://openreview.net/forum?id=B1lLw6EYwB": {
    "title": "Gap-Aware Mitigation of Gradient Staleness",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4b25d3b729875a9efc193c92dc4ca34f6dbb96e2",
    "semantic_title": "gap aware mitigation of gradient staleness",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=BygSP6Vtvr": {
    "title": "Ensemble Distribution Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "117deae28bc57323f446d7f7700395909b8d19c6",
    "semantic_title": "ensemble distribution distillation",
    "citation_count": 233,
    "authors": []
  },
  "https://openreview.net/forum?id=SkxSv6VFvS": {
    "title": "Deformable Kernels: Adapting Effective Receptive Fields for Object Deformation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "83e8fdf6d7e7773777d508fec52c252652287935",
    "semantic_title": "deformable kernels: adapting effective receptive fields for object deformation",
    "citation_count": 65,
    "authors": []
  },
  "https://openreview.net/forum?id=SygXPaEYvH": {
    "title": "VL-BERT: Pre-training of Generic Visual-Linguistic Representations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4aa6298b606941a282d735fa3143da293199d2ca",
    "semantic_title": "vl-bert: pre-training of generic visual-linguistic representations",
    "citation_count": 1662,
    "authors": []
  },
  "https://openreview.net/forum?id=r1xGP6VYwH": {
    "title": "Optimistic Exploration even with a Pessimistic Initialisation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "62c1bc6a8bffe09a1e1138ffd37a64988dc27d48",
    "semantic_title": "optimistic exploration even with a pessimistic initialisation",
    "citation_count": 44,
    "authors": []
  },
  "https://openreview.net/forum?id=BkeWw6VFwr": {
    "title": "Certified Robustness for Top-k Predictions against Adversarial Perturbations via Randomized Smoothing",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "947a4b2e31dcaeffa8d86bc8d6888665ec33c5f6",
    "semantic_title": "certified robustness for top-k predictions against adversarial perturbations via randomized smoothing",
    "citation_count": 93,
    "authors": []
  },
  "https://openreview.net/forum?id=SklOUpEYvB": {
    "title": "Identifying through Flows for Recovering Latent Representations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8686bcdb0a9c8ba269ad85fad96e301c8b622697",
    "semantic_title": "identifying through flows for recovering latent representations",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=ryxOUTVYDH": {
    "title": "Robust training with ensemble consensus",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d5fc1374bfb839a65e928c8554ec09421739c2b7",
    "semantic_title": "robust training with ensemble consensus",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=B1l8L6EtDS": {
    "title": "Self-Adversarial Learning with Comparative Discrimination for Text Generation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0a2a6ceb81855761e8e5d14ec43901714d455b92",
    "semantic_title": "self-adversarial learning with comparative discrimination for text generation",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=SkxBUpEKwH": {
    "title": "Vid2Game: Controllable Characters Extracted from Real-World Videos",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3ca6e880e3163a1e799ceaf002563a6950188745",
    "semantic_title": "vid2game: controllable characters extracted from real-world videos",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=ryg48p4tPH": {
    "title": "Action Semantics Network: Considering the Effects of Actions in Multiagent Systems",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "33b677c1e7723a70220ab195f9f07874b842dbee",
    "semantic_title": "action semantics network: considering the effects of actions in multiagent systems",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=rJxX8T4Kvr": {
    "title": "Learning Efficient Parameter Server Synchronization Policies for Distributed SGD",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ea233929465e0dde47234a44a781641a5621a416",
    "semantic_title": "learning efficient parameter server synchronization policies for distributed sgd",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=B1lGU64tDr": {
    "title": "Relational State-Space Model for Stochastic Multi-Object Systems",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7a1e5377b08489c2969f73c56efc557e34f578e1",
    "semantic_title": "relational state-space model for stochastic multi-object systems",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=B1x6BTEKwr": {
    "title": "Piecewise linear activations substantially shape the loss surfaces of neural networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ab99207924dcc0fa9e5e4d40f1f85adf9f832a19",
    "semantic_title": "piecewise linear activations substantially shape the loss surfaces of neural networks",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=ByeNra4FDB": {
    "title": "Novelty Detection Via Blurring",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0098c071deea5c6723e00053eeb58d52594ca596",
    "semantic_title": "novelty detection via blurring",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=BkgXHTNtvS": {
    "title": "Bounds on Over-Parameterization for Guaranteed Existence of Descent Paths in Shallow ReLU Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "868dcec83ced27639c2d5d7ba44c9b92d80bb2ce",
    "semantic_title": "bounds on over-parameterization for guaranteed existence of descent paths in shallow relu networks",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=H1gmHaEKwB": {
    "title": "Data-Independent Neural Pruning via Coresets",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "56f66951145444036cb6ec748d90a04ffc487cc1",
    "semantic_title": "data-independent neural pruning via coresets",
    "citation_count": 60,
    "authors": []
  },
  "https://openreview.net/forum?id=SJxWS64FwH": {
    "title": "Deep Network Classification by Scattering and Homotopy Dictionary Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6f2eaa0b543d414f540b1bc23febcd3b221dec0e",
    "semantic_title": "deep network classification by scattering and homotopy dictionary learning",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=BkglSTNFDB": {
    "title": "Q-learning with UCB Exploration is Sample Efficient for Infinite-Horizon MDP",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "225cc727daeca281f4f932a70765e0a32d849d6b",
    "semantic_title": "q-learning with ucb exploration is sample efficient for infinite-horizon mdp",
    "citation_count": 95,
    "authors": []
  },
  "https://openreview.net/forum?id=HkgaETNtDB": {
    "title": "Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "222b9a7b8038120671a1610e857d3edbc7ac5550",
    "semantic_title": "mixout: effective regularization to finetune large-scale pretrained language models",
    "citation_count": 208,
    "authors": []
  },
  "https://openreview.net/forum?id=rJehNT4YPr": {
    "title": "I Am Going MAD: Maximum Discrepancy Competition for Comparing Classifiers Adaptively",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "430163a32870c800ad2a51dbe9224651b6781007",
    "semantic_title": "i am going mad: maximum discrepancy competition for comparing classifiers adaptively",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=SJxhNTNYwB": {
    "title": "Black-Box Adversarial Attack with Transferable Model-based Embedding",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3f2095bbbf3e47fbac26da70e95a219c23e3bac9",
    "semantic_title": "black-box adversarial attack with transferable model-based embedding",
    "citation_count": 119,
    "authors": []
  },
  "https://openreview.net/forum?id=HkePNpVKPB": {
    "title": "Compositional languages emerge in a neural iterated learning model",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6e482052ac6f73d225387b27ccbfb94375c04ec8",
    "semantic_title": "compositional languages emerge in a neural iterated learning model",
    "citation_count": 98,
    "authors": []
  },
  "https://openreview.net/forum?id=rJeINp4KwH": {
    "title": "Population-Guided Parallel Policy Search for Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3da43bd9e6d691600e97cd04322660d324d623c0",
    "semantic_title": "population-guided parallel policy search for reinforcement learning",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=r1lL4a4tDB": {
    "title": "Variational Recurrent Models for Solving Partially Observable Control Tasks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "96e681ad6a7c0f3694302e86295abb32d5c7aa07",
    "semantic_title": "variational recurrent models for solving partially observable control tasks",
    "citation_count": 62,
    "authors": []
  },
  "https://openreview.net/forum?id=SJeQEp4YDH": {
    "title": "GAT: Generative Adversarial Training for Adversarial Example Detection and Robust Classification",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "95239030dcb8d9213b77cf04b03f2e14a900e08b",
    "semantic_title": "gat: generative adversarial training for adversarial example detection and robust classification",
    "citation_count": 44,
    "authors": []
  },
  "https://openreview.net/forum?id=Skgy464Kvr": {
    "title": "Detecting and Diagnosing Adversarial Images with Class-Conditional Capsule Reconstructions",
    "volume": "poster",
    "abstract": "Adversarial examples raise questions about whether neural network models are sensitive to the same visual features as humans. In this paper, we first detect adversarial examples or otherwise corrupted images based on a class-conditional reconstruction of the input. To specifically attack our detection mechanism, we propose the Reconstructive Attack which seeks both to cause a misclassification and a low reconstruction error. This reconstructive attack produces undetected adversarial examples but with much smaller success rate. Among all these attacks, we find that CapsNets always perform better than convolutional networks. Then, we diagnose the adversarial examples for CapsNets and find that the success of the reconstructive attack is highly related to the visual similarity between the source and target class. Additionally, the resulting perturbations can cause the input image to appear visually more like the target class and hence become non-adversarial. This suggests that CapsNets use features that are more aligned with human perception and have the potential to address the central issue raised by adversarial examples",
    "checked": true,
    "id": "c9b39229642885f46fc35b5b83a01b2c329293a6",
    "semantic_title": "detecting and diagnosing adversarial images with class-conditional capsule reconstructions",
    "citation_count": 72,
    "authors": []
  },
  "https://openreview.net/forum?id=rJx1Na4Fwr": {
    "title": "MACER: Attack-free and Scalable Robust Training via Maximizing Certified Radius",
    "volume": "poster",
    "abstract": "Adversarial training is one of the most popular ways to learn robust models but is usually attack-dependent and time costly. In this paper, we propose the MACER algorithm, which learns robust models without using adversarial training but performs better than all existing provable l2-defenses. Recent work shows that randomized smoothing can be used to provide a certified l2 radius to smoothed classifiers, and our algorithm trains provably robust smoothed classifiers via MAximizing the CErtified Radius (MACER). The attack-free characteristic makes MACER faster to train and easier to optimize. In our experiments, we show that our method can be applied to modern deep neural networks on a wide range of datasets, including Cifar-10, ImageNet, MNIST, and SVHN. For all tasks, MACER spends less training time than state-of-the-art adversarial training algorithms, and the learned models achieve larger average certified radius",
    "checked": true,
    "id": "4136cbc5f7f1fa34b91bf7bd335b173afaaf68d6",
    "semantic_title": "macer: attack-free and scalable robust training via maximizing certified radius",
    "citation_count": 176,
    "authors": []
  },
  "https://openreview.net/forum?id=ByxT7TNFvH": {
    "title": "Semantically-Guided Representation Learning for Self-Supervised Monocular Depth",
    "volume": "poster",
    "abstract": "Self-supervised learning is showing great promise for monocular depth estimation, using geometry as the only source of supervision. Depth networks are indeed capable of learning representations that relate visual appearance to 3D properties by implicitly leveraging category-level patterns. In this work we investigate how to leverage more directly this semantic structure to guide geometric representation learning, while remaining in the self-supervised regime. Instead of using semantic labels and proxy losses in a multi-task approach, we propose a new architecture leveraging fixed pretrained semantic segmentation networks to guide self-supervised representation learning via pixel-adaptive convolutions. Furthermore, we propose a two-stage training process to overcome a common semantic bias on dynamic objects via resampling. Our method improves upon the state of the art for self-supervised monocular depth prediction over all pixels, fine-grained details, and per semantic categories",
    "checked": true,
    "id": "86be6c7ba5ca77a668e5d6ab342445b72ca24208",
    "semantic_title": "semantically-guided representation learning for self-supervised monocular depth",
    "citation_count": 229,
    "authors": []
  },
  "https://openreview.net/forum?id=HJepXaVYDr": {
    "title": "Stochastic AUC Maximization with Deep Neural Networks",
    "volume": "poster",
    "abstract": "Stochastic AUC maximization has garnered an increasing interest due to better fit to imbalanced data classification. However, existing works are limited to stochastic AUC maximization with a linear predictive model, which restricts its predictive power when dealing with extremely complex data. In this paper, we consider stochastic AUC maximization problem with a deep neural network as the predictive model. Building on the saddle point reformulation of a surrogated loss of AUC, the problem can be cast into a {\\it non-convex concave} min-max problem. The main contribution made in this paper is to make stochastic AUC maximization more practical for deep neural networks and big data with theoretical insights as well. In particular, we propose to explore Polyak-\\L{}ojasiewicz (PL) condition that has been proved and observed in deep learning, which enables us to develop new stochastic algorithms with even faster convergence rate and more practical step size scheme. An AdaGrad-style algorithm is also analyzed under the PL condition with adaptive convergence rate. Our experimental results demonstrate the effectiveness of the proposed algorithms",
    "checked": false,
    "id": "935635c8f0e800bb904353f5ae772696499873ef",
    "semantic_title": "stochastic battery operations using deep neural networks",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=rygjmpVFvB": {
    "title": "Difference-Seeking Generative Adversarial Network--Unseen Sample Generation",
    "volume": "poster",
    "abstract": "Unseen data, which are not samples from the distribution of training data and are difficult to collect, have exhibited importance in numerous applications, ({\\em e.g.,} novelty detection, semi-supervised learning, and adversarial training). In this paper, we introduce a general framework called \\textbf{d}ifference-\\textbf{s}eeking \\textbf{g}enerative \\textbf{a}dversarial \\textbf{n}etwork (DSGAN), to generate various types of unseen data. Its novelty is the consideration of the probability density of the unseen data distribution as the difference between two distributions $p_{\\bar{d}}$ and $p_{d}$ whose samples are relatively easy to collect. The DSGAN can learn the target distribution, $p_{t}$, (or the unseen data distribution) from only the samples from the two distributions, $p_{d}$ and $p_{\\bar{d}}$. In our scenario, $p_d$ is the distribution of the seen data, and $p_{\\bar{d}}$ can be obtained from $p_{d}$ via simple operations, so that we only need the samples of $p_{d}$ during the training. Two key applications, semi-supervised learning and novelty detection, are taken as case studies to illustrate that the DSGAN enables the production of various unseen data. We also provide theoretical analyses about the convergence of the DSGAN",
    "checked": true,
    "id": "30a3b3f0f05d70324e7c7ec6bbc238e7a15a746b",
    "semantic_title": "difference-seeking generative adversarial network-unseen sample generation",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=BJgqQ6NYvB": {
    "title": "FasterSeg: Searching for Faster Real-time Semantic Segmentation",
    "volume": "poster",
    "abstract": "We present FasterSeg, an automatically designed semantic segmentation network with not only state-of-the-art performance but also faster speed than current methods. Utilizing neural architecture search (NAS), FasterSeg is discovered from a novel and broader search space integrating multi-resolution branches, that has been recently found to be vital in manually designed segmentation models. To better calibrate the balance between the goals of high accuracy and low latency, we propose a decoupled and fine-grained latency regularization, that effectively overcomes our observed phenomenons that the searched networks are prone to \"collapsing\" to low-latency yet poor-accuracy models. Moreover, we seamlessly extend FasterSeg to a new collaborative search (co-searching) framework, simultaneously searching for a teacher and a student network in the same single run. The teacher-student distillation further boosts the student model's accuracy. Experiments on popular segmentation benchmarks demonstrate the competency of FasterSeg. For example, FasterSeg can run over 30% faster than the closest manually designed competitor on Cityscapes, while maintaining comparable accuracy",
    "checked": true,
    "id": "a5bb1a662883d502ca3d8ddfa9321e8830860fd2",
    "semantic_title": "fasterseg: searching for faster real-time semantic segmentation",
    "citation_count": 190,
    "authors": []
  },
  "https://openreview.net/forum?id=SJetQpEYvB": {
    "title": "LEARNING EXECUTION THROUGH NEURAL CODE FUSION",
    "volume": "poster",
    "abstract": "As the performance of computer systems stagnates due to the end of Moore's Law, there is a need for new models that can understand and optimize the execution of general purpose code. While there is a growing body of work on using Graph Neural Networks (GNNs) to learn static representations of source code, these representations do not understand how code executes at runtime. In this work, we propose a new approach using GNNs to learn fused representations of general source code and its execution. Our approach defines a multi-task GNN over low-level representations of source code and program state (i.e., assembly code and dynamic memory states), converting complex source code constructs and data structures into a simpler, more uniform format. We show that this leads to improved performance over similar methods that do not use execution and it opens the door to applying GNN models to new tasks that would not be feasible from static code alone. As an illustration of this, we apply the new model to challenging dynamic tasks (branch prediction and prefetching) from the SPEC CPU benchmark suite, outperforming the state-of-the-art by 26% and 45% respectively. Moreover, we use the learned fused graph embeddings to demonstrate transfer learning with high performance on an indirectly related algorithm classification task",
    "checked": true,
    "id": "961009061a5b8d1e514fb67be12b25a20d396f1c",
    "semantic_title": "learning execution through neural code fusion",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=HJedXaEtvS": {
    "title": "Editable Neural Networks",
    "volume": "poster",
    "abstract": "These days deep neural networks are ubiquitously used in a wide range of tasks, from image classification and machine translation to face identification and self-driving cars. In many applications, a single model error can lead to devastating financial, reputational and even life-threatening consequences. Therefore, it is crucially important to correct model mistakes quickly as they appear. In this work, we investigate the problem of neural network editing - how one can efficiently patch a mistake of the model on a particular sample, without influencing the model behavior on other samples. Namely, we propose Editable Training, a model-agnostic training technique that encourages fast editing of the trained model. We empirically demonstrate the effectiveness of this method on large-scale image classification and machine translation tasks",
    "checked": true,
    "id": "e165b379f983152874299e0f5a6e0c9596c9a3e8",
    "semantic_title": "editable neural networks",
    "citation_count": 182,
    "authors": []
  },
  "https://openreview.net/forum?id=rklB76EKPr": {
    "title": "Can gradient clipping mitigate label noise?",
    "volume": "poster",
    "abstract": "Gradient clipping is a widely-used technique in the training of deep networks, and is generally motivated from an optimisation lens: informally, it controls the dynamics of iterates, thus enhancing the rate of convergence to a local minimum. This intuition has been made precise in a line of recent works, which show that suitable clipping can yield significantly faster convergence than vanilla gradient descent. In this paper, we propose a new lens for studying gradient clipping, namely, robustness: informally, one expects clipping to provide robustness to noise, since one does not overly trust any single sample. Surprisingly, we prove that for the common problem of label noise in classification, standard gradient clipping does not in general provide robustness. On the other hand, we show that a simple variant of gradient clipping is provably robust, and corresponds to suitably modifying the underlying loss function. This yields a simple, noise-robust alternative to the standard cross-entropy loss which performs well empirically",
    "checked": true,
    "id": "c5f8eee2d9eb59133c378a0c1014fad585955e67",
    "semantic_title": "can gradient clipping mitigate label noise?",
    "citation_count": 139,
    "authors": []
  },
  "https://openreview.net/forum?id=BJlzm64tDH": {
    "title": "Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model",
    "volume": "poster",
    "abstract": "Recent breakthroughs of pretrained language models have shown the effectiveness of self-supervised learning for a wide range of natural language processing (NLP) tasks. In addition to standard syntactic and semantic NLP tasks, pretrained models achieve strong improvements on tasks that involve real-world knowledge, suggesting that large-scale language modeling could be an implicit method to capture knowledge. In this work, we further investigate the extent to which pretrained models such as BERT capture knowledge using a zero-shot fact completion task. Moreover, we propose a simple yet effective weakly supervised pretraining objective, which explicitly forces the model to incorporate knowledge about real-world entities. Models trained with our new objective yield significant improvements on the fact completion task. When applied to downstream tasks, our model consistently outperforms BERT on four entity-related question answering datasets (i.e., WebQuestions, TriviaQA, SearchQA and Quasar-T) with an average 2.7 F1 improvements and a standard fine-grained entity typing dataset (i.e., FIGER) with 5.7 accuracy gains",
    "checked": true,
    "id": "c7fc1cac162c0e2a934704184c7554fd6b6253f0",
    "semantic_title": "pretrained encyclopedia: weakly supervised knowledge-pretrained language model",
    "citation_count": 201,
    "authors": []
  },
  "https://openreview.net/forum?id=rJeg7TEYwB": {
    "title": "Pruned Graph Scattering Transforms",
    "volume": "poster",
    "abstract": "Graph convolutional networks (GCNs) have achieved remarkable performance in a variety of network science learning tasks. However, theoretical analysis of such approaches is still at its infancy. Graph scattering transforms (GSTs) are non-trainable deep GCN models that are amenable to generalization and stability analyses. The present work addresses some limitations of GSTs by introducing a novel so-termed pruned (p)GST approach. The resultant pruning algorithm is guided by a graph-spectrum-inspired criterion, and retains informative scattering features on-the-fly while bypassing the exponential complexity associated with GSTs. It is further established that pGSTs are stable to perturbations of the input graph signals with bounded energy. Experiments showcase that i) pGST performs comparably to the baseline GST that uses all scattering features, while achieving significant computational savings; ii) pGST achieves comparable performance to state-of-the-art GCNs; and iii) Graph data from various domains lead to different scattering patterns, suggesting domain-adaptive pGST network architectures",
    "checked": true,
    "id": "bdca02f2b8bf771572d689a3c599623074536411",
    "semantic_title": "pruned graph scattering transforms",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=BkxpMTEtPB": {
    "title": "GLAD: Learning Sparse Graph Recovery",
    "volume": "poster",
    "abstract": "Recovering sparse conditional independence graphs from data is a fundamental problem in machine learning with wide applications. A popular formulation of the problem is an $\\ell_1$ regularized maximum likelihood estimation. Many convex optimization algorithms have been designed to solve this formulation to recover the graph structure. Recently, there is a surge of interest to learn algorithms directly based on data, and in this case, learn to map empirical covariance to the sparse precision matrix. However, it is a challenging task in this case, since the symmetric positive definiteness (SPD) and sparsity of the matrix are not easy to enforce in learned algorithms, and a direct mapping from data to precision matrix may contain many parameters. We propose a deep learning architecture, GLAD, which uses an Alternating Minimization (AM) algorithm as our model inductive bias, and learns the model parameters via supervised learning. We show that GLAD learns a very compact and effective model for recovering sparse graphs from data",
    "checked": true,
    "id": "33ba823f005017cc921c13e4f6059b67e1def976",
    "semantic_title": "glad: learning sparse graph recovery",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=rJgUfTEYvH": {
    "title": "VideoFlow: A Conditional Flow-Based Model for Stochastic Video Generation",
    "volume": "poster",
    "abstract": "Generative models that can model and predict sequences of future events can, in principle, learn to capture complex real-world phenomena, such as physical interactions. However, a central challenge in video prediction is that the future is highly uncertain: a sequence of past observations of events can imply many possible futures. Although a number of recent works have studied probabilistic models that can represent uncertain futures, such models are either extremely expensive computationally as in the case of pixel-level autoregressive models, or do not directly optimize the likelihood of the data. To our knowledge, our work is the first to propose multi-frame video prediction with normalizing flows, which allows for direct optimization of the data likelihood, and produces high-quality stochastic predictions. We describe an approach for modeling the latent space dynamics, and demonstrate that flow-based generative models offer a viable and competitive approach to generative modeling of video",
    "checked": true,
    "id": "e0eecfcd233bb0c32ec21fec18bcd7c66885eb3e",
    "semantic_title": "videoflow: a conditional flow-based model for stochastic video generation",
    "citation_count": 132,
    "authors": []
  },
  "https://openreview.net/forum?id=HJgEMpVFwB": {
    "title": "Adversarial Policies: Attacking Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "Deep reinforcement learning (RL) policies are known to be vulnerable to adversarial perturbations to their observations, similar to adversarial examples for classifiers. However, an attacker is not usually able to directly modify another agent's observations. This might lead one to wonder: is it possible to attack an RL agent simply by choosing an adversarial policy acting in a multi-agent environment so as to create natural observations that are adversarial? We demonstrate the existence of adversarial policies in zero-sum games between simulated humanoid robots with proprioceptive observations, against state-of-the-art victims trained via self-play to be robust to opponents. The adversarial policies reliably win against the victims but generate seemingly random and uncoordinated behavior. We find that these policies are more successful in high-dimensional environments, and induce substantially different activations in the victim policy network than when the victim plays against a normal opponent. Videos are available at https://adversarialpolicies.github.io/",
    "checked": true,
    "id": "6ff50528f3d7c72772f8c0e3f8398f9dd8e06575",
    "semantic_title": "adversarial policies: attacking deep reinforcement learning",
    "citation_count": 355,
    "authors": []
  },
  "https://openreview.net/forum?id=rkeNfp4tPr": {
    "title": "Escaping Saddle Points Faster with Stochastic Momentum",
    "volume": "poster",
    "abstract": "Stochastic gradient descent (SGD) with stochastic momentum is popular in nonconvex stochastic optimization and particularly for the training of deep neural networks. In standard SGD, parameters are updated by improving along the path of the gradient at the current iterate on a batch of examples, where the addition of a ``momentum'' term biases the update in the direction of the previous change in parameters. In non-stochastic convex optimization one can show that a momentum adjustment provably reduces convergence time in many settings, yet such results have been elusive in the stochastic and non-convex settings. At the same time, a widely-observed empirical phenomenon is that in training deep networks stochastic momentum appears to significantly improve convergence time, variants of it have flourished in the development of other popular update methods, e.g. ADAM, AMSGrad, etc. Yet theoretical justification for the use of stochastic momentum has remained a significant open question. In this paper we propose an answer: stochastic momentum improves deep network training because it modifies SGD to escape saddle points faster and, consequently, to more quickly find a second order stationary point. Our theoretical results also shed light on the related question of how to choose the ideal momentum parameter--our analysis suggests that $\\beta \\in [0,1)$ should be large (close to 1), which comports with empirical findings. We also provide experimental findings that further validate these conclusions",
    "checked": true,
    "id": "b9e508e168cac7e0207bc4e3e03df2d8883fae09",
    "semantic_title": "escaping saddle points faster with stochastic momentum",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=H1emfT4twB": {
    "title": "Few-shot Text Classification with Distributional Signatures",
    "volume": "poster",
    "abstract": "In this paper, we explore meta-learning for few-shot text classification. Meta-learning has shown strong performance in computer vision, where low-level patterns are transferable across learning tasks. However, directly applying this approach to text is challenging--lexical features highly informative for one task may be insignificant for another. Thus, rather than learning solely from words, our model also leverages their distributional signatures, which encode pertinent word occurrence patterns. Our model is trained within a meta-learning framework to map these signatures into attention scores, which are then used to weight the lexical representations of words. We demonstrate that our model consistently outperforms prototypical networks learned on lexical knowledge (Snell et al., 2017) in both few-shot text classification and relation classification by a significant margin across six benchmark datasets (20.0% on average in 1-shot classification)",
    "checked": true,
    "id": "2579c9c2f74a01f382082fc2b1fbc04ba4834718",
    "semantic_title": "few-shot text classification with distributional signatures",
    "citation_count": 167,
    "authors": []
  },
  "https://openreview.net/forum?id=SJezGp4YPr": {
    "title": "Geometric Insights into the Convergence of Nonlinear TD Learning",
    "volume": "poster",
    "abstract": "While there are convergence guarantees for temporal difference (TD) learning when using linear function approximators, the situation for nonlinear models is far less understood, and divergent examples are known. Here we take a first step towards extending theoretical convergence guarantees to TD learning with nonlinear function approximation. More precisely, we consider the expected learning dynamics of the TD(0) algorithm for value estimation. As the step-size converges to zero, these dynamics are defined by a nonlinear ODE which depends on the geometry of the space of function approximators, the structure of the underlying Markov chain, and their interaction. We find a set of function approximators that includes ReLU networks and has geometry amenable to TD learning regardless of environment, so that the solution performs about as well as linear TD in the worst case. Then, we show how environments that are more reversible induce dynamics that are better for TD learning and prove global convergence to the true value function for well-conditioned function approximators. Finally, we generalize a divergent counterexample to a family of divergent problems to demonstrate how the interaction between approximator and environment can go wrong and to motivate the assumptions needed to prove convergence",
    "checked": true,
    "id": "f8d7bcf0e8cfa30ac77e5ff78530e4fdf68c223f",
    "semantic_title": "geometric insights into the convergence of nonlinear td learning",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=rke-f6NKvS": {
    "title": "Learning Self-Correctable Policies and Value Functions from Demonstrations with Negative Sampling",
    "volume": "poster",
    "abstract": "Imitation learning, followed by reinforcement learning algorithms, is a promising paradigm to solve complex control tasks sample-efficiently. However, learning from demonstrations often suffers from the covariate shift problem, which results in cascading errors of the learned policy. We introduce a notion of conservatively extrapolated value functions, which provably lead to policies with self-correction. We design an algorithm Value Iteration with Negative Sampling (VINS) that practically learns such value functions with conservative extrapolation. We show that VINS can correct mistakes of the behavioral cloning policy on simulated robotics benchmark tasks. We also propose the algorithm of using VINS to initialize a reinforcement learning algorithm, which is shown to outperform prior works in sample efficiency",
    "checked": true,
    "id": "bc32aab2970d62a097cee080c8bbd25837a5443b",
    "semantic_title": "learning self-correctable policies and value functions from demonstrations with negative sampling",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=H1exf64KwH": {
    "title": "Exploring Model-based Planning with Policy Networks",
    "volume": "poster",
    "abstract": "Model-based reinforcement learning (MBRL) with model-predictive control or online planning has shown great potential for locomotion control tasks in both sample efficiency and asymptotic performance. Despite the successes, the existing planning methods search from candidate sequences randomly generated in the action space, which is inefficient in complex high-dimensional environments. In this paper, we propose a novel MBRL algorithm, model-based policy planning (POPLIN), that combines policy networks with online planning. More specifically, we formulate action planning at each time-step as an optimization problem using neural networks. We experiment with both optimization w.r.t. the action sequences initialized from the policy network, and also online optimization directly w.r.t. the parameters of the policy network. We show that POPLIN obtains state-of-the-art performance in the MuJoCo benchmarking environments, being about 3x more sample efficient than the state-of-the-art algorithms, such as PETS, TD3 and SAC. To explain the effectiveness of our algorithm, we show that the optimization surface in parameter space is smoother than in action space. Further more, we found the distilled policy network can be effectively applied without the expansive model predictive control during test time for some environments such as Cheetah. Code is released",
    "checked": true,
    "id": "db1614b97aec1e0ead9554a038aa0f8dd9a26e30",
    "semantic_title": "exploring model-based planning with policy networks",
    "citation_count": 148,
    "authors": []
  },
  "https://openreview.net/forum?id=BJg1f6EFDB": {
    "title": "On Identifiability in Transformers",
    "volume": "poster",
    "abstract": "In this paper we delve deep in the Transformer architecture by investigating two of its core components: self-attention and contextual embeddings. In particular, we study the identifiability of attention weights and token embeddings, and the aggregation of context into hidden tokens. We show that, for sequences longer than the attention head dimension, attention weights are not identifiable. We propose effective attention as a complementary tool for improving explanatory interpretations based on attention. Furthermore, we show that input tokens retain to a large degree their identity across the model. We also find evidence suggesting that identity information is mainly encoded in the angle of the embeddings and gradually decreases with depth. Finally, we demonstrate strong mixing of input information in the generation of contextual embeddings by means of a novel quantification method based on gradient attribution. Overall, we show that self-attention distributions are not directly interpretable and present tools to better understand and further investigate Transformer models",
    "checked": true,
    "id": "1fe62a928bf5cfac0f373728f3a4de3cefe0951d",
    "semantic_title": "on identifiability in transformers",
    "citation_count": 188,
    "authors": []
  },
  "https://openreview.net/forum?id=H1e0Wp4KvH": {
    "title": "Automated curriculum generation through setter-solver interactions",
    "volume": "poster",
    "abstract": "Reinforcement learning algorithms use correlations between policies and rewards to improve agent performance. But in dynamic or sparsely rewarding environments these correlations are often too small, or rewarding events are too infrequent to make learning feasible. Human education instead relies on curricula –the breakdown of tasks into simpler, static challenges with dense rewards– to build up to complex behaviors. While curricula are also useful for artificial agents, hand-crafting them is time consuming. This has lead researchers to explore automatic curriculum generation. Here we explore automatic curriculum generation in rich,dynamic environments. Using a setter-solver paradigm we show the importance of considering goal validity, goal feasibility, and goal coverage to construct useful curricula. We demonstrate the success of our approach in rich but sparsely rewarding 2D and 3D environments, where an agent is tasked to achieve a single goal selected from a set of possible goals that varies between episodes, and identify challenges for future work. Finally, we demonstrate the value of a novel technique that guides agents towards a desired goal distribution. Altogether, these results represent a substantial step towards applying automatic task curricula to learn complex, otherwise unlearnable goals, and to our knowledge are the first to demonstrate automated curriculum generation for goal-conditioned agents in environments where the possible goals vary between episodes",
    "checked": true,
    "id": "ea1c1e4dc1df4b2b4aa81ed215b6affbf9c64034",
    "semantic_title": "automated curriculum generation through setter-solver interactions",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=BkepbpNFwr": {
    "title": "Progressive Memory Banks for Incremental Domain Adaptation",
    "volume": "poster",
    "abstract": "This paper addresses the problem of incremental domain adaptation (IDA) in natural language processing (NLP). We assume each domain comes one after another, and that we could only access data in the current domain. The goal of IDA is to build a unified model performing well on all the domains that we have encountered. We adopt the recurrent neural network (RNN) widely used in NLP, but augment it with a directly parameterized memory bank, which is retrieved by an attention mechanism at each step of RNN transition. The memory bank provides a natural way of IDA: when adapting our model to a new domain, we progressively add new slots to the memory bank, which increases the number of parameters, and thus the model capacity. We learn the new memory slots and fine-tune existing parameters by back-propagation. Experimental results show that our approach achieves significantly better performance than fine-tuning alone. Compared with expanding hidden states, our approach is more robust for old domains, shown by both empirical and theoretical results. Our model also outperforms previous work of IDA including elastic weight consolidation and progressive neural networks in the experiments",
    "checked": true,
    "id": "dc1ac510df69b73a24c00ebbe3c589bd2cdf35b7",
    "semantic_title": "progressive memory banks for incremental domain adaptation",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=B1l2bp4YwS": {
    "title": "What graph neural networks cannot learn: depth vs width",
    "volume": "poster",
    "abstract": "This paper studies the expressive power of graph neural networks falling within the message-passing framework (GNNmp). Two results are presented. First, GNNmp are shown to be Turing universal under sufficient conditions on their depth, width, node attributes, and layer expressiveness. Second, it is discovered that GNNmp can lose a significant portion of their power when their depth and width is restricted. The proposed impossibility statements stem from a new technique that enables the repurposing of seminal results from distributed computing and leads to lower bounds for an array of decision, optimization, and estimation problems involving graphs. Strikingly, several of these problems are deemed impossible unless the product of a GNNmp's depth and width exceeds a polynomial of the graph size; this dependence remains significant even for tasks that appear simple or when considering approximation",
    "checked": true,
    "id": "7f42da4abfadf9d5a464affe22d0bd4bf21c0edb",
    "semantic_title": "what graph neural networks cannot learn: depth vs width",
    "citation_count": 299,
    "authors": []
  },
  "https://openreview.net/forum?id=SJgob6NKvH": {
    "title": "RTFM: Generalising to New Environment Dynamics via Reading",
    "volume": "poster",
    "abstract": "Obtaining policies that can generalise to new environments in reinforcement learning is challenging. In this work, we demonstrate that language understanding via a reading policy learner is a promising vehicle for generalisation to new environments. We propose a grounded policy learning problem, Read to Fight Monsters (RTFM), in which the agent must jointly reason over a language goal, relevant dynamics described in a document, and environment observations. We procedurally generate environment dynamics and corresponding language descriptions of the dynamics, such that agents must read to understand new environment dynamics instead of memorising any particular information. In addition, we propose txt2π, a model that captures three-way interactions between the goal, document, and observations. On RTFM, txt2π generalises to new environments with dynamics not seen during training via reading. Furthermore, our model outperforms baselines such as FiLM and language-conditioned CNNs on RTFM. Through curriculum learning, txt2π produces policies that excel on complex RTFM tasks requiring several reasoning and coreference steps",
    "checked": true,
    "id": "04bd5cf9096f9a28a3505a171959a0bde4feb85e",
    "semantic_title": "rtfm: generalising to new environment dynamics via reading",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=SyxIWpVYvr": {
    "title": "Input Complexity and Out-of-distribution Detection with Likelihood-based Generative Models",
    "volume": "poster",
    "abstract": "Likelihood-based generative models are a promising resource to detect out-of-distribution (OOD) inputs which could compromise the robustness or reliability of a machine learning system. However, likelihoods derived from such models have been shown to be problematic for detecting certain types of inputs that significantly differ from training data. In this paper, we pose that this problem is due to the excessive influence that input complexity has in generative models' likelihoods. We report a set of experiments supporting this hypothesis, and use an estimate of input complexity to derive an efficient and parameter-free OOD score, which can be seen as a likelihood-ratio, akin to Bayesian model comparison. We find such score to perform comparably to, or even better than, existing OOD detection approaches under a wide range of data sets, models, model sizes, and complexity estimates",
    "checked": true,
    "id": "1322719978980a831e1aee78aa80a141379c44dd",
    "semantic_title": "input complexity and out-of-distribution detection with likelihood-based generative models",
    "citation_count": 276,
    "authors": []
  },
  "https://openreview.net/forum?id=Bylx-TNKvH": {
    "title": "Functional vs. parametric equivalence of ReLU networks",
    "volume": "poster",
    "abstract": "We address the following question: How redundant is the parameterisation of ReLU networks? Specifically, we consider transformations of the weight space which leave the function implemented by the network intact. Two such transformations are known for feed-forward architectures: permutation of neurons within a layer, and positive scaling of all incoming weights of a neuron coupled with inverse scaling of its outgoing weights. In this work, we show for architectures with non-increasing widths that permutation and scaling are in fact the only function-preserving weight transformations. For any eligible architecture we give an explicit construction of a neural network such that any other network that implements the same function can be obtained from the original one by the application of permutations and rescaling. The proof relies on a geometric understanding of boundaries between linear regions of ReLU networks, and we hope the developed mathematical tools are of independent interest",
    "checked": true,
    "id": "a789aee59905e5d5e29d2e2236d8deba694f5e7e",
    "semantic_title": "functional vs. parametric equivalence of relu networks",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=SygagpEKwB": {
    "title": "Disentangling Factors of Variations Using Few Labels",
    "volume": "poster",
    "abstract": "Learning disentangled representations is considered a cornerstone problem in representation learning. Recently, Locatello et al. (2019) demonstrated that unsupervised disentanglement learning without inductive biases is theoretically impossible and that existing inductive biases and unsupervised methods do not allow to consistently learn disentangled representations. However, in many practical settings, one might have access to a limited amount of supervision, for example through manual labeling of (some) factors of variation in a few training examples. In this paper, we investigate the impact of such supervision on state-of-the-art disentanglement methods and perform a large scale study, training over 52000 models under well-defined and reproducible experimental conditions. We observe that a small number of labeled examples (0.01--0.5% of the data set), with potentially imprecise and incomplete labels, is sufficient to perform model selection on state-of-the-art unsupervised models. Further, we investigate the benefit of incorporating supervision into the training process. Overall, we empirically validate that with little and imprecise supervision it is possible to reliably learn disentangled representations",
    "checked": true,
    "id": "c0e98b2ae199e75205197facf07dcd6982937746",
    "semantic_title": "disentangling factors of variations using few labels",
    "citation_count": 71,
    "authors": []
  },
  "https://openreview.net/forum?id=B1esx6EYvr": {
    "title": "A critical analysis of self-supervision, or what we can learn from a single image",
    "volume": "poster",
    "abstract": "We look critically at popular self-supervision techniques for learning deep convolutional neural networks without manual labels. We show that three different and representative methods, BiGAN, RotNet and DeepCluster, can learn the first few layers of a convolutional network from a single image as well as using millions of images and manual labels, provided that strong data augmentation is used. However, for deeper layers the gap with manual supervision cannot be closed even if millions of unlabelled images are used for training. We conclude that: (1) the weights of the early layers of deep networks contain limited information about the statistics of natural images, that (2) such low-level statistics can be learned through self-supervision just as well as through strong supervision, and that (3) the low-level statistics can be captured via synthetic transformations instead of using a large image dataset",
    "checked": true,
    "id": "3ed8624298e7c8947cf0a4ac987e412170aed4d0",
    "semantic_title": "a critical analysis of self-supervision, or what we can learn from a single image",
    "citation_count": 146,
    "authors": []
  },
  "https://openreview.net/forum?id=r1gixp4FPH": {
    "title": "Accelerating SGD with momentum for over-parameterized learning",
    "volume": "poster",
    "abstract": "Nesterov SGD is widely used for training modern neural networks and other machine learning models. Yet, its advantages over SGD have not been theoretically clarified. Indeed, as we show in this paper, both theoretically and empirically, Nesterov SGD with any parameter selection does not in general provide acceleration over ordinary SGD. Furthermore, Nesterov SGD may diverge for step sizes that ensure convergence of ordinary SGD. This is in contrast to the classical results in the deterministic setting, where the same step size ensures accelerated convergence of the Nesterov's method over optimal gradient descent. To address the non-acceleration issue, we introduce a compensation term to Nesterov SGD. The resulting algorithm, which we call MaSS, converges for same step sizes as SGD. We prove that MaSS obtains an accelerated convergence rates over SGD for any mini-batch size in the linear setting. For full batch, the convergence rate of MaSS matches the well-known accelerated rate of the Nesterov's method. We also analyze the practically important question of the dependence of the convergence rate and optimal hyper-parameters on the mini-batch size, demonstrating three distinct regimes: linear scaling, diminishing returns and saturation. Experimental evaluation of MaSS for several standard architectures of deep networks, including ResNet and convolutional networks, shows improved performance over SGD, Nesterov SGD and Adam",
    "checked": true,
    "id": "842173699096351c9423be6cda570bff59e16111",
    "semantic_title": "accelerating sgd with momentum for over-parameterized learning",
    "citation_count": 82,
    "authors": []
  },
  "https://openreview.net/forum?id=S1xFl64tDr": {
    "title": "Interpretable Complex-Valued Neural Networks for Privacy Protection",
    "volume": "poster",
    "abstract": "Previous studies have found that an adversary attacker can often infer unintended input information from intermediate-layer features. We study the possibility of preventing such adversarial inference, yet without too much accuracy degradation. We propose a generic method to revise the neural network to boost the challenge of inferring input attributes from features, while maintaining highly accurate outputs. In particular, the method transforms real-valued features into complex-valued ones, in which the input is hidden in a randomized phase of the transformed features. The knowledge of the phase acts like a key, with which any party can easily recover the output from the processing result, but without which the party can neither recover the output nor distinguish the original input. Preliminary experiments on various datasets and network structures have shown that our method significantly diminishes the adversary's ability in inferring about the input while largely preserves the resulting accuracy",
    "checked": true,
    "id": "82a59b00d5088279a17305d9aaf8d994e366f903",
    "semantic_title": "interpretable complex-valued neural networks for privacy protection",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=SylOlp4FvH": {
    "title": "V-MPO: On-Policy Maximum a Posteriori Policy Optimization for Discrete and Continuous Control",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e7bb4419a88d15fa8e52c1f4f9cfd65ed58c7379",
    "semantic_title": "v-mpo: on-policy maximum a posteriori policy optimization for discrete and continuous control",
    "citation_count": 124,
    "authors": []
  },
  "https://openreview.net/forum?id=rklOg6EFwS": {
    "title": "Improving Adversarial Robustness Requires Revisiting Misclassified Examples",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "764eff31d9596033859895d9513b838d2c57a6fb",
    "semantic_title": "improving adversarial robustness requires revisiting misclassified examples",
    "citation_count": 540,
    "authors": []
  },
  "https://openreview.net/forum?id=HJgExaVtwr": {
    "title": "DivideMix: Learning with Noisy Labels as Semi-supervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ce435482acc0e195be8d8f002b2655b4c7b08be6",
    "semantic_title": "dividemix: learning with noisy labels as semi-supervised learning",
    "citation_count": 1028,
    "authors": []
  },
  "https://openreview.net/forum?id=rJl31TNYPr": {
    "title": "Fooling Detection Alone is Not Enough: Adversarial Attack against Multiple Object Tracking",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4732294c1682894f8de0b425ff4dbc422134b11d",
    "semantic_title": "fooling detection alone is not enough: adversarial attack against multiple object tracking",
    "citation_count": 96,
    "authors": []
  },
  "https://openreview.net/forum?id=SJg5J6NtDr": {
    "title": "Watch, Try, Learn: Meta-Learning from Demonstrations and Rewards",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "00753de4e5553de8a569e951f531bd683d8dcb16",
    "semantic_title": "watch, try, learn: meta-learning from demonstrations and reward",
    "citation_count": 49,
    "authors": []
  },
  "https://openreview.net/forum?id=rkecJ6VFvr": {
    "title": "Logic and the 2-Simplicial Transformer",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "beffa5291c6c0ddd3420ef66dd7b00007753c729",
    "semantic_title": "logic and the 2-simplicial transformer",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=HkldyTNYwH": {
    "title": "AE-OT: A NEW GENERATIVE MODEL BASED ON EXTENDED SEMI-DISCRETE OPTIMAL TRANSPORT",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f13cd90fad03be74cb538ff1f3ea094fa07a4bda",
    "semantic_title": "ae-ot: a new generative model based on extended semi-discrete optimal transport",
    "citation_count": 57,
    "authors": []
  },
  "https://openreview.net/forum?id=SkeIyaVtwB": {
    "title": "Exploration in Reinforcement Learning with Deep Covering Options",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1d89394ae89349847e9c8df994539ed58db1088e",
    "semantic_title": "exploration in reinforcement learning with deep covering options",
    "citation_count": 55,
    "authors": []
  },
  "https://openreview.net/forum?id=HkxBJT4YvB": {
    "title": "Learning Disentangled Representations for CounterFactual Regression",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1df4204e14da51b05a14781e2a4dc3e0d7da562d",
    "semantic_title": "learning disentangled representations for counterfactual regression",
    "citation_count": 151,
    "authors": []
  },
  "https://openreview.net/forum?id=rJgQkT4twH": {
    "title": "Analysis of Video Feature Learning in Two-Stream CNNs on the Example of Zebrafish Swim Bout Classification",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ae922b26e35895891c275bb5ed880bb588671b6b",
    "semantic_title": "analysis of video feature learning in two-stream cnns on the example of zebrafish swim bout classification",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=H1lZJpVFvr": {
    "title": "Robust Local Features for Improving the Generalization of Adversarial Training",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7c83832f00579685ce454d0f2d61758db2b635d6",
    "semantic_title": "robust local features for improving the generalization of adversarial training",
    "citation_count": 69,
    "authors": []
  },
  "https://openreview.net/forum?id=ryxgJTEYDr": {
    "title": "Reinforcement Learning with Competitive Ensembles of Information-Constrained Primitives",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "103cb5d78e89e63f0fbb8d9a5b2afbfc43cedd64",
    "semantic_title": "reinforcement learning with competitive ensembles of information-constrained primitives",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=rylJkpEtwS": {
    "title": "Learning the Arrow of Time for Problems in Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4cf5067a8d4393b911873c580152d78a8b9c8be3",
    "semantic_title": "learning the arrow of time for problems in reinforcement learning",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=Hye1kTVFDS": {
    "title": "The Variational Bandwidth Bottleneck: Stochastic Evaluation on an Information Budget",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b68a38f6fb6061d641f85b0de5daf7372eb29da2",
    "semantic_title": "the variational bandwidth bottleneck: stochastic evaluation on an information budget",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=H1lj0nNFwB": {
    "title": "The Implicit Bias of Depth: How Incremental Learning Drives Generalization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9ed7493054dbf3509c570bbbebfa572aec825f01",
    "semantic_title": "the implicit bias of depth: how incremental learning drives generalization",
    "citation_count": 81,
    "authors": []
  },
  "https://openreview.net/forum?id=Byg9A24tvB": {
    "title": "Rethinking Softmax Cross-Entropy Loss for Adversarial Robustness",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bbca889e21fcc71f6808eceeaf4833c9d594a078",
    "semantic_title": "reproducibility report: rethinking softmax cross-entropy loss for adversarial robustness",
    "citation_count": 160,
    "authors": []
  },
  "https://openreview.net/forum?id=SygcCnNKwr": {
    "title": "Measuring Compositional Generalization: A Comprehensive Method on Realistic Data",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5cdab78acc4f3aab429a0dd41c3ec7e605d42e7b",
    "semantic_title": "measuring compositional generalization: a comprehensive method on realistic data",
    "citation_count": 353,
    "authors": []
  },
  "https://openreview.net/forum?id=HJgK0h4Ywr": {
    "title": "Theory and Evaluation Metrics for Learning Disentangled Representations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f73b1a55e79271089dd2f5e3dcada577e48837bd",
    "semantic_title": "theory and evaluation metrics for learning disentangled representations",
    "citation_count": 96,
    "authors": []
  },
  "https://openreview.net/forum?id=ByxtC2VtPB": {
    "title": "Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1c1003f06c6cf0f61dfe0fcc0346c23d3c681fae",
    "semantic_title": "mixup inference: better exploiting mixup to defend adversarial attacks",
    "citation_count": 105,
    "authors": []
  },
  "https://openreview.net/forum?id=rkeuAhVKvB": {
    "title": "Dynamically Pruned Message Passing Networks for Large-scale Knowledge Graph Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "093db28bc1e785039cfcb0c3b4fe7fb8e42c9880",
    "semantic_title": "dynamically pruned message passing networks for large-scale knowledge graph reasoning",
    "citation_count": 53,
    "authors": []
  },
  "https://openreview.net/forum?id=H1xPR3NtPB": {
    "title": "Are Pre-trained Language Models Aware of Phrases? Simple but Strong Baselines for Grammar Induction",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7cf8510d5905bd8a63f1e098e05ab591d689e0fd",
    "semantic_title": "are pre-trained language models aware of phrases? simple but strong baselines for grammar induction",
    "citation_count": 90,
    "authors": []
  },
  "https://openreview.net/forum?id=HJgBA2VYwH": {
    "title": "FSPool: Learning Set Representations with Featurewise Sort Pooling",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "07d01345dd30f1400e41d5124ff74e2b0d655040",
    "semantic_title": "fspool: learning set representations with featurewise sort pooling",
    "citation_count": 66,
    "authors": []
  },
  "https://openreview.net/forum?id=Syx7A3NFvH": {
    "title": "Multi-agent Reinforcement Learning for Networked System Control",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5f647e435e6fece4d6ec36d08dfbaafce44fad28",
    "semantic_title": "multi-agent reinforcement learning for networked system control",
    "citation_count": 81,
    "authors": []
  },
  "https://openreview.net/forum?id=H1gzR2VKDH": {
    "title": "Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks via Visual Subgoal Generation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4d40f7df809576d4db22b95c4ca9cc4c66e6928d",
    "semantic_title": "hierarchical foresight: self-supervised learning of long-horizon tasks via visual subgoal generation",
    "citation_count": 138,
    "authors": []
  },
  "https://openreview.net/forum?id=rkxxA24FDr": {
    "title": "Neural Stored-program Memory",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "abbcd4314a19d1f641d41360a851679580eff7a2",
    "semantic_title": "neural stored-program memory",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=S1exA2NtDB": {
    "title": "ES-MAML: Simple Hessian-Free Meta Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "837ca5b8e57262398d3540649bd9545e6d6291d9",
    "semantic_title": "es-maml: simple hessian-free meta learning",
    "citation_count": 109,
    "authors": []
  },
  "https://openreview.net/forum?id=rkeJRhNYDH": {
    "title": "TabFact: A Large-scale Dataset for Table-based Fact Verification",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ee4e24bdedd4d2e4be977bd0ca9f68a06ebb4d96",
    "semantic_title": "tabfact: a large-scale dataset for table-based fact verification",
    "citation_count": 317,
    "authors": []
  },
  "https://openreview.net/forum?id=HkgTTh4FDH": {
    "title": "Implicit Bias of Gradient Descent based Adversarial Training on Separable Data",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5f7d3a9299c78dde60106324c7a145afaba82bb2",
    "semantic_title": "implicit bias of gradient descent based adversarial training on separable data",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=Hyg9anEFPS": {
    "title": "Image-guided Neural Object Rendering",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "72ddc27fd3bbf51d48dbf0553b785f6ec12ab43e",
    "semantic_title": "image-guided neural object rendering",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=BJeS62EtwH": {
    "title": "Knowledge Consistency between Neural Networks and Beyond",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d68504afb7e91187cf2e2b1f729d6422a1fb5e64",
    "semantic_title": "knowledge consistency between neural networks and beyond",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=rJx4p3NYDB": {
    "title": "Lazy-CFR: fast and near-optimal regret minimization for extensive games with imperfect information",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0301c84ae7947ca2fa1680740963d0792b0765c2",
    "semantic_title": "lazy-cfr: fast and near-optimal regret minimization for extensive games with imperfect information",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=BkgXT24tDS": {
    "title": "Additive Powers-of-Two Quantization: An Efficient Non-uniform Discretization for Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "973404c13a5d50469182b76f299860bf71c12fb3",
    "semantic_title": "additive powers-of-two quantization: an efficient non-uniform discretization for neural networks",
    "citation_count": 203,
    "authors": []
  },
  "https://openreview.net/forum?id=BkgWahEFvr": {
    "title": "Enhancing Transformation-Based Defenses Against Adversarial Attacks with a Distribution Classifier",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1056dc5c3e42bfbca6aac87a76de626bf02aa695",
    "semantic_title": "enhancing transformation-based defenses against adversarial attacks with a distribution classifier",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=HJli2hNKDH": {
    "title": "Observational Overfitting in Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "675770f77cd913abc7b7f0466f281cb6c4b383f7",
    "semantic_title": "observational overfitting in reinforcement learning",
    "citation_count": 139,
    "authors": []
  },
  "https://openreview.net/forum?id=rkxoh24FPH": {
    "title": "On Mutual Information Maximization for Representation Learning",
    "volume": "poster",
    "abstract": "Many recent methods for unsupervised or self-supervised representation learning train feature extractors by maximizing an estimate of the mutual information (MI) between different views of the data. This comes with several immediate problems: For example, MI is notoriously hard to estimate, and using it as an objective for representation learning may lead to highly entangled representations due to its invariance under arbitrary invertible transformations. Nevertheless, these methods have been repeatedly shown to excel in practice. In this paper we argue, and provide empirical evidence, that the success of these methods cannot be attributed to the properties of MI alone, and that they strongly depend on the inductive bias in both the choice of feature extractor architectures and the parametrization of the employed MI estimators. Finally, we establish a connection to deep metric learning and argue that this interpretation may be a plausible explanation for the success of the recently introduced methods",
    "checked": true,
    "id": "41fcef711faca9013fd0980a9f6ec1d23c9c76c8",
    "semantic_title": "on mutual information maximization for representation learning",
    "citation_count": 417,
    "authors": []
  },
  "https://openreview.net/forum?id=rJld3hEYvS": {
    "title": "Ranking Policy Gradient",
    "volume": "poster",
    "abstract": "Sample inefficiency is a long-lasting problem in reinforcement learning (RL). The state-of-the-art estimates the optimal action values while it usually involves an extensive search over the state-action space and unstable optimization. Towards the sample-efficient RL, we propose ranking policy gradient (RPG), a policy gradient method that learns the optimal rank of a set of discrete actions. To accelerate the learning of policy gradient methods, we establish the equivalence between maximizing the lower bound of return and imitating a near-optimal policy without accessing any oracles. These results lead to a general off-policy learning framework, which preserves the optimality, reduces variance, and improves the sample-efficiency. We conduct extensive experiments showing that when consolidating with the off-policy learning framework, RPG substantially reduces the sample complexity, comparing to the state-of-the-art",
    "checked": true,
    "id": "3ec6c900567a94a9a4525a729a07025c1353f0bc",
    "semantic_title": "ranking policy gradient",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=r1xPh2VtPB": {
    "title": "SVQN: Sequential Variational Soft Q-Learning Networks",
    "volume": "poster",
    "abstract": "Partially Observable Markov Decision Processes (POMDPs) are popular and flexible models for real-world decision-making applications that demand the information from past observations to make optimal decisions. Standard reinforcement learning algorithms for solving Markov Decision Processes (MDP) tasks are not applicable, as they cannot infer the unobserved states. In this paper, we propose a novel algorithm for POMDPs, named sequential variational soft Q-learning networks (SVQNs), which formalizes the inference of hidden states and maximum entropy reinforcement learning (MERL) under a unified graphical model and optimizes the two modules jointly. We further design a deep recurrent neural network to reduce the computational complexity of the algorithm. Experimental results show that SVQNs can utilize past information to help decision making for efficient inference, and outperforms other baselines on several challenging tasks. Our ablation study shows that SVQNs have the generalization ability over time and are robust to the disturbance of the observation",
    "checked": true,
    "id": "1b7c242baf54b5bda48e8f2ed1259ee4b3116686",
    "semantic_title": "svqn: sequential variational soft q-learning networks",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=BJxH22EKPS": {
    "title": "Understanding Architectures Learnt by Cell-based Neural Architecture Search",
    "volume": "poster",
    "abstract": "Neural architecture search (NAS) searches architectures automatically for given tasks, e.g., image classification and language modeling. Improving the search efficiency and effectiveness has attracted increasing attention in recent years. However, few efforts have been devoted to understanding the generated architectures. In this paper, we first reveal that existing NAS algorithms (e.g., DARTS, ENAS) tend to favor architectures with wide and shallow cell structures. These favorable architectures consistently achieve fast convergence and are consequently selected by NAS algorithms. Our empirical and theoretical study further confirms that their fast convergence derives from their smooth loss landscape and accurate gradient information. Nonetheless, these architectures may not necessarily lead to better generalization performance compared with other candidate architectures in the same search space, and therefore further improvement is possible by revising existing NAS algorithms",
    "checked": true,
    "id": "d47f600c2e306be7b8e85e0ab8be70704574ff0c",
    "semantic_title": "understanding architectures learnt by cell-based neural architecture search",
    "citation_count": 79,
    "authors": []
  },
  "https://openreview.net/forum?id=rygfnn4twS": {
    "title": "AutoQ: Automated Kernel-Wise Neural Network Quantization",
    "volume": "poster",
    "abstract": "Network quantization is one of the most hardware friendly techniques to enable the deployment of convolutional neural networks (CNNs) on low-power mobile devices. Recent network quantization techniques quantize each weight kernel in a convolutional layer independently for higher inference accuracy, since the weight kernels in a layer exhibit different variances and hence have different amounts of redundancy. The quantization bitwidth or bit number (QBN) directly decides the inference accuracy, latency, energy and hardware overhead. To effectively reduce the redundancy and accelerate CNN inferences, various weight kernels should be quantized with different QBNs. However, prior works use only one QBN to quantize each convolutional layer or the entire CNN, because the design space of searching a QBN for each weight kernel is too large. The hand-crafted heuristic of the kernel-wise QBN search is so sophisticated that domain experts can obtain only sub-optimal results. It is difficult for even deep reinforcement learning (DRL) DDPG-based agents to find a kernel-wise QBN configuration that can achieve reasonable inference accuracy. In this paper, we propose a hierarchical-DRL-based kernel-wise network quantization technique, AutoQ, to automatically search a QBN for each weight kernel, and choose another QBN for each activation layer. Compared to the models quantized by the state-of-the-art DRL-based schemes, on average, the same models quantized by AutoQ reduce the inference latency by 54.06%, and decrease the inference energy consumption by 50.69%, while achieving the same inference accuracy",
    "checked": true,
    "id": "74559f96e4a21e0402d1fbae6f6bf853eee85a0f",
    "semantic_title": "autoq: automated kernel-wise neural network quantization",
    "citation_count": 76,
    "authors": []
  },
  "https://openreview.net/forum?id=SkxgnnNFvH": {
    "title": "Poly-encoders: Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring",
    "volume": "poster",
    "abstract": "The use of deep pre-trained transformers has led to remarkable progress in a number of applications (Devlin et al., 2018). For tasks that make pairwise comparisons between sequences, matching a given input with a corresponding label, two approaches are common: Cross-encoders performing full self-attention over the pair and Bi-encoders encoding the pair separately. The former often performs better, but is too slow for practical use. In this work, we develop a new transformer architecture, the Poly-encoder, that learns global rather than token level self-attention features. We perform a detailed comparison of all three approaches, including what pre-training and fine-tuning strategies work best. We show our models achieve state-of-the-art results on four tasks; that Poly-encoders are faster than Cross-encoders and more accurate than Bi-encoders; and that the best results are obtained by pre-training on large datasets similar to the downstream tasks",
    "checked": true,
    "id": "301e13f3df8f9dfb79ea782da3693e2942392279",
    "semantic_title": "poly-encoders: architectures and pre-training strategies for fast and accurate multi-sentence scoring",
    "citation_count": 245,
    "authors": []
  },
  "https://openreview.net/forum?id=BJe1334YDH": {
    "title": "A Learning-based Iterative Method for Solving Vehicle Routing Problems",
    "volume": "poster",
    "abstract": "This paper is concerned with solving combinatorial optimization problems, in particular, the capacitated vehicle routing problems (CVRP). Classical Operations Research (OR) algorithms such as LKH3 \\citep{helsgaun2017extension} are inefficient and difficult to scale to larger-size problems. Machine learning based approaches have recently shown to be promising, partly because of their efficiency (once trained, they can perform solving within minutes or even seconds). However, there is still a considerable gap between the quality of a machine learned solution and what OR methods can offer (e.g., on CVRP-100, the best result of learned solutions is between 16.10-16.80, significantly worse than LKH3's 15.65). In this paper, we present ``Learn to Improve'' (L2I), the first learning based approach for CVRP that is efficient in solving speed and at the same time outperforms OR methods. Starting with a random initial solution, L2I learns to iteratively refine the solution with an improvement operator, selected by a reinforcement learning based controller. The improvement operator is selected from a pool of powerful operators that are customized for routing problems. By combining the strengths of the two worlds, our approach achieves the new state-of-the-art results on CVRP, e.g., an average cost of 15.57 on CVRP-100",
    "checked": true,
    "id": "3f1d79e8c9392836fb7083aea893eebb64fc4952",
    "semantic_title": "a learning-based iterative method for solving vehicle routing problems",
    "citation_count": 173,
    "authors": []
  },
  "https://openreview.net/forum?id=rJxAo2VYwr": {
    "title": "Transferable Perturbations of Deep Feature Distributions",
    "volume": "poster",
    "abstract": "Almost all current adversarial attacks of CNN classifiers rely on information derived from the output layer of the network. This work presents a new adversarial attack based on the modeling and exploitation of class-wise and layer-wise deep feature distributions. We achieve state-of-the-art targeted blackbox transfer-based attack results for undefended ImageNet models. Further, we place a priority on explainability and interpretability of the attacking process. Our methodology affords an analysis of how adversarial attacks change the intermediate feature distributions of CNNs, as well as a measure of layer-wise and class-wise feature distributional separability/entanglement. We also conceptualize a transition from task/data-specific to model-specific features within a CNN architecture that directly impacts the transferability of adversarial examples",
    "checked": true,
    "id": "c723da5454a0f00cf7804fbf6ba71831355ce9da",
    "semantic_title": "transferable perturbations of deep feature distributions",
    "citation_count": 77,
    "authors": []
  },
  "https://openreview.net/forum?id=Hklso24Kwr": {
    "title": "Continual Learning with Adaptive Weights (CLAW)",
    "volume": "poster",
    "abstract": "Approaches to continual learning aim to successfully learn a set of related tasks that arrive in an online manner. Recently, several frameworks have been developed which enable deep learning to be deployed in this learning scenario. A key modelling decision is to what extent the architecture should be shared across tasks. On the one hand, separately modelling each task avoids catastrophic forgetting but it does not support transfer learning and leads to large models. On the other hand, rigidly specifying a shared component and a task-specific part enables task transfer and limits the model size, but it is vulnerable to catastrophic forgetting and restricts the form of task-transfer that can occur. Ideally, the network should adaptively identify which parts of the network to share in a data driven way. Here we introduce such an approach called Continual Learning with Adaptive Weights (CLAW), which is based on probabilistic modelling and variational inference. Experiments show that CLAW achieves state-of-the-art performance on six benchmarks in terms of overall continual learning performance, as measured by classification accuracy, and in terms of addressing catastrophic forgetting",
    "checked": true,
    "id": "63390804dd27b8dde3dd1d3cedbe4956cedb4ac0",
    "semantic_title": "continual learning with adaptive weights (claw)",
    "citation_count": 55,
    "authors": []
  },
  "https://openreview.net/forum?id=r1gdj2EKPB": {
    "title": "Scalable and Order-robust Continual Learning with Additive Parameter Decomposition",
    "volume": "poster",
    "abstract": "While recent continual learning methods largely alleviate the catastrophic problem on toy-sized datasets, there are issues that remain to be tackled in order to apply them to real-world problem domains. First, a continual learning model should effectively handle catastrophic forgetting and be efficient to train even with a large number of tasks. Secondly, it needs to tackle the problem of order-sensitivity, where the performance of the tasks largely varies based on the order of the task arrival sequence, as it may cause serious problems where fairness plays a critical role (e.g. medical diagnosis). To tackle these practical challenges, we propose a novel continual learning method that is scalable as well as order-robust, which instead of learning a completely shared set of weights, represents the parameters for each task as a sum of task-shared and sparse task-adaptive parameters. With our Additive Parameter Decomposition (APD), the task-adaptive parameters for earlier tasks remain mostly unaffected, where we update them only to reflect the changes made to the task-shared parameters. This decomposition of parameters effectively prevents catastrophic forgetting and order-sensitivity, while being computation- and memory-efficient. Further, we can achieve even better scalability with APD using hierarchical knowledge consolidation, which clusters the task-adaptive parameters to obtain hierarchically shared parameters. We validate our network with APD, APD-Net, on multiple benchmark datasets against state-of-the-art continual learning methods, which it largely outperforms in accuracy, scalability, and order-robustness",
    "checked": true,
    "id": "bc4bcc5c62092349ad0ef82af958b16e0c3ec856",
    "semantic_title": "scalable and order-robust continual learning with additive parameter decomposition",
    "citation_count": 132,
    "authors": []
  },
  "https://openreview.net/forum?id=B1xIj3VYvr": {
    "title": "Weakly Supervised Clustering by Exploiting Unique Class Count",
    "volume": "poster",
    "abstract": "A weakly supervised learning based clustering framework is proposed in this paper. As the core of this framework, we introduce a novel multiple instance learning task based on a bag level label called unique class count (ucc), which is the number of unique classes among all instances inside the bag. In this task, no annotations on individual instances inside the bag are needed during training of the models. We mathematically prove that with a perfect ucc classifier, perfect clustering of individual instances inside the bags is possible even when no annotations on individual instances are given during training. We have constructed a neural network based ucc classifier and experimentally shown that the clustering performance of our framework with our weakly supervised ucc classifier is comparable to that of fully supervised learning models where labels for all instances are known. Furthermore, we have tested the applicability of our framework to a real world task of semantic segmentation of breast cancer metastases in histological lymph node sections and shown that the performance of our weakly supervised framework is comparable to the performance of a fully supervised Unet model",
    "checked": true,
    "id": "42af250e63126ae920e3025b55126387c94b4a4d",
    "semantic_title": "weakly supervised clustering by exploiting unique class count",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=H1lBj2VFPS": {
    "title": "Linear Symmetric Quantization of Neural Networks for Low-precision Integer Hardware",
    "volume": "poster",
    "abstract": "With the proliferation of specialized neural network processors that operate on low-precision integers, the performance of Deep Neural Network inference becomes increasingly dependent on the result of quantization. Despite plenty of prior work on the quantization of weights or activations for neural networks, there is still a wide gap between the software quantizers and the low-precision accelerator implementation, which degrades either the efficiency of networks or that of the hardware for the lack of software and hardware coordination at design-phase. In this paper, we propose a learned linear symmetric quantizer for integer neural network processors, which not only quantizes neural parameters and activations to low-bit integer but also accelerates hardware inference by using batch normalization fusion and low-precision accumulators (e.g., 16-bit) and multipliers (e.g., 4-bit). We use a unified way to quantize weights and activations, and the results outperform many previous approaches for various networks such as AlexNet, ResNet, and lightweight models like MobileNet while keeping friendly to the accelerator architecture. Additional, we also apply the method to object detection models and witness high performance and accuracy in YOLO-v2. Finally, we deploy the quantized models on our specialized integer-arithmetic-only DNN accelerator to show the effectiveness of the proposed quantizer. We show that even with linear symmetric quantization, the results can be better than asymmetric or non-linear methods in 4-bit networks. In evaluation, the proposed quantizer induces less than 0.4\\% accuracy drop in ResNet18, ResNet34, and AlexNet when quantizing the whole network as required by the integer processors",
    "checked": true,
    "id": "db1eb78f0f8a138d17f5768720d1b8c5b1980318",
    "semantic_title": "linear symmetric quantization of neural networks for low-precision integer hardware",
    "citation_count": 51,
    "authors": []
  },
  "https://openreview.net/forum?id=Sylgsn4Fvr": {
    "title": "To Relieve Your Headache of Training an MRF, Take AdVIL",
    "volume": "poster",
    "abstract": "We propose a black-box algorithm called {\\it Adversarial Variational Inference and Learning} (AdVIL) to perform inference and learning on a general Markov random field (MRF). AdVIL employs two variational distributions to approximately infer the latent variables and estimate the partition function of an MRF, respectively. The two variational distributions provide an estimate of the negative log-likelihood of the MRF as a minimax optimization problem, which is solved by stochastic gradient descent. AdVIL is proven convergent under certain conditions. On one hand, compared with contrastive divergence, AdVIL requires a minimal assumption about the model structure and can deal with a broader family of MRFs. On the other hand, compared with existing black-box methods, AdVIL provides a tighter estimate of the log partition function and achieves much better empirical results",
    "checked": true,
    "id": "07722e8c8d4cb51c385fe98d54235f2b8c116260",
    "semantic_title": "to relieve your headache of training an mrf, take advil",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=rklp93EtwH": {
    "title": "Automated Relational Meta-learning",
    "volume": "poster",
    "abstract": "In order to efficiently learn with small amount of data on new tasks, meta-learning transfers knowledge learned from previous tasks to the new ones. However, a critical challenge in meta-learning is the task heterogeneity which cannot be well handled by traditional globally shared meta-learning methods. In addition, current task-specific meta-learning methods may either suffer from hand-crafted structure design or lack the capability to capture complex relations between tasks. In this paper, motivated by the way of knowledge organization in knowledge bases, we propose an automated relational meta-learning (ARML) framework that automatically extracts the cross-task relations and constructs the meta-knowledge graph. When a new task arrives, it can quickly find the most relevant structure and tailor the learned structure knowledge to the meta-learner. As a result, the proposed framework not only addresses the challenge of task heterogeneity by a learned meta-knowledge graph, but also increases the model interpretability. We conduct extensive experiments on 2D toy regression and few-shot image classification and the results demonstrate the superiority of ARML over state-of-the-art baselines",
    "checked": true,
    "id": "2c2b2d35ec9006ed83a034a05d316bf6f712d0f9",
    "semantic_title": "automated relational meta-learning",
    "citation_count": 79,
    "authors": []
  },
  "https://openreview.net/forum?id=r1ecqn4YwB": {
    "title": "N-BEATS: Neural basis expansion analysis for interpretable time series forecasting",
    "volume": "poster",
    "abstract": "We focus on solving the univariate times series point forecasting problem using deep learning. We propose a deep neural architecture based on backward and forward residual links and a very deep stack of fully-connected layers. The architecture has a number of desirable properties, being interpretable, applicable without modification to a wide array of target domains, and fast to train. We test the proposed architecture on several well-known datasets, including M3, M4 and TOURISM competition datasets containing time series from diverse domains. We demonstrate state-of-the-art performance for two configurations of N-BEATS for all the datasets, improving forecast accuracy by 11% over a statistical benchmark and by 3% over last year's winner of the M4 competition, a domain-adjusted hand-crafted hybrid between neural network and statistical time series models. The first configuration of our model does not employ any time-series-specific components and its performance on heterogeneous datasets strongly suggests that, contrarily to received wisdom, deep learning primitives such as residual blocks are by themselves sufficient to solve a wide range of forecasting problems. Finally, we demonstrate how the proposed architecture can be augmented to provide outputs that are interpretable without considerable loss in accuracy",
    "checked": true,
    "id": "13c185b8c461034af2634f25dd8a85889e8ee135",
    "semantic_title": "n-beats: neural basis expansion analysis for interpretable time series forecasting",
    "citation_count": 676,
    "authors": []
  },
  "https://openreview.net/forum?id=rkeIq2VYPr": {
    "title": "Deep Learning of Determinantal Point Processes via Proper Spectral Sub-gradient",
    "volume": "poster",
    "abstract": "Determinantal point processes (DPPs) is an effective tool to deliver diversity on multiple machine learning and computer vision tasks. Under deep learning framework, DPP is typically optimized via approximation, which is not straightforward and has some conflict with diversity requirement. We note, however, there has been no deep learning paradigms to optimize DPP directly since it involves matrix inversion which may result in highly computational instability. This fact greatly hinders the wide use of DPP on some specific objectives where DPP serves as a term to measure the feature diversity. In this paper, we devise a simple but effective algorithm to address this issue to optimize DPP term directly expressed with L-ensemble in spectral domain over gram matrix, which is more flexible than learning on parametric kernels. By further taking into account some geometric constraints, our algorithm seeks to generate valid sub-gradients of DPP term in case when the DPP gram matrix is not invertible (no gradients exist in this case). In this sense, our algorithm can be easily incorporated with multiple deep learning tasks. Experiments show the effectiveness of our algorithm, indicating promising performance for practical learning problems",
    "checked": true,
    "id": "354a49d9f67a097ac27dff226775a7aa2172c12c",
    "semantic_title": "deep learning of determinantal point processes via proper spectral sub-gradient",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=BJeB5hVtvB": {
    "title": "Distance-Based Learning from Errors for Confidence Calibration",
    "volume": "poster",
    "abstract": "Deep neural networks (DNNs) are poorly calibrated when trained in conventional ways. To improve confidence calibration of DNNs, we propose a novel training method, distance-based learning from errors (DBLE). DBLE bases its confidence estimation on distances in the representation space. In DBLE, we first adapt prototypical learning to train classification models. It yields a representation space where the distance between a test sample and its ground truth class center can calibrate the model's classification performance. At inference, however, these distances are not available due to the lack of ground truth labels. To circumvent this by inferring the distance for every test sample, we propose to train a confidence model jointly with the classification model. We integrate this into training by merely learning from mis-classified training samples, which we show to be highly beneficial for effective learning. On multiple datasets and DNN architectures, we demonstrate that DBLE outperforms alternative single-model confidence calibration approaches. DBLE also achieves comparable performance with computationally-expensive ensemble approaches with lower computational cost and lower number of parameters",
    "checked": true,
    "id": "096033bd0d4bc867c7be1b8220d9afdc22c03cdc",
    "semantic_title": "distance-based learning from errors for confidence calibration",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=BylEqnVFDB": {
    "title": "Curvature Graph Network",
    "volume": "poster",
    "abstract": "Graph-structured data is prevalent in many domains. Despite the widely celebrated success of deep neural networks, their power in graph-structured data is yet to be fully explored. We propose a novel network architecture that incorporates advanced graph structural features. In particular, we leverage discrete graph curvature, which measures how the neighborhoods of a pair of nodes are structurally related. The curvature of an edge (x, y) defines the distance taken to travel from neighbors of x to neighbors of y, compared with the length of edge (x, y). It is a much more descriptive feature compared to previously used features that only focus on node specific attributes or limited topological information such as degree. Our curvature graph convolution network outperforms state-of-the-art on various synthetic and real-world graphs, especially the larger and denser ones",
    "checked": true,
    "id": "71c4aa93c40d2f867744c7120342b36540e5f09b",
    "semantic_title": "curvature graph network",
    "citation_count": 55,
    "authors": []
  },
  "https://openreview.net/forum?id=ryeG924twB": {
    "title": "Learning Expensive Coordination: An Event-Based Deep RL Approach",
    "volume": "poster",
    "abstract": "Existing works in deep Multi-Agent Reinforcement Learning (MARL) mainly focus on coordinating cooperative agents to complete certain tasks jointly. However, in many cases of the real world, agents are self-interested such as employees in a company and clubs in a league. Therefore, the leader, i.e., the manager of the company or the league, needs to provide bonuses to followers for efficient coordination, which we call expensive coordination. The main difficulties of expensive coordination are that i) the leader has to consider the long-term effect and predict the followers' behaviors when assigning bonuses and ii) the complex interactions between followers make the training process hard to converge, especially when the leader's policy changes with time. In this work, we address this problem through an event-based deep RL approach. Our main contributions are threefold. (1) We model the leader's decision-making process as a semi-Markov Decision Process and propose a novel multi-agent event-based policy gradient to learn the leader's long-term policy. (2) We exploit the leader-follower consistency scheme to design a follower-aware module and a follower-specific attention module to predict the followers' behaviors and make accurate response to their behaviors. (3) We propose an action abstraction-based policy gradient algorithm to reduce the followers' decision space and thus accelerate the training process of followers. Experiments in resource collections, navigation, and the predator-prey game reveal that our approach outperforms the state-of-the-art methods dramatically",
    "checked": true,
    "id": "16dcf3c84c8ca44a5e4322de77abe0d791ca9ff2",
    "semantic_title": "learning expensive coordination: an event-based deep rl approach",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=Skgxcn4YDS": {
    "title": "LAMOL: LAnguage MOdeling for Lifelong Language Learning",
    "volume": "poster",
    "abstract": "Most research on lifelong learning applies to images or games, but not language. We present LAMOL, a simple yet effective method for lifelong language learning (LLL) based on language modeling. LAMOL replays pseudo-samples of previous tasks while requiring no extra memory or model capacity. Specifically, LAMOL is a language model that simultaneously learns to solve the tasks and generate training samples. When the model is trained for a new task, it generates pseudo-samples of previous tasks for training alongside data for the new task. The results show that LAMOL prevents catastrophic forgetting without any sign of intransigence and can perform five very different language tasks sequentially with only one model. Overall, LAMOL outperforms previous methods by a considerable margin and is only 2-3% worse than multitasking, which is usually considered the LLL upper bound. The source code is available at https://github.com/jojotenya/LAMOL",
    "checked": true,
    "id": "3161e2b6787d304c29dddb7d5fc188ca41be7bda",
    "semantic_title": "lamol: language modeling for lifelong language learning",
    "citation_count": 152,
    "authors": []
  },
  "https://openreview.net/forum?id=HygpthEtvr": {
    "title": "ProxSGD: Training Structured Neural Networks under Regularization and Constraints",
    "volume": "poster",
    "abstract": "In this paper, we consider the problem of training neural networks (NN). To promote a NN with specific structures, we explicitly take into consideration the nonsmooth regularization (such as L1-norm) and constraints (such as interval constraint). This is formulated as a constrained nonsmooth nonconvex optimization problem, and we propose a convergent proximal-type stochastic gradient descent (Prox-SGD) algorithm. We show that under properly selected learning rates, momentum eventually resembles the unknown real gradient and thus is crucial in analyzing the convergence. We establish that with probability 1, every limit point of the sequence generated by the proposed Prox-SGD is a stationary point. Then the Prox-SGD is tailored to train a sparse neural network and a binary neural network, and the theoretical analysis is also supported by extensive numerical tests",
    "checked": true,
    "id": "d3a40ade460c40991b4719188f4f762518b9a0dc",
    "semantic_title": "proxsgd: training structured neural networks under regularization and constraints",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=ryxnY3NYPS": {
    "title": "Diverse Trajectory Forecasting with Determinantal Point Processes",
    "volume": "poster",
    "abstract": "The ability to forecast a set of likely yet diverse possible future behaviors of an agent (e.g., future trajectories of a pedestrian) is essential for safety-critical perception systems (e.g., autonomous vehicles). In particular, a set of possible future behaviors generated by the system must be diverse to account for all possible outcomes in order to take necessary safety precautions. It is not sufficient to maintain a set of the most likely future outcomes because the set may only contain perturbations of a dominating single outcome (major mode). While generative models such as variational autoencoders (VAEs) have been shown to be a powerful tool for learning a distribution over future trajectories, randomly drawn samples from the learned implicit likelihood model may not be diverse -- the likelihood model is derived from the training data distribution and the samples will concentrate around the major mode of the data. In this work, we propose to learn a diversity sampling function (DSF) that generates a diverse yet likely set of future trajectories. The DSF maps forecasting context features to a set of latent codes which can be decoded by a generative model (e.g., VAE) into a set of diverse trajectory samples. Concretely, the process of identifying the diverse set of samples is posed as DSF parameter estimation. To learn the parameters of the DSF, the diversity of the trajectory samples is evaluated by a diversity loss based on a determinantal point process (DPP). Gradient descent is performed over the DSF parameters, which in turn moves the latent codes of the sample set to find an optimal set of diverse yet likely trajectories. Our method is a novel application of DPPs to optimize a set of items (forecasted trajectories) in continuous space. We demonstrate the diversity of the trajectories produced by our approach on both low-dimensional 2D trajectory data and high-dimensional human motion data",
    "checked": true,
    "id": "fdfa7ccfd5f118f859baa1f57cf2748d79e6c27b",
    "semantic_title": "diverse trajectory forecasting with determinantal point processes",
    "citation_count": 108,
    "authors": []
  },
  "https://openreview.net/forum?id=H1loF2NFwr": {
    "title": "Evaluating The Search Phase of Neural Architecture Search",
    "volume": "poster",
    "abstract": "Neural Architecture Search (NAS) aims to facilitate the design of deep networks for new tasks. Existing techniques rely on two stages: searching over the architecture space and validating the best architecture. NAS algorithms are currently compared solely based on their results on the downstream task. While intuitive, this fails to explicitly evaluate the effectiveness of their search strategies. In this paper, we propose to evaluate the NAS search phase. To this end, we compare the quality of the solutions obtained by NAS search policies with that of random architecture selection. We find that: (i) On average, the state-of-the-art NAS algorithms perform similarly to the random policy; (ii) the widely-used weight sharing strategy degrades the ranking of the NAS candidates to the point of not reflecting their true performance, thus reducing the effectiveness of the search process. We believe that our evaluation framework will be key to designing NAS strategies that consistently discover architectures superior to random ones",
    "checked": true,
    "id": "6eb3a62cd365e4f9792eedca43c90595e1a862ba",
    "semantic_title": "evaluating the search phase of neural architecture search",
    "citation_count": 322,
    "authors": []
  },
  "https://openreview.net/forum?id=rylBK34FDS": {
    "title": "DeepHoyer: Learning Sparser Neural Network with Differentiable Scale-Invariant Sparsity Measures",
    "volume": "poster",
    "abstract": "In seeking for sparse and efficient neural network models, many previous works investigated on enforcing L1 or L0 regularizers to encourage weight sparsity during training. The L0 regularizer measures the parameter sparsity directly and is invariant to the scaling of parameter values. But it cannot provide useful gradients and therefore requires complex optimization techniques. The L1 regularizer is almost everywhere differentiable and can be easily optimized with gradient descent. Yet it is not scale-invariant and causes the same shrinking rate to all parameters, which is inefficient in increasing sparsity. Inspired by the Hoyer measure (the ratio between L1 and L2 norms) used in traditional compressed sensing problems, we present DeepHoyer, a set of sparsity-inducing regularizers that are both differentiable almost everywhere and scale-invariant. Our experiments show that enforcing DeepHoyer regularizers can produce even sparser neural network models than previous works, under the same accuracy level. We also show that DeepHoyer can be applied to both element-wise and structural pruning",
    "checked": true,
    "id": "8d42c1e2bf782e81b991e7251fed8134b330b04a",
    "semantic_title": "deephoyer: learning sparser neural network with differentiable scale-invariant sparsity measures",
    "citation_count": 77,
    "authors": []
  },
  "https://openreview.net/forum?id=SJg7KhVKPH": {
    "title": "Depth-Adaptive Transformer",
    "volume": "poster",
    "abstract": "State of the art sequence-to-sequence models for large scale tasks perform a fixed number of computations for each input sequence regardless of whether it is easy or hard to process. In this paper, we train Transformer models which can make output predictions at different stages of the network and we investigate different ways to predict how much computation is required for a particular sequence. Unlike dynamic computation in Universal Transformers, which applies the same set of layers iteratively, we apply different layers at every step to adjust both the amount of computation as well as the model capacity. On IWSLT German-English translation our approach matches the accuracy of a well tuned baseline Transformer while using less than a quarter of the decoder layers",
    "checked": true,
    "id": "4585611042d2be0d997ee135e3fe219d668db9ec",
    "semantic_title": "depth-adaptive transformer",
    "citation_count": 125,
    "authors": []
  },
  "https://openreview.net/forum?id=HJezF3VYPB": {
    "title": "Federated Adversarial Domain Adaptation",
    "volume": "poster",
    "abstract": "Federated learning improves data privacy and efficiency in machine learning performed over networks of distributed devices, such as mobile phones, IoT and wearable devices, etc. Yet models trained with federated learning can still fail to generalize to new devices due to the problem of domain shift. Domain shift occurs when the labeled data collected by source nodes statistically differs from the target node's unlabeled data. In this work, we present a principled approach to the problem of federated domain adaptation, which aims to align the representations learned among the different nodes with the data distribution of the target node. Our approach extends adversarial adaptation techniques to the constraints of the federated setting. In addition, we devise a dynamic attention mechanism and leverage feature disentanglement to enhance knowledge transfer. Empirically, we perform extensive experiments on several image and text classification tasks and show promising results under unsupervised federated domain adaptation setting",
    "checked": true,
    "id": "00dd3e0bc0339e1630cc01604486c59db13b2ca8",
    "semantic_title": "federated adversarial domain adaptation",
    "citation_count": 227,
    "authors": []
  },
  "https://openreview.net/forum?id=Bkg0u3Etwr": {
    "title": "Maxmin Q-learning: Controlling the Estimation Bias of Q-learning",
    "volume": "poster",
    "abstract": "Q-learning suffers from overestimation bias, because it approximates the maximum action value using the maximum estimated action value. Algorithms have been proposed to reduce overestimation bias, but we lack an understanding of how bias interacts with performance, and the extent to which existing algorithms mitigate bias. In this paper, we 1) highlight that the effect of overestimation bias on learning efficiency is environment-dependent; 2) propose a generalization of Q-learning, called \\emph{Maxmin Q-learning}, which provides a parameter to flexibly control bias; 3) show theoretically that there exists a parameter choice for Maxmin Q-learning that leads to unbiased estimation with a lower approximation variance than Q-learning; and 4) prove the convergence of our algorithm in the tabular case, as well as convergence of several previous Q-learning variants, using a novel Generalized Q-learning framework. We empirically verify that our algorithm better controls estimation bias in toy environments, and that it achieves superior performance on several benchmark problems",
    "checked": true,
    "id": "b9c529a242ffc453386befc6e95db204e7e9c603",
    "semantic_title": "maxmin q-learning: controlling the estimation bias of q-learning",
    "citation_count": 144,
    "authors": []
  },
  "https://openreview.net/forum?id=BJl2_nVFPB": {
    "title": "Automatically Discovering and Learning New Visual Categories with Ranking Statistics",
    "volume": "poster",
    "abstract": "We tackle the problem of discovering novel classes in an image collection given labelled examples of other classes. This setting is similar to semi-supervised learning, but significantly harder because there are no labelled examples for the new classes. The challenge, then, is to leverage the information contained in the labelled images in order to learn a general-purpose clustering model and use the latter to identify the new classes in the unlabelled data. In this work we address this problem by combining three ideas: (1) we suggest that the common approach of bootstrapping an image representation using the labeled data only introduces an unwanted bias, and that this can be avoided by using self-supervised learning to train the representation from scratch on the union of labelled and unlabelled data; (2) we use rank statistics to transfer the model's knowledge of the labelled classes to the problem of clustering the unlabelled images; and, (3) we train the data representation by optimizing a joint objective function on the labelled and unlabelled subsets of the data, improving both the supervised classification of the labelled data, and the clustering of the unlabelled data. We evaluate our approach on standard classification benchmarks and outperform current methods for novel category discovery by a significant margin",
    "checked": true,
    "id": "5b4d3770b7d2b5ecd46205fe245fa50eeeca5968",
    "semantic_title": "automatically discovering and learning new visual categories with ranking statistics",
    "citation_count": 138,
    "authors": []
  },
  "https://openreview.net/forum?id=rJlnOhVYPS": {
    "title": "Mutual Mean-Teaching: Pseudo Label Refinery for Unsupervised Domain Adaptation on Person Re-identification",
    "volume": "poster",
    "abstract": "Person re-identification (re-ID) aims at identifying the same persons' images across different cameras. However, domain diversities between different datasets pose an evident challenge for adapting the re-ID model trained on one dataset to another one. State-of-the-art unsupervised domain adaptation methods for person re-ID transferred the learned knowledge from the source domain by optimizing with pseudo labels created by clustering algorithms on the target domain. Although they achieved state-of-the-art performances, the inevitable label noise caused by the clustering procedure was ignored. Such noisy pseudo labels substantially hinders the model's capability on further improving feature representations on the target domain. In order to mitigate the effects of noisy pseudo labels, we propose to softly refine the pseudo labels in the target domain by proposing an unsupervised framework, Mutual Mean-Teaching (MMT), to learn better features from the target domain via off-line refined hard pseudo labels and on-line refined soft pseudo labels in an alternative training manner. In addition, the common practice is to adopt both the classification loss and the triplet loss jointly for achieving optimal performances in person re-ID models. However, conventional triplet loss cannot work with softly refined labels. To solve this problem, a novel soft softmax-triplet loss is proposed to support learning with soft pseudo triplet labels for achieving the optimal domain adaptation performance. The proposed MMT framework achieves considerable improvements of 14.4%, 18.2%, 13.1% and 16.4% mAP on Market-to-Duke, Duke-to-Market, Market-to-MSMT and Duke-to-MSMT unsupervised domain adaptation tasks",
    "checked": true,
    "id": "22b52d3f18eaf43993a3a91053f5efe6267144e7",
    "semantic_title": "mutual mean-teaching: pseudo label refinery for unsupervised domain adaptation on person re-identification",
    "citation_count": 460,
    "authors": []
  },
  "https://openreview.net/forum?id=r1eiu2VtwH": {
    "title": "Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data",
    "volume": "poster",
    "abstract": "Nowadays, deep neural networks (DNNs) have become the main instrument for machine learning tasks within a wide range of domains, including vision, NLP, and speech. Meanwhile, in an important case of heterogenous tabular data, the advantage of DNNs over shallow counterparts remains questionable. In particular, there is no sufficient evidence that deep learning machinery allows constructing methods that outperform gradient boosting decision trees (GBDT), which are often the top choice for tabular problems. In this paper, we introduce Neural Oblivious Decision Ensembles (NODE), a new deep learning architecture, designed to work with any tabular data. In a nutshell, the proposed NODE architecture generalizes ensembles of oblivious decision trees, but benefits from both end-to-end gradient-based optimization and the power of multi-layer hierarchical representation learning. With an extensive experimental comparison to the leading GBDT packages on a large number of tabular datasets, we demonstrate the advantage of the proposed NODE architecture, which outperforms the competitors on most of the tasks. We open-source the PyTorch implementation of NODE and believe that it will become a universal framework for machine learning on tabular data",
    "checked": true,
    "id": "0b22dbd48ce4e13bdbf0c9d5e86a9cefdaf6d40a",
    "semantic_title": "neural oblivious decision ensembles for deep learning on tabular data",
    "citation_count": 212,
    "authors": []
  },
  "https://openreview.net/forum?id=S1xKd24twB": {
    "title": "SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards",
    "volume": "poster",
    "abstract": "Learning to imitate expert behavior from demonstrations can be challenging, especially in environments with high-dimensional, continuous observations and unknown dynamics. Supervised learning methods based on behavioral cloning (BC) suffer from distribution shift: because the agent greedily imitates demonstrated actions, it can drift away from demonstrated states due to error accumulation. Recent methods based on reinforcement learning (RL), such as inverse RL and generative adversarial imitation learning (GAIL), overcome this issue by training an RL agent to match the demonstrations over a long horizon. Since the true reward function for the task is unknown, these methods learn a reward function from the demonstrations, often using complex and brittle approximation techniques that involve adversarial training. We propose a simple alternative that still uses RL, but does not require learning a reward function. The key idea is to provide the agent with an incentive to match the demonstrations over a long horizon, by encouraging it to return to demonstrated states upon encountering new, out-of-distribution states. We accomplish this by giving the agent a constant reward of r=+1 for matching the demonstrated action in a demonstrated state, and a constant reward of r=0 for all other behavior. Our method, which we call soft Q imitation learning (SQIL), can be implemented with a handful of minor modifications to any standard Q-learning or off-policy actor-critic algorithm. Theoretically, we show that SQIL can be interpreted as a regularized variant of BC that uses a sparsity prior to encourage long-horizon imitation. Empirically, we show that SQIL outperforms BC and achieves competitive results compared to GAIL, on a variety of image-based and low-dimensional tasks in Box2D, Atari, and MuJoCo. This paper is a proof of concept that illustrates how a simple imitation method based on RL with constant rewards can be as effective as more complex methods that use learned rewards",
    "checked": true,
    "id": "b4e69b0172d69c80f83366c296b6222805360445",
    "semantic_title": "sqil: imitation learning via reinforcement learning with sparse rewards",
    "citation_count": 210,
    "authors": []
  },
  "https://openreview.net/forum?id=r1evOhEKvH": {
    "title": "Graph inference learning for semi-supervised classification",
    "volume": "poster",
    "abstract": "In this work, we address the semi-supervised classification of graph data, where the categories of those unlabeled nodes are inferred from labeled nodes as well as graph structures. Recent works often solve this problem with the advanced graph convolution in a conventional supervised manner, but the performance could be heavily affected when labeled data is scarce. Here we propose a Graph Inference Learning (GIL) framework to boost the performance of node classification by learning the inference of node labels on graph topology. To bridge the connection of two nodes, we formally define a structure relation by encapsulating node attributes, between-node paths and local topological structures together, which can make inference conveniently deduced from one node to another node. For learning the inference process, we further introduce meta-optimization on structure relations from training nodes to validation nodes, such that the learnt graph inference capability can be better self-adapted into test nodes. Comprehensive evaluations on four benchmark datasets (including Cora, Citeseer, Pubmed and NELL) demonstrate the superiority of our GIL when compared with other state-of-the-art methods in the semi-supervised node classification task",
    "checked": true,
    "id": "e86bfa44b69b59414c76cb11b34eb63cfbc7e70a",
    "semantic_title": "graph inference learning for semi-supervised classification",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=rJgBd2NYPH": {
    "title": "Learning deep graph matching with channel-independent embedding and Hungarian attention",
    "volume": "poster",
    "abstract": "Graph matching aims to establishing node-wise correspondence between two graphs, which is a classic combinatorial problem and in general NP-complete. Until very recently, deep graph matching methods start to resort to deep networks to achieve unprecedented matching accuracy. Along this direction, this paper makes two complementary contributions which can also be reused as plugin in existing works: i) a novel node and edge embedding strategy which stimulates the multi-head strategy in attention models and allows the information in each channel to be merged independently. In contrast, only node embedding is accounted in previous works; ii) a general masking mechanism over the loss function is devised to improve the smoothness of objective learning for graph matching. Using Hungarian algorithm, it dynamically constructs a structured and sparsely connected layer, taking into account the most contributing matching pairs as hard attention. Our approach performs competitively, and can also improve state-of-the-art methods as plugin, regarding with matching accuracy on three public benchmarks",
    "checked": true,
    "id": "b8bd415d5c294e4e928f135d19b6b6f978322d28",
    "semantic_title": "learning deep graph matching with channel-independent embedding and hungarian attention",
    "citation_count": 78,
    "authors": []
  },
  "https://openreview.net/forum?id=BJxg_hVtwH": {
    "title": "StructPool: Structured Graph Pooling via Conditional Random Fields",
    "volume": "poster",
    "abstract": "Learning high-level representations for graphs is of great importance for graph analysis tasks. In addition to graph convolution, graph pooling is an important but less explored research area. In particular, most of existing graph pooling techniques do not consider the graph structural information explicitly. We argue that such information is important and develop a novel graph pooling technique, know as the StructPool, in this work. We consider the graph pooling as a node clustering problem, which requires the learning of a cluster assignment matrix. We propose to formulate it as a structured prediction problem and employ conditional random fields to capture the relationships among assignments of different nodes. We also generalize our method to incorporate graph topological information in designing the Gibbs energy function. Experimental results on multiple datasets demonstrate the effectiveness of our proposed StructPool",
    "checked": true,
    "id": "6c252187647a437b32b163a295d62b65cda6e0fe",
    "semantic_title": "structpool: structured graph pooling via conditional random fields",
    "citation_count": 152,
    "authors": []
  },
  "https://openreview.net/forum?id=H1eCw3EKvH": {
    "title": "On the Weaknesses of Reinforcement Learning for Neural Machine Translation",
    "volume": "poster",
    "abstract": "Reinforcement learning (RL) is frequently used to increase performance in text generation tasks, including machine translation (MT), notably through the use of Minimum Risk Training (MRT) and Generative Adversarial Networks (GAN). However, little is known about what and how these methods learn in the context of MT. We prove that one of the most common RL methods for MT does not optimize the expected reward, as well as show that other methods take an infeasibly long time to converge. In fact, our results suggest that RL practices in MT are likely to improve performance only where the pre-trained parameters are already close to yielding the correct translation. Our findings further suggest that observed gains may be due to effects unrelated to the training signal, concretely, changes in the shape of the distribution curve",
    "checked": true,
    "id": "c3172ea74996bc7d390a1bebdc53e373db903b1d",
    "semantic_title": "on the weaknesses of reinforcement learning for neural machine translation",
    "citation_count": 79,
    "authors": []
  },
  "https://openreview.net/forum?id=rkgpv2VFvr": {
    "title": "Sharing Knowledge in Multi-Task Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "We study the benefit of sharing representations among tasks to enable the effective use of deep neural networks in Multi-Task Reinforcement Learning. We leverage the assumption that learning from different tasks, sharing common properties, is helpful to generalize the knowledge of them resulting in a more effective feature extraction compared to learning a single task. Intuitively, the resulting set of features offers performance benefits when used by Reinforcement Learning algorithms. We prove this by providing theoretical guarantees that highlight the conditions for which is convenient to share representations among tasks, extending the well-known finite-time bounds of Approximate Value-Iteration to the multi-task setting. In addition, we complement our analysis by proposing multi-task extensions of three Reinforcement Learning algorithms that we empirically evaluate on widely used Reinforcement Learning benchmarks showing significant improvements over the single-task counterparts in terms of sample efficiency and performance",
    "checked": true,
    "id": "e74aded7d0839af48706c51a7b55af2ea20f0603",
    "semantic_title": "sharing knowledge in multi-task deep reinforcement learning",
    "citation_count": 97,
    "authors": []
  },
  "https://openreview.net/forum?id=HygnDhEtvr": {
    "title": "Reinforcement Learning Based Graph-to-Sequence Model for Natural Question Generation",
    "volume": "poster",
    "abstract": "Natural question generation (QG) aims to generate questions from a passage and an answer. Previous works on QG either (i) ignore the rich structure information hidden in text, (ii) solely rely on cross-entropy loss that leads to issues like exposure bias and inconsistency between train/test measurement, or (iii) fail to fully exploit the answer information. To address these limitations, in this paper, we propose a reinforcement learning (RL) based graph-to-sequence (Graph2Seq) model for QG. Our model consists of a Graph2Seq generator with a novel Bidirectional Gated Graph Neural Network based encoder to embed the passage, and a hybrid evaluator with a mixed objective combining both cross-entropy and RL losses to ensure the generation of syntactically and semantically valid text. We also introduce an effective Deep Alignment Network for incorporating the answer information into the passage at both the word and contextual levels. Our model is end-to-end trainable and achieves new state-of-the-art scores, outperforming existing methods by a significant margin on the standard SQuAD benchmark",
    "checked": true,
    "id": "e47e6c814d2742527fdd352db13a5fd95b7ce24b",
    "semantic_title": "reinforcement learning based graph-to-sequence model for natural question generation",
    "citation_count": 135,
    "authors": []
  },
  "https://openreview.net/forum?id=HkgsPhNYPS": {
    "title": "SELF: Learning to Filter Noisy Labels with Self-Ensembling",
    "volume": "poster",
    "abstract": "Deep neural networks (DNNs) have been shown to over-fit a dataset when being trained with noisy labels for a long enough time. To overcome this problem, we present a simple and effective method self-ensemble label filtering (SELF) to progressively filter out the wrong labels during training. Our method improves the task performance by gradually allowing supervision only from the potentially non-noisy (clean) labels and stops learning on the filtered noisy labels. For the filtering, we form running averages of predictions over the entire training dataset using the network output at different training epochs. We show that these ensemble estimates yield more accurate identification of inconsistent predictions throughout training than the single estimates of the network at the most recent training epoch. While filtered samples are removed entirely from the supervised training loss, we dynamically leverage them via semi-supervised learning in the unsupervised loss. We demonstrate the positive effect of such an approach on various image classification tasks under both symmetric and asymmetric label noise and at different noise ratios. It substantially outperforms all previous works on noise-aware learning across different datasets and can be applied to a broad set of network architectures",
    "checked": true,
    "id": "c385e811f98260b673d52abcfdb981b60e880695",
    "semantic_title": "self: learning to filter noisy labels with self-ensembling",
    "citation_count": 256,
    "authors": []
  },
  "https://openreview.net/forum?id=Syx4wnEtvH": {
    "title": "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes",
    "volume": "poster",
    "abstract": "Training large deep neural networks on massive datasets is computationally very challenging. There has been recent surge in interest in using large batch stochastic optimization methods to tackle this issue. The most prominent algorithm in this line of research is LARS, which by employing layerwise adaptive learning rates trains ResNet on ImageNet in a few minutes. However, LARS performs poorly for attention models like BERT, indicating that its performance gains are not consistent across tasks. In this paper, we first study a principled layerwise adaptation strategy to accelerate training of deep neural networks using large mini-batches. Using this strategy, we develop a new layerwise adaptive large batch optimization technique called LAMB; we then provide convergence analysis of LAMB as well as LARS, showing convergence to a stationary point in general nonconvex settings. Our empirical results demonstrate the superior performance of LAMB across various tasks such as BERT and ResNet-50 training with very little hyperparameter tuning. In particular, for BERT training, our optimizer enables use of very large batch sizes of 32868 without any degradation of performance. By increasing the batch size to the memory limit of a TPUv3 Pod, BERT training time can be reduced from 3 days to just 76 minutes",
    "checked": true,
    "id": "bc789aef715498e79a74f857fa090ece9e383bf1",
    "semantic_title": "large batch optimization for deep learning: training bert in 76 minutes",
    "citation_count": 824,
    "authors": []
  }
}