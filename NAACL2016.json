{
  "https://aclanthology.org/N16-1001": {
    "title": "Achieving Accurate Conclusions in Evaluation of Automatic Machine Translation Metrics",
    "abstract": "Automatic Machine Translation metrics, such   as BLEU, are widely used in empirical evaluation as a substitute for human assessment.   Subsequently, the performance of a given metric is measured by its strength of correlation   with human judgment. When a newly proposed metric achieves a stronger correlation   over that of a baseline, it is important to take   into account the uncertainty inherent in correlation point estimates prior to concluding   improvements in metric performance. Confidence intervals for correlations with human   judgment are rarely reported in metric evaluations, however, and when they have been   reported, the most suitable methods have unfortunately not been applied. For example,   incorrect assumptions about correlation sampling distributions made in past evaluations   risk over-estimation of significant differences   in metric performance. In this paper, we provide analysis of each of the issues that may   lead to inaccuracies before providing detail of   a method that overcomes previous challenges.   Additionally, we propose a new method of   translation sampling that in contrast achieves   genuine high conclusivity in evaluation of the   relative performance of metrics",
    "volume": "main",
    "checked": true,
    "id": "67228eecd5563e9c9c60fabafaed8e305bdba4a0",
    "citation_count": 20
  },
  "https://aclanthology.org/N16-1002": {
    "title": "Flexible Non-Terminals for Dependency Tree-to-Tree Reordering",
    "abstract": "A major benefit of tree-to-tree over treeto-string translation is that we can use target-side syntax to improve reordering. While this is relatively simple for binarized constituency parses, the reordering problem is considerably harder for dependency parses, in which words can have arbitrarily many children. Previous approaches have tackled this problem by restricting grammar rules, reducing the expressive power of the translation model. In this paper we propose a general model for dependency tree-to-tree reordering based on flexible non-terminals that can compactly encode multiple insertion positions. We explore how insertion positions can be selected even in cases where rules do not entirely cover the children of input sentence words. The proposed method greatly improves the flexibility of translation rules at the cost of only a 30% increase in decoding time, and we demonstrate a 1.2–1.9 BLEU improvement over a strong tree-to-tree baseline",
    "volume": "main",
    "checked": true,
    "id": "e6538a8daed9b41a61701e26a9d6b6d199d5755f",
    "citation_count": 1
  },
  "https://aclanthology.org/N16-1003": {
    "title": "Selecting Syntactic, Non-redundant Segments in Active Learning for Machine Translation",
    "abstract": "Active learning is a framework that makes it possible to efficiently train statistical models by selecting informative examples from a pool of unlabeled data. Previous work has found this framework effective for machine translation (MT), making it possible to train better translation models with less effort, particularly when annotators translate short phrases instead of full sentences. However, previous methods for phrase-based active learning in MT fail to consider whether the selected units are coherent and easy for human translators to translate, and also have problems with selecting redundant phrases with similar content. In this paper, we tackle these problems by proposing two new methods for selecting more syntactically coherent and less redundant segments in active learning for MT. Experiments using both simulation and extensive manual translation by professional translators find the proposed method effective, achieving both greater gain of BLEU score for the same number of translated words, and allowing translators to be more confident in their translations1",
    "volume": "main",
    "checked": true,
    "id": "d21a0e01514732f241b9c138eceb76ecaef17a27",
    "citation_count": 17
  },
  "https://aclanthology.org/N16-1004": {
    "title": "Multi-Source Neural Translation",
    "abstract": "We build a multi-source machine translation model and train it to maximize the probability of a target English string given French and German sources. Using the neural encoder-decoder framework, we explore several combination methods and report up to +4.8 Bleu increases on top of a very strong attention-based neural translation model",
    "volume": "main",
    "checked": true,
    "id": "f4610bbad14cf5c722cfe11fcca3d7e6382452dd",
    "citation_count": 300
  },
  "https://aclanthology.org/N16-1005": {
    "title": "Controlling Politeness in Neural Machine Translation via Side Constraints",
    "abstract": "Many languages use honoriﬁcs to express politeness, social distance, or the relative social status between the speaker and their ad-dressee(s). In machine translation from a language without honoriﬁcs such as English, it is difﬁcult to predict the appropriate honoriﬁc, but users may want to control the level of politeness in the output. In this paper, we perform a pilot study to control honoriﬁcs in neural machine translation (NMT) via side constraints , focusing on English → German. We show that by marking up the (English) source side of the training data with a feature that en-codes the use of honoriﬁcs on the (German) target side, we can control the honoriﬁcs produced at test time. Experiments show that the choice of honoriﬁcs has a big impact on translation quality as measured by B LEU , and oracle experiments show that substantial im-provements are possible by constraining the translation to the desired level of politeness",
    "volume": "main",
    "checked": true,
    "id": "bf0f141bae83bd6d5ca0c37839d53f0d06059b34",
    "citation_count": 285
  },
  "https://aclanthology.org/N16-1006": {
    "title": "An Empirical Evaluation of Noise Contrastive Estimation for the Neural Network Joint Model of Translation",
    "abstract": "The neural network joint model of translation or NNJM (Devlin et al., 2014) combines source and target context to produce a powerful translation feature. However, its softmax layer necessitates a sum over the entire output vocabulary, which results in very slow maximum likelihood (MLE) training. This has led some groups to train using Noise Contrastive Estimation (NCE), which side-steps this sum. We carry out the first direct comparison of MLE and NCE training objectives for the NNJM, showing that NCE is significantly outperformed by MLE on large-scale ArabicEnglish and Chinese-English translation tasks. We also show that this drop can be avoided by using a recently proposed translation noise distribution",
    "volume": "main",
    "checked": true,
    "id": "6ad11e618cd4a31cd058ef57557b83d90c51d8a2",
    "citation_count": 3
  },
  "https://aclanthology.org/N16-1007": {
    "title": "Neural Network-Based Abstract Generation for Opinions and Arguments",
    "abstract": "We study the problem of generating abstractive summaries for opinionated text. We propose an attention-based neural network model that is able to absorb information from multiple text units to construct informative, concise, and fluent summaries. An importance-based sampling method is designed to allow the encoder to integrate information from an important subset of input. Automatic evaluation indicates that our system outperforms state-of-the-art abstractive and extractive summarization systems on two newly collected datasets of movie reviews and arguments. Our system summaries are also rated as more informative and grammatical in human evaluation",
    "volume": "main",
    "checked": true,
    "id": "e8b7225037dfa77623824f28f58927212bc3949e",
    "citation_count": 116
  },
  "https://aclanthology.org/N16-1008": {
    "title": "A Low-Rank Approximation Approach to Learning Joint Embeddings of News Stories and Images for Timeline Summarization",
    "abstract": "A key challenge for timeline summarization is to generate a concise, yet complete storyline from large collections of news stories. Previous studies in extractive timeline generation are limited in two ways: first, most prior work focuses on fully-observable ranking models or clustering models with hand-designed features that may not generalize well. Second, most summarization corpora are text-only, which means that text is the sole source of information considered in timeline summarization, and thus, the rich visual content from news images is ignored. To solve these issues, we leverage the success of matrix factorization techniques from recommender systems, and cast the problem as a sentence recommendation task, using a representation learning approach. To augment text-only corpora, for each candidate sentence in a news article, we take advantage of top-ranked relevant images from the Web and model the image using a convolutional neural network architecture. Finally, we propose a scalable low-rank approximation approach for learning joint embeddings of news stories and images. In experiments, we compare our model to various competitive baselines, and demonstrate the stateof-the-art performance of the proposed textbased and multimodal approaches",
    "volume": "main",
    "checked": true,
    "id": "860c908be33a164566c6d881feec593c69dca9ee",
    "citation_count": 43
  },
  "https://aclanthology.org/N16-1009": {
    "title": "Entity-balanced Gaussian pLSA for Automated Comparison",
    "abstract": "Community created content (e.g., product descriptions, reviews) typically discusses one entity at a time and it can be hard as well as time consuming for a user to compare two or more entities. In response, we define a novel task of automatically generating entity comparisons from text. Our output is a table that semantically clusters descriptive phrases about entities. Our clustering algorithm is a Gaussian extension of probabilistic latent semantic analysis (pLSA), in which each phrase is represented in word vector embedding space. In addition, our algorithm attempts to balance information about entities in each cluster to generate meaningful comparison tables, where possible. We test our system's effectiveness on two domains, travel articles and movie reviews, and find that entitybalanced clusters are strongly preferred by users",
    "volume": "main",
    "checked": true,
    "id": "77220147b3c3439c9d44da83fa12c15893f9a74d",
    "citation_count": 7
  },
  "https://aclanthology.org/N16-1010": {
    "title": "Automatic Summarization of Student Course Feedback",
    "abstract": "Student course feedback is generated daily in both classrooms and online course discussion forums. Traditionally, instructors manually analyze these responses in a costly manner. In this work, we propose a new approach to summarizing student course feedback based on the integer linear programming (ILP) framework. Our approach allows different student responses to share co-occurrence statistics and alleviates sparsity issues. Experimental results on a student feedback corpus show that our approach outperforms a range of baselines in terms of both ROUGE scores and human evaluation",
    "volume": "main",
    "checked": true,
    "id": "dd0ef807f34549048aea8903c4d72f5f6e12a815",
    "citation_count": 25
  },
  "https://aclanthology.org/N16-1011": {
    "title": "Knowledge-Guided Linguistic Rewrites for Inference Rule Verification",
    "abstract": "A corpus of inference rules between a pair of relation phrases is typically generated using the statistical overlap of argument-pairs associated with the relations (e.g., PATTY, CLEAN). We investigate knowledge-guided linguistic rewrites as a secondary source of evidence and find that they can vastly improve the quality of inference rule corpora, obtaining 27 to 33 point precision improvement while retaining substantial recall. The facts inferred using cleaned inference rules are 29-32 points more accurate",
    "volume": "main",
    "checked": true,
    "id": "fca64d8c549f151cc89771a4bf480547687c8818",
    "citation_count": 8
  },
  "https://aclanthology.org/N16-1012": {
    "title": "Abstractive Sentence Summarization with Attentive Recurrent Neural Networks",
    "abstract": "Abstractive Sentence Summarization generates a shorter version of a given sentence while attempting to preserve its meaning. We introduce a conditional recurrent neural network (RNN) which generates a summary of an input sentence. The conditioning is provided by a novel convolutional attention-based encoder which ensures that the decoder focuses on the appropriate input words at each step of generation. Our model relies only on learned features and is easy to train in an end-to-end fashion on large data sets. Our experiments show that the model significantly outperforms the recently proposed state-of-the-art method on the Gigaword corpus while performing competitively on the DUC-2004 shared task.ive Sentence Summarization generates a shorter version of a given sentence while attempting to preserve its meaning. We introduce a conditional recurrent neural network (RNN) which generates a summary of an input sentence. The conditioning is provided by a novel convolutional attention-based encoder which ensures that the decoder focuses on the appropriate input words at each step of generation. Our model relies only on learned features and is easy to train in an end-to-end fashion on large data sets. Our experiments show that the model significantly outperforms the recently proposed state-of-the-art method on the Gigaword corpus while performing competitively on the DUC-2004 shared task",
    "volume": "main",
    "checked": true,
    "id": "7a67159fc7bc76d0b37930b55005a69b51241635",
    "citation_count": 783
  },
  "https://aclanthology.org/N16-1013": {
    "title": "Integer Linear Programming for Discourse Parsing",
    "abstract": "In this paper we present the first, to the best of our knowledge, discourse parser that is able to predict non-tree DAG structures. We use Integer Linear Programming (ILP) to encode both the objective function and the constraints as global decoding over local scores. Our underlying data come from multi-party chat dialogues, which require the prediction of DAGs. We use the dependency parsing paradigm, as has been done in the past (Muller et al., 2012; Li et al., 2014; Afantenos et al., 2015), but we use the underlying formal framework of SDRT and exploit SDRT's notions of left and right distributive relations. We achieve an F-measure of 0.531 for fully labeled structures which beats the previous state of the art",
    "volume": "main",
    "checked": true,
    "id": "ee32e44ddcb71b044f68c077b9c68dfe20339574",
    "citation_count": 40
  },
  "https://aclanthology.org/N16-1014": {
    "title": "A Diversity-Promoting Objective Function for Neural Conversation Models",
    "abstract": "Sequence-to-sequence neural network models for generation of conversational responses tend to generate safe, commonplace responses (e.g., \"I don't know\") regardless of the input. We suggest that the traditional objective function, i.e., the likelihood of output (response) given input (message) is unsuited to response generation tasks. Instead we propose using Maximum Mutual Information (MMI) as the objective function in neural models. Experimental results demonstrate that the proposed MMI models produce more diverse, interesting, and appropriate responses, yielding substantive gains in BLEU scores on two conversational datasets and in human evaluations",
    "volume": "main",
    "checked": true,
    "id": "651e5bcc14f14605a879303e97572a27ea8c7956",
    "citation_count": 1781
  },
  "https://aclanthology.org/N16-1015": {
    "title": "Multi-domain Neural Network Language Generation for Spoken Dialogue Systems",
    "abstract": "©2016 Association for Computational Linguistics. Moving from limited-domain natural language generation (NLG) to open domain is difficult because the number of semantic input combinations grows exponentially with the number of domains. Therefore, it is important to leverage existing resources and exploit similarities between domains to facilitate domain adaptation. In this paper, we propose a procedure to train multi-domain, Recurrent Neural Network-based (RNN) language generators via multiple adaptation steps. In this procedure, a model is first trained on counterfeited data synthesised from an out-of-domain dataset, and then fine tuned on a small set of in-domain utterances with a discriminative objective function. Corpus-based evaluation results show that the proposed procedure can achieve competitive performance in terms of BLEU score and slot error rate while significantly reducing the data needed to train generators in new, unseen domains. In subjective testing, human judges confirm that the procedure greatly improves generator performance when only a small amount of data is available in the domain",
    "volume": "main",
    "checked": true,
    "id": "9ffaa3a91071f0b9b7676b8cfc73e06069329724",
    "citation_count": 174
  },
  "https://aclanthology.org/N16-1016": {
    "title": "A Long Short-Term Memory Framework for Predicting Humor in Dialogues",
    "abstract": "We propose a first-ever attempt to employ a Long Short-Term memory based framework to predict humor in dialogues. We analyze data from a popular TV-sitcom, whose canned laughters give an indication of when the audience would react. We model the setuppunchline relation of conversational humor with a Long Short-Term Memory, with utterance encodings obtained from a Convolutional Neural Network. Out neural network framework is able to improve the F-score of 8% over a Conditional Random Field baseline. We show how the LSTM effectively models the setup-punchline relation reducing the number of false positives and increasing the recall. We aim to employ our humor prediction model to build effective empathetic machine able to understand jokes",
    "volume": "main",
    "checked": true,
    "id": "015c26ca824f9d20b6523f44e6b9dc3b3dd65b2d",
    "citation_count": 79
  },
  "https://aclanthology.org/N16-1017": {
    "title": "Conversational Flow in Oxford-style Debates",
    "abstract": "Public debates are a common platform for presenting and juxtaposing diverging views on important issues. In this work we propose a methodology for tracking how ideas flow between participants throughout a debate. We use this approach in a case study of Oxford-style debates---a competitive format where the winner is determined by audience votes---and show how the outcome of a debate depends on aspects of conversational flow. In particular, we find that winners tend to make better use of a debate's interactive component than losers, by actively pursuing their opponents' points rather than promoting their own ideas over the course of the conversation",
    "volume": "main",
    "checked": true,
    "id": "36c3e9e30de89220caef968706ba2758f2f0b094",
    "citation_count": 53
  },
  "https://aclanthology.org/N16-1018": {
    "title": "Counter-fitting Word Vectors to Linguistic Constraints",
    "abstract": "In this work, we present a novel counter-fitting method which injects antonymy and synonymy constraints into vector space representations in order to improve the vectors' capability for judging semantic similarity. Applying this method to publicly available pre-trained word vectors leads to a new state of the art performance on the SimLex-999 dataset. We also show how the method can be used to tailor the word vector space for the downstream task of dialogue state tracking, resulting in robust improvements across different dialogue domains",
    "volume": "main",
    "checked": true,
    "id": "a9487f431f87546ce36c9ffa98b6321872d9e7f1",
    "citation_count": 394
  },
  "https://aclanthology.org/N16-1019": {
    "title": "Grounded Semantic Role Labeling",
    "abstract": "Semantic Role Labeling (SRL) captures semantic roles (or participants) such as agent, patient, and theme associated with verbs from the text. While it provides important intermediate semantic representations for many traditional NLP tasks (such as information extraction and question answering), it does not capture grounded semantics so that an artificial agent can reason, learn, and perform the actions with respect to the physical environment. To address this problem, this paper extends traditional SRL to grounded SRL where arguments of verbs are grounded to participants of actions in the physical world. By integrating language and vision processing through joint inference, our approach not only grounds explicit roles, but also grounds implicit roles that are not explicitly mentioned in language descriptions. This paper describes our empirical results and discusses challenges and future directions",
    "volume": "main",
    "checked": true,
    "id": "bb06a75c5848c638148ea9649639484f1323a576",
    "citation_count": 39
  },
  "https://aclanthology.org/N16-1020": {
    "title": "Black Holes and White Rabbits: Metaphor Identification with Visual Features",
    "abstract": "Metaphor is pervasive in our communication, which makes it an important problem for natural language processing (NLP). Numerous approaches to metaphor processing have thus been proposed, all of which relied on linguistic features and textual data to construct their models. Human metaphor comprehension is, however, known to rely on both our linguistic and perceptual experience, and vision can play a particularly important role when metaphorically projecting imagery across domains. In this paper, we present the first metaphor identification method that simultaneously draws knowledge from linguistic and visual data. Our results demonstrate that it outperforms linguistic and visual models in isolation, as well as being competitive with the best-performing metaphor identification methods, that rely on hand-crafted knowledge about domains and perception",
    "volume": "main",
    "checked": true,
    "id": "f4fa88834ce0b460bb80acbaceb5fd5418611286",
    "citation_count": 143
  },
  "https://aclanthology.org/N16-1021": {
    "title": "Bridge Correlational Neural Networks for Multilingual Multimodal Representation Learning",
    "abstract": "Recently there has been a lot of interest in learning common representations for multiple views of data. Typically, such common representations are learned using a parallel corpus between the two views (say, 1M images and their English captions). In this work, we address a real-world scenario where no direct parallel data is available between two views of interest (say, $V_1$ and $V_2$) but parallel data is available between each of these views and a pivot view ($V_3$). We propose a model for learning a common representation for $V_1$, $V_2$ and $V_3$ using only the parallel data available between $V_1V_3$ and $V_2V_3$. The proposed model is generic and even works when there are $n$ views of interest and only one pivot view which acts as a bridge between them. There are two specific downstream applications that we focus on (i) transfer learning between languages $L_1$,$L_2$,...,$L_n$ using a pivot language $L$ and (ii) cross modal access between images and a language $L_1$ using a pivot language $L_2$. Our model achieves state-of-the-art performance in multilingual document classification on the publicly available multilingual TED corpus and promising results in multilingual multimodal retrieval on a new dataset created and released as a part of this work",
    "volume": "main",
    "checked": true,
    "id": "bb0f00d8fa7cac167e213c0828835a8ff78791c8",
    "citation_count": 48
  },
  "https://aclanthology.org/N16-1022": {
    "title": "Unsupervised Visual Sense Disambiguation for Verbs using Multimodal Embeddings",
    "abstract": "We introduce a new task, visual sense disambiguation for verbs: given an image and a verb, assign the correct sense of the verb, i.e., the one that describes the action depicted in the image. Just as textual word sense disambiguation is useful for a wide range of NLP tasks, visual sense disambiguation can be useful for multimodal tasks such as image retrieval, image description, and text illustration. We introduce VerSe, a new dataset that augments existing multimodal datasets (COCO and TUHOI) with sense labels. We propose an unsupervised algorithm based on Lesk which performs visual sense disambiguation using textual, visual, or multimodal embeddings. We find that textual embeddings perform well when gold-standard textual annotations (object labels and image descriptions) are available, while multimodal embeddings perform well on unannotated images. We also verify our findings by using the textual and multimodal embeddings as features in a supervised setting and analyse the performance of visual sense disambiguation task. VerSe is made publicly available and can be downloaded at: this https URL",
    "volume": "main",
    "checked": true,
    "id": "3e3cf09d619cff79b8379b639cddfcd09451995b",
    "citation_count": 43
  },
  "https://aclanthology.org/N16-1023": {
    "title": "Stating the Obvious: Extracting Visual Common Sense Knowledge",
    "abstract": "Obtaining common sense knowledge using current information extraction techniques is extremely challenging. In this work, we instead propose to derive simple common sense statements from fully annotated object detection corpora such as the Microsoft Common Objects in Context dataset. We show that many thousands of common sense facts can be extracted from such corpora at high quality. Furthermore, using WordNet and a novel submodular k-coverage formulation, we are able to generalize our initial set of common sense assertions to unseen objects and uncover over 400k potentially useful facts",
    "volume": "main",
    "checked": true,
    "id": "0dcc768631d9ede8a3679e980b37204b782781b2",
    "citation_count": 46
  },
  "https://aclanthology.org/N16-1024": {
    "title": "Recurrent Neural Network Grammars",
    "abstract": "We introduce recurrent neural network grammars, probabilistic models of sentences with explicit phrase structure. We explain efficient inference procedures that allow application to both parsing and language modeling. Experiments show that they provide better parsing in English than any single previously published supervised generative model and better language modeling than state-of-the-art sequential RNNs in English and Chinese",
    "volume": "main",
    "checked": true,
    "id": "7345843e87c81e24e42264859b214d26042f8d51",
    "citation_count": 466
  },
  "https://aclanthology.org/N16-1025": {
    "title": "Expected F-Measure Training for Shift-Reduce Parsing with Recurrent Neural Networks",
    "abstract": "Xu acknowledges the Carnegie Trust for the Universities of Scotland and the Cambridge Trusts for funding. Clark is supported by ERC Starting Grant DisCoTex (306920) and EPSRC grant EP/I037512/1",
    "volume": "main",
    "checked": true,
    "id": "0e08a9fa9d85646f52665be81e82b6755fba9c77",
    "citation_count": 19
  },
  "https://aclanthology.org/N16-1026": {
    "title": "LSTM CCG Parsing",
    "abstract": "We demonstrate that a state-of-the-art parser can be built using only a lexical tagging model and a deterministic grammar, with no explicit model of bi-lexical dependencies. Instead, all dependencies are implicitly encoded in an LSTM supertagger that assigns CCG lexical categories. The parser significantly outperforms all previously published CCG results, supports efficient and optimal A decoding, and benefits substantially from semisupervised tri-training. We give a detailed analysis, demonstrating that the parser can recover long-range dependencies with high accuracy and that the semi-supervised learning enables significant accuracy gains. By running the LSTM on a GPU, we are able to parse over 2600 sentences per second while improving state-of-the-art accuracy by 1.1 F1 in domain and up to 4.5 F1 out of domain",
    "volume": "main",
    "checked": true,
    "id": "440dc6e5a06f35890d04b36768fe7b6e6d320684",
    "citation_count": 87
  },
  "https://aclanthology.org/N16-1027": {
    "title": "Supertagging With LSTMs",
    "abstract": "In this paper we present new state-of-the-art performance on CCG supertagging and parsing. Our model outperforms existing approaches by an absolute gain of 1.5%. We analyze the performance of several neural models and demonstrate that while feed-forward architectures can compete with bidirectional LSTMs on POS tagging, models that encode the complete sentence are necessary for the long range syntactic information encoded in supertags",
    "volume": "main",
    "checked": true,
    "id": "0f5bb9ae0c060b349597c0b2582bf271a5a2156a",
    "citation_count": 67
  },
  "https://aclanthology.org/N16-1028": {
    "title": "An Empirical Study of Automatic Chinese Word Segmentation for Spoken Language Understanding and Named Entity Recognition",
    "abstract": "Word segmentation is usually recognized as the first step for many Chinese natural language processing tasks, yet its impact on these subsequent tasks is relatively under-studied. For example, how to solve the mismatch problem when applying an existing word segmenter to new data? Does a better word segmenter yield a better subsequent NLP task performance? In this work, we conduct an initial attempt to answer these questions on two related subsequent tasks: semantic slot filling in spoken language understanding and named entity recognition. We propose three techniques to solve the mismatch problem: using word segmentation outputs as additional features, adaptation with partial-learning and taking advantage of n-best word segmentation list. Experimental results demonstrate the effectiveness of these techniques for both tasks and we achieve an error reduction of about 11% for spoken language understanding and 24% for named entity recognition over the baseline systems",
    "volume": "main",
    "checked": true,
    "id": "b889fb0a917a1fdccd0fb36248535f613400f0a4",
    "citation_count": 13
  },
  "https://aclanthology.org/N16-1029": {
    "title": "Name Tagging for Low-resource Incident Languages based on Expectation-driven Learning",
    "abstract": "In this paper we tackle a challenging name tagging problem in an emergent setting the tagger needs to be complete within a few hours for a new incident language (IL) using very few resources. Inspired by observing how human annotators attack this challenge, we propose a new expectation-driven learning framework. In this framework we rapidly acquire, categorize, structure and zoom in on ILspecific expectations (rules, features, patterns, gazetteers, etc.) from various non-traditional sources: consulting and encoding linguistic knowledge from native speakers, mining and projecting patterns from both mono-lingual and cross-lingual corpora, and typing based on cross-lingual entity linking. We also propose a cost-aware combination approach to compose expectations. Experiments on seven low-resource languages demonstrate the effectiveness and generality of this framework: we are able to setup a name tagger for a new IL within two hours, and achieve 33.8%-65.1% F-score 1",
    "volume": "main",
    "checked": true,
    "id": "6afb411e0944740801f95f101a90a4e4532921dc",
    "citation_count": 27
  },
  "https://aclanthology.org/N16-1030": {
    "title": "Neural Architectures for Named Entity Recognition",
    "abstract": "Comunicacio presentada a la 2016 Conference of the North American Chapter of the Association for Computational Linguistics, celebrada a San Diego (CA, EUA) els dies 12 a 17 de juny 2016",
    "volume": "main",
    "checked": true,
    "id": "24158c9fc293c8a998ac552b1188404a877da292",
    "citation_count": 3421
  },
  "https://aclanthology.org/N16-1031": {
    "title": "Dynamic Feature Induction: The Last Gist to the State-of-the-Art",
    "abstract": "We introduce a novel technique called dynamic feature induction that keeps inducing high dimensional features automatically until the feature space becomes 'more' linearly separable. Dynamic feature induction searches for the feature combinations that give strong clues for distinguishing certain label pairs, and generates joint features from these combinations. These induced features are trained along with the primitive low dimensional features. Our approach was evaluated on two core NLP tasks, part-of-speech tagging and named entity recognition, and showed the state-of-the-art results for both tasks, achieving the accuracy of 97.64 and the F1-score of 91.00 respectively, with about a 25% increase in the feature space",
    "volume": "main",
    "checked": true,
    "id": "b335347036b79868421d5afb6af4de67eaff4147",
    "citation_count": 55
  },
  "https://aclanthology.org/N16-1032": {
    "title": "Drop-out Conditional Random Fields for Twitter with Huge Mined Gazetteer",
    "abstract": "In named entity recognition task especially for massive data like Twitter, having a large amount of high quality gazetteers can alleviate the problem of training data scarcity. One could collect large gazetteers from knowledge graph and phrase embeddings to obtain high coverage of gazetteers. However, large gazetteers cause a side-effect called \"feature under-training\", where the gazetteer features overwhelm the context features. To resolve this problem, we propose the dropout conditional random ﬁelds, which decrease the inﬂuence of gazetteer features with a high weight. Our experiments on named entity recognition with Twitter data lead to higher F1 score of 69.38%, about 4% better than the strong baseline presented in Smith and Osborne (2006)",
    "volume": "main",
    "checked": true,
    "id": "8b43fb323f6cd2aef551eb4a191ad5a5a4d3aaaf",
    "citation_count": 10
  },
  "https://aclanthology.org/N16-1033": {
    "title": "Joint Extraction of Events and Entities within a Document Context",
    "abstract": "Events and entities are closely related; entities are often actors or participants in events and events without entities are uncommon. The interpretation of events and entities is highly contextually dependent. Existing work in information extraction typically models events separately from entities, and performs inference at the sentence level, ignoring the rest of the document. In this paper, we propose a novel approach that models the dependencies among variables of events, entities, and their relations, and performs joint inference of these variables across a document. The goal is to enable access to document-level contextual information and facilitate context-aware predictions. We demonstrate that our approach substantially outperforms the state-of-the-art methods for event extraction as well as a strong baseline for entity extraction",
    "volume": "main",
    "checked": true,
    "id": "c558e2b5dcab8d89f957f3045a9bbd43fd6a28ed",
    "citation_count": 197
  },
  "https://aclanthology.org/N16-1034": {
    "title": "Joint Event Extraction via Recurrent Neural Networks",
    "abstract": "Event extraction is a particularly challenging problem in information extraction. The stateof-the-art models for this problem have either applied convolutional neural networks in a pipelined framework (Chen et al., 2015) or followed the joint architecture via structured prediction with rich local and global features (Li et al., 2013). The former is able to learn hidden feature representations automatically from data based on the continuous and generalized representations of words. The latter, on the other hand, is capable of mitigating the error propagation problem of the pipelined approach and exploiting the inter-dependencies between event triggers and argument roles via discrete structures. In this work, we propose to do event extraction in a joint framework with bidirectional recurrent neural networks, thereby benefiting from the advantages of the two models as well as addressing issues inherent in the existing approaches. We systematically investigate different memory features for the joint model and demonstrate that the proposed model achieves the state-of-the-art performance on the ACE 2005 dataset",
    "volume": "main",
    "checked": true,
    "id": "7afc4b1b89081d118700801064e251870d05617b",
    "citation_count": 465
  },
  "https://aclanthology.org/N16-1035": {
    "title": "Top-down Tree Long Short-Term Memory Networks",
    "abstract": "Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have been successfully applied to a variety of sequence modeling tasks. In this paper we develop Tree Long Short-Term Memory (TreeLSTM), a neural network model based on LSTM, which is designed to predict a tree rather than a linear sequence. TreeLSTM defines the probability of a sentence by estimating the generation probability of its dependency tree. At each time step, a node is generated based on the representation of the generated sub-tree. We further enhance the modeling power of TreeLSTM by explicitly representing the correlations between left and right dependents. Application of our model to the MSR sentence completion challenge achieves results beyond the current state of the art. We also report results on dependency parsing reranking achieving competitive performance",
    "volume": "main",
    "checked": true,
    "id": "e5f0762b9cfd07f88608f5502ed4467a8b5546cb",
    "citation_count": 99
  },
  "https://aclanthology.org/N16-1036": {
    "title": "Recurrent Memory Networks for Language Modeling",
    "abstract": "Recurrent Neural Networks (RNN) have obtained excellent result in many natural language processing (NLP) tasks. However, understanding and interpreting the source of this success remains a challenge. In this paper, we propose Recurrent Memory Network (RMN), a novel RNN architecture, that not only amplifies the power of RNN but also facilitates our understanding of its internal functioning and allows us to discover underlying patterns in data. We demonstrate the power of RMN on language modeling and sentence completion tasks. On language modeling, RMN outperforms Long Short-Term Memory (LSTM) network on three large German, Italian, and English dataset. Additionally we perform in-depth analysis of various linguistic dimensions that RMN captures. On Sentence Completion Challenge, for which it is essential to capture sentence coherence, our RMN obtains 69.2% accuracy, surpassing the previous state-of-the-art by a large margin",
    "volume": "main",
    "checked": true,
    "id": "889e57259a1d6017701fb2c2ceece82f9f4eff4c",
    "citation_count": 92
  },
  "https://aclanthology.org/N16-1037": {
    "title": "A Latent Variable Recurrent Neural Network for Discourse-Driven Language Models",
    "abstract": "This paper presents a novel latent variable recurrent neural network architecture for jointly modeling sequences of words and (possibly latent) discourse relations between adjacent sentences. A recurrent neural network generates individual words, thus reaping the benefits of discriminatively-trained vector representations. The discourse relations are represented with a latent variable, which can be predicted or marginalized, depending on the task. The resulting model can therefore employ a training objective that includes not only discourse relation classification, but also word prediction. As a result, it outperforms state-of-the-art alternatives for two tasks: implicit discourse relation classification in the Penn Discourse Treebank, and dialog act classification in the Switchboard corpus. Furthermore, by marginalizing over latent discourse relations at test time, we obtain a discourse informed language model, which improves over a strong LSTM baseline",
    "volume": "main",
    "checked": true,
    "id": "3dfd76e4b5083ee71049099c3105e8af69edb44f",
    "citation_count": 91
  },
  "https://aclanthology.org/N16-1038": {
    "title": "Questioning Arbitrariness in Language: a Data-Driven Study of Conventional Iconicity",
    "abstract": "This paper presents a data-driven investigation of phonesthemes, phonetic units said to carry meaning associations, thus challenging the traditionally assumed arbitrariness of language. Phonesthemes have received a substantial amount of attention within the cognitive science literature on sound iconicity, but nevertheless remain a controversial and understudied phenomenon. Here we employ NLP techniques to address two main questions: How can the existence of phonesthemes be tested at a large scale with quantitative methods? And how can the meaning arguably carried by a phonestheme be induced automatically from word embeddings? We develop novel methods to make progress on these fronts and compare our results to previous work, obtaining substantial improvements",
    "volume": "main",
    "checked": true,
    "id": "d74b98b7e1952215beb27c1e326d5320d5155ea3",
    "citation_count": 9
  },
  "https://aclanthology.org/N16-1039": {
    "title": "Distinguishing Literal and Non-Literal Usage of German Particle Verbs",
    "abstract": "This paper provides a binary, token-based classification of German particle verbs (PVs) into literal vs. non-literal usage. A random forest improving standard features (e.g., bagof-words; affective ratings) with PV-specific information and abstraction over common nouns significantly outperforms the majority baseline. In addition, PV-specific classification experiments demonstrate the role of shared particle semantics and semantically related base verbs in PV meaning shifts",
    "volume": "main",
    "checked": true,
    "id": "6ae6480afcd27cd5257ad0b72d485d4529d3000a",
    "citation_count": 28
  },
  "https://aclanthology.org/N16-1040": {
    "title": "Phrasal Substitution of Idiomatic Expressions",
    "abstract": "Idioms pose a great challenge to natural language understanding. A system that can automatically paraphrase idioms in context has applications in many NLP tasks. This paper proposes a phrasal substitution method to replace idioms with their figurative meanings in literal English. Our approach identifies relevant replacement phrases from an idiom's dictionary definition and performs appropriate grammatical and referential transformations to ensure that the idiom substitution fits seamlessly into the original context. The proposed method has been evaluated both by automatic metrics and human judgments. Results suggest that high quality paraphrases of idiomatic expressions can be achieved",
    "volume": "main",
    "checked": true,
    "id": "508e9d056edee01e6d8b8526afcb7c1106686b3f",
    "citation_count": 17
  },
  "https://aclanthology.org/N16-1041": {
    "title": "Leverage Financial News to Predict Stock Price Movements Using Word Embeddings and Deep Neural Networks",
    "abstract": "Financial news contains useful information on public companies and the market. In this paper we apply the popular word embedding methods and deep neural networks to leverage financial news to predict stock price movements in the market. Experimental results have shown that our proposed methods are simple but very effective, which can significantly improve the stock prediction accuracy on a standard financial database over the baseline system using only the historical price information",
    "volume": "main",
    "checked": true,
    "id": "9d11d6dc35221007787da75622d2030424ab367c",
    "citation_count": 72
  },
  "https://aclanthology.org/N16-1042": {
    "title": "Grammatical error correction using neural machine translation",
    "abstract": "This paper presents the first study using neural machine translation (NMT) for grammatical error correction (GEC). We propose a twostep approach to handle the rare word problem in NMT, which has been proved to be useful and effective for the GEC task. Our best NMTbased system trained on the CLC outperforms our SMT-based system when testing on the publicly available FCE test set. The same system achieves an F0.5 score of 39.90% on the CoNLL-2014 shared task test set, outperforming the state-of-the-art and demonstrating that the NMT-based GEC system generalises effectively",
    "volume": "main",
    "checked": true,
    "id": "558044f59a1855fbd08a747fb0fd1ea1ee7c19af",
    "citation_count": 184
  },
  "https://aclanthology.org/N16-1043": {
    "title": "Multimodal Semantic Learning from Child-Directed Input",
    "abstract": "Children learn the meaning of words by being exposed to perceptually rich situations (linguistic discourse, visual scenes, etc). Current computational learning models typically simulate these rich situations through impoverished symbolic approximations. In this work, we present a distributed word learning model that operates on child-directed speech paired with realistic visual scenes. The model integrates linguistic and extra-linguistic information (visual and social cues), handles referential uncertainty, and correctly learns to associate words with objects, even in cases of limited linguistic exposure",
    "volume": "main",
    "checked": true,
    "id": "0bde6c7f1d4a24701ac50f615af4b2b3febaa6fa",
    "citation_count": 20
  },
  "https://aclanthology.org/N16-1044": {
    "title": "Recurrent Support Vector Machines For Slot Tagging In Spoken Language Understanding",
    "abstract": "We propose recurrent support vector machine (RSVM) for slot tagging. This model is a combination of the recurrent neural network (RNN) and the structured support vector machine. RNN extracts features from the input sequence. The structured support vector machine uses a sequence-level discriminative objective function. The proposed model therefore combines the sequence representation capability of an RNN with the sequence-level discriminative objective. We have observed new state-ofthe-art results on two benchmark datasets and one private dataset. RSVM obtained statistical significant 4% and 2% relative average F1 score improvement on ATIS dataset and Chunking dataset, respectively. Out of eight domains in Cortana live log dataset, RSVM achieved F1 score improvement on seven domains. Experiments also show that RSVM significantly speeds up the model training by skipping the weight updating for non-support vector training samples, compared against training using RNN with CRF or minimum cross-entropy objectives",
    "volume": "main",
    "checked": true,
    "id": "0216df07704f7084f097fd5b5467dc50e0f29f4b",
    "citation_count": 13
  },
  "https://aclanthology.org/N16-1045": {
    "title": "Expectation-Regulated Neural Model for Event Mention Extraction",
    "abstract": "We tackle the task of extracting tweets that mention a specific event from all tweets that contain relevant keywords, for which the main challenges include unbalanced positive and negative cases, and the unavailability of manually labeled training data. Existing methods leverage a few manually given seed events and large unlabeled tweets to train a classifier, by using expectation regularization training with discrete ngram features. We propose a LSTM-based neural model that learns tweet-level features automatically. Compared with discrete ngram features, the neural model can potentially capture non-local dependencies and deep semantic information, which are more effective for disambiguating subtle semantic differences between true event mentions and false cases that use similar wording patterns. Results on both tweets and forum posts show that our neural model is more effective compared with a state-of-the-art discrete baseline",
    "volume": "main",
    "checked": true,
    "id": "ca4d99c8aac9be88dd3c43ccd7c9cf754834ee3e",
    "citation_count": 15
  },
  "https://aclanthology.org/N16-1046": {
    "title": "Agreement on Target-bidirectional Neural Machine Translation",
    "abstract": "Neural machine translation (NMT) with recurrent neural networks, has proven to be an effective technique for end-to-end machine translation. However, in spite of its promising advances over traditional translation methods, it typically suffers from an issue of unbalanced outputs, that arise from both the nature of recurrent neural networks themselves, and the challenges inherent in machine translation. To overcome this issue, we propose an agreement model for neural machine translation and show its effectiveness on large-scale Japaneseto-English and Chinese-to-English translation tasks. Our results show the model can achieve improvements of up to 1.4 BLEU over the strongest baseline NMT system. With the help of an ensemble technique, this new end-to-end NMT approach finally outperformed phrasebased and hierarchical phrase-based Moses baselines by up to 5.6 BLEU points",
    "volume": "main",
    "checked": true,
    "id": "b8e7aa5ee3d280a9032fa9b15a4b79cc021f33ed",
    "citation_count": 61
  },
  "https://aclanthology.org/N16-1047": {
    "title": "Psycholinguistic Features for Deceptive Role Detection in Werewolf",
    "abstract": "We tackle the problem of identifying deceptive agents in highly-motivated high-conflict dialogues. We consider the case where we only have textual information. We show the usefulness of psycho-linguistic deception and persuasion features on a small dataset for the game of Werewolf. We analyse the role of syntax and we identify some characteristics of players in deceptive roles",
    "volume": "main",
    "checked": true,
    "id": "d6bf5a4c62f836ba981a41cbd77e4f1caf0f4dc4",
    "citation_count": 9
  },
  "https://aclanthology.org/N16-1048": {
    "title": "Individual Variation in the Choice of Referential Form",
    "abstract": "This study aims to measure the variation between writers in their choices of referential form by collecting and analysing a new and publicly available corpus of referring expressions. The corpus is composed of referring expressions produced by different participants in identical situations. Results, measured in terms of normalized entropy, reveal substantial individual variation. We discuss the problems and prospects of this finding for automatic text generation applications",
    "volume": "main",
    "checked": true,
    "id": "5dd284c13c4bce6a00b7b6fb5b2e238254646a2a",
    "citation_count": 6
  },
  "https://aclanthology.org/N16-1049": {
    "title": "Joint Learning Templates and Slots for Event Schema Induction",
    "abstract": "Automatic event schema induction (AESI) means to extract meta-event from raw text, in other words, to find out what types (templates) of event may exist in the raw text and what roles (slots) may exist in each event type. In this paper, we propose a joint entity-driven model to learn templates and slots simultaneously based on the constraints of templates and slots in the same sentence. In addition, the entities' semantic information is also considered for the inner connectivity of the entities. We borrow the normalized cut criteria in image segmentation to divide the entities into more accurate template clusters and slot clusters. The experiment shows that our model gains a relatively higher result than previous work",
    "volume": "main",
    "checked": true,
    "id": "a477778d8ae4cb70b67d02d094d030c44d0c6ffd",
    "citation_count": 24
  },
  "https://aclanthology.org/N16-1050": {
    "title": "Inferring Psycholinguistic Properties of Words",
    "abstract": "We introduce a bootstrapping algorithm for regression that exploits word embedding models. We use it to infer four psycholinguistic properties of words: Familiarity, Age of Acquisition, Concreteness and Imagery and further populate the MRC Psycholinguistic Database with these properties. The approach achieves 0.88 correlation with humanproduced values and the inferred psycholinguistic features lead to state-of-the-art results when used in a Lexical Simplification task",
    "volume": "main",
    "checked": true,
    "id": "2c6e169cea5b26dd11f2b14723d2d4c1a1bc892c",
    "citation_count": 37
  },
  "https://aclanthology.org/N16-1051": {
    "title": "Intra-Topic Variability Normalization based on Linear Projection for Topic Classification",
    "abstract": "This paper proposes a variability normalization algorithm to reduce the variability between intra-topic documents for topic classification. Firstly, an optimization problem is constructed based on linear variability removable assumption. Secondly, a new feature space for document representation is found by solving the optimization problem with kernel principle component analysis (KPCA). Finally, effective feature transformation is taken through linear projection. As for experiments, state-of-the-art SVM and KNN algorithm are adopted for topic classification respectively. Experimental results on a free-style conversational corpus show that the proposed variability normalization algorithm for topic classification achieves 3.8% absolute improvement for micro-F1 measure",
    "volume": "main",
    "checked": true,
    "id": "5fc2bdb63517afcbddc78a861c9078fecb205f4e",
    "citation_count": 0
  },
  "https://aclanthology.org/N16-1052": {
    "title": "Shift-Reduce CCG Parsing using Neural Network Models",
    "abstract": "We present a neural network based shift- reduce CCG parser, the first neural-network based parser for CCG. We also study the im- pact of neural network based tagging mod- els, and greedy versus beam-search parsing, by using a structured neural network model. Our greedy parser obtains a labeled F-score of 83.27%, the best reported result for greedy CCG parsing in the literature (an improve- ment of 2.5% over a perceptron based greedy parser) and is more than three times faster. With a beam, our structured neural network model gives a labeled F-score of 85.57% which is 0.6% better than the perceptron based counterpart",
    "volume": "main",
    "checked": true,
    "id": "d938fb451585c84aff53706d58caf86c31677f1b",
    "citation_count": 16
  },
  "https://aclanthology.org/N16-1053": {
    "title": "Online Multilingual Topic Models with Multi-Level Hyperpriors",
    "abstract": "For topic models, such as LDA, that use a bag-of-words assumption, it becomes especially important to break the corpus into appropriately-sized \"documents\". Since the models are estimated solely from the term cooccurrences, extensive documents such as books or long journal articles lead to diffuse statistics, and short documents such as forum posts or product reviews can lead to sparsity. This paper describes practical inference procedures for hierarchical models that smooth topic estimates for smaller sections with hyperpriors over larger documents. Importantly for large collections, these online variational Bayes inference methods perform a single pass over a corpus and achieve better perplexity than \"flat\" topic models on monolingual and multilingual data. Furthermore, on the task of detecting document translation pairs in large multilingual collections, polylingual topic models (PLTM) with multi-level hyperpriors (mlhPLTM) achieve significantly better performance than existing online PLTM models while retaining computational efficiency",
    "volume": "main",
    "checked": true,
    "id": "c02aeb7a6cef2dd329085d18464952aef59ed30e",
    "citation_count": 7
  },
  "https://aclanthology.org/N16-1054": {
    "title": "STransE: a novel embedding model of entities and relationships in knowledge bases",
    "abstract": "Knowledge bases of real-world facts about entities and their relationships are useful resources for a variety of natural language processing tasks. However, because knowledge bases are typically incomplete, it is useful to be able to perform link prediction or knowledge base completion, i.e., predict whether a relationship not in the knowledge base is likely to be true. This paper combines insights from several previous link prediction models into a new embedding model STransE that represents each entity as a low-dimensional vector, and each relation by two matrices and a translation vector. STransE is a simple combination of the SE and TransE models, but it obtains better link prediction performance on two benchmark datasets than previous embedding models. Thus, STransE can serve as a new baseline for the more complex models in the link prediction task",
    "volume": "main",
    "checked": true,
    "id": "50de83b8a00f448b3e344701a60dfdcfd84881f4",
    "citation_count": 175
  },
  "https://aclanthology.org/N16-1055": {
    "title": "An Unsupervised Model of Orthographic Variation for Historical Document Transcription",
    "abstract": "Historical documents frequently exhibit extensive orthographic variation, including archaic spellings and obsolete shorthand. OCR tools typically seek to produce so-called diplomatic transcriptions that preserve these variants, but many end tasks require transcriptions with normalized orthography. In this paper, we present a novel joint transcription model that learns, unsupervised, a probabilistic mapping between modern orthography and that used in the document. Our system thus produces dual diplomatic and normalized transcriptions simultaneously, and achieves a 35% relative error reduction over a state-of-the-art OCR model on diplomatic transcription, and a 46% reduction on normalized transcription",
    "volume": "main",
    "checked": true,
    "id": "9fd1f77436f66a79d279b1d9babc5c937d712388",
    "citation_count": 14
  },
  "https://aclanthology.org/N16-1056": {
    "title": "Bidirectional RNN for Medical Event Detection in Electronic Health Records",
    "abstract": "Sequence labeling for extraction of medical events and their attributes from unstructured text in Electronic Health Record (EHR) notes is a key step towards semantic understanding of EHRs. It has important applications in health informatics including pharmacovigilance and drug surveillance. The state of the art supervised machine learning models in this domain are based on Conditional Random Fields (CRFs) with features calculated from fixed context windows. In this application, we explored recurrent neural network frameworks and show that they significantly out-performed the CRF models",
    "volume": "main",
    "checked": true,
    "id": "f9d4a708d391f7c4bc6b3b3906a3291c636856f7",
    "citation_count": 269
  },
  "https://aclanthology.org/N16-1057": {
    "title": "The Sensitivity of Topic Coherence Evaluation to Topic Cardinality",
    "abstract": "©2016 Association for Computational Linguistics. When evaluating the quality of topics generated by a topic model, the convention is to score topic coherence - either manually or automatically - using the top-N topic words. This hyper-parameter N, or the cardinality of the topic, is often overlooked and selected arbitrarily. In this paper, we investigate the impact of this cardinality hyper-parameter on topic coherence evaluation. For two automatic topic coherence methodologies, we observe that the correlation with human ratings decreases systematically as the cardinality increases. More interestingly, we find that performance can be improved if the system scores and human ratings are aggregated over several topic cardinalities before computing the correlation. In contrast to the standard practice of using a fixed value of N (e.g. N = 5 or N = 10), our results suggest that calculating topic coherence over several different cardinalities and averaging results in a substantially more stable and robust evaluation. We release the code and the datasets used in this research, for reproducibility.1",
    "volume": "main",
    "checked": true,
    "id": "934be2cdd0c06f73b01e704de4d6162dc354e173",
    "citation_count": 44
  },
  "https://aclanthology.org/N16-1058": {
    "title": "Transition-Based Syntactic Linearization with Lookahead Features",
    "abstract": "It has been shown that transition-based methods can be used for syntactic word ordering and tree linearization, achieving significantly faster speed compared with traditional best-first methods. State-of-the-art transitionbased models give competitive results on abstract word ordering and unlabeled tree linearization, but significantly worse results on labeled tree linearization. We demonstrate that the main cause for the performance bottleneck is the sparsity of SHIFT transition actions rather than heavy pruning. To address this issue, we propose a modification to the standard transition-based feature structure, which reduces feature sparsity and allows lookahead features at a small cost to decoding efficiency. Our model gives the best reported accuracies on all benchmarks, yet still being over 30 times faster compared with best-first-search",
    "volume": "main",
    "checked": true,
    "id": "57fe31ee17f4e03fcb152130dec9240813e309bd",
    "citation_count": 17
  },
  "https://aclanthology.org/N16-1059": {
    "title": "A Recurrent Neural Networks Approach for Estimating the Quality of Machine Translation Output",
    "abstract": "This paper presents a novel approach using recurrent neural networks for estimating the quality of machine translation output. A sequence of vectors made by the prediction method is used as the input of the final recurrent neural network. The prediction method uses bi-directional recurrent neural network architecture both on source and target sentence to fully utilize the bi-directional quality information from source and target sentence. Our experiments show that the proposed recurrent neural networks approach achieves a performance comparable to the existing stateof-the-art models for estimating the sentencelevel quality of English-to-Spanish translation",
    "volume": "main",
    "checked": true,
    "id": "d362e225376fcce589a3087f9c87b3e245825f2d",
    "citation_count": 28
  },
  "https://aclanthology.org/N16-1060": {
    "title": "Symmetric Patterns and Coordinations: Fast and Enhanced Representations of Verbs and Adjectives",
    "abstract": "State-of-the-art word embeddings, which are often trained on bag-of-words (BOW) contexts, provide a high quality representation of aspects of the semantics of nouns. However, their quality decreases substantially for the task of verb similarity prediction. In this paper we show that using symmetric pattern contexts (SPs, e.g., \"X and Y\") improves word2vec verb similarity performance by up to 15% and is also instrumental in adjective similarity prediction. The unsupervised SP contexts are even superior to a variety of dependency contexts extracted using a supervised dependency parser. Moreover, we observe that SPs and dependency coordination contexts (Coor) capture a similar type of information, and demonstrate that Coor contexts are superior to other dependency contexts including the set of all dependency contexts, although they are still inferior to SPs. Finally, there are substantially fewer SP contexts compared to alternative representations, leading to a massive reduction in training time. On an 8G words corpus and a 32 core machine, the SP model trains in 11 minutes, compared to 5 and 11 hours with BOW and all dependency contexts, respectively",
    "volume": "main",
    "checked": true,
    "id": "62c4fc68d1dcaf99a6c0b4e7f657861ae573c060",
    "citation_count": 20
  },
  "https://aclanthology.org/N16-1061": {
    "title": "Breaking the Closed World Assumption in Text Classification",
    "abstract": "Existing research on multiclass text classification mostly makes the closed world assumption, which focuses on designing accurate classifiers under the assumption that all test classes are known at training time. A more realistic scenario is to expect unseen classes during testing (open world). In this case, the goal is to design a learning system that classifies documents of the known classes into their respective classes and also to reject documents from unknown classes. This problem is called open (world) classification. This paper approaches the problem by reducing the open space risk while balancing the empirical risk. It proposes to use a new learning strategy, called center-based similarity (CBS) space learning (or CBS learning), to provide a novel solution to the problem. Extensive experiments across two datasets show that CBS learning gives promising results on multiclass open text classification compared to state-ofthe-art baselines",
    "volume": "main",
    "checked": true,
    "id": "c5b556807c19548384886a060216672c11121a72",
    "citation_count": 86
  },
  "https://aclanthology.org/N16-1062": {
    "title": "Sequential Short-Text Classification with Recurrent and Convolutional Neural Networks",
    "abstract": "Recent approaches based on artificial neural networks (ANNs) have shown promising results for short-text classification. However, many short texts occur in sequences (e.g., sentences in a document or utterances in a dialog), and most existing ANN-based systems do not leverage the preceding short texts when classifying a subsequent one. In this work, we present a model based on recurrent neural networks and convolutional neural networks that incorporates the preceding short texts. Our model achieves state-of-the-art results on three different datasets for dialog act prediction",
    "volume": "main",
    "checked": true,
    "id": "f292139b371f5b34e835f4dbff102dc28f972876",
    "citation_count": 398
  },
  "https://aclanthology.org/N16-1063": {
    "title": "Improved Neural Network-based Multi-label Classification with Better Initialization Leveraging Label Co-occurrence",
    "abstract": "In a multi-label text classification task, in which multiple labels can be assigned to one text, label co-occurrence itself is informative. We propose a novel neural network initialization method to treat some of the neurons in the final hidden layer as dedicated neurons for each pattern of label co-occurrence. These dedicated neurons are initialized to connect to the corresponding co-occurring labels with stronger weights than to others. In experiments with a natural language query classification task, which requires multi-label classification, our initialization method improved classification accuracy without any computational overhead in training and evaluation",
    "volume": "main",
    "checked": true,
    "id": "ba98f3e910160e2b699d6db34af5bf52778c983c",
    "citation_count": 69
  },
  "https://aclanthology.org/N16-1064": {
    "title": "Learning Distributed Word Representations For Bidirectional LSTM Recurrent Neural Network",
    "abstract": "Bidirectional long short-term memory (BLSTM) recurrent neural network (RNN) has been successfully applied in many tagging tasks. BLSTM-RNN relies on the distributed representation of words, which implies that the former can be futhermore improved through learning the latter better. In this work, we propose a novel approach to learn distributed word representations by training BLSTM-RNN on a specially designed task which only relies on unlabeled data. Our experimental results show that the proposed approach learns useful distributed word representations, as the trained representations significantly elevate the performance of BLSTM-RNN on three tagging tasks: part-ofspeech tagging, chunking and named entity recognition, surpassing word representations trained by other published methods",
    "volume": "main",
    "checked": true,
    "id": "4a4e473c696150687be151b20cce2d0b1588c37e",
    "citation_count": 33
  },
  "https://aclanthology.org/N16-1065": {
    "title": "Combining Recurrent and Convolutional Neural Networks for Relation Classification",
    "abstract": "This paper investigates two different neural architectures for the task of relation classification: convolutional neural networks and recurrent neural networks. For both models, we demonstrate the effect of different architectural choices. We present a new context representation for convolutional neural networks for relation classification (extended middle context). Furthermore, we propose connectionist bi-directional recurrent neural networks and introduce ranking loss for their optimization. Finally, we show that combining convolutional and recurrent neural networks using a simple voting scheme is accurate enough to improve results. Our neural models achieve state-of-the-art results on the SemEval 2010 relation classification task",
    "volume": "main",
    "checked": true,
    "id": "b04ad4b86f46b0c6670f0da898a1300f385bf3d2",
    "citation_count": 165
  },
  "https://aclanthology.org/N16-1066": {
    "title": "Building Chinese Affective Resources in Valence-Arousal Dimensions",
    "abstract": "An increasing amount of research has recently focused on representing affective states as continuous numerical values on multiple dimensions, such as the valence-arousal (VA) space. Compared to the categorical approach that represents affective states as several classes (e.g., positive and negative), the dimensional approach can provide more finegrained sentiment analysis. However, affective resources with valence-arousal ratings are still very rare, especially for the Chinese language. Therefore, this study builds 1) an affective lexicon called Chinese valence-arousal words (CVAW) containing 1,653 words, and 2) an affective corpus called Chinese valencearousal text (CVAT) containing 2,009 sentences extracted from web texts. To improve the annotation quality, a corpus cleanup procedure is used to remove outlier ratings and improper texts. Experiments using CVAW words to predict the VA ratings of the CVAT corpus show results comparable to those obtained using English affective resources",
    "volume": "main",
    "checked": true,
    "id": "0d1444970774aa59c763cdf3a39ff547f1b7dd96",
    "citation_count": 129
  },
  "https://aclanthology.org/N16-1067": {
    "title": "Improving event prediction by representing script participants",
    "abstract": "Automatically learning script knowledge has proved difficult, with previous work not or just barely beating a most-frequent baseline. Script knowledge is a type of world knowledge which can however be useful for various task in NLP and psycholinguistic modelling. We here propose a model that includes participant information (i.e., knowledge about which participants are relevant for a script) and show, on the Dinners from Hell corpus as well as the InScript corpus, that this knowledge helps us to significantly improve prediction performance on the narrative cloze task",
    "volume": "main",
    "checked": true,
    "id": "262642ea902da3010b7b56f49fe6fc50834a5f37",
    "citation_count": 12
  },
  "https://aclanthology.org/N16-1068": {
    "title": "Structured Prediction with Output Embeddings for Semantic Image Annotation",
    "abstract": "We address the task of annotating images with semantic tuples. Solving this problem requires an algorithm which is able to deal with hundreds of classes for each argument of the tuple. In such contexts, data sparsity becomes a key challenge, as there will be a large number of classes for which only a few examples are available. We propose handling this by incorporating feature representations of both the inputs (images) and outputs (argument classes) into a factorized log-linear model, and exploiting the flexibility of scoring functions based on bilinear forms. Experiments show that integrating feature representations of the outputs in the structured prediction model leads to better overall predictions. We also conclude that the best output representation is specific for each type of argument",
    "volume": "main",
    "checked": true,
    "id": "99a054369f3be3a7eef8dfca77793ba4d44e491b",
    "citation_count": 2
  },
  "https://aclanthology.org/N16-1069": {
    "title": "Large-scale Multitask Learning for Machine Translation Quality Estimation",
    "abstract": "Multitask learning has been proven a useful technique in a number of Natural Language Processing applications where data is scarce and naturally diverse. Examples include learning from data of different domains and learning from labels provided by multiple annotators. Tasks in these scenarios would be the domains or the annotators. When faced with limited data for each task, a framework for the learning of tasks in parallel while using a shared representation is clearly helpful: what is learned for a given task can be transferred to other tasks while the peculiarities of each task are still modelled. Focusing on machine translation quality estimation as application, in this paper we show that multitask learning is also useful in cases where data is abundant. Based on two large-scale datasets, we explore models with multiple annotators and multiple languages and show that state-of-the-art multitask learning algorithms lead to improved results in all settings",
    "volume": "main",
    "checked": true,
    "id": "79c6f1f5f1342cf0852c72b37217e448ae538575",
    "citation_count": 7
  },
  "https://aclanthology.org/N16-1070": {
    "title": "Conversational Markers of Constructive Discussions",
    "abstract": "Group discussions are essential for organizing every aspect of modern life, from faculty meetings to senate debates, from grant review panels to papal conclaves. While costly in terms of time and organization effort, group discussions are commonly seen as a way of reaching better decisions compared to solutions that do not require coordination between the individuals (e.g. voting)---through discussion, the sum becomes greater than the parts. However, this assumption is not irrefutable: anecdotal evidence of wasteful discussions abounds, and in our own experiments we find that over 30% of discussions are unproductive.   We propose a framework for analyzing conversational dynamics in order to determine whether a given task-oriented discussion is worth having or not. We exploit conversational patterns reflecting the flow of ideas and the balance between the participants, as well as their linguistic choices. We apply this framework to conversations naturally occurring in an online collaborative world exploration game developed and deployed to support this research. Using this setting, we show that linguistic cues and conversational patterns extracted from the first 20 seconds of a team discussion are predictive of whether it will be a wasteful or a productive one",
    "volume": "main",
    "checked": true,
    "id": "605be8c6067018ae315d5ae35d69e4ae6797a1fa",
    "citation_count": 41
  },
  "https://aclanthology.org/N16-1071": {
    "title": "Vision and Feature Norms: Improving automatic feature norm learning through cross-modal maps",
    "abstract": "Property norms have the potential to aid a wide range of semantic tasks, provided that they can be obtained for large numbers of concepts. Recent work has focused on text as the main source of information for automatic property extraction. In this paper we examine property norm prediction from visual, rather than textual, data, using cross-modal maps learnt between property norm and visual spaces. We also investigate the importance of having a complete feature norm dataset, for both training and testing. Finally, we evaluate how these datasets and cross-modal maps can be used in an image retrieval task",
    "volume": "main",
    "checked": true,
    "id": "9e694a8aa4448627bb34b22fad2edae95b0a6f17",
    "citation_count": 20
  },
  "https://aclanthology.org/N16-1072": {
    "title": "Cross-lingual Wikification Using Multilingual Embeddings",
    "abstract": "Cross-lingual Wikification is the task of grounding mentions written in non-English documents to entries in the English Wikipedia. This task involves the problem of comparing textual clues across languages, which requires developing a notion of similarity between text snippets across languages. In this paper, we address this problem by jointly training multilingual embeddings for words and Wikipedia titles. The proposed method can be applied to all languages represented in Wikipedia, including those for which no machine translation technology is available. We create a challenging dataset in 12 languages and show that our proposed approach outperforms various baselines. Moreover, our model compares favorably with the best systems on the TAC KBP2015 Entity Linking task including those that relied on the availability of translation from the target language to English",
    "volume": "main",
    "checked": true,
    "id": "ae93607a4c49c49bc684ba4cafb182fc4268b2e3",
    "citation_count": 102
  },
  "https://aclanthology.org/N16-1073": {
    "title": "Deconstructing Complex Search Tasks: a Bayesian Nonparametric Approach for Extracting Sub-tasks",
    "abstract": "Search tasks, comprising a series of search queries serving a common informational need, have steadily emerged as accurate units for developing the next generation of task-aware web search systems. Most prior research in this area has focused on segmenting chronologically ordered search queries into higher level tasks. A more naturalistic viewpoint would involve treating query logs as convoluted structures of tasks-subtasks, with complex search tasks being decomposed into more focused sub-tasks. In this work, we focus on extracting sub-tasks from a given collection of on-task search queries. We jointly leverage insights from Bayesian nonparametrics and word embeddings to identify and extract sub-tasks from a given collection of ontask queries. Our proposed model can inform the design of the next generation of task-based search systems that leverage user's task behavior for better support and personalization",
    "volume": "main",
    "checked": true,
    "id": "2b5bc8a1be2f14005965348d648bcdfbef3a52c7",
    "citation_count": 16
  },
  "https://aclanthology.org/N16-1074": {
    "title": "Probabilistic Models for Learning a Semantic Parser Lexicon",
    "abstract": "We introduce several probabilistic models for learning the lexicon of a semantic parser. Lexicon learning is the first step of training a semantic parser for a new application domain and the quality of the learned lexicon significantly affects both the accuracy and efficiency of the final semantic parser. Existing work on lexicon learning has focused on heuristic methods that lack convergence guarantees and require significant human input in the form of lexicon templates or annotated logical forms. In contrast, our probabilistic models are trained directly from question/answer pairs using EM and our simplest model has a concave objective that guarantees convergence to a global optimum. An experimental evaluation on a set of 4th grade science questions demonstrates that our models improve semantic parser accuracy (35-70% error reduction) and efficiency (4-25x more sentences per second) relative to prior work despite using less human input. Our models also obtain competitive results on GEO880 without any datasetspecific engineering",
    "volume": "main",
    "checked": true,
    "id": "40b132a0127a51658581262cc4acc8fba0524069",
    "citation_count": 11
  },
  "https://aclanthology.org/N16-1075": {
    "title": "Unsupervised Compound Splitting With Distributional Semantics Rivals Supervised Methods",
    "abstract": "In this paper we present a word decompounding method that is based on distributional semantics. Our method does not require any linguistic knowledge and is initialized using a large monolingual corpus. The core idea of our approach is that parts of compounds (like \"candle\" and \"stick\") are semantically similar to the entire compound, which helps to exclude spurious splits (like \"candles\" and \"tick\"). We report results for German and Dutch: For German, our unsupervised method comes on par with the performance of a rule-based and a supervised method and significantly outperforms two unsupervised baselines. For Dutch, our method performs only slightly below a rule-based optimized compound splitter",
    "volume": "main",
    "checked": true,
    "id": "c7b5f35bb40c2070fa032bc10b4153af91c4f382",
    "citation_count": 26
  },
  "https://aclanthology.org/N16-1076": {
    "title": "Weighting Finite-State Transductions With Neural Context",
    "abstract": "How should one apply deep learning to tasks such as morphological reinflection, which stochastically edit one string to get another? A recent approach to such sequence-to-sequence tasks is to compress the input string into a vector that is then used to generate the output string, using recurrent neural networks. In contrast, we propose to keep the traditional architecture, which uses a finite-state transducer to score all possible output strings, but to augment the scoring function with the help of recurrent networks. A stack of bidirectional LSTMs reads the input string from leftto-right and right-to-left, in order to summarize the input context in which a transducer arc is applied. We combine these learned features with the transducer to define a probability distribution over aligned output strings, in the form of a weighted finite-state automaton. This reduces hand-engineering of features, allows learned features to examine unbounded context in the input string, and still permits exact inference through dynamic programming. We illustrate our method on the tasks of morphological reinflection and lemmatization",
    "volume": "main",
    "checked": true,
    "id": "053b3605632c69d0c74c8a054840cfad89268ce3",
    "citation_count": 73
  },
  "https://aclanthology.org/N16-1077": {
    "title": "Morphological Inflection Generation Using Character Sequence to Sequence Learning",
    "abstract": "Morphological inflection generation is the task of generating the inflected form of a given lemma corresponding to a particular linguistic transformation. We model the problem of inflection generation as a character sequence to sequence learning problem and present a variant of the neural encoder-decoder model for solving it. Our model is language independent and can be trained in both supervised and semi-supervised settings. We evaluate our system on seven datasets of morphologically rich languages and achieve either better or comparable results to existing state-of-the-art models of inflection generation",
    "volume": "main",
    "checked": true,
    "id": "6413e6a4f68be0ea6aed0082b205147d9f893699",
    "citation_count": 136
  },
  "https://aclanthology.org/N16-1078": {
    "title": "Towards Unsupervised and Language-independent Compound Splitting using Inflectional Morphological Transformations",
    "abstract": "In this paper, we address the task of languageindependent, knowledge-lean and unsupervised compound splitting, which is an essential component for many natural language processing tasks such as machine translation. Previous methods on statistical compound splitting either include language-specific knowledge (e.g., linking elements) or rely on parallel data, which results in limited applicability. We aim to overcome these limitations by learning compounding morphology from inflectional information derived from lemmatized monolingual corpora. In experiments for Germanic languages, we show that our approach significantly outperforms language-dependent stateof-the-art methods in finding the correct split point and that word inflection is a good approximation for compounding morphology",
    "volume": "main",
    "checked": true,
    "id": "f05257d1b6b8e9627f39527d310337e775899082",
    "citation_count": 19
  },
  "https://aclanthology.org/N16-1079": {
    "title": "Phonological Pun-derstanding",
    "abstract": "Many puns create humor through the relationship between a pun and its phonologically similar target. For example, in \"Don't take geologists for granite\" the word \"granite\" is a pun with the target \"granted\". The recovery of the target in the mind of the listener is essential to the success of the pun. This work introduces a new model for automatic target recovery and provides the first empirical test for this task. The model draws upon techniques for automatic speech recognition using weighted finite-state transducers, and leverages automatically learned phone edit probabilities that give insight into how people perceive sounds and into what makes a good pun. The model is evaluated on a small corpus where it is able to automatically recover a large fraction of the pun targets",
    "volume": "main",
    "checked": true,
    "id": "e1d102db68a7284c3c1ff2502b4928c00049bc9f",
    "citation_count": 22
  },
  "https://aclanthology.org/N16-1080": {
    "title": "A Joint Model of Orthography and Morphological Segmentation",
    "abstract": "We present a model of morphological segmentation that jointly learns to segment and restore orthographic changes, e.g., funniest7! fun-y-est. We term this form of analysis canonical segmentation and contrast it with the traditional surface segmentation, which segments a surface form into a sequence of substrings, e.g., funniest7! funn-i-est. We derive an importance sampling algorithm for approximate inference in the model and report experimental results on English, German and Indonesian",
    "volume": "main",
    "checked": true,
    "id": "0a275c52c61ecf2429189387426d6173e40d695a",
    "citation_count": 38
  },
  "https://aclanthology.org/N16-1081": {
    "title": "Syntactic Parsing of Web Queries with Question Intent",
    "abstract": "Accurate automatic processing of Web queries is important for high-quality information retrieval from the Web. While the syntactic structure of a large portion of these queries is trivial, the structure of queries with question intent is much richer. In this paper we therefore address the task of statistical syntactic parsing of such queries. We first show that the standard dependency grammar does not account for the full range of syntactic structures manifested by queries with question intent. To alleviate this issue we extend the dependency grammar to account for segments – independent syntactic units within a potentially larger syntactic structure. We then propose two distant supervision approaches for the task. Both algorithms do not require manually parsed queries for training. Instead, they are trained on millions of (query, page title) pairs from the Community Question Answering (CQA) domain, where the CQA page was clicked by the user who initiated the query in a search engine. Experiments on a new treebank consisting of 5,000 Web queries from the CQA domain, manually parsed using the proposed grammar, show that our algorithms outperform alternative approaches trained on various sources: tens of thousands of manually parsed OntoNotes sentences, millions of unlabeled CQA queries and thousands of manually segmented CQA queries",
    "volume": "main",
    "checked": true,
    "id": "31dbf6792d8c21579e18f34f22683a99746a0b93",
    "citation_count": 21
  },
  "https://aclanthology.org/N16-1082": {
    "title": "Visualizing and Understanding Neural Models in NLP",
    "abstract": "While neural networks have been successfully applied to many NLP tasks the resulting vector-based models are very difficult to interpret. For example it's not clear how they achieve {\\em compositionality}, building sentence meaning from the meanings of words and phrases. In this paper we describe four strategies for visualizing compositionality in neural models for NLP, inspired by similar work in computer vision. We first plot unit values to visualize compositionality of negation, intensification, and concessive clauses, allow us to see well-known markedness asymmetries in negation. We then introduce three simple and straightforward methods for visualizing a unit's {\\em salience}, the amount it contributes to the final composed meaning: (1) gradient back-propagation, (2) the variance of a token from the average word node, (3) LSTM-style gates that measure information flow. We test our methods on sentiment using simple recurrent nets and LSTMs. Our general-purpose methods may have wide applications for understanding compositionality and other semantic properties of deep networks , and also shed light on why LSTMs outperform simple recurrent nets,",
    "volume": "main",
    "checked": true,
    "id": "fafa541419b3756968fe5b3156c6f0257cb29c23",
    "citation_count": 602
  },
  "https://aclanthology.org/N16-1083": {
    "title": "Bilingual Word Embeddings from Parallel and Non-parallel Corpora for Cross-Language Text Classification",
    "abstract": "In many languages, sparse availability of resources causes numerous challenges for textual analysis tasks. Text classification is one of such standard tasks that is hindered due to limited availability of label information in lowresource languages. Transferring knowledge (i.e. label information) from high-resource to low-resource languages might improve text classification as compared to the other approaches like machine translation. We introduce BRAVE (Bilingual paRAgraph VEctors), a model to learn bilingual distributed representations (i.e. embeddings) of words without word alignments either from sentencealigned parallel or label-aligned non-parallel document corpora to support cross-language text classification. Empirical analysis shows that classification models trained with our bilingual embeddings outperforms other stateof-the-art systems on three different crosslanguage text classification tasks",
    "volume": "main",
    "checked": true,
    "id": "41cd54e6c66684dd2dd627ebcaec03a18a50cf52",
    "citation_count": 85
  },
  "https://aclanthology.org/N16-1084": {
    "title": "Joint Learning with Global Inference for Comment Classification in Community Question Answering",
    "abstract": "This paper addresses the problem of comment classification in community Question Answering. Following the state of the art, we approach the task with a global inference process to exploit the information of all comments in the answer-thread in the form of a fully connected graph. Our contribution comprises two novel joint learning models that are on-line and integrate inference within learning. The first one jointly learns two nodeand edge-level MaxEnt classifiers with stochastic gradient descent and integrates the inference step with loopy belief propagation. The second model is an instance of fully connected pairwise CRFs (FCCRF). The FCCRF model significantly outperforms all other approaches and yields the best results on the task to date. Crucial elements for its success are the global normalization and an Ising-like edge potential",
    "volume": "main",
    "checked": true,
    "id": "84d654ba3b9b7c1c54da85bbbcca6f3f6011580a",
    "citation_count": 25
  },
  "https://aclanthology.org/N16-1085": {
    "title": "Weak Semi-Markov CRFs for Noun Phrase Chunking in Informal Text",
    "abstract": "This paper introduces a new annotated corpus based on an existing informal text corpus: the NUS SMS Corpus (Chen and Kan, 2013). The new corpus includes 76,490 noun phrases from 26,500 SMS messages, annotated by university students. We then explored several graphical models, including a novel variant of the semi-Markov conditional random fields (semi-CRF) for the task of noun phrase chunking. We demonstrated through empirical evaluations on the new dataset that the new variant yielded similar accuracy but ran in significantly lower running time compared to the conventional semi-CRF",
    "volume": "main",
    "checked": true,
    "id": "e52438a03201d9c2cdc70a7e5f1c3fdb9860c3be",
    "citation_count": 15
  },
  "https://aclanthology.org/N16-1086": {
    "title": "What to talk about and how? Selective Generation using LSTMs with Coarse-to-Fine Alignment",
    "abstract": "We propose an end-to-end, domain-independent neural encoder-aligner-decoder model for selective generation, i.e., the joint task of content selection and surface realization. Our model first encodes a full set of over-determined database event records via an LSTM-based recurrent neural network, then utilizes a novel coarse-to-fine aligner to identify the small subset of salient records to talk about, and finally employs a decoder to generate free-form descriptions of the aligned, selected records. Our model achieves the best selection and generation results reported to-date (with 59% relative improvement in generation) on the benchmark WeatherGov dataset, despite using no specialized features or linguistic resources. Using an improved k-nearest neighbor beam filter helps further. We also perform a series of ablations and visualizations to elucidate the contributions of our key model components. Lastly, we evaluate the generalizability of our model on the RoboCup dataset, and get results that are competitive with or better than the state-of-the-art, despite being severely data-starved",
    "volume": "main",
    "checked": true,
    "id": "f273adbfe0e6ba39a583b9669d94cc8d828d8c25",
    "citation_count": 279
  },
  "https://aclanthology.org/N16-1087": {
    "title": "Generation from Abstract Meaning Representation using Tree Transducers",
    "abstract": "Language generation from purely semantic representations is a challenging task. This paper addresses generating English from the Abstract Meaning Representation (AMR), consisting of re-entrant graphs whose nodes are concepts and edges are relations. The new method is trained statistically from AMRannotated English and consists of two major steps: (i) generating an appropriate spanning tree for the AMR, and (ii) applying tree-tostring transducers to generate English. The method relies on discriminative learning and an argument realization model to overcome data sparsity. Initial tests on held-out data show good promise despite the complexity of the task. The system is available open-source as part of JAMR at: http://github.com/jflanigan/jamr",
    "volume": "main",
    "checked": true,
    "id": "0729515f62042d1274c131360c33a121df71c856",
    "citation_count": 95
  },
  "https://aclanthology.org/N16-1088": {
    "title": "A Corpus and Semantic Parser for Multilingual Natural Language Querying of OpenStreetMap",
    "abstract": "We present a corpus of 2,380 natural language queries paired with machine readable formulae that can be executed against world wide geographic data of the OpenStreetMap (OSM) database. We use the corpus to learn an accurate semantic parser that builds the basis of a natural language interface to OSM. Furthermore, we use response-based learning on parser feedback to adapt a statistical machine translation system for multilingual database access to OSM. Our framework allows to map fuzzy natural language expressions such as \"nearby\", \"north of\", or \"in walking distance\" to spatial polygons on an interactive map. Furthermore, it combines syntactic complexity and compositionality with a reasonable lexical variability of queries, making it an interesting new publicly available dataset for research on semantic parsing",
    "volume": "main",
    "checked": true,
    "id": "a418437e3f75865bcad2da65ebf422c49a36eacd",
    "citation_count": 34
  },
  "https://aclanthology.org/N16-1089": {
    "title": "Natural Language Communication with Robots",
    "abstract": "We propose a framework for devising empirically testable algorithms for bridging the communication gap between humans and robots. We instantiate our framework in the context of a problem setting in which humans give instructions to robots using unrestricted natural language commands, with instruction sequences being subservient to building complex goal configurations in a blocks world. We show how one can collect meaningful training data and we propose three neural architectures for interpreting contextually grounded natural language commands. The proposed architectures allow us to correctly understand/ground the blocks that the robot should move when instructed by a human who uses unrestricted language. The architectures have more difficulty in correctly understanding/grounding the spatial relations required to place blocks correctly, especially when the blocks are not easily identifiable",
    "volume": "main",
    "checked": true,
    "id": "c9d65eee1b5df8ccda87c024b88e1b620099b316",
    "citation_count": 98
  },
  "https://aclanthology.org/N16-1090": {
    "title": "Inter-document Contextual Language model",
    "abstract": "In this paper, we examine the impact of employing contextual, structural information from a tree-structured document set to derive a language model. Our results show that this information significantly improves the accuracy of the resultant model",
    "volume": "main",
    "checked": true,
    "id": "df3a6b58cf0b4f4aa3d424922c681aeb09611d2d",
    "citation_count": 9
  },
  "https://aclanthology.org/N16-1091": {
    "title": "Ultradense Word Embeddings by Orthogonal Transformation",
    "abstract": "Embeddings are generic representations that are useful for many NLP tasks. In this paper, we introduce DENSIFIER, a method that learns an orthogonal transformation of the embedding space that focuses the information relevant for a task in an ultradense subspace of a dimensionality that is smaller by a factor of 100 than the original space. We show that ultradense embeddings generated by DENSIFIER reach state of the art on a lexicon creation task in which words are annotated with three types of lexical information - sentiment, concreteness and frequency. On the SemEval2015 10B sentiment analysis task we show that no information is lost when the ultradense subspace is used, but training is an order of magnitude more efficient due to the compactness of the ultradense space",
    "volume": "main",
    "checked": true,
    "id": "4ebe5a18f1bef09680da77086c12fa996b818c80",
    "citation_count": 113
  },
  "https://aclanthology.org/N16-1092": {
    "title": "Separating Actor-View from Speaker-View Opinion Expressions using Linguistic Features",
    "abstract": "We examine different features and classifiers for the categorization of opinion words into actor and speaker view. To our knowledge, this is the first comprehensive work to address sentiment views on the word level taking into consideration opinion verbs, nouns and adjectives. We consider many high-level features requiring only few labeled training data. A detailed feature analysis produces linguistic insights into the nature of sentiment views. We also examine how far global constraints between different opinion words help to increase classification performance. Finally, we show that our (prior) word-level annotation correlates with contextual sentiment views",
    "volume": "main",
    "checked": true,
    "id": "7f7df62b8118099d9a59f660519ff5507eec9653",
    "citation_count": 6
  },
  "https://aclanthology.org/N16-1093": {
    "title": "Clustering for Simultaneous Extraction of Aspects and Features from Reviews",
    "abstract": "This paper presents a clustering approach that simultaneously identifies product features and groups them into aspect categories from online reviews. Unlike prior approaches that first extract features and then group them into categories, the proposed approach combines feature and aspect discovery instead of chaining them. In addition, prior work on feature extraction tends to require seed terms and focus on identifying explicit features, while the proposed approach extracts both explicit and implicit features, and does not require seed terms. We evaluate this approach on reviews from three domains. The results show that it outperforms several state-of-the-art methods on both tasks across all three domains",
    "volume": "main",
    "checked": true,
    "id": "a3f4df1cd21ebcfae1a73ce4cdec1f9d3898a06f",
    "citation_count": 32
  },
  "https://aclanthology.org/N16-1094": {
    "title": "Opinion Holder and Target Extraction on Opinion Compounds – A Linguistic Approach",
    "abstract": "We present an approach to the new task of opinion holder and target extraction on opinion compounds. Opinion compounds (e.g. user rating or victim support) are noun compounds whose head is an opinion noun. We do not only examine features known to be effective for noun compound analysis, such as paraphrases and semantic classes of heads and modifiers, but also propose novel features tailored to this new task. Among them, we examine paraphrases that jointly consider holders and targets, a verb detour in which noun heads are replaced by related verbs, a global head constraint allowing inferencing between different compounds, and the categorization of the sentiment view that the head conveys",
    "volume": "main",
    "checked": true,
    "id": "6437107a7a8b146a50affaee33bbe1641dcaca6c",
    "citation_count": 16
  },
  "https://aclanthology.org/N16-1095": {
    "title": "Capturing Reliable Fine-Grained Sentiment Associations by Crowdsourcing and Best–Worst Scaling",
    "abstract": "Access to word-sentiment associations is useful for many applications, including sentiment analysis, stance detection, and linguistic analysis. However, manually assigning fine-grained sentiment association scores to words has many challenges with respect to keeping annotations consistent. We apply the annotation technique of Best-Worst Scaling to obtain real-valued sentiment association scores for words and phrases in three different domains: general English, English Twitter, and Arabic Twitter. We show that on all three domains the ranking of words by sentiment remains remarkably consistent even when the annotation process is repeated with a different set of annotators. We also, for the first time, determine the minimum difference in sentiment association that is perceptible to native speakers of a language",
    "volume": "main",
    "checked": true,
    "id": "332b05ff46f779754a8fefbfe3695f98aed9ed81",
    "citation_count": 109
  },
  "https://aclanthology.org/N16-1096": {
    "title": "Mapping Verbs in Different Languages to Knowledge Base Relations using Web Text as Interlingua",
    "abstract": "In recent years many knowledge bases (KBs) have been constructed, yet there is not yet a verb resource that maps to these growing KB resources. A resource that maps verbs in different languages to KB relations would be useful for extracting facts from text into the KBs, and to aid alignment and integration of knowledge across different KBs and languages. Such a multi-lingual verb resource would also be useful for tasks such as machine translation and machine reading. In this paper, we present a scalable approach to automatically construct such a verb resource using a very large web text corpus as a kind of interlingua to relate verb phrases to KB relations. Given a text corpus in any language and any KB, it can produce a mapping of that language's verb phrases to the KB relations. Experiments with the English NELL KB and ClueWeb corpus show that the learned English verb-to-relation mapping is effective for extracting relation instances from English text. When applied to a Portuguese NELL KB and a Portuguese text corpus, the same method automatically constructs a verb resource in Portuguese that is effective for extracting relation instances from Portuguese text",
    "volume": "main",
    "checked": true,
    "id": "5e3f5dcf2d3729c418154ef5d9805f0b3e2a9762",
    "citation_count": 11
  },
  "https://aclanthology.org/N16-1097": {
    "title": "Comparing Convolutional Neural Networks to Traditional Models for Slot Filling",
    "abstract": "We address relation classification in the context of slot filling, the task of finding and evaluating fillers like \"Steve Jobs\" for the slot X in \"X founded Apple\". We propose a convolutional neural network which splits the input sentence into three parts according to the relation arguments and compare it to state-ofthe-art and traditional approaches of relation classification. Finally, we combine different methods and show that the combination is better than individual approaches. We also analyze the effect of genre differences on performance",
    "volume": "main",
    "checked": true,
    "id": "4063846a1396b3c201641b5459c1670f1a3c578a",
    "citation_count": 45
  },
  "https://aclanthology.org/N16-1098": {
    "title": "A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories",
    "abstract": "Representation and learning of commonsense knowledge is one of the foundational problems in the quest to enable deep language understanding. This issue is particularly challenging for understanding casual and correlational relationships between events. While this topic has received a lot of interest in the NLP community, research has been hindered by the lack of a proper evaluation framework. This paper attempts to address this problem with a new framework for evaluating story understanding and script learning: the `Story Cloze Test'. This test requires a system to choose the correct ending to a four-sentence story. We created a new corpus of 50k five-sentence commonsense stories, ROCStories, to enable this evaluation. This corpus is unique in two ways: (1) it captures a rich set of causal and temporal commonsense relations between daily events, and (2) it is a high quality collection of everyday life stories that can also be used for story generation. Experimental evaluation shows that a host of baselines and state-of-the-art models based on shallow language understanding struggle to achieve a high score on the Story Cloze Test. We discuss these implications for script and story learning, and offer suggestions for deeper language understanding",
    "volume": "main",
    "checked": true,
    "id": "85b68477a6e031d88b963833e15a4b4fc6855264",
    "citation_count": 499
  },
  "https://aclanthology.org/N16-1099": {
    "title": "Dynamic Entity Representation with Max-pooling Improves Machine Reading",
    "abstract": "We propose a novel neural network model for machine reading, DER Network, which explicitly implements a reader building dynamic meaning representations for entities by gathering and accumulating information around the entities as it reads a document. Evaluated on a recent large scale dataset (Hermann et al., 2015), our model exhibits better results than previous research, and we find that max-pooling is suited for modeling the accumulation of information on entities. Further analysis suggests that our model can put together multiple pieces of information encoded in different sentences to answer complicated questions. Our code for the model is available at https://github. com/soskek/der-network",
    "volume": "main",
    "checked": true,
    "id": "c4916a5fb50bcc73213b6f054c42ad10c68c52cd",
    "citation_count": 41
  },
  "https://aclanthology.org/N16-1100": {
    "title": "Speed-Constrained Tuning for Statistical Machine Translation Using Bayesian Optimization",
    "abstract": "We address the problem of automatically finding the parameters of a statistical machine translation system that maximize BLEU scores while ensuring that decoding speed exceeds a minimum value. We propose the use of Bayesian Optimization to efficiently tune the speed-related decoding parameters by easily incorporating speed as a noisy constraint function. The obtained parameter values are guaranteed to satisfy the speed constraint with an associated confidence margin. Across three language pairs and two speed constraint values, we report overall optimization time reduction compared to grid and random search. We also show that Bayesian Optimization can decouple speed and BLEU measurements, resulting in a further reduction of overall optimization time as speed is measured over a small subset of sentences",
    "volume": "main",
    "checked": true,
    "id": "3c3bcd889a37da4db3efe91c2c87605eb9c6cad2",
    "citation_count": 5
  },
  "https://aclanthology.org/N16-1101": {
    "title": "Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism",
    "abstract": "We propose multi-way, multilingual neural machine translation. The proposed approach enables a single neural translation model to translate between multiple languages, with a number of parameters that grows only linearly with the number of languages. This is made possible by having a single attention mechanism that is shared across all language pairs. We train the proposed multi-way, multilingual model on ten language pairs from WMT'15 simultaneously and observe clear performance improvements over models trained on only one language pair. In particular, we observe that the proposed model significantly improves the translation quality of low-resource language pairs",
    "volume": "main",
    "checked": true,
    "id": "9ed9bff37ec952134564b3b2a022b7aba9479ff2",
    "citation_count": 571
  },
  "https://aclanthology.org/N16-1102": {
    "title": "Incorporating Structural Alignment Biases into an Attentional Neural Translation Model",
    "abstract": "Neural encoder-decoder models of machine translation have achieved impressive results, rivalling traditional translation models. However their modelling formulation is overly simplistic, and omits several key inductive biases built into traditional models. In this paper we extend the attentional neural translation model to include structural biases from word based alignment models, including positional bias, Markov conditioning, fertility and agreement over translation directions. We show improvements over a baseline attentional model and standard phrase-based model over several language pairs, evaluating on difficult languages in a low resource setting",
    "volume": "main",
    "checked": true,
    "id": "ada937c9f51316c6ac87f9d1d4509383d23e0c21",
    "citation_count": 176
  },
  "https://aclanthology.org/N16-1103": {
    "title": "Multilingual Relation Extraction using Compositional Universal Schema",
    "abstract": "Universal schema builds a knowledge base (KB) of entities and relations by jointly embedding all relation types from input KBs as well as textual patterns expressing relations from raw text. In most previous applications of universal schema, each textual pattern is represented as a single embedding, preventing generalization to unseen patterns. Recent work employs a neural network to capture patterns' compositional semantics, providing generalization to all possible input text. In response, this paper introduces significant further improvements to the coverage and flexibility of universal schema relation extraction: predictions for entities unseen in training and multilingual transfer learning to domains with no annotation. We evaluate our model through extensive experiments on the English and Spanish TAC KBP benchmark, outperforming the top system from TAC 2013 slot-filling using no handwritten patterns or additional annotation. We also consider a multilingual setting in which English training data entities overlap with the seed KB, but Spanish text does not. Despite having no annotation for Spanish data, we train an accurate predictor, with additional improvements obtained by tying word embeddings across languages. Furthermore, we find that multilingual training improves English relation extraction accuracy. Our approach is thus suited to broad-coverage automated knowledge base construction in a variety of languages and domains",
    "volume": "main",
    "checked": true,
    "id": "eda32bba3e4e8a42d63e1d6a2648b8f831b782a4",
    "citation_count": 93
  },
  "https://aclanthology.org/N16-1104": {
    "title": "Effective Crowd Annotation for Relation Extraction",
    "abstract": "Can crowdsourced annotation of training data boost performance for relation extraction over methods based solely on distant supervision? While crowdsourcing has been shown effective for many NLP tasks, previous researchers found only minimal improvement when applying the method to relation extraction. This paper demonstrates that a much larger boost is possible, e.g., raising F1 from 0.40 to 0.60. Furthermore, the gains are due to a simple, generalizable technique, Gated Instruction, which combines an interactive tutorial, feedback to correct errors during training, and improved screening",
    "volume": "main",
    "checked": true,
    "id": "e1b3a5622bc06510e8797e0c11593aa16d7b6523",
    "citation_count": 72
  },
  "https://aclanthology.org/N16-1105": {
    "title": "A Translation-Based Knowledge Graph Embedding Preserving Logical Property of Relations",
    "abstract": "This paper proposes a novel translation-based knowledge graph embedding that preserves the logical properties of relations such as transitivity and symmetricity. The embedding space generated by existing translation-based embeddings do not represent transitive and symmetric relations precisely, because they ignore the role of entities in triples. Thus, we introduce a role-specific projection which maps an entity to distinct vectors according to its role in a triple. That is, a head entity is projected onto an embedding space by a head projection operator, and a tail entity is projected by a tail projection operator. This idea is applied to TransE, TransR, and TransD to produce lppTransE, lppTransR, and lppTransD, respectively. According to the experimental results on link prediction and triple classification, the proposed logical property preserving embeddings show the state-of-the-art performance at both tasks. These results prove that it is critical to preserve logical properties of relations while embedding knowledge graphs, and the proposed method does it effectively",
    "volume": "main",
    "checked": true,
    "id": "e4e7bc893b6fb4ff8ebbff899be65d96d50ccd1d",
    "citation_count": 47
  },
  "https://aclanthology.org/N16-1106": {
    "title": "DAG-Structured Long Short-Term Memory for Semantic Compositionality",
    "abstract": "Recurrent neural networks, particularly long short-term memory (LSTM), have recently shown to be very effective in a wide range of sequence modeling problems, core to which is effective learning of distributed representation for subsequences as well as the sequences they form. An assumption in almost all the previous models, however, posits that the learned representation (e.g., a distributed representation for a sentence), is fully compositional from the atomic components (e.g., representations for words), while non-compositionality is a basic phenomenon in human languages. In this paper, we relieve the assumption by extending the chain-structured LSTM to directed acyclic graphs (DAGs), with the aim to endow linear-chain LSTMs with the capability of considering compositionality together with non-compositionality in the same semantic composition framework. From a more general viewpoint, the proposed models incorporate additional prior knowledge into recurrent neural networks, which is interesting to us, considering most NLP tasks have relatively small training data and appropriate prior knowledge could be beneficial to help cover missing semantics. Our experiments on sentiment composition demonstrate that the proposed models achieve the state-of-the-art performance, outperforming models that lack this ability",
    "volume": "main",
    "checked": true,
    "id": "831cfd0c5120f84a857b90f17ac761339fad0dd9",
    "citation_count": 34
  },
  "https://aclanthology.org/N16-1107": {
    "title": "Bayesian Supervised Domain Adaptation for Short Text Similarity",
    "abstract": "Identiﬁcation of short text similarity ( STS ) is a high-utility NLP task with applications in a variety of domains. We explore adaptation of STS algorithms to different target domains and applications. A two-level hierarchical Bayesian model is employed for domain adaptation ( DA ) of a linear STS model to text from different sources (e.g., news, tweets). This model is then further extended for multitask learning ( MTL ) of three related tasks: STS , short answer scoring ( SAS ) and answer sentence ranking ( ASR ). In our experiments, the adaptive model demonstrates better overall cross-domain and cross-task performance over two non-adaptive baselines",
    "volume": "main",
    "checked": true,
    "id": "696616b97cd621a9af676582d8c34eea9a986e48",
    "citation_count": 8
  },
  "https://aclanthology.org/N16-1108": {
    "title": "Pairwise Word Interaction Modeling with Deep Neural Networks for Semantic Similarity Measurement",
    "abstract": "Textual similarity measurement is a challenging problem, as it requires understanding the semantics of input sentences. Most previous neural network models use coarse-grained sentence modeling, which has difficulty capturing fine-grained word-level information for semantic comparisons. As an alternative, we propose to explicitly model pairwise word interactions and present a novel similarity focus mechanism to identify important correspondences for better similarity measurement. Our ideas are implemented in a novel neural network architecture that demonstrates state-ofthe-art accuracy on three SemEval tasks and two answer selection tasks",
    "volume": "main",
    "checked": true,
    "id": "29006d8c9c2247fca4cd3a22822c2b042e85572d",
    "citation_count": 226
  },
  "https://aclanthology.org/N16-1109": {
    "title": "An Attentional Model for Speech Translation Without Transcription",
    "abstract": "For many low-resource languages, spoken language resources are more likely to be annotated with translations than transcriptions. This bilingual speech data can be used for word-spotting, spoken document retrieval, and even for documentation of endangered languages. We experiment with the neural, attentional model applied to this data. On phoneto-word alignment and translation reranking tasks, we achieve large improvements relative to several baselines. On the more challenging speech-to-word alignment task, our model nearly matches GIZA++'s performance on gold transcriptions, but without recourse to transcriptions or to a lexicon",
    "volume": "main",
    "checked": true,
    "id": "c0570d2981d1698c94070e9d6ef78bc588e307ff",
    "citation_count": 129
  },
  "https://aclanthology.org/N16-1110": {
    "title": "Information Density and Quality Estimation Features as Translationese Indicators for Human Translation Classification",
    "abstract": "This paper introduces information density and machine translation quality estimation inspired features to automatically detect and classify human translated texts. We investigate two settings: discriminating between translations and comparable originally authored texts, and distinguishing two levels of translation professionalism. Our framework is based on delexicalised sentence-level dense feature vector representations combined with a supervised machine learning approach. The results show state-of-the-art performance for mixed-domain translationese detection with information density and quality estimation based features, while results on translation expertise classification are mixed",
    "volume": "main",
    "checked": true,
    "id": "79140081f9bdb267952a1af5b733bf6b72101f6f",
    "citation_count": 27
  },
  "https://aclanthology.org/N16-1111": {
    "title": "Interpretese vs. Translationese: The Uniqueness of Human Strategies in Simultaneous Interpretation",
    "abstract": "Computational approaches to simultaneous interpretation are stymied by how little we know about the tactics human interpreters use. We produce a parallel corpus of translated and simultaneously interpreted text and study differences between them through a computational approach. Our analysis reveals that human interpreters regularly apply several effective tactics to reduce translation latency, including sentence segmentation and passivization. In addition to these unique, clever strategies, we show that limited human memory also causes other idiosyncratic properties of human interpretation such as generalization and omission of source content",
    "volume": "main",
    "checked": true,
    "id": "37e57091f17cbc016e29cacbfe90240b4d36db37",
    "citation_count": 45
  },
  "https://aclanthology.org/N16-1112": {
    "title": "LSTM Neural Reordering Feature for Statistical Machine Translation",
    "abstract": "Artificial neural networks are powerful models, which have been widely applied into many aspects of machine translation, such as language modeling and translation modeling. Though notable improvements have been made in these areas, the reordering problem still remains a challenge in statistical machine translations. In this paper, we present a novel neural reordering model that directly models word pairs and alignment. By utilizing LSTM recurrent neural networks, much longer context could be learned for reordering prediction. Experimental results on NIST OpenMT12 Arabic-English and Chinese-English 1000-best rescoring task show that our LSTM neural reordering feature is robust and achieves significant improvements over various baseline systems",
    "volume": "main",
    "checked": true,
    "id": "7f0232b9ffbc637cdddaed2520098cddc066e5d6",
    "citation_count": 30
  },
  "https://aclanthology.org/N16-1113": {
    "title": "A Novel Approach to Dropped Pronoun Translation",
    "abstract": "Dropped Pronouns (DP) in which pronouns   are frequently dropped in the source language   but should be retained in the target language   are challenge in machine translation. In response to this problem, we propose a semisupervised approach to recall possibly missing   pronouns in the translation. Firstly, we build   training data for DP generation in which the   DPs are automatically labelled according to   the alignment information from a parallel corpus. Secondly, we build a deep learning-based   DP generator for input sentences in decoding   when no corresponding references exist. More   specifically, the generation is two-phase: (1)   DP position detection, which is modeled as a   sequential labelling task with recurrent neural   networks; and (2) DP prediction, which employs a multilayer perceptron with rich features. Finally, we integrate the above outputs   into our translation system to recall missing   pronouns by both extracting rules from the   DP-labelled training data and translating the   DP-generated input sentences. Experimental   results show that our approach achieves a significant improvement of 1.58 BLEU points in   translation performance with 66% F-score for   DP generation accuracy",
    "volume": "main",
    "checked": true,
    "id": "f0652f506a9369e95aa94f9717e2c5e92844ab17",
    "citation_count": 36
  },
  "https://aclanthology.org/N16-1114": {
    "title": "Learning Global Features for Coreference Resolution",
    "abstract": "There is compelling evidence that coreference prediction would benefit from modeling global information about entity-clusters. Yet, state-of-the-art performance can be achieved with systems treating each mention prediction independently, which we attribute to the inherent difficulty of crafting informative cluster-level features. We instead propose to use recurrent neural networks (RNNs) to learn latent, global representations of entity clusters directly from their mentions. We show that such representations are especially useful for the prediction of pronominal mentions, and can be incorporated into an end-to-end coreference system that outperforms the state of the art without requiring any additional search",
    "volume": "main",
    "checked": true,
    "id": "77770099cd73e6da90f046ac92fa2f9d32e469f6",
    "citation_count": 175
  },
  "https://aclanthology.org/N16-1115": {
    "title": "Search Space Pruning: A Simple Solution for Better Coreference Resolvers",
    "abstract": "There is a significant gap between the performance of a coreference resolution system on gold mentions and on system mentions. This gap is due to the large and unbalanced search space in coreference resolution when using system mentions. In this paper we show that search space pruning is a simple but efficient way of improving coreference resolvers. By incorporating our pruning method in one of the state-of-the-art coreference resolution systems, we achieve the best reported overall score on the CoNLL 2012 English test set. A version of our pruning method is available with the Cort coreference resolution source code",
    "volume": "main",
    "checked": true,
    "id": "39e78f1baaca27c7956b153b78273e6a12c4b775",
    "citation_count": 9
  },
  "https://aclanthology.org/N16-1116": {
    "title": "Unsupervised Ranking Model for Entity Coreference Resolution",
    "abstract": "Coreference resolution is one of the first stages in deep language understanding and its importance has been well recognized in the natural language processing community. In this paper, we propose a generative, unsupervised ranking model for entity coreference resolution by introducing resolution mode variables. Our unsupervised system achieves 58.44% F1 score of the CoNLL metric on the English data from the CoNLL-2012 shared task (Pradhan et al., 2012), outperforming the Stanford deterministic system (Lee et al., 2013) by 3.01%",
    "volume": "main",
    "checked": true,
    "id": "967585e1c7546db6136a2d68c4eec3da39c22a67",
    "citation_count": 16
  },
  "https://aclanthology.org/N16-1117": {
    "title": "Embedding Lexical Features via Low-Rank Tensors",
    "abstract": "Modern NLP models rely heavily on engineered features, which often combine word and contextual information into complex lexical features. Such combination results in large numbers of features, which can lead to over-fitting. We present a new model that represents complex lexical features---comprised of parts for words, contextual information and labels---in a tensor that captures conjunction information among these parts. We apply low-rank tensor approximations to the corresponding parameter tensors to reduce the parameter space and improve prediction speed. Furthermore, we investigate two methods for handling features that include $n$-grams of mixed lengths. Our model achieves state-of-the-art results on tasks in relation extraction, PP-attachment, and preposition disambiguation",
    "volume": "main",
    "checked": true,
    "id": "d9e1a62f7d8c0a6bdd7a7ec809e11415e7c4a95d",
    "citation_count": 8
  },
  "https://aclanthology.org/N16-1118": {
    "title": "The Role of Context Types and Dimensionality in Learning Word Embeddings",
    "abstract": "We provide the first extensive evaluation of how using different types of context to learn skip-gram word embeddings affects performance on a wide range of intrinsic and extrinsic NLP tasks. Our results suggest that while intrinsic tasks tend to exhibit a clear preference to particular types of contexts and higher dimensionality, more careful tuning is required for finding the optimal settings for most of the extrinsic tasks that we considered. Furthermore, for these extrinsic tasks, we find that once the benefit from increasing the embedding dimensionality is mostly exhausted, simple concatenation of word embeddings, learned with different context types, can yield further performance gains. As an additional contribution, we propose a new variant of the skip-gram model that learns word embeddings from weighted contexts of substitute words",
    "volume": "main",
    "checked": true,
    "id": "cf9830bf8d2babc3c32d192cdccb27aeaf46a048",
    "citation_count": 114
  },
  "https://aclanthology.org/N16-1119": {
    "title": "Improve Chinese Word Embeddings by Exploiting Internal Structure",
    "abstract": "Recently, researchers have demonstrated that both Chinese word and its component characters provide rich semantic information when learning Chinese word embeddings. However, they ignored the semantic similarity across component characters in a word. In this paper, we learn the semantic contribution of characters to a word by exploiting the similarity between a word and its component characters with the semantic knowledge obtained from other languages. We propose a similaritybased method to learn Chinese word and character embeddings jointly. This method is also capable of disambiguating Chinese characters and distinguishing non-compositional Chinese words. Experiments on word similarity and text classification demonstrate the effectiveness of our method",
    "volume": "main",
    "checked": true,
    "id": "289af7e0f6d9892b7803255aa4c826e982073a06",
    "citation_count": 59
  },
  "https://aclanthology.org/N16-1120": {
    "title": "Assessing Relative Sentence Complexity using an Incremental CCG Parser",
    "abstract": "Given a pair of sentences, we present computational models to assess if one sentence is simpler to read than the other. While existing models explored the usage of phrase structure features using a non-incremental parser, experimental evidence suggests that the human language processor works incrementally. We empirically evaluate if syntactic features from an incremental CCG parser are more useful than features from a non-incremental phrase structure parser. Our evaluation on Simple and Standard Wikipedia sentence pairs suggests that incremental CCG features are indeed more useful than phrase structure features achieving 0.44 points gain in performance. Incremental CCG parser also gives significant improvements in speed (12 times faster) in comparison to the phrase structure parser. Furthermore, with the addition of psycholinguistic features, we achieve the strongest result to date reported on this task. Our code and data can be downloaded from https://github. com/bharatambati/sent-compl",
    "volume": "main",
    "checked": true,
    "id": "fe6faa5b0fd267c7c2b99d688aa8ec40e6d47060",
    "citation_count": 31
  },
  "https://aclanthology.org/N16-1121": {
    "title": "Frustratingly Easy Cross-Lingual Transfer for Transition-Based Dependency Parsing",
    "abstract": "In this paper, we present a straightforward strategy for transferring dependency parsers across languages. The proposed method learns a parser from partially annotated data obtained through the projection of annotations across unambiguous word alignments. It does not rely on any modeling of the reliability of dependency and/or alignment links and is therefore easy to implement and parameter free. Experiments on six languages show that our method is at par with recent algorithmically demanding methods, at a much cheaper computational cost. It can thus serve as a fair baseline for transferring dependencies across languages with the use of parallel corpora",
    "volume": "main",
    "checked": true,
    "id": "11f52425dc5824c6caeece5019fd714ab70cf735",
    "citation_count": 38
  },
  "https://aclanthology.org/N16-1122": {
    "title": "Geolocation for Twitter: Timing Matters",
    "abstract": "Automated geolocation of social media messages can benefit a variety of downstream applications. However, these geolocation systems are typically evaluated without attention to how changes in time impact geolocation. Since different people, in different locations write messages at different times, these factors can significantly vary the performance of a geolocation system over time. We demonstrate cyclical temporal effects on geolocation accuracy in Twitter, as well as rapid drops as test data moves beyond the time period of training data. We show that temporal drift can effectively be countered with even modest online model updates",
    "volume": "main",
    "checked": true,
    "id": "afd30325958a35fe26406ba9ad381902905e84ab",
    "citation_count": 53
  },
  "https://aclanthology.org/N16-1123": {
    "title": "Fast and Easy Short Answer Grading with High Accuracy",
    "abstract": "We present a fast, simple, and high-accuracy short answer grading system. Given a shortanswer question and its correct answer, key measures of the correctness of a student response can be derived from its semantic similarity with the correct answer. Our supervised model (1) utilizes recent advances in the identification of short-text similarity, and (2) augments text similarity features with key grading-specific constructs. We present experimental results where our model demonstrates top performance on multiple benchmarks",
    "volume": "main",
    "checked": true,
    "id": "985443d2ab56323269551a6f895657aa5ff1a7d7",
    "citation_count": 88
  },
  "https://aclanthology.org/N16-1124": {
    "title": "Interlocking Phrases in Phrase-based Statistical Machine Translation",
    "abstract": "This paper presents an study of the use of interlocking phrases in phrase-based statistical machine translation. We examine the effect on translation quality when the translation units used in the translation hypotheses are allowed to overlap on the source side, on the target side and on both sides. A large-scale evaluation on 380 language pairs was conducted. Our results show that overall the use of overlapping phrases improved translation quality by 0.3 BLEU points on average. Further analysis revealed that language pairs requiring a larger amount of re-ordering benefited the most from our approach. When the evaluation was restricted to such pairs, the average improvement increased to up to 0.75 BLEU points with over 97% of the pairs improving. Our approach requires only a simple modification to the decoding algorithm and we believe it should be generally applicable to improve the performance of phrase-based decoders",
    "volume": "main",
    "checked": true,
    "id": "12619dff4bcf493d8691722cfec81ab1e65390f6",
    "citation_count": 0
  },
  "https://aclanthology.org/N16-1125": {
    "title": "Eyes Don't Lie: Predicting Machine Translation Quality Using Eye Movement",
    "abstract": "Poorly translated text is often disfluent and difficult to read. In contrast, well-formed translations require less time to process. In this paper, we model the differences in reading patterns of Machine Translation (MT) evaluators using novel features extracted from their gaze data, and we learn to predict the quality scores given by those evaluators. We test our predictions in a pairwise ranking scenario, measuring Kendall's tau correlation with the judgments. We show that our features provide information beyond fluency, and can be combined with BLEU for better predictions. Furthermore, our results show that reading patterns can be used to build semi-automatic metrics that anticipate the scores given by the evaluators",
    "volume": "main",
    "checked": true,
    "id": "c5c8ef5e42bd76d6c50cfc47ffa15e8c91930255",
    "citation_count": 5
  },
  "https://aclanthology.org/N16-1126": {
    "title": "Making Dependency Labeling Simple, Fast and Accurate",
    "abstract": "This work addresses the task of dependency labeling—assigning labels to an (unlabeled) dependency tree. We employ and extend a feature representation learning approach, optimizing it for both high speed and accuracy. We apply our labeling model on top of state-of-the-art parsers and evaluate its performance on standard benchmarks including the CoNLL-2009 and the English PTB datasets. Our model processes over 1,700 English sentences per second, which is 30 times faster than the sparse-feature method. It improves labeling accuracy over the outputs of top parsers, achieving the best LAS on 5 out of 7 datasets1",
    "volume": "main",
    "checked": true,
    "id": "a73cd7ac2fe83c58ad0fcccfc5138b18ca1d82ce",
    "citation_count": 1
  },
  "https://aclanthology.org/N16-1127": {
    "title": "Deep Lexical Segmentation and Syntactic Parsing in the Easy-First Dependency Framework",
    "abstract": "We explore the consequences of representing token segmentations as hierarchical structures (trees) for the task of Multiword Expression (MWE) recognition, in isolation or in combination with dependency parsing. We propose a novel representation of token segmentation as trees on tokens, resembling dependency trees. Given this new representation, we present and evaluate two different architectures to combine MWE recognition and dependency parsing in the easy-first framework: a pipeline and a joint system, both taking advantage of lexical and syntactic dimensions. We experimentally validate that MWE recognition significantly helps syntactic parsing",
    "volume": "main",
    "checked": true,
    "id": "296eaf037949a4fb7f929169e33b3221eac3bff4",
    "citation_count": 9
  },
  "https://aclanthology.org/N16-1128": {
    "title": "Sentiment Composition of Words with Opposing Polarities",
    "abstract": "In this paper, we explore sentiment composition in phrases that have at least one positive and at least one negative word—phrases like happy accident and best winter break. We compiled a dataset of such opposing polarity phrases and manually annotated them with real-valued scores of sentiment association. Using this dataset, we analyze the linguistic patterns present in opposing polarity phrases. Finally, we apply several unsupervised and supervised techniques of sentiment composition to determine their efficacy on this dataset. Our best system, which incorporates information from the phrase's constituents, their parts of speech, their sentiment association scores, and their embedding vectors, obtains an accuracy of over 80% on the opposing polarity phrases",
    "volume": "main",
    "checked": true,
    "id": "52a358e57d14c949d41d838ffe488f149ba665a8",
    "citation_count": 31
  },
  "https://aclanthology.org/N16-1129": {
    "title": "Learning to Recognize Ancillary Information for Automatic Paraphrase Identification",
    "abstract": "Previous work on Automatic Paraphrase Identification (PI) is mainly based on modeling text similarity between two sentences. In contrast, we study methods for automatically detecting whether a text fragment only appearing in a sentence of the evaluated sentence pair is important or ancillary information with respect to the paraphrase identification task. Engineering features for this new task is rather difficult, thus, we approach the problem by representing text with syntactic structures and applying tree kernels on them. The results show that the accuracy of our automatic Ancillary Text Classifier (ATC) is promising, i.e., 68.6%, and its output can be used to improve the state of the art in PI",
    "volume": "main",
    "checked": true,
    "id": "8746bedca9067c703c29d5dce3563eda443faa18",
    "citation_count": 2
  },
  "https://aclanthology.org/N16-1130": {
    "title": "Learning a POS tagger for AAVE-like language",
    "abstract": "Part-of-speech (POS) taggers trained on newswire perform much worse on domains such as subtitles, lyrics, or tweets. In addition, these domains are also heterogeneous, e.g., with respect to registers and dialects. In this paper, we consider the problem of learning a POS tagger for subtitles, lyrics, and tweets associated with African-American Vernacular English (AAVE). We learn from a mixture of randomly sampled and manually annotated Twitter data and unlabeled data, which we automatically and partially label using mined tag dictionaries. Our POS tagger obtains a tagging accuracy of 89% on subtitles, 85% on lyrics, and 83% on tweets, with up to 55% error reductions over a state-of-the-art newswire POS tagger, and 15-25% error reductions over a state-of-the-art Twitter POS tagger",
    "volume": "main",
    "checked": true,
    "id": "1f6c731c701b5581f831bb88bd9c63f01e4a68db",
    "citation_count": 30
  },
  "https://aclanthology.org/N16-1131": {
    "title": "PIC a Different Word: A Simple Model for Lexical Substitution in Context",
    "abstract": "The Lexical Substitution task involves selecting and ranking lexical paraphrases for a target word in a given sentential context. We present PIC, a simple measure for estimating the appropriateness of substitutes in a given context. PIC outperforms another simple, comparable model proposed in recent work, especially when selecting substitutes from the entire vocabulary. Analysis shows that PIC improves over baselines by incorporating frequency biases into predictions",
    "volume": "main",
    "checked": true,
    "id": "f1f2b416bfe7d30f76ec55d75088835f62ebf980",
    "citation_count": 22
  },
  "https://aclanthology.org/N16-1132": {
    "title": "Bootstrapping Translation Detection and Sentence Extraction from Comparable Corpora",
    "abstract": "Most work on extracting parallel text from comparable corpora depends on linguistic resources such as seed parallel documents or translation dictionaries. This paper presents a simple baseline approach for bootstrapping a parallel collection. It starts by observing documents published on similar dates and the cooccurrence of a small number of identical tokens across languages. It then uses fast, online inference for a latent variable model to represent multilingual documents in a shared topic space where it can do efficient nearestneighbor search. Starting from the Gigaword collections in English and Spanish, we train a translation system that outperforms one trained on the WMT'11 parallel training set",
    "volume": "main",
    "checked": true,
    "id": "279448c1a33e7c8d2702299a18e6d85e1d503908",
    "citation_count": 8
  },
  "https://aclanthology.org/N16-1133": {
    "title": "Discriminative Reranking for Grammatical Error Correction with Statistical Machine Translation",
    "abstract": "Research on grammatical error correction has received considerable attention. For dealing with all types of errors, grammatical error correction methods that employ statistical machine translation (SMT) have been proposed in recent years. An SMT system generates candidates with scores for all candidates and selects the sentence with the highest score as the correction result. However, the 1-best result of an SMT system is not always the best result. Thus, we propose a reranking approach for grammatical error correction. The reranking approach is used to re-score N-best results of the SMT and reorder the results. Our experiments show that our reranking system using parts of speech and syntactic features improves performance and achieves state-of-theart quality, with an F0.5 score of 40.0",
    "volume": "main",
    "checked": true,
    "id": "61dc593697033409d954134bc0bee98944a8ffe5",
    "citation_count": 36
  },
  "https://aclanthology.org/N16-1134": {
    "title": "Patterns of Wisdom: Discourse-Level Style in Multi-Sentence Quotations",
    "abstract": "Quotations are kernels not just of wisdom but also of beautiful and striking language. While recent studies have characterized the stylistic features of quotations, we characterize the order of stylistic information within quotations. Analyzing a corpus of two-sentence quotations collected from the social network Tumblr, we explore the ways that both low-level features and high-level features tend to occur in either the first or second sentence. Through analysis of examples, we interpret these tendencies as manifestations of rhetorical patterns. Results from a prediction task suggest that stylistic patterns are more prominent in quotations than in a comparison corpus",
    "volume": "main",
    "checked": true,
    "id": "7935c330c12b0d720a55bab69a3042e3aaea131a",
    "citation_count": 7
  },
  "https://aclanthology.org/N16-1135": {
    "title": "Right-truncatable Neural Word Embeddings",
    "abstract": "This paper proposes an incremental learning strategy for neural word embedding methods, such as SkipGrams and Global Vectors. Since our method iteratively generates embedding vectors one dimension at a time, obtained vectors equip a unique property. Namely, any right-truncated vector matches the solution of the corresponding lower-dimensional embedding. Therefore, a single embedding vector can manage a wide range of dimensional requirements imposed by many different uses and applications",
    "volume": "main",
    "checked": true,
    "id": "21bb36c7d4f7e5ce15f83a1caf2a9637393b03be",
    "citation_count": 2
  },
  "https://aclanthology.org/N16-1136": {
    "title": "MAWPS: A Math Word Problem Repository",
    "abstract": "Recent work across several AI subdisciplines has focused on automatically solving math word problems. In this paper we introduce MAWPS, an online repository of Math Word Problems, to provide a unified testbed to evaluate different algorithms. MAWPS allows for the automatic construction of datasets with particular characteristics, providing tools for tuning the lexical and template overlap of a dataset as well as for filtering ungrammatical problems from web-sourced corpora. The online nature of this repository facilitates easy community contribution. At present, we have amassed 3,320 problems, including the full datasets used in several prominent works",
    "volume": "main",
    "checked": true,
    "id": "2bdbb07bc12b8d8c332b7a84aa05e76218c07cd9",
    "citation_count": 147
  },
  "https://aclanthology.org/N16-1137": {
    "title": "Cross-genre Event Extraction with Knowledge Enrichment",
    "abstract": "The goal of Event extraction is to extract structured information of events that are of interest from unstructured documents. Existing event extractors for social media suffer from two major problems: lack of context and informal nature. In this paper, instead of conducting event extraction solely on each social media message, we incorporate cross-genre knowledge to boost the event extractor performance. Experiment results demonstrate that without any additional annotations, our proposed approach is able to provide 15% absolute F-score improvement over the state-of-the-art",
    "volume": "main",
    "checked": true,
    "id": "ad54d318bb465960fc53e728fb13cf76e15e6f1e",
    "citation_count": 4
  },
  "https://aclanthology.org/N16-1138": {
    "title": "Emergent: a novel data-set for stance classification",
    "abstract": "We present Emergent, a novel data-set derived from a digital journalism project for rumour debunking. The data-set contains 300 rumoured claims and 2,595 associated news articles, collected and labelled by journalists with an estimation of their veracity (true, false or unverified). Each associated article is summarized into a headline and labelled to indicate whether its stance is for, against, or observing the claim, where observing indicates that the article merely repeats the claim. Thus, Emergent provides a real-world data source for a variety of natural language processing tasks in the context of fact-checking. Further to presenting the dataset, we address the task of determining the article headline stance with respect to the claim. For this purpose we use a logistic regression classifier and develop features that examine the headline and its agreement with the claim. The accuracy achieved was 73% which is 26% higher than the one achieved by the Excitement Open Platform (Magnini et al., 2014)",
    "volume": "main",
    "checked": true,
    "id": "efada589efdb0adf3aa9dc2b6cb6979a50658276",
    "citation_count": 330
  },
  "https://aclanthology.org/N16-1139": {
    "title": "BIRA: Improved Predictive Exchange Word Clustering",
    "abstract": "Word clusters are useful for many NLP tasks including training neural network language models, but current increases in datasets are outpacing the ability of word clusterers to handle them. Little attention has been paid thus far on inducing high-quality word clusters at a large scale. The predictive exchange algorithm is quite scalable, but sometimes does not provide as good perplexity as other slower clustering algorithms. We introduce the bidirectional, interpolated, refining, and alternating (BIRA) predictive exchange algorithm. It improves upon the predictive exchange algorithm's perplexity by up to 18%, giving it perplexities comparable to the slower two-sided exchange algorithm, and better perplexities than the slower Brown clustering algorithm. Our BIRA implementation is fast, clustering a 2.5 billion token English News Crawl corpus in 3 hours. It also reduces machine translation training time while preserving translation quality. Our implementation is portable and freely available",
    "volume": "main",
    "checked": true,
    "id": "c91649b5d82b276f05429aa8906e640553d7797f",
    "citation_count": 9
  },
  "https://aclanthology.org/N16-1140": {
    "title": "Integrating Morphological Desegmentation into Phrase-based Decoding",
    "abstract": "When translating into a morphologically complex language, segmenting the target language can reduce data sparsity, while introducing the complication of desegmenting the system output. We present a method for decoderintegrated desegmentation, allowing features that consider the desegmented target, such as a word-level language model, to be considered throughout the entire search space. Our results on a large-scale, English to Arabic translation task show significant improvement over the 1-best desegmentation baseline",
    "volume": "main",
    "checked": true,
    "id": "0c47e0ca2e30b9ecbfe1b0f13f2e1279ef13c594",
    "citation_count": 0
  },
  "https://aclanthology.org/N16-1141": {
    "title": "The Instantiation Discourse Relation: A Corpus Analysis of Its Properties and Improved Detection",
    "abstract": "INSTANTIATION is a fairly common discourse relation and past work has suggested that it plays special roles in local coherence, in sentiment expression and in content selection in summarization. In this paper we provide the first systematic corpus analysis of the relation and show that relation-specific features can improve considerably the detection of the relation. We show that sentences involved in INSTANTIATION are set apart from other sentences by the use of gradable (subjective) adjectives, the occurrence of rare words and by different patterns in part-of-speech usage. Words across arguments of INSTANTIATION are connected through hypernym and meronym relations significantly more often than in other sentences and that they stand out in context by being significantly less similar to each other than other adjacent sentence pairs. These factors provide substantial predictive power that improves the identification of implicit INSTANTIATION relation by more than 5% F-measure",
    "volume": "main",
    "checked": true,
    "id": "bbcd2bbd4aa0da971ccd13d75cfb5881f2cb928a",
    "citation_count": 8
  },
  "https://aclanthology.org/N16-1142": {
    "title": "Sparse Bilingual Word Representations for Cross-lingual Lexical Entailment",
    "abstract": "We introduce the task of cross-lingual lexical entailment, which aims to detect whether the meaning of a word in one language can be inferred from the meaning of a word in another language. We construct a gold standard for this task, and propose an unsupervised solution based on distributional word representations. As commonly done in the monolingual setting, we assume a worde entails a wordf if the prominent context features of e are a subset of those of f . To address the challenge of comparing contexts across languages, we propose a novel method for inducing sparse bilingual word representations from monolingual and parallel texts. Our approach yields an Fscore of 70%, and significantly outperforms strong baselines based on translation and on existing word representations",
    "volume": "main",
    "checked": true,
    "id": "c41abeb23f4f052f0fb9ecf130379f3f4f9b419b",
    "citation_count": 30
  },
  "https://aclanthology.org/N16-1143": {
    "title": "Automatic Prediction of Linguistic Decline in Writings of Subjects with Degenerative Dementia",
    "abstract": "Given the limited success of medication in reversing the effects of Alzheimer's and other dementias, a lot of the neuroscience research has been focused on early detection, in order to slow the progress of the disease through different interventions. We propose a Natural Language Processing approach applied to descriptive writing to attempt to discriminate decline due to normal aging from decline due to predementia conditions. Within the context of a longitudinal study on Alzheimer's disease, we created a unique corpus of 201 descriptions of a control image written by subjects of the study. Our classifier, computing linguistic features, was able to discriminate normal from cognitively impaired patients to an accuracy of 86.1% using lexical and semantic irregularities found in their writing. This is a promising result towards elucidating the existence of a general pattern in linguistic deterioration caused by dementia that might be detectable from a subject's written descriptive language",
    "volume": "main",
    "checked": true,
    "id": "1ed47bfd8c9937e196d9a2aba8468f063578c782",
    "citation_count": 7
  },
  "https://aclanthology.org/N16-1144": {
    "title": "Consensus Maximization Fusion of Probabilistic Information Extractors",
    "abstract": "Current approaches to Information Extraction (IE) are capable of extracting large amounts of facts with associated probabilities. Because no current IE system is perfect, complementary and conflicting facts are obtained when different systems are run over the same data. Knowledge Fusion (KF) is the problem of aggregating facts from different extractors. Existing methods approach KF using supervised learning or deep linguistic knowledge, which either lack sufficient data or are not robust enough. We propose a semi-supervised application of Consensus Maximization to the KF problem, using a combination of supervised and unsupervised models. Consensus Maximization Fusion (CM Fusion) is able to promote high quality facts and eliminate incorrect ones. We demonstrate the effectiveness of our system on the NIST Slot Filler Validation contest, which seeks to evaluate and aggregate multiple independent information extractors. Our system achieved the highest F1 score relative to other system submissions",
    "volume": "main",
    "checked": true,
    "id": "153106b2b079cda1c9f888b6f1af34cbb74b49cd",
    "citation_count": 2
  },
  "https://aclanthology.org/N16-1145": {
    "title": "Simple, Fast Noise-Contrastive Estimation for Large RNN Vocabularies",
    "abstract": "We present a simple algorithm to efficiently train language models with noise-contrastive estimation (NCE) on graphics processing units (GPUs). Our NCE-trained language models achieve significantly lower perplexity on the One Billion Word Benchmark language modeling challenge, and contain one sixth of the parameters in the best single model in Chelba et al. (2013). When incorporated into a strong Arabic-English machine translation system they give a strong boost in translation quality. We release a toolkit so that others may also train large-scale, large vocabulary LSTM language models with NCE, parallelizing computation across multiple GPUs",
    "volume": "main",
    "checked": true,
    "id": "8d6a67a5f4192280a35ccaaf4e660c1772c54a63",
    "citation_count": 44
  },
  "https://aclanthology.org/N16-1146": {
    "title": "Automatically Inferring Implicit Properties in Similes",
    "abstract": "Author(s): Qadir, A; Riloff, E; Walker, MA | Abstract: ©2016 Association for Computational Linguistics. A simile is a figure of speech comparing two fundamentally different things. Sometimes, a simile will explain the basis of a comparison by explicitly mentioning a shared property. For example, \"my room is as cold as Antarctica\" gives \"cold\" as the property shared by the room and Antarctica. But most similes do not give an explicit property (e.g., \"my room feels like Antarctica\") leaving the reader to infer that the room is cold. We tackle the problem of automatically inferring implicit properties evoked by similes. Our approach involves three steps: (1) generating candidate properties from different sources, (2) evaluating properties based on the influence of multiple simile components, and (3) aggregated ranking of the properties. We also present an analysis showing that the difficulty of inferring an implicit property for a simile correlates with its interpretive diversity",
    "volume": "main",
    "checked": true,
    "id": "86f4b7d5f63f1baff0432faea1b2791210310fa2",
    "citation_count": 19
  },
  "https://aclanthology.org/N16-1147": {
    "title": "Visual Storytelling",
    "abstract": "We introduce the first dataset for sequential vision-to-language, and explore how this data may be used for the task of visual storytelling. The first release of this dataset, SIND1 v.1, includes 81,743 unique photos in 20,211 sequences, aligned to both descriptive (caption) and story language. We establish several strong baselines for the storytelling task, and motivate an automatic metric to benchmark progress. Modelling concrete description as well as figurative and social language, as provided in this dataset and the storytelling task, has the potential to move artificial intelligence from basic understandings of typical visual scenes towards more and more human-like understanding of grounded event structure and subjective expression",
    "volume": "main",
    "checked": true,
    "id": "927987a48c2a519bbc097d8b6c925b64a85b7d8e",
    "citation_count": 320
  },
  "https://aclanthology.org/N16-1148": {
    "title": "PRIMT: A Pick-Revise Framework for Interactive Machine Translation",
    "abstract": "Interactive machine translation (IMT) is a method which uses human-computer interactions to improve the quality of MT. Traditional IMT methods employ a left-to-right order for the interactions, which is difficult to directly modify critical errors at the end of the sentence. In this paper, we propose an IMT framework in which the interaction is decomposed into two simple human actions: picking a critical translation error (Pick) and revising the translation (Revise). The picked phrase could be at any position of the sentence, which improves the efficiency of human computer interaction. We also propose automatic suggestion models for the two actions to further reduce the cost of human interaction. Experiment results demonstrate that by interactions through either one of the actions, the translation quality could be significantly improved. Greater gains could be achieved by iteratively performing both actions",
    "volume": "main",
    "checked": true,
    "id": "3f33479d0b2f3a993e9e1619ceef30bc19cb4e90",
    "citation_count": 23
  },
  "https://aclanthology.org/N16-1149": {
    "title": "Incorporating Side Information into Recurrent Neural Network Language Models",
    "abstract": "Recurrent neural network language models (RNNLM) have recently demonstrated vast potential in modelling long-term dependencies for NLP problems, ranging from speech recognition to machine translation. In this work, we propose methods for conditioning RNNLMs on external side information, e.g., metadata such as keywords, description, document title or topic headline. Our experiments show consistent improvements of RNNLMs using side information over the baselines for two different datasets and genres in two languages. Interestingly, we found that side information in a foreign language can be highly beneficial in modelling texts in another language, serving as a form of cross-lingual language modelling",
    "volume": "main",
    "checked": true,
    "id": "0ac70d4d5db5df83ae610723a377a6904c5ada30",
    "citation_count": 36
  },
  "https://aclanthology.org/N16-1150": {
    "title": "Capturing Semantic Similarity for Entity Linking with Convolutional Neural Networks",
    "abstract": "A key challenge in entity linking is making effective use of contextual information to disambiguate mentions that might refer to different entities in different contexts. We present a model that uses convolutional neural networks to capture semantic correspondence between a mention's context and a proposed target entity. These convolutional networks operate at multiple granularities to exploit various kinds of topic information, and their rich parameterization gives them the capacity to learn which n-grams characterize different topics. We combine these networks with a sparse linear model to achieve state-of-the-art performance on multiple entity linking datasets, outperforming the prior systems of Durrett and Klein (2014) and Nguyen et al. (2014)",
    "volume": "main",
    "checked": true,
    "id": "3b294fb99aa967558befd9b0e2d6f925915080ae",
    "citation_count": 150
  },
  "https://aclanthology.org/N16-1151": {
    "title": "K-Embeddings: Learning Conceptual Embeddings for Words using Context",
    "abstract": "We describe a technique for adding contextual distinctions to word embeddings by extending the usual embedding process — into two phases. The first phase resembles existing methods, but also constructs K classifications of concepts. The second phase uses these classifications in developing refined K embeddings for words, namely word K-embeddings. The technique is iterative, scalable, and can be combined with other methods (including Word2Vec) in achieving still more expressive representations. Experimental results show consistently large performance gains on a Semantic-Syntactic Word Relationship test set for different K settings. For example, an overall gain of 20% is recorded at K = 5. In addition, we demonstrate that an iterative process can further tune the embeddings and gain an extra 1% (K = 10 in 3 iterations) on the same benchmark. The examples also show that polysemous concepts are meaningfully embedded in our K different conceptual embeddings for words",
    "volume": "main",
    "checked": true,
    "id": "f0fea713dcceb86d67811aa0abfc4c5895a84b1f",
    "citation_count": 15
  },
  "https://aclanthology.org/N16-1152": {
    "title": "Convolutional Neural Networks vs. Convolution Kernels: Feature Engineering for Answer Sentence Reranking",
    "abstract": "In this paper, we study, compare and combine two state-of-the-art approaches to automatic feature engineering: Convolution Tree Kernels (CTKs) and Convolutional Neural Networks (CNNs) for learning to rank answer sentences in a Question Answering (QA) setting. When dealing with QA, the key aspect is to encode relational information between the constituents of question and answer in learning algorithms. For this purpose, we propose novel CNNs using relational information and combined them with relational CTKs. The results show that (i) both approaches achieve the state of the art on a question answering task, where CTKs produce higher accuracy and (ii) combining such methods leads to unprecedented high results",
    "volume": "main",
    "checked": true,
    "id": "59f456d31d6e90d1c07d31e438e5be652d0529a2",
    "citation_count": 57
  },
  "https://aclanthology.org/N16-1153": {
    "title": "Semi-supervised Question Retrieval with Gated Convolutions",
    "abstract": "Question answering forums are rapidly growing in size with no effective automated ability to refer to and reuse answers already available for previous posted questions. In this paper, we develop a methodology for finding semantically related questions. The task is difficult since 1) key pieces of information are often buried in extraneous details in the question body and 2) available annotations on similar questions are scarce and fragmented. We design a recurrent and convolutional model (gated convolution) to effectively map questions to their semantic representations. The models are pre-trained within an encoder-decoder framework (from body to title) on the basis of the entire raw corpus, and fine-tuned discriminatively from limited annotations. Our evaluation demonstrates that our model yields substantial gains over a standard IR baseline and various neural network architectures (including CNNs, LSTMs and GRUs)",
    "volume": "main",
    "checked": true,
    "id": "f3399e9a516983e5f4c5a27abb8663aa1f745d74",
    "citation_count": 86
  },
  "https://aclanthology.org/N16-1154": {
    "title": "This is how we do it: Answer Reranking for Open-domain How Questions with Paragraph Vectors and Minimal Feature Engineering",
    "abstract": "We present a simple yet powerful approach to non-factoid answer reranking whereby question-answer pairs are represented by concatenated distributed representation vectors and a multilayer perceptron is used to compute the score for an answer. Despite its simplicity, our approach achieves state-of-the-art performance on a public dataset of How questions, outperforming systems which employ sophisticated feature sets. We attribute this good performance to the use of paragraph instead of word vector representations and to the use of suitable data for training these representations",
    "volume": "main",
    "checked": true,
    "id": "388595bde88e178ad9dba6aa494de16648b7307b",
    "citation_count": 18
  },
  "https://aclanthology.org/N16-1155": {
    "title": "Multilingual Language Processing From Bytes",
    "abstract": "We describe an LSTM-based model which we call Byte-to-Span (BTS) that reads text as bytes and outputs span annotations of the form [start, length, label] where start positions, lengths, and labels are separate entries in our vocabulary. Because we operate directly on unicode bytes rather than language-specific words or characters, we can analyze text in many languages with a single model. Due to the small vocabulary size, these multilingual models are very compact, but produce results similar to or better than the state-of- the-art in Part-of-Speech tagging and Named Entity Recognition that use only the provided training datasets (no external data sources). Our models are learning \"from scratch\" in that they do not rely on any elements of the standard pipeline in Natural Language Processing (including tokenization), and thus can run in standalone fashion on raw text",
    "volume": "main",
    "checked": true,
    "id": "4dabd6182ce2681c758f654561d351739e8df7bf",
    "citation_count": 207
  },
  "https://aclanthology.org/N16-1156": {
    "title": "Ten Pairs to Tag – Multilingual POS Tagging via Coarse Mapping between Embeddings",
    "abstract": "In the absence of annotations in the target language, multilingual models typically draw on extensive parallel resources. In this paper, we demonstrate that accurate multilingual part-of-speech (POS) tagging can be done with just a few (e.g., ten) word translation pairs. We use the translation pairs to establish a coarse linear isometric (orthonormal) mapping between monolingual embeddings. This enables the supervised source model expressed in terms of embeddings to be used directly on the target language. We further reﬁne the model in an unsupervised manner by initializing and regularizing it to be close to the direct transfer model. Averaged across six languages, our model yields a 37.5% absolute improvement over the monolingual prototype-driven method (Haghighi and Klein, 2006) when using a comparable amount of supervision. Moreover, to highlight key linguistic characteristics of the generated tags, we use them to predict typological properties of languages, obtaining a 50% error reduction relative to the prototype model. 1",
    "volume": "main",
    "checked": true,
    "id": "269de8607ca71a2a6a3b8bec36b8fcb168ab4274",
    "citation_count": 108
  },
  "https://aclanthology.org/N16-1157": {
    "title": "Part-of-Speech Tagging for Historical English",
    "abstract": "As more historical texts are digitized, there is interest in applying natural language processing tools to these archives. However, the performance of these tools is often unsatisfactory, due to language change and genre differences. Spelling normalization heuristics are the dominant solution for dealing with historical texts, but this approach fails to account for changes in usage and vocabulary. In this empirical paper, we assess the capability of domain adaptation techniques to cope with historical texts, focusing on the classic benchmark task of part-of-speech tagging. We evaluate several domain adaptation methods on the task of tagging Early Modern English and Modern British English texts in the Penn Corpora of Historical English. We demonstrate that the Feature Embedding method for unsupervised domain adaptation outperforms word embeddings and Brown clusters, showing the importance of embedding the entire feature space, rather than just individual words. Feature Embeddings also give better performance than spelling normalization, but the combination of the two methods is better still, yielding a 5% raw improvement in tagging accuracy on Early Modern English texts",
    "volume": "main",
    "checked": true,
    "id": "129b1098fe3cf3e5eec22992d4d34d83f159cabe",
    "citation_count": 37
  },
  "https://aclanthology.org/N16-1158": {
    "title": "Statistical Modeling of Creole Genesis",
    "abstract": "Creole languages do not fit into the traditional tree model of evolutionary history because multiple languages are involved in their formation. In this paper, we present several statistical models to explore the nature of creole genesis. After reviewing quantitative studies on creole genesis, we first tackle the question of whether creoles are typologically distinct from non-creoles. By formalizing this question as a binary classification problem, we demonstrate that a linear classifier fails to separate creoles from non-creoles although the two groups have substantially different distributions in the feature space. We then model a creole language as a mixture of source languages plus a special restructurer. We find a pervasive influence of the restructurer in creole genesis and some statistical universals in it, paving the way for more elaborate statistical models",
    "volume": "main",
    "checked": true,
    "id": "a653249faa460872881dfbaef8135d682516a6bd",
    "citation_count": 8
  },
  "https://aclanthology.org/N16-1159": {
    "title": "Shallow Parsing Pipeline - Hindi-English Code-Mixed Social Media Text",
    "abstract": "In this study, the problem of shallow parsing of Hindi-English code-mixed social media text (CSMT) has been addressed. We have annotated the data, developed a language identifier, a normalizer, a part-of-speech tagger and a shallow parser. To the best of our knowledge, we are the first to attempt shallow parsing on CSMT. The pipeline developed has been made available to the research community with the goal of enabling better text analysis of Hindi English CSMT. The pipeline is accessible at 1",
    "volume": "main",
    "checked": true,
    "id": "40d92879a1630f40865092f8d9afb1c053847d7b",
    "citation_count": 76
  },
  "https://aclanthology.org/N16-1160": {
    "title": "Bilingual Learning of Multi-sense Embeddings with Discrete Autoencoders",
    "abstract": "We present an approach to learning multi-sense word embeddings relying both on monolingual and bilingual information. Our model consists of an encoder, which uses monolingual and bilingual context (i.e. a parallel sentence) to choose a sense for a given word, and a decoder which predicts context words based on the chosen sense. The two components are estimated jointly. We observe that the word representations induced from bilingual data outperform the monolingual counterparts across a range of evaluation tasks, even though crosslingual information is not available at test time",
    "volume": "main",
    "checked": true,
    "id": "e0ea3b226c8336eaa78ea440f5c81476e1e8b6ad",
    "citation_count": 44
  },
  "https://aclanthology.org/N16-1161": {
    "title": "Polyglot Neural Language Models: A Case Study in Cross-Lingual Phonetic Representation Learning",
    "abstract": "We introduce polyglot language models, recurrent neural network models trained to predict symbol sequences in many different languages using shared representations of symbols and conditioning on typological information about the language to be predicted. We apply these to the problem of modeling phone sequences---a domain in which universal symbol inventories and cross-linguistically shared feature representations are a natural fit. Intrinsic evaluation on held-out perplexity, qualitative analysis of the learned representations, and extrinsic evaluation in two downstream applications that make use of phonetic features show (i) that polyglot models better generalize to held-out data than comparable monolingual models and (ii) that polyglot phonetic feature representations are of higher quality than those learned monolingually",
    "volume": "main",
    "checked": true,
    "id": "6b5da884d4405eed8da18e5fe4dc7127545a3175",
    "citation_count": 64
  },
  "https://aclanthology.org/N16-1162": {
    "title": "Learning Distributed Representations of Sentences from Unlabelled Data",
    "abstract": "Unsupervised methods for learning distributed representations of words are ubiquitous in today's NLP research, but far less is known about the best ways to learn distributed phrase or sentence representations from unlabelled data. This paper is a systematic comparison of models that learn such representations. We find that the optimal approach depends critically on the intended application. Deeper, more complex models are preferable for representations to be used in supervised systems, but shallow log-linear models work best for building representation spaces that can be decoded with simple spatial distance metrics. We also propose two new unsupervised representation-learning objectives designed to optimise the trade-off between training time, domain portability and performance",
    "volume": "main",
    "checked": true,
    "id": "26e743d5bd465f49b9538deaf116c15e61b7951f",
    "citation_count": 514
  },
  "https://aclanthology.org/N16-1163": {
    "title": "Retrofitting Sense-Specific Word Vectors Using Parallel Text",
    "abstract": "Jauhar et al. (2015) recently proposed to learn sense-specific word representations by \"retrofitting\" standard distributional word representations to an existing ontology. We observe that this approach does not require an ontology, and can be generalized to any graph defining word senses and relations between them. We create such a graph using translations learned from parallel corpora. On a set of lexical semantic tasks, representations learned using parallel text perform roughly as well as those derived from WordNet, and combining the two representation types significantly improves performance",
    "volume": "main",
    "checked": true,
    "id": "346acdf3f3e49150987e19264a64230962fa5cad",
    "citation_count": 23
  },
  "https://aclanthology.org/N16-1164": {
    "title": "End-to-End Argumentation Mining in Student Essays",
    "abstract": "Understanding the argumentative structure of a persuasive essay involves addressing two challenging tasks: identifying the components of the essay's argument and identifying the relations that occur between them. We examine the under-investigated task of end-toend argument mining in persuasive student essays, where we (1) present the first results on end-to-end argument mining in student essays using a pipeline approach; (2) address error propagation inherent in the pipeline approach by performing joint inference over the outputs of the tasks in an Integer Linear Programming (ILP) framework; and (3) propose a novel objective function that enables F-score to be maximized directly by an ILP solver. We evaluate our joint-inference approach with our novel objective function on a publiclyavailable corpus of 90 essays, where it yields an 18.5% relative error reduction in F-score over the pipeline system",
    "volume": "main",
    "checked": true,
    "id": "fc3f8183d8cab19903761580b3e49d2b14caf32a",
    "citation_count": 99
  },
  "https://aclanthology.org/N16-1165": {
    "title": "Cross-Domain Mining of Argumentative Text through Distant Supervision",
    "abstract": "Argumentation mining is considered as a key technology for future search engines and automated decision making. In such applications, argumentative text segments have to be mined from large and diverse document collections. However, most existing argumentation mining approaches tackle the classification of argumentativeness only for a few manually annotated documents from narrow domains and registers. This limits their practical applicability. We hence propose a distant supervision approach that acquires argumentative text segments automatically from online debate portals. Experiments across domains and registers show that training on such a corpus improves the effectiveness and robustness of mining argumentative text. We freely provide the underlying corpus for research",
    "volume": "main",
    "checked": true,
    "id": "9d640315d47dbb9c8bcc6dfd4b14c890166e74a2",
    "citation_count": 52
  },
  "https://aclanthology.org/N16-1166": {
    "title": "A Study of the Impact of Persuasive Argumentation in Political Debates",
    "abstract": "Persuasive communication is the process of shaping, reinforcing and changing others' responses. In political debates, speakers express their views towards the debated topics by choosing both the content of their discourse and the argumentation process. In this work we study the use of semantic frames for modelling argumentation in speakers' discourse. We investigate the impact of a speaker's argumentation style and their effect in influencing an audience in supporting their candidature. We model the influence index of each candidate based on their relative standings in the polls released prior to the debate and present a system which ranks speakers in terms of their relative influence using a combination of content and persuasive argumentation features. Our results show that although content alone is predictive of a speaker's influence rank, persuasive argumentation also affects such indices",
    "volume": "main",
    "checked": true,
    "id": "52b02ce306a46320d6ab5df3bab36deb3c5e1de0",
    "citation_count": 31
  },
  "https://aclanthology.org/N16-1167": {
    "title": "Lexical Coherence Graph Modeling Using Word Embeddings",
    "abstract": "Coherence is established by semantic connections between sentences of a text which can be modeled by lexical relations. In this paper, we introduce the lexical coherence graph (LCG), a new graph-based model to represent lexical relations among sentences. The frequency of subgraphs (coherence patterns) of this graph captures the connectivity style of sentence nodes in this graph. The coherence of a text is encoded by a vector of these frequencies. We evaluate the LCG model on the readability ranking task. The results of the experiments show that the LCG model obtains higher accuracy than state-of-the-art coherence models. Using larger subgraphs yields higher accuracy, because they capture more structural information. However, larger subgraphs can be sparse. We adapt Kneser-Ney smoothing to smooth subgraphs' frequencies. Smoothing improves performance",
    "volume": "main",
    "checked": true,
    "id": "7835450d0d4ccb44105283cc0b18f6a3cfdfbd8a",
    "citation_count": 21
  },
  "https://aclanthology.org/N16-1168": {
    "title": "Using Context to Predict the Purpose of Argumentative Writing Revisions",
    "abstract": "While there is increasing interest in automatically recognizing the argumentative structure of a text, recognizing the argumentative purpose of revisions to such texts has been less explored. Furthermore, existing revision classification approaches typically ignore contextual information. We propose two approaches for utilizing contextual information when predicting argumentative revision purposes: developing contextual features for use in the classification paradigm of prior work, and transforming the classification problem to a sequence labeling task. Experimental results using two corpora of student essays demonstrate the utility of contextual information for predicting argumentative revision purposes",
    "volume": "main",
    "checked": true,
    "id": "df8f000a04ca074dafa8aec0eb6925dfbd9c7847",
    "citation_count": 16
  },
  "https://aclanthology.org/N16-1169": {
    "title": "Automatic Generation and Scoring of Positive Interpretations from Negated Statements",
    "abstract": "This paper presents a methodology to extract positive interpretations from negated statements",
    "volume": "main",
    "checked": true,
    "id": "26d30177fc324fe7b7a98a37a09f11da3b553889",
    "citation_count": 9
  },
  "https://aclanthology.org/N16-1170": {
    "title": "Learning Natural Language Inference with LSTM",
    "abstract": "Natural language inference (NLI) is a fundamentally important task in natural language processing that has many applications. The recently released Stanford Natural Language Inference (SNLI) corpus has made it possible to develop and evaluate learning-centered methods such as deep neural networks for natural language inference (NLI). In this paper, we propose a special long short-term memory (LSTM) architecture for NLI. Our model builds on top of a recently proposed neural attention model for NLI but is based on a significantly different idea. Instead of deriving sentence embeddings for the premise and the hypothesis to be used for classification, our solution uses a match-LSTM to perform word-by-word matching of the hypothesis with the premise. This LSTM is able to place more emphasis on important word-level matching results. In particular, we observe that this LSTM remembers important mismatches that are critical for predicting the contradiction or the neutral relationship label. On the SNLI corpus, our model achieves an accuracy of 86.1%, outperforming the state of the art",
    "volume": "main",
    "checked": true,
    "id": "596c882de006e4bb4a93f1fa08a5dd467bee060a",
    "citation_count": 391
  },
  "https://aclanthology.org/N16-1171": {
    "title": "Activity Modeling in Email",
    "abstract": "We introduce a latent activity model for workplace emails, positing that communication at work is purposeful and organized by activities. We pose the problem as probabilistic inference in graphical models that jointly capture the interplay between latent activities and the email contexts they govern, such as the recipients, subject and body. The model parameters are learned using maximum likelihood estimation with an expectation maximization algorithm. We present three variants of the model that incorporate the recipients, co-occurrence of the recipients, and email body and subject. We demonstrate the model's effectiveness in an email recipient recommendation task and show that it outperforms a state-of-the-art generative model. Additionally, we show that the activity model can be used to identify email senders who engage in similar activities, resulting in further improvements in recipient recommendation",
    "volume": "main",
    "checked": true,
    "id": "02a86aa3bb903640cf8ea2185f29f69673797bb7",
    "citation_count": 14
  },
  "https://aclanthology.org/N16-1172": {
    "title": "Clustering Paraphrases by Word Sense",
    "abstract": "Automatically generated databases of English paraphrases have the drawback that they return a single list of paraphrases for an input word or phrase. This means that all senses of polysemous words are grouped together, unlike WordNet which partitions different senses into separate synsets. We present a new method for clustering paraphrases by word sense, and apply it to the Paraphrase Database (PPDB). We investigate the performance of hierarchical and spectral clustering algorithms, and systematically explore different ways of defining the similarity matrix that they use as input. Our method produces sense clusters that are qualitatively and quantitatively good, and that represent a substantial improvement to the PPDB resource",
    "volume": "main",
    "checked": true,
    "id": "eb50e6a723bf59c2b89295995fd130f0e6b8e8b2",
    "citation_count": 14
  },
  "https://aclanthology.org/N16-1173": {
    "title": "Unsupervised Learning of Prototypical Fillers for Implicit Semantic Role Labeling",
    "abstract": "Gold annotations for supervised implicit semantic role labeling are extremely sparse and costly. As a lightweight alternative, this paper describes an approach based on unsupervised parsing which can do without iSRL-specific training data: We induce prototypical roles from large amounts of explicit SRL annotations paired with their distributed word representations. An evaluation shows competitive performance with supervised methods on the SemEval 2010 data, and our method can easily be applied to predicates (or languages) for which no training annotations are available",
    "volume": "main",
    "checked": true,
    "id": "4b629c1b1e5eaf06002867d02f8d301749538d92",
    "citation_count": 12
  },
  "https://aclanthology.org/N16-1174": {
    "title": "Hierarchical Attention Networks for Document Classification",
    "abstract": "We propose a hierarchical attention network for document classification. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the wordand sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences",
    "volume": "main",
    "checked": true,
    "id": "455afd748e8834ef521e4b67c7c056d3c33429e2",
    "citation_count": 3851
  },
  "https://aclanthology.org/N16-1175": {
    "title": "Dependency Based Embeddings for Sentence Classification Tasks",
    "abstract": "We compare different word embeddings from a standard window based skipgram model, a skipgram model trained using dependency context features and a novel skipgram variant that utilizes additional information from dependency graphs. We explore the effectiveness of the different types of word embeddings for word similarity and sentence classification tasks. We consider three common sentence classification tasks: question type classification on the TREC dataset, binary sentiment classification on Stanford's Sentiment Treebank and semantic relation classification on the SemEval 2010 dataset. For each task we use three different classification methods: a Support Vector Machine, a Convolutional Neural Network and a Long Short Term Memory Network. Our experiments show that dependency based embeddings outperform standard window based embeddings in most of the settings, while using dependency context embeddings as additional features improves performance in all tasks regardless of the classification method. Our embeddings and code are available at https://www.cs.york.ac.uk/nlp/",
    "volume": "main",
    "checked": true,
    "id": "32ece3fe025b43d44086ebf4141e09786ceecb7e",
    "citation_count": 137
  },
  "https://aclanthology.org/N16-1176": {
    "title": "Deep LSTM based Feature Mapping for Query Classification",
    "abstract": "Traditional convolutional neural network (CNN) based query classification uses linear feature mapping in its convolution operation. The recurrent neural network (RNN), differs from a CNN in representing word sequence with their ordering information kept explicitly. We propose using a deep long-short-term-memory (DLSTM) based feature mapping to learn feature representation for CNN. The DLSTM, which is a stack of LSTM units, has different order of feature representations at different depth of LSTM unit. The bottom LSTM unit equipped with input and output gates, extracts the first order feature representation from current word. To extract higher order nonlinear feature representation, the LSTM unit at higher position gets input from two parts. First part is the lower LSTM unit's memory cell from previous word. Second part is the lower LSTM unit's hidden output from current word. In this way, the DLSTM captures the nonlinear nonconsecutive interaction within n-grams. Using an architecture that combines a stack of the DLSTM layers with a tradition CNN layer, we have observed new state-of-the-art query classification accuracy on benchmark data sets for query classification",
    "volume": "main",
    "checked": true,
    "id": "84268c79f5eadf721224adf700d1555de33b2856",
    "citation_count": 52
  },
  "https://aclanthology.org/N16-1177": {
    "title": "Dependency Sensitive Convolutional Neural Networks for Modeling Sentences and Documents",
    "abstract": "The goal of sentence and document modeling is to accurately represent the meaning of sentences and documents for various Natural Language Processing tasks. In this work, we present Dependency Sensitive Convolutional Neural Networks (DSCNN) as a generalpurpose classification system for both sentences and documents. DSCNN hierarchically builds textual representations by processing pretrained word embeddings via Long ShortTerm Memory networks and subsequently extracting features with convolution operators. Compared with existing recursive neural models with tree structures, DSCNN does not rely on parsers and expensive phrase labeling, and thus is not restricted to sentencelevel tasks. Moreover, unlike other CNNbased models that analyze sentences locally by sliding windows, our system captures both the dependency information within each sentence and relationships across sentences in the same document. Experiment results demonstrate that our approach is achieving state-ofthe-art performance on several tasks, including sentiment analysis, question type classification, and subjectivity classification",
    "volume": "main",
    "checked": true,
    "id": "ec64f650fea9b49cbf7d8bddf458388c4b362475",
    "citation_count": 117
  },
  "https://aclanthology.org/N16-1178": {
    "title": "MGNC-CNN: A Simple Approach to Exploiting Multiple Word Embeddings for Sentence Classification",
    "abstract": "We introduce a novel, simple convolution neural network (CNN) architecture - multi-group norm constraint CNN (MGNC-CNN) that capitalizes on multiple sets of word embeddings for sentence classification. MGNC-CNN extracts features from input embedding sets independently and then joins these at the penultimate layer in the network to form a final feature vector. We then adopt a group regularization strategy that differentially penalizes weights associated with the subcomponents generated from the respective embedding sets. This model is much simpler than comparable alternative architectures and requires substantially less training time. Furthermore, it is flexible in that it does not require input word embeddings to be of the same dimensionality. We show that MGNC-CNN consistently outperforms baseline models",
    "volume": "main",
    "checked": true,
    "id": "1258db72eec4bbf02e29edf5bb0c300491a01242",
    "citation_count": 98
  },
  "https://aclanthology.org/N16-1179": {
    "title": "Improving sentence compression by learning to predict gaze",
    "abstract": "We show how eye-tracking corpora can be used to improve sentence compression models, presenting a novel multi-task learning algorithm based on multi-layer LSTMs. We obtain performance competitive with or better than state-of-the-art approaches",
    "volume": "main",
    "checked": true,
    "id": "9ec751343e8343d74b7763cba511a9653a491ca4",
    "citation_count": 106
  },
  "https://aclanthology.org/N16-1180": {
    "title": "Feuding Families and Former Friends: Unsupervised Learning for Dynamic Fictional Relationships",
    "abstract": "Understanding how a fictional relationship between two characters changes over time (e.g., from best friends to sworn enemies) is a key challenge in digital humanities scholarship. We present a novel unsupervised neural network for this task that incorporates dictionary learning to generate interpretable, accurate relationship trajectories. While previous work on characterizing literary relationships relies on plot summaries annotated with predefined labels, our model jointly learns a set of global relationship descriptors as well as a trajectory over these descriptors for each relationship in a dataset of raw text from novels. We find that our model learns descriptors of events (e.g., marriage or murder) as well as interpersonal states (love, sadness). Our model outperforms topic model baselines on two crowdsourced tasks, and we also find interesting correlations to annotations in an existing dataset",
    "volume": "main",
    "checked": true,
    "id": "b3979990dc2080138021cb3d767c7ec6d3e96194",
    "citation_count": 131
  },
  "https://aclanthology.org/N16-1181": {
    "title": "Learning to Compose Neural Networks for Question Answering",
    "abstract": "We describe a question answering model that applies to both images and structured knowledge bases. The model uses natural language strings to automatically assemble neural networks from a collection of composable modules. Parameters for these modules are learned jointly with network-assembly parameters via reinforcement learning, with only (world, question, answer) triples as supervision. Our approach, which we term a dynamic neural model network, achieves state-of-the-art results on benchmark datasets in both visual and structured domains",
    "volume": "main",
    "checked": true,
    "id": "75ddc7ee15be14013a3462c01b38b0548486fbcb",
    "citation_count": 529
  },
  "https://aclanthology.org/N16-2001": {
    "title": "An End-to-end Approach to Learning Semantic Frames with Feedforward Neural Network",
    "abstract": "We present an end-to-end method for learning verb-specific semantic frames with feedforward neural network (FNN). Previous works in this area mainly adopt a multi-step procedure including part-of-speech tagging, dependency parsing and so on. On the contrary, our method uses a FNN model that maps verbspecific sentences directly to semantic frames. The simple model gets good results on annotated data and has a good generalization ability. Finally we get 0.82 F-score on 63 verbs and 0.73 F-score on 407 verbs",
    "volume": "student",
    "checked": true,
    "id": "9d04258144f2b97ef492bf762b0e46e8b4065bf9",
    "citation_count": 0
  },
  "https://aclanthology.org/N16-2002": {
    "title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't",
    "abstract": "Following up on numerous reports of analogybased identification of \"linguistic regularities\" in word embeddings, this study applies the widely used vector offset method to 4 types of linguistic relations: inflectional and derivational morphology, and lexicographic and encyclopedic semantics. We present a balanced test set with 99,200 questions in 40 categories, and we systematically examine how accuracy for different categories is affected by window size and dimensionality of the SVD-based word embeddings. We also show that GloVe and SVD yield similar patterns of results for different categories, offering further evidence for conceptual similarity between count-based and neural-net based models",
    "volume": "student",
    "checked": true,
    "id": "3f31c4e394d5f5623b95b832d182996c622c75cd",
    "citation_count": 181
  },
  "https://aclanthology.org/N16-2003": {
    "title": "Argument Identification in Chinese Editorials",
    "abstract": "In this paper, we develop and evaluate several techniques for identifying argumentative paragraphs in Chinese editorials. We first use three methods of evaluation to score a paragraph's argumentative nature: a relative word frequency approach; a method which targets known argumentative words in our corpus; and a combined approach which uses elements from the previous two. Then, we determine the best score thresholds for separating argumentative and non-argumentative paragraphs. The results of our experimentation show that our relative word frequency approach provides a reliable way to identify argumentative paragraphs with a F1 score of 0.91, though challenges in accurate scoring invite improvement through context-aware means",
    "volume": "student",
    "checked": true,
    "id": "96454392c32f7028958a94529bb0c4524bd0b47d",
    "citation_count": 5
  },
  "https://aclanthology.org/N16-2004": {
    "title": "Automatic tagging and retrieval of E-Commerce products based on visual features",
    "abstract": "This paper proposes an automatic tag assignment approach to various e-commerce products where tag allotment is done solely based on the visual features in the image. It then builds a tag based product retrieval system upon these allotted tags. The explosive growth of e-commerce products being sold online has made manual annotation infeasible. Without such tags it's impossible for customers to be able to find these products. Hence a scalable approach catering to such large number of product images and allocating meaningful tags is essential and could be used to make an efficient tag based product retrieval system. In this paper we propose one such approach based on feature extraction using Deep Convolutional Neural Networks to learn descriptive semantic features from product images. Then we use inverse distance weighted K-nearest neighbours classifiers along with several other multi-label classification approaches to assign appropriate tags to our images. We demonstrate the functioning of our algorithm for the Amazon product dataset for various categories of products like clothing and apparel, electronics, sports equipment etc",
    "volume": "student",
    "checked": true,
    "id": "0f96459447373a32b7cbf40ddcdec9ae5620a4d5",
    "citation_count": 5
  },
  "https://aclanthology.org/N16-2005": {
    "title": "Combining syntactic patterns and Wikipedia's hierarchy of hyperlinks to extract meronym relations",
    "abstract": "We present here two methods for extraction o, meronymic relation : (a) the first one relies solely on syntactic information. Unlike other approaches based on simple patterns, we determine their optimal combination to extract word pairs linked via a given semantic relation; (b) the second approach consists in combining syntactic patterns with the semantic information extracted from the Wikipedia hyperlink hierarchy (WHH) of the constituent words. By comparing our work with SemEval 2007 (Task 4 test set) and WordNet (WN) we found that our system clearly outperforms its competitors",
    "volume": "student",
    "checked": true,
    "id": "c39ab0733afcb58ceba3051b31b79d454fce9452",
    "citation_count": 3
  },
  "https://aclanthology.org/N16-2006": {
    "title": "Data-driven Paraphrasing and Stylistic Harmonization",
    "abstract": "This thesis proposal outlines the use of unsupervised data-driven methods for paraphrasing tasks. We motivate the development of knowledge-free methods at the guiding use case of multi-document summarization, which requires a domain-adaptable system for both the detection and generation of sentential paraphrases. First, we define a number of guiding research questions that will be addressed in the scope of this thesis. We continue to present ongoing work in unsupervised lexical substitution. An existing supervised approach is first adapted to a new language and dataset. We observe that supervised lexical substitution relies heavily on lexical semantic resources, and present an approach to overcome this dependency. We describe a method for unsupervised relation extraction, which we aim to leverage in lexical substitution as a replacement for knowledge-based resources",
    "volume": "student",
    "checked": true,
    "id": "813ae059a9e35c0a809a707c456c6bd0380a5d00",
    "citation_count": 1
  },
  "https://aclanthology.org/N16-2007": {
    "title": "Detecting \"Smart\" Spammers on Social Network: A Topic Model Approach",
    "abstract": "Spammer detection on social network is a challenging problem. The rigid anti-spam rules have resulted in emergence of \"smart\" spammers. They resemble legitimate users who are difficult to identify. In this paper, we present a novel spammer classification approach based on Latent Dirichlet Allocation(LDA), a topic model. Our approach extracts both the local and the global information of topic distribution patterns, which capture the essence of spamming. Tested on one benchmark dataset and one self-collected dataset, our proposed method outperforms other state-of-the-art methods in terms of averaged F1-score",
    "volume": "student",
    "checked": true,
    "id": "8f19d14aaff77fea8897bea69b5b56495dd7ada0",
    "citation_count": 31
  },
  "https://aclanthology.org/N16-2008": {
    "title": "Developing language technology tools and resources for a resource-poor language: Sindhi",
    "abstract": "Sindhi, an Indo-Aryan language with more than 75 million native speakers 1 is a resourcepoor language in terms of the availability of language technology tools and resources. In this thesis, we discuss the approaches taken to develop resources and tools for a resourcepoor language with special focus on Sindhi. The major contributions of this work include raw and annotated datasets, a POS Tagger, a Morphological Analyser, a Transliteration and a Machine Translation System",
    "volume": "student",
    "checked": true,
    "id": "c158923f83d23245e28c5e91f68f445035075144",
    "citation_count": 8
  },
  "https://aclanthology.org/N16-2009": {
    "title": "Effects of Communicative Pressures on Novice L2 Learners' Use of Optional Formal Devices",
    "abstract": "We conducted an Artificial Language Learning experiment to examine the production behavior of language learners in a dynamic communicative setting. Participants were exposed to a miniature language with two optional formal devices and were then asked to use the acquired language to transfer information in a cooperative game. The results showed that language learners optimize their use of the optional formal devices to transfer information efficiently and that they avoid the production of ambiguous information. These results could be used within the context of a language model such that the model can more accurately reflect the production behavior of human language learners",
    "volume": "student",
    "checked": true,
    "id": "83120950097c676258cfecd015fe50afc31c09a3",
    "citation_count": 1
  },
  "https://aclanthology.org/N16-2010": {
    "title": "Explicit Argument Identification for Discourse Parsing In Hindi: A Hybrid Pipeline",
    "abstract": "Shallow discourse parsing enables us to study discourse as a coherent piece of information rather than a sequence of clauses, sentences and paragraphs. In this paper, we identify arguments of explicit discourse relations in Hindi. This is the first such work carried out for Hindi. Building upon previous work carried out on discourse connective identification in Hindi, we propose a hybrid pipeline which makes use of both sub-tree extraction and linear tagging approaches. We report state-ofthe-art performance for this task",
    "volume": "student",
    "checked": true,
    "id": "618264dd73a94ec50d4a6038c0dcb161a6ecced5",
    "citation_count": 2
  },
  "https://aclanthology.org/N16-2011": {
    "title": "Exploring Fine-Grained Emotion Detection in Tweets",
    "abstract": "We examine if common machine learning techniques known to perform well in coarsegrained emotion and sentiment classification can also be applied successfully on a set of fine-grained emotion categories. We first describe the grounded theory approach used to develop a corpus of 5,553 tweets manually annotated with 28 emotion categories. From our preliminary experiments, we have identified two machine learning algorithms that perform well in this emotion classification task and demonstrated that it is feasible to train classifiers to detect 28 emotion categories without a huge drop in performance compared to coarser-grained classification schemes",
    "volume": "student",
    "checked": true,
    "id": "717fbab57c817ab6afc8132257179b3ac24c21c1",
    "citation_count": 41
  },
  "https://aclanthology.org/N16-2012": {
    "title": "Extraction of Bilingual Technical Terms for Chinese-Japanese Patent Translation",
    "abstract": "The translation of patents or scientific papers is a key issue that should be helped by the use of statistical machine translation (SMT). In this paper, we propose a method to improve Chinese–Japanese patent SMT by premarking the training corpus with aligned bilingual multi-word terms. We automatically extract multi-word terms from monolingual corpora by combining statistical and linguistic filtering methods. We use the sampling-based alignment method to identify aligned terms and set some threshold on translation probabilities to select the most promising bilingual multi-word terms. We pre-mark a Chinese– Japanese training corpus with such selected aligned bilingual multi-word terms. We obtain the performance of over 70% precision in bilingual term extraction and a significant improvement of BLEU scores in our experiments on a Chinese–Japanese patent parallel corpus",
    "volume": "student",
    "checked": true,
    "id": "4f5887a3b90a62b985c1c92cb5641d01d3b21a70",
    "citation_count": 6
  },
  "https://aclanthology.org/N16-2013": {
    "title": "Hateful Symbols or Hateful People? Predictive Features for Hate Speech Detection on Twitter",
    "abstract": "Hate speech in the form of racist and sexist remarks are a common occurrence on social media. For that reason, many social media services address the problem of identifying hate speech, but the definition of hate speech varies markedly and is largely a manual effort (BBC, 2015; Lomas, 2015). We provide a list of criteria founded in critical race theory, and use them to annotate a publicly available corpus of more than 16k tweets. We analyze the impact of various extra-linguistic features in conjunction with character n-grams for hatespeech detection. We also present a dictionary based the most indicative words in our data",
    "volume": "student",
    "checked": true,
    "id": "df704cca917666dace4e42b4d3a50f65597b8f06",
    "citation_count": 1188
  },
  "https://aclanthology.org/N16-2014": {
    "title": "Non-decreasing Sub-modular Function for Comprehensible Summarization",
    "abstract": "Extractive summarization techniques typically aim to maximize the information coverage of the summary with respect to the original corpus and report accuracies in ROUGE scores. Automated text summarization techniques should consider the dimensions of comprehensibility, coherence and readability. In the current work, we identify the discourse structure which provides the context for the creation of a sentence. We leverage the information from the structure to frame a monotone (non-decreasing) sub-modular scoring function for generating comprehensible summaries. Our approach improves the overall quality of comprehensibility of the summary in terms of human evaluation and gives sufficient content coverage with comparable ROUGE score. We also formulate a metric to measure summary comprehensibility in terms of Contextual Independence of a sentence. The metric is shown to be representative of human judgement of text comprehensibility",
    "volume": "student",
    "checked": true,
    "id": "d1f567bbc0f0c456ec64c822d7bec0c38b21d490",
    "citation_count": 3
  },
  "https://aclanthology.org/N16-2015": {
    "title": "Phylogenetic simulations over constraint-based grammar formalisms",
    "abstract": "Computational phylogenetics has been shown to be effective over grammatical characteristics. Recent work suggests that constraintbased formalisms are compatible with such an approach (Eden, 2013). In this paper, we report on simulations to determine how useful constraint-based formalisms are in phylogenetic research and under what conditions",
    "volume": "student",
    "checked": true,
    "id": "34ed201f2da24dcfc06b65f7ffa58d61f2297c4d",
    "citation_count": 1
  },
  "https://aclanthology.org/N16-2016": {
    "title": "Question Answering over Knowledge Base using Factual Memory Networks",
    "abstract": "In the task of question answering, Memory Networks have recently shown to be quite effective towards complex reasoning as well as scalability, in spite of limited range of topics covered in training data. In this paper, we introduce Factual Memory Network, which learns to answer questions by extracting and reasoning over relevant facts from a Knowledge Base. Our system generate distributed representation of questions and KB in same word vector space, extract a subset of initial candidate facts, then try to find a path to answer entity using multi-hop reasoning and refinement. Additionally, we also improve the run-time efficiency of our model using various computational heuristics",
    "volume": "student",
    "checked": true,
    "id": "dc4cbf25503ece9707cd0fa9c6fe992a74495f57",
    "citation_count": 64
  },
  "https://aclanthology.org/N16-2017": {
    "title": "Using Related Languages to Enhance Statistical Language Models",
    "abstract": "The success of many language modeling methods and applications relies heavily on the amount of data available. This problem is further exacerbated in statistical machine translation, where parallel data in the source and target languages is required. However, large amounts of data are only available for a small number of languages; as a result, many language modeling techniques are inadequate for the vast majority of languages. In this paper, we attempt to lessen the problem of a lack of training data for low-resource languages by adding data from related high-resource languages in three experiments. First, we interpolate language models trained on the target language and on the related language. In our second experiment, we select the sentences most similar to the target language and add them to our training corpus. Finally, we integrate data from the related language into a translation model for a statistical machine translation application. Although we do not see many significant improvements over baselines trained on a small amount of data in the target language, we discuss some further experiments that could be attempted in order to augment language models and translation models with data from related languages",
    "volume": "student",
    "checked": true,
    "id": "bac5fafcd36990bfbb8f01e2c2365075ca85273e",
    "citation_count": 5
  },
  "https://aclanthology.org/N16-3001": {
    "title": "rstWeb - A Browser-based Annotation Interface for Rhetorical Structure Theory and Discourse Relations",
    "abstract": "This paper presents rstWeb, a new browserbased interface for Rhetorical Structure Theory and other discourse relation annotations. Expanding on previous tools for RST, rstWeb allows annotators to work online using only a browser. Project administrators can easily collect multiple annotations of the same documents on a central server, keep track of annotation processes and assign tasks and annotation schemes to users. A local version using an embedded web framework is also available, running offline on a desktop browser under the localhost",
    "volume": "demo",
    "checked": true,
    "id": "421e362a39ba9427a8c0cbc3352719a20cced5ff",
    "citation_count": 39
  },
  "https://aclanthology.org/N16-3002": {
    "title": "Instant Feedback for Increasing the Presence of Solutions in Peer Reviews",
    "abstract": "We present the design and evaluation of a web-based peer review system that uses natural language processing to automatically evaluate and provide instant feedback regarding the presence of solutions in peer reviews. Student reviewers can then choose to either revise their reviews to address the system's feedback, or ignore the feedback and submit their original reviews. A system deployment in multiple high school classrooms shows that our solution prediction model triggers instant feedback with high precision, and that the feedback is successful in increasing the number of peer reviews with solutions",
    "volume": "demo",
    "checked": true,
    "id": "6845d2b29b5ed16b1de5432d66729e28671e782f",
    "citation_count": 17
  },
  "https://aclanthology.org/N16-3003": {
    "title": "Farasa: A Fast and Furious Segmenter for Arabic",
    "abstract": "In this paper, we present Farasa, a fast and accurate Arabic segmenter. Our approach is based on SVM-rank using linear kernels. We measure the performance of the segmenter in terms of accuracy and efficiency, in two NLP tasks, namely Machine Translation (MT) and Information Retrieval (IR). Farasa outperforms or is at par with the stateof-the-art Arabic segmenters (Stanford and MADAMIRA), while being more than one order of magnitude faster",
    "volume": "demo",
    "checked": true,
    "id": "a1aaa7c75464e7ebe41cfe5c5258241ca34c6414",
    "citation_count": 283
  },
  "https://aclanthology.org/N16-3004": {
    "title": "iAppraise: A Manual Machine Translation Evaluation Environment Supporting Eye-tracking",
    "abstract": "We present iAppraise: an open-source framework that enables the use of eye-tracking for MT evaluation. It connects Appraise, an opensource toolkit for MT evaluation, to a low-cost eye-tracking device, to make its usage accessible to a broader audience. It also provides a set of tools for extracting and exploiting gaze data, which facilitate eye-tracking analysis. In this paper, we describe different modules of the framework, and explain how the tool can be used in a MT evaluation scenario. During the demonstration, the users will be able to perform an evaluation task, observe their own reading behavior during a replay of the session, and export and extract features from the data",
    "volume": "demo",
    "checked": true,
    "id": "0d96446d9ffacef9a6abf81df1c27f85282b141f",
    "citation_count": 4
  },
  "https://aclanthology.org/N16-3005": {
    "title": "Linguistica 5: Unsupervised Learning of Linguistic Structure",
    "abstract": "This paper introduces Linguistica 5, a software for unsupervised learning of linguistic structure. It is a descendant of Goldsmith's (2001, 2006) Linguistica. Open-source and written in Python, the new Linguistica 5 is both a graphical user interface software and a Python library. WhileLinguistica 5inherits its predecessors' strength in unsupervised learning of natural language morphology, it incorporates significant improvements in multiple ways. Notable new features include tools for data visualization as well as straightforward extensions for both its components and embedding in other programs",
    "volume": "demo",
    "checked": true,
    "id": "519328d47e52a78b30b264660336383161c75793",
    "citation_count": 11
  },
  "https://aclanthology.org/N16-3006": {
    "title": "TransRead: Designing a Bilingual Reading Experience with Machine Translation Technologies",
    "abstract": "In this paper, we use multilingual Natural Language Processing (NLP) tools to improve the reading experience of parallel texts on mobile devices. Such enterprise poses multiple challenging issues both from the NLP and from the Human Computer Interaction (HCI) perspectives. We discuss these problems, and report on our own solutions, now implemented in a full-fledged bilingual reading device",
    "volume": "demo",
    "checked": true,
    "id": "e70cf059065476859286c83295a283fe545d769b",
    "citation_count": 3
  },
  "https://aclanthology.org/N16-3007": {
    "title": "New Dimensions in Testimony Demonstration",
    "abstract": "New Dimensions in Testimony is a prototype dialogue system that allows users to conduct a conversation with a real person who is not available for conversation in real time. Users talk to a persistent representation of Holocaust survivor Pinchas Gutter on a screen, while a dialogue agent selects appropriate responses to user utterances from a set of pre-recorded video statements, simulating a live conversation. The technology is similar to existing conversational agents, but to our knowledge this is the first system to portray a real person. The demonstration will show the system on a range of screens (from mobile phones to large TVs), and allow users to have individual conversations with Mr. Gutter",
    "volume": "demo",
    "checked": true,
    "id": "a1f0c83ddeec91140b36fd4827e26b2941a47e1d",
    "citation_count": 10
  },
  "https://aclanthology.org/N16-3008": {
    "title": "ArgRewrite: A Web-based Revision Assistant for Argumentative Writings",
    "abstract": "While intelligent writing assistants have become more common, they typically have little support for revision behavior. We present ArgRewrite, a novel web-based revision assistant that focus on rewriting analysis. The system supports two major functionalities: 1) to assist students as they revise, the system automatically extracts and analyzes revisions; 2) to assist teachers, the system provides an overview of students' revisions and allows teachers to correct the automatically analyzed results, ensuring that students get the correct feedback",
    "volume": "demo",
    "checked": true,
    "id": "9d052381535eb37d341715e2bc35139336037962",
    "citation_count": 21
  },
  "https://aclanthology.org/N16-3009": {
    "title": "Scaling Up Word Clustering",
    "abstract": "Word clusters improve performance in many NLP tasks including training neural network language models, but current increases in datasets are outpacing the ability of word clusterers to handle them. In this paper we present a novel bidirectional, interpolated, refining, and alternating (BIRA) predictive exchange algorithm and introduce ClusterCat, a clusterer based on this algorithm. We show that ClusterCat is 3‐85 times faster than four other well-known clusterers, while also improving upon the predictive exchange algorithm's perplexity by up to 18% . Notably, ClusterCat clusters a 2.5 billion token English News Crawl corpus in 3 hours. We also evaluate in a machine translation setting, resulting in shorter training times achieving the same translation quality measured in BLEU scores. ClusterCat is portable and freely available",
    "volume": "demo",
    "checked": true,
    "id": "d7ae2d108666f09614095f847de74ddb4da941c1",
    "citation_count": 2
  },
  "https://aclanthology.org/N16-3010": {
    "title": "Task Completion Platform: A self-serve multi-domain goal oriented dialogue platform",
    "abstract": "We demonstrate the Task Completion Platform (TCP); a multi-domain dialogue platform that can host and execute large numbers of goal-orientated dialogue tasks. The platform features a task configuration language, TaskForm, that allows the definition of each individual task to be decoupled from the overarching dialogue policy used by the platform to complete those tasks. This separation allows for simple and rapid authoring of new tasks, while dialogue policy and platform functionality evolve independent of the tasks. The current platform includes machine learnt models that provide contextual slot carry-over, flexible item selection, and task selection/switching. Any new task immediately gains the benefit of these pieces of built-in platform functionality. The platform is used to power many of the multi-turn dialogues supported by the Cortana personal assistant",
    "volume": "demo",
    "checked": true,
    "id": "60ae26f50d76126b1d680e9c27763771512b2079",
    "citation_count": 22
  },
  "https://aclanthology.org/N16-3011": {
    "title": "Illinois Math Solver: Math Reasoning on the Web",
    "abstract": "There has been a recent interest in understanding text to perform mathematical reasoning. In particular, most of these efforts have focussed on automatically solving school level math word problems. In order to make advancements in this area accessible to people, as well as to facilitate this line of research, we release the ILLINOIS MATH SOLVER, a web based tool that supports performing mathematical reasoning. ILLINOIS MATH SOLVER can answer a wide range of mathematics questions, ranging from compositional operation questions like \"What is the result when 6 is divided by the sum of 7 and 5 ?\" to elementary school level math word problems, like \"I bought 6 apples. I ate 3 of them. How many do I have left ?\". The web based demo can be used as a tutoring tool for elementary school students, since it not only outputs the final result, but also the mathematical expression to compute it. The tool will allow researchers to understand the capabilities and limitations of a state of the art arithmetic problem solver, and also enable crowd based data acquisition for mathematical reasoning. The system is currently online at https://cogcomp.cs.illinois. edu/page/demo_view/Math",
    "volume": "demo",
    "checked": true,
    "id": "038a496d70f8edc2a3bf6c8e112a9dd4f9a3ea22",
    "citation_count": 11
  },
  "https://aclanthology.org/N16-3012": {
    "title": "LingoTurk: managing crowdsourced tasks for psycholinguistics",
    "abstract": "LingoTurk is an open-source, freely available crowdsourcing client/server system aimed primarily at psycholinguistic experimentation where custom and specialized user interfaces are required but not supported by popular crowdsourcing task management platforms. LingoTurk enables user-friendly local hosting of experiments as well as condition management and participant exclusion. It is compatible with Amazon Mechanical Turk and Prolific Academic. New experiments can easily be set up via the Play Framework and the LingoTurk API, while multiple experiments can be managed from a single system",
    "volume": "demo",
    "checked": true,
    "id": "79a392a4923a82c03287879c4a763548fbc7eb13",
    "citation_count": 29
  },
  "https://aclanthology.org/N16-3013": {
    "title": "Sentential Paraphrasing as Black-Box Machine Translation",
    "abstract": "We present a simple, prepackaged solution to generating paraphrases of English sentences. We use the Paraphrase Database (PPDB) for monolingual sentence rewriting and provide machine translation language packs: prepackaged, tuned models that can be downloaded and used to generate paraphrases on a standard Unix environment. The language packs can be treated as a black box or customized to specific tasks. In this demonstration, we will explain how to use the included interactive webbased tool to generate sentential paraphrases",
    "volume": "demo",
    "checked": true,
    "id": "58607ab9c515bdfcbb9c47eddcf2eb60282a2a0f",
    "citation_count": 15
  },
  "https://aclanthology.org/N16-3014": {
    "title": "A Tag-based English Math Word Problem Solver with Understanding, Reasoning and Explanation",
    "abstract": "This paper presents a meaning-based statistical math word problem (MWP) solver with understanding, reasoning and explanation. It comprises a web user interface and pipelined modules for analysing the text, transforming both body and question parts into their logic forms, and then performing inference on them. The associated context of each quantity is represented with proposed role-tags (e.g., nsubj, verb, etc.), which provides the flexibility for annotating the extracted math quantity with its associated syntactic and semantic information (which specifies the physical meaning of that quantity). Those role-tags are then used to identify the desired operands and filter out irrelevant quantities (so that the answer can be obtained precisely). Since the physical meaning of each quantity is explicitly represented with those role-tags and used in the inference process, the proposed approach could explain how the answer is obtained in a human comprehensible way",
    "volume": "demo",
    "checked": true,
    "id": "5a8c3529db32639feee9ee1016fb31529b9173fb",
    "citation_count": 15
  },
  "https://aclanthology.org/N16-3015": {
    "title": "Cross-media Event Extraction and Recommendation",
    "abstract": "The sheer volume of unstructured multimedia data (e.g., texts, images, videos) posted on the Web during events of general interest is overwhelming and difficult to distill if seeking information relevant to a particular concern. We have developed a comprehensive system that searches, identifies, organizes and summarizes complex events from multiple data modalities. It also recommends events related to the user's ongoing search based on previously selected attribute values and dimensions of events being viewed. In this paper we briefly present the algorithms of each component and demonstrate the system's capabilities",
    "volume": "demo",
    "checked": true,
    "id": "1be855c7f5bb8077f4bb80248c968d9d76a0a007",
    "citation_count": 13
  },
  "https://aclanthology.org/N16-3016": {
    "title": "SODA:Service Oriented Domain Adaptation Architecture for Microblog Categorization",
    "abstract": "We demonstrate SODA (Service Oriented Domain Adaptation) for efficient and scalable cross-domain microblog categorization which works on the principle of transfer learning. It is developed on a novel similarity-based iterative domain adaptation algorithm while extended with features such as active learning and interactive GUI to be used by business professionals. SODA demonstrates efficient classification accuracy on new collections while minimizing and sometimes eliminating the need for expensive data labeling efforts. SODA also implements an active learning (AL) technique to select informative instances from the new collection to seek annotations, if a small amount of labeled data is required by the adaptation algorithm",
    "volume": "demo",
    "checked": true,
    "id": "b1dfc66d270a15cc4d9bfb9ca426203f232c21fe",
    "citation_count": 0
  },
  "https://aclanthology.org/N16-3017": {
    "title": "Lecture Translator - Speech translation framework for simultaneous lecture translation",
    "abstract": "Foreign students at German universities often have difficulties following lectures as they are often held in German. Since human interpreters are too expensive for universities we are addressing this problem via speech translation technology deployed in KIT's lecture halls. Our simultaneous lecture translation system automatically translates lectures from German to English in real-time. Other supported language directions are English to Spanish, English to French, English to German and German to French. Automatic simultaneous translation is more than just the concatenation of automatic speech recognition and machine translation technology, as the input is an unsegmented, practically infinite stream of spontaneous speech. The lack of segmentation and the spontaneous nature of the speech makes it especially difficult to recognize and translate it with sufficient quality. In addition to quality, speed and latency are of the utmost importance in order for the system to enable students to follow lectures. In this paper we present our system that performs the task of simultaneous speech translation of university lectures by performing speech translation on a stream of audio in real-time and with low latency. The system features several techniques beyond the basic speech translation task, that make it fit for real-world use. Examples of these features are a continuous stream speech recognition without any prior segmentation of the input audio, punctuation prediction, run-on decoding and run-on translation with continuously updating displays in order to keep the latency as low as possible",
    "volume": "demo",
    "checked": true,
    "id": "5914a2c8f655c322fee59e595db43ed60c9014be",
    "citation_count": 29
  },
  "https://aclanthology.org/N16-3018": {
    "title": "Zara The Supergirl: An Empathetic Personality Recognition System",
    "abstract": "Zara the Supergirl is an interactive system that, while having a conversation with a user, uses its built in sentiment analysis, emotion recognition, facial and speech recognition modules, to exhibit the human-like response of sharing emotions. In addition, at the end of a 5-10 minute conversation with the user, it can give a comprehensive personality analysis based on the user's interaction with Zara. This is a first prototype that has incorporated a full empathy module, the recognition and response of human emotions, into a spoken language interactive system that enhances human-robot understanding. Zara was shown at the World Economic Forum in Dalian in September 2015",
    "volume": "demo",
    "checked": true,
    "id": "f4136bd7f3948be30c4c11876a1bf933e3cc8549",
    "citation_count": 29
  },
  "https://aclanthology.org/N16-3019": {
    "title": "Kathaa: A Visual Programming Framework for NLP Applications",
    "abstract": "In this paper, we present Kathaa1, an open source web based Visual Programming Framework for NLP applications. It supports design, execution and analysis of complex NLP systems by choosing and visually connecting NLP modules from an already available and easily extensible Module library. It models NLP systems as a Directed Acyclic Graph of optionally parallalized information flow, and lets the user choose and use available modules in their NLP applications irrespective of their technical proficiency. Kathaa exposes a precise Module definition API to allow easy integration of external NLP components (along with their associated services as docker containers), it allows everyone to publish their services in a standardized format for everyone else to use it out of the box",
    "volume": "demo",
    "checked": true,
    "id": "b39c01e50a7b24928d0da35be4162d338eda1658",
    "citation_count": 6
  },
  "https://aclanthology.org/N16-3020": {
    "title": "Why Should I Trust You?\": Explaining the Predictions of Any Classifier",
    "abstract": "Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted",
    "volume": "demo",
    "checked": true,
    "id": "5091316bb1c6db6c6a813f4391911a5c311fdfe0",
    "citation_count": 10464
  },
  "https://aclanthology.org/N16-4001": {
    "title": "English Resource Semantics",
    "abstract": "Recent years have seen a dramatic increase in interest in semantically­informed natural language processing, including parsing into semantic representations, grounded language processing that connects linguistic structures to world representations, proposals to integrate compositional and distributional approaches to semantics, and approaches to semantically­sensitive tasks including sentiment analysis, summarization, generation, machine translation, and information extraction which take into account linguistic structure beyond n­grams. The semantic inputs to this work include a wide range of representations, from word embeddings, to syntactic dependencies used as a proxy for semantic dependencies, to sentence­level semantic representations either partial (e.g. semantic role labels) or fully articulated",
    "volume": "tutorial",
    "checked": true,
    "id": "0f95557becc063a92f4fe36affb6ba942be6801f",
    "citation_count": 4
  },
  "https://aclanthology.org/N16-4002": {
    "title": "Multilingual Multimodal Language Processing Using Neural Networks",
    "abstract": "We live in an increasingly multilingual multimodal world where it is common to find multiple views of the same entity across modalities and languages. For example, news articles which get published in multiple languages are essentially different views of the same entity. Similarly, video, audio and multilingual subtitles are multiple views of the same movie clip. Given the proliferation of such multilingual multimodal content it is no longer sufficient to process a single modality or language at a time. Specifically, there is an increasing demand for allowing transfer, conversion and access across such multiple views of the data. For example, users want to translate/convert news articles to their native language, automatically caption their travel photos and even ask natural language questions over videos and images. This has led to a lot of excitement around this interdisciplinary research area which requires ideas from Machine Learning, Natural Language Processing, Speech and Computer Vision among other fields. In this tutorial we focus on neural network based models for addressing various problems in this space. We will first introduce the participants to some of the basic concepts and building blocks that such approaches rely on. We will then describe some of these approaches in detail. There are two important parts to the tutorial. In the first part, we will talk about approaches which aim to learn a common representation for entities across languages and modalities thereby enabling cross lingual and cross modal access and transfer. In the second part we will talk about multilingual multimodal generation. For example, we will discuss neural network based approaches which aim at (i) generating translations in multiple languages, (ii) generating images given a natural language description and (iii) generating captions in multiple languages",
    "volume": "tutorial",
    "checked": true,
    "id": "60f3bb4173b57c6cd38a2f17f4b7c7176cb560f6",
    "citation_count": 0
  },
  "https://aclanthology.org/N16-4003": {
    "title": "Question Answering with Knowledge Base, Web and Beyond",
    "abstract": "In this tutorial, we give the audience a coherent overview of the research of question answering (QA). We first introduce a variety of QA problems proposed by pioneer researchers and briefly describe the early efforts. By contrasting with the current research trend in this domain, the audience can easily comprehend what technical problems remain challenging and what the main breakthroughs and opportunities are during the past half century. For the rest of the tutorial, we select three categories of the QA problems that have recently attracted a great deal of attention in the research community, and present the tasks with the latest technical survey. We conclude the tutorial by discussing the new opportunities and future directions of QA research",
    "volume": "tutorial",
    "checked": true,
    "id": "73af3eac9add4d4aaec4d739b8c54e470948de93",
    "citation_count": 15
  },
  "https://aclanthology.org/N16-4004": {
    "title": "Recent Progress in Deep Learning for NLP",
    "abstract": "Neural network­based methods have been viewed as one of the major driving force in the recent development of natural language processing (NLP). We all have witnessed with great excitement how this subfield advances: new ideas emerge at an unprecedented speed and old ideas resurge in unexpected ways. In a nutshell, there are two major trends: ● Ideas and techniques from other fields of machine learning and artificial intelligence (A.I.) have increasing impact on neural network­based NLP methods. ● With end­to­end models taking on more complex tasks, the design of architecture and mechanisms often needs more domain knowledge from linguists and other domain experts. Both trends are important to researchers in the computational linguistics community. Fundamental ideas like external memory or reinforcement learning, although introduced to NLP only recently, have quickly lead to significant improvement on tasks like natural language generation and question answering. On the other hand, with complicated neural systems with many cooperating components, it calls for linguistic knowledge in designing the right mechanism, architecture, and sometimes training setting. As a simple example, the introducing of automatic alignment in neural machine translation, has quickly led to the state­of­the­art performance in machine translation and triggered a large body of sequence­to­sequence models. It is therefore important to get the researchers in computational linguistics community acquainted with the recent progress in deep learning for NLP. We will focus on the work and ideas strongly related to the core of natural language and yet not so familiar to the majority of the community, which can be roughly categorized into: 1) the differentiable data­structures, and 2) the learning paradigms for NLP. Differentiable data­structures, starting with the memory equipped with continuous operations in Neural Turing Machine, have been the foundation of deep models with sophisticated operations. Some members of it, such as Memory Network, have become famous on tasks like question answering and machine translation, while other development in this direction, including those with clear and important application in NLP, are relatively new to this community. Deep learning, with its promise on end­to­end learning, not only enables the training of complex NLP models from scratch, but also extends the training setting to include remote and indirect supervision. We will introduce not only the end­to­end learning in its general notion, but also newly emerged",
    "volume": "tutorial",
    "checked": true,
    "id": "44cd48cbcc44373329e173aebf4807b5254f0621",
    "citation_count": 5
  },
  "https://aclanthology.org/N16-4005": {
    "title": "Scalable Statistical Relational Learning for NLP",
    "abstract": "Statistical Relational Learning (SRL) is an interdisciplinary research area that combines first­order logic and machine learning methods for probabilistic inference. Although many Natural Language Processing (NLP) tasks (including text classification, semantic parsing, information extraction, coreference resolution, and sentiment analysis) can be formulated as inference in a first­order logic, most probabilistic first­order logics are not efficient enough to be used for large­scale versions of these tasks. In this tutorial, we provide a gentle introduction to the theoretical foundation of probabilistic logics, as well as their applications in NLP. We describe recent advances in designing scalable probabilistic logics, with a special focus on ProPPR. Finally, we provide a hands­on demo about scalable probabilistic logic programming for solving practical NLP problems",
    "volume": "tutorial",
    "checked": true,
    "id": "143183584a8ebaad93490f4550295a9cb6cf9817",
    "citation_count": 0
  },
  "https://aclanthology.org/N16-4006": {
    "title": "Statistical Machine Translation between Related Languages",
    "abstract": "Language­independent Statistical Machine Translation (SMT) has proven to be very challenging. The diversity of languages makes high accuracy difficult and requires substantial parallel corpus as well as linguistic resources (parsers, morph analyzers, etc.). An interesting observation is that a large chunk of machine translation (MT) requirements involve related languages. They are either : (i) between related languages, or (ii) between a lingua franca (like English) and a set of related languages. For instance, India, the European Union and South­East Asia have such translation requirements due to government, business and socio­cultural communication needs. Related languages share a lot of linguistic features and the divergences among them are at a lower level of the NLP pipeline. The objective of the tutorial is to discuss how the relatedness among languages can be leveraged to bridge this language divergence thereby achieving some/all of these goals: (i) improving translation quality, (ii) achieving better generalization, (iii) sharing linguistic resources, and (iv) reducing resource requirements. We will look at the existing research in SMT from the perspective of related languages, with the goal to build a toolbox of methods that are useful for translation between related languages. This tutorial would be relevant to Machine Translation researchers and developers, especially those interested in translation between low­resource languages which have resource­rich related languages. It will also be relevant for researchers interested in multilingual computation. We start with a motivation for looking at the SMT problem from the perspective of related languages. We introduce notions of language relatedness useful for MT. We explore how lexical, morphological and syntactic similarity among related languages can help MT. Lexical similarity will receive special attention since related languages share a significant vocabulary in terms of cognates, loanwords, etc. Then, we look beyond bilingual MT and present how pivot­based and multi­source methods incorporate knowledge from multiple languages, and handle language pairs lacking parallel corpora. We present some studies concerning the implications of languages relatedness to pivot­based SMT, and ways of handling language divergence in the pivot­based SMT scenario. Recent advances in deep learning have made it possible to train multi­language neural MT systems, which we think would be relevant to training between related languages",
    "volume": "tutorial",
    "checked": true,
    "id": "95d371d711c8b7f68c687a79d62afaf6aaa8fd40",
    "citation_count": 13
  }
}