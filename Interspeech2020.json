{
  "https://www.isca-speech.org/archive/interspeech_2020/pierrehumbert20_interspeech.html": {
    "title": "The cognitive status of simple and complex models",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "68b1551cc73dc27a1f836e064a7f7b08916e163a",
    "semantic_title": "the cognitive status of simple and complex models",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20_interspeech.html": {
    "title": "On the Comparison of Popular End-to-End Models for Large Scale Speech Recognition",
    "volume": "main",
    "abstract": "Recently, there has been a strong push to transition from hybrid models to end-to-end (E2E) models for automatic speech recognition. Currently, there are three promising E2E methods: recurrent neural network transducer (RNN-T), RNN attention-based encoder-decoder (AED), and Transformer-AED. In this study, we conduct an empirical comparison of RNN-T, RNN-AED, and Transformer-AED models, in both non-streaming and streaming modes. We use 65 thousand hours of Microsoft anonymized training data to train these models. As E2E models are more data hungry, it is better to compare their effectiveness with large amount of training data. To the best of our knowledge, no such comprehensive study has been conducted yet. We show that although AED models are stronger than RNN-T in the non-streaming mode, RNN-T is very competitive in streaming mode if its encoder can be properly initialized. Among all three E2E models, transformer-AED achieved the best accuracy in both streaming and non-streaming mode. We show that both streaming RNN-T and transformer-AED models can obtain better accuracy than a highly-optimized hybrid model",
    "checked": true,
    "id": "be934c378c897e5bc3b3767376a59a4679093286",
    "semantic_title": "on the comparison of popular end-to-end models for large scale speech recognition",
    "citation_count": 109
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gao20_interspeech.html": {
    "title": "SAN-M: Memory Equipped Self-Attention for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "End-to-end speech recognition has become popular in recent years, since it can integrate the acoustic, pronunciation and language models into a single neural network. Among end-to-end approaches, attention-based methods have emerged as being superior. For example, Transformer, which adopts an encoder-decoder architecture. The key improvement introduced by Transformer is the utilization of self-attention instead of recurrent mechanisms, enabling both encoder and decoder to capture long-range dependencies with lower computational complexity. In this work, we propose boosting the self-attention ability with a DFSMN memory block, forming the proposed memory equipped self-attention (SAN-M) mechanism. Theoretical and empirical comparisons have been made to demonstrate the relevancy and complementarity between self-attention and the DFSMN memory block. Furthermore, the proposed SAN-M provides an efficient mechanism to integrate these two modules. We have evaluated our approach on the public AISHELL-1 benchmark and an industrial-level 20,000-hour Mandarin speech recognition task. On both tasks, SAN-M systems achieved much better performance than the self-attention based Transformer baseline system. Specially, it can achieve a CER of 6.46% on the AISHELL-1 task even without using any external LM, comfortably outperforming other state-of-the-art systems",
    "checked": true,
    "id": "96da36f81c065a154f5423c0dfbb1b1de69c2574",
    "semantic_title": "san-m: memory equipped self-attention for end-to-end speech recognition",
    "citation_count": 23
  },
  "https://www.isca-speech.org/archive/interspeech_2020/jain20_interspeech.html": {
    "title": "Contextual RNN-T for Open Domain ASR",
    "volume": "main",
    "abstract": "End-to-end (E2E) systems for automatic speech recognition (ASR), such as RNN Transducer (RNN-T) and Listen-Attend-Spell (LAS) blend the individual components of a traditional hybrid ASR system — acoustic model, language model, pronunciation model — into a single neural network. While this has some nice advantages, it limits the system to be trained using only paired audio and text. Because of this, E2E models tend to have difficulties with correctly recognizing rare words that are not frequently seen during training, such as entity names. In this paper, we propose modifications to the RNN-T model that allow the model to utilize additional metadata text with the objective of improving performance on these named entity words. We evaluate our approach on an in-house dataset sampled from de-identified public social media videos, which represent an open domain ASR task. By using an attention model to leverage the contextual metadata that accompanies a video, we observe a relative improvement of about 16% in Word Error Rate on Named Entities (WER-NE) for videos with related metadata",
    "checked": true,
    "id": "c728dcae48bbca45b8dfed60f245c005e5de4b48",
    "semantic_title": "contextual rnn-t for open domain asr",
    "citation_count": 56
  },
  "https://www.isca-speech.org/archive/interspeech_2020/pan20_interspeech.html": {
    "title": "ASAPP-ASR: Multistream CNN and Self-Attentive SRU for SOTA Speech Recognition",
    "volume": "main",
    "abstract": "In this paper we present state-of-the-art (SOTA) performance on the LibriSpeech corpus with two novel neural network architectures, a multistream CNN for acoustic modeling and a self-attentive simple recurrent unit (SRU) for language modeling. In the hybrid ASR framework, the multistream CNN acoustic model processes an input of speech frames in multiple parallel pipelines where each stream has a unique dilation rate for diversity. Trained with the SpecAugment data augmentation method, it achieves relative word error rate (WER) improvements of 4% on test-clean and 14% on test-other. We further improve the performance via N-best rescoring using a 24-layer self-attentive SRU language model, achieving WERs of 1.75% on test-clean and 4.46% on test-other",
    "checked": true,
    "id": "04d4543a2d38895bbaee0475fc6bc6c97ec6b696",
    "semantic_title": "asapp-asr: multistream cnn and self-attentive sru for sota speech recognition",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kadetotad20_interspeech.html": {
    "title": "Compressing LSTM Networks with Hierarchical Coarse-Grain Sparsity",
    "volume": "main",
    "abstract": "The long short-term memory (LSTM) network is one of the most widely used recurrent neural networks (RNNs) for automatic speech recognition (ASR), but is parametrized by millions of parameters. This makes it prohibitive for memory-constrained hardware accelerators as the storage demand causes higher dependence on off-chip memory, which bottlenecks latency and power. In this paper, we propose a new LSTM training technique based on hierarchical coarse-grain sparsity (HCGS), which enforces hierarchical structured sparsity by randomly dropping static block-wise connections between layers. HCGS maintains the same hierarchical structured sparsity throughout training and inference; this reduces weight storage for both training and inference hardware systems. We also jointly optimize in-training quantization with HCGS on 2-/3-layer LSTM networks for the TIMIT and TED-LIUM corpora. With 16× structured compression and 6-bit weight precision, we achieved a phoneme error rate (PER) of 16.9% for TIMIT and a word error rate (WER) of 18.9% for TED-LIUM, showing the best trade-off between error rate and LSTM memory compression compared to prior works",
    "checked": true,
    "id": "e9dc99949241a2ef2a161ecaaf20cb3e36b588e1",
    "semantic_title": "compressing lstm networks with hierarchical coarse-grain sparsity",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lohrenz20_interspeech.html": {
    "title": "BLSTM-Driven Stream Fusion for Automatic Speech Recognition: Novel Methods and a Multi-Size Window Fusion Example",
    "volume": "main",
    "abstract": "Optimal fusion of streams for ASR is a nontrivial problem. Recently, so-called posterior-in-posterior-out (PIPO-)BLSTMs have been proposed that serve as state sequence enhancers and have highly attractive training properties. In this work, we adopt the PIPO-BLSTMs and employ them in the context of stream fusion for ASR. Our contributions are the following: First, we show the positive effect of a PIPO-BLSTM as state sequence enhancer for various stream fusion approaches. Second, we confirm the advantageous context-free (CF) training property of the PIPO-BLSTM for all investigated fusion approaches. Third, we show with a fusion example of two streams, stemming from different short-time Fourier transform window lengths, that all investigated fusion approaches take profit. Finally, the turbo fusion approach turns out to be best, employing a CF-type PIPO-BLSTM with a novel iterative augmentation in training",
    "checked": true,
    "id": "4d634e9dff89704d68fc0f49e06c94db4ab831d2",
    "semantic_title": "blstm-driven stream fusion for automatic speech recognition: novel methods and a multi-size window fusion example",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2020/pham20_interspeech.html": {
    "title": "Relative Positional Encoding for Speech Recognition and Direct Translation",
    "volume": "main",
    "abstract": "Transformer models are powerful sequence-to-sequence architectures that are capable of directly mapping speech inputs to transcriptions or translations. However, the mechanism for modeling positions in this model was tailored for text modeling, and thus is less ideal for acoustic inputs. In this work, we adapt the relative position encoding scheme to the Speech Transformer, where the key addition is relative distance between input states in the self-attention network. As a result, the network can better adapt to the variable distributions present in speech data. Our experiments show that our resulting model achieves the best recognition result on the Switchboard benchmark in the non-augmentation condition, and the best published result in the MuST-C speech translation benchmark. We also show that this model is able to better utilize synthetic data than the Transformer, and adapts better to variable sentence segmentation quality for speech translation",
    "checked": true,
    "id": "0a82b81fbc0bc25bf4d60f9a18c8ee3571e80d7d",
    "semantic_title": "relative positional encoding for speech recognition and direct translation",
    "citation_count": 27
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kanda20_interspeech.html": {
    "title": "Joint Speaker Counting, Speech Recognition, and Speaker Identification for Overlapped Speech of any Number of Speakers",
    "volume": "main",
    "abstract": "We propose an end-to-end speaker-attributed automatic speech recognition model that unifies speaker counting, speech recognition, and speaker identification on monaural overlapped speech. Our model is built on serialized output training (SOT) with attention-based encoder-decoder, a recently proposed method for recognizing overlapped speech comprising an arbitrary number of speakers. We extend SOT by introducing a speaker inventory as an auxiliary input to produce speaker labels as well as multi-speaker transcriptions. All model parameters are optimized by speaker-attributed maximum mutual information criterion, which represents a joint probability for overlapped speech recognition and speaker identification. Experiments on LibriSpeech corpus show that our proposed method achieves significantly better speaker-attributed word error rate than the baseline that separately performs overlapped speech recognition and speaker identification",
    "checked": true,
    "id": "5390502397185775e3bceceb442b7d52d17383be",
    "semantic_title": "joint speaker counting, speech recognition, and speaker identification for overlapped speech of any number of speakers",
    "citation_count": 43
  },
  "https://www.isca-speech.org/archive/interspeech_2020/fukuda20_interspeech.html": {
    "title": "Implicit Transfer of Privileged Acoustic Information in a Generalized Knowledge Distillation Framework",
    "volume": "main",
    "abstract": "This paper proposes a novel generalized knowledge distillation framework, with an implicit transfer of privileged information. In our proposed framework, teacher networks are trained with two input branches on pairs of time-synchronous lossless and lossy acoustic features. While one branch of the teacher network processes a privileged view of the data using lossless features, the second branch models a student view, by processing lossy features corresponding to the same data. During the training step, weights of this teacher network are updated using a composite two-part cross entropy loss. The first part of this loss is computed between the predicted output labels of the lossless data and the actual ground truth. The second part of the loss is computed between the predicted output labels of the lossy data and lossless data. In the next step of generating soft labels, only the student view branch of the teacher is used with lossy data. The benefit of this proposed technique is shown on speech signals with long-term time-frequency bandwidth loss due to recording devices and network conditions. Compared to conventional generalized knowledge distillation with privileged information, the proposed method has a relative improvement of 9.5% on both lossless and lossy test sets",
    "checked": true,
    "id": "39f416ae6187f223d7242fa8fb11aabace0d382f",
    "semantic_title": "implicit transfer of privileged acoustic information in a generalized knowledge distillation framework",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2020/park20_interspeech.html": {
    "title": "Effect of Adding Positional Information on Convolutional Neural Networks for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "Attention-based models with convolutional encoders enable faster training and inference than recurrent neural network-based ones. However, convolutional models often require a very large receptive field to achieve high recognition accuracy, which not only increases the parameter size but also the computational cost and run-time memory footprint. A convolutional encoder with a short receptive field length can suffer from looping or skipping problems when the input utterance contains the same words as nearby sentences. We believe that this is due to the insufficient receptive field length, and try to remedy this problem by adding positional information to the convolution-based encoder. It is shown that the word error rate (WER) of a convolutional encoder with a short receptive field size can be reduced significantly by augmenting it with positional information. Visualization results are presented to demonstrate the effectiveness of adding positional information. The proposed method improves the accuracy of attention models with a convolutional encoder and achieves a WER of 10.60% on TED-LIUMv2 for an end-to-end speech recognition task",
    "checked": true,
    "id": "2a5db3e4a2dc8c9f0f5c221cb8a9b1fa4e2cc9a3",
    "semantic_title": "effect of adding positional information on convolutional neural networks for end-to-end speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20b_interspeech.html": {
    "title": "Deep Neural Network-Based Generalized Sidelobe Canceller for Robust Multi-Channel Speech Recognition",
    "volume": "main",
    "abstract": "The elastic spatial filter (ESF) proposed in recent years is a popular multi-channel speech enhancement front end based on deep neural network (DNN). It is suitable for real-time processing and has shown promising automatic speech recognition (ASR) results. However, the ESF only utilizes the knowledge of fixed beamforming, resulting in limited noise reduction capabilities. In this paper, we propose a DNN-based generalized sidelobe canceller (GSC) that can automatically track the target speaker's direction in real time and use the blocking technique to generate reference noise signals to further reduce noise from the fixed beam pointing to the target direction. The coefficients in the proposed GSC are fully learnable and an ASR criterion is used to optimize the entire network. The 4-channel experiments show that the proposed GSC achieves a relative word error rate improvement of 27.0% compared to the raw observation, 20.6% compared to the oracle direction-based traditional GSC, 10.5% compared to the ESF and 7.9% compared to the oracle mask-based generalized eigenvalue (GEV) beamformer",
    "checked": true,
    "id": "8756bb23d9ba9e0b8b1aa381965f245bd9a67399",
    "semantic_title": "deep neural network-based generalized sidelobe canceller for robust multi-channel speech recognition",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/xu20_interspeech.html": {
    "title": "Neural Spatio-Temporal Beamformer for Target Speech Separation",
    "volume": "main",
    "abstract": "Purely neural network (NN) based speech separation and enhancement methods, although can achieve good objective scores, inevitably cause nonlinear speech distortions that are harmful for the automatic speech recognition (ASR). On the other hand, the minimum variance distortionless response (MVDR) beamformer with NN-predicted masks, although can significantly reduce speech distortions, has limited noise reduction capability. In this paper, we propose a multi-tap MVDR beamformer with complex-valued masks for speech separation and enhancement. Compared to the state-of-the-art NN-mask based MVDR beamformer, the multi-tap MVDR beamformer exploits the inter-frame correlation in addition to the inter-microphone correlation that is already utilized in prior arts. Further improvements include the replacement of the real-valued masks with the complex-valued masks and the joint training of the complex-mask NN. The evaluation on our multi-modal multi-channel target speech separation and enhancement platform demonstrates that our proposed multi-tap MVDR beamformer improves both the ASR accuracy and the perceptual speech quality against prior arts",
    "checked": true,
    "id": "1cba72db8b8ce089680f0c7c078cab622bd841c3",
    "semantic_title": "neural spatio-temporal beamformer for target speech separation",
    "citation_count": 28
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20c_interspeech.html": {
    "title": "Online Directional Speech Enhancement Using Geometrically Constrained Independent Vector Analysis",
    "volume": "main",
    "abstract": "This paper proposes an online dual-microphone system for directional speech enhancement, which employs geometrically constrained independent vector analysis (IVA) based on the auxiliary function approach and vectorwise coordinate descent. Its offline version has recently been proposed and shown to outperform the conventional auxiliary function approach-based IVA (AuxIVA) thanks to the properly designed spatial constraints. We extend the offline algorithm to online by incorporating the autoregressive approximation of an auxiliary variable. Experimental evaluations revealed that the proposed online algorithm could work in real-time and achieved superior speech enhancement performance to online AuxIVA in both situations where a fixed target was interfered by a spatially stationary or dynamic interference",
    "checked": true,
    "id": "5e71f69790abe9af869c24108c47c762e7c6878e",
    "semantic_title": "online directional speech enhancement using geometrically constrained independent vector analysis",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yu20_interspeech.html": {
    "title": "End-to-End Multi-Look Keyword Spotting",
    "volume": "main",
    "abstract": "The performance of keyword spotting (KWS), measured in false alarms and false rejects, degrades significantly under the far field and noisy conditions. In this paper, we propose a multi-look neural network modeling for speech enhancement which simultaneously steers to listen to multiple sampled look directions. The multi-look enhancement is then jointly trained with KWS to form an end-to-end KWS model which integrates the enhanced signals from multiple look directions and leverages an attention mechanism to dynamically tune the model's attention to the reliable sources. We demonstrate, on our large noisy and far-field evaluation sets, that the proposed approach significantly improves the KWS performance against the baseline KWS system and a recent beamformer based multi-beam KWS system",
    "checked": true,
    "id": "0b3b732864e65c8882d4d9b014596de127c616a7",
    "semantic_title": "end-to-end multi-look keyword spotting",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2020/huang20_interspeech.html": {
    "title": "Differential Beamforming for Uniform Circular Array with Directional Microphones",
    "volume": "main",
    "abstract": "Use of omni-directional microphones is commonly assumed in the differential beamforming with uniform circular arrays. The conventional differential beamforming with omni-directional elements tends to suffer in low white-noise-gain (WNG) at the low frequencies and decrease of directivity factor (DF) at high frequencies. WNG measures the robustness of beamformer and DF evaluates the array performance in the presence of reverberation. The major contributions of this paper are as follows: First, we extends the existing work by presenting a new approach with the use of the directional microphone elements, and show clearly the connection between the conventional beamforming and the proposed beamforming. Second, a comparative study is made to show that the proposed approach brings about the noticeable improvement in WNG at the low frequencies and some improvement in DF at the high frequencies by exploiting an additional degree of freedom in the differential beamforming design. In addition, the beampattern appears more frequency-invariant than that of the conventional method. Third, we study how the proposed beamformer performs as the number of microphone elements and the radius of the array vary",
    "checked": true,
    "id": "2372ab78474ed348060ddf3fe215b1a1ece4d03c",
    "semantic_title": "differential beamforming for uniform circular array with directional microphones",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2020/qi20_interspeech.html": {
    "title": "Exploring Deep Hybrid Tensor-to-Vector Network Architectures for Regression Based Speech Enhancement",
    "volume": "main",
    "abstract": "This paper investigates different trade-offs between the number of model parameters and enhanced speech qualities by employing several deep tensor-to-vector regression models for speech enhancement. We find that a hybrid architecture, namely CNN-TT, is capable of maintaining a good quality performance with a reduced model parameter size. CNN-TT is composed of several convolutional layers at the bottom for feature extraction to improve speech quality and a tensor-train (TT) output layer on the top to reduce model parameters. We first derive a new upper bound on the generalization power of the convolutional neural network (CNN) based vector-to-vector regression models. Then, we provide experimental evidence on the Edinburgh noisy speech corpus to demonstrate that, in single-channel speech enhancement, CNN outperforms DNN at the expense of a small increment of model sizes. Besides, CNN-TT slightly outperforms the CNN counterpart by utilizing only 32% of the CNN model parameters. Besides, further performance improvement can be attained if the number of CNN-TT parameters is increased to 44% of the CNN model size. Finally, our experiments of multi-channel speech enhancement on a simulated noisy WSJ0 corpus demonstrate that our proposed hybrid CNN-TT architecture achieves better results than both DNN and CNN models in terms of better-enhanced speech qualities and smaller parameter sizes",
    "checked": true,
    "id": "797cac6bd3207e1a2398f28087194541f44fc2f2",
    "semantic_title": "exploring deep hybrid tensor-to-vector network architectures for regression based speech enhancement",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wu20_interspeech.html": {
    "title": "An End-to-End Architecture of Online Multi-Channel Speech Separation",
    "volume": "main",
    "abstract": "Multi-speaker speech recognition has been one of the key challenges in conversation transcription as it breaks the single active speaker assumption employed by most state-of-the-art speech recognition systems. Speech separation is considered as a remedy to this problem. Previously, we introduced a system, called unmixing, fixed-beamformer and extraction (UFE), that was shown to be effective in addressing the speech overlap problem in conversation transcription. With UFE, an input mixed signal is processed by fixed beamformers, followed by a neural network post filtering. Although promising results were obtained, the system contains multiple individually developed modules, leading potentially sub-optimum performance. In this work, we introduce an end-to-end modeling version of UFE. To enable gradient propagation all the way, an attentional selection module is proposed, where an attentional weight is learnt for each beamformer and spatial feature sampled over space. Experimental results show that the proposed system achieves comparable performance in an offline evaluation with the original separate processing-based pipeline, while producing remarkable improvements in an online evaluation",
    "checked": true,
    "id": "e909128e078ecb6148ce0f102953d8448b7b5c54",
    "semantic_title": "an end-to-end architecture of online multi-channel speech separation",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2020/nakagome20_interspeech.html": {
    "title": "Mentoring-Reverse Mentoring for Unsupervised Multi-Channel Speech Source Separation",
    "volume": "main",
    "abstract": "Mentoring-reverse mentoring, which is a novel knowledge transfer framework for unsupervised learning, is introduced in multi-channel speech source separation. This framework aims to improve two different systems, which are referred to as a senior and a junior system, by mentoring each other. The senior system, which is composed of a neural separator and a statistical blind source separation (BSS) model, generates a pseudo-target signal. The junior system, which is composed of a neural separator and a post-filter, was constructed using teacher-student learning with the pseudo-target signal generated from the senior system i.e, imitating the output from the senior system (mentoring step). Then, the senior system can be improved by propagating the shared neural separator of the grown-up junior system to the senior system (reverse mentoring step). Since the improved neural separator can give better initial parameters for the statistical BSS model, the senior system can yield more accurate pseudo-target signals, leading to iterative improvement of the pseudo-target signal generator and the neural separator. Experimental comparisons conducted under the condition where mixture-clean parallel data are not available demonstrated that the proposed mentoring-reverse mentoring framework yielded improvements in speech source separation over the existing unsupervised source separation methods",
    "checked": true,
    "id": "2f8b71c27626e58ec251d992ffcf8e19b3dec7bf",
    "semantic_title": "mentoring-reverse mentoring for unsupervised multi-channel speech source separation",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2020/nakatani20_interspeech.html": {
    "title": "Computationally Efficient and Versatile Framework for Joint Optimization of Blind Speech Separation and Dereverberation",
    "volume": "main",
    "abstract": "This paper proposes new blind signal processing techniques for optimizing a multi-input multi-output (MIMO) convolutional beamformer (CBF) in a computationally efficient way to simultaneously perform dereverberation and source separation. For effective CBF optimization, a conventional technique factorizes it into a multiple-target weighted prediction error (WPE) based dereverberation filter and a separation matrix. However, this technique requires the calculation of a huge spatio-temporal covariance matrix that reflects the statistics of all the sources, which makes the computational cost very high. For computationally efficient optimization, this paper introduces two techniques: one that decomposes the huge covariance matrix into ones for individual sources, and another that decomposes the CBF into sub-filters for estimating individual sources. Both techniques effectively and substantively reduce the size of the covariance matrices that must calculated, and allow us to greatly reduce the computational cost without loss of optimality",
    "checked": true,
    "id": "ab7d3d318a9bceab3c605962f5a99c57a95e2790",
    "semantic_title": "computationally efficient and versatile framework for joint optimization of blind speech separation and dereverberation",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tu20_interspeech.html": {
    "title": "A Space-and-Speaker-Aware Iterative Mask Estimation Approach to Multi-Channel Speech Recognition in the CHiME-6 Challenge",
    "volume": "main",
    "abstract": "We propose a space-and-speaker-aware iterative mask estimation (SSA-IME) approach to improving complex angular central Gaussian distributions (cACGMM) based beamforming in an iterative manner by leveraging upon the complementary information obtained from SSA-based regression. First, a mask calculated by beamformed speech features is proposed to enhance the estimation accuracy of the ideal ratio mask from noisy speech. Second, the outputs of cACGMM-beamformed speech with given time annotation as initial values are used to extract the log-power spectral and inter-phase difference features of different speakers serving as inputs to estimate the regression-based SSA model. Finally, in decoding, the mask estimated by the SSA model is also used to iteratively refine cACGMM-based masks, yielding enhanced multi-array speech. Tested on the recent CHiME-6 Challenge Track 1 tasks, the proposed SSA-IME framework significantly and consistently outperforms state-of-the-art approaches, and achieves the lowest word error rates for both Track 1 speech recognition tasks",
    "checked": true,
    "id": "b6128b94c124ad4cfba360cbbfaae39a3780d245",
    "semantic_title": "a space-and-speaker-aware iterative mask estimation approach to multi-channel speech recognition in the chime-6 challenge",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2020/youssef20_interspeech.html": {
    "title": "Identifying Causal Relationships Between Behavior and Local Brain Activity During Natural Conversation",
    "volume": "main",
    "abstract": "Characterizing precisely neurophysiological activity involved in natural conversations remains a major challenge. We explore in this paper the relationship between multimodal conversational behavior and brain activity during natural conversations. This is challenging due to Functional Magnetic Resonance Imaging (fMRI) time resolution and to the diversity of the recorded multimodal signals. We use a unique corpus including localized brain activity and behavior recorded during a fMRI experiment when several participants had natural conversations alternatively with a human and a conversational robot. The corpus includes fMRI responses as well as conversational signals that consist of synchronized raw audio and their transcripts, video and eye-tracking recordings. The proposed approach includes a first step to extract discrete neurophysiological time-series from functionally well defined brain areas, as well as behavioral time-series describing specific behaviors. Then, machine learning models are applied to predict neurophysiological time-series based on the extracted behavioral features. The results show promising prediction scores, and specific causal relationships are found between behaviors and the activity in functional brain areas for both conditions, i.e., human-human and human-robot conversations",
    "checked": true,
    "id": "a7869468bad54521f19bb1176a4c208a5a89fe6e",
    "semantic_title": "identifying causal relationships between behavior and local brain activity during natural conversation",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhou20_interspeech.html": {
    "title": "Neural Entrainment to Natural Speech Envelope Based on Subject Aligned EEG Signals",
    "volume": "main",
    "abstract": "Reconstruction of speech envelope from neural signal is a general way to study neural entrainment, which helps to understand the neural mechanism underlying speech processing. Previous neural entrainment studies were mainly based on single-trial neural activities, and the reconstruction accuracy of speech envelope is not high enough, probably due to the interferences from diverse noises such as breath and heartbeat. Considering that such noises independently emerge in the consistent neural processing of the subjects responding to the same speech stimulus, we proposed a method to align and average electroencephalograph (EEG) signals of the subjects for the same stimuli to reduce the noises of neural signals. Pearson correlation of constructed speech envelops with the original ones showed a great improvement comparing to the single-trial based method. Our study improved the correlation coefficient in delta band from around 0.25 to 0.5, where 0.25 was obtained in previous leading studies based on single-trial. The speech tracking phenomenon not only occurred in the commonly reported delta and theta band, but also occurred in the gamma band of EEG. Moreover, the reconstruction accuracy for regular speech was higher than that for the time-reversed speech, suggesting that neural entrainment to natural speech envelope reflects speech semantics",
    "checked": true,
    "id": "0407dd4edfd9bcd3af5801f8b2bdb75fdd27eb06",
    "semantic_title": "neural entrainment to natural speech envelope based on subject aligned eeg signals",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lian20_interspeech.html": {
    "title": "Does Lexical Retrieval Deteriorate in Patients with Mild Cognitive Impairment? Analysis of Brain Functional Network Will Tell",
    "volume": "main",
    "abstract": "Alterations in speech and language are typical signs of mild cognitive impairment (MCI), considered to be the prodromal stage of Alzheimer's disease (AD). Yet, very few studies have pointed out at what stage their speech production is disrupted. To bridge this knowledge gap, the present study focused on lexical retrieval, a specific process during speech production, and investigated how it is affected in cognitively impairment patients with the state-of-the-art analysis of brain functional network. 17 patients with MCI and 20 age-matched controls were invited to complete a primed picture naming task, of which the prime was either semantically related or unrelated to the target. Using electroencephalography (EEG) signals collected during task performance, even-related potentials (ERPs) were analyzed, together with the construction of the brain functional network. Results showed that whereas MCI patients did not exhibit significant differences in reaction time and ERP responses, their brain functional network did alter associated with a significant main effect in accuracy. The observation of increased cluster coefficients and characteristic path length indicated deteriorations in global information processing, which provided evidence that deficits in lexical retrieval might have occurred even at the preclinical stage of AD",
    "checked": true,
    "id": "0457d416107d7905a07bc59c37a2c5e6a34d0e43",
    "semantic_title": "does lexical retrieval deteriorate in patients with mild cognitive impairment? analysis of brain functional network will tell",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/fu20_interspeech.html": {
    "title": "Congruent Audiovisual Speech Enhances Cortical Envelope Tracking During Auditory Selective Attention",
    "volume": "main",
    "abstract": "Listeners usually have the ability to selectively attend to the target speech while ignoring competing sounds. The mechanism that top-down attention modulates the cortical envelope tracking to speech was proposed to account for this ability. Additional visual input, such as lipreading was considered beneficial for speech perception, especially in noise. However, the effect of audiovisual (AV) congruency on the dynamic properties of cortical envelope tracking activities was not discussed explicitly. And the involvement of cortical regions processing AV speech was unclear. To solve these issues, electroencephalography (EEG) was recorded while participants attending to one talker from a mixture for several AV conditions (audio-only, congruent and incongruent). Approaches of temporal response functions (TRFs) and inter-trial phase coherence (ITPC) analysis were utilized to index the cortical envelope tracking for each condition. Comparing with the audio-only condition, both indices were enhanced only for the congruent AV condition, and the enhancement was prominent over both the auditory and visual cortex. In addition, timings of different cortical regions involved in cortical envelope tracking activities were subject to stimulus modality. The present work provides new insight into the neural mechanisms of auditory selective attention when visual input is available",
    "checked": true,
    "id": "891bd3df3421dea4c50b182a7baea3b6f39915c9",
    "semantic_title": "congruent audiovisual speech enhances cortical envelope tracking during auditory selective attention",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20_interspeech.html": {
    "title": "Contribution of RMS-Level-Based Speech Segments to Target Speech Decoding Under Noisy Conditions",
    "volume": "main",
    "abstract": "Human listeners can recognize target speech streams in complex auditory scenes. The cortical activities can robustly track the amplitude fluctuations of target speech with auditory attentional modulation under a range of signal-to-masker ratios (SMRs). The root-mean-square (RMS) level of the speech signal is a crucial acoustic cue for target speech perception. However, in most studies, the neural-tracking activities were analyzed with the intact speech temporal envelopes, ignoring the characteristic decoding features in different RMS-level-specific speech segments. This study aimed to explore the contributions of high- and middle-RMS-level segments to target speech decoding in noisy conditions based on electroencephalogram (EEG) signals. The target stimulus was mixed with a competing speaker at five SMRs (i.e., 6, 3, 0, -3, and -6 dB), and then the temporal response function (TRF) was used to analyze the relationship between neural responses and high/middle-RMS-level segments. Experimental results showed that target and ignored speech streams had significantly different TRF responses under conditions with the high- or middle-RMS-level segments. Besides, the high- and middle-RMS-level segments elicited different TRF responses in morphological distributions. These results suggested that distinct models could be used in different RMS-level-specific speech segments to better decode target speech with corresponding EEG signals",
    "checked": true,
    "id": "999f83c0dc899dbe653a536f804b68d3d1d1040f",
    "semantic_title": "contribution of rms-level-based speech segments to target speech decoding under noisy conditions",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhao20_interspeech.html": {
    "title": "Cortical Oscillatory Hierarchy for Natural Sentence Processing",
    "volume": "main",
    "abstract": "Human speech processing, either for listening or oral reading, requires dynamic cortical activities that are not only driven by sensory stimuli externally but also influenced by semantic knowledge and speech planning goals internally. Each of these functions has been known to accompany specific rhythmic oscillations and be localized in distributed networks. The question is how the brain organizes these spatially and spectrally distinct functional networks in such a temporal precision that endows us with incredible speech abilities. For clarification, this study conducted an oral reading task with natural sentences and collected simultaneously the involved brain waves, eye movements, and speech signals with high-density EEG and eye movement equipment. By examining the regional oscillatory spectral perturbation and modeling the frequency-specific interregional connections, our results revealed a hierarchical oscillatory mechanism, in which gamma oscillation entrains with the fine-structured sensory input while beta oscillation modulated the sensory output. Alpha oscillation mediated between sensory perception and cognitive function via selective suppression. Theta oscillation synchronized local networks for large-scale coordination. Differing from a single function-frequency-correspondence, the coexistence of multi-frequency oscillations was found to be critical for local regions to communicate remotely and diversely in a larger network",
    "checked": true,
    "id": "834bfee413d7f6ffb4fdf0db207559e8a743c1c5",
    "semantic_title": "cortical oscillatory hierarchy for natural sentence processing",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/bosch20_interspeech.html": {
    "title": "Comparing EEG Analyses with Different Epoch Alignments in an Auditory Lexical Decision Experiment",
    "volume": "main",
    "abstract": "In processing behavioral data from auditory lexical decision, reaction times (RT) can be defined relative to stimulus onset or relative to stimulus offset. Using stimulus onset as the reference invokes models that assumes that relevant processing starts immediately, while stimulus offset invokes models that assume that relevant processing can only start when the acoustic input is complete. It is suggested that EEG recordings can be used to tear apart putative processes. EEG analysis requires some kind of time-locking of epochs, so that averaging of multiple signals does not mix up effects of different processes. However, in many lexical decision experiments the duration of the speech stimuli varies substantially. Consequently, processes tied to stimulus offset are not appropriately aligned and might get lost in the averaging process. In this paper we investigate whether the time course of putative processes such as phonetic encoding, lexical access and decision making can be derived from ERPs and from instantaneous power representations in several frequency bands when epochs are time-locked at stimulus onset or stimulus offset. In addition, we investigate whether time-locking at the moment when the response is given can shed light on the decision process per sé",
    "checked": true,
    "id": "3db247f5d8d212f5d51d3601afdadeece380c0ad",
    "semantic_title": "comparing eeg analyses with different epoch alignments in an auditory lexical decision experiment",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/talkar20_interspeech.html": {
    "title": "Detection of Subclinical Mild Traumatic Brain Injury (mTBI) Through Speech and Gait",
    "volume": "main",
    "abstract": "Between 15% to 40% of mild traumatic brain injury (mTBI) patients experience incomplete recoveries or provide subjective reports of decreased motor abilities, despite a clinically-determined complete recovery. This demonstrates a need for objective measures capable of detecting subclinical residual mTBI, particularly in return-to-duty decisions for warfighters and return-to-play decisions for athletes. In this paper, we utilize features from recordings of directed speech and gait tasks completed by ten healthy controls and eleven subjects with lingering subclinical impairments from an mTBI. We hypothesize that decreased coordination and precision during fine motor movements governing speech production (articulation, phonation, and respiration), as well as during gross motor movements governing gait, can be effective indicators of subclinical mTBI. Decreases in coordination are measured from correlations of vocal acoustic feature time series and torso acceleration time series. We apply eigenspectra derived from these correlations to machine learning models to discriminate between the two subject groups. The fusion of correlation features derived from acoustic and gait time series achieve an AUC of 0.98. This highlights the potential of using the combination of vocal acoustic features from speech tasks and torso acceleration during a simple gait task as a rapid screening tool for subclinical mTBI ",
    "checked": true,
    "id": "d81bd4c3ed6ad4562d0681ea1d1a2798f1c82176",
    "semantic_title": "detection of subclinical mild traumatic brain injury (mtbi) through speech and gait",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shor20_interspeech.html": {
    "title": "Towards Learning a Universal Non-Semantic Representation of Speech",
    "volume": "main",
    "abstract": "The ultimate goal of transfer learning is to reduce labeled data requirements by exploiting a pre-existing embedding model trained for different datasets or tasks. The visual and language communities have established benchmarks to compare embeddings, but the speech community has yet to do so. This paper proposes a benchmark for comparing speech representations on non-semantic tasks, and proposes a representation based on an unsupervised triplet-loss objective. The proposed representation outperforms other representations on the benchmark, and even exceeds state-of-the-art performance on a number of transfer learning tasks. The embedding is trained on a publicly available dataset, and it is tested on a variety of low-resource downstream tasks, including personalization tasks and medical domain. The benchmark , models , and evaluation code are publicly released",
    "checked": true,
    "id": "1042714c5be82d980066fd038105112e601a848e",
    "semantic_title": "towards learning a universal non-semantic representation of speech",
    "citation_count": 115
  },
  "https://www.isca-speech.org/archive/interspeech_2020/rajan20_interspeech.html": {
    "title": "Poetic Meter Classification Using i-Vector-MTF Fusion",
    "volume": "main",
    "abstract": "In this paper, a deep neural network (DNN)-based poetic meter classification scheme is proposed using a fusion of musical texture features (MTF) and i-vectors. The experiment is performed in two phases. Initially, the mel-frequency cepstral coefficient (MFCC) features are fused with MTF and classification is done using DNN. MTF include timbral, rhythmic, and melodic features. Later, in the second phase, the MTF is fused with i-vectors and classification is performed. The performance is evaluated using a newly created poetic corpus in Malayalam, one of the prominent languages in India. While the MFCC-MTF/DNN system reports an overall accuracy of 80.83%, the i-vector/MTF fusion reports an overall accuracy of 86.66%. The performance is also compared with a baseline support vector machine (SVM)-based classifier. The results show that the architectural choice of i-vector fusion with MTF on DNN has merit in recognizing meters from recited poems",
    "checked": true,
    "id": "39dd2d0bc5494d7ec4c9b9a6083efe389503ed27",
    "semantic_title": "poetic meter classification using i-vector-mtf fusion",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2020/dai20_interspeech.html": {
    "title": "Formant Tracking Using Dilated Convolutional Networks Through Dense Connection with Gating Mechanism",
    "volume": "main",
    "abstract": "Formant tracking is one of the most fundamental problems in speech processing. Traditionally, formants are estimated using signal processing methods. Recent studies showed that generic convolutional architectures can outperform recurrent networks on temporal tasks such as speech synthesis and machine translation. In this paper, we explored the use of Temporal Convolutional Network (TCN) for formant tracking. In addition to the conventional implementation, we modified the architecture from three aspects. First, we turned off the \"causal\" mode of dilated convolution, making the dilated convolution see the future speech frames. Second, each hidden layer reused the output information from all the previous layers through dense connection. Third, we also adopted a gating mechanism to alleviate the problem of gradient disappearance by selectively forgetting unimportant information. The model was validated on the open access formant database VTR. The experiment showed that our proposed model was easy to converge and achieved an overall mean absolute percent error (MAPE) of 8.2% on speech-labeled frames, compared to three competitive baselines of 9.4% (LSTM), 9.1% (Bi-LSTM) and 8.9% (TCN)",
    "checked": true,
    "id": "011a36d6e092e018998b8502966a61a67ad83aa0",
    "semantic_title": "formant tracking using dilated convolutional networks through dense connection with gating mechanism",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hu20_interspeech.html": {
    "title": "Automatic Analysis of Speech Prosody in Dutch",
    "volume": "main",
    "abstract": "In this paper we present a publicly available tool for automatic analysis of speech prosody (AASP) in Dutch. Incorporating the state-of-the-art analytical frameworks, AASP enables users to analyze prosody at two levels from different theoretical perspectives. Holistically, by means of the Functional Principal Component Analysis (FPCA) it generates mathematical functions that capture changes in the shape of a pitch contour. The tool outputs the weights of principal components in a table for users to process in further statistical analysis. Structurally, AASP analyzes prosody in terms of prosodic events within the auto-segmental metrical framework, hypothesizing prosodic labels in accordance with Transcription of Dutch Intonation (ToDI) with accuracy comparable to similar tools for other languages. Published as a Docker container, the tool can be set up on various operating systems in only two steps. Moreover, the tool is accessed through a graphic user interface, making it accessible to users with limited programming skills",
    "checked": true,
    "id": "1c0d78af159c753c974496eca2925910b516fc3d",
    "semantic_title": "automatic analysis of speech prosody in dutch",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gresse20_interspeech.html": {
    "title": "Learning Voice Representation Using Knowledge Distillation for Automatic Voice Casting",
    "volume": "main",
    "abstract": "The search for professional voice-actors for audiovisual productions is a sensitive task, performed by the artistic directors (ADs). The ADs have a strong appetite for new talents/voices but cannot perform large scale auditions. Automatic tools able to suggest the most suited voices are of a great interest for audiovisual industry. In previous works, we showed the existence of acoustic information allowing to mimic the AD's choices. However, the only available information is the ADs' choices from the already dubbed multimedia productions. In this paper, we propose a representation-learning based strategy to build a character/role representation, called p-vector. In addition, the large variability between audiovisual productions makes it difficult to have homogeneous training datasets. We overcome this difficulty by using knowledge distillation methods to take advantage of external datasets. Experiments are conducted on video-game voice excerpts. Results show a significant improvement using the p-vector, compared to the speaker-based x-vector representation",
    "checked": false,
    "id": "f0a867e24fe77a65bf19ba9ea6759b39cf11f8e0",
    "semantic_title": "apprentissage automatique de représentation de voix à l'aide d'une distillation de la connaissance pour le casting vocal (learning voice representation using knowledge distillation for automatic voice casting )",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yegnanarayana20_interspeech.html": {
    "title": "Enhancing Formant Information in Spectrographic Display of Speech",
    "volume": "main",
    "abstract": "Formants are resonances of the time varying vocal tract system, and their characteristics are reflected in the response of the system for a sequence of impulse-like excitation sequence originated at the glottis. This paper presents a method to enhance the formants information in the display of spectrogram of the speech signal, especially for high pitched voices. It is well known that in the narrowband spectrogram, the presence of pitch harmonics masks the formant information, whereas in the wideband spectrogram, the formant regions are smeared. Using single frequency filtering (SFF) analysis, we show that the wideband equivalent SFF spectrogram can be modified to enhance the formant information in the display by improving the frequency resolution. For this, we obtain two SFF spectrograms by using single frequency filtering of the speech signal at two closely spaced roots on the real axis in the z-plane. The ratio or difference of the two SFF spectrograms is processed to enhance the formant information in the spectrographic display. This will help in tracking rapidly changing formants and in resolving closely spaced formants. The effect is more pronounced in the case of high-pitched voices, like female and children speech",
    "checked": true,
    "id": "4c96a8ca9b37c69c4052fd68dc779dc8096ae7b8",
    "semantic_title": "enhancing formant information in spectrographic display of speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gump20_interspeech.html": {
    "title": "Unsupervised Methods for Evaluating Speech Representations",
    "volume": "main",
    "abstract": "Disentanglement is a desired property in representation learning and a significant body of research has tried to show that it is a useful representational prior. Evaluating disentanglement is challenging, particularly for real world data like speech, where ground truth generative factors are typically not available. Previous work on disentangled representation learning in speech has used categorical supervision like phoneme or speaker identity in order to disentangle grouped feature spaces. However, this work differs from the typical dimension-wise view of disentanglement in other domains. This paper proposes to use low-level acoustic features to provide the structure required to evaluate dimension-wise disentanglement. By choosing well-studied acoustic features, grounded and descriptive evaluation is made possible for unsupervised representation learning. This work produces a toolkit for evaluating disentanglement in unsupervised representations of speech and evaluates its efficacy on previous research",
    "checked": true,
    "id": "26a006d2ad970efd5c25f5fe849740f3ff3ab48d",
    "semantic_title": "unsupervised methods for evaluating speech representations",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tran20_interspeech.html": {
    "title": "Robust Pitch Regression with Voiced/Unvoiced Classification in Nonstationary Noise Environments",
    "volume": "main",
    "abstract": "Accurate voiced/unvoiced information is crucial in estimating the pitch of a target speech signal in severe nonstationary noise environments. Nevertheless, state-of-the-art pitch estimators based on deep neural networks (DNN) lack a dedicated mechanism for robustly detecting voiced and unvoiced segments in the target speech in noisy conditions. In this work, we proposed an end-to-end deep learning-based pitch estimation framework which jointly detects voiced/unvoiced segments and predicts pitch values for the voiced regions of the ground-truth speech. We empirically showed that our proposed framework significantly more robust than state-of-the-art DNN based pitch detectors in nonstationary noise settings. Our results suggest that joint training of voiced/unvoiced detection and voiced pitch prediction can significantly improve pitch estimation performance",
    "checked": true,
    "id": "ef2d3fe969b497df3bd60247458df2e1c254cf57",
    "semantic_title": "robust pitch regression with voiced/unvoiced classification in nonstationary noise environments",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2020/setlur20_interspeech.html": {
    "title": "Nonlinear ISA with Auxiliary Variables for Learning Speech Representations",
    "volume": "main",
    "abstract": "This paper extends recent work on nonlinear Independent Component Analysis ( ica) by introducing a theoretical framework for nonlinear Independent Subspace Analysis ( isa) in the presence of auxiliary variables. Observed high dimensional acoustic features like log Mel spectrograms can be considered as surface level manifestations of nonlinear transformations over individual multivariate sources of information like speaker characteristics, phonological content etc. Under assumptions of energy based models we use the theory of nonlinear isa to propose an algorithm that learns unsupervised speech representations whose subspaces are independent and potentially highly correlated with the original non-stationary multivariate sources. We show how nonlinear ica with auxiliary variables can be extended to a generic identifiable model for subspaces as well while also providing sufficient conditions for the identifiability of these high dimensional subspaces. Our proposed methodology is generic and can be integrated with standard unsupervised approaches to learn speech representations with subspaces that can theoretically capture independent higher order speech signals. We evaluate the gains of our algorithm when integrated with the Autoregressive Predictive Coding ( apc) model by showing empirical results on the speaker verification and phoneme recognition tasks",
    "checked": true,
    "id": "1db6f2396fd6315044a552351baa3bcac1eabf8c",
    "semantic_title": "nonlinear isa with auxiliary variables for learning speech representations",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/takeuchi20_interspeech.html": {
    "title": "Harmonic Lowering for Accelerating Harmonic Convolution for Audio Signals",
    "volume": "main",
    "abstract": "Convolutional neural networks have been successfully applied to a variety of audio signal processing tasks including sound source separation, speech recognition and acoustic scene understanding. Since many pitched sounds have a harmonic structure, an operation, called harmonic convolution, has been proposed to take advantages of the structure appearing in the audio signals. However, the computational cost involved is higher than that of normal convolution. This paper proposes a faster calculation method of harmonic convolution called Harmonic Lowering. The method unrolls the input data to a redundant layout so that the normal convolution operation can be applied. The analysis of the runtimes and the number of multiplication operations show that the proposed method accelerates the harmonic convolution 2 to 7 times faster than the conventional method under realistic parameter settings, while no approximation is introduced",
    "checked": true,
    "id": "759d08cf11bd60dffaec8d4abb6964c4c9f4526b",
    "semantic_title": "harmonic lowering for accelerating harmonic convolution for audio signals",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ai20_interspeech.html": {
    "title": "Knowledge-and-Data-Driven Amplitude Spectrum Prediction for Hierarchical Neural Vocoders",
    "volume": "main",
    "abstract": "In our previous work, we have proposed a neural vocoder called HiNet which recovers speech waveforms by predicting amplitude and phase spectra hierarchically from input acoustic features. In HiNet, the amplitude spectrum predictor (ASP) predicts log amplitude spectra (LAS) from input acoustic features. This paper proposes a novel knowledge-and-data-driven ASP (KDD-ASP) to improve the conventional one. First, acoustic features (i.e., F0 and mel-cepstra) pass through a knowledge-driven LAS recovery module to obtain approximate LAS (ALAS). This module is designed based on the combination of STFT and source-filter theory, in which the source part and the filter part are designed based on input F0 and mel-cepstra, respectively. Then, the recovered ALAS are processed by a data-driven LAS refinement module which consists of multiple trainable convolutional layers to get the final LAS. Experimental results show that the HiNet vocoder using KDD-ASP can achieve higher quality of synthetic speech than that using conventional ASP and the WaveRNN vocoder on a text-to-speech (TTS) task",
    "checked": true,
    "id": "383133f2cf579661146a792df7c6ebf49c6ca96b",
    "semantic_title": "knowledge-and-data-driven amplitude spectrum prediction for hierarchical neural vocoders",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tian20_interspeech.html": {
    "title": "FeatherWave: An Efficient High-Fidelity Neural Vocoder with Multi-Band Linear Prediction",
    "volume": "main",
    "abstract": "In this paper, we propose the FeatherWave, yet another variant of WaveRNN vocoder combining the multi-band signal processing and the linear predictive coding. The LPCNet, a recently proposed neural vocoder which utilized the linear predictive characteristic of speech signal in the WaveRNN architecture, can generate high quality speech with a speed faster than real-time on a single CPU core. However, LPCNet is still not efficient enough for online speech generation tasks. To address this issue, we adopt the multi-band linear predictive coding for WaveRNN vocoder. The multi-band method enables the model to generate several speech samples in parallel at one step. Therefore, it can significantly improve the efficiency of speech synthesis. The proposed model with 4 sub-bands needs less than 1.6 GFLOPS for speech generation. In our experiments, it can generate 24 kHz high-fidelity audio 9× faster than real-time on a single CPU, which is much faster than the LPCNet vocoder. Furthermore, our subjective listening test shows that the FeatherWave can generate speech with better quality than LPCNet",
    "checked": true,
    "id": "cf8fbc40e9f92729b904e835640e32ae71416c16",
    "semantic_title": "featherwave: an efficient high-fidelity neural vocoder with multi-band linear prediction",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yang20_interspeech.html": {
    "title": "VocGAN: A High-Fidelity Real-Time Vocoder with a Hierarchically-Nested Adversarial Network",
    "volume": "main",
    "abstract": "We present a novel high-fidelity real-time neural vocoder called VocGAN. A recently developed GAN-based vocoder, MelGAN, produces speech waveforms in real-time. However, it often produces a waveform that is insufficient in quality or inconsistent with acoustic characteristics of the input mel spectrogram. VocGAN is nearly as fast as MelGAN, but it significantly improves the quality and consistency of the output waveform. VocGAN applies a multi-scale waveform generator and a hierarchically-nested discriminator to learn multiple levels of acoustic properties in a balanced way. It also applies the joint conditional and unconditional objective, which has shown successful results in high-resolution image synthesis. In experiments, VocGAN synthesizes speech waveforms 416.7× faster on a GTX 1080Ti GPU and 3.24× faster on a CPU than real-time. Compared with MelGAN, it also exhibits significantly improved quality in multiple evaluation metrics including mean opinion score (MOS) with minimal additional overhead. Additionally, compared with Parallel WaveGAN, another recently developed high-fidelity vocoder, VocGAN is 6.98× faster on a CPU and exhibits higher MOS",
    "checked": true,
    "id": "4cd2c3440b3c770ce3e1e75a0371a892bdb30945",
    "semantic_title": "vocgan: a high-fidelity real-time vocoder with a hierarchically-nested adversarial network",
    "citation_count": 55
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kanagawa20_interspeech.html": {
    "title": "Lightweight LPCNet-Based Neural Vocoder with Tensor Decomposition",
    "volume": "main",
    "abstract": "This paper proposes a lightweight neural vocoder based on LPCNet. The recently proposed LPCNet exploits linear predictive coding to represent vocal tract characteristics, and can rapidly synthesize high-quality waveforms with fewer parameters than WaveRNN. For even greater speeds, it is necessary to reduce the time-heavy two GRUs and the DualFC. Although the original work only pruned the first GRU weight, there is room for improvements in the other GRU and DualFC. Accordingly, we use tensor decomposition to reduce these remaining parameters by more than 80%. For the proposed method we demonstrate that 1) it is 1.26 times faster on a CPU, and 2) it matched naturalness of the original LPCNet for acoustic features extracted from natural speech and for those predicted by TTS",
    "checked": true,
    "id": "f75b0d27a5175e1a8d0c643f26259022997803ae",
    "semantic_title": "lightweight lpcnet-based neural vocoder with tensor decomposition",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hsu20_interspeech.html": {
    "title": "WG-WaveNet: Real-Time High-Fidelity Speech Synthesis Without GPU",
    "volume": "main",
    "abstract": "In this paper, we propose WG-WaveNet, a fast, lightweight, and high-quality waveform generation model. WG-WaveNet is composed of a compact flow-based model and a post-filter. The two components are jointly trained by maximizing the likelihood of the training data and optimizing loss functions on the frequency domains. As we design a flow-based model that is heavily compressed, the proposed model requires much less computational resources compared to other waveform generation models during both training and inference time; even though the model is highly compressed, the post-filter maintains the quality of generated waveform. Our PyTorch implementation can be trained using less than 8 GB GPU memory and generates audio samples at a rate of more than 960 kHz on an NVIDIA 1080Ti GPU. Furthermore, even if synthesizing on a CPU, we show that the proposed method is capable of generating 44.1 kHz speech waveform 1.2 times faster than real-time. Experiments also show that the quality of generated audio is comparable to those of other methods. Audio samples are publicly available online",
    "checked": true,
    "id": "67b59158d4925767bb8e615d9ffa372384f8b81c",
    "semantic_title": "wg-wavenet: real-time high-fidelity speech synthesis without gpu",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2020/stephenson20_interspeech.html": {
    "title": "What the Future Brings: Investigating the Impact of Lookahead for Incremental Neural TTS",
    "volume": "main",
    "abstract": "In incremental text to speech synthesis (iTTS), the synthesizer produces an audio output before it has access to the entire input sentence. In this paper, we study the behavior of a neural sequence-to-sequence TTS system when used in an incremental mode, i.e. when generating speech output for token n, the system has access to n+k tokens from the text sequence. We first analyze the impact of this incremental policy on the evolution of the encoder representations of token n for different values of k (the lookahead parameter). The results show that, on average, tokens travel 88% of the way to their full context representation with a one-word lookahead and 94% after 2 words. We then investigate which text features are the most influential on the evolution towards the final representation using a random forest analysis. The results show that the most salient factors are related to token length. We finally evaluate the effects of lookahead k at the decoder level, using a MUSHRA listening test. This test shows results that contrast with the above high figures: speech synthesis quality obtained with 2 word-lookahead is significantly lower than the one obtained with the full sentence",
    "checked": true,
    "id": "d890606b654e7ec1d90fa368290b306626090b75",
    "semantic_title": "what the future brings: investigating the impact of lookahead for incremental neural tts",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2020/popov20_interspeech.html": {
    "title": "Fast and Lightweight On-Device TTS with Tacotron2 and LPCNet",
    "volume": "main",
    "abstract": "We present a fast and lightweight on-device text-to-speech system based on state-of-art methods of feature and speech generation i.e. Tacotron2 and LPCNet. We show that modification of the basic pipeline combined with hardware-specific optimizations and extensive usage of parallelization enables running TTS service even on low-end devices with faster than realtime waveform generation. Moreover, the system preserves high quality of speech without noticeable degradation of Mean Opinion Score compared to the non-optimized baseline. While the system is mostly oriented on low-to-mid range hardware we believe that it can also be used in any CPU-based environment",
    "checked": true,
    "id": "ee9f2afd6c383979e01d7ff72dd5192f3b1898c3",
    "semantic_title": "fast and lightweight on-device tts with tacotron2 and lpcnet",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2020/song20_interspeech.html": {
    "title": "Efficient WaveGlow: An Improved WaveGlow Vocoder with Enhanced Speed",
    "volume": "main",
    "abstract": "Neural vocoder, such as WaveGlow, has become an important component in recent high-quality text-to-speech (TTS) systems. In this paper, we propose Efficient WaveGlow (EWG), a flow-based generative model serving as an efficient neural vocoder. Similar to WaveGlow, EWG has a normalizing flow backbone where each flow step consists of an affine coupling layer and an invertible 1×1 convolution. To reduce the number of model parameters and enhance the speed without sacrificing the quality of the synthesized speech, EWG improves WaveGlow in three aspects. First, the WaveNet-style transform network in WaveGlow is replaced with an FFTNet-style dilated convolution network. Next, to reduce the computation cost, group convolution is applied to both audio and local condition features. At last, the local condition is shared among the transform network layers in each coupling layer. As a result, EWG can reduce the number of floating-point operations (FLOPs) required to generate one-second audio and the number of model parameters both by more than 12 times. Experimental results show that EWG can reduce real-world inference time cost by more than twice, without any obvious reduction in the speech quality",
    "checked": true,
    "id": "96ec36cac1b7693f01282ce44735f928dd584c98",
    "semantic_title": "efficient waveglow: an improved waveglow vocoder with enhanced speed",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2020/maguer20_interspeech.html": {
    "title": "Can Auditory Nerve Models Tell us What's Different About WaveNet Vocoded Speech?",
    "volume": "main",
    "abstract": "Nowadays, synthetic speech is almost indistinguishable from human speech. The remarkable quality is mainly due to the displacing of signal processing based vocoders in favour of neural vocoders and, in particular, the WaveNet architecture. At the same time, speech synthesis evaluation is still facing difficulties in adjusting to these improvements. These difficulties are even more prevalent in the case of objective evaluation methodologies which do not correlate well with human perception. Yet, an often forgotten use of objective evaluation is to uncover prominent differences between speech signals. Such differences are crucial to decipher the improvement introduced by the use of WaveNet. Therefore, abandoning objective evaluation could be a serious mistake. In this paper, we analyze vocoded synthetic speech re-rendered using WaveNet, comparing it to standard vocoded speech. To do so, we objectively compare spectrograms and neurograms, the latter being the output of AN models. The spectrograms allow us to look at the speech production side, and the neurograms relate to the speech perception path. While we were not yet able to pinpoint how WaveNet and WORLD differ, our results suggest that the Mean-Rate (MR) neurograms in particular warrant further investigation",
    "checked": true,
    "id": "7a9e655652a5be68224b76d9c73ce41041780730",
    "semantic_title": "can auditory nerve models tell us what's different about wavenet vocoded speech?",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/paul20_interspeech.html": {
    "title": "Speaker Conditional WaveRNN: Towards Universal Neural Vocoder for Unseen Speaker and Recording Conditions",
    "volume": "main",
    "abstract": "Recent advancements in deep learning led to human-level performance in single-speaker speech synthesis. However, there are still limitations in terms of speech quality when generalizing those systems into multiple-speaker models especially for unseen speakers and unseen recording qualities. For instance, conventional neural vocoders are adjusted to the training speaker and have poor generalization capabilities to unseen speakers. In this work, we propose a variant of WaveRNN, referred to as speaker conditional WaveRNN (SC-WaveRNN). We target towards the development of an efficient universal vocoder even for unseen speakers and recording conditions. In contrast to standard WaveRNN, SC-WaveRNN exploits additional information given in the form of speaker embeddings. Using publicly-available data for training, SC-WaveRNN achieves significantly better performance over baseline WaveRNN on both subjective and objective metrics. In MOS, SC-WaveRNN achieves an improvement of about 23% for seen speaker and seen recording condition and up to 95% for unseen speaker and unseen condition. Finally, we extend our work by implementing a multi-speaker text-to-speech (TTS) synthesis similar to zero-shot speaker adaptation. In terms of performance, our system has been preferred over the baseline TTS system by 60% over 15.5% and by 60.9% over 32.6%, for seen and unseen speakers, respectively",
    "checked": true,
    "id": "f56e37bf467f8e93efe97f9014ee133e8adfd3c7",
    "semantic_title": "speaker conditional wavernn: towards universal neural vocoder for unseen speaker and recording conditions",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2020/liu20_interspeech.html": {
    "title": "Neural Homomorphic Vocoder",
    "volume": "main",
    "abstract": "In this paper, we propose the neural homomorphic vocoder (NHV), a source-filter model based neural vocoder framework. NHV synthesizes speech by filtering impulse trains and noise with linear time-varying (LTV) filters. A neural network controls the LTV filters by estimating complex cepstrums of time-varying impulse responses given acoustic features. The proposed framework can be trained with a combination of multi-resolution STFT loss and adversarial loss functions. Due to the use of DSP-based synthesis methods, NHV is highly efficient, fully controllable and interpretable. A vocoder was built under the framework to synthesize speech given log-Mel spectrograms and fundamental frequencies. While the model cost only 15 kFLOPs per sample, the synthesis quality remained comparable to baseline neural vocoders in both copy-synthesis and text-to-speech",
    "checked": true,
    "id": "638f2c213e221d75f3dd45518d47ace3630ac524",
    "semantic_title": "neural homomorphic vocoder",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gretter20_interspeech.html": {
    "title": "Overview of the Interspeech TLT2020 Shared Task on ASR for Non-Native Children's Speech",
    "volume": "main",
    "abstract": "We present an overview of the ASR challenge for non-native children's speech organized for a special session at Interspeech 2020. The data for the challenge was obtained in the context of a spoken language proficiency assessment administered at Italian schools for students between the ages of 9 and 16 who were studying English and German as a foreign language. The corpus distributed for the challenge was a subset of the English recordings. Participating teams competed either in a closed track, in which they could use only the training data released by the organizers of the challenge, or in an open track, in which they were allowed to use additional training data. The closed track received 9 entries and the open track received 7 entries, with the best scoring systems achieving substantial improvements over a state-of-the-art baseline system. This paper describes the corpus of non-native children's speech that was used for the challenge, analyzes the results, and discusses some points that should be considered for subsequent challenges in this domain in the future",
    "checked": true,
    "id": "f8d0fc38ffeed4afe26013eb87143bec4d38a42e",
    "semantic_title": "overview of the interspeech tlt2020 shared task on asr for non-native children's speech",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lo20_interspeech.html": {
    "title": "The NTNU System at the Interspeech 2020 Non-Native Children's Speech ASR Challenge",
    "volume": "main",
    "abstract": "This paper describes the NTNU ASR system participating in the Interspeech 2020 Non-Native Children's Speech ASR Challenge supported by the SIG-CHILD group of ISCA. This ASR shared task is made much more challenging due to the coexisting diversity of non-native and children speaking characteristics. In the setting of closed-track evaluation, all participants were restricted to develop their systems merely based on the speech and text corpora provided by the organizer. To work around this under-resourced issue, we built our ASR system on top of CNN-TDNNF-based acoustic models, meanwhile harnessing the synergistic power of various data augmentation strategies, including both utterance- and word-level speed perturbation and spectrogram augmentation, alongside a simple yet effective data-cleansing approach. All variants of our ASR system employed an RNN-based language model to rescore the first-pass recognition hypotheses, which was trained solely on the text dataset released by the organizer. Our system with the best configuration came out in second place, resulting in a word error rate (WER) of 17.59%, while those of the top-performing, second runner-up and official baseline systems are 15.67%, 18.71%, 35.09%, respectively",
    "checked": true,
    "id": "e719b336c0267c274881bff7646bea1aa9586f30",
    "semantic_title": "the ntnu system at the interspeech 2020 non-native children's speech asr challenge",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2020/knill20_interspeech.html": {
    "title": "Non-Native Children's Automatic Speech Recognition: The INTERSPEECH 2020 Shared Task ALTA Systems",
    "volume": "main",
    "abstract": "Automatic spoken language assessment (SLA) is a challenging problem due to the large variations in learner speech combined with limited resources. These issues are even more problematic when considering children learning a language, with higher levels of acoustic and lexical variability, and of code-switching compared to adult data. This paper describes the ALTA system for the INTERSPEECH 2020 Shared Task on Automatic Speech Recognition for Non-Native Children's Speech. The data for this task consists of examination recordings of Italian school children aged 9–16, ranging in ability from minimal, to basic, to limited but effective command of spoken English. A variety of systems were developed using the limited training data available, 49 hours. State-of-the-art acoustic models and language models were evaluated, including a diversity of lexical representations, handling code-switching and learner pronunciation errors, and grade specific models. The best single system achieved a word error rate (WER) of 16.9% on the evaluation data. By combining multiple diverse systems, including both grade independent and grade specific models, the error rate was reduced to 15.7%. This combined system was the best performing submission for both the closed and open tasks",
    "checked": true,
    "id": "0cc4a4cdc1a4c0d214a72470833cb127cfeef0a5",
    "semantic_title": "non-native children's automatic speech recognition: the interspeech 2020 shared task alta systems",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kathania20_interspeech.html": {
    "title": "Data Augmentation Using Prosody and False Starts to Recognize Non-Native Children's Speech",
    "volume": "main",
    "abstract": "This paper describes AaltoASR's speech recognition system for the INTERSPEECH 2020 shared task on Automatic Speech Recognition (ASR) for non-native children's speech. The task is to recognize non-native speech from children of various age groups given a limited amount of speech. Moreover, the speech being spontaneous has false starts transcribed as partial words, which in the test transcriptions leads to unseen partial words. To cope with these two challenges, we investigate a data augmentation-based approach. Firstly, we apply the prosody-based data augmentation to supplement the audio data. Secondly, we simulate false starts by introducing partial-word noise in the language modeling corpora creating new words. Acoustic models trained on prosody-based augmented data outperform the models using the baseline recipe or the SpecAugment-based augmentation. The partial-word noise also helps to improve the baseline language model. Our ASR system, a combination of these schemes, is placed third in the evaluation period and achieves the word error rate of 18.71%. Post-evaluation period, we observe that increasing the amounts of prosody-based augmented data leads to better performance. Furthermore, removing low-confidence-score words from hypotheses can lead to further gains. These two improvements lower the ASR error rate to 17.99%",
    "checked": true,
    "id": "10ea7cc20e56dc6d97e4646bf93cb9e7e4f4716b",
    "semantic_title": "data augmentation using prosody and false starts to recognize non-native children's speech",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shahin20_interspeech.html": {
    "title": "UNSW System Description for the Shared Task on Automatic Speech Recognition for Non-Native Children's Speech",
    "volume": "main",
    "abstract": "In this paper we describe our children's Automatic Speech Recognition (ASR) system for the first shared task on ASR for English non-native children's speech. The acoustic model comprises 6 Convolutional Neural Network (CNN) layers and 12 Factored Time-Delay Neural Network (TDNN-F) layers, trained by data from 5 different children's speech corpora. Speed perturbation, Room Impulse Response (RIR), babble noise and non-speech noise data augmentation methods were utilized to enhance the model robustness. Three Language Models (LMs) were employed: an in-domain LM trained on written data and speech transcriptions of non-native children, a LM trained on non-native written data and transcription of both native and non-native children's speech and a TEDLIUM LM trained on adult TED talks transcriptions. Lattices produced from the different ASR systems were combined and decoded using the Minimum Bayes-Risk (MBR) decoding algorithm to get the final output. Our system achieved a final Word Error Rate (WER) of 17.55% and 16.59% for both developing and testing sets respectively and ranked second among the 10 teams participating in the task",
    "checked": true,
    "id": "d66a6bb4a681cd291d56163c0e3f9a6ead33c7d4",
    "semantic_title": "unsw system description for the shared task on automatic speech recognition for non-native children's speech",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2020/horiguchi20_interspeech.html": {
    "title": "End-to-End Speaker Diarization for an Unknown Number of Speakers with Encoder-Decoder Based Attractors",
    "volume": "main",
    "abstract": "End-to-end speaker diarization for an unknown number of speakers is addressed in this paper. Recently proposed end-to-end speaker diarization outperformed conventional clustering-based speaker diarization, but it has one drawback: it is less flexible in terms of the number of speakers. This paper proposes a method for encoder-decoder based attractor calculation (EDA), which first generates a flexible number of attractors from a speech embedding sequence. Then, the generated multiple attractors are multiplied by the speech embedding sequence to produce the same number of speaker activities. The speech embedding sequence is extracted using the conventional self-attentive end-to-end neural speaker diarization (SA-EEND) network. In a two-speaker condition, our method achieved a 2.69% diarization error rate (DER) on simulated mixtures and a 8.07% DER on the two-speaker subset of CALLHOME, while vanilla SA-EEND attained 4.56% and 9.54%, respectively. In unknown numbers of speakers conditions, our method attained a 15.29% DER on CALLHOME, while the x-vector-based clustering method achieved a 19.43% DER",
    "checked": true,
    "id": "ee33d61522fd70fa4e6470decbdac6c17f8b4fdb",
    "semantic_title": "end-to-end speaker diarization for an unknown number of speakers with encoder-decoder based attractors",
    "citation_count": 112
  },
  "https://www.isca-speech.org/archive/interspeech_2020/medennikov20_interspeech.html": {
    "title": "Target-Speaker Voice Activity Detection: A Novel Approach for Multi-Speaker Diarization in a Dinner Party Scenario",
    "volume": "main",
    "abstract": "Speaker diarization for real-life scenarios is an extremely challenging problem. Widely used clustering-based diarization approaches perform rather poorly in such conditions, mainly due to the limited ability to handle overlapping speech. We propose a novel Target-Speaker Voice Activity Detection (TS-VAD) approach, which directly predicts an activity of each speaker on each time frame. TS-VAD model takes conventional speech features (e.g., MFCC) along with i-vectors for each speaker as inputs. A set of binary classification output layers produces activities of each speaker. I-vectors can be estimated iteratively, starting with a strong clustering-based diarization We also extend the TS-VAD approach to the multi-microphone case using a simple attention mechanism on top of hidden representations extracted from the single-channel TS-VAD model. Moreover, post-processing strategies for the predicted speaker activity probabilities are investigated. Experiments on the CHiME-6 unsegmented data show that TS-VAD achieves state-of-the-art results outperforming the baseline x-vector-based system by more than 30% Diarization Error Rate (DER) abs",
    "checked": true,
    "id": "50a6b28d4b9d13f030407e3d860d1c852dc213bd",
    "semantic_title": "target-speaker voice activity detection: a novel approach for multi-speaker diarization in a dinner party scenario",
    "citation_count": 108
  },
  "https://www.isca-speech.org/archive/interspeech_2020/aronowitz20_interspeech.html": {
    "title": "New Advances in Speaker Diarization",
    "volume": "main",
    "abstract": "Recently, speaker diarization based on speaker embeddings has shown excellent results in many works. In this paper we propose several enhancements throughout the diarization pipeline. This work addresses two clustering frameworks: agglomerative hierarchical clustering (AHC) and spectral clustering (SC) First, we use multiple speaker embeddings. We show that fusion of x-vectors and d-vectors boosts accuracy significantly. Second, we train neural networks to leverage both acoustic and duration information for scoring similarity of segments or clusters. Third, we introduce a novel method to guide the AHC clustering mechanism using a neural network. Fourth, we handle short duration segments in SC by deemphasizing their effect on setting the number of speakers Finally, we propose a novel method for estimating the number of clusters in the SC framework. The method takes each eigenvalue and analyzes the projections of the SC similarity matrix on the corresponding eigenvector We evaluated our system on NIST SRE 2000 CALLHOME and, using cross-validation, we achieved an error rate of 5.1%, going beyond state-of-the-art speaker diarization",
    "checked": true,
    "id": "9a840a0aef4a28fa503d62be7dc1b881d6236221",
    "semantic_title": "new advances in speaker diarization",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lin20_interspeech.html": {
    "title": "Self-Attentive Similarity Measurement Strategies in Speaker Diarization",
    "volume": "main",
    "abstract": "Speaker diarization can be described as the process of extracting sequential speaker embeddings from an audio stream and clustering them according to speaker identities. Nowadays, deep neural network based approaches like x-vector have been widely adopted for speaker embedding extraction. However, in the clustering back-end, probabilistic linear discriminant analysis (PLDA) is still the dominant algorithm for similarity measurement. PLDA works in a pair-wise and independent manner, which may ignore the positional correlation of adjacent speaker embeddings. To address this issue, our previous work proposed the long short-term memory (LSTM) based scoring model, followed by the spectral clustering algorithm. In this paper, we further propose two enhanced methods based on the self-attention mechanism, which no longer focuses on the local correlation but searches for similar speaker embeddings in the whole sequence. The first approach achieves state-of-the-art performance on the DIHARD II Eval Set (18.44% DER after resegmentation), while the second one operates with higher efficiency",
    "checked": true,
    "id": "52dc0f0576d75064f7ff06d69ef717f95487e25c",
    "semantic_title": "self-attentive similarity measurement strategies in speaker diarization",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20b_interspeech.html": {
    "title": "Speaker Attribution with Voice Profiles by Graph-Based Semi-Supervised Learning",
    "volume": "main",
    "abstract": "Speaker attribution is required in many real-world applications, such as meeting transcription, where speaker identity is assigned to each utterance according to speaker voice profiles. In this paper, we propose to solve the speaker attribution problem by using graph-based semi-supervised learning methods. A graph of speech segments is built for each session, on which segments from voice profiles are represented by labeled nodes while segments from test utterances are unlabeled nodes. The weight of edges between nodes is evaluated by the similarities between the pretrained speaker embeddings of speech segments. Speaker attribution then becomes a semi-supervised learning problem on graphs, on which two graph-based methods are applied: label propagation (LP) and graph neural networks (GNNs). The proposed approaches are able to utilize the structural information of the graph to improve speaker attribution performance. Experimental results on real meeting data show that the graph based approaches reduce speaker attribution error by up to 68% compared to a baseline speaker identification approach that processes each utterance independently",
    "checked": true,
    "id": "114ed6d345da665393493b52669a9e9b0ecbdd12",
    "semantic_title": "speaker attribution with voice profiles by graph-based semi-supervised learning",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2020/singh20_interspeech.html": {
    "title": "Deep Self-Supervised Hierarchical Clustering for Speaker Diarization",
    "volume": "main",
    "abstract": "The state-of-the-art speaker diarization systems use agglomerative hierarchical clustering (AHC) which performs the clustering of previously learned neural embeddings. While the clustering approach attempts to identify speaker clusters, the AHC algorithm does not involve any further learning. In this paper, we propose a novel algorithm for hierarchical clustering which combines the speaker clustering along with a representation learning framework. The proposed approach is based on principles of self-supervised learning where the self-supervision is derived from the clustering algorithm. The representation learning network is trained with a regularized triplet loss using the clustering solution at the current step while the clustering algorithm uses the deep embeddings from the representation learning step. By combining the self-supervision based representation learning along with the clustering algorithm, we show that the proposed algorithm improves significantly (29% relative improvement) over the AHC algorithm with cosine similarity for a speaker diarization task on CALLHOME dataset. In addition, the proposed approach also improves over the state-of-the-art system with PLDA affinity matrix with 10% relative improvement in DER",
    "checked": true,
    "id": "425303b773b2746beeb96bf6653c9995311d56b8",
    "semantic_title": "deep self-supervised hierarchical clustering for speaker diarization",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chung20_interspeech.html": {
    "title": "Spot the Conversation: Speaker Diarisation in the Wild",
    "volume": "main",
    "abstract": "The goal of this paper is speaker diarisation of videos collected ‘in the wild' We make three key contributions. First, we propose an automatic audio-visual diarisation method for YouTube videos. Our method consists of active speaker detection using audio-visual methods and speaker verification using self-enrolled speaker models. Second, we integrate our method into a semi-automatic dataset creation pipeline which significantly reduces the number of hours required to annotate videos with diarisation labels. Finally, we use this pipeline to create a large-scale diarisation dataset called VoxConverse, collected from ‘in the wild' videos, which we will release publicly to the research community. Our dataset consists of overlapping speech, a large and diverse speaker pool, and challenging background conditions",
    "checked": true,
    "id": "49528263d02e03f5b877dc081a1e049ab88fe504",
    "semantic_title": "spot the conversation: speaker diarisation in the wild",
    "citation_count": 86
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20_interspeech.html": {
    "title": "Learning Contextual Language Embeddings for Monaural Multi-Talker Speech Recognition",
    "volume": "main",
    "abstract": "End-to-end multi-speaker speech recognition has been a popular topic in recent years, as more and more researches focus on speech processing in more realistic scenarios. Inspired by the hearing mechanism of human beings, which enables us to concentrate on the interested speaker from the multi-speaker mixed speech by utilizing both audio and context knowledge, this paper explores the contextual information to improve the multi-talker speech recognition. In the proposed architecture, the novel embedding learning model is designed to accurately extract the contextual embedding from the multi-talker mixed speech directly. Then two advanced training strategies are further proposed to improve the new model. Experimental results show that our proposed method achieves a very large improvement on multi-speaker speech recognition, with ~25% relative WER reduction against the baseline end-to-end multi-talker ASR model",
    "checked": true,
    "id": "9e4047639948c4676861585c302431e41998a96e",
    "semantic_title": "learning contextual language embeddings for monaural multi-talker speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/du20_interspeech.html": {
    "title": "Double Adversarial Network Based Monaural Speech Enhancement for Robust Speech Recognition",
    "volume": "main",
    "abstract": "To improve the noise robustness of automatic speech recognition (ASR), the generative adversarial network (GAN) based enhancement methods are employed as the front-end processing, which comprise a single adversarial process of an enhancement model and a discriminator. In this single adversarial process, the discriminator is encouraged to find differences between the enhanced and clean speeches, but the distribution of clean speeches is ignored. In this paper, we propose a double adversarial network (DAN) by adding another adversarial generation process (AGP), which forces the discriminator not only to find the differences but also to model the distribution. Furthermore, a functional mean square error (f-MSE) is proposed to utilize the representations learned by the discriminator. Experimental results reveal that AGP and f-MSE are crucial for the enhancement performance on ASR task, which are missed in previous GAN-based methods. Specifically, our DAN achieves 13.00% relative word error rate improvements over the noisy speeches on the test set of CHiME-2, which outperforms several recent GAN-based enhancement methods significantly",
    "checked": true,
    "id": "25fa6cf7cae30d347884438ba1126264e2619256",
    "semantic_title": "double adversarial network based monaural speech enhancement for robust speech recognition",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2020/bruguier20_interspeech.html": {
    "title": "Anti-Aliasing Regularization in Stacking Layers",
    "volume": "main",
    "abstract": "Shift-invariance is a desirable property of many machine learning models. It means that delaying the input of a model in time should only result in delaying its prediction in time. A model that is shift-invariant, also eliminates undesirable side effects like frequency aliasing. When building sequence models, not only should the shift-invariance property be preserved when sampling input features, it must also be respected inside the model itself. Here, we study the impact of the commonly used stacking layer in LSTM-based ASR models and show that aliasing is likely to occur. Experimentally, by adding merely 7 parameters to an existing speech recognition model that has 120 million parameters, we are able to reduce the impact of aliasing. This acts as a regularizer that discards frequencies the model shouldn't be relying on for predictions. Our results show that under conditions unseen at training, we are able to reduce the relative word error rate by up to 5%",
    "checked": true,
    "id": "76acec25016547c1f5db19ebe5591e3866c4c270",
    "semantic_title": "anti-aliasing regularization in stacking layers",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2020/andrusenko20_interspeech.html": {
    "title": "Towards a Competitive End-to-End Speech Recognition for CHiME-6 Dinner Party Transcription",
    "volume": "main",
    "abstract": "While end-to-end ASR systems have proven competitive with the conventional hybrid approach, they are prone to accuracy degradation when it comes to noisy and low-resource conditions. In this paper, we argue that, even in such difficult cases, some end-to-end approaches show performance close to the hybrid baseline. To demonstrate this, we use the CHiME-6 Challenge data as an example of challenging environments and noisy conditions of everyday speech. We experimentally compare and analyze CTC-Attention versus RNN-Transducer approaches along with RNN versus Transformer architectures. We also provide a comparison of acoustic features and speech enhancements. Besides, we evaluate the effectiveness of neural network language models for hypothesis re-scoring in low-resource conditions. Our best end-to-end model based on RNN-Transducer, together with improved beam search, reaches quality by only 3.8% WER abs. worse than the LF-MMI TDNN-F CHiME-6 Challenge baseline. With the Guided Source Separation based training data augmentation, this approach outperforms the hybrid baseline system by 2.7% WER abs. and the end-to-end system best known before by 25.7% WER abs",
    "checked": true,
    "id": "c3c58177c073d677ec97de834340815b9dd89b42",
    "semantic_title": "towards a competitive end-to-end speech recognition for chime-6 dinner party transcription",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20b_interspeech.html": {
    "title": "End-to-End Far-Field Speech Recognition with Unified Dereverberation and Beamforming",
    "volume": "main",
    "abstract": "Despite successful applications of end-to-end approaches in multi-channel speech recognition, the performance still degrades severely when the speech is corrupted by reverberation. In this paper, we integrate the dereverberation module into the end-to-end multi-channel speech recognition system and explore two different frontend architectures. First, a multi-source mask-based weighted prediction error (WPE) module is incorporated in the frontend for dereverberation. Second, another novel frontend architecture is proposed, which extends the weighted power minimization distortionless response (WPD) convolutional beamformer to perform simultaneous separation and dereverberation. We derive a new formulation from the original WPD, which can handle multi-source input, and replace eigenvalue decomposition with the matrix inverse operation to make the back-propagation algorithm more stable. The above two architectures are optimized in a fully end-to-end manner, only using the speech recognition criterion. Experiments on both spatialized wsj1-2mix corpus and REVERB show that our proposed model outperformed the conventional methods in reverberant scenarios",
    "checked": true,
    "id": "721d7c82b80f14246d353251837e1711824a9e60",
    "semantic_title": "end-to-end far-field speech recognition with unified dereverberation and beamforming",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2020/qiu20_interspeech.html": {
    "title": "Quaternion Neural Networks for Multi-Channel Distant Speech Recognition",
    "volume": "main",
    "abstract": "Despite the significant progress in automatic speech recognition (ASR), distant ASR remains challenging due to noise and reverberation. A common approach to mitigate this issue consists of equipping the recording devices with multiple microphones that capture the acoustic scene from different perspectives. These multi-channel audio recordings contain specific internal relations between each signal. In this paper, we propose to capture these inter- and intra- structural dependencies with quaternion neural networks, which can jointly process multiple signals as whole quaternion entities. The quaternion algebra replaces the standard dot product with the Hamilton one, thus offering a simple and elegant way to model dependencies between elements. The quaternion layers are then coupled with a recurrent neural network, which can learn long-term dependencies in the time domain. We show that a quaternion long-short term memory neural network (QLSTM), trained on the concatenated multi-channel speech signals, outperforms equivalent real-valued LSTM on two different tasks of multi-channel distant speech recognition",
    "checked": true,
    "id": "12957c9dc9045a04c229d25dcc02f0b636fbed78",
    "semantic_title": "quaternion neural networks for multi-channel distant speech recognition",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chen20_interspeech.html": {
    "title": "Improved Guided Source Separation Integrated with a Strong Back-End for the CHiME-6 Dinner Party Scenario",
    "volume": "main",
    "abstract": "The CHiME-6 dataset presents a difficult task with extreme speech overlap, severe noise and a natural speaking style. The gap of the word error rate (WER) is distinct between the audios recorded by the distant microphone arrays and the individual headset microphones. The official baseline exhibits a WER gap of approximately 10% even though the guided source separation (GSS) has achieved considerable WER reduction. In the paper, we make an effort to integrate an improved GSS with a strong automatic speech recognition (ASR) back-end, which bridges the WER gap and achieves substantial ASR performance improvement. Specifically, the proposed GSS is initialized by masks from data-driven deep-learning models, utilizes the spectral information and conducts a selection of the input channels. Meanwhile, we propose a data augmentation technique via random channel selection and deep convolutional neural network-based multi-channel acoustic models for back-end modeling. In the experiments, our framework largely reduced the WER to 34.78%/36.85% on the CHiME-6 development/evaluation set. Moreover, a narrower gap of 0.89%/4.67% was observed between the distant and headset audios. This framework is also the foundation of the IOA's submission to the CHiME-6 competition, which is ranked among the top systems",
    "checked": true,
    "id": "37bc21a50850348a0a3fa1db50e05b3714a172a5",
    "semantic_title": "improved guided source separation integrated with a strong back-end for the chime-6 dinner party scenario",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20c_interspeech.html": {
    "title": "Neural Speech Separation Using Spatially Distributed Microphones",
    "volume": "main",
    "abstract": "This paper proposes a neural network based speech separation method using spatially distributed microphones. Unlike with traditional microphone array settings, neither the number of microphones nor their spatial arrangement is known in advance, which hinders the use of conventional multi-channel speech separation neural networks based on fixed size input. To overcome this, a novel network architecture is proposed that interleaves inter-channel processing layers and temporal processing layers. The inter-channel processing layers apply a self-attention mechanism along the channel dimension to exploit the information obtained with a varying number of microphones. The temporal processing layers are based on a bidirectional long short term memory (BLSTM) model and applied to each channel independently. The proposed network leverages information across time and space by stacking these two kinds of layers alternately. Our network estimates time-frequency (TF) masks for each speaker, which are then used to generate enhanced speech signals either with TF masking or beamforming. Speech recognition experimental results show that the proposed method significantly outperforms baseline multi-channel speech separation systems",
    "checked": true,
    "id": "a87879088eb6b5425497edaf7f736b5377fcdbee",
    "semantic_title": "neural speech separation using spatially distributed microphones",
    "citation_count": 28
  },
  "https://www.isca-speech.org/archive/interspeech_2020/horiguchi20b_interspeech.html": {
    "title": "Utterance-Wise Meeting Transcription System Using Asynchronous Distributed Microphones",
    "volume": "main",
    "abstract": "A novel framework for meeting transcription using asynchronous microphones is proposed in this paper. It consists of audio synchronization, speaker diarization, utterance-wise speech enhancement using guided source separation, automatic speech recognition, and duplication reduction. Doing speaker diarization before speech enhancement enables the system to deal with overlapped speech without considering sampling frequency mismatch between microphones. Evaluation on our real meeting datasets showed that our framework achieved a character error rate (CER) of 28.7% by using 11 distributed microphones, while a monaural microphone placed on the center of the table had a CER of 38.2%. We also showed that our framework achieved CER of 21.8%, which is only 2.1 percentage points higher than the CER in headset microphone-based transcription",
    "checked": true,
    "id": "dac0722cf97410467eef8e9e6884f93020267baf",
    "semantic_title": "utterance-wise meeting transcription system using asynchronous distributed microphones",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2020/deadman20_interspeech.html": {
    "title": "Simulating Realistically-Spatialised Simultaneous Speech Using Video-Driven Speaker Detection and the CHiME-5 Dataset",
    "volume": "main",
    "abstract": "Simulated data plays a crucial role in the development and evaluation of novel distant microphone ASR techniques. However, the commonly used simulated datasets adopt uninformed and potentially unrealistic speaker location distributions. We wish to generate more realistic simulations driven by recorded human behaviour. By using devices with a paired microphone array and camera, we analyse unscripted dinner party scenarios (CHiME-5) to estimate the distribution of speaker separation in a realistic setting. We deploy face-detection, and pose-detection techniques on 114 cameras to automatically locate speakers in 20 dinner party sessions. Our analysis found that on average, the separation between speakers was only 17 degrees. We use this analysis to create datasets with realistic distributions and compare it with commonly used datasets of simulated signals. By changing the position of speakers, we show that the word error rate can increase by over 73.5% relative when using a strong speech enhancement and ASR system",
    "checked": true,
    "id": "f3734b4d4a6d0ef227224e31734ef37376ddf0ad",
    "semantic_title": "simulating realistically-spatialised simultaneous speech using video-driven speaker detection and the chime-5 dataset",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2020/botelho20_interspeech.html": {
    "title": "Toward Silent Paralinguistics: Speech-to-EMG — Retrieving Articulatory Muscle Activity from Speech",
    "volume": "main",
    "abstract": "Electromyographic (EMG) signals recorded during speech production encode information on articulatory muscle activity and also on the facial expression of emotion, thus representing a speech-related biosignal with strong potential for paralinguistic applications. In this work, we estimate the electrical activity of the muscles responsible for speech articulation directly from the speech signal. To this end, we first perform a neural conversion of speech features into electromyographic time domain features, and then attempt to retrieve the original EMG signal from the time domain features. We propose a feed forward neural network to address the first step of the problem (speech features to EMG features) and a neural network composed of a convolutional block and a bidirectional long short-term memory block to address the second problem (true EMG features to EMG signal). We observe that four out of the five originally proposed time domain features can be estimated reasonably well from the speech signal. Further, the five time domain features are able to predict the original speech-related EMG signal with a concordance correlation coefficient of 0.663. We further compare our results with the ones achieved on the inverse problem of generating acoustic speech features from EMG features",
    "checked": false,
    "id": "3a4c3bd6a78aad367f49f903b31c588825c3bf3c",
    "semantic_title": "toward silent paralinguistics: speech-to-emg - retrieving articulatory muscle activity from speech",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20c_interspeech.html": {
    "title": "Multimodal Deception Detection Using Automatically Extracted Acoustic, Visual, and Lexical Features",
    "volume": "main",
    "abstract": "Deception detection in conversational dialogue has attracted much attention in recent years. Yet existing methods for this rely heavily on human-labeled annotations that are costly and potentially inaccurate. In this work, we present an automated system that utilizes multimodal features for conversational deception detection, without the use of human annotations. We study the predictive power of different modalities and combine them for better performance. We use openSMILE to extract acoustic features after applying noise reduction techniques to the original audio. Facial landmark features are extracted from the visual modality. We experiment with training facial expression detectors and applying Fisher Vectors to encode sequences of facial landmarks with varying length. Linguistic features are extracted from automatic transcriptions of the data. We examine the performance of these methods on the Box of Lies dataset of deception game videos, achieving 73% accuracy using features from all modalities. This result is significantly better than previous results on this corpus which relied on manual annotations, and also better than human performance",
    "checked": true,
    "id": "8339aa76f8ebb8b506d9688e50e98b886c71b472",
    "semantic_title": "multimodal deception detection using automatically extracted acoustic, visual, and lexical features",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2020/pan20b_interspeech.html": {
    "title": "Multi-Modal Attention for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Emotion represents an essential aspect of human speech that is manifested in speech prosody. Speech, visual, and textual cues are complementary in human communication. In this paper, we study a hybrid fusion method, referred to as multi-modal attention network (MMAN) to makes use of visual and textual cues in speech emotion recognition. We propose a novel multi-modal attention mechanism, cLSTM-MMA, which facilitates the attention across three modalities and selectively fuse the information. cLSTM-MMA is fused with other uni-modal sub-networks in the late fusion. The experiments show that speech emotion recognition benefits significantly from visual and textual cues, and the proposed cLSTM-MMA alone is as competitive as other fusion methods in terms of accuracy, but with a much more compact network structure. The proposed hybrid network MMAN achieves state-of-the-art performance on IEMOCAP database for emotion recognition",
    "checked": true,
    "id": "2e762956621d3a3906b5891afa0f755e01ffcbde",
    "semantic_title": "multi-modal attention for speech emotion recognition",
    "citation_count": 43
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shen20_interspeech.html": {
    "title": "WISE: Word-Level Interaction-Based Multimodal Fusion for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "While having numerous real-world applications, speech emotion recognition is still a technically challenging problem. How to effectively leverage the inherent multiple modalities in speech data (e.g., audio and text) is key to accurate classification. Existing studies normally choose to fuse multimodal features at the utterance level and largely neglect the dynamic interplay of features from different modalities at a fine-granular level over time. In this paper, we explicitly model dynamic interactions between audio and text at the word level via interaction units between two long short-term memory networks representing audio and text. We also devise a hierarchical representation of audio information from the frame, phoneme and word levels, which largely improves the expressiveness of resulting audio features. We finally propose WISE, a novel word-level interaction-based multimodal fusion framework for speech emotion recognition, to accommodate the aforementioned components. We evaluate WISE on the public benchmark IEMOCAP corpus and demonstrate that it outperforms state-of-the-art methods",
    "checked": true,
    "id": "3d1353331d71fec03fd11d8fdaee2435ca23f247",
    "semantic_title": "wise: word-level interaction-based multimodal fusion for speech emotion recognition",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chen20b_interspeech.html": {
    "title": "A Multi-Scale Fusion Framework for Bimodal Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Speech emotion recognition (SER) is a challenging task that requires to learn suitable features for achieving good performance. The development of deep learning techniques makes it possible to automatically extract features rather than construct hand-crafted features. In this paper, a multi-scale fusion framework named STSER is proposed for bimodal SER by using speech and text information. A smodel, which takes advantage of convolutional neural network (CNN), bi-directional long short-term memory (Bi-LSTM) and the attention mechanism, is proposed to learn speech representation from the log-mel spectrogram extracted from speech data. Specifically, the CNN layers are utilized to learn local correlations. Then the Bi-LSTM layer is applied to learn long-term dependencies and contextual information. Finally, the multi-head self-attention layer makes the model focus on the features that are most related to the emotions. A tmodel using a pre-trained ALBERT model is applied for learning text representation from text data. Finally, a multi-scale fusion strategy, including feature fusion and ensemble learning, is applied to improve the overall performance. Experiments conducted on the public emotion dataset IEMOCAP have shown that the proposed STSER can achieve comparable recognition accuracy with fewer feature inputs",
    "checked": true,
    "id": "5c6942e94661dd67933c660b3661e9e5cb26ccbd",
    "semantic_title": "a multi-scale fusion framework for bimodal speech emotion recognition",
    "citation_count": 37
  },
  "https://www.isca-speech.org/archive/interspeech_2020/liu20b_interspeech.html": {
    "title": "Group Gated Fusion on Attention-Based Bidirectional Alignment for Multimodal Emotion Recognition",
    "volume": "main",
    "abstract": "Emotion recognition is a challenging and actively-studied research area that plays a critical role in emotion-aware human-computer interaction systems. In a multimodal setting, temporal alignment between different modalities has not been well investigated yet. This paper presents a new model named as Gated Bidirectional Alignment Network (GBAN), which consists of an attention-based bidirectional alignment network over LSTM hidden states to explicitly capture the alignment relationship between speech and text, and a novel group gated fusion (GGF) layer to integrate the representations of different modalities. We empirically show that the attention-aligned representations outperform the last-hidden-states of LSTM significantly, and the proposed GBAN model outperforms existing state-of-the-art multimodal approaches on the IEMOCAP dataset",
    "checked": true,
    "id": "7d60294d3e19dafa1fb0143136c9efae4b8f59de",
    "semantic_title": "group gated fusion on attention-based bidirectional alignment for multimodal emotion recognition",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2020/khare20_interspeech.html": {
    "title": "Multi-Modal Embeddings Using Multi-Task Learning for Emotion Recognition",
    "volume": "main",
    "abstract": "General embeddings like word2vec, GloVe and ELMo have shown a lot of success in natural language tasks. The embeddings are typically extracted from models that are built on general tasks such as skip-gram models and natural language generation. In this paper, we extend the work from natural language understanding to multi-modal architectures that use audio, visual and textual information for machine learning tasks. The embeddings in our network are extracted using the encoder of a transformer model trained using multi-task training. We use person identification and automatic speech recognition as the tasks in our embedding generation framework. We tune and evaluate the embeddings on the downstream task of emotion recognition and demonstrate that on the CMU-MOSEI dataset, the embeddings can be used to improve over previous state of the art results",
    "checked": true,
    "id": "1904958461d165d1cfddcecb5358f413cba1f639",
    "semantic_title": "multi-modal embeddings using multi-task learning for emotion recognition",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20d_interspeech.html": {
    "title": "Using Speaker-Aligned Graph Memory Block in Multimodally Attentive Emotion Recognition Network",
    "volume": "main",
    "abstract": "Integrating multimodal emotion sensing modules in realizing human-centered technologies is rapidly growing. Despite recent advancement of deep architectures in improving recognition performances, inability to handle individual differences in the expressive cues creates a major hurdle for real world applications. In this work, we propose a Speaker-aligned Graph Memory Network (SaGMN) that leverages the use of speaker embedding learned from a large speaker verification network to characterize such an individualized personal difference across speakers. Specifically, the learning of the gated memory block is jointly optimized with a speaker graph encoder which aligns similar vocal characteristics samples together while effectively enlarge the discrimination across emotion classes. We evaluate our multimodal emotion recognition network on the CMU-MOSEI database and achieve a state-of-art accuracy of 65.1% UAR and 74.7% F1 score. Further visualization experiments demonstrate the effect of speaker space alignment with the use of graph memory blocks",
    "checked": true,
    "id": "90eff998afb10d8f41e15b26f2db3b360fa0ebee",
    "semantic_title": "using speaker-aligned graph memory block in multimodally attentive emotion recognition network",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lian20b_interspeech.html": {
    "title": "Context-Dependent Domain Adversarial Neural Network for Multimodal Emotion Recognition",
    "volume": "main",
    "abstract": "Emotion recognition remains a complex task due to speaker variations and low-resource training samples. To address these difficulties, we focus on the domain adversarial neural networks (DANN) for emotion recognition. The primary task is to predict emotion labels. The secondary task is to learn a common representation where speaker identities can not be distinguished. By using this approach, we bring the representations of different speakers closer. Meanwhile, through using the unlabeled data in the training process, we alleviate the impact of low-resource training samples. In the meantime, prior work found that contextual information and multimodal features are important for emotion recognition. However, previous DANN based approaches ignore these information, thus limiting their performance. In this paper, we propose the context-dependent domain adversarial neural network for multimodal emotion recognition. To verify the effectiveness of our proposed method, we conduct experiments on the benchmark dataset IEMOCAP. Experimental results demonstrate that the proposed method shows an absolute improvement of 3.48% over state-of-the-art strategies",
    "checked": true,
    "id": "29b972f985c53d960b96a5bf9916cb132c138923",
    "semantic_title": "context-dependent domain adversarial neural network for multimodal emotion recognition",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yang20b_interspeech.html": {
    "title": "ATCSpeech: A Multilingual Pilot-Controller Speech Corpus from Real Air Traffic Control Environment",
    "volume": "main",
    "abstract": "Automatic Speech Recognition (ASR) technique has been greatly developed in recent years, which expedites many applications in other fields. For the ASR research, speech corpus is always an essential foundation, especially for the vertical industry, such as Air Traffic Control (ATC). There are some speech corpora for common applications, public or paid. However, for the ATC domain, it is difficult to collect raw speeches from real systems due to safety issues. More importantly, annotating the transcription is a more laborious work for the supervised learning ASR task, which hugely restricts the prospect of ASR application. In this paper, a multilingual speech corpus (ATCSpeech) from real ATC systems, including accented Mandarin Chinese and English speeches, is built and released to encourage the non-commercial ASR research in the ATC domain. The corpus is detailly introduced from the perspective of data amount, speaker gender and role, speech quality and other attributions. In addition, the performance of baseline ASR models is also reported. A community edition for our speech database can be applied and used under a special contract. To our best knowledge, this is the first work that aims at building a real and multilingual ASR corpus for the ATC related research",
    "checked": true,
    "id": "b6287bd90206b14fd43d9ac17e51417bc2fcaad5",
    "semantic_title": "atcspeech: a multilingual pilot-controller speech corpus from real air traffic control environment",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gutkin20_interspeech.html": {
    "title": "Developing an Open-Source Corpus of Yoruba Speech",
    "volume": "main",
    "abstract": "This paper introduces an open-source speech dataset for Yoruba — one of the largest low-resource West African languages spoken by at least 22 million people. Yoruba is one of the official languages of Nigeria, Benin and Togo, and is spoken in other neighboring African countries and beyond. The corpus consists of over four hours of 48 kHz recordings from 36 male and female volunteers and the corresponding transcriptions that include disfluency annotation. The transcriptions have full diacritization, which is vital for pronunciation and lexical disambiguation. The annotated speech dataset described in this paper is primarily intended for use in text-to-speech systems, serve as adaptation data in automatic speech recognition and speech-to-speech translation, and provide insights in West African corpus linguistics. We demonstrate the use of this corpus in a simple statistical parametric speech synthesis (SPSS) scenario evaluating it against the related languages from the CMU Wilderness dataset and the Yoruba Lagos-NWU corpus",
    "checked": true,
    "id": "d5ae43312497cd9648e7b2ed3b9370747d3317f0",
    "semantic_title": "developing an open-source corpus of yoruba speech",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ha20_interspeech.html": {
    "title": "ClovaCall: Korean Goal-Oriented Dialog Speech Corpus for Automatic Speech Recognition of Contact Centers",
    "volume": "main",
    "abstract": "Automatic speech recognition (ASR) via call is essential for various applications, including AI for contact center (AICC) services. Despite the advancement of ASR, however, most publicly available call-based speech corpora such as Switchboard are old-fashioned. Also, most existing call corpora are in English and mainly focus on open domain dialog or general scenarios such as audiobooks. Here we introduce a new large-scale Korean call-based speech corpus under a goal-oriented dialog scenario from more than 11,000 people, i.e., ClovaCall corpus. ClovaCall includes approximately 60,000 pairs of a short sentence and its corresponding spoken utterance in a restaurant reservation domain. We validate the effectiveness of our dataset with intensive experiments using two standard ASR models. Furthermore, we release our ClovaCall dataset and baseline source codes to be available via ",
    "checked": true,
    "id": "a216567dac02d770a6ab860984c53ddaa2aab6ac",
    "semantic_title": "clovacall: korean goal-oriented dialog speech corpus for automatic speech recognition of contact centers",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20d_interspeech.html": {
    "title": "LAIX Corpus of Chinese Learner English: Towards a Benchmark for L2 English ASR",
    "volume": "main",
    "abstract": "This paper introduces a corpus of Chinese Learner English containing 82 hours of L2 English speech by Chinese learners from all major dialect regions, collected through mobile apps developed by LAIX Inc. The LAIX corpus was created to serve as a benchmark dataset for evaluating Automatic Speech Recognition (ASR) performance on L2 English, the first of this kind as far as we know. The paper describes our effort to build the corpus, including corpus design, data selection and transcription. Multiple rounds of quality check were conducted in the transcription process. Transcription errors were analyzed in terms of error types, rounds of reviewing, and learners' proficiency levels. Word error rates of state-of-the-art ASR systems on the benchmark corpus were also reported",
    "checked": true,
    "id": "3c9e473a3367e0346ce97751a24b5ba77f21ca8e",
    "semantic_title": "laix corpus of chinese learner english: towards a benchmark for l2 english asr",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ramanarayanan20_interspeech.html": {
    "title": "Design and Development of a Human-Machine Dialog Corpus for the Automated Assessment of Conversational English Proficiency",
    "volume": "main",
    "abstract": "This paper presents a carefully designed corpus of scored spoken conversations between English language learners and a dialog system to facilitate research and development of both human and machine scoring of dialog interactions. We collected speech, demographic and user experience data from non-native speakers of English who interacted with a virtual boss as part of a workplace pragmatics skill building application. Expert raters then scored the dialogs on a custom rubric encompassing 12 aspects of conversational proficiency as well as an overall holistic performance score. We analyze key corpus statistics and discuss the advantages of such a corpus for both human and machine scoring",
    "checked": true,
    "id": "23d00bf0fc60f7a1745c20b21a899cfe64365c21",
    "semantic_title": "design and development of a human-machine dialog corpus for the automated assessment of conversational english proficiency",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ng20_interspeech.html": {
    "title": "CUCHILD: A Large-Scale Cantonese Corpus of Child Speech for Phonology and Articulation Assessment",
    "volume": "main",
    "abstract": "This paper describes the design and development of CUCHILD, a large-scale Cantonese corpus of child speech. The corpus contains spoken words collected from 1,986 child speakers aged from 3 to 6 years old. The speech materials include 130 words of 1 to 4 syllables in length. The speakers cover both typically developing (TD) children and children with speech disorder. The intended use of the corpus is to support scientific and clinical research, as well as technology development related to child speech assessment. The design of the corpus, including selection of words, participants recruitment, data acquisition process, and data pre-processing are described in detail. The results of acoustical analysis are presented to illustrate the properties of child speech. Potential applications of the corpus in automatic speech recognition, phonological error detection and speaker diarization are also discussed",
    "checked": true,
    "id": "bc3146e04dcef351442febdb76549c56d69273f1",
    "semantic_title": "cuchild: a large-scale cantonese corpus of child speech for phonology and articulation assessment",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2020/leino20_interspeech.html": {
    "title": "FinChat: Corpus and Evaluation Setup for Finnish Chat Conversations on Everyday Topics",
    "volume": "main",
    "abstract": "Creating open-domain chatbots requires large amounts of conversational data and related benchmark tasks to evaluate them. Standardized evaluation tasks are crucial for creating automatic evaluation metrics for model development; otherwise, comparing the models would require resource-expensive human evaluation. While chatbot challenges have recently managed to provide a plethora of such resources for English, resources in other languages are not yet available. In this work, we provide a starting point for Finnish open-domain chatbot research. We describe our collection efforts to create the Finnish chat conversation corpus FinChat, which is made available publicly. FinChat includes unscripted conversations on seven topics from people of different ages. Using this corpus, we also construct a retrieval-based evaluation task for Finnish chatbot development. We observe that off-the-shelf chatbot models trained on conversational corpora do not perform better than chance at choosing the right answer based on automatic metrics, while humans can do the same task almost perfectly. Similarly, in a human evaluation, responses to questions from the evaluation set generated by the chatbots are predominantly marked as incoherent. Thus, FinChat provides a challenging evaluation set, meant to encourage chatbot development in Finnish",
    "checked": true,
    "id": "6f9c117084576eebd4a878318d0d5d293ab9170a",
    "semantic_title": "finchat: corpus and evaluation setup for finnish chat conversations on everyday topics",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2020/segbroeck20_interspeech.html": {
    "title": "DiPCo — Dinner Party Corpus",
    "volume": "main",
    "abstract": "We present a speech data corpus that simulates a \"dinner party\" scenario taking place in an everyday home environment. The corpus was created by recording multiple groups of four Amazon employee volunteers having a natural conversation in English around a dining table. The participants were recorded by a single-channel close-talk microphone and by five far-field 7-microphone array devices positioned at different locations in the recording room. The dataset contains the audio recordings and human labeled transcripts of a total of 10 sessions with a duration between 15 and 45 minutes. The corpus was created to advance in the field of noise robust and distant speech processing and is intended to serve as a public research and benchmarking data set",
    "checked": false,
    "id": "c83eeb918a38a1dadc0c76bc87708a0c6f411d4c",
    "semantic_title": "dipco - dinner party corpus",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20e_interspeech.html": {
    "title": "Learning to Detect Bipolar Disorder and Borderline Personality Disorder with Language and Speech in Non-Clinical Interviews",
    "volume": "main",
    "abstract": "Bipolar disorder (BD) and borderline personality disorder (BPD) are both chronic psychiatric disorders. However, their overlapping symptoms and common comorbidity make it challenging for the clinicians to distinguish the two conditions on the basis of a clinical interview. In this work, we first present a new multi-modal dataset containing interviews involving individuals with BD or BPD being interviewed about a non-clinical topic . We investigate the automatic detection of the two conditions, and demonstrate a good linear classifier that can be learnt using a down-selected set of features from the different aspects of the interviews and a novel approach of summarising these features. Finally, we find that different sets of features characterise BD and BPD, thus providing insights into the difference between the automatic screening of the two conditions",
    "checked": true,
    "id": "4c6ea1e1399f7574bfc4f3b003ae430505059836",
    "semantic_title": "learning to detect bipolar disorder and borderline personality disorder with language and speech in non-clinical interviews",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kirkedal20_interspeech.html": {
    "title": "FT Speech: Danish Parliament Speech Corpus",
    "volume": "main",
    "abstract": "This paper introduces FT Speech, a new speech corpus created from the recorded meetings of the Danish Parliament, otherwise known as the Folketing (FT). The corpus contains over 1,800 hours of transcribed speech by a total of 434 speakers. It is significantly larger in duration, vocabulary, and amount of spontaneous speech than the existing public speech corpora for Danish, which are largely limited to read-aloud and dictation data. We outline design considerations, including the preprocessing methods and the alignment procedure. To evaluate the quality of the corpus, we train automatic speech recognition systems (ASR) on the new resource and compare them to the systems trained on the Danish part of Språkbanken, the largest public ASR corpus for Danish to date. Our baseline results show that we achieve a 14.01 WER on the new corpus. A combination of FT Speech with in-domain language data provides comparable results to models trained specifically on Språkbanken, showing that FT Speech transfers well to this data set. Interestingly, our results demonstrate that the opposite is not the case. This shows that FT Speech provides a valuable resource for promoting research on Danish ASR with more spontaneous speech",
    "checked": true,
    "id": "88094567d6d93d6c95b61f8bddebac554174dd39",
    "semantic_title": "ft speech: danish parliament speech corpus",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2020/duroselle20_interspeech.html": {
    "title": "Metric Learning Loss Functions to Reduce Domain Mismatch in the x-Vector Space for Language Recognition",
    "volume": "main",
    "abstract": "State-of-the-art language recognition systems are based on discriminative embeddings called x-vectors. Channel and gender distortions produce mismatch in such x-vector space where embeddings corresponding to the same language are not grouped in an unique cluster. To control this mismatch, we propose to train the x-vector DNN with metric learning objective functions. Combining a classification loss with the metric learning n-pair loss allows to improve the language recognition performance. Such a system achieves a robustness comparable to a system trained with a domain adaptation loss function but without using the domain information. We also analyze the mismatch due to channel and gender, in comparison to language proximity, in the x-vector space. This is achieved using the Maximum Mean Discrepancy divergence measure between groups of x-vectors. Our analysis shows that using the metric learning loss function reduces gender and channel mismatch in the x-vector space, even for languages only observed on one channel in the train set",
    "checked": true,
    "id": "12769b9cee7e23ea0fbb1645a346dec1b71329d2",
    "semantic_title": "metric learning loss functions to reduce domain mismatch in the x-vector space for language recognition",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20e_interspeech.html": {
    "title": "The XMUSPEECH System for the AP19-OLR Challenge",
    "volume": "main",
    "abstract": "In this paper, we present our XMUSPEECH system for the oriental language recognition (OLR) challenge, AP19-OLR. The challenge this year contained three tasks: (1) short-utterance LID, (2) cross-channel LID, and (3) zero-resource LID. We leveraged the system pipeline from three aspects, including front-end training, back-end processing, and fusion strategy. We implemented many encoder networks for Tasks 1 and 3, such as extended x-vector, multi-task learning x-vector with phonetic information, and our previously presented multi-feature integration structure. Furthermore, our previously proposed length expansion method was used in the test set for Task 1. I-vector systems based on different acoustic features were built for the cross-channel task. For all of three tasks, the same back-end procedure was used for the sake of stability but with different settings for three tasks. Finally, the greedy fusion strategy helped to choose the subsystems to compose the final fusion systems (submitted systems). Cavg values of 0.0263, 0.2813, and 0.1697 from the development set for Task 1, 2, and 3 were obtained from our submitted systems, and we achieved rank 3rd, 3rd, and 1st in the three tasks in this challenge, respectively",
    "checked": true,
    "id": "f57951f0dcf93c6011fe8281ed23fe7639f2a081",
    "semantic_title": "the xmuspeech system for the ap19-olr challenge",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20f_interspeech.html": {
    "title": "On the Usage of Multi-Feature Integration for Speaker Verification and Language Identification",
    "volume": "main",
    "abstract": "In this paper, we study the technology of multiple acoustic feature integration for the applications of Automatic Speaker Verification (ASV) and Language Identification (LID). In contrast to score level fusion, a common method for integrating subsystems built upon various acoustic features, we explore a new integration strategy, which integrates multiple acoustic features based on the x-vector framework. The frame level, statistics pooling level, segment level, and embedding level integrations are investigated in this study. Our results indicate that frame level integration of multiple acoustic features achieves the best performance in both speaker and language recognition tasks, and the multi-feature integration strategy can be generalized in both classification tasks. Furthermore, we introduce a time-restricted attention mechanism into the frame level integration structure to further improve the performance of multi-feature integration. The experiments are conducted on VoxCeleb 1 for ASV and AP-OLR-17 for LID, and we achieve 28% and 19% relative improvement in terms of Equal Error Rate (EER) in ASV and LID tasks, respectively",
    "checked": true,
    "id": "e133476a758af69f2862de51001db51664de176d",
    "semantic_title": "on the usage of multi-feature integration for speaker verification and language identification",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chowdhury20_interspeech.html": {
    "title": "What Does an End-to-End Dialect Identification Model Learn About Non-Dialectal Information?",
    "volume": "main",
    "abstract": "An end-to-end dialect identification system generates the likelihood of each dialect, given a speech utterance. The performance relies on its capabilities to discriminate the acoustic properties between the different dialects, even though the input signal contains non-dialectal information such as speaker and channel. In this work, we study how non-dialectal information are encoded inside the end-to-end dialect identification model. We design several proxy tasks to understand the model's ability to represent speech input for differentiating non-dialectal information — such as (a) gender and voice identity of speakers, (b) languages, (c) channel (recording and transmission) quality — and compare with dialectal information (i.e., predicting geographic region of the dialects). By analyzing non-dialectal representations from layers of an end-to-end Arabic dialect identification (ADI) model, we observe that the model retains gender and channel information throughout the network while learning a speaker-invariant representation. Our findings also suggest that the CNN layers of the end-to-end model mirror feature extractors capturing voice-specific information, while the fully-connected layers encode more dialectal information",
    "checked": true,
    "id": "92479e46b15740410dc445be548d67ab66b47e42",
    "semantic_title": "what does an end-to-end dialect identification model learn about non-dialectal information?",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lindgren20_interspeech.html": {
    "title": "Releasing a Toolkit and Comparing the Performance of Language Embeddings Across Various Spoken Language Identification Datasets",
    "volume": "main",
    "abstract": "In this paper, we propose a software toolkit for easier end-to-end training of deep learning based spoken language identification models across several speech datasets. We apply our toolkit to implement three baseline models, one speaker recognition model, and three x-vector architecture variations, which are trained on three datasets previously used in spoken language identification experiments. All models are trained separately on each dataset (closed task) and on a combination of all datasets (open task), after which we compare if the open task training yields better language embeddings. We begin by training all models end-to-end as discriminative classifiers of spectral features, labeled by language. Then, we extract language embedding vectors from the trained end-to-end models, train separate Gaussian Naive Bayes classifiers on the vectors, and compare which model provides best language embeddings for the backend classifier. Our experiments show that the open task condition leads to improved language identification performance on only one of the datasets. In addition, we discovered that increasing x-vector model robustness with random frequency channel dropout significantly reduces its end-to-end classification performance on the test set, while not affecting back-end classification performance of its embeddings. Finally, we note that two baseline models consistently outperformed all other models",
    "checked": true,
    "id": "a7ecb2e3ec61fe56a952a467c767f2950e3b9553",
    "semantic_title": "releasing a toolkit and comparing the performance of language embeddings across various spoken language identification datasets",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/alvarez20_interspeech.html": {
    "title": "Learning Intonation Pattern Embeddings for Arabic Dialect Identification",
    "volume": "main",
    "abstract": "This article presents a full end-to-end pipeline for Arabic Dialect Identification (ADI) using intonation patterns and acoustic representations. Recent approaches to language and dialect identification use linguistic-aware deep architectures that are able to capture phonetic differences amongst languages and dialects. Specifically, in ADI tasks, different combinations of linguistic features and acoustic representations have been successful with deep learning models. The approach presented in this article uses intonation patterns and hybrid residual and bidirectional LSTM networks to learn acoustic embeddings with no additional linguistic information. Results of the experiments show that intonation patterns for Arabic dialects provide sufficient information to achieve state-of-the-art results on the VarDial 17 ADI dataset, outperforming single-feature systems. The pipeline presented is robust to data sparsity, in contrast to other deep learning approaches that require large quantities of data. We conjecture on the importance of sufficient information as a criterion for optimality in a deep learning ADI task, and more generally, its application to acoustic modeling problems. Small intonation patterns, when sufficient in an information-theoretic sense, allow deep learning architectures to learn more accurate speech representations",
    "checked": true,
    "id": "cb21a1abb4cade1e53e01603ed87d5aaa333d7e9",
    "semantic_title": "learning intonation pattern embeddings for arabic dialect identification",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2020/abdullah20_interspeech.html": {
    "title": "Cross-Domain Adaptation of Spoken Language Identification for Related Languages: The Curious Case of Slavic Languages",
    "volume": "main",
    "abstract": "State-of-the-art spoken language identification (LID) systems, which are based on end-to-end deep neural networks, have shown remarkable success not only in discriminating between distant languages but also between closely-related languages or even different spoken varieties of the same language. However, it is still unclear to what extent neural LID models generalize to speech samples with different acoustic conditions due to domain shift. In this paper, we present a set of experiments to investigate the impact of domain mismatch on the performance of neural LID systems for a subset of six Slavic languages across two domains (read speech and radio broadcast) and examine two low-level signal descriptors (spectral and cepstral features) for this task. Our experiments show that (1) out-of-domain speech samples severely hinder the performance of neural LID models, and (2) while both spectral and cepstral features show comparable performance within-domain, spectral features show more robustness under domain mismatch. Moreover, we apply unsupervised domain adaptation to minimize the discrepancy between the two domains in our study. We achieve relative accuracy improvements that range from 9% to 77% depending on the diversity of acoustic conditions in the source domain",
    "checked": true,
    "id": "bcd4c5509f7e97ee9a47574308673389e406f44f",
    "semantic_title": "cross-domain adaptation of spoken language identification for related languages: the curious case of slavic languages",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tits20_interspeech.html": {
    "title": "ICE-Talk: An Interface for a Controllable Expressive Talking Machine",
    "volume": "main",
    "abstract": "ICE-Talk is an open source web-based GUI that allows the use of a TTS system with controllable parameters via a text field and a clickable 2D plot. It enables the study of latent spaces for controllable TTS. Moreover it is implemented as a module that can be used as part of a Human-Agent interaction",
    "checked": true,
    "id": "d2807e907e53a3a220ba020d200f1c4e0cb10b94",
    "semantic_title": "ice-talk: an interface for a controllable expressive talking machine",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hu20b_interspeech.html": {
    "title": "Kaldi-Web: An Installation-Free, On-Device Speech Recognition System",
    "volume": "main",
    "abstract": "Speech provides an intuitive interface to communicate with machines. Today, developers willing to implement such an interface must either rely on third-party proprietary software or become experts in speech recognition. Conversely, researchers in speech recognition wishing to demonstrate their results need to be familiar with technologies that are not relevant to their research (e.g., graphical user interface libraries). In this demo, we introduce Kaldi-web : an open-source, cross-platform tool which bridges this gap by providing a user interface built around the online decoder of the Kaldi toolkit. Additionally, because we compile Kaldi to Web Assembly, speech recognition is performed directly in web browsers. This addresses privacy issues as no data is transmitted to the network for speech recognition",
    "checked": true,
    "id": "2e514ca531039f70c0c1fa66698716ddb5492fbf",
    "semantic_title": "kaldi-web: an installation-free, on-device speech recognition system",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kelly20_interspeech.html": {
    "title": "Soapbox Labs Verification Platform for Child Speech",
    "volume": "main",
    "abstract": "SoapBox Labs' child speech verification platform is a service designed specifically for identifying keywords and phrases in children's speech. Given an audio file containing children's speech and one or more target keywords or phrases, the system will return the confidence score of recognition for the word(s) or phrase(s) within the the audio file. The confidence scores are provided at utterance level, word level and phoneme level. The service is available online through an cloud API service, or offline on Android and iOS. The platform is accurate for child speech from children as young as 3, and is robust to noisy environments. In this demonstration we show how to access the online API and give some examples of common use cases in literacy and language learning, gaming and robotics",
    "checked": true,
    "id": "02a4284e70775516073d386c4f189395eb1162f1",
    "semantic_title": "soapbox labs verification platform for child speech",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kelly20b_interspeech.html": {
    "title": "SoapBox Labs Fluency Assessment Platform for Child Speech",
    "volume": "main",
    "abstract": "The SoapBox Labs Fluency API service allows the automatic assessment of a child's reading fluency. The system uses automatic speech recognition (ASR) to transcribe the child's speech as they read a passage. The ASR output is then compared to the text of the reading passage, and the fluency algorithm returns information about the accuracy of the child's reading attempt. In this show and tell paper we describe how the fluency cloud API is accessed and demonstrate how the fluency demo system processes an audio file, as shown in the accompanying video",
    "checked": true,
    "id": "299984cf309a78a11bde031176006d0187d58417",
    "semantic_title": "soapbox labs fluency assessment platform for child speech",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kulebi20_interspeech.html": {
    "title": "CATOTRON — A Neural Text-to-Speech System in Catalan",
    "volume": "main",
    "abstract": "We present Catotron, a neural network-based open-source speech synthesis system in Catalan. Catotron consists of a sequence-to-sequence model trained with two small open-source datasets based on semi-spontaneous and read speech. We demonstrate how a neural TTS can be built for languages with limited resources using found-data optimization and cross-lingual transfer learning. We make the datasets, initial models and source code publicly available for both commercial and research purposes",
    "checked": false,
    "id": "7506f739f1043f93da479c9bd7055f7a8a83eabe",
    "semantic_title": "catotron - a neural text-to-speech system in catalan",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ramanarayanan20b_interspeech.html": {
    "title": "Toward Remote Patient Monitoring of Speech, Video, Cognitive and Respiratory Biomarkers Using Multimodal Dialog Technology",
    "volume": "main",
    "abstract": "We demonstrate a multimodal conversational platform for remote patient diagnosis and monitoring. The platform engages patients in an interactive dialog session and automatically computes metrics relevant to speech acoustics and articulation, oro-motor and oro-facial movement, cognitive function and respiratory function. The dialog session includes a selection of exercises that have been widely used in both speech language pathology research as well as clinical practice — an oral motor exam, sustained phonation, diadochokinesis, read speech, spontaneous speech, spirometry, picture description, emotion elicitation and other cognitive tasks. Finally, the system automatically computes speech, video, cognitive and respiratory biomarkers that have been shown to be useful in capturing various aspects of speech motor function and neurological health and visualizes them in a user-friendly dashboard",
    "checked": true,
    "id": "1d9f50a8f3deb38a4fc2d71636149912594c58ef",
    "semantic_title": "toward remote patient monitoring of speech, video, cognitive and respiratory biomarkers using multimodal dialog technology",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lin20b_interspeech.html": {
    "title": "VoiceID on the Fly: A Speaker Recognition System that Learns from Scratch",
    "volume": "main",
    "abstract": "We proposed a novel AI framework to conduct real-time multi-speaker recognition without any prior registration or pretraining by learning the speaker identification on the fly. We considered the practical problem of online learning with episodically revealed rewards and introduced a solution based on semi-supervised and self-supervised learning methods in a web-based application at ",
    "checked": true,
    "id": "f32abeeaa7dbf66d472a9d798aa045dddd0a6322",
    "semantic_title": "voiceid on the fly: a speaker recognition system that learns from scratch",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ren20_interspeech.html": {
    "title": "Enhancing Transferability of Black-Box Adversarial Attacks via Lifelong Learning for Speech Emotion Recognition Models",
    "volume": "main",
    "abstract": "Well-designed adversarial examples can easily fool deep speech emotion recognition models into misclassifications. The transferability of adversarial attacks is a crucial evaluation indicator when generating adversarial examples to fool a new target model or multiple models. Herein, we propose a method to improve the transferability of black-box adversarial attacks using lifelong learning. First, black-box adversarial examples are generated by an atrous Convolutional Neural Network (CNN) model. This initial model is trained to attack a CNN target model. Then, we adapt the trained atrous CNN attacker to a new CNN target model using lifelong learning. We use this paradigm, as it enables multi-task sequential learning, which saves more memory space than conventional multi-task learning. We verify this property on an emotional speech database, by demonstrating that the updated atrous CNN model can attack all target models which have been learnt, and can better attack a new target model than an attack model trained on one target model only",
    "checked": true,
    "id": "161534164e5dd1d1d3970d707850c147daaf697a",
    "semantic_title": "enhancing transferability of black-box adversarial attacks via lifelong learning for speech emotion recognition models",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2020/feng20_interspeech.html": {
    "title": "End-to-End Speech Emotion Recognition Combined with Acoustic-to-Word ASR Model",
    "volume": "main",
    "abstract": "In this paper, we propose speech emotion recognition (SER) combined with an acoustic-to-word automatic speech recognition (ASR) model. While acoustic prosodic features are primarily used for SER, textual features are also useful but are error-prone, especially in emotional speech. To solve this problem, we integrate ASR model and SER model in an end-to-end manner. This is done by using an acoustic-to-word model. Specifically, we utilize the states of the decoder in the ASR model with the acoustic features and input them into the SER model. On top of a recurrent network to learn features from this input, we adopt a self-attention mechanism to focus on important feature frames. Finally, we finetune the ASR model on the new dataset using a multi-task learning method to jointly optimize ASR with the SER task. Our model has achieved a 68.63% weighted accuracy (WA) and 69.67% unweighted accuracy (UA) on the IEMOCAP database, which is state-of-the-art performance",
    "checked": true,
    "id": "7643de22414f5ddc6a23219b67130ecccbe7e24d",
    "semantic_title": "end-to-end speech emotion recognition combined with acoustic-to-word asr model",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2020/su20_interspeech.html": {
    "title": "Improving Speech Emotion Recognition Using Graph Attentive Bi-Directional Gated Recurrent Unit Network",
    "volume": "main",
    "abstract": "The manner that human encodes emotion information within an utterance is often complex and could result in a diverse salient acoustic profile that is conditioned on emotion types. In this work, we propose a framework in imposing a graph attention mechanism on gated recurrent unit network (GA-GRU) to improve utterance-based speech emotion recognition (SER). Our proposed GA-GRU combines both long-range time-series based modeling of speech and further integrates complex saliency using a graph structure. We evaluate our proposed GA-GRU on the IEMOCAP and the MSP-IMPROV database and achieve a 63.8% UAR and 57.47% UAR in a four class emotion recognition task. The GA-GRU obtains consistently better performances as compared to recent state-of-art in per-utterance emotion classification model, and we further observe that different emotion categories would require distinct flexible structures in modeling emotion information in the acoustic data that is beyond conventional left-to-right or vice versa",
    "checked": true,
    "id": "49383e7ddb6fb74e1a2690c67e8df811d7971720",
    "semantic_title": "improving speech emotion recognition using graph attentive bi-directional gated recurrent unit network",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mallolragolta20_interspeech.html": {
    "title": "An Investigation of Cross-Cultural Semi-Supervised Learning for Continuous Affect Recognition",
    "volume": "main",
    "abstract": "One of the keys for supervised learning techniques to succeed resides in the access to vast amounts of labelled training data. The process of data collection, however, is expensive, time-consuming, and application dependent. In the current digital era, data can be collected continuously. This continuity renders data annotation into an endless task, which potentially, in problems such as emotion recognition, requires annotators with different cultural backgrounds. Herein, we study the impact of utilising data from different cultures in a semi-supervised learning approach to label training material for the automatic recognition of arousal and valence. Specifically, we compare the performance of culture-specific affect recognition models trained with manual or cross-cultural automatic annotations. The experiments performed in this work use the dataset released for the Cross-cultural Emotion Sub-challenge of the Audio/Visual Emotion Challenge (AVEC) 2019. The results obtained convey that the cultures used for training impact on the system performance. Furthermore, in most of the scenarios assessed, affect recognition models trained with hybrid solutions, combining manual and automatic annotations, surpass the baseline model, which was exclusively trained with manual annotations",
    "checked": true,
    "id": "9a67b2343c47a93fd534633b98d187634a7aea59",
    "semantic_title": "an investigation of cross-cultural semi-supervised learning for continuous affect recognition",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2020/sridhar20_interspeech.html": {
    "title": "Ensemble of Students Taught by Probabilistic Teachers to Improve Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Reliable and generalizable speech emotion recognition (SER) systems have wide applications in various fields including healthcare, customer service, and security and defense. Towards this goal, this study presents a novel teacher-student (T-S) framework for SER, relying on an ensemble of probabilistic predictions of teacher embeddings to train an ensemble of students. We use uncertainty modeling with Monte-Carlo (MC) dropout to create a distribution for the embeddings of an intermediate dense layer of the teacher. The embeddings guiding the student models are derived by sampling from this distribution. The final prediction combines the results obtained by the student ensemble. The proposed model not only increases the prediction performance over the teacher model, but also generates more consistent predictions. As a T-S formulation, the approach allows the use of unlabeled data to improve the performance of the students in a semi-supervised manner. An ablation analysis shows the importance of the MC-based ensemble and the use of unlabeled data. The results show relative improvements in concordance correlation coefficient (CCC) up to 4.25% for arousal, 2.67% for valence and 4.98% for dominance from their baseline results. The results also show that the student ensemble decreases the uncertainty in the predictions, leading to more consistent results",
    "checked": true,
    "id": "b49ac7589181422587eebb3990a416396196894f",
    "semantic_title": "ensemble of students taught by probabilistic teachers to improve speech emotion recognition",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2020/latif20_interspeech.html": {
    "title": "Augmenting Generative Adversarial Networks for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Generative adversarial networks (GANs) have shown potential in learning emotional attributes and generating new data samples. However, their performance is usually hindered by the unavailability of larger speech emotion recognition (SER) data. In this work, we propose a framework that utilises the mixup data augmentation scheme to augment the GAN in feature learning and generation. To show the effectiveness of the proposed framework, we present results for SER on (i) synthetic feature vectors, (ii) augmentation of the training data with synthetic features, (iii) encoded features in compressed representation. Our results show that the proposed framework can effectively learn compressed emotional representations as well as it can generate synthetic samples that help improve performance in within-corpus and cross-corpus evaluation",
    "checked": true,
    "id": "9f5f3ce04ea6f559ff4a82e5449a4e4717ecdac5",
    "semantic_title": "augmenting generative adversarial networks for speech emotion recognition",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2020/dissanayake20_interspeech.html": {
    "title": "Speech Emotion Recognition ‘in the Wild' Using an Autoencoder",
    "volume": "main",
    "abstract": "Speech Emotion Recognition (SER) has been a challenging task on which researchers have been working for decades. Recently, Deep Learning (DL) based approaches have been shown to perform well in SER tasks; however, it has been noticed that their superior performance is limited to the distribution of the data used to train the model. In this paper, we present an analysis of using autoencoders to improve the generalisability of DL based SER solutions. We train a sparse autoencoder using a large speech corpus extracted from social media. Later, the trained encoder part of the autoencoder is reused as the input to a long short-term memory (LSTM) network, and the encoder-LSTM modal is re-trained on an aggregation of five commonly used speech emotion corpora. Our evaluation uses an unseen corpus in the training & validation stages to simulate ‘in the wild' condition and analyse the generalisability of our solution. A performance comparison is carried out between the encoder based model and a model trained without an encoder. Our results show that the autoencoder based model improves the unweighted accuracy of the unseen corpus by 8%, indicating autoencoder based pre-training can improve the generalisability of DL based SER solutions",
    "checked": false,
    "id": "948e47fb86a41dccb338ab10d79aa4b9f7fcd4de",
    "semantic_title": "speech emotion recognition 'in the wild' using an autoencoder",
    "citation_count": 23
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mao20_interspeech.html": {
    "title": "Emotion Profile Refinery for Speech Emotion Classification",
    "volume": "main",
    "abstract": "Human emotions are inherently ambiguous and impure. When designing systems to anticipate human emotions based on speech, the lack of emotional purity must be considered. However, most of the current methods for speech emotion classification rest on the consensus, e. g., one single hard label for an utterance. This labeling principle imposes challenges for system performance considering emotional impurity. In this paper, we recommend the use of emotional profiles (EPs), which provides a time series of segment-level soft labels to capture the subtle blends of emotional cues present across a specific speech utterance. We further propose the emotion profile refinery (EPR), an iterative procedure to update EPs. The EPR method produces soft, dynamically-generated, multiple probabilistic class labels during successive stages of refinement, which results in significant improvements in the model accuracy. Experiments on three well-known emotion corpora show noticeable gain using the proposed method",
    "checked": true,
    "id": "4640e84e19aa969f9133a9f7783d90253b5b80c6",
    "semantic_title": "emotion profile refinery for speech emotion classification",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yeh20_interspeech.html": {
    "title": "Speech Representation Learning for Emotion Recognition Using End-to-End ASR with Factorized Adaptation",
    "volume": "main",
    "abstract": "Developing robust speech emotion recognition (SER) systems is challenging due to small-scale of existing emotional speech datasets. However, previous works have mostly relied on handcrafted acoustic features to build SER models that are difficult to handle a wide range of acoustic variations. One way to alleviate this problem is by using speech representations learned from deep end-to-end models trained on large-scale speech database. Specifically, in this paper, we leverage an end-to-end ASR to extract ASR-based representations for speech emotion recognition. We further devise a factorized domain adaptation approach on the pre-trained ASR model to improve both the speech recognition rate and the emotion recognition accuracy on the target emotion corpus, and we also provide an analysis in the effectiveness of representations extracted from different ASR layers. Our experiments demonstrate the importance of ASR adaptation and layer depth for emotion recognition",
    "checked": true,
    "id": "c09f00730abf867c8f16c7009df679075fe8eafa",
    "semantic_title": "speech representation learning for emotion recognition using end-to-end asr with factorized adaptation",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kumar20_interspeech.html": {
    "title": "Fast and Slow Acoustic Model",
    "volume": "main",
    "abstract": "In this work we layout a Fast & Slow (F&S) acoustic model (AM) in an encoder-decoder architecture for streaming automatic speech recognition (ASR). The Slow model represents our baseline ASR model; it's significantly larger than Fast model and provides stronger accuracy. The Fast model is generally developed for related speech applications. It has weaker ASR accuracy but is faster to evaluate and consequently leads to better user-perceived latency. We propose a joint F&S model that encodes output state information from Fast model, feeds that to Slow model to improve overall model accuracy from F&S AM. We demonstrate scenarios where individual Fast and Slow models are already available to build the joint F&S model. We apply our work on a large vocabulary ASR task. Compared to Slow AM, our Fast AM is 3–4× smaller and 11.5% relatively weaker in ASR accuracy. The proposed F&S AM achieves 4.7% relative gain over the Slow AM. We also report a progression of techniques and improve the relative gain to 8.1% by encoding additional Fast AM outputs. Our proposed framework has generic attributes — we demonstrate a specific extension by encoding two Slow models to achieve 12.2% relative gain",
    "checked": true,
    "id": "103b90fc1cdb02df97157eaec55a81a462fe8f02",
    "semantic_title": "fast and slow acoustic model",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2020/moriya20_interspeech.html": {
    "title": "Self-Distillation for Improving CTC-Transformer-Based ASR Systems",
    "volume": "main",
    "abstract": "We present a novel training approach for encoder-decoder-based sequence-to-sequence (S2S) models. S2S models have been used successfully by the automatic speech recognition (ASR) community. The important key factor of S2S is the attention mechanism as it captures the relationships between input and output sequences. The attention weights inform which time frames should be attended to for predicting the output labels. In previous work, we proposed distilling S2S knowledge into connectionist temporal classification (CTC) based models by using the attention characteristics to create pseudo-targets for an auxiliary cross-entropy loss term. This approach can significantly improve CTC models. However, it remained unclear whether our proposal could be used to improve S2S models. In this paper, we extend our previous work to create a strong S2S model, i.e. Transformer with CTC (CTC-Transformer). We utilize Transformer outputs and the source attention weights for making pseudo-targets that contain both the posterior and the timing information of each Transformer output. These pseudo-targets are used to train the shared encoder of the CTC-Transformer through the use of direct feedback from the Transformer-decoder and thus obtain more informative representations. Experiments on public and private datasets to perform various tasks demonstrate that our proposal is also effective for enhancing S2S model training. In particular, on a Japanese ASR task, our best system outperforms the previous state-of-the-art alternative",
    "checked": true,
    "id": "838259c2b061b4025d42e1cfc89b2e0229d39845",
    "semantic_title": "self-distillation for improving ctc-transformer-based asr systems",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tuske20_interspeech.html": {
    "title": "Single Headed Attention Based Sequence-to-Sequence Model for State-of-the-Art Results on Switchboard",
    "volume": "main",
    "abstract": "It is generally believed that direct sequence-to-sequence (seq2seq) speech recognition models are competitive with hybrid models only when a large amount of data, at least a thousand hours, is available for training. In this paper, we show that state-of-the-art recognition performance can be achieved on the Switchboard-300 database using a single headed attention, LSTM based model. Using a cross-utterance language model, our single-pass speaker independent system reaches 6.4% and 12.5% word error rate (WER) on the Switchboard and CallHome subsets of Hub5'00, without a pronunciation lexicon. While careful regularization and data augmentation are crucial in achieving this level of performance, experiments on Switchboard-2000 show that nothing is more useful than more data. Overall, the combination of various regularizations and a simple but fairly large model results in a new state of the art, 4.8% and 8.3% WER on the Switchboard and CallHome sets, using SWB-2000 without any external data resources",
    "checked": false,
    "id": "21dc7acda83295748ed4416918e9b6b4ad609491",
    "semantic_title": "single headed attention based sequence-to-sequence model for state-of-the-art results on switchboard-300",
    "citation_count": 61
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chen20c_interspeech.html": {
    "title": "Improving Speech Recognition Using GAN-Based Speech Synthesis and Contrastive Unspoken Text Selection",
    "volume": "main",
    "abstract": "Text-to-Speech synthesis (TTS) based data augmentation is a relatively new mechanism for utilizing text-only data to improve automatic speech recognition (ASR) training without parameter or inference architecture changes. However, efforts to train speech recognition systems on synthesized utterances suffer from limited acoustic diversity of TTS outputs. Additionally, the text-only corpus is always much larger than the transcribed speech corpus by several orders of magnitude, which makes speech synthesis of all the text data impractical. In this work, we propose to combine generative adversarial network (GAN) and multi-style training (MTR) to increase acoustic diversity in the synthesized data. We also present a contrastive language model-based data selection technique to improve the efficiency of learning from unspoken text. We demonstrate that our proposed method allows ASR models to learn from synthesis of large-scale unspoken text sources and achieves a 35% relative WER reduction on a voice-search task",
    "checked": true,
    "id": "42d863cb3afccbb0d5564b687e80c7dfe87ce10b",
    "semantic_title": "improving speech recognition using gan-based speech synthesis and contrastive unspoken text selection",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shao20_interspeech.html": {
    "title": "PyChain: A Fully Parallelized PyTorch Implementation of LF-MMI for End-to-End ASR",
    "volume": "main",
    "abstract": "We present PyChain, a fully parallelized PyTorch implementation of end-to-end lattice-free maximum mutual information (LF-MMI) training for the so-called chain models in the Kaldi automatic speech recognition (ASR) toolkit. Unlike other PyTorch and Kaldi based ASR toolkits, PyChain is designed to be as flexible and light-weight as possible so that it can be easily plugged into new ASR projects, or other existing PyTorch-based ASR tools, as exemplified respectively by a new project PyChain-example, and Espresso, an existing end-to-end ASR toolkit. PyChain's efficiency and flexibility is demonstrated through such novel features as full GPU training on numerator/denominator graphs, and support for unequal length sequences. Experiments on the WSJ dataset show that with simple neural networks and commonly used machine learning techniques, PyChain can achieve competitive results that are comparable to Kaldi and better than other end-to-end ASR systems",
    "checked": true,
    "id": "b861122b8b5006093756182dbcbf4109b2d8e42d",
    "semantic_title": "pychain: a fully parallelized pytorch implementation of lf-mmi for end-to-end asr",
    "citation_count": 28
  },
  "https://www.isca-speech.org/archive/interspeech_2020/an20_interspeech.html": {
    "title": "CAT: A CTC-CRF Based ASR Toolkit Bridging the Hybrid and the End-to-End Approaches Towards Data Efficiency and Low Latency",
    "volume": "main",
    "abstract": "In this paper, we present a new open source toolkit for speech recognition, named CAT (CTC-CRF based ASR Toolkit). CAT inherits the data-efficiency of the hybrid approach and the simplicity of the E2E approach, providing a full-fledged implementation of CTC-CRFs and complete training and testing scripts for a number of English and Chinese benchmarks. Experiments show CAT obtains state-of-the-art results, which are comparable to the fine-tuned hybrid models in Kaldi but with a much simpler training pipeline. Compared to existing non-modularized E2E models, CAT performs better on limited-scale datasets, demonstrating its data efficiency. Furthermore, we propose a new method called contextualized soft forgetting, which enables CAT to do streaming ASR without accuracy degradation. We hope CAT, especially the CTC-CRF based framework and software, will be of broad interest to the community, and can be further explored and improved",
    "checked": true,
    "id": "75fe456e3a740a0f300b5a546ceeb1412ebc87cb",
    "semantic_title": "cat: a ctc-crf based asr toolkit bridging the hybrid and the end-to-end approaches towards data efficiency and low latency",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2020/inaguma20_interspeech.html": {
    "title": "CTC-Synchronous Training for Monotonic Attention Model",
    "volume": "main",
    "abstract": "Monotonic chunkwise attention (MoChA) has been studied for the online streaming automatic speech recognition (ASR) based on a sequence-to-sequence framework. In contrast to connectionist temporal classification (CTC), backward probabilities cannot be leveraged in the alignment marginalization process during training due to left-to-right dependency in the decoder. This results in the error propagation of alignments to subsequent token generation. To address this problem, we propose CTC-synchronous training (CTC-ST), in which MoChA uses CTC alignments to learn optimal monotonic alignments. Reference CTC alignments are extracted from a CTC branch sharing the same encoder with the decoder. The entire model is jointly optimized so that the expected boundaries from MoChA are synchronized with the alignments. Experimental evaluations of the TEDLIUM release-2 and Librispeech corpora show that the proposed method significantly improves recognition, especially for long utterances. We also show that CTC-ST can bring out the full potential of SpecAugment for MoChA",
    "checked": true,
    "id": "39adbd03530b2df771fc821132100a2ca843341d",
    "semantic_title": "ctc-synchronous training for monotonic attention model",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2020/houston20_interspeech.html": {
    "title": "Continual Learning for Multi-Dialect Acoustic Models",
    "volume": "main",
    "abstract": "Using data from multiple dialects has shown promise in improving neural network acoustic models. While such training can improve the performance of an acoustic model on a single dialect, it can also produce a model capable of good performance on multiple dialects. However, training an acoustic model on pooled data from multiple dialects takes a significant amount of time and computing resources, and it needs to be retrained every time a new dialect is added to the model. In contrast, sequential transfer learning (fine-tuning) does not require retraining using all data, but may result in catastrophic forgetting of previously-seen dialects. Using data from four english dialects, we demonstrate that by using loss functions that mitigate catastrophic forgetting, sequential transfer learning can be used to train multi-dialect acoustic models that narrow the WER gap between the best (combined training) and worst (fine-tuning) case by up to 65%. Continual learning shows great promise in minimizing training time while approaching the performance of models that require much more training time",
    "checked": true,
    "id": "b5a090fac7c7db8dd7b7669ddaa0f306f085873b",
    "semantic_title": "continual learning for multi-dialect acoustic models",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2020/song20b_interspeech.html": {
    "title": "SpecSwap: A Simple Data Augmentation Method for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "Recently, End-to-End (E2E) models have achieved state-of-the-art performance for automatic speech recognition (ASR). Within these large and deep models, overfitting remains an important problem that heavily influences the model performance. One solution to deal with the overfitting problem is to increase the quantity and variety of the training data with the help of data augmentation. In this paper, we present SpecSwap, a simple data augmentation scheme for automatic speech recognition that acts directly on the spectrogram of input utterances. The augmentation policy consists of swapping blocks of frequency channels and swapping blocks of time steps. We apply SpecSwap on Transformer-based networks for end-to-end speech recognition task. Our experiments on Aishell-1 show state-of-the-art performance for E2E models that are trained solely on the speech training data. Further, by increasing the depth of model, the Transformers trained with augmentations can outperform certain hybrid systems, even without the aid of a language model",
    "checked": true,
    "id": "bfa2005b148bd822465f148995855ee8537a733a",
    "semantic_title": "specswap: a simple data augmentation method for end-to-end speech recognition",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2020/stan20_interspeech.html": {
    "title": "RECOApy: Data Recording, Pre-Processing and Phonetic Transcription for End-to-End Speech-Based Applications",
    "volume": "main",
    "abstract": "Deep learning enables the development of efficient end-to-end speech processing applications while bypassing the need for expert linguistic and signal processing features. Yet, recent studies show that good quality speech resources and phonetic transcription of the training data can enhance the results of these applications. In this paper, the RECOApy tool is introduced. RECOApy streamlines the steps of data recording and pre-processing required in end-to-end speech-based applications. The tool implements an easy-to-use interface for prompted speech recording, spectrogram and waveform analysis, utterance-level normalisation and silence trimming, as well grapheme-to-phoneme conversion of the prompts in eight languages: Czech, English, French, German, Italian, Polish, Romanian and Spanish The grapheme-to-phoneme (G2P) converters are deep neural network (DNN) based architectures trained on lexicons extracted from the Wiktionary online collaborative resource. With the different degree of orthographic transparency, as well as the varying amount of phonetic entries across the languages, the DNN's hyperparameters are optimised with an evolution strategy. The phoneme and word error rates of the resulting G2P converters are presented and discussed. The tool, the processed phonetic lexicons and trained G2P models are made freely available",
    "checked": true,
    "id": "39ffae91be0ff12196f967a99dcfff9d194f9531",
    "semantic_title": "recoapy: data recording, pre-processing and phonetic transcription for end-to-end speech-based applications",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shangguan20_interspeech.html": {
    "title": "Analyzing the Quality and Stability of a Streaming End-to-End On-Device Speech Recognizer",
    "volume": "main",
    "abstract": "The demand for fast and accurate incremental speech recognition increases as the applications of automatic speech recognition (ASR) proliferate. Incremental speech recognizers output chunks of partially recognized words while the user is still talking. Partial results can be revised before the ASR finalizes its hypothesis, causing instability issues. We analyze the quality and stability of on-device streaming end-to-end (E2E) ASR models. We first introduce a novel set of metrics that quantify the instability at word and segment levels. We study the impact of several model training techniques that improve E2E model qualities but degrade model stability. We categorize the causes of instability and explore various solutions to mitigate them in a streaming E2E ASR system",
    "checked": true,
    "id": "0bc8ca164372265e9cacbfe3148d06cb5e211f3d",
    "semantic_title": "analyzing the quality and stability of a streaming end-to-end on-device speech recognizer",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2020/liu20c_interspeech.html": {
    "title": "Statistical Testing on ASR Performance via Blockwise Bootstrap",
    "volume": "main",
    "abstract": "A common question being raised in automatic speech recognition (ASR) evaluations is how reliable is an observed word error rate (WER) improvement comparing two ASR systems, where statistical hypothesis testing and confidence interval (CI) can be utilized to tell whether this improvement is real or only due to random chance. The bootstrap resampling method has been popular for such significance analysis which is intuitive and easy to use. However, this method fails in dealing with dependent data, which is prevalent in speech world — for example, ASR performance on utterances from the same speaker could be correlated. In this paper we present blockwise bootstrap approach — by dividing evaluation utterances into nonoverlapping blocks, this method resamples these blocks instead of original data. We show that the resulting variance estimator of absolute WER difference between two ASR systems is consistent under mild conditions. We also demonstrate the validity of blockwise bootstrap method on both synthetic and real-world speech data",
    "checked": true,
    "id": "21ff0af7bdd58cc8b073c85e172cfeffc4cef613",
    "semantic_title": "statistical testing on asr performance via blockwise bootstrap",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ramakrishna20_interspeech.html": {
    "title": "Sentence Level Estimation of Psycholinguistic Norms Using Joint Multidimensional Annotations",
    "volume": "main",
    "abstract": "Psycholinguistic normatives represent various affective and mental constructs using numeric scores and are used in a variety of applications in natural language processing. They are commonly used at the sentence level, the scores of which are estimated by extrapolating word level scores using simple aggregation strategies, which may not always be optimal. In this work, we present a novel approach to estimate the psycholinguistic norms at sentence level. We apply a multidimensional annotation fusion model on annotations at the word level to estimate a parameter which captures relationships between different norms. We then use this parameter at sentence level to estimate the norms. We evaluate our approach by predicting sentence level scores for various normative dimensions and compare with standard word aggregation schemes",
    "checked": true,
    "id": "e36577f5b907b60bcb859eb089e00cbe120540b6",
    "semantic_title": "sentence level estimation of psycholinguistic norms using joint multidimensional annotations",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/fan20_interspeech.html": {
    "title": "Neural Zero-Inflated Quality Estimation Model for Automatic Speech Recognition System",
    "volume": "main",
    "abstract": "The performances of automatic speech recognition (ASR) systems are usually evaluated by the metric word error rate (WER) when the manually transcribed data are provided, which are, however, expensively available in the real scenario. In addition, the empirical distribution of WER for most ASR systems usually tends to put a significant mass near zero, making it difficult to simulate with a single continuous distribution. In order to address the two issues of ASR quality estimation (QE), we propose a novel neural zero-inflated model to predict the WER of the ASR result without transcripts. We design a neural zero-inflated beta regression on top of a bidirectional transformer language model conditional on speech features (speech-BERT). We adopt the pre-training strategy of token level masked language modeling for speech-BERT as well, and further fine-tune with our zero-inflated layer for the mixture of discrete and continuous outputs. The experimental results show that our approach achieves better performance on WER prediction compared with strong baselines",
    "checked": true,
    "id": "8d54d9fbe12ceea95abe4421e87f5691b5c6ca27",
    "semantic_title": "neural zero-inflated quality estimation model for automatic speech recognition system",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2020/woodward20_interspeech.html": {
    "title": "Confidence Measures in Encoder-Decoder Models for Speech Recognition",
    "volume": "main",
    "abstract": "Recent improvements in Automatic Speech Recognition (ASR) systems have enabled the growth of myriad applications such as voice assistants, intent detection, keyword extraction and sentiment analysis. These applications, which are now widely used in the industry, are very sensitive to the errors generated by ASR systems. This could be overcome by having a reliable confidence measurement associated to the predicted output. This work presents a novel method which uses internal neural features of a frozen ASR model to train an independent neural network to predict a softmax temperature value. This value is computed in each decoder time step and multiplied by the logits in order to redistribute the output probabilities. The resulting softmax values corresponding to predicted tokens constitute a more reliable confidence measure. Moreover, this work also studies the effect of teacher forcing on the training of the proposed temperature prediction module. The output confidence estimation shows an improvement of -25.78% in EER and +7.59% in AUC-ROC with respect to the unaltered softmax values of the predicted tokens, evaluated on a proprietary dataset consisting on News and Entertainment videos",
    "checked": true,
    "id": "0fe72f4614ab9c8be3150213dfc3cc7d7d2ef3c7",
    "semantic_title": "confidence measures in encoder-decoder models for speech recognition",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ali20_interspeech.html": {
    "title": "Word Error Rate Estimation Without ASR Output: e-WER2",
    "volume": "main",
    "abstract": "Measuring the performance of automatic speech recognition (ASR) systems requires manually transcribed data in order to compute the word error rate (WER), which is often time-consuming and expensive. In this paper, we continue our effort in estimating WER using acoustic, lexical and phonotactic features. Our novel approach to estimate the WER uses a multistream end-to-end architecture. We report results for systems using internal speech decoder features (glass-box), systems without speech decoder features (black-box), and for systems without having access to the ASR system (no-box). The no-box system learns joint acoustic-lexical representation from phoneme recognition results along with MFCC acoustic features to estimate WER. Considering WER per sentence, our no-box system achieves 0.56 Pearson correlation with the reference evaluation and 0.24 root mean square error (RMSE) across 1,400 sentences. The estimated overall WER by e-WER2 is 30.9% for a three hours test set, while the WER computed using the reference transcriptions was 28.5%",
    "checked": true,
    "id": "a2c1dc798e1606f07874457dc1e7f819616715eb",
    "semantic_title": "word error rate estimation without asr output: e-wer2",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ludusan20_interspeech.html": {
    "title": "An Evaluation of Manual and Semi-Automatic Laughter Annotation",
    "volume": "main",
    "abstract": "With laughter research seeing a development in recent years, there is also an increased need in materials having laughter annotations. We examine in this study how one can leverage existing spontaneous speech resources to this goal. We first analyze the process of manual laughter annotation in corpora, by establishing two important parameters of the process: the amount of time required and its inter-rater reliability. Next, we propose a novel semi-automatic tool for laughter annotation, based on a signal-based representation of speech rhythm. We test both annotation approaches on the same recordings, containing German dyadic spontaneous interactions, and employing a larger pool of annotators than previously done. We then compare and discuss the obtained results based on the two aforementioned parameters, highlighting the benefits and costs associated to each approach",
    "checked": true,
    "id": "29f803f96366dd5b107a00133f09bba8f303b3ef",
    "semantic_title": "an evaluation of manual and semi-automatic laughter annotation",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/martin20_interspeech.html": {
    "title": "Understanding Racial Disparities in Automatic Speech Recognition: The Case of Habitual \"be",
    "volume": "main",
    "abstract": "Recent research has highlighted that state-of-the-art automatic speech recognition (ASR) systems exhibit a bias against African American speakers. In this research, we investigate the underlying causes of this racially based disparity in performance, focusing on a unique morpho-syntactic feature of African American English (AAE), namely habitual \"be\", an invariant form of \"be\" that encodes the habitual aspect. By looking at over 100 hours of spoken AAE, we evaluated two ASR systems — DeepSpeech and Google Cloud Speech — to examine how well habitual \"be\" and its surrounding contexts are inferred. While controlling for local language and acoustic factors such as the amount of context, noise, and speech rate, we found that habitual \"be\" and its surrounding words were more error prone than non-habitual \"be\" and its surrounding words. These findings hold both when the utterance containing \"be\" is processed in isolation and in conjunction with surrounding utterances within speaker turn. Our research highlights the need for equitable ASR systems to take into account dialectal differences beyond acoustic modeling",
    "checked": true,
    "id": "f1211e38820dae7c1909d8c352c6fb1e90f18513",
    "semantic_title": "understanding racial disparities in automatic speech recognition: the case of habitual \"be",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zellou20_interspeech.html": {
    "title": "Secondary Phonetic Cues in the Production of the Nasal Short-a System in California English",
    "volume": "main",
    "abstract": "A production study explored the acoustic characteristics of /æ/ in CVC and CVN words spoken by California speakers who raise /æ/ in pre-nasal contexts. Results reveal that the phonetic realization of the /æ/-/ε/ contrast in these contexts is multidimensional. Raised pre-nasal /æ/ is close in formant space to /ε/, particularly over the second half of the vowel. Yet, systematic differences in the realization of the secondary acoustic features of duration, formant movement, and degree of coarticulatory vowel nasalization keep these vowels phonetically distinct. These findings have implications for systems of vowel contrast and the use of secondary phonetic properties to maintain lexical distinctions",
    "checked": true,
    "id": "79e8f6446b8a85270a406a098f6f3302912af08b",
    "semantic_title": "secondary phonetic cues in the production of the nasal short-a system in california english",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lorin20_interspeech.html": {
    "title": "Acoustic Properties of Strident Fricatives at the Edges: Implications for Consonant Discrimination",
    "volume": "main",
    "abstract": "Languages tend to license segmental contrasts where they are maximally perceptible, i.e. where more perceptual cues to the contrast are available. For strident fricatives, the most salient cues to the presence of voicing are low-frequency energy concentrations and fricative duration, as voiced fricatives are systematically shorter than voiceless ones. Cross-linguistically, the voicing contrast is more frequently realized word-initially than word-finally, as for obstruents. We investigate the phonetic underpinnings of this asymmetric behavior at the word edges, focusing on the availability of durational cues to the contrast in the two positions. To assess segmental duration, listeners rely on temporal markers, i.e. jumps in acoustic energy which demarcate segmental boundaries, thereby facilitating duration discrimination. We conducted an acoustic analysis of word-initial and word-final strident fricatives in American English. We found that temporal markers are sharper at the left edge of word-initial fricatives than at the right edge of word-final fricatives, in terms of absolute value of the intensity slope, in the high-frequency region. These findings allow us to make predictions about the availability of durational cues to the voicing contrast in the two positions",
    "checked": true,
    "id": "65ad549157645665e5bef38a67be16c60707f3d2",
    "semantic_title": "acoustic properties of strident fricatives at the edges: implications for consonant discrimination",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/luo20_interspeech.html": {
    "title": "Processes and Consequences of Co-Articulation in Mandarin V1N.(C2)V2 Context: Phonology and Phonetics",
    "volume": "main",
    "abstract": "It is well known that in Mandarin Chinese (MC) nasal rhymes, non-high vowels /a/ and /e/ undergo Vowel Nasalization and Backness Feature Specification processes to harmonize with the nasal coda in both manner and place of articulation. Specifically, the vowel is specified with the [+front] feature when followed by the /n/ coda and the [+back] feature when followed by /ŋ/. On the other hand, phonetic experiments in recent researches have shown that in MC disyllabic words, the nasal coda tends to undergo place assimilation in the V N.C V context and complete deletion in the V N.V context These processes raises two questions: firstly, will V in V N.C V contexts also change in its backness feature to harmonize with the assimilated nasal coda? Secondly, will the duration of V N reduce significantly after nasal coda deletion in the V N.(G)V context? A production experiment and a perception experiment were designed to answer these two questions. Results show that the vowel backness feature of V is not re-specified despite the appropriate environment, and the duration of V N is not reduced after nasal deletion. The phonological consequences of these findings will be discussed",
    "checked": true,
    "id": "224a27920b8de2a5b46d351c9f74cdc29c74f4e0",
    "semantic_title": "processes and consequences of co-articulation in mandarin v1n.(c2)v2 context: phonology and phonetics",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yue20_interspeech.html": {
    "title": "Voicing Distinction of Obstruents in the Hangzhou Wu Chinese Dialect",
    "volume": "main",
    "abstract": "This paper gives an acoustic phonetic description of the obstruents in the Hangzhou Wu Chinese dialect. Based on the data from 8 speakers (4 male and 4 female), obstruents were examined in terms of VOT, silent closure duration, segment duration, and spectral properties such as H1-H2, H1-F1 and H1-F3. Results suggest that VOT cannot differentiate the voiced obstruents from their voiceless counterparts, but the silent closure duration can. There is no voiced aspiration. And breathiness was detected on the vowel following the voiced category of obstruents. An acoustic consequence is that there is no segment for the voiced glottal fricative [ɦ], since it was realized as the breathiness on the following vowel. But interestingly, it is observed that syllables with [ɦ] are longer than their onset-less counterparts",
    "checked": true,
    "id": "5fd1aef797d6361d3d3723e2043a1d5535003e58",
    "semantic_title": "voicing distinction of obstruents in the hangzhou wu chinese dialect",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20f_interspeech.html": {
    "title": "The Phonology and Phonetics of Kaifeng Mandarin Vowels",
    "volume": "main",
    "abstract": "In this present study, we re-analyze the vowel system in Kaifeng Mandarin, adopting a phoneme-based approach. Our analysis deviates from the previous syllable-based analyses in a number of ways. First, we treat apical vowels [ɿ ʅ] as syllabic approximants and analyze them as allophones of the retroflex approximant /ɻ/. Second, the vowel inventory is of three sets, monophthongs, diphthongs and retroflex vowels. The classification of monophthongs and diphthongs is based on the phonological distribution of the coda nasal. That is, monophthongs can be followed by a nasal coda, while diphthongs cannot. This argument has introduced two new opening diphthongs /eε ɤʌ/ in the inventory, which have traditionally been described as monophthongs. Our phonological characterization of the vowels in Kaifeng Mandarin is further backed up by acoustic data. It is argued that the present study has gone some way towards enhancing our understanding of Mandarin segmental phonology in general",
    "checked": true,
    "id": "dde7584e0b273b5059a95a243b1baa9cd85e298b",
    "semantic_title": "the phonology and phonetics of kaifeng mandarin vowels",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zellers20_interspeech.html": {
    "title": "Microprosodic Variability in Plosives in German and Austrian German",
    "volume": "main",
    "abstract": "Fundamental frequency (F0) contours may show slight, microprosodic variations in the vicinity of plosive segments, which may have distinctive patterns relative to the place of articulation and voicing. Similarly, plosive bursts have distinctive characteristics associated with these articulatory features. The current study investigates the degree to which such microprosodic variations arise in two varieties of German, and how the two varieties differ. We find that microprosodic effects indeed arise in F0 as well as burst intensity and Center of Gravity, but that the extent of the variability is different in the two varieties under investigation, with northern German tending towards more variability in the microprosody of plosives than Austrian German. Coarticulatory effects on the burst with the following segment also arise, but also have different features in the two varieties. This evidence is consistent with the possibility that the fortis-lenis contrast is not equally stable in Austrian German and northern German",
    "checked": true,
    "id": "16ee81483a93f6f60faf9f23f9d1c544de4b5120",
    "semantic_title": "microprosodic variability in plosives in german and austrian german",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/huang20b_interspeech.html": {
    "title": "Er-Suffixation in Southwestern Mandarin: An EMA and Ultrasound Study",
    "volume": "main",
    "abstract": "This paper is an articulatory study of the er-suffixation (a.k.a. erhua) in Southwestern Mandarin (SWM), using co-registered EMA and ultrasound. Data from two female speakers in their twenties were analyzed and discussed. Our recording materials contain unsuffixed stems, er-suffixed forms and the rhotic schwa [ɚ], a phonemic vowel in its own right. Results suggest that the er-suffixation in SWM involves suffixing a rhotic schwa [ɚ] to the stem, unlike its counterpart in Beijing and Northeastern Mandarin [5]. Specifically, an entire rime will be replaced with the er-suffix if the nucleus vowel is non-high; only high vocoids will be preserved after the er-suffixation. The \"rhoticity\" is primarily realized as a bunched tongue shape configuration (i.e. a domed tongue body), while the Tongue Tip gesture plays a more limited role in SWM. A phonological analysis is accordingly proposed for the er-suffixation in SWM",
    "checked": true,
    "id": "a44a248609aa108aa0b008776013a9ab37919eb8",
    "semantic_title": "er-suffixation in southwestern mandarin: an ema and ultrasound study",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20g_interspeech.html": {
    "title": "Electroglottographic-Phonetic Study on Korean Phonation Induced by Tripartite Plosives in Yanbian Korean",
    "volume": "main",
    "abstract": "This paper examined the phonatory features induced by the tripartite plosives in Yanbian Korean, broadly considered as Hamkyungbukdo Korean dialect. Electroglottographic (EGG) and acoustic analysis was applied for five elderly Korean speakers. The results show that fortis-induced phonation is characterized with more constricted glottis, slower spectral tilt, and higher sub-harmonic-harmonic ratio. Lenis-induced phonation is shown to be breathier with smaller Contact Quotient and faster spectral tilt. Most articulatory and acoustic measures for the aspirated are shown to be patterned with the lenis; However, sporadic difference between the two indicates that the lenis induces more breathier phonation. The diplophonia phonation is argued to be a salient feature for the fortis-head syllables in Yanbian Korean. The vocal fold medial compression and adductive tension mechanisms are tentatively argued to be responsible for the production of the fortis. At last, gender difference is shown to be salient in the fortis-induced phonation",
    "checked": true,
    "id": "a4c95290ecb7182c2eb30c6804b6f8fb58b743ac",
    "semantic_title": "electroglottographic-phonetic study on korean phonation induced by tripartite plosives in yanbian korean",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wilkins20_interspeech.html": {
    "title": "Modeling Global Body Configurations in American Sign Language",
    "volume": "main",
    "abstract": "In this paper we consider the problem of computationally representing American Sign Language (ASL) phonetics. We specifically present a computational model inspired by the sequential phonological ASL representation, known as the Movement-Hold (MH) Model. Our computational model is capable of not only capturing ASL phonetics, but also has generative abilities. We present a Probabilistic Graphical Model (PGM) which explicitly models holds and implicitly models movement in the MH model. For evaluation, we introduce a novel data corpus, ASLing, and compare our PGM to other models (GMM, LDA, and VAE) and show its superior performance. Finally, we demonstrate our model's interpretability by computing various phonetic properties of ASL through the inspection of our learned model",
    "checked": true,
    "id": "da0a040a5d7507f226f0eb8656d018f70af5eb38",
    "semantic_title": "modeling global body configurations in american sign language",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20h_interspeech.html": {
    "title": "Augmenting Turn-Taking Prediction with Wearable Eye Activity During Conversation",
    "volume": "main",
    "abstract": "In a variety of conversation contexts, accurately predicting the time point at which a conversational participant is about to speak can help improve computer-mediated human-human communications. Although it is not difficult for a human to perceive turn-taking intent in conversations, it has been a challenging task for computers to date. In this study, we employed eye activity acquired from low-cost wearable hardware during natural conversation and studied how pupil diameter, blink and gaze direction could assist speech in voice activity and turn-taking prediction. Experiments on a new 2-hour corpus of natural conversational speech between six pairs of speakers wearing near-field eye video glasses revealed that the F1 score for predicting the voicing activity up to 1s ahead of the current instant can be above 80%, for speech and non-speech detection with fused eye and speech features. Further, extracting features synchronously from both interlocutors provides a relative reduction in error rate of 8.5% compared with a system based on just a single speaker. The performance of four turn-taking states based on the predicted voice activity also achieved F1 scores significantly higher than chance level. These findings suggest that wearable eye activity can play a role in future speech communication systems",
    "checked": true,
    "id": "802679ceb3c485373896cbecfd78b3ff9b91e250",
    "semantic_title": "augmenting turn-taking prediction with wearable eye activity during conversation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lu20_interspeech.html": {
    "title": "CAM: Uninteresting Speech Detector",
    "volume": "main",
    "abstract": "Voice assistants such as Siri, Alexa, etc. usually adopt a pipeline to process users' utterances, which generally include transcribing the audio into text, understanding the text, and finally responding back to users. One potential issue is that some utterances could be devoid of any interesting speech, and are thus not worth being processed through the entire pipeline. Examples of uninteresting utterances include those that have too much noise, are devoid of intelligible speech, etc. It is therefore desirable to have a model to filter out such useless utterances before they are ingested for downstream processing, thus saving system resources. Towards this end, we propose the Combination of Audio and Metadata (CAM) detector to identify utterances that contain only uninteresting speech. Our experimental results show that the CAM detector considerably outperforms using either an audio model or a metadata model alone, which demonstrates the effectiveness of the proposed system",
    "checked": true,
    "id": "d00775a9d1ec65600c1ac4b08e0c38b7d1ec834c",
    "semantic_title": "cam: uninteresting speech detector",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/caseiro20_interspeech.html": {
    "title": "Mixed Case Contextual ASR Using Capitalization Masks",
    "volume": "main",
    "abstract": "End-to-end (E2E) mixed-case automatic speech recognition (ASR) systems that directly predict words in the written domain are attractive due to being simple to build, not requiring explicit capitalization models, allowing streaming capitalization without additional effort beyond that required for streaming ASR, and their small size. However, the fact that these systems produce various versions of the same word with different capitalizations, and even different word segmentations for different case variants when wordpieces (WP) are predicted, leads to multiple problems with contextual ASR. In particular, the size of and time to build contextual models grows considerably with the number of variants per word. In this paper, we propose separating orthographic recognition from capitalization, so that the ASR system first predicts a word, then predicts its capitalization in the form of a capitalization mask. We show that the use of capitalization masks achieves the same low error rate as traditional mixed-case ASR, while reducing the size and compilation time of contextual models. Furthermore, we observe significant improvements in capitalization quality",
    "checked": true,
    "id": "4d8914604aa739ed3133c32d3d0aff8a434497f7",
    "semantic_title": "mixed case contextual asr using capitalization masks",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mao20b_interspeech.html": {
    "title": "Speech Recognition and Multi-Speaker Diarization of Long Conversations",
    "volume": "main",
    "abstract": "Speech recognition (ASR) and speaker diarization (SD) models have traditionally been trained separately to produce rich conversation transcripts with speaker labels. Recent advances [1] have shown that joint ASR and SD models can learn to leverage audio-lexical inter-dependencies to improve word diarization performance. We introduce a new benchmark of hour-long podcasts collected from the weekly This American Life radio program to better compare these approaches when applied to extended multi-speaker conversations. We find that training separate ASR and SD models perform better when utterance boundaries are known but otherwise joint models can perform better. To handle long conversations with unknown utterance boundaries, we introduce a striding attention decoding algorithm and data augmentation techniques which, combined with model pre-training, improves ASR and SD",
    "checked": true,
    "id": "003102b6c478c08dc84f66b59f0bd78beb703a62",
    "semantic_title": "speech recognition and multi-speaker diarization of long conversations",
    "citation_count": 30
  },
  "https://www.isca-speech.org/archive/interspeech_2020/geng20_interspeech.html": {
    "title": "Investigation of Data Augmentation Techniques for Disordered Speech Recognition",
    "volume": "main",
    "abstract": "Disordered speech recognition is a highly challenging task. The underlying neuro-motor conditions of people with speech disorders, often compounded with co-occurring physical disabilities, lead to the difficulty in collecting large quantities of speech required for system development. This paper investigates a set of data augmentation techniques for disordered speech recognition, including vocal tract length perturbation (VTLP), tempo perturbation and speed perturbation. Both normal and disordered speech were exploited in the augmentation process. Variability among impaired speakers in both the original and augmented data was modeled using learning hidden unit contributions (LHUC) based speaker adaptive training. The final speaker adapted system constructed using the UASpeech corpus and the best augmentation approach based on speed perturbation produced up to 2.92% absolute (9.3% relative) word error rate (WER) reduction over the baseline system without data augmentation, and gave an overall WER of 26.37% on the test set containing 16 dysarthric speakers",
    "checked": true,
    "id": "91072fe3351ce60767a19e678be4d6a16b97ec95",
    "semantic_title": "investigation of data augmentation techniques for disordered speech recognition",
    "citation_count": 36
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wei20_interspeech.html": {
    "title": "A Real-Time Robot-Based Auxiliary System for Risk Evaluation of COVID-19 Infection",
    "volume": "main",
    "abstract": "In this paper, we propose a real-time robot-based auxiliary system for risk evaluation of COVID-19 infection. It combines real-time speech recognition, temperature measurement, keyword detection, cough detection and other functions in order to convert live audio into actionable structured data to achieve the COVID-19 infection risk assessment function. In order to better evaluate the COVID-19 infection, we propose an end-to-end method for cough detection and classification for our proposed system. It is based on real conversation data from human-robot, which processes speech signals to detect cough and classifies it if detected. The structure of our model are maintained concise to be implemented for real-time applications. And we further embed this entire auxiliary diagnostic system in the robot and it is placed in the communities, hospitals and supermarkets to support COVID-19 testing. The system can be further leveraged within a business rules engine, thus serving as a foundation for real-time supervision and assistance applications. Our model utilizes a pretrained, robust training environment that allows for efficient creation and customization of customer-specific health states",
    "checked": true,
    "id": "89a1a85826b39ad6142d226666f18b85445cd643",
    "semantic_title": "a real-time robot-based auxiliary system for risk evaluation of covid-19 infection",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2020/barbera20_interspeech.html": {
    "title": "An Utterance Verification System for Word Naming Therapy in Aphasia",
    "volume": "main",
    "abstract": "Anomia (word finding difficulties) is the hallmark of aphasia an acquired language disorder, most commonly caused by stroke. Assessment of speech performance using picture naming tasks is therefore a key method for identification of the disorder and monitoring patient's response to treatment interventions. Currently, this assessment is conducted manually by speech and language therapists (SLT). Surprisingly, despite advancements in ASR and artificial intelligence with technologies like deep learning, research on developing automated systems for this task has been scarce. Here we present an utterance verification system incorporating a deep learning element that classifies ‘correct'/‘incorrect' naming attempts from aphasic stroke patients. When tested on 8 native British-English speaking aphasics the system's performance accuracy ranged between 83.6% to 93.6%, with a 10 fold cross validation mean of 89.5%. This performance was not only significantly better than one of the leading commercially available ASRs (Google speech-to-text service) but also comparable in some instances with two independent SLT ratings for the same dataset",
    "checked": true,
    "id": "5d491622cbf22f814659921b0aa3c53699b6dc27",
    "semantic_title": "an utterance verification system for word naming therapy in aphasia",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2020/liu20d_interspeech.html": {
    "title": "Exploiting Cross-Domain Visual Feature Generation for Disordered Speech Recognition",
    "volume": "main",
    "abstract": "Audio-visual speech recognition (AVSR) technologies have been successfully applied to a wide range of tasks. When developing AVSR systems for disordered speech characterized by severe degradation of voice quality and large mismatch against normal, it is difficult to record large amounts of high quality audio-visual data. In order to address this issue, a cross-domain visual feature generation approach is proposed in this paper. Audio-visual inversion DNN system constructed using widely available out-of-domain audio-visual data was used to generate visual features for disordered speakers for whom video data is either very limited or unavailable. Experiments conducted on the UASpeech corpus suggest that the proposed cross-domain visual feature generation based AVSR system consistently outperformed the baseline ASR system and AVSR system using original visual features. An overall word error rate reduction of 3.6% absolute (14% relative) was obtained over the previously published best system on the 8 UASpeech dysarthric speakers with audio-visual data of the same task",
    "checked": true,
    "id": "ab28955ce441e6b9fdebd3bf9acc5e1771285a3d",
    "semantic_title": "exploiting cross-domain visual feature generation for disordered speech recognition",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lin20c_interspeech.html": {
    "title": "Joint Prediction of Punctuation and Disfluency in Speech Transcripts",
    "volume": "main",
    "abstract": "Spoken language transcripts generated from Automatic speech recognition (ASR) often contain a large portion of disfluency and lack punctuation symbols. Punctuation restoration and disfluency removal of the transcripts can facilitate downstream tasks such as machine translation, information extraction and syntactic analysis [1]. Various studies have shown the influence between these two tasks and thus performed modeling based on a multi-task learning (MTL) framework [2, 3], which learns general representations in the shared layers and separate representations in the task-specific layers. However, task dependencies are normally ignored in the task-specific layers. To model the dependencies of tasks, we propose an attention-based structure in the task-specific layers of the MTL framework incorporating the pretrained BERT (a state-of-art NLP-related model) [4]. Experimental results based on English IWSLT dataset and the Switchboard dataset show the proposed architecture outperforms the separate modeling methods as well as the traditional MTL methods",
    "checked": true,
    "id": "0d1775622abf795c9a6c8f55040fc134a9bb3c92",
    "semantic_title": "joint prediction of punctuation and disfluency in speech transcripts",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yi20_interspeech.html": {
    "title": "Focal Loss for Punctuation Prediction",
    "volume": "main",
    "abstract": "Many approaches have been proposed to predict punctuation marks. Previous results demonstrate that these methods are effective. However, there still exists class imbalance problem during training. Most of the classes in the training set for punctuation prediction are non-punctuation marks. This will affect the performance of punctuation prediction tasks. Therefore, this paper uses a focal loss to alleviate this issue. The focal loss can down-weight easy examples and focus training on a sparse set of hard examples. Experiments are conducted on IWSLT2011 datasets. The results show that the punctuation predicting models trained with a focal loss obtain performance improvement over that trained with a cross entropy loss by up to 2.7% absolute overall F -score on test set. The proposed model also outperforms previous state-of-the-art models",
    "checked": true,
    "id": "f5b61e466febfef7404edc1c3b47eb5a3ce45054",
    "semantic_title": "focal loss for punctuation prediction",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chen20d_interspeech.html": {
    "title": "Improving X-Vector and PLDA for Text-Dependent Speaker Verification",
    "volume": "main",
    "abstract": "Recently, the pipeline consisting of an x-vector speaker embedding front-end and a Probabilistic Linear Discriminant Analysis (PLDA) back-end has achieved state-of-the-art results in text-independent speaker verification. In this paper, we further improve the performance of x-vector and PLDA based system for text-dependent speaker verification by exploring the choice of layer to produce embedding and modifying the back-end training strategies. In particular, we probe that x-vector based embeddings, specifically the standard deviation statistics in the pooling layer, contain the information related to both speaker characteristics and spoken content. Accordingly, we modify the back-end training labels by utilizing both of the speaker-id and phrase-id. A correlation-alignment-based PLDA adaptation is also adopted to make use of the text-independent labeled data during back-end training. Experimental results on the SDSVC 2020 dataset show that our proposed methods achieve significant performance improvement compared with the x-vector and HMM based i-vector baselines",
    "checked": true,
    "id": "6fee4c2cf4fa7ae751abcebd1b86e82899e2f36b",
    "semantic_title": "improving x-vector and plda for text-dependent speaker verification",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zeinali20_interspeech.html": {
    "title": "SdSV Challenge 2020: Large-Scale Evaluation of Short-Duration Speaker Verification",
    "volume": "main",
    "abstract": "Modern approaches to speaker verification represent speech utterances as fixed-length embeddings. With these approaches, we implicitly assume that speaker characteristics are independent of the spoken content. Such an assumption generally holds when sufficiently long utterances are given. In this context, speaker embeddings, like i-vector and x-vector, have shown to be extremely effective. For speech utterances of short duration (in the order of a few seconds), speaker embeddings have shown significant dependency on the phonetic content. In this regard, the SdSV Challenge 2020 was organized with a broad focus on systematic benchmark and analysis on varying degrees of phonetic variability on short-duration speaker verification (SdSV). In addition to text-dependent and text-independent tasks, the challenge features an unusual and difficult task of cross-lingual speaker verification (English vs. Persian). This paper describes the dataset and tasks, the evaluation rules and protocols, the performance metric, baseline systems, and challenge results. We also present insights gained from the evaluation and future research directions",
    "checked": true,
    "id": "8b3937b1bc9f9d9fc207a49fdbfafadcf47295db",
    "semantic_title": "sdsv challenge 2020: large-scale evaluation of short-duration speaker verification",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2020/jiang20_interspeech.html": {
    "title": "The XMUSPEECH System for Short-Duration Speaker Verification Challenge 2020",
    "volume": "main",
    "abstract": "In this paper, we present our XMUSPEECH system for Task 1 in the Short-duration Speaker Verification (SdSV) Challenge. In this challenge, Task 1 is a Text-Dependent (TD) mode where speaker verification systems are required to automatically determine whether a test segment with specific phrase belongs to the target speaker. We leveraged the system pipeline from three aspects, including the data processing, front-end training and back-end processing. In addition, we have explored some training strategies such as spectrogram augmentation and transfer learning. The experimental results show that the attempts we had done are effective and our best single system, a transferred model with spectrogram augmentation and attentive statistic pooling, significantly outperforms the official baseline on both progress subset and evaluation subset. Finally, a fusion of seven subsystems are chosen as our primary system which yielded 0.0856 and 0.0862 in term of minDCF, for the progress subset and evaluation subset respectively",
    "checked": true,
    "id": "7c8e448f235d0a0bd8abd9cab13f5c7c37d698ae",
    "semantic_title": "the xmuspeech system for short-duration speaker verification challenge 2020",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mun20_interspeech.html": {
    "title": "Robust Text-Dependent Speaker Verification via Character-Level Information Preservation for the SdSV Challenge 2020",
    "volume": "main",
    "abstract": "This paper describes our submission to Task 1 of the Short-duration Speaker Verification (SdSV) challenge 2020. Task 1 is a text-dependent speaker verification task, where both the speaker and phrase are required to be verified. The submitted systems were composed of TDNN-based and ResNet-based front-end architectures, in which the frame-level features were aggregated with various pooling methods (e.g., statistical, self-attentive, ghostVLAD pooling). Although the conventional pooling methods provide embeddings with a sufficient amount of speaker-dependent information, our experiments show that these embeddings often lack phrase-dependent information. To mitigate this problem, we propose a new pooling and score compensation methods that leverage a CTC-based automatic speech recognition (ASR) model for taking the lexical content into account. Both methods showed improvement over the conventional techniques, and the best performance was achieved by fusing all the experimented systems, which showed 0.0785% MinDCF and 2.23% EER on the challenge's evaluation subset",
    "checked": true,
    "id": "7bf29aae080ef83a38cd58baee623d2e54849e8b",
    "semantic_title": "robust text-dependent speaker verification via character-level information preservation for the sdsv challenge 2020",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/alumae20_interspeech.html": {
    "title": "The TalTech Systems for the Short-Duration Speaker Verification Challenge 2020",
    "volume": "main",
    "abstract": "This paper presents the Tallinn University of Technology systems submitted to the Short-duration Speaker Verification Challenge 2020. The challenge consists of two tasks, focusing on text-dependent and text-independent speaker verification with some cross-lingual aspects. We used speaker embedding models that consist of squeeze-and-attention based residual layers, multi-head attention and either cross-entropy-based or additive angular margin based objective function. In order to encourage the model to produce language-independent embeddings, we trained the models in a multi-task manner, using dataset specific output layers. In the text-dependent task we employed a phrase classifier to reject trials with non-matching phrases. In the text-independent task we used a language classifier to boost the scores of trials where the language of the test and enrollment utterances does not match. Our final primary metric score was 0.075 in Task 1 (ranked as 6th) and 0.118 in Task 2 (rank 8)",
    "checked": true,
    "id": "29abbaef73524207732d2a5091aec2f8712425c9",
    "semantic_title": "the taltech systems for the short-duration speaker verification challenge 2020",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shen20b_interspeech.html": {
    "title": "Investigation of NICT Submission for Short-Duration Speaker Verification Challenge 2020",
    "volume": "main",
    "abstract": "In this paper, we describe the NICT speaker verification system for the text-independent task of the short-duration speaker verification (SdSV) challenge 2020. We firstly present the details of the training data and feature preparation. Then, x-vector-based front-ends by considering different network configurations, back-ends of probabilistic linear discriminant analysis (PLDA), simplified PLDA, cosine similarity, and neural network-based PLDA are investigated and explored. Finally, we apply a greedy fusion and calibration approach to select and combine the subsystems. To improve the performance of the speaker verification system on short-duration evaluation data, we introduce our investigations on how to reduce the duration mismatch between training and test datasets. Experimental results showed that our primary fusion yielded minDCF of 0.074 and EER of 1.50 on the evaluation subset, which was the 2nd best result in the text-independent speaker verification task",
    "checked": true,
    "id": "b9a988e38451eb7d4b46dde21172535352fc3ed8",
    "semantic_title": "investigation of nict submission for short-duration speaker verification challenge 2020",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2020/thienpondt20_interspeech.html": {
    "title": "Cross-Lingual Speaker Verification with Domain-Balanced Hard Prototype Mining and Language-Dependent Score Normalization",
    "volume": "main",
    "abstract": "In this paper we describe the top-scoring IDLab submission for the text-independent task of the Short-duration Speaker Verification (SdSV) Challenge 2020. The main difficulty of the challenge exists in the large degree of varying phonetic overlap between the potentially cross-lingual trials, along with the limited availability of in-domain DeepMine Farsi training data. We introduce domain-balanced hard prototype mining to finetune the state-of-the-art ECAPA-TDNN x-vector based speaker embedding extractor. The sample mining technique efficiently exploits speaker distances between the speaker prototypes of the popular AAM-softmax loss function to construct challenging training batches that are balanced on the domain-level. To enhance the scoring of cross-lingual trials, we propose a language-dependent s-norm score normalization. The imposter cohort only contains data from the Farsi target-domain which simulates the enrollment data always being Farsi. In case a Gaussian-Backend language model detects the test speaker embedding to contain English, a cross-language compensation offset determined on the AAM-softmax speaker prototypes is subtracted from the maximum expected imposter mean score. A fusion of five systems with minor topological tweaks resulted in a final MinDCF and EER of 0.065 and 1.45% respectively on the SdSVC evaluation set",
    "checked": true,
    "id": "ab5b5496bda3002c0072b5ef670b310029ad7b3d",
    "semantic_title": "cross-lingual speaker verification with domain-balanced hard prototype mining and language-dependent score normalization",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lozanodiez20_interspeech.html": {
    "title": "BUT Text-Dependent Speaker Verification System for SdSV Challenge 2020",
    "volume": "main",
    "abstract": "In this paper, we present the winning BUT submission for the text-dependent task of the SdSV challenge 2020. Given the large amount of training data available in this challenge, we explore successful techniques from text-independent systems in the text-dependent scenario. In particular, we trained x-vector extractors on both in-domain and out-of-domain datasets and combine them with i-vectors trained on concatenated MFCCs and bottleneck features, which have proven effective for the text-dependent scenario. Moreover, we proposed the use of phrase-dependent PLDA backend for scoring and its combination with a simple phrase recognizer, which brings up to 63% relative improvement on our development set with respect to using standard PLDA. Finally, we combine our different i-vector and x-vector based systems using a simple linear logistic regression score level fusion, which provides 28% relative improvement on the evaluation set with respect to our best single system",
    "checked": true,
    "id": "e670271ad073e7217590a688462b960896e972ee",
    "semantic_title": "but text-dependent speaker verification system for sdsv challenge 2020",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ravi20_interspeech.html": {
    "title": "Exploring the Use of an Unsupervised Autoregressive Model as a Shared Encoder for Text-Dependent Speaker Verification",
    "volume": "main",
    "abstract": "In this paper, we propose a novel way of addressing text-dependent automatic speaker verification (TD-ASV) by using a shared-encoder with task-specific decoders. An autoregressive predictive coding (APC) encoder is pre-trained in an unsupervised manner using both out-of-domain (LibriSpeech, VoxCeleb) and in-domain (DeepMine) unlabeled datasets to learn generic, high-level feature representation that encapsulates speaker and phonetic content. Two task-specific decoders were trained using labeled datasets to classify speakers (SID) and phrases (PID). Speaker embeddings extracted from the SID decoder were scored using a PLDA. SID and PID systems were fused at the score level. There is a 51.9% relative improvement in minDCF for our system compared to the fully supervised x-vector baseline on the cross-lingual DeepMine dataset. However, the i-vector/HMM method outperformed the proposed APC encoder-decoder system. A fusion of the x-vector/PLDA baseline and the SID/PLDA scores prior to PID fusion further improved performance by 15% indicating complementarity of the proposed approach to the x-vector system. We show that the proposed approach can leverage from large, unlabeled, data-rich domains, and learn speech patterns independent of downstream tasks. Such a system can provide competitive performance in domain-mismatched scenarios where test data is from data-scarce domains",
    "checked": true,
    "id": "54c70e278eb41b8578d78ea85725fe331a644cd8",
    "semantic_title": "exploring the use of an unsupervised autoregressive model as a shared encoder for text-dependent speaker verification",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20d_interspeech.html": {
    "title": "Recognition-Synthesis Based Non-Parallel Voice Conversion with Adversarial Learning",
    "volume": "main",
    "abstract": "This paper presents an adversarial learning method for recognition-synthesis based non-parallel voice conversion. A recognizer is used to transform acoustic features into linguistic representations while a synthesizer recovers output features from the recognizer outputs together with the speaker identity. By separating the speaker characteristics from the linguistic representations, voice conversion can be achieved by replacing the speaker identity with the target one. In our proposed method, a speaker adversarial loss is adopted in order to obtain speaker-independent linguistic representations using the recognizer. Furthermore, discriminators are introduced and a generative adversarial network (GAN) loss is used to prevent the predicted features from being over-smoothed. For training model parameters, a strategy of pre-training on a multi-speaker dataset and then fine-tuning on the source-target speaker pair is designed. Our method achieved higher similarity than the baseline model that obtained the best performance in Voice Conversion Challenge 2018",
    "checked": true,
    "id": "9dcbc9a682606f93d620e3d7126d42a9a80a023b",
    "semantic_title": "recognition-synthesis based non-parallel voice conversion with adversarial learning",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ding20_interspeech.html": {
    "title": "Improving the Speaker Identity of Non-Parallel Many-to-Many Voice Conversion with Adversarial Speaker Recognition",
    "volume": "main",
    "abstract": "Phonetic Posteriorgrams (PPGs) have received much attention for non-parallel many-to-many Voice Conversion (VC), and have been shown to achieve state-of-the-art performance. These methods implicitly assume that PPGs are speaker-independent and contain only linguistic information in an utterance. In practice, however, PPGs carry speaker individuality cues, such as accent, intonation, and speaking rate. As a result, these cues can leak into the voice conversion, making it sound similar to the source speaker. To address this issue, we propose an adversarial learning approach that can remove speaker-dependent information in VC models based on a PPG2speech synthesizer. During training, the encoder output of a PPG2speech synthesizer is fed to a classifier trained to identify the corresponding speaker, while the encoder is trained to fool the classifier. As a result, a more speaker-independent representation is learned. The proposed method is advantageous as it does not require pre-training the speaker classifier, and the adversarial speaker classifier is jointly trained with the PPG2speech synthesizer end-to-end. We conduct objective and subjective experiments on the CSTR VCTK Corpus under standard and one-shot VC conditions. Results show that the proposed method significantly improves the speaker identity of VC syntheses when compared with a baseline system trained without adversarial learning",
    "checked": true,
    "id": "cef10a1225d4626748d5ab445e125558b970ae50",
    "semantic_title": "improving the speaker identity of non-parallel many-to-many voice conversion with adversarial speaker recognition",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20i_interspeech.html": {
    "title": "Non-Parallel Many-to-Many Voice Conversion with PSR-StarGAN",
    "volume": "main",
    "abstract": "Voice Conversion (VC) aims at modifying source speaker's speech to sound like that of target speaker while preserving linguistic information of given speech. StarGAN-VC was recently proposed, which utilizes a variant of Generative Adversarial Networks (GAN) to perform non-parallel many-to-many VC. However, the quality of generated speech is not satisfactory enough. An improved method named \"PSR-StarGAN-VC\" is proposed in this paper by incorporating three improvements. Firstly, perceptual loss functions are introduced to optimize the generator in StarGAN-VC aiming to learn high-level spectral features. Secondly, considering that Switchable Normalization (SN) could learn different operations in different normalization layers of model, it is introduced to replace Batch Normalization (BN) in StarGAN-VC. Lastly, Residual Network (ResNet) is applied to establish the mapping of different layers between the encoder and decoder of generator aiming to retain more semantic features when converting speech, and to reduce the difficulty of training. Experiment results on the VCC 2018 datasets demonstrate superiority of the proposed method in terms of naturalness and speaker similarity",
    "checked": true,
    "id": "384ba1dc809f744e81daab3b8ebb539cd4df1313",
    "semantic_title": "non-parallel many-to-many voice conversion with psr-stargan",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/polyak20_interspeech.html": {
    "title": "TTS Skins: Speaker Conversion via ASR",
    "volume": "main",
    "abstract": "We present a fully convolutional wav-to-wav network for converting between speakers' voices, without relying on text. Our network is based on an encoder-decoder architecture, where the encoder is pre-trained for the task of Automatic Speech Recognition, and a multi-speaker waveform decoder is trained to reconstruct the original signal in an autoregressive manner. We train the network on narrated audiobooks, and demonstrate multi-voice TTS in those voices, by converting the voice of a TTS robot",
    "checked": true,
    "id": "de8cd74ac86eaddcbe2ac642e5ed8d07a7d2bf29",
    "semantic_title": "tts skins: speaker conversion via asr",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20e_interspeech.html": {
    "title": "GAZEV: GAN-Based Zero-Shot Voice Conversion Over Non-Parallel Speech Corpus",
    "volume": "main",
    "abstract": "Non-parallel many-to-many voice conversion is recently attracting huge research efforts in the speech processing community. A voice conversion system transforms an utterance of a source speaker to another utterance of a target speaker by keeping the content in the original utterance and replacing by the vocal features from the target speaker. Existing solutions, e.g., StarGAN-VC2, present promising results, only when speech corpus of the engaged speakers is available during model training. AUTOVC is able to perform voice conversion on unseen speakers, but it needs an external pretrained speaker verification model. In this paper, we present our new GAN-based zero-shot voice conversion solution, called GAZEV, which targets to support unseen speakers on both source and target utterances. Our key technical contribution is the adoption of speaker embedding loss on top of the GAN framework, as well as adaptive instance normalization strategy, in order to address the limitations of speaker identity transfer in existing solutions. Our empirical evaluations demonstrate significant performance improvement on output speech quality, and comparable speaker similarity to AUTOVC",
    "checked": true,
    "id": "0bef3a374ea631e58fc7985a54f1170edc8d5a76",
    "semantic_title": "gazev: gan-based zero-shot voice conversion over non-parallel speech corpus",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20g_interspeech.html": {
    "title": "Spoken Content and Voice Factorization for Few-Shot Speaker Adaptation",
    "volume": "main",
    "abstract": "The low similarity and naturalness of synthesized speech remain a challenging problem for speaker adaptation with few resources. Since the acoustic model is too complex to interpret, overfitting will occur when training with few data. To prevent the model from overfitting, this paper proposes a novel speaker adaptation framework that decomposes the parameter space of the end-to-end acoustic model into two parts, with the one on predicting spoken content and the other on modeling speaker's voice. The spoken content is represented by phone posteriorgram (PPG) which is speaker independent. By adapting the two sub-modules separately, the overfitting can be alleviated effectively. Moreover, we propose two different adaptation strategies based on whether the data has text annotation. In this way, speaker adaptation can also be performed without text annotations. Experimental results confirm the adaptability of our proposed method of factorizating spoken content and voice. Listening tests demonstrate that our proposed method can achieve better performance with just 10 sentences than speaker adaptation conducted on Tacotron in terms of naturalness and speaker similarity",
    "checked": true,
    "id": "5bc8ca8cf3a699d203cbfd59359defd51553cb7b",
    "semantic_title": "spoken content and voice factorization for few-shot speaker adaptation",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2020/polyak20b_interspeech.html": {
    "title": "Unsupervised Cross-Domain Singing Voice Conversion",
    "volume": "main",
    "abstract": "We present a wav-to-wav generative model for the task of singing voice conversion from any identity. Our method utilizes both an acoustic model, trained for the task of automatic speech recognition, together with melody extracted features to drive a waveform-based generator. The proposed generative architecture is invariant to the speaker's identity and can be trained to generate target singers from unlabeled training data, using either speech or singing sources. The model is optimized in an end-to-end fashion without any manual supervision, such as lyrics, musical notes or parallel samples. The proposed approach is fully-convolutional and can generate audio in realtime. Experiments show that our method significantly outperforms the baseline methods while generating convincingly better audio samples than alternative attempts",
    "checked": true,
    "id": "49eddc0b2c1c400a93960c0287484078497a07d4",
    "semantic_title": "unsupervised cross-domain singing voice conversion",
    "citation_count": 29
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ishihara20_interspeech.html": {
    "title": "Attention-Based Speaker Embeddings for One-Shot Voice Conversion",
    "volume": "main",
    "abstract": "This paper proposes a novel approach to embed speaker information to feature vectors at frame level using an attention mechanism, and its application to one-shot voice conversion. A one-shot voice conversion system is a type of voice conversion system where only one utterance from a target speaker is available for conversion. In many one-shot voice conversion systems, a speaker encoder mechanism compresses an utterance of the target speaker into a fixed-size vector for propagating speaker information. However, the obtained representation has lost temporal information related to speaker identities and it could degrade conversion quality. To alleviate this problem, we propose a novel way to embed speaker information using an attention mechanism. Instead of compressing into a fixed-size vector, our proposed speaker encoder outputs a sequence of speaker embedding vectors. The obtained sequence is selectively combined with input frames of a source speaker by an attention mechanism. Finally the obtained time varying speaker information is utilized for a decoder to generate the converted features. Objective evaluation showed that our method reduced the averaged mel-cepstrum distortion to 5.23 dB from 5.34 dB compared with the baseline system. The subjective preference test showed that our proposed system outperformed the baseline one",
    "checked": true,
    "id": "af78210c07e4630084d3b93c47737febd2fbcdd5",
    "semantic_title": "attention-based speaker embeddings for one-shot voice conversion",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2020/cong20_interspeech.html": {
    "title": "Data Efficient Voice Cloning from Noisy Samples with Domain Adversarial Training",
    "volume": "main",
    "abstract": "Data efficient voice cloning aims at synthesizing target speaker's voice with only a few enrollment samples at hand. To this end, speaker adaptation and speaker encoding are two typical methods based on base model trained from multiple speakers. The former uses a small set of target speaker data to transfer the multi-speaker model to target speaker's voice through direct model update, while in the latter, only a few seconds of target speaker's audio directly goes through an extra speaker encoding model along with the multi-speaker model to synthesize target speaker's voice without model update. Nevertheless, the two methods need clean target speaker data. However, the samples provided by user may inevitably contain acoustic noise in real applications. It's still challenging to generating target voice with noisy data. In this paper, we study the data efficient voice cloning problem from noisy samples under the sequence-to-sequence based TTS paradigm. Specifically, we introduce domain adversarial training (DAT) to speaker adaptation and speaker encoding, which aims to disentangle noise from speech-noise mixture. Experiments show that for both speaker adaptation and encoding, the proposed approaches can consistently synthesize clean speech from noisy speaker samples, apparently outperforming the method adopting state-of-the-art speech enhancement module",
    "checked": true,
    "id": "0e2446b6c99e5a22847155d1828b5d9fb93a5236",
    "semantic_title": "data efficient voice cloning from noisy samples with domain adversarial training",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hong20_interspeech.html": {
    "title": "Gated Multi-Head Attention Pooling for Weakly Labelled Audio Tagging",
    "volume": "main",
    "abstract": "Multiple instance learning (MIL) has recently been used for weakly labelled audio tagging, where the spectrogram of an audio signal is divided into segments to form instances in a bag, and then the low-dimensional features of these segments are pooled for tagging. The choice of a pooling scheme is the key to exploiting the weakly labelled data. However, the traditional pooling schemes are usually fixed and unable to distinguish the contributions, making it difficult to adapt to the characteristics of the sound events. In this paper, a novel pooling algorithm is proposed for MIL, named gated multi-head attention pooling (GMAP), which is able to attend to the information of events from different heads at different positions. Each head allows the model to learn information from different representation subspaces. Furthermore, in order to avoid the redundancy of multi-head information, a gating mechanism is used to fuse individual head features. The proposed GMAP increases the modeling power of the single-head attention with no computational overhead. Experiments are carried out on Audioset, which is a large-scale weakly labelled dataset, and show superior results to the non-adaptive pooling and the vanilla attention pooling schemes",
    "checked": true,
    "id": "54094ffca09bdb9cd66c9ac38e02ca2fd685f217",
    "semantic_title": "gated multi-head attention pooling for weakly labelled audio tagging",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20h_interspeech.html": {
    "title": "Environmental Sound Classification with Parallel Temporal-Spectral Attention",
    "volume": "main",
    "abstract": "Convolutional neural networks (CNN) are one of the best-performing neural network architectures for environmental sound classification (ESC). Recently, temporal attention mechanisms have been used in CNN to capture the useful information from the relevant time frames for audio classification, especially for weakly labelled data where the onset and offset times of the sound events are not applied. In these methods, however, the inherent spectral characteristics and variations are not explicitly exploited when obtaining the deep features. In this paper, we propose a novel parallel temporal-spectral attention mechanism for CNN to learn discriminative sound representations, which enhances the temporal and spectral features by capturing the importance of different time frames and frequency bands. Parallel branches are constructed to allow temporal attention and spectral attention to be applied respectively in order to mitigate interference from the segments without the presence of sound events. The experiments on three environmental sound classification (ESC) datasets and two acoustic scene classification (ASC) datasets show that our method improves the classification performance and also exhibits robustness to noise",
    "checked": true,
    "id": "32002b4e55d65ce530577db19ecaf580a21cc840",
    "semantic_title": "environmental sound classification with parallel temporal-spectral attention",
    "citation_count": 37
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20i_interspeech.html": {
    "title": "Contrastive Predictive Coding of Audio with an Adversary",
    "volume": "main",
    "abstract": "With the vast amount of audio data available, powerful sound representations can be learned with self-supervised methods even in the absence of explicit annotations. In this work we investigate learning general audio representations directly from raw signals using the Contrastive Predictive Coding objective. We further extend it by leveraging ideas from adversarial machine learning to produce additive perturbations that effectively makes the learning harder, such that the predictive tasks will not be distracted by trivial details. We also look at the effects of different design choices for the objective, including the nonlinear similarity measure and the way the negatives are drawn. Combining these contributions our models are able to considerably outperform previous spectrogram-based unsupervised methods. On AudioSet we observe a relative improvement of 14% in mean average precision over the state of the art with half the size of the training data",
    "checked": true,
    "id": "22aa9614306f0500b34c74c803f07ac0b5322f1e",
    "semantic_title": "contrastive predictive coding of audio with an adversary",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2020/pankajakshan20_interspeech.html": {
    "title": "Memory Controlled Sequential Self Attention for Sound Recognition",
    "volume": "main",
    "abstract": "In this paper we investigate the importance of the extent of memory in sequential self attention for sound recognition. We propose to use a memory controlled sequential self attention mechanism on top of a convolutional recurrent neural network (CRNN) model for polyphonic sound event detection (SED). Experiments on the URBAN-SED dataset demonstrate the impact of the extent of memory on sound recognition performance with the self attention induced SED model. We extend the proposed idea with a multi-head self attention mechanism where each attention head processes the audio embedding with explicit attention width values. The proposed use of memory controlled sequential self attention offers a way to induce relations among frames of sound event tokens. We show that our memory controlled self attention model achieves an event based F-score of 33.92% on the URBAN-SED dataset, outperforming the F-score of 20.10% reported by the model without self attention",
    "checked": true,
    "id": "b49ef7264f3ed96a5a197c80b3119f43a1d88973",
    "semantic_title": "memory controlled sequential self attention for sound recognition",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kim20_interspeech.html": {
    "title": "Dual Stage Learning Based Dynamic Time-Frequency Mask Generation for Audio Event Classification",
    "volume": "main",
    "abstract": "Audio based event recognition becomes quite challenging in real world noisy environments. To alleviate the noise issue, time-frequency mask based feature enhancement methods have been proposed. While these methods with fixed filter settings have been shown to be effective in familiar noise backgrounds, they become brittle when exposed to unexpected noise. To address the unknown noise problem, we develop an approach based on dynamic filter generation learning. In particular, we propose a dual stage dynamic filter generator networks that can be trained to generate a time-frequency mask specifically created for each input audio. Two alternative approaches of training the mask generator network are developed for feature enhancements in high noise environments. Our proposed method shows improved performance and robustness in both clean and unseen noise environments",
    "checked": true,
    "id": "61c6efc1e23665dc002237a4bd4e180760e39202",
    "semantic_title": "dual stage learning based dynamic time-frequency mask generation for audio event classification",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zheng20_interspeech.html": {
    "title": "An Effective Perturbation Based Semi-Supervised Learning Method for Sound Event Detection",
    "volume": "main",
    "abstract": "Mean teacher based methods are increasingly achieving state-of-the-art performance for large-scale weakly labeled and unlabeled sound event detection (SED) tasks in recent DCASE challenges. By penalizing inconsistent predictions under different perturbations, mean teacher methods can exploit large-scale unlabeled data in a self-ensembling manner. In this paper, an effective perturbation based semi-supervised learning (SSL) method is proposed based on the mean teacher method. Specifically, a new independent component (IC) module is proposed to introduce perturbations for different convolutional layers, designed as a combination of batch normalization and dropblock operations. The proposed IC module can reduce correlation between neurons to improve performance. A global statistics pooling based attention module is further proposed to explicitly model inter-dependencies between the time-frequency domain and channels, using statistics information (e.g. mean, standard deviation, max) along different dimensions. This can provide an effective attention mechanism to adaptively re-calibrate the output feature map. Experimental results on Task 4 of the DCASE2018 challenge demonstrate the superiority of the proposed method, achieving about 39.8% F1-score, outperforming the previous winning system's 32.4% by a significant margin",
    "checked": true,
    "id": "b75ae37d137ac354eb2ed42917e461b4dccdc977",
    "semantic_title": "an effective perturbation based semi-supervised learning method for sound event detection",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kao20_interspeech.html": {
    "title": "A Joint Framework for Audio Tagging and Weakly Supervised Acoustic Event Detection Using DenseNet with Global Average Pooling",
    "volume": "main",
    "abstract": "This paper proposes a network architecture mainly designed for audio tagging, which can also be used for weakly supervised acoustic event detection (AED). The proposed network consists of a modified DenseNet as the feature extractor, and a global average pooling (GAP) layer to predict frame-level labels at inference time. This architecture is inspired by the work proposed by Zhou et al., a well-known framework using GAP to localize visual objects given image-level labels. While most of the previous works on weakly supervised AED used recurrent layers with attention-based mechanism to localize acoustic events, the proposed network directly localizes events using the feature map extracted by DenseNet without any recurrent layers. In the audio tagging task of DCASE 2017, our method significantly outperforms the state-of-the-art method in F1 score by 5.3% on the dev set, and 6.0% on the eval set in terms of absolute values. For weakly supervised AED task in DCASE 2018, our model outperforms the state-of-the-art method in event-based F1 by 8.1% on the dev set, and 0.5% on the eval set in terms of absolute values, by using data augmentation and tri-training to leverage unlabeled data",
    "checked": true,
    "id": "0a0aa4d370beb182e542bcd2275d3df6bb0132b8",
    "semantic_title": "a joint framework for audio tagging and weakly supervised acoustic event detection using densenet with global average pooling",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chang20_interspeech.html": {
    "title": "Intra-Utterance Similarity Preserving Knowledge Distillation for Audio Tagging",
    "volume": "main",
    "abstract": "Knowledge Distillation (KD) is a popular area of research for reducing the size of large models while still maintaining good performance. The outputs of larger teacher models are used to guide the training of smaller student models. Given the repetitive nature of acoustic events, we propose to leverage this information to regulate the KD training for Audio Tagging. This novel KD method, Intra-Utterance Similarity Preserving KD (IUSP), shows promising results for the audio tagging task. It is motivated by the previously published KD method: Similarity Preserving KD (SP). However, instead of preserving the pairwise similarities between inputs within a mini-batch, our method preserves the pairwise similarities between the frames of a single input utterance. Our proposed KD method, IUSP, shows consistent improvements over SP across student models of different sizes on the DCASE 2019 Task 5 dataset for audio tagging. There is a 27.1% to 122.4% percent increase in improvement of micro AUPRC over the baseline relative to SPs improvement of over the baseline",
    "checked": true,
    "id": "9d2940ef6b4b24c20077b481d785523baac679c3",
    "semantic_title": "intra-utterance similarity preserving knowledge distillation for audio tagging",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2020/park20b_interspeech.html": {
    "title": "Two-Stage Polyphonic Sound Event Detection Based on Faster R-CNN-LSTM with Multi-Token Connectionist Temporal Classification",
    "volume": "main",
    "abstract": "We propose a two-stage sound event detection (SED) model to deal with sound events overlapping in time-frequency. In the first stage which consists of a faster R-CNN and an attention-LSTM, each log-mel spectrogram segment is divided into one or more proposed regions (PRs) according to the coordinates of a region proposal network. To efficiently train polyphonic sound, we take only one PR for each sound event from a bounding box regressor associated with the attention-LSTM. In the second stage, the original input image and the difference image between adjacent segments are separately pooled according to the coordinate of each PR predicted in the first stage. Then, two feature maps using CNNs are concatenated and processed further by LSTM. Finally, CTC-based n-best SED is conducted using the softmax output from the CNN-LSTM, where CTC has two tokens for each event so that the start and ending time frames are accurately detected. Experiments on SED using DCASE 2019 Task 3 show that the proposed two-stage model with multi-token CTC achieves an F1-score of 97.5%, while the first stage alone and the two-stage model with a conventional CTC yield F1-scores of 91.9% and 95.6%, respectively",
    "checked": true,
    "id": "5a1faf63c7e7007c9fbd248f63687c5708e2eac8",
    "semantic_title": "two-stage polyphonic sound event detection based on faster r-cnn-lstm with multi-token connectionist temporal classification",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2020/jindal20_interspeech.html": {
    "title": "SpeechMix — Augmenting Deep Sound Recognition Using Hidden Space Interpolations",
    "volume": "main",
    "abstract": "This paper presents SpeechMix, a regularization and data augmentation technique for deep sound recognition. Our strategy is to create virtual training samples by interpolating speech samples in hidden space. SpeechMix has the potential to generate an infinite number of new augmented speech samples since the combination of speech samples is continuous. Thus, it allows downstream models to avoid overfitting drastically. Unlike other mixing strategies that only work on the input space, we apply our method on the intermediate layers to capture a broader representation of the feature space. Through an extensive quantitative evaluation, we demonstrate the effectiveness of SpeechMix in comparison to standard learning regimes and previously applied mixing strategies. Furthermore, we highlight how different hidden layers contribute to the improvements in classification using an ablation study",
    "checked": false,
    "id": "a50680d557f74456eda52aa30f96c87b77c83661",
    "semantic_title": "speechmix - augmenting deep sound recognition using hidden space interpolations",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2020/radfar20_interspeech.html": {
    "title": "End-to-End Neural Transformer Based Spoken Language Understanding",
    "volume": "main",
    "abstract": "Spoken language understanding (SLU) refers to the process of inferring the semantic information from audio signals. While the neural transformers consistently deliver the best performance among the state-of-the-art neural architectures in field of natural language processing (NLP), their merits in a closely related field, i.e., spoken language understanding (SLU) have not been investigated. In this paper, we introduce an end-to-end neural transformer-based SLU model that can predict the variable-length domain, intent, and slots vectors embedded in an audio signal with no intermediate token prediction architecture. This new architecture leverages the self-attention mechanism by which the audio signal is transformed to various sub-subspaces allowing to extract the semantic context implied by an utterance. Our end-to-end transformer SLU predicts the domains, intents and slots in the Fluent Speech Commands dataset with accuracy equal to 98.1%, 99.6%, and 99.6%, respectively and outperforms the SLU models that leverage a combination of recurrent and convolutional neural networks by 1.4% while the size of our model is 25% smaller than that of these architectures. Additionally, due to independent sub-space projections in the self-attention layer, the model is highly parallelizable which makes it a good candidate for on-device SLU",
    "checked": true,
    "id": "c0f2ce69250fb1c995fec0fb2567ac495e1aefe6",
    "semantic_title": "end-to-end neural transformer based spoken language understanding",
    "citation_count": 51
  },
  "https://www.isca-speech.org/archive/interspeech_2020/liu20e_interspeech.html": {
    "title": "Jointly Encoding Word Confusion Network and Dialogue Context with BERT for Spoken Language Understanding",
    "volume": "main",
    "abstract": "Spoken Language Understanding (SLU) converts hypotheses from automatic speech recognizer (ASR) into structured semantic representations. ASR recognition errors can severely degenerate the performance of the subsequent SLU module. To address this issue, word confusion networks (WCNs) have been used as the input for SLU, which contain richer information than 1-best or n-best hypotheses list. To further eliminate ambiguity, the last system act of dialogue context is also utilized as additional input. In this paper, a novel BERT based SLU model (WCN-BERT SLU) is proposed to encode WCNs and the dialogue context jointly. It can integrate both structural information and ASR posterior probabilities of WCNs in the BERT architecture. Experiments on DSTC2, a benchmark of SLU, show that the proposed method is effective and can outperform previous state-of-the-art models significantly",
    "checked": true,
    "id": "5a5143ef339c01689c5ed0c8fd054799fb25bc2a",
    "semantic_title": "jointly encoding word confusion network and dialogue context with bert for spoken language understanding",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2020/rao20_interspeech.html": {
    "title": "Speech to Semantics: Improve ASR and NLU Jointly via All-Neural Interfaces",
    "volume": "main",
    "abstract": "We consider the problem of spoken language understanding (SLU) of extracting natural language intents and associated slot arguments or named entities from speech that is primarily directed at voice assistants. Such a system subsumes both automatic speech recognition (ASR) as well as natural language understanding (NLU). An end-to-end joint SLU model can be built to a required specification opening up the opportunity to deploy on hardware constrained scenarios like devices enabling voice assistants to work offline, in a privacy preserving manner, whilst also reducing server costs We first present models that extract utterance intent directly from speech without intermediate text output. We then present a compositional model, which generates the transcript using the Listen Attend Spell ASR system and then extracts interpretation using a neural NLU model. Finally, we contrast these methods to a jointly trained end-to-end joint SLU model, consisting of ASR and NLU subsystems which are connected by a neural network based interface instead of text, that produces transcripts as well as NLU interpretation. We show that the jointly trained model shows improvements to ASR incorporating semantic information from NLU and also improves NLU by exposing it to ASR confusion encoded in the hidden layer",
    "checked": true,
    "id": "6525c1e5e40f3b7819bffc9f83658d476b23130a",
    "semantic_title": "speech to semantics: improve asr and nlu jointly via all-neural interfaces",
    "citation_count": 34
  },
  "https://www.isca-speech.org/archive/interspeech_2020/denisov20_interspeech.html": {
    "title": "Pretrained Semantic Speech Embeddings for End-to-End Spoken Language Understanding via Cross-Modal Teacher-Student Learning",
    "volume": "main",
    "abstract": "Spoken language understanding is typically based on pipeline architectures including speech recognition and natural language understanding steps. These components are optimized independently to allow usage of available data, but the overall system suffers from error propagation. In this paper, we propose a novel training method that enables pretrained contextual embeddings to process acoustic features. In particular, we extend it with an encoder of pretrained speech recognition systems in order to construct end-to-end spoken language understanding systems. Our proposed method is based on the teacher-student framework across speech and text modalities that aligns the acoustic and the semantic latent spaces. Experimental results in three benchmarks show that our system reaches the performance comparable to the pipeline architecture without using any training data and outperforms it after fine-tuning with ten examples per class on two out of three benchmarks",
    "checked": true,
    "id": "547a136e7fc26dc9feeae12886ddd9247679d7a3",
    "semantic_title": "pretrained semantic speech embeddings for end-to-end spoken language understanding via cross-modal teacher-student learning",
    "citation_count": 23
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chetupalli20_interspeech.html": {
    "title": "Context Dependent RNNLM for Automatic Transcription of Conversations",
    "volume": "main",
    "abstract": "Conversational speech, while being unstructured at an utterance level, typically has a macro topic which provides larger context spanning multiple utterances. The current language models in speech recognition systems using recurrent neural networks (RNNLM) rely mainly on the local context and exclude the larger context. In order to model the long term dependencies of words across multiple sentences, we propose a novel architecture where the words from prior utterances are converted to an embedding. The relevance of these embeddings for the prediction of next word in the current sentence is found using a gating network. The relevance weighted context embedding vector is combined in the language model to improve the next word prediction, and the entire model including the context embedding and the relevance weighting layers is jointly learned for a conversational language modeling task. Experiments are performed on two conversational datasets — AMI corpus and the Switchboard corpus. In these tasks, we illustrate that the proposed approach yields significant improvements in language model perplexity over the RNNLM baseline. In addition, the use of proposed conversational LM for ASR rescoring results in absolute WER reduction of 1.2% on Switchboard dataset and 1.0% on AMI dataset over the RNNLM based ASR baseline",
    "checked": true,
    "id": "e7c2f44eff4a50f7295a8d71fe579d4d97344252",
    "semantic_title": "context dependent rnnlm for automatic transcription of conversations",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tian20b_interspeech.html": {
    "title": "Improving End-to-End Speech-to-Intent Classification with Reptile",
    "volume": "main",
    "abstract": "End-to-end spoken language understanding (SLU) systems have many advantages over conventional pipeline systems, but collecting in-domain speech data to train an end-to-end system is costly and time consuming. One question arises from this: how to train an end-to-end SLU with limited amounts of data? Many researchers have explored approaches that make use of other related data resources, typically by pre-training parts of the model on high-resource speech recognition. In this paper, we suggest improving the generalization performance of SLU models with a non-standard learning algorithm, Reptile. Though Reptile was originally proposed for model-agnostic meta learning, we argue that it can also be used to directly learn a target task and result in better generalization than conventional gradient descent. In this work, we employ Reptile to the task of end-to-end spoken intent classification. Experiments on four datasets of different languages and domains show improvement of intent prediction accuracy, both when Reptile is used alone and used in addition to pre-training",
    "checked": true,
    "id": "5248cc6ee4f5a6cb32e23a87c6eb28ccebf4051a",
    "semantic_title": "improving end-to-end speech-to-intent classification with reptile",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2020/cho20_interspeech.html": {
    "title": "Speech to Text Adaptation: Towards an Efficient Cross-Modal Distillation",
    "volume": "main",
    "abstract": "Speech is one of the most effective means of communication and is full of information that helps the transmission of utterer's thoughts. However, mainly due to the cumbersome processing of acoustic features, phoneme or word posterior probability has frequently been discarded in understanding the natural language. Thus, some recent spoken language understanding (SLU) modules have utilized end-to-end structures that preserve the uncertainty information. This further reduces the propagation of speech recognition error and guarantees computational efficiency. We claim that in this process, the speech comprehension can benefit from the inference of massive pre-trained language models (LMs). We transfer the knowledge from a concrete Transformer-based text LM to an SLU module which can face a data shortage, based on recent cross-modal distillation methodologies. We demonstrate the validity of our proposal upon the performance on Fluent Speech Command, an English SLU benchmark. Thereby, we experimentally verify our hypothesis that the knowledge could be shared from the top layer of the LM to a fully speech-based module, in which the abstracted speech is expected to meet the semantic representation",
    "checked": true,
    "id": "0d5df0745bc961e2e76aef9a940172226ac1172b",
    "semantic_title": "speech to text adaptation: towards an efficient cross-modal distillation",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ruan20_interspeech.html": {
    "title": "Towards an ASR Error Robust Spoken Language Understanding System",
    "volume": "main",
    "abstract": "A modern Spoken Language Understanding (SLU) system usually contains two sub-systems, Automatic Speech Recognition (ASR) and Natural Language Understanding (NLU), where ASR transforms voice signal to text form and NLU provides intent classification and slot filling from the text. In practice, such decoupled ASR/NLU design facilitates fast model iteration for both components. However, this makes downstream NLU susceptible to errors from the upstream ASR, causing significant performance degradation. Therefore, dealing with such errors is a major opportunity to improve overall SLU model performance. In this work, we first propose a general evaluation criterion that requires an ASR error robust model to perform well on both transcription and ASR hypothesis. Then robustness training techniques for both classification task and NER task are introduced. Experimental results on two datasets show that our proposed approaches improve model robustness to ASR errors for both tasks",
    "checked": true,
    "id": "f504e124de6ea694367e0aa3276df629eab69976",
    "semantic_title": "towards an asr error robust spoken language understanding system",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kuo20_interspeech.html": {
    "title": "End-to-End Spoken Language Understanding Without Full Transcripts",
    "volume": "main",
    "abstract": "An essential component of spoken language understanding (SLU) is slot filling: representing the meaning of a spoken utterance using semantic entity labels. In this paper, we develop end-to-end (E2E) spoken language understanding systems that directly convert speech input to semantic entities and investigate if these E2E SLU models can be trained solely on semantic entity annotations without word-for-word transcripts. Training such models is very useful as they can drastically reduce the cost of data collection. We created two types of such speech-to-entities models, a CTC model and an attention-based encoder-decoder model, by adapting models trained originally for speech recognition. Given that our experiments involve speech input, these systems need to recognize both the entity label and words representing the entity value correctly. For our speech-to-entities experiments on the ATIS corpus, both the CTC and attention models showed impressive ability to skip non-entity words: there was little degradation when trained on just entities versus full transcripts. We also explored the scenario where the entities are in an order not necessarily related to spoken order in the utterance. With its ability to do re-ordering, the attention model did remarkably well, achieving only about 2% degradation in speech-to-bag-of-entities F1 score",
    "checked": true,
    "id": "8f2386acebdcc7ff4363a1c83e324f1142030c78",
    "semantic_title": "end-to-end spoken language understanding without full transcripts",
    "citation_count": 23
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gopalakrishnan20_interspeech.html": {
    "title": "Are Neural Open-Domain Dialog Systems Robust to Speech Recognition Errors in the Dialog History? An Empirical Study",
    "volume": "main",
    "abstract": "Large end-to-end neural open-domain chatbots are becoming increasingly popular. However, research on building such chatbots has typically assumed that the user input is written in nature and it is not clear whether these chatbots would seamlessly integrate with automatic speech recognition (ASR) models to serve the speech modality. We aim to bring attention to this important question by empirically studying the effects of various types of synthetic and actual ASR hypotheses in the dialog history on TransferTransfo, a state-of-the-art Generative Pre-trained Transformer (GPT) based neural open-domain dialog system from the NeurIPS ConvAI2 challenge. We observe that TransferTransfo trained on written data is very sensitive to such hypotheses introduced to the dialog history during inference time. As a baseline mitigation strategy, we introduce synthetic ASR hypotheses to the dialog history during training and observe marginal improvements, demonstrating the need for further research into techniques to make end-to-end open-domain chatbots fully speech-robust. To the best of our knowledge, this is the first study to evaluate the effects of synthetic and actual ASR hypotheses on a state-of-the-art neural open-domain dialog system and we hope it promotes speech-robustness as an evaluation criterion in open-domain dialog",
    "checked": true,
    "id": "902a5f49f6a072c2c4ae7f26c62b89f030db6b6b",
    "semantic_title": "are neural open-domain dialog systems robust to speech recognition errors in the dialog history? an empirical study",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ding20b_interspeech.html": {
    "title": "AutoSpeech: Neural Architecture Search for Speaker Recognition",
    "volume": "main",
    "abstract": "Speaker recognition systems based on Convolutional Neural Networks (CNNs) are often built with off-the-shelf backbones such as VGG-Net or ResNet. However, these backbones were originally proposed for image classification, and therefore may not be naturally fit for speaker recognition. Due to the prohibitive complexity of manually exploring the design space, we propose the first neural architecture search approach for the speaker recognition tasks, named as AutoSpeech. Our algorithm first identifies the optimal operation combination in a neural cell and then derives a CNN model by stacking the neural cell for multiple times. The final speaker recognition model can be obtained by training the derived CNN model through the standard scheme. To evaluate the proposed approach, we conduct experiments on both speaker identification and speaker verification tasks using the VoxCeleb1 dataset. Results demonstrate that the derived CNN architectures from the proposed approach significantly outperform current speaker recognition systems based on VGG-M, ResNet-18, and ResNet-34 backbones, while enjoying lower model complexity",
    "checked": true,
    "id": "7269b8641bc67a53c2b62757fea44f18f836d41e",
    "semantic_title": "autospeech: neural architecture search for speaker recognition",
    "citation_count": 41
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yu20b_interspeech.html": {
    "title": "Densely Connected Time Delay Neural Network for Speaker Verification",
    "volume": "main",
    "abstract": "Time delay neural network (TDNN) has been widely used in speaker verification tasks. Recently, two TDNN-based models, including extended TDNN (E-TDNN) and factorized TDNN (F-TDNN), are proposed to improve the accuracy of vanilla TDNN. But E-TDNN and F-TDNN increase the number of parameters due to deeper networks, compared with vanilla TDNN. In this paper, we propose a novel TDNN-based model, called densely connected TDNN (D-TDNN), by adopting bottleneck layers and dense connectivity. D-TDNN has fewer parameters than existing TDNN-based models. Furthermore, we propose an improved variant of D-TDNN, called D-TDNN-SS, to employ multiple TDNN branches with short-term and long-term contexts. D-TDNN-SS can integrate the information from multiple TDNN branches with a newly designed channel-wise selection mechanism called statistics-and- selection (SS). Experiments on VoxCeleb datasets show that both D-TDNN and D-TDNN-SS can outperform existing models to achieve state-of-the-art accuracy with fewer parameters, and D-TDNN-SS can achieve better accuracy than D-TDNN",
    "checked": true,
    "id": "be913bd97a8071e7550140249d3b9e73d1d58d63",
    "semantic_title": "densely connected time delay neural network for speaker verification",
    "citation_count": 29
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zheng20b_interspeech.html": {
    "title": "Phonetically-Aware Coupled Network For Short Duration Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "In this paper we propose an end-to-end phonetically-aware coupled network for short duration speaker verification tasks. Phonetic information is shown to be beneficial for identifying short utterances. A coupled network structure is proposed to exploit phonetic information. The coupled convolutional layers allow the network to provide frame-level supervision based on phonetic representations of the corresponding frames. The end-to-end training scheme using triplet loss function provides direct comparison of speech contents between two utterances and hence enabling phonetic-based normalization. Our systems are compared against the current mainstream speaker verification systems on both NIST SRE and VoxCeleb evaluation datasets. Relative reductions of up to 34% in equal error rate are reported",
    "checked": true,
    "id": "ecdbe581215e3697669c036f3827ed2c1e2ca150",
    "semantic_title": "phonetically-aware coupled network for short duration text-independent speaker verification",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2020/jung20_interspeech.html": {
    "title": "Multi-Task Network for Noise-Robust Keyword Spotting and Speaker Verification Using CTC-Based Soft VAD and Global Query Attention",
    "volume": "main",
    "abstract": "Keyword spotting (KWS) and speaker verification (SV) have been studied independently although it is known that acoustic and speaker domains are complementary. In this paper, we propose a multi-task network that performs KWS and SV simultaneously to fully utilize the interrelated domain information. The multi-task network tightly combines sub-networks aiming at performance improvement in challenging conditions such as noisy environments, open-vocabulary KWS, and short-duration SV, by introducing novel techniques of connectionist temporal classification (CTC)-based soft voice activity detection (VAD) and global query attention. Frame-level acoustic and speaker information is integrated with phonetically originated weights so that forms a word-level global representation. Then it is used for the aggregation of feature vectors to generate discriminative embeddings. Our proposed approach shows 4.06% and 26.71% relative improvements in equal error rate (EER) compared to the baselines for both tasks. We also present a visualization example and results of ablation experiments",
    "checked": true,
    "id": "cbb9bf8092a76266ad13782f938c961e2217ffcf",
    "semantic_title": "multi-task network for noise-robust keyword spotting and speaker verification using ctc-based soft vad and global query attention",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wu20b_interspeech.html": {
    "title": "Vector-Based Attentive Pooling for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "The pooling mechanism plays an important role in deep neural network based systems for text-independent speaker verification, which aggregates the variable-length frame-level vector sequence across all frames into a fixed-dimensional utterance-level representation. Previous attentive pooling methods employ scalar attention weights for each frame-level vector, resulting in insufficient collection of discriminative information. To address this issue, this paper proposes a vector-based attentive pooling method, which adopts vectorial attention instead of scalar attention. The vectorial attention can extract fine-grained features for discriminating different speakers. Besides, the vector-based attentive pooling is extended in a multi-head way for better speaker embeddings from multiple aspects. The proposed pooling method is evaluated with the x-vector baseline system. Experiments are conducted on two public datasets, VoxCeleb and Speaker in the Wild (SITW). The results show that the vector-based attentive pooling method achieves superior performance compared with statistics pooling and three state-of-the-art attentive pooling methods, with the best equal error rate (EER) of 2.734 and 3.062 in SITW as well as the best EER of 2.466 in VoxCeleb",
    "checked": true,
    "id": "0d620f53d7d6fbcaa325d8a130d462e0f0517279",
    "semantic_title": "vector-based attentive pooling for text-independent speaker verification",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2020/safari20_interspeech.html": {
    "title": "Self-Attention Encoding and Pooling for Speaker Recognition",
    "volume": "main",
    "abstract": "The computing power of mobile devices limits the end-user applications in terms of storage size, processing, memory and energy consumption. These limitations motivate researchers for the design of more efficient deep models. On the other hand, self-attention networks based on Transformer architecture have attracted remarkable interests due to their high parallelization capabilities and strong performance on a variety of Natural Language Processing (NLP) applications. Inspired by the Transformer, we propose a tandem Self-Attention Encoding and Pooling (SAEP) mechanism to obtain a discriminative speaker embedding given non-fixed length speech utterances. SAEP is a stack of identical blocks solely relied on self-attention and position-wise feed-forward networks to create vector representation of speakers. This approach encodes short-term speaker spectral features into speaker embeddings to be used in text-independent speaker verification. We have evaluated this approach on both VoxCeleb1 & 2 datasets. The proposed architecture is able to outperform the baseline x-vector, and shows competitive performance to some other benchmarks based on convolutions, with a significant reduction in model size. It employs 94%, 95%, and 73% less parameters compared to ResNet-34, ResNet-50, and x-vector, respectively. This indicates that the proposed fully attention based architecture is more efficient in extracting time-invariant features from speaker utterances",
    "checked": true,
    "id": "d75def276d70f9468811a93eff1670a4e6ca5091",
    "semantic_title": "self-attention encoding and pooling for speaker recognition",
    "citation_count": 47
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20f_interspeech.html": {
    "title": "ARET: Aggregated Residual Extended Time-Delay Neural Networks for Speaker Verification",
    "volume": "main",
    "abstract": "The time-delay neural network (TDNN) is widely used in speaker verification to extract long-term temporal features of speakers. Although common TDNN approaches well capture time-sequential information, they lack the delicate transformations needed for deep representation. To solve this problem, we propose two TDNN architectures. RET integrates shortcut connections into conventional time-delay blocks, and ARET adopts a split-transform-merge strategy to extract more discriminative representation. Experiments on VoxCeleb datasets without augmentation indicate that ARET realizes satisfactory performance on the VoxCeleb1 test set, VoxCeleb1-E, and VoxCeleb1-H, with 1.389%, 1.520%, and 2.614% equal error rate (EER), respectively. Compared to state-of-the-art results on these test sets, RET achieves a 23%~43% relative reduction in EER, and ARET reaches 32%~45%",
    "checked": true,
    "id": "b23a06e93f6f5ad6cf4076f3c31cc788de4e301e",
    "semantic_title": "aret: aggregated residual extended time-delay neural networks for speaker verification",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20g_interspeech.html": {
    "title": "Adversarial Separation Network for Speaker Recognition",
    "volume": "main",
    "abstract": "Deep neural networks (DNN) have achieved great success in speaker recognition systems. However, it is observed that DNN based systems are easily deceived by adversarial examples leading to wrong predictions. Adversarial examples, which are generated by adding purposeful perturbations on natural examples, pose a serious security threat. In this study, we propose the adversarial separation network ( AS-Net) to protect the speaker recognition system against adversarial attacks. Our proposed AS-Net is featured by its ability to separate adversarial perturbation from the test speech to restore the natural clean speech. As a standalone component, each input speech is pre-processed by AS-Net first. Furthermore, we incorporate the compression structure and the speaker quality loss to enhance the capacity of the AS-Net. Experimental results on the VCTK dataset demonstrated that the AS-Net effectively enhanced the robustness of speaker recognition systems against adversarial examples. It also significantly outperformed other state-of-the-art adversarial-detection mechanisms, including adversarial perturbation elimination network (APE-GAN), feature squeezing, and adversarial training",
    "checked": true,
    "id": "d11a3dc55c45043f9506952315b0e31baa0f1d0c",
    "semantic_title": "adversarial separation network for speaker recognition",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20j_interspeech.html": {
    "title": "Text-Independent Speaker Verification with Dual Attention Network",
    "volume": "main",
    "abstract": "This paper presents a novel design of attention model for text-independent speaker verification. The model takes a pair of input utterances and generates an utterance-level embedding to represent speaker-specific characteristics in each utterance. The input utterances are expected to have highly similar embeddings if they are from the same speaker. The proposed attention model consists of a self-attention module and a mutual attention module, which jointly contributes to the generation of the utterance-level embedding. The self-attention weights are computed from the utterance itself while the mutual-attention weights are computed with the involvement of the other utterance in the input pairs. As a result, each utterance is represented by a self-attention weighted embedding and a mutual-attention weighted embedding. The similarity between the embeddings is measured by a cosine distance score and a binary classifier output score. The whole model, named Dual Attention Network, is trained end-to-end on Voxceleb database. The evaluation results on Voxceleb 1 test set show that the Dual Attention Network significantly outperforms the baseline systems. The best result yields an equal error rate of 1.6%",
    "checked": true,
    "id": "6f1eeac03077409f83a5e787db0950296477d960",
    "semantic_title": "text-independent speaker verification with dual attention network",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2020/qu20_interspeech.html": {
    "title": "Evolutionary Algorithm Enhanced Neural Architecture Search for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "State-of-the-art speaker verification models are based on deep learning techniques, which heavily depend on the hand-designed neural architectures from experts or engineers. We borrow the idea of neural architecture search (NAS) for the text-independent speaker verification task. As NAS can learn deep network structures automatically, we introduce the NAS conception into the well-known x-vector network. Furthermore, this paper proposes an evolutionary algorithm enhanced neural architecture search method called Auto-Vector to automatically discover promising networks for the speaker verification task. The experimental results demonstrate our NAS-based model outperforms state-of-the-art speaker verification models",
    "checked": true,
    "id": "dfa78e42d91aef11bdd333c43eabcb9b9323302a",
    "semantic_title": "evolutionary algorithm enhanced neural architecture search for text-independent speaker verification",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2020/weng20_interspeech.html": {
    "title": "Minimum Bayes Risk Training of RNN-Transducer for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "In this work, we propose minimum Bayes risk (MBR) training of RNN-Transducer (RNN-T) for end-to-end speech recognition. Specifically, initialized with a RNN-T trained model, MBR training is conducted via minimizing the expected edit distance between the reference label sequence and on-the-fly generated N-best hypothesis. We also introduce a heuristic to incorporate an external neural network language model (NNLM) in RNN-T beam search decoding and explore MBR training with the external NNLM. Experimental results demonstrate an MBR trained model outperforms a RNN-T trained model substantially and further improvements can be achieved if trained with an external NNLM. Our best MBR trained system achieves absolute character error rate (CER) reductions of 1.2% and 0.5% on read and spontaneous Mandarin speech respectively over a strong convolution and transformer based RNN-T baseline trained on ~21,000 hours of speech",
    "checked": true,
    "id": "04fea2e8edc678532dbf406346fb8d868712041a",
    "semantic_title": "minimum bayes risk training of rnn-transducer for end-to-end speech recognition",
    "citation_count": 35
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20j_interspeech.html": {
    "title": "Semantic Mask for Transformer Based End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "Attention-based encoder-decoder model has achieved impressive results for both automatic speech recognition (ASR) and text-to-speech (TTS) tasks. This approach takes advantage of the memorization capacity of neural networks to learn the mapping from the input sequence to the output sequence from scratch, without the assumption of prior knowledge such as the alignments. However, this model is prone to overfitting, especially when the amount of training data is limited. Inspired by SpecAugment and BERT, in this paper, we propose a semantic mask based regularization for training such kind of end-to-end (E2E) model. The idea is to mask the input features corresponding to a particular output token, e.g., a word or a word-piece, in order to encourage the model to fill the token based on the contextual information. While this approach is applicable to the encoder-decoder framework with any type of neural network architecture, we study the transformer-based model for ASR in this work. We perform experiments on Librispeech 960h and TedLium2 data sets, and achieve the state-of-the-art performance on the test set in the scope of E2E models",
    "checked": true,
    "id": "3da68533095f8401fe5b754d885dc3de704dae5e",
    "semantic_title": "semantic mask for transformer based end-to-end speech recognition",
    "citation_count": 49
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20h_interspeech.html": {
    "title": "Faster, Simpler and More Accurate Hybrid ASR Systems Using Wordpieces",
    "volume": "main",
    "abstract": "In this work, we first show that on the widely used LibriSpeech benchmark, our transformer-based context-dependent connectionist temporal classification (CTC) system produces state-of-the-art results. We then show that using wordpieces as modeling units combined with CTC training, we can greatly simplify the engineering pipeline compared to conventional frame-based cross-entropy training by excluding all the GMM bootstrapping, decision tree building and force alignment steps, while still achieving very competitive word-error-rate. Additionally, using wordpieces as modeling units can significantly improve runtime efficiency since we can use larger stride without losing accuracy. We further confirm these findings on two internal VideoASR datasets: German, which is similar to English as a fusional language, and Turkish, which is an agglutinative language",
    "checked": false,
    "id": "3a5774f0a415bcc590a5ac948dcf265cb02de977",
    "semantic_title": "fast, simpler and more accurate hybrid asr systems using wordpieces",
    "citation_count": 24
  },
  "https://www.isca-speech.org/archive/interspeech_2020/dimitriadis20_interspeech.html": {
    "title": "A Federated Approach in Training Acoustic Models",
    "volume": "main",
    "abstract": "In this paper, a novel platform for Acoustic Model training based on Federated Learning (FL) is described. This is the first attempt to introduce Federated Learning techniques in Speech Recognition (SR) tasks. Besides the novelty of the task, the paper describes an easily generalizable FL platform and presents the design decisions used for this task. Amongst the novel algorithms introduced is a hierarchical optimization scheme employing pairs of optimizers and an algorithm for gradient selection, leading to improvements in training time and SR performance. The gradient selection algorithm is based on weighting the gradients during the aggregation step. It effectively acts as a regularization process right before the gradient propagation. This process may address one of the FL challenges, i.e. training on vastly heterogeneous data. The experimental validation of the proposed system is based on the LibriSpeech task, presenting a speed-up of ×1.5 and 6% WERR. The proposed Federated Learning system appears to outperform the golden standard of distributed training in both convergence speed and overall model performance. Further improvements have been experienced in internal tasks",
    "checked": true,
    "id": "877c90e38fabf71b2fc6bed68b243ebd4d07b692",
    "semantic_title": "a federated approach in training acoustic models",
    "citation_count": 29
  },
  "https://www.isca-speech.org/archive/interspeech_2020/sheikh20_interspeech.html": {
    "title": "On Semi-Supervised LF-MMI Training of Acoustic Models with Limited Data",
    "volume": "main",
    "abstract": "This work investigates semi-supervised training of acoustic models (AM) with the lattice-free maximum mutual information (LF-MMI) objective in practically relevant scenarios with a limited amount of labeled in-domain data. An error detection driven semi-supervised AM training approach is proposed, in which an error detector controls the hypothesized transcriptions or lattices used as LF-MMI training targets on additional unlabeled data. Under this approach, our first method uses a single error-tagged hypothesis whereas our second method uses a modified supervision lattice. These methods are evaluated and compared with existing semi-supervised AM training methods in three different matched or mismatched, limited data setups. Word error recovery rates of 28 to 89% are reported",
    "checked": true,
    "id": "9d09ee3b5d91af56adf85001c362b1b92fbfc5f5",
    "semantic_title": "on semi-supervised lf-mmi training of acoustic models with limited data",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gao20b_interspeech.html": {
    "title": "On Front-End Gain Invariant Modeling for Wake Word Spotting",
    "volume": "main",
    "abstract": "Wake word (WW) spotting is challenging in far-field due to the complexities and variations in acoustic conditions and the environmental interference in signal transmission. A suite of carefully designed and optimized audio front-end (AFE) algorithms help mitigate these challenges and provide better quality audio signals to the downstream modules such as WW spotter. Since the WW model is trained with the AFE-processed audio data, its performance is sensitive to AFE variations, such as gain changes. In addition, when deploying to new devices, the WW performance is not guaranteed because the AFE is unknown to the WW model. To address these issues, we propose a novel approach to use a new feature called ΔLFBE to decouple the AFE gain variations from the WW model. We modified the neural network architectures to accommodate the delta computation, with the feature extraction module unchanged. We evaluate our WW models using data collected from real household settings and showed the models with the ΔLFBE is robust to AFE gain changes. Specifically, when AFE gain changes up to ±12dB, the baseline CNN model lost up to relative 19.0% in false alarm rate or 34.3% in false reject rate, while the model with ΔLFBE demonstrates no performance loss",
    "checked": true,
    "id": "212a07356a93361d3b33d27594ac223787d99ee3",
    "semantic_title": "on front-end gain invariant modeling for wake word spotting",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ding20c_interspeech.html": {
    "title": "Unsupervised Regularization-Based Adaptive Training for Speech Recognition",
    "volume": "main",
    "abstract": "In this paper, we propose two novel regularization-based speaker adaptive training approaches for connectionist temporal classification (CTC) based speech recognition. The first method is center loss (CL) regularization, which is used to penalize the distances between the embeddings of different speakers and the only center. The second method is speaker variance loss (SVL) regularization in which we directly minimize the speaker interclass variance during model training. Both methods achieve the purpose of training an adaptive model on the fly by adding regularization terms to the training loss function. Our experiment on the AISHELL-1 Mandarin recognition task shows that both methods are effective at adapting the CTC model without requiring any specific fine-tuning or additional complexity, achieving character error rate improvements of up to 8.1% and 8.6% over the speaker independent (SI) model, respectively",
    "checked": true,
    "id": "a588ddebcd345891a650b4fb02d7d1b04012c39b",
    "semantic_title": "unsupervised regularization-based adaptive training for speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/loweimi20_interspeech.html": {
    "title": "On the Robustness and Training Dynamics of Raw Waveform Models",
    "volume": "main",
    "abstract": "We investigate the robustness and training dynamics of raw waveform acoustic models for automatic speech recognition (ASR). It is known that the first layer of such models learn a set of filters, performing a form of time-frequency analysis. This layer is liable to be under-trained owing to gradient vanishing, which can negatively affect the network performance. Through a set of experiments on TIMIT, Aurora-4 and WSJ datasets, we investigate the training dynamics of the first layer by measuring the evolution of its average frequency response over different epochs. We demonstrate that the network efficiently learns an optimal set of filters with a high spectral resolution and the dynamics of the first layer highly correlates with the dynamics of the cross entropy (CE) loss and word error rate (WER). In addition, we study the robustness of raw waveform models in both matched and mismatched conditions. The accuracy of these models is found to be comparable to, or better than, their MFCC-based counterparts in matched conditions and notably improved by using a better alignment. The role of raw waveform normalisation was also examined and up to 4.3% absolute WER reduction in mismatched conditions was achieved",
    "checked": true,
    "id": "35bf88dc0f12d0297d9204b0b300529f6077d40e",
    "semantic_title": "on the robustness and training dynamics of raw waveform models",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2020/xu20b_interspeech.html": {
    "title": "Iterative Pseudo-Labeling for Speech Recognition",
    "volume": "main",
    "abstract": "Pseudo-labeling has recently shown promise in end-to-end automatic speech recognition (ASR). We study Iterative Pseudo-Labeling (IPL), a semi-supervised algorithm which efficiently performs multiple iterations of pseudo-labeling on unlabeled data as the acoustic model evolves. In particular, IPL fine tunes an existing model at each iteration using both labeled data and a subset of unlabeled data. We study the main components of IPL: decoding with a language model and data augmentation. We then demonstrate the effectiveness of IPL by achieving state-of-the-art word-error rate on the LibriSpeech test sets in both standard and low-resource setting. We also study the effect of language models trained on different corpora to show IPL can effectively utilize additional text. Finally, we release a new large in-domain text corpus which does not overlap with the LibriSpeech training transcriptions to foster research in low-resource, semi-supervised ASR",
    "checked": true,
    "id": "1977998d796a0cc15de11a8713181ab468c398a1",
    "semantic_title": "iterative pseudo-labeling for speech recognition",
    "citation_count": 92
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kawamura20_interspeech.html": {
    "title": "Smart Tube: A Biofeedback System for Vocal Training and Therapy Through Tube Phonation",
    "volume": "main",
    "abstract": "Tube phonation, or straw phonation, is a frequently used vocal training technique to improve the efficiency of the vocal mechanism by repeatedly producing a speech sound into a tube or straw. Use of the straw results in a semi-occluded vocal tract in order to maximize the interaction between the vocal fold vibration and the vocal tract. This method requires a voice trainer or therapist to raise the trainee or patient's awareness of the vibrations around his or her mouth, guiding him/her to maximize the vibrations, which results in efficient phonation. A major problem with this process is that the trainer cannot monitor the trainee/patient's vibratory state in a quantitative manner. This study proposes the use of Smart Tube, a straw with an attached acceleration sensor and LED strip that can measure vibrations and provide corresponding feedback through LED lights in real-time. The biofeedback system was implemented using a microcontroller board, Arduino Uno, to minimize cost. Possible system function enhancements include Bluetooth compatibility with personal computers and/or smartphones. Smart Tube can facilitate improved phonation for trainees/patients by providing quantitative visual feedback",
    "checked": true,
    "id": "ab75796105a58dffee7ce03abc9d81181deff8dc",
    "semantic_title": "smart tube: a biofeedback system for vocal training and therapy through tube phonation",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/choi20_interspeech.html": {
    "title": "VCTUBE : A Library for Automatic Speech Data Annotation",
    "volume": "main",
    "abstract": "We introduce an open-source Python library, VCTUBE, which can automatically generate <audio, text> pair of speech data from a given Youtube URL. We believe VCTUBE is useful for collecting, processing, and annotating speech data easily toward developing speech synthesis systems",
    "checked": true,
    "id": "65c2287b14892ee6002566ba7a7ff4f03d0427b2",
    "semantic_title": "vctube : a library for automatic speech data annotation",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/xie20_interspeech.html": {
    "title": "A Mandarin L2 Learning APP with Mispronunciation Detection and Feedback",
    "volume": "main",
    "abstract": "In this paper, an APP with Mispronunciation Detection and Feedback for Mandarin L2 Learners is shown. The APP could detect the mispronunciation in the words and highlight it with red at the phone level. Also, the score will be shown to evaluate the overall pronunciation. When touching the highlight, the pronunciation of the learner's and the standard's is played. Then the flash animation that describes the movement of the tongue, mouth, and other articulators will be shown to the learner. The learner could repeat the process to improve and excise the pronunciation. The App called ‘SAIT Hànyǔ' can be downloaded at App Store",
    "checked": true,
    "id": "f52ff3f1500a6afc72d2261733aab6bfbaf1be87",
    "semantic_title": "a mandarin l2 learning app with mispronunciation detection and feedback",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2020/udayakumar20_interspeech.html": {
    "title": "Rapid Enhancement of NLP Systems by Acquisition of Data in Correlated Domains",
    "volume": "main",
    "abstract": "In a generation where industries are going through a paradigm shift because of the rampant growth of deep learning, structured data plays a crucial role in the automation of various tasks. Textual structured data is one such kind which is extensively used in systems like chat bots and automatic speech recognition. Unfortunately, a majority of these textual data available is unstructured in the form of user reviews and feedback, social media posts etc. Automating the task of categorizing or clustering these data into meaningful domains will reduce the time and effort needed in building sophisticated human-interactive systems. In this paper, we present a web tool that builds a domain specific data based on a search phrase from a database of highly unstructured user utterances. We also show the usage of Elasticsearch database with custom indexes for full correlated text-search. This tool uses the open sourced Glove model combined with cosine similarity and performs a graph based search to provide semantically and syntactically meaningful corpora. In the end, we discuss its applications with respect to natural language processing",
    "checked": true,
    "id": "1302e702c7e1fba97f91742e764caecfd5d1018d",
    "semantic_title": "rapid enhancement of nlp systems by acquisition of data in correlated domains",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shi20_interspeech.html": {
    "title": "Computer-Assisted Language Learning System: Automatic Speech Evaluation for Children Learning Malay and Tamil",
    "volume": "main",
    "abstract": "We present a computer-assisted language learning system that automatically evaluates the pronunciation and fluency of spoken Malay and Tamil. Our system consists of a server and a user-facing Android application, where the server is responsible for speech-to-text alignment as well as pronunciation and fluency scoring. We describe our system architecture and discuss the technical challenges associated with low resource languages. To the best of our knowledge, this work is the first pronunciation and fluency scoring system for Malay and Tamil",
    "checked": true,
    "id": "699b2d3dc5372fee3e4998511337deea36f40847",
    "semantic_title": "computer-assisted language learning system: automatic speech evaluation for children learning malay and tamil",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2020/saeki20_interspeech.html": {
    "title": "Real-Time, Full-Band, Online DNN-Based Voice Conversion System Using a Single CPU",
    "volume": "main",
    "abstract": "We present a real-time, full-band, online voice conversion (VC) system that uses a single CPU. For practical applications, VC must be high quality and able to perform real-time, online conversion with fewer computational resources. Our system achieves this by combining non-linear conversion with a deep neural network and short-tap, sub-band filtering. We evaluate our system and demonstrate that it 1) achieves the estimated complexity around 2.5 GFLOPS and measures real-time factor (RTF) around 0.5 with a single CPU and 2) can attain converted speech with a 3.4 / 5.0 mean opinion score (MOS) of naturalness",
    "checked": true,
    "id": "c547c339c36bbd9082e79665e534eb0fd2528259",
    "semantic_title": "real-time, full-band, online dnn-based voice conversion system using a single cpu",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2020/feng20b_interspeech.html": {
    "title": "A Dynamic 3D Pronunciation Teaching Model Based on Pronunciation Attributes and Anatomy",
    "volume": "main",
    "abstract": "In this paper, a dynamic three dimensional (3D) head model is introduced which is built based on knowledge of (the human) anatomy and the theory of distinctive features. The model is used to help Chinese learners understand the exact location and method of the phoneme articulation intuitively. You can access the phonetic learning system, choose the target sound you want to learn and then watch the 3D dynamic animations of the phonemes. You can look at the lips, tongue, soft palate, uvula, and other dynamic vocal organs as well as teeth, gums, hard jaw, and other passive vocal organs from different angles. In this process, you can make the skin and some of the muscles semi-transparent, or zoom in or out the model to see the dynamic changes of articulators clearly. By looking at the 3D model, learners can find the exact location of each sound and imitate the pronunciation actions",
    "checked": true,
    "id": "87e2bac1ff95aae5a685c5e6e5a51d97716a7de2",
    "semantic_title": "a dynamic 3d pronunciation teaching model based on pronunciation attributes and anatomy",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kimura20_interspeech.html": {
    "title": "End-to-End Deep Learning Speech Recognition Model for Silent Speech Challenge",
    "volume": "main",
    "abstract": "This work is the first attempt to apply an end-to-end, deep neural network-based automatic speech recognition (ASR) pipeline to the Silent Speech Challenge dataset (SSC), which contains synchronized ultrasound images and lip images captured when a single speaker read the TIMIT corpus without uttering audible sounds. In silent speech research using SSC dataset, established methods in ASR have been utilized with some modifications to use it in visual speech recognition. In this work, we tested the SOTA method of ASR on the SSC dataset using the End-to-End Speech Processing Toolkit, ESPnet. The experimental results show that this end-to-end method achieved a character error rate (CER) of 10.1% and a WER of 20.5% by incorporating SpecAugment, demonstrating the possibility to further improve the performance with additional data collection",
    "checked": true,
    "id": "af1047d15c7f60ece1c477cf7b860f13390fe5d3",
    "semantic_title": "end-to-end deep learning speech recognition model for silent speech challenge",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20k_interspeech.html": {
    "title": "Autosegmental Neural Nets: Should Phones and Tones be Synchronous or Asynchronous?",
    "volume": "main",
    "abstract": "Phones, the segmental units of the International Phonetic Alphabet (IPA), are used for lexical distinctions in most human languages; Tones, the suprasegmental units of the IPA, are used in perhaps 70%. Many previous studies have explored cross-lingual adaptation of automatic speech recognition (ASR) phone models, but few have explored the multilingual and cross-lingual transfer of synchronization between phones and tones. In this paper, we test four Connectionist Temporal Classification (CTC)-based acoustic models, differing in the degree of synchrony they impose between phones and tones. Models are trained and tested multilingually in three languages, then adapted and tested cross-lingually in a fourth. Both synchronous and asynchronous models are effective in both multilingual and cross-lingual settings. Synchronous models achieve lower error rate in the joint phone+tone tier, but asynchronous training results in lower tone error rate",
    "checked": true,
    "id": "58787eae28c47002539396a2df9c38aa2ce652fd",
    "semantic_title": "autosegmental neural nets: should phones and tones be synchronous or asynchronous?",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tachbelie20_interspeech.html": {
    "title": "Development of Multilingual ASR Using GlobalPhone for Less-Resourced Languages: The Case of Ethiopian Languages",
    "volume": "main",
    "abstract": "In this paper, we present the cross-lingual and multilingual experiments we have conducted using existing resources of other languages for the development of speech recognition system for less-resourced languages. In our experiments, we used the Globalphone corpus as source and considered four Ethiopian languages namely Amharic, Oromo, Tigrigna and Wolaytta as targets. We have developed multilingual (ML) Automatic Speech Recognition (ASR) systems and decoded speech of the four Ethiopian languages. A multilingual acoustic model (AM) trained with speech data of 22 Globalphone languages but the target languages, achieved a Word Error Rate (WER) of 15.79%. Moreover, including training speech of one closely related language (in terms of phonetic overlap) in ML AM training resulted in a relative WER reduction of 51.41%. Although adaptation of ML systems did not give significant WER reduction over the monolingual ones, it enables us to rapidly adapt existing ML ASR systems to new languages. In sum, our experiments demonstrated that ASR systems can be developed rapidly with a pronunciation dictionary (PD) of low out of vocabulary (OOV) rate and a strong language model (LM)",
    "checked": true,
    "id": "e8de0a99aa27dea21519d59e428e4ed36ba0208c",
    "semantic_title": "development of multilingual asr using globalphone for less-resourced languages: the case of ethiopian languages",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hou20_interspeech.html": {
    "title": "Large-Scale End-to-End Multilingual Speech Recognition and Language Identification with Multi-Task Learning",
    "volume": "main",
    "abstract": "In this paper, we report a large-scale end-to-end language-independent multilingual model for joint automatic speech recognition (ASR) and language identification (LID). This model adopts hybrid CTC/attention architecture and achieves word error rate (WER) of 52.8 and LID accuracy of 93.5 on 42 languages with around 5000 hours of training data. We also compare the effects of using subword-level or character-level vocabulary for large-scale multilingual tasks. Furthermore, we transfer the pre-trained model to 14 low-resource languages. Results show that the pre-trained model achieves significantly better results than non-pretrained baselines on both language-specific and multilingual low-resource ASR tasks in terms of WER, which is reduced by 28.1% and 11.4% respectively",
    "checked": true,
    "id": "e2e91fcae687655f696926bb88727753a6728a23",
    "semantic_title": "large-scale end-to-end multilingual speech recognition and language identification with multi-task learning",
    "citation_count": 44
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhou20b_interspeech.html": {
    "title": "Multi-Encoder-Decoder Transformer for Code-Switching Speech Recognition",
    "volume": "main",
    "abstract": "Code-switching (CS) occurs when a speaker alternates words of two or more languages within a single sentence or across sentences. Automatic speech recognition (ASR) of CS speech has to deal with two or more languages at the same time. In this study, we propose a Transformer-based architecture with two symmetric language-specific encoders to capture the individual language attributes, that improve the acoustic representation of each language. These representations are combined using a language-specific multi-head attention mechanism in the decoder module. Each encoder and its corresponding attention module in the decoder are pre-trained using a large monolingual corpus aiming to alleviate the impact of limited CS training data. We call such a network a multi-encoder-decoder (MED) architecture. Experiments on the SEAME corpus show that the proposed MED architecture achieves 10.2% and 10.8% relative error rate reduction on the CS evaluation sets with Mandarin and English as the matrix language respectively",
    "checked": true,
    "id": "aaa44f9d68c8ba18d5c5b895156cd37422338c74",
    "semantic_title": "multi-encoder-decoder transformer for code-switching speech recognition",
    "citation_count": 28
  },
  "https://www.isca-speech.org/archive/interspeech_2020/abate20_interspeech.html": {
    "title": "Multilingual Acoustic and Language Modeling for Ethio-Semitic Languages",
    "volume": "main",
    "abstract": "Development of Multilingual Automatic Speech Recognition (ASR) systems enables to share existing speech and text corpora among languages. We have conducted experiments on the development of multilingual Acoustic Models (AM) and Language Models (LM) for Tigrigna. Using Amharic Deep Neural Network (DNN) AM, Tigrigna pronunciation dictionary and trigram LM, we achieved a Word Error Rate (WER) of 30.9% for Tigrigna. Adding training speech from the target language (Tigrigna) to the whole training speech of the donor language (Amharic) continuously reduces WER with the amount of added data. We have also developed different (including recurrent neural networks based) multilingual LMs and achieved a relative WER reduction of 3.56% compared to the use of monolingual trigram LMs. Considering scarcity of computational resources to decode with very large vocabularies, we have also experimented on the use of morphemes as pronunciation and language modeling units. We have achieved character error rate (CER) of 7.9% which is relatively lower by 38.3% to 1.3% than the CER of the word-based models of smaller vocabularies than 162k. Our results show the possibility of developing ASR system for an Ethio-Semitic language using an existing speech and text corpora of another language in the family",
    "checked": true,
    "id": "76d498b19cc07ea221df6b10ad1e7e7ff5362e90",
    "semantic_title": "multilingual acoustic and language modeling for ethio-semitic languages",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hu20c_interspeech.html": {
    "title": "Multilingual Jointly Trained Acoustic and Written Word Embeddings",
    "volume": "main",
    "abstract": "Acoustic word embeddings (AWEs) are vector representations of spoken word segments. AWEs can be learned jointly with embeddings of character sequences, to generate phonetically meaningful embeddings of written words, or acoustically grounded word embeddings (AGWEs). Such embeddings have been used to improve speech retrieval, recognition, and spoken term discovery. In this work, we extend this idea to multiple low-resource languages. We jointly train an AWE model and an AGWE model, using phonetically transcribed data from multiple languages. The pre-trained models can then be used for unseen zero-resource languages, or fine-tuned on data from low-resource languages. We also investigate distinctive features, as an alternative to phone labels, to better share cross-lingual information. We test our models on word discrimination tasks for twelve languages. When trained on eleven languages and tested on the remaining unseen language, our model outperforms traditional unsupervised approaches like dynamic time warping. After fine-tuning the pre-trained models on one hour or even ten minutes of data from a new language, performance is typically much better than training on only the target-language data. We also find that phonetic supervision improves performance over character sequences, and that distinctive feature supervision is helpful in handling unseen phones in the target language",
    "checked": true,
    "id": "d551512cd584f8dc77dedd972c0a3e6ad42a7cbc",
    "semantic_title": "multilingual jointly trained acoustic and written word embeddings",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20l_interspeech.html": {
    "title": "Improving Code-Switching Language Modeling with Artificially Generated Texts Using Cycle-Consistent Adversarial Networks",
    "volume": "main",
    "abstract": "This paper presents our latest effort on improving Code-switching language models that suffer from data scarcity. We investigate methods to augment Code-switching training text data by artificially generating them. Concretely, we propose a cycle-consistent adversarial networks based framework to transfer monolingual text into Code-switching text, considering Code-switching as a speaking style. Our experimental results on the SEAME corpus show that utilizing artificially generated Code-switching text data improves consistently the language model as well as the automatic speech recognition performance",
    "checked": true,
    "id": "6ec93e8d2dd53c73feb7dafaba2d2c0d93593a17",
    "semantic_title": "improving code-switching language modeling with artificially generated texts using cycle-consistent adversarial networks",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hu20d_interspeech.html": {
    "title": "Data Augmentation for Code-Switch Language Modeling by Fusing Multiple Text Generation Methods",
    "volume": "main",
    "abstract": "To deal with the problem of data scarce in training language model (LM) for code-switching (CS) speech recognition, we proposed an approach to obtain augmentation texts from three different viewpoints. The first one is to enhance monolingual LM by selecting corresponding sentences for existing conversational corpora; The second one is based on replacements using syntactic constraint for a monolingual Chinese corpus, with the helps of an aligned word list obtained from a pseudo-parallel corpus, and part-of-speech (POS) of words; The third one is to use text generation based on a pointer-generator network with copy mechanism, using a real CS text data for training. All sentences from these approaches show improvement for CS LMs, and they are finally fused into an LM for CS ASR tasks Evaluations on LMs built by the above augmented data were conducted on two Mandarin-English CS speech sets DTANG, and SEAME. The perplexities were greatly reduced with all kinds of augmented texts, and speech recognition performances were steadily improved. The mixed word error rate (MER) of DTANG and SEAME evaluation dataset got relative reduction by 9.10% and 29.73%, respectively",
    "checked": true,
    "id": "43f5f8511acca56f1dbecfe45f8a6b3b5a741114",
    "semantic_title": "data augmentation for code-switch language modeling by fusing multiple text generation methods",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20m_interspeech.html": {
    "title": "A 43 Language Multilingual Punctuation Prediction Neural Network Model",
    "volume": "main",
    "abstract": "Punctuation prediction is a critical component for speech recognition readability and speech translation segmentation. When considering multiple language support, traditional monolingual neural network models used for punctuation prediction can be costly to manage and may not produce the best accuracy. In this paper, we investigate multilingual Long Short-Term Memory (LSTM) modeling using Byte Pair Encoding (BPE) for punctuation prediction to support 43 languages across 69 countries. Our findings show a single multilingual BPE-based model can achieve similar or even better performance than separate monolingual word-based models by benefiting from shared information across different languages. On an in-domain news text test set, the multilingual model achieves on average 80.2% F1-score while on out-of-domain speech recognition text, it achieves 73.5% F1-score. We also show that the shared information can help in fine-tuning for low-resource languages as well",
    "checked": true,
    "id": "7412abc81724815bf0c41b2af1138cbfc943593f",
    "semantic_title": "a 43 language multilingual punctuation prediction neural network model",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20k_interspeech.html": {
    "title": "Exploring Lexicon-Free Modeling Units for End-to-End Korean and Korean-English Code-Switching Speech Recognition",
    "volume": "main",
    "abstract": "Automatic speech recognition (ASR) tasks are usually solved using lexicon-based hybrid systems or character-based acoustic models to automatically translate speech data into written text. While hybrid systems require a manually designed lexicon, end-to-end models can process character-based speech data. This resolves the need to define a lexicon for non-English languages for which a standard lexicon may be absent. Korean is relatively phonetic and has a unique writing system, and it is thus worth investigating useful modeling units for end-to-end Korean ASR. Our work is the first to compare the performance of deep neural networks (DNNs), designed as a combination of connectionist temporal classification and attention-based encoder-decoder, on various lexicon-free Korean models. Experiments on the Zeroth-Korean dataset and medical records, which consist of Korean-only and Korean-English code-switching corpora respectively, show how DNNs based on syllables and sub-words significantly outperform Jamo-based models on Korean ASR tasks. Our successful application of using lexicon-free modeling units on non-English ASR tasks provides compelling evidence that lexicon-free approaches can alleviate the heavy code-switching involved in non-English medical transcriptions",
    "checked": true,
    "id": "cf73fc9162d2fa63395521fa810933bc6bf3d371",
    "semantic_title": "exploring lexicon-free modeling units for end-to-end korean and korean-english code-switching speech recognition",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2020/platen20_interspeech.html": {
    "title": "Multi-Task Siamese Neural Network for Improving Replay Attack Detection",
    "volume": "main",
    "abstract": "Automatic speaker verification systems are vulnerable to audio replay attacks which bypass security by replaying recordings of authorized speakers. Replay attack detection (RA) systems built upon Residual Neural Networks (ResNet)s have yielded astonishing results on the public benchmark ASVspoof 2019 Physical Access challenge. With most teams using fine-tuned feature extraction pipelines and model architectures, the generalizability of such systems remains questionable though. In this work, we analyse the effect of discriminative feature learning in a multi-task learning (MTL) setting can have on the generalizability and discriminability of RA detection systems. We use a popular ResNet architecture optimized by the cross-entropy criterion as our baseline and compare it to the same architecture optimized by MTL using Siamese Neural Networks (SNN). It can be shown that 26.8% relative improvement on Equal Error Rate (EER) is obtained by leveraging SNN.We further enhance the model's architecture and demonstrate that SNN with additional reconstruction loss yield another significant improvement of relative 13.8% EER",
    "checked": true,
    "id": "e3440a85ea9887af01e2831696e0dc5538abaa15",
    "semantic_title": "multi-task siamese neural network for improving replay attack detection",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2020/akimoto20_interspeech.html": {
    "title": "POCO: A Voice Spoofing and Liveness Detection Corpus Based on Pop Noise",
    "volume": "main",
    "abstract": "We present a new database of voice recordings with the goal of promoting research on protection of automatic speaker verification systems from voice spoofing, such as replay attacks. Specifically, we focus on the liveness feature of live speech, i.e., pop noise, and the corresponding voice recordings without this feature, for the purpose of combating spoofing via liveness detection. Our database includes simultaneous recordings using a microphone array, as well as recordings at various distances and positions. To the best of our knowledge, this is the first publicly available database that has been particularly designed to study the liveness features of voice recordings under various conditions ",
    "checked": true,
    "id": "5473450deb405ebc46749931b51508dd03529f72",
    "semantic_title": "poco: a voice spoofing and liveness detection corpus based on pop noise",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20l_interspeech.html": {
    "title": "Dual-Adversarial Domain Adaptation for Generalized Replay Attack Detection",
    "volume": "main",
    "abstract": "Despite tremendous progress in speaker verification recently, replay spoofing attacks are still a major threat to these systems. Focusing on dataset-specific scenarios, anti-spoofing systems have achieved promising in-domain performance at the cost of poor generalization towards unseen out-of-domain datasets. This is treated as a domain mismatch problem with a domain adversarial training (DAT) framework, which has previously been applied to enhance generalization. However, since only one domain discriminator is adopted, DAT suffers from the false alignment of cross-domain spoofed and genuine pairs, thus failing to acquire a strong spoofing-discriminative capability. In this work, we propose the dual-adversarial domain adaptation (DADA) framework to enable fine-grained alignment of spoofed and genuine data separately by using two domain discriminators, which effectively alleviates the above problem and further improves spoofing detection performance. Experiments on the ASVspoof 2017 V.2 dataset and the physical access portion of BTAS 2016 dataset demonstrate that the proposed DADA framework significantly outperforms the baseline model and DAT framework in cross-domain evaluation scenarios. It is shown that the newly proposed DADA architecture is more robust and effective for generalized replay attack detection",
    "checked": true,
    "id": "914d8143f747d4025afed64e72ab3a1735423822",
    "semantic_title": "dual-adversarial domain adaptation for generalized replay attack detection",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shim20_interspeech.html": {
    "title": "Self-Supervised Pre-Training with Acoustic Configurations for Replay Spoofing Detection",
    "volume": "main",
    "abstract": "Constructing a dataset for replay spoofing detection requires a physical process of playing an utterance and re-recording it, presenting a challenge to the collection of large-scale datasets. In this study, we propose a self-supervised framework for pre-training acoustic configurations using datasets published for other tasks, such as speaker verification. Here, acoustic configurations refer to the environmental factors generated during the process of voice recording but not the voice itself, including microphone types, place and ambient noise levels. Specifically, we select pairs of segments from utterances and train deep neural networks to determine whether the acoustic configurations of the two segments are identical. We validate the effectiveness of the proposed method based on the ASVspoof 2019 physical access dataset utilizing two well-performing systems. The experimental results demonstrate that the proposed method outperforms the baseline approach by 30%",
    "checked": true,
    "id": "6c238824f57a1b88bc6fea7e50b69dc79153f970",
    "semantic_title": "self-supervised pre-training with acoustic configurations for replay spoofing detection",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2020/g20_interspeech.html": {
    "title": "Competency Evaluation in Voice Mimicking Using Acoustic Cues",
    "volume": "main",
    "abstract": "The fusion of i-vector with prosodic features is used to identify the most competent voice imitator through a deep neural network framework (DNN) in this paper. This experiment is conducted by analyzing the spectral and prosodic characteristics during voice imitation. Spectral features include mel-frequency cepstral features (MFCC) and modified group delay features (MODGDF). Prosodic features, computed by the Legendre polynomial approximation, are used as complementary information to the i-vector model. Proposed system evaluates the competence of artists in voice mimicking and ranks them according to the scores from a classifier based on mean opinion score (MOS). If the artist with the highest MOS is identified as rank-1 by the proposed system, a hit occurs. DNN-based classifier makes the decision based on the probability value on the nodes at the output layer. The performance is evaluated using top X-hit criteria on a mimicry dataset. Top-2 hit rate of 81.81% is obtained for fusion experiment. The experiments demonstrate the potential of i-vector framework and its fusion in competency evaluation of voice mimicking",
    "checked": true,
    "id": "26e95043395179ce2b551aa52abda4b0cd94000b",
    "semantic_title": "competency evaluation in voice mimicking using acoustic cues",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wu20c_interspeech.html": {
    "title": "Light Convolutional Neural Network with Feature Genuinization for Detection of Synthetic Speech Attacks",
    "volume": "main",
    "abstract": "Modern text-to-speech (TTS) and voice conversion (VC) systems produce natural sounding speech that questions the security of automatic speaker verification (ASV). This makes detection of such synthetic speech very important to safeguard ASV systems from unauthorized access. Most of the existing spoofing countermeasures perform well when the nature of the attacks is made known to the system during training. However, their performance degrades in face of unseen nature of attacks. In comparison to the synthetic speech created by a wide range of TTS and VC methods, genuine speech has a more consistent distribution. We believe that the difference between the distribution of synthetic and genuine speech is an important discriminative feature between the two classes. In this regard, we propose a novel method referred to as feature genuinization that learns a transformer with convolutional neural network (CNN) using the characteristics of only genuine speech. We then use this genuinization transformer with a light CNN classifier. The ASVspoof 2019 logical access corpus is used to evaluate the proposed method. The studies show that the proposed feature genuinization based LCNN system outperforms other state-of-the-art spoofing countermeasures, depicting its effectiveness for detection of synthetic speech attacks",
    "checked": true,
    "id": "327f6b2d290ef1fba4e375b18793bc1d1832a4d9",
    "semantic_title": "light convolutional neural network with feature genuinization for detection of synthetic speech attacks",
    "citation_count": 59
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tak20_interspeech.html": {
    "title": "Spoofing Attack Detection Using the Non-Linear Fusion of Sub-Band Classifiers",
    "volume": "main",
    "abstract": "The threat of spoofing can pose a risk to the reliability of automatic speaker verification. Results from the biannual ASVspoof evaluations show that effective countermeasures demand front-ends designed specifically for the detection of spoofing artefacts. Given the diversity in spoofing attacks, ensemble methods are particularly effective. The work in this paper shows that a bank of very simple classifiers, each with a front-end tuned to the detection of different spoofing attacks and combined at the score level through non-linear fusion, can deliver superior performance than more sophisticated ensemble solutions that rely upon complex neural network architectures. Our comparatively simple approach outperforms all but 2 of the 48 systems submitted to the logical access condition of the most recent ASVspoof 2019 challenge",
    "checked": true,
    "id": "b2f693aa5196d45138a821d8c0fbe3a255e7fc60",
    "semantic_title": "spoofing attack detection using the non-linear fusion of sub-band classifiers",
    "citation_count": 42
  },
  "https://www.isca-speech.org/archive/interspeech_2020/parasu20_interspeech.html": {
    "title": "Investigating Light-ResNet Architecture for Spoofing Detection Under Mismatched Conditions",
    "volume": "main",
    "abstract": "Current approaches to Voice Presentation Attack (VPA) detection have largely focused on spoofing detection within a single database and/or attack type. However, for practical Presentation Attack Detection (PAD) systems to be adopted by industry, they must be able to generalise to detect diverse and previously unseen VPAs. Inspired by successful aspects of deep learning systems for image classification such as the introduction of residual mappings through shortcut connections, this paper proposes a novel Light-ResNet architecture that provides good generalisation across databases and attack types. The introduction of skip connections within residual modules enables the training of deeper spoofing classifiers that can leverage more useful discriminative information learned in the hidden layers, while still generalising well under mismatched conditions. Utilising the wide variety of databases available for VPA research, this paper also proposes a set of generalisation evaluations which a practical PAD system should be able to meet: generalising within a database, generalising across databases within attack type and generalising across all spoofing classes. Evaluations on the ASVspoof 2015, BTAS 2016 (replay) and ASVspoof 2017 V2.0 databases show that the proposed Light-ResNet architecture is able to generalise across these diverse tasks consistently, outperforming CQCC-GMM and Attentive Filtering Network (AFN) baselines",
    "checked": true,
    "id": "0ee0a89e94ec0734b83694e201a51708b8994a9b",
    "semantic_title": "investigating light-resnet architecture for spoofing detection under mismatched conditions",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lei20_interspeech.html": {
    "title": "Siamese Convolutional Neural Network Using Gaussian Probability Feature for Spoofing Speech Detection",
    "volume": "main",
    "abstract": "The security and reliability of automatic speaker verification systems can be threatened by different types of spoofing attacks using speech synthetic, voice conversion, or replay. The 2-class Gaussian Mixture Model classifier for genuine and spoofed speech is usually used as the baseline in the ASVspoof challenge, which is designed to develop the generalized countermeasures with potential to detect varying and unforeseen spoofing attacks. In the scoring phase, the GMM accumulates the scores on all frames in a test speech independently, and does not consider the relationship between adjacent frames. We propose the 1-D Convolutional Neural Network whose input is the log-probabilities of the speech frames on the GMM components. The new model considers not only the score distribution of GMM components, but also the local relationship of frames. And the pooling is used to extract the speech global character. The Siamese CNN is also proposed, which is based on two GMMs trained on genuine and spoofed speech respectively. Experiments on the ASVspoof 2019 challenge logical and physical access scenarios show that the proposed model can improve performance greatly compared with the baseline systems",
    "checked": true,
    "id": "2ad4e87937f42376ae841bcdfd58dd65d89c95b5",
    "semantic_title": "siamese convolutional neural network using gaussian probability feature for spoofing speech detection",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2020/schroter20_interspeech.html": {
    "title": "Lightweight Online Noise Reduction on Embedded Devices Using Hierarchical Recurrent Neural Networks",
    "volume": "main",
    "abstract": "Deep-learning based noise reduction algorithms have proven their success especially for non-stationary noises, which makes it desirable to also use them for embedded devices like hearing aids (HAs). This, however, is currently not possible with state-of-the-art methods. They either require a lot of parameters and computational power and thus are only feasible using modern CPUs. Or they are not suitable for online processing, which requires constraints like low-latency by the filter bank and the algorithm itself In this work, we propose a mask-based noise reduction approach. Using hierarchical recurrent neural networks, we are able to drastically reduce the number of neurons per layer while including temporal context via hierarchical connections. This allows us to optimize our model towards a minimum number of parameters and floating-point operations (FLOPs), while preserving noise reduction quality compared to previous work. Our smallest network contains only 5k parameters, which makes this algorithm applicable on embedded devices. We evaluate our model on a mixture of EUROM and a real-world noise database and report objective metrics on unseen noise",
    "checked": true,
    "id": "67e3089d290253adff426619d7dc505cb4e82834",
    "semantic_title": "lightweight online noise reduction on embedded devices using hierarchical recurrent neural networks",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tagliasacchi20_interspeech.html": {
    "title": "SEANet: A Multi-Modal Speech Enhancement Network",
    "volume": "main",
    "abstract": "We explore the possibility of leveraging accelerometer data to perform speech enhancement in very noisy conditions. Although it is possible to only partially reconstruct user's speech from the accelerometer, the latter provides a strong conditioning signal that is not influenced from noise sources in the environment. Based on this observation, we feed a multi-modal input to SEANet (Sound EnhAncement Network), a wave-to-wave fully convolutional model, which adopts a combination of feature losses and adversarial losses to reconstruct an enhanced version of user's speech. We trained our model with data collected by sensors mounted on an earbud and synthetically corrupted by adding different kinds of noise sources to the audio signal. Our experimental results demonstrate that it is possible to achieve very high quality results, even in the case of interfering speech at the same level of loudness. A sample of the output produced by our model is available at ",
    "checked": true,
    "id": "7726349b6f2d5104a14964bbb1a444b42ca554c0",
    "semantic_title": "seanet: a multi-modal speech enhancement network",
    "citation_count": 30
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chuang20_interspeech.html": {
    "title": "Lite Audio-Visual Speech Enhancement",
    "volume": "main",
    "abstract": "Previous studies have confirmed the effectiveness of incorporating visual information into speech enhancement (SE) systems. Despite improved denoising performance, two problems may be encountered when implementing an audio-visual SE (AVSE) system: (1) additional processing costs are incurred to incorporate visual input and (2) the use of face or lip images may cause privacy problems. In this study, we propose a Lite AVSE (LAVSE) system to address these problems. The system includes two visual data compression techniques and removes the visual feature extraction network from the training model, yielding better online computation efficiency. Our experimental results indicate that the proposed LAVSE system can provide notably better performance than an audio-only SE system with a similar number of model parameters. In addition, the experimental results confirm the effectiveness of the two techniques for visual data compression",
    "checked": true,
    "id": "49ac1c2bd894625496760b32b67837e5791c470e",
    "semantic_title": "lite audio-visual speech enhancement",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2020/bergler20_interspeech.html": {
    "title": "ORCA-CLEAN: A Deep Denoising Toolkit for Killer Whale Communication",
    "volume": "main",
    "abstract": "In bioacoustics, passive acoustic monitoring of animals living in the wild, both on land and underwater, leads to large data archives characterized by a strong imbalance between recorded animal sounds and ambient noises. Bioacoustic datasets suffer extremely from such large noise-variety, caused by a multitude of external influences and changing environmental conditions over years. This leads to significant deficiencies/problems concerning the analysis and interpretation of animal vocalizations by biologists and machine-learning algorithms. To counteract such huge noise diversity, it is essential to develop a denoising procedure enabling automated, efficient, and robust data enhancement. However, a fundamental problem is the lack of clean/denoised ground-truth samples. The current work is the first presenting a fully-automated deep denoising approach for bioacoustics, not requiring any clean ground-truth, together with one of the largest data archives recorded on killer whales ( Orcinus Orca) — the Orchive. Therefore, an approach, originally developed for image restoration, known as Noise2Noise (N2N), was transferred to the field of bioacoustics, and extended by using automatic machine-generated binary masks as additional network attention mechanism. Besides a significant cross-domain signal enhancement, our previous results regarding supervised orca/noise segmentation and orca call type identification were outperformed by applying ORCA-CLEAN as additional data preprocessing/enhancement step",
    "checked": true,
    "id": "0d51b082c7913900d203e93c5cd2f0cc828a23ee",
    "semantic_title": "orca-clean: a deep denoising toolkit for killer whale communication",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20i_interspeech.html": {
    "title": "A Deep Learning Approach to Active Noise Control",
    "volume": "main",
    "abstract": "We formulate active noise control (ANC) as a supervised learning problem and propose a deep learning approach, called deep ANC, to address the nonlinear ANC problem. A convolutional recurrent network (CRN) is trained to estimate the real and imaginary spectrograms of the canceling signal from the reference signal so that the corresponding anti-noise can eliminate or attenuate the primary noise in the ANC system. Large-scale multi-condition training is employed to achieve good generalization and robustness against a variety of noises. The deep ANC method can be trained to achieve active noise cancellation no matter whether the reference signal is noise or noisy speech. In addition, a delay-compensated strategy is introduced to address the potential latency problem of ANC systems. Experimental results show that the proposed method is effective for wide-band noise reduction and generalizes well to untrained noises. Moreover, the proposed method can be trained to achieve ANC within a quiet zone",
    "checked": true,
    "id": "3b8fe602473c18fb3f1ed73aa570a3f2e33edc21",
    "semantic_title": "a deep learning approach to active noise control",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2020/dinh20_interspeech.html": {
    "title": "Improving Speech Intelligibility Through Speaker Dependent and Independent Spectral Style Conversion",
    "volume": "main",
    "abstract": "Increasing speech intelligibility for hearing-impaired listeners and normal-hearing listeners in noisy environments remains a challenging problem. Spectral style conversion from habitual to clear speech is a promising approach to address the problem. Motivated by the success of generative adversarial networks (GANs) in various applications of image and speech processing, we explore the potential of conditional GANs (cGANs) to learn the mapping from habitual speech to clear speech. We evaluated the performance of cGANs in three tasks: 1) speaker-dependent one-to-one mappings, 2) speaker-independent many-to-one mappings, and 3) speaker-independent many-to-many mappings. In the first task, cGANs outperformed a traditional deep neural network mapping in terms of average keyword recall accuracy and the number of speakers with improved intelligibility. In the second task, we significantly improved intelligibility of one of three speakers, without any source speaker training data. In the third and most challenging task, we improved keyword recall accuracy for two of three speakers, but without statistical significance",
    "checked": true,
    "id": "466143d0b67aae3948419243c723fe4da568872a",
    "semantic_title": "improving speech intelligibility through speaker dependent and independent spectral style conversion",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/pedersen20_interspeech.html": {
    "title": "End-to-End Speech Intelligibility Prediction Using Time-Domain Fully Convolutional Neural Networks",
    "volume": "main",
    "abstract": "Data-driven speech intelligibility prediction has been slow to take off. Datasets of measured speech intelligibility are scarce, and so current models are relatively small and rely on hand-picked features. Classical predictors based on psychoacoustic models and heuristics are still the state-of-the-art. This work proposes a U-Net inspired fully convolutional neural network architecture, NSIP, trained and tested on ten datasets to predict intelligibility of time-domain speech. The architecture is compared to a frequency domain data-driven predictor and to the classical state-of-the-art predictors STOI, ESTOI, HASPI and SIIB. The performance of NSIP is found to be superior for datasets seen in the training phase. On unseen datasets NSIP reaches performance comparable to classical predictors",
    "checked": true,
    "id": "bd493c29f3cb53f9967ac04c1802e9fc6bb194b8",
    "semantic_title": "end-to-end speech intelligibility prediction using time-domain fully convolutional neural networks",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2020/arai20_interspeech.html": {
    "title": "Predicting Intelligibility of Enhanced Speech Using Posteriors Derived from DNN-Based ASR System",
    "volume": "main",
    "abstract": "The measurement of speech intelligibility (SI) still mainly relies on time-consuming and expensive subjective experiments because no versatile objective measure can predict SI. One promising candidate of an SI prediction method is an approach with a deep neural network (DNN)-based automatic speech recognition (ASR) system, due to its recent great advance. In this paper, we propose and evaluate SI prediction methods based on the posteriors of DNN-based ASR systems. Posteriors, which are the probabilities of phones given acoustic features, are derived using forced alignments between clean speech and a phone sequence. We evaluated some variations of the posteriors to improve the prediction performance. As a result of our experiments, a prediction method using a squared cumulative posterior probability achieved better accuracy than the conventional SI predictors based on well-established objective measures (STOI and eSTOI)",
    "checked": true,
    "id": "3d7e405cba583e22dbb2348285cda04096d223e7",
    "semantic_title": "predicting intelligibility of enhanced speech using posteriors derived from dnn-based asr system",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2020/abavisani20_interspeech.html": {
    "title": "Automatic Estimation of Intelligibility Measure for Consonants in Speech",
    "volume": "main",
    "abstract": "In this article, we provide a model to estimate a real-valued measure of the intelligibility of individual speech segments. We trained regression models based on Convolutional Neural Networks (CNN) for stop consonants /p,t,k,b,d,ɡ/ associated with vowel /ɑ/, to estimate the corresponding Signal to Noise Ratio (SNR) at which the Consonant-Vowel (CV) sound becomes intelligible for Normal Hearing (NH) ears. The intelligibility measure for each sound is called SNR , and is defined to be the SNR level at which human participants are able to recognize the consonant at least 90% correctly, on average, as determined in prior experiments with NH subjects. Performance of the CNN is compared to a baseline prediction based on automatic speech recognition (ASR), specifically, a constant offset subtracted from the SNR at which the ASR becomes capable of correctly labeling the consonant. Compared to baseline, our models were able to accurately estimate the SNR intelligibility measure with less than 2 [dB ] Mean Squared Error (MSE) on average, while the baseline ASR-defined measure computes SNR with a variance of 5.2 to 26.6 [dB ], depending on the consonant",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/trinh20_interspeech.html": {
    "title": "Large Scale Evaluation of Importance Maps in Automatic Speech Recognition",
    "volume": "main",
    "abstract": "This paper proposes a metric that we call the structured saliency benchmark (SSBM) to evaluate importance maps computed for automatic speech recognizers on individual utterances. These maps indicate time-frequency points of the utterance that are most important for correct recognition of a target word. Our evaluation technique is not only suitable for standard classification tasks, but is also appropriate for structured prediction tasks like sequence-to-sequence models. Additionally, we use this approach to perform a comparison of the importance maps created by our previously introduced technique using \"bubble noise\" to identify important points through correlation with a baseline approach based on smoothed speech energy and forced alignment. Our results show that the bubble analysis approach is better at identifying important speech regions than this baseline on 100 sentences from the AMI corpus",
    "checked": true,
    "id": "2636e8bb45d7c76a01f5540f5ceda2ab562e9337",
    "semantic_title": "large scale evaluation of importance maps in automatic speech recognition",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20n_interspeech.html": {
    "title": "Neural Architecture Search on Acoustic Scene Classification",
    "volume": "main",
    "abstract": "Convolutional neural networks are widely adopted in Acoustic Scene Classification (ASC) tasks, but they generally carry a heavy computational burden. In this work, we propose a high-performance yet lightweight baseline network inspired by MobileNetV2, which replaces square convolutional kernels with unidirectional ones to extract features alternately in temporal and frequency dimensions. Furthermore, we explore a dynamic architecture space built on the basis of the proposed baseline with the recent Neural Architecture Search (NAS) paradigm, which first train a supernet that incorporates all candidate architectures and then apply a well-known evolutionary algorithm NSGA-II to discover more efficient networks with higher accuracy and lower computational cost from the supernet. Experimental results demonstrate that our searched network is competent in ASC tasks, which achieves 90.3% F1-score on the DCASE2018 task 5 evaluation set, marking a new state-of-the-art performance while saving 25% of FLOPs compared to our baseline network",
    "checked": true,
    "id": "04e7c81548b3421c1607d13841fb4e92d2156b30",
    "semantic_title": "neural architecture search on acoustic scene classification",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2020/jung20b_interspeech.html": {
    "title": "Acoustic Scene Classification Using Audio Tagging",
    "volume": "main",
    "abstract": "Acoustic scene classification systems using deep neural networks classify given recordings into pre-defined classes. In this study, we propose a novel scheme for acoustic scene classification which adopts an audio tagging system inspired by the human perception mechanism. When humans identify an acoustic scene, the existence of different sound events provides discriminative information which affects the judgement. The proposed framework mimics this mechanism using various approaches. Firstly, we employ three methods to concatenate tag vectors extracted using an audio tagging system with an intermediate hidden layer of an acoustic scene classification system. We also explore the multi-head attention on the feature map of an acoustic scene classification system using tag vectors. Experiments conducted on the detection and classification of acoustic scenes and events 2019 task 1-a dataset demonstrate the effectiveness of the proposed scheme. Concatenation and multi-head attention show a classification accuracy of 75.66% and 76.58%, respectively, compared to 73.63% accuracy of the baseline. The system with the proposed two approaches combined demonstrates an accuracy of 76.75%",
    "checked": true,
    "id": "983dae6a12c12dd93693335d8f10d3ec53773553",
    "semantic_title": "acoustic scene classification using audio tagging",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20j_interspeech.html": {
    "title": "ATReSN-Net: Capturing Attentive Temporal Relations in Semantic Neighborhood for Acoustic Scene Classification",
    "volume": "main",
    "abstract": "Convolutional Neural Networks (CNNs) have been widely investigated on Acoustic Scene Classification (ASC). Where the convolutional operation can extract useful semantic contents from a local receptive field in the input spectrogram within certain Manhattan distance, i.e., the kernel size. Although stacking multiple convolution layers can increase the range of the receptive field, without explicitly considering the temporal relations of different receptive fields, the increased range is limited around the kernel. In this paper, we propose a 3D CNN for ASC, named ATReSN-Net, which can capture temporal relations of different receptive fields from arbitrary time-frequency locations by mapping the semantic features obtained from the residual block into a semantic space. The ATReSN module has two primary components: first, a k-NN-based grouper for gathering a semantic neighborhood for each feature point in the feature maps. Second, an attentive pooling-based temporal relations aggregator for generating the temporal relations embedding of each feature point and its neighborhood. Experiments showed that our ATReSN-Net outperforms most of the state-of-the-art CNN models. We shared our code at ATReSN-Net",
    "checked": true,
    "id": "be119130b883d83f8338d9c4e2b2bc7f94f2bcce",
    "semantic_title": "atresn-net: capturing attentive temporal relations in semantic neighborhood for acoustic scene classification",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2020/sharma20_interspeech.html": {
    "title": "Environment Sound Classification Using Multiple Feature Channels and Attention Based Deep Convolutional Neural Network",
    "volume": "main",
    "abstract": "In this paper, we propose a model for the Environment Sound Classification Task (ESC) that consists of multiple feature channels given as input to a Deep Convolutional Neural Network (CNN) with Attention mechanism. The novelty of the paper lies in using multiple feature channels consisting of Mel-Frequency Cepstral Coefficients (MFCC), Gammatone Frequency Cepstral Coefficients (GFCC), the Constant Q-transform (CQT) and Chromagram. And, we employ a deeper CNN (DCNN) compared to previous models, consisting of spatially separable convolutions working on time and feature domain separately. Alongside, we use attention modules that perform channel and spatial attention together. We use the mix-up data augmentation technique to further boost performance. Our model is able to achieve state-of-the-art performance on three benchmark environment sound classification datasets, i.e. the UrbanSound8K (97.52%), ESC-10 (94.75%) and ESC-50 (87.45%)",
    "checked": true,
    "id": "2aeeec17925ad1249b4f1939fd1f342fcf9103e4",
    "semantic_title": "environment sound classification using multiple feature channels and attention based deep convolutional neural network",
    "citation_count": 37
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20m_interspeech.html": {
    "title": "Acoustic Scene Analysis with Multi-Head Attention Networks",
    "volume": "main",
    "abstract": "Acoustic Scene Classification (ASC) is a challenging task, as a single scene may involve multiple events that contain complex sound patterns. For example, a cooking scene may contain several sound sources including silverware clinking, chopping, frying, etc. What complicates ASC more is that classes of different activities could have overlapping sounds patterns (e.g. both cooking and dishwashing could have silverware clinking sound). In this paper, we propose a multi-head attention network to model the complex temporal input structures for ASC. The proposed network takes the audio's time-frequency representation as input, and it leverages standard VGG plus LSTM layers to extract high-level feature representation. Further more, it applies multiple attention heads to summarize various patterns of sound events into fixed dimensional representation, for the purpose of final scene classification. The whole network is trained in an end-to-end fashion with backpropagation. Experimental results confirm that our model discovers meaningful sound patterns through the attention mechanism, without using explicit supervision in the alignment. We evaluated our proposed model using DCASE 2018 Task 5 dataset, and achieved competitive performance on par with previous winner's results",
    "checked": true,
    "id": "41d7bb012e08578e7b6c87b1d41a39bf44cb2589",
    "semantic_title": "acoustic scene analysis with multi-head attention networks",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hu20e_interspeech.html": {
    "title": "Relational Teacher Student Learning with Neural Label Embedding for Device Adaptation in Acoustic Scene Classification",
    "volume": "main",
    "abstract": "In this paper, we propose a domain adaptation framework to address the device mismatch issue in acoustic scene classification leveraging upon neural label embedding (NLE) and relational teacher student learning (RTSL). Taking into account the structural relationships between acoustic scene classes, our proposed framework captures such relationships which are intrinsically device-independent. In the training stage, transferable knowledge is condensed in NLE from the source domain. Next in the adaptation stage, a novel RTSL strategy is adopted to learn adapted target models without using paired source-target data often required in conventional teacher student learning. The proposed framework is evaluated on the DCASE 2018 Task1b data set. Experimental results based on AlexNet-L deep classification models confirm the effectiveness of our proposed approach for mismatch situations. NLE-alone adaptation compares favourably with the conventional device adaptation and teacher student based adaptation techniques. NLE with RTSL further improves the classification accuracy",
    "checked": true,
    "id": "00ee64055f70340760b0f120946b525565019052",
    "semantic_title": "relational teacher student learning with neural label embedding for device adaptation in acoustic scene classification",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hu20f_interspeech.html": {
    "title": "An Acoustic Segment Model Based Segment Unit Selection Approach to Acoustic Scene Classification with Partial Utterances",
    "volume": "main",
    "abstract": "In this paper, we propose a sub-utterance unit selection framework to remove acoustic segments in audio recordings that carry little information for acoustic scene classification (ASC). Our approach is built upon a universal set of acoustic segment units covering the overall acoustic scene space. First, those units are modeled with acoustic segment models (ASMs) used to tokenize acoustic scene utterances into sequences of acoustic segment units. Next, paralleling the idea of stop words in information retrieval, stop ASMs are automatically detected. Finally, acoustic segments associated with the stop ASMs are blocked, because of their low indexing power in retrieval of most acoustic scenes. In contrast to building scene models with whole utterances, the ASM-removed sub-utterances, i.e., acoustic utterances without stop acoustic segments, are then used as inputs to the AlexNet-L back-end for final classification. On the DCASE 2018 dataset, scene classification accuracy increases from 68%, with whole utterances, to 72.1%, with segment selection. This represents a competitive accuracy without any data augmentation, and/or ensemble strategy. Moreover, our approach compares favourably to AlexNet-L with attention",
    "checked": true,
    "id": "3559a1cfccd1a0a8d94971190eb1c68c3fc4e86c",
    "semantic_title": "an acoustic segment model based segment unit selection approach to acoustic scene classification with partial utterances",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2020/devalraju20_interspeech.html": {
    "title": "Attention-Driven Projections for Soundscape Classification",
    "volume": "main",
    "abstract": "Acoustic soundscapes can be made up of background sound events and foreground sound events. Many times, either the background (or the foreground) may provide useful cues in discriminating one soundscape from another. A part of the background or a part of the foreground can be suppressed by using subspace projections. These projections can be learnt by utilising the framework of robust principal component analysis. In this work, audio signals are represented as embeddings from a convolutional neural network, and meta-embeddings are derived using an attention mechanism. This representation enables the use of class-specific projections for effective suppression, leading to good discrimination. Our experimental evaluation demonstrates the effectiveness of the method on standard datasets for acoustic scene classification",
    "checked": true,
    "id": "fac938afdacb332ef85aaf2631757b61cfa1a673",
    "semantic_title": "attention-driven projections for soundscape classification",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tzirakis20_interspeech.html": {
    "title": "Computer Audition for Continuous Rainforest Occupancy Monitoring: The Case of Bornean Gibbons' Call Detection",
    "volume": "main",
    "abstract": "Auditory data is used by ecologists for a variety of purposes, including identifying species ranges, estimating population sizes, and studying behaviour. Autonomous recording units (ARUs) enable auditory data collection over a wider area, and can provide improved consistency over traditional sampling methods. The result is an abundance of audio data — much more than can be analysed by scientists with the appropriate taxonomic skills. In this paper, we address the divide between academic machine learning research on animal vocalisation classifiers, and their application to conservation efforts. As a unique case study, we build a Bornean gibbon call detection system by first manually annotating existing data, and then comparing audio analysis tool kits including end-to-end and bag-of-audio-word modelling. Finally, we propose a deep architecture that outperforms the other approaches with respect to unweighted average recall. The code is available at: ",
    "checked": true,
    "id": "b15417da3b7f93de38f3782fc162d0671c600089",
    "semantic_title": "computer audition for continuous rainforest occupancy monitoring: the case of bornean gibbons' call detection",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kwiatkowska20_interspeech.html": {
    "title": "Deep Learning Based Open Set Acoustic Scene Classification",
    "volume": "main",
    "abstract": "In this work, we compare the performance of three selected techniques in open set acoustic scenes classification (ASC). We test thresholding of the softmax output of a deep network classifier, which is the most popular technique nowadays employed in ASC. Further we compare the results with the Openmax classifier which is derived from the computer vision field. As the third model, we use the Adapted Class-Conditioned Autoencoder (Adapted C2AE) which is our variation of another computer vision related technique called C2AE. Adapted C2AE encompasses a more fair comparison of the given experiments and simplifies the original inference procedure, making it more applicable in the real-life scenarios. We also analyse two training scenarios: without additional knowledge of unknown classes and another where a limited subset of examples from the unknown classes is available. We find that the C2AE based method outperforms the thresholding and Openmax, obtaining 85.5% Area Under the Receiver Operating Characteristic curve (AUROC) and 66% of open set accuracy on data used in Detection and Classification of Acoustic Scenes and Events Challenge 2019 Task 1C",
    "checked": true,
    "id": "a12d51445d022448e815668ef03b49c0a40d5688",
    "semantic_title": "deep learning based open set acoustic scene classification",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2020/angelini20_interspeech.html": {
    "title": "Singing Synthesis: With a Little Help from my Attention",
    "volume": "main",
    "abstract": "We present UTACO, a singing synthesis model based on an attention-based sequence-to-sequence mechanism and a vocoder based on dilated causal convolutions. These two classes of models have significantly affected the field of text-to-speech, but have never been thoroughly applied to the task of singing synthesis. UTACO demonstrates that attention can be successfully applied to the singing synthesis field and improves naturalness over the state of the art. The system requires considerably less explicit modelling of voice features such as F0 patterns, vibratos, and note and phoneme durations, than previous models in the literature. Despite this, it shows a strong improvement in naturalness with respect to previous neural singing synthesis models. The model does not require any durations or pitch patterns as inputs, and learns to insert vibrato autonomously according to the musical context. However, we observe that, by completely dispensing with any explicit duration modelling it becomes harder to obtain the fine control of timing needed to exactly match the tempo of a song",
    "checked": true,
    "id": "0d8a4677eb34e60a0c911fba5b88a84862925e32",
    "semantic_title": "singing synthesis: with a little help from my attention",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wu20d_interspeech.html": {
    "title": "Peking Opera Synthesis via Duration Informed Attention Network",
    "volume": "main",
    "abstract": "Peking Opera has been the most dominant form of Chinese performing art since around 200 years ago. A Peking Opera singer usually exhibits a very strong personal style via introducing improvisation and expressiveness on stage which leads the actual rhythm and pitch contour to deviate significantly from the original music score. This inconsistency poses a great challenge in Peking Opera singing voice synthesis from a music score. In this work, we propose to deal with this issue and synthesize expressive Peking Opera singing from the music score based on the Duration Informed Attention Network (DurIAN) framework. To tackle the rhythm mismatch, Lagrange multiplier is used to find the optimal output phoneme duration sequence with the constraint of the given note duration from music score. As for the pitch contour mismatch, instead of directly inferring from music score, we adopt a pseudo music score generated from the real singing and feed it as input during training. The experiments demonstrate that with the proposed system we can synthesize Peking Opera singing voice with high-quality timbre, pitch and expressiveness",
    "checked": true,
    "id": "92381a737a4425b0def8068baa8dc1d4ebe537dc",
    "semantic_title": "peking opera synthesis via duration informed attention network",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20k_interspeech.html": {
    "title": "DurIAN-SC: Duration Informed Attention Network Based Singing Voice Conversion System",
    "volume": "main",
    "abstract": "Singing voice conversion is converting the timbre in the source singing to the target speaker's voice while keeping singing content the same. However, singing data for target speaker is much more difficult to collect compared with normal speech data. In this paper, we introduce a singing voice conversion algorithm that is capable of generating high quality target speaker's singing using only his/her normal speech data. First, we manage to integrate the training and conversion process of speech and singing into one framework by unifying the features used in standard speech synthesis system and singing synthesis system. In this way, normal speech data can also contribute to singing voice conversion training, making the singing voice conversion system more robust especially when the singing database is small. Moreover, in order to achieve one-shot singing voice conversion, a speaker embedding module is developed using both speech and singing data, which provides target speaker identify information during conversion. Experiments indicate proposed sing conversion system can convert source singing to target speaker's high-quality singing with only 20 seconds of target speaker's enrollment speech data",
    "checked": true,
    "id": "ca1c80748f64b4c321873f82c96b2834833183ec",
    "semantic_title": "durian-sc: duration informed attention network based singing voice conversion system",
    "citation_count": 29
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hou20b_interspeech.html": {
    "title": "Transfer Learning for Improving Singing-Voice Detection in Polyphonic Instrumental Music",
    "volume": "main",
    "abstract": "Detecting singing-voice in polyphonic instrumental music is critical to music information retrieval. To train a robust vocal detector, a large dataset marked with vocal or non-vocal label at frame-level is essential. However, frame-level labeling is time-consuming and labor expensive, resulting there is little well-labeled dataset available for singing-voice detection (S-VD). Hence, we propose a data augmentation method for S-VD by transfer learning. In this study, clean speech clips with voice activity endpoints and separate instrumental music clips are artificially added together to simulate polyphonic vocals to train a vocal /non-vocal detector. Due to the different articulation and phonation between speaking and singing, the vocal detector trained with the artificial dataset does not match well with the polyphonic music which is singing vocals together with the instrumental accompaniments. To reduce this mismatch, transfer learning is used to transfer the knowledge learned from the artificial speech-plus-music training set to a small but matched polyphonic dataset, i.e., singing vocals with accompaniments. By transferring the related knowledge to make up for the lack of well-labeled training data in S-VD, the proposed data augmentation method by transfer learning can improve S-VD performance with an F-score improvement from 89.5% to 93.2%",
    "checked": true,
    "id": "95ef41004fe70375ed121a3ed26e422a554098e0",
    "semantic_title": "transfer learning for improving singing-voice detection in polyphonic instrumental music",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2020/liu20f_interspeech.html": {
    "title": "Channel-Wise Subband Input for Better Voice and Accompaniment Separation on High Resolution Music",
    "volume": "main",
    "abstract": "This paper presents a new input format, channel-wise subband input (CWS), for convolutional neural networks (CNN) based music source separation (MSS) models in the frequency domain. We aim to address the major issues in CNN-based high-resolution MSS model: high computational cost and weight sharing between distinctly different bands. Specifically, in this paper, we decompose the input mixture spectra into several bands and concatenate them channel-wise as the model input. The proposed approach enables effective weight sharing in each subband and introduces more flexibility between channels. For comparison purposes, we perform voice and accompaniment separation (VAS) on models with different scales, architectures, and CWS settings. Experiments show that the CWS input is beneficial in many aspects. We evaluate our method on musdb18hq test set, focusing on SDR, SIR and SAR metrics. Among all our experiments, CWS enables models to obtain 6.9% performance gain on the average metrics. With even a smaller number of parameters, less training data, and shorter training time, ourMDenseNet with 8-bands CWS input still surpasses the original MMDenseNet with a large margin. Moreover, CWS also reduces computational cost and training time to a large extent",
    "checked": true,
    "id": "d9f29e067ab26dc7555a2d64d36cc897f5d0e7df",
    "semantic_title": "channel-wise subband input for better voice and accompaniment separation on high resolution music",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2020/sadhu20_interspeech.html": {
    "title": "Continual Learning in Automatic Speech Recognition",
    "volume": "main",
    "abstract": "We emulate continual learning observed in real life, where new training data, which represent new application domain, are used for gradual improvement of an Automatic Speech Recognizer (ASR) trained on old domains. The data on which the original classifier was trained is no longer required and we observe no loss of performance on the original domain. Further, on previously unseen domain, our technique appears to yield slight advantage over offline multi-condition training. The proposed learning technique is consistent with our previously studied ad hoc stream attention based multi-stream ASR",
    "checked": true,
    "id": "802731161ad5db91bac8569307d8e75019d9ffdd",
    "semantic_title": "continual learning in automatic speech recognition",
    "citation_count": 26
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wan20_interspeech.html": {
    "title": "Speaker Adaptive Training for Speech Recognition Based on Attention-Over-Attention Mechanism",
    "volume": "main",
    "abstract": "In our previous work, we introduced a speaker adaptive training method based on frame-level attention mechanism for speech recognition, which has been proved an effective way to do speaker adaptive training. In this paper, we present an improved method by introducing the attention-over-attention mechanism. This attention module is used to further measure the contribution of each frame to the speaker embeddings in an utterance, and then generate an utterance-level speaker embedding to perform speaker adaptive training. Compared with the frame-level ones, the generated utterance-level speaker embeddings are more representative and stable. Experiments on both the Switchboard and AISHELL-2 tasks show that our method can achieve a relative word error rate reduction of approximately 8.0% compared with the speaker independent model, and over 6.0% compared with the traditional utterance-level d-vector-based speaker adaptive training method",
    "checked": true,
    "id": "8c07fbc3bd556267d3aaa5a50383567747f336ad",
    "semantic_title": "speaker adaptive training for speech recognition based on attention-over-attention mechanism",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2020/huang20c_interspeech.html": {
    "title": "Rapid RNN-T Adaptation Using Personalized Speech Synthesis and Neural Language Generator",
    "volume": "main",
    "abstract": "Rapid unsupervised speaker adaptation in an E2E system posits us new challenges due to its end-to-end unified structure in addition to its intrinsic difficulty of data sparsity and imperfect label [1]. Previously we proposed utilizing the content relevant personalized speech synthesis for rapid speaker adaptation and achieved significant performance breakthrough in a hybrid system [2]. In this paper, we answer the following two questions: First, how to effectively perform rapid speaker adaptation in an RNN-T. Second, whether our previously proposed approach is still beneficial for the RNN-T and what are the modification and distinct observations. We apply the proposed methodology to a speaker adaptation task in a state-of-art presentation transcription RNN-T system. In the 1 min setup, it yields 11.58% or 7.95% relative word error rate (WER) reduction for the sup/unsup adaptation, comparing to the negligible gain when adapting with 1 min source speech. In the 10 min setup, it yields 15.71% or 8.00% relative WER reduction, doubling the gain of the source speech adaptation. We further apply various data filtering techniques and significantly bridge the gap between sup/unsup adaptation",
    "checked": true,
    "id": "1678dd5acdbe5be82e1c38afb278285eef788d56",
    "semantic_title": "rapid rnn-t adaptation using personalized speech synthesis and neural language generator",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhao20b_interspeech.html": {
    "title": "Speech Transformer with Speaker Aware Persistent Memory",
    "volume": "main",
    "abstract": "End-to-end models have been introduced into automatic speech recognition (ASR) successfully and achieved superior performance compared with conventional hybrid systems, especially with the newly proposed transformer model. However, speaker mismatch between training and test data remains a problem, and speaker adaptation for transformer model can be further improved. In this paper, we propose to conduct speaker aware training for ASR in transformer model. Specifically, we propose to embed speaker knowledge through a persistent memory model into speech transformer encoder at utterance level. The speaker information is represented by a number of static speaker i-vectors, which is concatenated to speech utterance at each encoder self-attention layer. Persistent memory is thus formed by carrying speaker information through the depth of encoder. The speaker knowledge is captured from self-attention between speech and persistent memory vector in encoder. Experiment results on LibriSpeech, Switchboard and AISHELL-1 ASR task show that our proposed model brings relative 4.7%–12.5% word error rate (WER) reductions, and achieves superior results compared with other models with the same objective. Furthermore, our model brings relative 2.1%–8.3% WER reductions compared with the first persistent memory model used in ASR",
    "checked": true,
    "id": "44765630c55bce4a438c65ed2b739011ca4c1f23",
    "semantic_title": "speech transformer with speaker aware persistent memory",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ding20d_interspeech.html": {
    "title": "Adaptive Speaker Normalization for CTC-Based Speech Recognition",
    "volume": "main",
    "abstract": "In this paper, we propose a new speaker normalization technique for acoustic model adaptation in connectionist temporal classification (CTC)-based automatic speech recognition. In the proposed method, for the inputs of a hidden layer, the mean and variance of each activation are first estimated at the speaker level. Then, we normalize each speaker representation independently by making them follow a standard normal distribution. Furthermore, we propose using an auxiliary network to dynamically generate the scaling and shifting parameters of speaker normalization, and an attention mechanism is introduced to improve performance. The experiments are conducted on the public Chinese dataset AISHELL-1. Our proposed methods present high effectiveness in adapting the CTC model, achieving up to 17.5% character error rate improvement over the speaker-independent (SI) model",
    "checked": true,
    "id": "699a114935afd33f37cef85bee2fd46d23d46f37",
    "semantic_title": "adaptive speaker normalization for ctc-based speech recognition",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mathur20_interspeech.html": {
    "title": "Unsupervised Domain Adaptation Under Label Space Mismatch for Speech Classification",
    "volume": "main",
    "abstract": "Unsupervised domain adaptation using adversarial learning has shown promise in adapting speech models from a labeled source domain to an unlabeled target domain. However, prior works make a strong assumption that the label spaces of source and target domains are identical, which can be easily violated in real-world conditions. We present AMLS, an end-to-end architecture that performs Adaptation under Mismatched Label Spaces using two weighting schemes to separate shared and private classes in each domain. An evaluation on three speech adaptation tasks, namely gender, microphone, and emotion adaptation, shows that AMLS provides significant accuracy gains over baselines used in speech and vision adaptation tasks. Our contribution paves the way for applying UDA to speech models in unconstrained settings with no assumptions on the source and target label spaces",
    "checked": true,
    "id": "c2281095b9e98851fac2d30ceda5cca3900ac86f",
    "semantic_title": "unsupervised domain adaptation under label space mismatch for speech classification",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2020/winata20_interspeech.html": {
    "title": "Learning Fast Adaptation on Cross-Accented Speech Recognition",
    "volume": "main",
    "abstract": "Local dialects influence people to pronounce words of the same language differently from each other. The great variability and complex characteristics of accents create a major challenge for training a robust and accent-agnostic automatic speech recognition (ASR) system. In this paper, we introduce a cross-accented English speech recognition task as a benchmark for measuring the ability of the model to adapt to unseen accents using the existing CommonVoice corpus. We also propose an accent-agnostic approach that extends the model-agnostic meta-learning (MAML) algorithm for fast adaptation to unseen accents. Our approach significantly outperforms joint training in both zero-shot, few-shot, and all-shot in the mixed-region and cross-region settings in terms of word error rate",
    "checked": true,
    "id": "4e4a031305beefec1935eac801b81fd64f55f82f",
    "semantic_title": "learning fast adaptation on cross-accented speech recognition",
    "citation_count": 55
  },
  "https://www.isca-speech.org/archive/interspeech_2020/khandelwal20_interspeech.html": {
    "title": "Black-Box Adaptation of ASR for Accented Speech",
    "volume": "main",
    "abstract": "We introduce the problem of adapting a black-box, cloud-based ASR system to speech from a target accent. While leading online ASR services obtain impressive performance on mainstream accents, they perform poorly on sub-populations — we observed that the word error rate (WER) achieved by Google's ASR API on Indian accents is almost twice the WER on US accents. Existing adaptation methods either require access to model parameters or overlay an error correcting module on output transcripts. We highlight the need for correlating outputs with the original speech to fix accent errors. Accordingly, we propose a novel coupling of an open-source accent-tuned local model with the black-box service where the output from the service guides frame-level inference in the local model. Our fine-grained merging algorithm is better at fixing accent errors than existing word-level combination strategies. Experiments on Indian and Australian accents with three leading ASR models as service, show that we achieve upto 28% relative reduction in WER over both the local and service models",
    "checked": true,
    "id": "10a948860e0f6bbaa21439ecbfa6fd6d1463d0e1",
    "semantic_title": "black-box adaptation of asr for accented speech",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2020/turan20_interspeech.html": {
    "title": "Achieving Multi-Accent ASR via Unsupervised Acoustic Model Adaptation",
    "volume": "main",
    "abstract": "Current automatic speech recognition (ASR) systems trained on native speech often perform poorly when applied to non-native or accented speech. In this work, we propose to compute x-vector-like accent embeddings and use them as auxiliary inputs to an acoustic model trained on native data only in order to improve the recognition of multi-accent data comprising native, non-native, and accented speech. In addition, we leverage untranscribed accented training data by means of semi-supervised learning. Our experiments show that acoustic models trained with the proposed accent embeddings outperform those trained with conventional i-vector or x-vector speaker embeddings, and achieve a 15% relative word error rate (WER) reduction on non-native and accented speech w.r.t. acoustic models trained with regular spectral features only. Semi-supervised training using just 1 hour of untranscribed speech per accent yields an additional 15% relative WER reduction w.r.t. models trained on native data only",
    "checked": true,
    "id": "80156fce0b29017e7f6f8751f1f471bc5d5370cb",
    "semantic_title": "achieving multi-accent asr via unsupervised acoustic model adaptation",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2020/takeda20_interspeech.html": {
    "title": "Frame-Wise Online Unsupervised Adaptation of DNN-HMM Acoustic Model from Perspective of Robust Adaptive Filtering",
    "volume": "main",
    "abstract": "We present a new frame-wise online unsupervised adaptation method for an acoustic model based on a deep neural network (DNN). This is in contrast to many existing methods that assume offline and supervised processing. We use a likelihood cost function conditioned by past observations, which mathematically integrate the unsupervised adaptation and decoding process for automatic speech recognition (ASR). The issue is that the parameter update of the DNN should be less affected by outliers (model mismatch) and ASR recognition errors. Inspired by the robust adaptive filter techniques, we propose 1) parameter update control to remove the influence of the outliers and 2) regularization using L2-norm of DNN's posterior probabilities of specific phonemes that are prone to recognition errors. Experiments showed that the phoneme recognition accuracies were improved by a maximum of 6.3 points, with an average error reduction rate of 10%, for various speakers",
    "checked": true,
    "id": "5baf962207e26ef658c94cc5238b4e23bb0c2c5a",
    "semantic_title": "frame-wise online unsupervised adaptation of dnn-hmm acoustic model from perspective of robust adaptive filtering",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wu20e_interspeech.html": {
    "title": "Adversarially Trained Multi-Singer Sequence-to-Sequence Singing Synthesizer",
    "volume": "main",
    "abstract": "This paper presents a high quality singing synthesizer that is able to model a voice with limited available recordings. Based on the sequence-to-sequence singing model, we design a multi-singer framework to leverage all the existing singing data of different singers. To attenuate the issue of musical score unbalance among singers, we incorporate an adversarial task of singer classification to make encoder output less singer dependent. Furthermore, we apply multiple random window discriminators (MRWDs) on the generated acoustic features to make the network be a GAN. Both objective and subjective evaluations indicate that the proposed synthesizer can generate higher quality singing voice than baseline (4.12 vs 3.53 in MOS). Especially, the articulation of high-pitched vowels is significantly enhanced",
    "checked": true,
    "id": "17cb81590dde7f2a9c4bcf70dd8ec2b9da7f4a6e",
    "semantic_title": "adversarially trained multi-singer sequence-to-sequence singing synthesizer",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lu20b_interspeech.html": {
    "title": "Prediction of Head Motion from Speech Waveforms with a Canonical-Correlation-Constrained Autoencoder",
    "volume": "main",
    "abstract": "This study investigates the direct use of speech waveforms to predict head motion for speech-driven head-motion synthesis, whereas the use of spectral features such as MFCC as basic input features together with additional features such as energy and F0 is common in the literature. We show that, rather than combining different features that originate from waveforms, it is more effective to use waveforms directly predicting corresponding head motion. The challenge with the waveform-based approach is that waveforms contain a large amount of information irrelevant to predict head motion, which hinders the training of neural networks. To overcome the problem, we propose a canonical-correlation-constrained autoencoder (CCCAE), where hidden layers are trained to not only minimise the error but also maximise the canonical correlation with head motion. Compared with an MFCC-based system, the proposed system shows comparable performance in objective evaluation, and better performance in subject evaluation",
    "checked": true,
    "id": "29b28d2f4214e0a2826bd0f7ec4a174648c066a5",
    "semantic_title": "prediction of head motion from speech waveforms with a canonical-correlation-constrained autoencoder",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lu20c_interspeech.html": {
    "title": "XiaoiceSing: A High-Quality and Integrated Singing Voice Synthesis System",
    "volume": "main",
    "abstract": "This paper presents XiaoiceSing, a high-quality singing voice synthesis system which employs an integrated network for spectrum, F0 and duration modeling. We follow the main architecture of FastSpeech while proposing some singing-specific design: 1) Besides phoneme ID and position encoding, features from musical score (e.g. note pitch and length) are also added. 2) To attenuate off-key issues, we add a residual connection in F0 prediction. 3) In addition to the duration loss of each phoneme, the duration of all the phonemes in a musical note is accumulated to calculate the syllable duration loss for rhythm enhancement. Experiment results show that XiaoiceSing outperforms the baseline system of convolutional neural networks by 1.44 MOS on sound quality, 1.18 on pronunciation accuracy and 1.38 on naturalness respectively. In two A/B tests, the proposed F0 and duration modeling methods achieve 97.3% and 84.3% preference rate over baseline respectively, which demonstrates the overwhelming advantages of XiaoiceSing",
    "checked": true,
    "id": "ef264beb5ed8b995ada36c67f1d2d20e61e4c0f7",
    "semantic_title": "xiaoicesing: a high-quality and integrated singing voice synthesis system",
    "citation_count": 57
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yadav20_interspeech.html": {
    "title": "Stochastic Talking Face Generation Using Latent Distribution Matching",
    "volume": "main",
    "abstract": "The ability to envisage the visual of a talking face based just on hearing a voice is a unique human capability. There have been a number of works that have solved for this ability recently. We differ from these approaches by enabling a variety of talking face generations based on single audio input. Indeed, just having the ability to generate a single talking face would make a system almost robotic in nature. In contrast, our unsupervised stochastic audio-to-video generation model allows for diverse generations from a single audio input. Particularly, we present an unsupervised stochastic audio-to-video generation model that can capture multiple modes of the video distribution. We ensure that all the diverse generations are plausible. We do so through a principled multi-modal variational autoencoder framework. We demonstrate its efficacy on the challenging LRWand GRID datasets and demonstrate performance better than the baseline, while having the ability to generate multiple diverse lip synchronized videos",
    "checked": true,
    "id": "411cb36c01d780524bf022d342f41b2db96b2bca",
    "semantic_title": "stochastic talking face generation using latent distribution matching",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wu20f_interspeech.html": {
    "title": "Speech-to-Singing Conversion Based on Boundary Equilibrium GAN",
    "volume": "main",
    "abstract": "This paper investigates the use of generative adversarial network (GAN)-based models for converting a speech signal into a singing one, without reference to the phoneme sequence underlying the speech. This is achieved by viewing speech-to-singing conversion as a style transfer problem. Specifically, given a speech input, and the F0 contour of the target singing output, the proposed model generates the spectrogram of a singing signal with a progressive-growing encoder/decoder architecture. Moreover, the model uses a boundary equilibrium GAN loss term such that it can learn from both paired and unpaired data. The spectrogram is finally converted into wave with a separate GAN-based vocoder. Our quantitative and qualitative analysis show that the proposed model generates singing voices with much higher naturalness than an existing non adversarially-trained baseline",
    "checked": true,
    "id": "2c5345f8264fe642577bc806267a51b0ebd909d5",
    "semantic_title": "speech-to-singing conversion based on boundary equilibrium gan",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2020/goto20_interspeech.html": {
    "title": "Face2Speech: Towards Multi-Speaker Text-to-Speech Synthesis Using an Embedding Vector Predicted from a Face Image",
    "volume": "main",
    "abstract": "We are quite able to imagine voice characteristics of a speaker from his/her appearance, especially a face. In this paper, we propose Face2Speech, which generates speech with its characteristics predicted from a face image. This framework consists of three separately trained modules: a speech encoder, a multi-speaker text-to-speech (TTS), and a face encoder. The speech encoder outputs an embedding vector which is distinguishable from other speakers. The multi-speaker TTS synthesizes speech by using the embedding vector, and then the face encoder outputs the embedding vector of a speaker from the speaker's face image. Experimental results of matching and naturalness tests demonstrate that synthetic speech generated with the face-derived embedding vector is comparable to one with the speech-derived embedding vector",
    "checked": true,
    "id": "b96f42659acbe92bba23dfad70c82d54e594b3f6",
    "semantic_title": "face2speech: towards multi-speaker text-to-speech synthesis using an embedding vector predicted from a face image",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20n_interspeech.html": {
    "title": "Speech Driven Talking Head Generation via Attentional Landmarks Based Representation",
    "volume": "main",
    "abstract": "Previous talking head generation methods mostly focus on frontal face synthesis while neglecting natural person head motion. In this paper, a generative adversarial network (GAN) based method is proposed to generate talking head video with not only high quality facial appearance, accurate lip movement, but also natural head motion. To this aim, the facial landmarks are detected and used to represent lip motion and head pose, and the conversions from speech to these middle level representations are learned separately through Convolutional Neural Networks (CNN) with wingloss. The Gated Recurrent Unit (GRU) is adopted to regularize the sequential transition. The representations for different factors of talking head are jointly feeded to a Generative Adversarial Network (GAN) based model with an attentional mechanism to synthesize the talking video. Extensive experiments on the benchmark dataset as well as our own collected dataset validate that the propose method can yield talking videos with natural head motions, and the performance is superior to state-of-the-art talking face generation methods",
    "checked": true,
    "id": "ed73963ceafc08f3c0c5ca379a8270a0a4a4748d",
    "semantic_title": "speech driven talking head generation via attentional landmarks based representation",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2020/schadler20_interspeech.html": {
    "title": "Optimization and Evaluation of an Intelligibility-Improving Signal Processing Approach (IISPA) for the Hurricane Challenge 2.0 with FADE",
    "volume": "main",
    "abstract": "This contributions describes the \"IISPA\" submission to the Hurricane Challenge 2.0. The challenge organizers called for submissions of speech signals processed with the aim to improve their intelligibility in adverse listening conditions. They evaluated the submissions with matrix sentence tests in an international listening experiment. An intelligibility-improving signal processing approach (IISPA) inspired from research on speech perception of listeners with impaired hearing was designed. Its parameters were optimized with an objective intelligibility model, the simulation framework for auditory discrimination experiments (FADE). In FADE, a re-purposed automatic speech recognition (ASR) system is employed as a models for human speech recognition performance. The model predicted an improvement in speech recognition threshold (SRT) of approximately 5.0 dB due to the optimized IISPA. The processed speech signals were evaluated in the Hurricane Challenge 2.0. The measured improvements were language-dependent: up to 4.8 dB for the Spanish test, up to 3.8 dB for the German test, and up to 2.1 dB for the English test. The results show on the one hand the potential of using an ASR-based speech recognition model to optimize an intelligibility-improving signal processing scheme, and on the other hand the need for thorough listening experiments",
    "checked": true,
    "id": "0e226ca8b44debce2748a883421ac8854fcba789",
    "semantic_title": "optimization and evaluation of an intelligibility-improving signal processing approach (iispa) for the hurricane challenge 2.0 with fade",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20o_interspeech.html": {
    "title": "iMetricGAN: Intelligibility Enhancement for Speech-in-Noise Using Generative Adversarial Network-Based Metric Learning",
    "volume": "main",
    "abstract": "The intelligibility of natural speech is seriously degraded when exposed to adverse noisy environments. In this work, we propose a deep learning-based speech modification method to compensate for the intelligibility loss, with the constraint that the root mean square (RMS) level and duration of the speech signal are maintained before and after modifications. Specifically, we utilize an iMetricGAN approach to optimize the speech intelligibility metrics with generative adversarial networks (GANs). Experimental results show that the proposed iMetricGAN outperforms conventional state-of-the-art algorithms in terms of objective measures, i.e., speech intelligibility in bits (SIIB) and extended short-time objective intelligibility (ESTOI), under a Cafeteria noise condition. In addition, formal listening tests reveal significant intelligibility gains when both noise and reverberation exist",
    "checked": true,
    "id": "1334e3b23bc960d60b613df4812c8d8bce495d9b",
    "semantic_title": "imetricgan: intelligibility enhancement for speech-in-noise using generative adversarial network-based metric learning",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2020/rennies20_interspeech.html": {
    "title": "Intelligibility-Enhancing Speech Modifications — The Hurricane Challenge 2.0",
    "volume": "main",
    "abstract": "Understanding speech played back in noisy and reverberant conditions remains a challenging task. This paper describes the Hurricane Challenge 2.0, the second large-scale evaluation of algorithms aiming to solve the near-end listening enhancement problem. The challenge consisted of modifying German, English, and Spanish speech, which was then evaluated by a total of 187 listeners at three sites. Nine algorithms participated in the challenge. Results indicate a large variability in performance between the algorithms, and that some entries achieved large speech intelligibility benefits. The largest observed benefits corresponded to intensity changes of about 7 dB, which exceeded the results obtained in the previous challenge despite more complex listening conditions. A priori information about the acoustic conditions did not provide a general advantage",
    "checked": false,
    "id": "82734ce2076d97390f5b916049758d9775fb7526",
    "semantic_title": "intelligibility-enhancing speech modifications - the hurricane challenge 2.0",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2020/simantiraki20_interspeech.html": {
    "title": "Exploring Listeners' Speech Rate Preferences",
    "volume": "main",
    "abstract": "Fast speech may reduce intelligibility, but there is little agreement as to whether listeners benefit from slower speech in noisy conditions. The current study explored the relationship between speech rate and masker properties using a listening preference technique in which participants were able to control speech rate in real time. Spanish listeners adjusted speech rate while listening to word sequences in quiet, in stationary noise at signal-to-noise ratios of 0, +6 and +12 dB, and in modulated noise for 5 envelope modulation rates. Following selection of a preferred rate, participants went on to identify words presented at that rate. Listeners favoured faster speech in quiet, chose increasingly slower rates in increasing levels of stationary noise, and showed a preference for speech rates that led to a contrast with masker envelope modulation rates. Participants showed distinct preferences even when intelligibility was near ceiling levels. These outcomes suggest that individuals attempt to compensate for the decrement in cognitive resources availability in more adverse conditions by reducing speech rate and are able to exploit differences in modulation properties of the target speech and masker. The listening preference approach provides insights into factors such as listening effort that are not measured in intelligibility-based metrics",
    "checked": true,
    "id": "25569c479c92e49f01b336c1cbc2b4f58ca800ac",
    "semantic_title": "exploring listeners' speech rate preferences",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/bederna20_interspeech.html": {
    "title": "Adaptive Compressive Onset-Enhancement for Improved Speech Intelligibility in Noise and Reverberation",
    "volume": "main",
    "abstract": "Near-end listening enhancement (NELE) algorithms aim to pre-process speech prior to playback via loudspeakers so as to maintain high speech intelligibility even when listening conditions are not optimal, e.g., due to noise or reverberation. Often NELE algorithms are designed for scenarios considering either only the detrimental effect of noise or only reverberation, but not both disturbances. In many typical applications scenarios, however, both factors are present. In this paper, we evaluate a new combination of a noise-dependent and a reverberation-dependent algorithm implemented in a common framework. Specifically, we use instrumental measures as well as subjective ratings of listening effort for acoustic scenarios with different reverberation times and realistic signal-to-noise ratios. The results show that the noise-dependent algorithm also performs well in reverberation, and that the combination of both algorithms can yield slightly better performance than the individual algorithms alone. This benefit appears to depend strongly on the specific acoustic condition, indicating that further work is required to optimize the adaptive algorithm behavior",
    "checked": true,
    "id": "969a51b7b95677c34c44a5d53435d043e98425f4",
    "semantic_title": "adaptive compressive onset-enhancement for improved speech intelligibility in noise and reverberation",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chermaz20_interspeech.html": {
    "title": "A Sound Engineering Approach to Near End Listening Enhancement",
    "volume": "main",
    "abstract": "We present the beta version of ASE (the Automatic Sound Engineer), a NELE (Near End Listening Enhancement) algorithm based on audio engineering knowledge. Generations of sound engineers have improved the intelligibility of speech against competing sounds and reverberation, while maintaining high sound quality and artistic integrity (e.g., audio track mixing in music and movies). We try to grasp the essential aspects of this expert knowledge and apply it to the more mundane context of speech playback in realistic noise. The algorithm described here was entered into the Hurricane Challenge 2.0, an evaluation of NELE algorithms. Results from those listening tests across three languages show the potential of our approach, which achieved improvements of over 7 dB EIC (Equivalent Intensity Change), corresponding to an absolute increase of 58% WAR (Word Accuracy Rate)",
    "checked": true,
    "id": "4c56e0d5b8bcc3ac53870825d052b0ae2cabe630",
    "semantic_title": "a sound engineering approach to near end listening enhancement",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2020/paul20b_interspeech.html": {
    "title": "Enhancing Speech Intelligibility in Text-To-Speech Synthesis Using Speaking Style Conversion",
    "volume": "main",
    "abstract": "The increased adoption of digital assistants makes text-to-speech (TTS) synthesis systems an indispensable feature of modern mobile devices. It is hence desirable to build a system capable of generating highly intelligible speech in the presence of noise. Past studies have investigated style conversion in TTS synthesis, yet degraded synthesized quality often leads to worse intelligibility. To overcome such limitations, we proposed a novel transfer learning approach using Tacotron and WaveRNN based TTS synthesis. The proposed speech system exploits two modification strategies: (a) Lombard speaking style data and (b) Spectral Shaping and Dynamic Range Compression (SSDRC) which has been shown to provide high intelligibility gains by redistributing the signal energy on the time-frequency domain. We refer to this extension as Lombard-SSDRC TTS system. Intelligibility enhancement as quantified by the Intelligibility in Bits (SIIB ) measure shows that the proposed Lombard-SSDRC TTS system shows significant relative improvement between 110% and 130% in speech-shaped noise (SSN), and 47% to 140% in competing-speaker noise (CSN) against the state-of-the-art TTS approach. Additional subjective evaluation shows that Lombard-SSDRC TTS successfully increases the speech intelligibility with relative improvement of 455% for SSN and 104% for CSN in median keyword correction rate compared to the baseline TTS method",
    "checked": true,
    "id": "8ba8e09fb5a5855ffb378b64bca34216282b1907",
    "semantic_title": "enhancing speech intelligibility in text-to-speech synthesis using speaking style conversion",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2020/arai20b_interspeech.html": {
    "title": "Two Different Mechanisms of Movable Mandible for Vocal-Tract Model with Flexible Tongue",
    "volume": "main",
    "abstract": "In 2017 and 2018, two types of vocal-tract models with physical materials were developed that resemble anatomical models and can physically produce human-like speech sounds. The 2017 model is a static-model, and its vocal-tract configuration is set to produce the vowel /a/. The 2018 model is a dynamic-model, and portions of the articulators including the top surface of the tongue are made of a gel-type material. This allows a user to manipulate the shape of the tongue and articulate different vowels and a certain set of consonants. However, the mandible of the model is fixed, making it difficult to manipulate different sounds with different jaw openings, such as high vs. low vowels. Therefore, in 2019, two types were developed by adding an additional mandible mechanism to the 2018 model. For the first type, the mandible was designed to move between the open and closed positions by creating an arc-shape rail. For the second type, the mandible moves the same trajectory with an additional support. As a result, various speech sounds with a flexible-tongue and moveable mandible can be easily produced. These models are more realistic than the anatomical models proposed in 2017 and 2018 in terms of articulatory movements",
    "checked": true,
    "id": "b29c0242d0e2ca0a227926e25d66bfcffe364a1d",
    "semantic_title": "two different mechanisms of movable mandible for vocal-tract model with flexible tongue",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/fang20_interspeech.html": {
    "title": "Improving the Performance of Acoustic-to-Articulatory Inversion by Removing the Training Loss of Noncritical Portions of Articulatory Channels Dynamically",
    "volume": "main",
    "abstract": "For decades, average Root Mean Square Error (RMSE) over all the articulatory channels is one of the most prevalent cost functions for training statistical models for the task of acoustic-to-articulatory inversion (AAI). One of the underlying assumptions is that the samples of all the articulatory channels used for training are balanced and play the same role in AAI. However, this is not true from speech production point view. In this study, at each time instant, each articulatory channel is classified to be critical or noncritical according to their roles in the formation of constrictions along the vocal tract when producing speech sound. It is found that the training set is dominated by the samples of noncritical articulatory channels. To deal with the unbalanced dataset problem, several Bi-LSTM networks are trained by removing the of noncritical portions of each articulatory channels if the training errors are less than some dynamic threshold. The results indicate that the average RMSE over all the articulatory channels, the average RMSE over the critical articulators, and the average RMSE over the noncritical articulators can be reduced significantly by the proposed method",
    "checked": true,
    "id": "a78946785f631e9a2f10ce9a1d8c288e3eede27f",
    "semantic_title": "improving the performance of acoustic-to-articulatory inversion by removing the training loss of noncritical portions of articulatory channels dynamically",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/illa20_interspeech.html": {
    "title": "Speaker Conditioned Acoustic-to-Articulatory Inversion Using x-Vectors",
    "volume": "main",
    "abstract": "Speech production involves the movement of various articulators, including tongue, jaw, and lips. Estimating the movement of the articulators from the acoustics of speech is known as acoustic-to-articulatory inversion (AAI). Recently, it has been shown that instead of training AAI in a speaker specific manner, pooling the acoustic-articulatory data from multiple speakers is beneficial. Further, additional conditioning with speaker specific information by one-hot encoding at the input of AAI along with acoustic features benefits the AAI performance in a closed-set speaker train and test condition. In this work, we carry out an experimental study on the benefit of using x-vectors for providing speaker specific information to condition AAI. Experiments with 30 speakers have shown that the AAI performance benefits from the use of x-vectors in a closed set seen speaker condition. Further, x-vectors also generalizes well for unseen speaker evaluation",
    "checked": true,
    "id": "bc201db81e3d93fed3493a8f437c5767724e7624",
    "semantic_title": "speaker conditioned acoustic-to-articulatory inversion using x-vectors",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2020/liu20g_interspeech.html": {
    "title": "Coarticulation as Synchronised Sequential Target Approximation: An EMA Study",
    "volume": "main",
    "abstract": "In this study we tested the hypothesis that consonant and vowel articulations start at the same time at syllable onset [1]. Articulatory data was collected for Mandarin Chinese using Electromagnetic Articulography (EMA), which tracks flesh-point movements in time and space. Unlike the traditional velocity threshold method [2], we used a triplet method based on the minimal pair paradigm [3] that detects divergence points between contrastive pairs of C or V respectively, before comparing their relative timing. Results show that articulatory onsets of consonant and vowel in CV syllables do not differ significantly from each other, which is consistent with the CV synchrony hypothesis. At the same time, the results also show some evidence that articulators that are shared by both C and V are engaged in sequential articulation, i.e., approaching the V target after approaching the C target",
    "checked": true,
    "id": "8905153838030f184933450244189e488c0ba13d",
    "semantic_title": "coarticulation as synchronised sequential target approximation: an ema study",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/santos20_interspeech.html": {
    "title": "Improved Model for Vocal Folds with a Polyp with Potential Application",
    "volume": "main",
    "abstract": "A new model for vocal folds with a polyp is proposed, based on a mass-spring-damper system and body-cover structure. The model was used to synthesize a wide variety of sustained vowels samples, with and without vocal polyps. Analytical conjectures regarding the effect of a polyp on synthesized voice signals corresponding to sustained vowels were performed. These conjectures are then used to estimate intrinsic dimension and differential entropy. These parameters were used to implement a naive classifier with the samples of the public Saarbruecken Voice Database, as a proof of concept. The results obtained suggests that the model presented in this paper might be a useful tool for tuning actual polyp detectors",
    "checked": true,
    "id": "fd51a4d0a36fb99fcf168e45e0bab9b8b25e5429",
    "semantic_title": "improved model for vocal folds with a polyp with potential application",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20l_interspeech.html": {
    "title": "Regional Resonance of the Lower Vocal Tract and its Contribution to Speaker Characteristics",
    "volume": "main",
    "abstract": "This study attempts to describe a plausible causal mechanism of generating individual vocal characteristics in higher spectra. The lower vocal tract has been suggested to be such a causal region, but a question remains as to how this region modulates vowels' higher spectra. Based on existing data, this study predicts that resonance of the lower vocal tract modulates higher vowel spectra into a peak-dip-peak pattern. A preliminary acoustic simulation was made to confirm that complexity of lower vocal-tract cavities generates such a pattern with the second peak. This spectral modulation pattern was further examined to see to what extent it contributes to generating static speaker characteristics. To do so, a statistical analysis of male and female F-ratio curves was conducted based on a speech database. In the result, three frequency regions for the peak-dip-peak patterns correspond to three regions in the gender-specific F-ratio curves. Thus, this study suggests that, while the first peak may be the major determinant by the human ears, the whole frequency pattern facilitates speaker recognition by machines",
    "checked": true,
    "id": "b2d4479e6aed6667b0b989dd49c758da487eea61",
    "semantic_title": "regional resonance of the lower vocal tract and its contribution to speaker characteristics",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mannem20_interspeech.html": {
    "title": "Air-Tissue Boundary Segmentation in Real Time Magnetic Resonance Imaging Video Using 3-D Convolutional Neural Network",
    "volume": "main",
    "abstract": "The real-time Magnetic Resonance Imaging (rtMRI) is often used for speech production research as it captures the complete view of the vocal tract during speech. Air-tissue boundaries (ATBs) are the contours that trace the transition between high-intensity tissue region and low-intensity airway cavity region in an rtMRI video. The ATBs are used in several speech related applications. However, the ATB segmentation is a challenging task as the rtMRI frames have low resolution and low signal-to-noise ratio. Several works have been proposed in the past for ATB segmentation. Among these, the supervised algorithms have been shown to perform well compared to the unsupervised algorithms. However, the supervised algorithms have limited generalizability towards subjects not involved in training. In this work, we propose a 3-dimensional convolutional neural network (3D-CNN) which utilizes both spatial and temporal information from the rtMRI video for accurate ATB segmentation. The 3D-CNN model captures the vocal tract dynamics in an rtMRI video independent of the morphology of the subject leading to an accurate ATB segmentation for unseen subjects. In a leave-one-subject-out experimental setup, it is observed that the proposed approach provides ~32% relative improvement in the performance compared to the best (SegNet based) baseline approach",
    "checked": true,
    "id": "34f6d30f390a21b4a2d5bfd67a6a9f63aeea23ce",
    "semantic_title": "air-tissue boundary segmentation in real time magnetic resonance imaging video using 3-d convolutional neural network",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2020/purohit20_interspeech.html": {
    "title": "An Investigation of the Virtual Lip Trajectories During the Production of Bilabial Stops and Nasal at Different Speaking Rates",
    "volume": "main",
    "abstract": "We propose a technique to estimate virtual upper lip (VUL) and virtual lower lip (VLL) trajectories during production of bilabial stop consonants (/p/, /b/) and nasal (/m/). A VUL (VLL) is a hypothetical trajectory below (above) the measured UL (LL) trajectory which could have been achieved by UL (LL) if UL and LL were not in contact with each other during bilabial stops and nasal. Maximum deviation of UL from VUL and its location as well as the range of VUL are used as features, denoted by VUL MD, VUL MDL, and VUL R, respectively. Similarly, VLL MD, VLL MDL, and VLL R are also computed. Analyses of these six features are carried out for /p/, /b/, and /m/ at slow, normal and fast rates based on electromagnetic articulograph (EMA) recordings of VCV stimuli spoken by ten subjects. While no significant differences were observed among /p/, /b/, and /m/ in every rate, all six features except VLL MD were found to drop significantly from slow to fast rates. These six features were also found to perform better in an automatic classification task between slow vs fast rates compared to five baseline features computed from UL and LL comprising their ranges, velocities and minimum distance from each other",
    "checked": true,
    "id": "1c86e02458fa7c8c4dcbd3cd61db49cf0b6707ea",
    "semantic_title": "an investigation of the virtual lip trajectories during the production of bilabial stops and nasal at different speaking rates",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ge20_interspeech.html": {
    "title": "SpEx+: A Complete Time Domain Speaker Extraction Network",
    "volume": "main",
    "abstract": "Speaker extraction aims to extract the target speech signal from a multi-talker environment given a target speaker's reference speech. We recently proposed a time-domain solution, SpEx, that avoids the phase estimation in frequency-domain approaches. Unfortunately, SpEx is not fully a time-domain solution since it performs time-domain speech encoding for speaker extraction, while taking frequency-domain speaker embedding as the reference. The size of the analysis window for time-domain and the size for frequency-domain input are also different. Such mismatch has an adverse effect on the system performance. To eliminate such mismatch, we propose a complete time-domain speaker extraction solution, that is called SpEx+. Specifically, we tie the weights of two identical speech encoder networks, one for the encoder-extractor-decoder pipeline, another as part of the speaker encoder. Experiments show that the SpEx+ achieves 0.8dB and 2.1dB SDR improvement over the state-of-the-art SpEx baseline, under different and same gender conditions on WSJ0-2mix-extr database respectively",
    "checked": true,
    "id": "d1c98e90744c1ff65f1ed9170605063b788a9718",
    "semantic_title": "spex+: a complete time domain speaker extraction network",
    "citation_count": 70
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20p_interspeech.html": {
    "title": "Atss-Net: Target Speaker Separation via Attention-Based Neural Network",
    "volume": "main",
    "abstract": "Recently, Convolutional Neural Network (CNN) and Long short-term memory (LSTM) based models have been introduced to deep learning-based target speaker separation. In this paper, we propose an Attention-based neural network (Atss-Net) in the spectrogram domain for the task. It allows the network to compute the correlation between each feature parallelly, and using shallower layers to extract more features, compared with the CNN-LSTM architecture. Experimental results show that our Atss-Net yields better performance than the VoiceFilter, although it only contains half of the parameters. Furthermore, our proposed model also demonstrates promising performance in speech enhancement",
    "checked": true,
    "id": "69a2c9f5356db524fee6fb00c7016e84e82dd745",
    "semantic_title": "atss-net: target speaker separation via attention-based neural network",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2020/qu20b_interspeech.html": {
    "title": "Multimodal Target Speech Separation with Voice and Face References",
    "volume": "main",
    "abstract": "Target speech separation refers to isolating target speech from a multi-speaker mixture signal by conditioning on auxiliary information about the target speaker. Different from the mainstream audio-visual approaches which usually require simultaneous visual streams as additional input, e.g. the corresponding lip movement sequences, in our approach we propose the novel use of a single face profile of the target speaker to separate expected clean speech. We exploit the fact that the image of a face contains information about the person's speech sound. Compared to using a simultaneous visual sequence, a face image is easier to obtain by pre-enrollment or on websites, which enables the system to generalize to devices without cameras. To this end, we incorporate face embeddings extracted from a pre-trained model for face recognition into the speech separation, which guide the system in predicting a target speaker mask in the time-frequency domain. The experimental results show that a pre-enrolled face image is able to benefit separating expected speech signals. Additionally, face information is complementary to voice reference and we show that further improvement can be achieved when combining both face and voice embeddings ",
    "checked": true,
    "id": "3bc8a42ed4e6537063a4d20fc285e33bd47c1baa",
    "semantic_title": "multimodal target speech separation with voice and face references",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20m_interspeech.html": {
    "title": "X-TaSNet: Robust and Accurate Time-Domain Speaker Extraction Network",
    "volume": "main",
    "abstract": "Extracting the speech of a target speaker from mixed audios, based on a reference speech from the target speaker, is a challenging yet powerful technology in speech processing. Recent studies of speaker-independent speech separation, such as TasNet, have shown promising results by applying deep neural networks over the time-domain waveform. Such separation neural network does not directly generate reliable and accurate output when target speakers are specified, because of the necessary prior on the number of speakers and the lack of robustness when dealing with audios with absent speakers. In this paper, we break these limitations by introducing a new speaker-aware speech masking method, called X-TaSNet. Our proposal adopts new strategies, including a distortion-based loss and corresponding alternating training scheme, to better address the robustness issue. X-TaSNet significantly enhances the extracted speech quality, doubling SDRi and SI-SNRi of the output speech audio over state-of-the-art voice filtering approach. X-TaSNet also improves the reliability of the results by improving the accuracy of speaker identity in the output audio to 95.4%, such that it returns silent audios in most cases when the target speaker is absent. These results demonstrate X-TaSNet moves one solid step towards more practical applications of speaker extraction technology",
    "checked": true,
    "id": "f7d157ad17c746720e49e2ac2a25cb24a9cda2bd",
    "semantic_title": "x-tasnet: robust and accurate time-domain speaker extraction network",
    "citation_count": 23
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20q_interspeech.html": {
    "title": "Listen, Watch and Understand at the Cocktail Party: Audio-Visual-Contextual Speech Separation",
    "volume": "main",
    "abstract": "Solving the cocktail party problem with the multi-modal approach has become popular in recent years. Humans can focus on the speech that they are interested in for the multi-talker mixed speech, by hearing the mixed speech, watching the speaker, and understanding the context what the speaker is talking about. In this paper, we try to solve the speaker-independent speech separation problem with all three audio-visual-contextual modalities at the first time, and those are hearing speech, watching speaker and understanding contextual language. Compared to the previous methods applying pure audio modal or audio-visual modals, a specific model is further designed to extract contextual language information for all target speakers directly from the speech mixture. Then these extracted contextual knowledge are further incorporated into the multi-modal based speech separation architecture with an appropriate attention mechanism. The experiments show that a significant performance improvement can be observed with the newly proposed audio-visual-contextual speech separation",
    "checked": true,
    "id": "d9afbe7fd2bec4308454765ce84477046c79eeed",
    "semantic_title": "listen, watch and understand at the cocktail party: audio-visual-contextual speech separation",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hao20_interspeech.html": {
    "title": "A Unified Framework for Low-Latency Speaker Extraction in Cocktail Party Environments",
    "volume": "main",
    "abstract": "Speech recognition technology in single-talker scenes has matured in recent years. However, in noisy environments, especially in multi-talker scenes, speech recognition performance is significantly reduced. Towards cocktail party problem, we propose a unified time-domain target speaker extraction framework. In this framework, we obtain a voiceprint from a clean speech of the target speaker and then extract the speech of the same speaker in a mixed speech based on the previously obtained voiceprint. This framework uses voiceprint information to avoid permutation problems. In addition, a time-domain model can avoid the phase reconstruction problem of traditional time-frequency domain models. Our framework is suitable for scenes where people are relatively fixed and their voiceprints are easily registered, such as in a car, home, meeting room, or other such scenes. The proposed global model based on the dual-path recurrent neural network (DPRNN) block achieved state-of-the-art under speaker extraction tasks on the WSJ0-2mix dataset. We also built corresponding low-latency models. Results showed comparable model performance and a much shorter upper limit latency than time-frequency domain models. We found that performance of the low-latency model gradually decreased as latency decreased, which is important when deploying models in actual application scenarios",
    "checked": true,
    "id": "729fe42fb04bfc4858e6bd76674dd9ee925fc56b",
    "semantic_title": "a unified framework for low-latency speaker extraction in cocktail party environments",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhao20c_interspeech.html": {
    "title": "Time-Domain Target-Speaker Speech Separation with Waveform-Based Speaker Embedding",
    "volume": "main",
    "abstract": "Target-speaker speech separation, due to its essence in industrial applications, has been heavily researched for long by many. The key metric for qualifying a good separation algorithm still lies on the separation performance, i.e., the quality of the separated voice. In this paper, we presented a novel high-performance time-domain waveform based target-speaker speech separation architecture (WaveFilter) for this task. Unlike most previous researches which adopted Time-Frequency based approaches, WaveFilter does the job by applying Convolutional Neural Network (CNN) based feature extractors directly on the raw Time-domain audio data, for both the speech separation network and the auxiliary target-speaker feature extraction network. We achieved a 10.46 Signal to Noise Ratio (SNR) improvement on the WSJ0 2-mix dataset and a 10.44 SNR improvement on the Librispeech dataset as our final results, which is much higher than the existing approaches. Our method also achieved an 4.9 SNR improvement on the WSJ0 3-mix data. This proves the feasibility of WaveFilter on separating the target-speaker's voice from multi-speaker voice mixtures without knowing the exact number of speakers in advance, which in turn proves the readiness of our method for real-world applications",
    "checked": true,
    "id": "b85a4bea65c15ec63c4e6f93163aee668c77b7b6",
    "semantic_title": "time-domain target-speaker speech separation with waveform-based speaker embedding",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ochiai20_interspeech.html": {
    "title": "Listen to What You Want: Neural Network-Based Universal Sound Selector",
    "volume": "main",
    "abstract": "Being able to control the acoustic events (AEs) to which we want to listen would allow the development of more controllable hearable devices. This paper addresses the AE sound selection (or removal) problems, that we define as the extraction (or suppression) of all the sounds that belong to one or multiple desired AE classes. Although this problem could be addressed with a combination of source separation followed by AE classification, this is a sub-optimal way of solving the problem. Moreover, source separation usually requires knowing the maximum number of sources, which may not be practical when dealing with AEs. In this paper, we propose instead a universal sound selection neural network that enables to directly select AE sounds from a mixture given user-specified target AE classes. The proposed framework can be explicitly optimized to simultaneously select sounds from multiple desired AE classes, independently of the number of sources in the mixture. We experimentally show that the proposed method achieves promising AE sound selection performance and could be generalized to mixtures with a number of sources that are unseen during training",
    "checked": true,
    "id": "7a05c8d879ea4eb980a2f136d510d7adec02c763",
    "semantic_title": "listen to what you want: neural network-based universal sound selector",
    "citation_count": 35
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yasuda20_interspeech.html": {
    "title": "Crossmodal Sound Retrieval Based on Specific Target Co-Occurrence Denoted with Weak Labels",
    "volume": "main",
    "abstract": "Recent advancements in representation learning enable cross-modal retrieval by modeling an audio-visual co-occurrence in a single aspect, such as physical and linguistic. Unfortunately, in real-world media data, since co-occurrences in various aspects are complexly mixed, it is difficult to distinguish a specific target co-occurrence from many other non-target co-occurrences, resulting in failure in crossmodal retrieval. To overcome this problem, we propose a triplet-loss-based representation learning method that incorporates an awareness mechanism. We adopt weakly-supervised event detection, which provides a constraint in representation learning so that our method can \"be aware\" of a specific target audio-visual co-occurrence and discriminate it from other non-target co-occurrences. We evaluated the performance of our method by applying it to a sound effect retrieval task using recorded TV broadcast data. In the task, a sound effect appropriate for a given video input should be retrieved. We then conducted objective and subjective evaluations, the results indicating that the proposed method produces significantly better associations of sound and visual effects than baselines with no awareness mechanism",
    "checked": true,
    "id": "81da4385a561b48763d140be8bbb30212bbd467d",
    "semantic_title": "crossmodal sound retrieval based on specific target co-occurrence denoted with weak labels",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2020/xu20c_interspeech.html": {
    "title": "Speaker-Aware Monaural Speech Separation",
    "volume": "main",
    "abstract": "Predicting and applying Time-Frequency (T-F) masks on mixture signals have been successfully utilized for speech separation. However, existing studies have not well utilized the identity context of a speaker for the inference of masks. In this paper, we propose a novel speaker-aware monaural speech separation model. We firstly devise an encoder to disentangle speaker identity information with the supervision from the auxiliary speaker verification task. Then, we develop a spectrogram masking network to predict speaker masks, which would be applied to the mixture signal for the reconstruction of source signals. Experimental results on two WSJ0 mixed datasets demonstrate that our proposed model outperforms existing models in different separation scenarios",
    "checked": true,
    "id": "d11bfa7e51a97b1337fad874185419ce83c67757",
    "semantic_title": "speaker-aware monaural speech separation",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shinncunningham20_interspeech.html": {
    "title": "Brain networks enabling speech perception in everyday settings",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "1a872c15a8f655fd863121f4d2154d060b61f0a5",
    "semantic_title": "brain networks enabling speech perception in everyday settings",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20o_interspeech.html": {
    "title": "A DNN-HMM-DNN Hybrid Model for Discovering Word-Like Units from Spoken Captions and Image Regions",
    "volume": "main",
    "abstract": "Discovering word-like units without textual transcriptions is an important step in low-resource speech technology. In this work, we demonstrate a model inspired by statistical machine translation and hidden Markov model/deep neural network (HMM-DNN) hybrid systems. Our learning algorithm is capable of discovering the visual and acoustic correlates of K distinct words in an unknown language by simultaneously learning the mapping from image regions to concepts (the first DNN), the mapping from acoustic feature vectors to phones (the second DNN), and the optimum alignment between the two (the HMM). In the simulated low-resource setting using MSCOCO and SpeechCOCO datasets, our model achieves 62.4% alignment accuracy and outperforms the audio-only segmental embedded GMM approach on standard word discovery evaluation metrics",
    "checked": true,
    "id": "98ade41d9814fb150f35ab0bd710b4aecf5f75c2",
    "semantic_title": "a dnn-hmm-dnn hybrid model for discovering word-like units from spoken captions and image regions",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2020/elbayad20_interspeech.html": {
    "title": "Efficient Wait-k Models for Simultaneous Machine Translation",
    "volume": "main",
    "abstract": "Simultaneous machine translation consists in starting output generation before the entire input sequence is available. Wait-k decoders offer a simple but efficient approach for this problem. They first read k source tokens, after which they alternate between producing a target token and reading another source token. We investigate the behavior of wait-k decoding in low resource settings for spoken corpora using IWSLT datasets. We improve training of these models using unidirectional encoders, and training across multiple values of k. Experiments with Transformer and 2D-convolutional architectures show that our wait-k models generalize well across a wide range of latency levels. We also show that the 2D-convolution architecture is competitive with Transformers for simultaneous translation of spoken language",
    "checked": true,
    "id": "8f9f7a0714408fb49eeb7060ab16bc1eedb219fd",
    "semantic_title": "efficient wait-k models for simultaneous machine translation",
    "citation_count": 45
  },
  "https://www.isca-speech.org/archive/interspeech_2020/nguyen20_interspeech.html": {
    "title": "Investigating Self-Supervised Pre-Training for End-to-End Speech Translation",
    "volume": "main",
    "abstract": "Self-supervised learning from raw speech has been proven beneficial to improve automatic speech recognition (ASR). We investigate here its impact on end-to-end automatic speech translation (AST) performance. We use a contrastive predictive coding (CPC) model pre-trained from unlabeled speech as a feature extractor for a downstream AST task. We show that self-supervised pre-training is particularly efficient in low resource settings and that fine-tuning CPC models on the AST training data further improves performance. Even in higher resource settings, ensembling AST models trained with filter-bank and CPC representations leads to near state-of-the-art models without using any ASR pre-training. This might be particularly beneficial when one needs to develop a system that translates from speech in a language with poorly standardized orthography or even from speech in an unwritten language",
    "checked": true,
    "id": "3ec725d0e992243044e30806a04baefad6569f20",
    "semantic_title": "investigating self-supervised pre-training for end-to-end speech translation",
    "citation_count": 28
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gaido20_interspeech.html": {
    "title": "Contextualized Translation of Automatically Segmented Speech",
    "volume": "main",
    "abstract": "Direct speech-to-text translation (ST) models are usually trained on corpora segmented at sentence level, but at inference time they are commonly fed with audio split by a voice activity detector (VAD). Since VAD segmentation is not syntax-informed, the resulting segments do not necessarily correspond to well-formed sentences uttered by the speaker but, most likely, to fragments of one or more sentences. This segmentation mismatch degrades considerably the quality of ST models' output. So far, researchers have focused on improving audio segmentation towards producing sentence-like splits. In this paper, instead, we address the issue in the model, making it more robust to a different, potentially sub-optimal segmentation. To this aim, we train our models on randomly segmented data and compare two approaches: fine-tuning and adding the previous segment as context. We show that our context-aware solution is more robust to VAD-segmented input, outperforming a strong base model and the fine-tuning on different VAD segmentations of an English-German test set by up to 4.25 BLEU points",
    "checked": true,
    "id": "0a762dd516b261dcd8e0cdcb133783da8d7306e5",
    "semantic_title": "contextualized translation of automatically segmented speech",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2020/pino20_interspeech.html": {
    "title": "Self-Training for End-to-End Speech Translation",
    "volume": "main",
    "abstract": "One of the main challenges for end-to-end speech translation is data scarcity. We leverage pseudo-labels generated from unlabeled audio by a cascade and an end-to-end speech translation model. This provides 8.3 and 5.7 BLEU gains over a strong semi-supervised baseline on the MuST-C English-French and English-German datasets, reaching state-of-the art performance. The effect of the quality of the pseudo-labels is investigated. Our approach is shown to be more effective than simply pre-training the encoder on the speech recognition task. Finally, we demonstrate the effectiveness of self-training by directly generating pseudo-labels with an end-to-end model instead of a cascade model",
    "checked": true,
    "id": "2d3182458ea6173d8897f0e53695c768c684bef6",
    "semantic_title": "self-training for end-to-end speech translation",
    "citation_count": 47
  },
  "https://www.isca-speech.org/archive/interspeech_2020/federico20_interspeech.html": {
    "title": "Evaluating and Optimizing Prosodic Alignment for Automatic Dubbing",
    "volume": "main",
    "abstract": "Automatic dubbing aims at replacing all speech contained in a video with speech in a different language, so that the result sounds and looks as natural as the original. Hence, in addition to conveying the same content of an original utterance (which is the typical objective of speech translation), dubbed speech should ideally also match its duration, the lip movements and gestures in the video, timbre, emotion and prosody of the speaker, and finally background noise and reverberation of the environment. In this paper, after describing our dubbing architecture, we focus on recent progress on the prosodic alignment component, which aims at synchronizing the translated transcript with the original utterances. We present empirical results for English-to-Italian dubbing on a publicly available collection of TED Talks. Our new prosodic alignment model, which allows for small relaxations in synchronicity, shows to significantly improve both prosodic alignment accuracy and overall subjective dubbing quality of previous work",
    "checked": true,
    "id": "50ab94273d050a48fb9739ff7dcdd43c708aaf93",
    "semantic_title": "evaluating and optimizing prosodic alignment for automatic dubbing",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ohishi20_interspeech.html": {
    "title": "Pair Expansion for Learning Multilingual Semantic Embeddings Using Disjoint Visually-Grounded Speech Audio Datasets",
    "volume": "main",
    "abstract": "We propose a data expansion method for learning a multilingual semantic embedding model using disjoint datasets containing images and their multilingual audio captions. Here, disjoint means that there are no shared images among the multiple language datasets, in contrast to existing works on multilingual semantic embedding based on visually-grounded speech audio, where it has been assumed that each image is associated with spoken captions of multiple languages. Although learning on disjoint datasets is more challenging, we consider it crucial in practical situations. Our main idea is to refer to another paired data when evaluating a loss value regarding an anchor image. We call this scheme \"pair expansion\". The motivation behind this idea is to utilize even disjoint pairs by finding similarities, or commonalities, that may exist in different images. Specifically, we examine two approaches for calculating similarities: one using image embedding vectors and the other using object recognition results. Our experiments show that expanded pairs improve crossmodal and cross-lingual retrieval accuracy compared with non-expanded cases. They also show that similarities measured by the image embedding vectors yield better accuracy than those based on object recognition results",
    "checked": true,
    "id": "68f6d0789f1f904f38c347f945a5c8232a5bb857",
    "semantic_title": "pair expansion for learning multilingual semantic embeddings using disjoint visually-grounded speech audio datasets",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wu20g_interspeech.html": {
    "title": "Self-Supervised Representations Improve End-to-End Speech Translation",
    "volume": "main",
    "abstract": "End-to-end speech-to-text translation can provide a simpler and smaller system but is facing the challenge of data scarcity. Pre-training methods can leverage unlabeled data and have been shown to be effective on data-scarce settings. In this work, we explore whether self-supervised pre-trained speech representations can benefit the speech translation task in both high- and low-resource settings, whether they can transfer well to other languages, and whether they can be effectively combined with other common methods that help improve low-resource end-to-end speech translation such as using a pre-trained high-resource speech recognition system. We demonstrate that self-supervised pre-trained features can consistently improve the translation performance, and cross-lingual transfer allows to extend to a variety of languages without or with little tuning",
    "checked": true,
    "id": "422b4f24677fe295c41401daa773254e367d0b22",
    "semantic_title": "self-supervised representations improve end-to-end speech translation",
    "citation_count": 32
  },
  "https://www.isca-speech.org/archive/interspeech_2020/jung20c_interspeech.html": {
    "title": "Improved RawNet with Feature Map Scaling for Text-Independent Speaker Verification Using Raw Waveforms",
    "volume": "main",
    "abstract": "Recent advances in deep learning have facilitated the design of speaker verification systems that directly input raw waveforms. For example, RawNet [1] extracts speaker embeddings from raw waveforms, which simplifies the process pipeline and demonstrates competitive performance. In this study, we improve RawNet by scaling feature maps using various methods. The proposed mechanism utilizes a scale vector that adopts a sigmoid non-linear function. It refers to a vector with dimensionality equal to the number of filters in a given feature map. Using a scale vector, we propose to scale the feature map multiplicatively, additively, or both. In addition, we investigate replacing the first convolution layer with the sinc-convolution layer of SincNet. Experiments performed on the VoxCeleb1 evaluation dataset demonstrate the effectiveness of the proposed methods, and the best performing system reduces the equal error rate by half compared to the original RawNet. Expanded evaluation results obtained using the VoxCeleb1-E and VoxCeleb-H protocols marginally outperform existing state-of-the-art systems",
    "checked": true,
    "id": "f0c4a61662d67e2caa569b51e7362b9976e8debc",
    "semantic_title": "improved rawnet with feature map scaling for text-independent speaker verification using raw waveforms",
    "citation_count": 54
  },
  "https://www.isca-speech.org/archive/interspeech_2020/jung20d_interspeech.html": {
    "title": "Improving Multi-Scale Aggregation Using Feature Pyramid Module for Robust Speaker Verification of Variable-Duration Utterances",
    "volume": "main",
    "abstract": "Currently, the most widely used approach for speaker verification is the deep speaker embedding learning. In this approach, we obtain a speaker embedding vector by pooling single-scale features that are extracted from the last layer of a speaker feature extractor. Multi-scale aggregation (MSA), which utilizes multi-scale features from different layers of the feature extractor, has recently been introduced and shows superior performance for variable-duration utterances. To increase the robustness dealing with utterances of arbitrary duration, this paper improves the MSA by using a feature pyramid module. The module enhances speaker-discriminative information of features from multiple layers via a top-down pathway and lateral connections. We extract speaker embeddings using the enhanced features that contain rich speaker information with different time scales. Experiments on the VoxCeleb dataset show that the proposed module improves previous MSA methods with a smaller number of parameters. It also achieves better performance than state-of-the-art approaches for both short and long utterances",
    "checked": true,
    "id": "cab95193f7871aabaa4c9b48d72747db7b09a0df",
    "semantic_title": "improving multi-scale aggregation using feature pyramid module for robust speaker verification of variable-duration utterances",
    "citation_count": 25
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gu20_interspeech.html": {
    "title": "An Adaptive X-Vector Model for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "In this paper, adaptive mechanisms are applied in deep neural network (DNN) training for x-vector-based text-independent speaker verification. First, adaptive convolutional neural networks (ACNNs) are employed in frame-level embedding layers, where the parameters of the convolution filters are adjusted based on the input features. Compared with conventional CNNs, ACNNs have more flexibility in capturing speaker information. Moreover, we replace conventional batch normalization (BN) with adaptive batch normalization (ABN). By dynamically generating the scaling and shifting parameters in BN, ABN adapts models to the acoustic variability arising from various factors such as channel and environmental noises. Finally, we incorporate these two methods to further improve performance. Experiments are carried out on the speaker in the wild (SITW) and VOiCES databases. The results demonstrate that the proposed methods significantly outperform the original x-vector approach",
    "checked": true,
    "id": "9d1f7e74d941e5b39b4d513cd391d952a51a3ded",
    "semantic_title": "an adaptive x-vector model for text-independent speaker verification",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2020/prieto20_interspeech.html": {
    "title": "Shouted Speech Compensation for Speaker Verification Robust to Vocal Effort Conditions",
    "volume": "main",
    "abstract": "The performance of speaker verification systems degrades when vocal effort conditions between enrollment and test (e.g., shouted vs. normal speech) are different. This is a potential situation in non-cooperative speaker verification tasks. In this paper, we present a study on different methods for linear compensation of embeddings making use of Gaussian mixture models to cluster shouted and normal speech domains. These compensation techniques are borrowed from the area of robustness for automatic speech recognition and, in this work, we apply them to compensate the mismatch between shouted and normal conditions in speaker verification. Before compensation, shouted condition is automatically detected by means of logistic regression. The process is computationally light and it is performed in the back-end of an x-vector system. Experimental results show that applying the proposed approach in the presence of vocal effort mismatch yields up to 13.8% equal error rate relative improvement with respect to a system that applies neither shouted speech detection nor compensation",
    "checked": true,
    "id": "15d3231edcb50604359c872c09384e0a6b1321d8",
    "semantic_title": "shouted speech compensation for speaker verification robust to vocal effort conditions",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/nicolson20_interspeech.html": {
    "title": "Sum-Product Networks for Robust Automatic Speaker Identification",
    "volume": "main",
    "abstract": "We introduce sum-product networks (SPNs) for robust speech processing through a simple robust automatic speaker identification (ASI) task SPNs are deep probabilistic graphical models capable of answering multiple probabilistic queries. We show that SPNs are able to remain robust by using the marginal probability density function (PDF) of the spectral features that reliably represent speech. Though current SPN toolkits and learning algorithms are in their infancy, we aim to show that SPNs have the potential to become a useful tool for robust speech processing in the future. SPN speaker models are evaluated here on real-world non-stationary and coloured noise sources at multiple signal-to-noise ratio (SNR) levels. In terms of ASI accuracy, we find that SPN speaker models are more robust than two recent convolutional neural network (CNN)-based ASI systems. Additionally, SPN speaker models consist of significantly fewer parameters than their CNN-based counterparts. The results indicate that SPN speaker models could be a robust, parameter-efficient alternative for ASI. Additionally, this work demonstrates that SPNs have potential in related tasks, such as robust automatic speech recognition (ASR) and automatic speaker verification (ASV)",
    "checked": true,
    "id": "80a442e279229f6c6c5f3dcf77b9486debdece60",
    "semantic_title": "sum-product networks for robust automatic speaker identification",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kim20b_interspeech.html": {
    "title": "Segment Aggregation for Short Utterances Speaker Verification Using Raw Waveforms",
    "volume": "main",
    "abstract": "Most studies on speaker verification systems focus on long-duration utterances, which are composed of sufficient phonetic information. However, the performances of these systems are known to degrade when short-duration utterances are inputted due to the lack of phonetic information as compared to the long utterances. In this paper, we propose a method that compensates for the performance degradation of speaker verification for short utterances, referred to as \" segment aggregation\". The proposed method adopts an ensemble-based design to improve the stability and accuracy of speaker verification systems. The proposed method segments an input utterance into several short utterances and then aggregates the segment embeddings extracted from the segmented inputs to compose a speaker embedding. Then, this method simultaneously trains the segment embeddings and the aggregated speaker embedding. In addition, we also modified the teacher-student learning method for the proposed method. Experimental results on different input duration using the VoxCeleb1 test set demonstrate that the proposed technique improves speaker verification performance by about 45.37% relatively compared to the baseline system with 1-second test utterance condition",
    "checked": true,
    "id": "baeaab2ba223b556584751762e99703703ce3a1d",
    "semantic_title": "segment aggregation for short utterances speaker verification using raw waveforms",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/rozenberg20_interspeech.html": {
    "title": "Siamese X-Vector Reconstruction for Domain Adapted Speaker Recognition",
    "volume": "main",
    "abstract": "With the rise of voice-activated applications, the need for speaker recognition is rapidly increasing. The x-vector, an embedding approach based on a deep neural network (DNN), is considered the state-of-the-art when proper end-to-end training is not feasible. However, the accuracy significantly decreases when recording conditions (noise, sample rate, etc.) are mismatched, either between the x-vector training data and the target data or between enrollment and test data. We introduce the Siamese x-vector Reconstruction (SVR) for domain adaptation. We reconstruct the embedding of a higher quality signal from a lower quality counterpart using a lean auxiliary Siamese DNN. We evaluate our method on several mismatch scenarios and demonstrate significant improvement over the baseline",
    "checked": true,
    "id": "a4e0ac46bb7cce36c7f1861eab65f04caeeccc4c",
    "semantic_title": "siamese x-vector reconstruction for domain adapted speaker recognition",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shi20b_interspeech.html": {
    "title": "Speaker Re-Identification with Speaker Dependent Speech Enhancement",
    "volume": "main",
    "abstract": "While the use of deep neural networks has significantly boosted speaker recognition performance, it is still challenging to separate speakers in poor acoustic environments. Here speech enhancement methods have traditionally allowed improved performance. The recent works have shown that adapting speech enhancement can lead to further gains. This paper introduces a novel approach that cascades speech enhancement and speaker recognition. In the first step, a speaker embedding vector is generated, which is used in the second step to enhance the speech quality and re-identify the speakers. Models are trained in an integrated framework with joint optimisation. The proposed approach is evaluated using the Voxceleb1 dataset, which aims to assess speaker recognition in real world situations. In addition three types of noise at different signal-noise-ratios were added for this work. The obtained results show that the proposed approach using speaker dependent speech enhancement can yield better speaker recognition and speech enhancement performances than two baselines in various noise conditions",
    "checked": true,
    "id": "6b6d1884fa3ccbfcd3bc58948c466ec42203ba44",
    "semantic_title": "speaker re-identification with speaker dependent speech enhancement",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lavrentyeva20_interspeech.html": {
    "title": "Blind Speech Signal Quality Estimation for Speaker Verification Systems",
    "volume": "main",
    "abstract": "The problem of system performance degradation in mismatched acoustic conditions has been widely acknowledged in the community and is common for different fields. The present state-of-the-art deep speaker embedding models are domain-sensitive. The main idea of the current research is to develop a single method for automatic signal quality estimation, which allows to evaluate short-term signal characteristics This paper presents a neural network based approach for blind speech signal quality estimation in terms of signal-to-noise ratio (SNR) and reverberation time (RT60), which is able to classify the type of underlying additive noise. Additionally, current research revealed the need for an accurate voice activity detector that performs well in both clean and noisy unseen environments. Therefore a novel neural network VAD based on U-net architecture is presented.The proposed algorithms allow to perform the analysis of NIST, SITW, Voices datasets commonly used for objective comparison of speaker verification systems from the new point of view and consider effective calibration steps to improve speaker recognition quality on them",
    "checked": true,
    "id": "2ce9b2123bc96160fb2419f42f5f1c60aa139a11",
    "semantic_title": "blind speech signal quality estimation for speaker verification systems",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20r_interspeech.html": {
    "title": "Investigating Robustness of Adversarial Samples Detection for Automatic Speaker Verification",
    "volume": "main",
    "abstract": "Recently adversarial attacks on automatic speaker verification (ASV) systems attracted widespread attention as they pose severe threats to ASV systems. However, methods to defend against such attacks are limited. Existing approaches mainly focus on retraining ASV systems with adversarial data augmentation. Also, countermeasure robustness against different attack settings are insufficiently investigated. Orthogonal to prior approaches, this work proposes to defend ASV systems against adversarial attacks with a separate detection network, rather than augmenting adversarial data into ASV training. A VGG-like binary classification detector is introduced and demonstrated to be effective on detecting adversarial samples. To investigate detector robustness in a realistic defense scenario where unseen attack settings may exist, we analyze various kinds of unseen attack settings' impact and observe that the detector is robust (6.27% EER degradation in the worst case) against unseen substitute ASV systems, but it has weak robustness (50.37% EER degradation in the worst case) against unseen perturbation methods. The weak robustness against unseen perturbation methods shows a direction for developing stronger countermeasures",
    "checked": true,
    "id": "1be374b0f65c9b375fbd4a85a5f7d1757f795b63",
    "semantic_title": "investigating robustness of adversarial samples detection for automatic speaker verification",
    "citation_count": 29
  },
  "https://www.isca-speech.org/archive/interspeech_2020/pal20_interspeech.html": {
    "title": "Modeling ASR Ambiguity for Neural Dialogue State Tracking",
    "volume": "main",
    "abstract": "Spoken dialogue systems typically use one or several (top-N) ASR sequence(s) for inferring the semantic meaning and tracking the state of the dialogue. However, ASR graphs, such as confusion networks (confnets), provide a compact representation of a richer hypothesis space than a top-N ASR list. In this paper, we study the benefits of using confusion networks with a neural dialogue state tracker (DST). We encode the 2-dimensional confnet into a 1-dimensional sequence of embeddings using a confusion network encoder which can be used with any DST system. Our confnet encoder is plugged into the ‘Global-locally Self-Attentive Dialogue State Tacker' (GLAD) model for DST and obtains significant improvements in both accuracy and inference time compared to using top-N ASR hypotheses",
    "checked": true,
    "id": "1f2f4e864a5d965cc80009166c4a3b5a49c4916a",
    "semantic_title": "modeling asr ambiguity for neural dialogue state tracking",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20p_interspeech.html": {
    "title": "ASR Error Correction with Augmented Transformer for Entity Retrieval",
    "volume": "main",
    "abstract": "Domain-agnostic Automatic Speech Recognition (ASR) systems suffer from the issue of mistranscribing domain-specific words, which leads to failures in downstream tasks. In this paper, we present a post-editing ASR error correction method using the Transformer model for entity mention correction and retrieval. Specifically, we propose a novel augmented variant of the Transformer model that encodes both the word and phoneme sequence of an entity, and attends to phoneme information in addition to word-level information during decoding to correct mistranscribed named entities. We evaluate our method on both the ASR error correction task and the downstream retrieval task. Our method achieves 48.08% entity error rate (EER) reduction in ASR error correction task and 26.74% mean reciprocal rank (MRR) improvement for the retrieval task. In addition, our augmented Transformer model significantly outperforms the vanilla Transformer model with 17.89% EER reduction and 1.98% MRR increase, demonstrating the effectiveness of incorporating phoneme information in the correction model",
    "checked": true,
    "id": "785ddc45a06926f30e08f986615569b949fb121e",
    "semantic_title": "asr error correction with augmented transformer for entity retrieval",
    "citation_count": 27
  },
  "https://www.isca-speech.org/archive/interspeech_2020/jia20_interspeech.html": {
    "title": "Large-Scale Transfer Learning for Low-Resource Spoken Language Understanding",
    "volume": "main",
    "abstract": "End-to-end Spoken Language Understanding (SLU) models are made increasingly large and complex to achieve the state-of-the-art accuracy. However, the increased complexity of a model can also introduce high risk of over-fitting, which is a major challenge in SLU tasks due to the limitation of available data. In this paper, we propose an attention-based SLU model together with three encoder enhancement strategies to overcome data sparsity challenge. The first strategy focuses on the transfer-learning approach to improve feature extraction capability of the encoder. It is implemented by pre-training the encoder component with a quantity of Automatic Speech Recognition annotated data relying on the standard Transformer architecture and then fine-tuning the SLU model with a small amount of target labelled data. The second strategy adopts multi-task learning strategy, the SLU model integrates the speech recognition model by sharing the same underlying encoder, such that improving robustness and generalization ability. The third strategy, learning from Component Fusion (CF) idea, involves a Bidirectional Encoder Representation from Transformer (BERT) model and aims to boost the capability of the decoder with an auxiliary network. It hence reduces the risk of over-fitting and augments the ability of the underlying encoder, indirectly. Experiments on the FluentAI dataset show that cross-language transfer learning and multi-task strategies have been improved by up to 4.52% and 3.89% respectively, compared to the baseline",
    "checked": true,
    "id": "f58d4ab9770e54a8e283da1ecbdc0d1e7f4eff4d",
    "semantic_title": "large-scale transfer learning for low-resource spoken language understanding",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gaspers20_interspeech.html": {
    "title": "Data Balancing for Boosting Performance of Low-Frequency Classes in Spoken Language Understanding",
    "volume": "main",
    "abstract": "Despite the fact that data imbalance is becoming more and more common in real-world Spoken Language Understanding (SLU) applications, it has not been studied extensively in the literature. To the best of our knowledge, this paper presents the first systematic study on handling data imbalance for SLU. In particular, we discuss the application of existing data balancing techniques for SLU and propose a multi-task SLU model for intent classification and slot filling. Aiming to avoid over-fitting, in our model methods for data balancing are leveraged indirectly via an auxiliary task which makes use of a class-balanced batch generator and (possibly) synthetic data. Our results on a real-world dataset indicate that i) our proposed model can boost performance on low frequency intents significantly while avoiding a potential performance decrease on the head intents, ii) synthetic data are beneficial for bootstrapping new intents when realistic data are not available, but iii) once a certain amount of realistic data becomes available, using synthetic data in the auxiliary task only yields better performance than adding them to the primary task training data, and iv) in a joint training scenario, balancing the intent distribution individually improves not only intent classification but also slot filling performance",
    "checked": true,
    "id": "2139ae669f07a6dbc2fc54ecf689bff6169e923b",
    "semantic_title": "data balancing for boosting performance of low-frequency classes in spoken language understanding",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20q_interspeech.html": {
    "title": "An Interactive Adversarial Reward Learning-Based Spoken Language Understanding System",
    "volume": "main",
    "abstract": "Most of the existing spoken language understanding systems can perform only semantic frame parsing based on a single-round user query. They cannot take users' feedback to update/add/remove slot values through multiround interactions with users. In this paper, we introduce a novel interactive adversarial reward learning-based spoken language understanding system that can leverage the multiround users' feedback to update slot values. We perform two experiments on the benchmark ATIS dataset and demonstrate that the new system can improve parsing performance by at least 2.5% in terms of F1, with only one round of feedback. The improvement becomes even larger when the number of feedback rounds increases. Furthermore, we also compare the new system with state-of-the-art dialogue state tracking systems and demonstrate that the new interactive system can perform better on multiround spoken language understanding tasks in terms of slot- and sentence-level accuracy",
    "checked": true,
    "id": "55c49338547ae84d5e4f3cffd29110959320c64c",
    "semantic_title": "an interactive adversarial reward learning-based spoken language understanding system",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2020/cao20_interspeech.html": {
    "title": "Style Attuned Pre-Training and Parameter Efficient Fine-Tuning for Spoken Language Understanding",
    "volume": "main",
    "abstract": "Neural models have yielded state-of-the-art results in deciphering spoken language understanding (SLU) problems; however, these models require a significant amount of domain-specific labeled examples for training, which is prohibitively expensive. While pre-trained language models like BERT have been shown to capture a massive amount of knowledge by learning from unlabeled corpora and solve SLU using fewer labeled examples for adaption, the encoding of knowledge is implicit and agnostic to downstream tasks. Such encoding results in model inefficiencies in parameter usage: an entirely new model is required for every domain. To address these challenges, we introduce a novel SLU framework, comprising a conversational language modeling (CLM) pre-training task and a light encoder architecture. The CLM pre-training enables networks to capture the representation of the language in conversation style with the presence of ASR errors. The light encoder architecture separates the shared pre-trained networks from the mappings of generally encoded knowledge to specific domains of SLU, allowing for the domain adaptation to be performed solely at the light encoder and thus increasing efficiency. With the framework, we match the performance of state-of-the-art SLU results on Alexa internal datasets and on two public ones (ATIS, SNIPS), adding only 4.4% parameters per task",
    "checked": true,
    "id": "ea0cf15285a37a506e594f2fb2a4e9928ea3a352",
    "semantic_title": "style attuned pre-training and parameter efficient fine-tuning for spoken language understanding",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2020/orihashi20_interspeech.html": {
    "title": "Unsupervised Domain Adaptation for Dialogue Sequence Labeling Based on Hierarchical Adversarial Training",
    "volume": "main",
    "abstract": "This paper presents a novel unsupervised domain adaptation method for dialogue sequence labeling. Dialogue sequence labeling is a supervised learning task that estimates labels for each utterance in the given dialogue document, and is useful for many applications such as topic segmentation and dialogue act estimation. Accurate labeling often requires a large amount of labeled training data, but it is difficult to collect such data every time we need to support a new domain, such as contact centers in a new business field. In order to solve this difficulty, we propose an unsupervised domain adaptation method for dialogue sequence labeling. Our key idea is to construct dialogue sequence labeling using labeled source domain data and unlabeled target domain data so as to remove domain dependencies at utterance-level and dialogue-level contexts. The proposed method adopts hierarchical adversarial training; two domain adversarial networks, an utterance-level context independent network and a dialogue-level context dependent network, are introduced for improving domain invariance in the dialogue sequence labeling. Experiments on Japanese simulated contact center dialogue datasets demonstrate the effectiveness of the proposed method",
    "checked": true,
    "id": "a880f9fbfc8d81adcfc7f3dbc2fea027910e5517",
    "semantic_title": "unsupervised domain adaptation for dialogue sequence labeling based on hierarchical adversarial training",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2020/sar20_interspeech.html": {
    "title": "Deep F-Measure Maximization for End-to-End Speech Understanding",
    "volume": "main",
    "abstract": "Spoken language understanding (SLU) datasets, like many other machine learning datasets, usually suffer from the label imbalance problem. Label imbalance usually causes the learned model to replicate similar biases at the output which raises the issue of unfairness to the minority classes in the dataset. In this work, we approach the fairness problem by maximizing the F-measure instead of accuracy in neural network model training. We propose a differentiable approximation to the F-measure and train the network with this objective using standard back-propagation. We perform experiments on two standard fairness datasets, Adult, and Communities and Crime, and also on speech-to-intent detection on the ATIS dataset and speech-to-image concept classification on the Speech-COCO dataset. In all four of these tasks, F-measure maximization results in improved micro-F1 scores, with absolute improvements of up to 8% absolute, as compared to models trained with the cross-entropy loss function. In the two multi-class SLU tasks, the proposed approach significantly improves class coverage, i.e., the number of classes with positive recall",
    "checked": true,
    "id": "d211e37a507800d08a2dde77510d62da9b63319d",
    "semantic_title": "deep f-measure maximization for end-to-end speech understanding",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/whang20_interspeech.html": {
    "title": "An Effective Domain Adaptive Post-Training Method for BERT in Response Selection",
    "volume": "main",
    "abstract": "We focus on multi-turn response selection in a retrieval-based dialog system. In this paper, we utilize the powerful pre-trained language model Bi-directional Encoder Representations from Transformer (BERT) for a multi-turn dialog system and propose a highly effective post-training method on domain-specific corpus. Although BERT is easily adopted to various NLP tasks and outperforms previous baselines of each task, it still has limitations if a task corpus is too focused on a certain domain. Post-training on domain-specific corpus (e.g., Ubuntu Corpus) helps the model to train contextualized representations and words that do not appear in general corpus (e.g., English Wikipedia). Experimental results show that our approach achieves new state-of-the-art on two response selection benchmarks (i.e., Ubuntu Corpus V1, Advising Corpus) performance improvement by 5.9% and 6% on R @1",
    "checked": true,
    "id": "5c456e1cbc2756f790023e26632aa44427385b9d",
    "semantic_title": "an effective domain adaptive post-training method for bert in response selection",
    "citation_count": 66
  },
  "https://www.isca-speech.org/archive/interspeech_2020/caubriere20_interspeech.html": {
    "title": "Confidence Measure for Speech-to-Concept End-to-End Spoken Language Understanding",
    "volume": "main",
    "abstract": "Recent studies have led to the introduction of Speech-to-Concept End-to-End (E2E) neural architectures for Spoken Language Understanding (SLU) that reach state of the art performance. In this work, we propose a way to compute confidence measures on semantic concepts recognized by a Speech-to-Text E2E SLU system. We investigate the use of the hidden representations of our CTC-based SLU system to train an external simple classifier. We experiment two kinds of external simple classifiers to analyze subsequences of hidden representations involved in recognized semantic concepts. The first external classifier is based on a MLP while the second one is based on a bLSTM neural network. We compare them to a baseline confidence measure computed directly from the softmax outputs of the E2E system. On the French challenging MEDIA corpus, when the confidence measure is used to reject, experiments show that using an external BLSTM significantly outperforms the other approaches in terms of precision/recall. To evaluate the additional information provided by this confidence measure, we compute the value of Normalised Cross-Entropy (NCE). Reaching a value equal to 0.288, we show that our best proposed confidence measure brings relevant information about the reliability of a recognized concept",
    "checked": true,
    "id": "43914df89492bc7c7e52e09f2d5f5f3a9ba3d58c",
    "semantic_title": "confidence measure for speech-to-concept end-to-end spoken language understanding",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mcguire20_interspeech.html": {
    "title": "Attention to Indexical Information Improves Voice Recall",
    "volume": "main",
    "abstract": "In an exposure phase, two groups of listeners were exposed to a set of 10 voices. These groups differed in terms of the task assigned during exposure: one group was asked to make a decision about the regional affiliation of the voices (Indexical Condition), while the other group orthographically transcribed the words presented (Lexical Condition). Both groups were given an identical test phase where they were presented with 20 voices (10 old, 10 new) and asked to make old/new decisions on the voices. While both groups of listeners performed at above chance accuracy levels in recognizing voices at test as old/new, listeners in the Indexical Condition performed more accurately. These results suggest that the nature of attention during exposure has consequences for subsequent performance, suggesting encoding differences as a result of task demands",
    "checked": true,
    "id": "8be889607e4d10d096675e05fbe722d00cf4b402",
    "semantic_title": "attention to indexical information improves voice recall",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ngoc20_interspeech.html": {
    "title": "Categorization of Whistled Consonants by French Speakers",
    "volume": "main",
    "abstract": "Whistled speech is a form of modified speech where some frequencies of vowels and consonants are augmented and transposed to whistling, modifying the timbre and the construction of each phoneme. These transformations cause only some elements of the signal to be intelligible for naive listeners, which, according to previous studies, includes vowel recognition. Here, we analyze naive listeners' capacities for whistled consonant categorization for four consonants: /p/, /k/, /t/ and /s/ by presenting the findings of two behavioral experiments. Though both experiments measure whistled consonant categorization, we used modified frequencies — lowered with a phase vocoder — of the whistled stimuli in the second experiment to better identify the relative nature of pitch cues employed in this process. Results show that participants obtained approximately 50% of correct responses (when chance is at 25%). These findings show specific consonant preferences for \"s\" and \"t\" over \"k\" and \"p\", specifically when stimuli is unmodified. Previous research on whistled consonants systems has often opposed \"s\" and \"t\" to \"k\" and \"p\", due to their strong pitch modulations. The preference for these two consonants underlines the importance of these cues in phoneme processing",
    "checked": true,
    "id": "3b439b2ffd774f972b6fddfd4f5593b942eab91f",
    "semantic_title": "categorization of whistled consonants by french speakers",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ngoc20b_interspeech.html": {
    "title": "Whistled Vowel Identification by French Listeners",
    "volume": "main",
    "abstract": "In this paper, we analyzed whistled vowel categorization by native French listeners. Whistled speech, a natural, yet modified register of speech, is used here as a tool to investigate perceptual processes in languages. We focused on four whistled vowels: /i, e, a, o/. After a detailed description of the vowels, we built and ran a behavioral experiment in which we asked native French speakers to categorize whistled vowel stimuli in which we introduced intra- and inter- production variations. In addition, half of the participants performed the experiment in person (at the laboratory) while the other half participated online, allowing us to evaluate the impact of the testing set up. Our results confirm that the categorization rate of whistled vowels is above chance. They reveal significant differences in performance for different vowels and suggest an influence of certain acoustic parameters from the whistlers' vowel range on categorization. Moreover, no effect or interaction was found for testing location and circumstances in our data set. This study confirms that whistled stimuli are a useful tool for studying how listeners process modified speech and which parameters impact sound categorization",
    "checked": true,
    "id": "1a04f5c8d0dcfa46e844690c225789e0aabdd84d",
    "semantic_title": "whistled vowel identification by french listeners",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2020/cordero20_interspeech.html": {
    "title": "F0 Slope and Mean: Cues to Speech Segmentation in French",
    "volume": "main",
    "abstract": "This paper evaluates the use of intonational cues during word segmentation in French. Specifically, we aim to examine how the characteristics of the fundamental frequency (F0) that can be observed at the beginning of words influence their processing. Native speakers of French were presented with phonemically identical sequences, such as /selami/ ( c'est l'amie/la mie \"it's the friend/the crumb\"). To test which properties of the F0 affect the perceived segmentation, we manipulated the F0 slope and/or the mean value of the first vowel /a/ in consonant-initial items (e.g., la mie). To assess differences in off-line vs online processing, we used a two-alternative, forced-choice task in Experiment 1 and a lexical decision task in Experiment 2. A previous study showed that vowel-initial segmentation was enhanced when the F0 mean value increased. However, the present study shows that modifying the F0 slope while keeping the F0 mean value constant also influences speech segmentation in both off-line and online tasks. This suggests that listeners use the F0 slope as a cue at the beginning of content words",
    "checked": true,
    "id": "43397413a576e2387196d74f26317188adbe56bb",
    "semantic_title": "f0 slope and mean: cues to speech segmentation in french",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/michelas20_interspeech.html": {
    "title": "Does French Listeners' Ability to Use Accentual Information at the Word Level Depend on the Ear of Presentation?",
    "volume": "main",
    "abstract": "In two long-term repetition priming experiments, we investigated how accentual information is processed and represented in the French listeners' mind. Repeated prime and target words either matched (/bãˈdo/ - / bãˈdo/ ‘headband') or mismatched in their accentual patterns (/bãdo/ - /bãˈdo/). In experiment 1, the target words were presented in the left ear only, and attenuation in the repetition priming effect was observed when the primes and the targets mismatched in their accentual pattern. The differential priming effect between match and mismatch primes was no longer observed in Experiment 2 when the targets were presented in the right ear only. Together, these results showed that accentual variation at the word level in French is treated as related-talker variation, and only influences word recognition under specific circumstances, in particular, when we push word processing in the right hemisphere",
    "checked": true,
    "id": "11f4be0e067b8dbd19ba879dbfb1229e0a791c98",
    "semantic_title": "does french listeners' ability to use accentual information at the word level depend on the ear of presentation?",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/liu20h_interspeech.html": {
    "title": "A Perceptual Study of the Five Level Tones in Hmu (Xinzhai Variety)",
    "volume": "main",
    "abstract": "Previous studies have shown that the perception is categorical when tones have different contours, whereas continuous when tones have the same contour. In this study, a perceptual experiment of the five level tones in Hmu (Xinzhai variety) was conducted to further examine this conclusion. Results show that in the identification test, continua between different level tones have different boundary width, which has a negative correlation with the pitch interval of two level tones. In the discrimination test, though there is no peak in discrimination curve, the discrimination accuracy reveals an important phenomenon that the accuracy is approximately 50% between two neighboring level tones, but higher when the level tones have a larger pitch interval. Besides, the boundary width is highly correlated with the discrimination accuracy (e.g., the narrower the boundary width, the higher the discrimination accuracy). These results reveal the basic characteristic of continuous perception, especially for level tones. Finally, the results also demonstrate that the category in categorical perception is not equal to phonological category",
    "checked": true,
    "id": "38d8d54b62fa8a6bdb3470aadd7f05c7c9dcb1d7",
    "semantic_title": "a perceptual study of the five level tones in hmu (xinzhai variety)",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zeng20_interspeech.html": {
    "title": "Mandarin and English Adults' Cue-Weighting of Lexical Stress",
    "volume": "main",
    "abstract": "Listeners segment speech based on the rhythm of their native language(s) (e.g., stress- vs. syllable-timed, tone vs. non-tone) [1,2]. In English, the perception of speech rhythm relies on analyzing auditory cues pertinent to lexical stress, including pitch, duration and intensity [3]. Focusing on cross-linguistic impact on English lexical stress cue processing, the present study aims to explore English stress cue-weighting by Mandarin-speaking adults (with English adults as control), using an MMN multi-feature paradigm Preliminary ERP data revealed cross-linguistic perceptual differences to pitch and duration cues, but not to intensity cues in the bisyllabic non-word /dede/. Specifically, while English adults were similarly sensitive to pitch change at the initial and final syllable of the non-word, they were more sensitive to the duration change at the initial syllable. Comparatively, Mandarin adults were similarly sensitive to duration change at each position, but more sensitive to pitch at the final syllable. Lastly, both the Mandarin group and the English group were more sensitive to the intensity sound change at the second syllable. Possible explanations for these findings are discussed",
    "checked": true,
    "id": "482222a264cd33aaa03a2bbf98a32c2c108c5c02",
    "semantic_title": "mandarin and english adults' cue-weighting of lexical stress",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2020/feng20c_interspeech.html": {
    "title": "Age-Related Differences of Tone Perception in Mandarin-Speaking Seniors",
    "volume": "main",
    "abstract": "This study examined age-related differences in categorical perception of Mandarin lexical tones through comparing identification and discrimination performance among young adults, seniors aged 60–65 years, and older seniors aged 75–80 years. Results showed a significantly wider boundary and smaller peakedness in older seniors. There was also a positive correlation between the hearing level at 125 Hz and boundary width, and a negative correlation between hearing level (125 Hz) and peakedness in older seniors, indicating that the decline of tone perception in this population might be associated with degradation of hearing sensitivity. However, there was no significant difference between young adults and seniors aged 60–65 years, which might reveal that younger seniors could maintain normal ability to perceive tones categorically",
    "checked": true,
    "id": "ae3a745cee8636ff53a886e360ae0bce91070fb6",
    "semantic_title": "age-related differences of tone perception in mandarin-speaking seniors",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zellou20b_interspeech.html": {
    "title": "Social and Functional Pressures in Vocal Alignment: Differences for Human and Voice-AI Interlocutors",
    "volume": "main",
    "abstract": "Increasingly, people are having conversational interactions with voice-AI systems, such as Amazon's Alexa. Do the same social and functional pressures that mediate alignment toward human interlocutors also predict align patterns toward voice-AI? We designed an interactive dialogue task to investigate this question. Each trial consisted of scripted, interactive turns between a participant and a model talker (pre-recorded from either a natural production or voice-AI): First, participants produced target words in a carrier phrase. Then, a model talker responded with an utterance containing the target word. The interlocutor responses varied by 1) communicative affect (social) and 2) correctness (functional). Finally, participants repeated the carrier phrase. Degree of phonetic alignment was assessed acoustically between the target word in the model's response and participants' response. Results indicate that social and functional factors distinctly mediate alignment toward AI and humans. Findings are discussed with reference to theories of alignment and human-computer interaction",
    "checked": true,
    "id": "b9cfabec32cc3b7eb3641fcd1e9aaf36080d34a8",
    "semantic_title": "social and functional pressures in vocal alignment: differences for human and voice-ai interlocutors",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kavaki20_interspeech.html": {
    "title": "Identifying Important Time-Frequency Locations in Continuous Speech Utterances",
    "volume": "main",
    "abstract": "Human listeners use specific cues to recognize speech and recent experiments have shown that certain time-frequency regions of individual utterances are more important to their correct identification than others. A model that could identify such cues or regions from clean speech would facilitate speech recognition and speech enhancement by focusing on those important regions. Thus, in this paper we present a model that can predict the regions of individual utterances that are important to an automatic speech recognition (ASR) \"listener\" by learning to add as much noise as possible to these utterances while still permitting the ASR to correctly identify them. This work utilizes a continuous speech recognizer to recognize multi-word utterances and builds upon our previous work that performed the same process for an isolated word recognizer. Our experimental results indicate that our model can apply noise to obscure 90.5% of the spectrogram while leaving recognition performance nearly unchanged",
    "checked": true,
    "id": "fb137bc621f66bf6a8878797df299f638d31aae7",
    "semantic_title": "identifying important time-frequency locations in continuous speech utterances",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/loweimi20b_interspeech.html": {
    "title": "Raw Sign and Magnitude Spectra for Multi-Head Acoustic Modelling",
    "volume": "main",
    "abstract": "In this paper we investigate the usefulness of the sign spectrum and its combination with the raw magnitude spectrum in acoustic modelling for automatic speech recognition (ASR). The sign spectrum is a sequence of ±1s, capturing one bit of the phase spectrum. It encodes information overlooked by the magnitude spectrum enabling unique signal characterisation and reconstruction. In particular, we demonstrate it carries information related to the temporal structure of the signal as well as the speech's source component. Furthermore, we investigate the usefulness of combining it with the raw magnitude spectrum via multi-head CNNs at different fusion levels for ASR. While information-wise these two streams of information are together equivalent to the raw waveform signal the overall performance is noticeably higher than raw waveform and classic features such as MFCC and filterbank. This has been observed and verified in TIMIT, NTIMT, Aurora-4 and WSJ tasks and up to 14.5% relative WER reduction has been achieved",
    "checked": true,
    "id": "1b56aef95d725db330b6338f2ab8b894f53c6ffb",
    "semantic_title": "raw sign and magnitude spectra for multi-head acoustic modelling",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2020/agrawal20_interspeech.html": {
    "title": "Robust Raw Waveform Speech Recognition Using Relevance Weighted Representations",
    "volume": "main",
    "abstract": "Speech recognition in noisy and channel distorted scenarios is often challenging as the current acoustic modeling schemes are not adaptive to the changes in the signal distribution in the presence of noise. In this work, we develop a novel acoustic modeling framework for noise robust speech recognition based on relevance weighting mechanism. The relevance weighting is achieved using a sub-network approach that performs feature selection. A relevance sub-network is applied on the output of first layer of a convolutional network model operating on raw speech signals while a second relevance sub-network is applied on the second convolutional layer output. The relevance weights for the first layer correspond to an acoustic filterbank selection while the relevance weights in the second layer perform modulation filter selection. The model is trained for a speech recognition task on noisy and reverberant speech. The speech recognition experiments on multiple datasets (Aurora-4, CHiME-3, VOiCES) reveal that the incorporation of relevance weighting in the neural network architecture improves the speech recognition word error rates significantly (average relative improvements of 10% over the baseline systems)",
    "checked": true,
    "id": "63a468c794678dca0637211b806084480441ee41",
    "semantic_title": "robust raw waveform speech recognition using relevance weighted representations",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2020/oglic20_interspeech.html": {
    "title": "A Deep 2D Convolutional Network for Waveform-Based Speech Recognition",
    "volume": "main",
    "abstract": "Due to limited computational resources, acoustic models of early automatic speech recognition ( asr) systems were built in low-dimensional feature spaces that incur considerable information loss at the outset of the process. Several comparative studies of automatic and human speech recognition suggest that this information loss can adversely affect the robustness of asr systems. To mitigate that and allow for learning of robust models, we propose a deep 2 d convolutional network in the waveform domain. The first layer of the network decomposes waveforms into frequency sub-bands, thereby representing them in a structured high-dimensional space. This is achieved by means of a parametric convolutional block defined via cosine modulations of compactly supported windows. The next layer embeds the waveform in an even higher-dimensional space of high-resolution spectro-temporal patterns, implemented via a 2 d convolutional block. This is followed by a gradual compression phase that selects most relevant spectro-temporal patterns using wide-pass 2 d filtering. Our results show that the approach significantly outperforms alternative waveform-based models on both noisy and spontaneous conversational speech (24% and 11% relative error reduction, respectively). Moreover, this study provides empirical evidence that learning directly from the waveform domain could be more effective than learning using hand-crafted features",
    "checked": true,
    "id": "1800d9180ab03ad2a2390377fc2c8949f71bee53",
    "semantic_title": "a deep 2d convolutional network for waveform-based speech recognition",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kurzinger20_interspeech.html": {
    "title": "Lightweight End-to-End Speech Recognition from Raw Audio Data Using Sinc-Convolutions",
    "volume": "main",
    "abstract": "Many end-to-end Automatic Speech Recognition (ASR) systems still rely on pre-processed frequency-domain features that are handcrafted to emulate the human hearing. Our work is motivated by recent advances in integrated learnable feature extraction. For this, we propose Lightweight Sinc-Convolutions (LSC) that integrate Sinc-convolutions with depthwise convolutions as a low-parameter machine-learnable feature extraction for end-to-end ASR systems We integrated LSC into the hybrid CTC/attention architecture for evaluation. The resulting end-to-end model shows smooth convergence behaviour that is further improved by applying SpecAugment in the time domain. We also discuss filter-level improvements, such as using log-compression as activation function. Our model achieves a word error rate of 10.7% on the TEDlium v2 test dataset, surpassing the corresponding architecture with log-mel filterbank features by an absolute 1.9%, but only has 21% of its model size",
    "checked": true,
    "id": "b560864bef00a2b0e0ad30dfe39ad7bf96f8294e",
    "semantic_title": "lightweight end-to-end speech recognition from raw audio data using sinc-convolutions",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ghahramani20_interspeech.html": {
    "title": "An Alternative to MFCCs for ASR",
    "volume": "main",
    "abstract": "The Mel scale is the most commonly used frequency warping function to extract features for automatic speech recognition (ASR) and is known to be quite effective. However, it is not specifically designed for ASR acoustic models based on deep neural networks (DNN). In this study, we introduce a frequency warping function which is a modified version of Mel scale. This warping function is parameterized using 2 parameters and we use it to propose a new set of features called modified Mel-frequency cepstral coefficients (MFCC), which use cosine-shaped filters. The bandwidths are computed using a new function. By evaluating the proposed features on a variety of ASR data sets, we see consistent improvements over regular MFCCs and (log) Mel filter bank energies",
    "checked": true,
    "id": "ce6fca70a2e54733a501648f9f7f3b346a57096a",
    "semantic_title": "an alternative to mfccs for asr",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/dutta20_interspeech.html": {
    "title": "Phase Based Spectro-Temporal Features for Building a Robust ASR System",
    "volume": "main",
    "abstract": "Spectro-temporal feature extraction has shown its robustness in the field of speech recognition. However, these features are derived from magnitude spectrum of the complex Fourier Transform (FT). In this work, we investigate to see if phase information can substitute magnitude based spectro-temporal features. We compared with different state of art phase spectrum and evaluated its performance. The experiments are carried out in different noisy environments. We found Modified Group Delay (MODGD) spectrum to closely resemble the structure of power spectrum. A relative performance difference of 0.03% on average is observed for the MODGD spectro-temporal features compared to the magnitude based features. The analysis showed that phase can indeed carry equivalent or complementary information to magnitude based spectro-temporal features",
    "checked": true,
    "id": "3bd6fbbc7a241fafd7854f3cc46cdce11e9cb025",
    "semantic_title": "phase based spectro-temporal features for building a robust asr system",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2020/joy20_interspeech.html": {
    "title": "Deep Scattering Power Spectrum Features for Robust Speech Recognition",
    "volume": "main",
    "abstract": "Deep scattering spectrum consists of a cascade of wavelet transforms and modulus non-linearity. It generates features of different orders, with the first order coefficients approximately equal to the Mel-frequency cepstrum, and higher order coefficients recovering information lost at lower levels. We investigate the effect of including the information recovered by higher order coefficients on the robustness of speech recognition. To that end, we also propose a modification to the original scattering transform tailored for noisy speech. In particular, instead of the modulus non-linearity we opt to work with power coefficients and, therefore, use the squared modulus non-linearity. We quantify the robustness of scattering features using the word error rates of acoustic models trained on clean speech and evaluated using sets of utterances corrupted with different noise types. Our empirical results show that the second order scattering power spectrum coefficients capture invariants relevant for noise robustness and that this additional information improves generalization to unseen noise conditions (almost 20% relative error reduction on aurora 4). This finding can have important consequences on speech recognition systems that typically discard the second order information and keep only the first order features (known for emulating mfcc and fbank values) when representing speech",
    "checked": true,
    "id": "ac4a8afdd8fcea6e3c63941f05d824794eb73665",
    "semantic_title": "deep scattering power spectrum features for robust speech recognition",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2020/parcollet20_interspeech.html": {
    "title": "FusionRNN: Shared Neural Parameters for Multi-Channel Distant Speech Recognition",
    "volume": "main",
    "abstract": "Distant speech recognition remains a challenging application for modern deep learning based Automatic Speech Recognition (ASR) systems, due to complex recording conditions involving noise and reverberation. Multiple microphones are commonly combined with well-known speech processing techniques to enhance the original signals and thus enhance the speech recognizer performance. These multi-channel follow similar input distributions with respect to the global speech information but also contain an important part of noise. Consequently, the input representation robustness is key to obtaining reasonable recognition rates. In this work, we propose a Fusion Layer (FL) based on shared neural parameters. We use it to produce an expressive embedding of multiple microphone signals, that can easily be combined with any existing ASR pipeline. The proposed model called FusionRNN showed promising results on a multi-channel distant speech recognition task, and consistently outperformed baseline models while maintaining an equal training time",
    "checked": true,
    "id": "32b089cef31b45305b2ee81e887075b91d5e814a",
    "semantic_title": "fusionrnn: shared neural parameters for multi-channel distant speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kumar20b_interspeech.html": {
    "title": "Bandpass Noise Generation and Augmentation for Unified ASR",
    "volume": "main",
    "abstract": "Data Simulation is a crucial technique for robust automatic speech recognition (ASR) systems. We develop this work in the scope of data augmentation and improve robustness by generating new bandpass noise resources from an existing noise corpus. We design numerous bandpass filters with varying center frequencies and filter bandwidths, and obtain corresponding bandpass noise samples. We augment our baseline data simulation with bandpass noises to ingest additional robustness and generalization to generic and unknown acoustic scenarios. This work targets ASR robustness to individual subband noises, and improves robustness to unseen real-world noise that can be approximated as a factorial combination of subband noises. We demonstrate our work for a large scale unified ASR task. We obtained 7% word error rate relative reduction (WERR) across unseen acoustic conditions and 11% WERR for kids speech. We also demonstrate generalization to new ASR applications",
    "checked": true,
    "id": "fa0fa2f60e851548f8bd688ca41d8678c2a3e833",
    "semantic_title": "bandpass noise generation and augmentation for unified asr",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2020/purushothaman20_interspeech.html": {
    "title": "Deep Learning Based Dereverberation of Temporal Envelopes for Robust Speech Recognition",
    "volume": "main",
    "abstract": "Automatic speech recognition in reverberant conditions is a challenging task as the long-term envelopes of the reverberant speech are temporally smeared. In this paper, we propose a neural model for enhancement of sub-band temporal envelopes for dereverberation of speech. The temporal envelopes are derived using the autoregressive modeling framework of frequency domain linear prediction (FDLP). The neural enhancement model proposed in this paper performs an envelop gain based enhancement of temporal envelopes and it consists of a series of convolutional and recurrent neural network layers. The enhanced sub-band envelopes are used to generate features for automatic speech recognition (ASR). The ASR experiments are performed on the REVERB challenge dataset as well as the CHiME-3 dataset. In these experiments, the proposed neural enhancement approach provides significant improvements over a baseline ASR system with beamformed audio (average relative improvements of 21% on the development set and about 11% on the evaluation set in word error rates for REVERB challenge dataset)",
    "checked": true,
    "id": "da46fd92498fcc1d48986d41946f59fff6947901",
    "semantic_title": "deep learning based dereverberation of temporal envelopes for robust speech recognition",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tomashenko20_interspeech.html": {
    "title": "Introducing the VoicePrivacy Initiative",
    "volume": "main",
    "abstract": "The VoicePrivacy initiative aims to promote the development of privacy preservation tools for speech technology by gathering a new community to define the tasks of interest and the evaluation methodology, and benchmarking solutions through a series of challenges. In this paper, we formulate the voice anonymization task selected for the VoicePrivacy 2020 Challenge and describe the datasets used for system development and evaluation. We also present the attack models and the associated objective and subjective evaluation metrics. We introduce two anonymization baselines and report objective evaluation results",
    "checked": true,
    "id": "44754474eb0b37c697c93b415df479793967ad37",
    "semantic_title": "introducing the voiceprivacy initiative",
    "citation_count": 87
  },
  "https://www.isca-speech.org/archive/interspeech_2020/nautsch20_interspeech.html": {
    "title": "The Privacy ZEBRA: Zero Evidence Biometric Recognition Assessment",
    "volume": "main",
    "abstract": "Mounting privacy legislation calls for the preservation of privacy in speech technology, though solutions are gravely lacking. While evaluation campaigns are long-proven tools to drive progress, the need to consider a privacy adversary implies that traditional approaches to evaluation must be adapted to the assessment of privacy and privacy preservation solutions. This paper presents the first step in this direction: metrics We introduce the zero evidence biometric recognition assessment (ZEBRA) framework and propose two new privacy metrics. They measure the average level of privacy preservation afforded by a given safeguard for a population and the worst-case privacy disclosure for an individual. The paper demonstrates their application to privacy preservation assessment within the scope of the VoicePrivacy challenge. While the ZEBRA framework is designed with speech applications in mind, it is a candidate for incorporation into biometric information protection standards and is readily extendable to the study of privacy in applications even beyond speech and biometrics",
    "checked": true,
    "id": "014be69a20bac8a6123a449764ad3973e3e7300a",
    "semantic_title": "the privacy zebra: zero evidence biometric recognition assessment",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mawalim20_interspeech.html": {
    "title": "X-Vector Singular Value Modification and Statistical-Based Decomposition with Ensemble Regression Modeling for Speaker Anonymization System",
    "volume": "main",
    "abstract": "Anonymizing speaker individuality is crucial for ensuring voice privacy protection. In this paper, we propose a speaker individuality anonymization system that uses singular value modification and statistical-based decomposition on an x-vector with ensemble regression modeling. An anonymization system requires speaker-to-speaker correspondence (each speaker corresponds to a pseudo-speaker), which may be possible by modifying significant x-vector elements. The significant elements were determined by singular value decomposition and variant analysis. Subsequently, the anonymization process was performed by an ensemble regression model trained using x-vector pools with clustering-based pseudo-targets. The results demonstrated that our proposed anonymization system effectively improves objective verifiability, especially in anonymized trials and anonymized enrollments setting, by preserving similar intelligibility scores with the baseline system introduced in the VoicePrivacy 2020 Challenge",
    "checked": true,
    "id": "79cdf9fa1a9311545f661f5a5c2433076d953ad3",
    "semantic_title": "x-vector singular value modification and statistical-based decomposition with ensemble regression modeling for speaker anonymization system",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2020/maouche20_interspeech.html": {
    "title": "A Comparative Study of Speech Anonymization Metrics",
    "volume": "main",
    "abstract": "Speech anonymization techniques have recently been proposed for preserving speakers' privacy. They aim at concealing speakers' identities while preserving the spoken content. In this study, we compare three metrics proposed in the literature to assess the level of privacy achieved. We exhibit through simulation the differences and blindspots of some metrics. In addition, we conduct experiments on real data and state-of-the-art anonymization techniques to study how they behave in a practical scenario. We show that the application-independent log-likelihood-ratio cost function C provides a more robust evaluation of privacy than the equal error rate (EER), and that detection-based metrics provide different information from linkability metrics. Interestingly, the results on real data indicate that current anonymization design choices do not induce a regime where the differences between those metrics become apparent",
    "checked": true,
    "id": "d2e0d2633665bc88dd3c7dfed3a476a0882e6a3d",
    "semantic_title": "a comparative study of speech anonymization metrics",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2020/srivastava20_interspeech.html": {
    "title": "Design Choices for X-Vector Based Speaker Anonymization",
    "volume": "main",
    "abstract": "The recently proposed x-vector based anonymization scheme converts any input voice into that of a random pseudo-speaker. In this paper, we present a flexible pseudo-speaker selection technique as a baseline for the first VoicePrivacy Challenge. We explore several design choices for the distance metric between speakers, the region of x-vector space where the pseudo-speaker is picked, and gender selection. To assess the strength of anonymization achieved, we consider attackers using an x-vector based speaker verification system who may use original or anonymized speech for enrollment, depending on their knowledge of the anonymization scheme. The Equal Error Rate (EER) achieved by the attackers and the decoding Word Error Rate (WER) over anonymized data are reported as the measures of privacy and utility. Experiments are performed using datasets derived from LibriSpeech to find the optimal combination of design choices in terms of privacy and utility",
    "checked": true,
    "id": "3a3fe1d75ee3add2a148407177bd91c19f1ef7f2",
    "semantic_title": "design choices for x-vector based speaker anonymization",
    "citation_count": 43
  },
  "https://www.isca-speech.org/archive/interspeech_2020/noe20_interspeech.html": {
    "title": "Speech Pseudonymisation Assessment Using Voice Similarity Matrices",
    "volume": "main",
    "abstract": "The proliferation of speech technologies and rising privacy legislation calls for the development of privacy preservation solutions for speech applications. These are essential since speech signals convey a wealth of rich, personal and potentially sensitive information. Anonymisation, the focus of the recent VoicePrivacy initiative, is one strategy to protect speaker identity information. Pseudonymisation solutions aim not only to mask the speaker identity and preserve the linguistic content, quality and naturalness, as is the goal of anonymisation, but also to preserve voice distinctiveness. Existing metrics for the assessment of anonymisation are ill-suited and those for the assessment of pseudonymisation are completely lacking. Based upon voice similarity matrices, this paper proposes the first intuitive visualisation of pseudonymisation performance for speech signals and two novel metrics for objective assessment. They reflect the two, key pseudonymisation requirements of de-identification and voice distinctiveness",
    "checked": true,
    "id": "98d2ff27c9d50a1d74eecb6bcefe144c7de9b30b",
    "semantic_title": "speech pseudonymisation assessment using voice similarity matrices",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2020/park20c_interspeech.html": {
    "title": "g2pM: A Neural Grapheme-to-Phoneme Conversion Package for Mandarin Chinese Based on a New Open Benchmark Dataset",
    "volume": "main",
    "abstract": "Conversion of Chinese graphemes to phonemes (G2P) is an essential component in Mandarin Chinese Text-To-Speech (TTS) systems. One of the biggest challenges in Chinese G2P conversion is how to disambiguate the pronunciation of polyphones — characters having multiple pronunciations. Although many academic efforts have been made to address it, there has been no open dataset that can serve as a standard benchmark for a fair comparison to date. In addition, most of the reported systems are hard to employ for researchers or practitioners who want to convert Chinese text into pinyin at their convenience. Motivated by these, in this work, we introduce a new benchmark dataset that consists of 99,000+ sentences for Chinese polyphone disambiguation. We train a simple Bi-LSTM model on it and find that it outperforms other pre-existing G2P systems and slightly underperforms pre-trained Chinese BERT. Finally, we package our project and share it on PyPi",
    "checked": false,
    "id": "13ce475af6c34d19e1feb14ed3419db9e46d913b",
    "semantic_title": "g2pm: a neural grapheme-to-phoneme conversion package for mandarinchinese based on a new open benchmark dataset",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20n_interspeech.html": {
    "title": "A Mask-Based Model for Mandarin Chinese Polyphone Disambiguation",
    "volume": "main",
    "abstract": "Polyphone disambiguation serves as an essential part of Mandarin text-to-speech (TTS) system. However, conventional system modelling the entire Pinyin set causes the case that prediction belongs to the unrelated polyphonic character instead of the current input one, which has negative impacts on TTS performance. To address this issue, we introduce a mask-based model for polyphone disambiguation. The model takes a mask vector extracted from the context as an extra input. In our model, the mask vector not only acts as a weighting factor in Weighted-softmax to prevent the case of mis-prediction but also eliminates the contribution of non-candidate set to the overall loss. Moreover, to mitigate the uneven distribution of pronunciation, we introduce a new loss called Modified Focal Loss. The experimental result shows the effectiveness of the proposed mask-based model. We also empirically studied the impact of Weighted-softmax and Modified Focal Loss. It was found that Weighted-softmax can effectively prevent the model from predicting outside the candidate set. Besides, Modified Focal Loss can reduce the adverse impacts of the uneven distribution of pronunciation",
    "checked": true,
    "id": "fd6b24d567e49a6b57e1a6a496cd5b18e0e06b9a",
    "semantic_title": "a mask-based model for mandarin chinese polyphone disambiguation",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2020/cohn20_interspeech.html": {
    "title": "Perception of Concatenative vs. Neural Text-To-Speech (TTS): Differences in Intelligibility in Noise and Language Attitudes",
    "volume": "main",
    "abstract": "This study tests speech-in-noise perception and social ratings of speech produced by different text-to-speech (TTS) synthesis methods. We used identical speaker training datasets for a set of 4 voices (using AWS Polly TTS), generated using neural and concatenative TTS. In Experiment 1, listeners identified target words in semantically predictable and unpredictable sentences in concatenative and neural TTS at two noise levels (-3 dB, -6 dB SNR). Correct word identification was lower for neural TTS than for concatenative TTS, in the lower SNR, and for semantically unpredictable sentences. In Experiment 2, listeners rated the voices on 4 social attributes. Neural TTS was rated as more human-like, natural, likeable, and familiar than concatenative TTS. Furthermore, how natural listeners rated the neural TTS voice was positively related to their speech-in-noise accuracy. Together, these findings show that the TTS method influences both intelligibility and social judgments of speech — and that these patterns are linked. Overall, this work contributes to our understanding of the nexus of speech technology and human speech perception",
    "checked": true,
    "id": "39d59de85ffbf404e8b110fe35c77456add9a466",
    "semantic_title": "perception of concatenative vs. neural text-to-speech (tts): differences in intelligibility in noise and language attitudes",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2020/taylor20_interspeech.html": {
    "title": "Enhancing Sequence-to-Sequence Text-to-Speech with Morphology",
    "volume": "main",
    "abstract": "Neural sequence-to-sequence (S2S) modelling encodes a single, unified representation for each input sequence. When used for text-to-speech synthesis (TTS), such representations must embed ambiguities between English spelling and pronunciation. For example, in pothole and there the character sequence th sounds different. This can be problematic when predicting pronunciation directly from letters. We posit pronunciation becomes easier to predict when letters are grouped into sub-word units like morphemes (e.g. a boundary lies between t and h in pothole but not there). Moreover, morphological boundaries can reduce the total number of, and increase the counts of, seen unit subsequences. Accordingly, we test here the effect of augmenting input sequences of letters with morphological boundaries. We find morphological boundaries substantially lower the Word and Phone Error Rates (WER and PER) for a Bi-LSTM performing G2P on one hand, and also increase the naturalness scores of Tacotrons performing TTS in a MUSHRA listening test on the other. The improvements to TTS quality are such that grapheme input augmented with morphological boundaries outperforms phone input without boundaries. Since morphological segmentation may be predicted with high accuracy, we highlight this simple pre-processing step has important potential for S2S modelling in TTS",
    "checked": true,
    "id": "0bee3db88cbb29b34ec3951d98abce004ec27381",
    "semantic_title": "enhancing sequence-to-sequence text-to-speech with morphology",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2020/choi20b_interspeech.html": {
    "title": "Deep MOS Predictor for Synthetic Speech Using Cluster-Based Modeling",
    "volume": "main",
    "abstract": "While deep learning has made impressive progress in speech synthesis and voice conversion, the assessment of the synthesized speech is still carried out by human participants. Several recent papers have proposed deep-learning-based assessment models and shown the potential to automate the speech quality assessment. To improve the previously proposed assessment model, MOSNet, we propose three models using cluster-based modeling methods: using a global quality token (GQT) layer, using an Encoding Layer, and using both of them. We perform experiments using the evaluation results of the Voice Conversion Challenge 2018 to predict the mean opinion score of synthesized speech and similarity score between synthesized speech and reference speech. The results show that the GQT layer helps to predict human assessment better by automatically learning the useful quality tokens for the task and that the Encoding Layer helps to utilize frame-level scores more precisely",
    "checked": true,
    "id": "53f90d5236237a0087e2b268c19800e22831db79",
    "semantic_title": "deep mos predictor for synthetic speech using cluster-based modeling",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mittag20_interspeech.html": {
    "title": "Deep Learning Based Assessment of Synthetic Speech Naturalness",
    "volume": "main",
    "abstract": "In this paper, we present a new objective prediction model for synthetic speech naturalness. It can be used to evaluate Text-To-Speech or Voice Conversion systems and works language independently. The model is trained end-to-end and based on a CNN-LSTM network that previously showed to give good results for speech quality estimation. We trained and tested the model on 16 different datasets, such as from the Blizzard Challenge and the Voice Conversion Challenge. Further, we show that the reliability of deep learning-based naturalness prediction can be improved by transfer learning from speech quality prediction models that are trained on objective POLQA scores. The proposed model is made publicly available and can, for example, be used to evaluate different TTS system configurations",
    "checked": true,
    "id": "d3d32f98fcff8d8b927720386320a9c1e2d97d96",
    "semantic_title": "deep learning based assessment of synthetic speech naturalness",
    "citation_count": 36
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20o_interspeech.html": {
    "title": "Distant Supervision for Polyphone Disambiguation in Mandarin Chinese",
    "volume": "main",
    "abstract": "Grapheme-to-phoneme (G2P) conversion plays an important role in building a Mandarin Chinese text-to-speech (TTS) system, where the polyphone disambiguation is an indispensable task. However, most of the previous polyphone disambiguation models are trained on manually annotated datasets, which are suffering from data scarcity, narrow coverage, and unbalanced data distribution. In this paper, we propose a framework that can predict the pronunciations of Chinese characters, and the core model is trained in a distantly supervised way. Specifically, we utilize the alignment procedure used for acoustic models to produce abundant character-phoneme sequence pairs, which are employed to train a Seq2Seq model with attention mechanism. We also make use of a language model that is trained on phoneme sequences to alleviate the impact of noises in the auto-generated dataset. Experimental results demonstrate that even without additional syntactic features and pre-trained embeddings, our approach achieves competitive prediction results, and especially improves the predictive accuracy for unbalanced polyphonic characters. In addition, compared with the manually annotated training datasets, the auto-generated one is more diversified and makes the results more consistent with the pronunciation habits of most people",
    "checked": true,
    "id": "591878effd4b1da27f2bb20cf85ee51d977bb98e",
    "semantic_title": "distant supervision for polyphone disambiguation in mandarin chinese",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gallegos20_interspeech.html": {
    "title": "An Unsupervised Method to Select a Speaker Subset from Large Multi-Speaker Speech Synthesis Datasets",
    "volume": "main",
    "abstract": "Large multi-speaker datasets for TTS typically contain diverse speakers, recording conditions, styles and quality of data. Although one might generally presume that more data is better, in this paper we show that a model trained on a carefully-chosen subset of speakers from LibriTTS provides significantly better quality synthetic speech than a model trained on a larger set. We propose an unsupervised methodology to find this subset by clustering per-speaker acoustic representations",
    "checked": true,
    "id": "d99bcf2c121cee1b6d48d6794adf358b5814b87d",
    "semantic_title": "an unsupervised method to select a speaker subset from large multi-speaker speech synthesis datasets",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2020/das20_interspeech.html": {
    "title": "Understanding the Effect of Voice Quality and Accent on Talker Similarity",
    "volume": "main",
    "abstract": "This paper presents a methodology to study the role of non-native accents on talker recognition by humans. The methodology combines a state-of-the-art accent-conversion system to resynthesize the voice of a speaker with a different accent of her/his own, and a protocol for perceptual listening tests to measure the relative contribution of accent and voice quality on speaker similarity. Using a corpus of non-native and native speakers, we generated accent conversions in two different directions: non-native speakers with native accents, and native speakers with non-native accents. Then, we asked listeners to rate the similarity between 50 pairs of real or synthesized speakers. Using a linear mixed effects model, we find that (for our corpus) the effect of voice quality is five times as large as that of non-native accent, and that the effect goes away when speakers share the same (native) accent. We discuss the potential significance of this work in earwitness identification and sociophonetics",
    "checked": true,
    "id": "c90e2a5c4e2f44bf7814e4b0ff1a3abc5aea0011",
    "semantic_title": "understanding the effect of voice quality and accent on talker similarity",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhou20c_interspeech.html": {
    "title": "Robust Beam Search for Encoder-Decoder Attention Based Speech Recognition Without Length Bias",
    "volume": "main",
    "abstract": "As one popular modeling approach for end-to-end speech recognition, attention-based encoder-decoder models are known to suffer the length bias and corresponding beam problem. Different approaches have been applied in simple beam search to ease the problem, most of which are heuristic-based and require considerable tuning. We show that heuristics are not proper modeling refinement, which results in severe performance degradation with largely increased beam sizes. We propose a novel beam search derived from reinterpreting the sequence posterior with an explicit length modeling. By applying the reinterpreted probability together with beam pruning, the obtained final probability leads to a robust model modification, which allows reliable comparison among output sequences of different lengths. Experimental verification on the LibriSpeech corpus shows that the proposed approach solves the length bias problem without heuristics or additional tuning effort. It provides robust decision making and consistently good performance under both small and very large beam sizes. Compared with the best results of the heuristic baseline, the proposed approach achieves the same WER on the ‘clean' sets and 4% relative improvement on the ‘other' sets. We also show that it is more efficient with the additional derived early stopping criterion",
    "checked": true,
    "id": "23ebfe762a3bb680fdbcdf12c942d50fc8db42b8",
    "semantic_title": "robust beam search for encoder-decoder attention based speech recognition without length bias",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chen20e_interspeech.html": {
    "title": "Transformer with Bidirectional Decoder for Speech Recognition",
    "volume": "main",
    "abstract": "Attention-based models have made tremendous progress on end-to-end automatic speech recognition (ASR) recently. However, the conventional transformer-based approaches usually generate the sequence results token by token from left to right, leaving the right-to-left contexts unexploited. In this work, we introduce a bidirectional speech transformer to utilize the different directional contexts simultaneously. Specifically, the outputs of our proposed transformer include a left-to-right target, and a right-to-left target. In inference stage, we use the introduced bidirectional beam search method, which can not only generate left-to-right candidates but also generate right-to-left candidates, and determine the best hypothesis by the score To demonstrate our proposed speech transformer with a bidirectional decoder (STBD), we conduct extensive experiments on the AISHELL-1 dataset. The results of experiments show that STBD achieves a 3.6% relative CER reduction (CERR) over the unidirectional speech transformer baseline. Besides, the strongest model in this paper called STBD-Big can achieve 6.64% CER on the test set, without language model rescoring and any extra data augmentation strategies ",
    "checked": true,
    "id": "f74082ee0529240fa14f8f0e17d2b1600cf6e6b2",
    "semantic_title": "transformer with bidirectional decoder for speech recognition",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20r_interspeech.html": {
    "title": "An Investigation of Phone-Based Subword Units for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "Phones and their context-dependent variants have been the standard modeling units for conventional speech recognition systems, while characters and subwords have demonstrated their effectiveness for end-to-end recognition systems. We investigate the use of phone-based subwords, in particular, byte pair encoder (BPE), as modeling units for end-to-end speech recognition. In addition, we also developed multi-level language model-based decoding algorithms based on a pronunciation dictionary. Besides the use of the lexicon, which is easily available, our system avoids the need of additional expert knowledge or processing steps from conventional systems. Experimental results show that phone-based BPEs tend to yield more accurate recognition systems than the character-based counterpart. In addition, further improvement can be obtained with a novel one-pass joint beam search decoder, which efficiently combines phone- and character-based BPE systems. For Switchboard, our phone-based BPE system achieves 6.8%/14.4% word error rate (WER) on the Switchboard/CallHome portion of the test set while joint decoding achieves 6.3%/13.3% WER. On Fisher + Switchboard, joint decoding leads to 4.9%/9.5% WER, setting new milestones for telephony speech recognition",
    "checked": true,
    "id": "1ae00bef8a43b37d435c7ce9d817ff76c0d48f13",
    "semantic_title": "an investigation of phone-based subword units for end-to-end speech recognition",
    "citation_count": 33
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wong20_interspeech.html": {
    "title": "Combination of End-to-End and Hybrid Models for Speech Recognition",
    "volume": "main",
    "abstract": "Recent studies suggest that it may now be possible to construct end-to-end Neural Network (NN) models that perform on-par with, or even outperform, hybrid models in speech recognition. These models differ in their designs, and as such, may exhibit diverse and complementary error patterns. A combination between the predictions of these models may therefore yield significant gains. This paper studies the feasibility of performing hypothesis-level combination between hybrid and end-to-end NN models. The end-to-end NN models often exhibit a bias in their posteriors toward short hypotheses, and this may adversely affect Minimum Bayes' Risk (MBR) combination methods. MBR training and length normalisation can be used to reduce this bias. Models are trained on Microsoft's 75 thousand hours of anonymised data and evaluated on test sets with 1.8 million words. The results show that significant gains can be obtained by combining the hypotheses of hybrid and end-to-end NN models together",
    "checked": true,
    "id": "17e8db9f2fc5ce97517a52a3d94f9a8fb5463ed2",
    "semantic_title": "combination of end-to-end and hybrid models for speech recognition",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kim20c_interspeech.html": {
    "title": "Evolved Speech-Transformer: Applying Neural Architecture Search to End-to-End Automatic Speech Recognition",
    "volume": "main",
    "abstract": "Neural architecture search (NAS) has been successfully applied to finding efficient, high-performance deep neural network architectures in a task-adaptive manner without extensive human intervention. This is achieved by choosing genetic, reinforcement learning, or gradient -based algorithms as automative alternatives of manual architecture design. However, a naive application of existing NAS algorithms to different tasks may result in architectures which perform sub-par to those manually designed. In this work, we show that NAS can provide efficient architectures that outperform manually designed attention-based architectures on speech recognition tasks, after which we named Evolved Speech-Transformer (EST). With a combination of carefully designed search space and Progressive dynamic hurdles, a genetic algorithm based, our algorithm finds a memory-efficient architecture which outperforms vanilla Transformer with reduced training time",
    "checked": true,
    "id": "f4cf310be52d53bb1f41c0d08c098b7a2e0c3562",
    "semantic_title": "evolved speech-transformer: applying neural architecture search to end-to-end automatic speech recognition",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2020/garg20_interspeech.html": {
    "title": "Hierarchical Multi-Stage Word-to-Grapheme Named Entity Corrector for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "In this paper, we propose a hierarchical multi-stage word-to-grapheme Named Entity Correction (NEC) algorithm. Conventional NEC algorithms use a single-stage grapheme or phoneme level edit distance to search and replace Named Entities (NEs) misrecognized by a speech recognizer. However, longer named entities like song titles cannot be easily handled by such a single stage correction. We propose a three-stage NEC, starting with a word-level matching, followed by a phonetic double metaphone based matching, and a final grapheme level candidate selection. We also propose a novel NE Rejection mechanism which is important to ensure that the NEC does not replace correctly recognized NEs with unintended but similar named entities. We evaluate our solution on two different test sets from the call and music domains, for both server as well as on-device speech recognition configurations. For the on-device model, our NEC outperforms an n-gram fusion when employed standalone. Our NEC reduces the word error rate by 14% and 63% relatively for music and call, respectively, when used after an n-gram based biasing language model. The average latency of our NEC is under 3 ms per input sentence while using only ~1 MB for an input NE list of 20,000 entries",
    "checked": true,
    "id": "7815f64cd8b4851f8b811bee6ab2210e96b7aecc",
    "semantic_title": "hierarchical multi-stage word-to-grapheme named entity corrector for automatic speech recognition",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2020/beck20_interspeech.html": {
    "title": "LVCSR with Transformer Language Models",
    "volume": "main",
    "abstract": "Neural network language models (LMs) based on self-attention have recently outperformed the previous state of the art, LSTM LMs. Transformer LMs today are often used as a postprocessing step in lattice or n-best list rescoring. In this work the main focus is on using them in one-pass recognition. We show that by a simple reduction of redundant computations in batched self-attention we can obtain a 15% reduction in overall RTF on a well-tuned system. We also show that through proper initialization the layer normalization inside the residual blocks can be removed, yielding a further increase in forwarding speed. This is done under the constraint of staying close to state-of-the-art in terms of word-error rate (5.4% on LibriSpeech test-other) and achieving a real-time factor of around 1. Last but not least we also present an approach to speed up classic push-forward rescoring by mixing it with n-best list rescoring to better utilize the inherent parallelizability of Transformer language models, cutting the time needed for rescoring in half",
    "checked": true,
    "id": "0fe7bf6c84a7494a6331646936d017148f989c34",
    "semantic_title": "lvcsr with transformer language models",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chen20f_interspeech.html": {
    "title": "DARTS-ASR: Differentiable Architecture Search for Multilingual Speech Recognition and Adaptation",
    "volume": "main",
    "abstract": "In previous works, only parameter weights of ASR models are optimized under fixed-topology architecture. However, the design of successful model architecture has always relied on human experience and intuition. Besides, many hyperparameters related to model architecture need to be manually tuned. Therefore in this paper, we propose an ASR approach with efficient gradient-based architecture search, DARTS-ASR. In order to examine the generalizability of DARTS-ASR, we apply our approach not only on many languages to perform monolingual ASR, but also on a multilingual ASR setting. Following previous works, we conducted experiments on a multilingual dataset, IARPA BABEL. The experiment results show that our approach outperformed the baseline fixed-topology architecture by 10.2% and 10.0% relative reduction on character error rates under monolingual and multilingual ASR settings respectively. Furthermore, we perform some analysis on the searched architectures by DARTS-ASR",
    "checked": true,
    "id": "d2e0a1f2aa5829a35abfabdc16f984060a418a50",
    "semantic_title": "darts-asr: differentiable architecture search for multilingual speech recognition and adaptation",
    "citation_count": 30
  },
  "https://www.isca-speech.org/archive/interspeech_2020/stappen20_interspeech.html": {
    "title": "Uncertainty-Aware Machine Support for Paper Reviewing on the Interspeech 2019 Submission Corpus",
    "volume": "main",
    "abstract": "The evaluation of scientific submissions through peer review is both the most fundamental component of the publication process, as well as the most frequently criticised and questioned. Academic journals and conferences request reviews from multiple reviewers per submission, which an editor, or area chair aggregates into the final acceptance decision. Reviewers are often in disagreement due to varying levels of domain expertise, confidence, levels of motivation, as well as due to the heavy workload and the different interpretations by the reviewers of the score scale. Herein, we explore the possibility of a computational decision support tool for the editor, based on Natural Language Processing, that offers an additional aggregated recommendation. We provide a comparative study of state-of-the-art text modelling methods on the newly crafted, largest review dataset of its kind based on Interspeech 2019, and we are the first to explore uncertainty-aware methods (soft labels, quantile regression) to address the subjectivity inherent in this problem",
    "checked": true,
    "id": "0a970e6f462a5c4eb9ee4f25b5d4043fc8a077c7",
    "semantic_title": "uncertainty-aware machine support for paper reviewing on the interspeech 2019 submission corpus",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2020/cohn20b_interspeech.html": {
    "title": "Individual Variation in Language Attitudes Toward Voice-AI: The Role of Listeners' Autistic-Like Traits",
    "volume": "main",
    "abstract": "More and more, humans are engaging with voice-activated artificially intelligent (voice-AI) systems that have names (e.g., Alexa), apparent genders, and even emotional expression; they are in many ways a growing ‘social'[ presence. But to what extent do people display sociolinguistic attitudes, developed from human-human interaction, toward these disembodied text-to-speech (TTS) voices? And how might they vary based on the cognitive traits of the individual user? The current study addresses these questions, testing native English speakers' judgments for 6 traits (intelligent, likeable, attractive, professional, human-like, and age) for a naturally-produced female human voice and the US-English default Amazon Alexa voice. Following exposure to the voices, participants completed these ratings for each speaker, as well as the Autism Quotient (AQ) survey, to assess individual differences in cognitive processing style. Results show differences in individuals' ratings of the likeability and human-likeness of the human and AI talkers based on AQ score. Results suggest that humans transfer social assessment of human voices to voice-AI, but that the way they do so is mediated by their own cognitive characteristics",
    "checked": true,
    "id": "e551c4646b201ab34a5921f1f17773cada66eda2",
    "semantic_title": "individual variation in language attitudes toward voice-ai: the role of listeners' autistic-like traits",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2020/cohn20c_interspeech.html": {
    "title": "Differences in Gradient Emotion Perception: Human vs. Alexa Voices",
    "volume": "main",
    "abstract": "The present study compares how individuals perceive gradient acoustic realizations of emotion produced by a human voice versus an Amazon Alexa text-to-speech (TTS) voice. We manipulated semantically neutral sentences spoken by both talkers with identical emotional synthesis methods, using three levels of increasing ‘happiness' (0%, 33%, 66% ‘happier'). On each trial, listeners (native speakers of American English, n=99) rated a given sentence on two scales to assess dimensions of emotion: valence (negative-positive) and arousal (calm-excited). Participants also rated the Alexa voice on several parameters to assess anthropomorphism (e.g., naturalness, human-likeness, etc.). Results showed that the emotion manipulations led to increases in perceived positive valence and excitement. Yet, the effect differed by interlocutor: increasing ‘happiness' manipulations led to larger changes for the human voice than the Alexa voice. Additionally, we observed individual differences in perceived valence/arousal based on participants' anthropomorphism scores. Overall, this line of research can speak to theories of computer personification and elucidate our changing relationship with voice-AI technology",
    "checked": true,
    "id": "ed1622469d73f8cfdccb747dea75324c325cf6cd",
    "semantic_title": "differences in gradient emotion perception: human vs. alexa voices",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2020/martinezlucas20_interspeech.html": {
    "title": "The MSP-Conversation Corpus",
    "volume": "main",
    "abstract": "Human-computer interactions can be very effective, especially if computers can automatically recognize the emotional state of the user. A key barrier for effective speech emotion recognition systems is the lack of large corpora annotated with emotional labels that reflect the temporal complexity of expressive behaviors, especially during multiparty interactions. This paper introduces the MSP-Conversation corpus, which contains interactions annotated with time-continuous emotional traces for arousal (calm to active), valence (negative to positive), and dominance (weak to strong). Time-continuous annotations offer the flexibility to explore emotional displays at different temporal resolutions while leveraging contextual information. This is an ongoing effort, where the corpus currently contains more than 15 hours of speech annotated by at least five annotators. The data is sourced from the MSP-Podcast corpus, which contains speech data from online audio-sharing websites annotated with sentence-level emotional scores. This data collection scheme is an easy, affordable, and scalable approach to obtain natural data with diverse emotional content from multiple speakers. This study describes the key features of the corpus. It also compares the time-continuous evaluations from the MSP-Conversation corpus with the sentence-level annotations of the MSP-Podcast corpus for the speech segments that overlap between the two corpora",
    "checked": true,
    "id": "07bc7ceaf81e335fdf37436af77e429fb70d795c",
    "semantic_title": "the msp-conversation corpus",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tao20_interspeech.html": {
    "title": "Spotting the Traces of Depression in Read Speech: An Approach Based on Computational Paralinguistics and Social Signal Processing",
    "volume": "main",
    "abstract": "This work investigates the use of a classification approach as a means to identify effective depression markers in read speech, i.e., observable and measurable traces of the pathology in the way people read a predefined text. This is important because the diagnosis of depression is still a challenging problem and reliable markers can, at least to a partial extent, contribute to address it. The experiments have involved 110 individuals and revolve around the tendency of depressed people to read slower and display silences that are both longer and more frequent. The results show that features expected to capture such differences reduce the error rate of a baseline classifier by more than 50% (from 31.8% to 15.5%). This is of particular interest when considering that the new features are less than 10% of the original set (3 out of 32). Furthermore, the results appear to be in line with the findings of neuroscience about brain-level differences between depressed and non-depressed individuals",
    "checked": true,
    "id": "75c8a49962f48dc57c6eaa0906b022455cbf907c",
    "semantic_title": "spotting the traces of depression in read speech: an approach based on computational paralinguistics and social signal processing",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kim20d_interspeech.html": {
    "title": "Speech Sentiment and Customer Satisfaction Estimation in Socialbot Conversations",
    "volume": "main",
    "abstract": "For an interactive agent, such as task-oriented spoken dialog systems or chatbots, measuring and adapting to Customer Satisfaction (CSAT) is critical in order to understand user perception of an agent's behavior and increase user engagement and retention. However, an agent often relies on explicit customer feedback for measuring CSAT. Such explicit feedback may result in potential distraction to users and it can be challenging to capture continuously changing user's satisfaction. To address this challenge, we present a new approach to automatically estimate CSAT using acoustic and lexical information in the Alexa Prize Socialbot data. We first explore the relationship between CSAT and sentiment scores at both the utterance and conversation level. We then investigate static and temporal modeling methods that use estimated sentiment scores as a mid-level representation. The results show that the sentiment scores, particularly valence and satisfaction, are correlated with CSAT. We also demonstrate that our proposed temporal modeling approach for estimating CSAT achieves competitive performance, relative to static baselines as well as human performance. This work provides insights into open domain social conversations between real users and socialbots, and the use of both acoustic and lexical information for understanding the relationship between CSAT and sentiment scores",
    "checked": true,
    "id": "eb6fbed58b2a7915b09c3665725656b70a127f05",
    "semantic_title": "speech sentiment and customer satisfaction estimation in socialbot conversations",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lepp20_interspeech.html": {
    "title": "Pardon the Interruption: An Analysis of Gender and Turn-Taking in U.S. Supreme Court Oral Arguments",
    "volume": "main",
    "abstract": "This study presents a corpus of turn changes between speakers in U.S. Supreme Court oral arguments. Each turn change is labeled on a spectrum of \"cooperative\" to \"competitive\" by a human annotator with legal experience in the United States. We analyze the relationship between speech features, the nature of exchanges, and the gender and legal role of the speakers. Finally, we demonstrate that the models can be used to predict the label of an exchange with moderate success. The automatic classification of the nature of exchanges indicates that future studies of turn-taking in oral arguments can rely on larger, unlabeled corpora ",
    "checked": true,
    "id": "a2bd095bec9dac031ef7cf86bb3fafce3c1f869e",
    "semantic_title": "pardon the interruption: an analysis of gender and turn-taking in u.s. supreme court oral arguments",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/neitsch20_interspeech.html": {
    "title": "Are Germans Better Haters Than Danes? Language-Specific Implicit Prosodies of Types of Hate Speech and How They Relate to Perceived Severity and Societal Rules",
    "volume": "main",
    "abstract": "Hate speech, both written and spoken, is a growing source of concern as it often discriminates societal minorities for their national origin, sexual orientation, gender or disabilities. Despite its destructive power, hardly anything is known about whether there are cross-linguistic mechanisms and acoustic-phonetic characteristics of hate speech. For this reason, our experiment analyzes the implicit prosodies that are caused by written Twitter and Facebook hate-speech items and made phonetically \"tangible\" through a special, introspective reading-aloud task. We compare the elicited (implicit) prosodies of Danish and German speakers with respect to f0, intensity, HNR, and the Hammarberg index. While we found no evidence for a consistent hate-speech-specific prosody either within or between the two languages, our results show clear prosodic differences associated with types of hate speech and their targeted minority groups. Moreover, language-specific differences suggest that — compared to Danish — German hate speech sounds more expressive and hateful. Results are discussed regarding their implications for the perceived severity and the automatic flagging and deletion of hate-speech posts in social media",
    "checked": true,
    "id": "7c273589761814f43d25c2c8f0ca583f1b104331",
    "semantic_title": "are germans better haters than danes? language-specific implicit prosodies of types of hate speech and how they relate to perceived severity and societal rules",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chen20g_interspeech.html": {
    "title": "An Objective Voice Gender Scoring System and Identification of the Salient Acoustic Measures",
    "volume": "main",
    "abstract": "Human voices vary in their perceived masculinity or femininity, and subjective gender scores provided by human raters have long been used in psychological studies to understand the complex psychosocial relationships between people. However, there has been limited research on developing objective gender scoring of voices and examining the correlation between objective gender scores (including the weighting of each acoustic factor) and subjective gender scores (i.e., perceived masculinity/ femininity). In this work we propose a gender scoring model based on Linear Discriminant Analysis (LDA) and using weakly labelled data to objectively rate speakers' masculinity and femininity. For 434 speakers, we investigated 29 acoustic measures of voice characteristics and their relationships to both the objective scores and subjective masculinity/femininity ratings. The results revealed close correspondence between objective scores and subjective ratings of masculinity for males and femininity for females (correlations of 0.667 and 0.505 respectively). Among the 29 measures, F0 was found to be the most important vocal characteristic influencing both objective and subjective ratings for both sexes. For female voices, local absolute jitter and Harmonic-to-Noise Ratio (HNR) were moderately associated with objective scores. For male voices, F0 variance influenced objective gender scores more than the subjective ratings provided by human listeners",
    "checked": true,
    "id": "145110d8083fddf3832f247346511709e0616ff2",
    "semantic_title": "an objective voice gender scoring system and identification of the salient acoustic measures",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/jayawardena20_interspeech.html": {
    "title": "How Ordinal Are Your Data?",
    "volume": "main",
    "abstract": "Many affective computing datasets are annotated using ordinal scales, as are many other forms of ground truth involving subjectivity, e.g. depression severity. When investigating these datasets, the speech processing community has chosen classification problems in some cases, and regression in others, while ordinal regression may also arguably be the correct approach for some. However, there is currently essentially no guidance on selecting a suitable machine learning and evaluation method. To investigate this problem, this paper proposes a neural network-based framework which can transition between different modelling methods with the help of a novel multi-term loss function. Experiments on synthetic datasets show that the proposed framework is empirically well-behaved and able to correctly identify classification-like, ordinal regression-like and regression-like properties within multidimensional datasets. Application of the proposed framework to six real datasets widely used in affective computing and related fields suggests that more focus should be placed on ordinal regression instead of classifying or predicting, which are the common practices to date",
    "checked": true,
    "id": "2ddb2554dd57170077b8b2708fbfc316f963c9ea",
    "semantic_title": "how ordinal are your data?",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hughes20_interspeech.html": {
    "title": "Correlating Cepstra with Formant Frequencies: Implications for Phonetically-Informed Forensic Voice Comparison",
    "volume": "main",
    "abstract": "A significant question for forensic voice comparison, and for speaker recognition more generally, is the extent to which different input features capture complementary speaker-specific information. Understanding complementarity allows us to make predictions about how combining methods using different features may produce better overall performance. In forensic contexts, it is also important to be able to explain to courts what information the underlying features are actually capturing. This paper addresses these issues by examining the extent to which MFCCs and LPCCs can predict F0, F1, F2, and F3 values using data extracted from the midpoint of the vocalic portion of the hesitation marker um for 89 speakers of standard southern British English. By-speaker correlations were calculated using multiple linear regression and performance was assessed using mean rho (ρ) values. Results show that the first two formants were more accurately predicted than F3 or F0. LPCCs consistently produced stronger correlations with the linguistic features than MFCCs, while increasing cepstral order up to 16 also increased the strength of the correlations. There was, however, considerable variability across speakers in terms of the accuracy of the predictions. We discuss the implications of these findings for forensic voice comparison",
    "checked": true,
    "id": "311d4c0a945f1c6dd8428a9ae76c7579f34d6b87",
    "semantic_title": "correlating cepstra with formant frequencies: implications for phonetically-informed forensic voice comparison",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/neitsch20b_interspeech.html": {
    "title": "Prosody and Breathing: A Comparison Between Rhetorical and Information-Seeking Questions in German and Brazilian Portuguese",
    "volume": "main",
    "abstract": "Several studies have shown that rhetorical wh-questions (RQs) and string-identical information-seeking wh-questions (ISQs) are realized with different prosodic characteristics. In contrast to ISQs, RQs have been shown to be phonetically realized with a breathier (i.e., softer) voice quality (e.g., German and English) and longer constituent durations (e.g., German, English, Icelandic). Based on similar results found for different languages, we investigate wh-RQs and sting-identical wh-ISQs in Brazilian Portuguese (BP) and German (G). We analyze (i) whether specific duration and voice-quality patterns characterize and separate the two illocution types (RQ and ISQ) in BP, and (ii) if direct measures of the respiratory sub-system reveal differences between illocution types, given that breathiness involves greater transglottal air flow which can be observed in the speakers' chest and/or abdomen movement Our data suggest that, similar to G, English, and Icelandic, duration and voice quality patterns play a role in the realization of RQs compared to ISQs in BP, reinforcing the assumption that there are cross-linguistically similar phonetic features in the realization of RQs compared to ISQs. We also find that speakers of G breathe in more deeply and dynamically than speakers of BP, suggesting a link between breathing and voice quality",
    "checked": true,
    "id": "164c71d449c839c21621f05d6f93da837955da39",
    "semantic_title": "prosody and breathing: a comparison between rhetorical and information-seeking questions in german and brazilian portuguese",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/defina20_interspeech.html": {
    "title": "Scaling Processes of Clause Chains in Pitjantjatjara",
    "volume": "main",
    "abstract": "Clause chains are a syntactic strategy for combining multiple clauses into a single unit. They are reported in many languages, including Korean and Turkish. However, they have seen relatively little focused research. In particular, prosodic features are often mentioned in descriptions of clause chaining, however there have been vanishingly few investigations. Corpus-based studies of the prosody of clause chains in two unrelated languages of Papua New Guinea report that they are typically produced as a sequence of Intonation phrases united by pitch-scaling of the L% boundary tones in each clause with only the final, finite, clause descending to a full L%. The present study is the first experimental investigation of the prosody of clause chains in Pitjantjatjara This paper focuses on one type of clause chain found in the Australian Indigenous language Pitjantjatjara. We examine a set of 120 clause chains read out by three native Pitjantjatjara speakers. Prosodic analysis reveals that these Pitjantjatjara clause chains are produced within a single Intonational Phrase. Speakers do not pause between the clauses in the chain, there is consistent linear downstep throughout the phrase and additionally phrase final lowering occurs at the end of the utterance. This differs from previous impressionistic studies of the prosody of clause chains",
    "checked": true,
    "id": "2a0d6d8fc1ea840e608eee3c51604745afdfccb9",
    "semantic_title": "scaling processes of clause chains in pitjantjatjara",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mizoguchi20_interspeech.html": {
    "title": "Neutralization of Voicing Distinction of Stops in Tohoku Dialects of Japanese: Field Work and Acoustic Measurements",
    "volume": "main",
    "abstract": "Research on Tohoku dialects, which is a variety of Japanese, has found that the voiceless stops /k/ and /t/ in the intervocalic position are frequently realized as voiced stops. However, the phenomenon has mainly been judged aurally in the Japanese linguistics literature and has not been confirmed by acoustic measurements. We measured the VOT of data originally collected in the survey of Tohoku dialects by [1]. The data used in this study includes two age groups from eight sites. The results demonstrate that for word medial stops, the VOT distribution of voiced and voiceless stops largely overlapped, while, the laryngeal contrast was maintained for the word initial stops. Intervocalic voicing neutralization was confirmed by quantitative acoustic measurements. The effects of neighboring vowels were also investigated to show that height, but not duration, had a significant effect on voicing neutralization. Our results shed light on the phonetic nature of Tohoku dialects as well as on their phonological structure, such as the role of voicing contrast",
    "checked": true,
    "id": "a289ffbd6cfb40bb26ee089571662b06e65bbf17",
    "semantic_title": "neutralization of voicing distinction of stops in tohoku dialects of japanese: field work and acoustic measurements",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lee20b_interspeech.html": {
    "title": "Correlation Between Prosody and Pragmatics: Case Study of Discourse Markers in French and English",
    "volume": "main",
    "abstract": "This paper investigates the prosodic characteristics of French and English discourse markers according to their pragmatic meaning in context. The study focusses on three French discourse markers ( alors [‘so'], bon [‘well'], and donc [‘so']) and three English markers ( now, so, and well). Hundreds of occurrences of discourse markers were automatically extracted from French and English speech corpora and manually annotated with pragmatic functions labels. The paper compares the prosodic characteristics of discourse markers in different speech styles and in two languages. The first comparison is carried out with respect to two different speech styles in French: spontaneous speech vs. prepared speech. The other comparison of the prosodic characteristics is conducted between two languages, French vs. English, on the prepared speech. Results show that some pragmatic functions of discourse markers bring about specific prosodic behaviour in terms of presence and position of pauses, and their F0 articulation in their immediate context. Moreover, similar pragmatic functions frequently share similar prosodic characteristics, even across languages",
    "checked": true,
    "id": "f4719920d23c831836cc2f7400398cfe4403a87f",
    "semantic_title": "correlation between prosody and pragmatics: case study of discourse markers in french and english",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zarka20_interspeech.html": {
    "title": "An Analysis of Prosodic Prominence Cues to Information Structure in Egyptian Arabic",
    "volume": "main",
    "abstract": "This study presents the first acoustic examination of prominence relations in entire contours associated with different information structures in Egyptian Arabic. Previous work has shown that topics and foci are typically associated with different pitch events, whereas it is still a matter of debate whether and how Egyptian Arabic uses prominence relations to mark narrow focus. The analysis of data from 17 native speakers showed that narrow focus was marked by on-focus pitch expansion as well as post-focus compression. Post-focus compression was realized as a large downstep after focus, compressed pitch range, lower intensity and shorter duration. The results also showed further register lowering after a contrastive focus, but no further pitch boost of the focused word. By contrast, a contrastive topic showed higher scaling of the topic as well as an expanded pitch range of the overall contour. The findings of this study stress the significance of whole contours to convey intonational meanings, revealing gradient prominence cues to focus across the utterance, specifically post-focus register lowering to enhance the prominence of a contrastive focus",
    "checked": true,
    "id": "8342062d2f67f4c08707b882b29f97a6b0e14fc5",
    "semantic_title": "an analysis of prosodic prominence cues to information structure in egyptian arabic",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mumtaz20_interspeech.html": {
    "title": "Lexical Stress in Urdu",
    "volume": "main",
    "abstract": "This study looks at the role of lexical stress in Urdu prosody. The literature on lexical stress is divided, with some authors developing algorithms for stress assignment, while others deny its relevance for prosody. We performed three experiments to investigate this issue. We found evidence that a strong increase in the duration of a syllable indicates stress and that lexical stress and phrasal intonation interact in a non-trivial manner. We also found that stress perception varies according to syllable weight with weight clash being a determining factor",
    "checked": true,
    "id": "5b64c2d6b1ff0281870c8b86f2983d8476cc3a85",
    "semantic_title": "lexical stress in urdu",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/riad20_interspeech.html": {
    "title": "Vocal Markers from Sustained Phonation in Huntington's Disease",
    "volume": "main",
    "abstract": "Disease-modifying treatments are currently assessed in neurodegenerative diseases. Huntington's Disease represents a unique opportunity to design automatic sub-clinical markers, even in premanifest gene carriers. We investigated phonatory impairments as potential clinical markers and propose them for both diagnosis and gene carriers follow-up. We used two sets of features: Phonatory features and Modulation Power Spectrum Features. We found that phonation is not sufficient for the identification of sub-clinical disorders of premanifest gene carriers. According to our regression results, Phonatory features are suitable for the predictions of clinical performance in Huntington's Disease",
    "checked": true,
    "id": "66335e222f997b734af63fa403669cc58146e928",
    "semantic_title": "vocal markers from sustained phonation in huntington's disease",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2020/dentel20_interspeech.html": {
    "title": "How Rhythm and Timbre Encode Mooré Language in Bendré Drummed Speech",
    "volume": "main",
    "abstract": "Human languages have the flexibility to be acoustically adapted to the context of communication, such as in shouting or whispering. Drummed forms of languages represent one of the most extreme natural expressions of such speech adaptability. A large amount of research has been conducted on drummed languages in anthropology or linguistics, particularly in West African societies. However, in spite of the clearly rhythmic nature of drumming, previous studies have largely neglected exploring systematically the role of speech rhythm. Here, we explore a unique corpus of the Bendré drummed speech form of the Mossi people, transcribed published in the 80's by the anthropologist Kawada Junzo. The analysis of this large database in Mooré language reveals that the rhythmic units encoded in the length of pauses between drumbeats match more closely with vowel-to-vowel intervals than with syllable parsing. Meanwhile, we confirm for the first time a result found recently on the drummed speech tradition of the Bora Amazonian language. However, the complex acoustic structure of the Bendré skin drum required much more attention than the simple two pitch hollow log drum of the Bora. Thus, we also present here results on how drummed Bendré timbre encodes tones of Mooré language",
    "checked": true,
    "id": "5602fa3d483202c99f47cfce06fa021b9b04ed4d",
    "semantic_title": "how rhythm and timbre encode mooré language in bendré drummed speech",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lee20_interspeech.html": {
    "title": "Doing Something we Never could with Spoken Language Technologies-from early days to the era of deep learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "9ae3723c77b98b15757c805eb14b908548e2abe9",
    "semantic_title": "doing something we never could with spoken language technologies-from early days to the era of deep learning",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lalhminghlui20_interspeech.html": {
    "title": "Interaction of Tone and Voicing in Mizo",
    "volume": "main",
    "abstract": "Since the production of fundamental frequency and voicing is determined by the tension in the vocal folds, it is noticed that VOT is affected by the F0 in tone languages. Similarly laryngeal contrasts also affect the F0 of tone. This work studies the interaction between tone and voicing in a lesser-known tone language, Mizo. Mizo has eight stops, that can be categorized into three laryngeal contrasts namely, voiced, voiceless unaspirated, and voiceless aspirated. In the current work, we look into CV syllables produced with the eight Mizo stops with all five vowel categories of the language, produced with four distinct tones in Mizo. The results show a predictable effect of onsets on the F0 of tone and weak effect of tone on VOT duration",
    "checked": true,
    "id": "fa08eb16b30dba8cc4ecaa83db88a58f63cc07fb",
    "semantic_title": "interaction of tone and voicing in mizo",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wu20h_interspeech.html": {
    "title": "Mandarin Lexical Tones: A Corpus-Based Study of Word Length, Syllable Position and Prosodic Position on Duration",
    "volume": "main",
    "abstract": "The present study aims to increase our knowledge of Mandarin lexical tones in fluent speech, more specifically their occurrence frequency distributions and their duration patterns. First, the occurrence frequency of each lexical tone was computed in a large speech corpus (~220 hours). Then the duration of each lexical tone, as well as the impact of word length, syllable position and the prosodic position were investigated. Overall, results show that Tone 3 tends to have the longest duration among all lexical tones. Nonetheless, the factors word length, syllable position and prosodic position are found to impact tone duration. Monosyllabic words exhibit tone durations closer to those of word-final syllables (especially for disyllabic words) than to other syllable positions. Moreover, tone duration tends to be the longest at word's right boundary in Mandarin, regardless of word length. An effect of prosodic position is also found: the duration of Mandarin lexical tones tends to increase with higher prosodic level. Tone durations are the longest in phrase-final position, followed by word-final position and word-medial position, regardless of the tone nature",
    "checked": true,
    "id": "e3f90f3fc64873b10d970a779c85eadc1c0c09cf",
    "semantic_title": "mandarin lexical tones: a corpus-based study of word length, syllable position and prosodic position on duration",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gao20c_interspeech.html": {
    "title": "An Investigation of the Target Approximation Model for Tone Modeling and Recognition in Continuous Mandarin Speech",
    "volume": "main",
    "abstract": "The complex f variations in continuous speech make it rather difficult to perform automatic recognition of tones in a language like Mandarin Chinese. In this study, we tested the use of target approximation model (TAM) for continuous tone recognition on two datasets. TAM simulates f production from the articulatory point of view and so allow to discover the underlying pitch targets from the surface f contour. The f contour of each tone represented by 30 equidistant points in the first dataset was simulated by the TAM model. Using a support vector machine (SVM) to classify tones showed that, compared to the representation by 30 f values, the estimated three-dimensional TAM parameters had a comparable performance in characterizing tone patterns. The TAM model was further tested on the second dataset containing more complex tonal variations. With equal or a fewer number of features, the TAM parameters provided better performance than the coefficients of the cosine transform and a slightly worse performance than the statistical f parameters for tone recognition. Furthermore, we investigated bidirectional LSTM neural network for modelling the sequential tonal variations, which proved to be more powerful than the SVM classifier. The BLSTM system incorporating TAM and statistical f parameters achieved the best accuracy of 87.56%",
    "checked": true,
    "id": "4e6ea5cd3fa7ce7b99a44ac7774801c873cd2ec6",
    "semantic_title": "an investigation of the target approximation model for tone modeling and recognition in continuous mandarin speech",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lai20_interspeech.html": {
    "title": "Integrating the Application and Realization of Mandarin 3rd Tone Sandhi in the Resolution of Sentence Ambiguity",
    "volume": "main",
    "abstract": "Chinese third tone sandhi (T3S) covaries with the prosodic hierarchy both in the probability of application and in the realization of pitch slope. This paper evaluates whether Mandarin-speaking listeners integrate the covariation between T3S and prosody to resolute sentence ambiguity. Twenty-seven structurally ambiguous sentences were designed, each containing two consecutive T3 syllables situated across a word boundary, and the strength of the T3-intervening boundary crucially differentiates different interpretations of the sentence. The first T3 was manipulated to bear either a low, a shallow-rising, or a sharp-rising pitch. Sixty native Mandarin-speaking listeners heard each of these sentences and chose from two written interpretations the one that was consistent with what they heard. The results show that listeners are more likely to report a major-juncture interpretation when T3S does not apply (low) than when it applies (rising), and in the latter case, when the T3S variant has a sharper rather than shallower slope. Post-hoc analyses show that the T3S application is a more robust parsing cue for short sentences (4–5 syllables long), whereas the pitch shape of T3S is a more efficient parsing cue for longer sentences, indicating that listeners make sophisticated use of tonal variation to facilitate sentence processing",
    "checked": true,
    "id": "d825647bf6d016c937ff7a744f5cc5af720bc45c",
    "semantic_title": "integrating the application and realization of mandarin 3rd tone sandhi in the resolution of sentence ambiguity",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20p_interspeech.html": {
    "title": "Neutral Tone in Changde Mandarin",
    "volume": "main",
    "abstract": "This paper discusses the phonetics and phonology of tones in Changde Mandarin and focuses on neutral tone in disyllabic words. Acoustic realizations of both citation and neutral tones are examined in terms of fundamental frequencies (F ), duration, and intensity. And phonetic and phonological descriptions are given on the basis of acoustic data. Acoustic data from 12 speakers show that Changde Mandarin has four lexical tones that distinguish in contour, namely level versus rising, and pitch height, namely high versus low. Neutral tone in Changde Mandarin is different from that in Beijing Mandarin or Standard Chinese. Neutral tone in Changde Mandarin is a reduced form of its citation tone, which is produced with a neutralized pitch height, and a significantly shorter duration and weaker intensity than the control",
    "checked": true,
    "id": "05c89b3c4e10ef06d10654cb1b54eca2c8d45296",
    "semantic_title": "neutral tone in changde mandarin",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/cui20_interspeech.html": {
    "title": "Pitch Declination and Final Lowering in Northeastern Mandarin",
    "volume": "main",
    "abstract": "Northeastern Mandarin has a similar lexical tone system as Beijing Mandarin. However, the two dialects significantly diverge at higher prosodic structures. T1 in Northeastern Mandarin always changes to a falling tone in domain-final positions. Previous studies have analyzed this variation as a type of tone sandhi, but we propose it is related to more global prosodic processes such as final lowering. We addressed this issue by conducting both production and perception experiments with native bidialectal speakers of Northeastern Mandarin and Beijing Mandarin. Our findings suggest that T1 variation is essentially a domain-final lowering effect. Other tones also show some kind of final lowering effects. Compared to Beijing Mandarin, Northeastern Mandarin generally has greater global pitch declination and greater final lowering effects. Our perception experiment further showed that both prosodic effects play important roles in identifying the Northeastern Mandarin accent, and final lowering cues are more perceptually salient than the global declination cues. These findings support the notion that pitch declination and final lowering effects are linguistically controlled, not just a by-product of the physiological mechanisms",
    "checked": true,
    "id": "04b4f25cbf113bc6766d60916b7f8bf31dccaba1",
    "semantic_title": "pitch declination and final lowering in northeastern mandarin",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/rose20_interspeech.html": {
    "title": "Variation in Spectral Slope and Interharmonic Noise in Cantonese Tones",
    "volume": "main",
    "abstract": "To provide reference data for studies of voice quality variation in lexical tone, an experiment is described to investigate the nature of intrinsic variation in spectral slope and interharmonic noise for Cantonese citation tones. 23 spectral slope and interharmonic noise measures are extracted with VoiceSauce from the tones on /o/ Rhymes of five male and five female speakers of conservative Cantonese. Significant correlation between F0 and both spectral slope and interharmonic noise is demonstrated. It is shown with probabilistic bivariate discriminant analysis that even tones with no extrinsic voice quality differences can be identified at rates considerably above chance from a combination of their spectral slope and interharmonic noise. Male tones, with a minimal error rate of 5.7%, are identified twice as well as female, with a minimal error rate of 14.5%. Combinations with uncorrected spectral slopes perform better than corrected. The best combinations for both sexes involve slope parameters H2H4 (difference between the 4 and 2 harmonic amplitudes); and H42K (difference between the 4 harmonic and nearest harmonic to 2 kHz), irrespective of noise parameters. The worst combinations involve CPP (cepstral peak prominence) as a noise parameter",
    "checked": true,
    "id": "a593d083e5170c4a0d79621122263bd77da0754f",
    "semantic_title": "variation in spectral slope and interharmonic noise in cantonese tones",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tang20_interspeech.html": {
    "title": "The Acoustic Realization of Mandarin Tones in Fast Speech",
    "volume": "main",
    "abstract": "Many studies have demonstrated that acoustic contrasts between speech segments (vowels and consonants) were reduced when speaking rate increases, while it was unclear whether tones in tonal languages also undergo similar modifications. Mandarin Chinese is a tonal language, while results regarding the rate effect on Mandarin tones in previous studies were mixed, probably driven by the material difference, i.e., the position of target tones within a sentence. Therefore, the present study examined the effect of speaking rate on Mandarin tones, comparing the pitch contour and tonal contrast of Mandarin tones between normal and fast speech across utterance initial, medial and final positions. The results showed that, relative to normal speech, lexical tones in Mandarin Chinese exhibited overall higher and flatter pitch contours, with smaller tonal space. Moreover, the rate effect on tones did not vary with position. The current results and previous studies on segments thus revealed a universal pattern of speech reduction in fast speech at both segmental and suprasegmental levels",
    "checked": true,
    "id": "cb3084d4135e2fe100dd4118db1917e84c7905bb",
    "semantic_title": "the acoustic realization of mandarin tones in fast speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/loukina20_interspeech.html": {
    "title": "Do Face Masks Introduce Bias in Speech Technologies? The Case of Automated Scoring of Speaking Proficiency",
    "volume": "main",
    "abstract": "The COVID-19 pandemic has led to a dramatic increase in the use of face masks worldwide. Face coverings can affect both acoustic properties of the signal as well as speech patterns and have unintended effects if the person wearing the mask attempts to use speech processing technologies. In this paper we explore the impact of wearing face masks on the automated assessment of English language proficiency. We use a dataset from a large-scale speaking test for which test-takers were required to wear face masks during the test administration, and we compare it to a matched control sample of test-takers who took the same test before the mask requirements were put in place. We find that the two samples differ across a range of acoustic measures and also show a small but significant difference in speech patterns. However, these differences do not lead to differences in human or automated scores of English language proficiency. Several measures of bias showed no differences in scores between the two groups",
    "checked": true,
    "id": "501b4deeca6da4b4d003c617f1110fee33cd5370",
    "semantic_title": "do face masks introduce bias in speech technologies? the case of automated scoring of speaking proficiency",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mhiri20_interspeech.html": {
    "title": "A Low Latency ASR-Free End to End Spoken Language Understanding System",
    "volume": "main",
    "abstract": "In recent years, developing a speech understanding system that classifies a waveform to structured data, such as intents and slots, without first transcribing the speech to text has emerged as an interesting research problem. This work proposes such as system with an additional constraint of designing a system that has a small enough footprint to run on small micro-controllers and embedded systems with minimal latency. Given a streaming input speech signal, the proposed system can process it segment-by-segment without the need to have the entire stream at the moment of processing. The proposed system is evaluated on the publicly available Fluent Speech Commands dataset. Experiments show that the proposed system yields state-of-the-art performance with the advantage of low latency and a much smaller model when compared to other published works on the same task",
    "checked": true,
    "id": "550997dc44e1d82b1deb544a73d8aa8ef4115f27",
    "semantic_title": "a low latency asr-free end to end spoken language understanding system",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20s_interspeech.html": {
    "title": "An Audio-Based Wakeword-Independent Verification System",
    "volume": "main",
    "abstract": "We propose an audio-based wakeword-independent verification model to determine whether a wakeword spotting model correctly woke and should respond or incorrectly woke and should not respond. Our model works on any wakeword-initiated audio, independent of the wakeword by operating only on the audio surrounding the wakeword, yielding a wakeword agnostic model. This model is based on two key assumptions: that audio surrounding the wakeword is informative to determine if the user intended to wake the device and that this audio is independent of the wakeword itself. We show experimentally that on wakewords not included in the training set, our model trained without examples or knowledge of the wakeword is able to achieve verification performance comparable to models trained on 5,000 to 10,000 annotated examples of the new wakeword",
    "checked": true,
    "id": "41d2d8b4ac2c6afa47b5854a65670f082c9c8cc7",
    "semantic_title": "an audio-based wakeword-independent verification system",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2020/vuong20_interspeech.html": {
    "title": "Learnable Spectro-Temporal Receptive Fields for Robust Voice Type Discrimination",
    "volume": "main",
    "abstract": "Voice Type Discrimination (VTD) refers to discrimination between regions in a recording where speech was produced by speakers that are physically within proximity of the recording device (\"Live Speech\") from speech and other types of audio that were played back such as traffic noise and television broadcasts (\"Distractor Audio\"). In this work, we propose a deep-learning-based VTD system that features an initial layer of learnable spectro-temporal receptive fields (STRFs). Our approach is also shown to provide very strong performance on a similar spoofing detection task in the ASVspoof 2019 challenge. We evaluate our approach on a new standardized VTD database that was collected to support research in this area. In particular, we study the effect of using learnable STRFs compared to static STRFs or unconstrained kernels. We also show that our system consistently improves a competitive baseline system across a wide range of signal-to-noise ratios on spoofing detection in the presence of VTD distractor noise",
    "checked": true,
    "id": "697c737f598f2c1ae42ca655dada4f389fed24cc",
    "semantic_title": "learnable spectro-temporal receptive fields for robust voice type discrimination",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chang20b_interspeech.html": {
    "title": "Low Latency Speech Recognition Using End-to-End Prefetching",
    "volume": "main",
    "abstract": "Latency is a crucial metric for streaming speech recognition systems. In this paper, we reduce latency by fetching responses early based on the partial recognition results and refer to it as prefetching. Specifically, prefetching works by submitting partial recognition results for subsequent processing such as obtaining assistant server responses or second-pass rescoring before the recognition result is finalized. If the partial result matches the final recognition result, the early fetched response can be delivered to the user instantly. This effectively speeds up the system by saving the execution latency that typically happens after recognition is completed Prefetching can be triggered multiple times for a single query, but this leads to multiple rounds of downstream processing and increases the computation costs. It is hence desirable to fetch the result sooner but meanwhile limiting the number of prefetches. To achieve the best trade-off between latency and computation cost, we investigated a series of prefetching decision models including decoder silence based prefetching, acoustic silence based prefetching and end-to-end prefetching In this paper, we demonstrate the proposed prefetching mechanism reduced latency by ~200 ms for a system that consists of a streaming first pass model using recurrent neural network transducer and a non-streaming second pass rescoring model using Listen, Attend and Spell. We observe that the end-to-end prefetching provides the best trade-off between cost and latency and is 120 ms faster compared to silence based prefetching at a fixed prefetch rate",
    "checked": true,
    "id": "6387abada387129b43c048c1784f67246005b37c",
    "semantic_title": "low latency speech recognition using end-to-end prefetching",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20t_interspeech.html": {
    "title": "AutoSpeech 2020: The Second Automated Machine Learning Challenge for Speech Classification",
    "volume": "main",
    "abstract": "The AutoSpeech challenge calls for automated machine learning (AutoML) solutions to automate the process of applying machine learning to speech processing tasks. These tasks, which cover a large variety of domains, will be shown to the automated system in a random order. Each time when the tasks are switched, the information of the new task will be hinted with its corresponding training set. Thus, every submitted solution should contain an adaptation routine which adapts the system to the new task. Compared to the first edition, the 2020 edition includes advances of 1) more speech tasks, 2) noisier data in each task, 3) a modified evaluation metric. This paper outlines the challenge and describe the competition protocol, datasets, evaluation metric, starting kit, and baseline systems",
    "checked": true,
    "id": "24844f83f9955e9eb4b96248a74940db04c9fc25",
    "semantic_title": "autospeech 2020: the second automated machine learning challenge for speech classification",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kumar20c_interspeech.html": {
    "title": "Building a Robust Word-Level Wakeword Verification Network",
    "volume": "main",
    "abstract": "Wakeword detection is responsible for switching on downstream systems in a voice-activated device. To prevent a response when the wakeword is detected by mistake, a secondary network is often utilized to verify the detected wakeword. Published verification approaches are formulated based on Automatic Speech Recognition (ASR) biased towards the wakeword. This approach has several drawbacks, including high model complexity and the necessity of large vocabulary training data. To address these shortcomings, we propose to use a large receptive field (LRF) word-level wakeword model, and in particular, a convolutional-recurrent-attention (CRA) network. CRA networks use a strided small receptive field convolutional front-end followed by fixed time-step recurrent layers optimized to model the temporal phonetic dependencies within the wakeword. We experimentally show that this type of modeling helps the system to be robust to errors in the location of the wakeword as estimated by the detection network. The proposed CRA network significantly outperforms previous baselines, including an LRF whole-word convolutional network and a 2-stage DNN-HMM system. Additionally, we study the importance of pre- and post-wakeword context. Finally, the CRA network has significantly fewer model parameters and multiplies, which makes it suitable for real-world production applications",
    "checked": true,
    "id": "1abf805f8e5cc891ebdab9a16aeef5c8250ccc76",
    "semantic_title": "building a robust word-level wakeword verification network",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2020/koizumi20_interspeech.html": {
    "title": "A Transformer-Based Audio Captioning Model with Keyword Estimation",
    "volume": "main",
    "abstract": "One of the problems with automated audio captioning (AAC) is the indeterminacy in word selection corresponding to the audio event/scene. Since one acoustic event/scene can be described with several words, it results in a combinatorial explosion of possible captions and difficulty in training. To solve this problem, we propose a Transformer-based audio-captioning model with keyword estimation called TRACKE. It simultaneously solves the word-selection indeterminacy problem with the main task of AAC while executing the sub-task of acoustic event detection/acoustic scene classification (i.e., keyword estimation). TRACKE estimates keywords, which comprise a word set corresponding to audio events/scenes in the input audio, and generates the caption while referring to the estimated keywords to reduce word-selection indeterminacy. Experimental results on a public AAC dataset indicate that TRACKE achieved state-of-the-art performance and successfully estimated both the caption and its keywords",
    "checked": true,
    "id": "0118c8b047587368b71308f67f2b4d28543ce600",
    "semantic_title": "a transformer-based audio captioning model with keyword estimation",
    "citation_count": 45
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mo20_interspeech.html": {
    "title": "Neural Architecture Search for Keyword Spotting",
    "volume": "main",
    "abstract": "Deep neural networks have recently become a popular solution to keyword spotting systems, which enable the control of smart devices via voice. In this paper, we apply neural architecture search to search for convolutional neural network models that can help boost the performance of keyword spotting based on features extracted from acoustic signals while maintaining an acceptable memory footprint. Specifically, we use differentiable architecture search techniques to search for operators and their connections in a predefined cell search space. The found cells are then scaled up in both depth and width to achieve competitive performance. We evaluated the proposed method on Google's Speech Commands Dataset and achieved a state-of-the-art accuracy of over 97% on the setting of 12-class utterance classification commonly reported in the literature",
    "checked": true,
    "id": "0c19513c71646ddb216ba81841faf8580be7a17f",
    "semantic_title": "neural architecture search for keyword spotting",
    "citation_count": 26
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20s_interspeech.html": {
    "title": "Small-Footprint Keyword Spotting with Multi-Scale Temporal Convolution",
    "volume": "main",
    "abstract": "Keyword Spotting (KWS) plays a vital role in human-computer interaction for smart on-device terminals and service robots. It remains challenging to achieve the trade-off between small footprint and high accuracy for KWS task. In this paper, we explore the application of multi-scale temporal modeling to the small-footprint keyword spotting task. We propose a multi-branch temporal convolution module (MTConv), a CNN block consisting of multiple temporal convolution filters with different kernel sizes, which enriches temporal feature space. Besides, taking advantage of temporal and depthwise convolution, a temporal efficient neural network (TENet) is designed for KWS system Based on the purposed model, we replace standard temporal convolution layers with MTConvs that can be trained for better performance. While at the inference stage, the MTConv can be equivalently converted to the base convolution architecture, so that no extra parameters and computational costs are added compared to the base model. The results on Google Speech Command Dataset show that one of our models trained with MTConv performs the accuracy of 96.8% with only 100K parameters",
    "checked": true,
    "id": "d1c7f128ff75a824953a5987d7793b47c76b5cef",
    "semantic_title": "small-footprint keyword spotting with multi-scale temporal convolution",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20u_interspeech.html": {
    "title": "Using Cyclic Noise as the Source Signal for Neural Source-Filter-Based Speech Waveform Model",
    "volume": "main",
    "abstract": "Neural source-filter (NSF) waveform models generate speech waveforms by morphing sine-based source signals through dilated convolution in the time domain. Although the sine-based source signals help the NSF models to produce voiced sounds with specified pitch, the sine shape may constrain the generated waveform when the target voiced sounds are less periodic. In this paper, we propose a more flexible source signal called cyclic noise, a quasi-periodic noise sequence given by the convolution of a pulse train and a static random noise with a trainable decaying rate that controls the signal shape. We further propose a masked spectral loss to guide the NSF models to produce periodic voiced sounds from the cyclic noise-based source signal. Results from a large-scale listening test demonstrated the effectiveness of the cyclic noise and the masked spectral loss on speaker-independent NSF models in copy-synthesis experiments on the CMU ARCTIC database",
    "checked": true,
    "id": "1143a743d2b14c99ed9dec3c1bca9bf80d03c1af",
    "semantic_title": "using cyclic noise as the source signal for neural source-filter-based speech waveform model",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2020/liu20i_interspeech.html": {
    "title": "Unconditional Audio Generation with Generative Adversarial Networks and Cycle Regularization",
    "volume": "main",
    "abstract": "In a recent paper, we have presented a generative adversarial network (GAN)-based model for unconditional generation of the mel-spectrograms of singing voices. As the generator of the model is designed to take a variable-length sequence of noise vectors as input, it can generate mel-spectrograms of variable length. However, our previous listening test shows that the quality of the generated audio leaves room for improvement. The present paper extends and expands that previous work in the following aspects. First, we employ a hierarchical architecture in the generator to induce some structure in the temporal dimension. Second, we introduce a cycle regularization mechanism to the generator to avoid mode collapse. Third, we evaluate the performance of the new model not only for generating singing voices, but also for generating speech voices. Evaluation result shows that new model outperforms the prior one both objectively and subjectively. We also employ the model to unconditionally generate sequences of piano and violin music and find the result promising. Audio examples, as well as the code for implementing our model, will be publicly available online upon paper publication",
    "checked": true,
    "id": "46a34e4239adfe222bdad453821372904b5d0b40",
    "semantic_title": "unconditional audio generation with generative adversarial networks and cycle regularization",
    "citation_count": 24
  },
  "https://www.isca-speech.org/archive/interspeech_2020/nakashika20_interspeech.html": {
    "title": "Complex-Valued Variational Autoencoder: A Novel Deep Generative Model for Direct Representation of Complex Spectra",
    "volume": "main",
    "abstract": "In recent years, variational autoencoders (VAEs) have been attracting interest for many applications and generative tasks. Although the VAE is one of the most powerful deep generative models, it still has difficulty representing complex-valued data such as the complex spectra of speech. In speech synthesis, we usually use the VAE to encode Mel-cepstra, or raw amplitude spectra, from a speech signal into normally distributed latent features and then synthesize the speech from the reconstruction by using the Griffin-Lim algorithm or other vocoders. Such inputs are originally calculated from complex spectra but lack the phase information, which leads to degradation when recovering speech. In this work, we propose a novel generative model to directly encode the complex spectra by extending the conventional VAE. The proposed model, which we call the complex-valued VAE (CVAE), consists of two complex-valued neural networks (CVNNs) of an encoder and a decoder. In the CVAE, not only the inputs and the parameters of the encoder and decoder but also the latent features are defined as complex-valued to preserve the phase information throughout the network. The results of our speech encoding experiments demonstrated the effectiveness of the CVAE compared to the conventional VAE in both objective and subjective criteria",
    "checked": true,
    "id": "4aecea022f249cf3ff4016f414c3902198b414c0",
    "semantic_title": "complex-valued variational autoencoder: a novel deep generative model for direct representation of complex spectra",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2020/choi20c_interspeech.html": {
    "title": "Attentron: Few-Shot Text-to-Speech Utilizing Attention-Based Variable-Length Embedding",
    "volume": "main",
    "abstract": "On account of growing demands for personalization, the need for a so-called few-shot TTS system that clones speakers with only a few data is emerging. To address this issue, we propose Attentron, a few-shot TTS model that clones voices of speakers unseen during training. It introduces two special encoders, each serving different purposes. A fine-grained encoder extracts variable-length style information via an attention mechanism, and a coarse-grained encoder greatly stabilizes the speech synthesis, circumventing unintelligible gibberish even for synthesizing speech of unseen speakers. In addition, the model can scale out to an arbitrary number of reference audios to improve the quality of the synthesized speech. According to our experiments, including a human evaluation, the proposed model significantly outperforms state-of-the-art models when generating speech for unseen speakers in terms of speaker similarity and quality",
    "checked": true,
    "id": "7850a6c7f9a4119703d2034d1ec336898cc0b449",
    "semantic_title": "attentron: few-shot text-to-speech utilizing attention-based variable-length embedding",
    "citation_count": 43
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ihm20_interspeech.html": {
    "title": "Reformer-TTS: Neural Speech Synthesis with Reformer Network",
    "volume": "main",
    "abstract": "Recent End-to-end text-to-speech (TTS) systems based on the deep neural network (DNN) have shown the state-of-the-art performance on the speech synthesis field. Especially, the attention-based sequence-to-sequence models have improved the quality of the alignment between the text and spectrogram successfully. Leveraging such improvement, speech synthesis using a Transformer network was reported to generate humanlike speech audio. However, such sequence-to-sequence models require intensive computing power and memory during training. The attention scores are calculated over the entire key at every query sequence, which increases memory usage. To mitigate this issue, we propose Reformer-TTS, the model using a Reformer network which utilizes the locality-sensitive hashing attention and the reversible residual network. As a result, we show that the Reformer network consumes almost twice smaller memory margin as the Transformer, which leads to the fast convergence of training end-to-end TTS system. We demonstrate such advantages with memory usage, objective, and subjective performance evaluation",
    "checked": true,
    "id": "fef57f4fba2e4f6b71de44c11995252d01b5406b",
    "semantic_title": "reformer-tts: neural speech synthesis with reformer network",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kaneko20_interspeech.html": {
    "title": "CycleGAN-VC3: Examining and Improving CycleGAN-VCs for Mel-Spectrogram Conversion",
    "volume": "main",
    "abstract": "Non-parallel voice conversion (VC) is a technique for learning mappings between source and target speeches without using a parallel corpus. Recently, cycle-consistent adversarial network (CycleGAN)-VC and CycleGAN-VC2 have shown promising results regarding this problem and have been widely used as benchmark methods. However, owing to the ambiguity of the effectiveness of CycleGAN-VC/VC2 for mel-spectrogram conversion, they are typically used for mel-cepstrum conversion even when comparative methods employ mel-spectrogram as a conversion target. To address this, we examined the applicability of CycleGAN-VC/VC2 to mel-spectrogram conversion. Through initial experiments, we discovered that their direct applications compromised the time-frequency structure that should be preserved during conversion. To remedy this, we propose CycleGAN-VC3, an improvement of CycleGAN-VC2 that incorporates time-frequency adaptive normalization (TFAN). Using TFAN, we can adjust the scale and bias of the converted features while reflecting the time-frequency structure of the source mel-spectrogram. We evaluated CycleGAN-VC3 on inter-gender and intra-gender non-parallel VC. A subjective evaluation of naturalness and similarity showed that for every VC pair, CycleGAN-VC3 outperforms or is competitive with the two types of CycleGAN-VC2, one of which was applied to mel-cepstrum and the other to mel-spectrogram ",
    "checked": true,
    "id": "ea2e1c8c0c27961a8efb8403c6bd0a068054e5f4",
    "semantic_title": "cyclegan-vc3: examining and improving cyclegan-vcs for mel-spectrogram conversion",
    "citation_count": 46
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ellinas20_interspeech.html": {
    "title": "High Quality Streaming Speech Synthesis with Low, Sentence-Length-Independent Latency",
    "volume": "main",
    "abstract": "This paper presents an end-to-end text-to-speech system with low latency on a CPU, suitable for real-time applications. The system is composed of an autoregressive attention-based sequence-to-sequence acoustic model and the LPCNet vocoder for waveform generation. An acoustic model architecture that adopts modules from both the Tacotron 1 and 2 models is proposed, while stability is ensured by using a recently proposed purely location-based attention mechanism, suitable for arbitrary sentence length generation. During inference, the decoder is unrolled and acoustic feature generation is performed in a streaming manner, allowing for a nearly constant latency which is independent from the sentence length. Experimental results show that the acoustic model can produce feature sequences with minimal latency about 31 times faster than real-time on a computer CPU and 6.5 times on a mobile CPU, enabling it to meet the conditions required for real-time applications on both devices. The full end-to-end system can generate almost natural quality speech, which is verified by listening tests",
    "checked": true,
    "id": "b4e3b7161e759430f2d979f041da7d32d2daae44",
    "semantic_title": "high quality streaming speech synthesis with low, sentence-length-independent latency",
    "citation_count": 28
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yu20c_interspeech.html": {
    "title": "DurIAN: Duration Informed Attention Network for Speech Synthesis",
    "volume": "main",
    "abstract": "In this paper, we present a robust and effective speech synthesis system that generates highly natural speech. The key component of proposed system is Duration Informed Attention Network (DurIAN), an autoregressive model in which the alignments between the input text and the output acoustic features are inferred from a duration model. This is different from the attention mechanism used in existing end-to-end speech synthesis systems that accounts for various unavoidable artifacts. To improve the audio generation efficiency of neural vocoders, we also propose a multi-band audio generation framework exploiting the sparseness characteristics of neural network. With proposed multi-band processing framework, the total computational complexity of WaveRNN model can be effectively reduced from 9.8 to 3.6 GFLOPS without any performance loss. Finally, we show that proposed DurIAN system could generate highly natural speech that is on par with current state of the art end-to-end systems, while being robust and stable at the same time",
    "checked": true,
    "id": "b04cd1045a10d054c110aa2e35832545115e4857",
    "semantic_title": "durian: duration informed attention network for speech synthesis",
    "citation_count": 58
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mitsui20_interspeech.html": {
    "title": "Multi-Speaker Text-to-Speech Synthesis Using Deep Gaussian Processes",
    "volume": "main",
    "abstract": "Multi-speaker speech synthesis is a technique for modeling multiple speakers' voices with a single model. Although many approaches using deep neural networks (DNNs) have been proposed, DNNs are prone to overfitting when the amount of training data is limited. We propose a framework for multi-speaker speech synthesis using deep Gaussian processes (DGPs); a DGP is a deep architecture of Bayesian kernel regressions and thus robust to overfitting. In this framework, speaker information is fed to duration/acoustic models using speaker codes. We also examine the use of deep Gaussian process latent variable models (DGPLVMs). In this approach, the representation of each speaker is learned simultaneously with other model parameters, and therefore the similarity or dissimilarity of speakers is considered efficiently. We experimentally evaluated two situations to investigate the effectiveness of the proposed methods. In one situation, the amount of data from each speaker is balanced (speaker-balanced), and in the other, the data from certain speakers are limited (speaker-imbalanced). Subjective and objective evaluation results showed that both the DGP and DGPLVM synthesize multi-speaker speech more effective than a DNN in the speaker-balanced situation. We also found that the DGPLVM outperforms the DGP significantly in the speaker-imbalanced situation",
    "checked": true,
    "id": "730f252b615ce45182f1f168d770ff2ee6a2aa87",
    "semantic_title": "multi-speaker text-to-speech synthesis using deep gaussian processes",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2020/m20_interspeech.html": {
    "title": "A Hybrid HMM-Waveglow Based Text-to-Speech Synthesizer Using Histogram Equalization for Low Resource Indian Languages",
    "volume": "main",
    "abstract": "Conventional text-to-speech (TTS) synthesis requires extensive linguistic processing for producing quality output. The advent of end-to-end (E2E) systems has caused a relocation in the paradigm with better synthesized voices. However, hidden Markov model (HMM) based systems are still popular due to their fast synthesis time, robustness to less training data, and flexible adaptation of voice characteristics, speaking styles, and emotions This paper proposes a technique that combines the classical parametric HMM-based TTS framework (HTS) with the neural-network-based Waveglow vocoder using histogram equalization (HEQ) in a low resource environment. The two paradigms are combined by performing HEQ across mel-spectrograms extracted from HTS generated audio and source spectra of training data. During testing, the synthesized mel-spectrograms are mapped to the source spectrograms using the learned HEQ. Experiments are carried out on Hindi male and female dataset of the Indic TTS database. Systems are evaluated based on degradation mean opinion scores (DMOS). Results indicate that the synthesis quality of the hybrid system is better than that of the conventional HTS system. These results are quite promising as they pave way to good quality TTS systems with less data compared to E2E systems",
    "checked": true,
    "id": "8dba1cc28d408b28c97c8b288079036cdcddba8b",
    "semantic_title": "a hybrid hmm-waveglow based text-to-speech synthesizer using histogram equalization for low resource indian languages",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2020/schuller20_interspeech.html": {
    "title": "The INTERSPEECH 2020 Computational Paralinguistics Challenge: Elderly Emotion, Breathing & Masks",
    "volume": "main",
    "abstract": "The INTERSPEECH 2020 Computational Paralinguistics Challenge addresses three different problems for the first time in a research competition under well-defined conditions: In the Elderly Emotion Sub-Challenge, arousal and valence in the speech of elderly individuals have to be modelled as a 3-class problem; in the Breathing Sub-Challenge, breathing has to be assessed as a regression problem; and in the Mask Sub-Challenge, speech without and with a surgical mask has to be told apart. We describe the Sub-Challenges, baseline feature extraction, and classifiers based on the ‘usual' ComParE and BoAW features as well as deep unsupervised representation learning using the auDeep toolkit, and deep feature extraction from pre-trained CNNs using the Deep Spectrum toolkit; in addition, we partially add deep end-to-end sequential modelling, and, for the first time in the challenge, linguistic analysis",
    "checked": true,
    "id": "58c36e6f26872e4efbcc8b5f88b4ad263f4bb272",
    "semantic_title": "the interspeech 2020 computational paralinguistics challenge: elderly emotion, breathing & masks",
    "citation_count": 95
  },
  "https://www.isca-speech.org/archive/interspeech_2020/koike20_interspeech.html": {
    "title": "Learning Higher Representations from Pre-Trained Deep Models with Data Augmentation for the COMPARE 2020 Challenge Mask Task",
    "volume": "main",
    "abstract": "Human hand-crafted features are always regarded as expensive, time-consuming, and difficult in almost all of the machine-learning-related tasks. First, those well-designed features extremely rely on human expert domain knowledge, which may restrain the collaboration work across fields. Second, the features extracted in such a brute-force scenario may not be easy to be transferred to another task, which means a series of new features should be designed. To this end, we introduce a method based on a transfer learning strategy combined with data augmentation techniques for the ComParE 2020 Challenge Mask Sub-Challenge. Unlike the previous studies mainly based on pre-trained models by image data, we use a pre-trained model based on large scale audio data, i. e., AudioSet. In addition, the SpecAugment and mixup methods are used to improve the generalisation of the deep models. Experimental results demonstrate that the best-proposed model can significantly (p < .001, by one-tailed z-test) improve the unweighted average recall (UAR) from 71.8% (baseline) to 76.2% on the test set. Finally, the best result, i. e., 77.5% of the UAR on the test set, is achieved by a late fusion of the two best proposed models and the best single model in the baseline",
    "checked": true,
    "id": "7f04a361390cedc6139af871ebfa94402f3856b0",
    "semantic_title": "learning higher representations from pre-trained deep models with data augmentation for the compare 2020 challenge mask task",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2020/illium20_interspeech.html": {
    "title": "Surgical Mask Detection with Convolutional Neural Networks and Data Augmentations on Spectrograms",
    "volume": "main",
    "abstract": "In many fields of research, labeled data-sets are hard to acquire. This is where data augmentation promises to overcome the lack of training data in the context of neural network engineering and classification tasks. The idea here is to reduce model over-fitting to the feature distribution of a small under-descriptive training data-set. We try to evaluate such data augmentation techniques to gather insights in the performance boost they provide for several convolutional neural networks on mel-spectrogram representations of audio data. We show the impact of data augmentation on the binary classification task of surgical mask detection in samples of human voice ( ComParE Challenge 2020). Also we consider four varying architectures to account for augmentation robustness. Results show that most of the baselines given by ComParE are outperformed",
    "checked": true,
    "id": "3fbb4189adeab0fc6eb9cea12f87c82e90fd6b30",
    "semantic_title": "surgical mask detection with convolutional neural networks and data augmentations on spectrograms",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2020/klumpp20_interspeech.html": {
    "title": "Surgical Mask Detection with Deep Recurrent Phonetic Models",
    "volume": "main",
    "abstract": "To solve the task of surgical mask detection from audio recordings in the scope of Interspeech's ComParE challenge, we introduce a phonetic recognizer which is able to differentiate between clear and mask samples A deep recurrent phoneme recognition model is first trained on spectrograms from a German corpus to learn the spectral properties of different speech sounds. Under the assumption that each phoneme sounds differently among clear and mask speech, the model is then used to compute frame-wise phonetic labels for the challenge data, including information about the presence of a surgical mask. These labels served to train a second phoneme recognition model which is finally able to differentiate between mask and clear phoneme productions. For a single utterance, we can compute a functional representation and learn a random forest classifier to detect whether a speech sample was produced with or without a mask Our method performed better than the baseline methods on both validation and test set. Furthermore, we could show how wearing a mask influences the speech signal. Certain phoneme groups were clearly affected by the obstruction in front of the vocal tract, while others remained almost unaffected",
    "checked": true,
    "id": "13f92a85ae97100d9e8217e651d6e2013124f51e",
    "semantic_title": "surgical mask detection with deep recurrent phonetic models",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2020/montacie20_interspeech.html": {
    "title": "Phonetic, Frame Clustering and Intelligibility Analyses for the INTERSPEECH 2020 ComParE Challenge",
    "volume": "main",
    "abstract": "The INTERSPEECH 2020 Compare Mask Sub-Challenge is to determine whether a speech signal was emitted with or without wearing a surgical mask. For this purpose, we have investigated phonetic context and intelligibility measurements related to speech changes caused by wearing a mask. Experiments were conducted on the Mask Augsburg Speech Corpus (MASC) and on the Mask Sorbonne Speech Corpus (MSSC) both in German language. We investigated the effects of mask wearing on the acoustical properties of phonemes at frame and segment levels. At the frame level, a phonetic mask detector has been developed to determine the most sensitive phonemes when wearing a mask. At the segmental level, a perceptual scoring of intelligibility has been developed and assessed on the MSCC. Two mask detector systems have been developed and assessed on the MASC: the first one used two large composite audio feature sets, the second one used a bottom-up approach based on phonetic analysis and frame clustering. Experiments have shown an improvement of 5.9% (absolute) on the Test set compared to the official baseline performance of the Challenge (71.8%)",
    "checked": true,
    "id": "e8540798f910087808a30d155919110a6b90391f",
    "semantic_title": "phonetic, frame clustering and intelligibility analyses for the interspeech 2020 compare challenge",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2020/juliao20_interspeech.html": {
    "title": "Exploring Text and Audio Embeddings for Multi-Dimension Elderly Emotion Recognition",
    "volume": "main",
    "abstract": "This paper investigates the use of audio and text embeddings for the classification of emotion dimensions within the scope of the Elderly Emotion Sub-Challenge of the INTERSPEECH 2020 Computational Paralinguistics Challenge. We explore speaker and time dependencies on the expression of emotions through the combination of well-known acoustic-prosodic features and speaker embeddings extracted for different time scales. We consider text information input through transformer language embeddings, both isolated and in combination with acoustic features. The combination of acoustic and text information is explored in early and late fusion schemes. Overall, early fusion of systems trained on top of hand-crafted acoustic-prosodic features (eGeMAPS and ComParE), acoustic model feature embeddings (x-vectors), and text feature embeddings provide the best classification results in development for both Arousal and Valence. The combination of modalities allows us to reach a multi-dimension emotion classification performance in the development challenge data set of up to 48.8% Unweighted Average Recall (UAR) and 61.0% UAR for Arousal and Valence, respectively. These results correspond to a 16.2% and a 8.7% relative UAR improvement",
    "checked": true,
    "id": "51ec44174b1a7b6882dbf1b678e8c6a74cfb48ad",
    "semantic_title": "exploring text and audio embeddings for multi-dimension elderly emotion recognition",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/markitantov20_interspeech.html": {
    "title": "Ensembling End-to-End Deep Models for Computational Paralinguistics Tasks: ComParE 2020 Mask and Breathing Sub-Challenges",
    "volume": "main",
    "abstract": "This paper describes deep learning approaches for the Mask and Breathing Sub-Challenges (SCs), which are addressed by the INTERSPEECH 2020 Computational Paralinguistics Challenge. Motivated by outstanding performance of state-of-the-art end-to-end (E2E) approaches, we explore and compare effectiveness of different deep Convolutional Neural Network (CNN) architectures on raw data, log Mel-spectrograms, and Mel-Frequency Cepstral Coefficients. We apply a transfer learning approach to improve model's efficiency and convergence speed. In the Mask SC, we conduct experiments with several pretrained CNN architectures on log-Mel spectrograms, as well as Support Vector Machines on baseline features. For the Breathing SC, we propose an ensemble deep learning system that exploits E2E learning and sequence prediction. The E2E model is based on 1D CNN operating on raw speech signals and is coupled with Long Short-Term Memory layers for sequence modeling. The second model works with log-Mel features and is based on a pretrained 2D CNN model stacked to Gated Recurrent Unit layers. To increase performance of our models in both SCs, we use ensembles of the best deep neural models obtained from N-fold cross-validation on combined challenge training and development datasets. Our results markedly outperform the challenge test set baselines in both SCs",
    "checked": true,
    "id": "2ba35af479392c1124034f55a819875a2a3a0374",
    "semantic_title": "ensembling end-to-end deep models for computational paralinguistics tasks: compare 2020 mask and breathing sub-challenges",
    "citation_count": 25
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mendonca20_interspeech.html": {
    "title": "Analyzing Breath Signals for the Interspeech 2020 ComParE Challenge",
    "volume": "main",
    "abstract": "This paper presents our contribution to the INTERSPEECH 2020 Breathing Sub-challenge. Besides fulfilling the main goal of the challenge, which involves the automatic prediction from conversational speech of the breath signals obtained from respiratory belts, we also analyse both original and predicted signals in an attempt to overcome the main pitfalls of the proposed systems. In particular, we identify the subsets of most irregular belt signals which yield the worst performance, measured by the Pearson correlation coefficient, and show how they affect the results that were obtained by both the baseline end-to-end system and variants such as a Bidirectional LSTM. The performance of this type of architecture indicates that future information is also relevant when predicting breathing patterns We also study the information retained from the AM-FM decomposition of the speech signal for this purpose, showing how the AM component significantly outperforms the FM component on all experiments, but fails to surpass the prediction results obtained using the original speech signal Finally, we validate the system's performance in video-conferencing conditions by using data augmentation and compare clinically relevant parameters, such as breathing rate, from both the original belt signals and the ones predicted from the simulated video-conferencing signals",
    "checked": true,
    "id": "b6289cd5069b2a901d8715c21f65a10918cf760f",
    "semantic_title": "analyzing breath signals for the interspeech 2020 compare challenge",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2020/macintyre20_interspeech.html": {
    "title": "Deep Attentive End-to-End Continuous Breath Sensing from Speech",
    "volume": "main",
    "abstract": "Modelling of the breath signal is of high interest to both healthcare professionals and computer scientists, as a source of diagnosis-related information, or a means for curating higher quality datasets in speech analysis research. The formation of a breath signal gold standard is, however, not a straightforward task, as it requires specialised equipment, human annotation budget, and even then, it corresponds to lab recording settings, that are not reproducible in-the-wild. Herein, we explore deep learning based methodologies, as an automatic way to predict a continuous-time breath signal by solely analysing spontaneous speech. We address two task formulations, those of continuous-valued signal prediction, as well as inhalation event prediction, that are of great use in various healthcare and Automatic Speech Recognition applications, and showcase results that outperform current baselines. Most importantly, we also perform an initial exploration into explaining which parts of the input audio signal are important with respect to the prediction",
    "checked": true,
    "id": "a586ff83860864760f33143623e2c830a4602c20",
    "semantic_title": "deep attentive end-to-end continuous breath sensing from speech",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2020/szep20_interspeech.html": {
    "title": "Paralinguistic Classification of Mask Wearing by Image Classifiers and Fusion",
    "volume": "main",
    "abstract": "In this study, we address the ComParE 2020 Paralinguistics Mask sub-challenge, where the task is the detection of wearing surgical masks from short speech segments. In our approach, we propose a computer-vision-based pipeline to utilize the capabilities of deep convolutional neural network-based image classifiers developed in recent years and apply this technology to a specific class of spectrograms. Several linear and logarithmic scale spectrograms were tested, and the best performance is achieved on linear-scale, 3-Channel Spectrograms created from the audio segments. A single model image classifier provided a 6.1% better result than the best single-dataset baseline model. The ensemble of our models further improves accuracy and achieves 73.0% UAR by training just on the ‘train' dataset and reaches 80.1% UAR on the test set when training includes the ‘devel' dataset, which result is 8.3% higher than the baseline. We also provide an activation-mapping analysis to identify frequency ranges that are critical in the ‘mask' versus ‘clear' classification",
    "checked": true,
    "id": "78cf1ef35c4487631e2408342f7e2308533a2fe6",
    "semantic_title": "paralinguistic classification of mask wearing by image classifiers and fusion",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yang20c_interspeech.html": {
    "title": "Exploration of Acoustic and Lexical Cues for the INTERSPEECH 2020 Computational Paralinguistic Challenge",
    "volume": "main",
    "abstract": "In this paper, we investigate various acoustic features and lexical features for the INTERSPEECH 2020 Computational Paralinguistic Challenge. For the acoustic analysis, we show that the proposed FV-MFCC feature is very promising, which has very strong prediction power on its own, and can also provide complementary information when fused with other acoustic features. For the lexical representation, we find that the corpus-dependent TF.IDF feature is by far the best representation. We also explore several model fusion techniques to combine different modalities together, and propose novel SVM models to aggregate the chunk-level predictions to the narrative-level predictions based on the chunk-level decision functionals. Finally we discuss the potential for improving prediction by combining the lexical and acoustic modalities together, and we find that fusion of lexical and acoustic modalities do not lead to consistent improvements over elderly Arousal, but substantially improve over the Valence. Our methods significantly outperform the official baselines on the test set in the participated Mask and Elderly Sub-challenges. We obtain an UAR of 75.1%, 54.3%, and 59.0% on the Mask, Elderly Arousal and Valence prediction tasks respectively",
    "checked": true,
    "id": "d51645adc35bbac7ecbd0d2a82a215272bcbfe40",
    "semantic_title": "exploration of acoustic and lexical cues for the interspeech 2020 computational paralinguistic challenge",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2020/sogancoglu20_interspeech.html": {
    "title": "Is Everything Fine, Grandma? Acoustic and Linguistic Modeling for Robust Elderly Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Acoustic and linguistic analysis for elderly emotion recognition is an under-studied and challenging research direction, but essential for the creation of digital assistants for the elderly, as well as unobtrusive telemonitoring of elderly in their residences for mental healthcare purposes. This paper presents our contribution to the INTERSPEECH 2020 Computational Paralinguistics Challenge (ComParE) - Elderly Emotion Sub-Challenge, which is comprised of two ternary classification tasks for arousal and valence recognition. We propose a bi-modal framework, where these tasks are modeled using state-of-the-art acoustic and linguistic features, respectively. In this study, we demonstrate that exploiting task-specific dictionaries and resources can boost the performance of linguistic models, when the amount of labeled data is small. Observing a high mismatch between development and test set performances of various models, we also propose alternative training and decision fusion strategies to better estimate and improve the generalization performance",
    "checked": true,
    "id": "2179a21ad2d770c4fff0e2bfcc2575751de74e38",
    "semantic_title": "is everything fine, grandma? acoustic and linguistic modeling for robust elderly speech emotion recognition",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ristea20_interspeech.html": {
    "title": "Are you Wearing a Mask? Improving Mask Detection from Speech Using Augmentation by Cycle-Consistent GANs",
    "volume": "main",
    "abstract": "The task of detecting whether a person wears a face mask from speech is useful in modelling speech in forensic investigations, communication between surgeons or people protecting themselves against infectious diseases such as COVID-19. In this paper, we propose a novel data augmentation approach for mask detection from speech. Our approach is based on (i) training Generative Adversarial Networks (GANs) with cycle-consistency loss to translate unpaired utterances between two classes (with mask and without mask), and on (ii) generating new training utterances using the cycle-consistent GANs, assigning opposite labels to each translated utterance. Original and translated utterances are converted into spectrograms which are provided as input to a set of ResNet neural networks with various depths. The networks are combined into an ensemble through a Support Vector Machines (SVM) classifier. With this system, we participated in the Mask Sub-Challenge (MSC) of the INTERSPEECH 2020 Computational Paralinguistics Challenge, surpassing the baseline proposed by the organizers by 2.8%. Our data augmentation technique provided a performance boost of 0.9% on the private test set. Furthermore, we show that our data augmentation approach yields better results than other baseline and state-of-the-art augmentation methods",
    "checked": true,
    "id": "5922cd54b5278a665686ee52b158aef6cecbe2ff",
    "semantic_title": "are you wearing a mask? improving mask detection from speech using augmentation by cycle-consistent gans",
    "citation_count": 33
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kumar20d_interspeech.html": {
    "title": "1-D Row-Convolution LSTM: Fast Streaming ASR at Accuracy Parity with LC-BLSTM",
    "volume": "main",
    "abstract": "In this work we develop a simple, efficient, and compact automatic speech recognition (ASR) model based on purely 1-dimensional row convolution (RC) operation. We refer to our proposed model as 1-dim row-convolution LSTM (RC-LSTM), where we embed limited future information to standard UniLSTMs in 1-dim RC operation. We target fast streaming ASR solutions and establish ASR accuracy parity with latency-control bidirectional-LSTM (LC-BLSTM). We develop an application of future information at ASR features and hidden layer stages. We study connections with related techniques, analyze tradeoffs and recommend uniform future lookahead to all hidden layers. We argue that our architecture implicitly factorizes training into orthogonal time and \"frequency\" dimensions for an effective learning on large scale tasks. We conduct a series of experiments on medium scale with 6k hrs of English corpus, as well as, large scale with 60k hrs training. We demonstrate our findings across unified ASR tasks. Compared to UniLSTM model, RC-LSTM achieved 16% relative reduction in word error rate (WER). RC-LSTM also achieved accuracy parity with LC-BLSTM on large scale tasks at significantly lower latency and computational cost",
    "checked": true,
    "id": "ee2253ae15ce4bcd7c824954710213fede024636",
    "semantic_title": "1-d row-convolution lstm: fast streaming asr at accuracy parity with lc-blstm",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20v_interspeech.html": {
    "title": "Low Latency End-to-End Streaming Speech Recognition with a Scout Network",
    "volume": "main",
    "abstract": "The attention-based Transformer model has achieved promising results for speech recognition (SR) in the offline mode. However, in the streaming mode, the Transformer model usually incurs significant latency to maintain its recognition accuracy when applying a fixed-length look-ahead window in each encoder layer. In this paper, we propose a novel low-latency streaming approach for Transformer models, which consists of a scout network and a recognition network. The scout network detects the whole word boundary without seeing any future frames, while the recognition network predicts the next subword by utilizing the information from all the frames before the predicted boundary. Our model achieves the best performance (2.7/6.4 WER) with only an average of 639 ms latency on the test-clean and test-other data sets of Librispeech",
    "checked": true,
    "id": "65825e1158d9c5925f0fe865fc46c3de7b456eb1",
    "semantic_title": "low latency end-to-end streaming speech recognition with a scout network",
    "citation_count": 33
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kurata20_interspeech.html": {
    "title": "Knowledge Distillation from Offline to Streaming RNN Transducer for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "End-to-end training of recurrent neural network transducers (RNN-Ts) does not require frame-level alignments between audio and output symbols. Because of that, the posterior lattices defined by the predictive distributions from different RNN-Ts trained on the same data can differ a lot, which poses a new set of challenges in knowledge distillation between such models. These discrepancies are especially prominent in the posterior lattices between an offline model and a streaming model, which can be expected from the fact that the streaming RNN-T emits symbols later than the offline RNN-T. We propose a method to train an RNN-T so that the posterior peaks at each node in the posterior lattice are aligned with the ones from a pretrained model for the same utterance. By utilizing this method, we can train an offline RNN-T that can serve as a good teacher to train a student streaming RNN-T. Experimental results on the standard Switchboard conversational telephone speech corpus demonstrate accuracy improvements for a streaming unidirectional RNN-T by knowledge distillation from an offline bidirectional counterpart",
    "checked": true,
    "id": "6188ed77f181a0d9a101bbe2c79157478dc749b4",
    "semantic_title": "knowledge distillation from offline to streaming rnn transducer for end-to-end speech recognition",
    "citation_count": 39
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20t_interspeech.html": {
    "title": "Parallel Rescoring with Transformer for Streaming On-Device Speech Recognition",
    "volume": "main",
    "abstract": "Recent advances of end-to-end models have outperformed conventional models through employing a two-pass model. The two-pass model provides better speed-quality trade-offs for on-device speech recognition, where a 1 st-pass model generates hypotheses in a streaming fashion, and a 2 nd-pass model rescores the hypotheses with full audio sequence context. The 2 nd-pass model plays a key role in the quality improvement of the end-to-end model to surpass the conventional model. One main challenge of the two-pass model is the computation latency introduced by the 2 nd-pass model. Specifically, the original design of the two-pass model uses LSTMs for the 2 nd-pass model, which are subject to long latency as they are constrained by the recurrent nature and have to run inference sequentially. In this work we explore replacing the LSTM layers in the 2 nd-pass rescorer with Transformer layers, which can process the entire hypothesis sequences in parallel and can therefore utilize the on-device computation resources more efficiently. Compared with an LSTM-based baseline, our proposed Transformer rescorer achieves more than 50% latency reduction with quality improvement",
    "checked": true,
    "id": "ae1110a36ff42ead2634b4be9126a5716a4eb96d",
    "semantic_title": "parallel rescoring with transformer for streaming on-device speech recognition",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2020/baqueroarnal20_interspeech.html": {
    "title": "Improved Hybrid Streaming ASR with Transformer Language Models",
    "volume": "main",
    "abstract": "Streaming ASR is gaining momentum due to its wide applicability, though it is still unclear how best to come close to the accuracy of state-of-the-art off-line ASR systems when the output must come within a short delay after the incoming audio stream. Following our previous work on streaming one-pass decoding with hybrid ASR systems and LSTM language models, in this work we report further improvements by replacing LSTMs with Transformer models. First, two key ideas are discussed so as to run these models fast during inference. Then, empirical results on LibriSpeech and TED-LIUM are provided showing that Transformer language models lead to improved recognition rates on both tasks. ASR systems obtained in this work can be seamlessly transferred to a streaming setup with minimal quality losses. Indeed, to the best of our knowledge, no better results have been reported on these tasks when assessed under a streaming setup",
    "checked": true,
    "id": "8db532fe1ae4fbaec64fa2ec433d216d78d34b47",
    "semantic_title": "improved hybrid streaming asr with transformer language models",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wu20i_interspeech.html": {
    "title": "Streaming Transformer-Based Acoustic Models Using Self-Attention with Augmented Memory",
    "volume": "main",
    "abstract": "Transformer-based acoustic modeling has achieved great success for both hybrid and sequence-to-sequence speech recognition. However, it requires access to the full sequence, and the computational cost grows quadratically with respect to the input sequence length. These factors limit its adoption for streaming applications. In this work, we proposed a novel augmented memory self-attention, which attends on a short segment of the input sequence and a bank of memories. The memory bank stores the embedding information for all the processed segments. On the librispeech benchmark, our proposed method outperforms all the existing streamable transformer methods by a large margin and achieved over 15% relative error reduction, compared with the widely used LC-BLSTM baseline. Our findings are also confirmed on some large internal datasets",
    "checked": true,
    "id": "768e5f9b019c27babbfaf817a5bb20316b9df113",
    "semantic_title": "streaming transformer-based acoustic models using self-attention with augmented memory",
    "citation_count": 46
  },
  "https://www.isca-speech.org/archive/interspeech_2020/inaguma20b_interspeech.html": {
    "title": "Enhancing Monotonic Multihead Attention for Streaming ASR",
    "volume": "main",
    "abstract": "We investigate a monotonic multihead attention (MMA) by extending hard monotonic attention to Transformer-based automatic speech recognition (ASR) for online streaming applications. For streaming inference, all monotonic attention (MA) heads should learn proper alignments because the next token is not generated until all heads detect the corresponding token boundaries. However, we found not all MA heads learn alignments with a naïve implementation. To encourage every head to learn alignments properly, we propose HeadDrop regularization by masking out a part of heads stochastically during training. Furthermore, we propose to prune redundant heads to improve consensus among heads for boundary detection and prevent delayed token generation caused by such heads. Chunkwise attention on each MA head is extended to the multihead counterpart. Finally, we propose head-synchronous beam search decoding to guarantee stable streaming inference",
    "checked": true,
    "id": "16a7af4333f7f8d71e036d2671dbde9827efe943",
    "semantic_title": "enhancing monotonic multihead attention for streaming asr",
    "citation_count": 24
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20q_interspeech.html": {
    "title": "Streaming Chunk-Aware Multihead Attention for Online End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "Recently, streaming end-to-end automatic speech recognition (E2E-ASR) has gained more and more attention. Many efforts have been paid to turn the non-streaming attention-based E2E-ASR system into streaming architecture. In this work, we propose a novel online E2E-ASR system by using Streaming Chunk-Aware Multihead Attention (SCAMA) and a latency control memory equipped self-attention network (LC-SAN-M). LC-SAN-M uses chunk-level input to control the latency of encoder. As to SCAMA, a jointly trained predictor is used to control the output of encoder when feeding to decoder, which enables decoder to generate output in streaming manner. Experimental results on the open 170-hour AISHELL-1 and an industrial-level 20000-hour Mandarin speech recognition tasks show that our approach can significantly outperform the MoChA-based baseline system under comparable setup. On the AISHELL-1 task, our proposed method achieves a character error rate (CER) of 7.39%, to the best of our knowledge, which is the best published performance for online ASR",
    "checked": true,
    "id": "35de6075abf76e6e83101d2dbbe8270ee79a8dcd",
    "semantic_title": "streaming chunk-aware multihead attention for online end-to-end speech recognition",
    "citation_count": 24
  },
  "https://www.isca-speech.org/archive/interspeech_2020/nguyen20b_interspeech.html": {
    "title": "High Performance Sequence-to-Sequence Model for Streaming Speech Recognition",
    "volume": "main",
    "abstract": "Recently sequence-to-sequence models have started to achieve state-of-the-art performance on standard speech recognition tasks when processing audio data in batch mode, i.e., the complete audio data is available when starting processing. However, when it comes to performing run-on recognition on an input stream of audio data while producing recognition results in real-time and with low word-based latency, these models face several challenges. For many techniques, the whole audio sequence to be decoded needs to be available at the start of the processing, e.g., for the attention mechanism or the bidirectional LSTM (BLSTM). In this paper, we propose several techniques to mitigate these problems. We introduce an additional loss function controlling the uncertainty of the attention mechanism, a modified beam search identifying partial, stable hypotheses, ways of working with BLSTM in the encoder, and the use of chunked BLSTM. Our experiments show that with the right combination of these techniques, it is possible to perform run-on speech recognition with low word-based latency without sacrificing in word error rate performance",
    "checked": true,
    "id": "3ccd3f6adaaac836f5be8720296ea9bbc2ba9dc8",
    "semantic_title": "high performance sequence-to-sequence model for streaming speech recognition",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2020/joshi20_interspeech.html": {
    "title": "Transfer Learning Approaches for Streaming End-to-End Speech Recognition System",
    "volume": "main",
    "abstract": "Transfer learning (TL) is widely used in conventional hybrid automatic speech recognition (ASR) system, to transfer the knowledge from source to target language. TL can be applied to end-to-end (E2E) ASR system such as recurrent neural network transducer (RNN-T) models, by initializing the encoder and/or prediction network of the target language with the pre-trained models from source language. In the hybrid ASR system, transfer learning is typically done by initializing the target language acoustic model (AM) with source language AM. Several transfer learning strategies exist in the case of the RNN-T framework, depending upon the choice of the initialization model for encoder and prediction networks. This paper presents a comparative study of four different TL methods for RNN-T framework. We show 10%–17% relative word error rate reduction with different TL methods over randomly initialized RNN-T model. We also study the impact of TL with varying amount of training data ranging from 50 hours to 1000 hours and show the efficacy of TL for languages with a very small amount of training data",
    "checked": true,
    "id": "d592e8ca7ad6096a7c0d590f0602ebee8b70a197",
    "semantic_title": "transfer learning approaches for streaming end-to-end speech recognition system",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2020/martinc20_interspeech.html": {
    "title": "Tackling the ADReSS Challenge: A Multimodal Approach to the Automated Recognition of Alzheimer's Dementia",
    "volume": "main",
    "abstract": "The paper describes a multimodal approach to the automated recognition of Alzheimer's dementia in order to solve the ADReSS (Alzheimer's Dementia Recognition through Spontaneous Speech) challenge at INTERSPEECH 2020. The proposed method exploits available audio and textual data from the benchmark speech dataset to address challenge's two subtasks, a classification task that deals with classifying speech as dementia or healthy control speech and the regression task of determining the mini-mental state examination scores (MMSE) for each speech segment. Our approach is based on evaluating the predictive power of different types of features and on an exhaustive grid search across several feature combinations and different classification algorithms. Results suggest that even though TF-IDF based textual features generally lead to better classification and regression results, specific types of audio and readability features can boost the overall performance of the classification and regression models",
    "checked": true,
    "id": "4d7b5d75f809885621c5e1df3f2983f1e00f5e46",
    "semantic_title": "tackling the adress challenge: a multimodal approach to the automated recognition of alzheimer's dementia",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yuan20_interspeech.html": {
    "title": "Disfluencies and Fine-Tuning Pre-Trained Language Models for Detection of Alzheimer's Disease",
    "volume": "main",
    "abstract": "Disfluencies and language problems in Alzheimer's Disease can be naturally modeled by fine-tuning Transformer-based pre-trained language models such as BERT and ERNIE. Using this method, we achieved 89.6% accuracy on the test set of the ADReSS (Alzheimer's Dementia Recognition through Spontaneous Speech) Challenge, a considerable improvement over the baseline of 75.0%, established by the organizers of the challenge. The best accuracy was obtained with ERNIE, plus an encoding of pauses. Robustness is a challenge for large models and small training sets. Ensemble over many runs of BERT/ERNIE fine-tuning reduced variance and improved accuracy. We found that um was used much less frequently in Alzheimer's speech, compared to uh. We discussed this interesting finding from linguistic and cognitive perspectives",
    "checked": true,
    "id": "134d608f4e78da82c9c3c119f57cbde32e220f5e",
    "semantic_title": "disfluencies and fine-tuning pre-trained language models for detection of alzheimer's disease",
    "citation_count": 59
  },
  "https://www.isca-speech.org/archive/interspeech_2020/balagopalan20_interspeech.html": {
    "title": "To BERT or not to BERT: Comparing Speech and Language-Based Approaches for Alzheimer's Disease Detection",
    "volume": "main",
    "abstract": "Research related to automatically detecting Alzheimer's disease (AD) is important, given the high prevalence of AD and the high cost of traditional methods. Since AD significantly affects the content and acoustics of spontaneous speech, natural language processing and machine learning provide promising techniques for reliably detecting AD. We compare and contrast the performance of two such approaches for AD detection on the recent ADReSS challenge dataset [1]: 1) using domain knowledge-based hand-crafted features that capture linguistic and acoustic phenomena, and 2) fine-tuning Bidirectional Encoder Representations from Transformer (BERT)-based sequence classification models. We also compare multiple feature-based regression models for a neuropsychological score task in the challenge. We observe that fine-tuned BERT models, given the relative importance of linguistics in cognitive impairment detection, outperform feature-based approaches on the AD detection task",
    "checked": true,
    "id": "00c5abdffe51ab33e745e6804d4821ca59db52d8",
    "semantic_title": "to bert or not to bert: comparing speech and language-based approaches for alzheimer's disease detection",
    "citation_count": 75
  },
  "https://www.isca-speech.org/archive/interspeech_2020/luz20_interspeech.html": {
    "title": "Alzheimer's Dementia Recognition Through Spontaneous Speech: The ADReSS Challenge",
    "volume": "main",
    "abstract": "The ADReSS Challenge at INTERSPEECH 2020 defines a shared task through which different approaches to the automated recognition of Alzheimer's dementia based on spontaneous speech can be compared. ADReSS provides researchers with a benchmark speech dataset which has been acoustically pre-processed and balanced in terms of age and gender, defining two cognitive assessment tasks, namely: the Alzheimer's speech classification task and the neuropsychological score regression task. In the Alzheimer's speech classification task, ADReSS challenge participants create models for classifying speech as dementia or healthy control speech. In the neuropsychological score regression task, participants create models to predict mini-mental state examination scores. This paper describes the ADReSS Challenge in detail and presents a baseline for both tasks, including feature extraction procedures and results for classification and regression models. ADReSS aims to provide the speech and language Alzheimer's research community with a platform for comprehensive methodological comparisons. This will hopefully contribute to addressing the lack of standardisation that currently affects the field and shed light on avenues for future research and clinical applicability",
    "checked": true,
    "id": "040af9f95aa39415e374e9671b9b7c73c8e5b499",
    "semantic_title": "alzheimer's dementia recognition through spontaneous speech: the adress challenge",
    "citation_count": 165
  },
  "https://www.isca-speech.org/archive/interspeech_2020/pappagari20_interspeech.html": {
    "title": "Using State of the Art Speaker Recognition and Natural Language Processing Technologies to Detect Alzheimer's Disease and Assess its Severity",
    "volume": "main",
    "abstract": "In this study, we analyze the use of state-of-the-art technologies for speaker recognition and natural language processing to detect Alzheimer's Disease (AD) and to assess its severity predicting Mini-mental status evaluation (MMSE) scores. With these purposes, we study the use of speech signals and transcriptions. Our work focuses on the adaptation of state-of-the-art models for both modalities individually and together to examine its complementarity. We used x-vectors to characterize speech signals and pre-trained BERT models to process human transcriptions with different back-ends in AD diagnosis and assessment. We evaluated features based on silence segments of the audio files as a complement to x-vectors. We trained and evaluated our systems in the Interspeech 2020 ADReSS challenge dataset, containing 78 AD patients and 78 sex and age-matched controls. Our results indicate that the fusion of scores obtained from the acoustic and the transcript-based models provides the best detection and assessment results, suggesting that individual models for two modalities contain complementary information. The addition of the silence-related features improved the fusion system even further. A separate analysis of the models suggests that transcript-based models provide better results than acoustic models in the detection task but similar results in the MMSE prediction task",
    "checked": true,
    "id": "4c25acf91e0b0b475e69cb9ab9f0041d16bc7c7d",
    "semantic_title": "using state of the art speaker recognition and natural language processing technologies to detect alzheimer's disease and assess its severity",
    "citation_count": 45
  },
  "https://www.isca-speech.org/archive/interspeech_2020/cummins20_interspeech.html": {
    "title": "A Comparison of Acoustic and Linguistics Methodologies for Alzheimer's Dementia Recognition",
    "volume": "main",
    "abstract": "In the light of the current COVID-19 pandemic, the need for remote digital health assessment tools is greater than ever. This statement is especially pertinent for elderly and vulnerable populations. In this regard, the INTERSPEECH 2020 Alzheimer's Dementia Recognition through Spontaneous Speech (ADReSS) Challenge offers competitors the opportunity to develop speech and language-based systems for the task of Alzheimer's Dementia (AD) recognition. The challenge data consists of speech recordings and their transcripts, the work presented herein is an assessment of different contemporary approaches on these modalities. Specifically, we compared a hierarchical neural network with an attention mechanism trained on linguistic features with three acoustic-based systems: (i) Bag-of-Audio-Words (BoAW) quantising different low-level descriptors, (ii) a Siamese Network trained on log-Mel spectrograms, and (iii) a Convolutional Neural Network (CNN) end-to-end system trained on raw waveforms. Key results indicate the strength of the linguistic approach over the acoustics systems. Our strongest test-set result was achieved using a late fusion combination of BoAW, End-to-End CNN, and hierarchical-attention networks, which outperformed the challenge baseline in both the classification and regression tasks",
    "checked": true,
    "id": "93bdefc9d8feccdef5ff1396fd3c117968899794",
    "semantic_title": "a comparison of acoustic and linguistics methodologies for alzheimer's dementia recognition",
    "citation_count": 37
  },
  "https://www.isca-speech.org/archive/interspeech_2020/rohanian20_interspeech.html": {
    "title": "Multi-Modal Fusion with Gating Using Audio, Lexical and Disfluency Features for Alzheimer's Dementia Recognition from Spontaneous Speech",
    "volume": "main",
    "abstract": "This paper is a submission to the Alzheimer's Dementia Recognition through Spontaneous Speech (ADReSS) challenge, which aims to develop methods that can assist in the automated prediction of severity of Alzheimer's Disease from speech data. We focus on acoustic and natural language features for cognitive impairment detection in spontaneous speech in the context of Alzheimer's Disease Diagnosis and the mini-mental state examination (MMSE) score prediction. We proposed a model that obtains unimodal decisions from different LSTMs, one for each modality of text and audio, and then combines them using a gating mechanism for the final prediction. We focused on sequential modelling of text and audio and investigated whether the disfluencies present in individuals' speech relate to the extent of their cognitive impairment. Our results show that the proposed classification and regression schemes obtain very promising results on both development and test sets. This suggests Alzheimer's Disease can be detected successfully with sequence modeling of the speech data of medical sessions",
    "checked": true,
    "id": "c77d14e649bc2a6a421676dc36c44da26a400f66",
    "semantic_title": "multi-modal fusion with gating using audio, lexical and disfluency features for alzheimer's dementia recognition from spontaneous speech",
    "citation_count": 42
  },
  "https://www.isca-speech.org/archive/interspeech_2020/searle20_interspeech.html": {
    "title": "Comparing Natural Language Processing Techniques for Alzheimer's Dementia Prediction in Spontaneous Speech",
    "volume": "main",
    "abstract": "Alzheimer's Dementia (AD) is an incurable, debilitating, and progressive neurodegenerative condition that affects cognitive function. Early diagnosis is important as therapeutics can delay progression and give those diagnosed vital time. Developing models that analyse spontaneous speech could eventually provide an efficient diagnostic modality for earlier diagnosis of AD. The Alzheimer's Dementia Recognition through Spontaneous Speech task offers acoustically pre-processed and balanced datasets for the classification and prediction of AD and associated phenotypes through the modelling of spontaneous speech. We exclusively analyse the supplied textual transcripts of the spontaneous speech dataset, building and comparing performance across numerous models for the classification of AD vs controls and the prediction of Mental Mini State Exam scores. We rigorously train and evaluate Support Vector Machines (SVMs), Gradient Boosting Decision Trees (GBDT), and Conditional Random Fields (CRFs) alongside deep learning Transformer based models. We find our top performing models to be a simple Term Frequency-Inverse Document Frequency (TF-IDF) vectoriser as input into a SVM model and a pre-trained Transformer based model ‘DistilBERT' when used as an embedding layer into simple linear models. We demonstrate test set scores of 0.81–0.82 across classification metrics and a RMSE of 4.58",
    "checked": true,
    "id": "3f424216d8a086defd73da2432f181ef88d8674b",
    "semantic_title": "comparing natural language processing techniques for alzheimer's dementia prediction in spontaneous speech",
    "citation_count": 25
  },
  "https://www.isca-speech.org/archive/interspeech_2020/edwards20_interspeech.html": {
    "title": "Multiscale System for Alzheimer's Dementia Recognition Through Spontaneous Speech",
    "volume": "main",
    "abstract": "This paper describes the Verisk submission to The ADReSS Challenge [1]. We analyze the text data at both the word level and phoneme level, which leads to our best-performing system in combination with audio features. Thus, the system is both multi-modal (audio and text) and multi-scale (word and phoneme levels). Experiments with larger neural language models did not result in improvement, given the small amount of text data available. By contrast, the phoneme representation has a vocabulary size of only 66 tokens and could be trained from scratch on the present data. Therefore, we believe this method to be useful in cases of limited text data, as in many medical settings",
    "checked": true,
    "id": "1fe4906d57dab8d93d5695533ba6e5eb0facbec8",
    "semantic_title": "multiscale system for alzheimer's dementia recognition through spontaneous speech",
    "citation_count": 27
  },
  "https://www.isca-speech.org/archive/interspeech_2020/pompili20_interspeech.html": {
    "title": "The INESC-ID Multi-Modal System for the ADReSS 2020 Challenge",
    "volume": "main",
    "abstract": "This paper describes a multi-modal approach for the automatic detection of Alzheimer's disease proposed in the context of the INESC-ID Human Language Technology Laboratory participation in the ADReSS 2020 challenge. Our classification framework takes advantage of both acoustic and textual feature embeddings, which are extracted independently and later combined. Speech signals are encoded into acoustic features using DNN speaker embeddings extracted from pre-trained models. For textual input, contextual embedding vectors are first extracted using an English Bert model and then used either to directly compute sentence embeddings or to feed a bidirectional LSTM-RNNs with attention. Finally, an SVM classifier with linear kernel is used for the individual evaluation of the three systems. Our best system, based on the combination of linguistic and acoustic information, attained a classification accuracy of 81.25%. Results have shown the importance of linguistic features in the classification of Alzheimer's Disease, which outperforms the acoustic ones in terms of accuracy. Early stage features fusion did not provide additional improvements, confirming that the discriminant ability conveyed by speech in this case is smooth out by linguistic data",
    "checked": true,
    "id": "8de5ba9bc1c66f8be5162bbde1507dcf76d61801",
    "semantic_title": "the inesc-id multi-modal system for the adress 2020 challenge",
    "citation_count": 30
  },
  "https://www.isca-speech.org/archive/interspeech_2020/farzana20_interspeech.html": {
    "title": "Exploring MMSE Score Prediction Using Verbal and Non-Verbal Cues",
    "volume": "main",
    "abstract": "The Mini Mental State Examination (MMSE) is a standardized cognitive health screening test. It is generally administered by trained clinicians, which may be time-consuming and costly. An intriguing and scalable alternative is to detect changes in cognitive function by automatically monitoring individuals' memory and language abilities from their conversational narratives. We work towards doing so by predicting clinical MMSE scores using verbal and non-verbal features extracted from the transcripts of 108 speech samples from the ADReSS Challenge dataset. We achieve a Root Mean Squared Error (RMSE) of 4.34, a percentage decrease of 29.3% over the existing performance benchmark. We also explore the performance impacts of acoustic versus linguistic, text-based features and find that linguistic features achieve lower RMSE scores, providing strong positive support for their inclusion in future MMSE score prediction models. Our best-performing model leverages a selection of verbal and non-verbal cues, demonstrating that MMSE score prediction is a rich problem that is best addressed using input from multiple perspectives",
    "checked": true,
    "id": "e1136fe5a127f88e4195ea01090f847dccdde675",
    "semantic_title": "exploring mmse score prediction using verbal and non-verbal cues",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2020/sarawgi20_interspeech.html": {
    "title": "Multimodal Inductive Transfer Learning for Detection of Alzheimer's Dementia and its Severity",
    "volume": "main",
    "abstract": "Alzheimer's disease is estimated to affect around 50 million people worldwide and is rising rapidly, with a global economic burden of nearly a trillion dollars. This calls for scalable, cost-effective, and robust methods for detection of Alzheimer's dementia (AD). We present a novel architecture that leverages acoustic, cognitive, and linguistic features to form a multimodal ensemble system. It uses specialized artificial neural networks with temporal characteristics to detect AD and its severity, which is reflected through Mini-Mental State Exam (MMSE) scores. We first evaluate it on the ADReSS challenge dataset, which is a subject-independent and balanced dataset matched for age and gender to mitigate biases, and is available through DementiaBank. Our system achieves state-of-the-art test accuracy, precision, recall, and F1-score of 83.3% each for AD classification, and state-of-the-art test root mean squared error (RMSE) of 4.60 for MMSE score regression. To the best of our knowledge, the system further achieves state-of-the-art AD classification accuracy of 88.0% when evaluated on the full benchmark DementiaBank Pitt database. Our work highlights the applicability and transferability of spontaneous speech to produce a robust inductive transfer learning model, and demonstrates generalizability through a task-agnostic feature-space. The source code is available at ",
    "checked": true,
    "id": "7c4224f253709a0797a956383c884770b65cd5f1",
    "semantic_title": "multimodal inductive transfer learning for detection of alzheimer's dementia and its severity",
    "citation_count": 41
  },
  "https://www.isca-speech.org/archive/interspeech_2020/koo20_interspeech.html": {
    "title": "Exploiting Multi-Modal Features from Pre-Trained Networks for Alzheimer's Dementia Recognition",
    "volume": "main",
    "abstract": "Collecting and accessing a large amount of medical data is very time-consuming and laborious, not only because it is difficult to find specific patients but also because it is required to resolve the confidentiality of a patient's medical records. On the other hand, there are deep learning models, trained on easily collectible, large scale datasets such as Youtube or Wikipedia, offering useful representations. It could therefore be very advantageous to utilize the features from these pre-trained networks for handling a small amount of data at hand. In this work, we exploit various multi-modal features extracted from pre-trained networks to recognize Alzheimer's Dementia using a neural network, with a small dataset provided by the ADReSS Challenge at INTERSPEECH 2020. The challenge regards to discern patients suspicious of Alzheimer's Dementia by providing acoustic and textual data. With the multi-modal features, we modify a Convolutional Recurrent Neural Network based structure to perform classification and regression tasks simultaneously and is capable of computing conversations with variable lengths. Our test results surpass baseline's accuracy by 18.75%, and our validation result for the regression task shows the possibility of classifying 4 classes of cognitive impairment with an accuracy of 78.70%",
    "checked": true,
    "id": "228896a28b18ef3ad956dd8000b5ea3b8a0bfd7e",
    "semantic_title": "exploiting multi-modal features from pre-trained networks for alzheimer's dementia recognition",
    "citation_count": 31
  },
  "https://www.isca-speech.org/archive/interspeech_2020/syed20_interspeech.html": {
    "title": "Automated Screening for Alzheimer's Dementia Through Spontaneous Speech",
    "volume": "main",
    "abstract": "Dementia is a neurodegenerative disease that leads to cognitive and (eventually) physical impairments. Individuals who are affected by dementia experience deterioration in their capacity to perform day-to-day tasks thereby significantly affecting their quality of life. This paper addresses the Interspeech 2020 Alzheimer's Dementia Recognition through Spontaneous Speech (ADReSS) challenge where the objective is to propose methods for two tasks. The first task is to identify speech recordings from individuals with dementia amongst a set of recordings which also include those from healthy individuals. The second task requires participants to estimate the Mini-Mental State Examination (MMSE) score based on an individual's speech alone. To this end, we investigated characteristics of speech paralinguistics such as prosody, voice quality, and spectra as well as VGGish based deep acoustic embedding for automated screening for dementia based on the audio modality. In addition to this, we also computed deep text embeddings for transcripts of speech. For the classification task, our method achieves an accuracy of 85.42% compared to the baseline of 62.50% on the test partition, meanwhile, for the regression task, our method achieves an RMSE = 4.30 compared to the baseline of 6.14. These results show the promise of our proposed methods for the task of automated screening for dementia based on speech alone",
    "checked": true,
    "id": "92e0608730573341ac43955f2946513f2a5814c4",
    "semantic_title": "automated screening for alzheimer's dementia through spontaneous speech",
    "citation_count": 44
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lee20c_interspeech.html": {
    "title": "NEC-TT Speaker Verification System for SRE'19 CTS Challenge",
    "volume": "main",
    "abstract": "The series of speaker recognition evaluations (SREs) organized by the National Institute of Standards and Technology (NIST) is widely accepted as the de facto benchmark for speaker recognition technology. This paper describes the NEC-TT speaker verification system developed for the recent SRE'19 CTS Challenge. Our system is based on an x-vector embedding front-end followed by a thin scoring back-end. We trained a very-deep neural network for x-vector extraction by incorporating residual connections, squeeze-and-excitation networks, and angular-margin softmax at the output layer. We enhanced the back-end with a tandem approach leveraging the benefit of supervised and unsupervised domain adaptation. We obtained over 30% relative reduction in error rate with each of these enhancements at the front-end and back-end, respectively",
    "checked": true,
    "id": "fcd24caebd149ef166f2a46bc0e49eb8853f67c4",
    "semantic_title": "nec-tt speaker verification system for sre'19 cts challenge",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20u_interspeech.html": {
    "title": "THUEE System for NIST SRE19 CTS Challenge",
    "volume": "main",
    "abstract": "In this paper, we present the system that THUEE submitted to NIST 2019 Speaker Recognition Evaluation CTS Challenge (SRE19). Similar to the previous SREs, domain mismatches, such as cross-lingual and cross-channel between the training sets and evaluation sets, remain the major challenges in this evaluation. To improve the robustness of our systems, we develop deeper and wider x-vector architectures. Besides, we use novel speaker discriminative embedding systems, hybrid multi-task learning architectures combined with phonetic information. To deal with domain mismatches, we follow a heuristic search scheme to select the best back-end strategy based on limited development corpus. An extended and factorized TDNN achieves the best single-system results on SRE18 DEV and SRE19 EVAL sets. The final system is a fusion of six subsystems, which yields EER 2.81% and minimum cost 0.262 on the SRE19 EVAL set",
    "checked": true,
    "id": "197d13047a855a41d38593603d18045c35c43a42",
    "semantic_title": "thuee system for nist sre19 cts challenge",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2020/antipov20_interspeech.html": {
    "title": "Automatic Quality Assessment for Audio-Visual Verification Systems. The LOVe Submission to NIST SRE Challenge 2019",
    "volume": "main",
    "abstract": "Fusion of scores is a cornerstone of multimodal biometric systems composed of independent unimodal parts. In this work, we focus on quality-dependent fusion for speaker-face verification. To this end, we propose a universal model which can be trained for automatic quality assessment of both face and speaker modalities. This model estimates the quality of representations produced by unimodal systems which are then used to enhance the score-level fusion of speaker and face verification modules. We demonstrate the improvements brought by this quality-dependent fusion on the recent NIST SRE19 Audio-Visual Challenge dataset",
    "checked": true,
    "id": "1bab0f5df6fc9506e922dcf6e2dee3cc8ad8a427",
    "semantic_title": "automatic quality assessment for audio-visual verification systems. the love submission to nist sre challenge 2019",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tao20b_interspeech.html": {
    "title": "Audio-Visual Speaker Recognition with a Cross-Modal Discriminative Network",
    "volume": "main",
    "abstract": "Audio-visual speaker recognition is one of the tasks in the recent 2019 NIST speaker recognition evaluation (SRE). Studies in neuroscience and computer science all point to the fact that vision and auditory neural signals interact in the cognitive process. This motivated us to study a cross-modal network, namely voice-face discriminative network (VFNet) that establishes the general relation between human voice and face. Experiments show that VFNet provides additional speaker discriminative information. With VFNet, we achieve 16.54% equal error rate relative reduction over the score level fusion audio-visual baseline on evaluation set of 2019 NIST SRE",
    "checked": true,
    "id": "7232c88607067648ec726caaa4c02f58a9808424",
    "semantic_title": "audio-visual speaker recognition with a cross-modal discriminative network",
    "citation_count": 24
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shon20_interspeech.html": {
    "title": "Multimodal Association for Speaker Verification",
    "volume": "main",
    "abstract": "In this paper, we propose a multimodal association on a speaker verification system for fine-tuning using both voice and face. Inspired by neuroscientific findings, the proposed approach is to mimic the unimodal perception system benefits from the multisensory association of stimulus pairs. To verify this, we use the SRE18 evaluation protocol for experiments and use out-of-domain data, Voxceleb, for the proposed multimodal fine-tuning. Although the proposed approach relies on voice-face paired multimodal data during the training phase, the face is no more needed after training is done and only speech audio is used for the speaker verification system. In the experiments, we observed that the unimodal model, i.e. speaker verification model, benefits from the multimodal association of voice and face and generalized better than before by learning channel invariant speaker representation",
    "checked": true,
    "id": "c08d78aa99ee6b882e681eeaddc975a89f2a7035",
    "semantic_title": "multimodal association for speaker verification",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chen20h_interspeech.html": {
    "title": "Multi-Modality Matters: A Performance Leap on VoxCeleb",
    "volume": "main",
    "abstract": "The information from different modalities usually compensates each other. In this paper, we use the audio and visual data in VoxCeleb dataset to do person verification. We explored different information fusion strategies and loss functions for the audio-visual person verification system at the embedding level. System performance is evaluated using the public trail lists on VoxCeleb1 dataset. Our best system using audio-visual knowledge at the embedding level achieves 0.585%, 0.427% and 0.735% EER on the three official trial lists of VoxCeleb1, which are the best reported results on this dataset. Moreover, to imitate more complex test environment with one modality corrupted or missing, we construct a noisy evaluation set based on VoxCeleb1 dataset. We use a data augmentation strategy at the embedding level to help our audio-visual system to distinguish the noisy and the clean embedding. With such data augmented strategy, the proposed audio-visual person verification system is more robust on the noisy evaluation set",
    "checked": true,
    "id": "29aad3d6ae4018381ab90bda636ff8475c584003",
    "semantic_title": "multi-modality matters: a performance leap on voxceleb",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20w_interspeech.html": {
    "title": "Cross-Domain Adaptation with Discrepancy Minimization for Text-Independent Forensic Speaker Verification",
    "volume": "main",
    "abstract": "Forensic audio analysis for speaker verification offers unique challenges due to location/scenario uncertainty and diversity mismatch between reference and naturalistic field recordings. The lack of real naturalistic forensic audio corpora with ground-truth speaker identity represents a major challenge in this field. It is also difficult to directly employ small-scale domain-specific data to train complex neural network architectures due to domain mismatch and loss in performance. Alternatively, cross-domain speaker verification for multiple acoustic environments is a challenging task which could advance research in audio forensics. In this study, we introduce a CRSS-Forensics audio dataset collected in multiple acoustic environments. We pre-train a CNN-based network using the VoxCeleb data, followed by an approach which fine-tunes part of the high-level network layers with clean speech from CRSS-Forensics. Based on this fine-tuned model, we align domain-specific distributions in the embedding space with the discrepancy loss and maximum mean discrepancy (MMD). This maintains effective performance on the clean set, while simultaneously generalizes the model to other acoustic domains. From the results, we demonstrate that diverse acoustic environments affect the speaker verification performance, and that our proposed approach of cross-domain adaptation can significantly improve the results in this scenario",
    "checked": true,
    "id": "15a3752e47f1a8907e8eda156a5c46729e462f2a",
    "semantic_title": "cross-domain adaptation with discrepancy minimization for text-independent forensic speaker verification",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2020/sang20_interspeech.html": {
    "title": "Open-Set Short Utterance Forensic Speaker Verification Using Teacher-Student Network with Explicit Inductive Bias",
    "volume": "main",
    "abstract": "In forensic applications, it is very common that only small naturalistic datasets consisting of short utterances in complex or unknown acoustic environments are available. In this study, we propose a pipeline solution to improve speaker verification on a small actual forensic field dataset. By leveraging large-scale out-of-domain datasets, a knowledge distillation based objective function is proposed for teacher-student learning, which is applied for short utterance forensic speaker verification. The objective function collectively considers speaker classification loss, Kullback-Leibler divergence, and similarity of embeddings. In order to advance the trained deep speaker embedding network to be robust for a small target dataset, we introduce a novel strategy to fine-tune the pre-trained student model towards a forensic target domain by utilizing the model as a finetuning start point and a reference in regularization. The proposed approaches are evaluated on the 1 48-UTD forensic corpus, a newly established naturalistic dataset of actual homicide investigations consisting of short utterances recorded in uncontrolled conditions. We show that the proposed objective function can efficiently improve the performance of teacher-student learning on short utterances and that our fine-tuning strategy outperforms the commonly used weight decay method by providing an explicit inductive bias towards the pre-trained model",
    "checked": true,
    "id": "806360e0dade852d70c45efdf438e69212179567",
    "semantic_title": "open-set short utterance forensic speaker verification using teacher-student network with explicit inductive bias",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chowdhury20b_interspeech.html": {
    "title": "JukeBox: A Multilingual Singer Recognition Dataset",
    "volume": "main",
    "abstract": "A text-independent speaker recognition system relies on successfully encoding speech factors such as vocal pitch, intensity, and timbre to achieve good performance. A majority of such systems are trained and evaluated using spoken voice or everyday conversational voice data. Spoken voice, however, exhibits a limited range of possible speaker dynamics, thus constraining the utility of the derived speaker recognition models. Singing voice, on the other hand, covers a broader range of vocal and ambient factors and can, therefore, be used to evaluate the robustness of a speaker recognition system. However, a majority of existing speaker recognition datasets only focus on the spoken voice. In comparison, there is a significant shortage of labeled singing voice data suitable for speaker recognition research. To address this issue, we assemble JukeBox — a speaker recognition dataset with multilingual singing voice audio annotated with singer identity, gender, and language labels. We use the current state-of-the-art methods to demonstrate the difficulty of performing speaker recognition on singing voice using models trained on spoken voice alone. We also evaluate the effect of gender and language on speaker recognition performance, both in spoken and singing voice data. The complete JukeBox dataset can be accessed at ",
    "checked": true,
    "id": "3ab63ec505afefe603de8525a521b14018c91c1d",
    "semantic_title": "jukebox: a multilingual singer recognition dataset",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20v_interspeech.html": {
    "title": "Speaker Identification for Household Scenarios with Self-Attention and Adversarial Training",
    "volume": "main",
    "abstract": "Speaker identification based on voice input is a fundamental capability in speech processing enabling versatile downstream applications, such as personalization and authentication. With the advent of deep learning, most state-of-the-art methods apply machine learning techniques and derive acoustic embeddings from utterances with convolutional neural networks (CNNs) and recurrent neural networks (RNNs). This paper addresses two inherent limitations of current approaches. First, voice characteristics over long time spans might not be fully captured by CNNs and RNNs, as they are designed to focus on local feature extraction and adjacent dependencies modeling, respectively. Second, complex deep learning models can be fragile with regard to subtle but intentional changes in model inputs, also known as adversarial perturbations. To distill informative global acoustic embedding representations from utterances and be robust to adversarial perturbations, we propose a Self-Attentive Adversarial Speaker-Identification method ( SAASI). In experiments on the VCTK dataset, SAASI significantly outperforms four state-of-the-art baselines in identifying both known and new speakers",
    "checked": true,
    "id": "ebb636fd2477014dbe0bcf3b4e12f741b35bd52d",
    "semantic_title": "speaker identification for household scenarios with self-attention and adversarial training",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2020/rybakov20_interspeech.html": {
    "title": "Streaming Keyword Spotting on Mobile Devices",
    "volume": "main",
    "abstract": "In this work we explore the latency and accuracy of keyword spotting (KWS) models in streaming and non-streaming modes on mobile phones. NN model conversion from non-streaming mode (model receives the whole input sequence and then returns the classification result) to streaming mode (model receives portion of the input sequence and classifies it incrementally) may require manual model rewriting. We address this by designing a Tensorflow/Keras based library which allows automatic conversion of non-streaming models to streaming ones with minimum effort. With this library we benchmark multiple KWS models in both streaming and non-streaming modes on mobile phones and demonstrate different tradeoffs between latency and accuracy. We also explore novel KWS models with multi-head attention which reduce the classification error over the state-of-art by 10% on Google speech commands data sets V2. The streaming library with all experiments is open-sourced",
    "checked": true,
    "id": "70dc02df0c5a465e4f051f82f04a4448feab2788",
    "semantic_title": "streaming keyword spotting on mobile devices",
    "citation_count": 66
  },
  "https://www.isca-speech.org/archive/interspeech_2020/liu20j_interspeech.html": {
    "title": "Metadata-Aware End-to-End Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kong20_interspeech.html": {
    "title": "Adversarial Audio: A New Information Hiding Method",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20x_interspeech.html": {
    "title": "S2IGAN: Speech-to-Image Generation via Adversarial Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zuluagagomez20_interspeech.html": {
    "title": "Automatic Speech Recognition Benchmark for Air-Traffic Communications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gudepu20_interspeech.html": {
    "title": "Whisper Augmented End-to-End/Hybrid Speech Recognition System — CycleGAN Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/sawhney20_interspeech.html": {
    "title": "Risk Forecasting from Earnings Calls Acoustics and Network Correlations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chen20i_interspeech.html": {
    "title": "SpecMark: A Spectral Watermarking Framework for IP Protection of Speech Recognition Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hout20_interspeech.html": {
    "title": "Evaluating Automatically Generated Phoneme Captions for Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lin20d_interspeech.html": {
    "title": "An Efficient Temporal Modeling Approach for Speech Emotion Recognition by Mapping Varied Duration Sentences into Fixed Number of Chunks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/latif20b_interspeech.html": {
    "title": "Deep Architecture Enhancing Robustness to Noise, Adversarial Attacks, and Cross-Corpus Setting for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/fujioka20_interspeech.html": {
    "title": "Meta-Learning for Speech Emotion Recognition Considering Ambiguity of Emotion Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/liu20k_interspeech.html": {
    "title": "Temporal Attention Convolutional Network for Speech Emotion Recognition with Latent Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhu20_interspeech.html": {
    "title": "Reconciliation of Multiple Corpora for Speech Emotion Recognition by Multiple Classifiers with an Adversarial Corpus Discriminator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lian20c_interspeech.html": {
    "title": "Conversational Emotion Recognition Using Self-Attention Mechanisms and Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mao20c_interspeech.html": {
    "title": "EigenEmo: Spectral Utterance Representation Using Dynamic Mode Decomposition for Speech Emotion Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mao20d_interspeech.html": {
    "title": "Advancing Multiple Instance Learning with Attention Modeling for Categorical Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/perezramon20_interspeech.html": {
    "title": "The Effect of Language Proficiency on the Perception of Segmental Foreign Accent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/liu20l_interspeech.html": {
    "title": "The Effect of Language Dominance on the Selective Attention of Segments and Tones in Urdu-Cantonese Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20w_interspeech.html": {
    "title": "The Effect of Input on the Production of English Tense and Lax Vowels by Chinese Learners: Evidence from an Elementary School in China",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/spinu20_interspeech.html": {
    "title": "Exploring the Use of an Artificial Accent of English to Assess Phonetic Learning in Monolingual and Bilingual Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chowdhury20c_interspeech.html": {
    "title": "Effects of Dialectal Code-Switching on Speech Modules: A Study Using Egyptian Arabic Broadcast Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/johnson20_interspeech.html": {
    "title": "Bilingual Acoustic Voice Variation is Similarly Structured Across Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20r_interspeech.html": {
    "title": "Monolingual Data Selection Analysis for English-Mandarin Hybrid Code-Switching Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/du20b_interspeech.html": {
    "title": "Perception and Production of Mandarin Initial Stops by Native Urdu Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/afouras20_interspeech.html": {
    "title": "Now You're Speaking My Language: Visual Language Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/rhee20_interspeech.html": {
    "title": "The Different Enhancement Roles of Covarying Cues in Thai and Mandarin Tones",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shi20c_interspeech.html": {
    "title": "Singing Voice Extraction with Attention-Based Spectrograms Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lu20d_interspeech.html": {
    "title": "Incorporating Broad Phonetic Information for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20x_interspeech.html": {
    "title": "A Recursive Network with Dynamic Attention for Monaural Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yu20d_interspeech.html": {
    "title": "Constrained Ratio Mask for Speech Enhancement Using DNN",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lee20d_interspeech.html": {
    "title": "SERIL: Noise Adaptive Speech Enhancement Using Regularization-Based Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/bando20_interspeech.html": {
    "title": "Adaptive Neural Speech Enhancement with a Denoising Variational Autoencoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/bulut20_interspeech.html": {
    "title": "Low-Latency Single Channel Speech Dereverberation Using U-Net Convolutional Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tran20b_interspeech.html": {
    "title": "Single-Channel Speech Enhancement by Subspace Affinity Minimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20y_interspeech.html": {
    "title": "Noise Tokens: Learning Neural Noise Templates for Environment-Aware Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/deng20_interspeech.html": {
    "title": "NAAGN: Noise-Aware Attention-Gated Network for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20z_interspeech.html": {
    "title": "Online Monaural Speech Enhancement Using Delayed Subband LSTM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/strake20_interspeech.html": {
    "title": "INTERSPEECH 2020 Deep Noise Suppression Challenge: A Fully Convolutional Recurrent Network (FCRN) for Joint Dereverberation and Denoising",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hu20g_interspeech.html": {
    "title": "DCCRN: Deep Complex Convolution Recurrent Network for Phase-Aware Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/westhausen20_interspeech.html": {
    "title": "Dual-Signal Transformation LSTM Network for Real-Time Noise Suppression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/valin20_interspeech.html": {
    "title": "A Perceptually-Motivated Approach for Low-Complexity, Real-Time Enhancement of Fullband Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/isik20_interspeech.html": {
    "title": "PoCoNet: Better Speech Enhancement with Frequency-Positional Embeddings, Semi-Supervised Conversational Data, and Biased Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/reddy20_interspeech.html": {
    "title": "The INTERSPEECH 2020 Deep Noise Suppression Challenge: Datasets, Subjective Testing Framework, and Challenge Results",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/akbarzadeh20_interspeech.html": {
    "title": "The Implication of Sound Level on Spatial Selective Auditory Attention for Cochlear Implant Users: Behavioral and Electrophysiological Measurement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wan20b_interspeech.html": {
    "title": "Enhancing the Interaural Time Difference of Bilateral Cochlear Implants with the Temporal Limits Encoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/irino20_interspeech.html": {
    "title": "Speech Clarity Improvement by Vocal Self-Training Using a Hearing Impairment Simulator and its Correlation with an Auditory Modulation Index",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20s_interspeech.html": {
    "title": "Investigation of Phase Distortion on Perceived Speech Quality for Hearing-Impaired Listeners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20t_interspeech.html": {
    "title": "EEG-Based Short-Time Auditory Attention Detection Using Multi-Task Deep Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/abderrazek20_interspeech.html": {
    "title": "Towards Interpreting Deep Learning Models to Understand Loss of Speech Intelligibility in Speech Disorders — Step 1: CNN Model-Based Phone Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mirheidari20_interspeech.html": {
    "title": "Improving Cognitive Impairment Classification by Generative Neural Network-Based Feature Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/moore20_interspeech.html": {
    "title": "UncommonVoice: A Crowdsourced Dataset of Dysphonic Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/barche20_interspeech.html": {
    "title": "Towards Automatic Assessment of Voice Disorders: A Clinical Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shivkumar20_interspeech.html": {
    "title": "BlaBla: Linguistic Feature Extraction for Clinical Analysis in Multiple Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/xu20d_interspeech.html": {
    "title": "Depthwise Separable Convolutional ResNet with Squeeze-and-Excitation Blocks for Small-Footprint Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/bluche20_interspeech.html": {
    "title": "Predicting Detection Filters for Small Footprint Open-Vocabulary Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ylmaz20_interspeech.html": {
    "title": "Deep Convolutional Spiking Neural Networks for Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wu20j_interspeech.html": {
    "title": "Domain Aware Training for Far-Field Small-Footprint Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20u_interspeech.html": {
    "title": "Re-Weighted Interval Loss for Handling Data Imbalance Problem of End-to-End Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20v_interspeech.html": {
    "title": "Deep Template Matching for Small-Footprint and Configurable Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yang20d_interspeech.html": {
    "title": "Multi-Scale Convolution for Robust Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chen20j_interspeech.html": {
    "title": "An Investigation of Few-Shot Learning in Spoken Term Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhao20d_interspeech.html": {
    "title": "End-to-End Keyword Search Based on Attention and Energy Scorer for Low Resource Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/higuchi20_interspeech.html": {
    "title": "Stacked 1D Convolutional Networks for End-to-End Small Footprint Voice Trigger Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/heitkaemper20_interspeech.html": {
    "title": "Statistical and Neural Network Based Speech Activity Detection in Non-Stationary Acoustic Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20w_interspeech.html": {
    "title": "Speaker Diarization System Based on DPCA Algorithm for Fearless Steps Challenge Phase-2",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lin20e_interspeech.html": {
    "title": "The DKU Speech Activity Detection and Speaker Identification Systems for Fearless Steps Challenge Phase-02",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gorin20_interspeech.html": {
    "title": "This is Houston. Say again, please\". The Behavox System for the Apollo-11 Fearless Steps Challenge (Phase II)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/joglekar20_interspeech.html": {
    "title": "FEARLESS STEPS Challenge (FS-2): Supervised Learning with Massive Naturalistic Apollo Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/luo20b_interspeech.html": {
    "title": "Separating Varying Numbers of Sources with Auxiliary Autoencoding Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chen20k_interspeech.html": {
    "title": "On Synthesis for Supervised Monaural Speech Separation in Time Domain",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20y_interspeech.html": {
    "title": "Learning Better Speech Representations by Worsening Interference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/pariente20_interspeech.html": {
    "title": "Asteroid: The PyTorch-Based Audio Source Separation Toolkit for Researchers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chen20l_interspeech.html": {
    "title": "Dual-Path Transformer Network: Direct Context-Aware Modeling for End-to-End Monaural Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/deng20b_interspeech.html": {
    "title": "Conv-TasSAN: Separative Adversarial Network Based on Conv-TasNet",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kinoshita20_interspeech.html": {
    "title": "Multi-Path RNN for Hierarchical Modeling of Long Sequential Data and its Application to Speaker Stream Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/narayanaswamy20_interspeech.html": {
    "title": "Unsupervised Audio Source Separation Using Generative Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/qiu20b_interspeech.html": {
    "title": "Adversarial Latent Representation Learning for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/xiang20_interspeech.html": {
    "title": "An NMF-HMM Speech Enhancement Method Based on Kullback-Leibler Divergence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20x_interspeech.html": {
    "title": "Multi-Scale TCN: Exploring Better Temporal DNN Model for Causal Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20z_interspeech.html": {
    "title": "VoiceFilter-Lite: Streaming Targeted Voice Separation for On-Device Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shi20d_interspeech.html": {
    "title": "Speech Separation Based on Multi-Stage Elaborated Dual-Path Deep BiLSTM with Auxiliary Identity Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hao20b_interspeech.html": {
    "title": "Sub-Band Knowledge Distillation Framework for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/roy20_interspeech.html": {
    "title": "A Deep Learning-Based Kalman Filter for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yu20e_interspeech.html": {
    "title": "Subband Kalman Filtering with DNN Estimated Parameters for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20aa_interspeech.html": {
    "title": "Bidirectional LSTM Network with Ordered Neurons for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shi20e_interspeech.html": {
    "title": "Speaker-Conditional Chain Model for Speech Separation and Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/nortje20_interspeech.html": {
    "title": "Unsupervised vs. Transfer Learning for Multimodal One-Shot Matching of Speech and Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lee20e_interspeech.html": {
    "title": "Multimodal Speech Emotion Recognition Using Cross Attention with Aligned Audio and Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/csapo20_interspeech.html": {
    "title": "Speaker Dependent Articulatory-to-Acoustic Mapping Using Real-Time MRI of the Vocal Tract",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/csapo20b_interspeech.html": {
    "title": "Ultrasound-Based Articulatory-to-Acoustic Mapping with WaveGlow Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/feng20d_interspeech.html": {
    "title": "Unsupervised Subword Modeling Using Autoregressive Pretraining and Cross-Lingual Phone-Aware Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/matsuura20_interspeech.html": {
    "title": "Generative Adversarial Training Data Adaptation for Very Low-Resource Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tsunematsu20_interspeech.html": {
    "title": "Neural Speech Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/milde20_interspeech.html": {
    "title": "Improving Unsupervised Sparsespeech Acoustic Models with Categorical Reparameterization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/papadimitriou20_interspeech.html": {
    "title": "Multimodal Sign Language Recognition via Temporal Deformable Convolutional Sequence Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/pratap20_interspeech.html": {
    "title": "MLS: A Large-Scale Multilingual Dataset for Speech Research",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/parmonangan20_interspeech.html": {
    "title": "Combining Audio and Brain Activity for Predicting Speech Quality",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/sharon20_interspeech.html": {
    "title": "The \"Sound of Silence\" in EEG — Cognitive Voice Activity Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/cai20_interspeech.html": {
    "title": "Low Latency Auditory Attention Detection with Common Spatial Pattern Analysis of EEG Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/angrick20_interspeech.html": {
    "title": "Speech Spectrogram Estimation from Intracranial Brain Activity Using a Quantization Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/dash20_interspeech.html": {
    "title": "Neural Speech Decoding for Amyotrophic Lateral Sclerosis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chen20m_interspeech.html": {
    "title": "Semi-Supervised ASR by End-to-End Self-Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tulsiani20_interspeech.html": {
    "title": "Improved Training Strategies for End-to-End Speech Recognition in Digital Voice Assistants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kanda20b_interspeech.html": {
    "title": "Serialized Output Training for End-to-End Overlapped Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/weninger20_interspeech.html": {
    "title": "Semi-Supervised Learning with Data Augmentation for End-to-End ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/guo20_interspeech.html": {
    "title": "Efficient Minimum Word Error Rate Training of RNN-Transducer for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zeyer20_interspeech.html": {
    "title": "A New Training Pipeline for an Improved Neural Transducer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/park20d_interspeech.html": {
    "title": "Improved Noisy Student Training for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/masumura20_interspeech.html": {
    "title": "Phoneme-to-Grapheme Conversion Based Large-Scale Pre-Training for End-to-End Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gowda20_interspeech.html": {
    "title": "Utterance Invariant Training for Hybrid Two-Pass End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20aa_interspeech.html": {
    "title": "SCADA: Stochastic, Consistent and Adversarial Data Augmentation to Improve ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/das20b_interspeech.html": {
    "title": "Fundamental Frequency Model for Postfiltering at Low Bitrates in a Transform-Domain Speech and Audio Codec",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/broucke20_interspeech.html": {
    "title": "Hearing-Impaired Bio-Inspired Cochlear Models for Real-Time Auditory Applications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/skoglund20_interspeech.html": {
    "title": "Improving Opus Low Bit Rate Quality with Neural Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/manocha20_interspeech.html": {
    "title": "A Differentiable Perceptual Audio Metric Learned from Just Noticeable Differences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/masztalski20_interspeech.html": {
    "title": "StoRIR: Stochastic Room Impulse Response Generation for Audio Data Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/naderi20_interspeech.html": {
    "title": "An Open Source Implementation of ITU-T Recommendation P.808 with Validation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mittag20b_interspeech.html": {
    "title": "DNN No-Reference PSTN Speech Quality Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/moller20_interspeech.html": {
    "title": "Non-Intrusive Diagnostic Monitoring of Fullband Speech Quality",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shahrebabaki20_interspeech.html": {
    "title": "Transfer Learning of Articulatory Information Through Phone Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shahrebabaki20b_interspeech.html": {
    "title": "Sequence-to-Sequence Articulatory Inversion Through Time Convolution of Sub-Band Frequency Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gatto20_interspeech.html": {
    "title": "Discriminative Singular Spectrum Analysis for Bioacoustic Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mannem20b_interspeech.html": {
    "title": "Speech Rate Task-Specific Representation Learning from Acoustic-Articulatory Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hernandez20_interspeech.html": {
    "title": "Dysarthria Detection and Severity Assessment Using Rhythm-Based Metrics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ma20_interspeech.html": {
    "title": "LungRN+NL: An Improved Adventitious Lung Sound Classification Using Non-Local Block ResNet Neural Network with Mixup Data Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/singh20b_interspeech.html": {
    "title": "Attention and Encoder-Decoder Based Models for Transforming Articulatory Movements at Different Speaking Rates",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yang20e_interspeech.html": {
    "title": "Adventitious Respiratory Classification Using Attentive Residual Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lenain20_interspeech.html": {
    "title": "Surfboard: Audio Feature Extraction for Modern Machine Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/naini20_interspeech.html": {
    "title": "Whisper Activity Detection Using CNN-LSTM Based Attention Pooling Network Trained for a Speaker Identification Task",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhao20e_interspeech.html": {
    "title": "Towards Natural Bilingual and Code-Switched Speech Synthesis Based on Mix of Monolingual Recordings and Cross-Lingual Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/liu20m_interspeech.html": {
    "title": "Multi-Lingual Multi-Speaker Text-to-Speech Synthesis for Voice Cloning with Online Speaker Enrollment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/fu20b_interspeech.html": {
    "title": "Dynamic Soft Windowing and Language Dependent Style Token for Code-Switching End-to-End Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/staib20_interspeech.html": {
    "title": "Phonological Features for 0-Shot Multilingual Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/xin20_interspeech.html": {
    "title": "Cross-Lingual Text-To-Speech Synthesis via Domain Adaptation and Perceptual Similarity Regression in Speaker Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/liu20n_interspeech.html": {
    "title": "Tone Learning in Low-Resource Bilingual TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/bansal20_interspeech.html": {
    "title": "On Improving Code Mixed Speech Synthesis with Mixlingual Grapheme-to-Phoneme Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/prakash20_interspeech.html": {
    "title": "Generic Indic Text-to-Speech Synthesisers with Rapid Adaptation in an End-to-End Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/korte20_interspeech.html": {
    "title": "Efficient Neural Speech Synthesis for Low-Resource Languages Through Multilingual Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/nekvinda20_interspeech.html": {
    "title": "One Model, Many Languages: Meta-Learning for Multilingual Text-to-Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chung20b_interspeech.html": {
    "title": "In Defence of Metric Learning for Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kye20_interspeech.html": {
    "title": "Meta-Learning for Short Utterance Speaker Recognition with Imbalance Length Pairs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20ba_interspeech.html": {
    "title": "Segment-Level Effects of Gender, Nationality and Emotion Information on Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shi20f_interspeech.html": {
    "title": "Weakly Supervised Training of Hierarchical Attention Networks for Speaker Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/montalvo20_interspeech.html": {
    "title": "Multi-Task Learning for Voice Related Recognition Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/khan20_interspeech.html": {
    "title": "Unsupervised Training of Siamese Networks for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/liu20o_interspeech.html": {
    "title": "An Effective Speaker Recognition Method Based on Joint Identification and Verification Supervisions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zheng20c_interspeech.html": {
    "title": "Speaker-Aware Linear Discriminant Analysis in Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chen20n_interspeech.html": {
    "title": "Adversarial Domain Adaptation for Speaker Verification Using Partially Shared Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lin20f_interspeech.html": {
    "title": "Automatic Scoring at Multi-Granularity for L2 Pronunciation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lo20b_interspeech.html": {
    "title": "An Effective End-to-End Modeling Approach for Mispronunciation Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yan20_interspeech.html": {
    "title": "An End-to-End Mispronunciation Detection System for L2 English Speech Leveraging Novel Anti-Phone Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/duan20_interspeech.html": {
    "title": "Unsupervised Feature Adaptation Using Adversarial Multi-Task Training for Automatic Evaluation of Children's Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yang20f_interspeech.html": {
    "title": "Pronunciation Erroneous Tendency Detection with Language Adversarial Represent Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/cheng20_interspeech.html": {
    "title": "ASR-Free Pronunciation Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kyriakopoulos20_interspeech.html": {
    "title": "Automatic Detection of Accent and Lexical Pronunciation Errors in Spontaneous Non-Native English Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shi20g_interspeech.html": {
    "title": "Context-Aware Goodness of Pronunciation for Computer-Assisted Pronunciation Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chu20_interspeech.html": {
    "title": "Recognize Mispronunciations to Improve Non-Native Acoustic Modeling Through a Phone Decoder Built from One Edit Distance Finite State Automaton",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gimeno20_interspeech.html": {
    "title": "Partial AUC Optimisation Using Recurrent Neural Networks for Music Detection with Limited Training Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lavechin20_interspeech.html": {
    "title": "An Open-Source Voice Type Classifier for Child-Centered Daylong Recordings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/peng20_interspeech.html": {
    "title": "Competing Speaker Count Estimation on the Fusion of the Spectral and Spatial Embedding Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lin20g_interspeech.html": {
    "title": "Audio-Visual Multi-Speaker Tracking Based on the GLMB Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/liu20p_interspeech.html": {
    "title": "Towards Speech Robustness for Acoustic Scene Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhu20b_interspeech.html": {
    "title": "Identify Speakers in Cocktail Parties with End-to-End Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/neumann20_interspeech.html": {
    "title": "Multi-Talker ASR for an Unknown Number of Sources: Joint Training of Source Counting, Separation and ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/upadhyay20_interspeech.html": {
    "title": "Attentive Convolutional Recurrent Neural Network Using Phoneme-Level Acoustic Representation for Rare Sound Event Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/cornell20_interspeech.html": {
    "title": "Detecting and Counting Overlapping Speakers in Distant Speech Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/moritz20_interspeech.html": {
    "title": "All-in-One Transformer: Unifying Speech Recognition, Audio Tagging, and Event Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/diener20_interspeech.html": {
    "title": "Towards Silent Paralinguistics: Deriving Speaking Mode and Speaker ID from Electromyographic Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhong20_interspeech.html": {
    "title": "Predicting Collaborative Task Performance Using Graph Interlocutor Acoustic Network in Small Group Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gosztolya20_interspeech.html": {
    "title": "Very Short-Term Conflict Intensity Estimation Using Fisher Vectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mori20_interspeech.html": {
    "title": "Gaming Corpus for Studying Social Screams",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/afshan20_interspeech.html": {
    "title": "Speaker Discrimination in Humans and Machines: Effects of Speaking Style Variability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/sabu20_interspeech.html": {
    "title": "Automatic Prediction of Confidence Level from Children's Oral Reading Recordings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/xue20_interspeech.html": {
    "title": "Towards a Comprehensive Assessment of Speech Intelligibility for Pathological Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lin20h_interspeech.html": {
    "title": "Effects of Communication Channels and Actor's Gender on Emotion Identification by Native Mandarin Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/anjos20_interspeech.html": {
    "title": "Detection of Voicing and Place of Articulation of Fricatives with Deep Learning in a Virtual Speech and Language Therapy Tutor",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20y_interspeech.html": {
    "title": "Unsupervised Learning for Sequence-to-Sequence Text-to-Speech for Low-Resource Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/palkama20_interspeech.html": {
    "title": "Conditional Spoken Digit Generation with StyleGAN",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yang20g_interspeech.html": {
    "title": "Towards Universal Text-to-Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/katsurada20_interspeech.html": {
    "title": "Speaker-Independent Mel-Cepstrum Estimation from Articulator Movements Using D-Vector Input",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/liang20_interspeech.html": {
    "title": "Enhancing Monotonicity for Robust Autoregressive Transformer TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mohan20_interspeech.html": {
    "title": "Incremental Text to Speech for Neural Sequence-to-Sequence Models Using Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tu20b_interspeech.html": {
    "title": "Semi-Supervised Learning for Multi-Speaker Text-to-Speech Synthesis Using Discrete Speech Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/saha20_interspeech.html": {
    "title": "Learning Joint Articulatory-Acoustic Representations with Normalizing Flows",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yamashita20_interspeech.html": {
    "title": "Investigating Effective Additional Contextual Factors in DNN-Based Spontaneous Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/webber20_interspeech.html": {
    "title": "Hider-Finder-Combiner: An Adversarial Architecture for General Speech Signal Modification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lin20i_interspeech.html": {
    "title": "Wav2Spk: A Simple DNN Architecture for Learning Speaker Embeddings from Waveforms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/pham20b_interspeech.html": {
    "title": "How Does Label Noise Affect the Quality of Speaker Embeddings?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/liu20q_interspeech.html": {
    "title": "A Comparative Re-Assessment of Feature Extractors for Deep Speaker Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/xia20_interspeech.html": {
    "title": "Speaker Representation Learning Using Global Context Guided Channel and Time-Frequency Transformations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kwon20_interspeech.html": {
    "title": "Intra-Class Variation Reduction of Speaker Representation in Disentanglement Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/georges20_interspeech.html": {
    "title": "Compact Speaker Embedding: lrx-Vector",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kreyssig20_interspeech.html": {
    "title": "Cosine-Distance Virtual Adversarial Training for Semi-Supervised Speaker-Discriminative Acoustic Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/peng20b_interspeech.html": {
    "title": "Deep Speaker Embedding with Long Short Term Centroid Learning for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20ca_interspeech.html": {
    "title": "Neural Discriminant Analysis for Deep Speaker Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/cho20b_interspeech.html": {
    "title": "Learning Speaker Embedding from Text-to-Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhao20f_interspeech.html": {
    "title": "Noisy-Reverberant Speech Enhancement Using DenseUNet with Time-Frequency Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20z_interspeech.html": {
    "title": "On Loss Functions and Recurrency Training for GAN-Based Speech Enhancement Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/du20c_interspeech.html": {
    "title": "Self-Supervised Adversarial Multi-Task Learning for Vocoder-Based Monaural Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kegler20_interspeech.html": {
    "title": "Deep Speech Inpainting of Time-Frequency Masks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shankar20_interspeech.html": {
    "title": "Real-Time Single-Channel Deep Neural Network-Based Speech Enhancement on Edge Devices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lin20j_interspeech.html": {
    "title": "Improved Speech Enhancement Using a Time-Domain GAN with Mask Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/defossez20_interspeech.html": {
    "title": "Real Time Speech Enhancement in the Waveform Domain",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/romaniuk20_interspeech.html": {
    "title": "Efficient Low-Latency Speech Enhancement with Mobile Audio Streaming Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chiba20_interspeech.html": {
    "title": "Multi-Stream Attention-Based BLSTM with Feature Segmentation for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20da_interspeech.html": {
    "title": "Microphone Array Post-Filter for Target Speech Enhancement Without a Prior Information of Point Interferers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hiroe20_interspeech.html": {
    "title": "Similarity-and-Independence-Aware Beamformer: Method for Target Source Extraction Using Magnitude Spectrogram as Reference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/golokolenko20_interspeech.html": {
    "title": "The Method of Random Directions Optimization for Stereo Audio Source Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/fan20b_interspeech.html": {
    "title": "Gated Recurrent Fusion of Spatial and Spectral Features for Multi-Channel Speech Separation with Deep Embedding Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/scheibler20_interspeech.html": {
    "title": "Generalized Minimal Distortion Principle for Blind Source Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhong20b_interspeech.html": {
    "title": "A Lightweight Model Based on Separable Convolution for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/cai20b_interspeech.html": {
    "title": "Meta Multi-Task Learning for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/grondin20_interspeech.html": {
    "title": "GEV Beamforming Supported by DOA-Based Masks Generated on Pairs of Microphones",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/jose20_interspeech.html": {
    "title": "Accurate Detection of Wake Word Start and End Using a CNN",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/adya20_interspeech.html": {
    "title": "Hybrid Transformer/CTC Networks for Hardware Efficient Voice Triggering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/majumdar20_interspeech.html": {
    "title": "MatchboxNet: 1D Time-Channel Separable Convolutional Neural Network Architecture for Speech Commands Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mehrotra20_interspeech.html": {
    "title": "Iterative Compression of End-to-End ASR Model Using AutoML",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/nguyen20c_interspeech.html": {
    "title": "Quantization Aware Training with Absolute-Cosine Regularization for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/garg20b_interspeech.html": {
    "title": "Streaming On-Device End-to-End ASR System for Privacy-Sensitive Voice-Typing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/pratap20b_interspeech.html": {
    "title": "Scaling Up Online Speech Recognition Using ConvNets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/bai20_interspeech.html": {
    "title": "Listen Attentively, and Spell Once: Whole Sentence Generation via a Non-Autoregressive Architecture for Low-Latency Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/strimel20_interspeech.html": {
    "title": "Rescore in a Flash: Compact, Cache Efficient Hashing Data Structures for n-Gram Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shankar20b_interspeech.html": {
    "title": "Multi-Speaker Emotion Conversion via Latent Variable Regularization and a Chained Encoder-Decoder-Predictor Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shankar20c_interspeech.html": {
    "title": "Non-Parallel Emotion Conversion Using a Deep-Generative Hybrid Network and an Adversarial Pair Discriminator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tits20b_interspeech.html": {
    "title": "Laughter Synthesis: Combining Seq2seq Modeling with Transfer Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/cao20b_interspeech.html": {
    "title": "Nonparallel Emotional Speech Conversion Using VAE-GAN",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/sorin20_interspeech.html": {
    "title": "Principal Style Components: Expressive Style Control and Cross-Speaker Transfer in Neural TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhou20d_interspeech.html": {
    "title": "Converting Anyone's Emotion: Towards Speaker-Independent Emotional Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/matsumoto20_interspeech.html": {
    "title": "Controlling the Strength of Emotions in Speech-Like Emotional Sound Generated by WaveNet",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20aa_interspeech.html": {
    "title": "Learning Syllable-Level Discrete Prosodic Representation for Expressive Speech Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kishida20_interspeech.html": {
    "title": "Simultaneous Conversion of Speaker Identity and Emotion Based on Multiple-Domain Adaptive RBM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yang20h_interspeech.html": {
    "title": "Exploiting Deep Sentential Context for Expressive End-to-End Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hono20_interspeech.html": {
    "title": "Hierarchical Multi-Grained Generative Model for Expressive Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/eskimez20_interspeech.html": {
    "title": "GAN-Based Data Generation for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/dhamyal20_interspeech.html": {
    "title": "The Phonetic Bases of Vocal Expressed Emotion: Natural versus Acted",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/qin20_interspeech.html": {
    "title": "The INTERSPEECH 2020 Far-Field Speaker Verification Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20ba_interspeech.html": {
    "title": "Deep Embedding Learning for Text-Dependent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gusev20_interspeech.html": {
    "title": "STC-Innovation Speaker Recognition Systems for Far-Field Speaker Verification Challenge 2020",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20ca_interspeech.html": {
    "title": "NPU Speaker Verification System for INTERSPEECH 2020 Far-Field Speaker Verification Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tong20_interspeech.html": {
    "title": "The JD AI Speaker Verification System for the FFSVC 2020 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chung20c_interspeech.html": {
    "title": "FaceFilter: Audio-Visual Speech Separation Using Still Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chung20d_interspeech.html": {
    "title": "Seeing Voices and Hearing Voices: Learning Discriminative Embeddings Using Cross-Modal Self-Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wand20_interspeech.html": {
    "title": "Fusion Architectures for Word-Based Audiovisual Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yu20f_interspeech.html": {
    "title": "Audio-Visual Multi-Channel Recognition of Overlapped Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20ea_interspeech.html": {
    "title": "TMT: A Transformer-Based Modal Translator for Improving Multimodal Sequence Representations in Audio Visual Scene-Aware Dialog",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/sterpu20_interspeech.html": {
    "title": "Should we Hard-Code the Recurrence Concept or Learn it Instead ? Exploring the Transformer Architecture for Audio-Visual Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/koumparoulis20_interspeech.html": {
    "title": "Resource-Adaptive Deep Learning for Visual Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mortazavi20_interspeech.html": {
    "title": "Speech-Image Semantic Alignment Does Not Depend on Any Prior Classification Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/liu20r_interspeech.html": {
    "title": "Lip Graph Assisted Audio-Visual Speech Recognition Using Bidirectional Synchronous Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/konda20_interspeech.html": {
    "title": "Caption Alignment for Low Resource Audio-Visual Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mevawalla20_interspeech.html": {
    "title": "Successes, Challenges and Opportunities for Speech Technology in Conversational Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/michelsanti20_interspeech.html": {
    "title": "Vocoder-Based Speech Synthesis from Silent Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wu20k_interspeech.html": {
    "title": "Quasi-Periodic Parallel WaveGAN Vocoder: A Non-Autoregressive Pitch-Dependent Dilated Convolution Model for Parametric Speech Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wu20l_interspeech.html": {
    "title": "A Cyclical Post-Filtering Approach to Mismatch Refinement of Neural Vocoder for Text-to-Speech Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yoon20_interspeech.html": {
    "title": "Audio Dequantization for High Fidelity Audio Generation in Flow-Based Neural Vocoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/sharma20b_interspeech.html": {
    "title": "StrawNet: Self-Training WaveNet for TTS in Low-Data Regimes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/cui20b_interspeech.html": {
    "title": "An Efficient Subband Linear Prediction for LPCNet-Based Neural Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ai20b_interspeech.html": {
    "title": "Reverberation Modeling for Source-Filter-Based Neural Vocoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/vipperla20_interspeech.html": {
    "title": "Bunched LPCNet: Vocoder for Low-Cost Neural Text-To-Speech Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/song20c_interspeech.html": {
    "title": "Neural Text-to-Speech with a Modeling-by-Generation Excitation Vocoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/vainer20_interspeech.html": {
    "title": "SpeedySpeech: Efficient Neural Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20da_interspeech.html": {
    "title": "Semi-Supervised End-to-End ASR via Teacher-Student Learning with Conditional Posterior Distribution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/sapru20_interspeech.html": {
    "title": "Leveraging Unlabeled Speech for Sequence Discriminative Training of Acoustic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20fa_interspeech.html": {
    "title": "Developing RNN-T Models Surpassing High-Performance Hybrid Models with Customization Capability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chang20c_interspeech.html": {
    "title": "End-to-End ASR with Adaptive Span Self-Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lakomkin20_interspeech.html": {
    "title": "Subword Regularization: An Analysis of Scalability and Generalization for End-to-End Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/michel20_interspeech.html": {
    "title": "Early Stage LM Integration Using Local and Global Log-Linear Combination",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/han20_interspeech.html": {
    "title": "ContextNet: Improving Convolutional Neural Networks for Automatic Speech Recognition with Global Context",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/sainath20_interspeech.html": {
    "title": "Emitting Word Timings with End-to-End Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/liu20s_interspeech.html": {
    "title": "Low-Latency Sequence-to-Sequence Speech Recognition and Translation by Partial Hypothesis Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20ga_interspeech.html": {
    "title": "Neural Language Modeling with Implicit Cache Pointers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/jain20b_interspeech.html": {
    "title": "Finnish ASR with Deep Transformer Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/futami20_interspeech.html": {
    "title": "Distilling the Knowledge of BERT for Sequence-to-Sequence ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chien20_interspeech.html": {
    "title": "Stochastic Convolutional Recurrent Networks for Language Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/huo20_interspeech.html": {
    "title": "Investigation of Large-Margin Softmax in Neural Language Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/liu20t_interspeech.html": {
    "title": "Contextualizing ASR Lattice Rescoring with Hybrid Pointer Network Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/higuchi20b_interspeech.html": {
    "title": "Mask CTC: Non-Autoregressive End-to-End ASR with CTC and Mask Predict",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/fujita20_interspeech.html": {
    "title": "Insertion-Based Modeling for End-to-End Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chen20o_interspeech.html": {
    "title": "Voice Activity Detection in the Wild via Weakly Supervised Sound Event Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lee20f_interspeech.html": {
    "title": "Dual Attention in Time and Frequency Domain for Voice Activity Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/xu20e_interspeech.html": {
    "title": "Polishing the Classical Likelihood Ratio Test by Supervised Learning for Voice Activity Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kumar20e_interspeech.html": {
    "title": "A Noise Robust Technique for Detecting Vowels in Speech Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lavechin20b_interspeech.html": {
    "title": "End-to-End Domain-Adversarial Voice Activity Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/agarwal20_interspeech.html": {
    "title": "VOP Detection in Variable Speech Rate Condition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zheng20d_interspeech.html": {
    "title": "MLNET: An Adaptive Multiple Receptive-Field Attention Neural Network for Voice Activity Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kreuk20_interspeech.html": {
    "title": "Self-Supervised Contrastive Learning for Unsupervised Phoneme Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zelasko20_interspeech.html": {
    "title": "That Sounds Familiar: An Analysis of Phonetic Representations Transfer Across Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/limonard20_interspeech.html": {
    "title": "Analyzing Read Aloud Speech by Primary School Pupils: Insights for Research and Development",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/rasilo20_interspeech.html": {
    "title": "Discovering Articulatory Speech Targets from Synthesized Random Babble",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/csapo20c_interspeech.html": {
    "title": "Speaker Dependent Acoustic-to-Articulatory Inversion Using Real-Time MRI of the Vocal Tract",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/bozorg20_interspeech.html": {
    "title": "Acoustic-to-Articulatory Inversion with Deep Autoregressive Articulatory-WaveNet",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/douros20_interspeech.html": {
    "title": "Using Silence MR Image to Synthesise Dynamic MRI Vocal Tract Data of CV",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/csapo20d_interspeech.html": {
    "title": "Quantification of Transducer Misalignment in Ultrasound Tongue Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/parrot20_interspeech.html": {
    "title": "Independent and Automatic Evaluation of Speaker-Independent Acoustic-to-Articulatory Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/diener20b_interspeech.html": {
    "title": "CSL-EMG_Array: An Open Access Corpus for EMG-to-Speech Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/penney20_interspeech.html": {
    "title": "Links Between Production and Perception of Glottalisation in Individual Australian English Speaker/Listeners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/siriwardhana20_interspeech.html": {
    "title": "Jointly Fine-Tuning \"BERT-Like\" Self Supervised Models to Improve Multimodal Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chung20e_interspeech.html": {
    "title": "Vector-Quantized Autoregressive Predictive Coding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/song20d_interspeech.html": {
    "title": "Speech-XLNet: Unsupervised Acoustic Model Pretraining for Self-Attention Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/singh20c_interspeech.html": {
    "title": "Large Scale Weakly and Semi-Supervised Learning for Low-Resource Video ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kumatani20_interspeech.html": {
    "title": "Sequence-Level Self-Learning with Multiple Hypotheses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wu20m_interspeech.html": {
    "title": "Defense for Black-Box Attacks on Anti-Spoofing Models by Self-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yang20i_interspeech.html": {
    "title": "Understanding Self-Attention of Self-Supervised Audio Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/khurana20_interspeech.html": {
    "title": "A Convolutional Deep Markov Model for Unsupervised Speech Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/abulimiti20_interspeech.html": {
    "title": "Automatic Speech Recognition for ILSE-Interviews: Longitudinal Conversational Speech Recordings Covering Aging and Cognitive Decline",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhou20e_interspeech.html": {
    "title": "Dynamic Margin Softmax Loss for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/rybicka20_interspeech.html": {
    "title": "On Parameter Adaptation in Softmax-Based Cross-Entropy Loss for Improved Convergence Speed and Accuracy in DNN-Based Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mingote20_interspeech.html": {
    "title": "Training Speaker Enrollment Models by Network Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/sarfjoo20_interspeech.html": {
    "title": "Supervised Domain Adaptation for Text-Independent Speaker Verification Using Limited Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wei20b_interspeech.html": {
    "title": "Angular Margin Centroid Loss for Text-Independent Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kang20_interspeech.html": {
    "title": "Domain-Invariant Speaker Vector Projection by Model-Agnostic Meta-Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/desplanques20_interspeech.html": {
    "title": "ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chen20p_interspeech.html": {
    "title": "Length- and Noise-Aware Training Techniques for Short-Utterance Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lu20e_interspeech.html": {
    "title": "Spoken Language ‘Grammatical Error Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/papi20_interspeech.html": {
    "title": "Mixtures of Deep Neural Experts for Automated Speech Scoring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20ba_interspeech.html": {
    "title": "Targeted Content Feedback in Spoken Language Learning and Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/raina20_interspeech.html": {
    "title": "Universal Adversarial Attacks on Spoken Language Assessment Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wu20n_interspeech.html": {
    "title": "Ensemble Approaches for Uncertainty in Spoken Language Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lin20k_interspeech.html": {
    "title": "Shadowability Annotation with Fine Granularity on L2 Utterances and its Improvement with Native Listeners' Script-Shadowing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/bai20b_interspeech.html": {
    "title": "ASR-Based Evaluation and Feedback for Individualized Reading Practice",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/woszczyk20_interspeech.html": {
    "title": "Domain Adversarial Neural Networks for Dysarthric Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hidaka20_interspeech.html": {
    "title": "Automatic Estimation of Pathological Voice Quality Based on Recurrent Neural Network Using Amplitude and Phase Spectrogram",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chien20b_interspeech.html": {
    "title": "Stochastic Curiosity Exploration for Dialogue Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/jeong20_interspeech.html": {
    "title": "Conditional Response Augmentation for Dialogue Using Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/luo20c_interspeech.html": {
    "title": "Prototypical Q Networks for Automatic Conversational Diagnosis and Few-Shot New Disease Adaption",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hong20b_interspeech.html": {
    "title": "End-to-End Task-Oriented Dialog System Through Template Slot Value Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/he20_interspeech.html": {
    "title": "Task-Oriented Dialog Generation with Enhanced Entity Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/dang20_interspeech.html": {
    "title": "End-to-End Speech-to-Dialog-Act Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/qian20_interspeech.html": {
    "title": "Discriminative Transfer Learning for Optimizing ASR and Semantic Labeling in Task-Oriented Spoken Dialog",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/xu20f_interspeech.html": {
    "title": "Datasets and Benchmarks for Task-Oriented Log Dialogue Ranking Task",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20ca_interspeech.html": {
    "title": "A Semi-Blind Source Separation Approach for Speech Dereverberation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yang20j_interspeech.html": {
    "title": "Virtual Acoustic Channel Expansion Based on Neural Networks for Weighted Prediction Error-Based Speech Dereverberation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kothapally20_interspeech.html": {
    "title": "SkipConvNet: Skip Convolutional Neural Network for Speech Dereverberation Using Optimally Smoothed Spectral Mapping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20ea_interspeech.html": {
    "title": "A Robust and Cascaded Acoustic Echo Cancellation Based on Deep Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20fa_interspeech.html": {
    "title": "Generative Adversarial Network Based Acoustic Echo Cancellation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/pfeifenberger20_interspeech.html": {
    "title": "Nonlinear Residual Echo Suppression Using a Recurrent Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gao20d_interspeech.html": {
    "title": "Independent Echo Path Modeling for Stereophonic Acoustic Echo Cancellation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chen20q_interspeech.html": {
    "title": "Nonlinear Residual Echo Suppression Based on Multi-Stream Conv-TasNet",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/fan20c_interspeech.html": {
    "title": "Improving Partition-Block-Based Acoustic Echo Canceler in Under-Modeling Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kim20e_interspeech.html": {
    "title": "Attention Wave-U-Net for Acoustic Echo Cancellation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/cai20c_interspeech.html": {
    "title": "From Speaker Verification to Multispeaker Speech Synthesis, Deep Transfer with Feedback Constraint",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/cooper20_interspeech.html": {
    "title": "Can Speaker Augmentation Improve Multi-Speaker End-to-End TTS?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20da_interspeech.html": {
    "title": "Non-Autoregressive End-to-End TTS with Coarse-to-Fine Decoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20ea_interspeech.html": {
    "title": "Bi-Level Speaker Supervision for One-Shot Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/peirolilja20_interspeech.html": {
    "title": "Naturalness Enhancement with Linguistic Information in End-to-End TTS Using Unsupervised Parallel Encoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20ha_interspeech.html": {
    "title": "MoBoAligner: A Neural Alignment Model for Non-Autoregressive TTS with Monotonic Boundary Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lim20_interspeech.html": {
    "title": "JDI-T: Jointly Trained Duration Informed Transformer for Text-To-Speech without Explicit Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/aso20_interspeech.html": {
    "title": "End-to-End Text-to-Speech Synthesis with Unaligned Multiple Language Units Based on Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/dou20_interspeech.html": {
    "title": "Attention Forcing for Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/fong20_interspeech.html": {
    "title": "Testing the Limits of Representation Mixing for Pronunciation Correction in End-to-End Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chen20r_interspeech.html": {
    "title": "MultiSpeech: Multi-Speaker Text to Speech with Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/papadopoulos20_interspeech.html": {
    "title": "Exploiting Conic Affinity Measures to Design Speech Enhancement Systems Operating in Unseen Noise Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ji20_interspeech.html": {
    "title": "Adversarial Dictionary Learning for Monaural Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/seki20_interspeech.html": {
    "title": "Semi-Supervised Self-Produced Speech Enhancement and Suppression Based on Joint Source Modeling of Air- and Body-Conducted Signals Using Variational Autoencoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/weisman20_interspeech.html": {
    "title": "Spatial Covariance Matrix Estimation for Reverberant Speech with Application to Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ho20_interspeech.html": {
    "title": "A Cross-Channel Attention-Based Wave-U-Net for Multi-Channel Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/fedorov20_interspeech.html": {
    "title": "TinyLSTMs: Efficient Neural Speech Enhancement for Hearing Aids",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hikosaka20_interspeech.html": {
    "title": "Intelligibility Enhancement Based on Speech Waveform Modification Using Hearing Impairment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hou20c_interspeech.html": {
    "title": "Speaker and Phoneme-Aware Speech Bandwidth Extension with Residual Dual-Path Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hou20d_interspeech.html": {
    "title": "Multi-Task Learning for End-to-End Noise-Robust Bandwidth Extension",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hu20h_interspeech.html": {
    "title": "Phase-Aware Music Super-Resolution Using Generative Adversarial Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/huang20d_interspeech.html": {
    "title": "Learning Utterance-Level Representations with Label Smoothing for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/jalal20_interspeech.html": {
    "title": "Removing Bias with Residual Mixture of Multi-View Attention for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/fan20d_interspeech.html": {
    "title": "Adaptive Domain-Aware Representation Learning for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhou20f_interspeech.html": {
    "title": "Speech Emotion Recognition with Discriminative Feature Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhou20g_interspeech.html": {
    "title": "Using Speech Enhancement Preprocessing for Speech Emotion Recognition in Realistic Noisy Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20ia_interspeech.html": {
    "title": "Comparison of Glottal Source Parameter Values in Emotional Vowels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chou20_interspeech.html": {
    "title": "Learning to Recognize Per-Rater's Emotion Perception Using Co-Rater Training Strategy with Soft and Hard Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/jalal20b_interspeech.html": {
    "title": "Empirical Interpretation of Speech Emotion Perception with Attention Based Model for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gessinger20_interspeech.html": {
    "title": "Phonetic Accommodation of L2 German Speakers to the Virtual Language Learning Tutor Mirabella",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gu20b_interspeech.html": {
    "title": "Characterization of Singaporean Children's English: Comparisons to American and British Counterparts Using Archetypal Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kaminskaia20_interspeech.html": {
    "title": "Rhythmic Convergence in Canadian French Varieties?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/manghat20_interspeech.html": {
    "title": "Malayalam-English Code-Switched: Grapheme to Phoneme System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hutin20_interspeech.html": {
    "title": "Ongoing Phonologization of Word-Final Voicing Alternations in Two Romance Languages: Romanian and French",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hope20_interspeech.html": {
    "title": "Cues for Perception of Gender in Synthetic Voices and the Role of Identity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/menshikova20_interspeech.html": {
    "title": "Phonetic Entrainment in Cooperative Dialogues: A Case of Russian",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/xu20g_interspeech.html": {
    "title": "Prosodic Characteristics of Genuine and Mock (Im)polite Mandarin Utterances",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20ja_interspeech.html": {
    "title": "Tone Variations in Regionally Accented Mandarin",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yang20k_interspeech.html": {
    "title": "F0 Patterns in Mandarin Statements of Mandarin and Cantonese Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chuang20b_interspeech.html": {
    "title": "SpeechBERT: An Audio-and-Text Jointly Learned Language Model for End-to-End Spoken Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kuo20b_interspeech.html": {
    "title": "An Audio-Enriched BERT-Based Framework for Spoken Multiple-Choice Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/huang20e_interspeech.html": {
    "title": "Entity Linking for Short Text Using Structured Knowledge Graph via Multi-Grained Text Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20ga_interspeech.html": {
    "title": "Sound-Image Grounding Based Focusing Mechanism for Efficient Automatic Spoken Language Acquisition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yamamoto20_interspeech.html": {
    "title": "Semi-Supervised Learning for Character Expression of Spoken Dialogue Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shi20h_interspeech.html": {
    "title": "Dimensional Emotion Prediction Based on Interactive Context in Conversation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/atamna20_interspeech.html": {
    "title": "HRI-RNN: A User-Robot Dynamics-Oriented RNN for Engagement Decrease Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/fuscone20_interspeech.html": {
    "title": "Neural Representations of Dialogical History for Improving Upcoming Turn Acoustic Parameters Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hu20i_interspeech.html": {
    "title": "Detecting Domain-Specific Credibility and Expertise in Text and Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/das20c_interspeech.html": {
    "title": "The Attacker's Perspective on Automatic Speaker Verification: An Overview",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/sholokhov20_interspeech.html": {
    "title": "Extrapolating False Alarm Rates in Automatic Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/jiang20b_interspeech.html": {
    "title": "Self-Supervised Spoofing Audio Detection Scheme",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20fa_interspeech.html": {
    "title": "Inaudible Adversarial Perturbations for Targeted Attack in Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/villalba20_interspeech.html": {
    "title": "x-Vectors Meet Adversarial Attacks: Benchmarking Adversarial Robustness in Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20ha_interspeech.html": {
    "title": "Black-Box Attacks on Spoofing Countermeasures Using Transferability of Adversarial Examples",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/n20_interspeech.html": {
    "title": "Multimodal Emotion Recognition Using Cross-Modal Attention and 1D Convolutional Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/manakul20_interspeech.html": {
    "title": "Abstractive Spoken Document Summarization Using Hierarchical Model with Multi-Stage Attention Diversity Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20ia_interspeech.html": {
    "title": "Improved Learning of Word Embeddings with Word Definitions and Semantic Injection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20ga_interspeech.html": {
    "title": "Wake Word Detection with Alignment-Free Lattice-Free MMI",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/nguyen20d_interspeech.html": {
    "title": "Improving Vietnamese Named Entity Recognition from Speech Using Word Capitalization and Punctuation Recovery Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yadav20b_interspeech.html": {
    "title": "End-to-End Named Entity Recognition from English Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mckenna20_interspeech.html": {
    "title": "Semantic Complexity in End-to-End Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tran20c_interspeech.html": {
    "title": "Analysis of Disfluency in Children's Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mittal20_interspeech.html": {
    "title": "Representation Based Meta-Learning for Few-Shot Spoken Intent Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/agarwal20b_interspeech.html": {
    "title": "Complementary Language Model and Parallel Bi-LRNN for False Trigger Mitigation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/liu20u_interspeech.html": {
    "title": "Speaker-Utterance Dual Attention for Speaker and Utterance Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yi20b_interspeech.html": {
    "title": "Adversarial Separation and Adaptation Network for Far-Field Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/han20b_interspeech.html": {
    "title": "MIRNet: Learning Multiple Identities Representations in Overlapped Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lin20l_interspeech.html": {
    "title": "Strategies for End-to-End Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hautamaki20_interspeech.html": {
    "title": "Why Did the x-Vector System Miss a Target Speaker? Impact of Acoustic Mismatch Upon Target Score on VoxCeleb Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/afshan20b_interspeech.html": {
    "title": "Variable Frame Rate-Based Data Augmentation to Handle Speaking-Style Variability for Automatic Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/seurin20_interspeech.html": {
    "title": "A Machine of Few Words: Interactive Speaker Recognition with Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/granqvist20_interspeech.html": {
    "title": "Improving On-Device Speaker Verification Using Federated Learning with Privacy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ramoji20_interspeech.html": {
    "title": "Neural PLDA Modeling for End-to-End Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/opatka20_interspeech.html": {
    "title": "State Sequence Pooling Training of Acoustic Models for Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hard20_interspeech.html": {
    "title": "Training Keyword Spotting Models on Non-IID Data with Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/huang20f_interspeech.html": {
    "title": "Class LM and Word Mapping for Contextual Biasing in End-to-End ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/borgholt20_interspeech.html": {
    "title": "Do End-to-End Speech Recognition Models Care About Context?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kumar20f_interspeech.html": {
    "title": "Utterance Confidence Measure for End-to-End Speech Recognition with Applications to Distributed Speech Recognition Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wu20o_interspeech.html": {
    "title": "Speaker Code Based Speaker Adaptive Training Using Model Agnostic Meta-Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhu20c_interspeech.html": {
    "title": "Domain Adaptation Using Class Similarity for Robust Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/novitasari20_interspeech.html": {
    "title": "Incremental Machine Speech Chain Towards Enabling Listening While Speaking in Real-Time",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/raissi20_interspeech.html": {
    "title": "Context-Dependent Acoustic Modeling Without Explicit Phone Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shahnawazuddin20_interspeech.html": {
    "title": "Voice Conversion Based Data Augmentation to Improve Children's Speech Recognition in Limited Data Scenario",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/karlapati20_interspeech.html": {
    "title": "CopyCat: Many-to-Many Fine-Grained Prosody Transfer for Neural Text-to-Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lin20m_interspeech.html": {
    "title": "Joint Detection of Sentence Stress and Phrase Boundary for Prosody",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kulkarni20_interspeech.html": {
    "title": "Transfer Learning of the Expressivity Using FLOW Metric Learning in Multispeaker Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/bae20_interspeech.html": {
    "title": "Speaking Speed Control of End-to-End Speech Synthesis Using Sentence-Level Conditioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tyagi20_interspeech.html": {
    "title": "Dynamic Prosody Generation for Speech Synthesis Using Linguistics-Driven Acoustic Embedding Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kenter20_interspeech.html": {
    "title": "Improving the Prosody of RNN-Based English Text-To-Speech Synthesis by Incorporating a BERT Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhao20g_interspeech.html": {
    "title": "Improved Prosody from Learned F0 Codebook Representations for VQ-VAE Speech Waveform Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zeng20b_interspeech.html": {
    "title": "Prosody Learning Mechanism for Speech Synthesis System Without Text Length Limit",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shirahata20_interspeech.html": {
    "title": "Discriminative Method to Extract Coarse Prosodic Structure and its Application for Statistical Phrase/Accent Command Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/raitio20_interspeech.html": {
    "title": "Controllable Neural Text-to-Speech Synthesis Using Intuitive Prosodic Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/morrison20_interspeech.html": {
    "title": "Controllable Neural Prosody Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/whitehill20_interspeech.html": {
    "title": "Multi-Reference Neural TTS Stylization with Adversarial Cycle Consistency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gao20e_interspeech.html": {
    "title": "Interactive Text-to-Speech System via Joint Style Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hirschi20_interspeech.html": {
    "title": "Mobile-Assisted Prosody Training for Limited English Proficiency: Learner Background and Speech Learning Pattern",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/niekerk20_interspeech.html": {
    "title": "Finding Intelligible Consonant-Vowel Sounds Using High-Quality Articulatory Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/krishnamohan20_interspeech.html": {
    "title": "Audiovisual Correspondence Learning in Humans and Machines",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lan20_interspeech.html": {
    "title": "Perception of English Fricatives and Affricates by Advanced Chinese Learners of English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tsukada20_interspeech.html": {
    "title": "Perception of Japanese Consonant Length by Native Speakers of Korean Differing in Japanese Learning Experience",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ng20b_interspeech.html": {
    "title": "Automatic Detection of Phonological Errors in Child Speech Using Siamese Recurrent Autoencoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ding20e_interspeech.html": {
    "title": "A Comparison of English Rhythm Produced by Native American Speakers and Mandarin ESL Primary School Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhou20h_interspeech.html": {
    "title": "Cross-Linguistic Interaction Between Phonological Categorization and Orthography Predicts Prosodic Effects in the Acquisition of Portuguese Liquids by L1-Mandarin Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20ka_interspeech.html": {
    "title": "Cross-Linguistic Perception of Utterances with Willingness and Reluctance in Mandarin by Korean L2 Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/cheng20b_interspeech.html": {
    "title": "Speech Enhancement Based on Beamforming and Post-Filtering by Combining Phase Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20ha_interspeech.html": {
    "title": "A Noise-Aware Memory-Attention Network Architecture for Regression-Based Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/su20b_interspeech.html": {
    "title": "HiFi-GAN: High-Fidelity Denoising and Dereverberation Based on Speech Deep Features in Adversarial Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/pandey20_interspeech.html": {
    "title": "Learning Complex Spectral Mapping for Speech Enhancement with Improved Cross-Corpus Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/richter20_interspeech.html": {
    "title": "Speech Enhancement with Stochastic Temporal Convolutional Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gogate20_interspeech.html": {
    "title": "Visual Speech In Real Noisy Environments (VISION): A Novel Benchmark Dataset and Deep Learning-Based Baseline System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/sivaraman20_interspeech.html": {
    "title": "Sparse Mixture of Local Experts for Efficient Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kishore20_interspeech.html": {
    "title": "Improved Speech Enhancement Using TCN with Multiple Encoder-Decoder Layers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/fan20e_interspeech.html": {
    "title": "Joint Training for Simultaneous Speech Denoising and Dereverberation with Deep Embedding Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/fontaine20_interspeech.html": {
    "title": "Unsupervised Robust Speech Enhancement Based on Alpha-Stable Fast Multichannel Nonnegative Matrix Factorization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/albes20_interspeech.html": {
    "title": "Squeeze for Sneeze: Compact Neural Networks for Cold and Flu Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/seneviratne20_interspeech.html": {
    "title": "Extended Study on the Use of Vocal Tract Variables to Quantify Neuromotor Coordination in Depression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/xezonaki20_interspeech.html": {
    "title": "Affective Conditioning on Hierarchical Attention Networks Applied to Depression Detection from Transcribed Clinical Interviews",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/huang20g_interspeech.html": {
    "title": "Domain Adaptation for Enhancing Speech-Based Depression Detection in Natural Environmental Conditions Using Dilated CNNs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gosztolya20b_interspeech.html": {
    "title": "Making a Distinction Between Schizophrenia and Bipolar Disorder Based on Temporal Parameters in Spontaneous Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/huckvale20_interspeech.html": {
    "title": "Prediction of Sleepiness Ratings from Voice by Man and Machine",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/teplansky20_interspeech.html": {
    "title": "Tongue and Lip Motion Patterns in Alaryngeal Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/yue20b_interspeech.html": {
    "title": "Autoencoder Bottleneck Features with Multi-Task Optimisation for Improved Continuous Dysarthric Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mallela20_interspeech.html": {
    "title": "Raw Speech Waveform Based Classification of Patients with ALS, Parkinson's Disease and Healthy Controls Using CNN-BLSTM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/pompili20b_interspeech.html": {
    "title": "Assessment of Parkinson's Disease Medication State Through Automatic Speech Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhang20ja_interspeech.html": {
    "title": "Improving Replay Detection System with Channel Consistency DenseNeXt for the ASVspoof 2019 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/falkowskigilski20_interspeech.html": {
    "title": "Subjective Quality Evaluation of Speech Signals Transmitted via BPL-PLC Wired System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chiu20_interspeech.html": {
    "title": "Investigating the Visual Lombard Effect with Gabor Based Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/huang20h_interspeech.html": {
    "title": "Exploration of Audio Quality Assessment and Anomaly Localisation Using Attention Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ragano20_interspeech.html": {
    "title": "Development of a Speech Quality Database Under Uncontrolled Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/algayres20_interspeech.html": {
    "title": "Evaluating the Reliability of Acoustic Speech Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20la_interspeech.html": {
    "title": "Frame-Level Signal-to-Noise Ratio Estimation Using Deep Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/dong20_interspeech.html": {
    "title": "A Pyramid Recurrent Network for Predicting Crowdsourced Speech-Quality Ratings of Real-World Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/brueggeman20_interspeech.html": {
    "title": "Effect of Spectral Complexity Reduction and Number of Instruments on Musical Enjoyment with Cochlear Implants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kosmider20_interspeech.html": {
    "title": "Spectrum Correction: Acoustic Scene Classification with Mismatched Recording Devices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/oconnor20_interspeech.html": {
    "title": "Distributed Summation Privacy for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/leschanowsky20_interspeech.html": {
    "title": "Perception of Privacy Measured in the Crowd — Paired Comparison on the Effect of Background Noises",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kreuk20b_interspeech.html": {
    "title": "Hide and Speak: Towards Deep Neural Networks for Speech Steganography",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/daubener20_interspeech.html": {
    "title": "Detecting Adversarial Examples for Speech Recognition via Uncertainty Quantification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/adelani20_interspeech.html": {
    "title": "Privacy Guarantees for De-Identifying Text Transformations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/jayashankar20_interspeech.html": {
    "title": "Detecting Audio Attacks on ASR Systems with Dropout Uncertainty",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/huang20i_interspeech.html": {
    "title": "Voice Transformer Network: Sequence-to-Sequence Voice Conversion Using Transformer with Text-to-Speech Pretraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/suda20_interspeech.html": {
    "title": "Nonparallel Training of Exemplar-Based Voice Conversion System Using INCA-Based Alignment Technique",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chen20s_interspeech.html": {
    "title": "Enhancing Intelligibility of Dysarthric Speech Using Gated Convolutional-Based Voice Conversion System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wu20p_interspeech.html": {
    "title": "VQVC+: One-Shot Voice Conversion by Vector Quantization and U-Net Architecture",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/park20e_interspeech.html": {
    "title": "Cotatron: Transcription-Guided Speech Encoder for Any-to-Many Voice Conversion Without Parallel Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/fu20c_interspeech.html": {
    "title": "Dynamic Speaker Representations Adjustment and Decoder Factorization for Speaker Adaptation in End-to-End Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lian20d_interspeech.html": {
    "title": "ARVC: An Auto-Regressive Voice Conversion System Without Parallel Training Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/nercessian20_interspeech.html": {
    "title": "Improved Zero-Shot Voice Conversion Using Explicit Conditioning Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chen20t_interspeech.html": {
    "title": "Non-Parallel Voice Conversion with Fewer Labeled Data by Conditional Generative Adversarial Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/liu20v_interspeech.html": {
    "title": "Transferring Source Style in Non-Parallel Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/albadawy20_interspeech.html": {
    "title": "Voice Conversion Using Speech-to-Speech Neuro-Style Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20ia_interspeech.html": {
    "title": "Improving Cross-Lingual Transfer Learning for End-to-End Speech Recognition with Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/thomas20_interspeech.html": {
    "title": "Transliteration Based Data Augmentation for Training Multilingual ASR Acoustic Models in Low Resource Settings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhu20d_interspeech.html": {
    "title": "Multilingual Speech Recognition with Self-Attention Structured Parameterization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/madikeri20_interspeech.html": {
    "title": "Lattice-Free Maximum Mutual Information Training of Multilingual Speech Recognition Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/pratap20c_interspeech.html": {
    "title": "Massively Multilingual ASR: 50 Languages, 1 Model, 1 Billion Parameters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/sailor20_interspeech.html": {
    "title": "Multilingual Speech Recognition Using Language-Specific Phoneme Recognition as Auxiliary Task for Indian Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chandu20_interspeech.html": {
    "title": "Style Variation as a Vantage Point for Code-Switching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lu20f_interspeech.html": {
    "title": "Bi-Encoder Transformer Network for Mandarin-English Code-Switching Speech Recognition Using Mixture of Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/sharma20c_interspeech.html": {
    "title": "Improving Low Resource Code-Switched ASR Using Augmented Code-Switched TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/qiu20c_interspeech.html": {
    "title": "Towards Context-Aware End-to-End Code-Switching Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/dinh20b_interspeech.html": {
    "title": "Increasing the Intelligibility and Naturalness of Alaryngeal Speech Using Voice Conversion and Synthetic Fundamental Frequency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tong20b_interspeech.html": {
    "title": "Automatic Assessment of Dysarthric Severity Level Using Audio-Video Cross-Modal Approach in Deep Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lin20n_interspeech.html": {
    "title": "Staged Knowledge Distillation for End-to-End Dysarthric Speech Recognition and Speech Attribute Transcription",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/takashima20_interspeech.html": {
    "title": "Dysarthric Speech Recognition Based on Deep Metric Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/degala20_interspeech.html": {
    "title": "Automatic Glottis Detection and Segmentation in Stroboscopic Videos Using Convolutional Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/pan20c_interspeech.html": {
    "title": "Acoustic Feature Extraction with Interpretable Deep Neural Network for Neurodegenerative Related Disorder Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/sharma20d_interspeech.html": {
    "title": "Coswara — A Database of Breathing, Cough, and Voice Sounds for COVID-19 Diagnosis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/rowe20_interspeech.html": {
    "title": "Acoustic-Based Articulatory Phenotypes of Amyotrophic Lateral Sclerosis and Parkinson's Disease: Towards an Interpretable, Hypothesis-Driven Framework of Motor Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/alhinti20_interspeech.html": {
    "title": "Recognising Emotions in Dysarthric Speech Using Typical Speech Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/halpern20_interspeech.html": {
    "title": "Detecting and Analysing Spontaneous Oral Cancer Speech in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/dunbar20_interspeech.html": {
    "title": "The Zero Resource Speech Challenge 2020: Discovering Discrete Subword and Word Units",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/niekerk20b_interspeech.html": {
    "title": "Vector-Quantized Neural Networks for Acoustic Unit Discovery in the ZeroSpeech 2020 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ds20_interspeech.html": {
    "title": "Exploration of End-to-End Synthesisers for Zero Resource Speech Challenge 2020",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gundogdu20_interspeech.html": {
    "title": "Vector Quantized Temporally-Aware Correspondence Sparse Autoencoders for Zero-Resource Acoustic Unit Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tjandra20_interspeech.html": {
    "title": "Transformer VQ-VAE for Unsupervised Unit Discovery and Speech Synthesis: ZeroSpeech 2020 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/morita20_interspeech.html": {
    "title": "Exploring TTS Without T Using Biologically/Psychologically Motivated Neural Network Modules (ZeroSpeech 2020)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tobing20_interspeech.html": {
    "title": "Cyclic Spectral Modeling for Unsupervised Unit Discovery into Voice Conversion with Excitation and Waveform Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/chen20u_interspeech.html": {
    "title": "Unsupervised Acoustic Unit Representation Learning for Voice Conversion Using WaveNet Auto-Encoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/rasanen20_interspeech.html": {
    "title": "Unsupervised Discovery of Recurring Speech Patterns Using Probabilistic Adaptive Metrics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/bhati20_interspeech.html": {
    "title": "Self-Expressing Autoencoders for Unsupervised Spoken Term Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/millet20_interspeech.html": {
    "title": "Perceptimatic: A Human Speech Perception Benchmark for Unsupervised Subword Modelling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/clayton20_interspeech.html": {
    "title": "Decoding Imagined, Heard, and Spoken Speech: Classification and Regression of EEG Using a 14-Channel Dry-Contact Mobile Headset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/m20b_interspeech.html": {
    "title": "Glottal Closure Instants Detection from EGG Signal by Classification Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20ma_interspeech.html": {
    "title": "Classify Imaginary Mandarin Tones with Cortical EEG Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/effendi20_interspeech.html": {
    "title": "Augmenting Images for ASR and TTS Through Single-Loop and Dual-Loop Multimodal Chain Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/augustyniak20_interspeech.html": {
    "title": "Punctuation Prediction in Spontaneous Conversations: Can We Mitigate ASR Errors with Retrofitted Word Embeddings?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/sunkara20_interspeech.html": {
    "title": "Multimodal Semi-Supervised Learning Framework for Punctuation Prediction in Conversational Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/huang20j_interspeech.html": {
    "title": "Efficient MDI Adaptation for n-Gram Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/peyser20_interspeech.html": {
    "title": "Improving Tail Performance of a Deliberation E2E ASR Model Using a Large Text Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/ogawa20_interspeech.html": {
    "title": "Language Model Data Augmentation Based on Text Domain Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wok20_interspeech.html": {
    "title": "Contemporary Polish Language Model (Version 2) Using Big Data and Sub-Word Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/pandey20b_interspeech.html": {
    "title": "Improving Speech Recognition of Compound-Rich Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wills20_interspeech.html": {
    "title": "Language Modeling for Speech Analytics in Under-Resourced Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/han20c_interspeech.html": {
    "title": "An Early Study on Intelligent Analysis of Speech Under COVID-19: Severity, Sleep Quality, Fatigue, and Anxiety",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/baird20_interspeech.html": {
    "title": "An Evaluation of the Effect of Anxiety on Speech — Computational Prediction of Anxiety from Sustained Vowels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhao20h_interspeech.html": {
    "title": "Hybrid Network Feature Extraction for Depression Assessment from Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/pan20d_interspeech.html": {
    "title": "Improving Detection of Alzheimer's Disease Using Automatic Speech Recognition to Identify High-Quality Segments for More Robust Feature Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/romana20_interspeech.html": {
    "title": "Classification of Manifest Huntington Disease Using Vowel Distortion Measures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kadiri20_interspeech.html": {
    "title": "Parkinson's Disease Detection from Speech Using Single Frequency Filtering Cepstral Coefficients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/quintas20_interspeech.html": {
    "title": "Automatic Prediction of Speech Intelligibility Based on X-Vectors in the Context of Head and Neck Cancer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/abraham20_interspeech.html": {
    "title": "Spectral Moment and Duration of Burst of Plosives in Speech of Children with Hearing Impairment and Typically Developing Children — A Comparative Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/perez20_interspeech.html": {
    "title": "Aphasic Speech Recognition Using a Mixture of Speech Intelligibility Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/kodrasi20_interspeech.html": {
    "title": "Automatic Discrimination of Apraxia of Speech and Dysarthria Using a Minimalistic Set of Handcrafted Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/shi20i_interspeech.html": {
    "title": "Weak-Attention Suppression for Transformer Based Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/huang20k_interspeech.html": {
    "title": "Conv-Transformer Transducer: Low Latency, Low Frame Rate, Streamable End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/li20na_interspeech.html": {
    "title": "Improving Transformer-Based Speech Recognition with Unsupervised Pre-Training and Multi-Task Semantic Knowledge Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hori20_interspeech.html": {
    "title": "Transformer-Based Long-Context End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhou20i_interspeech.html": {
    "title": "Self-and-Mixed Attention Decoder with Deep Acoustic Structure for Transformer-Based LVCSR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhao20i_interspeech.html": {
    "title": "Universal Speech Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/tian20c_interspeech.html": {
    "title": "Spike-Triggered Non-Autoregressive Transformer for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhao20j_interspeech.html": {
    "title": "Cross Attention with Monotonic Alignment for Speech Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/gulati20_interspeech.html": {
    "title": "Conformer: Convolution-augmented Transformer for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/lu20g_interspeech.html": {
    "title": "Exploring Transformers for Large-Scale Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/togami20_interspeech.html": {
    "title": "Sparseness-Aware DOA Estimation with Majorization Minimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/zhong20c_interspeech.html": {
    "title": "Spatial Resolution of Early Reflection for Speech and White Noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/raikar20_interspeech.html": {
    "title": "Effect of Microphone Position Measurement Error on RIR and its Impact on Speech Intelligibility and Quality",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/deng20c_interspeech.html": {
    "title": "Online Blind Reverberation Time Estimation Using CRNNs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/mack20_interspeech.html": {
    "title": "Single-Channel Blind Direct-to-Reverberation Ratio Estimation Using Masking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/beiton20_interspeech.html": {
    "title": "The Importance of Time-Frequency Averaging for Binaural Speaker Localization in Reverberant Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/hu20j_interspeech.html": {
    "title": "Acoustic Signal Enhancement Using Relative Harmonic Coefficients: Spherical Harmonics Domain Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/murthy20_interspeech.html": {
    "title": "Instantaneous Time Delay Estimation of Broadband Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/wang20ja_interspeech.html": {
    "title": "U-Net Based Direct-Path Dominance Test for Robust Direction-of-Arrival Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2020/xue20b_interspeech.html": {
    "title": "Sound Event Localization and Detection Based on Multiple DOA Beamforming and Multi-Task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  }
}